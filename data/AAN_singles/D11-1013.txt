Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 140?150,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsDomain-Assisted Product Aspect Hierarchy Generation: TowardsHierarchical Organization of Unstructured Consumer ReviewsJianxing Yu1, Zheng-Jun Zha1, Meng Wang1, Kai Wang2, Tat-Seng Chua11School of Computing, National University of Singapore2Institute for Infocomm Research, Singapore{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg kwang@i2r.a-star.edu.sgAbstractThis paper presents a domain-assisted ap-proach to organize various aspects of a prod-uct into a hierarchy by integrating domainknowledge (e.g., the product specifications),as well as consumer reviews.
Based on thederived hierarchy, we generate a hierarchicalorganization of consumer reviews on variousproduct aspects and aggregate consumer opin-ions on these aspects.
With such organiza-tion, user can easily grasp the overview ofconsumer reviews.
Furthermore, we apply thehierarchy to the task of implicit aspect identi-fication which aims to infer implicit aspects ofthe reviews that do not explicitly express thoseaspects but actually comment on them.
Theexperimental results on 11 popular products infour domains demonstrate the effectiveness ofour approach.1 IntroductionWith the rapidly expanding e-commerce, most retailWeb sites encourage consumers to write reviews toexpress their opinions on various aspects of prod-ucts.
Huge collections of consumer reviews arenow available on the Web.
These reviews have be-come an important resource for both consumers andfirms.
Consumers commonly seek quality informa-tion from online consumer reviews prior to purchas-ing a product, while many firms use online reviewsas an important resource in their product develop-ment, marketing, and consumer relationship man-agement.
However, the reviews are disorganized,leading to the difficulty in information navigationand knowledge acquisition.
It is impractical for userto grasp the overview of consumer reviews and opin-ions on various aspects of a product from such enor-mous reviews.
Among hundreds of product aspects,it is also inefficient for user to browse consumer re-views and opinions on a specific aspect.
Thus, thereis a compelling need to organize consumer reviews,so as to transform the reviews into a useful knowl-edge structure.
Since the hierarchy can improve in-formation representation and accessibility (Cimiano,2006), we propose to organize the aspects of a prod-uct into a hierarchy and generate a hierarchical or-ganization of consumer reviews accordingly.Towards automatically deriving an aspect hierar-chy from the reviews, we could refer to traditionalhierarchy generation methods in ontology learning,which first identify concepts from the text, thendetermine the parent-child relations between theseconcepts using either pattern-based or clustering-based methods (Murthy et al, 2010).
However,pattern-based methods usually suffer from inconsis-tency of parent-child relationships among the con-cepts, while clustering-based methods often resultin low accuracy.
Thus, by directly utilizing thesemethods to generate an aspect hierarchy from con-sumer reviews, the resulting hierarchy is usually in-accurate, leading to unsatisfactory review organiza-tion.
On the other hand, domain knowledge of prod-ucts is now available on the Web.
For example,there are more than 248,474 product specificationsin the product sellingWeb site CNet.com (Beckham,2005).
These product specifications cover someproduct aspects and provide coarse-grained parent-child relations among these aspects.
Such domainknowledge is useful to help organize the product as-140Figure 1: Sample hierarchical organization for iPhone 3Gpects into a hierarchy.
However, the initial hierarchyobtained from domain knowledge usually cannot fitthe review data well.
For example, the initial hierar-chy is usually too coarse and may not cover the spe-cific aspects commented in the reviews, while someaspects in the hierarchy may not be of interests tousers in the reviews.Motivated by the above observations, we proposein this paper to organize the product aspects into ahierarchy by simultaneously exploiting the domainknowledge (e.g., the product specification) and con-sumer reviews.
With derived aspect hierarchy, wegenerate a hierarchical organization of consumer re-views on various aspects and aggregate consumeropinions on these aspects.
Figure 1 illustrates a sam-ple of hierarchical review organization for the prod-uct ?iPhone 3G?.
With such organization, users caneasily grasp the overview of product aspects as wellas conveniently navigate the consumer reviews andopinions on any aspect.
For example, users can findthat 623 reviews, out of 9,245 reviews, are about theaspect ?price?, with 241 positive and 382 negativereviews.Given a collection of consumer reviews on a spe-cific product, we first automatically acquire an ini-tial aspect hierarchy from domain knowledge andidentify the aspects from the reviews.
Based on theinitial hierarchy, we develop a multi-criteria opti-mization approach to construct an aspect hierarchyto contain all the identified aspects.
Our approachincrementally inserts the aspects into the initial hi-erarchy based on inter-aspect semantic distance, ametric used to measure the semantic relation amongaspects.
In order to derive reliable semantic dis-tance, we propose to leverage external hierarchies,sampled from WordNet and Open Directory Project,to assist semantic distance learning.
With resultantaspect hierarchy, the consumer reviews are then or-ganized to their corresponding aspect nodes in thehierarchy.
We then perform sentiment classificationto determine consumer opinions on these aspects.Furthermore, we apply the hierarchy to the task ofimplicit aspect identification.
This task aims to inferimplicit aspects of the reviews that do not explic-itly express those aspects but actually comment onthem.
For example, the implicit aspect of the review?It is so expensive?
is ?price.?
Most existing aspectidentification approaches rely on the appearance ofaspect terms, and thus are not able to handle implicitaspect problem.
Based on our aspect hierarchy, wecan infer the implicit aspects by clustering the re-views into their corresponding aspect nodes in thehierarchy.
We conduct experiments on 11 popularproducts in four domains.
More details of the corpusare discussed in Section 4.
The experimental resultsdemonstrate the effectiveness of our approach.The main contributions of this work can be sum-marized as follows:1) We propose to hierarchically organize con-sumer reviews according to an aspect hierarchy, soas to transfer the reviews into a useful knowledgestructure.2) We develop a domain-assisted approach togenerate an aspect hierarchy by integrating domainknowledge and consumer reviews.
In order to de-rive reliable semantic distance between aspects, wepropose to leverage external hierarchies to assist se-mantic distance learning.3) We apply the aspect hierarchy to the task of im-plicit aspect identification, and achieve satisfactoryperformance.The rest of this paper is organized as follows.
Ourapproach is elaborated in Section 2 and applied toimplicit aspect identification in Section 3.
Section4 presents the evaluations, while Section 5 reviews141related work.
Finally, Section 6 concludes this paperwith future works.2 ApproachOur approach consists of four components, includ-ing initial hierarchy acquisition, aspect identifica-tion, semantic distance learning, and aspect hierar-chy generation.
Next, we first define some prelimi-nary and notations and then elaborate these compo-nents.2.1 Preliminary and NotationsPreliminary 1.
An aspect hierarchy is defined as atree that consists of a set of unique aspects A anda set of parent-child relations R between these as-pects.Given the consumer reviews of a product, letA = {a1, ?
?
?
, ak} denotes the product aspects com-mented in the reviews.
H0(A0,R0) denotes the ini-tial hierarchy derived from domain knowledge.
Itcontains a set of aspects A0 and relations R0.
Ourtask is to construct an aspect hierarchy H(A,R), tocover all the aspects in A and their parent-child re-lations R, so that the consumer reviews are hierar-chically organized.
Note that H0 can be empty.2.2 Initial Hierarchy AcquisitionAs aforementioned, product specifications on prod-uct selling websites cover some product aspects andcoarse-grained parent-child relations among theseaspects.
Such domain knowledge is useful to helporganize aspects into a hierarchy.
We here employthe approach proposed by Ye and Chua (2006) to au-tomatically acquire an initial aspect hierarchy fromthe product specifications.
The method first identi-fies the Web page region covering product descrip-tions and removes the irrelevant contents from theWeb page.
It then parses the region containing theproduct information to identify the aspects as well astheir structure.
Based on the aspects and their struc-ture, it generates an aspect hierarchy.2.3 Aspect IdentificationTo identify aspects in consumer reviews, we firstparse each review using the Stanford parser 1.
Sincethe aspects in consumer reviews are usually noun1http://nlp.stanford.edu/software/lex-parser.shtmlFigure 2: Sample Pros and Cons reviewsor noun phrases (Liu, 2009), we extract the nounphrases (NP) from the parse tree as aspect candi-dates.
While these candidates may contain muchnoise, we leverage Pros and Cons reviews (see Fig-ure 2), which are prevalent in forum Web sites,to assist identify aspects from the candidates.
Ithas been shown that simply extracting the frequentnoun terms from the Pros and Cons reviews can gethigh accurate aspect terms (Liu el al., 2005).
Thus,we extract the frequent noun terms from Pros andCons reviews as features, then train a one-class SVM(Manevitz et al, 2002) to identify aspects from thecandidates.
While the obtained aspects may con-tain some synonym terms, such as ?earphone?
and?headphone?, we further perform synonym cluster-ing to get unique aspects.
Specifically, we first ex-pand each aspect term with its synonym terms ob-tained from the synonym termsWeb site 2, then clus-ter them to obtain unique aspects based on unigramfeature.2.4 Semantic Distance LearningOur aspect hierarchy generation approach is essen-tially based on the semantic relations among as-pects.
We here define a metric, Semantic Distance,d(ax, ay), to quantitatively measure the semantic re-lation between aspects ax and ay.
We formulated(ax, ay) as the weighted sum of some underlyingfeatures,d(ax, ay) =?jwjfj(ax, ay), (1)where wj is the weight for j-th feature functionfj(?
).Next, we first introduce the linguistic featuresused in our work and then present the semantic dis-tance learning algorithm that aims to find the opti-mal weights in Eq.
(1).2http://thesaurus.com1422.4.1 Linguistic FeaturesGiven two aspects ax and ay, a feature is definedas a function generating a numeric score f(ax, ay)or a vector of scores.
The features include Contex-tual, Co-occurrence, Syntactic, Pattern and Lexicalfeatures (Yang and Callan, 2009).
These features aregenerated based on auxiliary documents collectedfrom Web.Specifically, we issue each aspect term and aspectterm pair as queries to Google and Wikipedia, re-spectively, and collect the top 100 returned docu-ments of each query.
We then split the documentsinto sentences.
Based on these documents and sen-tences, the features are generated as follows.Contextual features.
For each aspect, we collectthe documents containing the aspect as context tobuild a unigram language model without smoothing.Given two aspects, the KL-divergence between theirlanguage models is computed as the Global-Contextfeature between them.
Similarly, we collect the lefttwo and right two words surrounding each aspect ascontext and build a unigram language model.
TheKL-divergence between the language models of twoaspects is defined as the Local-Context feature.Co-occurrence features.
We measure the co-occurrence of two aspects by Pointwise MutualInformation (PMI): PMI(ax,ay)=log(Count(ax,ay)/Count(ax) Count(ay)), where Count(?)
stands for thenumber of documents or sentences containing theaspect(s), or the number of Google document hitsfor the aspect(s).
Based on different definitions ofCount(?
), we define the features of Document PMI,Sentence PMI, and Google PMI, respectively.Syntactic features.
We parse the sentences thatcontain each aspect pair into syntactic trees via theStanford Parser.
The Syntactic-path feature is de-fined as the average length of the shortest syntacticpath between the aspect pair in the tree.
In addi-tion, for each aspect, we collect a set of sentencescontaining it, and label the semantic role of the sen-tences via ASSERT parser 3.
Given two aspects,the number of the Subject terms overlaps betweentheir sentence sets is computed as the Subject Over-lap feature.
Similarly, for other semantic roles, suchas objects, modifiers, and verbs, we define the fea-tures of Object Overlap, Modifier Overlap, and Verb3http://cemantix.org/assert.htmlOverlap, respectively.Pattern features.
46 patterns are used in ourwork, including 6 patterns indicating the hypernymrelations of two aspects (Hearst, 1992), and 40 pat-terns measuring the part-of relations of two aspects(Girju et al, 2006).
These pattern features areasymmetric, and they take the parent-child relationsamong the aspects into consideration.
All the pat-terns are listed in Appendix A (submitted as supple-mentary material).
Based on these patterns, a 46-dimensional score vector is obtained for each aspectpair.
A score is 1 if two aspects match a pattern, and0 otherwise.Lexical features.
We take the word length differ-ence between two aspects, as Length Difference fea-ture.
In addition, we issue the query ?define:aspect?to Google, and collect the definition of each aspect.We then count the word overlaps between the defini-tions of two aspects, as Definition Overlap feature.2.4.2 Semantic Distance LearningThis section elaborates the learning algorithmthat optimizes the semantic distance metric, i.e.,the weighting parameters in Eq.(1).
Typically, wecan utilize the initial hierarchy as training data.The ground-truth distance between two aspectsdG(ax, ay) is generated by summing up all the edgedistances along the shortest path between ax and ay,where every edge weight is assumed as 1.
The dis-tance metric is then obtained by solving the follow-ing optimization problem,argminwj |mj=1?ax,ay?A0x<y(dG(ax, ay) ?m?j=1wjfj(ax, ay))2+?
?m?j=1w2j ,(2)where m is the dimension of linguistic feature, ?
isa tradeoff parameter.
Eq.
(2) can be rewrote to itsmatrix form as,argminw??d?
fTw?
?2 + ?
?
?w?2 , (3)where vector d contains the ground-truth distance ofall the aspect pairs.
Each element corresponds tothe distance of certain aspect pair, and f is the corre-sponding feature vector.
The optimal solution of wis given asw?
= (fT f + ?
?
I)?1(fTd) (4)143where I is the identity metric.The above learning algorithm can perform wellwhen sufficient training data (i.e., aspect (term)pairs) is available.
However, the initial hierarchy isusually too coarse and thus cannot provide sufficientinformation.
On the other hand, abundant hand-crafted hierarchies are available on the Web, suchas WordNet and Open Directory Project (ODP).
Wehere propose to leverage these external hierarchiesto assist semantic distance learning.
A distance met-ric w0 is learned from the external hierarchies us-ing the above algorithm.
Since w0 might be biasedto the characteristics of the external hierarchies, di-rectly using w0 in our task may not perform well.Alternatively, we use w0 as prior knowledge to as-sist learning the optimal distance metric w from theinitial hierarchy.
The learning problem is formulatedas follows,argminw??d?
fTw?
?2 + ?
?
?w?2 + ?
?
?w?
w0?2 ,(5)where ?
and ?
are tradeoff parameters.The optimal solution of w can be obtained asw?
= (fT f + (?
+ ?)
?
I)?1(fTd + ?
?
w0).
(6)As a result, we can compute the semantic distancebetween each two aspects according to Eq.
(1).2.5 Aspect Hierarchy GenerationGiven the aspectsA = {a1, ?
?
?
, ak} identified fromreviews and the initial hierarchy H0(A0,R0) ob-tained from domain knowledge, our task is to con-struct an aspect hierarchy to contain all the aspectsin A.
Inspired by Yang and Callan (2009), we adopta multi-criteria optimization approach to incremen-tally insert the aspects into appropriate positions inthe hierarchy based on multiple criteria.Before going to the details, we first introduce aninformation function to measure the amount of in-formation carried in a hierarchy.
An informationfunction Info(H) is defined as the sum of the se-mantic distances of all the aspect pairs in the hierar-chy (Yang and Callan, 2009).Info(H(A,R)) =?x<y;ax,ay?Ad(ax, ay).
(7)Based on this information function, we then intro-duce the following three criteria for aspect insertion:minimum Hierarchy Evolution, minimum HierarchyDiscrepancy and minimum Semantic Inconsistency.Hierarchy Evolution is designed to monitor thestructure evolution of a hierarchy.
The hierarchy isincrementally hosting more aspects until all the as-pects are allocated.
The insertion of a new aspect ainto different positions in the current hierarchy H(i)leads to different new hierarchies.
Among these newhierarchies, we here assume that the optimal oneH(i+1) should introduce the least changes of infor-mation to H(i).H?
(i+1) = argminH(i+1)?Info(H(i+1) ?H(i)).
(8)By plugging in Eq.
(7) and using least square tomeasure the information changes, we have,obj1 = argminH(i+1)(?x<y;ax,ay?Ai?
{a} d(ax, ay)?
?x<y;ax,ay?Ai d(ax, ay))2, (9)Hierarchy Discrepancy is used to measure theglobal changes of the structure.
We assume a goodhierarchy should bring the least changes to the initialhierarchy,H?
(i+1) = argminH(i+1)?Info(H(i+1) ?H(0))i + 1 .
(10)We then get,obj2 = argminH(i+1)1i+1(?x<y;ax,ay?Ai?
{a} d(ax, ay)?
?x<y;ax,ay?A0 d(ax, ay))2.
(11)Semantic Inconsistency is introduced to quantifythe inconsistency between the semantic distance es-timated via the hierarchy and that computed fromthe feature functions.
We assume that a good hier-archy should precisely reflect the semantic distancebetween aspects.
For two aspects, their semanticdistance reflected by the hierarchy is computed asthe sum of adjacent distances along the shortest pathbetween them,dH(ax, ay) =?p<q;(ap,aq)?SP (ax,ay)d(ap, aq),(12)where SP (ax, ay) is the shortest path between theaspects (ax, ay), (ap, aq) are the adjacent nodesalong the path.144We then define the following criteria to find thehierarchy with minimum semantic inconsistency,obj3 = argminH(i+1)?x<y;ax,ay?Ai?
{a};(dH(ax, ay)?d(ax, ay))2,(13)where d(ax, ay) is the distance computed based onthe feature functions in Section 2.4.Through integrating the above criteria, the multi-criteria optimization framework is formulated as,obj = argminH(i+1)(?1 ?
obj1 + ?2 ?
obj2 + ?3 ?
obj3)?1 + ?2 + ?3 = 1; 0 ?
?1, ?2, ?3 ?
1.
(14)where ?1, ?2, ?3 are the tradeoff parameters.To summarize, our aspect hierarchy generationprocess starts from an initial hierarchy and insertsthe aspects into it one-by-one until all the aspectsare allocated.
Each aspect is inserted to the op-timal position found by Eq.(14).
It is worth not-ing that the insertion order may influence the result.To avoid such influence, we select the aspect withthe least objective function value in Eq.
(14) to in-sert.
Based on resultant hierarchy, the consumer re-views are then organized to their corresponding as-pect nodes in the hierarchy.
We further prune out thenodes without reviews from the hierarchy.Moreover, we perform sentiment classification todetermine consumer opinions on various aspects.
Inparticular, we train a SVM sentiment classifier basedon the Pros and Cons reviews described in Section2.3.
We collect sentiment terms in the reviews asfeatures and represent reviews as feature vectors us-ing Boolean weighting.
Note that we define senti-ment terms as those appear in the sentiment lexiconprovided by MPQA project (Wilson et al, 2005).3 Implicit Aspect IdentificationIn this section, we apply the aspect hierarchy to thetask of implicit aspect identification.
This task aimsto infer the aspects of reviews that do not explic-itly express those aspects but actually comment onthem (Liu et al 2005).
Take the review ?The phoneis too large?
as an example, the task is to infer itsimplicit aspect ?size.?
It has been observed that thereviews commenting on a same aspect usually usesome same sentiment terms (Su et al, 2008).
There-fore, sentiment term is an effective feature for identi-fying implicit aspects.
We here collect the sentimentterms as features to represent each review into a fea-ture vector.
For each aspect node in the hierarchy,we define its centroid as the average of its featurevectors, i.e., the feature vectors of all the reviewsthat are allocated at this node.
We then calculatethe cosine similarity of each implicit-aspect reviewto the centriods of all the aspect nodes, and allo-cate the review into the node with maximum sim-ilarity.
As a result, the implicit aspect reviews aregrouped to their related aspect nodes.
In other word,their aspects are identified as the corresponding as-pect nodes.4 EvaluationsIn this section, we evaluate the effectiveness of ourapproach on aspect identification, aspect hierarchygeneration, and implicit aspect identification.4.1 Data and Experimental SettingThe details of our product review corpus are givenin Table 1.
This corpus contains consumer reviewson 11 popular products in four domains.
Thesereviews were crawled from several prevalent fo-rum Web sites, including cnet.com, viewpoints.com,reevoo.com and gsmarena.com.
All of the reviewswere posted between June, 2009 and Sep 2010.
Theaspects of the reviews, as well as the opinions onthe aspects were manually annotated.
We also in-vited five annotators to construct the gold-standardhierarchies for the products by providing them theinitial hierarchies and the aspects in reviews.
Theconflicts between annotators were resolved throughtheir discussions.
For semantic distance learning, wecollected 50 hierarchies from WordNet and ODP, re-spectively.
The details are shown in Table 2.
Welisted the topics of the hierarchies in Appendix B(submitted as supplementary material).Product Name Domain Review# Sentence#Canon EOS 450D (Canon EOS) camera 440 628Fujifilm Finepix AX245W (Fujifilm) camera 541 839Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546Apple MacBook Pro (MacBook) laptop 552 4,221Samsung NC10 (Samsung) laptop 2,712 4,946Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001Nokia N95 phone 15,939 44,379Table 1: Statistics of the reviews corpus, # denotes thesize of the reviews/sentences145Statistic WordNet ODPTotal # hierarchies 50 50Total # terms 1,964 2,210Average # depth 5.5 5.9Total # related topics 12 16Table 2: Statistics of the External HierarchiesFigure 3: Evaluations on Aspect Identification.
t-test, p-values<0.05We employed F1-measure, which is the combina-tion of precision and recall, as the evaluation metricfor all the evaluations.
For the evaluation on aspecthierarchy, we defined precision as the percentage ofcorrectly returned parent-child pairs out of the to-tal returned pairs, and recall as the percentage ofcorrectly returned parent-child pairs out of the to-tal pairs in the gold standard.
Throughout the ex-periments, we empirically set ?1 = 0.4, ?2 = 0.3,?3 = 0.3, ?
= 0.4 and ?
= 0.6.4.2 Evaluations on Aspect IdentificationWe compared our approach against two state-of-the-art methods: a) the method proposed by Hu and Liu(2004), which is based on the association rule min-ing, and b) the method proposed byWu et al (2009),which is based on the dependency parser.
The re-sults are presented in Figure 3.
On average, ourapproach significantly outperforms Hu?s and Wu?smethod in terms of F1-measure by over 5.87% and3.27%, respectively.4.3 Evaluations on Aspect Hierarchy4.3.1 Comparisons with the State-of-the-ArtsWe compared our approach against four tra-ditional hierarchy generation methods in the re-searches on ontology learning, including a) pattern-based method (Hearst, 1992) and b) clustering-basedmethod by Shi et al (2008), c) the method proposedFigure 4: Evaluations on Aspect Hierarchy Generation.
t-test, p-values<0.05.
w/ H denotes the methods with ini-tial hierarchy, accordingly, w/o H refers to the methodswithout initial hierarchy.by Snow et al (2006) which was based on a proba-bilistic model, and d) the method proposed by Yangand Callan (2009).
Since our approach and Yang?smethod can utilize initial hierarchy to assist hier-archy generation, we evaluated their performancewith or without initial hierarchy, respectively.
Forthe sake of fair comparison, Snow?s, Yang?s and ourmethods used the same linguistic features in Section2.4.1.Figure 4 shows the performance comparisonsof these five methods.
We can see that our ap-proach without using initial hierarchy outperformsthe pattern-based, clustering-based, Snow?s, andYang?s methods by over 17.9%, 19.8%, 2.9% and6.1% respectively in terms of average F1-measure.By exploiting initial hierarchy, our approach im-proves the performance significantly.
As comparedto the pattern-based, clustering-based and Snow?smethods, it improves the average performance byover 49.4%, 51.2% and 34.3% respectively.
Com-pared to Yang?s method with initial hierarchy, itachieves 4.7% improvements on the average perfor-mance.The results show that pattern-based andclustering-based methods perform poor.
Pattern-based method may suffer from the problem of lowcoverage of patterns, especially when the patternsare manually pre-defined, while the clustering-based method (Shi et al, 2008) may sustain to thebisection clustering mechanism which can onlygenerate a binary-tree.
The results also illustratethat our approach outperforms Snow?s and Yang?smethods.
By exploiting external hierarchies, our146Figure 5: Evaluations on the Impact of Initial Hierarchy.t-test, p-values<0.05.approach is able to derive reliable semantic distancebetween aspects and thus improve the performance.4.3.2 Evaluations on Effectiveness of InitialHierarchyIn this section, we show that even based on a smallpart of the initial hierarchy, our approach can stillgenerate a satisfactory hierarchy.
We explored dif-ferent proportion of initial hierarchy, including 0%,20%, 40%, 60% and 80% of the aspect pairs whichare collected top-down from the initial hierarchy.
Asshown in Figure 5, the performance increases whenlarger proportion of the initial hierarchy is used.Thus, we can speculate that domain knowledge isvaluable in aspect hierarchy generation.4.3.3 Evaluations on Effectiveness ofOptimization CriteriaWe conducted a leave-one-out study to evaluatethe effectiveness of each optimization criterion.
Inparticular, we set one of the tradeoff parameters (?1,?2, ?3) in Eq.
(14) to zero, and distributed its weightto the rest parameters averagely.
From Figure 6, wefind that removing any optimization criterion woulddegrade the performance on most products.
It is in-teresting to note that removing the third optimiza-tion criterion, i.e., minimum semantic inconsistency,slightly increases the performance on two products(ipad touch and sony MP3).
The reason might bethat the values of the three tradeoff parameters (em-pirically set in Section 4.1) are not suitable for thesetwo products.Figure 6: Evaluations of the Optimization Criteria.
% ofchange in F1-measure when a single criterion is removed.t-test, p-values<0.05.Figure 7: Evaluations on the Impact of Linguistic Fea-tures.
t-test, p-values<0.05.4.3.4 Evaluations on Semantic DistanceLearningIn this section, we evaluated the impact of the fea-tures and external hierarchies in semantic distancelearning.
We investigated five sets of features as de-scribed in Section 2.4.1, including contextual, co-occurrence, syntactic, pattern and lexical features.From Figure 7, we observe that the co-occurrenceand pattern features perform much better than con-textual and syntactic features.
A possible reasonis that co-occurrence and pattern features are morelikely to indicate parent-child aspect relationships,while contextual and syntactic features are proba-ble to measure sibling aspect relationships.
Amongthese features, the lexical features perform the worst.The combination of all the features achieves the bestperformance.Next, we evaluated the effectiveness of externalhierarchies in semantic distance learning.
We com-pared the performance of our approach with or with-out the external hierarchies.
From Figure 8, we findthat by exploiting the external hierarchies, our ap-147Figure 8: Evaluations on the Impact of External Hierar-chy.
t-test, p-values<0.05.proach improves the performance significantly.
Theimprovement is over 2.81% in terms of average F1-measure.
This implies that by using external hier-archies, our approach can obtain effective semanticdistance, and thus improve the performance of as-pect hierarchy generation.Additionally, for sentiment classification, ourSVM classifier achieves an average F1-measure of0.787 in the 11 products.4.4 Evaluations on Implicit AspectIdentificationTo evaluate the performance of our approach on im-plicit aspect identification, we collected 29,657 im-plicit aspect review sentences on the 11 productsfrom the four forum Web sites introduced in Section4.1.
While most existing approaches for implicit as-pect identification rely on hand-crafted rules (Liu,2009), the method proposed in Su et al (2008) canidentify implicit aspects without hand-crafted rulesbased on mutual clustering.
Therefore, we adoptSu?s method as the baseline here.
Figure 9 illustratesthe performance comparison between Su?s and ourapproach.
We can see that our approach outperformsSu?s method by over 9.18% in terms of average F1-measure.
This shows that our approach can iden-tify the implicit aspects accurately by exploiting theunderlying associations among the sentiment termsand each aspect in the hierarchy.5 Related WorkSome researches treated review organization as amulti-document summarization problem, and gen-erated a summary by selecting and ordering sen-tences taken from multiple reviews (Nishikawa etFigure 9: Evaluations on Implicit Aspects Identification.t-test, p-values<0.05al., 2010).
These works did not drill down to thefine-grained level to explore the opinions on theproduct aspects.
Other researchers proposed to pro-duce a summary covering consumer opinions oneach aspect.
For example, Hu and Liu (2004) fo-cused on extracting the aspects and determiningopinions on the aspects.
However, their gener-ated summary was unstructured, where the possiblerelationships between aspects were not recognized(Cadilhac et al, 2010).
Subsequently, Carenini etal.
(2006) proposed to map the aspect to a user-defined taxonomy, but the taxonomy was hand-crafted which was not scalable.Different from the previous works, we focus onautomatically generating an aspect hierarchy to hi-erarchically organize consumer reviews.
There aresome related works on ontology learning, whichfirst identify concepts from text, and then determineparent-child relations between these concepts us-ing either pattern-based or clustering-based methods(Murthy et al, 2010).
Pattern-based methods usu-ally defined some lexical syntactic patterns to extractthe relations, while clustering-based methods mostlyutilized the hierarchical clustering methods to builda hierarchy (Roy et al, 2006).
Some works proposedto integrate the pattern-based and clustering-basedmethods in a general model, such as the probabilisticmodel (Snow et al, 2006) and metric-based model(Yang and Callan, 2009).The researches on aspect identification are alsorelated to our work.
Various aspect identificationmethods have been proposed (Popescu et al, 2005),including supervised methods (Liu el al., 2005), andunsupervised methods (Mei et al, 2007).
Different148features have been investigated for this task.
Forexample, Wu et al (2009) identified aspects basedon the features explored by dependency parser.For implicit aspect identification, some works pro-posed to define rules for identification (Liu el al.,2005), while others suggested to automatically gen-erate rules via mutual clustering (Su et al, 2008).On the other hand, there are some related workson sentiment classification (Pang and Lee, 2008).These works can be categorized into four granu-larities: document-level, sentence-level, aspect-leveland word-level sentiment classification (Liu, 2009).Existing researches have been studied unsupervised(Kim et al, 2004), supervised (Pang et al, 2002;Pang et al, 2005) and semi-supervised classificationapproaches (Goldberg et al, 2006; Li et al, 2009)on these four levels.6 Conclusions and Future WorksIn this paper, we have developed a domain-assistedapproach to generate product aspect hierarchy by in-tegrating domain knowledge and consumer reviews.Based on the derived hierarchy, we can generatea hierarchical organization of consumer reviews aswell as consumer opinions on the aspects.
With suchorganization, user can easily grasp the overview ofconsumer reviews, as well as seek consumer reviewsand opinions on any specific aspect by navigatingthrough the hierarchy.
We have further applied thehierarchy to the task of implicit aspect identification.We have conducted evaluations on 11 different prod-ucts in four domains.
The experimental results havedemonstrated the effectiveness of our approach.
Inthe future, we will explore other linguistic featuresto learn the semantic distance between aspects, aswell as apply our approach to other applications.AcknowledgmentsThis work is supported by NUS-Tsinghua ExtremeSearch (NExT) project under the grant number: R-252-300-001-490.
We give warm thanks to theproject and anonymous reviewers for their valuablecomments.ReferencesP.
Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.An Exploration of Sentiment Summarization.
AAAI,2003.J.
Beckham.
The Cnet E-commerce Data set.
TechnicalReports, 2005.G.
Carenini, R. Ng, and E. Zwart.
Multi-document Sum-marization of Evaluative Text.
ACL, 2006.A.
Cadilhac, F. Benamara, and N. Aussenac-Gilles.
On-tolexical Resources for Feature based OpinionMining:a Case-study.
Ontolex, 2010.P.
Cimiano, A. Madche, S. Staab, and J. Volker.
OntologyLearning.
Handbook on Ontologies, Springer, 2004.P.
Cimiano, A. Hotho, and S. Staab.
Learning ConceptHierarchies from Text Corpora using Formal ConceptAnalysis.
Artificial Intelligence, 2005.P.
Cimiano.
Ontology Learning and Population fromText: Algorithms, Evaluation and Applications.Springer-Verlag New York, Inc. Secaucus, NJ, USA,2006.S.
Dasgupta and V. Ng.
Mine the Easy, Classify the Hard:A Semi-supervised Approach to Automatic SentimentClassification.
ACL, 2009.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.
Un-supervised Named-entity Extraction from theWeb: AnExperimental Study.
Artificial Intelligence, 2005.A.
Esuli and F. Sebastiani.
A Publicly Available LexicalResource for Opinion Mining.
LREC, 2006.M.
Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.Pulse: Mining Customer Opinions from Free Text.IDA, 2005.R.
Girju and A. Badulescu.
Automatic Discovery of Part-whole Relations Computational Linguistics, 2006.A.
Goldberg and X. Zhu.
Seeing Stars When ThereAren?t Many Stars: Graph-based Semi-supervisedLearning for Sentiment Categorization.
ACL, 2006.M.A.
Hearst.
Automatic Acquisition of Hyponyms fromLarge Text Corpora.
Coling, 1992.M.
Hu and B. Liu.
Mining and Summarizing CustomerReviews.
SIGKDD, 2004.X.
Hu, N. Sun, C. Zhang, and T.-S. Chua ExploitingInternal and External Semantics for the Clustering ofShort Texts Using World Knowledge.
CIKM, 2009.S.
Kim and E. Hovy.
Determining the Sentiment of Opin-ions.
COLING, 2004.A.
C. Konig and E. Brill.
Reducing the Human Overheadin Text Categorization.
KDD, 2006.Z.
Kozareva, E. Riloff, and E. Hovy.
Semantic ClassLearning from the Web with Hyponym Pattern Link-age Graphs.
ACL, 2008.T.
Li, Y. Zhang, and V. Sindhwani.
A Non-negative Ma-trix Tri-factorization Approach to Sentiment Classifi-cation with Lexical Prior Knowledge.
ACL, 2009.B.
Liu, M. Hu, and J. Cheng.
Opinion Observer: Ana-lyzing and Comparing Opinions on the Web.
WWW,2005.149B.
Liu.
Handbook Chapter: Sentiment Analysis and Sub-jectivity.
Handbook of Natural Language Processing.Marcel Dekker, Inc. New York, NY, USA, 2009.L.M.Manevitz andM.
Yousef.
One-class SVMs for Doc-ument Classification.
Machine Learning, 2002.Q.Mei, X. Ling, M.Wondra, H. Su, and C.X.
Zhai.
TopicSentiment Mixture: Modeling Facets and Opinions inWeblogs.
WWW, 2007.X.
Meng and H. Wang.
Mining User Reviews: fromSpecification to Summarization.
ACL-IJCNLP, 2009.K.
Murthy, T.A.
Faruquie, L.V.
Subramaniam,K.H.
Prasad, and M. Mohania.
AutomaticallyGenerating Term-frequency-induced Taxonomies.ACL, 2010.H.
Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.Optimizing Informativeness and Readability for Senti-ment Summarization.
ACL, 2010.B.
Pang, L. Lee, and S. Vaithyanathan.
Thumbs up?
Sen-timent Classification using Machine Learning Tech-niques.
EMNLP, 2002.B.
Pang and L. Lee.
Seeing Stars: Exploiting Class Rela-tionships for Sentiment Categorization with respect toRating Scales.
ACL, 2005.B.
Pang and L. Lee.
Opinion mining and sentiment anal-ysis.
Foundations and Trends in Information Retrieval,2008.HH.
Pang, J. Shen, and R. Krishnan Privacy-Preserving,Similarity-Based Text Retrieval.
ACM Transactionson Internet Technology, 2010.A.M.
Popescu and O. Etzioni.
Extracting Product Fea-tures and Opinions from Reviews.
HLT/EMNLP,2005.H.
Poon and P. Domingos.
Unsupervised Ontology In-duction from Text.
ACL, 2010.G.
Qiu, B. Liu, J. Bu, and C. Chen.
Expanding DomainSentiment Lexicon through Double Propagation.
IJ-CAI, 2009.S.
Roy and L.V.
Subramaniam.
Automatic Generationof Domain Models for Call Centers from Noisy Tran-scriptions.
ACL, 2009.B.
Shi and K. Chang.
Generating a Concept Hierarchyfor Sentiment Analysis.
SMC, 2008.R.
Snow and D. Jurafsky.
Semantic Taxonomy Inductionfrom Heterogenous Evidence.
ACL, 2006.Q.
Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, andZ.
Su.
Hidden Sentiment Association in Chinese WebOpinion Mining.
WWW, 2008.I.
Titov and R. McDonald.
A Joint Model of Text andAspect Ratings for Sentiment Summarization.
ACL,2008.P.
Turney.
Thumbs up or thumbs down?
Semantic Orien-tation Applied to Unsupervised Classification of Re-views.
ACL, 2002.Y.
Wu, Q. Zhang, X. Huang, and L. Wu.
Phrase Depen-dency Parsing for Opinion Mining.
ACL, 2009.T.
Wilson, J. Wiebe, and P. Hoffmann.
RecognizingContextual Polarity in Phrase-level Sentiment Analy-sis.
HLT/EMNLP, 2005.H.
Yang and J. Callan A Metric-based Framework forAutomatic Taxonomy Induction.
ACL, 2009.S.
Ye and T.-S. Chua.
Learning Object Models fromSemi-structured Web Documents.
IEEE Transactionson Knowledge and Data Engineering, 2006.J.
Yi, T. Nasukawa, W. Niblack, and R. Bunescu.
Senti-ment Analyzer: Extracting Sentiments about a GivenTopic using Natural Language Processing Techniques.ICDM, 2003.L.
Zhuang,F.
Jing, and X.Y.
Zhu Movie Review Miningand Summarization CIKM, 2006.150
