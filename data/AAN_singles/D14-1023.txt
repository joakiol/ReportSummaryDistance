Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsNeural Network Based Bilingual Language Model Growingfor Statistical Machine TranslationRui Wang1,3,?, Hai Zhao1,3, Bao-Liang Lu1,3, Masao Utiyama2and Eiichro Sumita21Center for Brain-Like Computing and Machine Intelligence,Department of Computer Science and Engineering,Shanghai Jiao Tong University, Shanghai, 200240, China2Multilingual Translation Laboratory, MASTAR Project,National Institute of Information and Communications Technology,3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan3Key Laboratory of Shanghai Education Commission for Intelligent Interactionand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, Chinawangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn,{mutiyama, eiichiro.sumita}@nict.go.jpAbstractSince larger n-gram Language Model(LM) usually performs better in StatisticalMachine Translation (SMT), how to con-struct efficient large LM is an importanttopic in SMT.
However, most of the ex-isting LM growing methods need an extramonolingual corpus, where additional LMadaption technology is necessary.
In thispaper, we propose a novel neural networkbased bilingual LM growing method, onlyusing the bilingual parallel corpus in SMT.The results show that our method can im-prove both the perplexity score for LM e-valuation and BLEU score for SMT, andsignificantly outperforms the existing LMgrowing methods without extra corpus.1 Introduction?Language Model (LM) Growing?
refers to addingn-grams outside the corpus together with theirprobabilities into the original LM.
This operationis useful as it can make LM perform better throughletting it become larger and larger, by only using asmall training corpus.There are various methods for adding n-gramsselected by different criteria from a monolingualcorpus (Ristad and Thomas, 1995; Niesler andWoodland, 1996; Siu and Ostendorf, 2000; Si-ivola et al., 2007).
However, all of these approach-es need additional corpora.
Meanwhile the extracorpora from different domains will not result inbetter LMs (Clarkson and Robinson, 1997; Iyer etal., 1997; Bellegarda, 2004; Koehn and Schroeder,?Part of this work was done as Rui Wang visited in NICT.2007).
In addition, it is very difficult or even im-possible to collect an extra large corpus for somespecial domains such as the TED corpus (Cettoloet al., 2012) or for some rare languages.
There-fore, to improve the performance of LMs, withoutassistance of extra corpus, is one of important re-search topics in SMT.Recently, Continues Space Language Model(CSLM), especially Neural Network based Lan-guage Model (NNLM) (Bengio et al., 2003;Schwenk, 2007; Mikolov et al., 2010; Le et al.,2011), is being actively used in SMT (Schwenket al., 2006; Son et al., 2010; Schwenk, 2010;Schwenk et al., 2012; Son et al., 2012; Niehuesand Waibel, 2012).
One of the main advantagesof CSLM is that it can more accurately predic-t the probabilities of the n-grams, which are not inthe training corpus.
However, in practice, CSLM-s have not been widely used in the current SMTsystems, due to their too high computational cost.Vaswani and colleagues (2013) propose amethod for reducing the training cost of CSLMand apply it to SMT decoder.
However, they donot show their improvement for decoding speed,and their method is still slower than the n-gramLM.
There are several other methods for attempt-ing to implement neural network based LM ortranslation model for SMT (Devlin et al., 2014;Liu et al., 2014; Auli et al., 2013).
However, thedecoding speed using n-gram LM is still state-of-the-art one.
Some approaches calculate the prob-abilities of the n-grams n-grams before decoding,and store them in the n-gram format (Wang et al.,2013a; Arsoy et al., 2013; Arsoy et al., 2014).
The?converted CSLM?
can be directly used in SMT.Though more n-grams which are not in the train-189ing corpus can be generated by using some ofthese ?converting?
methods, these methods onlyconsider the monolingual information, and do nottake the bilingual information into account.We observe that the translation output of aphrase-based SMT system is concatenation ofphrases from the phrase table, whose probabilitiescan be calculated by CSLM.
Based on this obser-vation, a novel neural network based bilingual LMgrowing method is proposed using the ?connectingphrases?.
The remainder of this paper is organizedas follows: In Section 2, we will review the exist-ing CSLM converting methods.
The new neuralnetwork based bilingual LM growing method willbe proposed in Section 3.
In Section 4, the exper-iments will be conducted and the results will beanalyzed.
We will conclude our work in Section5.2 Existing CSLM Converting MethodsTraditional Backoff N -gram LMs (BNLMs) havebeen widely used in many NLP tasks (Zhang andZhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013;Zhang et al., 2012; Xu and Zhao, 2012; Wang etal., 2013b; Jia and Zhao, 2013; Wang et al., 2014).Recently, CSLMs become popular because theycan obtain more accurate probability estimation.2.1 Continues Space Language ModelA CSLM implemented in a multi-layer neural net-work contains four layers: the input layer projects(first layer) all words in the context hionto theprojection layer (second layer); the hidden layer(third layer) and the output layer (fourth layer)achieve the non-liner probability estimation andcalculate the LM probability P (wi|hi) for the giv-en context (Schwenk, 2007).CSLM is able to calculate the probabilities ofall words in the vocabulary of the corpus given thecontext.
However, due to too high computationalcomplexity, CSLM is mainly used to calculate theprobabilities of a subset of the whole vocabulary(Schwenk, 2007).
This subset is called a short-list, which consists of the most frequent words inthe vocabulary.
CSLM also calculates the sum ofthe probabilities of all words not included in theshort-list by assigning a neuron with the help ofBNLM.
The probabilities of other words not in theshort-list are obtained from an BNLM (Schwenk,2007; Schwenk, 2010; Wang et al., 2013a).Let wiand hibe the current word and history,respectively.
CSLM with a BNLM calculates theprobability P (wi|hi) of wigiven hi, as follows:P (wi|hi) =??
?Pc(wi|hi)?w?V0Pc(w|hi)Ps(hi) if wi?
V0Pb(wi|hi) otherwise(1)where V0is the short-list, Pc(?)
is the probabil-ity calculated by CSLM,?w?V0Pc(w|hi) is thesummary of probabilities of the neuron for all thewords in the short-list, Pb(?)
is the probability cal-culated by the BNLM, andPs(hi) =?v?V0Pb(v|hi).
(2)We may regard that CSLM redistributes theprobability mass of all words in the short-list,which is calculated by using the n-gram LM.2.2 Existing Converting MethodsAs baseline systems, our approach proposed in(Wang et al., 2013a) only re-writes the probabil-ities from CSLM into the BNLM, so it can onlyconduct a convert LM with the same size as the o-riginal one.
The main difference between our pro-posed method in this paper and our previous ap-proach is that n-grams outside the corpus are gen-erated firstly and the probabilities using CSLM arecalculated by using the same method as our previ-ous approach.
That is, the proposed new methodis the same as our previous one when no grownn-grams are generated.The method developed by Arsoy and colleagues(Arsoy et al., 2013; Arsoy et al., 2014) adds al-l the words in the short-list after the tail word ofthe i-grams to construct the (i+1)-grams.
For ex-ample, if the i-gram is ?I want?, then the (i+1)-grams will be ?I want *?, where ?*?
stands for anyword in the short list.
Then the probabilities ofthe (i+1)-grams are calculated using (i+1)-CSLM.So a very large intermediate (i+1)-grams will haveto be grown1, and then be pruned into smallersuitable size using an entropy-based LM pruningmethod modified from (Stolcke, 1998).
The (i+2)-grams are grown using (i+1)-grams, recursively.1In practice, the probabilities of all the target/tail wordsin the short list for the history i-grams can be calculated bythe neurons in the output layer at the same time, which willsave some time.
According to our experiments, the time costfor Arsoy?s growing method is around 4 times more than ourproposed method, if the LMs which are 10 times larger thanthe original one are grown with other settings all the same.1903 Bilingual LM GrowingThe translation output of a phrase-based SMT sys-tem can be regarded as a concatenation of phrasesin the phrase table (except unknown words).
Thisleads to the following procedure:Step 1.
All the n-grams included in the phrasetable should be maintained at first.Step 2.
The connecting phrases are defined inthe following way.The wbais a target language phrase starting fromthe a-th word ending with the b-th word, and ?wba?is a phrase includingwbaas a part of it, where ?
and?
represent any word sequence or none.
An i-gramphrase wk1wik+1(1 ?
k ?
i ?
1) is a connectingphrase2, if :(1) wk1is the right (rear) part of one phrase ?wk1in the phrase table, or(2) wik+1is the left (front) part of one phrasewik+1?
in the phrase table.After the probabilities are calculated using C-SLM (Eqs.1 and 2), we combine the n-grams inthe phrase table from Step 1 and the connectingphrases from Step 2.3.1 Ranking the Connecting PhrasesSince the size of connecting phrases is too huge(usually more than one Terabyte), it is necessaryto decide the usefulness of connecting phrases forSMT.
The more useful connecting phrases can beselected, by ranking the appearing probabilities ofthe connecting phrases in SMT decoding.Each line of a phrase table can be simplified(without considering other unrelated scores in thephrase table) asf ||| e ||| P (e|f), (3)where the P (e|f) means the translation probabili-ty from f(source phrase) to e(target phrase),which can be calculated using bilingual paralleltraining data.
In decoding, the probability of a tar-get phrase e appearing in SMT should bePt(e) =?fPs(f) ?
P (e|f), (4)2We are aware that connecting phrases can be applied tonot only two phrases, but also three or more.
However the ap-pearing probabilities (which will be discussed in Eq.
5 of nextsubsection) of connecting phrases are approximately estimat-ed.
To estimate and compare probabilities of longer phrasesin different lengths will lead to serious bias, and the experi-ments also showed using more than two connecting phrasesdid not perform well (not shown for limited space), so onlytwo connecting phrases are applied in this paper.where the Ps(f) means the appearing probabilityof a source phrase, which can be calculated usingsource language part in the bilingual training data.Using Pt(e)3, we can select the connectingphrases e with high appearing probabilities asthe n-grams to be added to the original n-grams.
These n-grams are called ?grown n-grams?.
Namely, we build all the connectingphrases at first, and then we use the appearingprobabilities of the connecting phrases to decidewhich connecting phrases should be selected.
Foran i-gram connecting phrasewk1wik+1, wherewk1ispart of ?wk1and wik+1is part of wik+1?
(the ?wk1and wik+1?
are from the phrase table), the prob-ability of the connecting phrases can be roughlyestimated asPcon(wk1wik+1) =i?1?k=1(??Pt(?wk1)???Pt(wik+1?)).
(5)A threshold for Pcon(wk1wik+1) is set, and onlythe connecting phrases whose appearing probabil-ities are higher than the threshold will be selectedas the grown n-grams.3.2 Calculating the Probabilities of GrownN -grams Using CSLMTo our bilingual LM growing method, a 5-gramLM and n-gram (n=2,3,4,5) CSLMs are built byusing the target language of the parallel corpus,and the phrase table is learned from the parallelcorpus.The probabilities of unigram in the original n-gram LM will be maintained as they are.
Then-grams from the bilingual phrase table will begrown by using the ?connecting phrases?
method.As the whole connecting phrases are too huge, weuse the ranking method to select the more usefulconnecting phrases.
The distribution of differentn-grams (n=2,3,4,5) of the grown LMs are set asthe same as the original LM.The probabilities of the grown n-grams(n=2,3,4,5) are calculated using the 2,3,4,5-CSLM, respectively.
If the tail (target) words ofthe grown n-grams are not in the short-list of C-SLM, the Pb(?)
in Eq.
1 will be applied to calcu-late their probabilities.3This Pt(e) hence provides more bilingual information,in comparison with using monolingual target LMs only.191We combine the n-grams (n=1,2,3,4,5) togeth-er and re-normalize the probabilities and backof-f weights of the grown LM.
Finally the originalBNLM and the grown LM are interpolated.
Theentire process is illustrated in Figure 1.CorpusPhrase TableGrown n-gramswith ProbabilitiesGrown LMOutputInputInterpolateGrown n-gramsCSLMBNLMConnectingPhrasesFigure 1: NN based bilingual LM growing.4 Experiments and Results4.1 Experiment Setting upThe same setting up of the NTCIR-9 Chinese toEnglish translation baseline system (Goto et al.,2011) was followed, only with various LMs tocompare them.
The Moses phrase-based SMTsystem was applied (Koehn et al., 2007), togeth-er with GIZA++ (Och and Ney, 2003) for align-ment and MERT (Och, 2003) for tuning on the de-velopment data.
Fourteen standard SMT featureswere used: five translation model scores, one wordpenalty score, seven distortion scores, and one LMscore.
The translation performance was measuredby the case-insensitive BLEU on the tokenized testdata.We used the patent data for the Chinese to En-glish patent translation subtask from the NTCIR-9patent translation task (Goto et al., 2011).
The par-allel training, development, and test data sets con-sist of 1 million (M), 2,000, and 2,000 sentences,respectively.Using SRILM (Stolcke, 2002; Stolcke et al.,2011), we trained a 5-gram LM with the interpo-lated Kneser-Ney smoothing method using the 1MEnglish training sentences containing 42M wordswithout cutoff.
The 2,3,4,5-CSLMs were trainedon the same 1M training sentences using CSLMtoolkit (Schwenk, 2007; Schwenk, 2010).
The set-tings for CSLMs were: input layer of the samedimension as vocabulary size (456K), projectionlayer of dimension 256 for each word, hidden lay-er of dimension 384 and output layer (short-list) ofdimension 8192, which were recommended in theCSLM toolkit and (Wang et al., 2013a)4.4Arsoy used around 55 M words as the corpus, including4.2 ResultsThe experiment results were divided into fourgroups: the original BNLMs (BN), the CSLMRe-ranking (RE), our previous converting (WA),the Arsoy?s growing, and our growing methods.For our bilingual LM growing method, 5 bilingualgrown LMs (BI-1 to 5) were conducted in increas-ing sizes.
For the method of Arsoy, 5 grown LMs(AR-1 to 5) with similar size of BI-1 to 5 were alsoconducted, respectively.For the CSLM re-ranking, we used CSLM tore-rank the 100-best lists of SMT.
Our previousconverted LM, Arsoy?s grown LMs and bilingualgrown LMs were interpolated with the originalBNLMs, using default setting of SRILM5.
To re-duce the randomness of MERT, we used twometh-ods for tuning the weights of different SMT fea-tures, and two BLEU scores are corresponding tothese twomethods.
TheBLEU-s indicated that thesame weights of the BNLM (BN) features wereused for all the SMT systems.
The BLEU-i indi-cated that the MERT was run independently bythree times and the average BLEU scores weretaken.We also performed the paired bootstrap re-sampling test (Koehn, 2004)6.
Two thousandssamples were sampled for each significance test.The marks at the right of the BLEU score indicatedwhether the LMs were significantly better/worsethan the Arsoy?s grown LMs with the same IDsfor SMT (?++/???
: significantly better/worse at?
= 0.01, ?+/??
: ?
= 0.05, no mark: not signif-icantly better/worse at ?
= 0.05).From the results shown in Table 1, we can getthe following observations:(1) Nearly all the bilingual grown LMs outper-formed both BNLM and our previous convertedLM on PPL and BLEU.
As the size of grown LM-s is increased, the PPL always decreased and theBLEU scores trended to increase.
These indicatedthat our proposed method can give better probabil-ity estimation for LM and better performance forSMT.
(2) In comparison with the grown LMs in Ar-84K words as vocabulary, and 20K words as short-list.
In thispaper, we used the same setting as our previous work, whichcovers 92.89% of the frequency of words in the training cor-pus, for all the baselines and our method for fair comparison.5In our previous work, we used the development data totune the weights of interpolation.
In this paper, we used thedefault 0.5 as the interpolation weights for fair comparison.6We used the code available at http://www.ark.cs.cmu.edu/MT192Table 1: Performance of the Grown LMsLMs n-grams PPL BLEU-s BLEU-i ALHBN 73.9M 108.8 32.19 32.19 3.03RE N/A 97.5 32.34 32.42 N/AWA 73.9M 104.4 32.60 32.62 3.03AR-1 217.6M 103.3 32.55 32.75 3.14AR-2 323.8M 103.1 32.61 32.64 3.18AR-3 458.5M 103.0 32.39 32.71 3.20AR-4 565.6M 102.8 32.67 32.51 3.21AR-5 712.2M 102.5 32.49 32.60 3.22BI-1 223.5M 101.9 32.81+ 33.02+ 3.20BI-2 343.6M 101.0 32.92+ 33.11++ 3.24BI-3 464.5M 100.6 33.08++ 33.25++ 3.26BI-4 571.0M 100.3 33.15++ 33.12++ 3.28BI-5 705.5M 100.1 33.11++ 33.24++ 3.31soy?s method, our grown LMs obtained better P-PL and significantly better BLEU with the sim-ilar size.
Furthermore, the improvement of PPLand BLEU of the existing methods became satu-rated much more quickly than ours did, as the LMsgrew.
(3) The last column was the Average Length ofthe n-grams Hit (ALH) in SMT decoding for dif-ferent LMs using the following functionALH =5?i=1Pi?gram?
i, (6)where the Pi?grammeans the ratio of the i-gramshit in SMT decoding.
There were also positivecorrelations between ALH, PPL and BLEUs.
TheALH of bilingual grown LM was longer than thatof the Arsoy?s grown LM of the similar size.
Inanother word, less back-off was used for our pro-posed grown LMs in SMT decoding.4.3 Experiments on TED CorpusThe TED corpus is in special domain as discussedin the introduction, where large extra monolingualcorpora are hard to find.
In this subsection, weconducted the SMT experiments on TED corporausing our proposed LM growing method, to eval-uate whether our method was adaptable to somespecial domains.We mainly followed the baselines of the IWSLT2014 evaluation campaign7, only with a few mod-ifications such as the LM toolkits and n-gram or-der for constructing LMs.
The Chinese (CN) toEnglish (EN) language pair was chosen, using de-v2010 as development data and test2010 as evalu-ation data.
The same LM growing method was ap-7https://wit3.fbk.eu/plied on TED corpora as on NTCIR corpora.
Theresults were shown in Table 2.Table 2: CN-EN TED ExperimentsLMs n-grams PPL BLEU-sBN 7.8M 87.1 12.41WA 7.8M 85.3 12.73BI-1 23.1M 79.2 12.92BI-2 49.7M 78.3 13.16BI-3 73.4M 77.6 13.24Table 2 indicated that our proposed LM grow-ing method improved both PPL and BLEU in com-parison with both BNLM and our previous CSLMconverting method, so it was suitable for domainadaptation, which is one of focuses of the currentSMT research.5 ConclusionIn this paper, we have proposed a neural networkbased bilingual LM growing method by using thebilingual parallel corpus only for SMT.
The resultsshow that our proposed method can improve bothLM and SMT performance, and outperforms theexisting LM growing methods significantly with-out extra corpus.
The connecting phrase-basedmethod can also be applied to LM adaptation.AcknowledgmentsWe appreciate the helpful discussion with Dr.Isao Goto and Zhongye Jia, and three anony-mous reviewers for valuable comments and sug-gestions on our paper.
Rui Wang, Hai Zhaoand Bao-Liang Lu were partially supported bythe National Natural Science Foundation of Chi-na (No.
60903119, No.
61170114, and No.61272248), the National Basic Research Programof China (No.
2013CB329401), the Science andTechnology Commission of Shanghai Municipali-ty (No.
13511500200), the European Union Sev-enth Framework Program (No.
247619), the CaiYuanpei Program (CSC fund 201304490199 and201304490171), and the art and science interdis-cipline funds of Shanghai Jiao Tong University(A study on mobilization mechanism and alertingthreshold setting for online community, and mediaimage and psychology evaluation: a computation-al intelligence approach).
The corresponding au-thor of this paper, according to the meaning givento this role by Shanghai Jiao Tong University, isHai Zhao.193ReferencesEbru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,and Abhinav Sethy.
2013.
Converting neural net-work language models into back-off language mod-els for efficient decoding in automatic speech recog-nition.
In Proceedings of ICASSP-2013, Vancouver,Canada, May.
IEEE.Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,and Abhinav Sethy.
2014.
Converting neural net-work language models into back-off language mod-els for efficient decoding in automatic speech recog-nition.
IEEE/ACM Transactions on Audio, Speech,and Language, 22(1):184?192.Michael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint language and translationmodeling with recurrent neural networks.
In Pro-cessings of EMNLP-2013, pages 1044?1054, Seat-tle, Washington, USA, October.
Association forComputational Linguistics.Jerome R Bellegarda.
2004.
Statistical language mod-el adaptation: review and perspectives.
SpeechCommunication, 42(1):93?108.
Adaptation Meth-ods for Speech Recognition.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search (JMLR), 3:1137?1155, March.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
Wit3: Web inventory of transcribedand translated talks.
In Proceedings of EAMT-2012,pages 261?268, Trento, Italy, May.Philip Clarkson and A.J.
Robinson.
1997.
Lan-guage model adaptation using mixtures and an ex-ponentially decaying cache.
In Proceedings ofICASSP-1997, volume 2, pages 799?802 vol.2, Mu-nich,Germany.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of ACL-2014, pages 1370?1380, Baltimore, Maryland, June.Association for Computational Linguistics.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the paten-t machine translation task at the NTCIR-9 work-shop.
In Proceedings of NTCIR-9 Workshop Meet-ing, pages 559?578, Tokyo, Japan, December.Rukmini Iyer, Mari Ostendorf, and Herbert Gish.1997.
Using out-of-domain data to improve in-domain language models.
Signal Processing Letter-s, IEEE, 4(8):221?223.Zhongye Jia and Hai Zhao.
2013.
Kyss 1.0: aframework for automatic evaluation of chinese inputmethod engines.
In Proceedings of IJCNLP-2013,pages 1195?1201, Nagoya, Japan, October.
AsianFederation of Natural Language Processing.Zhongye Jia and Hai Zhao.
2014.
A joint graph mod-el for pinyin-to-chinese conversion with typo cor-rection.
In Proceedings of ACL-2014, pages 1512?1523, Baltimore, Maryland, June.
Association forComputational Linguistics.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of ACL-2007 Workshopon Statistical Machine Translation, pages 224?227,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertol-di, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of ACL-2007, pages 177?180,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Philipp Koehn.
2004.
Statistical significance testsfor machine translation evaluation.
In Proceedingsof EMNLP-2004, pages 388?395, Barcelona, Spain,July.
Association for Computational Linguistics.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-vain, and Franc?ois Yvon.
2011.
Structured outputlayer neural network language model.
In Proceed-ings of ICASSP-2011, pages 5524?5527, Prague,Czech Republic, May.
IEEE.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou.
2014.A recursive recurrent neural network for statisticalmachine translation.
In Proceedings of ACL-2014,pages 1491?1500, Baltimore, Maryland, June.
As-sociation for Computational Linguistics.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Re-current neural network based language model.
InProceedings of INTERSPEECH-2010, pages 1045?1048.Jan Niehues and Alex Waibel.
2012.
Continuousspace language models using restricted boltzman-n machines.
In Proceedings of IWSLT-2012, pages311?318, Hong Kong.Thomas Niesler and Phil Woodland.
1996.
A variable-length category-based n-gram language model.
InProceedings of ICASSP-1996, volume 1, pages 164?167 vol.
1.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmen-t models.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedingsof ACL-2003, pages 160?167, Sapporo, Japan, July.Association for Computational Linguistics.194Eric Sven Ristad and Robert G. Thomas.
1995.
Newtechniques for context modeling.
In Proceedingsof ACL-1995, pages 220?227, Cambridge, Mas-sachusetts.
Association for Computational Linguis-tics.Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-vain.
2006.
Continuous space language models forstatistical machine translation.
In Proceedings ofCOLING ACL-2006, pages 723?730, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, pruned or continuous spacelanguage models on a gpu for statistical machinetranslation.
In Proceedings of the NAACL-HLT 2012Workshop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling forHLT, WLM ?12, pages 11?19, Montreal, Canada,June.
Association for Computational Linguistics.Holger Schwenk.
2007.
Continuous space lan-guage models.
Computer Speech and Language,21(3):492?518.Holger Schwenk.
2010.
Continuous-space languagemodels for statistical machine translation.
ThePrague Bulletin of Mathematical Linguistics, pages137?146.Vesa Siivola, Teemu Hirsimki, and Sami Virpioja.2007.
On growing and pruning kneser-ney s-moothed n-gram models.
IEEE Transactions on Au-dio, Speech, and Language, 15(5):1617?1624.Manhung Siu and Mari Ostendorf.
2000.
Variable n-grams and extensions for conversational speech lan-guage modeling.
IEEE Transactions on Speech andAudio, 8(1):63?75.Le Hai Son, Alexandre Allauzen, Guillaume Wis-niewski, and Franc?ois Yvon.
2010.
Training con-tinuous space language models: some practical is-sues.
In Proceedings of EMNLP-2010, pages 778?788, Cambridge, Massachusetts, October.
Associa-tion for Computational Linguistics.Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of NAACL HLT-2012, pages 39?48, Montreal, Canada, June.
Asso-ciation for Computational Linguistics.Andreas Stolcke, Jing Zheng, Wen Wang, and Vic-tor Abrash.
2011.
SRILM at sixteen: Update andoutlook.
In Proceedings of INTERSPEECH 2011,Waikoloa, HI, USA, December.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proceedings of DARPABroadcast News Transcription and UnderstandingWorkshop, pages 270?274, Lansdowne, VA, USA.Andreas Stolcke.
2002.
Srilm-an extensiblelanguage modeling toolkit.
In Proceedings ofINTERSPEECH-2002, pages 257?286, Seattle, US-A, November.Ashish Vaswani, Yinggong Zhao, Victoria Fossum,and David Chiang.
2013.
Decoding with large-scale neural language models improves translation.In Proceedings of EMNLP-2013, pages 1387?1392,Seattle, Washington, USA, October.
Association forComputational Linguistics.Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-ta, Hai Zhao, and Bao-Liang Lu.
2013a.
Convert-ing continuous-space language models into n-gramlanguage models for statistical machine translation.In Proceedings of EMNLP-2013, pages 845?850,Seattle, Washington, USA, October.
Association forComputational Linguistics.Xiaolin Wang, Hai Zhao, and Bao-Liang Lu.
2013b.Labeled alignment for recognizing textual entail-ment.
In Proceedings of IJCNLP-2013, pages 605?613, Nagoya, Japan, October.
Asian Federation ofNatural Language Processing.Xiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and Bao-Liang Lu.
2014.
Parallelized extreme learning ma-chine ensemble based on minmax modular network.Neurocomputing, 128(0):31 ?
41.Qiongkai Xu and Hai Zhao.
2012.
Using deep lin-guistic features for finding deceptive opinion spam.In Proceedings of COLING-2012, pages 1341?1350,Mumbai, India, December.
The COLING 2012 Or-ganizing Committee.Jingyi Zhang and Hai Zhao.
2013.
Improving functionword alignment with frequency and syntactic infor-mation.
In Proceedings of IJCAI-2013, pages 2211?2217.
AAAI Press.Xiaotian Zhang, Hai Zhao, and Cong Hui.
2012.A machine learning approach to convert CCGbankto Penn treebank.
In Proceedings of COLING-2012, pages 535?542, Mumbai, India, December.The COLING 2012 Organizing Committee.Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-Liang Lu.
2013.
An empirical study on wordsegmentation for chinese machine translation.
InAlexander Gelbukh, editor, Computational Linguis-tics and Intelligent Text Processing, volume 7817 ofLecture Notes in Computer Science, pages 248?263.Springer Berlin Heidelberg.195
