Learning class-to-class selectional preferencesEneko AgirreIXA NLP GroupUniversity of the Basque Country649 pk.
20.080Donostia.
Spain.eneko@si.ehu.esDavid MartinezIXA NLP GroupUniversity of the Basque Country649 pk.
20.080Donostia.
Spain.jibmaird@si.ehu.esAbstractSelectional preference learningmethods have usually focused on word-to-class relations, e.g., a verb selects asits subject a given nominal class.
Thispapers extends previous statisticalmodels to class-to-class preferences,and presents a model that learnsselectional preferences for classes ofverbs.
The motivation is twofold:different senses of a verb may havedifferent preferences, and some classesof verbs can share preferences.
Themodel is tested on a word sensedisambiguation task which usessubject-verb and object-verbrelationships extracted from a smallsense-disambiguated corpus.1 IntroductionPrevious literature on selectional preference hasusually learned preferences for words in theform of classes, e.g., the object of eat is anedible entity.
This paper extends previousstatistical models to classes of verbs, yielding arelation between classes in a hierarchy, asopposed to a relation between a word and aclass.The model is trained using subject-verb andobject-verb associations extracted from Semcor,a corpus (Miller et al, 1993) tagged withWordNet word-senses (Miller et al, 1990).
Thesyntactic relations were extracted using theMinipar parser (Lin, 1993).
A peculiarity of thisexercise is the use of a small sense-disambiguated corpus, in contrast to using alarge corpus of ambiguous words.
We think thattwo factors can help alleviate the scarcity ofdata: the fact that using disambiguated wordsprovides purer data, and the ability to use classesof verbs in the preferences.
Nevertheless, theapproach can be easily extended to larger, non-disambiguated corpora.We have defined a word sensedisambiguation exercise in order to evaluate theextracted preferences, using a sample of wordsand a sample of documents, both from Semcor.Following this short introduction, section 2reviews selectional restriction acquisition.Section 3 explains our approach, which isformalized in sections 4 and 5.
Next, section 6shows the results on the WSD experiment.
Someof the acquired preferences are analysed insection 7.
Finally, some conclusions are drawnand future work is outlined.2 Selectional preference learningSelectional preferences try to capture the factthat linguistic elements prefer arguments of acertain semantic class, e.g.
a verb like ?eat?prefers as object edible things, and as subjectanimate entities, as in, (1) ?She was eating anapple?.
Selectional preferences get morecomplex than it might seem: (2) ?The acid atethe metal?, (3) ?This car eats a lot of gas?, (4)?We ate our savings?, etc.Corpus-based approaches for selectionalpreference learning extract a number of  (e.g.verb/subject) relations from large corpora anduse an algorithm to generalize from the set ofnouns for each verb separately.
Usually, nounsare generalized using classes (concepts) from alexical knowledge base (e.g.
WordNet).Resnik (1992, 1997) defines an information-theoretic measure of the association between averb and nominal WordNet classes: selectionalassociation.
He uses verb-argument pairs fromBrown.
Evaluation is performed applyingintuition and WSD.
Our measure follows in partfrom his formalization.Abe and Li (1995) follow a similar approach,but they employ a different information-theoretic measure (the minimum descriptionlength principle) to select the set of concepts in ahierarchy that generalize best the selectionalpreferences for a verb.
The argument pairs areextracted from the WSJ corpus, and evaluationis performed using intuition and PP-attachmentresolution.Stetina et al (1998) extract word-arg-wordtriples for all possible combinations, and use ameasure of  ?relational probability?
based onfrequency and similarity.
They provide analgorithm to disambiguate all words in asentence.
It is directly applied to WSD withgood results.3 Our approachThe model explored in this paper emerges as aresult of the following observations:?
Distinguishing verb senses can be useful.The examples for eat above are taken fromWordNet, and each corresponds to adifferent word sense1: example (1) is fromthe ?take in solid food?
sense of eat, (2)from the ?cause to rust?
sense, andexamples (3) and (4) from the ?use up?sense.?
If the word senses of a set of verbs aresimilar (e.g.
word senses of ingestion verbslike eat, devour, ingest, etc.)
they can haverelated selectional preferences, and we cangeneralize and say that a class of verbs has aparticular selectional preference.Our formalization thus distinguishes among verbsenses, that is, we treat each verb sense as a11 A note is in order to introduce the terminology used in thepaper.
We use concept and class indistinguishably, andthey refer to the so-called synsets in WordNet.
Concepts inWordNet are represented as sets of synonyms, e.g.
<food,nutrient>.
A word sense in WordNet is a word-conceptpairing, e.g.
given the concepts a=<chicken, poulet,volaille> and b=<wimp, chicken, crybaby> we can saythat chicken has at least two word senses, the pair chicken-a and the pair chicken-b.
In fact the former is sense 1 ofchicken, and the later is sense 3 of chicken.
For the sake ofsimplicity we also talk about <chicken, poulet, volaille>being a word sense of chicken.different unit that has a particular selectionalpreference.
From the selectional preferences ofsingle verb word senses, we also inferselectional preferences for classes of verbs.Contrary to other methods (e.g.
Li andAbe?s), we don?t try to find the classes whichgeneralize best the selectional preferences.
Allpossibilities, even the very low probability ones,are stored.The method stands as follows: we collect[noun-word-sense relation verb-word-sense]triples from Semcor, where the relation is eithersubject or object.
As word senses refer toconcepts, we also collect the triple for eachpossible combination of concepts that subsumethe word senses in the triple.
Direct frequenciesand estimates of frequencies for classes are thenused to compute probabilities for the triples.These probabilities could be used todisambiguate either nouns, verbs or both at thesame time.
For the time being, we have chosento disambiguate nouns only, and therefore wecompute the probability for a nominal concept,given that it is the subject/object of a particularverb.
Note that when disambiguating we ignorethe particular sense in which the governing verboccurs.4 FormalizationAs mentioned in the previous sections we areinterested in modelling the probability of anominal concept given that it is thesubject/object of a particular verb:)|( vrelcnP i  (1)Before providing the formalization for ourapproach we present a model based on wordsand a model based on nominal-classes.
Ourclass-to-class model is an extension of thesecond2.
The estimation of the frequencies ofclasses are presented in the following section.12 Notation: v stands for a verb, cn (cv) stand for nominal(verbal) concept, cni (cvi ) stands for the concept linked tothe i-th sense of the given noun (verb), rel could be anygrammatical relation (in our case object or subject), ?stands for the subsumption relation, fr stands for frequencyand rf?
.for the estimation of the frequencies of classes.4.1 Word-to-word model: eat chickeniAt this stage we do not use information of classsubsumption.
The probability of the first senseof chicken being an object of eat depends onhow often does the concept linked to chicken1appear as object of the word eat, divided by thenumber of occurrences of eat with an object.
)()()|( vrelfrvrelcnfrvrelcnP ii =   (2)Note that instead of )|( vrelsenseP i  we use)|( vrelcnP i , as we count occurrences ofconcepts rather than word senses.
This meansthat synonyms also count, e.g.
poulet assynonyms of the first sense of chicken.4.2 word-to-class model:eat <food, nutrient>The probability of eat chicken1 depends on theprobabilities of the concepts subsumed by andsubsuming chicken1 being objects of eat.
Forinstance, if chicken1 never appears as an objectof eat, but other word senses under <food,nutrient> do, the probability of chicken1 will notbe 0.Formula (3) shows that for all conceptssubsuming cni the probability of cni given themore general concept times the probability ofthe more general concept being a subject/objectof the verb is added.
The first probability isestimated dividing the class frequencies of cniwith the class frequencies of the more generalconcept.
The second probability is estimated asin 4.1.4.3 class-to-class model:<ingest, take in, ?> <food, nutrient>The probability of eat chicken1 depends on theprobabilities of all concepts above chicken1being objects of all concepts above the possiblesenses of eat.
For instance, if devour neverappeared on the training corpus, the model couldinfer its selectional preference from that of its????
?=?=icncnicncnvrelfrvrelcnrfcnrfcncnrfvrelcnPcncnPvrelcnP iii )()(?)(?),(?
)|()|()|(  (3)?
??
??
??
???=?
?=icncn cvcvvsenseofcvicncn cvcvvsenseofcvjjijjjijicvrelfrcvrelcnrfcvrfcvcvrfcnrfcncnrfcvrelcnPcvcvPcncnPvrelcnPmaxmax)()(?)(?),(?)(?),(?)|()|()|()|((4)??
?=cnicniicnfrcnclassescnrf )()(1)(?
(5)?????
??=?
?otherwisecncnifcnfrcnclassescncnrf iij jji cncn0)()(1),(?
(6)??
?=cnicnvrelcnfrcnclassesvrelcnrf ii)()(1)(?
(7)?
??
??
?=cnicn cnicviiiicvrelcnfrcvclassescnclassescvrelcnrf )()(1)(1)(?
(8)superclass <ingest, take in, ...>.
As the verb canbe polysemous, the sense with maximumprobability is selected.Formula (4) shows that the maximumprobability for the possible senses (cvj) of theverb is taken.
For each possible verb concept(cv) and noun concept (cn) subsuming the targetconcepts (cni,cvj), the probability of the targetconcept given the subsuming concept (this isdone twice, once for the verb, once for the noun)times the probability the nominal concept beingsubject/object of the verbal concept is added.5 Estimation of class frequenciesFrequencies for classes can be counted directlyfrom the corpus when the class is linked to aword sense that actually appears in the corpus,written as fr(cni).
Otherwise they have to beestimated using the direct counts for allsubsumed concepts, written as )(?
icnrf .
Formula(5) shows that all the counts for the subsumedconcepts (cni) are added, but divided by thenumber of classes for which ci is a subclass (thatis, all ancestors in the hierarchy).
This isnecessary to guarantee the following:??
icncncncnP i )|( = 1.Formula (6) shows the estimated frequencyof a concept given another concept.
In the caseof the first concept subsuming the second it is 0,otherwise the frequency is estimated as in (5).Formula (7) estimates the counts for[nominal-concept relation verb] triples for allpossible nominal-concepts, which is based onthe counts for the triples that actually occur inthe corpus.
All the counts for subsumedconcepts are added, divided by the number ofclasses in order to guarantee the following:?cnvsubjcnP )|( =1Finally, formula (8) extends formula (7) to[nominal-concept relation verbal-concept] in asimilar way.6 Training and testing on a WSDexerciseFor training we used the sense-disambiguatedpart of Brown, Semcor, which comprises around250.000 words tagged with WordNet wordsenses.
The parser we used is Minipar.
For thiscurrent experiment we only extracted verb-object and verb-subject pairs.
Overall 14.471verb-object pairs and 12.242 verb-subject pairswstclfoexnocoWcoTchleramofth13MNoun # sens # occ# occ.as obj# occ.as subjaccount 10 27 8 3age 5 104 10 9church 3 128 19 10duty 3 25 8 1head 30 179 58 16interest 7 140 31 13member 5 74 13 11people 4 282 41 83Overall  67 959 188 146Table 1.
Data for the selected nouns.Prec.ObjCov.
Rec Prec.SubjCov.
Rec.Random .192 1.00 .192 .192 1.00 .192MFS .690 1.00 .690 .690 1.00 .690Word2word .959 .260 .249 .742 .243 .180Word2class .669 .867 .580 .562 .834 .468Class2class .666 .973 .648 .540 .995 .537Table 2.
Average results for the 8 nouns.
ere extracted.
For the sake of efficiency, weored all possible class-to-class relations andass frequencies at this point, as defined inrmulas (5) to (8).The acquired data was tested on a WSDercise.
The goal was to disambiguate alluns occurring as subjects and objects, but ituld be also used to disambiguate verbs.
TheSD algorithm just gets the frequencies andmputes the probabilities as they are needed.he word sense with the highest probability isosen.Two experiments were performed: on thexical sample we selected a set of 8 nouns atndom3 and applied 10fold crossvalidation toake use of all available examples.
In the casewhole documents, they were withdrawn frome training corpus and tested in turn.This set was also used on a previous paper (Agirre &artinez, 2000).Table 1 shows the data for the set of nouns.Note that only 19% (15%) of the occurrences ofthe nouns are objects (subjects) of any verb.Table 2 shows the average results using subjectand object relations for each possibleformalization.
Each column shows respectively,the precision, the coverage over the occurrenceswith the given relation, and the recall.
Randomand most frequent baselines are also shown.Word-to-word gets the highest precision of allthree, but it can only be applied on a fewinstances.
Word-to-class gets slightly betterprecision than class-to-class, but class-to-class isnear complete coverage and thus gets the bestrecall of all three.
All are well above the randombaseline, but slightly below the most frequentsense.On the all-nouns experiment, wedisambiguated the nouns appearing in four filesextracted from Semcor.
We observed that notmany nouns were related to a verb as object orsubject (e.g.
in the file br-a01 only 40% (16%)of the polisemous nouns were tagged as object(subject)).
Table 3 illustrates the results on thistask.
Again, word-to-word obtains the bestprecision in all cases, but because of the lack ofdata the recall is low.
Class-to-class attains thebest recall.We think that given the small corpusavailable, the results are good.
Note that there isno smoothing or cut-off value involved, andsome decisions are taken with very little pointsof data.
Sure enough both smoothing and cut-offvalues will allow to improve the precision.
Onthe contrary, literature has shown that the mostfrequent sense baseline needs less training data.7 Analysis of the acquired selectionalpreferencesIn order to analyze the acquired selectionalpreferences, we wanted a word that did notoccur too often and which had clearlydistinguishable senses.
The goal is to study thepreferences that were applied in thedisambiguation for all occurrences, and checkwhat is the difference among each of themodels.The selected word was church, which has threesenses in WordNet, and occurs 19 times.
Figure1 shows the three word senses and thecorresponding subsuming concepts.
Table 4shows the results of the disambiguationalgorithm for church.Object SubjectFile Rand.
MFS word2word word2class class2class Rand.
MFS word2word word2class class2classbr-a01 .286 .746 .138 .447 .542 .313 .884 .312 .640 .749br-b20 .233 .776 .093 .418 .487 .292 .780 .354 .580 .677br-j09 .254 .645 .071 .429 .399 .256 .761 .200 .500 .499br-r05 .269 .639 .126 .394 .577 .294 .720 .144 .601 .710Table 3.
Average recall for the nouns in the four Semcor files.Sense 1church, Christian church, Christianity=> religion, faith=> institution, establishment=> organization, organisation=> social group=> group, groupingSense 2church, church building=> place of worship, house of prayer,house of God, house of worship=> building, edifice=> structure, construction=> artifact, artefact=> object, physical object=> entity, somethingSense 3church service, church=> service, religious service, divine service=> religious ceremony, religious ritual=> ceremony=> activity=> act, human action, human activityFigure 1.
Word senses and superclasses for churchIn the word-to-word model, the model isunable to tag any of the examples4 (all the verbsrelated to ?church?
were different).
For churchas object, both class-to-class and word-to-classhave similar recall, but word-to-class has betterprecision.
Notice that the majority of theexamples with church as object were not taggedwith the most frequent sense in Semcor, andtherefore the MFS precision is remarkably low(21%).
For church as subject, the class-to-classmodel has both better precision and coverage.We will now study in more detail each of theexamples.7.1 Church as objectThere were 19 examples with church as object(15 tagged in Semcor with sense 2 and 4 withsense 1).
Using the word-to-class model, 12were tagged correctly, 5 incorrectly and 2 hadnot enough data to answer.
In the class-to-classmodel 12 examples were tagged correctly and 7incorrectly.
Therefore there was no gain inrecall.First, we will analyze the results of theword-to-class model.
From the 12 hits, 10corresponded to sense 2 and the other 2 to sense1.
Here we show the 12 verbs and thesuperconcept of the senses of church that getsthe highest selectional preference probability,and thus selects the winning sense, in this case,correctly.?
Tagged with sense 2:look: <builhave: <buil14 Note that we applied 10fonot able to tag anything bsamples do not appear in tthe verbs governing churchdemolish: <building, edifice>move:  <structure, construction>support:  <structure, construction>build:  <structure, construction>enter:  <structure, construction>sell:  <artifact, artefact>abandon:  <artifact, artefact>see:  <artifact, artefact>?
Tagged with sense 1strengthen:  <organization, organisation>turn_to:  <organization, organisation>The five examples where the model failedrevealed different types of errors.
We will checkeach of the verbs in turn.1.
Attend (Semcor 2, word-to-class 1)5: Wequote the whole sentence:From many sides come remarks thatProtestant churches are badly attended andthe large medieval cathedrals look all butempty during services .We think that the correct sense should be 3 (?church services?
are attended, not thebuildings).
In any case, the class that gets thehigher weight is <institution, establishment>,pointing to sense 1 of church and beating themore appropriate class <religious ceremony,religious ritual> because of the lack ofexamples in the training.2.
Join (Semcor 1, word-to-class 2): It seemsthat this verb should be a good clue for sense 1.But among the few occurrences of join in thetraining set there were ?join-obj-temple?
andoth temple andhave organization-et and they were thusunder <building,se in Semcor (the correctigned by the model.#occ OK KO No ansr Prec.
Cov.
Rec.obj MFS 19 4 15 0 .210 1.00 .210obj word-to-word 19 0 0 19 .000 .000 .000obj word-to-class 19 12 5 2 .705 .894 .631obj class-to-class 19 12 7 0 .631 1.00 .631subj MFS 10 8 2 0 .800 1.00 .800subj word-to-word 10 0 0 10 .000 .000 .000subj word-to-class 10 4 3 3 .571 .700 .400subj class-to-class 10 6 4 0 .600 1.00 .600Table 4: Results disambiguating the word church.
ding, edifice>ding, edifice>ld crossvalidation.
The model isecause the verbs in the testinghe training samples.
In fact alloccur only once.?join-obj-synagogue?.
Bsynagogue have do notrelated concepts in WordNtagged with a concept15 For each verb we list the senreference sense) and the sense assedifice>.
This implies that <place of worship,house of prayer, house of God, house ofworship> gets most credit and the answer issense 2.3.
Imprison (Semcor 1, word-to-class 3): Thescarcity of training examples is very evidenthere.
There are only 2 examples of imprisonwith an object, one of them wrongly selected byMinipar (imprison-obj-trip) that falls under<act, human action, human activity> and pointsto sense 3.4.
Empty (Semcor 2, word-to-class 1): Thedifferent senses of empty introduce misleadingexamples.
The best credit is given to <group,grouping> (following an sense of empty whichis not appropriate here) which selects the sense 1of church.
The correct sense of empty in thiscontext relates with <object, physical object>,and would thus select the correct sense, but doesnot have enough credit.5.
Advance (Semcor 2, word-to-class 3): themisleading senses of ?advance?
and the lownumber of examples point to sense 3.We thus identified 4 sources of error in theword-to-class model:A.
Incorrect Semcor tagB.
Wrongly extracted verb-object relationsC.
Scarcity of dataD.
Misleading verb sensesThe class-to-class model should help tomitigate the effects of errors type C and D. Wewould specially hope for the class-to-classmodel to discard misleading verb senses.
Wenow turn to analyze the results of this model.From the 12 correct examples tagged usingword-to-class, we observed that 3 weremistagged using class-to-class.
The reason wasthat the class-to-class introduces new examplesfrom verbs that are superclasses of the targetverb, and these introduced noise.
For example,we examined the verb turn_to (tagged in Semcorwith sense 1):1. turn-to (Semcor 1, word-to-class 1): there arefewer training examples than in the class-to-class model and they get more credit.
Therelation ?turn_to-obj-platoon?
gives weight tothe class <organization, organisation>.2.
turn-to (Semcor 1, class-to-class 2): therelations ?take_up-obj-position?
and ?call_on-obj-esprit_de_corps?
introduce noise and pointto the class <artifact, artefact>.
As a result, thesense 2 is wrongly selected.From the 5 mistagged examples in class-to-class, only ?empty?
was tagged correctly usingclasses (in this case the class-to-class model isable to select the correct sense of the verb,discarding the misleading senses of empty):1.
Attend, Join, Advance: they had errors oftype A and B (incorrect Semcor tag/ misleadingverb-object relations) and we can not expect the?class-to-class?
model to handle them.2.
Imprison: still has not enough information tomake a good choice.3.
Empty (Semcor 2, class-to-class 2): newexamples associated to the appropriate sense ofempty give credit to the classes <place ofworship, house of prayer, house of God, houseof worship> and <church, church building>.With the weight of these classes the correctsense 2 is correctly chosen.Finally, the 2 examples that received noanswer in the ?word-to-class?
model weretagged correctly:1.
Flurry (Semcor 2, class-to-class 2): theanswer is correct although the choice is madewith few data.
The strongest class is <structure,construction>.2.
Rebuild (Semcor 2, class-to-class 2): the newinformation points to the appropriate sense.7.2 Church as subjectThe class2class model showed a better behaviorwith the examples in which church appeared assubject.
There were only 10 examples, 8 taggedwith sense 1 and 2 with sense 2.In this case, the class-to-class model taggedin the same way the examples tagged by theclass-to-word model, but it also tagged the 3occurrences that had not been tagged by theword-to-class model (2 correctly and 1incorrectly).8 ConclusionsWe presented a statistical model that extendsselectional preference to classes of verbs,yielding a relation between classes in ahierarchy, as opposed to a relation between aword and a class.
The motivation is twofold:different senses of a verb may have differentpreferences, and some classes of verbs can sharepreferences.The model is trained using subject-verb andobject-verb relations extracted from a sense-disambiguated corpus using Minipar.
Apeculiarity of this exercise is the use of a smallsense-disambiguated corpus, in contrast to usinga large corpus of ambiguous words.Contrary to other methods we do not try tofind the classes which generalize best theselectional preferences.
All possibilities, eventhe ones with very low probability, are stored.Evaluation is based on a word sensedisambiguation exercise for a sample of wordsand a sample of documents from Semcor.
Theproposed model gets similar results on precisionbut significantly better recall than the classicalword-to-class model.We plan to train the model on a largeuntagged corpus, in order to compare the qualityof the acquired selectional preferences withthose extracted from this small tagged corpora.The model can easily be extended todisambiguate other relations and POS.
Atpresent we are also integrating the model on asupervised WSD algorithm that uses decisionlists.ReferencesAbe, H. & Li, N. 1996.
Learning Word AssociationNorms Using Tree Cut Pair Models.
InProceedings of the 13th International Conferenceon Machine Learning ICML.Agirre E. and Martinez D. 2000.
Decision lists andautomatic word sense disambiguation.
COLING2000, Workshop on Semantic Annotation andIntelligent Content.
Luxembourg.Lin, D. 1993.
Principle Based parsing withoutOvergeneration.
In 31st Annual Meeting of theAssociation for Computational Linguistics.Columbus, Ohio.
pp 112-120.Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross,and K. Miller.
1990.
Five Papers on WordNet.Special Issue of the International Journal ofLexicography, 3(4).Miller, G. A., C. Leacock, R. Tengi, and R. T.Bunker.
1993.
A Semantic Concordance.Proceedings of the ARPA Workshop on HumanLanguage Technology.Resnik, P. 1992.
A class-based approach to lexicaldiscovery.
In Proceedings of the Proceedings ofthe 30th Annual Meeting of the Association forComputational Linguists., .
327-329.Resnik,P.
1997.
Selectional Preference and SenseDisambiguation..
In Proceedings of the ANLPWorkshop ``Tagging Text with LexicalSemantics: Why What and How?
''., Washington,DC.Stetina J., Kurohashi S., Nagao M. 1998.
GeneralWord Sense Disambiguation Method Based on aFull Sentential Context.
In Usage of WordNet inNatural Language Processing , Proceedings ofCOLING-ACL Workshop.
Montreal (C Canada).
