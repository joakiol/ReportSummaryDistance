Research in Information Extraction: 1996-98Ralph GrishmanDepar tment  o f  Computer  Sc ienceNew York  Un ivers i tyNew York ,  NY  10003grishman@cs, nyu.
eduDefinitions and GoalsInformation extraction involves picking outspecified types of information from natural anguagetext.
Recent Message Understanding Conferences\[1,2,3\] have developed a spectrum of such tasks, andwe have worked on two of them, at opposite nds ofthe spectrum: the named entity task, which involvesidentifying and classifying names, and the scenariotemplate task, which involves extracting criticalinformation (participants, location, date, etc.)
aboutspecified classes of events.We have of course been concerned aboutperformance: trying to build systems which comeclose to human accuracy, or at least perform withsufficient accuracy to be of practical value.
Inaddition, we have long been concerned withportability: the ability to adapt our systems to newclasses of events, to new domains, and even to newlanguages.
We want to create systems which can beported easily and, if possible, by people who don'tknow the internal workings of the system.
Only inthis way can systems for new tasks be createdcheaply enough to be widely used.Earlier systems were based entirely on hand-crafted rules.
As larger annotated training corporahave become available and methods for learning fromcorpora have become better understood, moreresearchers have focused on corpus-trained systems,avoiding separate hand-coded knowledge or rules.The approach we have taken has been more eclecticand opportunistic: to use corpus-driven methods, butalso to employ separate "world knowledge", hand-coded rules, and user interaction in rule acquisitionwhere appropriate.
In some cases this has allowed usto achieve greater performance or to learn rules fromfar fewer examples.Named EntityThe named entity task involves identifying andclassifying several types of names -- people,organizations, and locations -- as well as some otherphrases, such as dates and times.
Achieving fairlygood performance on this task is easy, butapproaching human performance is difficult becausethe hard cases must be resolved based on manydifferent types of evidence: the words starting orending a name ("Mr.", "Corp."), the context of aname C... died"); other mentions of a name in a text("Mr. Smith ... Smith reported...").
Corpus-basedlearning may be helpful in gathering and balancingthese types of evidence.
Fortunately, names are veryfrequent in many types of text, so it is easy to get asubstantial training set for this task.We explored two learning methods, decisiontrees and maximum entropy.
In both cases, wesought to combine criteria which could be gatheredfrom the training corpus with generalizations whichcould be obtained from external sources and rulesdeveloped by hand.Decision TreeOur decision tree method is described in detailin \[4,5\].
The internal nodes of the tree test variousproperties of a token; based on these properties, theleaves of the tree specify the probability that a giventoken starts, continues, or ends a name of a giventype (person, organization .
.
.
.
).
The tree is builtautomatically from a training corpus annotated withthe various types of names.
In tagging new text, wefirst use the decision tree to determine theseprobabilities; we then use a Viterbi algorithm to findthe most likely consistent tagging (e.g., one in whicha 'start person' is followed by an 'end person' and notan 'end organization').This approach was applied to the Japanesenamed entity task.
The decisions in the decision treeare based on the character type of the token, the part-of-speech (as determined by Juman), and variousword lists.
These word lists, which include commontitles, common company suffixes, major companynames, etc., were gathered from the training corpusand the WWW.57Maximum EntropyOur maximum entropy method is described indetail in \[6,7\].
Again, we are developing a functionwhich takes as input various features of the tokens ina text, and yields the probability that a given tokenstarts, continues, or ends a name of a given type.However, the form of the function is different:instead of being a sequence of discrete decisions (adecision tree), the probability is computed as aproduct of functions on the individual features, withcoefficients determined from the corpus.This approach was applied to both English andJapanese named entity tasks.
For Japanese namedentity, when used with the same set of features as thedecision tree model, the performance was about thesame.
However, when the feature set was extendedto include individual lexical items from the trainingcorpus, the performance was substantially improved(from F=80.0 to 83.81).
The decision tree model wasnot able to use individual words as features aseffectively, because these features fragmented thetraining data.For English named entity, the performance ofthe system with features based on word form (e.g.,capitalization), individual lexical items, and hand-collected word lists was already quite good (F=92.9on test data from the training domain).
However,there had been substantial work at NYU andelsewhere on building by hand patterns for namedentity classification, and we wanted to take advantageof that work.
In particular, while there might be gapsin these hand-written patterns, they did capture somesituations where complex combinations of featurescould be used to classify names with high precision... complex combinations that were not likely to belearned automatically.
We utilized this work bytreating the output of the hand-coded named entityrules as another set of features to be considered bythe maximum entropy method.
Adding our hand-coded rules (which, by themselves, performed atF=92.2) yielded a system with F=95.7; adding therules from two other sites brought he performance toF=97.4.
We thus demonstrated how the combinationof hand-coded and corpus-acquired rules could bemore effective than either alone.Scenario TemplateThe general goals for the scenario template(event extraction) task were the same as those for thenamed entity task: improve performance andThe F measure is a combination of the recall andprecision measures.portability.
However, the approach was somewhatdifferent because we expected that the environmentwould be different.
The named entity task isapplicable across a range of domains, and so we canjustify preparing a substantial number of trainingexamples; furthermore, such data is relatively easy toprepare because the task is simple and thephenomenon frequent.In contrast, scenario template is really a largecollection of very diverse tasks (one task for eachtype of event), and each instance is more complexthan the named entity task.
We expect hat "real life"training sets will be quite small -- even smaller thanthe 100-article sets which characterized the last twoMUCs.
Accordingly, while the process of building asystem for a new class of events is example driven,there is much more emphasis on having a person inthe loop to generalize and adjust the patterns andrules as they are being created.Proteus Extraction ToolOver the last two years we have built anincreasingly rich interface for the customization ofevent extraction systems.
Such a system in driven bya number of "knowledge bases", including a lexicon,a concept hierarchy, a set of task-specific templates(frames), and a set of patterns to be matched againstthe text.
Our interface is able to inspect and modifyall these knowledge bases, as well as manipulatedocuments and observe the results and intermediatestages of extraction on these documents.
At the heartof this interface is a capability for taking a samplesentence along with its mapping into templates andproduce an extraction pattern which is suitablygeneralized syntactically and semantically to operateon new text.
The syntactic generalization is donefully automatically, while the semantic generalizationis done in interaction with the user.
This system isdescribed more fully in \[8,9,10\].MultilinguafityAnother dimension of portability is portabilityto new languages.
We noted earlier our work onnamed entity systems which could operate in bothEnglish and Japanese.
Porting event extractionsystems is more complex, because there are morecomponents o the system.
In particular, the Englishsystem uses almost entirely locally-written softwarecoded in Lisp, and operates as a single process.
Inmoving to other languages, we found that we wantedto make use of pre-existing software for such tasks astokenization, part-of-speech tagging, and namerecognition.
We therefore moved towards a multi-58process structure in which information iscommunicated to the extraction engine in the form ofSGML annotations on the document.
Pre-existingcomponents are embedded in "wrappers" so that theymay communicate with the main extraction engineusing SGML mark-up.We have ported the entire extraction system(including the customization i terface) to Japanese.The external components for this system are theJuman tokenizer/tagger and the Japanese namedentity tagger described above.
We have implementedthe management succession scenario in Japaneseusing this system; the system is further described in\[11\] in this volume.We have also ported the core extraction systemto Swedish.
For the Swedish system, the texts werefirst processed by SweCG, the Swedish ConstraintGrammar developed at Helsinki University andcommercialized by Lingsoft.
The SweCG doeslexical lookup, two-level morphological nalysis anddisambiguation.
The analysis consists of part-of-speech tags, morphological features, and somesemantic information.
In the next module, theSweCG output was transformed into the SGMLformat required by the core extraction system.Already at this stage, some information (semantictags, knowledge bases, capitalization and otherheuristics) was used for name recognition.
Theremainder of the text analysis was performed bysyntactic and semantic patterns within the coresystem.
Since Swedish has a richer morphology thanEnglish, the pattern formalism was slightly extendedto allow for more of the morphological informationfrom the Swedish tagger to be used in the patterns.Performance EnhancementsThe final aspect of our work on informationextraction has been an effort to improve the level ofperformance on the scenario template task.Specifically, we have studied the managementsuccession task of MUC-6 \[1\], which requires thesystem to determine who has started or left whichmanagement position at which company.
We tried toimprove our performance on this task above theMUC-6 level (57% recall, 70% precision, F=62.82 ontraining corpus; 47% recall, 70% precision, F=56.39on test corpus).We made a large number of system changes in1997, some general, others specific to themanagement succession scenario, including?
improvements in name recognition?
improvements in reference resolution,including handling of conjoinedantecedents, coreference from copulaclauses, and headless anaphors?
analysis of verb tense (which was thenused to fill the "on the job" slot in thisscenario)?
some additional noun phrase and eventpatterns ("the late ...", "... was laidoff")These changes raised performance on thetraining corpus to 68% recall, 75% precision,F=71.34, and 55% recall, 74% precision, F=63.11 onthe test corpus.We continued making changes in 1998,although primarily of a scenario-specific nature.
Inparticular, we added several rules for suppressingspurious events, and rules for jobs which can be heldconcurrently (e.g., CEO and president of the samefirm).
Altogether these changes yielded animprovement of 2.3 F on the training corpus (69%recall, 79% precision); on the test corpus, however,the F was almost unchanged, with 0.4% gain inprecision but 0.6% loss of recall .
.
.
.At the 24-month Tipster meeting, colleaguesfrom SRI International reported similar problemswith improving MUC-6 performance.
They notedthat the official scoring procedure uses very liberalcriteria for matching a system response to the key,seeking to maximize the score, and will give somecredit even for wildly incorrect responses (e.g., for ahiring event where both the name of the person andthe name of the company are wrong, but it iscorrectly reported that someone was hired and is nowon the job).
"Precision improvingi' modificationsmay reduce these near-random matches, thusreducing recall while raising precision.
SRI reportedthat a metric which required a closer correspondencebetween key and response in order to get any credit(specifically, requiring that the person, position, andorganization name for an event all be correct)indicated substantial gains as they improved theirsystem, even though the official F measure waslargely unchanged.In a similar vein, we have examined a measurebased on only three slots from the template: theperson's name, the organization name, and themanagement post.
We reasoned that "junk"(erroneously generated) templates are less likely tohave correct values for these slots than for slots withbinary values, such as the IN_AND_OUT slot (whichindicates whether someone was starting or leaving ajob).
We used the official scorer to align the system59response to the key.
We then computed recall,precision, and F measure for the sum of these threeslots, and found steady improvement, even when theF score over all the slots showed a slight dip.
For thetest corpus, our MUC-6 official run had recall 55.2%,precision 75.4%, F=63.7 for these three slots; at theend of 1997, we had recall 60.0%, precision 75.1%,F=66.7; after the changes described above for 1998,we had recall 61.1%, precision 75.3%, F=67.4.AcknowledgementsThe porting of the event extraction system toSwedish was done by Kristofer Franz6n with fundingfrom the Swedish Foundation for InternationalCooperation in Research and Higher Education, theSwedish Institute, and the Swedish Council forResearch in the Humanities and Social Sciences.References\[1\] Proceedings of the Sixth MessageUnderstanding Conference (MUC-6).
Columbia,Maryland, November, 1995, Morgan Kaufmann.\[2\] Ralph Grishman, Beth Sundheim.
MessageUnderstanding Conference - 6: A Brief History.Proceedings of the 16th International Conference onComputational Linguistics, Copenhagen, Denmark,1996.\[3\] Proceedings of the Seventh MessageUnderstanding Conference (MUC- 7), Fairfax,Virginia, April 29 - May 1, 1998.
Available throughthe S.A.I.C.
MUC web site,http: //www.muc.
saic.com/.\[4\] Satoshi Sekine.
NYU: Description of theJapanese NE System used for MET-2.
Proceedings ofthe Seventh Message Understanding Conference(MUC-7), Fairfax, Virginia, April 29 - May 1, 1998.\[5\] Satoshi Sekine, Ralph Grishman andHiroyuki Shinnou.
A Decision Tree Method forFinding and Classifying Names in Japanese Texts.Proceedings of the Sixth Workshop on Very LargeCorpora, Montreal, Canada, August 1998.\[6\] Andrew Borthwick, John Sterling, EugeneAgichtein, and Ralph Grishman.
Description of theMENE Named Entity System as used in MUC-7.Proceedings of the Seventh Message UnderstandingConference (MUC-7), Fairfax, Virginia, April 29 -May 1, 1998.\[7\] Andrew Borthwick, John Sterling, EugeneAgichtein, and Ralph Grishman.
Exploiting DiverseKnowledge Sources via Maximum Entropy in NamedEntity Recognition.
Proceedings of the SixthWorkshop on Very Large Corpora, Montreal,Canada, August 1998.\[8\] Roman Yangarber and Ralph Grishman.Customization of Information Extraction Systems.Proceedings of International Workshop on ILexicallyDriven Information Extraction, Frascati, Italy, July16, 1997.\[9\] Roman Yangarber and Ralph Grishman.NYU: Description of the Proteus/PET System as usedfor MUC-7 ST.
Proceedings of the Seventh MessageUnderstanding Conference (MUC- 7), Fairfax,Virginia, April 29 - May 1, 1998.\[10\] Roman Yangarber and Ralph Grishman,Transforming Examples into Patterns for InformationExtraction, this volume.\[11\] Chikashi Nobata, Satoshi Sekine, andRoman Yangarber, Japanese IE System andCustomization Tool, this volume.60
