Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1036?1044,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsReordering with Source Language CollocationsZhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Ting Liu1, Sheng Li11Harbin Institute of Technology, Harbin, China2Baidu Inc., Beijing, China{liuzhanyi, wanghaifeng, wu_hua}@baidu.com{tliu, lisheng}@hit.edu.cnAbstractThis paper proposes a novel reordering modelfor statistical machine translation (SMT) bymeans of modeling the translation orders ofthe source language collocations.
The modelis learned from a word-aligned bilingual cor-pus where the collocated words in source sen-tences are automatically detected.
Duringdecoding, the model is employed to softlyconstrain the translation orders of the sourcelanguage collocations, so as to constrain thetranslation orders of those source phrases con-taining these collocated words.
The experi-mental results show that the proposed methodsignificantly improves the translation quality,achieving the absolute improvements of1.1~1.4 BLEU score over the baseline me-thods.1 IntroductionReordering for SMT is first proposed in IBM mod-els (Brown et al, 1993), usually called IBM con-straint model, where the movement of wordsduring translation is modeled.
Soon after, Wu(1997) proposed an ITG (Inversion TransductionGrammar) model for SMT, called ITG constraintmodel, where the reordering of words or phrases isconstrained to two kinds: straight and inverted.
Inorder to further improve the reordering perfor-mance, many structure-based methods are pro-posed, including the reordering model inhierarchical phrase-based SMT systems (Chiang,2005) and syntax-based SMT systems (Zhang et al,2007; Marton and Resnik, 2008; Ge, 2010; Vis-weswariah et al, 2010).
Although the sentencestructure has been taken into consideration, thesemethods don?t explicitly make use of the strongcorrelations between words, such as collocations,which can effectively indicate reordering in thetarget language.In this paper, we propose a novel method to im-prove the reordering for SMT by estimating thereordering score of the source-language colloca-tions (source collocations for short in this paper).Given a bilingual corpus, the collocations in thesource sentence are first detected automaticallyusing a monolingual word alignment (MWA) me-thod without employing additional resources (Liuet al, 2009), and then the reordering model basedon the detected collocations is learned from theword-aligned bilingual corpus.
The source colloca-tion based reordering model is integrated into SMTsystems as an additional feature to softly constrainthe translation orders of the source collocations inthe sentence to be translated, so as to constrain thetranslation orders of those source phrases contain-ing these collocated words.This method has two advantages: (1) it can au-tomatically detect and leverage collocated words ina sentence, including long-distance collocatedwords; (2) such a reordering model can be inte-grated into any SMT systems without resorting toany additional resources.We implemented the proposed reordering mod-el in a phrase-based SMT system, and the evalua-tion results show that our method significantlyimproves translation quality.
As compared to thebaseline systems, an absolute improvement of1.1~1.4 BLEU score is achieved.1036The paper is organized as follows: In section 2,we describe the motivation to use source colloca-tions for reordering, and briefly introduces the col-location extraction method.
In section 3, wepresent our reordering model.
And then we de-scribe the experimental results in section 4 and 5.In section 6, we describe the related work.
Lastly,we conclude in section 7.2 CollocationA collocation is generally composed of a group ofwords that occur together more often than bychance.
Collocations effectively reveal the strongassociation among words in a sentence and arewidely employed in a variety of NLP tasks(Mckeown and Radey, 2000).Given two words in a collocation, they can betranslated in the same order as in the source lan-guage, or in the inverted order.
We name the firstcase as straight, and the second inverted.
Based onthe observation that some collocations tend to havefixed translation orders such as ???
jin-rong ?fi-nancial?
??
wei-ji ?crisis??
(financial crisis)whose English translation order is usually straight,and  ???
fa-lv ?law?
??
fan-wei ?scope??
(scope of law) whose English translation order isgenerally inverted, some methods have been pro-posed to improve the reordering model for SMTbased on the collocated words crossing the neigh-boring components (Xiong et al, 2006).
We fur-ther notice that some words are translated indifferent orders when they are collocated with dif-ferent words.
For instance, when ???
chao-liu?trend??
is collocated with ???
shi-dai ?times?
?,they are often translated into the ?trend of times?
;when collocated with ???
li-shi ?history?
?, thetranslation usually becomes the ?historical trend?.Thus, if we can automatically detect the colloca-tions in the sentence to be translated and their or-ders in the target language, the reorderinginformation of the collocations could be used toconstrain the reordering of phrases during decod-ing.
Therefore, in this paper, we propose to im-prove the reordering model for SMT by estimatingthe reordering score based on the translation ordersof the source collocations.In general, the collocations can be automaticallyidentified based on syntactic information such asdependency trees (Lin, 1998).
However these me-thods may suffer from parsing errors.
Moreover,for many languages, no valid dependency parserexists.
Liu et al (2009) proposed to automaticallydetect the collocated words in a sentence with theMWA method.
The advantage of this method liesin that it can identify the collocated words in a sen-tence without additional resources.
In this paper,we employ MWA Model l~3 described in Liu et al(2009) to detect collocations in sentences, whichare shown in Eq.
(1)~(3).??
?ljcj jwwtSAp 11 ModelMWA)|()|((1)???
?ljjcj lcjdwwtSAp j12 ModelMWA),|()|()|((2)???????
?ljjcjliiilcjdwwtwnSApj113 ModelMWA),|()|()|()|((3)Where lwS 1?
is a monolingual sentence; i?
de-notes the number of words collocating withiw ;}&],1[|),{( icliciA ii ???
denotes the potentiallycollocated words in S.The MWA models measure the collocatedwords under different constraints.
MWA Model 1only models word collocation probabilities)|( jcj wwt.
MWA Model 2 additionally employsposition collocation probabilities),|( lcjd j. Be-sides the features in MWA Model 2, MWA Model3 also considers fertility probabilities )|( ii wn ?
.Given a sentence, the optimal collocated wordscan be obtained according to Eq.
(4).
)|(maxarg*  ModelMWA SApA iA?
(4)Given a monolingual word aligned corpus, thecollocation probabilities can be estimated as fol-lows.2)|()|(),( ijjiji wwpwwpwwr ??(5)Where,???
?wjjiji wwcountwwcountwwp ),(),()|(;),( ji wwdenotes the collocated words in the corpus and),( ji wwcountdenotes the co-occurrence frequency.10373 Reordering Model with Source Lan-guage CollocationsIn this section, we first describe how to estimatethe orientation probabilities for a given collocation,and then describe the estimation of the reorderingscore during translation.
Finally, we describe theintegration of the reordering model into the SMTsystem.3.1 Reordering probability estimationGiven a source collocation ),( ji ffand its corres-ponding translations),( ji aa eein a bilingual sen-tence pair, the reordering orientation of thecollocation can be defined as in Eq.
(6).???????????
?jijijijiaaji aajiaajiaajiaajio ji &or& ifinvertedor ifstraight,,,(6)In our method, only those collocated words insource language that are aligned to different targetwords, are taken into consideration, and those be-ing aligned to the same target word are ignored.Given a word-aligned bilingual corpus wherethe collocations in source sentences are detected,the probabilities of the translation orientation ofcollocations in the source language can be esti-mated, as follows:?
?
???
?o jijiji ffocountffocountffop ),,(),,straight(),|straight((7)?
?
???
?o jijiji ffocountffocountffop ),,(),,inverted(),|inverted((8)Here, ),,( ji ffocountis collected according tothe algorithm in Figure 1.3.2 Reordering modelGiven a sentence lfF 1?
to be translated, the col-locations are first detected using the algorithm de-scribed in Eq.
(4).
Then the reordering score isestimated according to the reordering probabilityweighted by the collocation probability of the col-located words.
Formally, for a generated transla-tion candidate T , the reordering score is calculatedas follows.
),|(log),(),( ,,,),( iiciii i ciaacici ciO ffopffrTFP ??
(9)Input: A word-aligned bilingual corpus wherethe source collocations are detectedInitialization:),,( ji ffocount=0for each sentence pair <F, E> in the corpus dofor each collocated word pair),( ici ffin F doificii aaci ??
&oricii aaci ??
&then???
),,( ici ffstraightocountificii aaci ??
&oricii aaci ??
&then???
),,( ici ffinvertedocountOutput: ),,( ji ffocountFigure 1.
Algorithm of estimatingreordering frequencyHere,),( ici ffrdenotes the collocation probabil-ity ofif  and icfas shown in Eq.
(5).In addition to the detected collocated words inthe sentence, we also consider other possible wordpairs whose collocation probabilities are higherthan a given threshold.
Thus, the reordering scoreis further improved according to Eq.
(10).???????????
),(&)},{(),(,,,,,,),()},|(log),(),|(log),(),(jiijiiiciiiiffrcijijiaajijiciaaciciciOffopffrffopffrTFP(10)Where ?
and ?
are two interpolation weights.?
is the threshold of collocation probability.
Theweights and the threshold can be tuned using a de-velopment set.3.3 Integrated into SMT systemThe SMT systems generally employ the log-linearmodel to integrate various features (Chiang, 2005;Koehn et al, 2007).
Given an input sentence F, thefinal translation E* with the highest score is chosenfrom candidates, as in Eq.
(11).}),({maxarg*1???
MmmmEFEhE ?
(11)Where hm(E, F) (m=1,...,M) denotes fea-tures.m?
is a feature weight.Our reordering model can be integrated into thesystem as one feature as shown in (10).1038Figure 2.
An example for reordering4 Evaluation of Our Method4.1 ImplementationWe implemented our method in a phrase-basedSMT system (Koehn et al, 2007).
Based on theGIZA++ package (Och and Ney, 2003), we im-plemented a MWA tool for collocation detection.Thus, given a sentence to be translated, we firstidentify the collocations in the sentence, and thenestimate the reordering score according to thetranslation hypothesis.
For a translation option tobe expanded, the reordering score inside thissource phrase is calculated according to their trans-lation orders of the collocations in the correspond-ing target phrase.
The reordering score crossing thecurrent translation option and the covered parts canbe calculated according to the relative position ofthe collocated words.
If the source phrase matchedby the current translation option is behind the cov-ered parts in the source sentence, then...)|staight(log ?op  is used, otherwise...)|inverted(log ?op .
For example, in Figure 2, thecurrent translation option is (4332 eeff ?
).
Thecollocations related to this translation option are),( 31 ff , ),( 32 ff , ),( 53 ff .
The reordering scorescan be estimated as follows:),|straight(log),( 3131 ffopffr ?
),|inverted(log),( 3232 ffopffr ?
),|inverted(log),( 5353 ffopffr ?In order to improve the performance of the de-coder, we design a heuristic function to estimatethe future score, as shown in Figure 3.
For any un-covered word and its collocates in the input sen-tence, if the collocate is uncovered, then the higherreordering probability is used.
If the collocate hasbeen covered, then the reordering orientation canInput: Input sentence LfF 1?Initialization: Score = 0for each uncovered wordif  dofor each wordjf(icj ?or??
)( , ji ffr) doifjfis covered thenif i > j thenScore+=),|straight(log)( , jiji ffopffr ?elseScore+=),|inverted(log)( , jiji ffopffr ?elseScore +=),|(log)(maxarg , jijio ffopffrOutput: ScoreFigure 3.
Heuristic function for estimating futurescorebe determined according to the relative positions ofthe words and the corresponding reordering proba-bility is employed.4.2 SettingsWe use the FBIS corpus (LDC2003E14) to train aChinese-to-English phrase-based translation model.And the SRI language modeling toolkit (Stolcke,2002) is used to train a 5-gram language model onthe English sentences of FBIS corpus.We used the NIST evaluation set of 2002 as thedevelopment set to tune the feature weights of theSMT system and the interpolation parameters,based on the minimum error rate training method(Och, 2003), and the NIST evaluation sets of 2004and 2008 (MT04 and MT08) as the test sets.We use BLEU (Papineni et al, 2002) as evalua-tion metrics.
We also calculate the statistical signi-ficance differences between our methods and thebaseline method by using the paired bootstrap re-sample method (Koehn, 2004).4.3 Translation resultsWe compare the proposed method with variousreordering methods in previous work.Monotone model: no reordering model is used.Distortion based reordering (DBR) model: adistortion based reordering method (Al-Onaizan & Papineni, 2006).
In this method, thedistortion cost is defined in terms of words, ra-ther than phrases.
This method considers out-bound, inbound, and pairwise distortions thatf1    f2     f3     f4      f5e4e3e2e11039Reorder models MT04 MT08Monotone model 26.99 18.30DBR model 26.64 17.83MSDR model (Baseline) 28.77 18.42MSDR+DBR model 28.91 18.58SCBR Model 1 29.21 19.28SCBR Model 2 29.44 19.36SCBR Model 3 29.50 19.44SCBR models (1+2) 29.65 19.57SCBR models (1+2+3) 29.75 19.61Table 1.
Translation results on various reordering modelsT1: The two sides are also the basic stand of not relaxed.T2: The basic stance of the two sides have not relaxed.Reference: The basic stances of both sides did not move.Figure 4.
Translation example.
(*/*) denotes (pstraight / pinverted)are directly estimated by simple counting overalignments in the word-aligned bilingual cor-pus.
This method is similar to our proposedmethod.
But our method considers the transla-tion order of the collocated words.msd-bidirectional-fe reordering (MSDR orBaseline) model: it is one of the reorderingmodels in Moses.
It considers three differentorientation types (monotone, swap, and discon-tinuous) on both source phrases and targetphrases.
And the translation orders of both thenext phrase and the previous phrase in respectto the current phrase are modeled.Source collocation based reordering (SCBR)model: our proposed method.
We investigatethree reordering models based on the corres-ponding MWA models and their combinations.In SCBR Model i (i=1~3), we use MWA Mod-el i as described in section 2 to obtain the col-located words and estimate the reorderingprobabilities according to section 3.The experiential results are shown in Table 1.The DBR model suffers from serious data sparse-ness.
For example, the reordering cases in thetrained pairwise distortion model only covered32~38% of those in the test sets.
So its perfor-mance is worse than that of the monotone model.The MSDR model achieves higher BLEU scoresthan the monotone model and the DBR model.
Ourmodels further improve the translation quality,achieving better performance than the combinationof MSDR model and DBR model.
The results inTable 1 show that ?MSDR + SCBR Model 3?
per-forms the best among the SCBR models.
This isbecause, as compared to MWA Model 1 and 2,MWA Model 3 takes more information into con-sideration, including not only the co-occurrenceinformation of lexical tokens and the position ofwords, but also the fertility of words in a sentence.And when the three SCBR models are combined,the performance of the SMT system is further im-proved.
As compared to other reordering models,our models achieve an absolute improvement of0.98~1.19 BLEU score on the test sets, which arestatistically significant (p < 0.05).Figure 4 shows an example: T1 is generated bythe baseline system and T2 is generated by the sys-tem where the SCBR models (1+2+3)1 are used.1 In the remainder of this paper, ?SCBR models?
means thecombination of the SCBR models (1+2+3) unless it is explicit-ly explained.Input:  ??
?
??
??
?
?
??
??
?shuang-fang    DE    ji-ben       li-chang   ye      dou mei-you song-dong .
(0.99/0.01)both-side       DE     basic          stance  also    both    not        loose     .
(0.21/0.79)(0.95/0.05)1040Reordering models MT04 MT08MSDR model 28.77 18.42MSDR+DBR model 28.91 18.58CBR model 28.96 18.77WCBR model 29.15 19.10WCBR+SCBRmodels29.87 19.83Table 2.
Translation results of co-occurrencebased reordering modelsCBR modelSCBRModel3Consecutive words 77.9% 73.5%Interrupted words 74.1% 87.8%Total 74.3% 84.9%Table 3.
Precisions of the reordering models onthe development setThe input sentence contains three collocations.
Thecollocation (?
?, ??)
is included in the samephrase and translated together as a whole.
Thus itstranslation is correct in both translations.
For theother two long-distance collocations (?
?, ??
)and (?
?, ??
), their translation orders are notcorrectly handled by the reordering model in thebaseline system.
For the collocation (?
?, ??
),since the SCBR models indicate p(o=straight|??,??)
< p(o=inverted|?
?, ??
), the system fi-nally generates the translation T2 by constrainingtheir translation order with the proposed model.5 Collocations vs. Co-occurring WordsWe compared our method with the method thatmodels the reordering orientations based on co-occurring words in the source sentences, ratherthan the collocations.5.1 Co-occurrence based reordering modelWe use the similar algorithm described in section 3to train the co-occurrence based reordering (CBR)model, except that the probability of the reorderingorientation is estimated on the co-occurring wordsand the relative distance.
Given an input sentenceand a translation candidate, the reordering score isestimated as shown in Eq.
(12).?
???
),( ,,, ),,|(log),( ji jijiaajiO ffopTFP ji(12)Here,ji?
?is the relative distance of two wordsin the source sentence.We also construct the weighted co-occurrencebased reordering (WCBR) model.
In this model,the probability of the reordering orientation is ad-ditionally weighted by the pointwise mutual infor-mation 2  score of the two words (Manning andSch?tze, 1999), which is estimated as shown in Eq.(13).?
???
),(,,,MI ),,|(log),(),(jijijiaajijiOffopffsTFPji(13)5.2 Translation resultsTable 2 shows the translation results.
It can be seenthat the performance of the SMT system is im-proved by integrating the CBR model.
The perfor-mance of the CBR model is also better than that ofthe DBR model.
It is because the former is trainedbased on all co-occurring aligned words, while thelatter only considers the adjacent aligned words.When the WCBR model is used, the translationquality is further improved.
However, its perfor-mance is still inferior to that of the SCBR models,indicating that our method (SCBR models) ofmodeling the translation orders of source colloca-tions is more effective.
Furthermore, we combinethe weighted co-occurrence based model and ourmethod, which outperform all the other models.5.3 Result analysisPrecision of predictionFirst of all, we investigate the performance ofthe reordering models by calculating precisions ofthe translation orders predicted by the reorderingmodels.
Based on the source sentences and refer-ence translations of the development set, where thesource words and target words are automaticallyaligned by the bilingual word alignment method,we construct the reference translation orders fortwo words.
Against the references, we calculatethree kinds of precisions as follows:|}1|||{||}&1{|,,,,,CW ??????
jioooj||iPjiaajiji ji(14)2 For occurring words extraction, the window size is set to [-6,+6].1041|}1|||{||}&1{|,,,,,IW ??????
jioooj||iPjiaajiji ji(15)|}{||}{|,,,,,totaljiaajijioooP ji??
(16)Here,jio ,denotes the translation order of (ji ff ,)predicted by the reordering models.
If)|straight( , ji ffop ?>),inverted( ji f|fop ?, thenstraight, ?jio, else if)|straight( , ji ffop ?<),inverted( ji f|fop ?, theninverted, ?jio.ji aajio ,,,denotes the translation order derived from the wordalignments.
Ifji aajiji oo ,,,, ?, then the predictedtranslation order is correct, otherwise wrong.CWPandIWP  denote the precisions calculated on theconsecutive words and the interrupted words in thesource sentences, respectively.totalP  denotes theprecision on both cases.
Here, the CBR model andSCBR Model 3 are compared.
The results areshown in Table 3.From the results in Table 3, it can be seen thatthe CBR model has a higher precision on the con-secutive words than the SCBR model, but lowerprecisions on the interrupted words.
It is mainlybecause the CBR model introduces more noisewhen the relative distance of words is set to a largenumber, while the MWA method can effectivelydetect the long-distance collocations in sentences(Liu et al, 2009).
This explains why the combina-tion of the two models can obtain the highestBLEU score as shown in Table 2.
On the whole,the SCBR Model 3 achieves higher precision thanthe CBR model.Effect of the reordering modelThen we evaluate the reordering results of thegenerated translations in the test sets.
Using theabove method, we construct the reference transla-tion orders of collocations in the test sets.
For agiven word pair in a source sentence, if the transla-tion order in the generated translation is the sameas that in the reference translations, then it is cor-rect, otherwise wrong.We compare the translations of the baseline me-thod, the co-occurrence based method, and our me-thod (SCBR models).
The precisions calculated onboth kinds of words are shown in Table 4.
FromTest setsBaseline(MSDR)MSDR+WCBRMSDR+SCBRMT04 78.9% 80.8% 82.5%MT08 80.7% 83.8% 85.0%Table 4.
Precisions (total) of the reorderingmodels on the test setsthe results, it can be seen that our method achieveshigher precisions than both the baseline and themethod modeling the translation orders of the co-occurring words.
It indicates that the proposed me-thod effectively constrains the reordering of sourcewords during decoding and improves the transla-tion quality.6 Related WorkReordering was first proposed in the IBM models(Brown et al, 1993), later was named IBM con-straint by Berger et al (1996).
This model treatsthe source word sequence as a coverage set that isprocessed sequentially and a source token is cov-ered when it is translated into a new target token.In 1997, another model called ITG constraint waspresented, in which the reordering order can behierarchically modeled as straight or inverted fortwo nodes in a binary branching structure (Wu,1997).
Although the ITG constraint allows moreflexible reordering during decoding, Zens and Ney(2003) showed that the IBM constraint results inhigher BLEU scores.
Our method models the reor-dering of collocated words in sentences instead ofall words in IBM models or two neighboringblocks in ITG models.For phrase-based SMT models, Koehn et al(2003) linearly modeled the distance of phrasemovements, which results in poor global reorder-ing.
More methods are proposed to explicitly mod-el the movements of phrases (Tillmann, 2004;Koehn et al, 2005) or to directly predict the orien-tations of phrases (Tillmann and Zhang, 2005;Zens and Ney, 2006), conditioned on currentsource phrase or target phrase.
Hierarchical phrase-based SMT methods employ SCFG bilingual trans-lation model and allow flexible reordering (Chiang,2005).
However, these methods ignored the corre-lations among words in the source language or inthe target language.
In our method, we automati-cally detect the collocated words in sentences and1042their translation orders in the target languages,which are used to constrain the ordering modelswith the estimated reordering (straight or inverted)score.
Moreover, our method allows flexible reor-dering by considering both consecutive words andinterrupted words.In order to further improve translation results,many researchers employed syntax-based reorder-ing methods (Zhang et al, 2007; Marton and Res-nik, 2008; Ge, 2010; Visweswariah et al, 2010).However these methods are subject to parsing er-rors to a large extent.
Our method directly obtainscollocation information without resorting to anylinguistic knowledge or tools, therefore is suitablefor any language pairs.In addition, a few models employed the collo-cation information to improve the performance ofthe ITG constraints (Xiong et al, 2006).
Xiong etal.
used the consecutive co-occurring words as col-location information to constrain the reordering,which did not lead to higher translation quality intheir experiments.
In our method, we first detectboth consecutive and interrupted collocated wordsin the source sentence, and then estimated thereordering score of these collocated words, whichare used to softly constrain the reordering of sourcephrases.7 ConclusionsWe presented a novel model to improve SMT bymeans of modeling the translation orders of sourcecollocations.
The model was learned from a word-aligned bilingual corpus where the potentially col-located words in source sentences were automati-cally detected by the MWA method.
Duringdecoding, the model is employed to softly con-strain the translation orders of the source languagecollocations.
Since we only model the reorderingof collocated words, our methods can partially al-leviate the data sparseness encountered by othermethods directly modeling the reordering based onsource phrases or target phrases.
In addition, thiskind of reordering information can be integratedinto any SMT systems without resorting to anyadditional resources.The experimental results show that the pro-posed method significantly improves the transla-tion quality of a phrase based SMT system,achieving an absolute improvement of 1.1~1.4BLEU score over the baseline methods.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion Models for Statistical Machine Translation.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meetingof the ACL, pp.
529-536.Adam L. Berger, Peter F. Brown, Stephen A. Della Pie-tra, Vincent J. Della Pietra, Andrew S. Kehler, andRobert L. Mercer.
1996.
Language Translation Appa-ratus and Method of Using Context-Based Transla-tion Models.
United States Patent, Patent Number5510981, April.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-la Pietra, and Robert.
L. Mercer.
1993.
The Mathe-matics of Statistical Machine Translation: Parameterestimation.
Computational Linguistics, 19(2): 263-311.David Chiang.
2005.
A Hierarchical Phrase-based Mod-el for Statistical Machine Translation.
In Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics, pp.
263-270.Niyu Ge.
2010.
A Direct Syntax-Driven ReorderingModel for Phrase-Based Machine Translation.
InProceedings of Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the ACL, pp.
849-857.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing, pp.
388-395.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the Joint Conference on Human Lan-guage Technologies and the Annual Meeting of theNorth American Chapter of the Association of Com-putational Linguistics, pp.
127-133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran Ri-chard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.
InProceedings of the 45th Annual Meeting of the ACL,Poster and Demonstration Sessions, pp.
177-180.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proceedings of International Workshop on SpokenLanguage Translation.1043Dekang Lin.
1998.
Extracting Collocations from TextCorpora.
In Proceedings of the 1st Workshop onComputational Terminology, pp.
57-63.Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.2009.
Collocation Extraction Using MonolingualWord Alignment Method.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pp.
487-495.Christopher D. Manning and Hinrich Sch?tze.
1999.Foundations of Statistical Natural LanguageProcessing, Cambridge, MA; London, U.K.: Brad-ford Book & MIT Press.Yuval Marton and Philip Resnik.
2008.
Soft SyntacticConstraints for Hierarchical Phrased-based Transla-tion.
In Proceedings of the 46st Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pp.
1003-1011.Kathleen R. McKeown and Dragomir R. Radev.
2000.Collocations.
In Robert Dale, Hermann Moisl, andHarold Somers (Ed.
), A Handbook of Natural Lan-guage Processing, pp.
507-523.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics, pp.
160-167.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1) : 19-51.Kishore Papineni, Salim Roukos, Todd Ward, and Weij-ing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pp.
311-318.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings for the In-ternational Conference on Spoken LanguageProcessing, pp.
901-904.Christoph Tillmann.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
In Pro-ceedings of the Joint Conference on Human Lan-guage Technologies and the Annual Meeting of theNorth American Chapter of the Association of Com-putational Linguistics, pp.
101-104.Christoph Tillmann and Tong Zhang.
2005.
A LocalizedPrediction Model for Statistical Machine Translation.In Proceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics, pp.
557-564.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan, and Nanda Kambhatla.2010.
Syntax Based Reordering with AutomaticallyDerived Rules for Improved Statistical MachineTranslation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, pp.
1119-1127.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377-403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pp.
521-528.Richard Zens and Herman Ney.
2003.
A ComparativeStudy on Reordering Constraints in Statistical Ma-chine Translation.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pp.
192-202.Richard Zens and Herman Ney.
2006.
DiscriminativeReordering Models for Statistical Machine Transla-tion.
In Proceedings of the Workshop on StatisticalMachine Translation, pp.
55-63.Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou.2007.
Phrase Reordering Model Integrating SyntacticKnowledge for SMT.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pp.
533-540.1044
