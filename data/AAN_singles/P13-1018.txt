Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 176?186,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsMicroblogs as Parallel CorporaWang Ling123 Guang Xiang2 Chris Dyer2 Alan Black2 Isabel Trancoso 13(1)L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA(3)Instituto Superior Te?cnico, Lisbon, Portugal{lingwang,guangx,cdyer,awb}@cs.cmu.eduisabel.trancoso@inesc-id.ptAbstractIn the ever-expanding sea of microblog data, thereis a surprising amount of naturally occurring par-allel text: some users create post multilingual mes-sages targeting international audiences while oth-ers ?retweet?
translations.
We present an efficientmethod for detecting these messages and extract-ing parallel segments from them.
We have beenable to extract over 1M Chinese-English parallelsegments from Sina Weibo (the Chinese counter-part of Twitter) using only their public APIs.
As asupplement to existing parallel training data, ourautomatically extracted parallel data yields sub-stantial translation quality improvements in trans-lating microblog text and modest improvementsin translating edited news commentary.
The re-sources in described in this paper are available athttp://www.cs.cmu.edu/?lingwang/utopia.1 IntroductionMicroblogs such as Twitter and Facebook havegained tremendous popularity in the past 10 years.In addition to being an important form of commu-nication for many people, they often contain ex-tremely current, even breaking, information aboutworld events.
However, the writing style of mi-croblogs tends to be quite colloquial, with fre-quent orthographic innovation (R U still with meor what?)
and nonstandard abbreviations (idk!shm)?quite unlike the style found in more tra-ditional, edited genres.
This poses considerableproblems for traditional NLP tools, which weredeveloped with other domains in mind, which of-ten make strong assumptions about orthographicuniformity (i.e., there is just one way to spell you).One approach to cope with this problem is to an-notate in-domain data (Gimpel et al, 2011).Machine translation suffers acutely from thedomain-mismatch problem caused by microblogtext.
On one hand, standard models are probablysuboptimal since they (like many models) assumeorthographic uniformity in the input.
However,more acutely, the data used to develop these sys-tems and train their models is drawn from formaland carefully edited domains, such as parallel webpages and translated legal documents.
MT trainingdata seldom looks anything like microblog text.This paper introduces a method for finding nat-urally occurring parallel microblog text, whichhelps address the domain-mismatch problem.Our method is inspired by the perhaps surpris-ing observation that a reasonable number of mi-croblog users tweet ?in parallel?
in two or morelanguages.
For instance, the American entertainerSnoop Dogg regularly posts parallel messages onSina Weibo (Mainland China?s equivalent of Twit-ter), for example, watup Kenny Mayne!!
- KennyMayne????????
?, where an Englishmessage and its Chinese translation are in thesame post, separated by a dash.
Our method is ableto identify and extract such translations.
Briefly,this requires determining if a tweet contains morethan one language, if these multilingual utterancescontain translated material (or are due to some-thing else, such as code switching), and what thetranslated spans are.The paper is organized as follows.
Section 2describes the related work in parallel data extrac-tion.
Section 3 presents our model to extract par-allel data within the same document.
Section 4describes our extraction pipeline.
Section 5 de-scribes the data we gathered from both Sina Weibo(Chinese-English) and Twitter (Chinese-Englishand Arabic-English).
We then present experimentsshowing that our harvested data not only substan-tially improves translations of microblog text with176existing (and arguably inappropriate) translationmodels, but that it improves the translation ofmore traditional MT genres, like newswire.
Weconclude in Section 6.2 Related WorkAutomatic collection of parallel data is a well-studied problem.
Approaches to finding par-allel web documents automatically have beenparticularly important (Resnik and Smith, 2003;Fukushima et al, 2006; Li and Liu, 2008; Uszko-reit et al, 2010; Ture and Lin, 2012).
Thesebroadly work by identifying promising candidatesusing simple features, such as URL similarity or?gist translations?
and then identifying truly par-allel segments with more expensive classifiers.More specialized resources were developed usingmanual procedures to leverage special features ofvery large collections, such as Europarl (Koehn,2005).Mining parallel or comparable messages frommicroblogs has mainly relied on Cross-Lingual In-formation Retrieval techniques (CLIR).
Jelh et al(2012) attempt to find pairs of tweets in Twitter us-ing Arabic tweets as search queries in a CLIR sys-tem.
Afterwards, the model described in (Xu et al,2001) is applied to retrieve a set of ranked trans-lation candidates for each Arabic tweet, which arethen used as parallel candidates.The work on mining parenthetical transla-tions (Lin et al, 2008), which attempts to findtranslations within the same document, has somesimilarities with our work, since parentheticaltranslations are within the same document.
How-ever, parenthetical translations are generally usedto translate names or terms, which is more lim-ited than our work which extracts whole sentencetranslations.Finally, crowd-sourcing techniques to obtaintranslations have been previously studied and ap-plied to build datasets for casual domains (Zbibet al, 2012; Post et al, 2012).
These approachesrequire remunerated workers to translate the mes-sages, and the amount of messages translated perday is limited.
We aim to propose a method thatacquires large amounts of parallel data for free.The drawback is that there is a margin of error inthe parallel segment identification and alignment.However, our system can be tuned for precision orfor recall.3 Parallel Segment RetrievalWe will first abstract from the domain of Mi-croblogs and focus on the task of retrieving par-allel segments from single documents.
Prior workon finding parallel data attempts to reason aboutthe probability that pairs of documents (x, y) areparallel.
In contrast, we only consider one doc-ument at a time, defined by x = x1, x2, .
.
.
, xn,and consisting of n tokens, and need to deter-mine whether there is parallel data in x, and ifso, where are the parallel segments and their lan-guages.
For simplicity, we assume that there areat most 2 continuous segments that are parallel.As representation for the parallel seg-ments within the document, we use the tuple([p, q], l, [u, v], r, a).
The word indexes [p, q] and[u, v] are used to identify the left segment (fromp to q) and right segment (from u to v), whichare parallel.
We shall refer [p, q] and [u, v] as thespans of the left and right segments.
To avoidoverlaps, we set the constraint p ?
q < u ?
v.Then, we use l and r to identify the language ofthe left and right segments, respectively.
Finally, arepresents the word alignment between the wordsin the left and the right segments.The main problem we address is to find theparallel data when the boundaries of the parallelsegments are not defined explicitly.
If we knewthe indexes [p, q] and [u, v], we could simply runa language detector for these segments to find land r. Then, we would use an word alignmentmodel (Brown et al, 1993; Vogel et al, 1996),with source s = xp, .
.
.
, xq, target t = xu, .
.
.
, xvand lexical table ?l,r to calculate the Viterbi align-ment a.
Finally, from the probability of the wordalignments, we can determine whether the seg-ments are parallel.Thus, our model will attempt to find the opti-mal values for the segments [p, q][u, v], languagesl, r and word alignments a jointly.
However, thereare two problems with this approach.
Firstly, wordalignment models generally attribute higher prob-abilities to smaller segments, since these are theresult of a smaller product chain of probabilities.In fact, because our model can freely choose thesegments to align, choosing only one word as theleft segment that is well aligned to a word in theright segment would be the best choice.
Thisis obviously not our goal, since we would notobtain any useful sentence pairs.
Secondly, in-ference must be performed over the combinationof all latent variables, which is intractable using177a brute force algorithm.
We shall describe ourmodel to solve the first problem in 3.1 and ourdynamic programming approach to make the in-ference tractable in 3.2.3.1 ModelWe propose a simple (non-probabilistic) three-factor model that models the spans of the parallelsegments, their languages, and word alignmentsjointly.
This model is defined as follows:S([u, v], r, [p, q],l, a | x) =S?S ([p, q], [u, v] | x)?S?L(l, r | [p, q], [u, v], x)?S?T (a | [p, q], l, [u, v], r, x)Each of the components is weighted by the pa-rameters ?, ?
and ?.
We set these values empiri-cally ?
= 0.3, ?
= 0.3 and ?
= 0.4, and leave theoptimization of these parameters as future work.We discuss the components of this model in turn.Span score SS .
We define the score of hypothe-sized pair of spans [p, q], [u, v] as:SS([p, q], [u, v] | x) =(q ?
p+ 1) + (v ?
u+ 1)?0<p??q?<u??v??n(q?
?
p?
+ 1) + (v?
?
u?
+ 1)??
([p, q], [u, v], x)The first factor is a distribution over all spans thatassigns higher probability to segmentations thatcover more words in the document.
It is highestfor segmentations that cover all the words in thedocument (this is desirable since there are manysentence pairs that can be extracted but we wantto find the largest sentence pair in the document).The function ?
takes on values of 0 or 1 depend-ing on whether certain constraints are violated,these include: parenthetical constraints that en-force that spans must not break text within par-enthetical characters and language constraints thatensure that we do break a sequence of Mandarincharacters, Arabic words or Latin words.Language score SL.
The language scoreSL(l, r | [p, q], [u, v], x) indicates whether the lan-guage labels l, r are appropriate to the documentcontents:SL(l, r | [p, q], [u, v], x) =?qi=p L(l, xi) +?vi=u L(r, xi)nwhere L(l, x) is a language detection function thatyields 1 if the word xi is in language l, and 0 oth-erwise.
We build the function simply by consid-ering all words that are composed of Latin char-acters as English, Arabic characters as Arabic andHan characters as Mandarin.
This approach is notperfect, but it is simple and works reasonably wellfor our purposes.Translation score ST .
The translation scoreST (a | [p, q], l, [u, v], r) indicates whether [p, q]is a reasonable translation of [u, v] with the align-ment a.
We rely on IBM Model 1 probabilities forthis score:ST (a | [p, q], l, [u, v], r, x) =1(q ?
p+ 1)v?u+2v?i=uPM1(xi | xai).The lexical tables PM1 for the various languagepairs are trained a priori using available parallelcorpora.
While IBM Model 1 produces worsealignments than other models, in our problem, weneed to efficiently consider all possible spans, lan-guage pairs and word alignments, which makesthe problem intractable.
We will show that dy-namic programing can be used to make this prob-lem tractable, using Model 1.
Furthermore, IBMModel 1 has shown good performance for sen-tence alignment systems previously (Xu et al,2005; Braune and Fraser, 2010).3.2 InferenceOur goal is to find the spans, language pair andalignments such that:argmax[p,q],l,[u,v],r,aS([p, q], l, [u, v], r, a | x) (1)A high score indicates that the predicted bispan islikely to correspond to a valid parallel span, so weset a constant threshold ?
to determine whether adocument has parallel data, i.e., the value of z:z?
= max[u,v],r,[p,q],l,aS([u, v], r, [p, q], l, a | x) > ?Naively maximizing Eq.
1 would requireO(|x|6) operations, which is too inefficient to bepractical on large datasets.
To process millionsof documents, this process would need to be op-timized.The main bottleneck of the naive algorithm isfinding new Viterbi Model 1 word alignments ev-ery time we change the spans.
Thus, we propose178an iterative approach to compute the Viterbi wordalignments for IBM Model 1 using dynamic pro-gramming.Dynamic programming search.
The insight weuse to improve the runtime is that the Viterbiword alignment of a bispan can be reused to cal-culate the Viterbi word alignments of larger bis-pans.
The algorithm operates on a 4-dimensionalchart of bispans.
It starts with the minimal validspan (i.e., [0, 0], [1, 1]) and progressively buildslarger spans from smaller ones.
Let Ap,q,u,v rep-resent the Viterbi alignment (under ST ) of the bis-pan [p, q], [u, v].
The algorithm uses the follow-ing recursions defined in terms of four operations?
{+v,+u,+p,+q} that manipulate a single dimensionof the bispan to construct larger spans:?
Ap,q,u,v+1 = ?+v(Ap,q,u,v) adds one token tothe end of the right span with index v + 1 andfind the viterbi alignment for that token.
Thisrequires iterating over all the tokens in the leftspan, [p, q] and possibly updating their align-ments.
See Fig.
1 for an illustration.?
Ap,q,u+1,v = ?+u(Ap,q,u,v) removes the first to-ken of the right span with index u, so we onlyneed to remove the alignment from u, which canbe done in time O(1).?
Ap,q+1,u,v = ?+q(Ap,q,u,v) adds one token tothe end of the left span with index q + 1, weneed to check for each word in the right span, ifaligning to the word in index q+1 yields a bettertranslation probability.
This update requires n?q + 1 operations.?
Ap+1,q,u,v = ?+p(Ap,q,u,v) removes the firsttoken of the left span with index p. After re-moving the token, we need to find new align-ments for all tokens that were aligned to p.Thus, the number of operations for this updateis K ?
(q ?
p + 1), where K is the number ofwords that were aligned to p. In the best case, nowords are aligned to the token in p, and we cansimply remove it.
In the worst case, if all targetwords were aligned to p, this update will resultin the recalculation of all Viterbi Alignments.The algorithm proceeds until all valid cells havebeen computed.
One important aspect is that theupdate functions differ in complexity, so the se-quence of updates we apply will impact the per-formance of the system.
Most spans are reach-able using any of the four update functions.
Forinstance, the span A2,3,4,5 can be reached us-ing ?+v(A2,3,4,4), ?+u(A2,3,3,5), ?+q(A2,2,4,5) or?+p(A1,3,4,5).
However, we want to use ?+ua b - A Bab-ABa b - A Bpqu vpqu v?+vFigure 1: Illustration of the ?+v operator.
Thelight gray boxes show the parallel span and thedark boxes show the span?s Viterbi alignment.In this example, the parallel message contains a?translation?
of a b to A B.whenever possible, since it only requires one op-eration, although that is not always possible.
Forinstance, the state A2,2,2,4 cannot be reached us-ing ?+u, since the state A2,2,1,4 is not valid, be-cause the spans overlap.
If this happens, incre-mentally more expensive updates need to be used,such as ?+v, then ?+q, which are in the same orderof complexity.
Finally, we want to minimize theuse of ?+p, which is quadratic in the worst case.Thus, we use the following recursive formulationthat guarantees the optimal outcome:Ap,q,u,v =?????????
?+u(Ap,q,u?1,v) if u > q + 1?+v(Ap,q,u,v?1) else if v > q + 1?+p(Ap?1,q,u,v) else if q = p+ 1?+q(Ap,q?1,u,v) otherwiseThis transition function applies the cheapestpossible update to reach state Ap,q,u,v.Complexity analysis.
We can see that ?+uis only needed in the following the cases[0, 1][2, 2], [1, 2][3, 3], ?
?
?
, [n ?
2, n ?
1][n, n].Since, this update is quadratic in the worstcase, the complexity of this operations isO(n3).
The update ?+q, is applied to the cases[?, 1][2, 2], [?, 2][3, 3], ?
?
?
, [?, n?1], [n, n], where?
denotes any number within the span constraintsbut not present in previous updates.
Since, theupdate is linear and we need to iterate throughall tokens twice, this update takes O(n3) opera-tions.
The update ?+v is applied for the cases[?, 1][2, ?
], [?, 2][3, ?
], ?
?
?
, [?, n?
1], [n, ?].
Thus,with three degrees of freedom and a linear update,it runs in O(n4) time.
Finally, update ?+u runs inconstant time, but is run for all remaining cases,which constitute O(n4) space.
By summing the179executions of all updates, we observe that the or-der of magnitude of our exact inference process isO(n4).
Note that for exact inference, it is not pos-sible to get a lower order of magnitude, since weneed to at least iterate through all possible spanvalues once, which takes O(n4) time.4 Parallel Data ExtractionWe will now describe our method to extract par-allel data from Microblogs.
The target domainsin this work are Twitter and Sina Weibo, andthe main language pair is Chinese-English.
Fur-thermore, we also run the system for the Arabic-English language pair using the Twitter data.For the Twitter domain, we use a previouslycrawled dataset from the years 2008 to 2013,where one million tweets are crawled every day.In total, we processed 1.6 billion tweets.Regarding Sina Weibo, we built a crawler thatcontinuously collects tweets from Weibo.
We startfrom one seed user and collect his posts, and thenwe find the users he follows that we have not con-sidered, and repeat.
Due to the rate limiting es-tablished by the Weibo API1, we are restricted interms of number of requests every hour, whichgreatly limits the amount of messages we can col-lect.
Furthermore, each request can only fetch upto 100 posts from a user, and subsequent pages of100 posts require additional API calls.
Thus, tooptimize the number of parallel posts we can col-lect per request, we only crawl all messages fromusers that have at least 10 parallel tweets in theirfirst 100 posts.
The number of parallel messagesis estimated by running our alignment model, andchecking if ?
> ?, where ?
was set empiricallyinitially, and optimized after obtaining annotateddata, which will be detailed in 5.1.
Using thisprocess, we crawled 65 million tweets from SinaWeibo within 4 months.In both cases, we first filter the collection oftweets for messages containing at least one trigramin each language of the target language pair, deter-mined by their Unicode ranges.
This means thatfor the Chinese-English language pair, we onlykeep tweets with more than 3 Mandarin charac-ters and 3 latin words.
Furthermore, based on thework in (Jelh et al, 2012), if a tweet A is iden-tified as a retweet, meaning that it references an-other tweetB, we also consider the hypothesis thatthese tweets may be mutual translations.
Thus, ifA and B contain trigrams in different languages,1http://open.weibo.com/wiki/API?
?/enthese are also considered for the extraction of par-allel data.
This is done by concatenating tweets Aand B, and adding the constraint that [p, q] mustbe within A and [u, v] must be within B. Finally,identical duplicate tweets are removed.After filtering, we obtained 1124k ZH-ENtweets from Sina Weibo, 868k ZH-EN and 136kAR-EN tweets from Twitter.
These language pairsare not definite, since we simply check if there isa trigram in each language.Finally, we run our alignment model describedin section 3, and obtain the parallel segments andtheir scores, which measure how likely those seg-ments are parallel.
In this process, lexical tablesfor EN-ZH language pair used by Model 1 werebuilt using the FBIS dataset (LDC2003E14) forboth directions, a corpus of 300K sentence pairsfrom the news domain.
Likewise, for the EN-AR language pair, we use a fraction of the NISTdataset, by removing the data originated from UN,which leads to approximately 1M sentence pairs.5 ExperimentsWe evaluate our method in two ways.
First, intrin-sically, by observing how well our method identi-fies tweets containing parallel data, the languagepair and what their spans are.
Second, extrinsi-cally, by looking at how well the data improvesa translation task.
This methodology is similar tothat of Smith et al (2010).5.1 Parallel Data ExtractionData.
Our method needs to determine if a giventweet contains parallel data, and if so, what isthe language pair of the data, and what segmentsare parallel.
Thus, we had a native Mandarinspeaker, also fluent in English, to annotate 2000tweets sampled from crawled Weibo tweets.
Oneimportant question of answer is what portion ofthe Microblogs contains parallel data.
Thus, wealso use the random sample Twitter and annotated1200 samples, identifying whether each samplecontains parallel data, for the EN-ZH and AR-ENfiltered tweets.Metrics.
To test the accuracy of the score S, weordered all 2000 samples by score.
Then, we cal-culate the precision, recall and accuracy at increas-ing intervals of 10% of the top samples.
We countas a true positive (tp) if we correctly identify a par-allel tweet, and as a false positive (fp) spuriouslydetect a parallel tweet.
Finally, a true negative (tn)occurs when we correctly detect a non-parallel180tweet, and a false negative (fn) if we miss a par-allel tweet.
Then, we set the precision as tptp+fp ,recall as tptp+fn and accuracy as tp+tntp+fp+tn+fn .
Forlanguage identification, we calculate the accuracybased on the number of instances that were iden-tified with the correct language pair.
Finally, toevaluate the segment alignment, we use the WordError Rate (WER) metric, without substitutions,where we compare the left and right spans of oursystem and the respective spans of the reference.We count an insertion error (I) for each word inour system?s spans that is not present in the refer-ence span and a deletion error (D) for each wordin the reference span that is not present in our sys-tem?s spans.
Thus, we set WER = D+IN , whereN is the number of tokens in the tweet.
To com-pute this score for the whole test set, we computethe average of the WER for each sample.Results.
The precision, recall and accuracycurves are shown in Figure 2.
The quality of theparallel sentence detection did not vary signifi-cantly with different setups, so we will only showthe results for the best setup, which is the baselinemodel with span constraints.0.2?0.3?0.4?0.5?0.6?0.7?0.8?0.9?1?10%?
20%?
30%?
40%?
50%?
60%?
70%?
80%?
90%?
100%?Precision?Recall?Accuracy?Figure 2: Precision, recall and accuracy curvesfor parallel data detection.
The y-axis denotes thescores for each metric, and the x-axis denotes thepercentage of the highest scoring sentence pairsthat are kept.From the precision and recall curves, we ob-serve that most of the parallel data can be foundat the top 30% of the filtered tweets, where 5 in 6tweets are detected correctly as parallel, and only1 in every 6 parallel sentences is lost.
We will de-note the score threshold at this point as ?, which isa good threshold to estimate on whether the tweetis parallel.
However, this parameter can be tunedfor precision or recall.
We also see that in total,30% of the filtered tweets are parallel.
If we gen-eralize this ratio for the complete set with 1124ktweets, we can expect approximately 337k paral-lel sentences.
Finally, since 65 million tweets wereextracted to generate the 337k tweets, we estimatethat approximately 1 parallel tweet can be foundfor every 200 tweets we process using our tar-geted approach.
On the other hand, from the 1200tweets from Twitter, we found that 27 had paralleldata in the ZH-EN pair, if we extrapolate for thewhole 868k filtered tweets, we expect that we canfind 19530.
19530 parallel sentences from 1.6 bil-lion tweets crawled randomly, represents 0.001%of the total corpora.
For AR-EN, a similar re-sult was obtained where we expect 12407 tweetsout of the 1.6 billion to be parallel.
This showsthat targeted approaches can substantially reducethe crawling effort required to find parallel tweets.Still, considering that billions of tweets are posteddaily, this is a substantial source of parallel data.The remainder of the tests will be performed onthe Weibo dataset, which contains more paralleldata.
Tests on the Twitter data will be conductedas future work, when we process Twitter data on alarger scale to obtain more parallel sentences.For the language identification task, we had anaccuracy of 99.9%, since distinguishing Englishand Mandarin is trivial.
The small percentage oferrors originated from other latin languages (Ex:French) due to our naive language detector.As for the segment alignment task.
Our base-line system with no constraints obtains a WER of12.86%, and this can be improved to 11.66% byadding constraints to possible spans.
This showsthat, on average, approximately 1 in 9 words onthe parallel segments is incorrect.
However, trans-lation models are generally robust to such kinds oferrors and can learn good translations even in thepresence of imperfect sentence pairs.Among the 578 tweets that are parallel, 496were extracted within the same tweet and 82 wereextracted from retweets.
Thus, we see that the ma-jority of the parallel data comes from within thesame tweet.Topic analysis.
To give an intuition about thecontents of the parallel data we found, we lookedat the distribution over topics of the paralleldataset inferred by LDA (Blei et al, 2003).
Thus,we grouped the Weibo filtered tweets by users,and ran LDA over the predicted English segments,with 12 topics.
The 7 most interpretable topics areshown in Table 1.
We see that the data contains a181# Topic Most probable words in topic1 (Dating) love time girl live mv back word night rt wanna2 (Entertainment) news video follow pong image text great day today fans3 (Music) cr day tour cn url amazon music full concert alive4 (Religion) man god good love life heart would give make lord5 (Nightlife) cn url beijing shanqi party adj club dj beijiner vt6 (Chinese News) china chinese year people world beijing years passion country government7 (Fashion) street fashion fall style photo men model vogue spring magazineTable 1: Most probable words inferred using LDA in several topics from the parallel data extracted fromWeibo.
Topic labels (in parentheses) were assigned manually for illustration purposes.variety of topics, both formal (Chinese news, reli-gion) and informal (entertainment, music).Example sentence pairs.
To gain some perspec-tive on the type of sentence pairs we are extract-ing, we will illustrate some sentence pairs wecrawled and aligned automatically.
Table 2 con-tains 5 English-Mandarin and 4 English-Arabicsentence pairs that were extracted automatically.These were chosen, since they contain some as-pects that are characteristic of the text present inMicroblogs and Social Media.
These are:?
Abbreviations - In most sentence pairs exam-ples, we can witness the use of abbreviatedforms of English words, such as wanna, TMI,4 and imma.
These can be normalized as wantto, too much information, for and I am goingto, respectively.
In sentence 5, we observe thatthis phenomena also occurs in Mandarin.
Wefind that TMD is a popular way to write??
?whose Pinyin rendering is ta?
ma?
de.
The mean-ing of this expression depends on the context itis used, and can convey a similar connotationas adding the intensifier the hell to an Englishsentence.?
Jargon - Another common phenomena is theappearance of words that are only used in sub-communities.
For instance, in sentence pair 4,we the jargon word cday is used, which is a col-loquial variant for birthday.?
Emoticons - In sentence 8, we observe the pres-ence of the emoticon :), which is frequentlyused in this media.
We found that emoticons areeither translated as they are or simply removed,in most cases.?
Syntax errors - In the domain of microblogs, itis also common that users do not write strictlysyntactic sentences, for instance, in sentencepair 7, the sentence onni this gift only 4 u, isclearly not syntactically correct.
Firstly, onniis a named entity, yet it is not capitalized.
Sec-ondly, a comma should follow onni.
Thirdly, theverb is should be used after gift.
Having exam-ples of these sentences in the training set, withcommon mistakes (intentional or not), mightbecome a key factor in training MT systems thatcan be robust to such errors.?
Dialects - We can observe a much broader rangeof dialects in our data, since there are no di-alect standards in microblogs.
For instance, insentence pair 6, we observe an arabic word (inbold) used in the spoken Arabic dialect used insome countries along the shores of the PersianGulf, which means means the next.
In standardArabic, a significantly different form is used.We can also see in sentence pair 9 that ouraligner does not alway make the correct choicewhen determining spans.
In this case, the segmentRT @MARYAMALKHAWAJA: was included in theEnglish segment spuriously, since it does not cor-respond to anything in the Arabic counterpart.5.2 Machine Translation ExperimentsWe report on machine translation experiments us-ing our harvested data in two domains: editednews and microblogs.News translation.
For the news test, we cre-ated a new test set from a crawl of the Chinese-English documents on the Project Syndicate web-site2, which contains news commentary articles.We chose to use this data set, rather than morestandard NIST test sets to ensure that we had re-cent documents in the test set (the most recentNIST test sets contain documents published in2007, well before our microblog data was created).We extracted 1386 parallel sentences for tuningand another 1386 sentences for testing, from themanually aligned segments.
For this test set, weused 8 million sentences from the full NIST par-allel dataset as the language model training data.We shall call this test set Syndicate.2http://www.project-syndicate.org/182ENGLISH MANDARIN1 i wanna live in a wes anderson world ?????
?Wes Anderson???
?2 Chicken soup, corn never truly digests.
TMI.
??????????????????.?
?3 To DanielVeuleman yea iknw imma work on that ?DanielVeuleman????????????????
?4 msg 4 Warren G his cday is today 1 yr older.
???
?Warren G???????????????
?5 Where the hell have you been all these years?
????TMD???
?ENGLISH ARABIC6 It?s gonna be a warm week!
Qk ?AJ?
@ ?
?J.
?B@7 onni this gift only 4 u ??
??
?
?KY??
@ ?
Y?
?G?
@8 sunset in aqaba :) (: ?J.
???
@ ??
??
??
@ H. ?Q?9 RT @MARYAMALKHAWAJA: there is a call @Y 	?
?
?A 	J?
?Y?
??
H@Q?A 	???
Z @Y 	K ?A 	J?for widespread protests in #bahrain tmrwTable 2: Examples of English-Mandarin and English-Arabic sentence pairs.
The English-Mandarinsentences were extracted from Sina Weibo and the English-Arabic sentences were extracted from Twitter.Some messages have been shorted to fit into the table.
Some interesting aspects of these sentence pairsare marked in bold.Microblog translation.
To carry out the mi-croblog translation experiments, we need a highquality parallel test set.
Since we are not awareof such a test set, we created one by manually se-lecting parallel messages from Weibo.
Our proce-dure was as follows.
We selected 2000 candidateWeibo posts from users who have a high num-ber of parallel tweets according to our automaticmethod (at least 2 in every 5 tweets).
To these, weadded another 2000 messages from our targetedWeibo crawl, but these had no requirement on theproportion of parallel tweets they had produced.We identified 2374 parallel segments, of which weused 1187 for development and 1187 for testing.We refer to this test set as Weibo.3Obviously, we removed the development andtest sets from our training data.
Furthermore, toensure that our training data was not too similar tothe test set in the Weibo translation task, we fil-tered the training data to remove near duplicatesby computing edit distance between each paral-lel sentence in the heldout set and each traininginstance.
If either the source or the target sidesof the a training instance had an edit distance ofless than 10%, we removed it.4 As for the lan-guage models, we collected a further 10M tweetsfrom Twitter for the English language model andanother 10M tweets from Weibo for the Chineselanguage model.3We acknowledge that self-translated messages are prob-ably not a typically representative sample of all microblogmessages.
However, we do not have the resources to producea carefully curated test set with a more broadly representativedistribution.
Still, we believe these results are informative aslong as this is kept in mind.4Approximately 150,000 training instances removed.Syndicate WeiboZH-EN EN-ZH ZH-EN EN-ZHFBIS 9.4 18.6 10.4 12.3NIST 11.5 21.2 11.4 13.9Weibo 8.75 15.9 15.7 17.2FBIS+Weibo 11.7 19.2 16.5 17.8NIST+Weibo 13.3 21.5 16.9 17.9Table 3: BLEU scores for different datasets in dif-ferent translation directions (left to right), brokenwith different training corpora (top to bottom).Baselines.
We report results on these test sets us-ing different training data.
First, we use the FBISdataset which contains 300K high quality sentencepairs, mostly in the broadcast news domain.
Sec-ond, we use the full 2012 NIST Chinese-Englishdataset (approximately 8M sentence pairs, includ-ing FBIS).
Finally, we use our crawled data (re-ferred as Weibo) by itself and also combined withthe two previous training sets.Setup.
We use the Moses phrase-based MT sys-tem with standard features (Koehn et al, 2003).For reordering, we use the MSD reorderingmodel (Axelrod et al, 2005).
As the languagemodel, we use a 5-gram model with Kneser-Ney smoothing.
The weights were tuned usingMERT (Och, 2003).
Results are presented withBLEU-4 (Papineni et al, 2002).Results.
The BLEU scores for the different par-allel corpora are shown in Table 3 and the top 10out-of-vocabulary (OOV) words for each datasetare shown in Table 4.
We observe that for theSyndicate test set, the NIST and FBIS datasets183Syndicate (test) Weibo (test)FBIS NIST Weibo FBIS NIST Weiboobama (83) barack (59) democracies (15) 2012 (24) showstudio (9) submissions (4)barack (59) namo (6) imbalances (13) alanis (13) crue (9) ivillage (4)princeton (40) mitt (6) mahmoud (12) crue (9) overexposed (8) scola (3)ecb (8) guant (6) millennium (9) showstudio (9) tweetmeian (5) rbst (3)bernanke (8) fairtrade (6) regimes (8) overexposed (8) tvd (5) curitiba (3)romney (7) hollande (5) wolfowitz (7) itunes (8) iheartradio (5) zeman (2)gaddafi (7) wikileaks (4) revolutions (7) havoc (8) xoxo (4) @yaptv (2)merkel (7) wilders (3) qaddafi (7) sammy (6) snoop (4) witnessing (2)fats (7) rant (3) geopolitical (7) obama (6) shinoda (4) whoohooo (2)dialogue (7) esm (3) genome (7) lol (6) scrapbook (4) wbr (2)Table 4: The most frequent out-of-vocabulary (OOV) words and their counts for the two English-sourcetest sets with three different training sets.perform better than our extracted parallel data.This is to be expected, since our dataset was ex-tracted from an extremely different domain.
How-ever, by combining the Weibo parallel data withthis standard data, improvements in BLEU are ob-tained.
Error analysis indicates that one major fac-tor is that names from current events, such as Rom-ney and Wikileaks do not occur in the older NISTand FBIS datasets, but they are represented in theWeibo dataset.
Furthermore, we also note that thesystem built on the Weibo dataset does not per-form substantially worse than the one trained onthe FBIS dataset, a further indication that harvest-ing parallel microblog data yields a diverse collec-tion of translated material.For the Weibo test set, a significant improve-ment over the news datasets can be achieved us-ing our crawled parallel data.
Once again newerterms, such as iTunes, are one of the reasons olderdatasets perform less well.
However, in this case,the top OOV words of the news domain datasetsare not the most accurate representation of cov-erage problems in this domain.
This is becausemany frequent words in microblogs, e.g., nonstan-dard abbreviations, like u and 4 are found in thenews domain as words, albeit with different mean-ings.
Thus, the OOV table gives an incompletepicture of the translation problems when usingthe news domain corpora to translate microblogs.Also, some structural errors occur when trainingwith the news domain datasets, one such exampleis shown in table 5, where the character ?
is in-correctly translated to said.
This occurs becausethis type of constructions is infrequent in newsdatasets.
Furthermore, we can see that compoundexpressions, such as the translation from ????
to party time are also learned.Finally, we observe that combining the datasetsSource ?sam farrar?????
?Reference to sam farrar , party timeFBIS farrar to sam said , in timeNIST to sam farrar said , the momentWEIBO to sam farrar , party timeTable 5: Translation Examples using differenttraining sets.yields another gain over individual datasets, bothin the Syndicate and in the Weibo test sets.6 ConclusionWe presented a framework to crawl parallel datafrom microblogs.
We find parallel data from sin-gle posts, with translations of the same sentencein two languages.
We show that a considerableamount of parallel sentence pairs can be crawledfrom microblogs and these can be used to improveMachine Translation by updating our translationtables with translations of newer terms.
Further-more, the in-domain data can substantially im-prove the translation quality on microblog data.The resources described in this paper and fur-ther developments are available to the general pub-lic at http://www.cs.cmu.edu/?lingwang/utopia.AcknowledgementsThe PhD thesis of Wang Ling is supported by FCTgrant SFRH/BD/51157/2010.
The authors wishto express their gratitude to thank William Cohen,Noah Smith, Waleed Ammar, and the anonymousreviewers for their insight and comments.
We arealso extremely grateful to Brendan O?Connor forproviding the Twitter data and to Philipp Koehnand Barry Haddow for providing the Project Syn-dicate data.184References[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,Chris Callison-burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh system description for the2005 iwslt speech translation evaluation.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation (IWSLT.
[Blei et al2003] David M. Blei, Andrew Y. Ng, andMichael I. Jordan.
2003.
Latent dirichlet alocation.J.
Mach.
Learn.
Res., 3:993?1022, March.
[Braune and Fraser2010] Fabienne Braune and Alexan-der Fraser.
2010.
Improved unsupervised sentencealignment for symmetrical and asymmetrical paral-lel corpora.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 81?89, Stroudsburg, PA, USA.Association for Computational Linguistics.
[Brown et al1993] Peter F. Brown, Vincent J. DellaPietra, Stephen A. Della Pietra, and Robert L. Mer-cer.
1993.
The mathematics of statistical machinetranslation: parameter estimation.
Comput.
Lin-guist., 19:263?311, June.
[Fukushima et al2006] Ken?ichi Fukushima, KenjiroTaura, and Takashi Chikayama.
2006.
A fast andaccurate method for detecting English-Japanese par-allel texts.
In Proceedings of the Workshop on Mul-tilingual Language Resources and Interoperability,pages 60?67, Sydney, Australia, July.
Associationfor Computational Linguistics.
[Gimpel et al2011] Kevin Gimpel, Nathan Schneider,Brendan O?Connor, Dipanjan Das, Daniel Mills, Ja-cob Eisenstein, Michael Heilman, Dani Yogatama,Jeffrey Flanigan, and Noah A. Smith.
2011.
Part-of-speech tagging for twitter: annotation, features,and experiments.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: shortpapers - Volume 2, HLT ?11, pages 42?47, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.
[Jelh et al2012] Laura Jelh, Felix Hiebel, and StefanRiezler.
2012.
Twitter translation using translation-based cross-lingual retrieval.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 410?421, Montre?al, Canada, June.
Asso-ciation for Computational Linguistics.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,and Daniel Marcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology - Volume 1, NAACL ?03, pages 48?54,Morristown, NJ, USA.
Association for Computa-tional Linguistics.
[Koehn2005] Philipp Koehn.
2005.
Europarl: A Par-allel Corpus for Statistical Machine Translation.
InProceedings of the tenth Machine Translation Sum-mit, pages 79?86, Phuket, Thailand.
AAMT, AAMT.
[Li and Liu2008] Bo Li and Juan Liu.
2008.
MiningChinese-English parallel corpora from the web.
InProceedings of the 3rd International Joint Confer-ence on Natural Language Processing (IJCNLP).
[Lin et al2008] Dekang Lin, Shaojun Zhao, BenjaminVan Durme, and Marius Pas?ca.
2008.
Mining par-enthetical translations from the web by word align-ment.
In Proceedings of ACL-08: HLT, pages 994?1002, Columbus, Ohio, June.
Association for Com-putational Linguistics.
[Och2003] Franz Josef Och.
2003.
Minimum error ratetraining in statistical machine translation.
In Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics - Volume 1, ACL ?03,pages 160?167, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: amethod for automatic evaluation of machine trans-lation.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, ACL?02, pages 311?318, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.
[Post et al2012] Matt Post, Chris Callison-Burch, andMiles Osborne.
2012.
Constructing parallel cor-pora for six indian languages via crowdsourcing.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 401?409, Montre?al,Canada, June.
Association for Computational Lin-guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.Smith.
2003.
The web as a parallel corpus.
Compu-tational Linguistics, 29:349?380.
[Smith et al2010] Jason R. Smith, Chris Quirk, andKristina Toutanova.
2010.
Extracting parallel sen-tences from comparable corpora using documentlevel alignment.
In Proceedings of the 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics.
[Ture and Lin2012] Ferhan Ture and Jimmy Lin.
2012.Why not grab a free lunch?
mining large corpora forparallel sentences to improve translation modeling.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 626?630, Montre?al, Canada, June.
Associa-tion for Computational Linguistics.
[Uszkoreit et al2010] Jakob Uszkoreit, Jay Ponte,Ashok C. Popat, and Moshe Dubiner.
2010.
Largescale parallel document mining for machine transla-tion.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, pages 1101?1109.
[Vogel et al1996] Stephan Vogel, Hermann Ney, andChristoph Tillmann.
1996.
Hmm-based word align-ment in statistical translation.
In Proceedings of the16th conference on Computational linguistics - Vol-ume 2, COLING ?96, pages 836?841, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.
[Xu et al2001] Jinxi Xu, Ralph Weischedel, and ChanhNguyen.
2001.
Evaluating a probabilistic model185for cross-lingual information retrieval.
In Proceed-ings of the 24th annual international ACM SIGIRconference on Research and development in infor-mation retrieval, SIGIR ?01, pages 105?110, NewYork, NY, USA.
ACM.
[Xu et al2005] Jia Xu, Richard Zens, and HermannNey.
2005.
Sentence segmentation using ibm wordalignment model 1.
In Proceedings of EAMT 2005(10th Annual Conference of the European Associa-tion for Machine Translation, pages 280?287.
[Zbib et al2012] Rabih Zbib, Erika Malchiodi, JacobDevlin, David Stallard, Spyros Matsoukas, RichardSchwarz, John Makhoul, Omar F. Zaidan, and ChrisCallison-Burch.
2012.
Machine translation of Ara-bic dialects.
In Proceedings of the 2012 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies.186
