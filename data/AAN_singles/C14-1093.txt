Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 984?995, Dublin, Ireland, August 23-29 2014.A Hybrid Approach to Features Representation for Fine-grained ArabicNamed Entity RecognitionFahd AlotaibiSchool of Computer ScienceUniversity of Birmingham, UKfsa081@cs.bham.ac.ukFaculty of ComputingKing Abdulaziz University, KSAfsalotaibi@kau.edu.saMark LeeSchool of Computer ScienceUniversity of Birmingham, UKm.g.lee@cs.bham.ac.ukAbstractDespite considerable research on the topic of Arabic Named Entity Recognition (NER), almostall efforts focus on a traditional set of semantic classes, features and token representations.
Inthis work, we advance previous research in a systematic manner and devise a novel methodto represent these features, relying on a dependency-based structure to capture further evidencewithin the sentence.
Moreover, the work also describes an evaluation of the method involving thecapture of global features and employing the clustering of unannotated textual data.
To meet thisset of goals, we conducted a series of evaluations to evaluate different aspects that demonstrategreat improvement when compared with the baseline model.1 IntroductionTraditionally, the focus of Arabic NER has been on a very limited number of semantic classes, i.e.PERSON, ORGANISATION and LOCATION, utilising the newswire domain such as those describedby Benajiba and Rosso (2008), Benajiba et al.
(2010) and Abdul-Hamid and Darwish (2010) .
This limitshigher-level applications (such as question answering) from extracting in-depth knowledge and workingon a relatively open domains.This paper addresses the issue of a fine-grained NER of 50 classes for Arabic and presents a com-prehensive set of experiments that evaluate innovative means of representing the features set.
Thus, thecontribution of this paper falls into different categories with unique outcomes, as follows:1.
A novel approach to representing the features is used, relying on dependency representation.
Thisrepresentation overcomes the drawback of current window-based representations of features.2.
The representation of global evidence involves clustering unannotated textual data, employing hier-atical clusters (Brown et al., 1992).3.
Due to the fact that there is no comparable work to use as a comparison in the task of Arabic fine-grained NER, a baseline model was developed, based on Conditional Random Fields (CRF), usingthe best features, as established and used elsewhere in the literature.4.
Development of publically available gold-standard fine-grained NER corpora1from two differentgenres, i.e.
Newswire and Wikipedia.Each contribution is discussed in more detail during in the remainder of this paper.2 Arabic Fine-grained Named Entity CorporaThe majority of Arabic NER approaches are supervised, ensuring that the machine learns from an an-notated corpus and aims to predict unseen text.
This approach requires a reasonable bank of labelleddata.
This section examines the availability of such an annotated dataset at the fine-grained level, and thecreation of gold-standard corpora.This work is licensed under a Creative Commons Attribution 4.0 International Licence.1Available at: http://sourceforge.net/projects/arabic-named-entity-corpora/ andMirror at: http://fsalotaibi.kau.edu.sa/Pages-Arabic-NE-Corpora.aspx9842.1 Available CorporaOne of the earliest corpora publically released was ANERcorp, developed by Benajiba et al.
(2007).
Thisis a newswire based corpus and follows the CoNLL format.
It annotates into four coarse-grained classes:PERSON, ORGANISATION, LOCATION, and MISCELLANEOUS.
This dataset has been extensivelyused such as in (Benajiba et al., 2008b; Benajiba et al., 2010; Abdul-Hamid and Darwish, 2010).Among corpora applying a fine-grained level of classes are those released by the Linguistic DataConsortium2(LDC).
They released two multilingual NE corpora including Arabic (Mitchell et al., 2005;Walker et al., 2006).
Both corpora were used in the Automatic Content Extraction (ACE) technologyevaluation, at the coarse-grained level only.
However, these corpora are governed by a costly annuallicense, which prevents the researcher from accessing and utilising them.
At present, we are not awareof a study tackling fine-grained Arabic NER using this dataset.Alotaibi and Lee (2013) released fine-grained Arabic NE corpora - WikiFANESelectiveandWikiFANEWhole.
These were built automatically using the Arabic version of Wikipedia and releasedunder the Creative Commons Attribution-ShareAlike 3.0 Unported Licence.3.
These corpora apply asimilar annotation taxonomy to that of the ACE corpus, but deliver increased coverage through the in-clusion of a new class, i.e.
PRODUCT, which includes Books, Movies, Sound, Hardware, Software,Food, Drugs and Other.
Moreover, the corpora divide the PERSON class into 10 fine-classes, in orderto provide wider coverage (i.e.
Politician, Athlete, Businessperson, Artist, Scientist, Police, Religious,Engineer and Group).
It is notable that this taxonomy can be easily mapped into CoNLL and ACE ateither the coarse or fine-grained levels.2.2 Creating Gold-standard Fine-grained Named Entity CorporaSince the aim of this paper is to conduct an in-depth experiment for fine-grained Arabic NE, we manuallycreated gold-standard fine-grained NE corpora for Arabic, drawing on two different genres.
This gives acritical benchmark for evaluation and comparison with the automatically constructed corpus.The first corpus is newswire-based, using the same textual data appearing in ANERcorp.
The completecorpus was re-annotated to the fine-grained level.
The second corpus is drawn from the Arabic version ofWikipedia.
The selection of articles was made using a random heuristic, i.e.
selecting articles discussinga named entity and maintaining a fair level of distribution among the classes.
Moreover, the amount oftextual data drawn from the Wikipedia article was restricted by avoiding such elements as lists, headings,and captions on images and tables.2.3 Annotation Strategy and Quality EvaluationFor both corpora, the two-level taxonomy presented by Alotaibi and Lee (2013) was applied.
This con-sists of 8 coarse-grained classes and 50 fine-grained classes.
An in-house tool to facilitate the annotationprocess was developed.
Two independent graduate-level Arabic native speakers were engaged to anno-tate the entire corpora.
They were provided with extended instructions to guide them in the annotationprocess and a number of feedback sessions were conducted in the early stages of the process to ensurethat any difficulties could be resolved.After its completion, the quality of the annotation was evaluated by calculating the inter-annotator agreement between both annotators.
The entity F-measure was used to evaluate the inter-annotation agreement as in (Hripcsak and Rothschild, 2005; Zhang, 2013).
The corpora were namedNewsFANEGoldand WikiFANEGold, referring to News-based, and Wikipedia-based, Fine-grained Ara-bic Named Entity Gold corpus, respectively.
Micro-averaging was used while matching exact phrases, inorder to calculate the agreement.
The size and the inter-annotator agreement of NewsFANEGoldis 170Kof tokens and 91% while WikiFANEGoldis 500K of tokens and 89% .2https://www.ldc.upenn.edu/3Available at: http://www.cs.bham.ac.uk/?fsa081/resources.htmlMirror at: http://sourceforge.net/projects/arabic-named-entity-corpora/985Corpus Token level Phrase levelNewsFANEGold10.7 6.7WikiFANEGold13.1 7.4WikiFANESelective10.8 6.4WikiFANEWhole7.08 4.9Table 1: The density of NEs on token and phrase levelsCorpusLength1 2 3 4 5 6 7 8NewsFANEGold58.19 30.77 8 1.73 0.82 0.21 0.2 0.04WikiFANEGold51.75 31.55 10.88 3.48 1.34 0.46 0.21 0.12WikiFANESelective48.27 37.95 10.22 2.98 0.41 0.11 0.05 0.01WikiFANEWhole66.22 25.85 6.02 1.58 0.05 0.02 0.01 0.01Table 2: The distribution of NE phrases relative to length.3 Corpus-based Evaluation and ComparisonIt is important to closely evaluate and compare different corpora.
The nature of the distribution of NEphrases is expected to differ to some extent, affecting the performance of learning the probabilistic model.Therefore, the coverage of NE phrases related to different aspects was studied, including the distributionof density, length and semantic classes.3.1 The Density of NEThe density represents the coverage of NEs at the level of tokens and phrases.
As can be seen in Ta-ble 1, WikiFANEGoldhas the greater density at both levels.
This demonstrates that the Wikipedia-based gold corpus tends to represent more NE phrases in context than that of the newswire-based.Although WikiFANEGoldis 0.7% denser than NewsFANEGoldin the phrase level, it reveals a notabledifference (2.4%) in the token level.
This indicates that WikiFANEGoldpossess a greater variety in thelength of NE phrases than the newswire-based corpus.
However, the automatically developed corpus,WikiFANESelective, has a similar density of coverage as NewsFANEGoldwhereas the WikiFANEWholedemonstrates a low level of density, due to its method of compilation.3.2 The Distribution of the Length of Named Entity PhrasesIt can be seen in Table 2, NewsFANEGoldand WikiFANEWholetend to have more single-word NEphrases than other corpora.
When it comes to the newswire corpus, this is due to differences in the waythe NE phrases are written in a newswire domain.
On the other hand, the boundaries of multi-word NEphrases are difficult to detect, in Arabic, due to the fact that the language has a complex morphology.This is demonstrated in the Wikipedia corpora, i.e.
the gold and the selective - less than half the NEphrases in WikiFANESelectiveare single-word, with a slightly higher rate found in WikiFANEGold.3.3 The Distribution of the Fine-grained ClassesThis demonstrates the distribution of NE phrases into fine-grained classes according to their annotation.As shown in Figure 1, the majority of classes tend (to some extent) to have a relatively harmonic dis-tribution.
In general, the newswire-based corpus tends to include more NE phrases related to politics,government, commerce, nations and cities, whereas the automatically-built corpora score a very highfrequency on NE types such as ?Nation?
and ?Population-centre?.
Moreover, WikiFANEGoldshows widedistribution on most of the fine-grained classes of ?PERSON?, ?LOCATION?, ?FACILITY?, ?VEHICLE?and ?PRODUCT?, compared to other corpora.986Figure 1: Distribution of Fine-grained Classes4 The Baseline Model for Fine-grained Arabic NERIn order to prepare the baseline model and conduct successive experiments, the dataset for each corpuswas divided into training, development and test.
It is important to emphasise that, due to the limitationsof computation power and the space allocated for the machine used, only a portion of WikiFANESelectiveand WikiFANEWholewere selected with a size of ?500K tokens each.
The following table shows eachcorpus and its size.Corpus Type Training Dev TestNewsFANEGoldgold-standard 120K 25K 25KWikiFANEGoldgold-standard 350K 75K 75KWikiFANESelectiveautomatically-developed 354K 73K 73KWikiFANEWholeautomatically-developed 356K 72K 72KTable 3: The size of the training, development and test for each corpusSince there is no comparative work in the form of a fine-grained Arabic NER to use as a comparison,a baseline model based on Conditional Random Fields (CRF) was developed.
It was decided to use themost successful features of the coarse-grained NER.
For this purpose, the following features were ex-tracted: Lexical and contextual features (current token, two tokens before and after the current token,first and last three characters of the token, and length of the token); Morphological features (gender,number and person); Syntactical features (part of speech and base phrase chunk); and External knowl-edge (the presence of the token in the gazetteer developed by Alotaibi and Lee (2013)).
It was decided touse the BILOU scheme representation for the baseline model and successive experiments, as suggestedby Ratinov and Roth (2009).
The performance of the baseline model is presented in Table 4.CorpusDevelopment TestP R F P R FNewsFANEGold79.58 57.87 67.01 73.07 53.34 61.67WikiFANEGold62.19 43.67 51.31 68.13 44.78 54.04WikiFANESelective89.01 68.92 77.69 88.69 60.37 71.84WikiFANEWhole82.35 49.83 62.09 84.27 58.63 69.15Table 4: The results of the baseline model by learning CRF classifier with traditional features9875 Dependency based Features RepresentationThe current representation of the sequence tagging classifier involves using a predefined window of to-kens (e.g.
with size 5, including the current token) in order to capture local evidence.
This representationhas the following three drawbacks:1.
It is restricted to only capturing local evidence.2.
It fails to capture the relationship between dependent tokens, particularly for long sentences andmultiword NE phrases.3.
Since Arabic has a relatively free word order, the window-based feature representation cannot cap-ture the order variation for different examples.In this paper, a new approach has been devised to utilise further evidence within a sentence in the classifi-cation process.
The key idea informing this approach was to rely on the dependency-based representationof sentences in order to extract valuable features.The dependency structure is one of syntactical representations, where a sentence is analysed by con-necting its words in a word-to-word relationship.
These relationships specify the head and dependenttokens in context, and assign a grammatical role for each token, e.g.
subject, object and modifier.To elaborate on the amount of knowledge that can be utilised based on the dependency structure,consider the following sentences:?
(?A... dm yJ ?
?rJ yJ ?A?wO?  ??
Ty?
?F?  ?
?Am?   A  Hl?
Hy?C ?A?
/qAlr?yys mjls AtHAd AlmHAkm AlAslAmy~ fy AlSwmAl ?syx ?sryf ?syx OHmd fy ...Alx/ ?The head ofthe Council of the Islamic Courts Union, Sheikh Sharif Sheikh Ahmed, said in Somalia ...etc.?)?
(Cwy?
?w Ah ?A?
?t?  ?ry?  ?CA?z?  d` ?zyl?
?  ?FAys?  ??Cw?
z?CAJ ?wq??A...
Ay?AW?r ? CE?
Hy?C /yqwl ?sArlz mwrfy AlsyAsy AlAnjlyzy b?d AlzyAr~ AlOxyr~ AltyqAm bhA jwn myjwr r?yys wzrA?
bryTAnyA ...Alx/ ?Charles Murphy, the English politician, saidafter the recent visit by John Major, Britain?s prime minister ...
etc.?)?
(2000 HWs?  ??
?A?wOl?
?Asy?C 	t?  ?s  ?}
? r?@?
/y?kr On SlAd Hsn Antxbr?yysA?a llSwmAl fy A?sTs?Ab 2000/ ?It was mentioned that, Salad Hassan was elected as presidentof Somalia in August 2000?
)The dependency representation and an English gloss of each example are shown in Figure 2.
The parsedoutput includes a new set of information, which can be utilised as features, as follows:1.
Head and Dependent Relation: The relationship between the head and the dependent is oneof the most important features to capture.
Consider the token (yJ /?syx/ ?Shaikh?
), in Figure 2a; thehead (Hy?C /r?yys/ ?the head of?)
is located far away and cannot be captured in the local window-basedrepresentation.
Moreover, the vice versa relationship between the dependent and head is also useful.Consider the example in Figure 2b: the token (?w /jwn/ ?John?)
has two dependents (Cwy?
/myjwr/?Major?)
and (Hy?C /r?yys/ ?Prime?
)4 where the latter dependent (i.e.
?Hy?C?)
gives a useful clue of theway in which it has been used in political contexts.
The sequence of heads or dependents can also beutilised in the same way.2.
Sibling Relation: The sibling tokens are those dependent on the same head.
Siblings can belocated near each other in context, or appear at a distance.
For example: the sibling of the token (yJ/?syx/ ?Shaikh?)
is (Hl?
/mjls/ ?council?
), in Figure 2a, is expected to appear in a political context,which gives a clue towards the target NE class.
Meanwhile, the token (??
/fy/ ?in?)
is also a sibling, andcan be avoided as it is a stop word.
This is also the case in the example presented in Figure 2c, wherethe token ( ?}
/SlAd/ ?Salad?)
is a sibling to the token (	t?  /Antxb/ ?elected?
), which relates to thepolitical context.4Different contexts yield different English translation of the token ?Hy?C?
as ?the head of?
and ?Prime?988?A?
Hy?C Hl?
A  ?
?Am?  Ty?
?F?  ??
?A?wO?  yJ ?
?rJ yJ dm11110010 1110000110 1110000111 111000010 1000 1101100 1111110 10100 1100100 1100100 1100100 1100100qAl rYys mjls At.hAd Alm.hAkm AlAslAmyh fy Al.swmAl ?syx ?sryf ?syx A.hmdsaid head council union courts islamic in Somalia Shaikh Sheriff Shaikh AhmedVRB NOM NOM NOM Al-NOM Al-NOM-p PRT-PREP NOM-PROP NOM-PROP NOM-PROP NOM-PROP NOM-PROPO O B-Gov I-Gov I-Gov L-Gov O U-Nation B-Poli I-Poli I-Poli L-PolirootSBJIDFIDFIDFMODMODOBJMOD?
?
?
(a) The first example.
(The second row represents the clusters according to the Brown algorithm)?wq?
z?CAJ ??Cw?
?FAys?  ?zyl?
?  d` ?CA?z?  ?ry?  ?t?  ?A?A?
?w Cwy?
Hy?C ? CE?
Ay?AW?ryqwl ?sArlz mwrfy AlsyAsy AlAnjlyzy bEd AlzyArp AlAxyrp Alty qAm b hA jwn myjwr rYys wzrA?
bryTAnyAsays Charles Murphy politician English after visit recent which did for it John Major prime minister BritainVRB NOM NOM-y Al-NOM-y Al-NOM-y NOM-PREP Al-NOM-p Al-NOM-p Al-NOM-y VRB PRT NOM-PRON NOM-PROP NOM-PROP NOM NOM NOM-PROPO B-Poli L-Poli O O O O O O O O O B-Poli L-Poli O O B-NationrootSBJIDFMODMODMODIDFMODMODMODMODOBJSBJ?MODIDFIDF(b) The second exampler?@?
?  ?}
?s 	t?  ?Asy?C ?
?A?wO?  ??
HWs?  2000mentioned that Salad Hasan elected president for Somalia in August August 2000y*kr An SlAd Hsn Antxb rYysA l AlSwmAl fy AgsTs Ab 2000mentioned that Salad Hasan elected president for Somalia in August August 2000VRB PRT-An NOM NOM-PROP VRB-PASS NOM PRT-l NOM-PROP PRT-PREP NOM-PROP NOM-PROP NUM-NOMO O B-Politician L-Politician O O B-Nation L-Nation O O O OrootSBJSBJMODMODMODMODOBJMODOBJ?MOD(c) The third exampleFigure 2: The examples of a dependency structure.
The rows show the Arabic token, Buckwalter translit-eration, English gloss, POS and NE tag, respectively (the sentence is displayed left to right).3.
Syntactic Roles: The syntactical roles also benefit by being utilised to capture NE phrases incontext.
Among those with concern for NER are:a. SBJ and OBJ: defines which subject and object roles are assigned to the head token of the NEphrase.
For example, the tokens ( ?}
/SlAd/ ?Salad?)
and (z?CAJ /?sArlz/ ?Charles?)
are tagged assubjects.b.
IDF5: the Idafa chain is another important syntactical role, which helps to identify multiword NEphrases.
For example: the token (??Cw?
/mwrfy/ ?Murphy?)
is tagged as an IDF of its previous token(z?CAJ /?sArlz/ ?Charles?
), where this indicates a multiword NE phrase.
This is also the case for theexample (Ty?
?F?  ?
?Am?   A  Hl?
/mjls AtHAd AlmHAkm AlIslAmy~/ ?Council of theIslamic Courts Union?)
where all tokens are assigned an IDF role except the last token.c.
Flat relation (?
): is a special role used by a CATiB pipeline parser for the sequence of propernouns.
For example: NE phrases such as (dm yJ ?
?rJ yJ /?syx ?sryf ?syx OHmd/ ?Sheikh SharifSheikh Ahmed?
), in which all tokens are assigned a flat relation.5The naming of this abbreviation is used in CATiB to represent the syntactical role of idafa.989CorpusDevelopment Test+|-P R F P R FNewsFANEGold79.84 56.75 66.34 76.14 57.70 65.65 +3.98WikiFANEGold71.17 46.95 56.58 75.18 45.10 56.38 +2.34WikiFANESelective87.00 73.55 79.71 85.78 69.18 76.59 +4.75WikiFANEWhole88.58 66.97 72.22 85.15 59.01 69.71 +0.56Table 5: The results of the dependency-based features representation.
(?+|-?
represents the variationcompared with the previous experiment)5.1 Dependency-based Features setThe representation of the dependency structure presents each token as a node.
A particular token (T)should have one node and only one head (H), except for the root, and zero or more dependents (D).
Atoken (T) can have zero or more siblings (S), where they are connected, (i.e.
are dependent), to the samehead.
Therefore, the following set of features has been extracted:1.
The current token T2.
POS of T3.
The presence of T in the Gazetteer4.
Syntactical role of T5.
Token of 1st, 2nd and 3rd Head H6.
Syntactical role of 1st, 2nd and 3rd H7.
POS of 1st, 2nd and 3rd H8.
Token of 1st, 2nd and 3rd Dependent D or ?NA?
otherwise9.
Syntactical role of 1st, 2nd and 3rd D or ?NA?
otherwise10.
POS of 1st, 2nd and 3rd D or ?NA?
otherwise11.
Token of 1st, 2nd and 3rd Sibling S or ?NA?
otherwise12.
Syntactical role of 1st, 2nd and 3rd S or ?NA?
otherwise13.
POS of 1st, 2nd and 3rd S or ?NA?
otherwiseThe 1st, 2nd and 3rd ?H?
represent the parent, grandparent and great grandparent heads; while the 1st,2nd and 3rd ?S?
represent the first three siblings (if any).5.2 EvaluationIt was decided to use the CATiB pipeline tool6(produced by Marton et al.
(2013)), to parse all corporaand produce the set of features presented in the previous section.
Since the POS tag set produced usingthe CATiB pipeline tool is very limited, it was decided instead to rely on the output of the AMIRA to-keniser and POS tagger produced by Diab (2009).
The same classifier (CRF) was used, with a similarencoding scheme.
Two experiments were conducted: the first was intended to evaluate the dependency-based representations.
This was important in examining the effectiveness of the approach, comparedwith the window-based representation of local evidence.
This is shown in Table 5, where in all corporathe performance of dependency-based representation alone outperforms that with window-based repre-sentation.
The recall metrics reveal improvement across corpora, suggesting that the dependency-baserepresentation has the ability to capture an increased number of NE phrases comparing to the traditionalwindow-based representation.The second experiment is intended to evaluate the integration in the classification process ofdependency-based and window-based representations.
This evaluation is expected to attain maximumbenefit from both approaches in one model.
The results in Table 6 demonstrate that the classifier tends toefficiently utilise both dependency-based and window-based representations in all corpora, apart fromWikiFANEWhole.
The reason behind the degradation of the performance over the WikiFANEWholedataset is due to the nature of the compiling of the corpus.
Alotaibi and Lee (2013) state that thisversion includes entire sentences from Wikipedia articles, with no further filtering, ensuring that it is6Not yet released to the public.
We would like to thank the author for permission for its use.990CorpusDevelopment Test+|-P R F P R FNewsFANEGold82.08 57.77 67.81 80.21 61.58 69.68 +4.03WikiFANEGold89.31 49.11 63.37 83.34 50.48 62.88 +4.63WikiFANESelective87.03 73.29 79.57 87.31 76.17 77.81 +1.22WikiFANEWhole82.44 57.91 68.03 75.88 52.45 62.03 -7.68Table 6: The results of the hybrid approach using dependency-based and window-based features repre-sentationpossible to have sentences including NE phrases that are mistakenly assigned to ?O?
class when using anautomatic approach, as these NE phrases have no known destination in a Wikipedia article.
This vari-ety of mis-annotation is expected to propagate at this stage.
It is worth noting that NewsFANEGoldandWikiFANEGold, as gold-standard corpora of different genres, reveal notable improvements of 4.03 and4.63 F-measure respectively by using hybrid representation.6 Further Exploiting of Global EvidencesThus far, this study has examined the window-based and dependency-based representation of evidence,in order to increase the performance of the classification process.
However, there is still room for im-provement.
Both approaches focus only at the sentence level.
This section will investigate the approachto capturing global evidence.
One means of achieving this is by utilising unannotated textual data, byclustering tokens into semantic groups based on context similarity.
The reasoning behind this approachis that a NE token such as (?
?AW?  /AlTA?yf/ ?Taif?)
(which is not seen in the training process) cannot becorrectly classified, as it contains neither window-based nor dependency-based evidence in the trainingphase.
Meanwhile, the token ???AW? ?
is assigned to the same cluster of (?dn?
/lndn/ ?London?)
wherethe classifier knows that ??dn??
is a location.
In this way, the knowledge capacity of the classifier hasbeen broadened to a global level.
A number of efforts have been undertaken for languages other thanArabic that demonstrate the usefulness of injecting clustering into NLP tasks, e.g.
PCFG parsing (Can-dito and Crabb?e, 2009) and dependency parsing (Koo et al., 2008).
Utilising unannotated textual data inthe supervised NER has already been variously studied with reference to English.
The studies in (Turianet al., 2009; Turian et al., 2010; Tkachenko et al., 2012; Ratinov and Roth, 2009; Miller et al., 2004)reveal improvements when using the Brown clustering algorithm (Brown et al., 1992) to extract usefulfeatures.This paper focuses on extracting a useful set of features from unannotated Arabic textual data, byrelying on the Brown algorithm.
We are not aware of any other study employing the Brown algorithm toArabic textual data and in an Arabic NER task.6.1 Brown Clustering and NERThe Brown clustering algorithm works by maximising the mutual information of bigrams.
It uses hier-archical representation for the clusters.
The hierarchal representation of the Brown clusters algorithmallows inclusion of different semantic levels of granularity.
The output from the clustering delivers valu-able information, which can be utilised by NER.
This information can be divided into three categories:1.
The cluster of tokens belongs to the named entity category.
For example, (w?AkyJ /?sykA?w/?Chicago?)
and (wy?wV /Twkyw/ ?Tokyo?)
belong to the same cluster, where both are NE type?LOCATION?.
In addition, (??d?
/hdyl/ ?Hadeel?)
and (?dm?
/mmdwH/ ?Mamdooh?)
fall intosimilar clusters, and are both Personal NE.2.
The cluster of keyword tokens that have an informal insight to the target NE classes.
For example,(	?At?
/ktA?yb/ ?Brigades?)
and (Tm\n?
/mn ?Dm~/ ?Organisation?)
are keywords which infer thecontext of organisational NE.
The context is expressed, for instance, as (YO?
?  ? dhJ 	?At?/ktA?yb ?shdA?
AlOqS?y/ ?Al Aqsa Martyrs Brigades?)
or (Ty?
?d?  wf`?  Tm\n?
/mn ?Dm~ Al?fw991Aldwly~/ ?Amnesty International?).
Both head tokens in the NE phrases refer to the same cluster,which indicates the ?ORGANISATION?.3.
The cluster of the head and dependent tokens the current token is pointing to.
For example, the token(yJ /?syx/ ?Shaikh?
), as shown in Figure 2a, is pointed to the head token (Hy?C /r?yys/ ?President?
)where the ?Hy?C?
belongs to the cluster ?1110000111?.
This clustering knowledge permits thebuilding of an abstract semantic representation for tokens.
This implies that the token ?Hy?C?
canbe replaced as (r?d?
/mdyr/ ?Manager?)
in other sentences where both tokens belong to the samecluster.Further examples are presented in the Figure 3, where the group?s heading shows both name and cluster.Locations: 0101101100 First names: 000011111111101(?yk /bkyn/ ?Beijing?)
(??d?
/hdyl/ ?Hadeel?
)(xAsk /tksAs/ ?Texas?)
(? dym /HmydAn/ ?Homaidan?
)(wy?wV /Twkyw/ ?Tokyo?)
(?dm?
/mmdwH/ ?Mumdooh?
)Last names: 0000110001(01|10) Organisational keywords: 0111111111111011000(r?As?  /AlsAhr/ ?Alsaher?)
(	?At?
/ktA?yb/ ?battalions?
)(?CAb?  /AlbxAry/ ?Albokhari?)
(Thb /jbh~/ ?front?)(?
?EA?  /AlHAzmy/ ?Alhazmi?)
(Tm\n?
/mn ?Dm~/ ?organization?
)Locational keywords: 011110110000 Facility-related keywords: 101101100111011(TnVwts?
/ktA?yb/ ?settlement?)
( AtF  /AstAd/ ?stadium?
)(TyAR /DAHy~/ ?suburb?)
(rs /jsr/ ?bridge?)(Tym?
/mHmy~/ ?protectress?)
(CAW?
/mTAr/ ?airport?
)Figure 3: Examples of the output of the Brown algorithm when applied to Arabic textual data.6.2 EvaluationThe goal of this experiment was to evaluate the usefulness of injecting the clustering information fromBrown algorithm into the supervised model.
However, the actual size of the corpora mentioned in section2.3 is too small to apply the Brown algorithm.
Instead, a different set of different unannotated corpora,of a reasonably large size from different sources, was prepared for use in this experiment, as shown inTable 7.Source of unannotated dataset Size Used forNewsFANEGold+ Gigaword 1.17M NewsFANEGoldWikiFANEGold+ 1/2(WikiFANESelective& WikiFANEWhole) 2.1M WikiFANEGoldWikiFANESelective2M WikiFANESelectiveWikiFANEWhole2M WikiFANEWholeTable 7: Different textual data used in Brown algorithmThe first and second columns in Table 7 show the source of the unlabelled textual data used in theBrown algorithm and the respective size.
The final column shows the target corpus using the knowledgein the CRF classifier.Random stories were selected from Arabic Gigaword (Parker et al., 2011) as well as textual data fromNewsFANEGold, to form unannotated data sized as 1.17M tokens.
The Gigaword subset was selecteddue to the similarity of its genre to NewsFANEGold.
The textual data for WikiFANEGold, and half of bothWikiFANESelectiveand WikiFANEWholewere compiled into one in order to induce clustering knowledgefor WikiFANEGold.The Brown algorithm was run in order to cluster the tokens into 1000 clusters, as suggested in (Milleret al., 2004; Liang, 2005; Ratinov and Roth, 2009; Tkachenko et al., 2012).
The output of the Brownalgorithm (which involves 1000 clusters) was injected as a set of features by extracting the clustering992CorpusDevelopment Test+|-P R F P R FNewsFANEGold86.13 70.38 77.46 81.66 68.36 74.42 +4.74WikiFANEGold77.80 62.36 69.23 79.87 60.19 68.64 +5.76WikiFANESelective89.17 74.04 80.90 88.64 73.18 80.17 +2.36WikiFANEWhole90.39 69.97 78.88 84.98 65.00 73.66 +11.63Table 8: The results of the injecting the output of Brown clustering into the CRF modelbits of (4, 6, 8, 10, 12) in a way that is similar to that presented by (Turian et al., 2010; Tkachenko etal., 2012).
The reason behind this representation of the output is to allow a flexible level of groupingtokens into semantic clusters.
For example, the tokens ?
?CAb? ?
and ???EA? ?
are clustered into?000011000101?
and ?000011000110?, respectively, where both are personal NE.
They share the first 10bits of the cluster.
This information allows for the extraction of useful knowledge to classify both tokensinto the same class.Table 8 shows notable improvement across all corpora.
WikiFANEWholeand WikiFANEGoldscorethe highest, while other corpora gain improvements.
It can be seen that the recall has sharply improvedfor approximately 7 to 13 points for NewsFANEGold, WikiFANEGoldand WikiFANEWhole.
This impliesthat the injecting of Brown clusters has improved the recall metric as a means of delimiting an increasednumber of NE phrases.7 Related WorkThis paper has addressed a series of issues, along with a discussion of the literature relevant to the con-text discussed in each section.
Additional works of particular relevance are noted here.
A large numberof studies undertaking traditional Arabic NER have been developed, using a variety of methodologiesto attain different goals.
Using machine learning for the traditional task of NER has been addressedin different dimensions.
Sequence labelling has also emerged, i.e.
Maximum Entropy (Benajiba et al.,2007; Benajiba and Rosso, 2007); Support Vector Machine (Benajiba et al., 2008a); Conditional Ran-dom Fields (Benajiba and Rosso, 2008) and Structured Perceptron (Farber et al., 2008).
Other hybridapproaches reliant on rule-based and ML are presented by (Shaalan and Oudah, 2013), a semi-supervisedpattern is described in (AbdelRahman et al., 2010; Althobaiti et al., 2013) and the involvement of ma-chine translation system to boost the performance of NER presented by (Zitouni and Florian, 2008).
Theresearcher is not aware of any study tackling the fine-grained level of Arabic NER.
Even that which hasbeen developed for other languages (such as English) remains limited (Ling and Weld, 2012).In terms of the representation of features, almost all studies in the Arabic NER apply the predefinedwindow-based representation as examples when using this approach (Shaalan and Oudah, 2013; Benajibaet al., 2009).
In English, Ratinov and Roth (2009) implemented two ways of capturing non-local features.The first approach is ?context aggregation?.
This works by searching the entire document for a giventoken and returning the context of size two around each matched token.
Ratinov and Roth (2009) limitedthe search to within 200 tokens.
The second approach is ?extended prediction history?, which looks upthe 1000 previous tokens and counts the frequency of the label of the target class.8 ConclusionThe majority of attempts to date to address NER focus on a limited number of semantic classes.
Thislimitation has implications for other applications, such as question answering.
This paper has presentedan extended series of experiments and ideas, with the aim of constructing a fine-grained NER detailingresource creation to evaluation.
Two approaches have been presented that rely on the output of thedependency parser and the clustering algorithm, instead of on a local window-based representation.993ReferencesSamir AbdelRahman, Mohamed Elarnaoty, Marwa Magdy, and Aly Fahmy.
2010.
Integrated machine learningtechniques for arabic named entity recognition.
IJCSI International Journal of Computer Science, 7(4):27?36.Ahmed Abdul-Hamid and Kareem Darwish.
2010.
Simplified feature set for arabic named entity recognition.
InProceedings of the 2010 Named Entities Workshop, pages 110?115, Uppsala, Sweden.
Association for Compu-tational Linguistics.Fahd Alotaibi and Mark Lee.
2013.
Automatically developing a fine-grained arabic named entity corpus andgazetteer by utilizing wikipedia.
In Proceedings of the Sixth International Joint Conference on Natural Lan-guage Processing, pages 392?400, Nagoya, Japan, October.
Asian Federation of Natural Language Processing.Maha Althobaiti, Udo Kruschwitz, and Massimo Poesio.
2013.
A semi-supervised learning approach to ara-bic named entity recognition.
In Proceedings of the International Conference Recent Advances in NaturalLanguage Processing RANLP 2013, pages 32?40, Hissar, Bulgaria, September.
INCOMA Ltd. Shoumen, BUL-GARIA.Yassine Benajiba and Paolo Rosso.
2007.
Anersys 2.0: Conquering the ner task for the arabic language by com-bining the maximum entropy with pos-tag information.
In Proceedings of the Workshop on Natural Language-Independent Engineering, 3rd Indian Int.
Conf.
on Artificial Intelligence, IICAI-2007, Pune, India.Yassine Benajiba and Paolo Rosso.
2008.
Arabic named entity recognition using conditional random fields.
InProceedings of the Workshop on HLT & NLP Within the Arabic World.
Arabic Language and Local LanguagesProcessing: Status Updates and Prospects, 6th International Conference on Language Resources and Evalua-tion, volume 8, pages 26?31, Marrakech, Morocco.
LREC-2008.Yassine Benajiba, Paolo Rosso, and Jos?e Miguel Bened??ruiz.
2007.
Anersys: An arabic named entity recognitionsystem based on maximum entropy.
In Alexander Gelbukh, editor, Computational Linguistics and IntelligentText Processing, volume 4394 of Lecture Notes in Computer Science, pages 143?153.
Springer Berlin / Heidel-berg.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2008a.
Arabic named entity recognition: An svm-based ap-proach.
In Proceedings of 2008 Arab International Conference on Information Technology (ACIT), pages 16?18, Amman, Jordan.
Association of Arab Universities.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2008b.
Arabic named entity recognition using optimized featuresets.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 284?293,Honolulu, Hawaii.
Association for Computational Linguistics.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2009.
Using language independent and language specific featuresto enhance arabic named entity recognition.
IEEE Transactions on Audio, Speech and Language Processing.Special Issue on Processing Morphologically Rich Languages, 17(5).Y.
Benajiba, I. Zitouni, M. Diab, and P. Rosso.
2010.
Arabic named entity recognition: Using features extractedfrom noisy data.
In Proceedings of the ACL 2010 Conference Short Papers, pages 281?285, Uppsala, Sweden.Association for Computational Linguistics.Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai.
1992.
Class-basedn-gram models of natural language.
Computational linguistics, 18(4):467?479.Marie Candito and Beno?
?t Crabb?e.
2009.
Improving generative statistical parsing with semi-supervised wordclustering.
In Proceedings of the 11th International Conference on Parsing Technologies, pages 138?141.Association for Computational Linguistics.Mona Diab.
2009.
Second generation amira tools for arabic processing: Fast and robust tokenization, pos tagging,and base phrase chunking.
In 2nd International Conference on Arabic Language Resources and Tools.Benjamin Farber, Dayne Freitag, Nizar Habash, and Owen Rambow.
2008.
Improving ner in arabic using a mor-phological tagger.
In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08),pages 2509?2514, Marrakech, Morocco.
European Language Resources Association (ELRA).George Hripcsak and Adam S Rothschild.
2005.
Agreement, the f-measure, and reliability in information retrieval.Journal of the American Medical Informatics Association, 12(3):296?298.Terry Koo, Xavier Carreras, and Michael Collins.
2008.
Simple semi-supervised dependency parsing.994Percy Liang.
2005.
Semi-supervised learning for natural language.
Ph.D. thesis, Massachusetts Institute ofTechnology.Xiao Ling and Daniel S Weld.
2012.
Fine-grained entity recognition.
In Proceedings of the 26th Conference onArtificial Intelligence (AAAI).Yuval Marton, Nizar Habash, and Owen Rambow.
2013.
Dependency parsing of modern standard arabic withlexical and inflectional features.
Computational Linguistics, 39(1):161?194.Scott Miller, Jethran Guinness, and Alex Zamanian.
2004.
Name tagging with word clusters and discriminativetraining.
In HLT-NAACL, volume 4, pages 337?342.
Citeseer.Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary.
2005.
Ace 2004 multilingual trainingcorpus [ldc2005t09], March 15.
[accessed 20 December 2013].Robert Parker, David Graff, Ke Chen, Junbo Kong, and Kazuaki Maeda.
2011.
Arabic gigaword fifth edition[ldc2011t11], October 21.
[accessed 20 December 2013].Lev Ratinov and Dan Roth.
2009.
Design challenges and misconceptions in named entity recognition.
In Pro-ceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147?155, Boulder,Colorado.
Association for Computational Linguistics.Khaled Shaalan and Mai Oudah.
2013.
A hybrid approach to arabic named entity recognition.
Journal of Infor-mation Science.Maksim Tkachenko, Andrey Simanovsky, and St Petersburg.
2012.
Named entity recognition: Exploring features.In Proceedings of KONVENS, pages 118?127.Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan Roth.
2009.
A preliminary evaluation of word representa-tions for named-entity recognition.
In NIPS Workshop on Grammar Induction, Representation of Language andLanguage Learning.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, pages 384?394.
Association for Computational Linguistics.Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda.
2006.
Ace 2005 multilingual trainingcorpus [ldc2006t06], February 15.
[accessed 20 December 2013].Ziqi Zhang.
2013.
Named entity recognition: challenges in document annotation, gazetteer construction anddisambiguation.
Ph.D. thesis.Imed Zitouni and Radu Florian.
2008.
Mention detection crossing the language barrier.
In Proceedings of the Con-ference on Empirical Methods in Natural Language Processing, pages 600?609.
Association for ComputationalLinguistics.995
