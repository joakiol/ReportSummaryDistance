Proceedings of the ACL 2010 Conference Short Papers, pages 189?193,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTree-Based Deterministic Dependency Parsing?
An Application to Nivre?s Method ?Kotaro Kitagawa Kumiko Tanaka-IshiiGraduate School of Information Science and Technology,The University of Tokyokitagawa@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jpAbstractNivre?s method was improved by en-hancing deterministic dependency parsingthrough application of a tree-based model.The model considers all words necessaryfor selection of parsing actions by includ-ing words in the form of trees.
It choosesthe most probable head candidate fromamong the trees and uses this candidate toselect a parsing action.In an evaluation experiment using thePenn Treebank (WSJ section), the pro-posed model achieved higher accuracythan did previous deterministic models.Although the proposed model?s worst-casetime complexity is O(n2), the experimen-tal results demonstrated an average pars-ing time not much slower than O(n).1 IntroductionDeterministic parsing methods achieve both effec-tive time complexity and accuracy not far fromthose of the most accurate methods.
One suchdeterministic method is Nivre?s method, an incre-mental parsing method whose time complexity islinear in the number of words (Nivre, 2003).
Still,deterministic methods can be improved.
As a spe-cific example, Nivre?s model greedily decides theparsing action only from two words and their lo-cally relational words, which can lead to errors.In the field of Japanese dependency parsing,Iwatate et al (2008) proposed a tournament modelthat takes all head candidates into account in judg-ing dependency relations.
This method assumesbackward parsing because the Japanese depen-dency structure has a head-final constraint, so thatany word?s head is located to its right.Here, we propose a tree-based model, applica-ble to any projective language, which can be con-sidered as a kind of generalization of Iwatate?sidea.
Instead of selecting a parsing action fortwo words, as in Nivre?s model, our tree-basedmodel first chooses the most probable head can-didate from among the trees through a tournamentand then decides the parsing action between twotrees.Global-optimization parsing methods are an-other common approach (Eisner, 1996; McDon-ald et al, 2005).
Koo et al (2008) studiedsemi-supervised learning with this approach.
Hy-brid systems have improved parsing by integrat-ing outputs obtained from different parsing mod-els (Zhang and Clark, 2008).Our proposal can be situated among global-optimization parsing methods as follows.
The pro-posed tree-based model is deterministic but takes astep towards global optimization by widening thesearch space to include all necessary words con-nected by previously judged head-dependent rela-tions, thus achieving a higher accuracy yet largelyretaining the speed of deterministic parsing.2 Deterministic Dependency Parsing2.1 Dependency ParsingA dependency parser receives an input sentencex = w1, w2, .
.
.
, wn and computes a dependencygraph G = (W,A).
The set of nodes W ={w0, w1, .
.
.
, wn} corresponds to the words of asentence, and the node w0 is the root of G. A isthe set of arcs (wi, wj), each of which represents adependency relation where wi is the head and wjis the dependent.In this paper, we assume that the resulting de-pendency graph for a sentence is well-formed andprojective (Nivre, 2008).
G is well-formed if andonly if it satisfies the following three conditions ofbeing single-headed, acyclic, and rooted.2.2 Nivre?s MethodAn incremental dependency parsing algorithmwas first proposed by (Covington, 2001).
After189Table 1: Transitions for Nivre?s method and the proposed method.Transition PreconditionNivre?sMethodLeft-Arc (?|wi, wj |?,A) ?
(?,wj |?,A ?
{(wj , wi)}) i ?= 0 ?
?
?wk (wk, wi) ?
ARight-Arc (?|wi, wj |?,A) ?
(?|wi|wj , ?, A ?
{(wi, wj)})Reduce (?|wi, ?, A) ?
(?, ?,A) ?wk (wk, wi) ?
AShift (?,wj |?,A) ?
(?|wj , ?, A)ProposedMethodLeft-Arc (?|ti, tj |?,A) ?
(?, tj |?,A ?
{(wj , wi)}) i ?= 0Right-Arc (?|ti, tj |?,A) ?
(?|ti, ?, A ?
{(mphc(ti, tj), wj)})Shift (?, tj |?,A) ?
(?|tj , ?, A)studies taking data-driven approaches, by (Kudoand Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incre-mental parser was generalized to a state transitionsystem in (Nivre, 2008).Nivre?s method applying an arc-eager algorithmworks by using a stack of words denoted as ?, fora buffer ?
initially containing the sentence x. Pars-ing is formulated as a quadruple (S, Ts, sinit, St),where each component is defined as follows:?
S is a set of states, each of which is denotedas (?, ?,A) ?
S.?
Ts is a set of transitions, and each element ofTs is a function ts : S ?
S.?
sinit = ([w0], [w1, .
.
.
, wn], ?)
is the initialstate.?
St is a set of terminal states.Syntactic analysis generates a sequence of optimaltransitions ts provided by an oracle o : S ?
Ts,applied to a target consisting of the stack?s top ele-ment wi and the first element wj in the buffer.
Theoracle is constructed as a classifier trained on tree-bank data.
Each transition is defined in the upperblock of Table 1 and explained as follows:Left-Arc Make wj the head of wi and pop wi,where wi is located at the stack top (denotedas ?|wi), when the buffer head is wj (denotedas wj |?
).Right-Arc Make wi the head of wj , and push wj .Reduce Pop wi, located at the stack top.Shift Push the word wj , located at the buffer head,onto the stack top.The method explained thus far has the followingdrawbacks.Locality of Parsing Action SelectionThe dependency relations are greedily determined,so when the transition Right-Arc adds a depen-dency arc (wi, wj), a more probable head of wjlocated in the stack is disregarded as a candidate.Features Used for Selecting ReduceThe features used in (Nivre and Scholz, 2004) todefine a state transition are basically obtained fromthe two target words wi and wj , and their relatedwords.
These words are not sufficient to select Re-duce, because this action means that wj has no de-pendency relation with any word in the stack.PreconditionsWhen the classifier selects a transition, the result-ing graph satisfies well-formedness and projectiv-ity only under the preconditions listed in Table 1.Even though the parsing seems to be formulated asa four-class classifier problem, it is in fact formedof two types of three-class classifiers.Solving these problems and selecting a moresuitable dependency relation requires a parser thatconsiders more global dependency relations.3 Tree-Based Parsing Applied to Nivre?sMethod3.1 Overall ProcedureTree-based parsing uses trees as the procedural el-ements instead of words.
This allows enhance-ment of previously proposed deterministic mod-els such as (Covington, 2001; Yamada and Mat-sumoto, 2003).
In this paper, we show the applica-tion of tree-based parsing to Nivre?s method.
Theparser is formulated as a state transition system(S, Ts, sinit, St), similarly to Nivre?s parser, but ?and ?
for a state s = (?, ?,A) ?
S denote a stackof trees and a buffer of trees, respectively.
A treeti ?
T is defined as the tree rooted by the word wi,and the initial state is sinit = ([t0], [t1, .
.
.
, tn], ?
),which is formed from the input sentence x.The state transitions Ts are decided through thefollowing two steps.1.
Select the most probable head candidate(MPHC): For the tree ti located at the stacktop, search for and select the MPHC for wj ,which is the root word of tj located at thebuffer head.
This procedure is denoted as a190	 	   w jFigure 1: Example of a tournament.function mphc(ti, tj), and its details are ex-plained in ?3.2.2.
Select a transition: Choose a transition,by using an oracle, from among the follow-ing three possibilities (explained in detail in?3.3):Left-Arc Make wj the head of wi and popti, where ti is at the stack top (denotedas ?|ti, with the tail being ?
), when thebuffer head is tj (denoted as tj |?
).Right-Arc Make the MPHC the head of wj ,and pop the MPHC.Shift Push the tree tj located at the bufferhead onto the stack top.These transitions correspond to three possibilitiesfor the relation between ti and tj : (1) a word of tiis a dependent of a word of tj ; (2) a word of tj is adependent of a word of ti; or (3) the two trees arenot related.The formulations of these transitions in thelower block of Table 1 correspond to Nivre?s tran-sitions of the same name, except that here a tran-sition is applied to a tree.
This enhancement fromwords to trees allows removal of both the Reducetransition and certain preconditions.3.2 Selection of Most Probable HeadCandidateBy using mphc(ti, tj), a word located far from wj(the head of tj) can be selected as the head can-didate in ti.
This selection process decreases thenumber of errors resulting from greedy decisionconsidering only a few candidates.Various procedures can be considered for im-plementing mphc(ti, tj).
One way is to apply thetournament procedure to the words in ti.
The tour-nament procedure was originally introduced forparsing methods in Japanese by (Iwatate et al,  	ti t jmphc ),( ji tt  	ti t jFigure 2: Example of the transition Right.2008).
Since the Japanese language has the head-final property, the tournament model itself consti-tutes parsing, whereas for parsing a general pro-jective language, the tournament model can onlybe used as part of a parsing algorithm.Figure 1 shows a tournament for the exampleof ?with,?
where the word ?watched?
finally wins.Although only the words on the left-hand side oftree tj are searched, this does not mean that thetree-based method considers only one side of a de-pendency relation.
For example, when we applythe tree-based parsing to Yamada?s method, thesearch problems on both sides are solved.To implement mphc(ti, tj), a binary classifieris built to judge which of two given words is moreappropriate as the head for another input word.This classifier concerns three words, namely, thetwo words l (left) and r (right) in ti, whose ap-propriateness as the head is compared for the de-pendent wj .
All word pairs of l and r in ti arecompared repeatedly in a ?tournament,?
and thesurvivor is regarded as the MPHC of wj .The classifier is generated through learning oftraining examples for all ti and wj pairs, eachof which generates examples comparing the truehead and other (inappropriate) heads in ti.
Ta-ble 2 lists the features used in the classifier.
Here,lex(X) and pos(X) mean the surface form and partof speech of X , respectively.
X left means thedependents of X located on the left-hand side ofX , while Xright means those on the right.
Also,Xhead means the head of X .
The feature designconcerns three additional words occurring afterwj , as well, denoted as wj+1, wj+2, wj+3.3.3 Transition SelectionA transition is selected by a three-class classifierafter deciding the MPHC, as explained in ?3.1.Table 1 lists the three transitions and one precon-191Table 2: Features used for a tournament.pos(l), lex(l)pos(lhead), pos(lleft), pos(lright)pos(r), lex(r)pos(rhead), pos(rleft), pos(rright)pos(wj), lex(wj), pos(wleftj )pos(wj+1), lex(wj+1), pos(wj+2), lex(wj+2)pos(wj+3), lex(wj+3)Table 3: Features used for a state transition.pos(wi), lex(wi)pos(wlefti ), pos(wrighti ), lex(wlefti ), lex(wrighti )pos(MPHC), lex(MPHC)pos(MPHChead), pos(MPHCleft), pos(MPHCright)lex(MPHChead), lex(MPHCleft), lex(MPHCright)pos(wj), lex(wj), pos(wleftj ), lex(wleftj )pos(wj+1), lex(wj+1), pos(wj+2), lex(wj+2), pos(wj+3), lex(wj+3)dition.
The transition Shift indicates that the tar-get trees ti and tj have no dependency relations.The transition Right-Arc indicates generation ofthe dependent-head relation between wj and theresult of mphc(ti, tj), i.e., the MPHC for wj .
Fig-ure 2 shows an example of this transition.
Thetransition Left-Arc indicates generation of the de-pendency relation in which wj is the head of wi.While Right-Arc requires searching for the MPHCin ti, this is not the case for Left-Arc1.The key to obtaining an accurate tree-basedparsing model is to extend the search space whileat the same time providing ways to narrow downthe space and find important information, such asthe MPHC, for proper judgment of transitions.The three-class classifier is constructed as fol-lows.
The dependency relation between the targettrees is represented by the three words wi, MPHC,and wj .
Therefore, the features are designed to in-corporate these words, their relational words, andthe three words next to wj .
Table 3 lists the exactset of features used in this work.
Since this transi-tion selection procedure presumes selection of theMPHC, the result of mphc(ti, tj) is also incorpo-rated among the features.4 Evaluation4.1 Data and Experimental SettingIn our experimental evaluation, we used Yamada?shead rule to extract unlabeled dependencies fromthe Wall Street Journal section of a Penn Treebank.Sections 2-21 were used as the training data, andsection 23 was used as the test data.
This test data1The head word of wi can only be wj without searchingwithin tj , because the relations between the other words in tjand wi have already been inferred from the decisions madewithin previous transitions.
If tj has a child wk that couldbecome the head of wi under projectivity, this wk must belocated between wi and wj .
The fact that wk?s head is wjmeans that there were two phases before ti and tj (i.e., wiand wj) became the target:?
ti and tk became the target, and Shift was selected.?
tk and tj became the target, and Left-Arc was selected.The first phase precisely indicates that wi and wk are unre-lated.was used in several other previous works, enablingmutual comparison with the methods reported inthose works.The SVMlight package2 was used to build thesupport vector machine classifiers.
The binaryclassifier for MPHC selection and the three-classclassifier for transition selection were built using acubic polynomial kernel.
The parsing speed wasevaluated on a Core2Duo (2.53 GHz) machine.4.2 Parsing AccuracyWe measured the ratio of words assigned correctheads to all words (accuracy), and the ratio of sen-tences with completely correct dependency graphsto all sentences (complete match).
In the evalua-tion, we consistently excluded punctuation marks.Table 4 compares our results for the proposedmethod with those reported in some previousworks using equivalent training and test data.The first column lists the four previous methodsand our method, while the second through fourthcolumns list the accuracy, complete match accu-racy, and time complexity, respectively, for eachmethod.
Here, we obtained the scores for the pre-vious works from the corresponding articles listedin the first column.
Note that every method useddifferent features, which depend on the method.The proposed method achieved higher accuracythan did the previous deterministic models.
Al-though the accuracy of our method did not reachthat of (McDonald and Pereira, 2006), the scoreswere competitive even though our method is de-terministic.
These results show the capability ofthe tree-based approach in effectively extendingthe search space.4.3 Parsing TimeSuch extension of the search space also concernsthe speed of the method.
Here, we compare itscomputational time with that of Nivre?s method.We re-implemented Nivre?s method to use SVMswith cubic polynomial kernel, similarly to our2http://svmlight.joachims.org/192Table 4: Dependency parsing performance.Accuracy Complete Time Global vs. Learningmatch complexity deterministic methodMcDonald & Pereira (2006) 91.5 42.1 O(n3) global MIRAMcDonald et al (2005) 90.9 37.5 O(n3) global MIRAYamada & Matsumoto (2003) 90.4 38.4 O(n2) deterministic support vector machineGoldberg & Elhadad (2010) 89.7 37.5 O(n log n) deterministic structured perceptronNivre (2004) 87.1 30.4 O(n) deterministic memory based learningProposed method 91.3 41.7 O(n2) deterministic support vector machine10 20 30 40 500102030405060Nivre?s Methodlength of input sentenceparsing time[sec]10 20 30 40 500102030405060Proposed Methodlength of input sentenceparsing time[sec]Figure 3: Parsing time for sentences.method.
Figure 3 shows plots of the parsing timesfor all sentences in the test data.
The average pars-ing time for our method was 8.9 sec, whereas thatfor Nivre?s method was 7.9 sec.Although the worst-case time complexity forNivre?s method is O(n) and that for our method isO(n2), worst-case situations (e.g., all words hav-ing heads on their left) did not appear frequently.This can be seen from the sparse appearance of theupper bound in the second figure.5 ConclusionWe have proposed a tree-based model that decideshead-dependency relations between trees insteadof between words.
This extends the search spaceto obtain the best head for a word within a deter-ministic model.
The tree-based idea is potentiallyapplicable to various previous parsing methods; inthis paper, we have applied it to enhance Nivre?smethod.Our tree-based model outperformed various de-terministic parsing methods reported previously.Although the worst-case time complexity of ourmethod is O(n2), the average parsing time is notmuch slower than O(n).ReferencesXavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parse.
Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL, pp.
957-961.Michael A. Covington.
2001.
A fundamental algorithm fordependency parsing.
Proceedings of ACM, pp.
95-102.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
Proceedings ofCOLING, pp.
340-345.Yoav Goldberg and Michael Elhadad.
2010.
An Efficient Al-gorithm for Easy-First Non-Directional Dependency Pars-ing.
Proceedings of NAACL.Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto.2008.
Japanese dependency parsing using a tournamentmodel.
Proceedings of COLING, pp.
361?368.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
Proceed-ings of ACL, pp.
595?603.Taku Kudo and Yuji Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking Proceedings ofCoNLL, pp.
63?69.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
Proceedings of ACL, pp.
91?98.Ryan McDonald and Fernando Pereira.
2006.
Online learn-ing of approximate dependency parsing algorithms.
Pro-ceedings of the EACL, pp.
81?88.Joakim Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
Proceedings of IWPT, pp.
149?160.Joakim Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics, vol.34, num.
4, pp.
513?553.Joakim Nivre and Mario Scholz.
2004.
Deterministic depen-dency parsing of English text.
Proceedings of COLING,pp.
64?70.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
Pro-ceedings of IWPT, pp.
195?206.Yue Zhang and Stephen Clark.
2008.
A tale of two parsers:investigating and combining graph-based and transition-based dependency parsing using beamsearch.
Proceed-ings of EMNLP, pp.
562?571.193
