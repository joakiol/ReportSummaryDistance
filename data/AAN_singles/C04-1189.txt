HITIQA: Towards Analytical Question AnsweringSharon Small1, Tomek Strzalkowski1, Ting Liu1, Sean Ryan1, Robert Salkin1,Nobuyuki Shimizu1, Paul Kantor2, Diane Kelly2, Robert Rittman2, Nina Wacholder21The State University of New York at Albany1400 Washington AvenueAlbany, NY 12222{small,tomek,tl7612,seanryan,rs6021,ns3202}@albany.edu2Rutgers University4 Huntington StreetNew Brunswick, NJ 08904{kantor,diane,hitiqa,wacholder}@scils.rutgers.eduAbstractIn this paper we describe the analyticquestion answering system HITIQA (High-Quality Interactive Question Answering)which has been developed over the last 2 yearsas an advanced research tool for informationanalysts.
HITIQA is an interactive open-domain question answering technologydesigned to allow analysts to pose complexexploratory questions in natural language andobtain relevant information units to preparetheir briefing reports.
The system uses noveldata-driven semantics to conduct aclarification dialogue with the user thatexplores the scope and the context of thedesired answer space.
The system hasundergone extensive hands-on evaluations bya group of intelligence analysts.
Thisevaluation validated the overall approach inHITIQA but also exposed limitations of thecurrent prototype.1 IntroductionOur objective in HITIQA is to allow the user tosubmit exploratory, analytical questions, such as?What has been Russia?s reaction to the U.S.bombing of Kosovo??
The distinguishing propertyof such questions is that one cannot generallyanticipate what might constitute the answer.
Whilecertain types of things may be expected (e.g.,diplomatic statements), the answer is heavilyconditioned by what information is in factavailable on the topic.
From a practical viewpoint,analytical questions are often underspecified, thuscasting a broad net on a space of possible answers.Questions posed by professional analysts areaimed to probe the available data along certaindimensions.
The results of these probes determinefollow up questions, if necessary.
Furthermore, atany stage clarifications may be needed to adjustthe scope and intent of each question.
Figure 1shows a fragment of an analytical session withHITIQA; note that these questions are not aimed atfactoids, despite their simple form.User: What is the history of the nuclear armsprogram linking Iraq and other countries in theregion?HITIQA: [responses and clarifications]User: Who financed the nuclear arms programin Iraq?HITIQA:?User: Has Iraq been able to import uranium?HITIQA:?User: What type of debt does exist between Iraqand her trading partners in the region?FIGURE 1: A fragment of an analyst?s sessionwith HITIQAHITIQA project is part of the ARDA AQUAINTprogram that aims to make significant advances inthe state of the art of automated questionanswering.
In this paper we focus on three aspectsof our work:1.
Question Semantics: how the system?understands?
user requests2.
Human-Computer Dialogue: how the user andthe system negotiate this understanding3.
User Evaluations and Results2 Factoid vs. Analytical QAThere are significant differences betweenfactoid, or fact-finding, and analytical questionanswering.
A factoid question is normallyunderstood to seek a piece of information thatwould make a corresponding statement true (i.e., itbecomes a fact): ?How many states are in theU.S.??
/ ?There are X states in the U.S.?
In thissense, a factoid question usually has just onecorrect answer that can generally be judged for itstruthfulness with respect to some informationsource.As noted by Harabagiu et al (1999), factoidquestions display a distinctive ?answer type?,which is the type of the information item neededfor the answer, e.g., ?person?
or ?country?, etc.Most existing factoid QA systems deduct thisexpected answer type from the form of thequestion using a finite list of possible answertypes.
For example, ?Who was the first man inspace?
expects a ?person?
as the answer type.
Thisis generally a very good strategy that has beenexploited successfully in a number of automatedQA systems, especially in the context of TRECQA1 evaluations.
Given the excellent results postedby the best systems and an adequate performanceattained even by some entry-level system, webelieve that the process of factoid questionanswering is now fairly well understood(Harabagiu et al, 2002; Hovy et al, 2000; Pragerat al., 2001, Wu et al, 2003).In contrast to a factoid question, an analyticalquestion has a virtually unlimited variety ofsyntactic forms with only a loose connectionbetween their syntax and the expected answer.Given the many possible forms of analyticalquestions, it would be counter-productive torestrict them to a predefined number ofquestion/answer types.
Therefore, the formation ofan answer in analytical QA should instead beguided by the user?s intended interest expressed inthe question, as well as through any follow updialogue with the system.
This clearly involvesuser's intentions (the speech acts) and how theyevolve with respect to the overall informationstrategy they are pursuing.In this paper we argue that the semantics(though not necessarily the intent) of an analyticalquestion is more likely to be deduced from theinformation that is considered relevant to thequestion than through a detailed analysis of itsparticular form.
We noted that the questionsanalysts ask, while clearly part of a strategy, aregenerally quite flexible and ?forgiving?, in thesense that there is always a strong possibility thatthe answer may not arrive in the expected form,and thus a change of strategy, and even the initialexpectations, may be warranted.
This suggestsstrongly that a solution to analytic QA mustinvolve a dialogue that combines informationseeking and problem solving strategies.3 Document RetrievalHITIQA works with unstructured text data,which means that a document retrieval step isrequired to detect any information that may berelevant to the user question.
It has to be noted thatdetermining ?relevant?
information is not the sameas finding an answer; indeed we can use relativelysimple information retrieval methods (keywordmatching, etc.)
to obtain perhaps 200 ?relevant?1 TREC QA is the annual Question Answering evaluationsponsored by the U.S. National Institute of Standards and Technologywww.trec.nist.govdocuments from a database.
This gives us an initialinformation space to work on in order to determinethe scope and complexity of the answer, but we arenowhere near the answer yet.
The current versionof HITIQA uses the INQUERY system (Callan etal., 1992), although we have also used SMART(Buckley, 1985) and other IR systems (such asGoogle).4 Text FramingIn HITIQA we use a text framing technique todelineate the gap between the possible meaning ofthe user?s question and the system ?understanding?of this question.
We can approximate the meaningof the question by extracting references to knownconcepts in it, including named entities.
Theinformation retrieved from the database may welllead to other interpretations of the question, and weneed to determine which of these are ?correct?.The framing process imposes a partial structureon the text passages that allows the system tosystematically compare different passages againsteach other and against the question.
Framing is notattempting to capture the entire meaning of thepassage; it needs to be just sufficient enough tocommunicate with the user about the differences intheir question and the returned text.
In particular,the framing process may uncover topics or aspectswithin the answer space which the user has notexplicitly asked for, and thus may be unaware oftheir existence.
If these topics or aspects alignclosely with the user?s question, (i.e., matchingmany of the salient attributes) we may want tomake the user aware of them and let him/herdecide if they should be included in the answer.Frames are built from the retrieved data, afterclustering it into several topical groups.
Passagesare clustered using a combination of hierarchicalclustering and n-bin classification (Hardy et al,2002a).
Each cluster represents a topic themewithin the retrieved set: usually an alternative orcomplimentary interpretation of the user?squestion.
Since clusters are built out of small textpassages, we initially associate a frame with eachpassage that serves as a seed of a cluster.
Wesubsequently merge passages and their associatedframes to arrive at one or more combined framesfor the cluster.HITIQA starts text framing by building ageneral frame on the seed passages of the clustersand any of the top N (currently N=10) scoredpassages that are not already in a cluster.
Thegeneral frame represents an event or a relationinvolving any number of entities, which make upthe frame?s attributes, such as LOCATION, PERSON,ORGANIZATION, DATE, etc.
Attributes are extractedfrom text passages by BBN?s Identifinder, whichtags 24 types of named entities.
The event/relationitself could be pretty much anything, e.g., accident,pollution, trade, etc.
and it is captured into theTOPIC attribute from the central verb or nounphrase of the passage.
In the general frame,attributes have no assigned roles; they are looselygrouped around the TOPIC (Figure 2).We have also defined three slightly morespecialized typed frames by assigning roles toselected attributes in the general frame.
Thesethree ?specialized?
frames are: (1) a Transferframe with three roles including FROM, TO andOBJECT; (2) a two-role Relation frame with AGENTand OBJECT roles; and (3) an one-role Propertyframe.
These typed frames represent certaingeneric events/relationships, which then map intomore specific event types in each domain.
Otherframe types may be defined if needed, but we donot anticipate there will be more than a handful alltogether.2 For example, another 3-role frame maybe State-Change frame with AGENT, OBJECT andINSTRUMENT roles, etc.3FRAME TYPE: GeneralTOPIC: importedLOCATION: Iraq, France, IsraelORGANIZATION: IAEA [missed: Nukem]PERSON: Leonard SpectorWEAPON: uranium, nuclear bombDATES: 1981, 30 November 1990, ..FIGURE 2: A general frame obtained from thetext passage in Figure 3 (not all attributes shown).Where the general frame is little more than justa ?bag of attributes?, the typed frames capturesome internal structure of an event, but only to theextent required to enable an efficient dialogue withthe user.
Typed frames are ?triggered?
byappearance of specific words in text, for examplethe word export may trigger a Transfer frame.
Asingle text passage may invoke one or more typedframes, or none at all.
When no typed frame isinvoked, the general frame is used as default.
If atyped frame is invoked, HITIQA will attempt toidentify the roles, e.g.
FROM, TO, OBJECT, etc.
Thisis done by mapping general frame attributesselected from text onto the typed attributes in theframes.
In any given domain, e.g., weapon non-proliferation, both the trigger words and the roleidentification rules can be specialized from a2 Scalability is certainly an outstanding issue here, and we areworking on effective frame acquisition methods, which is outside ofthe scope of this paper.
While classifications such as (Levin, 1993) orFrameNet (Fillmore, 2001) are relevant, we are currently aiming at aless detailed system.3 A more detailed discussion of possible frame types is beyond thescope of the current paper.training corpus of typical documents andquestions.
For example, the role-id rules rely bothon syntactic cues and the expected entity types,which are domain adaptable.Domain adaptation is desirable for obtainingmore focused dialogue, but it is not necessary forHITIQA to work.
We used both setups underdifferent conditions: the generic frames were usedwith TREC document collection to measure impactof IR precision on QA accuracy (Small et al,2004).
The domain-adapted frames were used forsessions with intelligence analysts working withthe WMD Domain (see below).
Currently, theadaptation process includes manual tuningfollowed by corpus bootstrapping using anunsupervised learning method (Strzalkowski &Wang, 1996).
We generally rely on BBN?sIdentifinder for extraction of basic entities, and usebootstrapping to define additional entity types aswell as to assign roles to attributes.The version of HITIQA reported here and usedby analysts during the evaluation has been adaptedto the Weapons of Mass Destruction Non-Proliferation domain (WMD domain, henceforth).Figure 3 contains an example passage from thisdata set.
In the WMD domain, the typed frameswere mapped onto WMDTransfer 3-role frame,and two 2-role frames WMDTreaty  andWMDDevelop.
Adapting the frames to the WMDdomain required very minimal modification, suchas adding the WEAPON entity to augment theIdentifinder entity set, generating a list ofinternational weapon control treaties, etc.The Bush Administration claimed that Iraq waswithin one year of producing a nuclear bomb.
On30 November 1990... Leonard Spector said thatIraq possesses 200 tons of natural uraniumimported and smuggled from several countries.Iraq possesses a few working centrifuges and theblueprints to build them.
Iraq imported centrifugematerials from Nukem of the FRG and from othersources.
One decade ago, Iraq imported 27 poundsof weapons-grade uranium from France, for Osiraknuclear research center.
In 1981, Israel destroyedthe Osirak nuclear reactor.
In November 1990, theIAEA inspected Iraq and found all materialaccounted for....FIGURE 3: A text passage from the WMDdomain dataHITIQA frames define top-down constraints onhow to interpret a given text passage, which isquite different from MUC4 template filling task4 MUC, the Message Understanding Conference, funded byDARPA, involved the evaluation of information extraction systemsapplied to a common task.
(Humphreys et al, 1998).
What we?re trying to dohere is to ?fit?
a frame over a text passage.
Thisalso means that multiple frames can be associatedwith a text passage, or to be exact, with a cluster ofpassages.
Since most of the passages that undergothe framing process are part of some cluster ofvery similar passages, the added redundancy helpsto reinforce the most salient features for extraction.This makes the framing process potentially lesserror-prone than MUC-style template filling.A very similar framing process is applied to theuser?s question, resulting in one or more Goalframes, which are subsequently compared to thedata frames obtained from retrieved text passages.A Goal frame can be a general frame or any of thetyped frames.
Goal frames generated from thequestion, ?Has Iraq been able to importuranium??
are shown in Figures 4 and 5.FRAME TYPE: GeneralTOPIC: importWEAPON:  uraniumLOCATION: IraqFIGURE 4: A general goal frame from the IraqquestionThe frame in Figure 4 is simply a Generalframe which is invoked first.
HITIQA thendiscovers that TOPIC=import denotes a Transfer-event in the WMD domain, so it creates aWMDTransfer frame that replaces the generalframe.
This new frame, shown in Figure 5, hasthree role attributes TRF_TO, TRF_FROM andTRF_OBJECT, plus the relation type (TRF_TYPE).Each role attribute is defined over an underlyinggeneral frame attribute (given in parentheses),which are used to compare frames of differenttypes.
The role-id rules rely both on syntactic cuesand the expected entity types, which are domainadaptable.FRAME TYPE: WMDTransferTRF_TYPE (TOPIC): importTRF_TO (LOCATION): IraqTRF_FROM (LOCATION, ORGANIZATION): ?TRF_OBJECT (WEAPON): uraniumFIGURE 5: A typed goal frame from the IraqquestionHITIQA automatically judges a particular dataframe as relevant, and subsequently thecorresponding segment of text as relevant, bycomparison to the Goal frame.
The data frames arescored based on the number of conflicts found withthe Goal frame.
The conflicts are mismatches onvalues of corresponding attributes, specificallywhen the data frame attribute list does not containany of the entities in the corresponding GoalFrame attribute list.
If a data frame is found tohave no conflicts, it is given the highest relevancerank, and a conflict score of zero.All other data frames are scored with anincreasing value based on the number of conflicts,score 1 for frames with one conflict with the Goalframe, score 2 for two conflicts etc.
Frames thatconflict with all information found in the query aregiven the score 99 indicating the lowest rank.Currently, frames with a conflict score 99 areexcluded from further processing as outliers.
Theframe in Figure 6 is scored as relevant to the user?squery and included in the answer space.FRAME TYPE: WMDTransferTRF_TYPE (TOPIC): importedTRF_TO (LOCATION): IraqTRF_FROM (LOCATION): FranceTRF_OBJECT (WEAPON): uraniumCONFLICT SCORE: 0FIGURE 6: A typed frame obtained from thetext passage in Figure 3, in response to the Iraqquestion5 Enabling Dialogue with the UserFramed information allows HITIQA toautomatically judge text passages as fully orpartially relevant and to conduct a meaningfuldialogue with the user about their content.
Thepurpose of the dialogue is to help the user navigatethe answer space and to negotiate more preciselywhat information he or she is seeking.
The mainprinciple here is that the dialogue is primarilycontent oriented.
Thus, it is okay to ask the userwhether information about the AIDS conference inCape Town should be included in the answer to aquestion about combating AIDS in Africa.However, the user should never be asked if aparticular keyword is useful or not, or if adocument is relevant or not.Our approach to dialogue in HITIQA ismodeled to some degree upon the mixed-initiativedialogue management adopted in the AMITIESproject (Hardy et al, 2002b).
The main advantageof the AMITIES model is its reliance on data-driven semantics which allows for spontaneousand mixed initiative dialogue to occur.
By contrast,the major approaches to implementation ofdialogue systems to date rely on systems offunctional transitions that make the resultingsystem much less flexible.
In the grammar-basedapproach, which is prevalent in commercialsystems, such as in various telephony products, aswell as in practically oriented research prototypes(e.g., DARPA Communicator; Seneff and Polifoni,2000; Ferguson and Allen, 1998), a completedialogue transition graph is designed to guide theconversation and predict user responses, which issuitable for closed domains only.
In the statisticalvariation of this approach, a transition graph isderived from a large body of annotatedconversations (e.g., Walker, 2000; Litman and Pan,2002).
This latter approach is facilitated through adialogue annotation process, e.g., using DialogueAct Markup in Several Layers (DAMSL) (Allenand Core, 1997), which is a system of functionaldialogue acts.Nonetheless, an efficient, spontaneous dialoguecannot be designed on a purely functional layer.Therefore, here we are primarily interested in thesemantic layer, that is, the information exchangeand information building effects of a conversation.In order to properly understand a dialogue, bothsemantic and functional layers need to beconsidered.
In this paper we are concentratingexclusively on the semantic layer.6 Clarification DialogueThe clarification dialogue is when the user andthe system negotiate the information task thatneeds to be performed.
Data frames with a conflictscore of 0 form the initial kernel answer space andHITIQA proceeds by generating an answer fromthis space.
Depending upon the presence of otherframes outside of this set, the system may initiate adialogue with the user.
When the Goal frame is ageneral frame HITIQA first initiates a clarificationdialogue on existing general data frames that haveone conflict.
All of these 1-conflict general framesare first grouped on their common conflictattribute.
HITIQA begins asking the user questionson these near-miss frame groups, with the largestgroup first.
The groups must be at least groups ofsize N, where N is a user controlled setting.
Thissetting restricts of all HITIQA?s generateddialogue.
HITIQA then check for the existence ofany data frames that are one of the three typedframes.
Clarification dialogue will be initiated onthese, when all of their general attributes agreewith the general attributes of the Goal framerespectively.
Alternatively, if the Goal frame is oneof the three type specific frames, a clarificationdialogue is first initiated on groups of one conflictdata frames that are the same type as the Goalframe.
The clarification dialogue will thencontinue to the remaining two type specific framesif any exist, and finally on to any General dataframes.A 1-conflict frame has only a single attributemismatch with the Goal frame.
This could be amismatch on any of the general frame attributes,for example, LOCATION, ORGANIZATION, TIME,etc., or in one of the role-assigned attributes, TO,FROM, OBJECT, etc.
A special case arises when theconflict occurs on the TOPIC attribute, whichindicated the event type.
Since all other attributesmatch, we may be looking at potentially differentevents (though similar type events) involving thesame entities, occurring at the same location ortime.
The purpose of the clarification dialogue inthis case is to probe which of these additionalevents may be of interest to the user.Another special case arises when the Goalframe has a different value of the TYPE attributethan a data frame.
The purpose of the clarificationdialogue here is to see if the user wishes to expandthe answer space to include events of a differenttype.
This situation is illustrated in the exchangeshown in Figure 7.The clarification question asked by HITIQA inFigure 7 was obtained by comparing the Goalframe (Figure 5) to a partly matching frame(Figure 8) generated from another text passage.We note first that the Goal frame for this exampleis WMDTransfer type, while the data frame inFigure 8 is WMDDevelop type.
Nonetheless, bothframes match on their general-frame attributesWEAPON and LOCATION.
Therefore, HITIQA asksthe user if it should expand the answer space toinclude development of uranium in Iraq as well.User: ?Has Iraq been able to import uranium?
?HITIQA: IRAQ REPORTED TO HAVEIMPORTED MISSILES AND URANIUMSupporting passage:...that Iraq possesses 200 tons of naturaluranium imported and smuggled from severalcountries.
Iraq possesses a few workingcentrifuges and the blueprints to build them.
Iraqimported centrifuge materials from Nukem of theFRG and from other sources.
One decade ago,Iraq imported 27 pounds of weapons-gradeuranium from France, for Osirak nuclear researchcenter...HITIQA: ?Are you also interested inbackground information on the uraniumdevelopment program in Iraq?
?User: ?FIGURE 7:  The clarification dialogue detailDuring the dialogue, as new information isobtained from the user, the Goal frame is updatedand the scores of all the data frames arereevaluated.
If the user responds the equivalent of?yes?
to the system clarification question in thedialogue in Figure 7, a correspondingWMDDevelop frame will be added to the set ofactive Goal frames and all WMDDevelop framesobtained from text passages will be re-scored forpossible inclusion in the answer.FRAME TYPE: WMDDevelopDEV_TYPE (TOPIC): development, producedDEV_OBJ (WEAPON): nuc.
weapons, uraniumDEV_AGENT (LOCATION): Iraq, TuwaithaCONFLICT SCORE: 2Conflicts with FRAME_TYPE and TOPICFIGURE 8: A 2-conflict frame against theIraq/uranium question that generated the dialoguein Figure 7.The user may end the dialogue at any point usingthe generated answer given the current state of theframes.
Currently, the answer is simply composedof text passages from the zero conflict frames.
Inaddition, HITIQA will generate a ?headline?
forthe text passages in the answer space.
This is doneusing a combination of text templates and simplegrammar rules applied to the attributes of thepassage frame.
Figure 7 shows a portion of theanswer generated by HITIQA for the Iraq query.7 HITIQA Preliminary EvaluationsWe have evaluated HITIQA in a series ofworkshops with professional analysts in order toobtain an in-depth and comprehensive assessmentof the system usability and performance.
Inaddition to evaluating our research progress, thepurpose of these workshops was to test severalevaluation instruments to see if they can bemeaningfully applied to a complex informationsystem such as HITIQA.For the participating analysts, the primaryactivity at these workshops involved preparation ofreports in response to ?scenarios?
?
complexquestions that often encompass multiple sub-questions, aspects and hypotheses.
For example, inone scenario, analysts were asked ti locateinformation about the al Qaeda terorist group: itsmembership, sources of funding and activities.
Inanother scenario, the analysts were requested tofind information on the chemical weapon Sarin.Figure 9 shows one of the analytical scenarios usedin these workshops.
We prepared a database ofover 1GByte of text documents; it included articlesfrom the Center for Non-proliferation (CNS) datacollected for the AQUAINT program and similardata retrieved from the web using Google.
Theanalysts?
task was to prepare a report ?as much likewhat you would do in your normal workenvironment as possible.?
Over the six days of theworkshops, each analyst prepared five such reportsin sessions of one to three hours.
Each sessioninvolved multiple questions posed to the system, aswell as clarification dialogue, visual browsing andreport construction.
Figure 10 shows an abridgedtranscript from another analytical session withHITIQA.Figure 9: A scenario level analytic taskOne of our primary concerns was to designtasks that were similar in scope and difficulty tothose that the analysts are used to performing atwork and to ensure that they felt comfortable usingthe system.
5 questions in the scenario evaluationdealt with this issue; for example, one questionasked how the scenarios compared in difficultywith the tasks the analysts normally perform atwork.
The mean score for these five questions was3.75 on a 5 point scale (five is the best score).
Thelowest score (M=2.88) was received on thequestion ?How did the scenario compare indifficulty to tasks that you normally perform atwork??
; this slightly above average rating ofdifficulty of the tasks was quite satisfactory for ourpurposes.In the final evaluation, analysts were asked torate their agreement with statements such as?Having HITIQA helps me find importantinformation?
(score 4.50), ?Having Hitiqa at workwould help me find information faster than I cancurrently find it?
(score 4.33), and ?Hitiqa wouldbe a useful addition to the tools that I already haveat work?
(score 4.25).
The mean normalized scorefor the combined final evaluation of Workshop Iwas 3.75 on the 5 point scale; this means that thesystem received many more ratings of 4 and 5 thanof 1 and 2.
Comments made by the analysts in thegroup discussion and in the individual interviewsconfirmed that analysts liked the interactivedialogue and were very pleased with the results.For example, one analyst said ?I learned moreabout Sarin gas in 30 minutes than I probablywould have at work in a half a day.?
As desired,the analysts also made many suggestions forimproving the interface and the interoperation ofThe department chief has requested a report by theclose of business today on the nuclear arms program inIraq and how it was influenced by the neighboringcountries.
List the extent of the nuclear program in eachinvolved country including funding, capabilities, quantity,etc.
Your report should also include key figures in Iraqnuclear program as well as in other countries in the region,and,any travels that these key figures have made to othercountries in regards to a nuclear program, any weaponsthat have been used in the past by either country, anypurchases or trades that have been made relevant toweapons of mass destruction (possibly oil trade, etc.
), anyingredients and chemicals that have been used, anypotential weapons that could be under development,countries that are involved or have close ties to Iraq or hertrade partners, possible locations of development sites, andpossible companies or organizations that these countrieswork with for their nuclear arms program.
Add any otherinformation relating to the Iraqi Nuclear Arms Programs.the visual and text display.
For a research systemundergoing its first rigorous evaluation, theseresults are very satisfactory ?
they support thevalue of the design of the HITIQA system,including the interactive mode and the visualdisplay and encourage us to move forward withthis approach.FIGURE 10: Fragment of an analytical session8 Future workThe AQUAINT Program has entered its secondphase in May 2004.
Over the next 2 years ourfocus will be on augmenting HITIQA to providemore advanced dialogue capabilities, includingproblem solving dialogue related to hypothesisformation and verification.
This implies buildingup system?s knowledge acquisition capabilities byexploiting diverse data sources, includingstructured databases and the internet.9 AcknowledgementsThis paper is based on work supported in part bythe Advanced Research and Development Activity(ARDA)?s Advanced Question Answering forIntelligence (AQUAINT) Program.
Special thanksto Heather McCallum-Bayliss and John Rogers forhelping to arrange the analyst workshops.Additional thanks for Google for extending theirlicense for this experiment, to Ralph Weischedel ofBBN/Verizon for the use of IdentiFinder, to ChuckMessenger and Peter LaMonica for assistance indevelopment of the analytical scenarios, and toBruce Croft at University of Massachusetts for theuse of INQUERY system.ReferencesAllen, J. and M. Core.
1997.
Draft of DAMSL:Dialog Act Markup in Several Layers.www.cs.rochester.edu/research/cisd.Buckley, C. 1985.
Implementation of the Smartinformation retrieval system.
TR85-686,Computer Science, Cornell University.Ferguson, G. and J. Allen.
1998.
TRIPS: AnIntelligent Integrated Problem-Solving Assistant.AAAI-98 Conf., pp.
567-573.Fillmore, C. & C. F. Baker.
2001.
Frame semanticsfor text understanding.
WordNet Workshop atNAACL.Hardy, H., et al 2002a.
Cross-DocumentSummarization by Concept Classification.Proceedings of SIGIR, Tampere, Finland.Hardy, H., et al 2002b.
Multi-layer DialogueAnnotation for Automated MultilingualCustomer Service.
ISLE Workshop, UK.Harabagiu, S., et.
al.
2002.
Answering Complex,List and Context questions with LCC?s QuestionAnswering Server.
TREC-10.Hovy, E., et al 2000.
Question Answering inWebclopedia.
Notebook.
Proceedings of TextRetrieval Conference TREC-9.Humphreys, R. et al 1998.
Description of theLaSIE-II System as Used for MUC-7.
Proc.
of7th Message Under.
Conf.
(MUC-7.
).Levin, B.
1993.
English Verb Class andAlternations: A Preliminary Investigation.Chicago: University of Chicago Press.Litman, Diane J. and Shimei Pan.
2002.
Designingand Evaluating an Adaptive Spoken DialogueSystem.
User Modeling and User-AdaptedInteraction.
Vol.
12, No.
2/3, pp.
111-137.Prager, J. et al 2003.
In Question-Answering TwoHeads are Better Than One.
Proceedings ofHLT-NAACL 2003, pp 24-31.Seneff, S. and J. Polifroni.
2000.
DialogueManagement in the MERCURY FlightReservation System.
ANLP-NAACL 2000.Small et al 2004.
A Data Driven Approach toInteractive Question Answering.
In M. Maybury(ed).
Future Directions in Automated QuestionAnswering.
MIT Press (to appear).Strzalkowski, T and J. Wang.
1996.
A self-learningUniversal Concept Spotter.
Proceedings ofCOLING-96, pp.
931-936.Walker, M. A.
2002.
An Application ofReinforcement Learning to Dialogue StrategySelection in a Spoken Dialogue System forEmail.
Journal of AI Research, vol 12., pp.
387-416.Wu, M. et al 2003.
Question Answering byPattern Matching, Web-Proofing, SemanticForm Proofing.
TREC-12.Notebook.User: What is the status of South Africa's chemical,biological, and nuclear programs?Clarification Dialogue: 1 minuteStudying Answer Panel: 60 minutesCopying 24 passages to reportVisual Panel Browsing: 5 minutesUser: Has South Africa provided CBW material orassistance to any other countries?Clarification Dialogue: 1 minuteStudying Answer Panel: 26 minutesCopying 6 passages to reportVisual Panel browsing: 1 minuteAdding 1 passage to reportUser: How was South Africa's CBW programfinanced?Clarification Dialogue: 40 secondsStudying Answer Panel: 11 minutesCopying 3 passages to report
