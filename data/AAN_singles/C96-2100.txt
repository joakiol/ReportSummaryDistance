Good BigramsChrister JohanssonDept.
of  Linguistics at Lund Univers i tyHelgonabacken 12223 62 Lund, Swedenemail:  Chr ister.
Johansson @ ling.lu, seAbstractA desired property of a measure ofconnective strength in bigrams is thatthe measure should be insensitive tocorpus size.
This paper investigatesthe stability of three different measuresover text genres and expansion of thecorpus.
The measures are (1) thecommonly used mutual information,(2) the difference in mutual informa-tion, and (3) raw occurrence.
Mutualinformation is further compared tousing knowledge about genres to re-move overlap between genres.
Thislast approach considers the differencebetween two products of the sameprocess (human text-generation) con-strained by different genres.
The can-cellation of overlap seems to providethe most specific word pairs for eachgenre.1 IntroductionStatistical methods have been used to find co-hesion between local items of language (suchas phonemes, morphemes, or words).
Earlywork (Stolz, 1965; Zellig, 1955) was inspiredby the advances in information science(Shannon, 1951; Shannon & Weaver, 1963).The research benefited from the possibility tostore huge amounts of information i  compu-ter systems, and the optimism could beoverwhelming when the problems weresimplified and thought mostly restricted bythe size of the corpus.
In this paper the stabi-lity of some bigram measures will be investi-gated.
Bigrams are items (i.e.
word forms)that occur frequently together in a specific or-der.
The meanings of bigrams are not discus-sed since there is no meaning outside of acontext.
Co-occurrence is still interesting be-cause bigrams occur non-randomly, someti-mes to such an extent hat we discern somestructure beyond co-occurrence.
The reasonwhy it should be so is probably that part ofthe use of words is reflected by the companythat words keep.Researchers (Church & Hanks, 1990; Kita& al., 1994, inter al.)
have noted that mutualinformation tends to be insensitive to highfi'equency patterns, and unstable for low fre-quency patterns.
Johansson (1994) comparedanother measure, the difference in mutual in-formation (Ag), of collocational strength withmutual information (g).
That measure rankedhigh frequency bigrams higher than other bi-grams if the order was consistent, whereasmutual information tended to pick out combi-nations of low frequency items.
Since lowfrequency items carry more specific informa-tion such bigrams give an illusion of semanticcontent.
It is usually this semantic illusion thatwe are interested in, but what says that "ofthe" or "in a" are worse bigrams than "wood-en spades" or "various pretexts".
Johanssonproposed the test of finding some of thecharacters in the children's tory "Alice inWonderland", and showed that a 'new'measure was to some degree "better" thanmutual information.
Unfortunately, some ofthat result was based on the fact that mutualinformation is very sensitive to low frequencyitems.2 Definitions2.1 Mutual informationIn the following p(x) will denote the observedprobability as defined by p(x)=F(x)/N whereF(x) is the frequency of occurrence of x, andN is the number of observed cases.
N is, inthe calculations, equal to the corpus size inwords.
Given this, the mutual informationratio (Church & Hanks, 1990; Church &Mercer, 1993; Steier & Belew, 1991) isexpressed by Formula 1.
(Church & Hanksrefer to this measure as the association ratiotbr technical reasons).592\]./ - - lOg2(RWT)) ) (~2 ) =(N  *Occ(\[wl,w2l)~Formula 1: The mutual information ratioThe instability of statistical measures seems tobe a problem in statistical bigralns.
Especiallylow frequency counts cause instability.
Toavoid this use the rule of thumb that a bigrammust occur more than four times (cf.
Church& Hanks, 1990:p.24) to be considered as acandidate/br an interesting bigram.2.2 The dif ference in mutual  informa-tion: temporal  co-occurrenceA reasonable way of using the temporal orde~ring in word pairs is to consider the oppositeordering of the word pair as negative videnceagainst he present order.
A reasonable mea-sure would be to use the difference in mutualinformation between the two orderings, here-after Ag.
The size of the corpus cancels outand Ag can be calculated by a ratio betweenfrequencies.
This is intuitively correct for acomparison between apples and pears, i.e.you can say that apples (wl w2) occur twiceas often as pears (w2 w l) in my fruit bowl(corpus).
(p is the probability in the fixedcorpus (fiN) which is different fi'om the pro-bability in the language.
It is impossible tohave a fixed corpus that equals the languagesince language does not have a fixed numberof words or word patterns).2.2.1 Handl ing zero negative evidenceIn the case that the reversed ordering of aword pair has not been observed in the cor-pus, the measure becomes undefined.
To reli-eve this the frequency t is multiplied by aconstant (10), and the frequency of the rever-sed ordering is set to 1.
Subtracting 9 fromthat value does not add anything to the mea-sure for a single occurrence (log(10-9)=0).Other ways of handling zero-frequenciesare evaluated in (Gale & Church, 1994), e.g.the Good-Turing method.
Relative frequen-cies of non-observed word pairs are hard toestimate.
For example, the frequencies of fre-quencies (X) and frequency (Y) used in the1 1 will use 'frequency' as equivalent to'occurrence' inthe sample corpus.Good-Turing method are linearly dependentin a log-log scale, i.e., there is an infinite fre-quency of non-observed items (which isanother way of saying that we cannot expectthe unexpected).A\].l =~\] (Occ(\[wl,w2\])~og 2 .
.
.
.
:it occ(\[w2,w \])>olog 2 (1 0* Oc'c(\[ Wl, W 2 \]) - 9)'if OCC(\[W2,Wl\]): 0Formula 2: Handling zero frequencies3 IllustrationThe difference between the two measures areperhaps best illustrated with some concreteexamples.
In a previous paper (Johansson,1.994) "Alice's adventures in Wonderhmd"(AIW) was used as an experimental corpus tocompare phrase finding for ~t, and a new me-asure - -  A~t.
A critique against hat corpus isthat the corpus is very small.
"Through theLooking Glass" and "The Hunting of theSnark" extend that corpus to about 63 000words of which 26 831 occurred more than 4times.
With the criterion that an interesting bi-gram occurs more than 4 times 1970 bigramcandidates were found in this larger corpus.Effect of Effect ofdella nm, .~ ,  - ,  , , ,215 188334 748 20233204291741602947136281400-9-519931932!190bigralncheshire cath u i ~lookingzlassmarch aremock tnrtlered kingreA queenthe dormousewhite kingwhite knightwhite queenwhite rabbit\[n the previous table the effect is measured bythe number of steps a bigram is moved upcompared to a sorted frequency list.
Theeffect of mutual information under theseconditions is higher than the proposedmeasure for finding most characters in A1W,except for some names defined by definitearticle + noun, and common adjective + noun.5934 Material 6 ResultsIn the rest of this paper, the corpus is theSUSANNE corpus (Sampson, 1994).
Thiscorpus consists of an extensively tagged andannotated subset from the Brown Corpus ofAmerican English.
The corpus is fairly small,but provides information on grammatical roleson the word and phrase level.
This makes theSUSANNE corpus suitable for further ese-arch.The SUSANNE corpus is divided into 4(approximately equally large) genre subcate-gones:"A: press reportageG: belles lettres, biography, memoirsJ: learned (mainly scientific andtechnical) writingN: adventure and Western fiction"(Sampson, 1994:p. 1.74)Each genre has approximately 20,000 uniqueword pairs 2.
The four genres will be used asone factor in the comparison between diffe-rent measures.
The question is whether thegenre interacts with the ability of the differentmeasures to discover bigrams.
In category A439 unique bigrams (occurring more than 4times) were found, in G 486, in J 598, N620, and 2573 for the used corpus 3.5 MethodThe highest ranking bigralns according to themeasure are sampled at 5 different levels: the10, 50, 100, 200 and 400 top collocations.Samples are sorted and compared for overlapby the UNIX command 'comm -12SAMPLE1 SAMPLE2 I wc -1', and the per-centage of overlap was calculated from thesize of the sample.Stability of bigrams was tested by three diffe-rent overlaps.
1) The overlap between sam-ples from genres, and samples for the entirecorpus for the same measure.
2) The overlapbetween different measures at the five diffe-rent levels for the different genres and the en-tire corpus.
3) The overlap between differentgenres.2(A 21198 unique / 29969 total / 5332 unique words;G 22248 / 31006 / 6048; J 19039 / 29484 / 4676; N20902 / 31959 / 4876; all 74126 / 12242\[ / 13458)3The last small part of each genre was excluded fi'omthe start for future purposes.6.1 Mutual  Informat ionThe average overlap between genres and thecorpus showed that the J sample was muchmore stabile than the other genres 4.
The Jgenre would be the genre that information re-trieval applications would be most interestedin.
The ranking of the genres according to thestability of the overlap is: JANG.
The highestcollocations are most stabile for J, where theother genres how less specificity (i.e.
equalor growing percentages as the overlapgrows).10 150 \[100 1200 \]400 \[mean20 22 30 27.5 21.5 24.20 6 10 14.5 16.7 9.460 62 48 36.5 31 47.5!10 6 7 15 22 12.0AGJN6.2 Delta Mutual  InformationDelta mutual information shows little effect ofgenre, and sample size.
Growing sample sizepredicts less overlap.
The ranking of genresis: GANJ.
Delta mutual information seems torank the less specific genres high.10 150 I lO0 1200 1400 Imear70 64 53 47.5 44.2 55.760 58 54 58.5 51.5 56.460 54 48 43 39.2 48.850 52 49 51 45.5 49.5AGJNA factorial ANOVA on measure and genreshows that there is a significant effect(p<0.001) of measure (Ag or g), genre andinteraction between measures.
F(measure,1df)=136.2, F(genre, 3df)=9.8, F(measure,genre, 1, 3)=15.4, p <0.001.
These two me-asures are significantly different.6.3 OccurrenceThe results for the samples are similar to a mThe overlap is generally higher for occurrencethan Ag, but the ranking of genres is thesame: GANJ.
An ANOVA on measure (Agand occurrence) and genre show lesssignificant effect on measure, and no signi-ficant effect of genre, or interaction (thesemeasures behave in the same direction).4In preliminary investigations the J genre was theleast stabile genre for mutual information.
This was'corrected' by the demand that candidate bigramsshould occur more than 4 times.59410 150 I i00 1200 1400 Imeanl60 70 65 60.5 51 61.360 70 69 65.5 61 65.170 62 53 48.5 43.5 55.470 64 57 54.5 54.2 59.9F(measure, ldf) = l l .
l  p<0.02, F(genre,3df) = 2.7, p>0.05, F(measure, genre, 1, 3)= 0.218, p>0.8.
Occurrence is significantlymore stabile than the other measure, but thereis only a small difference of genres(occurrence and Ag react in a similar way togenre - -  i.e.
on high occurrence).6.4 Comparison between measuresThe overlap between measures i calculatedfor all combinations of measures.
At the hig-her levels a high overlap can be expectedsince there is little possibility to fall out (e.g.in A 400 out of 439 is 91% of the sample).The results from this test indicate that theoverlap between D (Ag) and F (occurrence) issignificantly and consistently higher thanbetween the other combinations (especiallyfor the entire corpus).10 50 100 200 400 Genre Test  meanover -lap0 6 22 44.5 93.2 A(439)0 6 16 37.5 91.0 A90 64 74 78.0 91.2 A0 18 23 45.5 86.0 ~ _ _0 14 20 43.0 82.0 G80 76 78 77.5 84.0 G0 8 13 34.0 72.2 J(598)0 4 1 l 28.5 64.0 J60 84 78 72.5 75.5 J0 8 22 33.5 70.5 N(620)0 6 20 28.0 63.7 N- 40 68 71 67.0 72.5 N0 0 1 7.0 15.7 a11(2573)0 0 1 4.0 13.0 all40 54 58 58.0 59.5 allM=D 33.1M=F 30.1D=F 79.4M=D 34.5IM=F 31.8D=F 79.
IM=D 25.4M:FI 21.5D=F 74.0M=D 26.8M=F 23.5D:F  63.7M=D 4.7M=F 3.6D=-F 53.96.5 Overlap between genresTo estimate the overlap of the genres thenumber of common bigrams between twogenres were found and compared to the sizeof the smallest genre.
The results indicate anaverage overlap between the genres of 10%.OverlapAGNof genres (% of smallest genre)A G J N- - i11.0-9.4 11.010.0 12.0 7.56.6 Reduction of the bigramsThe bigrams that are rated high by the measu-res (especially mutual information) are mixedbetween two different ypes of bigrams: (1)bigrams with high internal cohesion betweenlow frequency items that may be associatedwith a specific interpretation (e.g.
"carbon te-trachloride" or "cheshire cat"), (2) bigramswith high internal cohesion with usually highfrequency of both items that may be associ-ated with a "syntactical" interpretation (e.g.
"in the").To separate type l from type 2 some in-formation about the overlap of genres mightbe used.
The type 2 bigrams are typicallyfound in most genres, whereas type 1 bi-grams are specific to a text.
The results aboveindicate that we can use the genres with leastoverlap to filter out common bigrams (i.e.
Ause J, G use J, J use N, N use J).In the following table the effect of the genre(column 2) is shown by the number of'surviving' bigrams from the candidate bi-grams (column 1).
The third column showsthe effect of removing the bigrams that occur(more than 4 times) in both directions aftercommon bigrams have been removed (firstparenthesis shows actual removed, secondshows those that would have been removed(i.e.
those bigrams with both orderings in thecandidate set).
The fourth column shows theeffect of removing bigrams that containswords that occur more than 4 times in the restof the corpus (i.e.
in A G N for J) after thebigrams have been formed.
The reason forfiltering after forming bigrams is that wordsthat are filtered out later work as place hol-ders, and prevent some bigrams to form.
Thereduction is most notable for removing bi-grams that contain common words betweengenres: genre G and N contain few goodcandidates ofcollocations type 1.Cand.
Genre Word order filter Freq.words439 216 179 (-63) (-80) 12486 159 119 (-40) (-127) 1.~8- \ ]  355 277 (-78)(-131) 37620 /395 291 (-104)(-159) 0AGJN595The following bigrams urvived the harshestcondition of removing bigrams containingwords of other genres.
(Genre J, later orderedby mutual information).
Some good candi-dates were (of course) removed, e.g.
"blackbody", "per cent", "united states".12.2 poynting robertson 9.1 pulmonary vein11.8 indirect coombs 8.9 active agent11.6 burning arcs 8.9 bronchial arteryl 1.4 anionic binding 8.9 liquid phase11.1 binding capacity 8.8 pulmonary artery11.0 starting buffer 8.6 anode holder10.7 antenna beam 8.3 solar radiation10.6 wave lengths 8.2 reaction tubes10.3 wave length 8.0 quadric surface10.1 multiple secant 7.8 brightness tempera-ture10.0 carbon tetrachloride 7.8 mass flow9.9 bronchial arteries 7.7 gas phase9.9 heat transfer 7.7 surface cleaning9.9 ideal gas 7.1 reaction cells9.8 agglutinin activity 7.1 surface active9.5 hydrogen atoms 6.7 artery puhnonary9.4 multiple secants 5.0 anode surface9.3 antibody activity 4.7 surface temperature9.1 particle sizeIn the A genre (News) the following 12 bi-grams survived:12.5 anne m:undel12.0 rhode island10.0 grand jury9.9 rule charter9.2 austin texas8.9 sunday sales8.9 sales tax8.9 payroll tax8.2 fulton county8.0 lbotball eague7.5 kennedy administra-tion7.3 tax billGenres G and N contain few candidates forcollocations (among the 'best' ones in N were"gray eyes", "picked up", "help me" and "sta-red at" which are quite telling about he proto-typical western story: "The gray eyes stared atthe villain who picked up his knife, while thegirl cried "help me".
"7 Other approachesThe temporal dependencies of an orderedcollocation \[wordl, word2\] has been seen asa problem since the theory of mutual infor-mation assumes the frequencies of word pairsto be symmetric (i.e., f(\[wl, w2\]) and f(\[w2,w 1\]) to be equal).
Delta mutual informationrelies on this difference in temporal ordering.
"\[...\] f(x, y) encodes linear prece-dence.
\[...\] Although we could fix thisproblem by redefining f(x, y) to besymmetric (by averaging the matrixwith its transpose), we have decidednot to do so, since order informationappears to be very interesting.
"(Church & Hanks, 1990:p.24)Merkel, Nilsson, & Ahrenberg (1994) haveconstructed a system that uses frequency ofrecurrent segments to determine long phrases.In their approach they have to chunk the textinto contiguous egments.
Significant fre-quency counts are achieved through the use ofa very large corpus, and/or a corpus speciali-sed for a specific task.
They report hat it waspossible for them to divide a large corpus intosmaller sub-sections with little loss.Smadja (1993)finds significant bigramsusing an estimate of z-score (deviation froman expected mean).
Smadja's method seemsto require very large corpora, since the met-hod needs to estimate a reliable measure of thevariance of the frequencies with which wordsco-occur.
This makes the method dependenton the corpus size.
Smadja reports the use ofa corpus of size 10 million words.
"More precisely, the statistical methodswe use do not seem to be effective onlow frequency words (fewer than 100occurrences)."
(Smadja, 1993:p.168)Kita & al.
(1994) proposed another measureof collocational strength that was based on thenotion of a reduction in 'processing cost' if afrequent chunk of text can be processed asone chunk.
Cost reduction tended to extractconventional 'predicate phrase patterns', e.g.,"is that so" and "thank you very much".Steier & Belew (1991) discuss the 'exporting'of phrases into a general vocabulary, where aword pair with high mutual informationwithin a topic tends to have lower mutual in-formation within the collection, and viceversa.
They relate a higher mutual informationwithin a topic than in the collection to a lowervalue of discrimination.Church & Gale (1995) have found it useful tocompare the distribution of terms across do-cuments.
They showed that a distribution dif-ferent from what could be expected by a(random) Poisson process indicates interest-ing terms.
This approach is similar to the useof one genre to find interesting items in596another.
However, removal of the overlapneeds some knowledge about the genres - -apart from checking explicitly for a genre withleast overlap.
Cancelling overlap has the ad-vantage that it can cancel out similar underly-ing causes, while it exaggerates the underly-ing causes that differ between genres.
Somequestions remain: at which level should over-lap be formed?
overlap in words or in bi-grams; how many repetitions does it take fora word or bigram to 'belong' to a genre?8 ConclusionThe question is "what is gained by using ameasure?".
Mutual infornmtion tends to findcombinations of words that are highly co-or-dinated with each other, but these bigramsshow both interesting bigrams (e.g.
"cheshirecat") and conventional (and uninteresting forkeywords) bigrams (e.g.
"in a").
The stabilityof interesting bigrams is improved by de-manding candidate bigrams to occur morethan a fixed number of times.In this paper it has been shown that genrematters, and can be used to extract items thatdiffer between genres.
Instead of balancingone big corpus, the analysis of one corpusmight benefit from finding out how it is diffe-rent from another corpus.
The bigrams thatwere formed by using different genres as fil-ters showed interesting characteristics.However, if we are to deal with largeramounts of data it might be unrealistic tocompare differences directly between twolarge genres without the exclusion of termsthat occur by chance.The method that could be recommendedfrom the results presented in this study is totriangulate a sample by the difference to othergcnres that we have some recta-knowledgeabout (i.e.
we know that Western Fiction andScientific Writing, at least on the surface,have little vocabulary in common).ReferencesChurch, K., & Gale, W. (1995).
InverseDocument Frequency (IDF): A Measure ofDeviations from Poisson.
D. Yarowsky & K.Church (Eds.
), rlhird Workshop on Very LargeCorpora (pp.
121-130), MIT, Cambridge,Mass.Church, K., & Hanks, P. (1990).
WordAssociation Norms, Mutual Information, andI,exicography.
Computational Linguistics,16(1), 22-29.Church, K., & Mercer, R. (1993).
Introduction tothe special issue on ComputationalLinguistics using Large Corpora.Computational Linguistics, 19(1), 1-24.Gale, W., & Church, K. (1994).
What is wrongwith adding one?
Ill N. Oostdijk & P. de Haan(Eds.
), Corpus based research into language(pp.
189-198).
Amsterdam - Atlanta: Rodopi..Iohansson, C. (1994).
Catching the CheshireCat, Coling (pp.
1021-1025), Kyoto, Japan.Kita, K., Omoto, T., Yano, Y., & Kato, Y.(1994).
Application of Corpora in SecondLanguage l~earning - -  The Problem ofCollocational Knowledge Acquisition --, se-cond annual workshop on very large corpora(pp.
43-56), Kyoto, Japan.Merkel, M., Nilsson, B., & Ahrenberg, L.(1994).
A phrase-retrieval system based onrecurrence, second annual workshop on verylarge coq)ora (pp.
99-108), Kyoto, Japan.Sampson, G. (1994).
SUSANNE: A DomesdayBook of English Grannnar.
In N. Oostdijk &P. de Haan (Eds.
), Colpus-based research intolanguage (pp.
169-187).
Amsterdam - Atlanta.Shannon, C.E.
(1951).
Prediction and Entropy ofprinted English.
Bell Systems TechnicalJournal, 30(1l), 50-65.Shannon, C.E., & Weaver, W. (1963).
TheMathematical Theory of Communication.Urbana: University of Illinois Press.Smadja, F. (1993).
Retrieving Collocations fromText: Xtract.
Computational Linguistics,19(1), 143-177.Steier, A.M., & Belew, R.K. (199l).
Exportingphrases: A statistical nalysis of topical ang-uage.
R. Casey & B. Croft (Eds.
), 2ndSymposium on Document Analysis andInformation Retrieval.Stolz, W. (1965).
A probabilistic procedure forgrouping words into phrases.
Language andSpeech, 8, 219-235.Zellig, H. (1955).
From phoneme to morpheme.Language, 31,190-222.597
