Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 78?85,Sydney, July 2006. c?2006 Association for Computational LinguisticsOn Distance between Deep Syntax and Semantic RepresentationVa?clav Nova?kInstitute of Formal and Applied LinguisticsCharles UniversityPraha, Czech Republicnovak@ufal.mff.cuni.czAbstractWe present a comparison of two for-malisms for representing natural languageutterances, namely deep syntactical Tec-togrammatical Layer of Functional Gen-erative Description (FGD) and a seman-tic formalism, MultiNet.
We discuss thepossible position of MultiNet in the FGDframework and present a preliminary map-ping of representational means of thesetwo formalisms.1 IntroductionThe Prague Dependency Treebank 2.0 (PDT 2.0)described in Sgall et al (2004) contains a largeamount of Czech texts with complex and inter-linked morphological (2 million words), syntactic(1.5M words), and complex semantic (tectogram-matical) annotation (0.8M words); in addition,certain properties of sentence information struc-ture and coreference relations are annotated at thesemantic level.The theoretical basis of the treebank lies in theFunctional Generative Description (FGD) of lan-guage system by Sgall et al (1986).PDT 2.0 is based on the long-standing Praguianlinguistic tradition, adapted for the currentcomputational-linguistics research needs.
Thecorpus itself is embedded into the latest annotationtechnology.
Software tools for corpus search, an-notation, and language analysis are included.
Ex-tensive documentation (in English) is provided aswell.An example of a tectogrammatical tree fromPDT 2.0 is given in figure 1.
Function words areremoved, their function is preserved in node at-tributes (grammatemes), information structure isannotated in terms of topic-focus articulation, andevery node receives detailed semantic label corre-sponding to its function in the utterance (e.g., ad-dressee, from where, how often, .
.
.
).
The squarenode indicates an obligatory but missing valent.The tree represents the following sentence:Letos !
!CCC se<<<snaz??
?ona?vratdopolitiky.This year he tries to return to politics.
(1)_ ._.
.
.._.
.. .
._._ ..t-ln94200-123-p12s3rootletost TWHEN basicadv.denot.ngrad.nneg#PersPront ACTn.pron.def.persanim sg 3 basicsna?it_se enuncf PREDv decl disp0 indproc it0 res0 simn?vratf PATn.denotinan sgpolitikaf DIR3 basicn.denotfem sgFigure 1: Tectogrammatical tree of sentence (1)1.1 MultiNetThe representational means of Multilayered Ex-tended Semantic Networks (MultiNet), which are78described in Helbig (2006), provide a universallyapplicable formalism for treatment of semanticphenomena of natural language.
To this end, theyoffer distinct advantages over the use of the clas-sical predicate calculus and its derivatives.
Theknowledge representation paradigm and semanticformalism MultiNet is used as a common back-bone for all aspects of natural language process-ing (be they theoretical or practical ones).
It iscontinually used for the development of intelligentinformation and communication systems and fornatural language interfaces to the Internet.
Withinthis framework, it is subject to permanent practicalevaluation and further development.The semantic representation of natural languageexpressions by means of MultiNet is mainly in-dependent of the considered language.
In con-trast, the syntactic constructs used in differentlanguages to describe the same content are ob-viously not identical.
To bridge the gap be-tween different languages we can employ the deepsyntactico-semantic representation available in theFGD framework.An example of a MultiNet structure is given infigure 2.
The figure represents the following dis-course:Max gave his brother several apples.This was a generous gift.Four of them were rotten.
(2)MultiNet is not explicitly model-theoretical andthe extensional level is created only in those situ-ations where the natural language expressions re-quire it.
It can be seen that the overall structureof the representation is not a tree unlike in Tec-togrammatical representation (TR).
The layer in-formation is hidden except for the most importantQUANT and CARD values.
These attributes con-vey information that is important with respect tothe content of the sentence.
TR lacks attributesdistinguishing intensional and extensional infor-mation and there are no relations like SUBM de-noting relation between a set and its subset.Note that the MultiNet representation crossesthe sentence boundaries.
First, the structure repre-senting a sentence is created and then this structureis assimilated into the existing representation.In contrast to CLASSIC (Brachman et al, 1991)and other KL-ONE networks, MultiNet contains apredefined final set of relation types, encapsula-tion of concepts, and attribute layers concerningcardinality of objects mentioned in discourse.In Section 2, we describe our motivation for ex-tending the annotation in FGD to an even deeperlevel.
Section 3 lists the MultiNet structural coun-terparts of tectogrammatical means.
We discussthe related work in Section 4.
Section 5 deals withvarious evaluation techniques and we conclude inSection 6.2 FGD layersPDT 2.0 contains three layers of information aboutthe text (as described in Hajic?
(1998)):Morphosyntactic Tagging.
This layer representsthe text in the original linear word order witha tag assigned unambiguously to each wordform occurence, much like the Brown corpusdoes.Syntactic Dependency Annotation.
It containsthe (unambiguous) dependency representa-tion of every sentence, with features describ-ing the morphosyntactic properties, the syn-tactic function, and the lexical unit itself.
Allwords from the sentence appear in its repre-sentation.Tectogrammatical Representation (TR).
Atthis level of description, we annotate every(autosemantic non-auxiliary) lexical unitwith its tectogrammatical function, positionin the scale of the communicative dynamismand its grammatemes (similar to the mor-phosyntactic tag, but only for categorieswhich cannot be derived from the word?sfunction, like number for nouns, but not itscase).There are several reasons why TR may not besufficient in a question answering system or MT:1.
The syntactic functors Actor and Patient dis-allow creating inference rules for cognitiveroles like Affected object or State carrier.
Forexample, the axiom stating that an affectedobject is changed by the event ((v AFF o) ?
(v SUBS change.2.1)) can not be usedin the TR framework.2.
There is no information about sorts of con-cepts represented by TR nodes.
Sorts (theupper conceptual ontology) are an importantsource of constraints for MultiNet relations.Every relation has its signature which in turn79Figure 2: MultiNet representation of example discourse (2)reduces ambiguity in the process of text anal-ysis and inferencing.3.
Lexemes of TR have no hierarchy which lim-its especially the search for an answer in aquestion answering system.
In TR there isno counterpart of SUB, SUBR, and SUBSMultiNet relations which connect subordi-nate concepts to superordinate ones and indi-vidual object representatves to correspondinggeneric concepts.4.
In TR, each sentence is isolated from therest of the text, except for coreference arrowsheading to preceding sentences.
This, in ef-fect, disallows inferences combining knowl-edge from multiple sentences in one infer-ence rule.5.
Nodes in TR always correspond to a wordor a group of words in the surface form ofsentence or to a deleted obligatory valencyof another node.
There are no means forrepresenting knowledge generated during theinference process, if the knowledge doesn?thave a form of TR.
For example, consider ax-iom of temporal precedence transitivity (3):(a ANTE b) ?
(b ANTE c) ?
(a ANTE c)(3)In TR, we can not add an edge denoting(a ANTE c).
We would have to include aproposition like ?a precedes c?
as a wholenew clause.For all these reasons we need to extend our textannotation to a form suitable to more advancedtasks.
It is shown in Helbig (2006) that MultiNetis capable to solve all the above mentioned issues.Helbig (1986) describes a procedure for auto-matic translation of natural language utterancesinto MultiNet structures used in WOCADI tool forGerman.
WOCADI uses no theoretical intermedi-ate structures and relies heavily on semanticallyannotated dictionary (HagenLex, see Hartrumpf etal.
(2003)).In our approach, we want to take advantage ofexisting tools for conversions between layers inFGD.
By combining several simpler proceduresfor translation between adjacent layers, we can im-prove the robustness of the whole procedure andthe modularity of the software tools.
Moreover,the process is divided to logical steps correspond-ing to theoretically sound and well defined struc-tures.
On the other hand, such a multistage pro-cessing is susceptible to accumulation of errorsmade by individual components.3 Structural Similarities3.1 Nodes and ConceptsIf we look at examples of TR and MultiNet struc-tures, at first sight we can see that the nodes ofTR mostly correspond to concepts in MultiNet.However, there is a major difference: TR does notinclude the concept encapsulation.
The encapsu-lation in MultiNet serves for distinguishing def-initional knowledge from assertional knowledgeabout given node, e.g., in the sentence ?The oldman is sleeping?, the connection to old will be inthe definitional part of man, while the connectionto the state is sleeping belongs to the assertional80part of the concept representing the man.
In TR,these differences in content are represented by dif-ferences in Topic-Focus Articulation (TFA) of cor-responding words.There are also TR nodes that correspond to noMultiNet concept (typically, the node representingthe verb ?be?)
and TR nodes corresponding to awhole subnetwork, e.g., Fred in the sentence ?Fredis going home.
?, where the TR node representingFred corresponds to the subnetwork1 in figure 3.SUBhumanATTRSUBfirst nameVALfredG01Figure 3: The MultiNet subnetwork correspond-ing to TR node representing Fred3.2 Edges, relations and functionsAn edge of TR between nodes that have theirconceptual counterparts in MultiNet alays corre-sponds to one or more relations and possibly alsosome functions.
In general, it can be said thatMultiNet representation of a text contains signif-icantly more connections (either as relations, or asfunctions) than TR, and some of them correspondto TR edges.3.3 Functors and types of relations andfunctionsThere are 67 functor types in TR (see Hajic?ova?et al (2000) for description), which correspond to94 relation types and 19 function types in Multi-Net (Helbig, 2006).
The mapping of TR functionsto MultiNet is given in table 1:TR functor MultiNet counterpartACMP ASSOCACT AFF, AGT, BENF, CSTR, EXP,MEXP, SCARADDR ORNTADVS SUBST, OPPOSAIM PURPAPP ASSOC, ATTCHcontinued .
.
.1In fact the concept representing the man is the conceptG01, i.e.
only one vertex.
However, the whole network cor-responds to the TR node representing Fred.TR functor MultiNet counterpartAPPS EQU, NAMEATT MODLAUTH AGT, ORIGBEN BENFCAUS CAUS, JUSTCNCS CONCCM *ITMS, MODLCOMPL PROP except for sentential com-plementsCOND CONDCONFR OPPOSCONJ *IMTS-I, *TUPLCONTRA OPPOSCONTRD CONCCPR *COMPCRIT METH, JUST, CIRC, CONFCSQ CAUS, JUST, GOALDIFF *MODP, *OPDIR1 ORIGL, ORIGDIR2 VIADIR3 DIRCL, ELMTDISJ *ALTN2, *VEL2EFF MCONT, PROP, RSLTEXT QMODHER AVRTID NAMEINTT PURPLOC LOC, LEXTMANN MANNR, METHMAT ORIGMMEANS MODE, INSTRMOD MODLOPER *OP, TEMPORIG AVRT, INIT, ORIGM, ORIGL,ORIGPARTL MODLPAT AFF, ATTR, BENF, ELMT,GOAL, OBJ, PARS, PROP,SSPE, VALPREC REAS, OPPOSREAS CAUS, GOALREG CONFRESL CAUS, GOALRESTR *DIFFRHEM MODLRSTR PROP, ATTRSUBS SUBSTcontinued .
.
.81TR functor MultiNet counterpartTFHL DURTFRWH TEMPTHL DURTHO QUANT layerTOWH SUBST, TEMPTPAR TEMP, DURTSIN STRTTTILL FINTWHEN TEMPTable 1: Mapping of TR functors to MultiNetThere are also TR functors with no appropriateMultiNet counterpart: CPHR, DENOM, DPHR,FPHR, GRAD, INTF, PAR, PRED and VOCATTable 2 shows the mapping from MultiNet rela-tions to TR functors:MultiNet TR counterpartRelations:AFF PAT, DIR1AGT ACTANTE TWHENARG1/2/3 ACT, PAT, .
.
.ASSOC ACMP, APPATTCH APPATTR RSTRAVRT ORIG, ADDR, DIR1BENF BENCAUS CAUS, RESL, REAS, GOALCIRC CRITCONC CNCSCOND CONDCONF REG, CRITCSTR ACTCTXT REGDIRCL DIR3DUR TFHL, PAR, THLELMT DIR3, DIR1EXP ACTFIN TTILLGOAL see RSLT, DIRCL and PURPIMPL CAUSINIT ORIGINSTR MEANSJUST CAUSLEXT LOCLOC LOCMANNR MANNcontinued .
.
.MultiNet TR counterpartMCONT PAT, EFFMERO see PARS, ORIGM, *ELMT,*SUBM and TEMPMETH MANN, CRITMEXP ACTMODE see INSTR, METH andMANNRMODL MOD, ATT, PARTL, RHEMNAME ID, APPSOBJ PATOPPOS CONTRAORIG ORIG, DIR1, AUTHORIGL DIR1ORIGM ORIGORNT ADDRPROP COMPL, RSTRPROPR COMPL, RSTRPURP AIMQMOD RSTRREAS see CAUS, JUST and IMPLRPRS LOC, MANNRSLT PAT, EFFSCAR ACTSITU see CIRC and CTXTSOURC see INIT, ORIG, ORIGL,ORIGM and AVRTSSPE PATSTRT TSINSUBST SUBSSUPPL PATTEMP TWHENVAL RSTR, PATVIA DIR2Functions:?ALTN1 CONJ?ALTN1 DISJ?COMP CPR, grammateme DEGCMP?DIFF RESTR?INTSC CONJ?ITMS CONJ?MODP MANN?MODQ RHEM?MODS MANNR?NON grammateme NEGATION?ORD grammateme NUMERTYPE?PMOD RSTR?QUANT MAT, RSTRcontinued .
.
.82MultiNet TR counterpart?SUPL grammateme DEGCMP?TUPL CONJ?UNION CONJ?VEL1 CONJ?VEL2 DISJTable 2: Mapping of MultiNet relations to TRThere are also MultiNet relations and functionswith no counterpart in TR (stars at the begin-ning denote a function): ANLG, ANTO, CHEA,CHPA, CHPE, CHPS, CHSA CHSP, CNVRS,COMPL, CONTR, CORR, DISTG, DPND, EQU,EXT, HSIT, MAJ, MIN, PARS, POSS, PRED0,PRED, PREDR, PREDS, SETOF, SUB, SYNO,VALR, *FLPJ and *OP.From the tables 1 and 2, we can conclude thatalthough the mapping is not one to one, the prepro-cessing of the input text to TR highly reduces theproblem of the appropriate text to MultiNet trans-formation.
However, it is not clear how to solvethe remaining ambiguity.3.4 Grammatemes and layer informationTR has at its disposal 15 grammatemes, whichcan be conceived as node attributes.
Note thatnot all grammatemes are applicable to all nodes.The grammatemes in TR roughly correspond tolayer information in MultiNet, but also to specificMultiNet relations.1.
NUMBER.
This TR grammateme is trans-formed to QUANT, CARD, and ETYPE at-tributes in MultiNet.2.
GENDER.
This syntactical information is nottransformed to the semantic representationwith the exception of occurences where thegrammateme distinguishes the gender of ananimal or a person and where MultiNet usesSUB relation with appropriate concepts.3.
PERSON.
This verbal grammateme is re-flected in cognitive roles connected to theevent or state and is semantically superfluous.4.
POLITENESS has no structural counterpartin MultiNet.
It can be represented in the con-ceptual hierarchy of SUB relation.5.
NUMERTYPE distinguishing e.g.
?three?from ?third?
and ?one third?
is transformed tocorresponding number and also to the mannerthis number is connected to the network.6.
INDEFTYPE corresponds to QUANT andVARIA layer attributes.7.
NEGATION is transformed to both FACTlayer attribute and *NON function combinedwith modality relation.8.
DEGCMP corresponds to *COMP and*SUPL functions.9.
VERBMOD: imp value is represented byMODL relation to imperative, cdn value isambiguous not only with respect to facticityof the condition but also with regard to othercriteria distinguishing CAUS, IMPL, JUSTand COND relatinos which can all result ina sentence with cdn verb.
Also the FACTlayer attribute of several concepts is affectedby this value.10.
DEONTMOD corresponds to MODL rela-tion.11.
DISPMOD is semantically superfluous.12.
ASPECT has no direct counterpart in Multi-Net.
It can be represented by the interplayof temporal specification and RSLT relationconnecting an action to its result.13.
TENSE is represented by relations ANTE,TEMP, DUR, STRT, and FIN.14.
RESULTATIVE has no direct counterpartand must be expressed using the RSLT rela-tion.15.
ITERATIVENESS should be represented bya combination of DUR and TEMP rela-tions where some of temporal concepts haveQUANT layer information set to several.3.5 TFA, quantifiers, and encapsulationIn TR, the information structure of every utteranceis annotated in terms of Topic-Focus Articulation(TFA):1.
Every autosemantic word is marked c, t, orf for contrastive topic, topic, or focus, re-spectively.
The values can distinguish whichpart of the sentence belongs to topic andwhich part to focus.2.
There is an ordering of all nodes according tocommunicative dynamism (CD).
Nodes withlower values of CD belong to topic and nodes83with greater values to focus.
In this way, thedegree of ?aboutness?
is distinguished eveninside topic and focus of sentences.MultiNet, on the other hand, doesn?t containany representational means devoted directly torepresentation of information structure.
Neverthe-less, the differences in the content of sentences dif-fering only in TFA can be represented in MultiNetby other means.
The TFA differences can be re-flected in these categories:?
Relations connecting the topic of sentencewith the remaining concepts in the sentenceare usually a part of definitional knowledgeabout the concepts in the topic, while the re-lations going to the focus belong to the asser-tional part of knowledge about the conceptsin focus.
In other words, TFA can be reflectedin different values of K TYPE attribute.?
TFA has an effect on the identification ofpresuppositions (Peregrin, 1995a) and allega-tions (Hajic?ova?, 1984).
In case of presuppo-sition, we need to know about them in theprocess of assimilation of new informationinto the existing network in order to detectpresupposition failures.
In case of allegation,there is a difference in FACT attribute of theallegation.?
The TFA has an influence on the scope ofquantifiers (Peregrin, 1995b; Hajic?ova?
et al,1998).
This information is fully transformedinto the quantifier scopes in MultiNet.4 Related WorkThere are various approaches trying to analyzetext to a semantic representation.
Some of themuse layered approach and others use only a sin-gle tool to directly produce the target struc-ture.
For German, there is the above mentionedWOCADI parser to MultiNet, for English, thereis a Discourse Representation Theory (DRT) ana-lyzer (Bos, 2005), and for Czech there is a Trans-parent Intensional Logic analyzer (Hora?k, 2001).The layered approaches: DeepThoughtproject (Callmeier et al, 2004) can combineoutput of various tools into one representation.It would be even possible to incorporate TR andMultiNet into this framework.
Meaning-TextTheory (Bolshakov and Gelbukh, 2000) usesan approach similar to Functional GenerativeDescription (Z?abokrtsky?, 2005) but it also has nolayer corresponding to MultiNet.There were attempts to analyze the seman-tics of TR, namely in question answering systemTIBAQ (Jirku?
and Hajic?, 1982), which used TR di-rectly as the semantic representation, and Kruijff-Korbayova?
(1998), who tried to transform the TFAinformation in TR into the DRT framework.5 EvaluationIt is a still open question how to evaluate systemsfor semantic representation.
Basically, three ap-proaches are used in similar projects:First, the coverage of the system may serve as abasis for evaluation.
This criterion is used in sev-eral systems (Bos, 2005; Hora?k, 2001; Callmeieret al, 2004).
However, this criterion is far fromideal, because it?s not applicable to robust systemsand can not tell anything about the quality of re-sulting representation.Second, the consistency of the semantic repre-sentation serves as an evaluation criterion in Bos(2005).
It is a desired state to have a consistentrepresentation of texts, but there is no guaranteethat a consistent semantic representation is in anysense also a good one.Third, the performance in an application(e.g., question answering system) is another cri-terion used for evaluating a semantic representa-tion (Hartrumpf, 2005).
A problem in this kindof evaluation is that we can not separate the eval-uation of the formalism itself from the evaluationof the automatic processing tools.
This problembecomes even bigger in a multilayered approachlike FGD or MTT, where the overall performancedepends on all participating transducers as well ason the quality of the theoretical description.
How-ever, from the user point of view, this is so farthe most reliable form of semantic representationevaluation.6 ConclusionWe have presented an outline of a procedure thatenables us to transform syntactical (tectogrammat-ical) structures into a fully equipped knowledgerepresentation framework.
We have comparedthe structural properties of TR and MultiNet andfound both similarities and differences suggest-ing which parts of such a task are more difficultand which are rather technical.
The comparisonshows that for applications requiring understand-84ing of texts (e.g., question answering system) it isdesirable to further analyze TR into another layerof knowledge representation.AcknowledgementThis work was supported by Czech Academyof Science grant 1ET201120505 and by CzechMinistry of Education, Youth and Sports projectLC536.
The views expressed are not necessarilyendorsed by the sponsors.
We also thank anony-mous reviewers for improvements in the final ver-sion.ReferencesIgor Bolshakov and Alexander Gelbukh.
2000.
TheMeaning-Text Model: Thirty Years After.
Interna-tional Forum on Information and Documentation,1:10?16.Johan Bos.
2005.
Towards Wide-Coverage Se-mantic Interpretation.
In Proceedings of Sixth In-ternational Workshop on Computational SemanticsIWCS-6, pages 42?53.Ronald J. Brachman, Deborah L. McGuinness, Pe-ter F. Patel-Schneider, Lori Alperin Resnick, andAlex Borgida.
1991.
Living with CLASSIC: Whenand How to Use a KL-ONE-like Language.
In JohnSowa, editor, Principles of Semantic Networks: Ex-plorations in the representation of knowledge, pages401?456.
Morgan-Kaufmann, San Mateo, Califor-nia.Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, andMelanie Siegel.
2004.
The DeepThought Core Ar-chitecture Framework.
In Proceedings of LREC,May.Jan Hajic?.
1998.
Building a Syntactically Anno-tated Corpus: The Prague Dependency Treebank.
InE.
Hajic?ova?, editor, Issues of Valency and Meaning.Studies in Honour of Jarmila Panevova?, pages 106?132.
Karolinum, Charles University Press, Prague,Czech Republic.Eva Hajic?ova?, Jarmila Panevova?, and Petr Sgall.2000.
A Manual for Tectogrammatic Tagging ofthe Prague Dependency Treebank.
Technical Re-port TR-2000-09, U?FAL MFF UK, Prague, CzechRepublic.
in Czech.Eva Hajic?ova?, Petr Sgall, and Barbara Partee.
1998.Topic-Focus Articulation, Tripartite Structures, andSemantic Content.
Kluwer, Dordrecht.Eva Hajic?ova?.
1984.
Presupposition and AllegationRevisited.
Journal of Pragmatics, 8:155?167.Sven Hartrumpf, Hermann Helbig, and Rainer Oss-wald.
2003.
The Semantically Based ComputerLexicon HaGenLex ?
Structure and TechnologicalEnvironment.
Traitement automatique des langues,44(2):81?105.Sven Hartrumpf.
2005.
University of hagen at qa@clef2005: Extending knowledge and deepening linguis-tic processing for question answering.
In CarolPeters, editor, Results of the CLEF 2005 Cross-Language System Evaluation Campaign, Work-ing Notes for the CLEF 2005 Workshop, Wien,O?sterreich.
Centromedia.Hermann Helbig.
1986.
Syntactic-Semantic Analy-sis of Natural Language by a New Word-Class Con-trolled Functional Analysis.
Computers and Artifi-cial Inteligence, 5(1):53?59.Hermann Helbig.
2006.
Knowledge Representationand the Semantics of Natural Language.
Springer-Verlag, Berlin Heidelberg.Ales?
Hora?k.
2001.
The Normal Translation Algorithmin Transparent Intensional Logic for Czech.
Ph.D.thesis, Faculty of Informatics, Masaryk University,Brno, Czech Republic.Petr Jirku?
and Jan Hajic?.
1982.
Inferencing and searchfor an answer in TIBAQ.
In Proceedings of the 9thconference on Computational linguistics ?
Volume2, pages 139?141, Prague, Czechoslovakia.Ivana Kruijff-Korbayova?.
1998.
The Dynamic Po-tential of Topic and Focus: A Praguian Approachto Discourse Representation Theory.
Ph.D. thesis,U?FAL, MFF UK, Prague, Czech Republic.Jaroslav Peregrin.
1995a.
Topic, Focus and the Logicof Language.
In Sprachtheoretische Grundlagen fu?rdie Computerlinguistik (Proceedings of the Goettin-gen Focus Workshop, 17.
DGfS), Heidelberg.
IBMDeutschland.Jaroslav Peregrin.
1995b.
Topic-Focus Articulationas Generalized Quantification.
In P. Bosch andR.
van der Sandt, editors, Proceedings of ?Focus andnatural language processing?, pages 49?57, Heidel-berg.
IBM Deutschland.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence in Its Semantic andPragmatic Aspects.
D. Reidel Publishing company,Dodrecht, Boston, London.Petr Sgall, Jarmila Panevova?, and Eva Hajic?ova?.
2004.Deep Syntactic Annotation: Tectogrammatical Rep-resentation and Beyond.
In A. Meyers, editor, Pro-ceedings of the HLT-NAACL 2004 Workshop: Fron-tiers in Corpus Annotation, pages 32?38, Boston,Massachusetts, USA.
Association for Computa-tional Linguistics.Zdene?k Z?abokrtsky?.
2005.
Resemblances betweenMeaning-Text Theory and Functional GenerativeDescription.
In Proceedings of the 2nd Interna-tional Conference of Meaning-Text Theory, pages549?557.85
