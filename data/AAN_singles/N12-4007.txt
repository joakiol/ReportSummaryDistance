T7: On-Demand Distributional SemanticDistance and ParaphrasingYuval MartonABSTRACTSemantic distance measures aim to answer questions such as: How close in meaningare words A and B?
Fore example: "couch" and "sofa"?
(very); "wave" and "ripple"?
(so-so); "wave" and "bank"?
(far).
Distributional measures do that by modeling which wordsoccur next to A and next to B in large corpora of text, and then comparing these modelsof A and B (based on the "Distributional Hypothesis").
Paraphrase generation is the taskof finding B (or a set of B's) given A. Semantic distance measures can be used for bothparaphrase detection and generation, in assessing this closeness between A and B.Both semantic measures and paraphrasing methods are extensible to other textual unitssuch as phrases, sentences, or documents.Paraphrase detection and generation have been gaining traction in various NLPsubfields, including:?
Statistical machine translation (e.g., phrase table expansion)?
MT evaluation (e.g., TERp or Meteor)?
Search, information retrieval and information extraction (e.g., query expansion)?
Question answering and Watson-like applications (e.g., passage or documentclustering)?
Event extraction / event discovery / machine reading (e.g, fitting to existingframes)?
Ontology expansion (e.g., WordNet)?
Language modeling (e.g., semantic LM)?
Textual entailment?
(Multi-)document summarization and natural language generation?
Sentiment analysis and opinion / social network mining (e.g., expansion ofpositive and negative classes)?
Computational cognitive modelingThis tutorial concentrates on paraphrasing words and short word sequences, a.k.a.
"phrases" -- and doing so overcoming previous working memory and representationlimitations.
We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton etal., 2009; Marton, to appear 2012).
We will also cover pivot paraphrasing (Bannard andCallison-Burch, 2005).We will discuss several weaknesses of distributional paraphrasing, and where the state-of-the-art is.
The most notable weakness of distributional paraphrasing is its tendency torank high antonymous (e.g., big-small) and ontological sibling (e.g., cow-sheep)paraphrase candidates.
What qualitative improvement can we hope to achieve withgrowing size of monolingual texts?
What else can be done to ameliorate this problem?
(Mohammad et al, EMNLP 2008; Hovy, 2010; Marton et al, WMT 2011).Another potential weakness is the difficulty in detecting and generating longer-than-word (phrasal) paraphrases, because pre-calculating a collocation matrix for phrasesbecomes prohibitive in the matrix size with longer phrases, even with sparserepresentation.
Unless all phrases are known in advance, this becomes a problem forreal-world applications.We will present an alternative to pre-calculation: on-demand paraphrasing, as describedin Marton (to appear 2012).
There, searching the monolingual text resource is done on-demand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993;Gusfield, 1997; Lopez, 2007).
This enables constructing large vector representation,since there is no longer a need to compute a whole matrix.
Searching for paraphrasecandidates can be done in a reasonable amount of time and memory, for phrases andparaphrases of an arbitrary maximal length.
The resulting technique enables usingricher -- and hence, potentially more accurate -- representations (including higher-dimension tensors).
It opens up a great potential for further gains in research andproduct systems alike, from SMT to search and IR, event discovery, and many otherNLP areas.
