Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 49?57,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Hierarchical Bayesian Model forUnsupervised Induction of Script KnowledgeLea Frermann1l.frermann@ed.ac.ukIvan Titov2titov@uva.nl1ILCC, School of Informatics, University of Edinburgh, United Kingdom2ILLC, University of Amsterdam, Netherlands3Department of Computational Linguistics, Saarland University, GermanyManfred Pinkal3pinkal@coli.uni-sb.deAbstractScripts representing common senseknowledge about stereotyped sequencesof events have been shown to be a valu-able resource for NLP applications.
Wepresent a hierarchical Bayesian model forunsupervised learning of script knowledgefrom crowdsourced descriptions of humanactivities.
Events and constraints on eventordering are induced jointly in one unifiedframework.
We use a statistical modelover permutations which captures eventordering constraints in a more flexibleway than previous approaches.
In orderto alleviate the sparsity problem causedby using relatively small datasets, weincorporate in our hierarchical model aninformed prior on word distributions.
Theresulting model substantially outperformsa state-of-the-art method on the eventordering task.1 IntroductionA script is a ?predetermined, stereotyped se-quence of actions that define a well-known sit-uation?
(Schank and Abelson, 1975).
Whilehumans acquire such common-sense knowledgeover their lifetime, it constitutes a bottleneck formany NLP systems.
Effective question answer-ing and summarization are impossible without aform of story understanding, which in turn hasbeen shown to benefit from access to databases ofscript knowledge (Mueller, 2004; Miikkulainen,1995).
Knowledge about the typical ordering ofevents can further help assessing document co-herence and generating coherent text.
Here, wepresent a general method for acquiring data basesof script knowledge.Our work may be regarded as complementary toexisting work on learning script knowledge fromnatural text (cf.
(Chambers and Jurafsky, 2008)),as not all types of scripts are elaborated in naturaltext ?
being left implicit because of assumed read-ers?
world knowledge.
Our model, operating ondata obtained in a cheap way by crowdsourcing,is applicable to any kind of script and can fill thisgap.
We follow work in inducing script knowl-edge from explicit instantiations of scripts, so-called event sequence descriptions (ESDs) (Reg-neri et al., 2010).
Our data consists of sets ofESDs, each set describing a well-known situationwe will call scenario (e.g., ?washing laundry?
).An ESD consists of a sequence of events, eachdescribing an action defining part of the scenario(e.g., ?place the laundry in the washing machine?
).We refer to descriptions of the same event acrossESDs as event types.
We refer to entities involvedin a scenario as participants (e.g., a ?washing ma-chine?
or a ?detergent?
), and to sets of participantdescriptions describing the same entity as partici-pant types.For each type of scenario, our model clustersdescriptions which refer to the same type of event,and infers constraints on the temporal order inwhich the events types occur in a particular sce-nario.
Common characteristics of ESDs such asevent optionality and varying degrees of temporalflexibility of event types make this task nontrivial.We propose a model which, in contrast to previ-ous approaches, explicitly targets these character-istics.
We develop a Bayesian formulation of thescript learning problem, and present a generativemodel for joint learning of event types and order-ing constraints, arguing that the temporal positionof an event in an ESD provides a strong cue for itstype, and vice versa.
Our model is unsupervisedin that no event- or participant labels are requiredfor training.We model constraints on the order of eventtypes using a statistical model over permutations,the Generalized Mallows Model (GMM; Fligner49and Verducci (1986)).
With the GMM we can flex-ibly model apparent characteristics of scripts, suchas event type-specific temporal flexibility.
Assum-ing that types of participants provide a strong cuefor the type of event they are observed in, we useparticipant types as a latent variable in our model.Finally, by modeling event type occurrence usingBinomial distributions, we can model event op-tionality, a characteristic of scripts that previousapproaches did not capture.We evaluate our model on a data set of ESDscollected via web experiments from non-expertannotators by Regneri et al.
(2010) and compareour model against their approach.
Our modelachieves an absolute average improvement of 7%over the model of Regneri et al.
on the task ofevent ordering.For our unsupervised Bayesian model the lim-ited size of this training set constitutes an ad-ditional challenge.
In order to alleviate thisproblem, we use an informed prior on the worddistributions.
Instead of using Dirichlet priorswhich do not encode a-priori correlations betweenwords, we incorporate a logistic normal distri-bution with the covariance matrix derived fromWordNet.
While we will show that prior knowl-edge as defined above enables the application ofour model to small data sets, we emphasize thatthe model is generally widely applicable for tworeasons.
First, the data, collected using crowd-sourcing, is comparatively easy and cheap to ex-tend.
Secondly, our model is domain independentand can be applied to scenario descriptions fromany domain without any modification.
Note thatparameters were tuned on held-out scenarios, andno scenario-specific tuning was performed.2 Related WorkIn the 1970s, scripts were introduced as a way toequip AI systems with world knowledge (Schankand Abelson, 1975; Barr and Feigenbaum, 1986).Task-specific script databases were developedmanually.
FrameNet (Baker et al., 1998) follows asimilar idea, in defining verb frames together withargument types that can fill the verbs?
argumentslots.
Frames can then be combined into ?scenarioframes?.
Manual composition of such databases,is arguably expensive and does not scale well.This paper follows a series of more recent workwhich aims to infer script knowledge automati-cally from data.
Chambers and Jurafsky (2008)present a system which learns narrative chainsfrom newswire texts.
Relevant phrases are iden-tified based on shared protagonists.
The phrasesare clustered into equivalence classes and tempo-rally ordered using a pipeline of methods.
Wework with explicit event sequence descriptions ofa specific scenario, arguing that large-scale com-mon sense knowledge is hard to acquire from nat-ural text, since it is often left implicit.
Regneriet al.
(2010) induce script knowledge from ex-plicit ESDs using a graph-based method.
Eventtypes and ordering constraints are induced byaligning descriptions of equivalent events usingWordNet-based semantic similarity.
On this basisan abstract graph-representation (Temporal ScriptGraph; TSG) of the scenario is computed, us-ing Multiple Sequence Alignment (MSA).
Ourwork follows the work of Regneri et al.
(2010),in that we use the same data and aim to focus onthe same task.
However, the two approaches de-scribed above employ a pipeline architecture andtreat event learning and learning ordering con-straints as separate problems.
In contrast, we pro-pose to learn both tasks jointly.
We incorporateboth tasks in a hierarchical Bayesian model, thususing one unified framework.A related task, unsupervised frame induction,has also been considered in the past (Titov andKlementiev, 2011; Modi et al., 2012; O?Connor,2012); the frame representations encode eventsand participants but ignore the temporal aspect ofscript knowledge.We model temporal constraints on event typeorderings with the Generalized Mallows Model(GMM; Mallows (1957); Fligner and Verducci(1986); Klementiev et al.
(2008)), a statisticalmodel over permutations.
The GMM is a flexi-ble model which can specify item-specific sensi-tivity to perturbation from the item?s position inthe canonical permutation.
With the GMM we arethus able to model event type-specific temporalflexibility ?
a feature of scripts that MSA cannotcapture.The GMM has been successfully applied tomodeling ordering constraints in NLP tasks.
Chenet al.
(2009) augment classical topic models witha GMM, under the assumption that topics in struc-tured domains (e.g., biographies in Wikipedia)tend to follow an underlying canonical ordering,an assumption which matches well our data (theannotators were asked to follow the temporal or-50der of events in their descriptions (Regneri et al.,2010)).
Chen et al.
show that for these domainstheir approach significantly outperforms Marko-vian modeling of topics.
This is expected asMarkov models (MMs) are not very appropriatefor representing linear structure with potentiallymissing topics (e.g., they cannot encode that ev-ery topic is assigned to at most one continuousfragment of text).
Also GMMs are preferable forsmaller collections such as ours, as the parameternumber is linear in the number of topics (i.e., forus, event types) rather than quadratic as in Markovmodels.
We are not aware of previous work onmodeling events with GMMs.
Conversely, MMswere considered in the very recent work of Che-ung et al.
(2013) in the context of script inductionfrom news corpora where the Markovian assump-tion is much more natural.There exists a body of work for learning par-ticipant types involved in scripts.
Regneri et al.
(2011) extend their work by inducing participanttypes on the basis of the TSG, using structural in-formation about participant mentions in the TSGas well as WordNet similarity, which they thencombine into an Integer Linear Program.
Simi-larly, Chambers and Jurafsky (2009) extend theirwork on narrative chains, presenting a system withwhich they jointly learn event types and semanticroles of the participants involved, but do not con-sider event orderings.
We include participant typesas a latent feature in our model, assuming that par-ticipant mentions in an event description are a pre-dictive feature for the corresponding event type.One way of alleviating the problem of smalldata sets is incorporating informed prior knowl-edge.
Raina et al.
(2006) encode word correlationsin a variance-covariance matrix of a multivariatenormal distribution (MVN), and sample prior pa-rameter vectors from it, thus introducing depen-dencies among the parameters.
They induce thecovariances from supervised learning tasks in thetransfer learning set-up.
We use the same idea, butobtain word covariances from WordNet relations.In a slightly different setting, covariance matricesof MVNs have been used in topic models to inducecorrelation between topics in documents (Blei andLafferty, 2006).3 Problem FormulationOur input consists of a corpus of scenario-specificESDs, and our goal is to label each event descrip-tion in an ESD with one event type e. We specifythe number of possible event types E a priori as anumber exceeding the number of event types in allthe scripts considered.
The model will select aneffective subset of those types.Assume a scenario-specific corpus c, consist-ing of D ESDs, c = {d1, ..., dD}.
EachESD diconsists of Ndevent descriptions di={di,1, ..., di,Ni}.
Boundaries between descriptionsof single events are marked in the data.
For eachevent description di,na bag of participant descrip-tions is extracted.
Each participant descriptioncorresponds to one noun phrase as identified au-tomatically by a dependency parser (cf.
Regneriet al.
(2011)).
We also associate participant typeswith participant descriptions, these types are latentand induced at the inference stage.Given such a corpus of ESDs, our model assignseach event description di,nin an ESD dione eventtype zdi,n= e, where e ?
{1, ..., E}.
Assumingthat all ESDs are generated from the same under-lying set of event types, our objective is to assignthe same event type to equivalent event descrip-tions across all ESDs in the corpus.We furthermore assume that there exists acanonical temporal ordering of event types foreach scenario type, and that events in observedscenarios tend to follow this ordering, but allowingfor some flexibility.
The event labeling sequencezdiof an entire ESD should reflect this canonicalordering.
This allows us to use global structuralpatterns of ESDs in the event type assignments,and thus introducing dependence between eventtypes through their position in the sequence.4 The ModelBefore we describe our model, we briefly explainthe Generalized Mallows Model (GMM) whichwe use to encode a preference for linear orderingof events in a script.4.1 The (Generalized) Mallows ModelThe Mallows Model (MM) is a statistical modelover orderings (Mallows, 1957).
It takes two pa-rameters ?, the canonical ordering, and ?
> 0,a dispersion parameter.
The dispersion parame-ter is a penalty for the divergence d(pi,?)
of anobserved ordering pi from the canonical ordering?.
The divergence can be any distance metric butKendall?s tau distance (?bubble-sort?
distance), anumber of swaps needed to bring pi in the order ?,51is arguably the most common choice.
The proba-bility of an observed ordering pi is defined asP (pi|?,?)
=e??
d(pi,?)?(?
),where ?(?)
is a normalization factor.
The distri-bution is centered around the canonical ordering(as d(?,?)
= 0), and the probability decreasesexponentially with an increasing distance.
For ourpurposes, without loss of generality, we can as-sume that ?
is the identity permutation, that is?
= [1, .
.
.
, n], where n is the number of items.The Mallows model has been generalized totake as a parameter a vector of item-specificdispersion parameters ?
(Fligner and Verducci,1986).
In order to introduce this extension, wefirst need to reformulate Kendall?s tau in a waythat captures item-specific distance.
An orderingpi of n items can be equivalently represented bya vector of inversion counts v of length n ?
1,where each component viequals the number ofitems j > i that occur before item i in pi.
Forexample, for an observed ordering pi = [2,1,0] theinversion vector v = (2, 1).1Then the generalizedMallows model (GMM) is defined asGMM(pi|?)
??ie?
?ivi.The GMM can be factorized into item-specificcomponents, which allows for efficient inference:GMMi(vi|?i) ?
e??ivi.
(1)Intuitively, we will be able to induce event type-specific penalty parameters, and will thus be ableto model individual degrees of temporal flexibilityamong the event types.Since the GMM is member of the exponentialfamily, a conjugate prior can be defined, whichallows for efficient learning of the parameters ?
(Fligner and Verducci, 1990).
Like the GMM, itsprior distribution GMM0can be factorized intoindependent components for each item i:GMM0(?i|vi,0, ?0) ?
e??ivi,0?log(?i(?i))?0.
(2)The parameters vi,0and ?0represent our priorbeliefs about flexibility for each item i, and thestrength of these beliefs, respectively.1Trivially, the inversion count for the last element in thecanonical ordering is always 0.4.2 The Generative StoryOur model encodes two fundamental assumptions,based on characteristics observed in the data: (1)We assume that each event type can occur at mostonce per ESD; (2) Each participant type is as-sumed to occur at most once per event type.The formalized generative story is given in Fig-ure 1.
For each document (ESD) d, we decide in-dependently for each event type e whether to re-alize it or not by drawing from Binomial(?e).2We obtain a binary event vector t where te= 1 ifevent type e is realized and te= 0 otherwise.
Wedraw an event ordering pi from GMM(?
), repre-sented as a vector of inversion counts.Now, we pass event types in the order definedby pi.
For each realized event type i (i.e., i :ti= 1), we first generate a word (normally apredicate) from the corresponding language modelMult(?i).
Then we independently decide for eachparticipant type p whether to realize it or not withthe probability Binomial(?ip).
If realized, theparticipant word (its syntactic head) is generatedfrom the participant language model Mult($p).Note that though the distribution controllingfrequency of participant generation (?ij) is eventtype-specific, the language model associated withthe participant (Mult($j)) is shared acrossevents, thus, ensuring that participant types are de-fined across events.The learnt binary realization parameters ?
and?eshould ensure that an appropriate number ofevents and participants is generated (e.g.
the real-ization probability for obligatory events, observedin almost every ESD for a particular scenario,should be close to 1).Priors We draw the parameters for the binomialdistributions from the Beta distribution, which al-lows us to model a global preference for usingonly few event types and only few participanttypes for each event type.
We draw the parame-ters of the multinomials from the Dirichlet distri-bution, and can thus model a preference towardssparsity.
The GMM parameter vector ?
is drawnfrom GMM0(c.f.
Equation (2)).4.3 Adding Prior KnowledgeSince we are faced with a limited amount of train-ing data, we augment the model described above2We slightly abuse the notation by dropping the super-script d for ESD-specific variables.52Generation of parametersfor event type e = 1, .
.
.
, E do?e?
Beta(?+, ??)
[ freq of event ]?e?
Dirichlet(?)
[event lang mod]for participant type p = 1, .
.
.
, P do?ep?
Beta(?+, ??)
[ freq of ptcpt ]for participant type p = 1, .
.
.
, P do$p?
Dirichlet(?)
[ ptcpt lang mod ]for event type e = 1, .
.
.
, E ?
1 do?e?
GMM0(?0,?0) [ ordering params]Generation of ESD dfor event type e = 1, .
.
.
, E dote?
Binomial(?e) [ realized events ]pi ?
GMM(?,?)
[ event ordering ]for event i from pi s.th.
ti=1 dowi?Mult(?i) [ event lexical unit ]for participant type p = 1, .
.
.
, P doup?
Binomial(?ep) [ realized ptcpts ]if up= 1 thenwp?Mult($p) [ ptcpt lexical unit]Figure 1: The generative story of the basic model.to encode correlations between semantically simi-lar words in the priors for language models.
Wedescribe our approach by first introducing themodel extension allowing for injecting prior cor-relations between words, and then explaining howthe word correlations are derived from WordNet(Fellbaum, 1998).
Since the event vocabularyand the participant vocabulary are separate in ourmodel, the following procedure is carried out sep-arately, but equivalently, for the two vocabularies.4.3.1 Modeling Word CorrelationDirichlet distributions do not provide a way to en-code correlations between words.
To tackle thisproblem we add another level in the model hier-archy: instead of specifying priors Dirichlet(?
)and Dirichlet(?)
directly, we generate them foreach event type e and participant type p using mul-tivariate normal distributions.The modification for the generative story isshown in Figure 2.
In this extension, each eventtype e and participant type p has a different associ-ated (nonsymmetric) Dirichlet prior, ?eand ?p, re-spectively.
The generative story for choosing ?eisthe following: A vector ?eis drawn from the zero-mean normal distribution N(?
?,0), where ?
?isGeneration of parameters ?eand $pfor event type e = 1, .
.
.
, E do?e?
N(?
?, 0)for all words w do?ew=exp(?ew)/?w?exp(?ew?)
[ Dir prior]?e?
Dirichlet(?e) [event lang mod]for participant type p = 1, .
.
.
, P do?p?
N(?
?, 0)for all words w do?pw=exp(?pw)/?w?exp(?pw?)
[ Dir prior]$p?
Dirichlet(?p) [ ptcpt lang mod ]Figure 2: The modified parameter generation pro-cedure for ?eand $pto encode word correlations.the covariance matrix encoding the semantic relat-edness of words (see Section 4.3.2).
The vector?sdimensionality corresponds to size of the vocab-ulary of event words.
Then, the vector is expo-nentiated and normalized to yield ?e.3The sameprocedure is used to choose ?pas shown in Figure2.4.3.2 Defining Semantic SimilarityWe use WordNet to obtain semantic similarityscores for each pair of words in our vocabulary.Since we work on limited domains, we define asubset of WordNet as all synsets that any word inour vocabulary is a member of, plus the hypernymsets of all these synsets.
We then create a featurevector for each word f(wi) as follows:f(wi)n={1 any sense of wi?
synset n0 otherwiseThe similarity of two words wiand wjis de-fined as the dot product f(wi) ?f(wj).
We use thissimilarity to define the covariance matrices ??and??.
Each component (i, j) stores the similaritybetween words wiand wjas defined above.
Notethat the matrices are guaranteed to be valid covari-ance matrices, as they are positive semidefinite byconstruction.5 InferenceOur goal is to infer the set of labelings z of ourcorpus of ESDs.
A labeling z consists of event3In fact, Dirichlet concentration parameters do not needto sum to one.
We experimented with normalizing them toyield a different constant, thus regulating the influence of theprior, but have not observed much of improvement from thisextension.53types t, participant types u and event ordering pi.Additionally, we induce parameters of our model:ordering dispersion parameters (?)
and the lan-guage model parameters ?
and ?.
We induce thesevariables conditioned on all the observable wordsin the data setw.
Since direct joint sampling fromthe posterior distributions is intractable, we useGibbs sampling for approximate inference.
Sincewe chose conjugate prior distributions over the pa-rameter distributions, we can ?collapse?
the Gibbssampler by integrating out all parameters (Grif-fiths and Steyvers, 2004), except for the ones listedabove.
The unnormalized posterior can be writtenas the following product of terms:P (z,?,?, ?|w) ?
?eDCMe?pDCMp?eBBMe?pBBMep?eGMMeMNe?pMNp.The terms DCMeand DCMpare Dirichlet com-pound multinomials associated with event-specificand participant-specific language models:DCMe=?(?v?ev)?
(?vNev+ ?ev)?v?
(Nev+ ?ev)?(?ev)DCMp=?(?v?pv)?
(?vNpv+ ?pv)?v?
(Npv+ ?pv)?
(?pv),where Nevand Npvis the number of times wordtype v is assigned to event e and participant p,respectively.
The terms BBMeand BBMeparethe Beta-Binomial distributions associated withgenerating event types and generating participanttypes for each event type (i.e.
encoding optionalityof events and participants):BBMe??
(N+e+ ?+)?
(N?e+ ??)?
(N+e+N?e+ ?++ ??)BBMep??e?p?
(N+ep+ ?+)?
(N?ep+ ??)?
(N+ep+N?ep+ ?++ ??
),where N+eand N?eis the number of ESDs whereevent type is generated and the number of ESDwhere it is not generated, respectively.
N+epandN?epare analogously defined for participant types(for each event type e).
The term GMMeis as-sociated with the inversion count distribution forevent type e and has the formGMMe?
GMM0(?e;?dvde+ ve,0?0N + ?0, N + ?0),where GMM0is defined in expression (2) and vdeis the inversion count for event e in ESD d. N isthe cumulative number of event occurrences in thedata set.Finally, MNeand MNpcorrespond to theprobability of drawing ?eand ?pfrom the cor-responding normal distributions, as discussed inSection 4.3.1.Though, at each step of Gibbs sampling, com-ponents of z could potentially be sampled byconsidering the full unnormalized posterior, thisclearly can be made much more efficient by ob-serving that only a fraction of terms affect the cor-responding conditional probability.
For example,when sampling an event type for a given eventin a ESD d, only the terms DCMe, BBMepandBBMefor all e and p are affected.
For DCMs itcan be simplified further as only a few word typesare affected.
Due to space constraints, we cannotdescribe the entire sampling algorithms but it natu-rally follows from the above equations and is sim-ilar to the one described in Chen et al.
(2009).For sampling the other parameters of our model,ranking dispersion parameters ?
and the languagemodel parameters ?
and ?, we use slice sampling(MacKay, 2002).
For each event type e we drawits dispersion parameter ?eindependently from theslice sampler.After every nthiteration we resample ?
and?
for all language models to capture the corre-lations.
However, to improve mixing time, wealso resample components ?kiand ?liwhen wordi has changed event membership from type k totype l. In addition we define classes of closelyrelated words (heuristically based on the covari-ance matrix) by classifying words as related whentheir similarity exceeds an empirically determinedthreshold.
We also resample all components ?kjand ?ljfor each word j that related to word i. Were-normalize ?mand ?nafter resampling to up-date the Dirichlet concentration parameters.
Thesame procedure is used for participant languagemodels (parameters ?
).6 EvaluationIn our evaluation, we evaluate the quality of theevent clusters induced by the model and the ex-tent to which the clusters capture the global eventordering underlying the script, as well as the bene-fit of the GMM and the informed prior knowledge.We start by describing data and evaluation metrics.54Scenario Name ]ESDs Avg lenOMICS corpusCook in microwave 59 5.03Answer the telephone 55 4.47Buy from vending machine 32 4.53Make coffee 38 5.00R10 corpusIron clothes 19 8.79Make scrambled eggs 20 10.3Eat in fast food restaurant 15 8.93Return food (in a restaurant) 15 5.93Take a shower 21 11.29Take the bus 19 8.53Table 1: Test scenarios used in experiments (left),the size of the corresponding corpus (middle), andthe average length of an ESD in events (right).6.1 DataWe use the data sets presented in Regneri et al.
(2010) (henceforth R10) for development and test-ing.
The data is comprised of ESDs from two cor-pora.
R10 collected a corpus, consisting of sets ofESDs for a variety of scenarios, via a web exper-iment from non-expert annotators.
In addition weuse ESDs from the OMICS corpus4(Kochender-fer and Gupta, 2003), which consists of instantia-tions of descriptions of several ?stories?, but is re-stricted to indoor activities.
The details of our dataare displayed in Table 1.
For each event descrip-tion we extract all noun phrases, as automaticallyidentified by Regneri et al.
(2011), separating par-ticipant descriptions from action descriptions.
Weremove articles and pronouns, and reduce NPs totheir head words.6.2 Gold Standard and Evaluation MetricsWe follow R10 in evaluating induced event typesand orderings in a binary classification setting.R10 collected a gold standard by classifying pairsof event descriptions w.r.t.
whether or not they areparaphrases.
Our model classifies two event de-scriptions as equivalent whenever ze1= ze2.Equivalently, R10 classify ordered pairs ofevent descriptions as to whether they are presentedin their natural order.
Assuming the identity order-ing as canonical ordering in the Generalized Mal-lows Model, event types tending to occur earlierin the script should be assigned lower cluster IDsthan event types occurring later.
Thus, wheneverze1< ze2, our the model predicts that two eventdescriptions occur in their natural order.4http://csc.media.mit.edu/Event Paraphrase Evt.
OrderingP R F P R FRet.
Food 0.92 0.52 0.67 0.87 0.72 0.79-GMM 0.70 0.30 0.42 0.46 0.44 0.45-COVAR 0.92 0.52 0.67 0.77 0.67 0.71Vending 0.76 0.78 0.77 0.90 0.74 0.81-GMM 0.74 0.39 0.51 0.64 0.47 0.54-COVAR 0.74 0.87 0.80 0.85 0.73 0.78Shower 0.68 0.67 0.67 0.85 0.84 0.85-GMM 0.36 0.17 0.23 0.42 0.38 0.40-COVAR 0.64 0.44 0.52 0.77 0.73 0.75Microwave 0.85 0.80 0.82 0.91 0.74 0.82-GMM 0.88 0.30 0.45 0.67 0.62 0.64-COVAR 0.89 0.81 0.85 0.92 0.82 0.87Table 2: Comparison of model variants: For eachscenario: The full model (top), a version withoutthe GMM (-GMM), and a version with a uniformDirichlet prior over language models (-COVAR).We evaluate the output of our model against thedescribed gold standard, using Precision, Recalland F1 as evaluation metrics, so that our results aredirectly comparable to R10.
We tune our parame-ters on a development set of 5 scenarios which arenot used in testing.6.3 ResultsTable 3 presents the results of our two evaluationtasks.
While on the event paraphrase task the R10system performs slightly better, our model out-performs the R10 system on the event orderingtask by a substantial margin of 7 points averageF-score.
While both systems perform similarly onthe task of event type induction, we induce a jointmodel for both objectives.
The results show that,despite the limited amount of data, and the morecomplex learning objective, our model succeeds ininducing event types and ordering constraints.In order to demonstrate the benefit of the GMM,we compare the performance of our model to avariant which excludes this component (-GMM),cf.
Table 2.
The results confirm our expectationthat biasing the model towards encouraging a lin-ear ordering on the event types provides a strongcue for event cluster inference.As an example of a clustering learnt by ourmodel, consider the following event chain:{get} ?
{open,take} ?
{put,place} ?
{close} ?
{set,select,enter,turn} ?
{start}?
{wait} ?
{remove,take,open} ?
{push,press,turn}We display the most frequent words in the clusters55Scenario Event Paraphrase Task Event Ordering TaskPrecision Recall F1 Precision Recall F1R10 BS R10 BS R10 BS R10 BS R10 BS R10 BSCoffee 0.50 0.47 0.94 0.58 0.65 0.52 0.70 0.68 0.78 0.57 0.74 0.62Telephone 0.93 0.92 0.85 0.72 0.89 0.81 0.83 0.92 0.86 0.87 0.84 0.89Bus 0.65 0.52 0.87 0.43 0.74 0.47 0.80 0.76 0.80 0.76 0.80 0.76Iron 0.52 0.65 0.94 0.56 0.67 0.60 0.78 0.87 0.72 0.69 0.75 0.77Scr.
Eggs 0.58 0.92 0.86 0.65 0.69 0.76 0.67 0.77 0.64 0.59 0.66 0.67Vending 0.59 0.76 0.83 0.78 0.69 0.77 0.84 0.90 0.85 0.74 0.84 0.81Microwave?
0.75 0.85 0.75 0.80 0.75 0.82 0.47 0.91 0.83 0.74 0.60 0.82Shower?
0.70 0.68 0.88 0.67 0.78 0.67 0.48 0.85 0.82 0.84 0.61 0.85Fastfood?
0.50 0.74 0.73 0.87 0.59 0.80 0.53 0.97 0.81 0.65 0.64 0.78Ret.
Food?
0.73 0.92 0.68 0.52 0.71 0.67 0.48 0.87 0.75 0.72 0.58 0.79Average 0.645 0.743 0.833 0.658 0.716 0.689 0.658 0.850 0.786 0.717 0.706 0.776Table 3: Results of our model for the event paraphrase task (left) and event type ordering task (right).Our system (BS) is compared to the system in Regneri et al.
(2010) (R10).
We were able to obtain theR10 system from the authors and evaluate on additional scenarios for which no results are reported inthe paper.
These additional scenarios are marked with a dot (?
).inferred for the ?Microwave?
scenario.
Clustersare sorted by event type ID.
Note that the word?open?
is assigned to two event types in the se-quence, which is intuitively reasonable.
This illus-trates why assuming a deterministic mapping frompredicates to events (as in Chambers and Jurafsky(2008)) is limiting for our dataset.We finally examined the influence of the in-formed prior component, comparing to a modelvariant which uses uniform Dirichlet parameters(-COVAR; see Table 2).
As expected, using an in-formed prior component leads to improved perfor-mance on scenario types with fewer training ESDsavailable (?Take a shower?
and ?Return food?
; cf.Table 1).
For scenarios with a larger set of trainingdocuments no reliable benefit from the informedprior is observable.
We did not optimize this com-ponent, e.g.
by testing more sophisticated meth-ods for construction of the covariance matrix, butexpect to be able to improve its reliability.7 DiscussionThe evaluation shows that our model is able tocreate meaningful event type clusters, which re-semble the underlying event ordering imposed bythe scenario.
We achieve an absolute average im-provement of 7% over a state-of-the-art model.
Incontrast to previous approaches to script induc-tion, our model does not include specifically cus-tomized components, and is thus flexibly applica-ble without additional engineering effort.Our model provides a clean, statistical formula-tion of the problem of jointly inducing event typesand their ordering.
Using a Bayesian model al-lows for flexible enhancement of the model.
Onestraightforward next step would be to explore theinfluence of participants, and try to jointly inferthem with our current set of latent variables.Statistical models highly rely on a sufficientamount of training data in order to be able toinduce latent structures.
The limited amount oftraining data in our case is a bottleneck for the per-formance.
The model performs best on the twoscenarios with the most training data (?Telephone?and ?Microwave?
), which supports this assump-tion.
We showed, however, that our model can beapplied to small data sets through incorporation ofinformed prior knowledge without supervision.8 ConclusionWe presented a hierarchical Bayesian model forjoint induction of event clusters and constraints ontheir orderings from sets of ESDs.
We incorporatethe Generalized Mallows Model over orderings.The evaluation shows that our model successfullyinduces event clusters and ordering constraints.We compare our joint, statistical model to apipeline based model using MSA for event clus-tering.
Our system outperforms the system on thetask of event ordering induction by a substantialmargin, while achieving comparable results in theevent induction task.
We could further explicitlyshow the benefit of modeling global ESD struc-ture, using the GMM.In future work we plan to apply our model tolarger data sets, and to examine the role of par-ticipants in our model, exploring the potential ofinferring them jointly with our current objectives.56AcknowledgmentsWe thank Michaela Regneri for substantial supportwith the script data, and Mirella Lapata for helpfulcomments.ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The berkeley framenet project.In Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguisticsand 17th International Conference on Compu-tational Linguistics, pages 86?90.A.
Barr and E.A.
Feigenbaum.
1986.
The hand-book of artificial intelligence.
1 (1981).
TheHandbook of Artificial Intelligence.
Addison-Wesley.David Blei and John Lafferty.
2006.
Correlatedtopic models.
In Advances in Neural Informa-tion Processing Systems 18, pages 147?154.Nathanael Chambers and Dan Jurafsky.
2008.
Un-supervised learning of narrative event chains.
InProceedings of ACL-08: HLT, pages 789?797.Nathanael Chambers and Dan Jurafsky.
2009.
Un-supervised learning of narrative schemas andtheir participants.
In Proceedings of the 47thAnnual Meeting of the Association for Compu-tational Linguistics, pages 602?610.H.
Chen, S. R. K. Branavan, R. Barzilay, and D. R.Karger.
2009.
Content modeling using latentpermutations.
Journal of Artificial IntelligenceResearch, 36(1):129?163.Christiane Fellbaum, editor.
1998.
WordNet: anelectronic lexical database.
MIT Press.M.
Fligner and J. Verducci.
1986.
Distance basedranking models.
Journal of the Royal StatisticalSociety, Series B, 48:359?369.M.
Fligner and J. Verducci.
1990.
Posterior prob-abilities for a consensus ordering.
Psychome-trika, 55:53?63.T.
L. Griffiths and M. Steyvers.
2004.
Findingscientific topics.
Proceedings of the NationalAcademy of Sciences, 101(Suppl.
1):5228?5235.Alexandre Klementiev, Dan Roth, and KevinSmall.
2008.
Unsupervised rank aggregationwith distance-based models.
In Proceedings ofthe 25th International Conference on MachineLearning, pages 472?479.Mykel J. Kochenderfer and Rakesh Gupta.
2003.Common sense data acquisition for indoor mo-bile robots.
In Proceedings of the NineteenthNational Conference on Artificial Intelligence(AAAI-04), pages 605?610.D.
J. C. MacKay.
2002.
Information Theory, Infer-ence & Learning Algorithms.
Cambridge Uni-versity Press, New York, NY, USA.C.
L. Mallows.
1957.
Non-null ranking models.Biometrika, 44:114?130.Risto Miikkulainen.
1995.
Script-based inferenceand memory retrieval in subsymbolic story pro-cessing.
Applied Intelligence, pages 137?163.Ashutosh Modi, Ivan Titov, and Alexandre Kle-mentiev.
2012.
Unsupervised induction offrame-semantic representations.
In Proceedingsof the NAACL-HLT Workshop on the Inductionof Linguistic Structure, pages 1?7.Erik T. Mueller.
2004.
Understanding script-basedstories using commonsense reasoning.
Cogni-tive Systems Research, 5(4):307?340.Brendan O?Connor.
2012.
Bayesian unsupervisedframe learning from text.
Technical report,Carnegie Mellon University.Rajat Raina, Andrew Y. Ng, and Daphne Koller.2006.
Constructing informative priors usingtransfer learning.
In Proceedings of the 23rd In-ternational Conference on Machine Learning,pages 713?720.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning script knowledge withweb experiments.
In Proceedings of the 48thAnnual Meeting of the Association for Compu-tational Linguistics, pages 979?988.Michaela Regneri, Alexander Koller, Josef Rup-penhofer, and Manfred Pinkal.
2011.
Learningscript participants from unlabeled data.
In Pro-ceedings of RANLP 2011, pages 463?470.Roger C. Schank and Robert P. Abelson.
1975.Scripts, plans, and knowledge.
In Proceedingsof the 4th International Joint Conference on Ar-tificial Intelligence, IJCAI?75, pages 151?157.Ivan Titov and Alexandre Klementiev.
2011.
Abayesian model for unsupervised semantic pars-ing.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Lin-guistics: Human Language Technologies, pages1445?1455.57
