FROM N-GRAMS TO COLLOCATIONSAN EVALUATION OF XTRACTFrank A. SmadjaDepar tment  of Computer  ScienceCo lumbia  UniversityNew York, NY  10027Abst ractIn previous papers we presented methods forretrieving collocations from large samples oftexts.
We described a tool, X t rac t ,  that im-plements these methods and able to retrievea wide range of collocations in a two stageprocess.
These methods a.s well as other re-lated methods however have some limitations.Mainly, the produced collocations do not in-clude any kind of functional information andmany of them are invalid.
In this paper weintroduce methods that address these issues.These methods are implemented in an addedthird stage to Xt ract  that examines the set ofcollocations retrieved uring the previous twostages to both filter out a number of invalid col-locations and add useful syntactic informationto the retained ones.
By combining parsing andstatistical techniques the addition of this thirdstage has raised the overall precision level ofX t rac t  from 40% to 80% With a precision of94%.
In the paper we describe the methodsand the evaluation experiments.1 INTRODUCTIONIn the past, several approaches have been proposed toretrieve various types of collocations from the analysisof large samples of textual data.
Pairwise associations(bigrams or 2-grams) (e.g., \[Smadja, 1988\], \[Church andHanks, 1989\]) as well as n-word (n > 2) associations(or n-grams) (e.g., \[Choueka el al., 1983\], \[Smadja ndMcKeown, 1990\]) were retrieved.
These techniques auto-matically produced large numbers of collocations alongwith statistical figures intended to reflect heir relevance.However, none of these techniques provides functional in-formation along with the collocation.
Also, the resultsproduced often contained improper word associations re-flecting some spurious aspect of the training corpus thatdid not stand for true collocations.
This paper addressesthese two problems.Previous papers (e.g., \[Smadja nd McKeown,1990\]) introduced a.set of tecl)niques and a. tool, Xt ract ,that produces various types of collocations from a two-stage statistical analysis of large textual corpora brieflysketched in the next section.
In Sections 3 and 4, weshow how robust parsing technology can be used to bothfilter out a number of invalid collocations as well as adduseful syntactic information to the retained ones.
Thisfilter/analyzer is implemented in a third stage of Xtractthat automatically goes over a the output collocations toreject the invalid ones and label the valid ones with syn-tactic information.
For example, if the first two stagesof Xtract produce the collocation "make-decision," thegoal of this third stage'is to identify it as a verb-objectcollocation.
If no such syntactic relation is observed,then the collocation is rejected.
In Section 5 we presentan evaluation of Xtract as a collocation retrieval sys-tem.
The addition of the third stage of Xtract has beenevaluated to raise the precision of X t rac t  from 40% to80??
and it has a recall of 94%.
In this paper we use ex-amples related to the word "takeover" from a 10 millionword corpus containing stock market reports originatingfrom the Associated Press newswire.2 F IRST 2 STAGES OF XTRACT,PRODUCING N-GRAMSIn af i rst  stage, X t rac t  uses statistical techniques toretrieve pairs of words (or bigrams) whose common ap-pearances within a single sentence are correlated in thecorpus.
A bigram is retrieved if its frequency of occur-rence is above a certain threshold and if the words areused in relatively rigid ways.
Some bigrams producedby the first stage of X t rac t  are given in Table 1: thebigrams all contain the word "takeover" and an adjec-tive.
In the table, the distance parameter indicates theusual distance between the two words.
For example,distance = 1 indicates that the two words are fre-quently adjacent in the corpus.In a second stage, X t rac t  uses the output bi-grams to produce collocations involving more than twowords (or n-grams).
It examines all the sentences con-taining the bigram and analyzes the statistical distri-bution of words and parts of speech for each positionaround the pair.
It retains words (or parts of speech) oc-cupying a position with probability greater than a given279threshold.
For example, the bigram "average-industrial"produces the n-gram "the Dow Jones industrial average"since the words are always used within this compoundin the training corpus.
Example.
outputs of the secondstage of X t raet  are given in Figure 1.
In the figure, thenumbers on the left indicate the frequency of the n-gramsin the corpus, NN indicates that.
a noun is expected atthis position, AT indicates that an article is expected,NP stands for a proper noun and VBD stands for a verbin the past tense.
See \[Smadja nd McKeown, 1990\] and\[Smadja, 1991\] for more details on these two stages.Table 1: Output of Stage 1Wihostilehostilecorporatehostileunwantedpotentialunsolicitedunsuccessfulfriendlytakeovertakeoverbigwjtakeoverstakeovertakeoverstakeoverstakeovertakeovertakeovertakeovertakeoverexpensivebigtakeoverdistance1112111112413 STAGE THREE:  SYNTACTICALLYLABEL ING COLLOCATIONSIn the past, Debili \[Debili, 1982\] parsed corpora of Frenchtexts to identify non-ambiguous predicate argument rela-tions.
He then used these relations for disambiguation inparsing.
Since then, the advent of robust parsers uch asCass \[Abney, 1990\], F idd i teh \[Itindle, 1983\] has made itpossible to process large amounts of text with good per-formance.
This enabled Itindle and Rooth \[Hindle andRooth, 1990\], to improve Debili's work by using bigramstatistics to enhance the task of prepositional phrase at-tachment.
Combining statistical and parsing methodshas also been done by Church and his colleagues.
In\[Church et al, 1989\] and \[Church'et ai., 1991\] they con-sider predicate argument relations in the form of ques-tions such as What does a boat typically do?
They arepreprocessing a corpus with the F idd l teh parser in orderto statistically analyze the distribution of the predicatesused with a given argument such as "boat.
"Our goal is different, since we analyze a set ofcollocations automatically produced by Xt rac t  to eitherenrich them with syntactic information or reject them.For example, i f ,  bigram collocation produced by Xt rac tinvolves a noun and a verb, the role of Stage 3 of X t rac tis to determine whether it is a subject-verb or a verb-object collocation.
If no such relation can be identified,then the collocation is rejected.
This section presentsthe algorithm for X t rac t  Stage 3 in some detail.
Forillustrative purposes we use the example words takeoverand thwart with a distance of 2.3.1 DESCRIPT ION OF THE ALGORITHMInput :  A bigram with some distance information in-dicating the most probable distance between the twowords.
For example, takeover and thwart with a distanceof 2.Output /Goah Either a syntactic label for the bigramor a rejection.
In the case of takeover and thwart thecollocation is accepted and its produced label is VO forverb-object.The algorithm works in the following 3 steps:3.1.1 Step 1: PRODUCE TAGGEDCONCORDANCESAll the sentences in the corpus that contain thetwo words in this given position are produced.
Thisis done with a concord,acing program which is part ofX t raet  (see \[Smadja, 1991\]).
The sentences are labeledwith part of speech information by preprocessing the cor-pus with an automatic stochastic tagger.
13.1.2 Step 2: PARSE THE SENTENCESEach sentence is then processed by Cass, abottom-up incremental parser \[Abney, 1990\].
2 Casstakes input sentences labeled with part of speech andattempts to identify syntactic structure.
One of Cassmodules identifies predicate argument relations.
We usethis module to produce binary syntactic relations (or la-bels) such as "verb-object" (VO), %erb-subject" (VS),"noun-adjective" (N J), and "noun-noun" ( N N ).
Con-sider Sentence (1) below and all the labels as producedby Cass on it.
(1) "Under the recapitalization plan it proposed tothwar t  the takeover .
"label bigrarnSV it proposedNN recapitalization planVO thwart takeoverFor each sentence in the concordance set, fromthe output of Cass, X t rac t  determines the syntacticrelation of the two words among VO, SV, N J, NN andassigns this label to the sentence.
If no such relation isobserved, X t rac t  associates the label U (for undefined)to the sentence.
We note label\[ia~ the label associated1For this, we use the part of speech tagger described in\[Church, 1988\].
This program was developed at Bell Labora-tories by Ken Church.UThe parser has been developed at Bell CommunicationResearch by Steve Abney, Cass stands for Cascaded Analysisof Syntactic Structure.
I am much grateful to Steve Abneyto help us use and customize Cass for this work.280681 .
.
.
.
takeover bid .
.
.
.
.
.310 .
.
.
.
takeover offer .
.
.
.
.
.258 .
.
.
.
takeover attempt .
.
.
.
.177 .
.
.
.
takeover battle .
.
.
.
.
.154 .
.
.
.
.
.
NN NN takeover defense .
.
.
.
.
.153 .
.
.
.
takeover target .
.
.
.
.
.
.119 .
.
.
.
.
a possible takeover NN .
.
.
.
.
.118 .
.
.
.
.
.
.
takeover law .
.
.
.
.
.
.109 .
.
.
.
.
.
.
takeover rumors .
.
.
.
.
.102 .
.
.
.
.
.
.
takeover speculation .
.
.
.
.
.84 .
.
.
.
takeover strategist .
.
.
.
.
.69 .
.
.
.
.
.
.
AT takeover fight .
.
.
.
.62 .
.
.
.
.
.
.
corporate takeover .
.
.50 .
.
.
.
takeover proposals .
.
.
.
.
.40 .
.
.
.
.
.
.
Federated's poison pill takeover defense .
.
.
.
.
.33 .
.
.
.
NN VBD a sweetened takeover offer from .
NP .
.
.Figure 1: Some n-grams containing "takeover"with Sentence id.
For example, the label for Sentence (1)is: label\[l\] - VO.4 A LEX ICOGRAPHICEVALUATION3.1.3 S tep  3: RE JECT OR LABELCOLLOCATIONThis last step consists o f  deciding on a label forthe bigram from the set of label\[i~'.s.
For this, we countthe frequency of each label for the bigram and performa statistical analysis of this distribution.
A collocationis accepted if the two seed words are consistently usedwith the same syntactic relation.
More precisely, thecollocation is accepted if and only if there is a label 12 ~:U satisfying the following inequation:\[probability(labeliid \] = ?
)> T Iin which T is a given threshold to be determinedby the experimenter.
A collocation is thus rejected if novalid label satisfies the inequation or if U satisfies it.Figure 2 lists some accepted collocations in theformat produced by Xt ract  with their syntactic labels.For these examples, the threshold T was set to 80%.For each collocation, the first line is the output of thefirst stage of X t rac t .
It is the seed bigram with thedistance between the two words.
The second line is theoutput of the second stage of X t rac t ,  it is a multipleword collocation (or n-gram).
The numbers on the leftindicate the frequency of occurrence of the n-gram inthe corpus.
The third line indicates the syntactic labelas determined by the third stage of X t rac t .
Finally,the last lines simply list an example sentence and theposition of the collocation in the sentence.Such collocations can then be used for vari-ous purposes including lexicography, spelling correction,speech recognition and language generation.
Ill \[Smadjaand McKeown, 1990\] and \[Smadja, 1991\] we describehow they are used to build a lexicon for language gener-ation in the domain of stock market reports.The third stage of X t rac t  can thus be considered as aretrieval system which retrieves valid collocations froma set of candidates.
This section describes an evaluationexperiment of the third stage of X t rac t  as a retrievalsystem.
Evaluation of retrieval systems is usually donewith the help of two parameters: precision and recall\[Salton, 1989\].
Precision of a retrieval system is definedas the ratio of retrieved valid elements divided by thetotal number of retrieved elements \[Salton, 1989\].
Itmeasures the quality of the retrieved material.
Recallis defined as the ratio of retrieved valid elements dividedby the total number of valid elements.
It measures theeffectiveness of the system.
This section presents an eval-uation of the retrieval performance of the third stage ofX t rac t .4.1 THE EVALUATION EXPERIMENTDeciding whether a given word combination is avalid or invahd collocation is actually a difficult taskthat is best done by a lexicographer.
Jeffery Triggs isa lexicographer working for Oxford English Dictionary(OED) coordinating the North American Readers pro-gram of OED at Bell Communication Research.
Jef-fery Triggs agreed to manually go over several thousandscollocations, aWe randomly selected a subset of about 4,000collocations that contained the information compiled byXt ract  after the first 2 stages.
This data  set was thenthe subject of the following experiment.We gave the 4,000 collocations to evaluate to thelexicographer, asking him to select the ones that he3I am grateful to Jeffery whose professionalism and kind-ness helped me understand some of the difficulty of lexicog-raphy.
Without him this evaluation would not have beenpossible.281takeover bid -1681 .
.
.
.
takeover bid IN .
.
.
.
.Syntactic Label: NN10 11An investment partnership on Friday offered to sweeten itstakeover bid for Gencorp Inc.takeover fight -169 .
.
.
.
.
.
.
AT takeover fight IN .
.
.
.
.
.
69Syntactic Label: NN10 11Later last year Hanson won a hostile 3.9 billion takeover fight for Imperial Groupthe giant British food tobacco and brewing conglomerate and raised more than 1.4billion pounds from the sale of Imperial s Courage brewing operation andits leisure products businesses.takeover thwart  244 .
.
.
.
.
to thwart  AT takeover NN .
.
.
.
.
.
.
44Syntactic Label: VO13 11The 48.50 a share offer announced Sunday is designed to thwart  a takeover bidby GAF  Corp.takeover make 268 .
.
.
.
.
MD make a takeover NN .
J J  .
.
.
.
.
68Syntactic Label: VO14 12Meanwhile the North Carolina Senate approved a bill Tuesday that  would make atakeover of North Carol ina based companies more difficult and the House wasexpected to approve the measure before the end of the week.takeover elated -159 .
.
.
.
takeover elated .
.
.
.
.
.
.
59Syntactic Label: SV23Among takeover elated issues Kidde jumped 2 to 66.F igure  2: Some examples  of  co l locat ions w i th  "takeover"YY=J20% Y=20% N = 60 % T = 40% U = 60%T w. 94% T = 94%U OU = 9,5%Y ---- t0%YY = 40%N - -  92%F igure 3: Over lap  of the manua l  and automat ic  eva luat ions282would consider for a domain specific dictionary and tocross out the others.
The lexicographer came up withthree simple tags, YY,  Y and N. Both Y and YY  aregood collocations, and N are bad collocations.
The dif-ference between YY  and Y is that Y collocations are ofbetter quality than YY  collocations.
YY  collocationsare often too specific to be included in a dictionary, orsome words are missing, etc.
After Stage 2, about 20%of the collocations are Y, about 20% are YY, and about60% are N. This told us that the precision of X t rac t  atStage 2 was only about 40 %.Although this would seem like a poor precision,one should compare it with the much lower rates cur-rently in practice in lexicography.
For the OED, forexample, the first stage roughly consists of reading nu-merous documents to identify new or interesting expres-sions.
This task is performed by professional readers.For the OED, the readers for the American programalone produce some 10,000 expressions a month.
Theselists are then sent off to the dictionary and go throughseveral rounds of careful analysis before actually beingsubmitted to the dictionary.
The ratio of proposed can-didates to good candidates i  usually low.
For example,out of the 10,000 expressions proposed each month, lessthan 400 are serious candidate for the OED, which rep-resents a current rate of 4%.
Automatically producinglists of candidate xpressions could actually be of greathelp to lexicographers and even a precision of 40% wouldbe helpful.
Such lexicographic tools could, for example,help readers retrieve sublanguage specific expressions byproviding them with lists of candidate collocations.
Thelexicographer then manually examines the list to removethe irrelevant data.
Even low precision is useful forlexicographers as manual filtering is much faster thanmanual scanning of the documents \[Marcus, 1990\].
Suchtechniques are not able to replace readers though, as theyare not designed to identify low frequency expressions,whereas a human reader immediately identifies interest-ing expressions with as few as one occurrence.The second stage of this experiment was to useXt rac t  Stage 3 to filter out and label the sample set ofcollocations.
As described in Section 3, there are severalvalid labels (VO, VS, NN, etc.).
In this experiment, wegrouped them under a single label: T. There is only onenon-valid label: U (for unlabeled}.
A T collocation isthus accepted by Xt rac t  Stage 3, and a U collocation isrejected.
The results of the use of Stage 3 on the sampleset of collocations are similar to the manual evaluationin terms of numbers: about 40% of the collocations werelabeled (T) by Xt rac t  Stage 3, and about 60% wererejected (U).Figure 3 shows the overlap of the classificationsmade by Xt rac t  and the lexicographer.
In the figure,the first diagram on the left represents the breakdown inT and U of each of the manual categories (Y - YY andN).
The diagram on the right represents the breakdownin Y - YY and N of the the T and U categories.
Forexample, the first column of the diagram on the left rep-resents the application of X t rac t  Stage 3 on the YY col-locations.
It shows that 94% of the collocations acceptedby the lexicographer were also accepted by Xt ract .
Inother words, this means that the recall ofthe third stageof X t rac t  is 94%.
The first column of the diagram on theright represents the lexicographic evaluation of the collo-cations automatically accepted by Xt ract .
It shows thatabout 80% of the T collocations were accepted by thelexicographer and that about 20% were rejected.
Thisshows that precision was raised from 40% to 80% withthe addition of X t rac t  Stage 3.
In summary, these ex-periments allowed us to evaluate Stage 3 as a retrievalsystem.
The results are:I Prec is ion = 80% Recal l  = 94% \]5 SUMMARY ANDCONTRIBUT IONSIn this paper, we described a new set of techniques forsyntactically filtering and labeling collocations.
Usingsuch techniques for post processing the set of colloca-tions produced by Xt rac t  has two major results.
First,it adds syntax to the collocations which is necessary forcomputational use.
Second, it provides considerable im-provement to the quality of the retrieved collocations asthe precision of X t rac t  is raised from 40% to 80% witha recall of 94%.By combining statistical techniques with a sophis-ticated robust parser we have been able to design andimplement some original techniques for the automaticextraction of collocations.
Results so far are very en-couraging and they indicate that more efforts should bemade at combining statistical techniques with more sym-bolic ones.ACKNOWLEDGMENTSThe research reported in this paper was partially sup-ported by DARPA grant N00039-84-C-0165, by NSFgrant IRT-84-51438 and by ONR grant N00014-89-J-1782.
Most of this work is also done in collaboration withBell Communication Research, 445 South Street, Mor-ristown, N3 07960-1910.
I wish to express my thanksto Kathy McKeown for her comments on the researchpresented in this paper.
I also wish to thank Dor~eSeligmann and Michael Elhadad for the time they spentdiscussing this paper and other topics with me.References\[Abney, 1990\] S. Abney.
Rapid Incremental Parsingwith Repair.
In Waterloo Conference on ElectronicText Research, 1990.\[Choueka el al., 1983\] Y. Choueka, T. Klein, andE.
Neuwitz.
Automatic Retrieval of Frequent Id-iomatic and Collocational Expressions in a Large Cot-283pus.
Journal for Literary and Linguistic computing,4:34-38, 1983.\[Church and Hanks, 1989\] K. Church and K. Hanks.Word Association Norms, Mutual Information, andLexicography.
In Proceedings of the 27th meeting ofthe A CL, pages 76-83.
Association for ComputationalLinguistics, 1989.
Also in Computational Linguistics,vol.
16.1, March 1990.\[Church et at., 1989\] K.W.
Church, W. Gale, P. Hanks,and D. Hindle.
Parsing, Word Associations and Typ-ical Predicate-Argument Relations.
In Proceedings ofthe International Workshop on Parsing Technologies,pages 103-112, Carnegie Mellon University, Pitts-burgh, PA, 1989.
Also appears in Masaru Tomita(ed.
), Current Issues in Parsing Technology, pp.
103-112, Kluwer Academic Publishers, Boston, MA, 1991.\[Church et at., 1991\] K.W.
Church, W. Gale, P. Hanks,and D. Hindle.
Using Statistics in Lexical Analysis.
InUri ~ernik, editor, Lexical Acquisition: Using on-lineresources to build a lexicon.
Lawrence Erlbaum, 1991.In press.\[Church, 1988\] K. Church.
Stochastic Parts Progralnand Noun Phrase Parser for Unrestricted Text.
InProceedings of the Second Conference on Applied Nat-ural Language Processing, Austin, Texas, 1988.\[Debili, 1982\] F. Debili.
Analyse Syntactico-SdmantiqueFondde sur une Acquisition Automatique de RelationsLexicales Sdmantiques.
PhD thesis, Paris XI Univer-sity, Orsay, France, 1982.
Th~se de Doctorat D'~tat.\[Hindle and Rooth, 1990\] D. Hindle and M. Rooth.Structural Ambiguity and Lexieal Relations.
InDARPA Speech and Natural Language Workshop, Hid-den Valley, PA, June 1990.\[Hindle, 1983\] D. Hindle.
User Manual for Fidditch, aDeterministic Parser.
Technical Memorandum 7590-142, Naval Research laboratory, 1983.\[Marcus, 1990\] M. Marcus.
Tutorial on Tagging andProcessing Large Textual Corpora.
Presented at the28th annual meeting of the ACL, June 1990.\[Salton, 1989\] J. Salton.
Automatic Text Processing,The Transformation, Analysis, and Retrieval of In-formation by Computer.
Addison-Wesley PublishingCompany, NY, 1989.\[Smadja and McKeown, 1990\] F. Smadja nd K. McKe-own.
Automatically Extracting and Representing Col-locations for Language Generation.
In Proceedings ofthe 28th annual meeting of the ACL, Pittsburgh, PA,June 1990.
Association for Computational Linguistics.\[Smadja, 1988\] F. Smadja.
Lexical Co-occurrence, TheMissing Link in Language Acquisition.
Ill Programand abstracts of the 15 th International ALLC, Con-ference of the Association for Literary and LinguisticComputing, Jerusalem, Israel, June 1988.\[Smadja, 1991\] F. Smadja.
Retrieving CollocationalKnowledge from Textual Corpora.
An Application:Language Generation.
PhD thesis, Computer ScienceDepartment, Columbia University, New York, NY,April 1991.284
