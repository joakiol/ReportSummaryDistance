Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 19?23,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsClassifying Negative Findings in Biomedical PublicationsBei YuSchool of Information StudiesSyracuse Universitybyu@syr.eduDaniele FanelliSchool of Library and Information ScienceUniversity of Montrealemail@danielefanelli.comAbstractPublication bias refers to the phenome-non that statistically significant, ?posi-tive?
results are more likely to be pub-lished than non-significant, ?negative?results.
Currently, researchers have tomanually identify negative results in alarge number of publications in order toexamine publication biases.
This paperproposes an NLP approach for automati-cally classifying negated sentences in bi-omedical abstracts as either reportingnegative findings or not.
Using multino-mial na?ve Bayes algorithm and bag-of-words features enriched by parts-of-speeches and constituents, we built aclassifier that reached 84% accuracybased on 5-fold cross validation on a bal-anced data set.1 IntroductionPublication bias refers to the phenomenon thatstatistically significant, ?positive?
results aremore likely to be published than non-significant,?negative?
results (Estabrook et al., 1991).
Dueto the ?file-drawer?
effect (Rosenthal, 1979),negative results are more likely to be ?filedaway?
privately than to be published publicly.Publication bias poses challenge for an accu-rate review of current research progress.
Itthreatens the quality of meta-analyses and sys-tematic reviews that rely on published researchresults (e.g., the Cochrane Review).
Publicationbias may be further spread through citation net-work, and amplified by citation bias, a phenome-non that positive results are more likely to becited than negative results (Greenberg, 2009).To address the publication bias problem, somenew journals were launched and dedicated topublishing negative results, such as the Journalof Negative Results in Biomedicine, Journal ofPharmaceutical Negative Results, Journal ofNegative Results in Ecology and EvolutionaryBiology, and All Results Journals: Chem.
Somequantitative methods like the funnel plot (Eggeret al., 1997) were used to measure publicationbias in publications retrieved for a certain topic.A key step in such manual analysis is to exam-ine the article abstracts or full-texts to see wheth-er the findings are negative or not.
For example,Hebert et al.
(2002) examined the full text of1,038 biomedical articles whose primary out-comes were hypothesis testing results, and found234 (23%) negative articles.
Apparently, suchmanual analysis approach is time consuming.
Anaccurate, automated classifier would be ideal toactively track positive and negative publications.This paper proposes an NLP approach for au-tomatically identifying negative results in bio-medical abstracts.
Because one publication mayhave multiple findings, we currently focus onclassifying negative findings at sentence level:for a sentence that contains the negation cues?no?
and/or ?not?, we predict whether the sen-tence reported negative finding or not.
We con-structed a training data set using manual annota-tion and convenience samples.
Two widely-usedtext classification algorithms, Multinomial naiveBayes (MNB) and Support Vector Machines(SVM), were compared in this study.
A few textrepresentation approaches were also comparedby their effectiveness in building the classifier.The approaches include (1) bag-of-words(BOW), (2) BOW with PoS tagging and shallowparsing, and (3) local contexts of the negationcues ?no?
and ?not?, including the words, PoStags, and constituents.
The best classifier wasbuilt using MNB and bag-of-words features en-riched with PoS tags and constituent markers.The best performance is 84% accuracy based on5-fold cross validation on a balanced data set.192 Related workThe problem of identifying negative results isrelated to several other BioNLP problems, espe-cially on negation and scientific claim identifica-tion.The first relevant task is to identify negationsignals and their scopes (e.g., Morante andDaelemans, 2008;2009; Farkas et al., 2010;Agarwal et al., 2011).
Manually-annotated cor-pora like BioScope (Szarvas et al., 2008) werecreated to annotate negations and their scopes inbiomedical abstracts in support of automatedidentification.
This task targets a wide range ofnegation types, such as the presence or absenceof clinical observations in narrative clinical re-ports (Chapman et al., 2001).
In comparison, ourtask focuses on identifying negative findings on-ly.
Although not all negations report negativeresults, negation signals are important rhetoricaldevice for authors to make negative claims.Therefore, in this study we also examine preci-sion and recall of using negation signals as pre-dictors of negative findings.The second relevant task is to identify thestrength and types of scientific claims.
Light etal.
(2004) developed a classifier to predict thelevel of speculations in sentences in biomedicalabstracts.
Blake (2010) proposed a ?claimframework?
that differentiates explicit claims,observations, correlations, comparisons, and im-plicit claims, based on the certainty of the causalrelationship that was presented.
Blake also foundthat abstracts contained only 7.84% of all scien-tific claims, indicating the need for full-textanalysis.
Currently, our preliminary study exam-ines abstracts only, assuming the most importantfindings are reported there.
We also focus oncoarse-grained classification of positive vs. nega-tive findings at this stage, and leave for futurework the task of differentiating negative claimsin finer-granularity.3 The NLP approach3.1 The definition of negative resultsWhen deciding what kinds of results count as?negative?, some prior studies used ?non-significant?
results as an equivalent for ?negativeresults?
(e.g.
Hebert et al., 2002; Fanelli, 2012).However, in practice, the definition of ?negativeresults?
is actually broader.
For example, theJournal of Negative Results in Biomedicine(JNRBM), launched in 2002, was devoted topublishing ?unexpected, controversial, provoca-tive and/or negative results,?
according to thejournal?s website.
This broader definition has itspros and cons.
The added ambiguity poses chal-lenge for manual and automated identification.At the same time, the broader definition allowsthe inclusion of descriptive studies, such as thefirst JNRBM article (Hebert et al., 2002).Interestingly, Hebert et al.
(2002) defined?negative results?
as ?non-significant outcomes?and drew a negative conclusion that ?prominentmedical journals often provide insufficient in-formation to assess the validity of studies withnegative results?, based on descriptive statistics,not hypothesis testing.
This finding would not becounted as ?negative?
unless the broader defini-tion is adopted.In our study, we utilized the JNRBM articlesas a convenience sample of negative results, andthus inherit its broader definition.3.2 The effectiveness of negation cues aspredictorsThe Bioscope corpus marked a number of nega-tion cues in the abstracts of research articles,such as ?not?, ?no?, ?without?, etc.
It is so far themost comprehensive negation cue collection wecan find for biomedical publications.
However,some challenges arise when applying these nega-tion cues to the task of identifying negative re-sults.First, instead of focusing on negative results,the Bioscope corpus was annotated with cuesexpressing general negation and speculations.Consequently, some negation cues such as ?un-likely?
was annotated as a speculation cue, not anegation cue, although ?unlikely?
was used toreport negative results like?These data indicate that changes in Wnt ex-pression per se are unlikely to be the cause ofthe observed dysregulation of ?-catenin ex-pression in DD?
(PMC1564412).Therefore, the Bioscope negation cues maynot have captured all negation cues for reportingnegative findings.
To test this hypothesis, weused the JNRBM abstracts (N=90) as a conven-ience sample of negative results, and found that81 abstracts (88.9%) contain at least one Bio-scope negation cue.
Note that because theJNRBM abstracts consist of multiple subsections?background?, ?objective?, ?method?, ?result?,and ?conclusion?, we used the ?result?
and ?con-clusions?
subsections only to narrow down thesearch range.20Among the 9 missed abstracts, 5 used cues notcaptured in Bioscope negation cues: ?insuffi-cient?, ?unlikely?, ?setbacks?, ?worsening?, and?underestimates?.
However, the authors?
writingstyle might be affected by the fact that JNRBMis dedicated to negative results.
One hypothesisis that the authors would feel less pressure to usenegative tones, and thus used more variety ofnegation words.
Hence we leave it as an openquestion whether the new-found negation cuesand their synonyms are generalizable to otherbiomedical journal articles.The rest 4 abstracts (PMC 1863432, 1865554,1839113, and 2746800) did not report explicitnegation results, indicating that sometimes ab-stracts alone are not enough to decide whethernegative results were reported, although the per-centage is relatively low (4.4%).
Hence, we de-cided that missing target is not a major concernfor our task, and thus would classify a researchfinding as positive if no negation cues werefound.Second, some positive research results may bemistaken as negative just because they used ne-gation cues.
For example, ?without?
is marked asa negation cue in Bioscope, but it can be used inmany contexts that do not indicate negative re-sults, such as?The effects are consistent with or without thepresence of hypertension and other comorbidi-ties and across a range of drug classes.?
(PMC2659734)To measure the percentage of false alarm, weapplied the aforementioned trivial classifier to acorpus of 600 abstracts in 4 biomedical disci-plines, which were manually annotated by Fan-elli (2012).
This corpus will be referred to as?Corpus-600?
hereafter.
Each abstract is markedas ?positive?, ?negative?, ?partially positive?, or?n/a?, based on hypothesis testing results.
Thelatter two types were excluded in our study.
Thetrivial classifier predicted an abstract as ?posi-tive?
if no negation cues were found.
Table 1reported the prediction results, including the pre-cision and recall in identifying negative results.This result corroborates with our previous find-ing that the inclusiveness of negation cues is notthe major problem since high recalls have beenobserved in both experiments.
However, the lowprecision is the major problem in that the falsenegative predictions are far more than the truenegative predictions.
Hence, weeding out thenegations that did not report negative results be-came the main purpose of this preliminary study.Discipline #abstracts Precision RecallPsychiatry 140 .11 .92ClinicalMedicine127 .16 .94Neuroscience 144 .20 .95Immunology 140 .18 .95Total 551 .16 .94Table 1: results of cue-based trivial classifier3.3 Classification task definitionThis preliminary study focuses on separatingnegations that reported negative results and thosenot.
We limit our study to abstracts at this time.Because a paper may report multiple findings,we performed the prediction at sentence level,and leave for future work the task of aggregatingsentence-level predictions to abstract-level orarticle-level.
By this definition, we will classifyeach sentence as reporting negative finding ornot.
A sentence that includes mixed findings willbe categorized as reporting negative finding.?Not?
and ?no?
are the most frequent negationcues in the Bioscope corpus, accounting for morethan 85% of all occurrences of negation cues.
Inthis study we also examined whether local con-text, such as the words, parts-of-speeches, andconstituents surrounding the negation cues,would be useful for predicting negative findings.Considering that different negation cues may beused in different contexts to report negative find-ings, we built a classifier based on the local con-texts of ?no?
and ?not?.
Contexts for other nega-tion cues will be studied in the future.Therefore, our goal is to extract sentences con-taining ?no?
or ?not?
from abstracts, and predictwhether they report negative findings or not.3.4 Training dataWe obtained a set of ?positive examples?,which are negative-finding sentences, and a setof ?negative examples?
that did not report nega-tive findings.
The examples were obtained in thefollowing way.Positive examples.
These are sentences thatused ?no?
or ?not?
to report negative findings.We extracted all sentences that contain ?no?
or?not?
in JNRBM abstracts, and manually markedeach sentence as reporting negative findings or21not.
Finally we obtained 158 sentences reportingnegative findings.To increase the number of negative-findingexamples and add variety to writing styles, werepeat the above annotations to all Lancet ab-stracts (?result?
and ?finding?
subsections only)in the PubMed Central open access subset, andobtained 55 more such sentences.
Now we haveobtained 213 negative-finding examples in total.Negative examples.
To reduce the workloadfor manual labeling, we utilized the heuristic rulethat a ?no?
or ?not?
does not report negative re-sult if it occurs in a positive abstract, thereforewe extracted such sentences from positive ab-stracts in ?Corpus-600?.
These are the negativeexamples we will use.
To balance the number ofpositive and negative examples, we used a totalof 231 negative examples in two domains (132 inclinical medicine and 99 in neuroscience) insteadof all four domains, because there are not enoughpositive examples.Now the training data is ready for use.3.5 Feature extractionWe compared three text representation methodsby their effectiveness in building the classifier.The approaches are (1) BOW: simple bag-of-words, (2) E-BOW: bag-of-words enriched withPoS tagging and shallow parsing, and (3) LCE-BOW: local contexts of the negation cues ?no?and ?not?, including the words, PoS tags, andconstituents.
For (2) and (3), we ran theOpenNLP chunker through all sentences in thetraining data.
For (3), we extracted the followingfeatures for each sentence:?
The type of chunk (constituent) where?no/not?
is in (e.g.
verb phrase ?VP?);?
The types of two chunks before and after thechunk where ?not?
is in;?
All words or punctuations in these chunks;?
The parts-of-speech of all these words.See Table 2 below for an example of negativefinding: row 1 is the original sentence; row 2 isthe chunked sentence, and row 3 is the extractedlocal context of the negation cue ?not?.
Thesethree representations were then converted to fea-ture vectors using the ?bag-of-words?
representa-tion.
To reduce vocabulary size, we removedwords that occurred only once.
(1)Vascular mortality did not differ signifi-cantly (0.19% vs 0.19% per year, p=0.7).
(2)"[NP Vascular/JJ mortality/NN ] [VPdid/VBD not/RB differ/VB ] [ADVP sig-nificantly/RB ] [PP (/-LRB- ] [NP 019/CD%/NN ] [PP vs/IN ] [NP 019/CD %/NN ][PP per/IN ] [NP year/NN ] ,/, [NPp=07/NNS ] [VP )/-RRB- ] ./.
"(3)?na na VP ADVP PP   did not differ signif-icantly    VBD RB VB RB?Table 2: text representations3.6 Classification resultWe applied two supervised learning algorithms,multinomial na?ve Bayes (MNB), and SupportVector Machines (Liblinear) to the unigram fea-ture vectors.
We used the Sci-kit Learn toolkit tocarry out the experiment, and compared the algo-rithms?
performance using 5-fold cross valida-tion.
All algorithms were set to the default pa-rameter setting.Representation MNB SVMPresencevs.absenceBOW .82 .79E-BOW .82 .79LCE-BOW .72 .72tfBOW .82 .79E-BOW .84 .79LCE-BOW .72 .72TfidfBOW .82 .75E-BOW .84 .73LCE-BOW .72 .75Table 3: classification accuracyTable 3 reports the classification accuracy.
Be-cause the data set contains 213 positive and 231negative examples, the majority vote baseline is.52.
Both algorithms combined with any text rep-resentation methods outperformed the majoritybaseline significantly.
Among them the best clas-sifier is a MNB classifier based on enriched bag-of-words representation and tfidf weighting.
Alt-hough LCE-BOW reached as high as .75 accura-cy using SVM and tfidf weighting, it did not per-form as well as the other text representationmethods, indicating that the local context with+/- 2 window did not capture all relevant indica-tors for negative findings.Tuning the regularization parameter C inSVM did not improve the accuracy.
Adding bi-22grams to the feature set resulted in slightly loweraccuracy.4 ConclusionIn this study we aimed for building a classifier topredict whether a sentence containing the words?no?
or ?not?
reported negative findings.
Builtwith MNB algorithms and enriched bag-of-words features with tfidf weighting, the bestclassifier reached .84 accuracy on a balanceddata set.This preliminary study shows promising re-sults for automatically identifying negative find-ings for the purpose of tracking publication bias.To reach this goal, we will have to aggregate thesentence-level predictions on individual findingsto abstract- or article-level negative results.
Theaggregation strategy is dependent on the decisionof which finding is the primary outcome whenmultiple findings are present.
We leave this asour future work.ReferenceS.
Agarwal, H. Yu, and I. Kohane, I.
2011.
BioNOT:A searchable database of biomedical negated sen-tences.
BMC bioinformatics, 12: 420.C.
Blake.
2010.
Beyond genes, proteins, and ab-stracts: Identifying scientific claims from full-textbiomedical articles.
Journal of biomedical infor-matics, 43(2): 173-189.W.
W. Chapman, W. Bridewell, P. Hanbury, G. F.Cooper, and B. G. Buchanan.
2001.
Evaluation ofnegation phrases in narrative clinical reports.
Pro-ceedings of the AMIA Symposium, 105.P.
J. Easterbrook, R. Gopalan, J.
A. Berlin, and D. R.Matthews.
1991.
Publication bias in clinical re-search.
Lancet, 337(8746): 867-872.M.
Egger, G. D. Smith, M. Schneider, and C. Minder.1997.
Bias in meta-analysis detected by a simple,graphical test.
BMJ 315(7109): 629-634.D.
Fanelli.
2012.
Negative results are disappearingfrom most disciplines and coun-tries.
Scientometrics 90(3): 891-904.R.
Farkas, V. Vincze, G. M?ra, J. Csirik, and G. Szar-vas.
2010.
The CoNLL-2010 shared task: learningto detect hedges and their scope in natural languagetext.
Proceedings of the Fourteenth Conference onComputational Natural Language Learning---Shared Task, 1-12.S.
A. Greenberg.
2009.
How citation distortions createunfounded authority: analysis of a citation net-work.
BMJ 339, b2680.R.
S. Hebert, S. M. Wright, R. S. Dittus, and T. A.Elasy.
2002.
Prominent medical journals often pro-vide insufficient information to assess the validityof studies with negative results.
Journal of Nega-tive Results in Biomedicine 1(1):1.M.
Light, X-Y Qiu, and P. Srinivasan.
2004.
The lan-guage of bioscience: Facts, speculations, andstatements in between.
In Proceedings of BioLink2004 workshop on linking biological literature, on-tologies and databases: tools for users, pp.
17-24.R.
Morante, A. Liekens, and W. Daelemans.
2008.Learning the scope of negation in biomedical texts.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pp.
715-724.R.
Morante, and W. Daelemans.
2009.
A metalearn-ing approach to processing the scope of negation.Proceedings of the Thirteenth Conference on Com-putational Natural Language Learning, 21-29.R.
Rosenthal.
1979.
The file drawer problem and tol-erance for null results.
Psychological Bulle-tin, 86(3): 638.G.
Szarvas, V. Vincze, R. Farkas, and J. Csirik.
2008.The BioScope corpus: annotation for negation, un-certainty and their scope in biomedical texts.In Proceedings of the Workshop on Current Trendsin Biomedical Natural Language Processing, pp.38-45.23
