Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1001?1008,Sydney, July 2006. c?2006 Association for Computational LinguisticsDiscriminative Pruning of Language Models forChinese Word SegmentationJianfeng Li      Haifeng Wang      Dengjun Ren      Guohua LiToshiba (China) Research and Development Center5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng DistrictBeijing, 100738, China{lijianfeng, wanghaifeng, rendengjun,liguohua}@rdc.toshiba.com.cnAbstractThis paper presents a discriminativepruning method of n-gram languagemodel for Chinese word segmentation.To reduce the size of the language modelthat is used in a Chinese word segmenta-tion system, importance of each bigram iscomputed in terms of discriminativepruning criterion that is related to the per-formance loss caused by pruning the bi-gram.
Then we propose a step-by-stepgrowing algorithm to build the languagemodel of desired size.
Experimental re-sults show that the discriminative pruningmethod leads to a much smaller modelcompared with the model pruned usingthe state-of-the-art method.
At the sameChinese word segmentation F-measure,the number of bigrams in the model canbe reduced by up to 90%.
Correlation be-tween language model perplexity andword segmentation performance is alsodiscussed.1 IntroductionChinese word segmentation is the initial stage ofmany Chinese language processing tasks, andhas received a lot of attention in the literature(Sproat et al, 1996; Sun and Tsou, 2001; Zhanget al, 2003; Peng et al, 2004).
In Gao et al(2003), an approach based on source-channelmodel for Chinese word segmentation was pro-posed.
Gao et al (2005) further developed it to alinear mixture model.
In these statistical models,language models are essential for word segmen-tation disambiguation.
However, an uncom-pressed language model is usually too large forpractical use since all realistic applications havememory constraints.
Therefore, language modelpruning techniques are used to produce smallermodels.
Pruning a language model is to eliminatea number of parameters explicitly stored in it,according to some pruning criteria.
The goal ofresearch for language model pruning is to findcriteria or methods, using which the model sizecould be reduced effectively, while the perform-ance loss is kept as small as possible.A few criteria have been presented for lan-guage model pruning, including count cut-off(Jelinek, 1990), weighted difference factor(Seymore and Rosenfeld, 1996), Kullback-Leibler distance (Stolcke, 1998), rank and en-tropy (Gao and Zhang, 2002).
These criteria aregeneral for language model pruning, and are notoptimized according to the performance of lan-guage model in specific tasks.In recent years, discriminative training hasbeen introduced to natural language processingapplications such as parsing (Collins, 2000), ma-chine translation (Och and Ney, 2002) and lan-guage model building (Kuo et al, 2002; Roark etal., 2004).
To the best of our knowledge, it hasnot been applied to language model pruning.In this paper, we propose a discriminativepruning method of n-gram language model forChinese word segmentation.
It differentiatesfrom the previous pruning approaches in tworespects.
First, the pruning criterion is based onperformance variation of word segmentation.Second, the model of desired size is achieved byadding valuable bigrams to a base model, insteadof by pruning bigrams from an unpruned model.We define a misclassification function thatapproximately represents the likelihood that asentence will be incorrectly segmented.
The1001variation value of the misclassification functioncaused by adding a parameter to the base modelis used as the criterion for model pruning.
Wealso suggest a step-by-step growing algorithmthat can generate models of any reasonably de-sired size.
We take the pruning method based onKullback-Leibler distance as the baseline.
Ex-perimental results show that our method outper-forms the baseline significantly with small modelsize.
With the F-Measure of 96.33%, number ofbigrams decreases by up to 90%.
In addition, bycombining the discriminative pruning methodwith the baseline method, we obtain models thatachieve better performance for any model size.Correlation between language model perplexityand system performance is also discussed.The remainder of the paper is organized as fol-lows.
Section 2 briefly discusses the related workon language model pruning.
Section 3 proposesour discriminative pruning method for Chineseword segmentation.
Section 4 describes the ex-perimental settings and results.
Result analysisand discussions are also presented in this section.We draw the conclusions in section 5.2 Related WorkA simple way to reduce the size of an n-gramlanguage model is to exclude those n-grams oc-curring infrequently in training corpus.
It isnamed as count cut-off method (Jelinek, 1990).Because counts are always integers, the size ofthe model can only be reduced to discrete values.Gao and Lee (2000) proposed a distribution-based pruning.
Instead of pruning n-grams thatare infrequent in training data, they prune n-grams that are likely to be infrequent in a newdocument.
Experimental results show that it isbetter than traditional count cut-off method.Seymore and Rosenfeld (1996) proposed amethod to measure the difference of the modelsbefore and after pruning each n-gram, and thedifference is computed as:)]|(log)|([log),( jijiij hwPhwPwhN ????
(1)Where P(wi|hj) denotes the conditional prob-abilities assigned by the original model, andP?
(wi|hj) denotes the probabilities in the prunedmodel.
N(hj, wi) is the discounted frequency of n-gram event hjwi.
Seymore and Rosenfeld (1996)showed that this method is more effective thanthe traditional cut-off method.Stolcke (1998) presented a more sound crite-rion for computing the difference of models be-fore and after pruning each n-gram, which iscalled relative entropy or Kullback-Leibler dis-tance.
It is computed as:?
??
?ji hwjijiji hwPhwPhwP,)]|(log)|()[log,(   (2)The sum is over all words wi and histories hj.This criterion removes some of the approxima-tions employed in Seymore and Rosenfeld(1996).
In addition, Stolcke (1998) presented amethod for efficient computation of the Kull-back-Leibler distance of each n-gram.In Gao and Zhang (2002), three measures arestudied for the purpose of language model prun-ing.
They are probability, rank, and entropy.Among them, probability is very similar to thatproposed by Seymore and Rosenfeld (1996).
Gaoand Zhang (2002) also presented a method ofcombining two criteria, and showed the combi-nation of rank and entropy achieved the smallestmodels.3 Discriminative Pruning for ChineseWord Segmentation3.1 Problem DefinitionIn this paper, discussions are restricted to bigramlanguage model P(wy|wx).
In a bigram model,three kinds of parameters are involved: bigramprobability Pm(wy|wx) for seen bigram wxwy intraining corpus, unigram probability Pm(w) andbackoff coefficient ?m(w) for any word w. Forany wx and wy in the vocabulary, bigram prob-ability P(wy|wx) is computed as:??
?=?>=0),()()(0),()|()|(yxymxmyxxymxy wwcifwPwwwcifwwPwwP ?
(3)As equation (3) shows, the probability of anunseen bigram is computed by the product of theunigram probability and the corresponding back-off coefficient.
If we remove a seen bigram fromthe model, we can still yield a bigram probabilityfor it, by regarding it as an unseen bigram.
Thus,we can reduce the number of bigram probabili-ties explicitly stored in the model.
By doing this,model size decreases.
This is the foundation forbigram model pruning.The research issue is to find an effective crite-rion to compute "importance" of each bigram.Here, "importance" indicates the performanceloss caused by pruning the bigram.
Generally,given a target model size, the method for lan-guage model pruning is described in Figure 1.In fact, deciding which bigrams should be ex-cluded from the model is equivalent to deciding1002which bigrams should be included in the model.Hence, we suggest a growing algorithm throughwhich a model of desired size can also beachieved.
It is illustrated in Figure 2.
Here, twoterms are introduced.
Full-bigram model is theunpruned model containing all seen bigrams intraining corpus.
And base model is currently theunigram model.For the discriminative pruning method sug-gested in this paper, growing algorithm insteadof pruning algorithm is applied to generate themodel of desired size.
In addition, "importance"of each bigram indicates the performance im-provement caused by adding a bigram into thebase model.Figure 1.
Language Model Pruning AlgorithmFigure 2.
Growing Algorithm for LanguageModel Pruning3.2 Discriminative Pruning CriterionGiven a Chinese character string S, a word seg-mentation system chooses a sequence of wordsW* as the segmentation result, satisfying:))|(log)((logmaxarg* WSPWPWW+=  (4)The sum of the two logarithm probabilities inequation (4) is called discriminant function:)|(log)(log),;,( WSPWPWSg +=??
(5)Where ?
denotes a language model that isused to compute P(W), and ?
denotes a genera-tive model that is used to compute P(S|W).
Inlanguage model pruning, ?
is an invariable.The discriminative pruning criterion is in-spired by the comparison of segmented sentencesusing full-bigram model ?F and using base model?B.
Given a sentence S, full-bigram modelchooses  as the segmentation result, and basemodel chooses  as the segmentation result,satisfying:B*FW*BW),;,(maxarg* FWF WSgW ?
?=                        (6)1.
Given the desired model size, computethe number of bigrams that should bepruned.
The number is denoted as m;2.
Compute "importance" of each bigram;3.
Sort all bigrams in the language model,according to their "importance";4.
Remove m most "unimportant" bigramsfrom the model;5.
Re-compute backoff coefficients in themodel.
),;,(maxarg* BWB WSgW ?
?=                       (7)Here, given a language model ?, we define amisclassification function representing the differ-ence between discriminant functions of  and:*FW*BW),;,(),;,(),;( ** ?????=??
FB WSgWSgSd            (8)The misclassification function reflects whichone of  and  is inclined to be chosen asthe segmentation result.
If , we mayextract some hints from the comparison of them,and select a few valuable bigrams.
By addingthese bigrams to base model, we should make themodel choose the correct answer betweenand .
If , no hints can be extracted.
*FW*BW**BF WW ?
*FW*BW**BF WW =1.
Given the desired model size, computethe number of bigrams that should beadded into the base model.
The numberis denoted as n;2.
Compute "importance" of each bigramincluded in the full-bigram model butexcluded from the base model;3.
Sort the bigrams according to their "im-portance";4.
Add n most "important" bigrams intothe base model;5.
Re-compute backoff coefficients in thebase model.Let W0 be the known correct word sequence.Under the precondition , we describeour method in the following three cases.
**BF WW ?Case 1:  and  0* WWF = 0* WWB ?Here, full-bigram model chooses the correctanswer, while base model does not.
Based onequation (6), (7) and (8), we know that d(S;?,?B)> 0 and d(S;?,?F) < 0.
It implies that adding bi-grams into base model may lead the misclassifi-cation function from positive to negative.
Whichbigram should be added depends on the variationof misclassification function caused by adding it.If adding a bigram makes the misclassificationfunction become smaller, it should be added withhigher priority.We add each bigram individually to ?B, andthen compute the variation of the misclassifica-tion function.
Let ??
denotes the model after add-B1003ing bigram wxwy into ?BB.
According to equation(5) and (8), we can write the misclassificationfunction using ?B and ??
separately: B)|(log)(log)|(log)(log),;(****FFBBBBBWSPWPWSPWPSd????+=??(9))|(log)(log)|(log)(log),;(****FFBBWSPWPWSPWPSd?????+?=???
(10)Where PB(.
), P?(.
), PB]]?(.)
represent probabilitiesin base model, model ??
and model ?
separately.The variation of the misclassification function iscomputed as:)](log)([log)](log)([log),;(),;();(****BBBFBFByxWPWPWPWPSdSdwwSd?????=??????=?
(11)Because the only difference between basemodel and model ??
is that model ??
involves thebigram probability P?
(wy|wx), we have:)](log)(log)|()[log,(]|(log)|([log)(log)(log**)1(*)(*)1(*)(**xByBxyyxFiiFiFBiFiFFBFwwPwwPwwWnwwPwwPWPWP????=??=???
??
(12)Where  denotes the number oftimes the bigram w),( * yxF wwWnxwy appears in sequence .Note that in equation (12), base model is treatedas a bigram model instead of a unigram model.The reason lies in two respects.
First, the uni-gram model can be regarded as a particular bi-gram model by setting all backoff coefficients to1.
Second, the base model is not always a uni-gram model during the step-by-step growing al-gorithm, which will be discussed in the next sub-section.
*FWIn fact, bigram probability P?
(wy|wx) is ex-tracted from full-bigram model, so P?
(wy|wx) =PF(wy|wx).
In addition, similar deductions can beconducted to the second bracket in equation (11).Thus, we have:[[ )(log)(log)|(log),(),();( **xByBxyFyxByxFyxwwPwwPwwWnwwWnwwSd?????=?
(13)Note that d(S;?,?)
approximately indicates thelikelihood that S will be incorrectly segmented,so ?d(S;wxwy) represents the performance im-provement caused by adding wxwy.
Thus, "impor-tance" of bigram wxwy on S is computed as:);();( yxyx wwSdSwwimp ?=                     (14)Case 2: and  0* WWF ?
0* WWB =Here, it is just contrary to case 1.
In this way,we have:);();( yxyx wwSdSwwimp ?
?=                   (15)Case 3:  *0*BF WWW ?
?In case 1 and 2, bigrams are added so that dis-criminant function of correct word sequence be-comes bigger, and that of incorrect word se-quence becomes smaller.
In case 3, both  andare incorrect.
Thus, the misclassificationfunction in equation (8) does not represent thelikelihood that S will be incorrectly segmented.Therefore, variation of the misclassificationfunction in equation (13) can not be used tomeasure the "importance" of a bigram.
Here, sen-tence S is ignored, and the "importance" of allbigrams on S are zero.
*FW*BWThe above three cases are designed for onesentence.
The "importance" of each bigram onthe whole training corpus is the sum of its "im-portance" on each single sentence, as equation(16) shows.
?=Syxyx Swwimpwwimp );()(                      (16)To sum up, the "importance" of each bigram iscomputed as Figure 3 shows.1.
For each wxwy, set imp(wxwy) = 0;2.
For each sentence in training corpus:For each wxwy:if W  and W : 0* WF = B ?F ?
B =0* Wimp(wxwy) += ?d(S;wxwy);else if W and W : 0* W 0* Wimp(wxwy) ?= ?d(S;wxwy);Figure 3.
Calculation of "Importance"of BigramsWe illustrate the process of computing "im-portance" of bigrams with a simple example.Suppose S is "?
(zhe4)?
(yang4)?
(cai2)?
(neng2) ?
(geng4) ?
(fang1) ?
(bian4)".
Thesegmented result using full-bigram model is "??(zhe4yang4)/?(cai2)/?(neng2)/?(geng4)/??
(fang1bian4)", which is the correct word se-quence.
The segmented result using base model1004is " ?
?
(zhe4yang4)/ ?
?
(cai2neng2)/ ?
(geng4)/ ?
?
(fang1bian4)".
Obviously, itmatches case 1.
For bigram "??(zhe4yang4)?
(cai2)", it occurs in  once, and does not occurin .
According to equation (13), its "impor-tance" on sentence S is:*FW*BWimp(??(zhe4yang4)?
(cai2);S)= logPF(?(cai2)|??
(zhe4yang4)) ?[logPB(?
(cai2)) + log?B BB(??
(zhe4yang4))]For bigram "?
(geng4)??
(fang1bian4)",since it occurs once both in  and , its"importance" on S is zero.
*FW*BW3.3 Step-by-step GrowingGiven the target model size, we can add exactnumber of bigrams to the base model at one timeby using the growing algorithm illustrated inFigure 2.
But it is more suitable to adopt a step-by-step growing algorithm illustrated in Figure 4.As shown in equation (13), the "importance"of each bigram depends on the base model.
Ini-tially, the base model is set to the unigram model.With bigrams added in, it becomes a growingbigram model.
Thus,  and *BW )(log xB w?
willchange.
So, the added bigrams will affect thecalculation of "importance" of bigrams to beadded.
Generally, adding more bigrams at onetime will lead to more negative impacts.
Thus, itis expected that models produced by step-by-stepgrowing algorithm may achieve better perform-ance than growing algorithm, and smaller stepsize will lead to even better performance.Figure 4.
Step-by-step Growing Algorithm4 Experiments4.1 Experiment SettingsThe training corpus comes from People's daily2000, containing about 25 million Chinese char-acters.
It is manually segmented into word se-quences, according to the word segmentationspecification of Peking University (Yu et al,2003).
The testing text that is provided by PekingUniversity comes from the second internationalChinese word segmentation bakeoff organizedby SIGHAN.
The testing text is a part of Peo-ple's daily 2001, consisting of about 170K Chi-nese characters.The vocabulary is automatically extractedfrom the training corpus, and the words occur-ring only once are removed.
Finally, about 67Kwords are included in the vocabulary.
The full-bigram model and the unigram model are trainedby CMU language model toolkit (Clarkson andRosenfeld, 1997).
Without any count cut-off, thefull-bigram model contains about 2 million bi-grams.The word segmentation system is developedbased on a source-channel model similar to thatdescribed in (Gao et al, 2003).
Viterbi algorithmis applied to find the best word segmentationpath.4.2 Evaluation MetricsThe language models built in our experimentsare evaluated by two metrics.
One is F-Measureof the word segmentation result; the other is lan-guage model perplexity.For F-Measure evaluation, we firstly segmentthe raw testing text using the model to be evalu-ated.
Then, the segmented result is evaluated bycomparing with the gold standard set.
Theevaluation tool is also from the word segmenta-tion bakeoff.
F-Measure is calculated as:1.
Given step size s;2.
Set the base model to be the unigrammodel;3.
Segment corpus with full-bigram model;4.
Segment corpus with base model;5.
Compute "importance" of each bigramincluded in the full-bigram model but ex-cluded from the base model;6.
Sort the bigrams according to their "im-portance";7.
Add s bigrams with the biggest "impor-tance" to the base model;8.
Re-compute backoff coefficients in thebase model;9.
If the base model is still smaller than thedesired size, go to step 4; otherwise, stop.F-MeasureRecallPrecisionRecallPrecision2+?
?=           (17)For perplexity evaluation, the language modelto be evaluated is used to provide the bigramprobabilities for each word in the testing text.The perplexity is the mean logarithm probabilityas shown in equation (18):?= ?
?=Ni iiwwPNMPP 112 )|(log12)(                       (18)4.3 Comparison of Pruning MethodsThe Kullback-Leibler Distance (KLD) basedmethod is the state-of-the-art method, and is1005taken as the baseline1.
Pruning algorithm illus-trated in Figure 1 is used for KLD based pruning.Growing algorithms illustrated in Figure 2 andFigure 4 are used for discriminative pruningmethod.
Growing algorithms are not applied toKLD based pruning, because the computation ofKLD is independent of the base model.At step 1 for KLD based pruning, m is set toproduce ten models containing 10K, 20K, ?,100K bigrams.
We apply each of the models tothe word segmentation system, and evaluate thesegmented results with the evaluation tool.
TheF-Measures of the ten models are illustrated inFigure 5, denoted by "KLD".For the discriminative pruning criterion, thegrowing algorithm illustrated in Figure 2 isfirstly used.
Unigram model acts as the basemodel.
At step 1, n is set to 10K, 20K, ?, 100Kseparately.
At step 2, "importance" of each bi-gram is computed following Figure 3.
Ten mod-els are produced and evaluated.
The F-Measuresare also illustrated in Figure 5, denoted by "Dis-crim".By adding bigrams step by step as illustratedin Figure 4, and setting step size to 10K, 5K, and2K separately, we obtain other three series ofmodels, denoted by "Step-10K", "Step-5K" and"Step-2K" in Figure 5.We also include in Figure 5 the performanceof the count cut-off method.
Obviously, it is infe-rior to other methods.96.096.196.296.396.496.596.61 2 3 4 5 6 7 8 9 10Bigram Num(10K)F-Measure(%)KLD DiscrimStep-10K Step-5KStep-2K Cut-offFigure 5.
Performance Comparison of DifferentPruning MethodsFirst, we compare the performance of "KLD"and "Discrim".
When the model size is small,1 Our pilot study shows that the method based on Kullback-Leibler distance outperforms methods based on other crite-ria introduced in section 2.such as those models containing less than 70Kbigrams, the performance of "Discrim" is betterthan "KLD".
For the models containing morethan 70K bigrams, "KLD" gets better perform-ance than "Discrim".
The reason is that the addedbigrams affect the calculation of "importance" ofbigrams to be added, which has been discussedin section 3.3.If we add the bigrams step by step, better per-formance is achieved.
From Figure 5, it can beseen that all of the models generated by step-by-step growing algorithm outperform "KLD" and"Discrim" consistently.
Compared with the base-line KLD based method, step-by-step growingmethods result in at least 0.2 percent improve-ment for each model size.Comparing "Step-10K", "Step-5K" and "Step-2K", they perform differently before the 60K-bigram point, and perform almost the same afterthat.
The reason is that they are approaching theirsaturation states, which will be discussed in sec-tion 4.5.
Before 60K-bigram point, smaller stepsize yields better performance.An example of detailed comparison result isshown in Table 1, where the F-Measure is96.33%.
The last column shows the relativemodel sizes with respect to the KLD prunedmodel.
It shows that with the F-Measure of96.33%, number of bigrams decreases by up to90%.# of bigrams % of KLDKLD 100,000   100%Step-10K 25,000   25%Step-5K 15,000   15%Step-2K 10,000   10%Table 1.
Comparison of Number of Bigramsat F-Measure 96.33%4.4 Correlation between Perplexity and F-MeasurePerplexities of the models built above are evalu-ated over the gold standard set.
Figure 6 showshow the perplexities vary with the bigram num-bers in models.
Here, we notice that the KLDmodels achieve the lowest perplexities.
It is not asurprising result, because the goal of KLD basedpruning is to minimize the Kullback-Leibler dis-tance that can be interpreted as a relative changeof perplexity (Stolcke, 1998).Now we compare Figure 5 and Figure 6.
Per-plexities of KLD models are much lower thanthat of the other models, but their F-Measures aremuch worse than that of step-by-step growing1006models.
It implies that lower perplexity does notalways lead to higher F-Measure.However, when the comparison is restricted ina single pruning method, the case is different.For each pruning method, as more bigrams areincluded in the model, the perplexity curve falls,and the F-Measure curve rises.
It implies thereare correlations between them.
We compute thePearson product-moment correlation coefficientfor each pruning method, as listed in Table 2.
Itshows that the correlation between perplexityand F-Measure is very strong.To sum up, the correlation between languagemodel perplexity and system performance (hererepresented by F-Measure) depends on whetherthe models come from the same pruning method.If so, the correlation is strong.
Otherwise, thecorrelation is weak.3003504004505005506006507001 2 3 4 5 6 7 8 9 10Bigram Num(10K)PerplexityKLD DiscrimStep-10K Step-5KStep-2K Cut-offFigure 6.
Perplexity Comparison of DifferentPruning MethodsPruning Method CorrelationCut-off -0.990KLD -0.991Discrim -0.979Step-10K -0.985Step-5K -0.974Step-2K -0.995Table 2.
Correlation between Perplexityand F-Measure4.5 Combination of Saturated Model andKLDThe above experimental results show that step-by-step growing models achieve the best per-formance when less than 100K bigrams areadded in.
Unfortunately, they can not grow upinto any desired size.
A bigram has no chance tobe added into the base model, unless it appears inthe mis-aligned part of the segmented corpus,where ?
.
It is likely that not all bigramshave the opportunity.
As more and more bigramsare added into the base model, the segmentedtraining corpus using the current base model ap-proaches to that using the full-bigram model.Gradually, none bigram can be added into thecurrent base model.
At that time, the model stopsgrowing, and reaches its saturation state.
Themodel that reaches its saturation state is namedas saturated model.
In our experiments, threestep-by-step growing models reach their satura-tion states when about 100K bigrams are addedin.
*FW*BWBy combining with the baseline KLD basedmethod, we obtain models that outperform thebaseline for any model size.
We combine themas follows.
If the desired model size is smallerthan that of the saturated model, step-by-stepgrowing is applied.
Otherwise, Kullback-Leiblerdistance is used for further growing over thesaturated model.
For instance, by growing overthe saturated model of "Step-2K", we obtaincombined models containing from 100K to 2million bigrams.
The performance of the com-bined models and that of the baseline KLD mod-els are illustrated in Figure 7.
It shows that thecombined model performs consistently betterthan KLD model over all of bigram numbers.Finally, the two curves converge at the perform-ance of the full-bigram model.96.396.496.596.696.796.896.997.010 30 50 70 90 110130150170190207Bigram Num(10K)F-Measure(%)KLDCombined ModelFigure 7.
Performance Comparison of CombinedModel and KLD Model5 Conclusions and Future WorkA discriminative pruning criterion of n-gram lan-guage model for Chinese word segmentation wasproposed in this paper, and a step-by-step grow-ing algorithm was suggested to generate themodel of desired size based on a full-bigrammodel and a base model.
Experimental results1007showed that the discriminative pruning methodachieves significant improvements over the base-line KLD based method.
At the same F-measure,the number of bigrams can be reduced by up to90%.
By combining the saturated model and thebaseline KLD based method, we achieved betterperformance for any model size.
Analysis showsthat, if the models come from the same pruningmethod, the correlation between perplexity andperformance is strong.
Otherwise, the correlationis weak.The pruning methods discussed in this paperfocus on bigram pruning, keeping unigram prob-abilities unchanged.
The future work will attemptto prune bigrams and unigrams simultaneously,according to a same discriminative pruning crite-rion.
And we will try to improve the efficiency ofthe step-by-step growing algorithm.
In addition,the method described in this paper can be ex-tended to other applications, such as IME andspeech recognition, where language models areapplied in a similar way.ReferencesPhilip Clarkson and Ronald Rosenfeld.
1997.
Statisti-cal Language Modeling Using the CMU-Cambridge Toolkit.
In Proc.
of the 5th EuropeanConference on Speech Communication and Tech-nology (Eurospeech-1997), pages 2707-2710.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
In Machine Learning:Proc.
of 17th International Conference (ICML-2000), pages 175-182.Jianfeng Gao and Kai-Fu Lee.
2000.
Distribution-based pruning of backoff language models.
In Proc.of the 38th Annual Meeting of Association for Com-putational Linguistics (ACL-2000), pages 579-585.Jianfeng Gao, Mu Li, and Chang-Ning Huang.
2003.Improved Source-channel Models for ChineseWord Segmentation.
In Proc.
of the 41st AnnualMeeting of Association for Computational Linguis-tics (ACL-2003), pages 272-279.Jianfeng Gao, Mu Li, Andi Wu, and Chang-NingHuang.
2005.
Chinese Word Segmentation andNamed Entity Recognition: A Pragmatic Approach.Computational Linguistics, 31(4): 531-574.Jianfeng Gao and Min Zhang.
2002.
Improving Lan-guage Model Size Reduction using Better PruningCriteria.
In Proc.
of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL-2002), pages 176-182.Fredrick Jelinek.
1990.
Self-organized language mod-eling for speech recognition.
In Alexander Waibeland Kai-Fu Lee (Eds.
), Readings in Speech Recog-nition, pages 450-506.Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,and Chin-Hui Lee.
2002.
Discriminative Trainingof Language Models for Speech Recognition.
InProc.
of the 27th International Conference OnAcoustics, Speech and Signal Processing (ICASSP-2002), pages 325-328.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native Training and Maximum Entropy Models forStatistical Machine Translation.
In Proc.
of the 40thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-2002), pages 295-302.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese Segmentation and New Word De-tection using Conditional Random Fields.
In Proc.of the 20th International Conference on Computa-tional Linguistics (COLING-2004), pages 562-568.Brian Roark, Murat Saraclar, Michael Collins, andMark Johnson.
2004.
Discriminative LanguageModeling with Conditional Random Fields and thePerceptron Algorithm.
In Proc.
of the 42nd AnnualMeeting of the Association for Computational Lin-guistics (ACL-2004), pages 47-54.Kristie Seymore and Ronald Rosenfeld.
1996.
Scal-able Backoff Language Models.
In Proc.
of the 4thInternational Conference on Spoken LanguageProcessing (ICSLP-1996), pages.
232-235.Richard Sproat, Chilin Shih, William Gale, andNancy Chang.
1996.
A Stochastic Finite-stateWord-segmentation Algorithm for Chinese.
Com-putational Linguistics,  22(3): 377-404.Andreas Stolcke.
1998.
Entropy-based Pruning ofBackoff Language Models.
In Proc.
of DARPANews Transcription and Understanding Workshop,pages 270-274.Maosong Sun and Benjamin K. Tsou.
2001.
A Re-view and Evaluation on Automatic Segmentationof Chinese.
Contemporary Linguistics, 3(1): 22-32.Shiwen Yu, Huiming Duan, Xuefeng Zhu, Bin Swen,and Baobao Chang.
2003.
Specification for CorpusProcessing at Peking University: Word Segmenta-tion, POS Tagging and Phonetic Notation.
Journalof Chinese Language and Computing, 13(2): 121-158.Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, andQun Liu.
2003.
HHMM-based Chinese LexicalAnalyzer ICTCLAS, In Proc.
of the ACL-2003Workshop on Chinese Language Processing(SIGHAN), pages 184-187.1008
