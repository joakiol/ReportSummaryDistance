Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 430?439,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAn Algorithm for Unsupervised Transliteration Mining with an Applicationto Word AlignmentHassan Sajjad Alexander Fraser Helmut SchmidInstitute for Natural Language ProcessingUniversity of Stuttgart{sajjad,fraser,schmid}@ims.uni-stuttgart.deAbstractWe propose a language-independent methodfor the automatic extraction of transliterationpairs from parallel corpora.
In contrast toprevious work, our method uses no form ofsupervision, and does not require linguisti-cally informed preprocessing.
We conductexperiments on data sets from the NEWS2010 shared task on transliteration mining andachieve an F-measure of up to 92%, out-performing most of the semi-supervised sys-tems that were submitted.
We also apply ourmethod to English/Hindi and English/Arabicparallel corpora and compare the results withmanually built gold standards which marktransliterated word pairs.
Finally, we integratethe transliteration module into the GIZA++word aligner and evaluate it on two wordalignment tasks achieving improvements inboth precision and recall measured againstgold standard word alignments.1 IntroductionMost previous methods for building transliterationsystems were supervised, requiring either hand-crafted rules or a clean list of transliteration pairs,both of which are expensive to create.
Such re-sources are also not applicable to other languagepairs.In this paper, we show that it is possible to ex-tract transliteration pairs from a parallel corpus us-ing an unsupervised method.
We first align a bilin-gual corpus at the word level using GIZA++ andcreate a list of word pairs containing a mix of non-transliterations and transliterations.
We train a sta-tistical transliterator on the list of word pairs.
Wethen filter out a few word pairs (those which havethe lowest transliteration probabilities according tothe trained transliteration system) which are likelyto be non-transliterations.
We retrain the translitera-tor on the filtered data set.
This process is iterated,filtering out more and more non-transliteration pairsuntil a nearly clean list of transliteration word pairsis left.
The optimal number of iterations is automat-ically determined by a novel stopping criterion.We compare our unsupervised transliteration min-ing method with the semi-supervised systems pre-sented at the NEWS 2010 shared task on translit-eration mining (Kumaran et al, 2010) using fourlanguage pairs.
We refer to this task as NEWS10.These systems used a manually labelled set of datafor initial supervised training, which means thatthey are semi-supervised systems.
In contrast, oursystem is fully unsupervised.
We achieve an F-measure of up to 92% outperforming most of thesemi-supervised systems.The NEWS10 data sets are extracted WikipediaInterLanguage Links (WIL) which consist of par-allel phrases, whereas a parallel corpus consists ofparallel sentences.
Transliteration mining on theWIL data sets is easier due to a higher percentageof transliterations than in parallel corpora.
We alsodo experiments on parallel corpora for two languagepairs.
To this end, we created gold standards inwhich sampled word pairs are annotated as eithertransliterations or non-transliterations.
These goldstandards have been submitted with the paper as sup-plementary material as they are available to the re-search community.430Finally we integrate a transliteration module intothe GIZA++ word aligner and show that it improvesword alignment quality.
The transliteration mod-ule is trained on the transliteration pairs which ourmining method extracts from the parallel corpora.We evaluate our word alignment system on two lan-guage pairs using gold standard word alignmentsand achieve improvements of 10% and 13.5% in pre-cision and 3.5% and 13.5% in recall.The rest of the paper is organized as follows.
Insection 2, we describe the filtering model and thetransliteration model.
In section 3, we present ouriterative transliteration mining algorithm and an al-gorithm which computes a stopping criterion for themining algorithm.
Section 4 describes the evaluationof our mining method through both gold standardevaluation and through using it to improve wordalignment quality.
In section 5, we present previouswork and we conclude in section 6.2 ModelsOur algorithms use two different models.
The firstmodel is a joint character sequence model whichwe apply to transliteration mining.
We use thegrapheme-to-phoneme converter g2p to implementthis model.
The other model is a standard phrase-based MT model which we apply to transliteration(as opposed to transliteration mining).
We build itusing the Moses toolkit.2.1 Joint Sequence Model Using g2pHere, we briefly describe g2p using notation fromBisani and Ney (2008).
The details of the model,its parameters and the utilized smoothing techniquescan be found in Bisani and Ney (2008).The training data is a list of word pairs (a sourceword and its presumed transliteration) extractedfrom a word-aligned parallel corpus.
g2p builds ajoint sequence model on the character sequences ofthe word pairs and infers m-to-n alignments betweensource and target characters with Expectation Maxi-mization (EM) training.
The m-to-n character align-ment units are referred to as ?multigrams?.The model built on multigrams consisting ofsource and target character sequences greater thanone learns too much noise (non-transliteration infor-mation) from the training data and performs poorly.In our experiments, we use multigrams with a maxi-mum of one character on the source and one charac-ter on the target side (i.e., 0,1-to-0,1 character align-ment units).The N-gram approximation of the joint probabil-ity can be defined in terms of multigrams qi as:p(qk1 ) ?k+1?j=1p(qj |qj?1j?N+1) (1)where q0, qk+1 are set to a special boundary symbol.N-gram models of order > 1 did not work wellbecause these models tended to learn noise (infor-mation from non-transliteration pairs) in the trainingdata.
For our experiments, we only trained g2p withthe unigram model.In test mode, we look for the best sequence ofmultigrams given a fixed source and target string andreturn the probability of this sequence.For the mining process, we trained g2p onlists containing both transliteration pairs and non-transliteration pairs.2.2 Statistical Machine Transliteration SystemWe build a phrase-based MT system for translitera-tion using the Moses toolkit (Koehn et al, 2003).
Wealso tried using g2p for implementing the translit-eration decoder but found Moses to perform bet-ter.
Moses has the advantage of using Minimum Er-ror Rate Training (MERT) which optimizes translit-eration accuracy rather than the likelihood of thetraining data as g2p does.
The training data con-tains more non-transliteration pairs than transliter-ation pairs.
We don?t want to maximize the like-lihood of the non-transliteration pairs.
Instead wewant to optimize the transliteration performance fortest data.
Secondly, it is easy to use a large languagemodel (LM) with Moses.
We build the LM on thetarget word types in the data to be filtered.For training Moses as a transliteration system, wetreat each word pair as if it were a parallel sentence,by putting spaces between the characters of eachword.
The model is built with the default settingsof the Moses toolkit.
The distortion limit ?d?
is setto zero (no reordering).
The LM is implemented asa five-gram model using the SRILM-Toolkit (Stol-cke, 2002), with Add-1 smoothing for unigrams andKneser-Ney smoothing for higher n-grams.4313 Extraction of Transliteration PairsTraining of a supervised transliteration system re-quires a list of transliteration pairs which is expen-sive to create.
Such lists are usually either built man-ually or extracted using a classifier trained on man-ually labelled data and using other language depen-dent information.
In this section, we present an it-erative method for the extraction of transliterationpairs from parallel corpora which is fully unsuper-vised and language pair independent.Initially, we extract a list of word pairs from aword-aligned parallel corpus using GIZA++.
Theextracted word pairs are either transliterations, otherkinds of translations, or misalignments.
In each it-eration, we first train g2p on the list of word pairs.Then we delete those 5% of the (remaining) train-ing data which are least likely to be transliterationsaccording to g2p.1 We determine the best iterationaccording to our stopping criterion and return the fil-tered data set from this iteration.
The stopping crite-rion uses unlabelled held-out data to predict the opti-mal stopping point.
The following sections describethe transliteration mining method in detail.3.1 MethodologyWe will first describe the iterative filtering algorithm(Algorithm 1) and then the algorithm for the stop-ping criterion (Algorithm 2).
In practice, we firstrun Algorithm 2 for 100 iterations to determine thebest number of iterations.
Then, we run Algorithm 1for that many iterations.Initially, the parallel corpus is word-aligned usingGIZA++ (Och and Ney, 2003), and the alignmentsare refined using the grow-diag-final-and heuristic(Koehn et al, 2003).
We extract all word pairs whichoccur as 1-to-1 alignments in the word-aligned cor-pus.
We ignore non-1-to-1 alignments because theyare less likely to be transliterations for most lan-guage pairs.
The extracted set of word pairs will becalled ?list of word pairs?
later on.
We use the listof word pairs as the training data for Algorithm 1.Algorithm 1 builds a joint sequence model usingg2p on the training data and computes the joint prob-ability of all word pairs according to g2p.
We nor-malize the probabilities by taking the nth square root1Since we delete 5% from the filtered data, the number ofdeleted data items decreases in each iteration.Algorithm 1 Mining of transliteration pairs1: training data?list of word pairs2: I?
03: repeat4: Build a joint source channel model on the trainingdata using g2p and compute the joint probabilityof every word pair.5: Remove the 5% word pairs with the lowest length-normalized probability from the training data.
{and repeat the process with the filtered trainingdata}6: I?
I+17: until I = Stopping iteration from Algorithm 2where n is the average length of the source and thetarget string.
The training data contains mostly non-transliteration pairs and a few transliteration pairs.Therefore the training data is initially very noisy andthe joint sequence model is not very accurate.
How-ever it can successfully be used to eliminate a fewword pairs which are very unlikely to be translitera-tions.On the filtered training data, we can train a modelwhich is slightly better than the previous model.
Us-ing this improved model, we can eliminate furthernon-transliterations.Our results show that at the iteration determinedby our stopping criterion, the filtered set mostlycontains transliterations and only a small numberof transliterations have been mistakenly eliminated(see section 4.2).Algorithm 2 automatically determines the beststopping point of the iterative transliteration min-ing process.
It is an extension of Algorithm 1.
Itruns the iterative process of Algorithm 1 on half ofthe list of word pairs (training data) for 100 itera-tions.
For every iteration, it builds a transliterationsystem on the filtered data.
The transliteration sys-tem is tested on the source side of the other half ofthe list of word pairs (held-out).
The output of thetransliteration system is matched against the targetside of the held-out data.
(These target words are ei-ther transliterations, translations or misalignments.
)We match the target side of the held-out data underthe assumption that all matches are transliterations.The iteration where the output of the transliterationsystem best matches the held-out data is chosen asthe stopping iteration of Algorithm 1.432Algorithm 2 Selection of the stopping iteration forthe transliteration mining algorithm1: Create clusters of word pairs from the list of wordpairs which have a common prefix of length 2 bothon the source and target language side.2: Randomly add each cluster either to the training dataor to the held-out data.3: I?
04: while I < 100 do5: Build a joint sequence model on the trainingdata using g2p and compute the length-normalizedjoint probability of every word pair in the trainingdata.6: Remove the 5% word pairs with the lowest prob-ability from the training data.
{The training datawill be reduced by 5% of the rest in each iteration}7: Build a transliteration system on the filtered train-ing data and test it using the source side of theheld-out and match the output against the targetside of the held-out.8: I?
I+19: end while10: Collect statistics of the matching results and take themedian from 9 consecutive iterations (median9).11: Choose the iteration with the best median9 score forthe transliteration mining process.We will now describe Algorithm 2 in detail.
Al-gorithm 2 initially splits the word pairs into trainingand held-out data.
This could be done randomly, butit turns out that this does not work well for sometasks.
The reason is that the parallel corpus con-tains inflectional variants of the same word.
If twovariants are distributed over training and held-outdata, then the one in the training data may cause thetransliteration system to produce a correct transla-tion (but not transliteration) of its variant in the held-out data.
This problem is further discussed in section4.2.2.
Instead of randomly splitting the data, we firstcreate clusters of word pairs which have a commonprefix of length 2 both on the source and target lan-guage side.
We randomly add each cluster either tothe training data or to the held-out data.We repeat the mining process (described in Algo-rithm 1) to eliminate non-transliteration pairs fromthe training data.
For each iteration of Algorithm 2,i.e., steps 4 to 9, we build a transliteration system onthe filtered training data and test it on the source sideof the held-out.
We collect statistics on how well theoutput of the system matches the target side of theheld-out.
The matching scores on the held-out dataoften make large jumps from iteration to iteration.We take the median of the results from 9 consecutiveiterations (the 4 iterations before, the current and the4 iterations after the current iteration) to smooth thescores.
We call this median9.
We choose the iter-ation with the best smoothed score as the stoppingpoint for the filtering process.
In our tests, the me-dian9 heuristic indicated an iteration close to the op-timal iteration.Sometimes several nearby iterations have thesame maximal smoothed score.
In that case, wechoose the one with the highest unsmoothed score.Section 4.2 explains the median9 heuristic in moredetail and presents experimental results showing thatit works well.4 ExperimentsWe evaluate our transliteration mining algorithm onthree tasks: transliteration mining from WikipediaInterLanguage Links, transliteration mining fromparallel corpora, and word alignment using a wordaligner with a transliteration component.
On theWIL data sets, we compare our fully unsupervisedsystem with the semi-supervised systems presentedat the NEWS10 (Kumaran et al, 2010).
In the eval-uation on parallel corpora, we compare our min-ing results with a manually built gold standard inwhich each word pair is either marked as a translit-eration or as a non-transliteration.
In the word align-ment experiment, we integrate a transliteration mod-ule which is trained on the transliterations pairs ex-tracted by our method into a word aligner and showa significant improvement.
The following sectionsdescribe the experiments in detail.4.1 Experiments Using Parallel Phrases ofWikipedia InterLanguage LinksWe conduct transliteration mining experiments onthe English/Arabic, English/Hindi, English/Tamiland English/Russian Wikipedia InterLanguageLinks (WIL) used in the NEWS10.2 All data sets2We do not evaluate on the English/Chinese data becausethe Chinese data requires word segmentation which is beyondthe scope of our work.
Another problem is that our extractionmethod was developed for alphabetic languages and probablyneeds to be adapted before it is applicable to logographic lan-guages such as Chinese.433Our S-Best S-Worst Systems RankEA 87.4 91.5 70.2 16 3ET 90.1 91.4 57.5 14 3EH 92.2 94.4 71.4 14 3Table 1: Summary of results on NEWS10 data sets where?EA?
is English/Arabic, ?ET?
is English/Tamil and ?EH?is English/Hindi.
?Our?
shows the F-measure of our fil-tered data against the gold standard using the suppliedevaluation tool, ?Systems?
is the total number of partic-ipants in the subtask, and ?Rank?
is the rank we wouldhave obtained if our system had participated.contain training data, seed data and reference data.We make no use of the seed data since our system isfully unsupervised.
We calculate the F-measure ofour filtered transliteration pairs against the suppliedgold standard using the supplied evaluation tool.For English/Arabic, English/Hindi and En-glish/Tamil, our system is better than most of thesemi-supervised systems presented at the NEWS2010 shared task for transliteration mining.
Table 1summarizes the F-scores on these data sets.On the English/Russian data set, our systemachieves 76% F-measure which is not good com-pared with the systems that participated in the sharedtask.
The English/Russian corpus contains manycognates which ?
according to the NEWS10 defi-nition ?
are not transliterations of each other.
Oursystem learns the cognates in the training data andextracts them as transliterations (see Table 2).The two best teams on the English/Russian taskpresented various extraction methods (Jiampoja-marn et al, 2010; Darwish, 2010).
Their sys-tems behave differently on English/Russian than onother language pairs.
Their best systems for En-glish/Russian are only trained on the seed data andthe use of unlabelled data does not help the perfor-mance.
Since our system is fully unsupervised, andthe unlabelled data is not useful, we perform badly.4.2 Experiments Using Parallel CorporaThe Wikipedia InterLanguage Links shared taskdata contains a much larger proportion of translitera-tions than a parallel corpus.
In order to examine howwell our method performs on parallel corpora, weapply it to parallel corpora of English/Hindi and En-glish/Arabic, and compare the transliteration miningresults with a gold standard.Table 2: Cognates from English/Russian corpus extractedby our system as transliteration pairs.
None of them arecorrect transliteration pairs according to the gold stan-dard.We use the English/Hindi corpus from the sharedtask on word alignment, organized as part of theACL 2005 Workshop on Building and Using Par-allel Texts (WA05) (Martin et al, 2005).
For En-glish/Arabic, we use a freely available parallel cor-pus from the United Nations (UN) (Eisele and Chen,2010).
We randomly take 200,000 parallel sentencesfrom the UN corpus of the year 2000.
We cre-ate gold standards for both language pairs by ran-domly selecting a few thousand word pairs from thelists of word pairs extracted from the two corpora.We manually tag them as either transliterations ornon-transliterations.
The English/Hindi gold stan-dard contains 180 transliteration pairs and 2084non-transliteration pairs and the English/Arabic goldstandard contains 288 transliteration pairs and 6639non-transliteration pairs.
We have submitted thesegold standards with the paper.
They are available tothe research community.In the following sections, we describe the me-dian9 heuristic and the splitting method of Algo-rithm 2.
The splitting method is used to avoid earlypeaks in the held-out statistics, and the median9heuristic smooths the held-out statistics in order toobtain a single peak.34.2.1 Motivation for Median9 HeuristicAlgorithm 2 collects statistics from the held-out data(step 10) and selects the stopping iteration.
Due tothe noise in the held-out data, the transliteration ac-curacy on the held-out data often jumps from itera-tion to iteration.
The dotted line in figure 1 (right)shows the held-out prediction accuracy for the En-3We do not use the seed data in our system.
However,to check the correctness of the stopping point, we testedthe transliteration system on the seed data (available withNEWS10) for every iteration of Algorithm 2.
We verified thatthe median9 held-out statistics and accuracy on the seed datahave their peaks at the same iteration.434glish/Hindi parallel corpus.
The curve is very noisyand has two peaks.
It is difficult to see the effect ofthe filtering.
We take the median of the results from9 consecutive iterations to smooth the scores.
Thesolid line in figure 1 (right) shows a smoothed curvebuilt using the median9 held-out scores.
A compari-son with the gold standard (section 4.2.3) shows thatthe stopping point (peak) reached using the median9heuristic is better than the stopping point obtainedwith unsmoothed scores.4.2.2 Motivation for Splitting MethodAlgorithm 2 initially splits the list of word pairs intotraining and held-out data.
A random split workedwell for the WIL data, but failed on the parallel cor-pora.
The reason is that parallel corpora contain in-flectional variants of the same word.
If these vari-ants are randomly distributed over training and held-out data, then a non-transliteration word pair such asthe English-Hindi pair ?change ?
badlao?
may endup in the training data and the related pair ?changes?
badlao?
in the held-out data.
The Moses systemused for transliteration will learn to ?transliterate?
(or actually translate) ?change?
to ?badlao?.
Fromother examples, it will learn that a final ?s?
can bedropped.
As a consequence, the Moses transliteratormay produce the non-transliteration ?badlao?
for theEnglish word ?changes?
in the held-out data.
Suchmatching predictions of the transliterator which areactually translations lead to an overestimate of thetransliteration accuracy and may cause Algorithm 2to predict a stopping iteration which is too early.By splitting the list of word pairs in such a waythat inflectional variants of a word are placed eitherin the training data, or in the held-out, but not inboth, this problem can be solved.4The left graph in Figure 1 shows that the median9held-out statistics obtained after a random data splitof a Hindi/English corpus contains two peaks whichoccur too early.
These peaks disappear in the rightgraph of Figure 1 which shows the results obtainedafter a split with the clustering method.The overall trend of the smoothed curve in fig-ure 1 (right) is very clear.
We start by filtering outnon-transliteration pairs from the data, so the results4This solution is appropriate for all of the language pairsused in our experiments, but should be revisited if there is in-flection realized as prefixes, etc.0.10.20.30.40.50.60.70 10 20 30 40 50 60 70 80 90accuracyiterationsheld outmedian900.10.20.30.40.50.60 10 20 30 40 50 60 70 80 90accuracyiterationsheld outmedian 9Figure 1: Statistics of held-out prediction of En-glish/Hindi data using modified Algorithm 2 with randomdivision of the list of word pairs (left) and using Algo-rithm 2 (right).
The dotted line shows unsmoothed held-out scores and solid line shows median9 held-out scoresof the transliteration system go up.
When no morenon-transliteration pairs are left, we start filteringout transliteration pairs and the results of the systemgo down.
We use this stopping criterion for all lan-guage pairs and achieve consistently good results.4.2.3 Results on Parallel CorporaAccording to the gold standard, the English/Hindiand English/Arabic data sets contain 8% and 4%transliteration pairs respectively.
We repeat the samemining procedure ?
run Algorithm 2 up to 100 itera-tions and return the stopping iteration.
Then, we runAlgorithm 1 up to the stopping iteration returned byAlgorithm 2 and obtain the filtered data.TP FN TN FPEH Filtered 170 10 2039 45EA Filtered 197 91 6580 59Table 3: Transliteration mining results using the parallelcorpus of English/Hindi (EH) and English/Arabic (EA)against the gold standardTable 3 shows the mining results on the En-glish/Hindi and English/Arabic corpora.
The goldstandard is a subset of the data sets.
The En-glish/Hindi gold standard contains 180 translitera-tion pairs and 2084 non-transliteration pairs.
TheEnglish/Arabic gold standard contains 288 translit-eration pairs and 6639 non-transliteration pairs.From the English/Hindi data, the mining system hasmined 170 transliteration pairs out of 180 transliter-ation pairs.
The English/Arabic mined data contains197 transliteration pairs out of 288 transliterationpairs.
The mining system has wrongly identified afew non-transliteration pairs as transliterations (see435table 3, last column).
Most of these word pairs areclose transliterations and differ by only one or twocharacters from perfect transliteration pairs.
Theclose transliteration pairs provide many valid multi-grams which may be helpful for the mining system.4.3 Integration into Word Alignment ModelIn the previous section, we presented a method forthe extraction of transliteration pairs from a parallelcorpus.
In this section, we will explain how to builda transliteration module on the extracted transliter-ation pairs and how to integrate it into MGIZA++(Gao and Vogel, 2008) by interpolating it with the t-table probabilities of the IBM models and the HMMmodel.
MGIZA++ is an extension of GIZA++.
Ithas the ability to resume training from any modelrather than starting with Model1.4.3.1 Modified EM Training of the WordAlignment ModelsGIZA++ applies the IBM models (Brown et al,1993) and the HMM model (Vogel et al, 1996)in both directions, i.e., source to target and targetto source.
The alignments are refined using thegrow-diag-final-and heuristic (Koehn et al, 2003).GIZA++ generates a list of translation pairs withalignment probabilities, which is called the t-table.In this section, we propose a method to modify thetranslation probabilities of the t-table by interpolat-ing the translation counts with transliteration counts.The interpolation is done in both directions.
In thefollowing, we will only consider the e-to-f direction.The transliteration module which is used to calcu-late the conditional transliteration probability is de-scribed in Algorithm 3.We build a transliteration system by trainingMoses on the filtered transliteration corpus (usingAlgorithm 1) and apply it to the e side of the listof word pairs.
For every source word, we gener-ate the list of 10-best transliterations nbestTI(e).Then, we extract every f that cooccurs with e in aparallel sentence and add it to nbestTI(e) whichgives us the list of candidate transliteration pairscandidateTI(e).
We use the sum of transliterationprobabilities?f ?
?CandidateTI(e) pmoses(f?, e) as anapproximation for the prior probability pmoses(e) =?f ?
pmoses(f?, e) which is needed to convert thejoint transliteration probability into a conditionalAlgorithm 3 Estimation of transliteration probabili-ties, e-to-f direction1: unfiltered data?list of word pairs2: filtered data ?transliteration pairs extracted usingAlgorithm 13: Train a transliteration system on the filtered data4: for all e do5: nbestTI(e) ?
10 best transliterations for e ac-cording to the transliteration system6: cooc(e) ?
set of all f that cooccur with e in aparallel sentence7: candidateTI(e)?
cooc(e) ?
nbestTI(e)8: end for9: for all f do10: pmoses(f, e)?
joint transliteration probability ofe and f according to the transliterator11: pti(f |e)?pmoses(f,e)Pf?
?CandidateTI(e) pmoses(f?,e)12: end forprobability.
We use the constraint decoding optionof Moses to compute the joint probability of e and f.It computes the probability by dividing the transla-tion score of the best target sentence given a sourcesentence by the normalization factor.We combine the transliteration probabilities withthe translation probabilities of the IBM models andthe HMM model.
The normal translation probabilitypta(f |e) of the word alignment models is computedwith relative frequency estimates.We smooth the alignment frequencies by addingthe transliteration probabilities weighted by the fac-tor ?
and get the following modified translationprobabilitiesp?
(f |e) =fta(f, e) + ?pti(f |e)fta(e) + ?
(2)where fta(f, e) = pta(f |e)f(e).
pta(f |e) is ob-tained from the original t-table of the alignmentmodel.
f(e) is the total corpus frequency of e. ?is the transliteration weight which is optimized forevery language pair (see section 4.3.2).
Apart fromthe definition of the weight ?, our smoothing methodis equivalent to Witten-Bell smoothing.We smooth after every iteration of the IBM mod-els and the HMM model except the last iteration ofeach model.
Algorithm 4 shows the smoothing forIBM Model4.
IBM Model1 and the HMM modelare smoothed in the same way.
We also apply Algo-rithm 3 and Algorithm 4 in the alignment direction436Algorithm 4 Interpolation with the IBM Model4, e-to-f direction1: {We want to run four iterations of Model4}2: f(e)?
total frequency of e in the corpus3: Run MGIZA++ for one iteration of Model44: I ?
15: while I < 4 do6: Look up pta(f |e) in the t-table of Model47: fta(f, e)?
pta(f |e)f(e) for all (f, e)8: p?
(f |e)?
fta(f,e)+?pti(f |e)fta(e)+?
for all (f, e)9: Resume MGIZA++ training for 1 iteration usingthe modified t-table probabilities p?
(f |e)10: I ?
I + 111: end whilef to e. The final alignments are generated using thegrow-diag-final-and heuristic (Koehn et al, 2003).4.3.2 EvaluationThe English/Hindi corpus available from WA05consists of training, development and test data.
Asdevelopment and test data for English/Arabic, weuse manually created gold standard word alignmentsfor 155 sentences extracted from the Hansards cor-pus released by LDC.
We use 50 sentences for de-velopment and 105 sentences for test.Baseline: We align the data sets using GIZA++(Och and Ney, 2003) and refine the alignments us-ing the grow-diag-final-and heuristic (Koehn et al,2003).
We obtain the baseline F-measure by com-paring the alignments of the test corpus with the goldstandard alignments.Experiments We use GIZA++ with 5 iterations ofModel1, 4 iterations of HMM and 4 iterations ofModel4.
We interpolate translation and translitera-tion probabilities at different iterations (and differentcombinations of iterations) of the three models andalways observe an improvement in alignment qual-ity.
For the final experiments, we interpolate at everyiteration of the IBM models and the HMM modelexcept the last iteration of every model where wecould not interpolate for technical reasons.5 Algo-5We had problems in resuming MGIZA++ training whentraining was supposed to continue from a different model, suchas if we stopped after the 5th iteration of Model1 and thentried to resume MGIZA++ from the first iteration of the HMMmodel.
In this case, we ran the 5th iteration of Model1, then thefirst iteration of the HMM and only then stopped for interpola-rithm 4 shows the interpolation of the transliterationprobabilities with IBM Model4.
We used the sameprocedure with IBM Model1 and the HMM model.The parameter ?
is optimized on developmentdata for every language pair.
The word alignmentsystem is not very sensitive to ?.
Any ?
in therange between 50 and 100 works fine for all lan-guage pairs.
The optimization helps to maximize theimprovement in word alignment quality.
For our ex-periments, we use ?
= 80.On test data, we achieve an improvement ofapproximately 10% and 13.5% in precision and3.5% and 13.5% in recall on English/Hindi and En-glish/Arabic word alignment, respectively.
Table 4shows the scores of the baseline and our word align-ment model.Lang Pb Rb Fb Pti Rti FtiEH 49.1 48.5 51.2 59.1 52.1 55.4EA 50.8 49.9 50.4 64.4 63.6 64Table 4: Word alignment results on the test data of En-glish/Hindi (EH) and English/Arabic (EA) where Pb isthe precision of baseline GIZA++ and Pti is the precisionof our word alignment systemWe compared our word alignment results with thesystems presented at WA05.
Three systems, onelimited and two un-limited, participated in the En-glish/Hindi task.
We outperform the limited systemand one un-limited system.5 Previous ResearchPrevious work on transliteration mining uses a man-ually labelled set of training data to extract translit-eration pairs from a parallel corpus or comparablecorpora.
The training data may contain a few hun-dred randomly selected transliteration pairs from atransliteration dictionary (Yoon et al, 2007; Sproatet al, 2006; Lee and Chang, 2003) or just a fewcarefully selected transliteration pairs (Sherif andKondrak, 2007; Klementiev and Roth, 2006).
Ourwork is more challenging as we extract translitera-tion pairs without using transliteration dictionariesor gold standard transliteration pairs.Klementiev and Roth (2006) initialize theirtransliteration model with a list of 20 transliterationtion; so we did not interpolate in just those iterations of trainingwhere we were transitioning from one model to the next.437pairs.
Their model makes use of temporal scoringto rank the candidate transliterations.
A lot of workhas been done on discovering and learning translit-erations from comparable corpora by using temporaland phonetic information (Tao et al, 2006; Klemen-tiev and Roth, 2006; Sproat et al, 2006).
We do nothave access to this information.Sherif and Kondrak (2007) train a probabilistictransducer on 14 manually constructed translitera-tion pairs of English/Arabic.
They iteratively extracttransliteration pairs from the test data and add themto the training data.
Our method is different from themethod of Sherif and Kondrak (2007) as our methodis fully unsupervised, and because in each iteration,they add the most probable transliteration pairs tothe training data, while we filter out the least proba-ble transliteration pairs from the training data.The transliteration mining systems of the fourNEWS10 participants are either based on discrim-inative or on generative methods.
All systems usemanually labelled (seed) data for the initial training.The system based on the edit distance method sub-mitted by Jiampojamarn et al (2010) performs bestfor the English/Russian task.
Jiampojamarn et al(2010) submitted another system based on a stan-dard n-gram kernel which ranked first for the En-glish/Hindi and English/Tamil tasks.6 For the En-glish/Arabic task, the transliteration mining systemof Noeman and Madkour (2010) was best.
Theynormalize the English and Arabic characters in thetraining data which increases the recall.7Our transliteration extraction method differs inthat we extract transliteration pairs from a paral-lel corpus without supervision.
The results of theNEWS10 experiments (Kumaran et al, 2010) showthat no single system performs well on all languagepairs.
Our unsupervised method seems robust as itsperformance is similar to or better than many of thesemi-supervised systems on three language pairs.We are only aware of one previous work whichuses transliteration information for word alignment.6They use the seed data as positive examples.
In order toobtain also negative examples, they generate all possible wordpairs from the source and target words in the seed data and ex-tract the ones which are not transliterations but have a commonsubstring of some minimal length.7They use the phrase table of Moses to build a mapping tablebetween source and target characters.
The mapping table is thenused to construct a finite state transducer.Hermjakob (2009) proposed a linguistically focusedword alignment system which uses many featuresincluding hand-crafted transliteration rules for Ara-bic/English alignment.
His evaluation did not ex-plicitly examine the effect of transliteration (alone)on word alignment.
We show that the integrationof a transliteration system based on unsupervisedtransliteration mining increases the word alignmentquality for the two language pairs we tested.6 ConclusionWe proposed a method to automatically extracttransliteration pairs from parallel corpora withoutsupervision or linguistic knowledge.
We evaluatedit against the semi-supervised systems of NEWS10and achieved high F-measure and performed bet-ter than most of the semi-supervised systems.
Wealso evaluated our method on parallel corpora andachieved high F-measure.
We integrated the translit-eration extraction module into the GIZA++ wordaligner and showed gains in alignment quality.
Wewill release our transliteration mining system andword alignment system in the near future.AcknowledgmentsThe authors wish to thank the anonymous re-viewers for their comments.
We would like tothank Christina Lioma for her valuable feedbackon an earlier draft of this paper.
Hassan Sajjadwas funded by the Higher Education Commission(HEC) of Pakistan.
Alexander Fraser was fundedby Deutsche Forschungsgemeinschaft grant Modelsof Morphosyntax for Statistical Machine Transla-tion.
Helmut Schmid was supported by DeutscheForschungsgemeinschaft grant SFB 732.ReferencesMaximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5).Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: parameter estimation.Computational Linguistics, 19(2):263?311.Kareem Darwish.
2010.
Transliteration mining withphonetic conflation and iterative training.
In Proceed-ings of the 2010 Named Entities Workshop, Uppsala,Sweden.
Association for Computational Linguistics.438Andreas Eisele and Yu Chen.
2010.
MultiUN: A multi-lingual corpus from United Nation documents.
In Pro-ceedings of the Seventh conference on InternationalLanguage Resources and Evaluation (LREC?10), Val-letta, Malta.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engineer-ing, Testing, and Quality Assurance for Natural Lan-guage Processing, Columbus, Ohio, June.
Associationfor Computational Linguistics.Ulf Hermjakob.
2009.
Improved word alignment withstatistics and linguistic heuristics.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 1 - Volume 1, EMNLP?09, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,Aditya Bhargava, Qing Dou, Mi-Young Kim, andGrzegorz Kondrak.
2010.
Transliteration generationand mining with limited training resources.
In Pro-ceedings of the 2010 Named Entities Workshop, Upp-sala, Sweden.
Association for Computational Linguis-tics.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting of theACL, Morristown, NJ, USA.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofthe Human Language Technology and North Ameri-can Association for Computational Linguistics Con-ference, pages 127?133, Edmonton, Canada.A Kumaran, Mitesh M. Khapra, and Haizhou Li.
2010.Whitepaper of news 2010 shared task on translitera-tion mining.
In Proceedings of the 2010 Named En-tities Workshop the 48th Annual Meeting of the ACL,Uppsala, Sweden.Chun-Jen Lee and Jason S. Chang.
2003.
Acqui-sition of English-Chinese transliterated word pairsfrom parallel-aligned texts using a statistical machinetransliteration model.
In Proceedings of the HLT-NAACL 2003Workshop on Building and using paralleltexts, Morristown, NJ, USA.
ACL.Joel Martin, Rada Mihalcea, and Ted Pedersen.
2005.Word alignment for languages with scarce resources.In ParaText ?05: Proceedings of the ACL Workshopon Building and Using Parallel Texts, Morristown, NJ,USA.
Association for Computational Linguistics.Sara Noeman and Amgad Madkour.
2010.
Languageindependent transliteration mining system using finitestate automata framework.
In Proceedings of the 2010Named Entities Workshop, Uppsala, Sweden.
Associ-ation for Computational Linguistics.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Tarek Sherif and Grzegorz Kondrak.
2007.
Boot-strapping a stochastic transducer for Arabic-Englishtransliteration extraction.
In ACL, Prague, Czech Re-public.Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006.Named entity transliteration with comparable corpora.In ACL.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Intl.
Conf.
Spoken Language Pro-cessing, Denver, Colorado.Tao Tao, Su-Yoon Yoon, Andrew Fister, Richard Sproat,and ChengXiang Zhai.
2006.
Unsupervised namedentity transliteration using temporal and phoneticcorrelation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), Sydney.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In 16th International Conference on Computa-tional Linguistics, pages 836?841, Copenhagen, Den-mark.Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.2007.
Multilingual transliteration using feature basedphonetic method.
In Proceedings of the 45th AnnualMeeting of the ACL, Prague, Czech Republic.439
