Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 102?108,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsLexical Selection for Hybrid MT with Sequence LabelingAlex Rudnick and Michael GasserIndiana University, School of Informatics and Computing{alexr,gasser}@indiana.eduAbstractWe present initial work on an inex-pensive approach for building large-vocabulary lexical selection modules forhybrid RBMT systems by framing lexi-cal selection as a sequence labeling prob-lem.
We submit that Maximum EntropyMarkov Models (MEMMs) are a sensibleformalism for this problem, due to theirability to take into account many featuresof the source text, and show how we canbuild a combination MEMM/HMM sys-tem that allows MT system implemen-tors flexibility regarding which words havetheir lexical choices modeled with classi-fiers.
We present initial results showingsuccessful use of this system both in trans-lating English to Spanish and Spanish toGuarani.1 IntroductionLexical ambiguity presents a serious challenge forrule-based machine translation (RBMT) systems,since many words have several possible transla-tions in a given target language, and more thanone of them may be syntactically valid in context.A translation system must choose a translation foreach word or phrase in the input sentence, andsimply taking the most common translation willoften fail, as a word in the source language mayhave translations in the target language with sig-nificantly different meanings.
Even when choos-ing among near-synonyms, we would like to re-spect selectional preferences and common collo-cations to produce natural-sounding output text.Writing lexical selection rules by hand is te-dious and error-prone; even if informants familiarwith both languages are available, they may not beable to enumerate the contexts under which theywould choose one translation alternative over an-other.
Thus we would like to learn from corporawhere possible.Framing the resolution of lexical ambiguitiesin machine translation as an explicit classificationtask has a long history, dating back at least to earlySMT work at IBM (Brown et al 1991).
More re-cently, Carpuat and Wu have shown how to useword-sense disambiguation techniques to improvemodern phrase-based SMT systems (Carpuat andWu, 2007), even though the language model andphrase tables of these systems can mitigate theproblem of lexical ambiguities somewhat.
Treat-ing lexical selection as a word-sense disambigua-tion problem, in which the sense inventory foreach source-language word is its set of possibletranslations, is often called cross-lingual WSD(CL-WSD).
This framing has received enough at-tention to warrant shared tasks at recent SemEvalworkshops; the most recent running of the task isdescribed in (Lefever and Hoste, 2013).Intuitively, machine translation implies an ?all-words?
WSD task: we need to choose a transla-tion for every word or phrase in the source sen-tence, and the sequence of translations shouldmake sense taken together.
Here we begin to ex-plore CL-WSD not just as a classification task, butas one of sequence labeling.
We describe our ap-proach and implementation, and present two ex-periments.
In the first experiment, we apply thesystem to the SemEval 2013 shared task on CL-WSD (Lefever and Hoste, 2013), translating fromEnglish to Spanish, and in the second, we performan all-words labeling task, translating text fromthe Bible from Spanish to Guarani.
This is workin progress and our code is currently ?research-quality?, but we are developing the software inthe open1, with the intention of using it with freeRBMT systems and producing an easily reusablepackage as the system matures.1http://github.com/alexrudnick/clwsd1022 Related WorkTo our knowledge, there has not been work specifi-cally on sequence labeling applied to lexical selec-tion for RBMT systems.
However, there has beenwork recently on using WSD techniques for trans-lation into lower-resourced languages, such as theEnglish-Slovene language pair, as in (Vintar et al2012).The Apertium team has a particular practicalinterest in improving lexical selection in RBMT;they recently have been developing a new sys-tem, described in (Tyers et al 2012), that learnsfinite-state transducers for lexical selection fromthe available parallel corpora.
It is intended to beboth very fast, for use in practical translation sys-tems, and to produce lexical selection rules thatare understandable and modifiable by humans.Outside of the CL-WSD setting, there has beenwork on framing all-words WSD as a sequence la-beling problem.
Particularly, Molina et al(2002)have made use of HMMs for all-words WSD in amonolingual setting.3 Sequence Labeling with HMMsIn building a sequence-based CL-WSD system,we first tried using the familiar HMM formalism.An HMM is a generative model, giving us a for-mula for P (S, T ) = P (T ) ?
P (S|T ).
Here byS we mean a sequence of source-language words,and by T we mean a sequence words or phrases inthe target language.
In practice, the input sequenceS is a given, and we want to find the sequence Tthat maximizes the joint probability, which meanspredicting an appropriate label for each word inthe input sequence.Using the (first-order) Markov assumption, weapproximate P (T ) as P (T ) =?iP (ti |ti?1 ),where i denotes each index in the input sentence.Then we imagine that each source-language wordsi is generated by the corresponding unobservedlabel ti , through the emission probabilities P (s|t).This generative model is admittedly less intu-itive for CL-WSD than for POS-tagging (where itis more traditionally applied), in that it requiresthe target-language words to be generated in thesource order.Training the transition model ?
roughly an n-gram language model ?
for target-language wordsor phrases in the source order is straightforwardwith sentence-aligned bitext.
We use one-to-many alignments in which each source word cor-responds with zero or more target-language words,and we take the sequence of target-language wordsaligned with a given source word to be its label.NULL labels are common; if a source word is notaligned to a target word, it gets a NULL label.Similarly , we can learn the emission probabilities,P (s|t), simply by counting which source wordsare paired with which target words and smoothing.For decoding with this model, we can usethe Viterbi algorithm, especially for a first-orderMarkov model ?
although we must be carefulin the inner loops only to consider the possibletarget-language words and not the entire target-language vocabulary.
The Viterbi algorithm maystill be used with second- or higher-order models,although it slows down considerably.
In the inter-est of speed, in this work we performed decodingfor second-order HMMs with a beam search.4 Sequence Labeling With MEMMs andHMMsContrastingly, an MEMM is a discriminative se-quence model, with which we can calculate theconditional probability P (T |S) using individualdiscriminative classifiers that model P (ti |F ) (forsome features F ).
Like an HMM, an MEMMmodels transitions over labels, although the in-put sequence is considered given.
This freesus to include any features we like from thesource-language sentence.
The ?Markov?
aspectof the MEMM is that, unlike a standard maxi-mum entropy classifier, we can include informa-tion from the previous k labels as features, fora k-th order MEMM.
So at every step in the se-quence labeling, we want a classifier that modelsP (ti |S, ti?1 ...ti?k ), and the probability of a se-quence T is just the product of each of the individ-ual transition probabilities.To avoid the intractable task of building a singleclassifier that might return thousands of differentlabels, we could in principle build a classifier foreach individual word in the source-language vo-cabulary, each of which will produce perhaps tensof possible target-language labels.
However, therewill be tens or hundreds of thousands of words inthe source-language vocabulary, and most word-types will only occur very rarely; it may be pro-hibitively expensive to train and store classifiersfor each of them.We would like a way to focus our effortson some words, but not all, and to back off103to a simpler model when a classifier is notavailable for a given word.
Here, in orderto approximate P (ti |S, ti?1 ...ti?k ), we use anHMM, as described in the previous section, withwhich we can estimate P (si , ti |ti?1 ...ti?k ) asP (ti |ti?1 ...ti?k ) ?
P (si |ti).
This gives us thejoint probability, which we divide by P (si) ?
priorprobabilities of each source-language word mustbe stored ahead of time ?
and thus we can approx-imate the conditional probability that we need tocontinue the sequence labeling.In the implementation, we can specify criteriaunder which a source-language word will have itstranslations explicitly modeled with a maximumentropy classifier.
When training a system, onemight choose, for example, the 100 most com-mon ambiguous words, all words that are observeda certain number of times in the training corpus,or words that are particularly of interest for someother reason.At training time, we find all of the instancesof the words that we want to model with clas-sifiers, along with their contexts, so that we canextract appropriate features for training the clas-sifiers.
Then we train classifiers for those words,and store the classifiers in a database for retrievalat inference time.For inference with this model, we use a beamsearch rather than the Viterbi algorithm, for con-venience and speed while using a second-orderMarkov model.
A sketch of the beam search im-plementation is presented in Figure 1.5 ExperimentsSo far, we have evaluated our sequence-labelingsystem in two different settings, the English-Spanish subset of a recent SemEval shared task(Lefever and Hoste, 2013), and an all-words pre-diction task in which we want to translate, fromSpanish to Guarani, each word in a test set sam-pled from the Bible.5.1 SemEval CL-WSD taskIn the SemEval CL-WSD task, systems must pro-vide translations for twenty ambiguous Englishnouns given a small amount of context, typically asingle sentence.
The test set for this task consistsof fifty short passages for each ambiguous word,for a thousand test instances in total.
Each pas-sage contains one or a few uses of the ambiguousword.
For each test passage, the system must pro-duce a translation of the noun of interest into thetarget language.
These translations may be a sin-gle word or a short phrase in the target language,and they should be lemmatized.
The task allowssystems to produce several output labels, althoughthe scoring metric encourages producing one bestguess, which is matched against several referencetranslations provided by human annotators.
Thedetails of the scoring are provided in the task de-scription paper, and the scores reported were cal-culated with a script provided by the task organiz-ers.As a concrete example, consider the followingsentences from the test set:(1) But a quick look at today?s letters to theeditor in the Times suggest that here atleast is one department of the paper thatcould use a little more fact-checking.
(2) All over the ice were little Cohens, littleLevys, their names sewed in block letterson the backs of their jerseys.A system should produce carta (a message ordocument) for Sentence (1) and letra or cara?cter(a symbol or handwriting) for (2).
During se-quence labeling, our system chooses a translationfor each word in the sentence, but the scoring onlytakes into account the translations for the wordsmarked in italics.For simplicity and comparability with previouswork, we trained our system on the Europarl In-tersection corpus, which was provided for devel-oping CL-WSD systems in the shared task.
TheEuroparl Intersection is a subset of the sentencesfrom Europarl (Koehn, 2005) that are available inEnglish and all five of the target languages for thetask, although for these initial experiments, weonly worked with Spanish.
There were 884603sentences in our training corpus.We preprocess the Europarl training data by to-kenizing with the default NLTK tokenizer (Birdet al 2009), getting part-of-speech tags for theEnglish text with the Stanford Tagger (Toutanovaet al 2003), and lemmatizing both sides withTreeTagger (Schmid, 1995).
We aligned the un-tagged English text with the Spanish text using theBerkeley Aligner (DeNero and Klein, 2007) to getone-to-many alignments from English to Spanish,since the target-language labels in this setting maybe multi-word phrases.
We used nearly the de-fault settings for Berkeley Aligner, except that we104def beam search(sequence, HMM, source word priors, classifiers):??
?Search over possible label sequences, return the best one we find.??
?candidates = [Candidate([], 0)] # empty label sequence with 0 penaltyfor t in range(len(sequence)):sourceword = sequence[t]for candidate in candidates:context = candidate.get context(t) # labels at positions (t?2, t?1)if sourceword in classifiers:features = extract features(sequence, t, context)label distribution = classifiers[sourceword].prob classify(features)else:label distribution = Distribution()for label in get vocabulary(sourceword):label distribution[label] = (HMM.transition(context, label) +HMM.emission(sourceword, label) ?source word priors[sourceword])# extend candidates for next time step to include labels for next wordadd new candidates(candidate, label distribution, new candidates)candidates = filter top k(new candidates, BEAMWIDTH)return get best(candidates)Figure 1: Python-style code sketch for MEMM/HMM beam search.
Here we are using negative log-probabilities, which we interpret as penalties to be minimized.ran 20 iterations each of IBM Model 1 and HMMalignment.We trained classifiers for all of the test words,and also for any words that appear more than 500times in the corpus.
The classifiers used the pre-vious two labels and all of the tagged, lemmatizedwords within three tokens on either side of the tar-get word as features.
Training was done with theMEGA Model optimization package 2 and its cor-responding NLTK interface.At testing time, for each test instance, welabeled the test sentences with four differentsequence labeling methods: first-order HMMs,second-order HMMs, MaxEnt classifiers with nosequence features, and the MEMMs with HMMbackoff.
We then compared the system outputagainst the reference translations for the targetwords using the script provided by the task orga-nizers.5.2 All-words Lexical Selection forSpanish-GuaraniSince we are primarily interested in lexical selec-tion for RBMT systems in lower-resource settings,we also experimented with translating from Span-ish to Guarani, using the Bible as bitext.
In thisexperiment, we labeled all of the text in the testset using each of the different sequence labelingmodels, and we report the classification accuracyover the test set.For example, for the following sentences ?2http://www.umiacs.umd.edu/?hal/megam/from Isaiah and Psalms, respectively ?
the systemshould predict the corresponding Guarani roots foreach Spanish word.
Here we show the inflectedSpanish and Guarani text with English translationfor the sake of readability, although the systemwas given the roots of the Spanish words as pro-duced by the morphological analyzer.
(3) a. Plantare?is vin?as y comere?is su fruto.b.
Pen?oty?
parral ha pe?u hi?a.c.
You will plant vineyards and eat theirfruit.
(4) a. Comieron y se saciaron.b.
Okaru hikua?i hygua?ta?
meve.c.
They ate and were well filled.In this example, the correct translation of comerdepends on transitivity: if transitive, it should bean inflected form of ?u as in (3), if intransitive itshould be karu, as in (4).In preparing the corpus, since different transla-tions of the Bible do not necessarily have directcorrespondences between verse numbers (they arenot unique identifiers across language!
), we se-lected only the chapters that contain the samenumber of verses in our Spanish and Guaranitranslations.
This only leaves 879 chapters out of1189, for a total of 22828 bitext verses of roughlyone sentence each.
We randomly sampled 100verses from the corpus and set these aside as thetest set.105Here we trained the HMM and MEMM as be-fore, but with lemmatized Spanish as the sourcelanguage, and the roots of Guarani words as thetarget.
As Guarani is a much more morphologi-cally rich language than either English or Spanish,this requires the use of a sophisticated morpholog-ical analyzer, described in section 6.
Due to themuch smaller data set, in this setting we storedclassifiers for any Spanish word that occurs morethan 20 times in the training data and backed offto the HMM during decoding otherwise.6 Morphological Analysis for GuaraniWe analyze the Spanish and Guarani Bible us-ing our in-house morphological analyzer, origi-nally developed for Ethiopian Semitic languages(Gasser, 2009).
As in other, more familiar, mod-ern morphological analyzers such as (Beesley andKarttunen, 2003), analysis in our system is mod-eled by cascades of finite-state transducers (FSTs).To solve the problem of long-distance dependen-cies, we extend the basic FST framework using anidea introduced by Amtrup (2003).
Amtrup startswith the well-understood framework of weightedFSTs, familiar from speech recognition.
Forspeech recognition, FST arcs are weighted withprobabilities, and a successful traversal of a paththrough a transducer results in a probability thatis the product of the probabilities on the arcs thatare traversed, as well as an output string as in con-ventional transducers.
Amtrup showed that proba-bilities could be replaced by feature structures andmultiplication by unification.
In an FST weightedwith feature structures, the result of a success-ful traversal is the unification of the feature struc-ture ?weights?
on the traversed arcs, as well asan output string.
Because a feature structure isaccumulated during the process of transduction,the transducer retains a sort of memory of whereit has been, permitting the incorporation of long-distance constraints such as those relating the neg-ative prefix and suffix of Guarani verbs.In our system, the output of the morphologicalanalysis of a word is a root and a feature struc-ture representing the grammatical features of theword.
We implemented separate FSTs for Span-ish verbs, for Guarani nouns, and for the two maincategories of Guarani verbs and adjectives.
SinceSpanish nouns and adjectives have very few forms,we simply list the alternatives in the lexicon forthese categories.
For this paper, we are only con-cerned with the roots of words in our corpora, sowe ignore the grammatical features that are outputwith each word.7 ResultsThe scores for the first experiment are presentedin Figure 2.
Here we use the precision metric cal-culated by the scripts for the SemEval shared task(Lefever and Hoste, 2013), which compare the an-swers produced by the system against several ref-erence answers given by human annotators.
Thereare two ?most frequent sense?
baselines reported.The first one (?with tag?
), is the baseline in whichwe always take the most frequent label for a givensource word, conditioned on its POS tag.
Theother MFS baseline is not conditioned on POS tag;this was the baseline for the SemEval task.
Per-haps unsurprisingly, we see part-of-speech taggingdoing some of the lexical disambiguation work.Neither of the HMM systems beat the most-frequent-sense baselines, but both the non-sequence MaxEnt classifier and the MEMM sys-tem did, which suggests that the window fea-tures are useful in selecting target-language words.Furthermore, the MEMM system outperforms theMaxEnt classifiers.The scores for the second experiment are pre-sented in Figure 3.
Here we did not have human-annotated reference translations for each word, sowe take the labels extracted from the alignments asground truth and can only report per-word classifi-cation accuracy, rather than the more sophisticatedprecision metric used in the shared task.Here we see similar results.
Neither of theHMM systems beat the MFS baseline, and the tri-gram model was noticeably worse.
The trainingset here is probably too sparse to train a good tri-gram model.
The MEMM system, however, didbeat the baseline, posting the highest results: justover two-thirds of the time, we were able to predictthe correct label for each Spanish word, whereasthe most frequent label was correct about 60% ofthe time.8 Conclusions and Future WorkWe have described a work-in-progress lexical se-lection system that takes a sequence labeling ap-proach, and shown some initial successes in us-ing it for cross-language word sense disambigua-tion tasks for English to Spanish and Spanish toGuarani.
We have demonstrated a hybrid se-106system features score (precision)MFS (with tag) 24.97MFS (without tag) 23.23HMM1 current word, previous label 21.17HMM2 current word, previous two labels 21.23MaxEnt three-word window 25.64MEMM three-word window, previous two labels 26.49Figure 2: Results for the first experiment; SemEval 2013 CL-WSD task.system features score (accuracy %)MFS 60.39HMM1 current word, previous label 57.40HMM2 current word, previous two labels 43.04MEMM three-word window, previous two labels 66.82Figure 3: Results for the second experiment; all-words lexical selection on the Guarani Biblequence labeling strategy that combines MEMMsand HMMs, which will allow users to set parame-ters sensibly for their computational resources andavailable training data.In future work, we will continue to refine theapproach, exploring different parameter settings,such as beam widths, numbers of classifiers forthe MEMM component, and the effects of differ-ent features as input to the classifiers.
We are alsointerested in making use of multilingual informa-tion sources, as in the work of Lefever and Hoste(2011).
We may also consider more sophisticatedsequence tagging models, such as CRFs (Laffertyet al 2001), although we may not have enoughtraining data to make use of richer models.Our goal for this work is practical; we are try-ing to produce a hybrid Spanish-Guarani MT sys-tem that can be used in Paraguay.
We have asmall amount of Guarani training data available,and plan to collect more.
At the time of writing,our lexical selection system is a prototype and notyet integrated with our RBMT engine, but this in-tegration is among our near-term goals.A limitation of the current design is that we donot yet have a good way to make use of monolin-gual training data.
In SMT, it is common practiceto train a language model for the target languagefrom a monolingual corpus that is much largerthan the available bitext.
There is a substantialamount of available Guarani text on the Web, butour current model can only be trained on alignedbitext.
Given Guarani text that had been rear-ranged into a Spanish-like word order, we couldbuild a better model for the transition probabilitiesin the HMM component of the system.
It mightbe feasible to use a Guarani-language parser andsome linguistic knowledge for this purpose.
Wewill also investigate ways to translate multiwordexpressions as a unit rather than word-by-word.ReferencesJan Amtrup.
2003.
Morphology in machine translationsystems: Efficient integration of finite state trans-ducers and feature structure descriptions.
MachineTranslation, 18.Kenneth R. Beesley and Lauri Karttunen.
2003.
FiniteState Morphology.
CSLI Publications.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1991.
Word-Sense Disambiguation Using Statistical Methods.
InProceedings of the 29th Annual Meeting of the As-sociation for Computational Linguistics.Marine Carpuat and Dekai Wu.
2007.
How PhraseSense Disambiguation Outperforms Word SenseDisambiguation for Statistical Machine Translation.In 11th Conference on Theoretical and Methodolog-ical Issues in Machine Translation.John DeNero and Dan Klein.
2007.
Tailoring WordAlignments to Syntactic Machine Translation.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics.
Associationfor Computational Linguistics, June.Michael Gasser.
2009.
Semitic morphological analy-sis and generation using finite state transducers with107feature structures.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofThe Tenth Machine Translation Summit.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and LabelingSequence Data.
In ICML.Els Lefever and Ve?ronique Hoste.
2013.
SemEval-2013 Task 10: Cross-Lingual Word Sense Disam-biguation.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013).Els Lefever, Ve?ronique Hoste, and Martine De Cock.2011.
ParaSense or How to Use Parallel Corporafor Word Sense Disambiguation.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies.Antonio Molina, Ferran Pla, and Encarna Segarra.2002.
A Hidden Markov Model Approach to WordSense Disambiguation.
In IBERAMIA.Helmut Schmid.
1995.
Improvements In Part-of-Speech Tagging With an Application To German.
InProceedings of the ACL SIGDAT-Workshop.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In PROCEEDINGS OF HLT-NAACL.F.
M. Tyers, F.
Sa?nchez-Mart?
?nez, and M. L. Forcada.2012.
Flexible finite-state lexical selection for rule-based machine translation.
In Proceedings of the17th Annual Conference of the European Associa-tion of Machine Translation, EAMT12.S?pela Vintar, Darja Fis?er, and Aljos?a Vrs?c?aj.
2012.Were the clocks striking or surprising?
Using WSDto improve MT performance.
In Proceedings ofthe Joint Workshop on Exploiting Synergies be-tween Information Retrieval and Machine Transla-tion (ESIRMT) and Hybrid Approaches to MachineTranslation (HyTra).108
