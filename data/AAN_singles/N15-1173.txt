Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1494?1504,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTranslating Videos to Natural LanguageUsing Deep Recurrent Neural NetworksSubhashini VenugopalanUT AustinAustin, TXvsub@cs.utexas.eduHuijuan XuUMass LowellLowell, MAhxu1@cs.uml.eduJeff DonahueUC Berkeley, ICSIBerkeley, CAjdonahue@eecs.berkeley.eduMarcus RohrbachUC Berkeley, ICSIBerkeley, CArohrbach@eecs.berkeley.eduRaymond MooneyUT AustinAustin, TXmooney@cs.utexas.eduKate SaenkoUMass LowellLowell, MAsaenko@cs.uml.eduAbstractSolving the visual symbol grounding prob-lem has long been a goal of artificial intel-ligence.
The field appears to be advancingcloser to this goal with recent breakthroughsin deep learning for natural language ground-ing in static images.
In this paper, we proposeto translate videos directly to sentences usinga unified deep neural network with both con-volutional and recurrent structure.
Describedvideo datasets are scarce, and most existingmethods have been applied to toy domainswith a small vocabulary of possible words.By transferring knowledge from 1.2M+ im-ages with category labels and 100,000+ im-ages with captions, our method is able tocreate sentence descriptions of open-domainvideos with large vocabularies.
We compareour approach with recent work using languagegeneration metrics, subject, verb, and objectprediction accuracy, and a human evaluation.1 IntroductionFor most people, watching a brief video and describ-ing what happened (in words) is an easy task.
Formachines, extracting the meaning from video pixelsand generating natural-sounding language is a verycomplex problem.
Solutions have been proposed fornarrow domains with a small set of known actionsand objects, e.g., (Barbu et al, 2012; Rohrbach etal., 2013), but generating descriptions for ?in-the-wild?
videos such as the YouTube domain (Figure 1)remains an open challenge.Progress in open-domain video description hasbeen difficult in part due to large vocabularies andInput video:Our output: A cat is playing with a toy.Humans: A Ferret and cat fighting with each other.
/ A cat anda ferret are playing.
/ A kitten is playing with a ferret.
/ A kittenand a ferret are playfully wrestling.Figure 1: Our system takes a short video as input and out-puts a natural language description of the main activity inthe video.very limited training data consisting of videos withassociated descriptive sentences.
Another seriousobstacle has been the lack of rich models that cancapture the joint dependencies of a sequence offrames and a corresponding sequence of words.
Pre-vious work has simplified the problem by detectinga fixed set of semantic roles, such as subject, verb,and object (Guadarrama et al, 2013; Thomason etal., 2014), as an intermediate representation.
Thisfixed representation is problematic for large vocabu-laries and also leads to oversimplified rigid sentencetemplates which are unable to model the complexstructures of natural language.In this paper, we propose to translate from videopixels to natural language with a single deep neu-ral network.
Deep NNs can learn powerful fea-tures (Donahue et al, 2013; Zeiler and Fergus,2014), but require a lot of supervised training data.We address the problem by transferring knowledgefrom auxiliary tasks.
Each frame of the video ismodeled by a convolutional (spatially-invariant) net-work pre-trained on 1.2M+ images with category la-bels (Krizhevsky et al, 2012).
The meaning state1494and sequence of words is modeled by a recurrent(temporally invariant) deep network pre-trained on100K+ Flickr (Hodosh and Hockenmaier, 2014) andCOCO (Lin et al, 2014) images with associated sen-tence captions.
We show that such knowledge trans-fer significantly improves performance on the videotask.Our approach is inspired by recent breakthroughsreported by several research groups in image-to-textgeneration, in particular, the work by Donahue etal.
(2014).
They applied a version of their modelto video-to-text generation, but stopped short ofproposing an end-to-end single network, using anintermediate role representation instead.
Also, theyshowed results only on the narrow domain of cook-ing videos with a small set of pre-defined objectsand actors.
Inspired by their approach, we utilizea Long-Short Term Memory (LSTM) recurrent neu-ral network (Hochreiter and Schmidhuber, 1997) tomodel sequence dynamics, but connect it directly toa deep convolutional neural network to process in-coming video frames, avoiding supervised interme-diate representations altogether.
This model is sim-ilar to their image-to-text model, but we adapt it forvideo sequences.Our proposed approach has several important ad-vantages over existing video description work.
TheLSTM model, which has recently achieved state-of-the-art results on machine translation tasks (Frenchand English (Sutskever et al, 2014)), effectivelymodels the sequence generation task without requir-ing the use of fixed sentence templates as in previouswork (Guadarrama et al, 2013).
Pre-training on im-age and text data naturally exploits related data tosupplement the limited amount of descriptive videocurrently available.
Finally, the deep convnet, thewinner of the ILSVRC2012 (Russakovsky et al,2014) image classification competition, provides astrong visual representation of objects, actions andscenes depicted in the video.Our main contributions are as follows:?
We present the first end-to-end deep model forvideo-to-text generation that simultaneouslylearns a latent ?meaning?
state, and a fluentgrammatical model of the associated language.?
We leverage still image classification and cap-tion data and transfer deep networks learned onsuch data to the video domain.?
We provide a detailed evaluation of our modelon the popular YouTube corpus (Chen andDolan, 2011) and demonstrate a significant im-provement over the state of the art.2 Related WorkMost of the existing research in video descriptionhas focused on narrow domains with limited vocab-ularies of objects and activities (Kojima et al, 2002;Lee et al, 2008; Khan and Gotoh, 2012; Barbu etal., 2012; Ding et al, 2012; Khan and Gotoh, 2012;Das et al, 2013b; Das et al, 2013a; Rohrbach etal., 2013; Yu and Siskind, 2013).
For example,Rohrbach et al (2013), Rohrbach et al (2014) pro-duce descriptions for videos of several people cook-ing in the same kitchen.
These approaches generatesentences by first predicting a semantic role repre-sentation, e.g., modeled with a CRF, of high-levelconcepts such as the actor, action and object.
Thenthey use a template or statistical machine transla-tion to translate the semantic representation to a sen-tence.Most work on ?in-the-wild?
online video has fo-cused on retrieval and predicting event tags ratherthan generating descriptive sentences; examples aretagging YouTube (Aradhye et al, 2009) and retriev-ing online video in the TRECVID competition (Overet al, 2012).
Work on TRECVID has also includedclustering both video and text features for video re-trieval, e.g., (Wei et al, 2010; Huang et al, 2013).The previous work on the YouTube corpus we em-ploy (Motwani and Mooney, 2012; Krishnamoorthyet al, 2013; Guadarrama et al, 2013; Thomason etal., 2014) used a two-step approach, first detecting afixed tuple of role words, such as subject, verb, ob-ject, and scene, and then using a template to generatea grammatical sentence.
They also utilize languagemodels learned from large text corpora to aid visualinterpretation as well as sentence generation.
Wecompare our method to the best-performing methodof Thomason et al (2014).
A recent paper by Xuet al (2015) extracts deep features from video and acontinuous vector from language, and projects bothto a joint semantic space.
They apply their joint em-bedding to SVO prediction and generation, but donot provide quantitative generation results.
Our net-work learns a joint state vector implicitly, and addi-tionally models sequence dynamics of the language.1495Predicting natural language desriptions of stillimages has received considerable attention, withsome of the earliest works by Aker and Gaizauskas(2010), Farhadi et al (2010), Yao et al (2010), andKulkarni et al (2011) amongst others.
Propelled bysuccesses of deep learning, several groups releasedrecord breaking results in just the past year (Don-ahue et al, 2014; Mao et al, 2014; Karpathy et al,2014; Fang et al, 2014; Kiros et al, 2014; Vinyalset al, 2014; Kuznetsova et al, 2014).In this work, we use deep recurrent nets (RNNs),which have recently demonstrated strong results formachine translation tasks using Long Short TermMemory (LSTM) RNNs (Sutskever et al, 2014; Choet al, 2014).
In contrast to traditional statisticalMT (Koehn, 2010), RNNs naturally combine withvector-based representations, such as those for im-ages and video.
Donahue et al (2014) and Vinyalset al (2014) simultaneously proposed a multimodalanalog of this model, with an architecture whichuses a visual convnet to encode a deep state vector,and an LSTM to decode the vector into a sentence.Our approach to video to text generation is in-spired by the work of Donahue et al (2014), whoalso applied a variant of their model to video-to-textgeneration, but stopped short of training an end-to-end model.
Instead they converted the video to anintermediate role representation using a CRF, thendecoded that representation into a sentence.
In con-trast, we bypass detection of high-level roles and usethe output of a deep convolutional network directlyas the state vector that is decoded into a sentence.This avoids the need for labeling semantic roles,which can be difficult to detect in the case of verylarge vocabularies.
It also allows us to first pre-trainthe model on a large image and caption database,and transfer the knowledge to the video domainwhere the corpus size is smaller.
While Donahue etal.
(2014) only showed results on a narrow domainof cooking videos with a small set of pre-definedobjects and actors, we generate sentences for open-domain YouTube videos with a vocabulary of thou-sands of words.3 ApproachFigure 2 depicts our model for sentence generationfrom videos.
Our framework is based on deep imagedescription models in Donahue et al (2014);VinyalsmeanpoolingInput Video    Convolutional Net              Recurrent Net              OutputLSTMLSTMLSTMLSTMLSTMLSTMAboyisplayinggolf<EOS>LSTMLSTMLSTMLSTMLSTMLSTMCNNCNNCNNCNNCNN......Figure 2: The structure of our video description network.We extract fc7features for each frame, mean pool thefeatures across the entire video and input this at everytime step to the LSTM network.
The LSTM outputs oneword at each time step, based on the video features (andthe previous word) until it picks the end-of-sentence tag.et al (2014) and extends them to generate sentencesdescribing events in videos.
These models workby first applying a feature transformation on an im-age to generate a fixed dimensional vector represen-tation.
They then use a sequence model, specifi-cally a Recurrent Neural Network (RNN), to ?de-code?
the vector into a sentence (i.e.
a sequence ofwords).
In this work, we apply the same principle of?translating?
a visual vector into an English sentenceand show that it works well for describing dynamicvideos as well as static images.We identify the most likely description for a givenvideo by training a model to maximize the log like-lihood of the sentence S, given the correspondingvideo V and the model parameters ?,?
?= argmax??
(V,S)log p(S|V ; ?)
(1)Assuming a generative model of S that produceseach word in the sequence in order, the log proba-bility of the sentence is given by the sum of the logprobabilities over the words and can be expressedas:log p(S|V ) =N?t=0log p(Swt|V, Sw1, .
.
.
, Swt?1)where Swirepresents the ithword in the sentenceand N is the total number of words.
Note that wehave dropped ?
for convenience.A sequence model would be apt to modelp(Swt|V, Sw1, .
.
.
, Swt?1), and we choose an RNN.An RNN, parameterized by ?, maps an input xt,and the previously seen words expressed as a hid-den state or memory, ht?1to an output ztand an1496updated state htusing a non-linear function f :ht= f?
(xt, ht?1) (2)where (h0= 0).
In our work we use the highlysuccessful Long Short-Term Memory (LSTM) netas the sequence model, since it has shown supe-rior performance on tasks such as speech recogni-tion (Graves and Jaitly, 2014), machine translation(Sutskever et al, 2014; Cho et al, 2014) and themore related task of generating sentence descrip-tions of images (Donahue et al, 2014; Vinyals et al,2014).
To be specific, we use two layers of LSTMs(one LSTM stacked atop another) as shown in Fig-ure 2.
We present details of the network in Section3.1.
To convert videos to a fixed length representa-tion (input xt), we use a Convolutional Neural Net-work (CNN).
We present details of how we applythe CNN model to videos in Section 3.2.3.1 LSTMs for sequence generationA Recurrent Neural Network (RNN) is a gener-alization of feed forward neural networks to se-quences.
Standard RNNs learn to map a sequenceof inputs (x1, .
.
.
, xt) to a sequence of hidden states(h1, .
.
.
, ht), and from the hidden states to a se-quence of outputs (z1, .
.
.
, zt) based on the follow-ing recurrences:ht= f(Wxhxt+Whhht?1) (3)zt= g(Wzhht) (4)where f and g are element-wise non-linear functionssuch as a sigmoid or hyperbolic tangent, xtis a fixedlength vector representation of the input, ht?
RNis the hidden state with N units, Wijare the weightsconnecting the layers of neurons, and ztthe outputvector.RNNs can learn to map sequences for which thealignment between the inputs and outputs is knownahead of time (Sutskever et al, 2014) however it?sunclear if they can be applied to problems where theinputs (xi) and outputs (zi) are of varying lengths.This problem is solved by learning to map sequencesof inputs to a fixed length vector using one RNN,and then map the vector to an output sequence usinganother RNN.
Another known problem with RNNsis that, it can be difficult to train them to learn long-range dependencies (Hochreiter et al, 2001).
How-ever, LSTMs (Hochreiter and Schmidhuber, 1997),xtht-1xtht-1xtht-1xtht-1ht(=zt)CellOutputGateInputGateForgetGateInputModulationGateLSTM UnitFigure 3: The LSTM unit replicated from (Donahue etal., 2014).
The memory cell is at the core of the LSTMunit and it is modulated by the input, output and forgetgates controlling how much knowledge is transferred ateach time step.which incorporate explicitly controllable memoryunits, are known to be able to learn long-range tem-poral dependencies.
In our work we use the LSTMunit in Figure 3, described in Zaremba and Sutskever(2014), and Donahue et al (2014).At the core of the LSTM model is a memory cell cwhich encodes, at every time step, the knowledge ofthe inputs that have been observed up to that step.The cell is modulated by gates which are all sig-moidal, having range [0, 1], and are applied multi-plicatively.
The gates determine whether the LSTMkeeps the value from the gate (if the layer evaluatesto 1) or discards it (if it evaluates to 0).
The threegates ?
input gate (i) controlling whether the LSTMconsiders its current input (xt), the forget gate (f )allowing the LSTM to forget its previous memory(ct?1), and the output gate (o) deciding how muchof the memory to transfer to the hidden state (ht),all enable the LSTM to learn complex long-term de-pendencies.
The recurrences for the LSTM are thendefined as:it= ?
(Wxixt+Whiht?1) (5)ft= ?
(Wxfxt+Whfht?1) (6)ot= ?
(Wxoxt+Whoht?1) (7)ct= ftct?1+ it?
(Wxcxt+Whcht?1) (8)ht= ot?
(ct) (9)where ?
is the sigmoidal non-linearity, ?
is thehyperbolic tangent non-linearity,  represents the1497product with the gate value, and the weight matri-ces denoted by Wijare the trained parameters.3.2 CNN-LSTMs for video descriptionWe use a two layer LSTM model for generating de-scriptions for videos based on experiments by Don-ahue et al (2014) which suggest two LSTM layersare better than four and a single layer for image totext tasks.
We employ the LSTM to ?decode?
a vi-sual feature vector representing the video to gener-ate textual output.
The first step in this process is togenerate a fixed-length visual input that effectivelysummarizes a short video.
For this we use a CNN,specifically the publicly available Caffe (Jia et al,2014) reference model, a minor variant of AlexNet(Krizhevsky et al, 2012).
The net is pre-trainedon the 1.2M image ILSVRC-2012 object classifica-tion subset of the ImageNet dataset (Russakovsky etal., 2014) and hence provides a robust initializationfor recognizing objects and thereby expedites train-ing.
We sample frames in the video (1 in every 10frames) and extract the output of the fc7layer andperform a mean pooling over the frames to generatea single 4,096 dimension vector for each video.
Theresulting visual feature vector forms the input to thefirst LSTM layer.
We stack another LSTM layer ontop as in Figure 2, and the hidden state of the LSTMin the first layer is the input to the LSTM unit in thesecond layer.
A word from the sentence forms thetarget of the output LSTM unit.
In this work, werepresent words using ?one-hot?
vectors (i.e 1-of-Ncoding, where is N is the vocabulary size).Training and Inference: The two-layer LSTMmodel is trained to predict the next word Swtinthe sentence given the visual features and the pre-vious t ?
1 words, p(Swt|V, Sw1, .
.
.
, Swt?1).
Dur-ing training the visual feature, sentence pair (V, S)is provided to the model, which then optimizes thelog-likelihood (Equation 1) over the entire trainingdataset using stochastic gradient descent.
At eachtime step, the input xtis fed to the LSTM along withthe previous time step?s hidden state ht?1and theLSTM emits the next hidden state vector ht(and aword).
For the first layer of the LSTM xtis the con-catenation of the visual feature vector and the pre-vious encoded word (Swt?1, the ground truth wordduring training and the predicted word during testtime).
For the second layer of the LSTM xtis ztofthe first layer.
Accordingly, inference must also beperformed sequentially in the order h1= fW(x1, 0),h2= fW(x2, h1), until the model emits the end-of-sentence (EOS) token at the final step T .
In ourmodel the output (ht= zt) of the second layer LSTMunit is used to obtain the emitted word.
We applythe Softmax function, to get a probability distribu-tion over the words w in the vocabulary D.p(w|zt) =exp(Wwzt)?w?
?Dexp(Ww?zt)(10)where Wwis a learnt embedding vector for word w.At test time, we choose the word w?
with the maxi-mum probability for each time step t until we obtainthe EOS token.3.3 Transfer Learning from Captioned ImagesSince the training data available for video descrip-tion is quite limited (described in Section 4.1), wealso leverage much larger datasets available for im-age captioning to train our LSTM model and thenfine tune it on the video dataset.
Our LSTM modelfor images is the same as the one described abovefor single video frames (in Section 3.1, and 3.2).
Aswith videos, we extract fc7layer features (4096 di-mensional vector) from the network (Section 3.2) forthe images.
This forms the visual feature that is in-put to the 2-layer LSTM description model.
The vo-cabulary is the combined set of words in the videoand image datasets.
After the model is trained onthe image dataset, we use the weights of the trainedmodel to initialize the LSTM model for the video de-scription task.
Additionally, we reduce the learningrate on our LSTM model to allow it to tune to thevideo dataset.
This speeds up training and allowsexploiting knowledge previously learned for imagedescription.4 Experiments4.1 DatasetsVideo dataset.
We perform all our experimentson the Microsoft Research Video Description Cor-pus (Chen and Dolan, 2011).
This video corpus isa collection of 1970 YouTube snippets.
The dura-tion of each clip is between 10 seconds to 25 sec-onds, typically depicting a single activity or a short1498sequence.
The dataset comes with several humangenerated descriptions in a number of languages;we use the roughly 40 available English descriptionsper video.
This dataset (or portions of it) have beenused in several prior works (Motwani and Mooney,2012; Krishnamoorthy et al, 2013; Guadarrama etal., 2013; Thomason et al, 2014; Xu et al, 2015) onaction recognition and video description tasks.
Forour task we pick 1200 videos to be used as train-ing data, 100 videos for validation and 670 videosfor testing, as used by the prior works on video de-scription (Guadarrama et al, 2013; Thomason et al,2014; Xu et al, 2015).Domain adaptation, image description datasets.Since the number of videos for the description task isquite small when compared to the size of the datasetsused by LSTM models in other tasks such as trans-lation (Sutskever et al, 2014) (12M sentences), weuse data from the Flickr30k and COCO2014 datasetsfor training and learn to adapt to the video datasetby fine-tuning the image description models.
TheFlickr30k (Hodosh and Hockenmaier, 2014) datasethas about 30,000 images, each with 5 or more de-scriptions.
We hold out 1000 images at random forvalidation and use the remaining for training.
In ad-dition to this, we use the recent COCO2014 (Linet al, 2014) image description dataset consisting of82,783 training images and 40,504 validation im-ages, each with 5 or more sentence descriptions.
Weperform ablation experiments by training models oneach dataset individually, and on the combinationand report results on the YouTube video test dataset.4.2 ModelsHVC This is the Highest Vision Confidencemodel described in (Thomason et al, 2014).
Themodel uses strong visual detectors to predict confi-dence over 45 subjects, 218 verbs and 241 objects.FGM (Thomason et al, 2014) also propose a fac-tor graph model (FGM) that combines knowledgemined from text corpora with visual confidencesfrom the HVC model using a factor graph and per-forms probabilistic inference to determine the mostlikely subject, verb, object and scene tuple.
Theythen use a simple template to generate a sentencefrom the tuple.
In this work, we compare the out-put of our model to the subject, verb, object wordspredicted by the HVC and FGM models and the sen-tences generated from the SVO triple.Our LSTM models We present four main mod-els.
LSTM-YT is our base two-layer LSTM modeltrained on the YouTube video dataset.
LSTM-YTflickris the model trained on the Flickr30k (Ho-dosh and Hockenmaier, 2014) dataset, and finetuned on the YouTube dataset as descibed in Section3.3.
LSTM-YTcocois first trained on the COCO2014(Lin et al, 2014) dataset and then fine-tuned on thevideo dataset.
Our final model, LSTM-YTcocoflickris trained on the combined data of both the Flickrand COCO models and is tuned on YouTube.
Tocompare the overlap in content between the im-age dataset and YouTube dataset, we use the modeltrained on just the Flickr images (LSTMflickr) andjust the COCO images (LSTMcoco) and evaluatetheir performance on the test videos.4.3 Evaluation Metrics and ResultsSVO accuracy.
Earlier works (Krishnamoorthy etal., 2013; Guadarrama et al, 2013) that reported re-sults on the YouTube dataset compared their methodbased on how well their model could predict the sub-ject, verb, and object (SVO) depicted in the video.Since these models first predicted the content (SVOtriples) and then generated the sentences, the S,V,Oaccuracy captured the quality of the content gener-ated by the models.
However, in our case the se-quential LSTM directly outputs the sentence, so weextract the S,V,O from the dependency parse of thegenerated sentence.
We present, in Table 1 and Ta-ble 2, the accuracy of S,V,O words comparing theperformance of our model against any valid groundtruth triple and the most frequent triple found in hu-man description for each video.
The latter evalua-tion was also reported by (Xu et al, 2015), so weinclude it here for comparison.Sentence Generation.
To evaluate the generatedsentences we use the BLEU (Papineni et al, 2002)and METEOR (Banerjee and Lavie, 2005) scoresagainst all ground truth sentences.
BLEU is themetric that is seen more commonly in image de-scription literature, but a more recent study (Elliottand Keller, 2014) has shown METEOR to be a bet-ter evaluation metric.
However, since both metricshave been shown to correlate well with human eval-1499Model S% V% O%HVC (Thomason et al, 2014) 86.87 38.66 22.09FGM (Thomason et al, 2014) 88.27 37.16 24.63LSTMflickr79.95 15.47 13.94LSTMcoco56.30 06.90 14.86LSTM-YT 79.40 35.52 20.59LSTM-YTflickr84.92 38.66 21.64LSTM-YTcoco86.58 42.23 26.69LSTM-YTcoco+flickr87.27 42.79 24.23Table 1: SVO accuracy: Binary SVO accuracy comparedagainst any valid S,V,O triples in the ground truth descrip-tions.
We extract S,V,O values from sentences output byour model using a dependency parser.
The model is cor-rect if it identifies S,V, or O mentioned in any one of themultiple human descriptions.Model S% V% O%HVC (Thomason et al, 2014) 76.57 22.24 11.94FGM (Thomason et al, 2014) 76.42 21.34 12.39JointEmbed1(Xu et al, 2015) 78.25 24.45 11.95LSTMflickr70.80 10.02 07.84LSTMcoco47.44 02.85 07.05LSTM-YT 71.19 19.40 09.70LSTM-YTflickr75.37 21.94 10.74LSTM-YTcoco76.01 23.38 14.03LSTM-YTcoco+flickr75.61 25.31 12.42Table 2: SVO accuracy: Binary SVO accuracy comparedagainst most frequent S,V,O triple in the ground truth de-scriptions.
We extract S,V,O values from parses of sen-tences output by our model using a dependency parser.The model is correct only if it outputs the most frequentlymentioned S, V, O among the human descriptions.uations, we compare the generated sentences usingboth and present our results in Table 3.Human Evaluation.
We used Amazon Mechan-ical Turk to also collect human judgements.
Wecreated a task which employed three Turk workersto watch each video, and rank sentences generatedby the different models from ?Most Relevant?
(5)to ?Least Relevant?
(1).
No two sentences couldhave the same rank unless they were identical.
Wealso evaluate sentences on grammatical correctness.We created a different task which required work-ers to rate sentences based on grammar.
This task1They evaluate against a filtered set of groundtruth SVOwords which provides a tiny boost to their scores.Model BLEU METEORFGM (Thomason et al, 2014) 13.68 23.90LSTM-YT 31.19 26.87LSTM-YTflickr32.03 27.87LSTM-YTcoco33.29 29.07LSTM-YTcoco+flickr33.29 28.88Table 3: Scores for BLEU at 4 (combined n-gram 1-4),and METEOR scores from automated evaluation metricscomparing the quality of the generation.
All values arereported as percentage (%).Model Relevance GrammarFGM (Thomason et al, 2014) 2.26 3.99LSTM-YT 2.74 3.84LSTM-YTcoco2.93 3.46LSTM-YTcoco+flickr2.83 3.64GroundTruth 4.65 4.61Table 4: Human evaluation mean scores.
Sentences wereuniquely ranked between 1 to 5 based on their relevanceto a given video.
Sentences were rated between 1 to 5 forgrammatical correctness.
Higher values are better.displayed only the sentences and did not show anyvideo.
Here, workers had to choose a rating be-tween 1-5 for each sentence.
Multiple sentencescould have the same rating.
We discard responsesfrom workers who fail gold-standard items and re-port the mean ranking/rating for each of the evalu-ated models in Table 4.Individual Frames.
In order to evaluate the ef-fectiveness of mean pooling, we performed exper-iments to train and test the model on individualframes from the video.
Our first set of experimentsinvolved testing how well the image descriptionmodels performed on a randomly sampled frame inthe video.
Similar to Tables 1 and 2, the modeltrained on Flickr30k when tested on random framesfrom the video scored better on subjects and verbswith any valid accuracy of 75.16% and 11.65% re-spectively; and 9.01% on objects.
The one trainedon COCO did better on objects (12.54%, any validaccuracy) but very poorly on subjects and verbs.In our next experiment, we used image descriptionmodels (trained on Flickr30k, COCO or a combi-nation of both) and fine-tuned them on individualframes in the video by picking a different frame1500Model (individual frames) BLEU METEORLSTMflickr08.62 18.56LSTMcoco11.39 20.03LSTM-YT-frameflickr26.75 26.51LSTM-YT-framecoco30.77 27.66LSTM-YT-framecoco+flickr29.72 27.65Table 5: Scores for BLEU at 4 (combined n-gram 1-4),and METEOR scores comparing the quality of sentencegeneration by the models trained on Flickr30k and COCOand tested on a random frame from the video.
LSTM-YT-frame models were fine tuned on individual framesfrom the Youtube video dataset.
All values are reportedas percentage (%).for each description in the YouTube dataset.
Thesemodels were tested on a random frame from the testvideo.
The overall trends in the results were similarto those seen in Tables 1 and 2.
The model trainedon COCO and fine-tuned on individual video framesperformed best with any valid S,V,O accuracies84.8%, 38.98%, and 22.34% respectively.
The onetrained on both COCO and Flickr30k had any validS,V,O accuracies of 85.67%, 38.83%, and 19.72%.We report the generation results for these models inTable 5.5 DiscussionImage only models.
The models trained purelyon the image description data LSTMflickrandLSTMcocoachieve lower accuracy on the verbs andobjects (Tables 1, 2) since the YouTube videos en-compass a wider domain and a variety of actions notdetectable from static images.Base LSTM model.
We note that in the SVObinary accuracy metrics (Tables 1 and 2), the baseLSTM model (LSTM-YT) achieves a slightly loweraccuracy compared to prior work.
This is likely dueto the fact that previous work explicitly optimizes toidentify the best subject, verb and object for a video;whereas the LSTM model is trained on objects andactions jointly in a sentence and needs to learn to in-terpret these in different contexts.
However, with re-gard to the generation metrics BLEU and METEOR,training based on the full sentence helps the LSTMmodel develop fluency and vocabulary similar to thatseen in the training descriptions and allows it to out-perform the template based generation.Transferring helps.
From our experiments, it isclear that learning from the image description dataimproves the performance of the model in all criteriaof evaluation.
We present a few examples demon-strating this in Figure 4.
The model that was pre-trained on COCO2014 shows a larger performanceimprovement, indicating that our model can effec-tively leverage a large auxiliary source of trainingdata to improve its object and verb predictions.
Themodel pre-trained on the combined data of Flickr30kand COCO2014 shows only a marginal improve-ment, perhaps due to overfitting.
Adding dropoutas in (Vinyals et al, 2014) is likely to help preventoverfitting and improve performance.From the automated evaluation in Table 3 it isclear that the fully deep video-to-text generationmodels outperform previous work.
As mentionedpreviously, training on the full sentences is probablythe main reason for the improvements.Testing on individual frames.
The experimentsthat evaluated models on individual frames (Section4.3) from the video have trends similar to those seenon mean pooled frame features.
Specifically, themodel trained on Flickr30k, when directly evaluatedon YouTube video frames performs better on sub-jects and verbs, whereas the one trained on COCOdoes better on objects.
This is explained by the factthat Flickr30k images are more varied but COCOhas more examples of a smaller collection of objects,thus increasing object accuracy.
Amongst the mod-els trained on images and individual video frames,the ones trained on COCO (and the combination ofboth) perform well, but are still a bit poorer com-pared to the models trained on mean-pooled fea-tures.
One point to note however is that, these mod-els were trained and evaluated on random framesfrom the video, and not necessarily a key-frame ormost-representative frame.
It?s likely that choosinga representative frame from the video might result ina small improvement.
But, on the whole, our exper-iments show that models trained on images alone donot directly perform well on video frames, and a bet-ter representation is required to learn from videos.Mean pooling is significant.
Our additionalexperiments that trained and tested on individualframes in the video, reported in section 4.3, suggestthat mean pooling frame features gives significantlybetter results.
This could potentially indicate thatmean pooling features across all frames in the video1501is a reasonable representation for short video clipsat least for the task of generating simple sententialdescriptions.Human evaluation.
We note that the sentencesgenerated by our model have been ranked more rel-evant (Table 4) to the content in the video than pre-vious models.
However, there is still a significantgap between the human ground truth sentence andthe ones generated by the LSTM models.
Addi-tionally, when we ask Turkers to rate only the sen-tences (they are not provided the video) on grammat-ical correctness, the template based FGM (Thoma-son et al, 2014) achieves the highest ratings.
Thiscan be explained by the fact that their work uses atemplate technique to generate sentences from con-tent, and is hence grammatically well formed.
Ourmodel sometimes predicts prepositions and articlesmore frequently, resulting in duplicates and henceincorrect grammar.6 ConclusionIn this paper we have proposed a model for videodescription which uses neural networks for the en-tire pipeline from pixels to sentences and can poten-tially allow for the training and tuning of the entirenetwork.
In an extensive experimental evaluation,we showed that our approach generates better sen-tences than related approaches.
We also showed thatexploiting image description data improves perfor-mance compared to relying only on video descrip-tion data.
However our approach falls short in betterutilizing the temporal information in videos, whichis a good direction for future work.
We will re-lease our Caffe-based implementation, as well as themodel and generated sentences.AcknowledgmentsThe authors thank Trevor Darrell for his valuable ad-vice.
We would also like to thank reviewers for theircomments and suggestions.
Marcus Rohrbach wassupported by a fellowship within the FITweltweit-Program of the German Academic Exchange Ser-vice (DAAD).
This research was partially supportedby ONR ATL Grant N00014-11-1-010, NSF AwardsIIS-1451244 and IIS-1212798.Figure 4: Examples to demonstrate effectiveness of trans-ferring from the image description domain.
YT refer tothe LSTM-YT, YTcoco to the LSTM-YTcoco, and YTco-coflickr to the LSTM-YTcoco+flickrmodels.
GT is a ran-dom human description in the ground truth.
Sentences inbold highlight the most accurate description for the videoamongst the models.
Bottom two examples show howtransfer can overfit.
Thus, while base LSTM-YT modeldetects water and monkey, the LSTM-YTcocoand LSTM-YTcocoflickrmodels fail to describe the event completely.1502ReferencesAhmet Aker and Robert Gaizauskas.
2010.
Generatingimage descriptions using dependency relational pat-terns.
In Association for Computational Linguistics(ACL).H.
Aradhye, G. Toderici, and J. Yagnik.
2009.Video2text: Learning to annotate video content.
InIEEE International Conference on Data Mining Work-shops (ICDMW).Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization.Andrei Barbu, Alexander Bridge, Zachary Burchill,Dan Coroian, Sven Dickinson, Sanja Fidler, AaronMichaux, Sam Mussman, Siddharth Narayanaswamy,Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jef-frey Mark Siskind, Jarrell Waggoner, Song Wang, Jin-lian Wei, Yifan Yin, and Zhiqi Zhang.
2012.
Video insentences out.
In Association for Uncertainty in Arti-ficial Intelligence (UAI).David L. Chen and William B. Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.
InAssociation for Computational Linguistics (ACL).Kyunghyun Cho, Bart van Merri?enboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the proper-ties of neural machine translation: Encoder-decoderapproaches.
arXiv preprint arXiv:1409.1259.P.
Das, R. K. Srihari, and J. J. Corso.
2013a.
Translatingrelated words to videos and back through latent topics.In Proceedings of Sixth ACM International Conferenceon Web Search and Data Mining (WSDM).P.
Das, C. Xu, R. F. Doell, and J. J. Corso.
2013b.
Athousand frames in just a few words: Lingual descrip-tion of videos through latent topics and sparse objectstitching.
In Conference on Computer Vision and Pat-tern Recognition (CVPR).D.
Ding, F. Metze, S. Rawat, P.F.
Schulam, S. Burger,E.
Younessian, L. Bao, M.G.
Christel, and A. Haupt-mann.
2012.
Beyond audio and video retrieval: to-wards multimedia summarization.
In Proceedings ofthe 2nd ACM International Conference on MultimediaRetrieval (ICMR).
ACM.Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-man, Ning Zhang, Eric Tzeng, and Trevor Darrell.2013.
Decaf: A deep convolutional activation fea-ture for generic visual recognition.
arXiv preprintarXiv:1310.1531.Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2014.
Long-term re-current convolutional networks for visual recognitionand description.
CoRR, abs/1411.4389.Desmond Elliott and Frank Keller.
2014.
Comparingautomatic evaluation measures for image description.In Association for Computational Linguistics (ACL).Hao Fang, Saurabh Gupta, Forrest N. Iandola, Ru-pesh Srivastava, Li Deng, Piotr Doll?ar, JianfengGao, Xiaodong He, Margaret Mitchell, John C. Platt,C.
Lawrence Zitnick, and Geoffrey Zweig.
2014.From captions to visual concepts and back.
CoRR,abs/1411.4952.A.
Farhadi, M. Hejrati, M. Sadeghi, P. Young,C.
Rashtchian, J. Hockenmaier, and D. Forsyth.
2010.Every picture tells a story: Generating sentences fromimages.
European Conference on Computer Vision(ECCV).Alex Graves and Navdeep Jaitly.
2014.
Towards end-to-end speech recognition with recurrent neural networks.In Proceedings of the 31st International Conference onMachine Learning (ICML-14).Sergio Guadarrama, Niveda Krishnamoorthy, GirishMalkarnenkar, Subhashini Venugopalan, RaymondMooney, Trevor Darrell, and Kate Saenko.
2013.Youtube2text: Recognizing and describing arbitraryactivities using semantic hierarchies and zero-shotrecognition.
In IEEE International Conference onComputer Vision (ICCV), December.Sepp Hochreiter and J?urgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8).Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, andJ?urgen Schmidhuber.
2001.
Gradient flow in recur-rent nets: the difficulty of learning long-term depen-dencies.Peter Young Alice Lai Micah Hodosh and Julia Hocken-maier.
2014.
From image descriptions to visual deno-tations: New similarity metrics for semantic inferenceover event descriptions.
Transactions of the Associa-tion for Computational Linguistics (TACL).Haiqi Huang, Yueming Lu, Fangwei Zhang, and SonglinSun.
2013.
A multi-modal clustering method forweb videos.
In Trustworthy Computing and Services.Springer.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Convo-lutional architecture for fast feature embedding.
arXivpreprint arXiv:1408.5093.Andrej Karpathy, Armand Joulin, and Li Fei-Fei.
2014.Deep fragment embeddings for bidirectional imagesentence mapping.
Advances in Neural InformationProcessing Systems (NIPS).Muhammad Usman Ghani Khan and Yoshihiko Gotoh.2012.
Describing video contents in natural language.1503Proceedings of the Workshop on Innovative HybridApproaches to the Processing of Textual Data.Ryan Kiros, Ruslan Salakhuditnov, and Richard.
SZemel.
2014.
Unifying visual-semantic embed-dings with multimodal neural language models.
arXivpreprint arXiv:1411.2539.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.A.
Kojima, T. Tamura, and K. Fukunaga.
2002.
Naturallanguage description of human activities from videoimages based on concept hierarchy of actions.
Inter-national Journal of Computer Vision (IJCV), 50(2).Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-mond J. Mooney, Kate Saenko, and Sergio Guadar-rama.
2013.
Generating natural-language video de-scriptions using text-mined knowledge.
In AAAI Con-ference on Artificial Intelligence (AAAI).Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.2012.
ImageNet classification with deep convolutionalneural networks.
In Advances in Neural InformationProcessing Systems (NIPS).Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C Berg, and Tamara L Berg.2011.
Baby talk: Understanding and generating sim-ple image descriptions.
In Conference on ComputerVision and Pattern Recognition (CVPR).Polina Kuznetsova, Vicente Ordonez, Tamara L Berg,UNC Chapel Hill, and Yejin Choi.
2014.
Treetalk:Composition and compression of trees for image de-scriptions.
Transactions of the Association for Com-putational Linguistics, 2(10).M.W.
Lee, A. Hakeem, N. Haering, and S.C. Zhu.
2008.Save: A framework for semantic annotation of visualevents.
In Conference on Computer Vision and PatternRecognition (CVPR).Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Doll?ar,and C Lawrence Zitnick.
2014.
MicrosoftCOCO: Common objects in context.
arXiv preprintarXiv:1405.0312.Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan LYuille.
2014.
Explain images with multimodal recur-rent neural networks.
arXiv preprint arXiv:1410.1090.Tanvi S. Motwani and Raymond J. Mooney.
2012.
Im-proving video activity recognition using object recog-nition and text mining.
In Proceedings of the 20th Eu-ropean Conference on Artificial Intelligence (ECAI).Paul Over, George Awad, Martial Michel, Jonathan Fis-cus, Greg Sanders, B Shaw, Alan F. Smeaton, andGeorges Qu?eenot.
2012.
TRECVID 2012 ?
anoverview of the goals, tasks, data, evaluation mecha-nisms and metrics.
In Proceedings of TRECVID 2012.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Association for Com-putational Linguistics (ACL).Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater,Manfred Pinkal, and Bernt Schiele.
2013.
Trans-lating video content to natural language descriptions.In IEEE International Conference on Computer Vision(ICCV).Anna Rohrbach, Marcus Rohrbach, Wei Qiu, AnnemarieFriedrich, Manfred Pinkal, and Bernt Schiele.
2014.Coherent multi-sentence video description with vari-able level of detail.
In German Conference on PatternRecognition (GCPR), September.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexan-der C. Berg, and Li Fei-Fei.
2014.
ImageNet LargeScale Visual Recognition Challenge.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.
Se-quence to sequence learning with neural networks.
InAdvances in Neural Information Processing Systems(NIPS).J.
Thomason, S. Venugopalan, S. Guadarrama,K.
Saenko, and R.J. Mooney.
2014.
Integratinglanguage and vision to generate natural languagedescriptions of videos in the wild.
In Proceedings ofthe 25th International Conference on ComputationalLinguistics (COLING), August.Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-mitru Erhan.
2014.
Show and tell: A neural imagecaption generator.
CoRR, abs/1411.4555.Shikui Wei, Yao Zhao, Zhenfeng Zhu, and Nan Liu.2010.
Multimodal fusion for video search reranking.IEEE Transactions on Knowledge and Data Engineer-ing,, 22(8).R.
Xu, C. Xiong, W. Chen, and J. J. Corso.
2015.
Jointlymodeling deep video and compositional text to bridgevision and language in a unified framework.
In AAAIConference on Artificial Intelligence (AAAI).B.Z.
Yao, X. Yang, L. Lin, M.W.
Lee, and S.C. Zhu.2010.
I2t: Image parsing to text description.
Proceed-ings of the IEEE, 98(8).Haonan Yu and Jeffrey Mark Siskind.
2013.
Groundedlanguage learning from videos described with sen-tences.
In Association for Computational Linguistics(ACL).Wojciech Zaremba and Ilya Sutskever.
2014.
Learningto execute.
arXiv preprint arXiv:1410.4615.Matthew D Zeiler and Rob Fergus.
2014.
Visualiz-ing and understanding convolutional networks.
InEuropean Conference on Computer Vision (ECCV).Springer.1504
