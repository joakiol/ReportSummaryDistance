Relating WordNet Senses for Word Sense DisambiguationDiana McCarthyDepartment of Informatics,University of SussexBrighton BN1 9QH, UKdianam@sussex.ac.ukAbstractThe granularity of word senses in currentgeneral purpose sense inventories is of-ten too fine-grained, with narrow sensedistinctions that are irrelevant for manyNLP applications.
This has particularlybeen a problem with WordNet which iswidely used for word sense disambigua-tion (WSD).
There have been several at-tempts to group WordNet senses given anumber of different information sourcesin order to reduce granularity.
We pro-pose relating senses as a matter of de-gree to permit a softer notion of relation-ships between senses compared to fixedgroupings so that granularity can be var-ied according to the needs of the applica-tion.
We compare two such approacheswith a gold-standard produced by humansfor this work.
We also contrast this gold-standard and another used in previous re-search with the automatic methods for re-lating senses for use with back-off meth-ods for WSD.1 IntroductionIt is likely that accurate word-level semantic dis-ambiguation would benefit a number of differenttypes of NLP application; however it is gener-ally acknowledged by word sense disambiguation(WSD) researchers that current levels of accuracyneed to be improved before WSD technology canusefully be integrated into applications (Ide andWilks, in press).
There are at least two major prob-lems facing researchers in this area.
One majorproblem is the lack of sufficient training data forsupervised WSD systems.
One response to this isWNs# gloss1 your basis for belief or disbelief; knowledge onwhich to base belief; ?the evidence that smokingcauses lung cancer is very compelling?2 an indication that makes something evident;?his trembling was evidence of his fear?3 (law) all the means by which any allegedmatter of fact whose truth is investigated atjudicial trial is established or disprovedFigure 1: The senses of evidence in WordNetto exploit the natural skew of the data and focus onfinding the first (predominant) sense from a sam-ple of text (McCarthy et al, 2004).
Further con-textual WSD may be required, but the techniqueprovides a useful unsupervised back-off method.The other major problem for WSD is the granu-larity of the sense inventory since a pre-existinglexical resource is often too fine-grained, with nar-row sense distinctions that are irrelevant for the in-tended application.
For example, WordNet (Fell-baum, 1998) which is widely used and publiclyavailable, has a great many subtle distinctions thatmay in the end not be required.
For example, infigure 1 we show the three senses (WNs#) for ev-idence from WordNet version 1.7.
1 These are allclearly related.One promising approach for improving accu-racy is to disambiguate to a coarser-grained inven-tory, which groups together the related senses ofa word.
This can be done either by defining theinventory specifically for the application, whichmight be most appropriate for machine translation,where correspondences across languages could1We use WordNet 1.7 throughout this paper since the re-sources we use for evaluation were produced for this version.17determine the inventory (Resnik and Yarowsky,2000).
There are however many systems usingman-made resources, particularly WordNet, whichhave other purposes in mind, such as entailmentfor applications such as question-answering andinformation-extraction (Dagan et al, 2005).
Therehave been several attempts to group WordNetsenses using various different types of informationsources.
This paper describes work to automati-cally relate WordNet word senses using automati-cally acquired thesauruses (Lin, 1998) and Word-Net similarity measures (Patwardhan and Peder-sen, 2003).This work proposes using graded word sense re-lationships rather than fixed groupings (clusters).Previous research has focused on clustering Word-Net senses into groups.
One problem is that todo this a stopping condition is required such asthe number of clusters required for each word.This has been done with the numbers determinedby the gold-standard for the purposes of evalu-ation (Agirre and Lopez de Lacalle, 2003) butultimately the right number of classes for eachword cannot usually be predetermined even if oneknows the application, unless only a sample ofwords are being handled.
In cases where a gold-standard is provided by humans it is clear thatfurther relationships could be drawn.
For exam-ple, in the groups (hereafter referred to as SEGR)made publicly available for the SENSEVAL-2 En-glish lexical sample (Kilgarriff, 2001) (hereafterreferred to as SEVAL-2 ENG LEX) child is groupedas shown in table 1.
Whilst it is perfectly reason-able the grouping decision was determined by the?youth?
vs ?descendant?
distinction, the relation-ships between non-grouped senses, notably sensenumbers 1 and 2 are apparent.
It is quite possiblethat these senses will share contextual cues use-ful for WSD and distinction between the two mightnot be relevant in a given application, for exam-ple because they are translated in the same way(nin?o/a in Spanish can mean both young boy/girland son/daughter) or have common substitutions(boy/girl can be used as both offspring or youngperson).
Instead of clustering senses into groupswe evaluate 2 methods that produce ranked lists ofrelated senses for each target word sense.
We referto these as RLISTs.
Such listings resemble nearestneighbour approaches for automatically acquiredthesauruses.
They allow for a sense to be relatedto others which may not themselves be closely re-WNs# SEGR gloss1 1 a young person2 2 a human offspring3 1 an immature childish person4 2 a member of a clan or tribeTable 1: SEGR for child in SEVAL-2 ENG LEXlated.
Since only a fixed number of senses are de-fined for each word, the RLISTs include all sensesof the word.
A cut-off can then be determined forany particular application.Previous research on clustering word senseshas focused on comparison to the SEGR gold-standard.
We evaluate the RLISTs against a newgold-standard produced by humans for this re-search since the SEGR does not have documenta-tion with figures for inter-tagger agreement.
Aswell as evaluating against a gold-standard, we alsolook at the effect of the RLISTs and the gold-standards themselves on WSD.
Since the focus ofthis paper is not the WSD system, but the senseinventory, we use a simple WSD heuristic whichuses the first sense of a word in all contexts, wherethe first sense of every word is specified by a re-source.
While contextual evidence is required foraccurate WSD, it is useful to look at this heuris-tic since it is so widely used as a back-off modelby many systems and is hard to beat on an all-words task (Snyder and Palmer, 2004).
We con-trast the performance of first sense heuristics i)from SemCor (Miller et al, 1993) and ii) derivedautomatically from the BNC following (McCarthyet al, 2004) and also iii) an upper-bound first senseheuristic extracted from the test data.The paper is organised as follows.
In the nextsection we describe some related work.
In sec-tion 3 we describe the two methods we will useto relate senses.
Our experiments are described insection 4.
In 4.1 we describe the construction of anew gold-standard produced using the same senseinventory used for SEGR, and give inter-annotatoragreement figures for the task.
In section 4.2 wecompare our methods to the new gold-standardand in section 4.3 we investigate how much effectcoarser grained sense distinctions have onWSD us-ing naive first sense heuristics.
We follow this witha discussion and conclusion.182 Related WorkThere is a significant amount of previous workon grouping WordNet word senses using a num-ber of different information sources, such as pred-icate argument structure (Palmer et al, forthcom-ing), information from WordNet (Mihalcea andMoldovan, 2001; Tomuro, 2001) 2 and other lex-ical resources (Peters and Peters, 1998) transla-tions, system confusability, topic signature andcontextual evidence (Agirre and Lopez de Lacalle,2003).
There is also work on grouping sensesof other inventories using information in the in-ventory (Dolan, 1994) along with information re-trieval techniques (Chen and Chang, 1998).One method presented here (referred to as DISTand described in section 3) relates most to thatof Agirre and Lopez de Lacalle (2003).
Theyuse contexts of the senses gathered directly fromeither manually sense tagged corpora, or usinginstances of ?monosemous relatives?
which aremonosemous words related to one of the targetword senses in WordNet.
We use contexts ofoccurrence indirectly.
We obtain ?nearest neigh-bours?
which occur in similar contexts to the tar-get word.
A vector is created for each word sensewith a WordNet similarity score between the senseand each nearest neighbour of the target word.
3While related senses may not have a lot of sharedcontexts directly, because of sparse data, they mayhave semantic associations with the same subsetof words that share similar distributional contextswith the target word.
This method avoids re-liance on sense-tagged data or monosemous rela-tives because the distributional neighbours can beobtained automatically from raw text.Our other method relates to the findings ofKohomban and Lee (2005).
We use the Jiang-Conrath score (JCN) in the WordNet SimilarityPackage.
This is a distance measure betweenWordNet senses given corpus frequency countsand the structure of the WordNet hierarchy.
It isdescribed in more detail below.
Kohomban andLee (2005) get good results on disambiguation ofthe SENSEVAL all-words tasks using the 25 uniquebeginners from the WordNet hierarchy for train-ing a coarse-grained WSD system and then using afirst sense heuristic (provided using the frequency2Mihalcea and Moldovan group WordNet synonym sets(synsets) rather than word senses.3We have not tried using these vectors for relating sensesof different words, but leave that for future research.data in SemCor) to determine the fine-grained out-put.
This shows that the structure of WordNet isindeed helpful when selecting coarse senses forWSD.
We use the JCN measure to contrast withour DIST measure which uses a combination ofdistributional neighbours and JCN.
We have exper-imented only with nouns to date, although in prin-ciple our method can be extended for other POS.3 Methods for producing RLISTsJCN This is a measure from the WordNet sim-ilarity package (Patwardhan and Pedersen, 2003)originally proposed as a distance measure (Jiangand Conrath, 1997).
JCN uses corpus data to pop-ulate classes (synsets) in the WordNet hierarchywith frequency counts.
Each synset is incrementedwith the frequency counts from the corpus of allwords belonging to that synset, directly or via thehyponymy relation.
The frequency data is used tocalculate the ?information content?
(IC) of a class(IC(s) = ?log(p(s))) and with this, Jiang andConrath specify a distance measure:Djcn(s1, s2) = IC(s1)+IC(s2)?2?IC(s3)where the third class (s3) is the most informative,or most specific, superordinate synset of the twosenses s1 and s2.
This is transformed from a dis-tance measure in the WN-Similarity package bytaking the reciprocal:jcn(s1, s2) = 1/Djcn(s1, s2)We use raw BNC data for calculating IC values.DIST We use a distributional similarity mea-sure (Lin, 1998) to obtain a fixed number (50)of the top ranked nearest neighbours for the tar-get nouns.
For input we used grammatical relationdata extracted using an automatic parser (Briscoeand Carroll, 2002).
We used the 90 million wordsof written English from the British National Cor-pus (BNC) (Leech, 1992).
For each noun wecollect co-occurrence triples featuring the nounin a grammatical relationship with another word.The words and relationships considered are co-occurring verbs in the direct object and subjectrelation, the modifying nouns in noun-noun rela-tions and the modifying adjectives in adjective-noun relations.
Using this data, we compute thedistributional similarity proposed by Lin betweeneach pair of nouns, where the nouns have at least10 triples.
Each noun (w) is then listed with k (=50) most similar nouns (the nearest neighbours).The nearest neighbours for a target noun (w)share distributional contexts and are typically se-19..jcn( president)>Vs43 chairperson4 electric chair1 seatchair2 professorshipsensesnearest neighboursstool, chairman.......president= <jcn(V professorship stool), jcn( ...professorship professorshipchairman),president)>..jcn(chairpersonchairman),...chairpersonstool), jcn(chairpersonVpresident)>electric chair..jcn(...chairman),electric chairstool), jcn(electric chair= <jcn(= <jcn(= <jcn( seat, stool), jcn( seat,V chairman), .....jcn(seat, president)>sss123Figure 2: Vectors for chairmantically related to the various senses (Sw) ofw.
The relationships between the various sensesare brought out by the shared semantic relation-ships with the neighbours.
For example the topnearest neighbours of chair include: stool, bench,chairman, furniture, staff, president.
The senses ofchair are 1 seat, 2 professorship, 3 chairpersonand 4 electric chair.
The seat and electric chairsenses share semantic relationships with neigh-bours such as stool, bench, furniture whilst theprofessorship and chairperson senses are relatedvia neighbours such as chairman, president, staff.The semantic similarity between a neighbour(n) e.g.
stool and a word sense (si ?
Sw) e.g.electric chair is measured using the JCN measuredescribed above.To relate the set of senses (Sw) of a word (w)we produce a vector ~Vsi = (f1...fk) with k fea-tures for each si ?
Sw.
The jth feature in ~Vsiis the highest JCN score between all senses of thejth neighbour and si.
Figure 2 illustrates thisprocess for chair.
In contrast to using JCN be-tween senses directly, the nearest neighbours per-mit senses in unrelated areas of WordNet to be re-lated e.g.
painting - activity and painting - ob-ject since both senses may have neighbours suchas drawing in common.
The vectors are used toproduce RLISTs for each si.
To produce the RLISTof a sense si of w we obtain a value for the Spear-man rank correlation coefficient (r) between thevector for si and that for each of the other sensesof w (sl ?
Sw, where l 6= i).
r is calculated byobtaining rankings for the neighbours on ~Vsi and~Vsl using the JCN values for ranking.
We then listsi with the other senses ordered according to the rvalue, for example the RLIST for sense 1 of chairis [4 (0.50), 3 (0.34), 2 (0.20)] where the sensenumber is indicated before the bracketed r score.4 ExperimentsFor our experiments we use the same set of 20nouns used by Agirre and Lopez de Lacalle(2003).
The gold standard used in that work wasSEGR.
These groupings were released for SEN-SEVAL-2 but we cannot find any documentationon how they were produced or on inter-annotatoragreement.
4 We have therefore produced a newgold-standard (referred to as RS) for these nounswhich we describe in section 4.1.
We comparethe results of our methods for relating senses andSEGR to RS.
We then look at the performance ofboth the gold-standard groupings (SEGR and RS)compared to our automatic methods for coarsergrained WSD of SEVAL-2 ENG LEX using somefirst sense heuristics.4.1 Creating a Gold StandardTo create the gold-standard we gave 3 native en-glish speakers a questionnaire with all possiblepairings of WordNet 1.7 word senses for each ofthe 20 nouns in turn.
The pairs were derived fromall possible combinations of senses of the givennoun and the judges were asked to indicate a ?re-lated?, ?unrelated?
or don?t know response foreach pair.
5 This task allows a sense to be re-lated to others which are not themselves related.The ordering of the senses was randomised andfake IDs were generated instead of using the sensenumbers provided with WordNet to avoid possi-ble bias from indications of sense predominance.The words were presented one at a time and eachcombination of senses was presented along withthe WordNet gloss.
6 Table 2 provides the pair-wise agreement (PWA) figures for each word alongwith the overall PWA figure.
The number of wordsenses for each noun is given in brackets.
Overall,more relationships were identified compared to therather fine-grained classes in SEGR, although therewas some variation.
The proportion of relateditems for our three judges were 52.2%, 56.5% and22.6% respectively.
Given this variation, the lastrow gives the pairwise agreement for pairs wherethe more lenient judge has said the pair is un-related.
These figures are reasonable given thathumans differ in their tendency to lump or split4We have asked Agirre and Lopez de Lacalle as well asthose involved with the original SENSEVAL-2 task.5We are grateful to Adam Kilgarriff for suggesting thetask.6We will make the questionnaire publicly available withthe gold standard.20word (#senses) PWAart (4) 44.44authority (7) 52.38bar (13) 87.07bum (4) 100.00chair (4) 43.75channel (7) 46.03child (4) 66.67circuit (6) 46.67day (10) 64.44facility (5) 86.67fatigue (4) 44.44feeling (6) 42.22hearth (3) 55.56mouth (8) 40.48nation (4) 100.00nature (5) 73.33post (8) 92.86restraint (6) 42.22sense (5) 73.33stress (5) 73.33overall PWA 66.94given leniency 88.10Table 2: Pairwise agreement %senses and the fact that figures for sense annotationwith three judges (as opposed to two, with a thirdto break ties) are reported in this region (Koelinget al, 2005).
Again, there are no details on anno-tation and agreement for SEGR.4.2 Agreement of automatic methods with RSFigure 3 shows the PWA of the automatic methodsJCN and DIST when calculated against the RS gold-standard at various threshold cut-offs.
The differ-ence of the best performance for these two meth-ods (61.1% DIST and 62.2% for JCN) are not statis-tically significant (using the chi-squared test).
Thebaseline which assumes that all pairs are unrelatedis 54.1%.
If we compare the SEGR to RS we get68.9% accuracy.
7 This shows that the SEGR ac-cords with RS more than the automatic methods.4.3 Application to SEVAL-2 ENG LEXWe used the same words as in the experimentabove and applied our methods as back-off tonaive WSD heuristics on the SEVAL-2 ENG LEX7Since these are groupings, there is only one possible an-swer and no thresholds are applied.44464850525456586062640  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1accuracythreshold"JCN""DISTSIM"Figure 3: Accuracy of match of RS to JCN andDISTtest data.
8 Using predominant senses is use-ful as a back-off method where local context isnot sufficient.
Disambiguation is performed us-ing the first sense heuristic from i) SemCor (Sem-cor FS) ii) automatic rankings from the BNC pro-duced using the method proposed by McCarthy etal.
(2004) (Auto FS) and iii) an upper-bound firstsense heuristic from the SEVAL-2 ENG LEX dataitself (SEVAL-2 FS).
This represents how well themethod would perform if we knew the first sense.The results are shown in table 3.
The accu-racy figures are equivalent to both recall and pre-cision as there were no words in this data with-out a first sense in either SemCor or the auto-matic rankings.
The fourth row provides a ran-dom baseline which incorporates the number ofrelated senses for each instance.
Usually this iscalculated as the sum of?w?tokens1|Sw| over allword tokens.
Since we are evaluating RLISTs,as well as groups, the number of senses for agiven word is not fixed, but depends in the tokensense.
We therefore calculate the random base-line as?ws?tokens|related senses to ws||Sw| , where wsis a word sense of word w. The columns show theresults for different ways of relating senses; thesenses are in the same group or above the thresh-old for RLISTs.
The second column (fine-grained)are the results for these first sense heuristics withthe raw WordNet synsets.
The third and fourthcolumns are the results for the SEGR and RS goldstandards.
The final four columns give the resultsfor RLISTs with JCN and DIST with the thresholdindicated.8We performed the experiment on both the SENSEVAL-2English lexical sample training and test data with very similarresults, but just show the results on the test corpus due to lackof space.21groupings thresh on RLISTsDIST JCNfine-grained SEGRs RS 0.90 0.20 0.09 0.0585SEVAL-2 FS 55.6 65.7 87.8 68.0 85.1 68.2 84.7SemCor FS 47.0 59.1 82.8 55.9 81.7 59.7 79.4Auto FS 35.5 48.8 82.9 50.2 72.3 53.4 83.3random BL 17.5 34.8 65.3 32.6 69.7 34.9 63.5Table 3: Accuracy of Coarse-grained first sense heuristic on SEVAL-2 ENG LEX1020304050607080901000  0.2  0.4  0.6  0.8  1accuracythreshold"senseval2""SemCor""auto""random"Figure 4: Accuracy on SEVAL-2 ENG LEX forFirst Sense Heuristics using DIST RLISTs and athresholdSemCor FS outperforms Auto FS, and is itselfoutperformed by the upper-bound, SEVAL-2 FS.All methods of relating WordNet synsets increasethe accuracy at the expense of an increased base-line because the task is easier with less senses todiscriminate between.
Both JCN and DIST havethreshold values which improve performance ofthe first sense heuristics more than the manuallycreated SEGR given a comparable or a lower base-line (smaller classes, and a harder task) e.g.
SE-VAL-2 FS and Auto FS for both types of RLISTsthough SemCor FS only for JCN.
RS should becompared to performance of JCN and DIST at asimilar baseline so we show these in the 6th and8th columns of the table.
In this case the RS seemsto outperform the automatic methods, but the re-sults for JCN are close enough to be encouraging,especially considering the baseline 63.5 is lowerthan that for RS (65.3).The RLISTs permit a trade-off between accuracyand granularity.
This can be seen by the graph infigure 5 which shows the accuracy obtained for thethree first sense heuristics at a range of thresholdvalues.
The random baseline is also shown.
Thedifference in performance compared to the base-1020304050607080901000  0.05  0.1  0.15  0.2accuracythreshold"senseval2""SemCor""auto""random"Figure 5: Accuracy on SEVAL-2 ENG LEX forFirst Sense Heuristics using JCN RLISTs and athresholdline for a given heuristic is typically better on thefine-grained task, however the benefits of a coarse-grained inventory will depend not on this differ-ence, but on the utility of the relationships and dis-tinctions made between senses.
We return to thispoint in the discussion and conclusions.5 DiscussionThe RLISTs show promising results when com-pared to the human produced gold-standards on aWSD task and even outperform the SEGR in mostcases.
There are other methods proposed in theliterature which also make use of information inWordNet, particularly looking for senses with re-lated words in common (Tomuro, 2001; Mihalceaand Moldovan, 2001).
Tomuro does this to findsystematic polysemy, by looking for overlap inwords in different areas of WordNet.
Evaluationis performed using WordNet cousins and inter-tagger agreement.
Mihalcea and Moldovan lookfor related words in common between differentsenses of words to merge WordNet synsets.
Theyalso use the hand tagged data in SemCor to removelow frequency synsets.
They demonstrate a largereduction in polysemy of the words in SemCor (up22sense JCN RLIST1 2 (0.11) 3 (0.096) 4 (0.095)2 4 (0.24) 1 (0.11) 3 (0.099)3 2 (0.099) 1 (0.096) 4 (0.089)4 2 (0.24) 1 (0.095) 3 (0.089)sense DIST RLIST1 3 (0.88) 4 (0.50) 2 (0.48)2 4 (0.99) 3 (0.60) 1 (0.48)3 1 (0.88) 4 (0.60) 2 (0.60)4 2 (0.99) 3 (0.60) 1 (0.50)Table 4: RLISTs for childto 39%) with a small error rate (5.6%) measuredon SemCor.
Our DIST approach relates to Agirreand Lopez de Lacalle (2003) though they pro-duced groups and evaluated against the SEGR.
Weuse nearest neighbours and associate these withword senses, rather than finding occurrences ofword senses in data directly.
Nearest neighbourshave been used previously to induce word sensesfrom raw data (Pantel and Lin, 2002), but not forrelating existing inventories of senses.
Measuresof distance in the WordNet hierarchy such as JCNhave been widely used for WSD (Patwardhan etal., 2003) as well as the information contained inthe structure of the hierarchy (Kohomban and Lee,2005) which has been used for backing off whentraining a supervised system.Though coarser groupings can improve inter-tagger agreement and WSD there is also a need toexamine which distinctions are useful since thereare many ways that items can be grouped (Palmeret al, forthcoming).
A major difference to previ-ous work is our use of RLISTs, allowing for thelevel of granularity to be determined for a givenapplication, and allowing for ?soft relationships?so that a sense can be related to several otherswhich are not themselves related.
This might alsobe done with soft hierarchical clusters, but has notyet been tried.
The idea of relating word senseas a matter of degree also relates to the methodsof Schu?tze (1998) although his work was evalu-ated using binary sense distinctions.The child example in table 1 demonstrate prob-lems with hard, fixed groupings.
Table 4 showsthe RLISTs obtained with our methods, with ther scores in brackets.
While many of the relation-ships in the SEGR are found, the relationships tothe other senses are apparent.
In SEGR no rela-tionship is retained between the offspring sense(2) and the young person sense (1).
According tothe RS, all paired meanings of child are related.
9A distance measure, rather than a fixed grouping,seems appropriate to us because one might wantthe young person sense to be related to both hu-man offspring and immature person, but not havethe latter two senses directly related.6 ConclusionWe have investigated methods for relating Word-Net word senses based on distributionally simi-lar nearest neighbours and using the JCN measure.Whilst the senses for a given word can be clusteredinto sense groups, we propose the use of rankedlists to relate the senses of a word to each other.In this way, the granularity can be determined fora given application and the appropriate number ofsenses for a given word is not needed a priori.
Wehave encouraging results for nouns when compar-ing RLISTs to manually created gold-standards.We have produced a new gold-standard for eval-uation based on the words used in SEVAL-2 ENGLEX.
We did this because there is no available doc-umentation on inter-annotator agreement for theSEGR.
In future, we hope to produce another gold-standard resource where the informants indicate adegree of relatedness, rather than a binary choiceof related or unrelated for each pair.We would like to see the impact that coarser-grained WSD has on a task or application.
Giventhe lack of a plug and play application for feedingdisambiguated data, we hope to examine the ben-efits on some lexical acquisition tasks that mightfeed into an application, for example sense rank-ing (McCarthy et al, 2004) or selectional prefer-ence acquisition.At this stage we have only experimented withnouns, we hope to go on relating senses in otherparts-of-speech, particularly verbs since they havevery fine-grained distinctions in WordNet andmany of the subtler distinctions are quite proba-bly not important for some applications.
(Palmeret al, forthcoming) has clearly demonstrated thenecessity for using predicate-argument structurewhen grouping verb senses, so we want to exploitsuch information for verbs.We have focused on improving the first senseheuristic, but we plan to use our groupings withcontext-based WSD.
To avoid a requirement for9The two more lenient judges related all the senses ofchild.23hand-tagged training data, we plan to exploit thecollocates of nearest neighbours.AcknowledgementsThis work was funded by a Royal Society Dorothy HodgkinFellowship and a UK EPSRC project ?
Ranking Word Sensesfor Disambiguation: Models and Applications.?
We thankSiddharth Patwardhan and Ted Pedersen for making the WNSimilarity package publicly available.
We would also liketo thank Adam Kilgarriff for suggesting the creation of theRS resource and Eneko Agirre, Oier Lopez de Lacalle, JohnCarroll and Rob Koeling for helpful discussions.ReferencesEneko Agirre and Oier Lopez de Lacalle.
2003.
Clusteringwordnet word senses.
In Recent Advances in Natural Lan-guage Processing, Borovets, Bulgaria.Edward Briscoe and John Carroll.
2002.
Robust accurate sta-tistical annotation of general text.
In Proceedings of theThird International Conference on Language Resourcesand Evaluation (LREC), pages 1499?1504, Las Palmas,Canary Islands, Spain.Jer Nan Chen and Jason S. Chang.
1998.
Topical clusteringof MRD senses based on information retrieval techniques.Computational Linguistics, 24(1):61?96.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.The pascal recognising textual entailment challenge.
InProceedings of the PASCAL First Challenge Workshop,pages 1?8, Southampton, UK.William B. Dolan.
1994.
Word sense disambiguation :Clustering related senses.
In Proceedings of the 15thInternational Conference of Computational Linguistics.COLING-94, volume II, pages 712?716.Christiane Fellbaum, editor.
1998.
WordNet, An ElectronicLexical Database.
The MIT Press, Cambridge, MA.Nancy Ide and Yorick Wilks.
in press.
Making senseabout sense.
In Eneko Agirre and Phil Edmonds, edi-tors, Word Sense Disambiguation, Algorithms and Appli-cations.
Springer.Jay Jiang and David Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
In Inter-national Conference on Research in Computational Lin-guistics, Taiwan.Adam Kilgarriff.
2001.
English lexical sample task descrip-tion.
In Proceedings of the SENSEVAL-2 workshop, pages17?20.Rob Koeling, Diana McCarthy, and John Carroll.
2005.Domain-specific sense distributions and predominantsense acquisition.
In Proceedings of the joint conferenceon Human Language Technology and Empirical methodsin Natural Language Processing, pages 419?426, Vancou-ver, B.C., Canada.Upali Kohomban and Wee Sun Lee.
2005.
Learning seman-tic classes for word sense disambiguation.
In Proceedingsof the 43rd Annual Meeting of the Association for Compu-tational Linguistics (ACL?05), pages 34?41, Ann Arbor,Michigan, June.
Association for Computational Linguis-tics.Geoffrey Leech.
1992.
100 million words of English: theBritish National Corpus.
Language Research, 28(1):1?13.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL 98, Mon-treal, Canada.Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-roll.
2004.
Finding predominant senses in untagged text.In Proceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, pages 280?287,Barcelona, Spain.Rada Mihalcea and Dan I. Moldovan.
2001.
Automaticgeneration of a coarse grained WordNet.
In Proceedingsof WordNet and Other Lexical Resources: Applications,Extensions and Customizations, NAACL 2001 Workshop,Pittsburgh, PA.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T Bunker.
1993.
A semantic concordance.
InProceedings of the ARPA Workshop on Human LanguageTechnology, pages 303?308.
Morgan Kaufman.Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.forthcoming.
Making fine-grained and coarse-grainedsense distinctions, both manually and automatically.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining, pages613?619, Edmonton, Canada.Siddharth Patwardhan and Ted Pedersen.2003.
The cpan wordnet::similarity package.http://search.cpan.org/author/SID/WordNet-Similarity-0.03/.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-sen. 2003.
Using measures of semantic relatedness forword sense disambiguation.
In Proceedings of the FourthInternational Conference on Intelligent Text Processingand Computational Linguistics (CICLing 2003), MexicoCity.Wim Peters and Ivonne Peters.
1998.
Automatic sense clus-tering in EuroWordNet.
In Proceedings of the First Inter-national Conference on Language Resources and Evalua-tion (LREC), pages 409?416, Granada, Spain.Philip Resnik and David Yarowsky.
2000.
Distinguishingsystems and distinguishing senses: New evaluation meth-ods for word sense disambiguation.
Natural LanguageEngineering, 5(3):113?133.Hinrich Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?123.Benjamin Snyder and Martha Palmer.
2004.
The Englishall-words task.
In Proceedings of the ACL SENSEVAL-3workshop, pages 41?43, Barcelona, Spain.Noriko Tomuro.
2001.
Tree-cut and a lexicon based on sys-tematic polysemy.
In Proceedings of the Second Meet-ing of the North American Chapter of the Associationfor Computational Linguistics.
(NAACL 2001), Pittsburgh,PA.24
