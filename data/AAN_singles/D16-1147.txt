Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1400?1409,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsKey-Value Memory Networks for Directly Reading DocumentsAlexander H. Miller1 Adam Fisch1 Jesse Dodge1,2 Amir-Hossein Karimi1Antoine Bordes1 Jason Weston11Facebook AI Research, 770 Broadway, New York, NY, USA2Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA{ahm,afisch,jessedodge,ahkarimi,abordes,jase}@fb.comAbstractDirectly reading documents and being able toanswer questions from them is an unsolvedchallenge.
To avoid its inherent difficulty, ques-tion answering (QA) has been directed towardsusing Knowledge Bases (KBs) instead, whichhas proven effective.
Unfortunately KBs oftensuffer from being too restrictive, as the schemacannot support certain types of answers, andtoo sparse, e.g.
Wikipedia contains much moreinformation than Freebase.
In this work weintroduce a new method, Key-Value MemoryNetworks, that makes reading documents moreviable by utilizing different encodings in the ad-dressing and output stages of the memory readoperation.
To compare using KBs, informationextraction or Wikipedia documents directly ina single framework we construct an analysistool, WIKIMOVIES, a QA dataset that containsraw text alongside a preprocessed KB, in thedomain of movies.
Our method reduces thegap between all three settings.
It also achievesstate-of-the-art results on the existing WIKIQAbenchmark.1 IntroductionQuestion answering (QA) has been a long stand-ing research problem in natural language processing,with the first systems attempting to answer questionsby directly reading documents (Voorhees and Tice,2000).
The development of large-scale KnowledgeBases (KBs) such as Freebase (Bollacker et al, 2008)helped organize information into structured forms,prompting recent progress to focus on answeringquestions by converting them into logical forms thatcan be used to query such databases (Berant et al,2013; Kwiatkowski et al, 2013; Fader et al, 2014).Unfortunately, KBs have intrinsic limitations suchas their inevitable incompleteness and fixed schemasthat cannot support all varieties of answers.
Sinceinformation extraction (IE) (Craven et al, 2000), in-tended to fill in missing information in KBs, is neitheraccurate nor reliable enough, collections of raw tex-tual resources and documents such as Wikipedia willalways contain more information.
As a result, even ifKBs can be satisfactory for closed-domain problems,they are unlikely to scale up to answer general ques-tions on any topic.
Starting from this observation,in this work we study the problem of answering bydirectly reading documents.Retrieving answers directly from text is harderthan from KBs because information is far less struc-tured, is indirectly and ambiguously expressed, andis usually scattered across multiple documents.
Thisexplains why using a satisfactory KB?typically onlyavailable in closed domains?is preferred over rawtext.
We postulate that before trying to provide an-swers that are not in KBs, document-based QA sys-tems should first reach KB-based systems?
perfor-mance in such closed domains, where clear compari-son and evaluation is possible.
To this end, this paperintroduces WIKIMOVIES, a new analysis tool thatallows for measuring the performance of QA systemswhen the knowledge source is switched from a KBto unstructured documents.
WIKIMOVIES contains?100k questions in the movie domain, and was de-signed to be answerable by using either a perfect KB(based on OMDb1), Wikipedia pages or an imper-1http://www.omdbapi.com1400fect KB obtained through running an engineered IEpipeline on those pages.To bridge the gap between using a KB and read-ing documents directly, we still lack appropriate ma-chine learning algorithms.
In this work we proposethe Key-Value Memory Network (KV-MemNN), anew neural network architecture that generalizes theoriginal Memory Network (Sukhbaatar et al, 2015)and can work with either knowledge source.
TheKV-MemNN performs QA by first storing facts ina key-value structured memory before reasoning onthem in order to predict an answer.
The memoryis designed so that the model learns to use keys toaddress relevant memories with respect to the ques-tion, whose corresponding values are subsequentlyreturned.
This structure allows the model to encodeprior knowledge for the considered task and to lever-age possibly complex transforms between keys andvalues, while still being trained using standard back-propagation via stochastic gradient descent.Our experiments on WIKIMOVIES indicate that,thanks to its key-value memory, the KV-MemNNconsistently outperforms the original Memory Net-work, and reduces the gap between answering froma human-annotated KB, from an automatically ex-tracted KB or from directly reading Wikipedia.
Weconfirm our findings on WIKIQA (Yang et al,2015), another Wikipedia-based QA benchmarkwhere no KB is available, where we demonstratethat KV-MemNN can reach state-of-the-art results?surpassing the most recent attention-based neuralnetwork models.2 Related WorkEarly QA systems were based on information re-trieval and were designed to return snippets of textcontaining an answer (Voorhees and Tice, 2000;Banko et al, 2002), with limitations in terms of ques-tion complexity and response coverage.
The creationof large-scale KBs (Auer et al, 2007; Bollacker et al,2008) have led to the development of a new class ofQA methods based on semantic parsing (Berant et al,2013; Kwiatkowski et al, 2013; Fader et al, 2014;Yih et al, 2015) that can return precise answers tocomplicated compositional questions.
Due to thesparsity of KB data, however, the main challengeshifts from finding answers to developing efficientinformation extraction methods to populate KBs auto-matically (Craven et al, 2000; Carlson et al, 2010)?not an easy problem.For this reason, recent initiatives are returning tothe original setting of directly answering from text us-ing datasets like TRECQA (Wang et al, 2007), whichis based on classical TREC resources (Voorhees et al,1999), and WIKIQA (Yang et al, 2015), which isextracted from Wikipedia.
Both benchmarks are or-ganized around the task of answer sentence selection,where a system must identify the sentence contain-ing the correct answer in a collection of documents,but need not return the actual answer as a KB-basedsystem would do.
Unfortunately, these datasets arevery small (hundreds of examples) and, because oftheir answer selection setting, do not offer the optionto directly compare answering from a KB againstanswering from pure text.
Using similar resourcesas the dialog dataset of Dodge et al (2016), our newbenchmark WIKIMOVIES addresses both deficien-cies by providing a substantial corpus of question-answer pairs that can be answered by either using aKB or a corresponding set of documents.Even though standard pipeline QA systems likeAskMR (Banko et al, 2002) have been recently re-visited (Tsai et al, 2015), the best published resultson TRECQA and WIKIQA have been obtained byeither convolutional neural networks (Santos et al,2016; Yin and Sch?tze, 2015; Wang et al, 2016)or recurrent neural networks (Miao et al, 2015)?both usually with attention mechanisms inspired by(Bahdanau et al, 2015).
In this work, we introduceKV-MemNNs, a Memory Network model that oper-ates a symbolic memory structured as (key, value)pairs.
Such structured memory is not employed inany existing attention-based neural network architec-ture for QA.
As we will show, it gives the modelgreater flexibility for encoding knowledge sourcesand helps shrink the gap between directly readingdocuments and answering from a KB.3 Key-Value Memory NetworksThe Key-Value Memory Network model is basedon the Memory Network (MemNNs) model (We-ston et al, 2015; Sukhbaatar et al, 2015) whichhas proven useful for a variety of document read-ing and question answering tasks: for reading chil-dren?s books and answering questions about them(Hill et al, 2016), for complex reasoning over sim-1401Figure 1: The Key-Value Memory Network model for question answering.
See Section 3 for details.ulated stories (Weston et al, 2016) and for utilizingKBs to answer questions (Bordes et al, 2015).Key-value paired memories are a generalizationof the way context (e.g.
knowledge bases or docu-ments to be read) are stored in memory.
The lookup(addressing) stage is based on the key memory whilethe reading stage (giving the returned result) uses thevalue memory.
This gives both (i) greater flexibilityfor the practitioner to encode prior knowledge abouttheir task; and (ii) more effective power in the modelvia nontrivial transforms between key and value.
Thekey should be designed with features to help matchit to the question, while the value should be designedwith features to help match it to the response (an-swer).
An important property of the model is thatthe entire model can be trained with key-value trans-forms while still using standard backpropagation viastochastic gradient descent.3.1 Model DescriptionOur model is based on the end-to-end Memory Net-work architecture of Sukhbaatar et al (2015).
Ahigh-level view of both models is as follows: onedefines a memory, which is a possibly very large ar-ray of slots which can encode both long-term andshort-term context.
At test time one is given a query(e.g.
the question in QA tasks), which is used to it-eratively address and read from the memory (theseiterations are also referred to as ?hops?)
looking forrelevant information to answer the question.
At eachstep, the collected information from the memory iscumulatively added to the original query to build con-text for the next round.
At the last iteration, the finalretrieved context and the most recent query are com-bined as features to predict a response from a list ofcandidates.Figure 1 illustrates the KV-MemNN model archi-tecture.In KV-MemNNs we define the memory slots aspairs of vectors (k1, v1) .
.
.
, (kM , vM ) and denotethe question x.
The addressing and reading of thememory involves three steps:?
Key Hashing: the question can be used to pre-select a small subset of the possibly large array.This is done using an inverted index that finds asubset (kh1 , vh1), .
.
.
, (khN , vhN ) of memoriesof size N where the key shares at least one wordwith the question with frequency < F = 1000(to ignore stop words), following Dodge et al(2016).
More sophisticated retrieval schemescould be used here, see e.g.
Manning et al(2008),?
Key Addressing: during addressing, each can-didate memory is assigned a relevance probabil-ity by comparing the question to each key:phi = Softmax(A?X(x) ?A?K(khi))where ??
are feature maps of dimension D, A isa d?D matrix and Softmax(zi) = ezi/?j ezj .We discuss choices of feature map in Sec.
3.2.?
Value Reading: in the final reading step, thevalues of the memories are read by taking theirweighted sum using the addressing probabilities,1402and the vector o is returned:o =?iphiA?V (vhi) .The memory access process is conducted by the?controller?
neural network using q = A?X(x) asthe query.
After receiving the result o, the query isupdated with q2 = R1(q + o) where R is a d ?
dmatrix.
The memory access is then repeated (specifi-cally, only the addressing and reading steps, but notthe hashing), using a different matrix Rj on eachhop, j.
The key addressing equation is transformedaccordingly to use the updated query:phi = Softmax(q>j+1A?K(khi)) .The motivation for this is that new evidence can becombined into the query to focus on and retrieve morepertinent information in subsequent accesses.
Finally,after a fixed number H hops, the resulting state ofthe controller is used to compute a final predictionover the possible outputs:a?
= argmaxi=1,...,CSoftmax(q>H+1B?Y (yi))where yi are the possible candidate outputs, e.g.
allthe entities in the KB, or all possible candidate an-swer sentences in the case of a dataset like WIKIQA(see Sec.
5.2).
The d?D matrix B can also be con-strained to be identical to A.
The whole network istrained end-to-end, and the model learns to performthe iterative accesses to output the desired target aby minimizing a standard cross-entropy loss betweena?
and the correct answer a. Backpropagation andstochastic gradient descent are thus used to learn thematrices A,B and R1, .
.
.
, RH .To obtain the standard End-To-End Memory Net-work of Sukhbaatar et al (2015) one can simply setthe key and value to be the same for all memories.Hashing was not used in that paper, but is importantfor computational efficiency for large memory sizes,as already shown in Dodge et al (2016).
We will nowgo on to describe specific applications of key-valuememories for the task of reading KBs or documents.3.2 Key-Value MemoriesThere are a variety of ways to employ key-value mem-ories that can have important effects on overall per-formance.
The ability to encode prior knowledge inthis way is an important component of KV-MemNNs,and we are free to define ?X ,?Y ,?K and ?V for thequery, answer, keys and values respectively.
We nowdescribe several possible variants of ?K and ?V thatwe tried in our experiments, for simplicity we kept?X and ?Y fixed as bag-of-words representations.KBTriple Knowledge base entries have a structureof triple ?subject relation object?
(see Table 1 for ex-amples).
The representation we consider is simple:the key is composed of the left-hand side entity (sub-ject) and the relation, and the value is the right-handside entity (object).
We double the KB and considerthe reversed relation as well (e.g.
we now have twotriples ?Blade Runner directed_by Ridley Scott?
and?Ridley Scott !directed_by Blade Runner?
where !di-rected_by is a different entry in the dictionary thandirected_by).
Having the entry both ways round isimportant for answering different kinds of questions(?Who directed Blade Runner??
vs. ?What did Rid-ley Scott direct??).
For a standard MemNN that doesnot have key-value pairs the whole triple has to beencoded into the same memory slot.Sentence Level For representing a document, onecan split it up into sentences, with each memory slotencoding one sentence.
Both the key and the valueencode the entire sentence as a bag-of-words.
Asthe key and value are the same in this case, this isidentical to a standard MemNN and this approachhas been used in several papers (Weston et al, 2016;Dodge et al, 2016).Window Level Documents are split up into win-dows of W words; in our tasks we only include win-dows where the center word is an entity.
Windows arerepresented using bag-of-words.
Window represen-tations for MemNNs have been shown to work wellpreviously (Hill et al, 2016).
However, in Key-ValueMemNNs we encode the key as the entire window,and the value as only the center word, which is notpossible in the MemNN architecture.
This makessense because the entire window is more likely tobe pertinent as a match for the question (as the key),whereas the entity at the center is more pertinent as amatch for the answer (as the value).
We will comparethese approaches in our experiments.Window + Center Encoding Instead of represent-ing the window as a pure bag-of-words, thus mixing1403the window center with the rest of the window, wecan also encode them with different features.
Here,we double the size, D, of the dictionary and encodethe center of the window and the value using the sec-ond dictionary.
This should help the model pick outthe relevance of the window center (more related tothe answer) as compared to the words either side ofit (more related to the question).Window + Title The title of a document is com-monly the answer to a question that relates to thetext it contains.
For example ?What did HarrisonFord star in??
can be (partially) answered by theWikipedia document with the title ?Blade Runner?.For this reason, we also consider a representationwhere the key is the word window as before, butthe value is the document title.
We also keep all thestandard (window, center) key-value pairs from thewindow-level representation as well, thus doublingthe number of memory slots in comparison.
To dif-ferentiate the two keys with different values we addan extra feature ?_window_?
or ?_title_?
to the key,depending on the value.
The ?_title_?
version alsoincludes the actual movie title in the key.
This rep-resentation can be combined with center encoding.Note that this representation is inherently specific todatasets in which there is an apparent or meaningfultitle for each document.4 The WikiMovies BenchmarkThe WIKIMOVIES benchmark consists of question-answer pairs in the domain of movies.
It was builtwith the following goals in mind: (i) machine learn-ing techniques should have ample training examplesfor learning; and (ii) one can analyze easily the perfor-mance of different representations of knowledge andbreak down the results by question type.
The datasetcan be downloaded from http://fb.ai/babi.4.1 Knowledge RepresentationsWe construct three forms of knowledge representa-tion: (i) Doc: raw Wikipedia documents consistingof the pages of the movies mentioned; (ii) KB: a clas-sical graph-based KB consisting of entities and rela-tions created from the Open Movie Database (OMDb)and MovieLens; and (iii) IE: information extractionperformed on the Wikipedia pages to build a KBin a similar form as (ii).
We take care to constructDoc: Wikipedia Article for Blade Runner (partially shown)Blade Runner is a 1982 American neo-noir dystopian science fiction filmdirected by Ridley Scott and starring Harrison Ford, Rutger Hauer, SeanYoung, and Edward James Olmos.
The screenplay, written by HamptonFancher and David Peoples, is a modified film adaptation of the 1968novel ?Do Androids Dream of Electric Sheep??
by Philip K. Dick.The film depicts a dystopian Los Angeles in November 2019 in whichgenetically engineered replicants, which are visually indistinguishablefrom adult humans, are manufactured by the powerful Tyrell Corporationas well as by other ?mega-corporations?
around the world.
Their useon Earth is banned and replicants are exclusively used for dangerous,menial, or leisure work on off-world colonies.
Replicants who defy theban and return to Earth are hunted down and ?retired?
by special policeoperatives known as ?Blade Runners?.
.
.
.KB entries for Blade Runner (subset)Blade Runner directed_by Ridley ScottBlade Runner written_by Philip K. Dick, Hampton FancherBlade Runner starred_actors Harrison Ford, Sean Young, .
.
.Blade Runner release_year 1982Blade Runner has_tags dystopian, noir, police, androids, .
.
.IE entries for Blade Runner (subset)Blade Runner, Ridley Scott directed dystopian, science fiction, filmHampton Fancher written Blade RunnerBlade Runner starred Harrison Ford, Rutger Hauer, Sean Young.
.
.Blade Runner labelled 1982 neo noirspecial police, Blade retired Blade RunnerBlade Runner, special police known BladeQuestions for Blade Runner (subset)Ridley Scott directed which films?What year was the movie Blade Runner released?Who is the writer of the film Blade Runner?Which films can be described by dystopian?Which movies was Philip K. Dick the writer of?Can you describe movie Blade Runner in a few words?Table 1: WIKIMOVIES: Questions, Doc, KB and IE sources.QA pairs such that they are all potentially answerablefrom either the KB from (ii) or the original Wikipediadocuments from (i) to eliminate data sparsity issues.However, it should be noted that the advantage ofworking from raw documents in real applications isthat data sparsity is less of a concern than for a KB,while on the other hand the KB has the informationalready parsed in a form amenable to manipulationby machines.
This dataset can help analyze whatmethods we need to close the gap between all threesettings, and in particular what are the best methodsfor reading documents when a KB is not available.
Asample of the dataset is shown in Table 1.Doc We selected a set of Wikipedia articles aboutmovies by identifying a set of movies from OMDb2that had an associated article by title match.
We keepthe title and the first section (before the contents box)for each article.
This gives?17k documents (movies)which comprise the set of documents our models willread from in order to answer questions.2http://beforethecode.com/projects/omdb/download.aspx1404KB Our set of movies were also matched to theMovieLens dataset3.
We built a KB using OMDband MovieLens metadata with entries for each movieand nine different relation types: director, writer, ac-tor, release year, language, genre, tags, IMDb ratingand IMDb votes, with ?10k related actors, ?6k di-rectors and?43k entities in total.
The KB is stored astriples; see Table 1 for examples.
IMDb ratings andvotes are originally real-valued but are binned andconverted to text (?unheard of?, ?unknown?, ?wellknown?, ?highly watched?, ?famous?).
We finallyonly retain KB triples where the entities also appearin the Wikipedia articles4 to try to guarantee that allQA pairs will be equally answerable by either theKB or Wikipedia document sources.IE As an alternative to directly reading documents,we explore leveraging information extraction tech-niques to transform documents into a KB format.An IE-KB representation has attractive propertiessuch as more precise and compact expressions offacts and logical key-value pairings based on subject-verb-object groupings.
This can come at the cost oflower recall due to malformed or completely missingtriplets.
For IE we use standard open-source soft-ware followed by some task-specific engineering toimprove the results.
We first employ coreference res-olution via the Stanford NLP Toolkit (Manning et al,2014) to reduce ambiguity by replacing pronominal(?he?, ?it?)
and nominal (?the film?)
references withtheir representative entities.
Next we use the SENNAsemantic role labeling tool (Collobert et al, 2011) touncover the grammatical structure of each sentenceand pair verbs with their arguments.
Each tripletis cleaned of words that are not recognized entities,and lemmatization is done to collapse different inflec-tions of important task-specific verbs to one form (e.g.stars, starring, star?
starred).
Finally, we appendthe movie title to each triple similar to the ?Window+ Title?
representation of Sec.
3.2, which improvedresults.4.2 Question-Answer PairsWithin the dataset?s more than 100,000 question-answer pairs, we distinguish 13 classes of question3http://grouplens.org/datasets/movielens/4The dataset alo includes the slightly larger version withoutthis constraint.Method KB IE Doc(Bordes et al, 2014) QA system 93.5 56.5 N/ASupervised Embeddings 54.4 54.4 54.4Memory Network 78.5 63.4 69.9Key-Value Memory Network 93.9 68.3 76.2Table 2: Test results (% hits@1) on WIKIMOVIES, comparinghuman-annotated KB (KB), information extraction-based KB(IE), and directly reading Wikipedia documents (Doc).Memory Representation DocSentence-level 52.4Window-level 66.8Window-level + Title 74.1Window-level + Center Encoding + Title 76.9Table 3: Development set performance (% hits@1) with differ-ent document memory representations for KV-MemNNs.corresponding to different kinds of edges in our KB.They range in scope from specific?such as actor tomovie: ?What movies did Harrison Ford star in??
andmovie to actors: ?Who starred in Blade Runner??
?tomore general, such as tag to movie: ?Which films canbe described by dystopian??
; see Table 4 for the fulllist.
For some question there can be multiple correctanswers.Using SimpleQuestions (Bordes et al, 2015), anexisting open-domain question answering datasetbased on Freebase, we identified the subset of ques-tions posed by human annotators that covered ourquestion types.
We created our question set by sub-stituting the entities in those questions with entitiesfrom all of our KB triples.
For example, if the orig-inal question written by an annotator was ?Whatmovies did Harrison Ford star in?
?, we created apattern ?What movies did [@actor] star in?
?, whichwe substitute for any other actors in our set, and re-peat this for all annotations.
We split the questionsinto disjoint training, development and test sets with?96k, 10k and 10k examples, respectively.
The samequestion (even worded differently) cannot appear inboth train and test sets.
Note that this is much largerthan most existing datasets; for example, the WIK-IQA dataset (Yang et al, 2015) for which we alsoconduct experiments in Sec.
5.2 has only ?1000training pairs.14055 ExperimentsThis section describes our experiments on WIKI-MOVIES and WIKIQA.5.1 WikiMoviesWe conducted experiments on the WIKI-MOVIES dataset described in Sec.
4.
Ourmain goal is to compare the performance of KB, IEand Wikipedia (Doc) sources when trying varyinglearning methods.
We compare four approaches: (i)the QA system of Bordes et al (2014) that performswell on existing datasets WebQuestions (Berant et al,2013) and SimpleQuestions (Bordes et al, 2015) thatuse KBs only; (ii) supervised embeddings that do notmake use of a KB at all but learn question-to-answerembeddings directly and hence act as a sanity check(Dodge et al, 2016); (iii) Memory Networks; and(iv) Key-Value Memory Networks.
Performance isreported using the accuracy of the top hit (singleanswer) over all possible answers (all entities), i.e.the hits@1 metric measured in percent.
In all caseshyperparameters are optimized on the developmentset, including the memory representations of Sec.3.2 for MemNNs and KV-MemNNs.
As MemNNsdo not support key-value pairs, we concatenate keyand value together when they differ instead.The main results are given in Table 2.
The QAsystem of Bordes et al (2014) outperforms Super-vised Embeddings and Memory Networks for KBand IE-based KB representations, but is designedto work with a KB, not with documents (hence theN/A in that column).
However, Key-Value MemoryNetworks outperform all other methods on all threedata source types.
Reading from Wikipedia docu-ments directly (Doc) outperforms an IE-based KB(IE), which is an encouraging result towards auto-mated machine reading though a gap to a human-annotated KB still remains (93.9 vs. 76.2).
Thebest memory representation for directly reading doc-uments uses ?Window-level + Center Encoding +Title?
(W = 7 and H = 2); see Table 3 for a compar-ison of results for different representation types.
Bothcenter encoding and title features help the window-level representation, while sentence-level is inferior.QA Breakdown A breakdown by question typecomparing the different data sources for KV-MemNNs is given in Table 4.
IE loses out especiallyQuestion Type KB IE DocWriter to Movie 97 72 91Tag to Movie 85 35 49Movie to Year 95 75 89Movie to Writer 95 61 64Movie to Tags 94 47 48Movie to Language 96 62 84Movie to IMDb Votes 92 92 92Movie to IMDb Rating 94 75 92Movie to Genre 97 84 86Movie to Director 93 76 79Movie to Actors 91 64 64Director to Movie 90 78 91Actor to Movie 93 66 83Table 4: Breakdown of test results (% hits@1) on WIKI-MOVIES for Key-Value Memory Networks using different knowl-edge representations.Knowledge Representation KV-MemNNKB 93.9One Template Sentence 82.9All Templates Sentences 80.0One Template + Coreference 76.0One Template + Conjunctions 74.0All Templates + Conj.
+ Coref.
72.5Wikipedia Documents 76.2Table 5: Analysis of test set results (% hits@1) for KB vs.Synthetic Docs on WIKIMOVIES.to Doc (and KB) on Writer, Director and Actor toMovie, perhaps because coreference is difficult inthese cases ?
although it has other losses elsewheretoo.
Note that only 56% of subject-object pairs inIE match the triples in the original KB, so losses areexpected.
Doc loses out to KB particularly on Tag toMovie, Movie to Tags, Movie to Writer and Movie toActors.
Tag questions are hard because they can ref-erence more or less any word in the entire Wikipediadocument; see Table 1.
Movie to Writer/Actor arehard because there is likely only one or a few refer-ences to the answer across all documents, whereasfor Writer/Actor to Movie there are more possibleanswers to find.KB vs.
Synthetic Document Analysis To furtherunderstand the difference between using a KB versusreading documents directly, we conducted an exper-iment where we constructed synthetic documentsusing the KB.
For a given movie, we use a simplegrammar to construct a synthetic ?Wikipedia?
doc-1406Method MAP MRRWord Cnt 0.4891 0.4924Wgt Word Cnt 0.5099 0.51322-gram CNN (Yang et al, 2015) 0.6520 0.6652AP-CNN (Santos et al, 2016) 0.6886 0.6957Attentive LSTM (Miao et al, 2015) 0.6886 0.7069Attentive CNN (Yin and Sch?tze, 2015) 0.6921 0.7108L.D.C.
(Wang et al, 2016) 0.7058 0.7226Memory Network 0.5170 0.5236Key-Value Memory Network 0.7069 0.7265Table 6: Test results on WikiQA.ument based on the KB triples: for each relationtype we have a set of template phrases (100 in to-tal) used to generate the fact, e.g.
?Blade Runnercame out in 1982?
for the entry BLADE RUNNERRELEASE_YEAR 1982.
We can then parameterizethe complexity of our synthetic documents: (i) usingone template, or all of them; (ii) using conjunctionsto combine facts into single sentences or not; and(iii) using coreference between sentences where wereplace the movie name with ?it?.5 The purpose ofthis experiment is to find which aspects are responsi-ble for the gap in performance to a KB.
The resultsare given in Table 5.
They indicate that some of theloss (93.9% for KB to 82.9% for One Template Sen-tence) in performance is due directly to representingin sentence form, making the subject, relation andobject harder to extract.
Moving to a larger numberof templates does not deteriorate performance much(80%).
The remaining performance drop seems tobe split roughly equally between conjunctions (74%)and coreference (76%).
The hardest synthetic datasetcombines these (All Templates + Conj.
+ Coref.
)and is actually harder than using the real Wikipediadocuments (72.5% vs. 76.2%).
This is possibly be-cause the amount of conjunctions and coreferenceswe make are artificially too high (50% and 80% ofthe time, respectively).5.2 WikiQAWIKIQA (Yang et al, 2015) is an existing datasetfor answer sentence selection using Wikipedia asthe knowledge source.
The task is, given a ques-tion, to select the sentence coming from a Wikipediadocument that best answers the question, where per-formance is measured using mean average preci-5This data is also part of the WIKIMOVIES benchmark.sion (MAP) and mean reciprocal rank (MRR) of theranked set of answers.
The dataset uses a pre-builtinformation retrieval step and hence provides a fixedset of candidate sentences per question, so systemsdo not have to consider ranking all of Wikipedia.In contrast to WIKIMOVIES, the training set sizeis small (?1000 examples) while the topic is muchmore broad (all of Wikipedia, rather than just movies)and the questions can only be answered by readingthe documents, so no comparison to the use of KBscan be performed.
However, a wide range of methodshave already been tried on WIKIQA, thus providing auseful benchmark to test if the same results found onWIKIMOVIES carry across to WIKIQA, in particularthe performance of Key-Value Memory Networks.Due to the size of the training set, following manyother works (Yang et al, 2015; Santos et al, 2016;Miao et al, 2015) we pre-trained the word vectors(matrices A and B which are constrained to be iden-tical) before training KV-MemNNs.
We employedSupervised Embeddings (Dodge et al, 2016) for thatgoal, training on all of Wikipedia while treating theinput as a random sentence and the target as the subse-quent sentence.
We then trained KV-MemNNs withdropout regularization: we sample words from thequestion, memory representations and the answers,choosing the dropout rate using the development set.Finally, again following other successful methods(Yin and Sch?tze, 2015), we combine our approachwith exact matching word features between questionand answers.
Key hashing was not used as candidateswere already pre-selected.
To represent the memo-ries, we used the Window-Level representation (thebest choice on the dev set was W = 7) as the keyand the whole sentence as the value, as the valueshould match the answer which in this case is a sen-tence.
Additionally, in the representation all numbersin the text and the phrase ?how many?
in the questionwere replaced with the feature ?_number_?.
The bestchoice of hops was also H = 2 for KV-MemNNs.The results are given in Table 6.
Key-Value Mem-ory Networks outperform a large set of other methods,although the results of the L.D.C.
method of (Wanget al, 2016) are very similar.
Memory Networks,which cannot easily pair windows to sentences, per-form much worse, highlighting the importance ofkey-value memories.14076 ConclusionWe studied the problem of directly reading docu-ments in order to answer questions, concentratingour analysis on the gap between such direct methodsand using human-annotated or automatically con-structed KBs.
We presented a new model, Key-ValueMemory Networks, which helps bridge this gap, out-performing several other methods across two datasets,WIKIMOVIES and WIKIQA.
However, some gap inperformance still remains.
WIKIMOVIES serves asan analysis tool to shed some light on the causes.Future work should try to close this gap further.Key-Value Memory Networks are versatile modelsfor reading documents or KBs and answering ques-tions about them?allowing to encode prior knowl-edge about the task at hand in the key and valuememories.
These models could be applied to storingand reading memories for other tasks as well, andfuture work should try them in other domains, suchas in a full dialog setting.ReferencesAuer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyga-niak, R., and Ives, Z.
(2007).
Dbpedia: A nucleusfor a web of open data.
In Semantic Web Confer-ence, 2007.Bahdanau, D., Cho, K., and Bengio, Y.
(2015).
Neu-ral machine translation by jointly learning to alignand translate.
In ICLR, 2015.Banko, M., Brill, E., Dumais, S., and Lin, J.
(2002).Askmsr: Question answering using the worldwideweb.
In AAAI Spring Symposium on Mining An-swers from Texts and Knowledge Bases, 2002.Berant, J., Chou, A., Frostig, R., and Liang, P.(2013).
Semantic parsing on freebase fromquestion-answer pairs.
In EMNLP, 2013.Bollacker, K., Evans, C., Paritosh, P., Sturge, T., andTaylor, J.
(2008).
Freebase: a collaboratively cre-ated graph database for structuring human knowl-edge.
In ACM SIGMOD International Conferenceon Management of Data, 2008.Bordes, A., Chopra, S., and Weston, J.
(2014).
Ques-tion answering with subgraph embeddings.
InEMNLP, 2014.Bordes, A., Usunier, N., Chopra, S., and We-ston, J.
(2015).
Large-scale simple question an-swering with memory networks.
arXiv preprintarXiv:1506.02075.Carlson, A., Betteridge, J., Kisiel, B., Settles, B.,Hruschka Jr, E. R., and Mitchell, T. M. (2010).Toward an architecture for never-ending languagelearning.
In AAAI Conference on Artificial Intelli-gence, 2010.Collobert, R., Weston, J., Bottou, L., Karlen, M.,Kavukcuoglu, K., and Kuksa, P. (2011).
Natu-ral language processing (almost) from scratch.
J.Mach.
Learn.
Res., 12, 2493?2537.Craven, M., DiPasquo, D., Freitag, D., McCallum,A., Mitchell, T., Nigam, K., and Slattery, S. (2000).Learning to construct knowledge bases from theworld wide web.
Artificial intelligence, 118, 69?113.Dodge, J., Gane, A., Zhang, X., Bordes, A., Chopra,S., Miller, A., Szlam, A., and Weston, J.
(2016).Evaluating prerequisite qualities for learning end-to-end dialog systems.
In ICLR, 2016.Fader, A., Zettlemoyer, L., and Etzioni, O.
(2014).Open question answering over curated and ex-tracted knowledge bases.
In KDD, 2014.Hill, F., Bordes, A., Chopra, S., and Weston, J.(2016).
The goldilocks principle: Reading chil-dren?s books with explicit memory representations.In ICLR, 2016.Kwiatkowski, T., Choi, E., Artzi, Y., and Zettlemoyer,L.
(2013).
Scaling semantic parsers with on-the-flyontology matching.
In EMNLP, 2013.Manning, C. D., Raghavan, P., and Sch?tze, H.(2008).
Introduction to Information Retrieval.Cambridge University Press, New York, NY, USA.Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J.,Bethard, S. J., and McClosky, D. (2014).
The stan-ford corenlp natural language processing toolkit.In ACL: System Demonstrations, 2014.Miao, Y., Yu, L., and Blunsom, P. (2015).
Neuralvariational inference for text processing.
arXivpreprint arXiv:1511.06038.Santos, C. d., Tan, M., Xiang, B., and Zhou, B.(2016).
Attentive pooling networks.
arXiv preprintarXiv:1602.03609.1408Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R.(2015).
End-to-end memory networks.
In NIPS,2015.Tsai, C., Yih, W.-t., and Burges, C. (2015).
Web-based question answering: Revisiting askmsr.Technical report, Technical Report MSR-TR-2015-20, Microsoft Research.Voorhees, E. M. et al (1999).
The trec-8 questionanswering track report.
In Trec, 1999.Voorhees, E. M. and Tice, D. M. (2000).
Buildinga question answering test collection.
In ACM SI-GIR Conference on Research and Development inInformation Retrieval, 2000.Wang, M., Smith, N. A., and Mitamura, T. (2007).What is the jeopardy model?
a quasi-synchronousgrammar for qa.
In EMNLP-CoNLL, 2007.Wang, Z., Mi, H., and Ittycheriah, A.
(2016).Sentence similarity learning by lexical decom-position and composition.
arXiv preprintarXiv:1602.07019.Weston, J., Chopra, S., and Bordes, A.
(2015).
Mem-ory networks.
In ICLR, 2015.Weston, J., Bordes, A., Chopra, S., and Mikolov, T.(2016).
Towards ai-complete question answering:a set of prerequisite toy tasks.
In ICLR, 2016.Yang, Y., Yih, W.-t., and Meek, C. (2015).
Wik-iqa: A challenge dataset for open-domain questionanswering.
In EMNLP, 2015.Yih, W.-t., Chang, M.-W., He, X., and Gao, J.
(2015).Semantic parsing via staged query graph genera-tion: Question answering with knowledge base.
InACL, 2015.Yin, W. and Sch?tze, H. (2015).
Convolutional neuralnetwork for paraphrase identification.
In NACL:Human Language Technologies, 2015.1409
