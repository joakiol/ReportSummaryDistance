The Problem of Precision in Restricted-Domain Question-Answering.Some Proposed Methods of ImprovementDOAN-NGUYEN Hai and Leila KOSSEIMCLaC Laboratory, Department of Computer Science, Concordia UniversityMontreal, Quebec, H3G-1M8,Canadahaidoan@cs.concordia.ca, kosseim@cs.concordia.caAbstractThis paper discusses some main difficulties ofrestricted-domain question-answering systems,in particular the problem of precisionperformance.
We propose methods forimproving the precision, which can beclassified into two main approaches:improving the Information Retrieval module,and improving its results.
We present theapplication of these methods in a real QAsystem for a large company, which yieldedvery good results.1 IntroductionRestricted-domain Question-Answering(RDQA) works on specific domains and often usesdocument collections restricted in subject andvolume.
It has some characteristics that maketechniques developed recently for open-domainQA, particularly those within TREC (TextREtrieval Conference, e.g.
(TREC, 2002))competitions, become less helpful.
First, in RDQA,correct answers to a question may often be foundin only very few documents.
Light et al(2001)give evidence that the performance on precision ofa system depends greatly on the redundancy ofanswer occurrences in the document collection1.Second, a RDQA system has often to work withdomain-specific terminology, including domain-specific word meaning.
Lexical and semantictechniques based on general lexicons and thesauri,such as WordNet, may not apply well here.
Third,if a QA system is to be used for a real application,e.g.
answering questions from clients of acompany, it should accept complex questions, of1 For example, they estimate that only about 27% ofthe systems participating in TREC-8 produced a correctanswer for questions with exactly one answeroccurrence, while about 50% of systems produced acorrect answer for questions with 7 answer occurrences.
(7 is the average answer occurrences per question in theTREC-8 collection.
)various forms and styles.
The system should thenreturn a complete answer, which can be long andcomplex, because it has to, e.g., clarify the contextof the problem posed in the question, explain theoptions of a service, give instructions, procedures,or suggestions, etc.
Contrarily, techniques fromTREC competitions, aiming at finding short andprecise answers, are often based on the hypothesisthat the questions are constituted by a single, andoften simple, sentence, and can be categorized intoa well-defined and simple semantic classification(e.g.
Person, Time, Location, Quantity, etc.
).RDQA has a long history, beginning withsystems working over databases (e.g., BASEBALL(Green et al 1961) and LUNAR (Woods, 1973)).Recently, research in QA has concentrated mostlyon open-domain QA, in particular on how to find avery precise and short answer.
Nonetheless, RDQAseems to be regaining attention, as shown by thisACL workshop.
Researchers are also beginning torecognize the importance of long and completeanswers.
Lin et al(2003) carried out experimentsshowing that users prefer an answer within context,e.g., an answer within its containing paragraph.Buchholz and Daelemans (2001) defined sometypes of complex answers, and proposed that thesystem presents a list of good candidates to theuser, and let him construct the reply by himself.Harabagiu et al(2001) mentioned the class ofquestions that need a listing answer.One well-known approach for RDQA wassemantic grammars (Brown and Burton, 1975),which build pre-defined patterns of questions for aspecific task.
Simple and easy to implement, thisapproach can only deal with very small tasks, and arestricted set of questions.
The most popular classof techniques for QA ?
whether it is restricted-domain or open-domain, includes using thesauriand lexicons, classifying documents, andcategorizing the questions.
Harabagiu et al(2000),for example, use WordNet extensively to generatekeyword alternations and infer the expected answercategory of a question.In this paper, we present several methods toimprove the precision of a RDQA system whichshould accept freely complex questions and returncomplete answers.
We use our experiments indeveloping a real system as demonstration.2 Overview of the demonstration systemThe objective of this system is to reply to clients'questions on services offered by a large company,here Bell Canada.
The company provides wide-range services on telephone, wireless, Internet,Web, etc.
for personal and enterprise clients.
Thedocument collection was derived from HTML andPDF files from the company's website(www.bell.ca).
As the structure of these files wasso complicated, documents were saved as pure textwith no mark-ups, sacrificing some importantformatting cues like titles, listings, tables.
Thecollection comprises more than 220 documents, ofa total of about 560K characters.The available question set has 140 questions.
Itwas assured that every question has an answerfrom the contents of the collection.
The form andstyle of the questions vary freely.
Most questionsare composed of one sentence, but some arecomposed of several sentences.
The average lengthof questions is 11.3 words (to compare, that ofTREC questions is 7.3 words).
The questions askabout what a service is, its details, whether aservice exists for a certain need, how to dosomething with a service, etc.
For the project, wedivided the question set at random into 80questions for training and 60 for testing.
Below aresome examples of questions:Do I have a customized domain name evenwith the Occasional Plan of Business InternetDial?With the Web Live Voice service, is it possiblethat a visitor activates a call to our companyfrom our web pages, but then the call isconnected over normal phone line?It seems that the First Rate Plan is only goodif most of my calls are in the evenings orweekends.
If so, is there another plan for longdistance calls anytime during the day?Although our collection was not very large, itwas not so small either so that a strategy ofsearching the answers directly in the collectioncould be obvious.
Hence we first followed theclassic two-step strategy of QA: informationretrieval (IR), and then candidate selection andanswer extraction.
For the first step, we usedOkapi, a well-known generic IR engine(www.soi.city.ac.uk/~andym/OKAPI-PACK/, also(Beaulieu et al 1995)).
For each question, Okapireturns an ordered list of answer candidates,together with a relevance score for each candidateand the name of the document containing it.
Ananswer candidate is a paragraph which Okapiconsiders most relevant to the question.2The candidates were then evaluated by a humanjudge using a binary scale: correct or incorrect.This kind of judgment is recommended in thecontext of communications between a companyand its clients, because the conditions and technicaldetails of a service should be edited as clearly aspossible in the reply to the client.
However we didalso accept some tolerance in the evaluation.
If aquestion is ambiguous, e.g., it asks about phonesbut does not specify whether it pertains to wiredphones or wireless phones, all correct candidates ofeither case will be accepted.
If a candidate is goodbut incomplete as a reply, it will be judged correctif it contains the principal theme of the supposedanswer, and if missing information can be found inparagraphs around the candidate's text in thecontaining document.Table 1 shows Okapi's performance on thetraining question set.
We kept at most the 10 bestcandidates for each question, because after rank 10a good answer was very rare.
C(n) is the number ofcandidates at rank n which are judged correct.
Q(n)is the number of questions in the training set whichhave at least one correct answer among the first nranks.
As for answer redundancy, among the 45questions having at least a correct answer (seeQ(10)), there were 33 questions (41.3% of theentire training set) having exactly 1 correct answer,10 questions (12.5%) having 2, and 2 questions(2.5%) having 3 correct answers.
Table 2 givesOkapi's precision on the test question set.The results show that Okapi's performance onprecision was not satisfying, conforming to ourdiscussion about characteristics of RDQA above.The precision was particularly weak for n's from 1to 5.
Unfortunately, these are cases that the systemaims at.
n=1 means that only one answer will bereturned ?
a totally automatic system.
n=2 to 5correspond to more practical scenarios of a semi-automatic system, where an agent of the companychooses the best one among the n candidates, editsit, and sends it to the client.
We stopped at n=5because a greater number of candidates seems tooheavy psychologically to the human agent.
Alsonote that the rank of the candidates is not importanthere, because they would be equally examined bythe agent.
This explains why we used Q(n) tomeasure the precision performance rather than2 A paragraph is a block of text separated by doublenewlines.
As formatted files were saved in plain text,original "logical" paragraphs may be joined up into oneparagraph, which may affect the precision of thecandidates.other well-known scoring such as mean reciprocalrank (MRR).Examining the correct candidates, we found thatthey were generally good enough to be sent to theuser as an understandable reply.
About 25% ofthem contained superfluous information for thecorresponding question, while 15% were lackingof information.
However, only 2/3 of the latter(that is 10% of all) looked difficult to be completedautomatically.
Building the answer from a goodcandidate therefore seemed less important thanimproving the precision of the IR module.
Wetherefore concentrated on how to improve Q(n), n=1 to 5, of the system.n 1 2 3 4 5 6 7 8 9 10C(n) 20 11 5 4 9 3 1 1 4 1%C(n) 25% 13.8% 6.3% 5% 11.3% 3.8% 1.3% 1.3% 5% 1.3%Q(n) 20 26 28 32 39 41 42 43 44 45%Q(n) 25% 32.5% 35% 40% 48.8% 51.3% 52.5% 53.8% 55% 56.3%Table 1: Precision performance of Okapi on the training question set (80 questions).n 1 2 3 4 5 6 7 8 9 10C(n) 18 8 7 2 4 3 3 2 1 1%C(n) 30% 13.3% 11.7% 3.3% 6.7% 5% 5% 3.3% 1.7% 1.7%Q(n) 18 23 28 29 32 33 35 36 36 37%Q(n) 30% 38.3% 46.7% 48.3% 53.3% 55% 58.3% 60% 60% 61.7%Table 2: Precision performance of Okapi on the test question set (60 questions).3 Methods for Improving PrecisionPerformanceThe first approach to improve the precisionperformance of the IR module is to use a betterengine, e.g.
by adjusting the parameters, modifyingthe formulas of the engine, or replacing a genericengine by a more domain-specific one, etc.Now suppose that the IR engine is already fixed,e.g.
because we have achieved the best engine, or,more practically, because we cannot make changesor afford another engine.
The second approachconsists in improving the results returned by the IRengine.
One main direction is candidate re-ranking,i.e.
pushing good candidates in the returnedcandidate list to the first ranks as much as possible,thus increasing Q(n).
To do this, we need someinformation that can characterize the relevance of acandidate to the corresponding question better thanthe IR engine did.
The most prominent kind ofsuch information may be the domain-specificlanguage used in the working domain of the QAsystem, particularly its vocabulary, or even morenarrowly, its terminological set.In the following, we will present ourdevelopment of the second approach on the BellCanada QA system first, because it seems lesscostly than the first one.
However, we will presentsome implementations of the first approach later.4 Improving Precision by Re-rankingCandidatesWe experimented with two methods of re-ranking, one with a strongly specificterminological set, and one with a good documentcharacterization.4.1 Re-ranking using specific vocabularyIn the first experiment, we noted that the namesof specific Bell services, such as 'Business InternetDial', 'Web Live Voice', etc., could be used as arelevance characterizing information, because theyoccurred very often in almost every document andquestion, and a service was often presented ormentioned in only one or a few documents, makingthese terms very discriminating.
To have a genericconcept, let's call these names 'special terms'.Luckily, these special terms occurred normally incapital letters, and could be automatically extractedeasily.
After a manual filtering, we obtained morethan 450 special terms.We designed a new scoring system which raisesthe score of the candidates containing occurrencesof special terms found in the correspondingquestion, as follows:(1) Score_of_candidate[i] = DC ?
(OW ?Okapi_score + RC[i] ?
Term_score + 1)Thus, the score of candidate i in the ranked listreturned by Okapi depends on: (i) The originalOkapi_score given by Okapi, weighted by someinteger value OW.
(ii) A Term_score thatmeasures the importance of common occurrencesof special terms, and, with less emphasis, othernoun phrases and open-class words, in the questionand the candidate.
It is weighted by some integervalue RC[i] (for rank coefficient) that representsthe role of the relative ranking of Okapi.
(iii) Adocument coefficient DC that indicates the relativeimportance of a candidate i coming or not comingfrom a document which contains at least a specialterm occurring in the question.
DC is thusrepresented by a 2-value pair; e.g., the pair (1, 0)corresponds to the extreme case of keeping onlycandidates coming from a document whichcontains at least one special term in the question,and throwing out all others.
We ran the systemwith 20 different values of DC, 50 of RC, and OWfrom 0 to 60, on the training question set.
See(Doan-Nguyen and Kosseim, 2004) for a detailedexplanation of how formula (1) was derived, andhow to design the values of DC, RC, and OW.Formula (1) gave very good improvements onthe training set (Table 3), but just modest resultswhen running the system with optimal trainingparameters on the test set (Table 4).
Note: ?Q(n) =System's Q(n) ?
Okapi?s Q(n); %?Q(n) =?Q(n)/Okapi?s Q(n).3n 1 2 3 4 5Q(n) 30 40 42 43 44?Q(n) 10 14 14 11 5%?Q(n) 50% 53.8% 50% 34.4% 12.8%Table 3: Best results of formula (1) on thetraining set.n 1 2 3 4 5Q(n) 22 29 32 33 34?Q(n) 4 6 4 4 2%?Q(n) 22.2% 26.1% 14.3% 13.8% 6.3%Table 4: Results of formula (1) on the test set.3 Okapi allows one to give it a list of phrases asindices, in addition to indices automatically createdfrom single words.
In fact, the results in Tables 1 and 2correspond to this kind of indexing, in which weprovided Okapi with the list of special terms.
Theseresults are much better than those of standard indexing,i.e.
without the special term list.4.2 Re-ranking with a better documentcharacterizationIn formula (1), the coefficient DC represents anestimate of the relevance of a document to aquestion based only on special terms; it cannothelp when the question and document do notcontain special terms.
To find another documentcharacterization which can complement this, wetried to map the documents into a system ofconcepts.
Each document says things about a set ofconcepts, and a concept is discussed in a set ofdocuments.
Building such a concept system seemsfeasible within closed-domain applications,because the domain of the document collection ispre-defined, the number of documents is in acontrolled range, and the documents are oftenalready classified topically, e.g.
by their creator.
Ifno such classification existed, one can usetechniques of building hierarchies of clusters (e.g.those summarized in (Kowalski, 1997)).We used the original document classification ofBell Canada, represented in the web page URLs, asthe basis for constructing the concept hierarchyand the mapping between it and the documentcollection.
Below is a small excerpt from thehierarchy:BellAllPersonalPersonal-PhonePersonal-Phone-LongDistancePersonal-Phone-LongDistance-BasicRatePersonal-Phone-LongDistance-FirstRateIn general, a leaf node concept corresponds toone or very few documents talking about it.
Aparent concept corresponds to the union ofdocuments of its child concepts.
Note that althoughmany concepts coincide in fact with a special term,e.g.
'First Rate', many others are not special terms,e.g.
'phone', 'wireless', 'long distance', etc.The use of the concept hierarchy in the QAsystem was based on the following assumption: Aquestion can be well understood only when we canrecognize the concepts implicit in it.
For example,the concepts in the question:It seems that the First Rate Plan is only goodif most of my calls are in the evenings orweekends.
If so, is there another plan for longdistance calls anytime during the day?include Personal-Phone-LongDistance andPersonal-Phone-LongDistance-FirstRate.Once the concepts are recognized, it is easy todetermine a small set of documents relevant tothese concepts, and carry out the search of answersin this set.To map a question to the concept hierarchy, wepostulated that the question should contain wordsexpressing the concepts.
These words may be thoseconstituting the concepts, e.g., 'long', 'distance','first', 'rate', etc., or synonyms/near synonyms ofthem, e.g., 'telephone' to 'phone'; 'mobile','cellphone' to 'wireless'.
For every concept, webuilt a bag of words which make up the concept,e.g., the bag of words for Personal-Phone-LongDistance-FirstRate is {'personal', 'phone','long', 'distance', 'first', 'rate'}.
We also builtmanually a small lexicon of (near) synonyms asmentioned above.Now, a question will be analyzed into separatewords (stop words removed), and we look forconcepts whose bags of words have elements incommon with them.
(Here we used the Porterstemmed form of words in comparison, and alsocounted cases of synonyms/near synonyms.)
Aconcept is judged more relevant to a question if: (i)its bag of words has more elements in commonwith the question's set of words; (ii) the quotient ofthe size of the common subset mentioned in (i)over the size of the entire bag of words is larger;and (iii) the question contains more occurrences ofwords in that subset.From the relevant concept set, it isstraightforward to derive the relevant document setfor a given question.
The documents will be rankedaccording to the order of the deriving concepts.
(Ifa document is derived from several concepts, thehighest rank will be used.)
As for the coverage ofthe mapping, there were only 4 questions in thetraining set and 6 in the test set (7% of the entirequestion set) having an empty relevant documentset.
In fact, these questions seemed to need acontext to be understood, e.g., a question like'What does Dot org mean?'
should be posed in aconversation about Internet services.Now the score of a candidate is calculated by:(2) Score_of_candidate[i] = (CC + DC) ?
(OW?
Okapi_score + RC[i] ?
Term_score + 1)The value of CC (concept-related coefficient)depends on the document that provides thecandidate.
CC should be high if the rank of thedocument is high, e.g.
CC=1 if rank=1, CC=0.9 ifrank=2, CC=0.8 if rank=3, etc.
If the documentdoes not occur in the concept-derived list, its CCshould be very small, e.g.
0.
The sum (CC + DC)represents a combination of the two kinds ofdocument characterization.
We ran the system with15 different values of the CC vector, with CC forrank 1 varying from 0 to 7, and CC for other ranksdecreasing accordingly.
Values for othercoefficients are the same as in the previousexperiment using formula (1).
Results (Tables 5and 6) are uniformly better than those of formula(1).
Good improvements show that the approach isappropriate and effective.n 1 2 3 4 5Q(n) 32 41 44 44 44?Q(n) 12 15 16 12 5%?Q(n) 60% 57.7% 57.1% 37.5% 12.8%Table 5: Best results of formula (2) on thetraining set.n 1 2 3 4 5Q(n) 30 32 35 35 36?Q(n) 12 9 7 6 4%?Q(n) 66.6% 39.1% 25% 20.7% 12.5%Table 6: Results of formula (2) on the test set.5 Two-Level Candidate SearchingAs the mapping in the previous section seems tobe able to point out the documents relevant to agiven question with a high precision, we tried tosee how to combine it with the IR engine Okapi.
Inthe previous experiments, the entire documentcollection was indexed by Okapi.
Now indexingwill be carried out separately for each question:only the document subset returned by the mapping,which usually contains no more than 20documents, is indexed, and Okapi will search forcandidate answers for the question only in thissubset.
We hoped that Okapi could achieve higherprecision in working with a much smallerdocument set.
This strategy can be considered as akind of two-level candidate searching.n 1 2 3 4 5MO Q(n) 18 33 38 45 46Q(n) 31 42 48 48 48?Q(n) 11 16 20 16 9%?Q(n) 55% 61.5% 71.4% 50% 23.1%Table 7: Best results of two-level searchcombined with re-ranking on the training set.n 1 2 3 4 5MO Q(n) 20 25 26 29 31Q(n) 24 28 32 32 33?Q(n) 6 5 4 3 1%?Q(n) 33.3% 21.8% 14.3% 10.3% 3.1%Table 8: Results of two-level search combinedwith re-ranking on the test set.Results show that Okapi did not do better in thiscase than when it worked with the entire documentcollection (compare MO Q(n) in Tables 7 and 8with Q(n) in Tables 1 and 2.
MO means 'mapping-then-Okapi').
We then applied formula (2) torearrange the candidate list as in the previoussection.
Although results on the training set (Table7) are generally better than those of the previoussection, results on the test set (Table 8) are worse,which leads to an unfavorable conclusion for thismethod.
(Note that ?Q(n) and %?Q(n) are alwayscomparisons of the new Q(n) with the originalOkapi Q(n) in Tables 1 and 2.
)6 Re-implementing the IR engineThe precision of the question-document mappingwas good, but the performance of the two-levelsystem based on Okapi in the previous section wasnot very persuasive.
This led us back to the firstapproach mentioned in Section 3, i.e.
replacingOkapi by another IR engine.
We would not lookfor another generic engine because it was notinteresting theoretically, but would insteadimplement a two-level engine using the question-document mapping.
As already known, themapping returns just a small set of relevantdocuments for a given question; the new enginewill search for candidate answers in this set.
If thedocument set is empty, the system takes thecandidates proposed by Okapi as results ("Okapi asLast Resort").We implemented just a simple IR engine.
Firstthe question is analyzed into separate words (stopwords removed).
For every document in the setreturned by the question-document mapping, thesystem scores each paragraph by counting in thisparagraph the number of occurrences of wordswhich also appear in the question (using thestemmed form of words).
Here 'paragraph' means ablock of text separated by one newline, not two asin Okapi sense.
Note that texts in the Bell Canadacollection contain a lot of short and emptyparagraphs.
The candidate passage is extracted bytaking the five consecutive paragraphs which havethe highest score sum.
However, if the document is"small", i.e.
contains less than 2000 characters, theentire document is taken as the candidate and itsscore is the sum of scores of all paragraphs.This choice seemed unfair to previousexperiments because about 60% of the collectionare such small documents.
However, we decided tohave a more realistic notion of answer candidateswhich reflects the nature of the collection and ofour current task: in fact, those small documents areoften dedicated to a very specific topic, and itseems necessary to present its contents in itsentirety to any related question for reasons ofunderstandability, or because of importantadditional information in the document.
Also, asize of 2000 characters (which are normally 70%of a page) seems acceptable for a humanjudgement in the scenario of semi-automaticsystems.4Let's call the score calculated as aboveOccurrence_score.
We also considered the role ofthe rank of the document in the list returned by thequestion-document mapping.
The final scoreformula is as follows:(3) Score_of_candidate = RC ?
(21 -Document_Rank) + Occurrence_scoreThe portion (21 - Document_Rank) guaranteesthat high-rank documents contribute high scores.That portion is always positive because weretained no more than 20 documents for everyquestion.
RC is a coefficient representing theimportance of the document rank.
Due to timelimit ?
judgement of candidates has to be donemanually and is very time consuming, we carriedout the experiment with only RC=0, 1, 1.5, and 2,and achieved the best results with RC=1.5.Results (Tables 9 and 10) show that except thecase of n=1 in the test set, the new systemperforms well in precision.
This might beexplained partly because it tolerates largercandidates than previous experiments.
Howeverwhat is interesting here is that the engine is verysimple but efficient because it does searching on awell selected and very small document subset.n 1 2 3 4 5Q(n) 42 55 60 60 61?Q(n) 22 29 32 28 22%?Q(n) 110% 112% 114% 88% 56%Table 9: Best results of the specific engine onthe training set.n 1 2 3 4 5Q(n) 23 37 41 42 42?Q(n) 5 14 13 13 10%?Q(n) 27.8% 60.9% 46.4% 44.8% 31.3%Table 10: Results of the specific engine on thetest set.4 In fact, candidates returned by Okapi are notuniform in length.
Some are very short (e.g.
one line),some are very long (more than 2000 characters).7 Second Approach Revisited: ExtendingAnswer CandidatesThe previous experiment has shown thatextending the size of answer candidates can greatlyease the task.
This can be considered as anothermethod belonging to the second approach ?
that ofimproving precision performance by improving theresults returned by the IR engine.
To be fair, it maybe necessary to see how precision performancewill be improved if this extending is used in otherexperiments.
We did two small experiments.
In thefirst one, any candidates returned by Okapi (cf.Tables 1 and 2) which came from a document ofless than 2000 characters were extended into theentire document.
Table 11 shows thatimprovements are not as good as those obtained byother methods.n 1 2 3 4 5Q(n) - A 24 32 35 39 47?Q(n) 4 6 7 7 8%?Q(n) 20% 23.1% 25% 21.9% 20.5%Q(n) - B 20 27 32 34 37?Q(n) 2 4 4 5 5%?Q(n) 11.1% 17.4% 14.3% 17.2% 15.6%Table 11: Results of extending Okapi candidateson the training set (A) and test set (B).In the second experiment, we similarly extendedcandidates returned by the two-level search process"mapping-then-Okapi" in Section 5.
Improvements(Table 12) seem comparable to those of theexperiment in Section 5 (Tables 7 and 8), but lessgood than those of experiments in Sections 4.2 and6.
The two experiments of this section suggest thatextending candidates helps improve the precision,but not so much unless it is combined with othermethods.
We have not yet, however, carried outexperiments of combining candidate extendingwith re-ranking.n 1 2 3 4 5Q(n) - A 25 43 48 57 60?Q(n) 5 17 20 25 21%?Q(n) 25% 65.4% 71.4% 78.1% 53.8%Q(n) - B 24 31 32 38 41?Q(n) 6 8 4 9 9%?Q(n) 25% 34.8% 14.3% 31% 23.1%Table 12: Results of extending two-level searchcandidates on the training set (A) and test set (B).8 Discussions and ConclusionsRDQA, working on small document collectionsand restricted subjects, seems to be a task no lessdifficult than open-domain QA.
Due to candidatescarcity, the precision performance of a RDQAsystem, and in particular that of its IR module,becomes a problematic issue.
It affects seriouslythe entire success of the system, because if most ofthe retrieved candidates are incorrect, it ismeaningless to apply further techniques of QA torefine the answers.In this paper, we have discussed several methodsto improve the precision performance of the IRmodule.
They include the use of domain-specificterminology to rearrange the candidate list and tobetter characterize the question-documentrelevance relationship.
Once this relationship hasbeen well established, one can expect to obtain asmall set of (almost) all relevant documents for agiven question, and use this to guide the IR enginein a two-level search strategy.Also, long and complex answers may be acommon characteristic of RDQA systems.
Beingaware of this, one can design appropriate systemswhich are more tolerant on answer size to achievea higher precision, and to avoid the need ofexpanding a short but insufficient answer into acomplete one.
However, what a good answershould be is still an open question, which wouldneed a lot more study to clarify.We have also presented applications of thesemethods in the real QA system for Bell Canada.Good improvements achieved compared to resultsof the original IR module show that these methodsare applicable and effective.Many other problems on the precisionperformance of a RDQA system have not beentackled in this paper.
Some of them relate to thefree form of the questions: how to identify thecategory of the question (e.g.
the mapping 'Who' ?Person, 'When' ?
Time, 'How many' ?
Quantity,etc.
), how to analyze the question into pragmaticparts (pre-suppositions, problem context, questionfocus), etc.
Certainly, they are also problems ofopen-domain QA if one wants to go further thanpre-defined question pattern tasks.9 AcknowledgementsThis project was funded by Bell UniversityLaboratories (BUL) and the Canada NaturalScience and Engineering Research Council(NSERC).ReferencesBeaulieu M., M. Gatford, X. Huang, S.E.Robertson, S. Walker, P. Williams (1995).
Okapiat TREC-3.
In: Overview of the Third TextREtrieval Conference (TREC-3).
Edited by D.K.Harman.
Gaithersburg, MD: NIST, April 1995.Brown, J., Burton, R. (1975).
Multiplerepresentations of knowledge for tutorialreasoning.
In Bobrow and Collins (Eds),Representation and Understanding.
AcademicPress, New York.Buchholz, S., Daelemans, W. (2001).
ComplexAnswers: A Case Study using a WWW QuestionAnswering System.
Natural LanguageEngineering, 7(4), 2001.Doan-Nguyen, H., Kosseim, L. (2004).
Improvingthe Precision of a Closed-Domain Question-Answering System with Semantic Information.Proceedings of RIAO (Recherche d'InformationAssist?e par Ordinateur (Computer AssistedInformation Retrieval)) 2004.
Avignon, France.pp.
850-859.Green, W., Chomsky, C., Laugherty, K. (1961).BASEBALL: An automatic question answerer.Proceedings of the Western Joint ComputerConference, pp.
219-224.Harabagiu, S., D. Moldovan, M. Pasca, R.Mihalcea, M. Surdeanu, R. Bunescu, R. G?rju, V.Rus, P. Morarescu (2000).
FALCON: BoostingKnowledge for Answer Engines.
Proceedings ofthe Ninth Text REtrieval Conference (TREC2000).Harabagiu, S., D. Moldovan, M. Pasca, M.Surdeanu, R. Mihalcea, R. Girju, V. Rus, F.Lactusu, P. Morarescu, R. Bunescu (2001).Answering Complex, List and Context Questionswith LCC's Question-Answering Server.Proceedings of the Tenth Text REtrievalConference (TREC 2001).Kowalski, G. (1997).
Information RetrievalSystems ?
Theory and Implementation.
KluwerAcademic Publishers, Boston/Dordrecht/London.Light, M., Mann, G., Riloff, E., Breck, E. (2001).Analyses for Elucidating Current QuestionAnswering Technology.
Natural LanguageEngineering, 7(4), 2001.Lin, J., Quan, D., Sinha, V., Bakshi, K., Huynh, D.,Katz, B., Karger, D. (2003).
The Role of Contextin Question Answering Systems.
Proceedings ofthe 2003 Conference on Human Factors inComputing Systems (CHI 2003), April 2003, FortLauderdale, Florida.TREC (2002).
Proceedings of The Eleventh TextRetrieval Conference.
NIST Special Publication:SP 500-251.
E. M. Voorhees and L. P. Buckland(Eds).Woods W. A.
(1973).
Progress in natural languageunderstanding: An application to lunar geology.AFIPS Conference Proceedings, Vol.
42, pp.441-450.
