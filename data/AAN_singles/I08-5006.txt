Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33?40,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingLanguage Independent Named Entity Recognition in  Indian LanguagesAsif Ekbal, Rejwanul Haque, Amitava Das, Venkateswarlu Pokaand Sivaji BandyopadhyayDepartment of Computer Science and EngineeringJadavpur UniversityKolkata-700032, Indiaasif.ekbal@gmail.com, rejwanul@gmail.com,amit_santu_kuntal@yahoo.com, venkat.ju@gmail.com andsivaji_cse_ju@yahoo.comAbstractThis paper reports about the developmentof a Named Entity Recognition (NER) sys-tem for South and South East Asian lan-guages, particularly for Bengali, Hindi, Te-lugu, Oriya and Urdu as part of theIJCNLP-08 NER Shared Task1.
We haveused the statistical Conditional RandomFields (CRFs).
The system makes use ofthe different contextual information of thewords along with the variety of featuresthat are helpful in predicting the variousnamed entity (NE) classes.
The system usesboth the language independent as well aslanguage dependent features.
The languageindependent features are applicable for allthe languages.
The language dependentfeatures have been used for Bengali andHindi only.
One of the difficult tasks ofIJCNLP-08 NER Shared task was to iden-tify the nested named entities (NEs) thoughonly the type of the maximal NEs weregiven.
To identify nested NEs, we haveused rules that are applicable for all thefive languages.
In addition to these rules,gazetteer lists have been used for Bengaliand Hindi.
The system has been trainedwith Bengali (122,467 tokens), Hindi(502,974 tokens), Telugu (64,026 tokens),Oriya (93,173 tokens) and Urdu (35,447tokens) data.
The system has been testedwith the 30,505 tokens of Bengali, 38,708tokens of Hindi, 6,356 tokens of Telugu,1http://ltrc.iiit.ac.in/ner-ssea-0824,640 tokens of Oriya and 3,782 tokens ofUrdu.
Evaluation results have demonstratedthe highest maximal F-measure of 53.36%,nested F-measure of 53.46% and lexical F-measure of 59.39% for Bengali.1 IntroductionNamed Entity Recognition (NER) is an impor-tant tool in almost all Natural Language Proc-essing (NLP) application areas.
Proper identifi-cation and classification of named entities arevery crucial and pose a very big challenge tothe NLP researchers.
The level of ambiguity innamed entity recognition (NER) makes it diffi-cult to attain human performance.NER has drawn more and more attentionfrom the named entity (NE) tasks (Chinchor95; Chinchor 98) in Message UnderstandingConferences (MUCs) [MUC6; MUC7].
Theproblem of correct identification of named enti-ties is specifically addressed and benchmarkedby the developers of Information ExtractionSystem, such as the GATE system (Cunning-ham, 2001).
NER also finds application inquestion-answering systems (Maldovan et al,2002) and machine translation (Babych andHartley, 2003).The current trend in NER is to use the ma-chine-learning approach, which is more attrac-tive in that it is trainable and adoptable and themaintenance of a machine-learning system ismuch cheaper than that of a rule-based one.The representative machine-learning ap-proaches used in NER are HMM (BBN?s Iden-tiFinder in (Bikel, 1999)), Maximum Entropy33(New York University?s MENE in (Borthwick,1999)), Decision Tree (New York University?ssystem in (Sekine 1998), SRA?s system in(Bennet, 1997) and Conditional Random Fields(CRFs) (Lafferty et al, 2001; McCallum andLi, 2003).There is no concept of capitalization in Indianlanguages (ILs) like English and this fact makesthe NER task more difficult and challenging inILs.
There has been very little work in the area ofNER in Indian languages.
In Indian languages par-ticularly in Bengali, the work in NER can be foundin (Ekbal and Bandyopadhyay, 2007a) and  (Ekbaland Bandyopadhyay, 2007b).
These two systemsare based on the pattern directed shallow parsingapproach.
An HMM-based NER in Bengali can befound in (Ekbal et al, 2007c).
Other than Bengali,the work on NER can be found in (Li andMcCallum, 2004) for Hindi.
This system is basedon CRF.In this paper, we have reported a named entityrecognition system for the south and south eastAsian languages, particularly for Bengali, Hindi,Telugu, Oriya and Urdu.
Bengali is the seventhpopular language in the world, second in India andthe national language of Bangladesh.
Hindi is thethird popular language in the world and the na-tional language of India.
Telugu is one of the popu-lar languages and predominantly spoken in thesouthern part of India.
Oriya and Urdu are theother two popular languages of India and widelyused in the eastern and the northern part, respec-tively.
The statistical Conditional Random Field(CRF) model has been used to develop the system,as it is more efficient than HMM to deal with thenon-independent and diverse overlapping featuresof the highly inflective Indian languages.
We haveused a fine-grained named entity tagset2, defined aspart of the IJCNLP-08 NER Shared Task forSSEA.
The system makes use of the different con-textual information of the words along with thevariety of orthographic word level features that arehelpful in predicting the various named entityclasses.
In this work, we have considered languageindependent features as well as the language de-pendent features.
Language independent featuresinclude the contextual words, prefix and suffix in-formation of all the words in the training corpus,several digit features depending upon the presence2http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3and/or the number of digits in a token and the fre-quency features of the words.
The system consid-ers linguistic features particularly for Bengali andHindi.
Linguistic features of Bengali include theset of known suffixes that may appear with namedentities, clue words that help in predicating the lo-cation and organization names, words that help torecognize measurement expressions, designationwords that help in identifying person names, thevarious gazetteer lists like the first names, middlenames, last names, location names and organiza-tion names.
As part of linguistic features for Hindi,the system uses only the lists of first names, middlenames and last names along with the list of wordsthat helps to recognize measurements.
No linguis-tic features have been considered for Telugu, Oriyaand Urdu.
It has been observed from the evaluationresults that the use of linguistic features improvesthe performance of the system.
A number of ex-periments have been carried out to find out thebest-suited set of features for named entity recog-nition in Bengali, Hindi, Telugu, Oriya and Urdu.2 Conditional Random FieldsConditional Random Fields (CRFs) (Lafferty et al,2001) are undirected graphical models, a specialcase of which corresponds to conditionally trainedprobabilistic finite state automata.
Beingconditionally trained, these CRFs can easilyincorporate a large number of arbitrary, non-independent features while still having efficientprocedures for non-greedy finite-state inferenceand training.
CRFs have shown success in varioussequence modeling tasks including noun phrasesegmentation (Sha and Pereira, 2003) and tableextraction (Pinto et al, 2003).CRFs are used to calculate the conditionalprobability of values on designated output nodesgiven values on other designated input nodes.
Theconditional probability of a state sequence1, 2, , TS s s s given an observationsequence 1 2,, ....., )TO o o o  is calculated as:1 ,11( | ) exp( ( , , )),Tk k t tot kP s o f s s o tZwhere1 ,( , , )k t tf s s o t is a feature function whose weightk is to be learned via training.
The values of thefeature functions may range between ..... ,but typically they are binary.
To make all34conditional probabilities sum up to 1, we mustcalculate the normalizationfactor, 0 1 ,1exp( ( , , ))Ts k k t tt kZ f s s o t ,which, as in HMMs, can be obtained efficiently bydynamic programming.To train a CRF, the objective function to bemaximized is the penalized log-likelihood of thestate sequences given observation sequences:2( ) ( )21log( ( | ))2Ni iki kL P s o ,where, {( ) ( ),i io s } is the labeled trainingdata.
The second sum corresponds to a zero-mean,2-variance Gaussaian prior over parameters,which facilitates optimization by making the like-lihood surface strictly convex.
Here, we set pa-rametersto maximize the penalized log-likelihood using Limited-memory BFGS (Sha andPereira, 2003), a quasi-Newton method that is sig-nificantly more efficient, and which results in onlyminor changes in accuracy due to changes in .When applying CRFs to the named entityrecognition problem, an obsevation sequence is atoken of a sentence or document of text and thestate sequence is its corresponding label sequence.While CRFs generally can use real-valuedfunctions, in our experiments maximum of thefeatures are binary.
A feature function1 ,( , , )k t tf s s o t has a value of 0 for most cases andis only set to be 1, when 1,t ts s are certain statesand the observation has certain properties.
Wehave used the C++based OpenNLP CRF++ pack-age3, a simple, customizable, and open source im-plementation of Conditional Random Fields(CRFs) for segmenting /labeling sequential data.3 Named Entity Recognition in IndianLanguagesNamed Entity Recognition in Indian languages(ILs) is difficult and challenging as capitalizationis not a clue in ILs.
The training data were pro-vided for five different Indian languages, namelyBengali, Hindi, Telugu, Oriya and Urdu in ShaktiStandard Format4.
The training data in all the lan-3http://crfpp.sourceforge.net4http://shiva.iiit.ac.in/SPSAL 2007/ssf.htmlguages were annotated with the twelve NE tags, asdefined for the IJCNLP-08 NER shared task taget5.Only the maximal named entities and not the inter-nal structures of the entities were annotated in thetraining data.
For example, mahatma gandhi roadwas annotated as location and assigned the tag?NEL?
even if mahatma and gandhi  are namedentity title person (NETP) and person name (NEP)respectively, according to the IJCNLP-08 sharedtask tagset.
These internal structures of the entitieswere to be identified during testing.
So, mahatmagandhi road will be tagged as mahatma /NETPgandhi/NEP road/NEL.
The structure of the taggedelement using the SSF form will be as follows:1 (( NP <ne=NEL>1.1 (( NP <ne=NEP>1.1.1 (( NP  <ne=NETP>1.1.1.1 mahatma))1.1.2 gandhi))1.2 road))3.1 Training Data Preparation for CRFTraining data for all the languages required somepreprocessing in order to use in the ConditionalRandom Field framework.
The training data issearched for the multiword NEs.
Each componentof the multiword NE is searched in the training setto find whether it occurs as a single-word NE.
Theconstituent components are then replaced by theirNE tags (NE type of the single-word NE).
For ex-ample, mahatma gandhi road/NEL will be taggedas mahatma/NETP gandhi/NEP road/NEL if theinternal components are found to appear with theseNE tags in the training set.
Each component of amultiword NE is also checked whether the compo-nent is made up of digits only.
If a component ismade up digits only, then it is assigned the tag?NEN?.
Various gazetteers for Bengali and Hindihave been also used in order to identify the internalstructure of the NEs properly.
The list of gazet-teers, which have been used in preparing the train-ing data, is shown in Table 1.The individual components (not occurring as asingle-word NE in the training data) of a multi-word NE are searched in the gazetteer lists and5http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=335assigned the appropriate NE tags.
Other than NEsare marked with the NNE tags.
The procedure isgiven below:Gazetteer list Number of entriesFirst person name in Ben-gali27,842Last person name in Ben-gali5,288Middle name in Bengali 1,491Person name designationin Bengali947Location name in Bengali 7,870First person name in Hindi 1,62,881Last person name in Hindi 3,573Middle name in Hindi 450Cardinals in Bengali,Hindi and Telugu100Ordinals in Bengali, Hindiand Telugu65Month names in Bengali,Hindi and Telugu24Weekdays in Bengali,Hindi and Telugu14Words that denote meas-urement in Bengali, Hindiand Telugu52Table 1.
Gazetteer lists used during training datapreparationStep 1: Search the multiword NE in the trainingdataStep 2: Extract each component from the mult-word NE.Step 3: Check whether the constituent individualcomponent (except the last one) appears in thetraining data as a single-word NE.Step 4: If the constituent NE appears in the trainingdata as a single-word NE thenStep 4.1: Assign the NE tag, extracted from thesingle-word NE, to the component of the multi-word NE.elseStep 4.2: Search the component in the gazetteerlists and assign the appropriate NE tag.Step 4.2.1: If the component is not found to appearin the gazetteer list then assign the NE tag of themaximal NE to the individual component.For example, if mahatma gandhi road is taggedas NEL, i.e., mahatma gandhi road/NEL then eachcomponent except the last one (road ) of this mult-word NE is searched in the training set to look forit?s appearance (Step 3).
Gazetteer lists aresearched in case the component is not found in thetraining set (Step 4.2).
If the components are foundeither in the training set or in the gazetteer list,then mahatma gandhi road/NEL will be tagged as:mahatma/NETP gandhi/NEP road/NEL.3.2 Named Entity FeaturesFeature selection plays a crucial role in CRFframework.
Experiments were carried out to findout most suitable features for NE tagging task.
Themain features for the NER task have been identi-fied based on the different possible combination ofavailable word and tag context.
The features alsoinclude prefix and suffix for all words.
The termprefix/suffix is a sequence of first/last few charac-ters of a word, which may not be a linguisticallymeaningful prefix/suffix.
The use of prefix/suffixinformation works well for highly inflected lan-guages like the Indian languages.
In addition, vari-ous gazetteer lists have been developed to use inthe NER task particularly for Bengali and Hindi.We have considered different combination fromthe following set for inspecting the best feature setfor the NER task:F={1 1,..., , , ,...,i m i i i i nw w w w w , |prefix| n,|suffix| n, previous NE tag, POS tags, First word,Digit information, Gazetteer lists}Following is the details of the set of featuresthat were applied to the NER task:Context word feature: Previous and next wordsof a particular word might be used as a feature.
Wehave considered the word window of size five, i.e.,previous and next two words from the current wordfor all the languages.Word suffix: Word suffix information is helpfulto identify NEs.
A fixed length word suffix of thecurrent and surrounding words might be treated asfeature.
In this work, suffixes of length up to threethe current word have been considered for all thelanguages.
More helpful approach is to modify thefeature as binary feature.
Variable length suffixesof a word can be matched with predefined lists ofuseful suffixes for different classes of NEs.
ForBengali, we have considered the different suffixesthat may be particularly helpful in detecting person(e.g., -babu, -da, -di etc.
).36Word prefix: Prefix information of a word is alsohelpful.
A fixed length prefix of the current and thesurrounding words might be treated as features.Here, the prefixes of length up to three have beenconsidered for all the language.Rare word: The lists of most frequently occurringwords in the training sets have been calculated forall the five languages.
The words that occur morethan 10 times are considered as the frequently oc-curring words in Bengali and Hindi.
For Telugu,Oriya and Urdu, the cutoff frequency was chosento be 5.
Now, a binary feature ?RareWord?
is de-fined as: If current word is found to appear in thefrequent word list then it is set to 1; otherwise, setto 0.First word: If the current token is the first word ofa sentence, then this feature is set to 1.
Otherwise,it is set to 0.Contains digit: For a token, if it contains digit(s)then the feature ?ContainsDigit?
is set to 1.
Thisfeature is helpful for identifying the numbers.Made up of four digits: For a token if all the char-acters are digits and having 4 digits then the fea-ture ?FourDigit?
is set to 1.
This is helpful in iden-tifying the time (e.g., 2007sal) and numerical (e.g.,2007) expressions.Made up of two digits: For a token if all the char-acters are digits and having 2 digits then the fea-ture ?TwoDigit?
is set to 1.
This is helpful for iden-tifying the time expressions (e.g., 12 ta, 8 am, 9 pm)in general.Contains digits and comma: For a token, if it con-tains digits and commas then the feature ?Con-tainsDigitsAndComma?
is set to 1.
This feature ishelpful in identifying named entity measurementexpressions (e.g., 120,45,330 taka) and numericalnumbers (e.g., 120,45,330)Contains digits and slash: If the token containsdigits and slash then the feature ?ContainsDigi-tAndslash?
is set to 1.
This helps in identifyingtime expressions (e.g., 15/8/2007).Contains digits and hyphen: If the token containsdigits and hyphen then the feature ?ContainsDigit-sAndHyphen?
is set to 1.
This is helpful for theidentification of time expressions (e.g., 15-8-2007).Contains digits and period: If the token containsdigits and periods then the feature ?ContainsDigit-sAndPeriod?
is set to 1.
This helps to recognizenumerical quantities (e.g., 120453.35) and meas-urements (e.g., 120453.35 taka).Contains digits and percentage: If the token con-tains digits and percentage symbol then the feature?ContainsDigitsAndPercentage?
is set to 1.
Thishelps to recognize measurements (e.g., 120%).Named Entity Information: The NE tag of theprevious word is also considered as the feature, i.e.,the combination of the current and the previousoutput token has been considered.
This is the onlydynamic feature in the experiment.Gazetteer Lists: Various gazetteer lists have beencreated from a tagged Bengali news corpus (Ekbaland Bandyopadhyay, 2007d) for Bengali.
The first,last and middle names of person for Hindi havebeen created from the election commission data6.The person name collections had to be processed inorder to use it in the CRF framework.
The simplestapproach of using these gazetteers is to comparethe current word with the lists and make decisions.But this approach is not good, as it can?t resolveambiguity.
So, it is better to use these lists as thefeatures of the CRF.
If the current token is in a par-ticular list, then the corresponding feature is set to1 for the current/previous/next token; otherwise,set to 0.
The list of gazetteers is shown in Table 2.3.3 Nested Named Entity IdentificationOne of the important tasks of the IJCNLP-NERshared task was to identify the internal named enti-ties within the maximal NEs.
In the training data,only the type of the maximal NEs were given.
Inorder to identify the internal NEs during testing,we have defined some rules.
After testing the un-annotated test data with the CRF based NER sys-tem, it is searched to find the sequence of NE tags.The last NE tag in the sequence is assigned as theNE tag of the maximal NE.
The NE tags of theconstituent NEs may either be changed or may notbe changed.
The NE tags are changed with the helpof rules and various gazetteer lists.
We identifiedNEM (Named entity measurement), NETI (Namedentity time expressions), NEO (Named entity or-ganization names), NEP (Named entity personnames) and NEL (Named entity locations) to bethe potential NE tags, where nesting could occur.A NEM expression may contain NEN, an NETImay contain NEN, an NEO may contain NEP/NEL, an NEL may contain NEP/NETP/NED andan NEP may contain NEL expressions.
The nested6http://www.eci.gov.in/DevForum/Fullname.asp37NEN tags could be identified by simply checkingwhether it contains digits only and checking thelists of cardinal and ordinal numbers.Gazetteer  Numberof entriesFeature Descrip-tionsDesignationwords in Bengali947 ?Designation?
set to1, otherwise 0Organizationnames in Bengali2, 225 ?Organization?
setto 1, otherwise 0.Organizationsuffixes in Ben-gali94 ?OrgSuffix?
set to1, otherwise 0Person prefix forBengali245 ?PersonPrefix?
setto 1, otherwise setto 0First personnames in Bengali27,842 ?FirstName?
set to1, otherwise 0Middle names inBengali1,491 ?MiddleName?
setto 1, otherwise 0Surnames inBengali5,288 ?SurName?
set to 1,otherwise 0Common loca-tion word inBengali75 ?CommonLocation?set 1, otherwise 0Action verb inBengali215 ?ActionVerb?
set to1, otherwise 0First personnames in Hindi1,62,881 ?FirstName?
set to1, otherwise 0Middle personnames in Hindi450 ?MiddleName?
setto 1, otherwise 0Last personnames in Hindi3,573 ?SurName?
set to 1,otherwise 0Location namesin Bengali7,870 ?LocationName?set to 1, otherwise0Week days inBengali, Hindiand Telugu14 ?WeekDay?
set to1, otherwise 0Month names inBengali, Hindiand Telugu24 ?MonthName?
setto 1, otherwise 0Measurements inBengali, Hindiand Telugu52 ?Measurement?
setto 1, otherwise 0.Table 2.
Named entity gazetteer listThe procedure for identifying the nested NEs areshown below:Step1: Test the unannotated test set.Step 2: Look for the sequence of NE tags.Step 3: All the words in the sequence will belongto a maximal NE.Step 4: Assign the last NE tag in the sequence tothe maximal NE.Step 5: The test set is searched to look whethereach component word appears with a NE tag.Step 6: Assign the particular NE tag to the compo-nent if it appears in the test set with that NE tag.Otherwise, search the gazetteer lists as shown inTables 1-2 to assign the tag.4 EvaluationThe evaluation measures used for all the five lan-guages are precision, recall and F-measure.
Thesemeasures are calculated in three different ways:(i).
Maximal matches: The largest possiblesnamed entities are matched with the reference data.(ii).
Nested matches: The largest possible aswell as nested named entities are matched.(iii).
Maximal lexical item matches: The lexicalitems inside the largest possible named entities arematched.(iv).
Nested lexical item matches: The lexicalitems inside the largest possible as well as nestednamed entities are matched.5 Experimental ResultsThe CRF based NER system has been trained andtested with five different Indian languages namely,Bengali, Hindi, Telugu, Oriya and Urdu data.
Thetraining and test sets statistics are presented in Ta-ble 3.
Results of evaluation as explained in theprevious section are shown in Table 4.
The F-measures for the nested lexical match are alsoshown individually for each named entity tag sepa-rately in Table 5.Experimental results of Table 4 show that theCRF based NER system performs best for Bengaliwith maximal F-measure of 55.36%, nested F-measure of 61.46% and lexical F-measure 59.39%.The system has demonstrated the F-measures of35.37%, 36.75% and 33.12%, respectively formaximal, nested and lexical matches.
The systemhas shown promising precision values for Hindi.But due to the low recall values, the F-measuresget reduced.
The large difference between the re-call and precision values in the evaluation resultsof Hindi indicates that the system is not able toretrieve a significant number of NEs from the test38data.
In comparison to Hindi, the precision valuesare low and the recall values are high for Bengali.It can be decided from the evaluation results thatsystem retrieves more NEs in Bengali than Hindibut involves more errors.
The lack of features inOriya, Telugu and Urdu might be the reason be-hind their poor performance.LanguageNumber oftokens in thetraining setNumber of to-kens in the testsetBengali 122,467 30,505Hindi 502,974 38,708Telugu 64,026 6,356Oriya 93,173 24,640Urdu 35,447 3,782Table 3: Training and Test Sets StatisticsTag Bengali Hindi Oriya Telugu UrduNEP 85.68 21.43 43.76 1.9 7.69NED 35.9 38.70 NF NF NFNEO 52.53 NF 5.60 NF 22.02NEA 26.92 30.77 NF NF NFNEB NF NF NF NF NFNETP 61.44 NF 12.55 NF NFNETO 45.98 NF NF NF NFNEL 80.00 22.70 31.49 0.73 50.14NETI 53.43 49.60 27.08 7.64 49.28NEN 30.12 85.40 9.19 9.16 NFNEM 79.08 36.64 7.56 NF 79.27NETE 18.06 1.64 NF 5.74 NFTable 4.
Evaluation for Specific NE Tags (F-Measures for nested lexical match) [NF: Nothingfound]Experimental results of Table 5 show the F-measures for the nested lexical item matches forindividual NE tags.
For Bengali, the system hasshown reasonably high F-measures for NEP, NELand NEM tags and medium F-measures for NETP,NETI, NEO and NETO tags.
The overall F-measures in Bengali might have reduced due torelatively poor F-measures for NETE, NEN, NEAand NED tags.
For Hindi, the highest F-measureobtained is 85.4% for NEN tag followed by NETI,NED, NEM, NEA, NEL and NEP tags.
In somecases, the system has shown better F-measures forHindi than Bengali also.
The system has performedbetter for NEN, NED and NEA tags in Hindi thanall other languages.6 ConclusionWe have developed a named entity recognitionsystem using Conditional Random Fields for thefive different Indian languages, namely Bengali,Hindi, Telugu, Oriya and Urdu.
We have consid-ered the contextual window of size five, prefix andsuffix of length upto three of the current word, NEinformation of the previous word, different digitfeatures and the frequently occurring word lists.The system also uses linguistic features extractedfrom the various gazetteer lists for Bengali andHindi.
Evaluation results show that the system per-forms best for Bengali.
The performance of thesystem for Bengali can further be improved by in-cluding the part of speech (POS) information of thecurrent and/or the surrounding word(s).
The per-formance of the system for other languages can beimproved with the use of different linguistic fea-tures as like Bengali.The system did not perform as expected due tothe problems faced during evaluation regarding thetokenization.
We have tested the system for Ben-gali with 10-fold cross validation and obtained im-pressive results.ReferencesBabych, Bogdan, A. Hartley.
Improving machine trans-lation quality with automatic named entity recogni-tion.
In Proceedings of EAMT/EACL 2003 Workshopon MT and other language technology tools, 1-8,Hungary.Bennet, Scott W.; C. Aone; C. Lovell.
1997.
Learning toTag Multilingual Texts Through Observation.
InProceedings of EMNLP, 109-116,  Rhode Island.Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel.1999.
An Algorithm that Learns What?s in Name.Machine Learning (Special Issue on NLP), 1-20.Bothwick, Andrew.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. Thesis,New York University.Chinchor, Nancy.
1995.
MUC-6 Named Entity TaskDefinition (Version 2.1).
MUC-6, Columbia, Mary-land.39Table 5.
Evaluation of the Five LanguagesChinchor, Nancy.
1998.
MUC-7 Named Entity TaskDefinition (Version 3.5).
MUC-7.
Fairfax, Vir-ginia.Cunningham, H. 2001.
GATE: A general architecturefor text engineering.
Comput.
Humanit.
(36), 223-254.Ekbal, Asif, and S. Bandyopadhyay.
2007a.
PatternBased Bootstrapping Method for Named EntityRecognition.
In Proceedings of 6thInternationalConference on Advances in Pattern Recognition,Kolkata,    India, 349-355.Ekbal, Asif, and S. Bandyopadhyay.
2007b.
LexicalPattern Learning from Corpus Data for Named En-tity Recognition.
In Proceedings of the 5thInterna-tional Conference on Natural Language Process-ing, Hyderabad, India, 123-128.Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.2007c.
Named Entity Recognition and Translitera-tion in Bengali.
Named Entities: Recognition,Classification and Use, Special Issue of Lingvisti-cae Investigationes Journal, 30:1 (2007), 95-114.Ekbal, Asif, and S. Bandyopadhyay.
2007d.
A Web-based Bengali News Corpus for Named EntityRecognition.
Language Resources and EvaluationJournal (Accepted)Lafferty, J., McCallum, A., and Pereira, F. 2001.Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
InProc.
of 18thInternational Conference on Machinelearning.Li, Wei and Andrew McCallum.
2003.
Rapid Devel-opment of Hindi Named Entity Recognition UsingConditional Random Fields and Feature Induc-tions, ACM TALIP, 2(3), (2003), 290-294.McCallum, A.; W. Li.
2003.
Early Results for NamedEntity Recognition with Conditional RandomFields, Feature Induction and Web-EnhancedLexicons.
In Proceedings CoNLL-03, Edmanton,Canada.Moldovan, Dan I., Sanda M. Harabagiu, RoxanaGirju, P. Morarescu, V. F. Lacatusu, A. Novischi,A.
Badulescu, O. Bolohan.
2002.
LCC Tools forQuestion Answering.
In Proceedings of the TREC,Maryland, 1-10.Pinto, D., McCallum, A., Wei, X., and Croft, W. B.2003.
Table extraction using conditional randomfields.
In Proceedings of SIGIR 03 Conference,Toronto, Canada.Sekine, Satoshi.
1998.
Description of the JapaneseNE System Used for MET-2, MUC-7, Fairfax,Virginia.Sha, F. and Pereira, F. 2003.
Shallow parsing withconditional random fields.
In Proceedings of Hu-man Language Technology, NAACL.MeasurePrecision Recall F-measureLanguage PmPnPlRmRnRlFmFnFlBengali 51.63 47.74 52.90 59.60 61.46 67.71 55.36 61.46 59.39Hindi 71.05 76.08 80.59 23.54 24.23 20.84 35.37 36.75 33.12Oriya 27.12 27.18 50.40 12.88 10.53 20.07 17.47 15.18 28.71Telugu 1.70 2.70 8.10 0.538 0.539 3.34 0.827 0.902 4.749Urdu 49.16 48.93 54.45 21.95 20.15 26.36 30.35 28.55 35.52M: Maximal,  n: Nested, l: Lexical40
