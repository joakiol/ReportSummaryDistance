Improving Data Driven Wordclass Taggingby System CombinationHans  van  Hal terenDept.
of Language and SpeechUniversity of Ni jmegenP.O.
Box 91036500 HD NijmegenThe Netherlandshvh@let.kun.nlJakub  Zavre l ,  Wa l te r  Dae lemansDept.
of Computat iona l  LinguisticsTi lburg UniversityP.O.
Box 901535000 LE Ti lburgThe NetherlandsJakub.Zavrel@kub.nl ,Walter.Daelemans@kub.nlAbst rac tIn this paper we examine how the differences inmodelling between different data driven systemsperforming the same NLP task can be exploitedto yield a higher accuracy than the best indi-vidual system.
We do this by means of an ex-periment involving the task of morpho-syntacticwordclass tagging.
Four well-known tagger gen-erators (Hidden Markov Model, Memory-Based,Transformation Rules and Maximum Entropy)are trained on the same corpus data.
Af-ter comparison, their outputs are combined us-ing several voting strategies and second stageclassifiers.
All combination taggers outperformtheir best component, with the best combina-tion showing a 19.1% lower error rate than thebest individual tagger.In t roduct ionIn all Natural Language Processing (NLP)systems, we find one or more languagemodels which are used to predict, classifyand/or interpret language related observa-tions.
Traditionally, these models were catego-rized as either rule-based/symbolic or corpus-based/probabilistic.
Recent work (e.g.
Brill1992) has demonstrated clearly that this cat-egorization is in fact a mix-up of two distinctCategorization systems: on the one hand there isthe representation used for the language model(rules, Markov model, neural net, case base,etc.)
and on the other hand the manner inwhich the model is constructed (hand craftedvs.
data driven).Data driven methods appear to be the morepopular.
This can be explained by the fact that,in general, hand crafting an explicit model israther difficult, especially since what is beingmodelled, natural language, is not (yet) well-understood.
When a data driven method isused, a model is automatically earned from theimplicit structure of an annotated training cor-pus.
This is much easier and can quickly leadto a model which produces results with a 'rea-sonably' good quality.Obviously, 'reasonably good quality' is notthe ultimate goal.
Unfortunately, the qualitythat can be reached for a given task is limited,and not merely by the potential of the learn-ing method used.
Other limiting factors are thepower of the hard- and software used to imple-ment the learning method and the availability oftraining material.
Because of these limitations,we find that for most tasks we are (at any pointin time) faced with a ceiling to the quality thatcan be reached with any (then) available ma-chine learning system.
However, the fact thatany given system cannot go beyond this ceilingdoes not mean that machine learning as a wholeis similarly limited.
A potential loophole is thateach type of learning method brings its own 'in-ductive bias' to the task and therefore differentmethods will tend to produce different errors.In this paper, we are concerned with the ques-tion whether these differences between modelscan indeed be exploited to yield a data drivenmodel with superior performance.In the machine learning literature this ap-proach is known as ensemble, stacked, or com-bined classifiers.
It has been shown that, whenthe errors are uncorrelated to a sufficient degree,the resulting combined classifier will often per-form better than all the individual systems (Aliand Pazzani 1996; Chan and Stolfo 1995; Tumerand Gosh 1996).
The underlying assumption istwofold.
First, the combined votes will makethe system more robust to the quirks of eachlearner's particular bias.
Also, the use of infor-mation about each individual method's behav-iour in principle even admits the possibility to491fix collective rrors.We will execute our investigation by meansof an experiment.
The NLP task used in theexperiment is morpho-syntactic wordclass tag-ging.
The reasons for this choice are several.First of all, tagging is a widely researched andwell-understood task (cf.
van Halteren (ed.)1998).
Second, current performance l vels onthis task still leave room for improvement:'state of the art' performance for data driven au-tomatic wordclass taggers (tagging English textwith single tags from a low detail tagset) is 96-97% correctly tagged words.
Finally, a numberof rather different methods are available thatgenerate a fully functional tagging system fromannotated text.1 Component  taggersIn 1992, van Halteren combined a number oftaggers by way of a straightforward majorityvote (cf.
van Halteren 1996).
Since the compo-nent taggers all used n-gram statistics to modelcontext probabilities and the knowledge repre-sentation was hence fundamentally the same ineach component, he results were limited.
Nowthere are more varied systems available, a va-riety which we hope will lead to better com-bination effects.
For this experiment we haveselected four systems, primarily on the basis ofavailability.
Each of these uses different featuresof the text to be tagged, and each has a com-pletely different representation f the languagemodel.The first and oldest system uses a tradi-tional trig-ram model (Steetskamp 1995; hence-forth tagger T, for Trigrams), based on contextstatistics P(ti\[ti-l,ti-2) and lexical statisticsP(tilwi) directly estimated from relative cor-pus frequencies.
The Viterbi algorithm is usedto determine the most probable tag sequence.Since this model has no facilities for handlingunknown words, a Memory-Based system (seebelow) is used to propose distributions of po-tential tags for words not in the lexicon.The second system is the TransformationBased Learning system as described by Brill(19941; henceforth tagger R, for Rules).
This1 Brill's system is available as a collec-tion of C programs and Perl scripts atftp ://ftp.
cs.
j hu.
edu/pub/brill/Programs/RULE_BASED_TAGGER_V.
I.
14. tar.
Zsystem starts with a basic corpus annotation(each word is tagged with its most likely tag)and then searches through a space of transfor-mation rules in order to reduce the discrepancybetween its current annotation and the correctone (in our case 528 rules were learned).
Dur-ing tagging these rules are applied in sequenceto new text.
Of all the four systems, this onehas access to the most information: contextualinformation (the words and tags in a windowspanning three positions before and after thefocus word) as well as lexical information (theexistence of words formed by suffix/prefix addi-tion/deletion).
However, the actual use of thisinformation is severely limited in that the indi-vidual information items can only be combinedaccording to the patterns laid down in the ruletemplates.The third system uses Memory-Based Learn-ing as described by Daelemans et al (1996;henceforth tagger M, for Memory).
Duringthe training phase, cases containing informa-tion about the word, the context and the cor-rect tag are stored in memory.
During tagging,the case most similar to that of the focus wordis retrieved from the memory, which is indexedon the basis of the Information Gain of eachfeature, and the accompanying tag is selected.The system used here has access to informationabout the focus word and the two positions be-fore and after, at least for known words.
Forunknown words, the single position before andafter, three suffix letters, and information aboutcapitalization and presence of a hyphen or adigit are used.The fourth and final system is the MXPOSTsystem as described by Ratnaparkhi (19962;henceforth tagger E, for Entropy).
It uses anumber of word and context features rather sim-ilar to system M, and trains a Maximum En-tropy model that assigns a weighting parameterto each feature-value and combination of fea-tures that is relevant o the estimation of theprobability P(tag\[features).
A beam search isthen used to find the highest probability tag se-quence.
Both this system and Brill's system areused with the default settings that are suggestedin their documentation.2Ratnaparkhi's Java implementation of this sys-tem is available at f tp : / / f tp .c i s .upenn.edu/pub/adwait/ jmx/4922 The  dataThe data we use for our experiment consists ofthe tagged LOB corpus (Johansson 1986).
Thecorpus comprises about one million words, di-vided over 500 samples of 2000 words from 15text types.
Its tagging, which was manuallychecked and corrected, is generally accepted tobe quite accurate.
Here we use a slight adapta-tion of the tagset.
The changes are mainly cos-metic, e.g.
non-alphabetic characters uch as"$" in tag names have been replaced.
However,there has also been some retokenization: geni-tive markers have been split off and the negativemarker "n't" has been reattached.
An examplesentence tagged with the resulting tagset is:The ATI singular or pluralarticleLord NPT singular titularnounMajor NPT singular titularnounextended VBD past tense of verban AT singular articleinvitation NN singular commonnounto IN prepositionall ABN pre-quantifierthe ATI singular or pluralarticleparliamentary JJ  adjectivecandidates NNS plural commonnounSPER periodThe tagset consists of 170 different ags (in-cluding ditto tags 3) and has an average ambigu-ity of 2.69 tags per wordform.
The difficulty ofthe tagging task can be judged by the two base-line measurements in Table 2 below, represent-ing a completely random choice from the poten-tial tags for each token (Random) and selectionof the lexically most likely tag (LexProb).For our experiment, we divide the corpus intothree parts.
The first part, called Train, consistsof 80% of the data (931062 tokens), constructed3Ditto tags are used for the components of multi-token units, e.g.
if "as well as" is taken to be a coor-dination conjunction, it is tagged "as_CC-1 well_CC-2as_CC-3", using three related but different ditto tags.by taking the first eight utterances of every ten.This part is used to train the individual tag-gers.
The second part, Tune, consists of 10% ofthe data (every ninth utterance, 114479 tokens)and is used to select he best tagger parameterswhere applicable and to develop the combina-tion methods.
The third and final part, Test,consists of the remaining 10% (.115101 tokens)and is used for the final performance measure-ments of all tuggers.
Both Tune and Test con-tain around 2.5% new tokens (wrt Train) and afurther 0.2% known tokens with new tags.The data in Train (for individual tuggers)and Tune (for combination tuggers) is to be theonly information used in tagger construction:all components of all tuggers (lexicon, contextstatistics, etc.)
are to be entirely data drivenand no manual adjustments are to be done.
Thedata in Test is never to be inspected in detailbut only used as a benchmark tagging for qual-ity measurement.
43 Potential  for improvementIn order to see whether combination of the com-ponent uggers is likely to lead to improvementsof tagging quality, we first examine the resultsof the individual taggers when applied to Tune.As far as we know this is also one of the firstrigorous measurements of the relative quality ofdifferent agger generators, using a single tagsetand dataset and identical circumstances.The quality of the individual tuggers (cf.
Ta-ble 2 below) certainly still leaves room for im-provement, although tagger E surprises us withan accuracy well above any results reported sofar and makes us less confident about the gainto be accomplished with combination.However, that there is room for improvementis not enough.
As explained above, for combi-nation to lead to improvement, the componenttaggers must differ in the errors that they make.That this is indeed the case can be seen in Ta-ble 1.
It shows that for 99.22% of Tune, at leastone tagger selects the correct tag.
However, itis unlikely that we will be able to identify this4This implies that it is impossible to note if errorscounted against a tagger are in fact errors in the bench-mark tagging.
We accept hat we are measuring qualityin relation to a specific tagging rather than the linguistictruth (if such exists) and can only hope the tagged LOBcorpus lives up to its reputation.493All Taggers Correct 92.49Majority Correct (3-1,2-1-1) 4.34Correct Present, No Majority 1.37(2-2,1-1-1-1)Minority Correct (1-3,1-2-1) 1.01All Taggers Wrong 0.78Table 1: Tagger agreement on Tune.
The pat-terns between the brackets give the distributionof correct/ incorrect tags over the systems.tag in each case.
We should rather aim for op-timal selection in those cases where the correcttag is not outvoted, which would ideally leadto correct tagging of 98.21% of the words (inTune).4 S imple  Vot ingThere are many ways in which the results ofthe component taggers can be combined, select-ing a single tag from the set proposed by thesetaggers.
In this and the following sections weexamine a number of them.
The accuracy mea-surements for all of them are listed in Table 2.
5The most straightforward selection method isan n-way vote.
Each tagger is allowed to votefor the tag of its choice and the tag with thehighest number of votes is selected.
6The question is how large a vote we alloweach tagger.
The most democratic option is togive each tagger one vote (Majority).
However,it appears more useful to give more weight totaggers which have proved their quality.
Thiscan be general quality, e.g.
each tagger votes itsoverall precision (TotPrecision), or quality in re-lation to the current situation, e.g.
each taggervotes its precision on the suggested tag (Tag-Precision).
The information about each tagger'squality is derived from an inspection of its re-sults on Tune.5For any tag X, precision measures which percentageof the tokens tagged X by the tagger are also tagged X inthe benchmark and recall measures which percentage ofthe tokens tagged X in the benchmark are also tagged Xby the tagger.
When abstracting away from individualtags, precision and recall are equal and measure howmany tokens are tagged correctly; in this case we alsouse the more generic term accuracy.6In our experiment, a random selection from amongthe winning tags is made whenever there is a tie.TuneTestBaselineRandom 73.68 73.74LexProb 92.05 92.27Single TaggerT 95.94 96.08R 96.34 96.46M 96.76 96.95E 97.34 97.43S imple  VotingMajority 97.53 97.63TotPrecision 97.72 97.80TagPrecision 97.55 97.68Precision-Recall 97.73 97.84Pairwise VotingTagPair 97.99 97.92Memory-BasedTags 98.31 97.87Tags+Word 99.21 97.82Tags+Context 99.46 97.69Decision treesTags 98.08 97.78Tags+Word - -Tags+Context 98.67 97.63taggers and Table 2: Accuracy of individualcombination methods.But we have even more information on howwell the taggers perform.
We not only knowwhether we should believe what they propose(precision) but also know how often they fail torecognize the correct ag (recall).
This informa-tion can be used by forcing each tagger also toadd to the vote for tags suggested by the oppo-sition, by an amount equal to 1 minus the recallon the opposing tag (Precision-Recall).As it turns out~ all voting systems outperformthe best single tagger, E. 7 Also, the best votingsystem is the one in which the most specific in-formation is used, Precision-Recall.
However,specific information is not always superior, forTotPrecision scores higher than TagPrecision.This might be explained by the fact that recallinformation is missing (for overall performancethis does not matter, since recall is equal to pre-cision).7Even the worst combinator, Majority, is significantlybetter than E: using McNemar's chi-square, p--0.4945 Pairwise VotingSo far, we have only used information on theperformance of individual taggers.
A next stepis to examine them in pairs.
We can investigateall situations where one tagger suggests T1 andthe other T2 and estimate the probability that inthis situation the tag should actually be Tx, e.g.if E suggests DT and T suggests CS (which canhappen if the token is "that") the probabilitiesfor the appropriate tag are:CS subordinating conjunction 0.3276DT determiner 0.6207QL quantifier 0.0172WPR wh-pronoun 0.0345When combining the taggers, every taggerpair is taken in turn and allowed to vote (withthe probability described above) for each pos-sible tag, i.e.
not just the ones suggested bythe component taggers.
If a tag pair T1-T2 hasnever been observed in Tune, we fall back oninformation on the individual taggers, viz.
theprobability of each tag Tx given that the taggersuggested tag Ti.Note that with this method (and those in thenext section) a tag suggested by a minority (oreven none) of the taggers till has a chance towin.
In principle, this could remove the restric-tion of gain only in 2-2 and 1-1-1-1 cases.
Inpractice, the chance to beat a majority is veryslight indeed and we should not get our hopesup too high that this should happen very often.When used on Test, the pairwise voting strat-egy (TagPair) clearly outperforms the other vot-ing strategies, 8 but does not yet approach thelevel where all tying majority votes are handledcorrectly (98.31%).6 S tacked  c lass i f iersFrom the measurements so far it appears thatthe use of more detailed information leads to abetter accuracy improvement.
It ought there-fore to be advantageous to step away from theunderlying mechanism of voting and to modelthe situations observed in Tune more closely.The practice of feeding the outputs of a num-ber of classifiers as features for a next learnersit is significantly better than the runner-up(Precision-Recall) with p=0.is usually called stacking (Wolpert 1992).
Thesecond stage can be provided with the first leveloutputs, and with additional information, e.g.about the original input pattern.The first choice for this is to use a Memory-Based second level learner.
In the basic ver-sion (Tags), each case consists of the tags sug-gested by the component taggers and the cor-rect tag.
In the more advanced versions wealso add information about the word in ques-tion (Tags+Word) and the tags suggested by alltaggers for the previous and the next position(Tags+Context).
For the first two the similaritymetric used during tagging is a straightforwardoverlap count; for the third we need to use anInformation Gain weighting (Daelemans ct al.1997).Surprisingly, none of the Memory-Basedbased methods reaches the quality of TagPair.
9The explanation for this can be found whenwe examine the differences within the Memory-Based general strategy: the more feature infor-mation is stored, the higher the accuracy onTune, but the lower the accuracy on Test.
Thisis most likely an overtraining effect: Tune isprobably too small to collect case bases whichcan leverage the stacking effect convincingly, es-pecially since only 7.51% of the second stagematerial shows disagreement between the fea-tured tags.To examine if the overtraining effects are spe-cific to this particular second level classifier, wealso used the C5.0 system, a commercial versionof the well-known program C4.5 (Quinlan 1993)for the induction of decision trees, on the sametraining material.
1?
Because C5.0 prunes thedecision tree, the overfitting of training material(Tune) is less than with Memory-Based learn-ing, but the results on Test are also worse.
Weconjecture that pruning is not beneficial whenthe interesting cases are very rare.
To realise thebenefits of stacking, either more data is neededor a second stage classifier that is better suitedto this type of problem.9Tags (Memory-Based) scores significantly worsethan TagPair (p=0.0274) and not significantly betterthan Precision-Recall (p=0.2766).1?Tags+Word could not be handled by C5.0 due to thehuge number of feature values.495Test Increase vsComponentAverageT 96.08 -R 96.46M 96.95MR 97.03 96.70+0.33RT 97.11 96.27+0.84MT 97.26 96.52+0.74E 97.43MRT 97.52 96.50+1.02ME 97.56 97.19+0.37ER 97.58 96.95+0.63ET 97.60 96.76+0.84MER 97.75 96.95+0.80ERT 97.79 96.66+1.13MET 97.86 96.82+1.04MERT 97.92 96.73+1.19% Reduc-tion ErrorRate BestComponent2.6 (M)18.4 (R)lO.2 (M)18.7 (M)5.1 (E)5.8 (E)6.6 (E)12.5 (E)14.0 (E)16.7 (E)19.1 (E)Table 3: Correctness scores on Test for PairwiseVoting with all tagger combinations7 The  va lue  o f  combinat ionThe relation between the accuracy of combina-tions (using TagPair) and that of the individualtaggers is shown in Table 3.
The most impor-tant observation is that every combination (sig-nificantly) outperforms the combination of anystrict subset of its components.
Also of noteis the improvement yielded by the best combi-nation.
The pairwise voting system, using allfour individual taggers, scores 97.92% correcton Test, a 19.1% reduction in error rate overthe best individual system, viz.
the MaximumEntropy tagger (97.43%).A major factor in the quality of the combi-nation results is obviously the quality of thebest component: all combinations with E scorehigher than those without E (although M, Rand T together are able to beat E alone11).
Af-ter that, the decisive factor appears to be thedifference in language model: T is generally abetter combiner than M and R, 12 even though ithas the lowest accuracy when operating alone.A possible criticism of the proposed combi-11By a margin at the edge of significance: p=0.0608.12Although not significantly better, e.g.
the differ-ences within the group ME/ER/ET  are not significant.nation scheme is the fact that for the most suc-cessful combination schemes, one has to reservea non-trivial portion (in the experiment 10%of the total material) of the annotated ata toset the parameters for the combination.
To seewhether this is in fact a good way to spend theextra data, we also trained the two best individ-ual systems (E and M, with exactly the samesettings as in the first experiments) on a con-catenation of Train and Tune, so that they hadaccess to every piece of data that the combina-tion had seen.
It turns out that the increasein the individual taggers is quite limited whencompared to combination.
The more exten-sively trained E scored 97.51% correct on Test(3.1% error reduction) and M 97.07% (3.9% er-ror reduction).Conc lus ionOur experiment shows that, at least for the taskat hand, combination of several different sys-tems allows us to raise the performance ceil-ing for data driven systems.
Obviously thereis still room for a closer examination of the dif-ferences between the combination methods, e.g.the question whether Memory-Based combina-tion would have performed better if we had pro-vided more training data than just Tune, andof the remaining errors, e.g.
the effects of in-consistency in the data (cf.
Ratnaparkhi 1996on such effects in the Penn Treebank corpus).Regardless of such closer investigation, we feelthat our results are encouraging enough to ex-tend our investigation of combination, startingwith additional component taggers and selec-tion strategies, and going on to shifts to othertagsets and/or languages.
But the investiga-tion need not be limited to wordclass tagging,for we expect that there are many other NLPtasks where combination could lead to worth-while improvements.AcknowledgementsOur thanks go to the creators of the tagger gen-erators used here for making their systems avail-able.Re ferencesAll K.M.
and Pazzani M.J. (1996) Error Reduc-tion through Learning Multiple Descriptions.Machine Learning, Vol.
24(3), pp.
173-202.496Brill E. (1992) A Simple Rule-Based Part ofSpeech Tagger.
In Proc.
ANLP'92, pp.
152-155.Brill E. (1994)  Some Advances inTransformation-Based Part-of-Speech Tag-ging.
In Proc.
AAAI'94.Chan P.K.
and Stolfo S.J.
(1995) A Compara-tive Evaluation of Voting and Meta-Learningof Partitioned Data.
In Proc.
12th Interna-tional Conference on Machine Learning, pp.90-98.Daelemans W., Zavrel J., Berck P. andGillis S.E.
(1996) MBT: a Memory-BasedPart of Speech Tagger-Generator.
In Proc.Fourth Workshop on Very Large Corpora,E.
Ejerhed and I. Dagan, eds., Copenhagen,Denmark, pp.
14-27.Daelemans W., van den Bosch A. and Wei-jters A.
(1997) IGTree: Using Treesfor Compression and Classification in LazyLearning Algorithms.
Artificial IntelligenceReview, 11, Special Issue on Lazy Learning,pp.
407-423.van Halteren H. (1996) Comparison of Tag-ging Strategies, a Prelude to Democratic Tag-ging.
In "Research in Humanities Computing4.
Selected papers for the ALLC/ACH Con-ference, Christ Church, Oxford, April 1992",S. Hockey and N. Ide (eds.
), Clarendon Press,Oxford, England, pp.
207-215.van Halteren H.
(ed.)
(1998, forthc.)
SyntacticWordclass Tagging.
Kluwer Academic Pub-lishers, Dordrecht, The Netherlands, 310 p.Johansson S. (1986) The Tagged LOB Corpus:User's Manual.
Norwegian Computing Cen-tre for the Humanities, Bergen, Norway.
149p.Quinlan J.R. (1993) C~.5: Programs for Ma-chine Learning.
San Mateo, CA.
Morgan Kaf-mann.Ratnaparkhi A.
(1996) A Maximum En-tropy Part of Speech Tagger.
In Proc.
ACL-SIGDAT Conference on Empirical Methodsin Natural Language Processing.Steetskamp R. (1995) An Implementation Ofa Probabilistic Tagger.
TOSCA ResearchGroup, University of Nijmegen, Nijmegen,The Netherlands.
48 p.Turner K. and Ghosh J.. (1996) Error Correla-tion and Error Reduction in Ensemble Clas-sifiers.
Connection Science, Special issue oncombining artificial neural networks: ensem-ble approaches, Vol.
8(3&4), pp.
385-404.Wolpert D.H. (1992) Stacked Generalization.Neural Networks, Vol.
5, pp.
241-259.497
