Data-driven, PCFG-based and Pseudo-PCFG-based Models for ChineseDependency ParsingWeiwei Sun and Xiaojun WanInstitute of Computer Science and Technology, Peking UniversityThe MOE Key Laboratory of Computational Linguistics, Peking University{ws,wanxiaojun}@pku.edu.cnAbstractWe present a comparative study of transition-,graph- and PCFG-based models aimed at il-luminating more precisely the likely contri-bution of CFGs in improving Chinese depen-dency parsing accuracy, especially by com-bining heterogeneous models.
Inspired bythe impact of a constituency grammar on de-pendency parsing, we propose several strate-gies to acquire pseudo CFGs only from de-pendency annotations.
Compared to linguisticgrammars learned from rich phrase-structuretreebanks, well designed pseudo grammarsachieve similar parsing accuracy and haveequivalent contributions to parser ensemble.Moreover, pseudo grammars increase the di-versity of base models; therefore, togetherwith all other models, further improve sys-tem combination.
Based on automatic POStagging, our final model achieves a UAS of87.23%, resulting in a significant improve-ment of the state of the art.1 IntroductionPopular approaches to dependency parsing canbe divided into two classes: grammar-free andgrammar-based.
Data-driven, grammar-free ap-proaches make essential use of machine learningfrom linguistic annotations in order to parse newsentences.
Such approaches, e.g.
transition-based(Nivre, 2008) and graph-based (McDonald, 2006;Torres Martins et al 2009) have attracted the mostattention in recent years.
In contrast, grammar-based approaches rely on linguistic grammars (ineither dependency or constituency formalisms) toshape the search space for possible syntactic anal-ysis.
In particular, CFG-based dependency parsingexploits a mapping between dependency and con-stituency representations and reuses parsing algo-rithms developed for CFG to produce dependencystructures.
In previous work, data-driven, discrim-inative approaches have been widely discussed forChinese dependency parsing.
On the other hand,various PCFG-based constituent parsing methodshave been applied to obtain phrase-structures aswell.
With rich linguistic rules, phrase-structures ofChinese sentences can be well transformed to theircorresponding dependency structures (Xue, 2007).Therefore, PCFG parsers with such conversion rulescan be taken as another type of dependency parser.We call them PCFG-based parsers, in this paper.Explicitly defining linguistic rules to expressprecisely generic grammatical regularities, a con-stituency grammar can be applied to arrange sen-tences into a hierarchy of nested phrases, which de-termines constructions between larger phrases andtheir smaller component phrases.
This type of infor-mation is different from, but highly related to, theinformation captured by a dependency representa-tion.
A constituency grammar, thus, has great possi-ble contributions to dependency parsing.
In orderto pave the way for new and better methods, westudy the impact of CFGs on Chinese dependencyparsing.
A series of empirical analysis of state-of-the-art graph-, transition-and PCFG-based parsers ispresented to illuminate more precisely the propertiesof heterogeneous models.
We show that CFGs havea great impact on dependency parsing and PCFG-based models have complementary predictive pow-ers to data-driven models.System ensemble is an effective and importanttechnique to build more accurate parsers based onmultiple, diverse, weaker models.
Exploiting differ-301Transactions of the Association for Computational Linguistics, 1 (2013) 301?314.
Action Editor: Jason Eisner.Submitted 6/2012; Revised 10/2012; Published 7/2013.
c?2013 Association for Computational Linguistics.ent data-driven models, e.g.
transition- and graph-based models, has received the most attention independency parser ensemble (Nivre and McDon-ald, 2008; Torres Martins et al 2008; Sagae andLavie, 2006).
Only a few works investigate inte-grating data-driven and PCFG-based models (Mc-Donald, 2006).
We argue that grammars can signif-icantly increase the diversity of base models, whichplays a central role in parser ensemble, and thereforelead to better and more promising hybrid systems.We introduce a general classifier enhancing tech-nique, i.e.
bootstrap aggregating (Bagging), to im-prove dependency parsing accuracy.
This techniquecan be applied to enhance a single-view parser, orto combine multiple heterogeneous parsers.
Exper-iments on the CoNLL 09 shared task data demon-strate its effectiveness: (1) Bagging can improve in-dividual single-view parsers, especially the PCFG-based one; (2) Bagging is more effective than pre-viously introduced ensemble methods to combinemulti-view parsers; (3) Integrating data-driven andPCFG-based models is more useful than combiningdifferent data-driven models.Although PCFG-based models have a big con-tribution to data-driven dependency parsing, theyhave a serious limitation: There are no corre-sponding constituency annotations for some depen-dency treebanks, e.g.
Chinese Dependency Tree-bank (LDC2012T05).
To overcome this limita-tion, we propose several strategies to acquire pseudogrammars only from dependency annotations.
Inparticular, dependency trees are converted to pseudoconstituency trees and PCFGs can be extracted fromsuch trees.
Another motivation of this study is to in-crease the diversity of candidate models for parserensemble.
Experiments show that pseudo-PCFG-based models are very competitive: (1) Pseudogrammars achieve similar or even better parsing re-sults than linguistic grammars learned from richconstituency annotations; (2) Compared to linguisticgrammars, well designed, single-view pseudo gram-mars have an equivalent contribution to parser en-semble; (3) Combining different pseudo grammarseven work better for ensemble than linguistic gram-mars; (4) Pseudo-PCFG-based models increase thediversity of base models, and therefore lead to fur-ther improvements for ensemble.Based on automatic POS tagging, our final modelachieves a UAS of 87.23% on the CoNLL data and84.65% on CTB5, which yield relative error reduc-tions of 18-24% over the best published results inthe literature.2 Background and related work2.1 Data-driven dependency parsingThe mainstream work on recent dependency pars-ing focuses on data-driven approaches that automat-ically learn to produce dependency graphs for sen-tences solely from a hand-crafted dependency tree-bank.
The advantage of such models is that theyare easily ported to any language in which labeledlinguistic resources exist.
Practically all statisti-cal models that have been proposed in recent yearscan be mainly described as either graph-based ortransition-based (McDonald and Nivre, 2007).
Bothmodels have been adopted to learn Chinese depen-dency structures (Zhang and Clark, 2011; Zhangand Nivre, 2011; Huang and Sagae, 2010; Hatoriet al 2011; Li et al 2011, 2012).
According topublished results, graph-based and transition-basedparsers achieve similar accuracy.In the graph-based framework, informative evalu-ation results have been presented in (Li et al 2011).First, second and third order projective parsing mod-els are well evaluated.
In the transition-based frame-work, two advanced techniques have been stud-ied.
First, developing features has been showncrucial to advancing parsing accuracy and a veryrich feature set is carefully evaluated by Zhang andNivre (2011).
Second, beyond deterministic greedysearch, principled dynamic programming strategiescan be employed to explore more possible hypothe-ses (Huang and Sagae, 2010).
Both techniques havebeen examined and shown helpful for Chinese de-pendency parsing.
Furthermore, Hatori et al(2011)combined both and obtained a state-of-the-art super-vised parsing result.2.2 PCFG-based dependency parsingPCFG-based dependency parsing approaches arebased on the finding that projective dependency treescan be transformed from constituency trees by ap-plying rich linguistic rules.
In such approaches, de-pendency parsing can be resolved by a two-step pro-cess: constituent parsing and rule-based extraction302of dependencies from phrase structures.
The ad-vantage of constituency-grammar-based approach isthat all the well-studied parsing methods for suchgrammars can be used for dependency parsing aswell.
Two language-specific properties essentiallymake PCFG-based approaches easy to be appliedto Chinese dependency parsing: (1) Chinese gram-maticians favor using projective structures;1 (2) Chi-nese phrase-structure annotations normally containricher information and thus are reliable for tree con-version.2.2.1 Constituency parsingCompared to many other languages, statisticalconstituent parsing for Chinese has reached earlysuccess, due to the fact that the language has rela-tively fixed word order and extremely poor inflec-tional morphology.
Both facts allow PCFG-basedstatistical modeling to perform well.
For the con-stituent parsing, the majority of the state-of-the-art parsers are based on generative PCFG learn-ing.
For example, the well-known and success-ful Collins and Charniak&Johnson parsers (Collins,2003; Charniak, 2000; Charniak and Johnson, 2005)implement generative lexicalized statistical models.Apart from lexicalized PCFG parsing, unlex-icalized parsing with latent variable grammars(PCFGLA) can also produce comparable accuracy(Matsuzaki et al 2005; Petrov et al 2006).
Latentvariable grammars model an observed treebank ofcoarse parse trees with a model over more refined,but unobserved, derivation trees that represent muchmore complex syntactic processes.
Rather thanattempting to manually specify fine-grained cate-gories, previous work shows that automatically in-ducing the sub-categories from data can work quitewell.
A PCFGLA parser leverages on an automaticprocedure to learn refined grammars and are there-fore more robust to parse non-English languages thatare not well studied.
For Chinese, such a parserachieves the state-of-the-art performance and de-feats many other types of parsers, including Collinsas well as Charniak parser (Che et al 2012) and1For example, as two popular dependency treebanks, theCoNLL 2009 data and the Chinese Dependency Treebank bothexcluede non-projective annotations.
It is worth noting that theformer one is converted from a constituency treebank while thelatter one is directly annotated by lingusitics.discriminative transition-based models (Zhang andClark, 2009).2.2.2 CS to DS conversionIn the absence of dependency and constituencystructures for a particular treebank, treebank-guidedparser developers normally apply rich linguisticrules to convert one representation formalism to an-other to get necessary data to train parsers.
Xue(2007) examines the linguistic adequacy of depen-dency structure annotation automatically convertedfrom phrase structure treebanks with rule-based ap-proaches.
A structural approach is introduced forthe constituency structure (CS) to dependency struc-ture (DS) conversion for the Chinese Treebank data,which is the basis of the CoNLL 2009 shared taskdata.
By applying this conversion procedure on theoutputs of an automatic phrase structure parser, wecan build a PCFG-based dependency parser.2.3 Parser ensembleNLP systems built on particular single views nor-mally capture different properties of an originalproblem, and therefore differ in predictive powers.As a result, NLP systems can take advantage of com-plementary strengths of multiple views.
Combiningthe outputs of several systems has been shown in thepast to improve parsing performance significantly,including integrating phrase-structure parsers (Hen-derson and Brill, 1999), dependency parsers (Nivreand McDonald, 2008), or both (McDonald, 2006).Several ensemble models have been proposed forthe parsing of syntactic constituents and dependen-cies, including learning-based stacking (Nivre andMcDonald, 2008; Torres Martins et al 2008) andlearning-free post-inference (Henderson and Brill,1999; Sagae and Lavie, 2006).
Surdeanu and Man-ning (2010) present a systematic analysis of theseensemble methods and find several non-obviousfacts:?
the diversity of base parsers is more importantthan complex models for learning, and?
simplest scoring model for voting and repars-ing performs essentially as well as other morecomplex models.3033 A comparative analysis of heterogeneousparsersThe information encoded in a dependency repre-sentation is different from the information capturedin a constituency representation.
While the depen-dency structure represents head-dependent relationsbetween words, the constituency structure repre-sents the grouping of words into phrases, classifiedby structural categories.
These differences concernwhat is explicitly encoded in the respective represen-tations, and affects data-driven and PCFG-based de-pendency parsing models substantially.
In this sec-tion, we give a comparative analysis of transition-,graph- and PCFG-based models aimed at illuminat-ing more precisely the likely contribution of CFGsin dependency parsing.3.1 Experimental setupPenn Chinese TreeBank (CTB) is a segmented,POS tagged, and fully bracketed corpus in the con-stituency formalism, and very popular to evaluatefundamental NLP tasks, including word segmenta-tion, POS tagging, constituent parsing as well as de-pendency parsing.
We use CTB 6 as our main corpusand define the training, development and test sets ac-cording to the CoNLL 2009 shared task.
To evaluateand analyze dependency parsers, we directly use theCoNLL data.
CTB?s syntactic annotations also in-cludes functional information and empty categories.Modern parsers, e.g.
Collins and Berkeley parsers,ignore these types of linguistic knowledge.
To traina constituent parser, we perform a heuristic proce-dure on the treebank data to delete function tags andempty categories as well as its associated redundantancestors.
Many papers reported parsing results ofan older version CTB (namely CTB 5).
To comparewith systems introduced in these papers, we evaluateour final ensemble model on CTB5 in Section 5.4.For dependency parsing, we choose a secondorder graph-based parser2 (Bohnet, 2010) and atransition-based parser (Hatori et al 2011), forexperiments.
For constituent parsing, we chooseBerkeley parser,3 a well known implementation ofthe unlexicalized PCFGLA model and Bikel parser,42code.google.com/p/mate-tools/3code.google.com/p/berkeleyparser/4cis.upenn.edu/?dbikel/software.htmla well known implementation of Collins?
lexical-ized model, for experiments.
In data-driven pars-ing, features consisting of POS tags are very effec-tive, so typically POS tagging is performed as a pre-processing.
We use the baseline sequential taggerdescribed in (Sun and Uszkoreit, 2012) to providesuch lexical information to the graph-based parser.Note that the transition-based parser performs a jointinference to acquire POS and dependency informa-tion simultaneously, so there is no need to offer extratagging results to it.3.2 Overall performanceTable 1 (Column 2-6) summarizes the overall accu-racy of different parsers.
Two transition-based pars-ing results are presented: The first one employ asimple feature set (Zhang and Clark, 2008) and asmall beam (16); the second one employ rich fea-tures (Zhang and Nivre, 2011) and a larger beam(32).
Two graph-based parsing results are reported;the difference between them is whether integrate re-lation labels into the parsing procedure.
Roughlyspeaking, currently state-of-the-art data-driven mod-els achieves slightly better precision than unlexical-ized PCFG-based models with regard to unlabeleddependency prediction.There is a big gap between lexicalized and unlexi-calized parsing.
The same phenomenon has been ob-served by (Che et al 2012) and (Zhuang and Zong,2010).
In addition to dependency parsing, Zhuangand Zong (2010) found that Berkeley parser pro-duce much more accurate syntactic analyses to assista Chinese semantic role labeler than Bikel parser.Charniak and Stanford parsers are two other well-known and frequently used tools that can providelexicalized parsing results.
According to (Che et al2012), they perform even worse than Bikel parser,at least for Stanford dependencies.
Due to the poorparsing performance, we only concentrate on the un-lexicalized model in the remainder of this paper.The performance of labeled dependency predic-tion of the unlexicalized PCFG-based parser is muchlower.
We can learn that the CS to DS conversion isnot robust to assign functional categories to depen-dencies and simple linguistic rules are not capableto do fine-grained classification.
Previous researchon English indicates that the main difficulty in de-pendency parsing is the prediction of dependency304Devel.
UAS LAS Compl.
Fsib FgrdTran[b=16,Z08] 82.80 N/A 29.00 66.55 79.74Tran[b=32,Z11] 83.80 N/A 31.61 68.58 80.87Graph[-lab] 83.66 N/A 29.28 67.96 80.82Graph[+lab] 84.24 80.55 30.99 69.11 81.38Unlex 82.86 67.44 27.98 69.07 81.22Lex 70.38 58.10 - - - - - -Bagging(15)Tran[b=16,Z08] 83.25 N/A 28.66 67.17 78.89Tran[b=32,Z11] 84.25 N/A 31.21 69.14 81.49Graph[-lab] 83.81 N/A 29.68 68.00 80.62Graph[+lab] 84.50 N/A 31.44 69.48 81.10Unlex 84.92 N/A 32.35 71.08 83.66Bagging(8)Unlex 84.35 N/A 31.16 70.49 83.57Table 1: Accuracy of different parsers.
The first blockpresents baseline parsers; the last two blocks presentBagging-enhanced parsers, where m is respectively set to15 and 8.
Z08 and Z11 distinguish different feature sets;b=16 and b=32 are beam sizes.
+/-lab means whether toincorporate relation labels to a model.structures, and an extra statistical classifier can beemployed to label automatically recognized depen-dencies with a high accuracy.
Although this issueis not well studied for Chinese dependency parsing,previous research on function tag labeling (Sun andSui, 2009) and semantic role labeling (Sun, 2010a)gives us some clues.
Their research shows that bothfunctional and predicate-argument structural infor-mation is relatively easy to predict if high-qualitysyntactic parses are available.
We mainly focus onthe UAS metric in the following experiments.3.3 ConstraintsA grammar-based model utilizes an explicitly de-fined formal grammar to shape the search space forpossible syntactic hypotheses.
Parameters of a sta-tistical grammar-based model are related to a gram-mar rule, and as a result specific language construc-tions are constrained by each other.
For example,parameters are assigned to rewrite rules for a CFG-based model.
Since the PCFG-based model lever-ages rewrite rules to locally constrain several possi-ble dependents for one head word, it does relativelybetter for locally connected dependencies.
The tra-ditional evaluation metrics, i.e.
UAS and LAS, onlyconsider bi-lexical (first order) dependencies, whichare smallest pieces of a dependency structure.
Be-sides bi-lexical dependencies, we report the predic-tion accuracy of grandparent and sibling dependen-cies, i.e.
second order dependencies.
The metricsare defined as follows.?
For every word d whose parent is not the root,we consider the word triple ?d, p, g?
among dand its parent p and grandparent g. A wordtriple ?d, p, g?
from a predicted tree is consid-ered as correct if it also apprears in the corre-sponding gold tree.
Based on this definition,precison, recall and f-score of grandparent de-pendency can be defined in a normal sense.
Allpunctuations are excluded for calculation.?
For every word h that governs at least two chil-dren (d1, ..., dn), we consider every word triple?h, di, di+1?, among h and its sibling depen-dents di as well as di+1 (0 ?
i < n).
Similarto the grandparent dependencies, we can defineevaluation metrics for sibling dependencies.From Table 1, we can see that the grammar-basedmodel parses relatively better for slightly larger frag-ments.
For example, the UAS of the graph-basedmodel is significantly higher than the grammar-based one, but their sibling and grandparent scoresare similar.
In the next section, we will introducea general parser enhancement technique and presentmore discussions based on enhanced parsing results(Column 7-14).3.4 Endocentric and exocentric constructions<-NN<- <-NR<- <-NT<- <-PN<- <-VA <- <-VC<- <-VE<- <-VV<-Unlex 2 7.61 19.3 17.2 5 14.0 9 39.72 45.51 49.83 41.44G raph[+lab] 2 4.82 17.45 12 .2 12 .1 38.12 49.9 51.18 42 .14Tran[b=32 ,Z0 8] 2 5.2 5 17.82 15.16 13.48 41.32 47.7 49.83 42 .34102 0304050ErrorrateFigure 1: Nominal vs. verbal constructions.Arguments in exocentric constructions help com-plete the meaning of a predicate and are taken to beobligatory and selected by their heads; adjuncts in305endocentric constructions are structurally dispens-able parts that provide auxiliary information andtaken to be optional and not selected by their heads.An important annotation policy of the CTB is ?onegrammatical relation per bracket?, which meanseach constituent falls into one of the three primitivegrammatical relations: (1) head-complementation,(2) head-adjunction and (3) coordination.
Addi-tionally, the argument is attached at a level that is?closer?
to the head than the adjuncts.
Due to thelinguistic properties of different dependents and theannotation strategies, a grammar-based model cancapture more syntactic preference properties of ar-guments via hard constraints, i.e.
grammar rules,and are therefore more suitable to analyze exocen-tric constructions.Figure 1 is the error rate of unlabeled dependen-cies considering different construction.
A construc-tion ??
X ??
is considered as correctly predictedif and only if all dependent words and head word ofX are completely correctly found.
The error ratein terms of this metric seems rather high becausethe units we consider are normally much larger thanword pairs.
From this figure, we can clearly see thatthe data-driven parser does better for the predictionof nominal constructions (NN/NR/NT/PN5), whichrelate more on optional adjuncts or modifiers; thegrammar-based parser performs better for the pre-diction of verbal constructions (VC/VE/VV), whichrelate more on obligatory arguments.
The evalua-tion of the nominal and verbal constructions roughlyconfirms the strength of grammar-based model topredict verbal constructions.4 Bagging parsersThe comparative analysis highlights the fundamen-tal diversity between data-driven and PCFG-basedmodels.
In order to exploit the diversity gain, we ad-dress the issue of parser combination.
We employa general ensemble learning technique, i.e.
Bag-ging, to enhance a single-view parser and to com-bine multi-view parsers.5For the definition and illustration of these tags, please referto the annotation guidelines (http://www.cis.upenn.edu/?chinese/posguide.3rd.ch.pdf).4.1 Applying Bagging to dependency parsingBagging is a machine learning ensemble meta-algorithm to improve classification and regressionmodels in terms of stability and classification accu-racy (Breiman, 1996).
It also reduces variance andhelps to avoid overfitting.
Given a training set D ofsize n, Bagging generates m new training sets Diof size n?
?
n, by sampling examples from D. mmodels are separately learned on the m new train-ing sets and combined by voting (for classification)or averaging the output (for regression).
Hender-son and Brill (2000) successfully applied Baggingto enhance a constituent parser.
Moreover, Bagginghas been applied to combine multiple solutions forChinese lexical processing (Sun, 2010b; Sun andUszkoreit, 2012).
In this paper, we apply Baggingto dependency parsing.
Since training even one sin-gle parser takes hours (if not days), experiments onBagging is time-consuming.
To save time, we con-duct data-driven parsing experiments based on sim-ple configuration.
More specifically, the beam sizeof the transition-based parser is set to 16, and thesimple feature set is utilized; dependency relationsare not incorporated for the graph-based parser.Bootrapping step.
In the training phase, given atraining set D of size n, our model generates m newtraining sets Di of size ?n by sampling uniformlywithout replacement.
Each Di can be used to traina single-view parser or multiple parsers accordingto different views.
Using this strategy, we can get mweak parsers or km parsers if multiple views are im-plemented.
In the parsing phase, for each sentence,the (k)m models output (k)m candidate analysesthat are combined in a post-inference procedure.Aggregating step.
Different from classificationproblems, simple voting scheme is not suitable forparsing, which is a typical structured predictionproblem.
To aggregate outputs of (k)m sub-models,a structured inference procedure is needed.
Sagaeand Lavie (2006) present a framework for combin-ing the output of several different parsers to produceresults that are superior to each of the individualparsers.
We implement their method to aggregatemodels.
Once we have obtained multiple depen-dency trees respectively from base parsers, we canbuild a graph where each word in the sentence is a306node.
We then create weighted directed edges be-tween the nodes corresponding to words for whichdependencies are obtained from each of the initialstructures.
The weights are the word-by-word votingresults of sub-models.
Based on this graph, the sen-tence can be reparsed by a graph-based algorithm.Taking Chinese as a projective language, we use Eis-ner?s algorithm (Eisner, 1996) to combine multipledependency parses.
Surdeanu and Manning (2010)indicates that reparsing performs essentially as wellas other simpler or more complex models.4.2 Parameter tuningWe evaluate our combination model on the samedata set used in the last section.
The two hyper-parameters (?
and m) of our Bagging model aretuned on the development (validation) set.
On onehand, with the increase of the size of sub-samples,i.e.
?, the performance of sub-models is improved.However, since the sub-models overlap more, the di-versity of base models for ensemble will decreaseand the final prediction accuracy may go down.
Toevaluate the effect of ?, we separately sample 50%,60%, 70% and 80% sentences from the originaltraining data 5 times, train 5 sub-models for eachparser, and combine them together.
The beam sizeof the transition-based parser is set to 16.
Table 2shows the influence of the choice of ??s.
For all fol-lowing experiments, we set ?
= 0.7.?
50% 60% 70% 80%Tran+Graph[-lab]+Unlex 83.50 85.96 86.15 85.60Table 2: UAS of Bagging(5) models with different ?.The second parameter for Bagging is the numberof sub-models to be used for combination.
Figure 2summarizes the Bagging performance when differ-ent models are employed and different number (i.e.m) of subsamples are used.
From this figure, we canlearn the influence of the number of sub-models.4.3 Bagging single-view parsers4.3.1 ResultsTable 1 indicates that Bagging can improve in-dividual single-view parsers, especially Berkeleyparser.
If we take Bagging as a general parser en-hancement technique and still consider a Bagging-enhanced parser as a single view, we conclude81.582 .583.584.585.586.53 4 5 6 7 8 9G raph[-lab]TranUnlexG raph[-lab]+UnlexTran+UnlexG raph[-lab]+TranFigure 2: Averaged UAS of different Bagging modelswith different numbers of sampling data sets.that Bagging-enhanced PCFG-based method worksbest among state-of-the-art approaches.
For thetransition-based parser, though the score over singlewords goes up, the score over sentences goes down.The main reason is that the reparsing algorithm is agraph-based one, which performs worse with regardto the prediction of a whole sentence.
The improve-ment for the graph-based parser is very modest.We train a Bagging(8)-enhanced Berkeley parser,which achieves equivalent overall UAS to data-driven parsers, and compare their parsing abilitiesof second order dependencies.
Now we can moreclearly see that the Bagging-enhanced PCFG-basedmodel performs better in the prediction of secondorder dependencies.4.3.2 Related experiments on sequence modelsBagging has been applied to enhance discrimina-tive sequence models for Chinese word segmenta-tion (Sun, 2010b) and POS tagging (Sun and Uszko-reit, 2012).
For word segmentation, experimentson discriminative Markov and semi-Markov taggingmodels are reported.
Their experiments showed thatBagging can consistently enhance a semi-Markovmodel but not the Markov one.
Experiments on POStagging indicated that BaggingMarkov models hurtstagging performance.
It seems that the relationshipsamong basic processing units affect Bagging.PCFGLA parsers are built upon generative mod-els with latent annotations.
The use of automati-cally induced latent variables may also affect Bag-ging.
Generative sequence models with latent anno-307tations can also achieve good performance for Chi-nese POS tagging.
Huang et al(2009) describedand evaluated a bi-gram HMM tagger that utilizeslatent annotations.
Different from negative results ofBagging discriminative models, our auxiliary exper-iment shows that Bagging Huang et als tagger canhelp Chinese POS tagging.
In other words, Baggingsubstantially improves both HMMLA and PCFGLAmodels, at least for Chinese POS tagging and con-stituency parsing.
It seems that Bagging favors theuse of latent variables.4.4 Bagging multi-view parsers4.4.1 ResultsFigure 2 clearly shows that the Bagging modeltaking both data-driven and PCFG-based models asbasic systems outperform the Bagging model takingeither model in isolation as basic systems.
The com-bination of a PCFG-based model and a data-drivenmodel (either graph-based or transition-based) ismore effective than the combination of two data-driven models, which has received the most atten-tion in dependency parser ensemble.
Table 3 isthe performance of reparsing on the developmentdata.
From this table, we can see by utilizing moreparsers, Bagging can enhance reparsing.
Accordingto Surdeanu and Manning (2010)?s findings, repars-ing performs as well as other combination mod-els.
Our auxiliary experiments confirm this finding:Learning-based stacking cannot achieve better per-formance.
Limited to the document length, we donot give descriptions of these experiments.Devel.
UASReparsing(Tran[b=16,Z08]+Graph[-lab]+Unlex) 85.82+Bagging(15) 86.37bagging(reparse(g, t, c)) 86.09reparse(bagging(g, t, c)) 85.86Table 3: UAS of reparsing and Bagging.4.4.2 AnalysisIn our proposed model, Bagging has a two-foldeffect: One is as a system combination techniqueand the other as a general parser enhancing tech-nique.
Two additional experiments are performedto evaluate these two effects.
To illustrate the differ-ences between these two experiments, respectivelydenote graph-based, transition-based and PCFG-based parsers as g, t and c; denote the reparsingprocedure as reparse and the Bagging procedure asbagging.
The two experiments are as follows.?
Bagging a hybrid parser.
In this experiment,for each sub-sample Di, we first train threeparsers: gi, ti and ci.
Then we combine thesethree parsers by reparsing and construct a hy-brid parser reparse(gi, ti, ci).
Finally, all hy-brid parsers are collected to build the finalparser: bagging(reparse(g, t, c)).?
Combining Bagging-enhanced parsers.
Inthis experiment, for each model, we first trainthree Bagging-enhanced parsers: bagging(g),bagging(t) and bagging(c).
Then thesethree Bagging-enhanced parsers are com-bined by reparsing to build the final parser:reparse(bagging(g, t, c)).Evaluation results are presented in Table 3.5 Pseudo-grammar-based modelsAlthough the combination of data-driven andgrammar-based models is very effective, it has aserious limitation: It is only applicable when con-stituency annotations are available to learning agrammar.
However, many treebanks, e.g.
ChineseDependency Treebank (LDC2012T05), do not havesuch linguistically rich structures.
Our experimentsalso suggest that a constituency grammar can sig-nificantly increase the diversity of base models forparser ensemble, which plays a major role in boost-ing prediction accuracy.In order to reduce the need for phrase-structureannotations, and to increase the diversity of candi-date parsers, we study learning pseudo grammarsfor dependency parsing.
The key idea is very sim-ple: By converting a dependency structure to aconstituency one, we can reuse the PCFGLA ap-proach to learn pseudo grammars for dependencyparsing.
Figure 3 is an example.
The first tree isan original dependency parse, while the second treeis the corresponding CTB annotation.
The next twotrees are two automatically converted pseudo con-stituency trees.
By applying DS to CS rules, wecan acquire pseudo constituency treebanks and thenlearn pseudo grammars from them.308(1) Dependency tree (2) Linguistic constituency tree(3) Flat constituency tree (4) Binarized constituency treeFigure 3: An example: China encourages private entrepreneurs to invest in national infrastructure.The basic idea of our method is to use parsingmodels in one formalism for parsing in another for-malism.
In previous work, PCFGs are used to solveparsing problems in many other formalisms, includ-ing dependency (Collins et al 1999), CCG (Fowlerand Penn, 2010), LFG (Cahill et al 2004) and HPSG(Zhang and Krieger, 2011) parsing.5.1 Strategies for DS to CS conversionThe conversion from DS to CS is a non-trivial prob-lem.
One main issue in the conversion is the indeter-minancy in the choice of a phrasal category given adependency relation, the level and position of attach-ment of a dependent in the constituency structure, asdependency relations typically do not encode suchinformation.
To convert a DS to a CS, especiallyfor dependency parsing, we should consider (1) howto transform between the topological structures, (2)how to induce a syntactic category, and (3) how toeasily recover dependency trees from pseudo con-stituency trees.
From these three aspects, we presentthe following strategies.5.1.1 Topological structureThe topological structures represent the boundaryinformation of constituents in a given sentence.
De-pendency structures do not directly represent suchboundary information.
Nevertheless, a completesubtree in a projective dependency tree should beconsidered as a constituent.
We can construct a veryflat constituent tree, of which nodes are associatedwith complete subtrees of a dependency parse.
Thethird tree in Figure 3 is an example of such conver-sion.Right-to-left binarization According to the studyin (Sun, 2010a), head words of most phrases inChinese are located at the first or the last position.That means for binarizing most phrases, we onlyneed sequentially combine the right or left parts to-gether with their head phrases.
Main exceptions areclauses, of which the head predicate locates inside,since Chinese is an SVO language.
To deal withthese exceptions, we split each phrase whose headchild is inside itself into three parts: left child(ren),head and right child(ren).
We first sequentially com-bine the head and its right child(ren) that are usu-ally objects as intermediate phrases, then sequen-tially combine the left child(ren) until reach the orig-inal parent node.
For example, the first rewrite rulein follows should be transferred into the second andthird types of rules.1.
Xp ?
X1, ..., Xi, ..., Xm3092.
X?p ?
Xi, Xi+1; Xp??
?
Xp?, Xi+2; ...3.
X?p ?
Xi?1, Xp?...?
; X?
?p ?
Xi?2, X?p; ...This right-to-left binarization strategy is consistentwith most Chinese treebank annotation schemes.The fourth tree in Figure 3 is an example of bina-rized pseudo tree.5.1.2 Phrasal categoryProjection principle is introduced by Chomskyto link together the levels of syntactic description.It connects syntactic structures with lexical entries:Lexical structure must be represented categoricallyat every syntactic level, and representations at eachlevel of syntax are projected from the lexicon in thatthey observe the subcategorisation properties of lex-ical items.
According to this principle, it is reason-able to use the lexical category (POS) of the headword as the phrasal category of a phrase.5.1.3 Auxiliary symbolWe can use auxiliary symbols to denote the headphrase position in a CFG rule.
In other words, somecategories may be splitted into subcategories accord-ing to if they are head phrases of their parent nodesor which children are their head phrases.
Auxiliarysymbols could be either assigned to one of the righthand side or the left hand side.
The first choice is toconveniently use a H symbol to indicate that currentphrase is the head of its parent node.
The secondchoice is to practically use an L or R symbol to indi-cate the head of current node is its left or right child,in a binarized tree.
The following table gives an ex-ample of different rules with auxiliary symbols.With head symbol With left/right symbolXl ?
Xl#H, Xr Xl#L?
Xl, XrXr ?
Xl, Xr#H Xr#R?
Xl, Xr5.2 Three conversionsTaking into account the above strategies, we proposethree concrete DS to CS conversions:Flat conversion with H auxiliary symbol (FlatH).Just as shown as the third tree in Figure 3, we canlearn a grammar from very flat constituency treeswhere the auxiliary symbol H is used for extractingdependencies.Right-to-left binarizing with H auxiliary symbol(BinH).
Different from the flat conversion, we bi-narize a tree according to the right-to-left principle.Auxiliary symbol H is chosen.Right-to-left binarizing with LR auxiliary sym-bol (BinLR).
Different from the second type ofconversion, we use auxiliary L/R symbols to denotehead phrases.
See the fourth tree in Figure 3 for in-stance.Practically, every constituency parse that is pro-duced by parsers trained with binarized trees exactlymaps to one dependency tree.
However, the parsertrained with flat trees may produce very bad con-stituency results.
Sometimes, one parent node mayhave zero child that is assigned with H or more thanone children that are are assigned H. In the first case,we select the right most child as the head of suchparent, while in the second case, we select the rightmost one from the children that are assigned H.5.3 Evaluation5.3.1 Equivalent parsing accuracyDevel.
Base Bagging(15)CTB 83.49% 84.92%FlatH 80.15% 83.53%BinH 81.80% 84.64%BinLR 82.46% 84.90%Table 4: UAS of pseudo-grammar-based models.Table 4 summarizes the performance of differ-ent pseudo-grammar-based models.
Compared tothe linguistic grammar learned from CTB, we cansee that pseudo grammars are very competitive.Not that, the FlatH/BinH/BinLR trees are derivedfrom the CoNLL data, rather than the original CTB.Among different DS to CS conversion strategies, theBinLR conversion works best.
More interestingly,when we enhance the PCFGLA method by usingBagging, the BinLR model performs as well as thereal-grammar-based model.5.3.2 Better contribution to ensembleThe experiments above indicate that we can eas-ily build good grammar-based dependency parserwithout any constituency annotations.
The fol-lowing experiments on parser combination showthat compared to the linguistic grammar, binH and310Devel.
UASTran+Graph+CTB 86.37%Tran+Graph+FlatH 86.14%Tran+Graph+BinH 86.29%Tran+Graph+BinLR 86.28%Tran+Graph+flat+BinH+BinLR 87.03%Tran+Graph+CTB+FlatH 86.96%Tran+Graph+CTB+BinH 87.10%Tran+Graph+CTB+BinLR 87.15%Tran+Graph+CTB+BinH+BinLR 87.38%Tran+Graph+CTB+FlatH+BinH+BinLR 87.35%Table 5: UAS of different Bagging(15) models.binLR grammars have equivalent contributions toparser ensemble.
Table 5 presents the ensem-ble performance on the development data.
ByBagging, the data-driven models together with ei-ther real grammar-based or pseudo-grammar-basedmodel reach a similar UAS.5.3.3 Increased parser diversitySince pseudo grammars are very different fromreal grammars that are induced from large-scale lin-guistic annotations.
Pseudo-grammar-based parsingmodels behave very differently with grammar-basedmodels.
In other words, they increase the diver-sity of model candidates for parser ensemble.
Asa result, pseudo-grammar-based models lead to fur-ther improvements for parser combination.
Table 5shows that the combination of data-driven, PCFG-based and binarized pseudo-grammar-based modelsis significantly better than the combination of data-driven and PCFG-based models.5.4 Comparison to the state-of-the-artTable 6 summarizes the parsing performance on thetest data set, as well as the best published result re-ported in Li et al(2012).
To fairly compare the per-formance of our parser and other systems which arebuilt without linguistic constituency trees, we onlyuse pseudo-PCFGs in this experiment.
Based onautomatic POS tagging, our final model achieves aUAS of 87.23%, which yields a relative error reduc-tion of 24% over the best published result.
Table6 also presents the results evaluated on the CTB5data that is more widely used for previous research.Li et al(2011) and Hatori et al(2011) respec-tively evaluated their graph-based and transition-based parsers; Zhang and Clark (2011) evaluatedCoNLL-test UAS(Li et al 2012) 83.23%Graph+Tran+FlatH+BinH+BinLR 87.23%CTB5-test UAS(Li et al 2011) 80.79%(Hatori et al 2011) 81.33%(Zhang and Clark, 2011) 81.21%Graph+Tran+FlatH+BinH+BinLR 84.65%Table 6: UAS of different models on the test data.a hybrid data-driven parser.
Our model is signifi-cantly better than these systems: It achieves a UASof 84.65%, which obtains an error reduction of 18%over the best system in the literature.6 Conclusion and Future WorkThere have been several attempts to develop highaccuracy parsers in both constituency and depen-dency formalisms for Chinese, and many successfulparsing algorithms designed for English have beenapplied.
However, the state-of-the-art still falls farshort when compared to English.
This paper stud-ies data-driven and PCFG-based models for Chinesedependency parsing.
We present a comparative anal-ysis of transition-, graph-, and PCFG-based parsers,which highlights the systematic differences betweendata-driven and PCFG-based models.
Our analysismay benefit parser ensemble, parser co-training, ac-tive learning for treebank construction, and so on.In order to exploit the diversity gain, we addressthe issue of parser combination.
To overcome thelimitation of the lack of constituency treebanks, westudy pseudo-grammar-based models.
Experimentalresults show that combining various data-driven andPCFG-based models significantly advance the state-of-the-art, and by converting parse trees, we can stilltake advantages of the constituency representationeven without constituency annotations.AcknowledgementWe would like to thank thank all anonymous review-ers whose valuable comments led to signilicant re-visions.
The first author would like to thank Prof.Hans Uszkoreit for discussion and feedback of anearly version of this work.The work was supported by NSFC (61170166),Beijing Nova Program (2008B03) and NationalHigh-Tech R&D Program (2012AA011101).311ReferencesBernd Bohnet.
2010.
Top accuracy and fast de-pendency parsing is not a contradiction.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), pages89?97.
Coling 2010 Organizing Committee, Bei-jing, China.
URL http://www.aclweb.org/anthology/C10-1011.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Aoife Cahill, Michael Burke, Ruth O?Donovan,Josef Van Genabith, and Andy Way.
2004.
Long-distance dependency resolution in automaticallyacquired wide-coverage pcfg-based lfg approx-imations.
In Proceedings of the 42nd Meet-ing of the Association for Computational Lin-guistics (ACL?04), Main Volume, pages 319?326.
Barcelona, Spain.
URL http://www.aclweb.org/anthology/P04-1041.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the first con-ference on North American chapter of the Associ-ation for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics (ACL?05), pages 173?180.
Associa-tion for Computational Linguistics, Ann Arbor,Michigan.Wanxiang Che, Valentin Spitkovsky, and TingLiu.
2012.
A comparison of chinese parsersfor stanford dependencies.
In Proceedingsof the 50th Annual Meeting of the Associ-ation for Computational Linguistics (Volume2: Short Papers), pages 11?16.
Associa-tion for Computational Linguistics, Jeju Island,Korea.
URL http://www.aclweb.org/anthology/P12-2003.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
ComputationalLinguistics, 29(4):589?637.Michael Collins, Jan Hajic, Lance Ramshaw, andChristoph Tillmann.
1999.
A statistical parser forczech.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Lin-guistics, pages 505?512.
Association for Com-putational Linguistics, College Park, Maryland,USA.
URL http://www.aclweb.org/anthology/P99-1065.Jason M. Eisner.
1996.
Three new probabilis-tic models for dependency parsing: an ex-ploration.
In Proceedings of the 16th con-ference on Computational linguistics - Vol-ume 1, COLING ?96, pages 340?345.
Associa-tion for Computational Linguistics, Stroudsburg,PA, USA.
URL http://dx.doi.org/10.3115/992628.992688.Timothy A. D. Fowler and Gerald Penn.
2010.
Ac-curate context-free parsing with combinatory cat-egorial grammar.
In Proceedings of the 48thAnnual Meeting of the Association for Com-putational Linguistics, pages 335?344.
Associ-ation for Computational Linguistics, Uppsala,Sweden.
URL http://www.aclweb.org/anthology/P10-1035.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental joint pos tag-ging and dependency parsing in chinese.
In Pro-ceedings of 5th International Joint Conference onNatural Language Processing, pages 1216?1224.Asian Federation of Natural Language Process-ing, Chiang Mai, Thailand.
URL http://www.aclweb.org/anthology/I11-1136.John Henderson and Eric Brill.
1999.
Exploiting di-versity in natural language processing: Combin-ing parsers.
In In Proceedings of the Fourth Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 187?194.John C. Henderson and Eric Brill.
2000.
Bag-ging and boosting a treebank parser.
In Pro-ceedings of the 1st North American chapter ofthe Association for Computational Linguisticsconference, NAACL 2000, pages 34?41.
Asso-ciation for Computational Linguistics, Strouds-burg, PA, USA.
URL http://dl.acm.org/citation.cfm?id=974305.974310.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, pages3121077?1086.
Association for Computational Lin-guistics, Uppsala, Sweden.
URL http://www.aclweb.org/anthology/P10-1110.Zhongqiang Huang, Vladimir Eidelman, and MaryHarper.
2009.
Improving a simple bigram hmmpart-of-speech tagger by latent annotation andself-training.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Asso-ciation for Computational Linguistics, Compan-ion Volume: Short Papers, pages 213?216.
As-sociation for Computational Linguistics, Boulder,Colorado.
URL http://www.aclweb.org/anthology/N/N09/N09-2054.Zhenghua Li, Ting Liu, and Wanxiang Che.2012.
Exploiting multiple treebanks for pars-ing with quasi-synchronous grammars.
In Pro-ceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Vol-ume 1: Long Papers), pages 675?684.
Associa-tion for Computational Linguistics, Jeju Island,Korea.
URL http://www.aclweb.org/anthology/P12-1071.Zhenghua Li, Min Zhang, Wanxiang Che, TingLiu, Wenliang Chen, and Haizhou Li.
2011.Joint models for Chinese pos tagging and depen-dency parsing.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 1180?1191.
Associationfor Computational Linguistics, Edinburgh, Scot-land, UK.
URL http://www.aclweb.org/anthology/D11-1109.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichiTsujii.
2005.
Probabilistic cfg with latent an-notations.
In Proceedings of the 43rd An-nual Meeting on Association for ComputationalLinguistics, ACL ?05, pages 75?82.
Associa-tion for Computational Linguistics, Stroudsburg,PA, USA.
URL http://dx.doi.org/10.3115/1219840.1219850.RyanMcDonald.
2006.
Discriminative learning andspanning tree algorithms for dependency pars-ing.
Ph.D. thesis, University of Pennsylvania,Philadelphia, PA, USA.
AAI3225503.Ryan McDonald and Joakim Nivre.
2007.
Char-acterizing the errors of data-driven dependencyparsing models.
In Proceedings of the 2007Joint Conference on Empirical Methods inNatural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL), pages 122?131.
Association for Com-putational Linguistics, Prague, Czech Repub-lic.
URL http://www.aclweb.org/anthology/D/D07/D07-1013.Joakim Nivre.
2008.
Algorithms for de-terministic incremental dependency pars-ing.
Comput.
Linguist., 34:513?553.
URLhttp://dx.doi.org/10.1162/coli.07-056-R1-07-027.Joakim Nivre and Ryan McDonald.
2008.
In-tegrating graph-based and transition-baseddependency parsers.
In Proceedings of ACL-08:HLT, pages 950?958.
Association for Compu-tational Linguistics, Columbus, Ohio.
URLhttp://www.aclweb.org/anthology/P/P08/P08-1108.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
Learning accurate, compact,and interpretable tree annotation.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages433?440.
Association for Computational Linguis-tics, Sydney, Australia.Kenji Sagae and Alon Lavie.
2006.
Parser com-bination by reparsing.
In Proceedings of theHuman Language Technology Conference of theNAACL, Companion Volume: Short Papers,NAACL-Short ?06, pages 129?132.
Associationfor Computational Linguistics, Stroudsburg, PA,USA.
URL http://portal.acm.org/citation.cfm?id=1614049.1614082.Weiwei Sun.
2010a.
Improving Chinese se-mantic role labeling with rich syntactic fea-tures.
In Proceedings of the ACL 2010 Con-ference Short Papers, pages 168?172.
Associ-ation for Computational Linguistics, Uppsala,Sweden.
URL http://www.aclweb.org/anthology/P10-2031.Weiwei Sun.
2010b.
Word-based and character-based word segmentation models: Compari-son and combination.
In Proceedings of the31323rd International Conference on ComputationalLinguistics (Coling 2010), pages 1211?1219.Coling 2010 Organizing Committee, Beijing,China.
URL http://www.aclweb.org/anthology/C10-2139.Weiwei Sun and Zhifang Sui.
2009.
Chinese func-tion tag labeling.
In Proceedings of the 23rd Pa-cific Asia Conference on Language, Informationand Computation.
Hong Kong.Weiwei Sun and Hans Uszkoreit.
2012.
Capturingparadigmatic and syntagmatic lexical relations:Towards accurate Chinese part-of-speech tagging.In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics.
Asso-ciation for Computational Linguistics.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: Cheapand good?
In Human Language Technologies:The 2010 Annual Conference of the North Amer-ican Chapter of the Association for Computa-tional Linguistics, pages 649?652.
Associationfor Computational Linguistics, Los Angeles, Cal-ifornia.
URL http://www.aclweb.org/anthology/N10-1091.Andre Torres Martins, Noah Smith, and Eric Xing.2009.
Concise integer linear programming for-mulations for dependency parsing.
In Proceed-ings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th InternationalJoint Conference on Natural Language Process-ing of the AFNLP, pages 342?350.
Associa-tion for Computational Linguistics, Suntec, Sin-gapore.
URL http://www.aclweb.org/anthology/P/P09/P09-1039.Andre?
Filipe Torres Martins, Dipanjan Das, Noah A.Smith, and Eric P. Xing.
2008.
Stacking de-pendency parsers.
In Proceedings of the 2008Conference on Empirical Methods in NaturalLanguage Processing, pages 157?166.
Associ-ation for Computational Linguistics, Honolulu,Hawaii.
URL http://www.aclweb.org/anthology/D08-1017.Nianwen Xue.
2007.
Tapping the implicit infor-mation for the PS to DS conversion of the Chi-nese treebank.
In Proceedings of the Sixth Inter-national Workshop on Treebanks and LinguisticsTheories.Yi Zhang and Hans-Ulrich Krieger.
2011.
Large-scale corpus-driven pcfg approximation of anhpsg.
In Proceedings of the 12th InternationalConference on Parsing Technologies, pages 198?208.
Association for Computational Linguistics,Dublin, Ireland.
URL http://www.aclweb.org/anthology/W11-2923.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Pro-ceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages562?571.
Association for Computational Linguis-tics, Honolulu, Hawaii.
URL http://www.aclweb.org/anthology/D08-1059.Yue Zhang and Stephen Clark.
2009.
Transition-based parsing of the Chinese treebank using aglobal discriminative model.
In Proceedingsof the 11th International Conference on Pars-ing Technologies (IWPT?09), pages 162?171.
As-sociation for Computational Linguistics, Paris,France.
URL http://www.aclweb.org/anthology/W09-3825.Yue Zhang and Stephen Clark.
2011.
Syntac-tic processing using the generalized perceptronand beam search.
Comput.
Linguist., 37(1):105?151.
URL http://dx.doi.org/10.1162/coli_a_00037.Yue Zhang and Joakim Nivre.
2011.
Transition-based dependency parsing with rich non-local fea-tures.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 188?193.
Association for Computational Linguistics,Portland, Oregon, USA.
URL http://www.aclweb.org/anthology/P11-2033.Tao Zhuang and Chengqing Zong.
2010.
A min-imum error weighting combination strategy forChinese semantic role labeling.
In Proceedingsof the 23rd International Conference on Compu-tational Linguistics (Coling 2010), pages 1362?1370.
Coling 2010 Organizing Committee, Bei-jing, China.
URL http://www.aclweb.org/anthology/C10-1153.314
