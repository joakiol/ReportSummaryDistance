Thumbs Up or Thumbs Down?
Semantic Orientation Applied toUnsupervised Classification of ReviewsPeter D. TurneyInstitute for Information TechnologyNational Research Council of CanadaOttawa, Ontario, Canada, K1A 0R6peter.turney@nrc.caAbstractThis paper presents a simple unsupervisedlearning algorithm for classifying reviewsas recommended (thumbs up) or not rec-ommended (thumbs down).
The classifi-cation of a review is predicted by theaverage semantic orientation of thephrases in the review that contain adjec-tives or adverbs.
A phrase has a positivesemantic orientation when it has good as-sociations (e.g., ?subtle nuances?)
and anegative semantic orientation when it hasbad associations (e.g., ?very cavalier?).
Inthis paper, the semantic orientation of aphrase is calculated as the mutual infor-mation between the given phrase and theword ?excellent?
minus the mutualinformation between the given phrase andthe word ?poor?.
A review is classified asrecommended if the average semantic ori-entation of its phrases is positive.
The al-gorithm achieves an average accuracy of74% when evaluated on 410 reviews fromEpinions, sampled from four differentdomains (reviews of automobiles, banks,movies, and travel destinations).
The ac-curacy ranges from 84% for automobilereviews to 66% for movie reviews.1 IntroductionIf you are considering a vacation in Akumal, Mex-ico, you might go to a search engine and enter thequery ?Akumal travel review?.
However, in thiscase, Google1 reports about 5,000 matches.
Itwould be useful to know what fraction of thesematches recommend Akumal as a travel destina-tion.
With an algorithm for automatically classify-ing a review as ?thumbs up?
or ?thumbs down?, itwould be possible for a search engine to reportsuch summary statistics.
This is the motivation forthe research described here.
Other potential appli-cations include recognizing ?flames?
(abusivenewsgroup messages) (Spertus, 1997) and develop-ing new kinds of search tools (Hearst, 1992).In this paper, I present a simple unsupervisedlearning algorithm for classifying a review as rec-ommended or not recommended.
The algorithmtakes a written review as input and produces aclassification as output.
The first step is to use apart-of-speech tagger to identify phrases in the in-put text that contain adjectives or adverbs (Brill,1994).
The second step is to estimate the semanticorientation of each extracted phrase (Hatzivassi-loglou & McKeown, 1997).
A phrase has a posi-tive semantic orientation when it has goodassociations (e.g., ?romantic ambience?)
and anegative semantic orientation when it has bad as-sociations (e.g., ?horrific events?).
The third step isto assign the given review to a class, recommendedor not recommended, based on the average seman-tic orientation of the phrases extracted from the re-view.
If the average is positive, the prediction isthat the review recommends the item it discusses.Otherwise, the prediction is that the item is notrecommended.The PMI-IR algorithm is employed to estimatethe semantic orientation of a phrase (Turney,2001).
PMI-IR uses Pointwise Mutual Information(PMI) and Information Retrieval (IR) to measurethe similarity of pairs of words or phrases.
The se-1http://www.google.comComputational Linguistics (ACL), Philadelphia, July 2002, pp.
417-424.Proceedings of the 40th Annual Meeting of the Association formantic orientation of a given phrase is calculatedby comparing its similarity to a positive referenceword (?excellent?)
with its similarity to a negativereference word (?poor?).
More specifically, aphrase is assigned a numerical rating by taking themutual information between the given phrase andthe word ?excellent?
and subtracting the mutualinformation between the given phrase and the word?poor?.
In addition to determining the direction ofthe phrase?s semantic orientation (positive or nega-tive, based on the sign of the rating), this numericalrating also indicates the strength of the semanticorientation (based on the magnitude of the num-ber).
The algorithm is presented in Section 2.Hatzivassiloglou and McKeown (1997) havealso developed an algorithm for predicting seman-tic orientation.
Their algorithm performs well, butit is designed for isolated adjectives, rather thanphrases containing adjectives or adverbs.
This isdiscussed in more detail in Section 3, along withother related work.The classification algorithm is evaluated on 410reviews from Epinions2, randomly sampled fromfour different domains: reviews of automobiles,banks, movies, and travel destinations.
Reviews atEpinions are not written by professional writers;any person with a Web browser can become amember of Epinions and contribute a review.
Eachof these 410 reviews was written by a different au-thor.
Of these reviews, 170 are not recommendedand the remaining 240 are recommended (theseclassifications are given by the authors).
Alwaysguessing the majority class would yield an accu-racy of 59%.
The algorithm achieves an averageaccuracy of 74%, ranging from 84% for automo-bile reviews to 66% for movie reviews.
The ex-perimental results are given in Section 4.The interpretation of the experimental results,the limitations of this work, and future work arediscussed in Section 5.
Potential applications areoutlined in Section 6.
Finally, conclusions are pre-sented in Section 7.2 Classifying ReviewsThe first step of the algorithm is to extract phrasescontaining adjectives or adverbs.
Past work hasdemonstrated that adjectives are good indicators ofsubjective, evaluative sentences (Hatzivassiloglou2http://www.epinions.com& Wiebe, 2000; Wiebe, 2000; Wiebe et al, 2001).However, although an isolated adjective may indi-cate subjectivity, there may be insufficient contextto determine semantic orientation.
For example,the adjective ?unpredictable?
may have a negativeorientation in an automotive review, in a phrasesuch as ?unpredictable steering?, but it could havea positive orientation in a movie review, in aphrase such as ?unpredictable plot?.
Therefore thealgorithm extracts two consecutive words, whereone member of the pair is an adjective or an adverband the second provides context.First a part-of-speech tagger is applied to thereview (Brill, 1994).3 Two consecutive words areextracted from the review if their tags conform toany of the patterns in Table 1.
The JJ tags indicateadjectives, the NN tags are nouns, the RB tags areadverbs, and the VB tags are verbs.4 The secondpattern, for example, means that two consecutivewords are extracted if the first word is an adverband the second word is an adjective, but the thirdword (which is not extracted) cannot be a noun.NNP and NNPS (singular and plural proper nouns)are avoided, so that the names of the items in thereview cannot influence the classification.Table 1.
Patterns of tags for extracting two-wordphrases from reviews.First Word Second Word Third Word(Not Extracted)1.
JJ NN or NNS anything2.
RB, RBR, orRBSJJ not NN nor NNS3.
JJ JJ not NN nor NNS4.
NN or NNS JJ not NN nor NNS5.
RB, RBR, orRBSVB, VBD,VBN, or VBGanythingThe second step is to estimate the semantic ori-entation of the extracted phrases, using the PMI-IRalgorithm.
This algorithm uses mutual informationas a measure of the strength of semantic associa-tion between two words (Church & Hanks, 1989).PMI-IR has been empirically evaluated using 80synonym test questions from the Test of English asa Foreign Language (TOEFL), obtaining a score of74% (Turney, 2001).
For comparison, Latent Se-mantic Analysis (LSA), another statistical measureof word association, attains a score of 64% on the3http://www.cs.jhu.edu/~brill/RBT1_14.tar.Z4See Santorini (1995) for a complete description of the tags.same 80 TOEFL questions (Landauer & Dumais,1997).The Pointwise Mutual Information (PMI) be-tween two words, word1 and word2, is defined asfollows (Church & Hanks, 1989):p(word1 & word2)PMI(word1, word2) = log2p(word1) p(word2)(1)Here, p(word1 & word2) is the probability thatword1 and word2 co-occur.
If the words are statisti-cally independent, then the probability that theyco-occur is given by the product p(word1)p(word2).
The ratio between p(word1 & word2) andp(word1) p(word2) is thus a measure of the degreeof statistical dependence between the words.
Thelog of this ratio is the amount of information thatwe acquire about the presence of one of the wordswhen we observe the other.The Semantic Orientation (SO) of a phrase,phrase, is calculated here as follows:SO(phrase) = PMI(phrase, ?excellent?
)- PMI(phrase, ?poor?)
(2)The reference words ?excellent?
and ?poor?
werechosen because, in the five star review rating sys-tem, it is common to define one star as ?poor?
andfive stars as ?excellent?.
SO is positive whenphrase is more strongly associated with ?excellent?and negative when phrase is more strongly associ-ated with ?poor?.PMI-IR estimates PMI by issuing queries to asearch engine (hence the IR in PMI-IR) and notingthe number of hits (matching documents).
The fol-lowing experiments use the AltaVista AdvancedSearch engine5, which indexes approximately 350million web pages (counting only those pages thatare in English).
I chose AltaVista because it has aNEAR operator.
The AltaVista NEAR operatorconstrains the search to documents that contain thewords within ten words of one another, in eitherorder.
Previous work has shown that NEAR per-forms better than AND when measuring thestrength of semantic association between words(Turney, 2001).Let hits(query) be the number of hits returned,given the query query.
The following estimate ofSO can be derived from equations (1) and (2) with5http://www.altavista.com/sites/search/advsome minor algebraic manipulation, if co-occurrence is interpreted as NEAR:SO(phrase) =hits(phrase NEAR ?excellent?)
hits(?poor?
)log2hits(phrase NEAR ?poor?)
hits(?excellent?
)(3)Equation (3) is a log-odds ratio (Agresti, 1996).To avoid division by zero, I added 0.01 to the hits.I also skipped phrase when both hits(phraseNEAR ?excellent?)
and  hits(phrase NEAR?poor?)
were (simultaneously) less than four.These numbers (0.01 and 4) were arbitrarily cho-sen. To eliminate any possible influence from thetesting data, I added ?AND (NOT host:epinions)?to every query, which tells AltaVista not to includethe Epinions Web site in its searches.The third step is to calculate the average seman-tic orientation of the phrases in the given reviewand classify the review as recommended if the av-erage is positive and otherwise not recommended.Table 2 shows an example for a recommendedreview and Table 3 shows an example for a notrecommended review.
Both are reviews of theBank of America.
Both are in the collection of 410reviews from Epinions that are used in the experi-ments in Section 4.Table 2.
An example of the processing of a review thatthe author has classified as recommended.6Extracted Phrase Part-of-SpeechTagsSemanticOrientationonline experience  JJ NN  2.253low fees  JJ NNS  0.333local branch  JJ NN  0.421small part  JJ NN  0.053online service  JJ NN  2.780printable version  JJ NN -0.705direct deposit  JJ NN  1.288well other  RB JJ  0.237inconvenientlylocatedRB VBN -1.541other bank  JJ NN -0.850true service  JJ NN -0.732Average Semantic Orientation  0.3226The semantic orientation in the following tables is calculatedusing the natural logarithm (base e), rather than base 2.
Thenatural log is more common in the literature on log-odds ratio.Since all logs are equivalent up to a constant factor, it makesno difference for the algorithm.Table 3.
An example of the processing of a review thatthe author has classified as not recommended.Extracted Phrase Part-of-SpeechTagsSemanticOrientationlittle difference  JJ NN -1.615clever tricks  JJ NNS -0.040programs such  NNS JJ  0.117possible moment  JJ NN -0.668unethical practices  JJ NNS -8.484low funds  JJ NNS -6.843old man  JJ NN -2.566other problems  JJ NNS -2.748probably wondering  RB VBG -1.830virtual monopoly  JJ NN -2.050other bank  JJ NN -0.850extra day  JJ NN -0.286direct deposits  JJ NNS  5.771online web  JJ NN  1.936cool thing  JJ NN  0.395very handy  RB JJ  1.349lesser evil  RBR JJ -2.288Average Semantic Orientation -1.2183 Related WorkThis work is most closely related to Hatzivassi-loglou and McKeown?s (1997) work on predictingthe semantic orientation of adjectives.
They notethat there are linguistic constraints on the semanticorientations of adjectives in conjunctions.
As anexample, they present the following three sen-tences (Hatzivassiloglou &  McKeown, 1997):1.
The tax proposal was simple and well-received by the public.2.
The tax proposal was simplistic but well-received by the public.3.
(*) The tax proposal  was simplistic andwell-received by the public.The third sentence is incorrect, because we use?and?
with adjectives that have the same semanticorientation (?simple?
and ?well-received?
are bothpositive), but we use ?but?
with adjectives thathave different semantic orientations (?simplistic?is negative).Hatzivassiloglou and McKeown (1997) use afour-step supervised learning algorithm to infer thesemantic orientation of adjectives from constraintson conjunctions:1.
All conjunctions of adjectives are extractedfrom the given corpus.2.
A supervised learning algorithm combinesmultiple sources of evidence to label pairs ofadjectives as having the same semantic orienta-tion or different semantic orientations.
The re-sult is a graph where the nodes are adjectivesand links indicate sameness or difference ofsemantic orientation.3.
A clustering algorithm processes the graphstructure to produce two subsets of adjectives,such that links across the two subsets aremainly different-orientation links, and links in-side a subset are mainly same-orientation links.4.
Since it is known that positive adjectivestend to be used more frequently than negativeadjectives, the cluster with the higher averagefrequency is classified as having positive se-mantic orientation.This algorithm classifies adjectives with accuraciesranging from 78% to 92%, depending on theamount of training data that is available.
The algo-rithm can go beyond a binary positive-negative dis-tinction, because the clustering algorithm (step 3above) can produce a ?goodness-of-fit?
measurethat indicates how well an adjective fits in its as-signed cluster.Although they do not consider the task of clas-sifying reviews, it seems their algorithm could beplugged into the classification algorithm presentedin Section 2, where it would replace PMI-IR andequation (3) in the second step.
However, PMI-IRis conceptually simpler, easier to implement, and itcan handle phrases and adverbs, in addition to iso-lated adjectives.As far as I know, the only prior published workon the task of classifying reviews as thumbs up ordown is Tong?s (2001) system for generating sen-timent timelines.
This system tracks online discus-sions about movies and displays a plot of thenumber of positive sentiment and negative senti-ment messages over time.
Messages are classifiedby looking for specific phrases that indicate thesentiment of the author towards the movie (e.g.,?great acting?, ?wonderful visuals?, ?terriblescore?, ?uneven editing?).
Each phrase must bemanually added to a special lexicon and manuallytagged as indicating positive or negative sentiment.The lexicon is specific to the domain (e.g., movies)and must be built anew for each new domain.
Thecompany Mindfuleye7 offers a technology calledLexant?
that appears similar to Tong?s (2001)system.Other related work is concerned with determin-ing subjectivity (Hatzivassiloglou & Wiebe, 2000;Wiebe, 2000; Wiebe et al, 2001).
The task is todistinguish sentences that present opinions andevaluations from sentences that objectively presentfactual information (Wiebe, 2000).
Wiebe et al(2001) list a variety of potential applications forautomated subjectivity tagging, such as recogniz-ing ?flames?
(Spertus, 1997), classifying email,recognizing speaker role in radio broadcasts, andmining reviews.
In several of these applications,the first step is to recognize that the text is subjec-tive and then the natural second step is to deter-mine the semantic orientation of the subjectivetext.
For example, a flame detector cannot merelydetect that a newsgroup message is subjective, itmust further detect that the message has a negativesemantic orientation; otherwise a message of praisecould be classified as a flame.Hearst (1992) observes that most search en-gines focus on finding documents on a given topic,but do not allow the user to specify the directional-ity of the documents (e.g., is the author in favor of,neutral, or opposed to the event or item discussedin the document?).
The directionality of a docu-ment is determined by its deep argumentativestructure, rather than a shallow analysis of its ad-jectives.
Sentences are interpreted metaphoricallyin terms of agents exerting force, resisting force,and overcoming resistance.
It seems likely thatthere could be some benefit to combining shallowand deep analysis of the text.4 ExperimentsTable 4 describes the 410 reviews from Epinionsthat were used in the experiments.
170 (41%) ofthe reviews are not recommended and the remain-ing 240 (59%) are recommended.
Always guessingthe majority class would yield an accuracy of 59%.The third column shows the average number ofphrases that were extracted from the reviews.Table 5 shows the experimental results.
Exceptfor the travel reviews, there is surprisingly littlevariation in the accuracy within a domain.
In addi-7http://www.mindfuleye.com/tion to recommended and not recommended, Epin-ions reviews are classified using the five star ratingsystem.
The third column shows the correlation be-tween the average semantic orientation and thenumber of stars assigned by the author of the re-view.
The results show a strong positive correla-tion between the average semantic orientation andthe author?s rating out of five stars.Table 4.
A summary of the corpus of reviews.Domain of Review Number ofReviewsAveragePhrases perReviewAutomobiles  75 20.87Honda  Accord         37        18.78Volkswagen Jetta        38        22.89Banks 120 18.52Bank of America        60        22.02Washington Mutual        60        15.02Movies 120 29.13The Matrix       60        19.08Pearl Harbor       60        39.17Travel Destinations  95 35.54Cancun       59        30.02Puerto Vallarta       36        44.58All 410 26.00Table 5.
The accuracy of the classification and the cor-relation of the semantic orientation with the star rating.Domain of Review Accuracy CorrelationAutomobiles 84.00 % 0.4618Honda Accord      83.78 %      0.2721Volkswagen Jetta      84.21 %      0.6299Banks 80.00 % 0.6167Bank of America      78.33 %      0.6423Washington Mutual      81.67 %      0.5896Movies 65.83 % 0.3608The Matrix      66.67 %      0.3811Pearl Harbor      65.00 %      0.2907Travel Destinations 70.53 % 0.4155Cancun      64.41 %      0.4194Puerto Vallarta      80.56 %      0.1447All 74.39 % 0.51745 Discussion of ResultsA natural question, given the preceding results, iswhat makes movie reviews hard to classify?
Table6 shows that classification by the average SO tendsto err on the side of guessing that a review is notrecommended, when it is actually recommended.This suggests the hypothesis that a good moviewill often contain unpleasant scenes (e.g., violence,death, mayhem), and a recommended movie re-view may thus have its average semantic orienta-tion reduced if it contains descriptions of these un-pleasant scenes.
However, if we add a constantvalue to the average SO of the movie reviews, tocompensate for this bias, the accuracy does notimprove.
This suggests that, just as positive re-views mention unpleasant things, so negative re-views often mention pleasant scenes.Table 6.
The confusion matrix for movie classifications.Author?s ClassificationAverageSemanticOrientationThumbsUpThumbsDownSumPositive  28.33 %  12.50 %  40.83 %Negative  21.67 %  37.50 %  59.17 %Sum  50.00 %  50.00 % 100.00 %Table 7 shows some examples that lend supportto this hypothesis.
For example, the phrase ?moreevil?
does have negative connotations, thus an SOof -4.384 is appropriate, but an evil character doesnot make a bad movie.
The difficulty with moviereviews is that there are two aspects to a movie, theevents and actors in the movie (the elements of themovie), and the style and art of the movie (themovie as a gestalt; a unified whole).
This is likelyalso the explanation for the lower accuracy of theCancun reviews: good beaches do not necessarilyadd up to a good vacation.
On the other hand, goodautomotive parts usually do add up to a goodautomobile and good banking services add up to agood bank.
It is not clear how to address this issue.Future work might look at whether it is possible totag sentences as discussing elements or wholes.Another area for future work is to empiricallycompare PMI-IR and the algorithm of Hatzivassi-loglou and McKeown (1997).
Although their algo-rithm does not readily extend to two-word phrases,I have not yet demonstrated that two-word phrasesare necessary for accurate classification of reviews.On the other hand, it would be interesting to evalu-ate PMI-IR on the collection of 1,336 hand-labeledadjectives that were used in the experiments ofHatzivassiloglou and McKeown (1997).
A relatedquestion for future work is the relationship of  ac-curacy of the estimation of semantic orientation atthe level of individual phrases to accuracy of re-view classification.
Since the review classificationis based on an average, it might be quite resistantto noise in the SO estimate for individual phrases.But it is possible that a better SO estimator couldproduce significantly better classifications.Table 7.
Sample phrases from misclassified reviews.Movie:  The MatrixAuthor?s Rating: recommended (5 stars)Average SO: -0.219 (not recommended)Sample Phrase:  more evil    [RBR JJ]SO of SamplePhrase:-4.384Context of SamplePhrase:The slow, methodical way hespoke.
I loved it!
It made himseem more arrogant and evenmore evil.Movie: Pearl HarborAuthor?s Rating: recommended (5 stars)Average SO: -0.378 (not recommended)Sample Phrase:  sick feeling    [JJ NN]SO of SamplePhrase:-8.308Context of SamplePhrase:During this period I had a sickfeeling, knowing what wascoming, knowing what waspart of our history.Movie: The MatrixAuthor?s Rating: not recommended (2 stars)Average SO: 0.177 (recommended)Sample Phrase:  very talented    [RB JJ]SO of SamplePhrase:1.992Context of SamplePhrase:Well as usual Keanu Reeves isnothing special, but surpris-ingly, the very talented Laur-ence Fishbourne is not so goodeither, I was surprised.Movie: Pearl HarborAuthor?s Rating: not recommended (3 stars)Average SO: 0.015 (recommended)Sample Phrase:  blue skies    [JJ NNS]SO of SamplePhrase:1.263Context of SamplePhrase:Anyone who saw the trailer inthe theater over the course ofthe last year will never forgetthe images of Japanese warplanes swooping out of theblue skies, flying past thechildren playing baseball, orthe truly remarkable shot of abomb falling from an enemyplane into the deck of the USSArizona.Equation (3) is a very simple estimator of se-mantic orientation.
It might benefit from more so-phisticated statistical analysis  (Agresti, 1996).
Onepossibility is to apply a statistical significance testto each estimated SO.
There is a large statisticalliterature on the log-odds ratio, which might leadto improved results on this task.This paper has focused on unsupervised classi-fication, but average semantic orientation could besupplemented by other features, in a supervisedclassification system.
The other features could bebased on the presence or absence of specificwords, as is common in most text classificationwork.
This could yield higher accuracies, but theintent here was to study this one feature in isola-tion, to simplify the analysis, before combining itwith other features.Table 5 shows a high correlation between theaverage semantic orientation and the star rating ofa review.
I plan to experiment with ordinal classi-fication of reviews in the five star rating system,using the algorithm of Frank and Hall (2001).
Forordinal classification, the average semantic orienta-tion would be supplemented with other features ina supervised classification system.A limitation of PMI-IR is the time required tosend queries to AltaVista.
Inspection of Equation(3) shows that it takes four queries to calculate thesemantic orientation of a phrase.
However, Icached all query results, and since there is no needto recalculate hits(?poor?)
and hits(?excellent?)
forevery phrase, each phrase requires an average ofslightly less than two queries.
As a courtesy toAltaVista, I used a five second delay between que-ries.8 The 410 reviews yielded 10,658 phrases, sothe total time required to process the corpus wasroughly 106,580 seconds, or about 30 hours.This might appear to be a significant limitation,but extrapolation of current trends in computermemory capacity suggests that, in about ten years,the average desktop computer will be able to easilystore and search AltaVista?s 350 million Webpages.
This will reduce the processing time to lessthan one second per review.6 ApplicationsThere are a variety of potential applications forautomated review rating.
As mentioned in the in-8This line of research depends on the good will of the majorsearch engines.
For a discussion of the ethics of Web robots,see http://www.robotstxt.org/wc/robots.html.
For query robots,the proposed extended standard for robot exclusion would beuseful.
See http://www.conman.org/people/spc/robots2.html.troduction, one application is to provide summarystatistics for search engines.
Given the query?Akumal travel review?, a search engine could re-port, ?There are 5,000 hits, of which 80% arethumbs up and 20% are thumbs down.?
The searchresults could be sorted by average semantic orien-tation, so that the user could easily sample the mostextreme reviews.
Similarly, a search engine couldallow the user to specify the topic and the rating ofthe desired reviews (Hearst, 1992).Preliminary experiments indicate that semanticorientation is also useful for summarization of re-views.
A positive review could be summarized bypicking out the sentence with the highest positivesemantic orientation and a negative review couldbe summarized by extracting the sentence with thelowest negative semantic orientation.Epinions asks its reviewers to provide a shortdescription of pros and cons for the reviewed item.A pro/con summarizer could be evaluated bymeasuring the overlap between the reviewer?s prosand cons and the phrases in the review that havethe most extreme semantic orientation.Another potential application is filtering?flames?
for newsgroups (Spertus, 1997).
Therecould be a threshold, such that a newsgroup mes-sage is held for verification by the human modera-tor when the semantic orientation of a phrase dropsbelow the threshold.
A related use might be a toolfor helping academic referees when reviewingjournal and conference papers.
Ideally, referees areunbiased and objective, but sometimes their criti-cism can be unintentionally harsh.
It might be pos-sible to highlight passages in a draft referee?sreport, where the choice of words should be modi-fied towards a more neutral tone.Tong?s (2001) system for detecting and track-ing opinions in on-line discussions could benefitfrom the use of a learning algorithm, instead of (orin addition to) a hand-built lexicon.
With auto-mated review rating (opinion rating), advertiserscould track advertising campaigns, politicianscould track public opinion, reporters could trackpublic response to current events, stock traderscould track financial opinions, and trend analyzerscould track entertainment and technology trends.7 ConclusionsThis paper introduces a simple unsupervised learn-ing algorithm for rating a review as thumbs up ordown.
The algorithm has three steps: (1) extractphrases containing adjectives or adverbs, (2) esti-mate the semantic orientation of each phrase, and(3) classify the review based on the average se-mantic orientation of the phrases.
The core of thealgorithm is the second step, which uses PMI-IR tocalculate semantic orientation (Turney, 2001).In experiments with 410 reviews from Epin-ions, the algorithm attains an average accuracy of74%.
It appears that movie reviews are difficult toclassify, because the whole is not necessarily thesum of the parts; thus the accuracy on movie re-views is about 66%.
On the other hand, for banksand automobiles, it seems that the whole is the sumof the parts, and the accuracy is 80% to 84%.Travel reviews are an intermediate case.Previous work on determining the semantic ori-entation of adjectives has used a complex algo-rithm that does not readily extend beyond isolatedadjectives to adverbs or longer phrases (Hatzivassi-loglou and McKeown, 1997).
The simplicity ofPMI-IR may encourage further work with semanticorientation.The limitations of this work include the timerequired for queries and, for some applications, thelevel of accuracy that was achieved.
The formerdifficulty will be eliminated by progress in hard-ware.
The latter difficulty might be addressed byusing semantic orientation combined with otherfeatures in a supervised classification algorithm.AcknowledgementsThanks to Joel Martin and Michael Littman forhelpful comments.ReferencesAgresti, A.
1996.
An introduction to categorical dataanalysis.
New York: Wiley.Brill, E. 1994.
Some advances in transformation-basedpart of speech tagging.
Proceedings of the TwelfthNational Conference on Artificial Intelligence (pp.722-727).
Menlo Park, CA: AAAI Press.Church, K.W., & Hanks, P. 1989.
Word associationnorms, mutual information and lexicography.
Pro-ceedings of the 27th Annual Conference of the ACL(pp.
76-83).
New Brunswick, NJ: ACL.Frank, E., & Hall, M. 2001.
A simple approach to ordi-nal classification.
Proceedings of the Twelfth Euro-pean Conference on Machine Learning (pp.
145-156).
Berlin: Springer-Verlag.Hatzivassiloglou, V., & McKeown, K.R.
1997.
Predict-ing the semantic orientation of adjectives.
Proceed-ings of the 35th Annual Meeting of the ACL and the8th Conference of the European Chapter of the ACL(pp.
174-181).
New Brunswick, NJ: ACL.Hatzivassiloglou, V., & Wiebe, J.M.
2000.
Effects ofadjective orientation and gradability on sentence sub-jectivity.
Proceedings of 18th International Confer-ence on Computational Linguistics.
New Brunswick,NJ: ACL.Hearst, M.A.
1992.
Direction-based text interpretationas an information access refinement.
In P.
Jacobs(Ed.
), Text-Based Intelligent Systems: Current Re-search and Practice in Information Extraction andRetrieval.
Mahwah, NJ: Lawrence Erlbaum Associ-ates.Landauer, T.K., & Dumais, S.T.
1997.
A solution toPlato?s problem: The latent semantic analysis theoryof the acquisition, induction, and representation ofknowledge.
Psychological Review, 104, 211-240.Santorini, B.
1995.
Part-of-Speech Tagging Guidelinesfor the Penn Treebank Project (3rd revision, 2ndprinting).
Technical Report, Department of Computerand Information Science, University of Pennsylvania.Spertus, E. 1997.
Smokey: Automatic recognition ofhostile messages.
Proceedings of the Conference onInnovative Applications of Artificial Intelligence (pp.1058-1065).
Menlo Park, CA: AAAI Press.Tong, R.M.
2001.
An operational system for detectingand tracking opinions in on-line discussions.
WorkingNotes of the ACM SIGIR 2001 Workshop on Opera-tional Text Classification (pp.
1-6).
New York, NY:ACM.Turney, P.D.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
Proceedings of theTwelfth European Conference on Machine Learning(pp.
491-502).
Berlin: Springer-Verlag.Wiebe, J.M.
2000.
Learning subjective adjectives fromcorpora.
Proceedings of the 17th National Confer-ence on Artificial Intelligence.
Menlo Park, CA:AAAI Press.Wiebe, J.M., Bruce, R., Bell, M., Martin, M., & Wilson,T.
2001.
A corpus study of evaluative and specula-tive language.
Proceedings of the Second ACL SIGon Dialogue Workshop on Discourse and Dialogue.Aalborg, Denmark.
