Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270?1279,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPFinding Short Definitions of Terms on Web PagesGerasimos Lampouras?and Ion Androutsopoulos?+?Department of Informatics, Athens University of Economics and Business, Greece+Digital Curation Unit, Research Centre ?Athena?, Athens, GreeceAbstractWe present a system that finds short def-initions of terms on Web pages.
It em-ploys a Maximum Entropy classifier, but itis trained on automatically generated ex-amples; hence, it is in effect unsupervised.We use ROUGE-W to generate training ex-amples from encyclopedias and Web snip-pets, a method that outperforms an alter-native centroid-based one.
After training,our system can be used to find definitionsof terms that are not covered by encyclo-pedias.
The system outperforms a compa-rable publicly available system, as well asa previously published form of our system.1 IntroductionDefinitions of terms are among the most com-mon types of information users search for on theWeb.
In the TREC 2001 QA track (Voorhees,2001), where the distribution of question types re-flected that of real user logs, 27% of the ques-tions were requests for definitions (e.g., ?What isgasohol?
?, ?Who was Duke Ellington??).
Conse-quently, some Web search engines provide specialfacilities (e.g., Google?s ?define:?
query prefix)that seek definitions of user-specified terms in on-line encyclopedias or glossaries; to save space, wecall both ?encyclopedias?.
There are, however, of-ten terms that are too recent, too old, or less widelyused to be included in encyclopedias.
Their defi-nitions may be present on other Web pages (e.g.,newspaper articles), but they may be provided in-directly (e.g., ?He said that gasohol, a mixture ofgasoline and ethanol, has been great for his busi-ness.?)
and they may be difficult to locate withgeneric search engines that may return dozens ofpages containing, but not defining the terms.We present a system to find short definitionsof user-specified terms on Web pages.
It can beused as an add-on to generic search engines, whenno definitions can be found in on-line encyclope-dias.
The system first invokes a search engine us-ing the (possibly multi-word) term whose defini-tion is sought, the target term, as the query.
Itthen scans the top pages returned by the searchengine to locate 250-character snippets with thetarget term at their centers; we call these snippetswindows.
The windows are candidate definitionsof the target term, and they are then classified asacceptable (positive class) or unacceptable (nega-tive class) using supervised machine learning.
Thesystem reports the windows for which it is mostconfident that they belong in the positive class.
Ta-ble 1 shows examples of short definitions found byour system.
In our experiments, we allow the sys-tem to return up to five windows per target term,and the system?s response is counted as correct ifany of the returned windows contains an accept-able short definition of the target.
This is similarto the treatment of definition questions in TREC2000 and 2001 (Voorhees, 2000; Voorhees, 2001),but the answer is sought on the Web, not in a givendocument collection of a particular genre.More recent TREC QA tracks required definitionquestions to be answered by lists of complemen-tary text snippets, jointly providing required or op-tional information nuggets (Voorhees, 2003).
Incontrast, we focus on locating single snippets thatinclude self-contained short definitions.
Despiteits simpler nature, we believe the task we addressis of practical use: a list of single-snippet defini-tions from Web pages accompanied by the sourceURLs is a good starting point for users seekingdefinitions of terms not covered by encyclopedias.We also note that evaluating multi-snippet defini-tions can be problematic, because it is often dif-ficult to agree which information nuggets shouldbe treated as required, or even optional (Hilde-brandt et al, 2004).
In contrast, earlier experimen-tal results we have reported (Androutsopoulos andGalanis, 2005) show strong inter-assessor agree-ment (K > 0.8) for single-snippet definitions (Eu-genio and Glass, 2004).
The task we address alsodiffers from DUC?s query focused summarization(Dang, 2005; Dang, 2006).
Our queries are sin-gle terms, whereas DUC queries are longer topic1270Target term: Babesiosis(...) Babesiosis is a rare, severe and sometimes fatal tick-borne disease caused by various types of Babesia, a micro-scopic parasite that infects red blood cells.
In New Yorkstate, the causative parasite is babesia microti.
Who getsBabesiosis?
Babesiosis (...)Target term: anorexia nervosa(...) anorexia nervosa is an illness that usually occurs inteenage girls, but it can also occur in teenage boys, and adultwomen and men.
People with anorexia are obsessed withbeing thin.
They lose a lot of weight and are terrified ofgaining weight.
The (...)Target term: Kinabalu(...) one hundred and thirty eight kilometers from Kota Kin-abalu, the capital of the Malaysian state of Sabah, rises themajestic mount Kinabalu.
With its peak at 4,101 meters(and growing), mount Kinabalu is the highest mountain insouth-east Asia.
This (...)Target term: Pythagoras(...) Pythagoras of Samos about 569 BC - about 475BC click the picture above to see eleven larger picturesPythagoras was a Greek philosopher who made importantdevelopments in mathematics, astronomy, and the theory ofmusic.
The theorem now known as (...)Target term: Sacajawea(...) Sacajawea was a Shoshone Indian princess.
TheShoshone lived from the rocky mountains to the plains.They lived primarily on buffalo meat.
The shoshone trav-eled for many days searching for buffalo.
They hunted onhorseback using the buffalo for food (...)Target term: tale of Genji(...) the tale of Genji This site aims to promote a widerunderstanding and appreciation of the tale of Genji - the11th century Japanese classic written by a Heian court ladyknown asMurasaki Shikibu.
It also serves as a kind of travelguide to the world (...)Target term: Jacques Lacan(...) who is Jacques Lacan?
John Haber in New Yorkcity a primer for pre-post-structuralists Jacques Lacan is aParisian psychoanalyst who has influenced literary criticismand feminism.
He began work in the 1950s, in the Freudiansociety there.
It was a (...)Table 1: Definitions found by our system.descriptions, often entire paragraphs; furthermore,we do not attempt to compose coherent and cohe-sive summaries from several snippets.The system we present is based on our ear-lier work (Miliaraki and Androutsopoulos, 2004),where an SVM classifier (Cristianini and Shawe-Taylor, 2000) was used to separate acceptable win-dows from unacceptable ones; the SVM also re-turned confidence scores, which were used to rankthe acceptable windows.
On datasets from theTREC 2000 and 2001 QA tracks, our earlier sys-tem clearly outperformed the methods of Joho andSanderson (2000; 2001) and Prager et al (2001;2002), as reported in previous work (Miliarakiand Androutsopoulos, 2004).
To train the SVM,however, thousands of training windows were re-quired, each tagged as a positive or negative exam-ple.
Obtaining large numbers of training windowsis easy, but manually tagging them is very time-consuming.
In the TREC 2000 and 2001 datasets,it was possible to tag the training windows auto-matically by using training target terms and ac-companying regular expression patterns providedby the TREC organizers.
The regular expressionscovered all the known acceptable definitions of thecorresponding terms that can be extracted from thedatasets.
When the training windows, however,are obtained from the Web, it is impossible to con-struct manually regular expressions for all the pos-sible phrasings of the acceptable definitions in thetraining windows.In subsequent work (Androutsopoulos andGalanis, 2005), we developed ATTW (automatictagging of training windows), a technique that pro-duces arbitrarily large collections of training win-dows from the Web with practically no manualeffort, in effect making our overall system unsu-pervised.
ATTW uses training terms for whichseveral encyclopedia definitions are available, andcompares each Web training window (each win-dow extracted from the pages the search enginereturned for a training term) to the correspondingencyclopedia definitions.
Web training windowsthat are very similar (or dissimilar) to the corre-sponding encyclopedia definitions are tagged aspositive (or negative) examples; if the similarity isneither too high nor too low, the window is not in-cluded in the classifier?s training data.
Previouslyreported experiments (Androutsopoulos and Gala-nis, 2005) showed that ATTW leads to significantlybetter results, compared to training the classifieron all the available TREC windows, for which reg-ular expressions are available, and then using it toclassify Web windows.Note that in ATTW the encyclopedia definitionsare used only during training.
Once the classifierhas been trained, it can be used to discover defini-tions on arbitrary Web pages.
In fact, during test-ing we discard windows originating from on-lineencyclopedias, simulating the case where we seekdefinitions of terms not covered by encyclopedias;we also ignore windows from on-line encyclope-dias during training.
Also, note that the classifieris trained on Web windows, not directly on ency-clopedia definitions, which allows it to avoid rely-ing excessively on phrasings that are common inencyclopedia definitions, but uncommon in moreindirect definitions of arbitrary Web pages.
Fur-1271thermore, training the classifier directly on ency-clopedia definitions would not provide negativeexamples.In our previous work with ATTW (Androut-sopoulos and Galanis, 2005) we used a mea-sure constructed by ourselves to assess the sim-ilarity between Web windows and encyclopediadefinitions.
Here, we use the more establishedROUGE-W measure (Lin, 2004) instead.
ROUGE-W and other versions of ROUGE have been used insummarization to measure how close a machine-authored summary is to multiple human sum-maries of the same input.
We use ROUGE-W ina similar setting, to measure how close a trainingwindow is to multiple encyclopedia definitions ofthe same term.
A further difference from our pre-vious work is that we also use ROUGE-W whencomputing the features of the windows to be clas-sified.
Previously, the SVM relied, among others,on Boolean features indicating if the target termwas preceded or followed in the window to beclassified by a particular phrase indicating a def-inition (e.g., ?target, a kind of?, ?such as target?
).The indicative phrases are selected automaticallyduring training, but now the corresponding fea-tures are not Boolean; their values are the ROUGE-W similarity scores between an indicative phraseand the context of the target term in the window.This allows the system to soft-match the phrasesto the windows (e.g., encountering ?target, anotherkind of?, instead of ?target, a kind of?
).1In our new system we also use a Maximum En-tropy (MAXENT) classifier (Ratnaparkhi, 1997) in-stead of an SVM, because much faster implemen-tations of the former are available.2We presentexperimental results showing that our new sys-tem significantly outperforms our previously pub-lished one.
The use of the MAXENT classifier by it-self improved slightly our results, but the improve-ments come mostly from using ROUGE-W.Apart from presenting an improved version ofour system, the main contribution of this paper is adetailed experimental comparison of our new sys-tem against Cui et al?s (2004; 2005; 2006; 2007).The latter is particularly interesting, because itis well published, it includes both an alterna-tive, centroid-based technique to automatically tagtraining examples and a soft-matching classifier,1We also experimented with other similarity measures(e.g., edit distance) and ROUGE variants, but we obtained thebest results with ROUGE-W.2We use Stanford?s classifier; see http://nlp.stanford.edu/.and it is publicly available.3We show that ATTWoutperforms Cui et al?s centroid-based technique,and that our overall system is also clearly betterthan Cui et al?s in the task we address.Section 2 discusses ATTW with ROUGE-W, Cuiet al?s centroid-based method to tag training ex-amples, and experiments showing that ATTW isbetter.
Section 3 describes our new overall system,the system of Cui et al, and the baselines.
Sec-tion 4 reports experimental results showing thatour system is better than Cui et al?s, and betterthan our previously published system.
Section 5discusses related work; and section 6 concludes.2 Tagging training windowsDuring both training and testing, for each tar-get term we keep the r most highly ranked Webpages the search engine returns.
We then extractthe first f windows of the target term from eachpage, since early occurrences of the target termson pages are more likely to be definitions.
We,thus, obtain r ?
f windows per term.4When test-ing, we return the k windows of the target termthat the classifier is most certain they belong in thepositive class.
In our experiments, r = 10, f = 5,k = 5.
During training, we train the classifier onthe q ?
r ?
f windows we obtain for q training tar-get terms; in our experiments, q ranged from 50 to1500.
Training requires tagging first the trainingwindows as positive or negative, possibly discard-ing windows that cannot be tagged automatically.2.1 ATTW with ROUGE-W similarityTo tag a training window w of a training term twith ATTW and ROUGE-W, we obtain a set Ctofdefinitions of t from encyclopedias.5Stop-words,punctuation, and non-alphanumeric characters areremoved from Ctand w, and a stemmer is ap-plied; the testing windows undergo the same pre-processing.6For each definition d ?
Ct, we findthe longest common word subsequence of w andd.
If w is the word sequence ?A,B, F,C,D,E?3See http://www.cuihang.com/software.html.
The soft-ware and a demo of our system, and the datasets we usedare also freely available; see http://nlp.cs.aueb.gr/.4We used Altavista in our experiments.
We remove HTMLtags and retain only the plain text of the pages.5The training terms were randomly selected from the in-dex of http://www.encyclopedia.com/.
We used Google?s?define:?
to obtain definitions from other encyclopedias.6We use the 100 most frequent words of the BNC corpus(http://www.natcorp.ox.ac.uk/) as the stop-list, and Porter?sstemmer (http://tartarus.org/?martin/PorterStemmer/).1272and d = ?A,B,E,C,G,D?, the longest com-mon subsequence is ?A,B,C,D?.
The longestcommon subsequence is divided into consecutivematches, producing in our example ?A,B|C|D?.We then compute the following score (weightedlongest common subsequence), where m is thenumber of consecutive matches, kiis the lengthof the i-th consecutive match, and f is a weight-ing function.
We use f(k) = ka, where a > 1 is aparameter we tune experimentally.WLCS (w, d) =?mi=0f(ki)We then compute the following quantities, where|?| is word length, and f?1is the inverse of f .P (w, d) = f?1(WLCS(w,d)f(|w|))R(w, d) = f?1(WLCS(w,d)f(|d|))F (w, d) =(1+?2)?R(w,d)?P (w,d)R(w,d)+?2?P (w,d)In effect, P (w, d) examines how close thelongest common substring is to w and R(w, d)how close it is to d. Following Lin (2004), we use?
= 8, assigning greater importance toR(w, d).
IfR(w, d) is high, the longest common substring isvery similar to d; then w (which also includes thelongest common substring) intuitively contains al-most all the information of d, i.e., all the informa-tion of a known acceptable definition (high recall).If P (w, d) is high, the longest common substringis very similar to w; then d (which also includesthe longest common substring) contains almost allthe information of w, i.e., w does not contain any(redundant) information not included in a knownacceptable definition, something we care less for.The ROUGE-W similarity sim(w,Ct) betweenw and Ctis the maximum F (w, d), for all d ?Ct.
Training windows with sim(w,Ct) > T+aretagged as positive; if sim(w,Ct) < T?, they aretagged as negative; and if T??
sim(w,Ct) ?T+, they are discarded.
We tune the thresholds T+and T?experimentally, as discussed below.2.2 The centroid-based tagging approachThis method is used in the system of Cui et al(2004; 2005; 2006; 2007).
For each training targetterm, we construct a ?centroid?
pseudo-text con-taining the words that co-occur most frequentlywith the target term.
We then compute the similar-ity between each training window and the centroidof its target term.
If it exceeds a threshold, the win-dow is tagged as positive; Cui et al produce onlypositive examples.The centroid of a training target term t is con-structed as follows.
For each word u in t?s trainingwindows, we compute the centrality score definedbelow, where SFtis the number of t?s trainingwindows, SFuis the number of u?s windows thatcan be extracted from the retained Web pages thesearch engine returned for t, SFt?uis the numberof windows on the same pages that contain botht and u, and idf(u) is the inverse document fre-quency of w.7Centrality scores are pointwise mu-tual information with an extra idf (u) factor.centrality(u) = ?log(SFt?uSFt+SFu) ?
idf (u)The words u whose centrality scores exceed themean by at least a standard deviation are addedto the centroid of t. Before computing the cen-trality scores, stop-words, punctuation, and non-alphanumeric characters are removed, and a stem-mer is applied, as in ATTW.
The similarities be-tween training windows and centroids are thencomputed using cosine similarity, after turning thecentroids and windows into binary vectors thatshow which words they contain.2.3 Comparing the tagging approachesTo evaluate the two methods that tag training win-dows, we selected randomly q = 200 target terms,different from those used for training and testing.We collected the q ?
r ?
f = 200 ?
10 ?
5 windowsfrom the corresponding Web pages, we selectedrandomly 400 from the collected 10,000 windows,and tagged them manually as positive or negative.Figure 1 plots the positive precision of the twomethods against their positive recall, and figure 2shows negative precision against negative recall.For different values of T+, we obtain a differentpoint in figure 1; similarly for T?and figure 2.Positive precision is TP/(TP +FP), positive re-call is TP/(TP + FN ), and likewise for nega-tive precision and recall; TP (true positives) arethe positive training windows the method has cor-rectly tagged as positive, FP are the negative win-dows the method has tagged as positives etc.For very high (strict) T+values, the methods tagvery few (or none) training windows as positive;hence, both TP and TP + FP approach (or be-come) zero; we take positive precision to be zeroin that case.
Positive recall also approaches (or be-comes) zero, which is why both positive recall and7We obtained idf (u) from BNC.
Cui et al use sentencesinstead of windows, reducing the risk of truncating defini-tions.
We used windows in all systems, to compare fairly.1273Figure 1: Results of generating positive examples.Figure 2: Results of generating negative examples.precision reach zero in the left of figure 1.
Simi-lar comments apply to figure 2, though both meth-ods always tagged correctly at least a few trainingwindows as negative, for the T?values we tried;hence, negative precision was never zero.Positive precision shows how certain we can bethat training windows tagged as positive are in-deed positive; whereas positive recall is the per-centage of true positive examples that we manageto tag as such.
Figure 1 shows that when usingATTW, we need to settle for a low positive recall,i.e., miss out many positive examples, in order toobtain a reasonably high precision.
It also showsthat the centroid method is clearly worse when tag-ging positive examples; its positive precision is al-most always less than 0.3.
Figure 2 shows thatboth methods achieve high negative precision andrecall; they manage to assign trustworthy nega-tive labels without missing many negative exam-ples.
However, ATTW is significantly better whentagging positive examples, as shown in figure 1;hence, it is better than the centroid method.88We tried different values of ROUGE-W?s a parameter inWhen using ATTW in practice, we need to se-lect T+and T?.
We assign more importance toselecting a T+(a point of ATTW?s curve in figure1) that yields high positive precision; the choice ofT?
(point in figure 2) is less important, becauseATTW?s negative precision is always reasonablyhigh.
Based on figure 1, we set T+to 0.58, whichcorresponds to positive precision 0.66 and posi-tive recall 0.16.
By tuning the two thresholds wecan control the number of positively or negativelytagged examples we produce (and their ratio), andthe number of examples we discard.
Having setT+, we set T?to 0.30, a value that maintains theratio of truly positive to truly negative windowsof the 400 manually tagged windows (0.2 to 1),since this is approximately the ratio the classifierwill confront during testing; we also experimentedwith a 1 to 1 ratio, but the results were worse.
ThisT?value corresponds negative precision 0.70 andnegative recall 0.02.
Thus, both positive and neg-ative precision is approximately 0.7, which meansthat approximately 30% of the tags we assign tothe examples are incorrect.
Our experiments, how-ever, indicate that the classifier is able to general-ize well over this noise.3 Finding new definitionsWe now present our overall system, the system ofCui et al, and the baselines.3.1 Our systemGiven a target term, our system extracts r ?
f =10 ?
5 windows from the pages returned by thesearch engine, and uses the MAXENT classifier toseparate them into acceptable and unacceptabledefinitions.9It then returns the k = 5 windowsthe classifier is most confident they are acceptable.The classifier is trained on windows tagged as pos-itive or negative using ATTW.
It views each win-dow as a vector of the following features:10SN: The ordinal number of the window on thepage it originates from (e.g., second window of thetarget term from the beginning of the page).
Earlymentions of a term are more likely to define it.RK: The ranking of the Web page the windoworiginates from, as returned by the search engine.the interval (1, 2].
We use a = 1.4, which was the value withthe best results on the 400 windows.
We did not try a > 2, asthe results were declining as a approached 2.9We do not discuss MAXENT classifiers, since they are awell documented in the literature.10SN andWC originate from Joho and Sanderson (2000).1274WC: We create a simple centroid of the window?starget term, much as in section 2.2.
The centroid?swords are chosen based on their frequency in the r?f windows of the target term; the 20 most frequentwords are chosen.
WC is the percentage of the 20words that appear in the vector?s window.Manual patterns: 13 Boolean features, each sig-naling if the window matches a different manuallyconstructed lexical pattern (e.g., ?target, a/an/the?,as in ?Tony Blair, the British prime minister?
).The patterns are those used by Joho and Sander-son (2000), and four more introduced in our pre-vious work (Androutsopoulos and Galanis, 2005)and (Miliaraki and Androutsopoulos, 2004).
Theyare intended to perform well across text genres.Automatic patterns: m numeric features, eachshowing the degree to which the window matchesa different automatically acquired lexical pattern.The patterns are word n-grams (n ?
{1, 2, 3}) thatmust occur directly before or after the target term(e.g., ?target which is?).
The patterns are acquiredas follows.
First, all the n-grams directly beforeor after any target term in the training windowsare collected.
The n-grams that have been en-countered at least 10 times are candidate patterns.From those, the m patterns with the highest pre-cision scores are retained, where precision is thenumber of positive training windows the patternmatches over the total number of training windowsit matches; we use m = 300 in our experiments,based on the results of our previous work.
The au-tomatically acquired patterns allow the system todetect definition contexts that are not captured bythe manual patterns, including genre-specific con-texts.
The value of each feature is the ROUGE-Wscore between a pattern and the left or right con-text of the target term in the window.3.2 Cui et al?s systemGiven a target term t, Cui et al (2004; 2005; 2006;2007) initially locate sentences containing t in rel-evant documents.
We use the r?f = 10?5windowsfrom the pages returned by the search engine, in-stead of sentences.
Cui et al then construct thecentroid of t, and compute the cosine similarity ofeach one of the r ?
f windows to the centroid, asin section 2.2.
The 10 windows that are closer tothe centroid are considered candidate answers.
Allcandidate answers are then processed by a part-of-speech (POS) tagger and a chunker.
The wordsof the centroid are replaced in all the candidateanswers by their POS tags; the target term, nounphrases, forms of the verb ?to be?, and articlesare replaced by special tags (e.g., TARGET, NP),while adjectives and adverbs are removed.
Thecandidate answers are then cropped to L tokensto the left and right of the target term, producingtwo subsequences (left and right) per candidate an-swer; we set L = 3, which is Cui et al?s default.Cui et al experimented with two approaches torank the candidate answers, called Bigram Modeland Profile Hidden Markov Model (PHMM).
Bothare learning components that produce soft pat-terns, though PHMM is much more complicated.
Intheir earlier work, Cui et al (2005) found the Bi-gramModel to perform better than PHMM; in morerecent experiments with more data (Cui, 2006; Cuiet al, 2007) they found PHMM to perform better,but the difference was not statistically significant.Given these results and the complexity of PHMM,we experimented only with the Bigram Model.In the Bigram Model, the left and right subse-quences of each candidate answer are consideredseparately.
Below S1, .
.
.
, SLrefer to the slots(word positions) of a (left or right) subsequence,and t1, .
.
.
, tLto the particular words in the slots.For each subsequence ?S1= t1, .
.
.
, SL= tL?
ofa candidate answer, we first estimate:P (ti|Si) =|Si(ti)| + ?
?t?|Si(ti)| + ?
?NP (ti|ti?1) =|Si(ti) ?
Si?1(ti?1)||Si(ti)|P (ti|Si) is the probability that tiwill appear inslot Siof a left or right subsequence (depending onthe subsequence considered) of an acceptable can-didate answer.
P (ti|ti?1) is the probability thattiwill follow ti?1in a (left or right) subsequenceof an acceptable candidate answer.
Cui et al useonly positive training examples, generated by thecentroid-based approach of section 2.2.
|Si(ti)| isthe number of times tiappeared in Siin the (leftor right) subsequences of the training examples.t?ranges over all the words that occurred in Siinthe training examples.
|Si(ti) ?
Si?1(ti?1)| is thenumber of times tiand ti?1co-occurred in the cor-responding slots in the training examples.
N is thenumber of different words that occurred in the (leftor right) training subsequences, and ?
is a constantset to 2, as in Cui et al?s experiments.
FollowingCui et al, if tiis a POS or other special tag thenthe probabilities above are estimated by counting1275only the tags of the training examples.
Similarly,if tiis an actual word, only the actual words (nottags) of the training examples are considered.The probability of each subsequence could thenbe estimated as:P (t1, .
.
.
, tL) = P (t1|S1) ?L?i=2(?
?
P (ti|ti?1) + (1 ?
?)
?
P (ti|Si))Instead, Cui et al use the following scoring mea-sure, which also accounts for the fact that somesubsequences may have length l < L. They tune?
by Expectation Maximization.Pnorm(t1, .
.
.
, tL) =1l?
[logP (t1|S1) +L?i=2log(?
?
P (ti|ti?1) + (1 ?
?)
?
P (ti|Si))]The overall score of a candidate answer is then:P = (1 ?
?)
?
Pnorm(left) + ?
?
Pnorm(right)Again, Cui et al tune a by Expectation Maximiza-tion.
Instead, we tuned ?
and ?
by a grid searchin [0, 1] ?
[0, 1], with step 0.1 for both parame-ters.
For the tuning, we trained Cui et al?s systemon 2,000 randomly selected target terms, exclud-ing terms used for other purposes.
We used 160manually tagged windows to evaluate the system?sperformance with the different values of ?
and ?
;the 160 windows were selected randomly from the10,000 windows of section 2.3, after excluding the400 manually tagged windows of that section.
Theresulting values for ?
and ?
were 0.7 and 0.6, re-spectively.
Apart from the modifications we men-tioned, we use Cui et al?s original implementation.3.3 Baseline methodsThe first baseline selects the first window of eachone of the five highest ranked Web pages, as re-turned by the search engine, and returns the fivewindows.
The second baseline returns five win-dows chosen randomly from the r ?
f = 10 ?
5available ones.
The third baseline (centroid base-line) creates a centroid of the r ?
f windows, as insection 2.2, and returns the five windows with thehighest cosine similarity to the centroid.1111We also reimplemented the definitions component ofChu-Carroll et al (2004; 2005), but its performance wasworse than our centroid baseline.Figure 3: Correct responses, 5 answers/question.4 Evaluation of systemsWe used q training target terms in the experi-ments of this section, with q ranging from 50 to1500, and 200 testing terms, with no overlap be-tween training and testing terms, and excludingterms that had been used for other purpose.12Wehad to use testing terms for which encyclopediadefinitions were also available, to judge the ac-ceptability of the systems?
responses, since manyterms are highly technical.
We discarded, how-ever, windows extracted from encyclopedia pageswhen testing, simulating the case where the targetterms are not covered by encyclopedias.As already mentioned, for each target term weextract r ?
f = 10 ?
5 windows (or fewer, if fewerare available) from the pages the search engine re-turns.
We then provide these windows to each ofthe systems, allowing them to return up to k = 5windows, ordered by decreasing confidence.
Ifany of the k windows contains an acceptable shortdefinition of the target term, as judged by a hu-man evaluator, the system?s response is counted ascorrect.
We also calculate the Mean ReciprocalRank (MRR) of each system?s responses, as in theTREC QA track: if the first acceptable definition ofa response is in the j-th position (1 ?
j ?
k), theresponse?s score is 1/j; MRR is the mean of the re-sponses?
scores, i.e., it rewards systems that returnacceptable definitions higher in their responses.Figures 3 and 4 show the results of our experi-ments as percentage of correct responses and MRR,respectively; the error bars of figure 3 correspondto 95% confidence intervals.
Our system clearlyoutperforms Cui et al?s, despite the fact that the12The reader is reminded that all terms were selected ran-domly from the index of an on-line encyclopedia.1276Figure 4: MRR scores, 5 answers per question.latter uses more linguistic resources (a POS tag-ger and a chunker).
Both systems outperform thebaselines, of which the centroid baseline is thebest, and both systems perform better as the sizeof the training set increases.
The baselines con-tain no learning components; hence, their curvesare flat.
We also show the results (Base-Attrs)of our system when the features that correspondto automatically acquired patterns are excluded.Clearly, these patterns help our system achievesignificantly better results; however, our systemoutperforms Cui et al?s even without them.
With-out the automatic patterns, our system also showssigns of saturation as the training data increase.Figures 5 and 6 show the performance of ournew system against our previously published one(Androutsopoulos and Galanis, 2005); the newsystem clearly outperforms the old one.
Addi-tional experiments we conducted with the old sys-tem replacing the SVM by the MAXENT classifier(without using ROUGE-W) indicate that the use ofMAXENT by itself also improved slightly the re-sults, but the differences are too minor to show; theimprovement is mostly due to the use of ROUGE-W instead of our previous measure.5 Related workXu et al (2004) use an information extraction en-gine to extract linguistic features from documentsrelevant to the target term.
The features are mostlyphrases, such as appositives, and phrases express-ing relations.
The features are then ranked by theirtype and similarity to a centroid, and the mosthighly ranked ones are returned.
Xu et al seemto aim at generating multi-snippet definitions, un-like the single-snippet definitions we seek.Blair-Goldensohn et al (2003; 2004) extractsentences that may provide definitional informa-Figure 5: Correct responses of our new and previ-ous system, allowing 5 answers per question.Figure 6: MRR of our new and previous system.tion from documents retrieved for the target term;a decision tree learner and manually tagged train-ing data are used.
The sentences are then matchedagainst manually constructed patterns, which op-erate on syntax trees, to detect sentences ex-pressing the target term?s genus, species, or both(genus+species).
The system composes its an-swer by placing first the genus+species sentencethat is closer to the centroid of the extracted sen-tences.
The remaining sentences are ranked bytheir distance from the centroid, and the mosthighly ranked ones are clustered.
The system thenselects iteratively the cluster that is closer to thecentroid of the extracted sentences and the mostrecently used cluster.
The cluster?s most repre-sentative sentence, i.e., the sentence closest to thecentroid of the cluster?s sentences, is added to theresponse.
The iterations stop when a maximum re-sponse length is reached.
Multi-snippet definitionsare generated.Han et al (2004; 2006) parse a definition ques-tion to locate the head word of the target term.They also use a named entity recognizer to deter-mine the target term?s type (person, organization,1277etc.).
They then extract from documents relevantto the target term sentences containing its headword, as well as sentences the extracted ones referto (e.g., via pronouns).
The resulting sentences arematched against manually constructed syntacticpatterns to detect phrases conveying definitionalinformation.
The resulting phrases are ranked bycriteria like the degree to which the phrase con-tains words common in definitions of the targetterm?s type, and the highest ranked phrases are in-cluded in a multi-snippet summary.
Other mecha-nisms discard phrases duplicating information.Xu et al (2005) aim to extract all the definitionsin a document collection.
They parse the docu-ments to detect base noun phrases (without em-bedded noun phrases).
Base noun phrases are pos-sible target terms; the paragraphs containing themare matched against manually constructed patternsthat look for definitions.
An SVM then separatesthe remaining paragraphs into good, indifferent,and bad definitions.
Redundant paragraphs, iden-tified by edit distance similarity, are removed.6 Conclusions and future workWe presented a freely available system that findsshort definitions of user-specified terms on Webpages.
It employs a MAXENT classifier, whichis trained on automatically generated examples;hence, the system is in effect unsupervised.
Weuse ROUGE-W to generate training examples fromWeb snippets and encyclopedias, a method thatoutperforms an alternative centroid-based one.Once our system has been trained, it can find shortdefinitions of terms that are not covered by ency-clopedias.
Experiments show our system outper-forms a comparable well-published system and apreviously published form of our system.Our system does not require linguistic process-ing tools, such as named entity recognizers, POStaggers, chunkers, parsers; hence, it can be easilyused in languages where such tools are unavail-able.
It could be improved by exploiting the HTMLmarkup of Web pages and the Web?s hyperlinks.For example, the target term is sometimes writtenin italics in definitions, and some definitions areprovided on pages (e.g., pop-up windows) that oc-currences of the target term link to.The work reported here was conducted in thecontext of project INDIGO, where an autonomousrobotic guide for museum collections is being de-veloped (Galanis et al, 2009).
The guide engagesthe museum?s visitors in spoken dialogues, and itdescribes the exhibits the visitors select by gen-erating spoken natural language descriptions froman ontology.
Among other requests, the visitorscan ask follow up questions, and we have foundthat the most common kind of follow up questionsare requests to define terms (e.g., names of per-sons, events, architectural terms, etc.)
mentionedin the generated exhibit descriptions.
Some ofthese definition requests can be handled by gener-ating new texts from the ontology, but some timesthe ontology contains no information for the targetterms.
We are, thus, experimenting with the possi-bility of obtaining short definitions from the Web,using the system we presented.AcknowledgementsThis work was carried out in INDIGO, an FP6 ISTproject funded by the European Union, with addi-tional funding provided by the Greek General Sec-retariat of Research and Technology.13ReferencesAndroutsopoulos, I., and Galanis, D. 2005.
A Prac-tically Unsupervised Learning Method to IdentifySingle-Snippet Answers to Definition Questions onthe Web.
In HLT/EMNLP, Vancouver, Canada, 323?330.Blair-Goldensohn, S., McKeown, K., Schlaikjer, A.H.2003.
A Hybrid Approach for QA Track DefinitionalQuestions.
In TREC 2003, Gaithersburg, MD, USA.Blair-Goldensohn, S., McKeown, K.R., and Schlaikjer,A.H.
2004.
Answering Definitional Questions: AHybrid Approach.
In Maybury, M.
(Ed.
), New Di-rections in Question answering, AAAI Press.Chu-Carroll, J., Czuba, K., Prager, J., Ittycheriah, A.,Blair-Goldensohn, S. 2004.
IBM?s PIQUANT II inTREC 2004.
In TREC 2004, Gaithersburg, MD, USA.Chu-Carroll, J., Czuba, K., Duboue, P., and Prager, J.2005.
IBM?s PIQUANT II in TREC 2005.
In TREC2005, Gaithersburg, MD, USA.Cristianini, N. and Shawe-Taylor, J.
2000.
An Intro-duction to SVMs.
Cambridge University Press.Cui, H., Kan, M.-Y., Chua, T.-S., and Xiao, J.
2004.
AComparative Study on Sentence Retrieval for Defi-nitional Question Answering.
In SIGIR workshop onInformation Retrieval for Question Answering, Sal-vador, Brazil.13Consult http://www.ics.forth.gr/indigo/.1278Cui, H., Kan, M.Y., Chua, T.S.
2004.
UnsupervisedLearning of Soft Patterns for Generating Definitionsfrom Online News.
In WWW, New York, NY, USA.Cui, H., Kan, M.Y., Chua, T.S.
2005.
Generic Soft Pat-tern Models for Definitional Question Answering.
InACM SIGIR, Salvador, Brazil.Cui, H. 2006.
Soft Matching for Question Answering.Ph.D.
thesis, National University of Singapore.Cui, H., Kan, M., and Chua, T. 2007.
Soft PatternMatching Models for Definitional Question Answer-ing.
ACM Transactions on Information Systems,25(2):1?30.Dang, H. T. 2005.
Overview of DUC 2005.
In DUC atHLT-EMNLP, Vancouver, Canada.Dang, H. T. 2006.
Overview of DUC 2006.
In DUC atHLT-NAACL, New York, NY, USA.Eugenio, B. D., Glass, M. 2004.
The Kappa Statis-tic: a Second Look.
Computational Linguistics,301(1):95-101.Galanis, D., Karakatsiotis, G., Lampouras, G., and An-droutsopoulos, I.
2009.
An Open-Source Natu-ral Language Generator for OWL Ontologies andits Use in Protege and Second Life.
EACL systemdemonstration, Athens, Greece.Han, K.S., Chung, H., Kim, S.B., Song, Y.I., Lee, J.Y.,Rim, H.C. 2004.
Korea University QA System atTREC 2004.
In TREC 2004, Gaithersburg, MD, USA.Han, K.S., Song, Y.I., Kim, S.B., and Rim, H.C. 2006.A Definitional Question Answering System Based onPhrase Extraction Using Syntactic Patterns.
IEICETransactions on Information and Systems, vol.
E89-D, No.
4, 1601?1605.Hildebrandt, W., Katz, B., and Lin, J.
2004.
Answer-ing Definition Questions Using Multiple KnowledgeSources.
In HLT-NAACL, Boston, MA, USA, 49?56.Joho, H. and Sanderson, M. 2000.
Retrieving Descrip-tive Phrases from Large Amounts of Free Text.
Inter-national Conference on Information and KnowledgeManagement, McLean, VA, USA, 180?186.Joho, H. and Sanderson, M. 2001.
Large Scale Testingof a Descriptive Phrase Finder.
In HLT-NAACL, SanDiego, CA, USA, 219?221.Lin, C.Y.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In ACL workshop ?TextSummarization Branches Out?, Barcelona, Spain.Miliaraki, S. and Androutsopoulos, I.
2004.
Learn-ing to Identify Single-Snippet Answers to DefinitionQuestions.
In COLING, Geneva, Switzerland, 1360?1366.Prager, J., Radev, D., and Czuba, K. 2001.
AnsweringWhat-Is Questions by Virtual Annotation.
In HLT-NAACL, San Diego, CA, USA, 26?30.Prager, J., Chu-Carroll, J., and Czuba, K. 2002.
Use ofWordNet Hypernyms for Answering What-Is Ques-tions.
In TREC 2001, Gaithersburg, MD, USA.Ratnaparkhi A.
1997.
A Simple Introduction to Max-imum Entropy Models for Natural Language Pro-cessing.
Technical Report 97-08, Institute for Re-search in Cognitive Science, University of Pennsyl-vania, 1997.Voorhees, E.M. 2000.
Overview of the TREC-9 Ques-tion Answering Track.
NIST, USA.Voorhees, E.M. 2001.
Overview of the TREC 2001Question Answering Track.
NIST, USA.Voorhees, E.M. 2001.
The TREC QA Track.
NaturalLanguage Engineering, 7(4):361?378.Voorhees, E.M. 2003.
Evaluating Answers to Defini-tion Questions.
In HLT-NAACL, Edmonton, Canada.Xu, J., Weischedel, R., Licuanan, A.
2004.
Evaluationof an Extraction-based Approach to Answering Def-initional Questions.
In ACM SIGIR, Sheffield, UK.Xu, J., Cao, Y., Li, H., Zhao, M. 2005.
Ranking Defini-tions with Supervised Learning Methods.
In WWW,Chiba, Japan, 811?819.1279
