Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152?156,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsRegression with Phrase Indicators for Estimating MT Quality?Chunyang Wu Hai Zhao?Center for Brain-Like Computing and Machine Intelligence,Department of Computer Science and Engineering, Shanghai Jiao Tong Universitychunyang506@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cnAbstractWe in this paper describe the regression sys-tem for our participation in the quality estima-tion task of WMT12.
This paper focuses onexploiting special phrases, or word sequences,to estimate translation quality.
Several featuretemplates on this topic are put forward sub-sequently.
We train a SVM regression modelfor predicting the scores and numerical resultsshow the effectiveness of our phrase indicatorsand method in both ranking and scoring tasks.1 IntroductionThe performance of machine translation (MT) sys-tems has been considerable promoted in the past twodecades.
However, since the quality of the sentencegiven by MT decoder is not guaranteed, an impor-tant issue is to automatically predict or identify itscharacteristics.
Recent studies on quality estimationor confidence estimation have focused on measuringthe translating quality at run-time, instead of involv-ing reference corpus.
Researches on this topic con-tribute to offering advices or warnings for users evenwithout knowledge about either side of languages?This work was partially supported by the National Natu-ral Science Foundation of China (Grant No.
60903119 andGrant No.
61170114), the National Research Foundation forthe Doctoral Program of Higher Education of China underGrant No.
20110073120022, the National Basic Research Pro-gram of China (Grant No.
2009CB320901), the Science andTechnology Commission of Shanghai Municipality (Grant No.09511502400), and the European Union Seventh FrameworkProgram (Grant No.
247619).
?corresponding authorand illuminating some other potential MT applica-tions.This paper describes the regression system for ourparticipation in the WMT12 quality estimation task.In this shared task, we analyzed the pattern of trans-lating errors and studied on capturing such patternsamong the corpus.
The basic objective in this pa-per is to recognize those phrases, or special word se-quence combinations which can indicate the qualityof a translation instance.
By introducing no exter-nal NLP toolkits, we exploited several feasible tech-niques to extract such patterns directly on the cor-pus.
One contribution of this paper is those featuretemplates on the basis of this topic.
Numerical re-sults show their positive effects on both ranking andscoring subtasks.The rest of this paper is organized as follows: InSection 2, we show the related work.
In Section3, we specify the details of our system architecture.The experimental results are reported in Section 4.Finally, the conclusion is given in Section 5.2 Related WorkCompared with traditional MT metrics such asBLEU (Papineni et al, 2002), the fundamental goalof quality estimation (QE) is predicting the qualityof output sentences without involving reference sen-tences.Early works (Quirk, 2004; Gamon et al, 2005)have demonstrated the consistency of the automaticscore and human evaluation.
Several further worksaimed at predicting automatic scores in order to bet-ter select MT n-best candidates (Specia and Farzin-dar, 2010), measure post-editing effort (Specia et152al., 2011) or combine SMT and TM systems (He etal., 2010).
Instead of estimating on word or sen-tence levels, Soricut and Echihabi (2010) proposeda document-level ranking system which grants theuser to set quality threshold.
Besides, recent stud-ies on the QE topic introduced syntactic and linguis-tic information for better estimating the quality suchas dependency preservation checking (Bach et al,2011).3 System DescriptionWe specify the details of our system in this section.Following previous approaches for quality estima-tion, it is first trained on the corpus with labeledquality scores and then it is able to predict the scorefor unlabeled instances.A major challenge for this estimating task is to ex-ploit effective indicators, or features, to identify thequality of the translating results.
In this paper, all thefeatures are extracted from the official corpora, in-volving no external tools such as pre-trained parsersor POS taggers.
Most of the feature templates focuson special phrases or word sequences.
Some of thephrases could introduce translation errors and other-s might declare the merit of the MT output.
Theirweights are automatically given by the regressor.3.1 Regression ModelFor obtaining MT quality predictor, we utilizeSVM light (Joachims, 1999)1 to train this regressionmodel.
The radial basis function kernel is chosen asthe kernel of this model.
The label for each instanceis the score annotated manually and the input vectorconsists of a large amount of indicators described inSection 3.2.3.2 FeaturesFor training the regression model, we utilize the17 baseline features: number of source/target to-kens, average source token length, source/targetLM probability, target-side average of target wordoccurrences, original/inverse frequency average oftranslations per source word, source/target percent-age of uni-/bi-/tri-grams in quartile 1 or 4, sourcepercentage of unigrams in the training corpus and1http://svmlight.joachims.org/source/target number of punctuation.
Besides, sev-eral features and templates are proposed as follows:?
Inverted Automatic Scores: For each Span-ish system output sentence, we translate it toEnglish and get its scores of BLEU and ME-TEOR (Denkowski and Lavie, 2011).
These s-cores are treated as features named inverted au-tomatic scores.
In order to obtain these numer-als, we train a Spanish-to-English phrase-basedMoses2 (Koehn et al, 2007) decoder with de-fault parameters on the official parallel corpus.The original training corpus is split into a de-veloping set containing the last 3000 sentencepairs at the end of the corpus and a training setwith the remained pairs.
The word alignmentinformation is generated by GIZA++ (Och andNey, 2003) and the feature weights are tuned onthe developing set by Z-MERT (Zaidan, 2009).?
Minimal/Maximal link likelihood of gener-al language model: In the word graph ofeach decoding instance, denote the minimaland maximal general language model likeli-hood of links as lmin and lmax.
We treatexp(lmin) and exp(lmax) as features respec-tively.?
Trace Density: Define the trace density ?T asthe quotient of decoding trace length and sen-tence length:?T = TraceLength / SentenceLength.
(1)?
Average of Phrase Length: This feature isalso obtained from the decoding trace informa-tion.?
Number of Name Entity: This feature can-not be obtained exactly due to the resource con-strains.
We in this task count the number of theword whose first letter is capitalized, and thatis not the first word in the sentence.We also extract several special phrases or se-quences.
The total of each phrase/sequence typeand each pattern are respectively defined as features.When an instance matches a pattern, the entry rep-resenting this pattern in its vector is set to |1/Z|.
In2http://www.statmt.org/moses/153this paper the regressor term Z is the size of the tem-plate which the pattern belongs to.
The detail de-scription of such templates is presented as follows:?
Reference 2?5-grams: All the 2?5-grams ofthe reference sentences provided in the officialdata are generated as features.?
Bi-gram Source Splitting: This templatecomes from the GIZA alignment document.We scan the parallel corpus: for each bi-gramin the source sentence, if its words?
counter-parts in the target side are separated, we add itto the bi-gram source splitting feature template.The part-of-speech tags of the words seem to beeffective to this task.
Since it is not provided, we uti-lize a trick design for obtaining similar information:?
Target Functional Word Patterns: On thetarget corpus, we scan those words whoselength is smaller than or equal to three.
Sucha word w is denoted as functional word.
Anybi-gram in the corpus starting or ending with wis added to a dictionary D. For each system-output translation instance, we compare theanalogous bi-grams in it with this dictionary,all bi-grams not in D are extracted as features.Denote the collection of 2?5-grams of thesystem-output sentences scored lower than 1.5 as B;that with scores higher than 4.5 as G. Here the ?s-core?
is the manual score provided in the official re-source.?
Target Bad 2?5-grams: B?G?
Target Good 2?5-grams: G?
B?
Source Bad/Good 2?5-grams: Analogousphrases on the source side are also extractedby the same methods as Target Bad/Good n-grams.For each output-postedit sentence pair, we con-struct a bipartite graph by aligning the same wordsbetween these two sentences.
By giving a maximalmatching, the output sentence can be split to severalsegments by the unmatched words.?
Output-Postedit Different 2?5-grams: Foreach unaligned segments, we attach the previ-ous word to the left side and the next word tothe right.
2?5-grams in this refined segmentare extracted as features.?
Output-Postedit Different Anchor: Denotethe refined unaligned segment assr = (prevWord, s1, s2, .
.
.
, sn, nextWord).A special sequence with two word segmentsprevWord s1 .
.
.
sn nextWordis given as a feature.In the source-side scenario with the inverted trans-lations, similar feature templates are extracted aswell:?
Source-Invert Different 2?5-grams/AnchorA significant issue to be considered in this sharedtask is that the training data set is not a huge one,containing about two thousand instances.
Althoughcarefully designed, the feature templates howevercannot involve enough cases.
In order to overcomethis drawback, we adopt the following strategy:For any template T, we compare its patterns withthe items in the phrase table.
If the phrase item p issimilar enough with the pattern g, p is added to thetemplate T. Two similarity metrics are utilized: De-note the longest common sequence as LCSQ(p, g)and the longest common segment as LCSG(p, g) 3,LCSQ(p, g)2|p||g|> 0.6, (2)LCSG(p, g) ?
3.
(3)Besides, when training the regression model ortesting, the entry representing the similar items inthe feature vector are also set to 1/|template size|.4 Experiments4.1 DataIn order to conduct this experiment, we randomlydivide the official training data into two parts: one3To simplify, the sequence allows separation while the seg-ment should be contiguous.
For example, LCSQ(p, g) andLCSG(p, g) for ?I am happy?
and ?I am very happy?
are ?I,am, happy?
and ?I, am?, respectively.154Data SetScore Distribution[1-2) [2-3) [3-4) [4-5)Original 3.2% 24.1% 38.7% 34.0%Train 3.2% 24.2% 38.3% 34.4%Dev 3.3% 24.0% 40.3% 32.4%Table 1: The comparison of the score distributions amongthree data sets: Original, Training (Train) and Develop-ment (Dev).Ranking ScoringDA SC MAE RMSEBaseline 0.47 0.49 0.61 0.79This paper 0.49 0.52 0.60 0.77Table 2: The experiment results on the ranking and scor-ing tasks.
In this table, DA, SC, MAE and RMSE areDeltaAvg, Spearman Correlation, Mean-Average-Errorand Root-Mean-Squared-Error respectively.training set with about 3/4 items and one develop-ment set with the other 1/4 items.
The comparisonof the score distribution among these data sets is list-ed in Table 1.4.2 ResultsThe baseline of this experiment is the regressionmodel trained on the 17 baseline features.
The pa-rameters of the classifier are firstly tuned on thebaseline features.
Then the settings for both thebaseline and our model remain unchanged.
Thenumerical results for the ranking and scoring tasksare listed in Table 2.
The ranking task is evalu-ated on the DeltaAvg metric (primary) and Spear-man correlation (secondary) and the scoring task isevaluated on Mean-Average-Error and Root-Mean-Squared-Error.
For the ranking task, our system out-performs 0.02 on DeltaAvg and 0.03 on Spearmancorrelation; for the scoring task, 0.01 lower on MAEand 0.02 lower on RMSE.The official evaluation results are listed in Table 3.The official LibSVM4 model is a bit better than oursubmission.
Our system was further improved af-ter the official submission.
Different combinationsof the rates defined in Equation 2?3 and regressorparameter settings are tested.
As a result, the ?Re-fined?
model in Table 3 is the results of the refined4http://www.csie.ntu.edu.tw/ cjlin/libsvm/Ranking ScoringDA SC MAE RMSESJTU 0.53 0.53 0.69 0.83Official SVM 0.55 0.58 0.69 0.82Refined 0.55 0.57 0.68 0.81Best Workshop 0.63 0.64 0.61 0.75Table 3: The official evaluation results.version.
Compared with the official model, it givessimilar ranking results and performs better on the s-coring task.5 ConclusionWe presented the SJTU regression system for thequality estimation task in WMT 2012.
It utilizeda support vector machine approach with several fea-tures or feature templates extracted from the decod-ing and corpus documents.
Numerical results showthe effectiveness of those features as indicators fortraining the regression model.
This work could beextended by involving syntax information for ex-tracting more effective indicators based on phrasesin the future.ReferencesNguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011.Goodness: a method for measuring machine transla-tion confidence.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, HLT?11, pages 211?219, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:Automatic Metric for Reliable Optimization and Eval-uation of Machine Translation Systems.
In Proceed-ings of the EMNLP 2011 Workshop on Statistical Ma-chine Translation.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level mt evaluation without referencetranslations: Beyond language modeling.
In In Euro-pean Association for Machine Translation (EAMT).Yifan He, Yanjun Ma, Josef van Genabith, and AndyWay.
2010.
Bridging smt and tm with translationrecommendation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 622?630, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.155Thorsten Joachims.
1999.
Advances in kernel method-s. chapter Making large-scale support vector machinelearning practical, pages 169?184.
MIT Press, Cam-bridge, MA, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the Association for Computational Lin-guistics Companion Demo and Poster Sessions, pages177?180, Prague, Czech Republic.
Association forComputational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A systemat-ic comparison of various statistical alignment models.Comput.
Linguist., 29(1):19?51, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computation-al Linguistics, ACL ?02, pages 311?318, Stroudsburg,PA, USA.
Association for Computational Linguistics.Christopher B. Quirk.
2004.
Training a sentence-levelmachine translation confidence metric.
LREC, 4:2004.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translations vi-a ranking.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistic-s, pages 612?621, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Lucia Specia and Atefeh Farzindar.
2010.
Estimat-ing machine translation post-editing effort with hter.In AMTA 2010- workshop, Bringing MT to the User:MT Research and the Translation Industry.
The NinthConference of the Association for Machine Transla-tion in the Americas, nov.Lucia Specia, Hajlaoui N., Hallett C., and Aziz W. 2011.Predicting machine translation adequacy.
In MachineTranslation Summit XIII.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.156
