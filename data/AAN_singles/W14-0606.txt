Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 42?46,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsEnhancing the possibilities of corpus-based investigations: Word sensedisambiguation on query results of large text corporaChristian PoelitzTechnical University DortmundArtificial Intelligence Group44227 Dortmund, Germanypoelitz@tu-dortmund.deThomas BartzTechnical University DortmundInstitute of German Language and Literature44227 Dortmund, Germanybartz@tu-dortmund.deAbstractCommon large digital text corpora do notdistinguish between different meanings ofword forms, intense manual effort has tobe done for disambiguation tasks whenquerying for homonyms or polysemes.
Toimprove this situation, we ran experimentswith automatic word sense disambiguationmethods operating directly on the outputof the corpus query.
In this paper, wepresent experiments with topic models tocluster search result snippets in order toseparate occurrences of homonymous orpolysemous queried words by their mean-ings.1 IntroductionLarge digital text corpora contain text documentsfrom different sources, genres and periods oftime as well as often structural and linguisticmarkups.
Nowadays, they provide novel and en-hanced possibilities of exploring research ques-tions at the basis of authentic language usage notonly in the field of linguistics, but for human-ities and social sciences in general.
But eventhough tools for query and analysis are gettingmore and more flexible and sophisticated (notleast thanks to the efforts been done in infras-tructure projects like CLARIN), automatically ob-tained data have to be reviewed manually in mostcases because of false positives.
Depending onthe amount of data, intense manual effort has tobe done for cleaning, classification or disambigua-tion tasks.
Hence, many research questions cannotbe addressed because of time constraints (Storrer,2011).
A project funded by the German BMBF(Bundesministerium f?ur Bildung und Forschung,?Federal Ministry of Education and Research?
),therefore, is investigating benefits and issues ofusing machine learning technology in order to per-form these tasks automatically.
In this paper, wefocus on the disambiguation task, which is an issueknown for a long time in the field of corpus-basedlexicography (Engelberg and Lemnitzer, 2009),but has not been satisfactorily solved, yet, and isstill highly relevant also to social scientists or his-torians.
In the humanities, researchers usually arenot examining word forms, but terms represent-ing relations of word forms and their meanings.While the common large corpora do not distin-guish between different meanings of word forms,the disambiguation task has to be carried out man-ually most of the times.
To improve this situa-tion, we ran experiments with word sense disam-biguation methods operating directly on the outputof the corpus queries, i.e.
search result lists con-taining small snippets with the occurrences of thesearch keyword, each in a context of about onlythree sentences.
In particular, we used topic mod-elling to automatically detect clusters of keywordoccurrences with similar contexts, that we con-sider corresponding to a certain meaning of thekeyword.
In the following, we report our findingsfrom experiments with the German terms Leiterand zeitnah, both supposed to provide interest-ing insights into processes of language change.Der Leiter ?chief?, ?director?
and die Leiter ?lad-der?
are homonyms with possible further sensesEnergieleiter ?conducting medium?
and Tonleiter?scale?
(in music), whereby der Leiter competesagainst borrowings like Boss or Chef.
Zeitnah, apolyseme meaning zeitgenssisch ?contemporary?,zeitkritisch ?critical of the times?
as well as un-verzglich ?prompt?, seems to have acquired thelatter meaning as a new sense not until the sec-ond half of the last century.
The basis of our ex-periments are search result lists derived from theDWDS Kernkorpus core corpus of the 20th cen-tury (for Leiter) and, in addition, from the ZEITcorpus (for zeitnah).
The DWDS Kernkorpus,constructed at the Berlin-Brandenburg Academyof Sciences (BBAW), contains approximately 10042million running words, balanced chronologically(over the decades of the 20th century) and by textgenre (over the genres journalism, literary texts,scientific literature and other nonfiction; (Geyken,2007)).
The ZEIT corpus covers all the issues ofthe German weekly newspaper Die Zeit from 1946to 2009, approximately 460 million running words(http://www.dwds.de/ressourcen/korpora).2 Related WorkWord sense disambiguation is a well studied prob-lem in Machine Learning and Natural LanguageProcessing.
For a given word, later mentioned asword of interest, we expect that there exist severalmeanings.
The differences in the meanings are re-flected by different words occurring and frequen-cies together with the word to be disambiguated.A very early statistical approach was proposed by(Brown et al., 1991).
The authors proposed toestimate the probability distribution of senses forgiven words from annotated examples.
A generalsurvey about the topic can be found in (Navigli,2009).
Latent Dirichlet Allocation (LDA) intro-duced by (Blei et al., 2003) can be used to esti-mate topic distributions for a given document cor-pus.
Each topic represent a sense in which thedocuments, respectively the words, appear.
(Grif-fiths and Steyvers, 2004) proposed efficient train-ing for LDA using Monte Carlo sampling.
Theyused Gibbs sampling to estimate the topic distribu-tion.
The authors in (Brody and Lapata, 2009) ex-tend the generative model by LDA by many paral-lel feature representations.
Hence, beside the purewords, additional features like part of speech tagscan be used.
Further, the authors perform analy-sis with different context sizes.
Investigations ofword sense disambiguation on small snippets havebeen done before on search engine results.
Thesnippets retrieved after a query has been sent toa search engine are used for disambiguation.
In(Navigli and Crisafulli, 2010) for instance, the au-thors search for word senses of web search resultsusing retrieved snippets.Our approach differs from these previous onessince we concentrate on snippets from a text cor-pus for linguistic and lexicographic research pur-poses (see Section 1).
Unlike results from searchengines, that refer to documents whose topics arestrongly related to the search keyword, result listsfrom text corpora contain snippets with occur-rences of the keyword in each document of thecorpus, irrespective of the document topic.
Thatis why keywords can occur in less typical, se-mantically less definite contexts.
In the liter-ary documents, they are not infrequently used asmetaphors.3 Snippet RepresentationIn order to properly apply Machine Learningmethods for word sense disambiguation we needto encode the snippets in an appropriate way.Therefore, we represent each snippet as bag-of-words.
This means we build a large vector thatcontains at the component i the number of timesword i - from the overall vocabulary of the docu-ment corpus - appears in the snippet.
These vec-tors are very sparse and can be efficiently saved ashash tables.Since we want to investigate different contextinformation for the disambiguation, we generatefor each snippet many different bag-of-words rep-resentations.
First, we use only those words thatappear in close proximity to the word we want todisambiguate.
This means, we place a window onthe text, that contains a certain number of wordsthat appear before and after the word of interest.Next, we filter out words that are not immediateconstituents (or immediate constituents of the 1st,2nd, nth superordinate node) of the word of inter-est.
In this case the proximity is not crucial but thesyntactical relatedness to the word of interest.These word vectors are used for the word sensedisambiguation.4 DisambiguationFor the word sense disambiguation we use LatentDirichlet Allocation (LDA) as introduced by (Bleiet al., 2003).
LDA estimates the probability dis-tributions of words and documents, respectivelysnippet, over a number of different topics.
Thetopics will be used to disambiguate the word of in-terest.
These distributions are drawn from Dirich-let distributions that depend on given meta param-eters ?
and ?.The probability of a topic, given a snippetis modelled as Multinomial distribution that de-pends on a Dirichlet distributed distribution ofthe snippets over the topics.
Formally we have:?
?
Dirichlet(?)
the probability distribution ofa snippet and p(zi|?
(j)) ?Multi(?
(j)) the prob-ability of topic zifor a given snippet j.To estimate the distributions we use a Gibbs43Leiter w10 w40 w80 all syntaxNMI 0.2086 0.2579 0.2414 0.2573 0.1944zeitnah w10 w40 w80 all syntaxNMI 0.1012 0.1926 0.1656 0.2230 0.0456Table 1: NMI of the extracted senses with respectto the given annotations of the text snippets.Leiter w10 w40 w80 all syntaxF1 0.7271 0.7487 0.7405 0.7416 0.6904zeitnah w10 w40 w80 all syntaxF1 0.7773 0.6919 0.7630 0.7488 0.4584Table 2: F1 score of the extracted senses with re-spect to the given annotations of the text snippets.sampler as proposed by (Griffiths and Steyvers,2004).
The Gibbs sampler models the probabil-ity distributions of a given topic zi, depending onall other topics and the words in the snippet asMarkov chain.
This Markov chain converges tothe posterior distribution of the topics given thewords in a certain snippet.
This posterior can beused to estimate the most likely topic for a givensnippet.Further, we use the author topic model as in-troduced by (Steyvers et al., 2004).
This modelintegrates additional indications about the authorfor each snippet into the topic modelling process.This method can also be used to model the textcategories instead of authors.
We simply treat thecategories as the authors.
Now, the probability dis-tribution of the topics additionally depends on therandom variable c over the categories.
This can beleveraged to estimate the probability of category cfor a given topic zi, hence p(c|zi).Using the author topic model, we estimatethe topic distribution over words and categories.Based on these distributions the stochastic processof generating topics is simulated.
Depending onthe number of times a topic is drawn for a givensnippet and category, we extract the most likelywords and categories for the topics.
The topicsrepresent the different senses of the word of inter-est.5 ExperimentsWe performed experiments on two data sets thatconsist of short snippets retrieved by corpusqueries for the words Leiter and zeitnah in theDWDS Kernkorpus www.dwds.de and the ZEITcorpus (see Section 1).
Each snippet consists ofthe three sentences, whereby the second sentencecontains the search keyword (the word to disam-biguate) in each case.
The snippets belong to thedifferent text categories covered by the mentionedcorpora: journalism, literary texts, scientific lit-erature and other nonfiction (see Section 1).
Foreach snippet, we have information to which cate-gory it belongs to.
This information is used onlyfor validation, not for the topic extraction.
Foreach data set, 30 percent of snippets were disam-biguated manually by two independent annotators,whereby doubtful cases were clarified by a thirdperson.
The annotations are not used for disam-biguation, but for the validation of the method.For each snippet we generate bag-of-words vec-tors using contexts of 10, 40, 80 or all wordsaround the word of interest.
Hence, for contextsize 10 we use the ten words before the token, thetoken itself and the ten following tokens, as repre-sentation of the snippet.
For further experimentswe used the Stanford Constituent Parser (Kleinand Manning, 2003) to get only the words thatsyntactically depend on the words of interest.
Forthe extraction of the topics and distribution overthe text categories we used the Gibbs sampler forLDA and the author topic model from the Mat-lab library Topictoolbox (Griffiths and Steyvers,2004).Based on the annotation mentioned above wecan estimate the Normalized Mutual Information(NMI) as score for the goodness of the method.NMI measures how many snippets that are anno-tated as being from different topics are placed intothe same topic based on the extracted topics fromLDA.
It is defined as the fraction of the sum ofthe entropies of the distributions of the annotationsand the disambiguation results, and the entropyof the joint distribution of annotations and results(Manning et al., 2008) (p. 357f).
Further, we useone of the standard measures to estimate the good-ness of a word sense disambiguation result, the F1score.
The F1 score is the weighted average ofthe precision and recall of the disambiguation re-sults for the given annotations.
This and furtherevaluation methods are described in (Navigli andVannella, 2013).In the Tables 1 and 2 we show the NMI and F1score for the extracted topics, respectively senses,by LDA.
We tested different context sizes from 10to 80 words around the word of interest.
Com-pared to the results when we use the whole snip-pets, we see that a context size of 40 results in the44Sense 1 Sense 2 Sense 3 Sense 4music standing GDR1governmentBerlin saw SED2gotProf up party BerlinComp above political ZK3Table 3: Translation of the most frequent wordsfor each of the extracted senses for the wordLeiter.Sense 1 Sense 2 Sense 3 Sense 4question society German publisherDM4just time bookyears examples film literaturemusic questions Berlin yearTable 4: Translation of the most frequent wordsfor each of the extracted senses for the word zeit-nah.best performance.
Less context decrease the per-formance and the filtering by constituencies givethe worst results.
The experiments show that awindowing approach is well suited to representdocuments for a word sense disambiguation task.The size of the window seems to be crucial andmust be chosen a priori.
Optimal window sizecould be found by cross validation techniques us-ing annotated snippets.Next, we investigated the distribution of the top-ics over the text categories.
We used the authortopic model as described above to estimate howthe categories distribute over the sense.
Tables 3and 4 show the most likely words to appear in thecorresponding senses translated into English forfour extracted topics.
In the Tables 5 and 6 thedistribution of the senses over the given categoriesare presented.
Based on the posterior distributionof the categories, we simulated the process of as-signing topics to categories for each word in thesnippets.
In the tables we present the number oftimes we assign sense i to category c.For the word Leiter in Table 5, we see that ineach category always one certain sense for theword is prominent.
For instance sense 2, hereLeiter Sense 1 Sense 2 Sense 3 Sense 4Literature 597 23818 7464 6718Non-fiction 3031 5295 63708 8733Science 41564 3269 1216 1046Journalism 5527 8845 23104 78645Table 5: The distribution of the senses among thetext categories during the simulation for the wordLeiter.zeitnah Sense 1 Sense 2 Sense 3 Sense 4Literature 23 0 12 6Non-fiction 1 0 574 10Science 211 0 478 1Journalism 2150 2438 1691 2924Table 6: The distribution of the senses among thetext categories during the simulation for the wordzeitnah.Leiter appears in the context of a ladder.
In thiscontext, the word is more likely to appear in a fic-tional text than in the other categories.
For zeit-nah in Table 6 the results are not very clear.
First,the word is most likely to appear in news papersrather than in literature or science articles.
This isdue to the fact that we have much more snippetsfrom news papers.
Only in sense 3, the word isalso likely to appear in other categories.
This con-text seems to be German films.
In contrast, we seesense 2 that is about social questions appears onlyin news papers.6 Conclusion and Future WorkWe used topic models to cluster search resultsnippets received by queries in two large digi-tal text corpora in order to separate occurrencesof homonymous or polysemous queried words bytheir meanings.
We showed that LDA performswell in extracting the senses in which the wordsappear.
Finally, we found that the author topicmodel can be used to estimate how the extractedsenses distribute over document categories.For the future, we want to further investigate thedistribution of the topics over different categoriesand time periods, as first experiments showed po-tential benefit of the author topic model.
An im-portant point for future work is, moreover, the in-tegration of syntactic features not only for filteringimportant words but also for enhancement of oursimple bag-of-words representation.
Especially,the integration of constituency and dependency in-formation will be further investigated.AknowledgementsThe authors are supported by the Bundesminis-terium f?ur Bildung und Forschung (BMBF) inproject KobRA5.5http://www.kobra.tu-dortmund.de/45ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
J. Mach.
Learn.Res., 3:993?1022, March.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In Proceedings of the 12thConference of the European Chapter of the Asso-ciation for Computational Linguistics, EACL ?09,pages 103?111, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1991.
Word-sense disambiguation using statistical methods.
InProceedings of the 29th Annual Meeting on Associa-tion for Computational Linguistics, ACL ?91, pages264?270, Stroudsburg, PA, USA.
Association forComputational Linguistics.Stefan Engelberg and Lothar Lemnitzer.
2009.
Lexiko-graphie und Woerterbuchbenutzung.
Stauffenburg,Tuebingen.Alexander Geyken.
2007.
The DWDS corpus.
A ref-erence corpus for the German language of the twen-tieth century.
In Christiane Fellbaum, editor, Idiomsand collocations.
corpus-based linguistic and lexi-cographic studies, pages 23?40.
Continuum, Lon-don.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101(Suppl.
1):5228?5235, April.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.Roberto Navigli and Giuseppe Crisafulli.
2010.
Induc-ing word senses to improve web search result clus-tering.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?10, pages 116?126, Stroudsburg, PA,USA.
Association for Computational Linguistics.Roberto Navigli and Daniele Vannella.
2013.Semeval-2013 task 11: Word sense induction anddisambiguation within an end-user application.
InSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 2: Proceedingsof the Seventh International Workshop on Seman-tic Evaluation (SemEval 2013), pages 193?201, At-lanta, Georgia, USA, June.
Association for Compu-tational Linguistics.Roberto Navigli.
2009.
Word sense disambiguation:A survey.
ACM Comput.
Surv., 41(2):10:1?10:69,February.Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi,and Thomas Griffiths.
2004.
Probabilistic author-topic models for information discovery.
In Proceed-ings of the Tenth ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,KDD ?04, pages 306?315, New York, NY, USA.ACM.Angelika Storrer.
2011.
Korpusgesttzte sprachanal-yse in lexikographie und phraseologie.
In Karl-fried Knapp et al., editor, Angewandte Linguistik.Ein Lehrbuch, pages 216?239.
Francke Verlag, Tue-bingen.46
