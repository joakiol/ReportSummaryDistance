Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495?500,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTwo-Stage Hashing for Fast Document RetrievalHao Li?Wei Liu?Heng Ji?
?Computer Science Department,Rensselaer Polytechnic Institute, Troy, NY, USA{lih13,jih}@rpi.edu?IBM T. J. Watson Research Center, Yorktown Heights, NY, USAweiliu@us.ibm.comAbstractThis work fulfills sublinear time Near-est Neighbor Search (NNS) in massive-scale document collections.
The primarycontribution is to propose a two-stageunsupervised hashing framework whichharmoniously integrates two state-of-the-art hashing algorithms Locality SensitiveHashing (LSH) and Iterative Quantization(ITQ).
LSH accounts for neighbor candi-date pruning, while ITQ provides an ef-ficient and effective reranking over theneighbor pool captured by LSH.
Further-more, the proposed hashing frameworkcapitalizes on both term and topic similar-ity among documents, leading to precisedocument retrieval.
The experimental re-sults convincingly show that our hashingbased document retrieval approach wellapproximates the conventional Informa-tion Retrieval (IR) method in terms of re-trieving semantically similar documents,and meanwhile achieves a speedup of overone order of magnitude in query time.1 IntroductionA Nearest Neighbor Search (NNS) task aims atsearching for top K objects (e.g., documents)which are most similar, based on pre-defined sim-ilarity metrics, to a given query object in an ex-isting dataset.
NNS is essential in dealing withmany search related tasks, and also fundamen-tal to a broad range of Natural Language Pro-cessing (NLP) down-stream problems includingperson name spelling correction (Udupa and Ku-mar, 2010), document translation pair acquisition(Krstovski and Smith, 2011), large-scale similarnoun list generation (Ravichandran et al, 2005),lexical variants mining (Gouws et al, 2011), andlarge-scale first story detection (Petrovic et al,2010).Hashing has recently emerged to be a popularsolution to tackling fast NNS, and been success-fully applied to a variety of non-NLP problemssuch as visual object detection (Dean et al, 2013)and recognition (Torralba et al, 2008a; Torralbaet al, 2008b), large-scale image retrieval (Kulisand Grauman, 2012; Liu et al, 2012; Gong et al,2013), and large-scale machine learning (Weiss etal., 2008; Liu et al, 2011; Liu, 2012).
However,hashing has received limited attention in the NLPfield to the date.
The basic idea of hashing is torepresent each data object as a binary code (eachbit of a code is one digit of ?0?
or ?1?).
Whenapplying hashing to handle NLP problems, the ad-vantages are two-fold: 1) the capability to storea large quantity of documents in the main mem-ory.
for example, one can store 250 million doc-uments with 1.9G memory using only 64 bits foreach document while a large news corpus such asthe English Gigaword fifth edition1stores 10 mil-lion documents in a 26G hard drive; 2) the timeefficiency of manipulating binary codes, for ex-ample, computing the hamming distance betweena pair of binary codes is several orders of magni-tude faster than computing the real-valued cosinesimilarity over a pair of document vectors.The early explorations of hashing focused onusing random permutations or projections to con-struct randomized hash functions, e.g., the well-known Min-wise Hashing (MinHash) (Broder etal., 1998) and Locality Sensitive Hashing (LSH)(Andoni and Indyk, 2008).
In contrast to suchdata-independent hashing schemes, recent re-search has been geared to studying data-dependenthashing through learning compact hash codesfrom a training dataset.
The state-of-the-art unsu-pervised learning-based hashing methods includeSpectral Hashing (SH) (Weiss et al, 2008), An-chor Graph Hashing (AGH) (Liu et al, 2011),and Iterative Quantization (ITQ) (Gong et al,1http://catalog.ldc.upenn.edu/LDC2011T074952013), all of which endeavor to make the learnedhash codes preserve or reveal some intrinsic struc-ture, such as local neighborhood structure, low-dimensional manifolds, or the closest hypercube,underlying the training data.
Despite achievingdata-dependent hash codes, most of these ?learn-ing to hash?
methods cannot guarantee a high suc-cess rate of looking a query code up in a hash ta-ble (referred to as hash table lookup in literature),which is critical to the high efficacy of exploit-ing hashing in practical uses.
It is worth notingthat we choose to use ITQ in the proposed two-stage hashing framework for its simplicity and ef-ficiency.
ITQ has been found to work better thanSH by Gong et al (2013), and be more efficientthan AGH in terms of training time by Liu (2012).To this end, in this paper we propose a noveltwo-stage unsupervised hashing framework to si-multaneously enhance the hash lookup successrate and increase the search accuracy by integrat-ing the advantages of both LSH and ITQ.
Further-more, we make the hashing framework applicableto combine different similarity measures in NNS.2 Background and Terminology?
Binary Codes: A bit (a single bit is ?0?
or?1?)
sequence assigned to represent a dataobject.
For example, represent a documentas a 8-bit code ?11101010?.?
Hash Table: A linear table in which all bi-nary codes of a data set are arranged to betable indexes, and each table bucket containsthe IDs of the data items sharing the samecode.?
Hamming Distance: The number of bit po-sitions in which bits of the two codes differ.?
Hash Table Lookup: Given a query q withits binary code hq, find the candidate neigh-bors in a hash table such that the Hammingdistances from their codes to hqare no morethan a small distance threshold .
In practice is usually set to 2 to maintain the efficiencyof table lookups.?
Hash Table Lookup Success Rate: Given aquery q with its binary code hq, the probabil-ity to find at least one neighbor in the tablebuckets whose corresponding codes (i.e., in-dexes) are within a Hamming ball of radius centered at hq.?
Hamming Ranking: Given a query q withits binary code hq, rank all data items accord-ing to the Hamming distances between theircodes and hq; the smaller the Hamming dis-tance, the higher the data item is ranked.3 Document Retrieval with HashingIn this section, we first provide an overview of ap-plying hashing techniques to a document retrievaltask, and then introduce two unsupervised hash-ing algorithms: LSH acts as a neighbor-candidatefilter, while ITQ works towards precise rerankingover the candidate pool returned by LSH.3.1 Document RetrievalThe most traditional way of retrieving nearestneighbors for documents is to represent each docu-ment as a term vector of which each element is thetf-idf weight of a term.
Given a query documentvector q, we use the Cosine similarity measure toevaluate the similarity between q and a documentx in a dataset:sim(q,x) =q>x?q??x?.
(1)Then the traditional document retrieval methodexhaustively scans all documents in the datasetand returns the most similar ones.
However, sucha brute-force search does not scale to massivedatasets since the search time complexity for eachquery is O(n); additionally, the computationalcost spent on Cosine similarity calculation is alsonontrivial.3.2 Locality Sensitive HashingThe core idea of LSH is that if two data points areclose, then after a ?projection?
operation they willremain close.
In other words, similar data pointsare more likely to be mapped into the same bucketwith a high collision probability.
In a typical LSHsetting of k bits and L hash tables, a query pointq ?
Rdand a dataset point x ?
Rdcollide if andonly ifhij(q) ?
hij(x), i ?
[1 : L], j ?
[1 : k], (2)where the hash function hij(?)
is defined ashij(x) = sgn(w>ijx), (3)in which wij?
Rdis a random projection di-rection with components being independently andidentically drawn from a normal distribution, andthe sign function sgn(x) returns 1 if x > 0 and -1otherwise.
Note that we use ?1/-1?
bits for deriva-tions and training, and ?1/0?
bits for the hashing496implementation including converting data to bi-nary codes, arranging binary codes into hash ta-bles, and hash table lookups.3.3 Iterative QuantizationThe central idea of ITQ is to learn the binary codesachieving the lowest quantization error that en-coding raw data to binary codes incurs.
This ispursued by seeking a rotation of the zero-centeredprojected data.
Suppose that a set of n data pointsX = {xi?
Rd}ni=1are provided.
The data matrixis defined as X = [x1,x2, ?
?
?
,xn]>?
Rn?d.In order to reduce the data dimension from d tothe desired code length c < d, Principal Compo-nent Analysis (PCA) or Latent Semantic Analy-sis (LSA) is first applied to X.
We thus obtainthe zero-centered projected data matrix as V =(I ?1n11>)XU where U ?
Rd?cis the projec-tion matrix.After the projection operation, ITQ minimizesthe quantization error as followsQ(B,R) = ?B?VR?2F, (4)where B ?
{1,?1}n?cis the code matrix eachrow of which contains a binary code, R ?
Rc?cis the target orthogonal rotation matrix, and ?
?
?Fdenotes the Frobenius norm.
Finding a local min-imum of the quantization error in Eq.
(4) beginswith a random initialization of R, and then em-ploys a K-means clustering like iterative proce-dure.
In each iteration, each (projected) data pointis assigned to the nearest vertex of the binary hy-percube, and R always satisfying RR>= I issubsequently updated to minimize the quantiza-tion loss given the current assignment; the twosteps run alternatingly until a convergence is en-countered.
Concretely, the two updating steps are:1.
Fix R and update B: minimize the follow-ing quantization lossQ(B,R) = ?B?2F+ ?VR?2F?
2tr(R>V>B)= nc+ ?V?2F?
2tr(R>V>B)= constant?
2tr(R>V>B),(5)achieving B = sgn(VR);2.
Fix B and update R: perform the SVD ofthe matrix V>B ?
Rc?cto obtain V>B =S?
?S>, and then set R = S?S>.Figure 1: The two-stage hashing framework.3.4 Two-Stage HashingThere are three main merits of LSH.
(1) It tries topreserve the Cosine similarity of the original datawith a probabilistic guarantee (Charikar, 2002).
(2) It is training free, and thus very efficient inhashing massive databases to binary codes.
(3) Ithas a very high hash table lookup success rate.
Forexample, in our experiments LSH with more thanone hash table is able to achieve a perfect 100%hash lookup success rate.
Unfortunately, its draw-back is the low search precision that is observedeven with long hash bits and multiple hash tables.ITQ tries to minimize the quantization error ofencoding data to binary codes, so its advantageis the high quality (potentially high precision ofHamming ranking) of the produced binary codes.Nevertheless, ITQ frequently suffers from a poorhash lookup success rate when longer bits (e.g.,?
48) are used (Liu, 2012).
For example, inour experiments ITQ using 384 bits has a 18.47%hash lookup success rate within Hamming radius2.
Hence, Hamming ranking (costing O(n) time)must be invoked for the queries for which ITQfails to return any neighbors via hash table lookup,which makes the searches inefficient especially onvery large datasets.Taking into account the above advantages anddisadvantages of LSH and ITQ, we propose a two-stage hashing framework to harmoniously inte-grate them.
Fig.
1 illustrates our two-stage frame-work with a toy example where identical shapesdenote ground-truth nearest neighbors.In this framework, LSH accounts for neigh-bor candidate pruning, while ITQ provides an ef-ficient and effective reranking over the neighborpool captured by LSH.
To be specific, the pro-497posed framework enjoys two advantages:1.
Provide a simple solution to accomplish botha high hash lookup success rate and high precision,which does not require scanning the whole list ofthe ITQ binary codes but scanning the short listreturned by LSH hash table lookup.
Therefore, ahigh hash lookup success rate is attained by theLSH stage, while maintaining high search preci-sion due to the ITQ reranking stage.2.
Enable a hybrid hashing scheme combiningtwo similarity measures.
The term similarity isused during the LSH stage that directly workson document tf-idf vectors; during the ITQ stage,the topic similarity is used since ITQ works onthe topic vectors obtained by applying Latent se-mantic analysis (LSA) (Deerwester et al, 1990)to those document vectors.
LSA (or PCA), thefirst step in running ITQ, can be easily acceler-ated via a simple sub-selective sampling strategywhich has been proven theoretically and empiri-cally sound by Li et al (2014).
As a result, thenearest neighbors returned by the two-stage hash-ing framework turns out to be both lexically andtopically similar to the query document.
To sum-marize, the proposed two-stage hashing frame-work works in an unsupervised manner, achieves asublinear search time complexity due to LSH, andattains high search precision thanks to ITQ.
Afterhashing all data (documents) to LSH and ITQ bi-nary codes, we do not need to save the raw data inmemory.
Thus, our approach can scale to gigan-tic datasets with compact storage and fast searchspeed.4 ExperimentsData and EvaluationsFor the experiments, we use the English portionof the standard TDT-5 dataset, which consists of278, 109 documents from a time spanning April2003 to September 2003.
126 topics are anno-tated with an average of 51 documents per topic,and other unlabeled documents are irrelevant tothem.
We select six largest topics for the top-KNNS evaluation, with each including more than250 documents.
We randomly select 60 docu-ments from each of the six topics for testing.
Thesix topics are (1).
Bombing in Riyadh, Saudi Ara-bia (2).
Mad cow disease in North America (3).Casablanca bombs (4).
Swedish Foreign Ministerkilled (5).
Liberian former president arrives in ex-ile and (6).
UN official killed in attack.
For eachdocument, we apply the Stanford Tokenizer2fortokenization; remove stopwords based on the stoplist from InQuery (Callan et al, 1992), and applyPorter Stemmer (Porter, 1980) for stemming.If one retrieved document shares the same topiclabel with the query document, they are true neigh-bors.
We evaluate the precision of the top-K candi-date documents returned by each method and cal-culate the average precision across all queries.ResultsWe first evaluate the quality of term vectors andITQ binary codes by conducting the whole listCosine similarity ranking and hamming distanceranking, respectively.
For each query document,the top-K candidate documents with highest Co-sine similarity scores and shortest hamming dis-tances are returned, then we calculate the averageprecision for each K. Fig.
2(a) demonstrates thatITQ binary codes could preserve document simi-larities as traditional term vectors.
It is interestingto notice that ITQ binary codes are able to outper-form traditional term vectors.
It is mainly becausesome documents are topically related but sharefew terms thus their relatedness can be captured byLSA.
Fig.
2(a) also shows that the NNS precisionkeep increasing as longer ITQ code length is usedand is converged when ITQ code length equals to384 bits.
Therefore we set ITQ code length as 384bits in the rest of the experiments.Fig.
2(b) - Fig.
2(e) show that our two-stagehashing framework surpasses LSH with a largemargin for both small K (e.g., K ?
10) andlarge K (e.g., K ?
100) in top-K NNS.
It alsodemonstrates that our hashing based document re-trieval approach with only binary codes from LSHand ITQ well approximates the conventional IRmethod.
Another crucial observation is that withITQ reranking, a small number of LSH hash ta-bles is needed in the pruning step.
For example,LSH(40bits) + ITQ(384bits) and LSH(48bits) +ITQ(384bits) are able to reach convergence withonly four LSH hash tables.
In that case, we canalleviate one main drawback of LSH as it usuallyrequires a large number of hash tables to maintainthe hashing quality.Since the LSH pruning time can be ignored,the search time of the two-stage hashing schemeequals to the time of hamming distance rerank-ing in ITQ codes for all candidates produced fromLSH pruning step, e.g., LSH(48bits, 4 tables) +2http://nlp.stanford.edu/software/corenlp.shtml498(a)0 50 100 1500.650.70.750.80.850.90.95number of top?K returned documentsPrecisionTraditional IRITQ(448bits)ITQ(384bits)ITQ(320bits)ITQ(256bits)ITQ(192bits)(b)1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.350.40.450.5number of hash tablesTop?10 PrecisionLSH(64bits)LSH(56bits)LSH(48bits)LSH(40bits)(c)1 2 3 4 5 6 7 8 9 100.450.50.550.60.650.70.750.8number of hash tablesTop?10 PrecisionTraditional IRLSH(40bits)+ITQ(384bits)LSH(48bits)+ITQ(384bits)LSH(56bits)+ITQ(384bits)LSH(64bits)+ITQ(384bits)(d)1 2 3 4 5 6 7 8 9 1000.020.040.060.080.10.120.140.160.180.2number of hash tablesTop?100 PrecisionLSH(64bits)LSH(56bits)LSH(48bits)LSH(40bits)(e)1 2 3 4 5 6 7 8 9 100.10.20.30.40.50.60.70.8number of hash tablesTop?100 PrecisionTraditional IRLSH(40bits)+ITQ(384bits)LSH(48bits)+ITQ(384bits)LSH(56bits)+ITQ(384bits)LSH(64bits)+ITQ(384bits)(f)1 2 3 4 5 6 7 8 9 1000.10.20.30.40.50.60.70.80.9number of hash tablesPercentageLSH(40bits)LSH(48bits)LSH(56bits)LSH(64bits)Figure 2: (a) ITQ code quality for different code length, (b) LSH Top-10 Precision, (c) LSH +ITQ(384bits) Top-10 Precision, (d) LSH Top-100 Precision, (e) LSH + ITQ(384bits) Top-100 Precision,(f) The percentage of visited data samples by LSH hash lookup.ITQ(384bits) takes only one thirtieth of the searchtime as the traditional IR method.
Fig.
2 (f)shows the ITQ data reranking percentage for dif-ferent LSH bit lengths and table numbers.
As theLSH bit length increases or the hash table num-ber decreases, a lower percentage of the candidateswill be selected for reranking, and thus costs lesssearch time.The percentage of visited data samples by LSHhash lookup is a key factor that influence theNNS precision in the two-stage hashing frame-work.
Generally, higher rerank percentage resultsin better top-K NNS Precision.
Further more, bycomparing Fig.
2 (c) and (e), it shows that ourframework works better for small K than for largeK.
For example, scanning 5.52% of the data isenough for achieving similar top-10 NNS resultas the traditional IR method while 36.86% of thedata is needed for top-100 NNS.
The reason of thelower performance with large K is that some trueneighbors with the same topic label do not sharehigh term similarities and may be filtered out inthe LSH step when the rerank percentage is low.5 ConclusionIn this paper, we proposed a novel two-stage un-supervised hashing framework for efficient and ef-fective nearest neighbor search in massive docu-ment collections.
The experimental results haveshown that this framework achieves not only com-parable search accuracy with the traditional IRmethod in retrieving semantically similar docu-ments, but also an order of magnitude speedup insearch time.
Moreover, our approach can com-bine two similarity measures in a hybrid hashingscheme, which is beneficial to comprehensivelymodeling the document similarity.
In our futurework, we plan to design better data representa-tion which can well fit into the two-stage hash-ing theme; we also intend to apply the proposedhashing approach to more informal genres (e.g.,tweets) and other down-stream NLP applications(e.g., first story detection).AcknowledgementsThis work was supported by the U.S. ARLNo.
W911NF-09-2-0053 (NSCTA), NSF IIS-0953149, DARPA No.
FA8750- 13-2-0041, IBM,Google and RPI.
The views and conclusions con-tained in this document are those of the authorsand should not be interpreted as representing theofficial policies, either expressed or implied, of theU.S.
Government.
The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernment purposes notwithstanding any copyrightnotation here on.499ReferencesA.
Andoni and P. Indyk.
2008.
Near-optimal hash-ing algorithms for approximate nearest neighbor inhigh dimensions.
Communications of the ACM,51(1):117?122.A.
Z. Broder, M. Charikar, A. M. Frieze, andM.
Mitzenmacher.
1998.
Min-wise independentpermutations.
In Proc.
STOC.J.
P. Callan, W. B. Croft, and S. M. Harding.
1992.
Theinquery retrieval system.
In Proc.
the Third Interna-tional Conference on Database and Expert SystemsApplications.M.
Charikar.
2002.
Similarity estimation techniquesfrom rounding algorithms.
In Proc.
STOC.T.
Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-narasimhan, and J. Yagnik.
2013.
Fast, accuratedetection of 100,000 object classes on a single ma-chine.
In Proc.
CVPR.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, and R. A. Harshman.
1990.
Indexing bylatent semantic analysis.
JASIS, 41(6):391?407.Y.
Gong, S. Lazebnik, A. Gordo, and F. Perronnin.2013.
Iterative quantization: A procrustean ap-proach to learning binary codes for large-scale im-age retrieval.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 35(12):2916?2929.S.
Gouws, D. Hovy, and D. Metzler.
2011.
Unsuper-vised mining of lexical variants from noisy text.
InProc.
EMNLP.K.
Krstovski and D. A. Smith.
2011.
A minimally su-pervised approach for detecting and ranking docu-ment translation pairs.
In Proc.
the sixth ACL Work-shop on Statistical Machine Translation.B.
Kulis and K. Grauman.
2012.
Kernelized locality-sensitive hashing.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 34(6):1092?1104.Y.
Li, C. Chen, W. Liu, and J. Huang.
2014.
Sub-selective quantization for large-scale image search.In Proc.
AAAI Conference on Artificial Intelligence(AAAI).W.
Liu, J. Wang, S. Kumar, and S.-F. Chang.
2011.Hashing with graphs.
In Proc.
ICML.W.
Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang.2012.
Supervised hashing with kernels.
In Proc.CVPR.W.
Liu.
2012.
Large-scale machine learning for classi-fication and search.
In PhD Thesis, Graduate Schoolof Arts and Sciences, Columbia University.S.
Petrovic, M. Osborne, and V. Lavrenko.
2010.Streaming first story detection with application totwitter.
In Proc.
HLT-NAACL.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.D.
Ravichandran, P. Pantel, and E. H. Hovy.
2005.Randomized algorithms and nlp: Using locality sen-sitive hash functions for high speed noun clustering.In Proc.
ACL.A.
Torralba, R. Fergus, and W. T. Freeman.
2008a.
80million tiny images: A large data set for nonpara-metric object and scene recognition.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,30(11):1958?1970.A.
Torralba, R. Fergus, and Y. Weiss.
2008b.
Smallcodes and large image databases for recognition.
InProc.
CVPR.R.
Udupa and S. Kumar.
2010.
Hashing-based ap-proaches to spelling correction of personal names.In Proc.
EMNLP.Y.
Weiss, A. Torralba, and R. Fergus.
2008.
Spectralhashing.
In NIPS 21.500
