Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 406?410,Dublin, Ireland, August 23-24, 2014.LT3: Sentiment Classification in User-Generated Content Using a RichFeature SetCynthia Van Hee, Marjan Van de Kauter, Orph?ee De Clercq, Els Lefever and V?eronique HosteLT3, Language and Translation Technology TeamDepartment of Translation, Interpreting and Communication ?
Ghent UniversityGroot-Brittanni?elaan 45, 9000 Ghent, BelgiumFirstname.Lastname@UGent.beAbstractThis paper describes our contribution to theSemEval-2014 Task 9 on sentiment analysis inTwitter.
We participated in both strands of thetask, viz.
classification at message-level (subtaskB), and polarity disambiguation of particular textspans within a message (subtask A).
Our experi-ments with a variety of lexical and syntactic fea-tures show that our systems benefit from rich fea-ture sets for sentiment analysis on user-generatedcontent.
Our systems ranked ninth among 27 andsixteenth among 50 submissions for task A and Brespectively.1 IntroductionOver the past few years, Web 2.0 applicationssuch as microblogging services, social network-ing sites, and short messaging services have con-siderably increased the amount of user-generatedcontent produced online.
Millions of people relyon these services to send messages, share theirviews or gather information about others.
Si-multaneously, companies, marketeers and politi-cians are anxious to detect sentiment in UGC sincethese messages might contain valuable informa-tion about the public opinion.
This explains whysentiment analysis has been a research area ofgreat interest in the last few years (Wiebe et al.,2005; Wilson et al., 2005; Pang and Lee, 2008;Mohammad and Yang, 2011).
Though first studiesfocussed more on product or movie reviews, wesee that analyzing sentiment in UGC is currentlybecoming increasingly popular.
The main differ-ence between these two sources of information isthat the former is rather long and contains quiteformal language whereas the latter one is gener-ally very brief and noisy and thus represents somedifferent challenges (Maynard et al., 2012).In this paper, we describe our contribution tothe SemEval-2014 Task 9: Sentiment Analysis inThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/Twitter (Rosenthal et al., 2014), which was a rerunof SemEval-2013 Task 2 (Nakov et al., 2013) andconsisted of two subtasks:?
Subtask A - Contextual PolarityDisambiguation: Given a message contain-ing a marked instance of a word or phrase,determine whether that instance is positive,negative or neutral in that context.?
Subtask B - Message PolarityClassification: Given a message, classifywhether the message is of positive, negative,or neutral sentiment.
For messages convey-ing both a positive and negative sentiment,whichever is the stronger sentiment should bechosen.The datasets for training, development and test-ing were provided by the task organizers.
Thetraining datasets consisted of Twitter messageson a variety of topics.
The test sets con-tained regular tweets (Twitter2013, Twitter2014),tweets labeled as sarcastic (TwitterSarcasm), SMSmessages (SMS2013), and blog posts (LiveJour-nal2014).
For both subtasks, the possible polar-ity labels were positive, negative, neutral, and ob-jective.
The datasets for subtask B contained anadditional label, i.e.
objective-OR-neutral.
Ta-ble 1 presents an overview of all provided datasets.For each task and test dataset, two runs could besubmitted: a constrained run using the providedtraining data only, and an unconstrained one us-ing additional training data.
For both tasks, wecreated a constrained model based on supervisedlearning, relying on additional lexicons and us-ing the test datasets of SemEval-2013 as develop-ment data.
Evaluation was based on averaged F-measure, considering averaged F-positive and F-negative.406Dataset Subtask A Subtask BTrainingTraining data 26,928 9,684Development data 1,135 1,654Total training data 28,063 11,338Dev-test (test SemEval-2013)Tweets 4,435 3,813SMS messages 2,334 2,094Test SemEval-2014Tweets + SMS messages + 10,681 8,987blog posts + sarcastic tweetsTable 1: Number of labeled instances containedby the training, development (test data SemEval-2013), and SemEval-2014 test sets.2 System DescriptionOur main goal was to develop, for each polarityclassification task, a classifier to label a messageor an instance of that message as either positive,negative, or neutral.
We ran several experiments toidentify the most discriminative classifier features.This section gives an overview of the pipeline wedeveloped and which features were implemented.2.1 Linguistic PreprocessingFirst, we performed manual cleaning on thedatasets to replace non-UTF-8 characters, and wetokenized all messages using the Carnegie MellonUniversity Twitter Part-of-Speech Tagger (Gimpelet al., 2011).
Subsequently, we Part-of-Speechtagged all instances using the CMU Twitter Part-of-Speech Tagger (Gimpel et al., 2011), and per-formed dependency parsing using a caseless pars-ing model of the Stanford parser (de Marneffe etal., 2006).
Besides that, we also tagged all namedentities using the Twitter NLP tools (Ritter et al.,2011) for Named Entity Recognition.
As a finalpreprocessing step, we decided to combine the la-bels neutral, objective and neutral-OR-objective,thus recasting the task as a three-way classifica-tion task.2.2 Feature ExtractionWe implemented a number of lexical and syntacticfeatures that represent every phrase (subtask A) ormessage (subtask B) within a feature vector:N-gram features?
Word token n-gram features: a binary valuefor every token unigram, bigram, and trigramfound in the training data.?
Character n-gram features: a binary valuefor every character trigram, and fourgram(within word tokens) found in the trainingdata.?
Normalized n-gram features: n-grams thatconsisted of URLs and mentions or @-replies were replaced by http://someurl andby @someuser, respectively.
We also nor-malized commonly used abbreviations1totheir full written form (e.g.
h8?
hate).Word shape features?
Character flooding: the number of word to-kens with a character repeated more than twotimes (e.g.
sooooooo join).?
Punctuation flooding: the number of con-tiguous sequences of exclamation/questionmarks (e.g.
GRADUATION?!?!).?
Punctuation of the last token: a binary valueindicating whether the last word token ofa message contains a question/exclamationmark (e.g.
Going to Helsinki tomorrow or onthe day after tomorrow, yay!).?
The number of capitalized words (e.g.
SOEXCITED).?
The number of hashtags (e.g.
#win).Lexicon features: As sentiment lexicons weconsulted existing resources: AFINN (Nielsen,2011), General Inquirer (Stone et al., 1966),MPQA (Wilson et al., 2005), NRC Emotion (Mo-hammad and Turney, 2010; Mohammad andYang, 2011), Bing Liu (Hu and Liu, 2004), andBounce (K?okciyan et al., 2013) ?
the latter threeare Twitter-specific.
Additionally, we created a listof emoticons extracted from the SemEval-2014training data.
Based on these resources, the fol-lowing features were extracted:?
The number of positive, negative, and neutrallexicon words averaged over text length?
The overall polarity, which is the sum of thevalues of identified sentiment wordsThese features were extracted by 1) looking at alltokens in the instance, and 2) looking at hash-tag tokens only (e.g.
win from #win).
We alsoconsidered negation cues by flipping the polarity1These were extracted from an existing list of chat abbre-viations (http://www.chatslang.com/terms/abbreviations).407sign of a sentiment word if it occurred in a nega-tion relation (e.g.
@ 2Shades maybe 3rd team bro,he?s not better than trey Burke from Michigan).Negation relations were identified using the outputof the dependency parser.
In the example above,the positive polarity of the sentiment word betteris flipped into negative since it occurs in a relationwith not.Syntactic features:?
Part-of-Speech ?
25 tags, including Twitter-specific tags such as # (hashtags), @ (at-mentions), ~ (retweets), U (URLs or e-mailaddresses), and E (emoticons): binary (tagoccurs in the tweet or not), ternary (tag oc-curs zero, one, or two or more times), abso-lute (number of occurrences), and frequency(frequency of the tag).?
Dependency relations ?
four binary values forevery dependency relation found in the train-ing data.
The first value indicates the pres-ence of the lexicalized dependency relationsin the test data.
Additionally, as proposedby (Joshi and Penstein-Ros?e, 2009), the de-pendency relation features are generalized inthree ways: by backing off the head word toits PoS-tag, by backing off the modifier wordto its PoS-tag, and by backing off both thehead and modifier word.Named entity features: This feature group con-sists of four features: binary (tweet contains NEsor not), absolute (number of NEs), absolute tokens(number of tokens that are part of an NE), and fre-quency tokens (frequency of NE tokens).PMI features: PMI (pointwise mutual informa-tion) values indicating the association of a wordwith positive and negative sentiment.
The higherthe PMI value, the stronger the word-sentiment as-sociation.
For each unigram and bigram in thetraining data, PMI values were extracted fromthe word-sentiment association lexicon created byNRC Canada (Mohammad et al., 2013).
A sec-ond PMI feature was considered for each unigrambased on the word-sentiment associations found inthe SemEval-2014 training dataset.
PMI valueswere calculated as follows:PMI(w) = PMI(w, positive)?
PMI(w, negative)(1)As the equation shows, the association score of aword with negative sentiment is subtracted fromthe word?s association score with positive senti-ment.2.3 Optimizing the Classification ResultsThe core of our approach consisted in evaluatingthe aforementioned features and selecting thosefeature groups contributing most to the classifica-tion results.
To this end, we trained an SVM clas-sifier using the LIBSVM package (Chang and Lin,2001) and created models for various feature com-binations.
A linear kernel and a cost value of 1were chosen as parameter settings for all furtherexperiments after cross-validation on the trainingdata.
Our experimental setup consisted of threesteps: 1) training an SVM on the original train-ing data provided by the task organizers (no de-velopment data was used), 2) generating a model,and 3) applying and evaluating the model on thedevelopment data (Twitter and SMS test data ofSemEval-2013).
We started our experiments withsentiment lexicon and n-gram features only, andgradually added other feature groups to identifythe most contributive features.
Tables 2 and 3 re-veal the obtained F-scores for each step.Features Dev Twitter Dev SMSlexicons 0.6855 0.6402n-grams 0.8482 0.8229n-grams + lexicons 0.8628 0.8489+ normalization n-grams 0.8632 (+ 0.0004) 0.8502 (+ 0.0013)+ Part-of-Speech 0.8646 (+ 0.0014) 0.8582 (+ 0.0080)+ negation 0.8650 (+ 0.0004) 0.8654 (+ 0.0072)+ word shape 0.8649 (- 0.0001) 0.8650 (- 0.0004)+ named entity 0.8642 (- 0.0007) 0.8660 (+ 0.0010)+ dependency 0.8642 (=) 0.8660 (=)+ PMI 0.8610 (- 0.0032) 0.8654 (- 0.0006)Table 2: F-scores obtained after adding other fea-tures for the Twitter and SMS development data(test data SemEval-2013) ?
subtask A.Features Dev Twitter Dev SMSlexicons 0.5342 0.5119n-grams 0.5896 0.5628n-grams + lexicons 0.6442 0.6040+ normalization n-grams 0.6414 (- 0.0028) 0.6084 (+ 0.0044)+ Part-of-Speech 0.6466 (+ 0.0052) 0.6333 (+ 0.0249)+ negation 0.6542 (+ 0.0076) 0.6384 (+ 0.0051)+ word shape 0.6581 (+ 0.0039) 0.6394 (+ 0.0010)+ named entity 0.6559 (- 0.0022) 0.6399 (+ 0.0005)+ dependency 0.6467 (- 0.0092) 0.6430 (+ 0.0031)+ PMI 0.6525 (+ 0.0058) 0.6525 (+ 0.0095)Table 3: F-scores obtained after adding other fea-tures for the Twitter and SMS development data(test data SemEval-2013) ?
subtask B.As can be inferred from the tables, F-scores408SMS2013 Twitter2013 LiveJournal2014 Twitter2014 Twitter2014 SarcasmTask A 85.26 (7/27) 86.28 (8/27) 80.44 (13/27) 81.02 (9/27) 70.76 (13/27)Task B 64.78 (7/50) 65.56 (14/50) 68.56 (20/50) 65.47 (16/50) 47.76 (22/50)Table 4: F-scores and rankings of our systems across the various data genres for subtask A (ContextualPolarity Disambiguation) and subtask B (Message Polarity Classification).were already relatively high (~0.8559 for subtaskA and ~0.6241 for subtask B) for the combinedlexicon and n-gram features (on average 0.8559for subtask A and 0.6241 for subtask B), which wetherefore consider as a our baseline setup.
Con-sidering the results for both subtasks and datagenres, we conclude that n-grams, sentiment lex-icons, and PoS-tags were the most contributivefeature groups, whereas named entity and depen-dency features did not improve the overall classi-fication performance.
However, using all featuregroups (n-grams, lexicons, normalized n-grams,Part-of-Speech features, negation features, wordshape features, named entity features, dependencyfeatures, and PMI features) improved the classi-fication results (reaching an averaged F= 0.8632for subtask A, and F= 0.6525 for subtask B) com-pared to classification based on lexicon (averagedF= 0.6629 for subtask A, and F= 0.5231 for sub-task B) or n-gram features only (averaged F=0.8356 for subtask A, and F= 0.5762 for subtaskB).
Based on these results, we conclude that usingthe full feature set for the classification of unseendata appears to be a promising approach, consid-ering that it achieves good performance and that itwould not tune the training model to a particulardata genre.For further optimization of the classification re-sults, we performed feature selection in the fea-ture groups by using a genetic algorithm approachwhich can explore different areas of the searchspace in parallel.
In order to do so, we made useof the Gallop (Genetic Algorithms for LinguisticLearner Optimization) python package (Desmetet al., 2013).
This enabled us to select the mostcontributive features from every feature group: n-gram features at token and character level, lexi-con features from General Inquirer, Liu, AFINN,and Bounce, character flooding and token capital-ization features, Part-of-Speech features (binary,ternary, and absolute), named entity features (bi-nary, absolute tokens, and frequency tokens), andPMI features based on the NRC lexicon.
None ofthe dependency relation features were selected.3 ResultsWe submitted sentiment labels for the ContextualPolarity Disambiguation (subtask A) and for theMessage Polarity Classification (subtask B).
Ourcompetition results are reported in Table 4.
Rank-ings for each dataset are added between brack-ets.
The results reveal that our systems achievedgood performance in the polarity classification ofunseen data across the various genres and tasks.Overall, we achieved our best classification per-formance on the Twitter2013 test set, obtaining anF-score of 86.28, while the best performance forthis data genre is an F-score of 90.14.
We saw adrop in performance on the Twitter2014 Sarcasmtest set.
This is consistent with most other teamsas sarcastic language is hard to handle in senti-ment analysis.
Considering the rankings, we con-clude that we performed particularly well on theSMS test dataset of SemEval-2013 for both sub-tasks, ranking seventh for this genre.
Our systemsranked ninth among 27 submissions and sixteenthamong 50 submissions for subtasks A and B re-spectively.4 Conclusions and Future WorkUsing a rich feature set proves to be beneficial forautomatic sentiment analysis on user-generatedcontent.
Feature selection experiments revealedthat features based on n-grams, sentiment lexi-cons, and PoS-tags were most contributive forboth classification tasks, while dependency fea-tures did not contribute to overall classificationperformance.
As future work it will be interestingto study the impact of normalization of the data onthe classification performance.Based on a shallow error analysis, we believethat including additional classification featuresmay also be promising: modifiers other than nega-tion cues (diminishers, increasers, modal verbs,etc.)
that affect the polarity intensity, emoticonflooding, and pre- and suffixes that indicate emo-tion (un-, dis-, -less, etc.).
Additionally, lemma-tization and hashtag segmentation on the trainingdata could also improve classification results.409ReferencesChih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProc.
of LREC?06).Bart Desmet, V?eronique Hoste, David Verstraeten, andJan Verhasselt.
2013.
Gallop Documentation.Technical Report LT3 13-03, University of Ghent.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: Annotation, features, and experiments.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies: Short Papers - Volume 2,HLT ?11, pages 42?47, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD04, pages168?177, New York, NY.
ACM.Mahesh Joshi and Carolyn Penstein-Ros?e.
2009.
Gen-eralizing dependency features for opinion mining.In Proceedings of the ACL-IJCNLP 2009 Confer-ence Short Papers, ACLShort ?09, pages 313?316,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Nadin K?okciyan, Arda C?elebi, Arzucan?Ozg?ur, andSuzan?Usk?udarli.
2013.
Bounce: Sentiment classi-fication in Twitter using rich feature sets.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 554?561, Atlanta, Geor-gia, USA.
ACL.Diane Maynard, Kalina Bontcheva, and Dominic Rout.2012.
Challenges in developing opinion miningtools for social media.
In Proc.
of the LREC work-shop NLP can u tag #usergeneratedcontent?
!Saif Mohammad and Peter Turney.
2010.
EmotionsEvoked by Common Words and Phrases: Using Me-chanical Turk to Create an Emotion Lexicon.
InProceedings of the NAACL-HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, LA, California.Saif Mohammad and Tony Yang.
2011.
TrackingSentiment in Mail: How Genders Differ on Emo-tional Axes.
In Proceedings of the 2nd Workshop onComputational Approaches to Subjectivity and Sen-timent Analysis (WASSA 2011), pages 70?79, Port-land, Oregon.
ACL.Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
CoRR,abs/1308.6242.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA, June.
Association forComputational Linguistics.Finn Nielsen.
2011.
A new anew: Evaluation of aword list for sentiment analysis in microblogs.
InProceedings of the ESWC2011 Workshop on Mak-ing Sense of Microposts: Big things come in smallpackages.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?11, pages 1524?1534, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Preslav Nakov andTorsten Zesch, editors, Proceedings of the 8th In-ternational Workshop on Semantic Evaluation, Se-mEval ?14, Dublin, Ireland.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions andemotions in language.
Computer Intelligence,39(2):165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT05, pages 347?354, Stroudsburg, PA. ACL.410
