First Joint Conference on Lexical and Computational Semantics (*SEM), pages 328?334,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUMichigan: A Conditional Random Field Model for Resolving the Scope ofNegationAmjad Abu-JbaraEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAamjbara@umich.eduDragomir RadevEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAradev@umich.eduAbstractIn this paper, we present a system for de-tecting negation in English text.
We addressthree tasks: negation cue detection, negationscope resolution and negated event identifi-cation.
We pose these tasks as sequence la-beling problems.
For each task, we train aConditional Random Field (CRF) model onlexical, structural, and syntactic features ex-tracted from labeled data.
The models aretrained and tested using the dataset distributedwith the *sem Shared Task 2012 on resolvingthe scope and focus of negation.
The systemdetects negation cues with 90.98% F1 mea-sure (94.3% and 87.88% recall).
It identifiesnegation scope with 82.70% F1 on token-by-token level and 64.78% F1 on full scope level.Negated events are detected with 51.10% F1measure.1 IntroductionNegation is a linguistic phenomenon present in alllanguages (Tottie, 1993; Horn, 1989).
The seman-tic function of negation is to transform an affirma-tive statement into its opposite meaning.
The auto-matic detection of negation and its scope is a prob-lem encountered in a wide range of natural languageprocessing applications including, but not limited to,data mining, relation extraction, question answering,and sentiment analysis.
For example, failing to ac-count for negation may result in giving wrong an-swers in question answering systems or in the pre-diction of opposite sentiment in sentiment analysissystems.The occurrence of negation in a sentence is deter-mined by the presence of a negation cue.
A nega-tion cue is a word, a phrase, a prefix, or a postfixthat triggers negation.
Scope of negation is the partof the meaning that is negated (Huddleston and Pul-lum, 2002).
The negated event is the event or the en-tity that the negation indicates its absence or deniesits occurrence.
For example, in the sentence belownever is the negation cue.
The scope is enclosed insquare brackets.
The negated event is underlined.
[Andrew had] never [liked smart phones],but he received one as a gift last week andstarted to use it.Negation cues and scopes may be discontinuous.For example, the negation cue neither ... nor is dis-continuous.In this chapter, we present a system for automat-ically detecting negation cues, negated events, andnegation scopes in English text.
The system usesconditional random field (CRF) models trained onlabeled sentences extracted from two classical En-glish novels.
The CRF models are trained using lex-ical, structural, and syntactic features.
The experi-ments show promising results.This paper is organized as follows.
Section 2 re-views previous work.
Section 3 describes the data.Section 4 describes the CRFs models.
Section 5presents evaluation, results, and discussion.2 Previous WorkMost research on negation has been done in thebiomedical domain (Chapman et al, 2001; Mutaliket al, 2001; Kim and Park, 2006; Morante et al,328Token Lemma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2She She PRP (S(NP*) - She - - - -would would MD (VP* - would - - - -not not RB * not - - - - -have have VB (VP* - have - - - -said say VBD (VP* - said - - - -?
?
?
(SBAR(S(NP* - ?
- - - -Godspeed Godspeed NNP * - Godspeed - - - -?
?
?
*) - ?
- - - -had have VBD (VP* - had - - had -it it PRP (ADVP* - it - - it -not not RB *) - not - not - -been be VBN (VP* - been - - been -so so RB (ADVP*)))))))) - so - - so -.
.
.
*) - - - - - -Table 1: Example sentence annotated for negation following sem shared task 2012 format2008a; Morante and Daelemans, 2009; Agarwal andYu, 2010; Morante, 2010; Read et al, 2011), mostlyon clinical reports.
The reason is that most NLP re-search in the biomedical domain is interested in au-tomatically extracting factual relations and pieces ofinformation from unstructured data.
Negation detec-tion is important here because information that fallsin the scope of a negation cue cannot be treated asfacts.Chapman et al (2001) proposed a rule-based al-gorithm called NegEx for determining whether afinding or disease mentioned within narrative med-ical reports is present or absent.
The algorithmuses regular-expression-based rules.
Mutalik etal.
(2001) developed another rule based systemcalled Negfinder that recognizes negation patternsin biomedical text.
It consists of two components:a lexical scanner, lexer that uses regular expres-sion rules to generate a finite state machine, and aparser.
Morante (2008b) proposed a supervised ap-proach for detecting negation cues and their scopesin biomedical text.
Their system consists of twomemory-based engines, one that decides if the to-kens in a sentence are negation signals, and anotherone that finds the full scope of these negation sig-nals.Negation has been also studied in the context ofsentiment analysis (Wilson et al, 2005; Jia et al,2009; Councill et al, 2010; Heerschop et al, 2011;Hogenboom et al, 2011).
Wiegand et al (2010) sur-veyed the recent work on negation scope detectionfor sentiment analysis.
Wilson et al (2005) studiedthe contextual features that affect text polarity.
Theyused a machine learning approach in which nega-tion is encoded using several features.
One featurechecks whether a negation expression occurs in afixed window of four words preceding the polar ex-pression.
Another feature accounts for a polar pred-icate having a negated subject.
They also have dis-ambiguation features to handle negation words thatdo not function as negation cues in certain contexts,e.g.
not to mention and not just.Jia et al (2009) proposed a rule based method todetermine the polarity of sentiments when one ormore occurrences of a negation term such as not ap-pear in a sentence.
The hand-crafted rules are ap-plied to syntactic and dependency parse tree repre-sentations of the sentence.Hogenboom et al (2011) found that applying asimple rule that considers two words, following anegation keyword, to be negated by that keyword,to be effective in improving the accuracy of senti-ment analysis in movie reviews.
This simple methodyields a significant increase in overall sentimentclassification accuracy and macro-level F1 of 5.5%and 6.2%, respectively, compared to not accountingfor negation.This work is characterized by addressing threetasks at once: negation cue detection, negatedevent identification, and negation scope resolution.Our proposed approach uses a supervised graphicalprobabilistic model trained using labeled data.3293 DataWe use the dataset distributed by the organizers ofthe *sem Shared Task 2012 on resolving the scopeand focus of negation.
This dataset includes two sto-ries by Conan Doyle, The Hound of the Baskervilles,The Adventures of Wisteria Lodge.
All occur-rences of negation are annotated accounting fornegation expressed by nouns, pronouns, verbs, ad-verbs, determiners, conjunctions and prepositions.For each negation cue, the negation cue and scopeare marked, as well as the negated event (if any ex-ists).
The annotation guidelines follow the proposalof Morante et al (2011)1.
The data is split into threesets: a training set containing 3,644 sentences, a de-velopment set containing 787 sentences, and a test-ing set containing 1,089 sentences.
The data is pro-vided in CoNLL format.
Each line corresponds to atoken and each annotation is provided in a column;empty lines indicate end of sentences.
The providedannotations are:?
Column 1: chapter name?
Column 2: sentence number within chapter?
Column 3: token number within sentence?
Column 4: word?
Column 5: lemma?
Column 6: part-of-speech?
Column 7: syntax?
Columns 8 to last:?
If the sentence has no negations, column8 has a ?***?
value and there are no morecolumns.?
If the sentence has negations, the annota-tion for each negation is provided in threecolumns.
The first column contains theword or part of the word (e.g., morpheme?un?
), that belongs to the negation cue.The second contains the word or part ofthe word that belongs to the scope of thenegation cue.
The third column containsthe word or part of the word that is the1http://www.clips.ua.ac.be/sites/default/files/ctrs-n3.pdfToken Lemma Punc.
Cat.
POS LabelSince Since 0 OTH IN Owe we 0 PRO PRP Ohave have 0 VB VBP Obeen be 0 VB VBN Oso so 0 ADVB RB Ounfortunate unfortunate 0 ADJ JJ PREas as 0 ADVB RB Oto to 0 OTH TO Omiss miss 0 VB VB Ohim him 0 PRO PRP Oand and 0 OTH CC Ohave have 0 VB VBP Ono no 0 OTH DT NEGnotion notion 0 NOUN NN Oof of 0 OTH IN Ohis his 0 PRO PRP$ Oerrand errand 0 NOUN NN O, , 1 OTH , Othis this 0 OTH DT Oaccidental accidental 0 ADJ JJ Osouvenir souvenir 0 NOUN NN Obecomes become 0 VB VBZ Oof of 0 OTH IN Oimportance importance 0 NOUN NN O. .
1 OTH .
OTable 2: Example sentence labeled for negation cue de-tectionnegated event or property.
It can be thecase that no negated event or property aremarked as negated.Table 1 shows an example of an annotated sen-tence that contains two negation cues.4 ApproachThe problem that we are trying to solve can be splitinto three tasks.
The first task is to detect negationcues.
The second task is to identify the scope of eachdetected negation cue.
The third task is to identifythe negated event.
We use a machine learning ap-proach to address these tasks.
We train a Condi-tional Random Field (CRF) (Lafferty et al, 2001)model on lexical, structural, and syntactic featuresextracted from the training dataset.
In the followingsubsections, we describe the CRF model that we usefor each task.4.1 Negation Cue DetectionNegation cues are lexical elements that indicate theexistence of negation in a sentence.
From lexical330point of view, negation cues can be divided into fourcategories:1.
Prefix (i.e.
in-, un-, im-, il-, dis-).
For example,un- in unsuitable) is a prefix negation cue.2.
Postfix (i.e.
-less).
for example, -less incareless.3.
Multi-word negation cues such as neither...nor,rather than, by no means, etc.4.
Single word negation cues such as not, no,none, nobody, etc.The goal of this task is to detect negation cues.We pose this problem as a sequence labeling task.The reason for this choice is that some negation cuesmay not indicate negation in some contexts.
Forexample, the negation cue not in the phrase not tomention does not indicate negation.
Also, as we sawabove, some negation cues may consist of multiplewords, some of them are continuous and others arediscontinuous.
Treating the task as a sequence label-ing problem help model the contextual factors thataffect the function of negation cues.
We train a CRFmodel using features extracted from the sentences ofthe training dataset.
The token level features that wetrain the model on are:?
Token: The word or the punctuation mark as itappears in the sentence.?
Lemma: The lemmatized form of the token.?
Part-Of-Speech tag: The part of speech tag ofthe token.?
Part-Of-Speech tag category: Part-of-speechtags reduced into 5 categories: Adjec-tive (ADJ), Verb (VB), Noun (NN), Adverb(ADVB), Pronoun (PRO), and other (OTH).?
Is punctuation mark: This feature takes thevalue 1 if the token is a punctuation mark and 0otherwise.?
Starts with negation prefix: This feature takesthe value 1 if the token is a word that starts withun-, in-, im-, il-, or dis- and 0 otherwise.?
Ends with negation postfix: This feature takesthe value 1 if the token is a word that ends with-less and 0 otherwise.The CRF model that we use considers at each to-ken the features of the current token, the two pre-ceding tokens, and the two proceeding tokens.
Themodel also uses token bigrams and trigrams, andpart-of-speech tag bigrams and trigrams as features.The labels are 5 types: ?O?
for tokens that arenot part of any negation cue; ?NEG?
for singleword negation cues; ?PRE?
for prefix negation cue;?POST?
for postfix negation cue; and ?MULTI-NEG?
for multi-word negation cues.
Table 2 showsan example labeled sentence.At testing time, if a token is labeled ?NEG?
or?MULTI-NEG?
the whole token is treated as a nega-tion cue or part of a negation cue respectively.
If atoken is labeled as ?PRE?
or ?POST?, a regular ex-pression is used to determine the prefix/postfix thattrigged the negation.4.2 Negation Scope DetectionScope of negation is the sequence of tokens (canbe discontinuous) that expresses the meaning thatis meant to be negated by a negation cue.
A sen-tence may contain zero or more negation cues.
Eachnegation cue has its own scope.
It is possible thatthe scope of two negation cues overlap.
We useeach negation instance (i.e.
each negation cue andits scope) as one training example.
Therefore, asentence that contains two negation cues providestwo training examples.
We train a CRF model onfeatures extracted from all negation instances in thetraining dataset.
The features that we use are:?
Token: The word or the punctuation mark as itappears in the sentence.?
Lemma: The lemmatized form of the token.?
Part-Of-Speech tag: The part of speech tag ofthe token.?
Part-Of-Speech tag category: Part-of-speechtags reduced into 5 categories: Adjec-tive (ADJ), Verb (VB), Noun (NN), Adverb(ADVB), Pronoun (PRO), and other (OTH).331?
Is punctuation mark: This feature takes thevalue 1 if the token is a punctuation mark and 0otherwise.?
Type of negation cue: Possible types are:?NEG?
for single word negation cues; ?PRE?for prefix negation cue; ?POST?
for postfixnegation cue; and ?MULTI?
for multi-wordnegation cues.?
Relative position: This feature takes the value1 if the token position in the sentence is be-fore the position of the negation cue, 2 if thetoken position is after the position of the nega-tion cue, and 3 if the token is the negation cueitself.?
Distance: The number of tokens between thecurrent token and the negation cue.?
Same segment: This feature takes the value 1if this token and the negation cue fall in thesegment in the sentence.
The sentence is seg-mented by punctuation marks.?
Chunk: This feature takes the value NP-B (VP-B) if this token is the first token of a noun (verb)phrase, NP-I (VP-I) if it is inside a noun (verb)phrase, NP-E (VP-E) if it is the last token of anoun (verb) phrase.?
Same chunk: This feature takes the value 1 ifthis token and the negation cue fall in the samechunk (noun phrase or verb phrase).?
Is negation: This feature takes the value 1 ifthis token is a negation cue, and 0 otherwise.?
Syntactic distance: The number of edges in theshortest path that connects the token and thenegation in the syntactic parse tree.?
Common ancestor node: The type of the nodein the syntactic parse tree that is the least com-mon ancestor of this token and the negation cuetoken.The CRF model considers the features of 4 tokensto the left and to the right at each position.
It alsouses bigram and trigram combinations of some ofthe features.At testing time a few postprocessing rules areused to fix sure labels if they were labeled incor-rectly.
For example, if a word starts with a prefixnegation cue, the word itself (without the prefix) isalways part of the scope and it is also the negatedevent.4.3 Negated Event IdentificationIt is possible that a negation cue comes associatedwith an event.
A negation has an event if it oc-curs in a factual context.
The dataset that we usewas labeled for negated events whenever one exists.We used the same features described in the previoussubsection to train a CRF model for negated eventidentification.
We have also tried to use one CRFmodel for both scope resolution and negated eventidentification, but we noticed that using two sepa-rate models results in significantly better results forboth tasks.5 EvaluationWe use the testing set described in Section 3 to eval-uate the system.
The testing set contains 1089 sen-tences 235 of which contains at least one negation.We use the standard precision, recall, and f-measure metrics to evaluate the system.
We performthe evaluation on different levels:1.
Cues: the metrics are computed only for cuedetection.2.
Scope (tokens): the metrics are calculated at to-ken level.
If a sentence has 2 scopes, one with5 tokens and another with 4, the total numberof scope tokens is 9.3.
Scope (full): the metrics are calculated at thefull scope level.
Both the negation cue andthe whole scope should be correctly identified.If a sentence contains 2 negation cues, then 2scopes are checked.
We report two values hereone the requires the cue match correctly andone that does not.4.
Negated Events: the metrics are computed onlyfor negated events identification (apart fromnegation cue and scope).332Variant Agold system tp fp fn precision recall F1Cues 264 250 232 14 32 94.31 87.88 90.98Scope (cue match) 249 227 126 14 123 90.00 50.60 64.78Scope (no cue match) 249 227 126 14 123 90.00 50.60 64.78Scope (tokens - no cue match) 1805 1716 1456 260 349 84.85 80.66 82.70Negated (no cue match) 173 183 70 70 64 50.00 52.24 51.10Full negation: 264 250 75 14 189 84.27 28.41 42.49Variant Bgold system tp fp fn precision recall F1Cues : 264 250 232 14 32 92.80 87.88 90.27Scope (cue match): 249 227 126 14 123 55.51 50.60 52.94Scope (no cue match): 249 227 126 14 123 55.51 50.60 52.94Negated (no cue match): 173 183 70 70 64 38.25 52.24 44.16Full negation : 264 250 75 14 189 30.00 28.41 29.18# Sentences 1089# Negation sentences 235# Negation sentences with errors 171% Correct sentences 83.47% Correct negation sentences 27.23Table 3: Results of negation cue, negated event, and negation scope detection5.
Full negation: the metrics are computed for allthe three tasks at once and requiring everythingto match correctly.For cue, scope and negated event to be correct,both the tokens and the words or parts of words haveto be correctly identified.
The final periods in abbre-viations are disregarded.
If gold has value ?Mr.?
andsystem ?Mr?, system is counted as correct.
Also,punctuation tokens are *not* taken into account forevaluation.Two variants of the metrics are computed.
In thefirst variant (A), precision is calculated as tp / (tp +fp) and recall is calculated as tp / (tp + fn) where tpis the count of true positive labels, fp is the countof false positive labels, and fn is the count of falsenegative labels.
In variant B, the precision is calcu-lated differently, using the formula precision = tp /system.Table 3 shows the results of our system.6 Error AnalysisThe system used no external resources outside thetraining data.
This means that the system recognizesonly negation cues that appeared in the training set.This was the first source of error.
For example, theword unacquainted that starts with the negation pre-fix un has never been seen in the training data.
In-tuitively, if no negation cue is detected, the systemdoes not attempt to produce scope levels.
This prob-lem can be overcome by using a lexicon of negationwords and those words that can be negated by addinga negation prefix to them.We noticed in several occasions that scope detec-tion accuracy can be improved if some simple rulescan be imposed after doing the initial labeling us-ing the CRF model (but we have not actually imple-mented any such rules in the system).
For example,the system can require all the tokens that belong tothe same chunk (noun group, verb group, etc.)
allhave the same label (e.g.
the majority vote label).The same thing could be also applied on the segmentrather than the chunk level where the boundaries ofsegments are determined by punctuation marks.7 ConclusionWe presented a supervised system for identifyingnegation in English sentences.
The system usesthree CRF trained models.
One model is trained fornegation cue detection.
Another model is trainedfor negated event identification.
A third one istrained for negation scope identification.
The mod-els are trained using features extracted from a la-beled dataset.
Our experiments show that the systemachieves promising results.333ReferencesShashank Agarwal and Hong Yu.
2010.
Biomedi-cal negation scope detection with conditional randomfields.
Journal of the American Medical InformaticsAssociation, 17(6):696?701.Wendy Webber Chapman, Will Bridewell, Paul Hanbury,Gregory F. Cooper, and Bruce G. Buchanan.
2001.
Asimple algorithm for identifying negated findings anddiseases in discharge summaries.
Journal of Biomedi-cal Informatics, pages 301?310.Isaac G. Councill, Ryan McDonald, and Leonid Ve-likovich.
2010.
What?s great and what?s not: learn-ing to classify the scope of negation for improved sen-timent analysis.
In Proceedings of the Workshop onNegation and Speculation in Natural Language Pro-cessing, NeSp-NLP ?10, pages 51?59, Stroudsburg,PA, USA.
Association for Computational Linguistics.Bas Heerschop, Paul van Iterson, Alexander Hogenboom,Flavius Frasincar, and Uzay Kaymak.
2011.
Analyz-ing sentiment in a large set of web data while account-ing for negation.
In AWIC, pages 195?205.Alexander Hogenboom, Paul van Iterson, Bas Heerschop,Flavius Frasincar, and Uzay Kaymak.
2011.
Deter-mining negation scope and strength in sentiment anal-ysis.
In SMC, pages 2589?2594.Laurence R. Horn.
1989.
A natural history of nega-tion / Laurence R. Horn.
University of Chicago Press,Chicago :.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press, April.Lifeng Jia, Clement Yu, and Weiyi Meng.
2009.
Theeffect of negation on sentiment analysis and retrievaleffectiveness.
In Proceedings of the 18th ACM con-ference on Information and knowledge management,CIKM ?09, pages 1827?1830, New York, NY, USA.ACM.Jung-Jae Kim and Jong C. Park.
2006.
Extracting con-trastive information from negation patterns in biomed-ical literature.
5(1):44?60, March.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Roser Morante and Walter Daelemans.
2009.
Learningthe scope of hedge cues in biomedical texts.
Pro-ceedings of the Workshop on BioNLP BioNLP 09,(June):28.Roser Morante, Anthony Liekens, and Walter Daele-mans.
2008a.
Learning the scope of negation inbiomedical texts.
Proceedings of the Conference onEmpirical Methods in Natural Language ProcessingEMNLP 08, (October):715?724.Roser Morante, Anthony Liekens, and Walter Daele-mans.
2008b.
Learning the scope of negation inbiomedical texts.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 715?724, Honolulu, Hawaii, October.Association for Computational Linguistics.Roser Morante, Sarah Schrauwen, and Walter Daele-mans.
2011.
Annotation of negation cues and theirscope.
Technical report.Roser Morante.
2010.
Descriptive analysis of negationcues in biomedical texts.
Language Resources AndEvaluation, pages 1?8.P.
G. Mutalik, A. Deshpande, and P. M. Nadkarni.
2001.Use of general-purpose negation detection to augmentconcept indexing of medical documents: a quantitativestudy using the UMLS.
Journal of the American Med-ical Informatics Association : JAMIA, 8(6):598?609.Jonathon Read, Erik Velldal, Stephan Oepen, and Liljavrelid.
2011.
Resolving speculation and negationscope in biomedical articles with a syntactic con-stituent ranker.
In Proceedings of the Fourth Inter-national Symposium on Languages in Biology andMedicine, Singapore.Gunnel Tottie.
1993.
Negation in English Speech andWriting: A Study in Variation.
Language, 69(3):590?593.Michael Wiegand, Alexandra Balahur, Benjamin Roth,Dietrich Klakow, and Andre?s Montoyo.
2010.
A sur-vey on the role of negation in sentiment analysis.
InProceedings of the Workshop on Negation and Spec-ulation in Natural Language Processing, NeSp-NLP?10, pages 60?68, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the confer-ence on Human Language Technology and EmpiricalMethods in Natural Language Processing, HLT ?05,pages 347?354, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.334
