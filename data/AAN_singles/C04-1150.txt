Quantitative and Qualitative Evaluation of the OntoLearn Ontology LearningSystemRoberto NAVIGLI, Paola VELARDIDipartimento di InformaticaUniversit?
?La Sapienza?via Salaria 113Roma, Italy, 00198{velardi,navigli}@di.uniroma1.itAlessandro CUCCHIARELLI, Francesca NERIDIIGAUniversit?
Politecnica delle Marchevia Brecce Bianche 12Ancona, Italy, 60131{cucchiarelli, neri}@diiga.univpm.itAbstractOntology evaluation is a critical task, even more sowhen the ontology is the output of an automaticsystem, rather than the result of a conceptualisationeffort produced by a team of domain specialistsand knowledge engineers.
This paper provides anevaluation of the OntoLearn ontology learningsystem.
The proposed evaluation strategy istwofold: first, we provide a detailed quantitativeanalysis of the ontology learning algorithms, inorder to compute the accuracy of OntoLearn underdifferent learning circumstances.
Second, weautomatically generate natural languagedescriptions of formal concept specifications, inorder to facilitate per-concept qualitative analysisby domain specialists.1 Evaluating ontologiesAutomatic methods for ontology learning andpopulation have been proposed in recent literature(e.g.
ECAI-2002 and KCAP-2003 workshops1) buta co-related issue then becomes the evaluation ofsuch automatically generated ontologies, not onlywith the goal of comparing the differentapproaches (Hovy, 2001) and ontology-based tools(Angele and Sure, 2002), but also to verify whetheran automatic process may actually compete withthe typically human process of converging on anagreed conceptualization of a given domain.Ontology construction, apart from the technicalaspects of a knowledge representation task (i.e.choice of representation languages, consistencyand correctness with respect to axioms, etc.
), is aconsensus building process, one that implies longand often harsh discussions among the specialistsof a given domain.
Can an automatic methodsimulate this process?
Can we provide domainspecialists with a means to measure the adequacyof a specific set of concepts as a model of a given1ECAI-2002 http://www-sop.inria.fr/acacia/WORKSHOPS/ECAI2002-OLT/accepted-papers.htmlKCAP-2003 http://km.aifb.uni-karlsruhe.de/ws/semannot2003/papers.htmldomain?, Specialists are often unable to evaluatethe formal content of a computational ontology(e.g.
the denotational theory, the formal notation,the knowledge representation system capabilitieslike property inheritance, consistency, etc.
).Evaluation of the formal content is rather tackledby computational scientists, or by automaticverification systems.
The role of the specialists isinstead to compare their intuition of a domain withthe description of this domain, as provided by theontology concepts.
To facilitate one suchqualitative per-concept evaluation, we devised amethod for automatic generation of textualexplanations (glosses) of automatically learnedconcepts.
Glosses provide a description, in naturallanguage, of the formal specifications assigned tothe learned concepts.
An expert can easily comparehis intuition with these natural languagedescriptions.The objective of the gloss-based evaluation is, aspreviously remarked, to obtain a judgement, bydomain specialists, concerning the adequacy of anautomatically derived domain conceptualisation.On the computational side, an ontology learningtool is based on a battery of software programsaimed at extracting and formalising domainknowledge, usually starting from unstructureddata.
Therefore, it is equally important to produce adetailed evaluation of these programs, on aquantitative ground, in order to gain insight on theinternal and external contingencies that may affectthe result of an ontology learning process.In what follows, we firstly provide a quantitativeevaluation of the OntoLearn ontology learningsystem, under different learning circumstances.Secondly, we describe the gloss-based per-conceptevaluation method.
Both evaluation strategies areexperimented in two application domains: Tourismand Economy.The subsequent section provides a sketchydescription of the OntoLearn algorithms.
Detailsare found in (Navigli and Velardi, 2004) and(Navigli, Velardi and Gangemi, 2003).
Sections 3and 4 are dedicated to the quantitative andqualitative analyses of OntoLearn.2 Summary of the OntoLearn systemOntoLearn is an ontology population methodbased on text mining and machine learningtechniques.
OntoLearn starts with an existinggeneric ontology (we use WordNet, though otherchoices are possible) and a set of documents in agiven domain, and produces a domain extendedand trimmed version of the initial ontology.
Theontology generated by OntoLearn is anchored totexts, it can be therefore classified as a linguisticontology (G?mez-P?rez et al 2004).OntoLearn has been applied to different domains(tourism, computer networks, economy) and inseveral European projects2.Concept learning is achieved in the followingthree phases:1) Terminology Extraction: A list of domainmulti-word expressions (MWE hereafter) isextracted from a set of documents that arejudged representative of a given domain.MWEs are extracted using natural languageprocessing and statistical techniques.Contrastive corpora and glossaries in differentdomains are used to prune terminology that isnot domain-specific.
Domain MWEs areselected also on the basis of an entropy-basedmeasure that simulates specialist consensus onconcepts choice: in words, the probabilitydistribution of a ?good?
domain MWE must beuniform across the individual documents of thedomain corpus.2) Semantic interpretation of MWEs: Semanticinterpretation is based on a principle,compositional interpretation, and on a novelalgorithm, called structural semanticinterconnections (SSI) .
Composi t ionalinterpretation signifies that the meaning of amulti-word expression (MWE) can be derivedcompositionally from its components3, e.g.
themeaning of business plan is derived first, byassociating the appropriate concept identifier,with reference to the initial top ontology, to thecomponent terms (i.e.
sense #2 of business andsense #1 of plan in WordNet), and then, byidentifying the semantic relations holdingamong the involved concepts (e.g.2E.g.
: Harmonize IST-2000-29329  and the INTEROP network ofexcellence, started on december 2003.3In the literature, multi word expressions are classified ascompositional, idiosyncratically compositional and non-compositional.
In mid-technical domains, compositional MWEscover about 60-70% of MWE (we cannot support with data ourstatitics for sake of space)plan#1 topic?
?
?
?
business# 2 ).3) Extending and trimming the initialo n t o l o g y : Once the terms have beensemantically interpreted, they are organized insub-trees, and appended under the appropriatenode  o f  t he  i n i t i a l  on to logy,e.g.
business _ plan# 1 kind _ of?
?
?
?
?
?
plan# 1 .Furthermore, certain upper and lower nodes ofthe initial ontology are pruned to create adomain-view  of the ontology.
The finalontology is output in OWL language.SSI lies in the area of syntactic pattern matchingalgorithms (Bunke and Sanfeliu, 1990).
It is a wordsense disambiguation algorithm used to determinethe correct sense (with reference to the initialontology) for each component of a complex MWE.The algorithm is based on building a graphrepresentation for alternative senses of each MWEcomponent4, and then selecting the appropriatesenses on the basis of detected s e m a n t i cinterconnection patterns between graph pairs.
TheSSI algorithm seeks for semantic interconnectionsamong the words of a context T. Contexts Ti aregenerated from groups of partially overlappingcomplex MWEs (extracted during phase 1 of theOntoLearn procedure) sharing the same syntactichead .
For example, given the list of complexMWEs securities portfolio, investment portfolio,real-estate portfolio, junk-bond portfolio,diversified portfolio, stock portfolio, bondportfolio, loan portfolio, the following list of termcomponents is created:T=[security, investment, real-estate, estate, bond,junk-bond, diversified, stock, portfolio, loan ]Relevant pattern types  are described by acontext free grammar G. An example of rule in Gis the following (S1 S2 and S are concepts, i.e.synsets in WordNet):Rule Name:gloss+hyperonymy/meronymy (S1,S2):Def: :SynsetsG??
S1gloss?
?
?
?
Sand there is ahyperonymy/meronymy path between S and S2For instance, in railways company, the gloss ofrailway#1 contains the word organization, andthere is an hyperonymy path of length 2 betweencompany#1 and organization#1.
That is:railway#1 gloss?
?
?
?
o r g a n i z a t i o n # 1 ,  a n d :company#1kind _of?
?
?
?
?
?
institution#1kind _of?
?
?
?
?
?organization#1.
This pattern (an instance of thegloss+hypeonymyr/meronymy rule) cumulates4We remark again that a detailed description of the SSI algorithmis in (Navigli & Velardi, 2004) and (Navigli, Velardi and Gangemi,2003).
Graphs are generated on the basis of lexico-semanticinformation in WordNet and in a variety of on-line resources, see thementioned papers for details.evidence for senses #1 of both ra i lway andcompany.In SSI, the correct sense St for a term t?T isselected depending upon the number and weight ofpatterns matching with rules in G. The weights ofpatterns are automatically learned using aperceptron5 model.
The weight function is givenby:)_1()()1(jjjj patternlengthpatternweight ??
+=where ?
j  is the weight of rule j in G, and thesecond addend is a smoothing parameter inverselyproportional to the length of the matching pattern(e.g.
2 in the previous example, since 2 is theminimal length of the rule, and the actual length ofthe pattern is 3).
The perceptron has been trainedon the SemCor6 semantically annotated corpus.In order to complete the semantic interpretationprocess, OntoLearn then attempts to determine thesemantic relations that hold between thecomponents of a complex concept.
In order to dothis, it was first necessary to select an inventory ofsemantic relations.
We examined severalproposals, like EuroWordnet (Vossen, 1999),DOLCE (Masolo et al, 2002), FrameNet(Ruppenhofer Fillmore & Baker, 2002) and others.As also remarked in (Hovy, 2001), nosystematic methods are available in literature tocompare the different sets of relations.
Since ourobjective was to define an automatic method forsemantic relation extraction, our final choice wasto use a reduced set of FrameNet relations, whichseemed general enough to cover our applicationdomains (tourism, computer networks, economy).The choice of FrameNet is motivated by theavailability of a sufficiently large set of annotatedexamples of conceptual relations7, that we used totrain an available machine learning algorithm,TiMBL (Daelemans et al, 2002).
The relationsused are: Material, Purpose, Use, Topic, Product,Constituent Parts, Attribute8.
Examples for eachrelation are the following:net # 1 attribute?
?
?
?
?
?
loss# 3takeover# 2 topic?
?
?
?
proposal# 1sand# 1 material?
?
?
?
?
?
beach# 1merger# 1 purpose?
?
?
?
?
?
agreement# 15http://www.cs.waikato.ac.nz/ml/weka/6http://www.cs.unt.edu/~rada/downloads.html#semcor7The choice of FrameNet was motivated more by availability thanappropriateness.8The relation Attribute is not in FrameNet, however it was auseful relation for terminological strings of the adjective_noun type.meeting# 1 use?
?
?
?
ro om# 1bond# 2 const _ part?
?
?
?
?
?
?
market# 1c om puter# 1 product?
?
?
?
?
com pany# 1We represented training instances as pairs ofconcepts annotated with the appropriate conceptualrelation, e.g.
:[(computer#1,maker#3),Product]Each concept is in turn represented by a feature-vec to r  where attributes are the concept?shyperonyms in WordNet, e.g.
:(computer#1,maker#3):((computer#1,machine#1,device#1,instrumentality#3),(maker#3,business#1,enterprise#2,organization#1))3 Quantitative Evaluation of OntoLearnThis section provides a quantitative evaluation ofOntoLearn s main algorithms.
We believe that aquantitative evaluation is particularly important incomplex learning systems, where errors can beproduced at almost any stage.
Even though someof these errors (e.g.
subtle sense distinctions) maynot have a percievable effect on the final ontology,as shown by the results of the qualitativeevaluation in Section 4.2, it is neverthelessimportant to gain insight on the actual systemcapabilities, as well as on the pararmeters andexternal circumstances that may positively ornegatively influence the final performance.3.1 Evaluating the MWE extractionalgorithmThe terminology extraction algorithm has beenevaluated in the context of the European projectHarmonise on Tourism interoperability.
We firstcollected a corpus of about 1 million words oftourism documents, mainly descriptions of traveland tourism sites.
From this corpus, a syntacticparser extracted an initial list of 14,383 candidatecomplex MWEs from which the statistical filtersselected a list of 3,840 domain-relevant complexMWEs, that were submitted to the domainspecialists.
The Harmonise ontology partners werenot skilled to evaluate the OntoLearn semanticinterpretation of MWEs, therefore we let themevaluate only the domain appropriateness of theterms.
The gloss generation method described inSection 4 was subsequently concieved to overcomethis limitation.We obtained a precision ranging from 72.9% toabout 80% and a recall of 52.74%.
The precisionshift is due to the well-known fact that experts mayhave different intuitions about the relevance of aconcept.
The recall estimate was produced bymanually inspecting 6,000 of the initial 14,383candidate MWEs, asking the experts to mark allthe MWEs judged as ?good?
domain MWEs, andcomparing the obtained list with the list of termsautomatically filtered by OntoLearn.We ran similar experiments on an Economycorpus and a Computer Network corpus, but in thiscase the evaluation was performed by the authors.Overall, the performance of the MWE extractiontask appears to be influenced by the dimension andthe focus of the starting corpus (e.g.
?generictourism?
vs. ?hotel accomodation descriptions?
).Small and unfocused corpora do not favor theefficacy of statistical analysis.
However, theavailability of sufficiently large and focusedcorpora seems a realistic requirement for mostapplications.3.2 Evaluating the ontology learningalgorithmsThe distinctive task performed by OntoLearn issemantic disambiguation.
The performance of theSSI algorithm critically depends upon two factors:the first is the ability to detect semanticinterrelations among concepts associated to thewords of complex MWEs, the second is thedimension of the context T available to start thedisambiguation process.As for the first factor, there are two possibleways of enhancing reliable identification ofsemantic interconnections: one is to tune at best theweight of individual rules in G (e.g.
formula (1) inSection 2), the second is to enrich the semanticinformation associated to alternative word senses.The latter is an on-going research activity.As far as the context T is concerned, theintuition is that, with a larger T , there are higherchances of detecting semantic patterns among the?correct?
senses of the terms in T. However, thedimension of contexts Ti is an externalcontingency, it depends upon the available corpus.Accordingly, we evaluated the SSI algorithmusing as parameters the dimension of T, T , andthe weights associated to rules in G. We ran severalexperiments over the full terminology extractedfrom the Economy and Tourism corpora, butperformances are computed only on, respectively,453 and 638 manually disambiguated terms.
Thismeans that in a context Ti including, e.g.
k terms,we evaluate OntoLearn?s sense choices only forthe fragment of j ?
k terms, for which the ?true?sense has been manually assigned.Table 1 shows the performance of SSI (precisionand recall) when using only patterns whose weight,computed with formula (1) is over a threshold ?
.The ?Core?
column in Table 1 shows theperformance of SSI when accepting only these corepatterns, while the third column refers to allmatching patterns.
With ?
= 0,7  a subset of 7-9rules9 in G (over a total of 20) are used by thealgorithm.
Interestingly enough, these rules have ahigh probability of being hired, as shown by therelatively low difference in recall.
The Baselinetower in Table 1 is computed selecting always thefirst sense (senses in WordNet are ordered byprobability in everyday language).Table 2 shows that performance of SSI is indeedaffected by the dimension of T. Large T , asexpected, improves the performance, however,overly large contexts (>80 terms) may favor thedetection of non-relevant patterns.In general, both experiments show that theEconomy corpus performs better than the Tourism,since the latter is less technical (the baseline isquite high), rather unfocused, and contexts Ti aremuch less populated.Table 1.
Performances as a function of pattern?sweightTable 2.
Performances as a function of |T|We remark that SSI performs better thanstandard WSD (word sense disambiguation) tasksbut this is also motivated by the fact that contextwords in T are more interrelated than co-occurringwords in generic sentences.
The SSI algorithm, by9in formula (1), ?
, that depends upon the rule, has a muchhigher influence than ?
, that depends upon the matching pattern)86.40%57.17% 53.76%59.72%80.21%76.48%81.77%85.48%71.30% 67.33%0%20%40%60%80%100%Baseline Core All Rules Baseline Core All RulesPrec.Recall Finance Tourism78.57% 81.82% 82.84% 80.29% 80%58.51% 63.28%73.16%55.46%73.20%0%20%40%60%80%100%|T| < 35 35 <= |T| < 65 |T| >= 65 |T| < 35 |T| >= 35Prec.
RecallFinance Tourismits very nature, is favored by focused and largecontexts.
In any case, it is worth mentioning thatSSI received the second best score in the latestSenSeval-310, gloss disambiguation exercise,placed about 1% below the first and about 11%before the third participant.3.3 Evaluating the semantic annotationalgorithmTo test the semantic relation annotation task, weused a learning set (including selected annotatedexamples from FrameNet (FN), Tourism (Tour),and Economy (Econ)), and a test set with adistribution of examples shown in Table 3.Table 3.
Distribution of examples in the learningand test set for the semantic annotation taskLearning Set Test SetSem_Rel FN Tour Econ Tot FN Tour Econ TotMATERIAL 8 3 0 11 5 2 0 7USE 9 32 2 43 6 20 1 27TOPIC 52 79 100 231 29 43 50 122C_PART 3 7 12 22 2 4 6 12PURPOSE 26 64 22 112 14 34 11 59PRODUCT 3 1 6 10 1 1 4 6Total 101 186 142 429 57 104 72 233Notice that the relation Attribute is generatedwhenever the term associated to one of theconcepts is an adjective.
Therefore, this semanticrelation is not included in the evaluationexperiment, since it would artificially increaseperformances.
We then tested the learner on testsets for individual domains11, leading to theresults shown in Table 4 a and b.Table 4 Performance of the semantic annotationtask on a) Tourism b) Economyd<=10% d<=30% d<=100%Precision MACRO 0,958 0,875 0,847Recall MACRO 0,283 0,636 0,793F1 MACRO 0,437 0,737 0,819Precision micro 0,900 0,857 0,798Recall micro 0,087 0,635 0,798F1 micro 0,158 0,721 0,798d<=10% d<=30% d<=100%Precision MACRO 1,000 0,804 0,651Recall MACRO 0,015 0,403 0,455F1 MACRO 0,030 0,537 0,536Precision micro 1,000 0,758 0,750Recall micro 0,042 0,653 0,750F1 micro 0,080 0,701 0,750The performance measures are those adopted inTREC competitions12.
The parameter d  in theabove Tables is a confidence factor defined in theTiMBL algorithm.
This parameter can be used to10SensEval?3   http://www.senseval.org/senseval311This of course penalised the results (the performance over a testset composed by examples of all the three domains is much higher),but provides a more realistic test bed of the generality of the approach.12http://trec.nist.gov/increase system?s robustness in the following way:whenever the confidence associated by TiMBL tothe classification of a new instance is lower than agiven threshold, we output a ?generic?
conceptualrelation, named Relatedness.
We experimentallyfixed the threshold for d  around 30% (centralcolumn of Table 4).Table 4 demonstrates rather good performances,however the main problem with semantic relationannotation is the unavailability of an agreed set ofconceptual relations, and a sufficiently large andbalanced training set.
Consequently, we need toupdate the set of used relations whenever weanalyse a new domain, and re-run the trainingphase enriching the training corpus with manuallytagged examples from the new domain (as for inTable 2).4  Qualitative evaluation: Evaluating thegenerated ontology on a per-concept basisThe lesson learned during the Harmonise ECproject was that the domain specialists, tourismoperators in our case, can hardly evaluate theformal aspects of a computational ontology.
Whenpresented with the domain extended and trimmedversion of WordNet (OntoLearn?s phase 3 inSection 2), they were only able to express a genericjudgment on each node of the hierarchy, based onthe concept label.
These judgments were used toevaluate the terminology extraction task, but theexperiment suggested that, indeed, it was necessaryto provide a better description for the learnedconcepts.4.1 Gloss generation grammarTo help human evaluation on a per-conceptbasis, we decided to enhance OntoLearn with agloss generation algorithm.
The idea is to generateglosses in a way that closely reflects the keyaspects of the concept learning process, i.e.semantic disambiguation and annotation with aconceptual relation.The gloss generation algorithm is based on thedefinition of a grammar with distinct generationrules for each type of semantic relation.Let Sihsem _ rel?
?
?
?
?
?
Sjkbe the complex conceptassociated to a complex term whw k (e.g.
jazzfestival, or long-term debt), and let:<H>= the syntactic head of whwk (e.g.
festival,debt)<M> = the syntactic modifier of whwk (e.g.
jazz,long-term)<GNC>= be the gloss of the new complex conceptShk<HYP>= the selected sense of <H>(e.g.respectively, festival#1 and debt#1).<MSGHYP>= the main sentence13 of theWordNet gloss of <HYP><MSGM>= the main sentence of the WordNetgloss of the selected sense for <M>Here we provide two examples of rules forgenerating GNCs:If sem_rel=Topic, <GNC>:: = a kind of <HYP>,<MSGHYP>, relating to the <M>, <MSGM>.e,g.
: GNC(jazz festival): a kind of festival,  aday or period of time set aside for feasting andcelebration,  relating to the jazz, a style of dancemusic popular in the 1920.If sem_rel=Attribute, <GNC>:= a kind of <HYP>,<MSGHYP>, <MSGM>.e.g.
:GNC(long term debt)= a kind of debt, thestate of owing something (especially money),relating to or extending over a relatively long time.4.2 Per-concept evaluation experimentTo verify the utility of gloss generation, theautomatically generated glosses were submitted forevaluation to two human experts, a tourismspecialist from ECCA14, and an economist fromthe University of Ancona.
The specialists were notaware of the method used to generate glosses; theyhave been presented with a list of concept-glosspairs and asked to fill in an evaluation form (seeAppendix) as follows: vote 1 means?unsatisfactory definition?, vote 2 means ?thedefinition is helpful?, vote 3 means ?the definitionis fully acceptable?.
Whenever he was not fullyhappy with a definition (vote 2 or 1), the specialistwas asked to provide a brief explanation.
Forcomparison, Appendix 2 shows also glossarydefinitions extracted from the web for the sameMWEs, that were not shown to the specialists.Table 5 provides a summary of theevaluation..Table 5.
Evaluation of glosses by domainspecialists.vote =1 vote=2 vote=3 uncertainaverageTourismtotal(97)33(34.7)14(14.4)45(46.3)5 (5.1) 2,13Ecomomy total(134)52(38.8)16(11.9)66(49.2)- 2.10The following conclusions can be drawn fromthis experiment:1 .
Overall, the two domain specialists fullyaccepted the system?s choices in 45-49% of thecases, and were reasonably satisfied in 12-14%13The main sentence is the gloss pruned of subordinates,examples, etc.14ECCA ?
eTourism Competence Center Austria.of the cases.
The average vote is above 2 inboth cases.2.
As expected, if a MWE is compositional, thegenerated definition is more often accepted orfully accepted (e.g.
examples 25_E and 14_Tin Appendix 2).
When a compositionalinterpretation is not accepted (vote=1), this ismotivated either by an OntoLearninterpretation error (wrong sense or wrongconceptual relations) or by the unavailabilityof a correct sense in WordNet, despite the factthat the sense is not idiosyncratic.
OntoLearnerrors for compositional MWEs are 7 (5%) inEconomy and 12 (13%) in Tourism.
Examplesof OntoLearn errors and core ontology?misses?
are the definitions 14_T (wrong senseof form) and 19_E (no good sense for bilateralin WordNet), respectively.3.
Sometimes the specialists found it acceptablealso an idiosyncratic or non compositionaldefinition.
This happens in 16 cases for theTourism domain (16%) and in 19 cases for theEconomy domain (13%).
Examples are theMWEs 45_E and 76_E, both idiosyncraticallydecomposable, in Appendix 2.One of the specialists is particularly involved inontology building projects, therefore we report hisvaluable comment: ?some of the descriptionswould not be appropriate to take them over in atourism ontology just as they are.
But most of themare quite helpful as basis for building the ontology.The most important problem from my point of viewis the too detailed descriptions of the componentsitself instead of the meaning of the overall term inthis context.
Best example is the term ?bed tax?.Nobody would expect a definition of a bed or atax.?
In other terms, he found disturbing the factthat a definition extensively reports the definitionsof its components.
On the other side, our objectiveis not only to produce concept definitions, but alsoto organize concepts in hierarchies.
Showing thedefinitions of individual components is a ?natural?mean to verify that the correct senses have beenselected (e.g.
the correct senses of bed and tax).This is clearly the case, since, for example indefinition 14_T (booking form), the specialist wasimmediately able to diagnose a sensedisambiguation error for form , though he wasunaware of the OntoLearn methodology.5 Concluding remarksThis paper presented an in-depth evaluation ofthe Ontolearn ontology learning system.
The threebasic algorithms (terminology extraction, sensedisambiguation and annotation with semanticrelation) have been individually evaluated in twodomains, under different parametrizations, toobtain a realistic and comprehensible picture ofsystem?s capabilities.
The critical algorithm, SSI,has very good performances that are favored by thefact that word sense disambiguation is applied togroup of words (domain MWEs) that are stronglysemantically related, unlike for generic WSD tasks(e.g.
Senseval).
The performance of the SSIalgorithm can be further improved through anextension of the grammar G, which is an on-goingresearch activity.6 AcknowledgementsOur thanks go to Dr. Wolfram H?pken, fromECCA ?
eTourism Competence Center Austria(wolfram@hoepken.org ) and Dr. RenatoIacobucci, from the University of Ancona, whogave up their precious time to evaluate our glosses.This work has been in part supported by theINTEROP Network of Excellence IST-2003-508011ReferencesJ.
Angele and Y.
Sure (2002) ?Whitepaper:Evaluation of Ontology-based Tools?, Workshopon evaluation of ontology-based tools(EON2002), at the 13th Int.
EKAW 2002,Sigueza (Spain), September 2002.H.
Bunke and A. Sanfeliu (editors) (1990).Syntactic and Structural pattern Recognition:Theory and Applications, World Scientific,Series in Computer Science vol.
7, 1990.Daelemans,W.
Zavrel, J.
Van den Sloot, K. & Vanden Bosch, A.
(2002).
TiMBL: Tilburg MemoryBased Learner.
Version 4.3 Reference Guide.Tilburg University.G?mez-P?rez, A., Fern?ndez-Lopez M. andCorcho O.
(2004).
Ontological Engineering,Springer Verlag, London, 2004.Hovy, E. (2001).
Comparing Sets of Semanticrelations in Ontologies.
In R. Geen, C.A.
Beanand S. Myaeng Semantic of relations.
Kluwer.Masolo, C., Borgo, S., Gangemi, A., Guarino, N.Oltramari, A.
& Schneider, L. (2002).Sweetening Ontologies with DOLCE.Proceedings of the 13th International Conferenceon Knowledge Engineering and KnowledgeManagement.
Ontologies and the Semantic Web.Navigli, R. & Velardi, P. (2004).
Learning DomainOntologies from Document Warehouses andDedicated Web Sites.
Computational Linguistics,MIT press, (50)2.Navigli, R., Velardi, P. Gangemi, A.
(2003).Corpus Driven Ontology Learning: a Methodand its Application to Automated TerminologyTranslation.
IEEE Intelligent Systems (18)1.22-31.Ruppenhofer, J., Fillmore, C.J.
& Baker, C.F.(2002).
Collocational Information in theFrameNet Database.
In Braasch, A. and Povlsen,C.
(eds.
), Proceedings of the Tenth EuralexInternational Congress.
Copenhagen, Denmark.Vol.
I: 359--369, 2002.Vossen, P. (1999).
EuroWordNet: GeneralD o c u m e n t .
V e r s i o n  3  F i n a l .http://www.hum.uva.nl/~ewnAPPENDIX: Excerpt of the per-concept evaluation formConcept #: 25_E Term: business_plan Synt: N-N Rel<w1,w2>: TopicGloss: a kind of plan, a series of steps to be carried out or goals to be accomplished, relating to the business, the activityof providing goods and services involving financial and commercial and industrial aspects.Specialist vote: 3Comment by Specialist: noneDiagnose: noneGlossary definition: a written report that states what a company (or a part of a company) aims to do increase sales,develop new products, etc.
within a certain period, and how it will obtain the necessary finances and resources.Concept #: 2_T Term: affiliated_hotel Synt: Agg-N Rel<w1,w2>: AttributeGloss: a kind of hotel, a building where travellers can pay for lodging and meals and other services, being joined in closeassociation.Specialist vote: 3Comment by Specialist: noneDiagnose: noneGlossary definition: a hotel that is a member of a chain, franchise, or referral system.
Membership provides specialadvantages, particularly a national reservation system.Concept #: 14_T Term: booking_form Synt: N-N Rel<w1,w2>: PurposeGloss: a kind of form, alternative names for the body of a human being, for booking, the act of reserving (a place orpassage) or engaging the services of (a person or group).Specialist vote: 1Comment by Specialist: definition of form  wrong in this contextDiagnose: OntoLearn disambiguation error for formGlossary definition: a document which purchasers of tours must complete to give the operator full particulars about whois buying the tour.Concept #: 19_E Term: bilateral_aid Synt: Agg-N Rel<w1,w2>: AttributeGloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, havingidentical parts on each side of an axis.Specialist vote: 1Comment by Specialist: Fully wrong definition.Diagnose: WordNet gloss of bilateral  is not adequate to domain (no better definition is available in WordNet).Glossary definition:  assistance given by one country to another.Concept #: 45_E Term: cyclical_uneployment Synt: Agg-N Rel<w1,w2>: AttributeGloss: a kind of unemployment, the state of being unemployed or not having a job, recurring in cycles.Specialist vote: 3Comment by Specialist: noneDiagnose: noneGlossary definition: workers are without a job because of a lack of aggregate demand due to a down turn in economicactivity.Concept #: 76_E Term: foreign_aid Synt: Agg-N Rel<w1,w2>: AttributeGloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, ofconcern to or concerning the affairs of other nations .Specialist vote: 3Comment by Specialist: noneDiagnose: noneGlossary defonition: the international transfer of public and private funds in the form of loans or grants from donorcountries to recipient countries.
