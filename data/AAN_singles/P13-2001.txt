Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1?6,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTranslating Dialectal Arabic to EnglishHassan Sajjad, Kareem DarwishQatar Computing Research InstituteQatar Foundation{hsajjad,kdarwish}@qf.org.qaYonatan BelinkovCSAILMassachusetts Institute of Technologybelinkov@mit.eduAbstractWe present a dialectal Egyptian Arabicto English statistical machine translationsystem that leverages dialectal to ModernStandard Arabic (MSA) adaptation.
Incontrast to previous work, we first nar-row down the gap between Egyptian andMSA by applying an automatic character-level transformational model that changesEgyptian to EG?, which looks simi-lar to MSA.
The transformations includemorphological, phonological and spellingchanges.
The transformation reducesthe out-of-vocabulary (OOV) words from5.2% to 2.6% and gives a gain of 1.87BLEU points.
Further, adapting largeMSA/English parallel data increases thelexical coverage, reduces OOVs to 0.7%and leads to an absolute BLEU improve-ment of 2.73 points.1 IntroductionModern Standard Arabic (MSA) is the linguafranca for the Arab world.
Arabic speakers gen-erally use dialects in daily interactions.
There are6 dominant dialects, namely Egyptian, Moroccan,Levantine, Iraqi, Gulf, and Yemeni1.
The dialectsmay differ in vocabulary, morphology, syntax, andspelling from MSA and most lack spelling con-ventions.Different dialects often make different lexicalchoices to express concepts.
For example, the con-cept corresponding to ?Oryd?
YKP@ (?I want?)
isexpressed as ?EAwz?
P?A?
in Egyptian, ?Abgy???K.
@ in Gulf, ?Aby?
?G.
@ in Iraqi, and ?bdy?
?YK.in Levantine2.
Often, words have different or op-posite meanings in different dialects.1http://en.wikipedia.org/wiki/Varieties_of_Arabic2All transliterations follow the Buckwalter schemeArabic dialects may differ morphologicallyfrom MSA.
For example, Egyptian Arabic uses anegation construct similar to the French ?ne pas?negation construct.
The Egyptian word ?mlEbt$??J.???
(or alternatively spelled ?J.??A?)
(?I didnot play?)
is composed of ?m+lEbt+$?.The pronunciations of letters often differ fromone dialect to another.
For example, the letter ?q??
is typically pronounced in MSA as an unvoiceduvular stop (as the ?q?
in ?quote?
), but as a glot-tal stop in Egyptian and Levantine (like ?A?
in?Alpine?)
and a voiced velar stop in the Gulf (like?g?
in ?gavel?).
Differing pronunciations often re-flect on spelling.Social media platforms allowed people to ex-press themselves more freely in writing.
AlthoughMSA is used in formal writing, dialects are in-creasingly being used on social media sites.
Somenotable trends on social platforms include (Dar-wish et al, 2012):- Mixed language texts where bilingual (or mul-tilingual) users code switch between Arabic andEnglish (or Arabic and French).
In the exam-ple ?wSlny mrsy?
???Q?
?????
(?got it thankyou?
), ?thank you?
is the transliterated Frenchword ?merci?.?
The use of phonetic transcription to match di-alectal pronunciation.
For example, ?Sdq?
?Y?(?truth?)
is often written as ?Sj?
l .?
in Gulf di-alect.?
Creative spellings, spelling mistakes, and wordelongations are ubiquitous in social texts.?
The use of new words like ?lol?
???
(?LOL?).?
The attachment of new meanings to words suchas using ?THn?
?j?
to mean ?very?
while itmeans ?grinding?
in MSA.The Egyptian dialect has the largest number ofspeakers and is the most commonly understood di-alect in the Arab world.
In this work, we focusedon translating dialectal Egyptian to English us-1ing Egyptian to MSA adaptation.
Unlike previouswork, we first narrowed the gap between Egyptianand MSA using character-level transformationsand word n-gram models that handle spelling mis-takes, phonological variations, and morphologicaltransformations.
Later, we applied an adaptationmethod to incorporate MSA/English parallel data.The contributions of this paper are as follows:?
We trained an Egyptian/MSA transformationmodel to make Egyptian look similar to MSA.
Wepublicly released the training data.?
We built a phrasal Machine Translation (MT)system on adapted Egyptian/English parallel data,which outperformed a non-adapted baseline by1.87 BLEU points.?
We used phrase-table merging (Nakov and Ng,2009) to utilize MSA/English parallel data withthe available in-domain parallel data.2 Previous WorkOur work is related to research on MT from a re-source poor language (to other languages) by piv-oting on a closely related resource rich language.This can be done by either translating betweenthe related languages using word-level translation,character level transformations, and language spe-cific rules (Durrani et al, 2010; Hajic?
et al, 2000;Nakov and Tiedemann, 2012), or by concatenatingthe parallel data for both languages (Nakov andNg, 2009).
These translation methods generallyrequire parallel data, for which hardly any existsbetween dialects and MSA.
Instead of translatingbetween a dialect and MSA, we tried to narrowdown the lexical, morphological and phonetic gapbetween them using a character-level conversionmodel, which we trained on a small set of paralleldialect/MSA word pairs.In the context of Arabic dialects3, most previouswork focused on converting dialects to MSA andvice versa to improve the processing of dialects(Sawaf, 2010; Chiang et al, 2006; Mohamed etal., 2012; Utiyama and Isahara, 2008).
Sawaf(2010) proposed a dialect to MSA normalizationthat used character-level rules and morphologicalanalysis.
Salloum and Habash (2011) also used arule-based method to generate MSA paraphrasesof dialectal out-of-vocabulary (OOV) and low fre-quency words.
Instead of rules, we automatically3Due to space limitations, we restrict discussion to workon dialects only.learnt character mappings from dialect/MSA wordpairs.Zbib et al (2012) explored several methods fordialect/English MT.
Their best Egyptian/Englishsystem was trained on dialect/English paralleldata.
They used two language models built fromthe English GigaWord corpus and from a largeweb crawl.
Their best system outperformed man-ually translating Egyptian to MSA then translat-ing using an MSA/English system.
In contrast, weshowed that training on in-domain dialectal datairrespective of its small size is better than trainingon large MSA/English data.
Our LM experimentsalso affirmed the importance of in-domain EnglishLMs.
We also showed that a conversion does notimply a straight forward usage of MSA resourcesand there is a need for adaptation which we ful-filled using phrase-table merging (Nakov and Ng,2009).2.1 BaselineWe constructed baselines that were based on thefollowing training data:- An Egyptian/English parallel corpus consist-ing of ?38k sentences, which is part of theLDC2012T09 corpus (Zbib et al, 2012).
We ran-domly divided it into 32k sentences for training,2k for development and 4k for testing.
We hence-forth refer to this corpus as EG and the Englishpart of it as EGen.
We did not have access to thetraining/test splits of Zbib et al (2012) to directlycompare to their results.- An MSA/English parallel corpus consisting of200k sentences from LDC4.
We refer to this cor-pus as the AR corpus.For language modeling, we used either EGenor the English side of the AR corpus plus the En-glish side of NIST12 training data and English Gi-gaWord v5.
We refer to this corpus as GW.We tokenized Egyptian and Arabic accord-ing to the ATB tokenization scheme using theMADA+TOKAN morphological analyzer and to-kenizer v3.1 (Roth et al, 2008).
Word elonga-tions were already fixed in the corpus.
We word-aligned the parallel data using GIZA++ (Och andNey, 2003), and symmetrized the alignments usinggrow-diag-final-and heuristic (Koehn et al, 2003).We trained a phrasal MT system (Koehn et al,2003).
We built five-gram LMs using KenLM4Arabic News (LDC2004T17), eTIRR (LDC2004E72),and parallel corpora the GALE program2Train LM BLEU OOVB1 AR GW 7.48 6.7B2 EG GW 12.82 5.2B3 EG EGen 13.94 5.2B4 EG EGenGW 14.23 5.2Table 1: Baseline results using the EG and ARtraining sets with GW and EGen corpora for LMtrainingwith modified Kneser-Ney smoothing (Heafield,2011).
In case of more than one LM, we tunedtheir weights on a development set using Mini-mum Error Rate Training (Och and Ney, 2003).We built several baseline systems as follows:?
B1 used AR for training a translation model andGW for LM.?
B2-B4 systems used identical training data,namely EG, with the GW, EGen, or both for B2,B3, and B4 respectively for language modeling.Table 1 reports the baseline results.
The systemtrained on AR (B1) performed poorly comparedto the one trained on EG (B2) with a 6.75 BLEUpoints difference.
This highlights the differencebetween MSA and Egyptian.
Using EG data fortraining both the translation and language modelswas effective.
B4 used two LMs and yielded thebest results.
For later comparison, we only use theB4 baseline.3 Proposed Methods3.1 Egyptian to EG?
ConversionAs mentioned previously, dialects differ fromMSA in vocabulary, morphology, and phonology.Dialectal spelling often follows dialectal pronun-ciation, and dialects lack standard spelling con-ventions.
To address the vocabulary problem, weused the EG corpus for training.To address the spelling and morphological dif-ferences, we trained a character-level mappingmodel to generate MSA words from dialectalones using character transformations.
To train themodel, we extracted the most frequent words froma dialectal Egyptian corpus, which had 12,527news comments (containing 327k words) from Al-Youm Al-Sabe news site (Zaidan and Callison-Burch, 2011) and translated them to their equiv-alent MSA words.
We hired a professional trans-lator, who generated one or more translations ofthe most frequent 5,581 words into MSA.
Out ofthese word pairs, 4,162 involved character-leveltransformations due to phonological, morphologi-cal, or spelling changes.
We aligned the translatedpairs at character level using GIZA++ and Mosesin the manner described in Section 2.1.
As in thebaseline of Kahki et al (2011), given a sourceword, we produced all of its possible segmenta-tions along with their associated character-levelmappings.
We restricted individual source char-acter sequences to be 3 characters at most.
Weretained all mapping sequences leading to validwords in a large lexicon.
We built the lexicon froma set of 234,638 Aljazeera articles5 that span a 10year period and contain 254M tokens.
Spellingmistakes in Aljazeera articles were very infre-quent.
We sorted the candidates by the product ofthe constituent mapping probabilities and kept thetop 10 candidates.
Then we used a trigram LM thatwe built from the aforementioned Aljazeera arti-cles to pick the most likely candidate in context.We simply multiplied the character-level transfor-mation probability with the LM probability ?
giv-ing them equal weight.
Since Egyptian has a ?nepas?
like negation construct that involves putting a???
and ?
??
at the beginning and end of verbs,we handled words that had negation by remov-ing these two letters, then applying our charactertransformation, and lastly adding the negation ar-ticle ?lA?
B before the verb.
We converted theEGtrain, tune, and test parts.
We refer to the convertedcorpus as EG?.As an example, our system transformedYg ?
.
j.
?J?
????jJK.
???
@ ?
.
(?what is hap-pening to them does not please anyone?)
toYg I. j.
?KB ???
??m'?Y?
@ ?
.
.
Transform-ing ?Ally?
???
@ to ?Al*y?
?Y?
@ involved a spellingcorrection.
The transformation of ?byHSlhm?????jJK.
to ?yHSl lhm?
???
?
?m'involved a mor-phological change and word splitting.
Chang-ing ?myEjb$?
?
.
j.
?J?
to ?lA yEjb?
I. j.
?KB in-volved morphologically transforming a negationconstruct.3.2 Combining AR and EG?The aforementioned conversion generated a lan-guage that is close, but not identical, to MSA.In order to maximize the gain using both paral-lel corpora, we used the phrase merging techniquedescribed in Nakov and Ng (2009) to merge thephrase tables generated from theAR andEG?
cor-pora.
If a phrase occurred in both phrase tables, we5http://www.aljazeera.net3adopted one of the following three solutions:- Only added the phrase with its translations andtheir probabilities from the AR phrase table.
Thisassumed AR alignments to be more reliable.- Only added the phrase with its translations andtheir probabilities from the EG?
phrase table.
Thisassumed EG?
alignments to be more reliable.- Added translations of the phrase from bothphrase tables and left the choice to the decoder.We added three additional features to the newphrase table to avail the information about the ori-gin of phrases (as in Nakov and Ng (2009)).3.3 Evaluation and DiscussionWe performed the following experiments:- S0 involved translating the EG?
test using AR.- S1 and S2 trained on the EG?
with EGen andboth EGen and GW for LM training respectively.- S?
used phrase merging technique.
All systemstrained on both EG?
and AR corpora.
We builtseparate phrase tables from the two corpora andmerged them.
When merging, we preferred AR orEG?
for SAR and SEG?
respectively.
For SALL,we kept phrases from both phrase tables.Table 2 summarizes results of using EG?
andphrase table merging.
S0 was slightly better thanB1, but lagged considerably behind training usingEG or EG?.
S1, which used only EG?
for train-ing showed an improvement of 1.67 BLEU pointsfrom the best baseline system (B4).
Using bothlanguage models (S2) led to slight improvement.Phrase merging that preferred phrases learnt fromEG?
data over AR data performed the best with aBLEU score of 16.96.Train LM BLEU OOVB4 EG EGenGW 14.23 5.2S0 AR EGen 8.61 2.0S1 EG?
EGen 15.90 2.6S2 EG?
EGenGW 16.10 2.6SAR PTAR EGenGW 16.14 0.7SEG?
PTEG?
EGenGW 16.96 0.7SALL PTEG?,AR EGenGW 16.73 0.7Table 2: Summary of results using different com-binations of EG?/English and MSA/English train-ing dataWe analyzed 100 test sentences that led to thegreatest absolute change in BLEU score, whetherpositive or negative, between training with EGand EG?.
The largest difference in BLEU was0.69 in favor of EG?.
Translating the Egyp-tian sentence ?wbyHtrmwA AlnAs AltAnyp??JK AJ?
@ ?AJ?
@ @??QjJK.
?
produced ?
@?
?QjJK.
?
(OOV)the second people?
(BLEU = 0.31).
Conver-sion changed ?wbyHtrmwA?
to ?wyHtrmwA?
and?AltAnyp?
?JK AJ?
@ to ?AlvAnyp?
?JK AJ?
@, leading to?and they respect other people?
(BLEU = 1).Training with EG?
outperformed EG for 63 of thesentences.
Conversion improved MT, because itreduced OOVs, enabled MADA+TOKAN to suc-cessfully analyze words, and reduced spelling mis-takes.In further analysis, we examined 1% of the sen-tences with the largest difference in BLEU score.Out of these, more than 70% were cases where theEG?
model achieved a higher BLEU score.
Foreach observed conversion error, we identified itslinguistic character, i.e.
whether it is lexical, syn-tactic, morphological or other.
We found that inmore than half of the cases (?57%) using morpho-logical information could have improved the con-version.
Consider the following example, where(1) is the original EG sentence and its EG/ENtranslation, and (2) is the converted EG?
sentenceand its EG?/EN translation:1.
?JJ.
?P I.
?k ?X?BlAn dy Hsb rgbtkbecause this is according to your desire2.
?JJ.
?P I.
?k ?
Y?
?BlOn h*h Hsb rgbthbecause this is according to his desireIn this case, ?rgbtk?
?JJ.
?P (?your wish?)
was con-verted to ?rgbth?
?JJ.
?P (?his wish?)
leading to anunwanted change in the translation.
This could beavoided, for instance, by running a morphologi-cal analyzer on the original and converted word,and making sure their morphological features (inthis case, the person of the possessive) correspond.In a similar case, the phrase ?mEndy$ AEdA?Z @Y?
@ ?YJ??
was converted to ?Endy OEdA?
?Z @Y?
@ ?YJ?, thereby changing the translation from?I don?t have enemies?
to ?I have enemies?.
Here,again, a morphological analyzer could verify theretaining of negation after conversion.In another sentence, ?knty?
??J?
(?you (fm.)were?)
was correctly converted to the MSA ?knt?IJ?, which is used for feminine and masculineforms.
However, the induced ambiguity ended uphurting translation.4Aside from morphological mistakes, conversionoften changed words completely.
In one sen-tence, the word ?lbAnh?
?
KAJ.
?
(?chewing gum?
)was wrongly converted to ?lOnh?
?
KB (?becauseit?
), resulting in a wrong translation.
Perhaps amorphological analyzer, or just a part-of-speechtagger, could enforce (or probabilistically encour-age) a match in parts of speech.The conversion also faces some other chal-lenges.
Consider the following example:1.
?JK@ A 	J???
A 	Jk@ @?
?hwA AHnA EmlnA Ayyyhhe is we did we What ?
?2.
?K@ A 	J???
?m 	' ?
?hw nHn EmlnA Ayhhe we did we do ?
?While the first two words ?hwA AHnA?
A 	Jk@ @?
?were correctly converted to ?hw nHn?
?m 	' ?
?, thefinal word ?Ayyyh?
?JK@ (?what?)
was shortenedbut remained dialectal ?Ayh?
?K@ rather than MSA?mA/mA*A?
A?/ @ 	XA?.
There is a syntactic chal-lenge in this sentence, since the Egyptian word or-der in interrogative sentences is normally differentfrom the MSA word order: the interrogative par-ticle appears at the end of the sentence instead ofat the beginning.
Addressing this problem mighthave improved translation.The above analysis suggests that incorporat-ing deeper linguistic information in the conversionprocedure could improve translation quality.
Inparticular, using a morphological analyzer seeemslike a promising possibility.
One approach couldbe to run a morphological analyzer for dialectalArabic (e.g.
MADA-ARZ (Habash et al, 2013))on the original EG sentence and another analyzerfor MSA (such as MADA) on the converted EG?sentence, and then to compare the morphologicalfeatures.
Discrepancies should be probabilisticallyincorporated in the conversion.
Exploring this ap-proach is left for future work.4 ConclusionWe presented an Egyptian to English MT system.In contrast to previous work, we used an auto-matic conversion method to map Egyptian closeto MSA.
The converted Egyptian EG?
had fewerOOV words and spelling mistakes and improvedlanguage handling.
The MT system built on theadapted parallel data showed an improvement of1.87 BLEU points over our best baseline.
Usingphrase table merging that combined AR and EG?training data in a way that preferred adapted di-alectal data yielded an extra 0.86 BLEU points.We will make the training data for our conversionsystem publicly available.For future work, we want to expand our workto other dialects, while utilizing dialectal morpho-logical analysis to improve conversion.
Also, webelieve that improving English language model-ing to match the genre of the translated sentencescan have significant positive impact on translationquality.ReferencesDavid Chiang, Mona T. Diab, Nizar Habash, OwenRambow, and Safiullah Shareef.
2006.
Parsing Ara-bic dialects.
In Proceedings of the 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics, Trento, Italy.Kareem Darwish, Walid Magdy, and Ahmed Mourad.2012.
Language processing for Arabic microblogretrieval.
In Proceedings of the 21st ACM inter-national conference on Information and knowledgemanagement, CIKM ?12, Maui, Hawaii, USA.Nadir Durrani, Hassan Sajjad, Alexander Fraser, andHelmut Schmid.
2010.
Hindi-to-Urdu machinetranslation through transliteration.
In Proceedingsof the 48th Annual Conference of the Association forComputational Linguistics, Uppsala, Sweden.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, , and Nadi Tomeh.
2013.
Morphologicalanalysis and disambiguation for dialectal Arabic.
InProceedings of the Main Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Atlanta, US.Jan Hajic?, Jan Hric, and Vladislav Kubon?.
2000.
Ma-chine translation of very close languages.
In Pro-ceedings of the sixth conference on Applied naturallanguage processing, Seattle, Washington.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, Edin-burgh, UK.Ali El Kahki, Kareem Darwish, Ahmed Saad ElDin, Mohamed Abd El-Wahab, Ahmed Hefny, andWaleed Ammar.
2011.
Improved transliterationmining using graph reinforcement.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, Edinburgh, UK.5Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology and NorthAmerican Association for Computational Linguis-tics Conference, Edmonton, Canada.Emad Mohamed, Behrang Mohit, and Kemal Oflazer.2012.
Transforming standard Arabic to colloquialArabic.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics, Short Paper, Jeju Island, Korea.Preslav Nakov and Hwee Tou Ng.
2009.
Improvedstatistical machine translation for resource-poor lan-guages using related resource-rich languages.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, Singa-pore.Preslav Nakov and Jo?rg Tiedemann.
2012.
Combiningword-level and character-level models for machinetranslation between closely-related languages.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics, Short Paper,Jeju Island, Korea.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1).Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic morphologicaltagging, diacritization, and lemmatization using lex-eme models and feature ranking.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, Columbus, Ohio.Wael Salloum and Nizar Habash.
2011.
Dialectalto standard Arabic paraphrasing to improve Arabic-English statistical machine translation.
In Proceed-ings of the First Workshop on Algorithms and Re-sources for Modelling of Dialects and Language Va-rieties, Edinburgh, Scotland.Hassan Sawaf.
2010.
Arabic dialect handling in hybridmachine translation.
In Proceedings of the Confer-ence of the Association for Machine Translation inthe Americas, Denver, Colorado.Masao Utiyama and Hitoshi Isahara.
2008.
A hybridapproach for converting written Egyptian colloquialdialect into diacritized Arabic.
In Proceedings ofthe 6th International Conference on Informatics andSystems, Cairo University, Egypt.Omar F. Zaidan and Chris Callison-Burch.
2011.
TheArabic online commentary dataset: an annotateddataset of informal Arabic with high dialectal con-tent.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: short papers - Volume2, Portland, Oregon.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012.
Machine translation of Arabic di-alects.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Montreal, Canada.6
