Proceedings of the ACL-HLT 2011 System Demonstrations, pages 32?37,Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational LinguisticsMemeTube: A Sentiment-based Audiovisual Systemfor Analyzing and Displaying Microblog MessagesCheng-Te Li1   Chien-Yuan Wang2   Chien-Lin Tseng2   Shou-De Lin1,21 Graduate Institute of Networking and Multimedia2 Department of Computer Science and Information EngineeringNational Taiwan University, Taipei, Taiwan{d98944005, sdlin}@csie.ntu.edu.tw   {gagedark, moonspirit.wcy}@gmail.comAbstractMicro-blogging services provide platformsfor users to share their feelings and ideason the move.
In this paper, we present asearch-based demonstration system, calledMemeTube, to summarize the sentimentsof microblog messages in an audiovisualmanner.
MemeTube provides three mainfunctions: (1) recognizing the sentiments ofmessages (2) generating music melody au-tomatically based on detected sentiments,and (3) produce an animation of real-timepiano playing for audiovisual display.
OurMemeTube system can be accessed via:http://mslab.csie.ntu.edu.tw/memetube/ .1    IntroductionMicro-blogging services such as Twitter1, Plurk2,and Jaiku3, are platforms that allow users to shareimmediate but short messages with friends.
Gener-ally, the micro-blogging services possess somesignature properties that differentiate them fromconventional weblogs and forum.
First, microblogsdeal with almost real-time messaging, includinginstant information, expression of feelings, andimmediate ideas.
It also provides a source of crowdintelligence that can be used to investigate com-mon feelings or potential trends about certain newsor concepts.
However, this real-time property canlead to the production of an enormous number ofmessages that recipients must digest.
Second, mi-cro-blogging is time-traceable.
The temporal in-formation is crucial because contextual posts thatappear close together are, to some extent, correlat-ed.
Third, the style of micro-blogging posts tendsto be conversation-based with a sequence of re-1 http://www.twitter.com2 http://www.plurk.com3 http://www.jaiku.com/sponses.
This phenomenon indicates that the postsand their responses are highly correlated in manyrespects.
Fourth, micro-blogging is friendship-influenced.
Posts from a particular user can also beviewed by his/her friends and might have an im-pact on them (e.g.
the empathy effect) implicitly orexplicitly.
Therefore, posts from friends in thesame period may be correlated sentiment-wise aswell as content-wise.We leverage the above properties to develop anautomatic and intuitive Web application, Me-meTube, to analyze and display the sentiments be-hind messages in microblogs.
Our system can beregarded as a sentiment-driven, music-based sum-marization framework as well as a novel audiovis-ual presentation of art.
MemeTube is designed as asearch-based tool.
The system flow is as shown inFigure 1.
Given a query (either a keyword or a userid), the system first extracts a series of relevantposts and replies based on keyword matching.Then sentiment analysis is applied to determine thesentiment of the posts.
Next a piece of music iscomposed to reflect the detected sentiments.
Final-ly, the messages and music are fed into the anima-tion generation model, which displays a pianokeyboard that plays automatically.Figure 1: The system flow of our MemeTube.The contributions of this work can be viewedfrom three different perspectives.?
From system perspective of view, we demo anovel Web-based system, MemeTube, as a kindof search-based sentiment presentation, musi-32calization, and visualization tool for microblogmessages.
It can serve as a real-time sentimentdetector or an interactive microblog audio-visual presentation system.?
Technically, we integrate a language-model-based classifier approach with a Markov-transition model to exploit three kinds of in-formation (i.e., contextual, response, andfriendship information) for sentiment recogni-tion.
We also integrate the sentiment-detectionsystem with a real-time rule-based harmonicmusic and animation generator to displaystreams of messages in an audiovisual format.?
Conceptually, our system demonstrates that,instead of simply using textual tags to expresssentiments, it is possible to exploit audio (i.e.,music) and visual (i.e., animation) cues to pre-sent microblog users?
feelings and experiences.In this respect, the system can also serve as aWeb-based art piece that uses NLP-technologies to concretize and portray senti-ments.2    Related WorksRelated works can be divided into two parts: sen-timent classification in microblogs, and sentiment-based audiovisual presentation for social media.For the first part, most of related literatures focuson exploiting different classification methods toseparate positive and negative sentiments by a va-riety of textual and linguistics features, as shown inTable 1.
Their accuracy ranges from 60%~85%depending on different setups.
The major differ-ence between our work and existing approaches isthat our model considers three kinds of additionalinformation (i.e., contextual, response and friend-ship information) for sentiment recognition.In recent years, a number of studies have inves-tigated integrating emotions and music in certainmedia applications.
For example, Ishizuka andOnisawa (2006) generated variations of theme mu-sic to fit the impressions of story scenes represent-ed by textual content or pictures.
Kaminskas (2009)aligned music with user-selected points of interestsfor recommendation.
Li and Shan (2007) producedpainting slideshows with musical accompaniment.Hua et al (2004) proposed a Photo2Video systemthat allows users to specify incident music that ex-presses their feelings about the photos.
To the bestof our knowledge, MemeTube is the first attemptto exploit AI techniques to create harmonic audio-visual experiences and interactive emotion-basedsummarization for microblogs.Table 1: Summary of related works thatdetect sentiments in microblogs.3    Sentiment Analysis of Microblog PostsFirst, we develop a classification model as ourbasic sentiment recognition mechanism.
Given atraining corpus of posts and responses annotatedwith sentiment labels, we train an n-gram languagemodel for each sentiment.
Then, we use such mod-el to calculate the probability that a post expressesthe sentiment s associated with that model:????|???
?????,?
,??|?????????????????,?
,???
?, ??,???
?where w is the sequence of words in the post.
Wealso use the common Laplace smoothing method.For each post p and each sentiment s?S, ourclassifier calculates the probability that such postexpresses the sentiment ????|??
using Bayes rule:????|??
?
?????????|???????
is estimated directly by counting, while????|??
can be derived by using the learned lan-Features MethodsPak and Paroubek2010statistic counting ofadjectives Naive BayesChen et al 2008 POS tags, emoticons SVMLewin and Pribu-la 2009 smileys, keywordsMaximum Entropy,SVMRiley 2009n-grams, smileys,hashtags, replies,URLs, usernames,emoticonsNaive Bayes,Prasad 2010 n-grams Na?ve BayesGo et al 2009usernames, sequentialpatterns of keywords,POS tags, n-gramsNaive Bayes, Max-imum Entropy,SVMLi et al 2009several dictionariesabout different kinds ofkeywordsKeyword MatchingBarbosa and Feng2010retweets, hashtag, re-plies, URLs, emoticons,upper casesSVMSun et al 2010 keyword counting and Chinese dictionaries Naive Bayes, SVMDavidov et al2010n-grams, word patterns,punctuation information k-Nearest NeighborBermingham andSmeaton 2010 n-grams and POS tagsBinary Classifica-tion33guage models.
This allow us to produce a distribu-tion of sentiments for a given post p, denoted as ?
?.However, the major challenge in the microblogsentiment detection task is that the length of eachpost is limited (i.e., posts on Twitter are limited to140 characters).
Consequently, there might not beenough information for a sentiment detection sys-tem to exploit.
To solve this problem, we proposeto utilize the three types of information mentionedearlier.
We discuss each type in detail below.3.1   Response FactorWe believe the sentiment of a post is highly corre-lated with (but not necessary similar to) that of re-sponses to the post.
For example, an angry postusually triggers angry responses, but a sad postusually solicits supportive responses.
We proposeto learn the correlation patterns of sentiments fromthe data and use them to improve the recognition.To achieve such goal, from the data, we learnthe probability ???????????????|?????????????????
?, which represents the conditional probability of apost given responses.
Then we use such probabilityto construct a transition matrix ??
, where ????
=???????????????
?
?
|	?????????????????
?
??.
With ?
?, we can generate the adjusted sentiment dis-tribution of the post ???
as:???
?
?
??
?????????????
?
?1 ?
????
, where ??
denotes the original sentiment distribu-tion of the post, and ???
is the sentiment distribu-tion of the ???
response determined by theabovementioned language model approach.
In ad-dition, ???
?
1 ???????????
?
???????
represents the weight of the response since it is preferable to as-sign higher weights to closer responses.
There isalso a global parameter ?
that determines howmuch the system should trust the information de-rived from the responses to the post.
If there is noresponse to a post, we simply assign ???
?
?
?.3.2    Context FactorIt is assumed that the sentiment of a microblogpost is correlated with the author?s previous posts(i.e., the ?context?
of the post).
We also assumethat, for each person, there is a sentiment transitionmatrix ??
that represents how his/her sentimentschange over time.
The ?
?, ????
element in ??
repre-sents the conditional probability from the senti-ment of the previous post to that of the current post:????????????
???
?
?
|	??????????
?????
?
?
?.The diagonal elements stand for the consistencyof the emotion state of a person.
Conceivably, acapricious person?s diagnostic ????
values will be lower than those of a calm person.
The matrix ??
can be learned directly from the annotated data.Let ??
represent the detected sentiment distribu-tion of an existing post at time t. We want to adjust??
based on the previous posts from ?
?
??
to ?
, where ??
is a given temporal threshold.
The sys-tem first extracts a set of posts from the same au-thor posted from time ?
?
??
to ?
and determinestheir sentiment distributions ???
?, ??
?, ?
, ????
, where ?
?
??
?
?
?, ?
?, ?
, ??
?
?
using the same classifier.
Then, the system utilizes the followingupdate equation to obtain an adjusted sentimentdistribution ???:???
?
?
??
?????????????
?
?1 ?
????
,where ???
?
1/??
?
???.
The parameters ??
?, ?, ?
are defined similar to the previous case.
If there isno post in the defined interval, the system willleave ??
unchanged.3.3    Friendship FactorWe also assume that the friends?
emotions are cor-related with each other.
This is because friendsaffect each other, and they are more likely to be inthe same circumstances, and thus enjoy/suffer sim-ilarly.
Our hypothesis is that the sentiment of apost and the sentiments of the author?s friends?recent posts might be correlated.
Therefore, we cantreat the friends?
recent posts in the same way asthe recent posts of the author, and learn the transi-tion matrix??
, where ????
?
????????????????
???
??
|	???????????????
???????
?????
?
?
?, and apply the tech-nique proposed in the previous section to improvethe recognition accuracy.However, it is not necessarily true that allfriends have similar emotional patterns.
One?s sen-timent transition matrix ??
might be very different from that of the other, so we need to be carefulwhen using such information to adjust our recogni-tion outcomes.
We propose to only consider postsfrom friends with similar emotional patterns.To achieve our goal, we first learn every user?scontextual sentiment transition matrix ??
from the data.
In ?
?, each row represents a distribution that sums to one; therefore, we can compare two ma-trixes ???
and ???
by averaging the symmetric KL-divergence of each row.
That is,34?????????????,????
???????????
????????
?, ?
?, ?????
?, ???.
Two persons are considered as having similaremotion pattern if their contextual sentiment transi-tion matrixes are similar.
After a set of similarfriends are identified, their recent posts (i.e., from?
?
??
to ? )
are treated in the same way as theposts by the author, and we use the method pro-posed previously to fine-tune the recogni-tion outcomes.4    Music GenerationFor each microblog post retrieved according to thequery, we can derive its sentiment distribution (asa vector of probabilities) by using the above meth-od.
Next, the system transforms every sentimentdistribution into an affective vector comprised of avalence value and an arousal value.
The valencevalue represents the positive-to-negative sentiment,while the arousal value represents the intense-to-silent level.We exploit the mapping from each type of sen-timent to a two-dimensional affective vector basedon the two-dimensional emotion model of Russell(1980).
Using the model we extract the affectivescore vectors of the six emotions (see Table 2)used in our experiments.
The mapping enables usto transform a sentiment distribution ??
into anaffective score vector by weighted sum approach.For example, given a distribution of (Anger=20%,Surprise=20%,Disgust=10%, Fear=10%, Joy=10%,Sadness=30%), the two-dimensional affective vec-tor can be computed as 0.2*(-0.25, 1) + 0.2*(0.5,0.75) + 0.1*(-0.75, -0.5) + 0.1*(-0.75, 0.5) +0.1*(1, 0.25) + 0.3*(-1, -0.25).
Finally, the affec-tive vector of each post will be summed to repre-sent the sentiment of the given query in terms ofthe valence and arousal values.Table 2: Affective score vector for each sentiment label.Sentiment Label Affective Score VectorAnger (-0.25, 1)Surprise (0.5, 0.75)Disgust (-0.75, -0.5)Fear (-0.75, 0.5)Joy (1, 0.25)Sadness (-1, -0.25)Next the system transforms the affective vectorinto music elements through chord set selection(based on the valence value) and rhythm determi-nation (based on the arousal value).
For chord setselection, we design nine basic chord sets as {A,Am, Bm, C, D, Dm, Em, F, G}, where each chordset consists of some basic notes.
The chord sets areused to compose twenty chord sequences.
Half ofthe chord sequences are used for weakly positive tostrongly positive sentiments and the other half areused for weakly negative to strongly negative sen-timents.
The valence value is therefore divided intotwenty levels, and gradually shifts from stronglypositive to strongly negative.
The chord sets ensurethat the resulting auditory presentation is in har-mony (Hewitt 2008).
For rhythm determination,we divide the arousal values into five levels to de-cide the tempo/speed of the music.
Higher arousalvalues generate music with a faster tempo whilelower ones lead to slow and easy-listening music.Figure 2: A snapshot of the proposed MemeTube.Figure 3: The animation with automatic piano playing.5    Animation GenerationIn this final stage, our system produces real-timeanimation for visualization.
The streams of mes-sages are designed to flow as if they were playing apiece of a piano melody.
We associate each mes-sage with a note in the generated music.
When apost message flows from right to left and touches apiano key, the key itself blinks once and the corre-sponding tone of the key is produced.
The messageflow and the chord/rhythm have to be synchro-nized so that it looks as if the messages themselvesare playing the piano.
The system also allows usersto highlight the body of a message by moving the35cursor over the flowing message.
A snapshot isshown in Figure 2 and the sequential snapshots ofthe animation are shown in Figure 3.6    Evaluations on Sentiment DetectionWe collect the posts and responses from every ef-fective user, users with more than 10 messages, ofPlurk from January 31st to May 23rd, 2009.
In orderto create the diversity for the music generation sys-tem, we decide to use six different sentiments, asshown in Table 2, rather than using only three sen-timent types, positive, negative and neutral, asmost of the systems in Table 1 have used.
The sen-timent of each sentence is labeled automaticallyusing the emoticons.
This is similar to what manypeople have proposed for evaluation (Davidov et al2010; Sun et al 2010; Bifet and Frank 2010; Go etal.
2009; Pak and Paroubek 2010; Chen et al2010).
We use data from January 31st to April 30thas training set, May 1st to 23rd as testing data.
Forthe purpose of observing the result of using thethree factors, we filter the users without friends,the posts without responses, and the posts withoutprevious post in 24 hour in testing data.
We alsomanually label the sentiments on the testing data(totally 1200 posts, 200 posts for each sentiment).We use three metrics to evaluate our model: ac-curacy, Root-Mean-Square Error for valence (de-noted by RMSE(V)) and RMSE for arousal(denoted by RMSE(A)).
The RMSE values aregenerated by comparing the affective vector of thepredicted sentiment distribution with the affectivevector of the answer.
Our basic model reaches33.8% in accuracy, 0.78 in the RMSE(V) and 0.64in RMSE(A).
Note that RMSE?0.5 means thatthere is roughly one quarter (25%) error in the va-lence/arousal values as they range from [-1,1].Note that the main reason the accuracy is not ex-tremely high is that we are dealing with 6 classes.When we combine angry, disgust, fear, and sad-ness into one negative sense and the rest as posi-tive senses, our system reaches 78.7% in accuracy,which is competitive to the state-of-the-art algo-rithms as shown in the related work section.
How-ever, doing such generalization will lose theflexibility of producing more fruitful and diversepieces of music.
Therefore we choose more fine-grained classes for our experiment.Figure 3 shows the results of exploiting the re-sponse, context, and friendship.
Note RMSE?0.5means that there is roughly one quarter (25%) errorin the valence/arousal values as they range from [-1,1].
The results show that considering all threeadditional factors can achieve the best results anddecent improvement over the basic LM model.Table 3: The results after adding addition info(note that for RMSE, the lower value the better)LM Response Context Friend CombineAccuracy 33.8% 34.7% 34.8% 35.1% 36.5%RMSE(V) 0.784 0.683 0.684 0.703 0.679RMSE(A) 0.640 0.522 0.516 0.538 0.5147    System DemoWe create video clips of five different queries fordemonstration, which is downloadable from:http://mslab.csie.ntu.edu.tw/memetube/demo/.
Thisdemo page contains the resulting clips of fourkeyword queries (including football, volcano,Monday, big bang) and a user id query mstcgeek.Here we briefly describe each case.
(1) The videofor query term, football, was recorded on February7th 2011, results in a relatively positive and ex-tremely intense atmosphere.
It is reasonable be-cause the NFL Super Bowl was played onFebruary 6th, 2011.
The valence value is not ashigh as the arousal value because some fans mightnot be very happy to see their favorite team losingthe game.
(2) The query, volcano, was also record-ed on February 7th 2011.
The resulting video ex-presses negative valence and neutral arousal.
Afterchecking the posts, we have learned that it is be-cause the Japanese volcano Mount Asama has con-tinued to erupt.
Some users are worried anddiscussed about the potential follow-up disasters.
(3) The query Monday was performed on February6th 2011, which is a Sunday night.
The negativevalence reflects the ?blue Monday?
phenomenon,which leads to some heavy, less smooth melody.
(4)The term big bang turns out to be very positive onboth valence and arousal, mainly because, besidesits relatively neutral meaning in physics, this termalso refers to a famous comic show that some peo-ple in Plurk love to watch.
We also use one user idas query: the user-id mstcgeek is the official ac-count of Microsoft Taiwan.
This user often usescheery texts to share some videos about their prod-ucts or provide some discounts of their product,which leads to relatively hyped music.368    ConclusionMicroblog, as a daily journey and social network-ing service, generally captures the dynamics of thechange of feelings over time of the authors andtheir friends.
In MemeTube, the affective vector isgenerated by aggregating the sentiment distributionof each post; thus, it represents the majority?s opin-ion (or sentiment) about a topic.
In this sense, oursystem can be regarded as providing users with anaudiovisual experience to learn collective opinionof a particular topic.
It also shows how NLP tech-niques can be integrated with knowledge aboutmusic and visualization to create a piece of inter-esting network art work.
Note that MemeTube canbe regarded as a flexible framework as well sinceeach component can be further refined inde-pendently.
Therefore, our future works are three-fold: For sentiment analysis, we will consider moresophisticated ways to improve the baseline accura-cy and to aggregate individual posts into a collec-tive consensus.
For music generation, we plan toadd more instruments and exploit learning ap-proaches to improve the selection of chords.
Forvisualization, we plan to add more interactions be-tween music, sentiments, and users.AcknowledgementsThis work was supported by National Science Council, Na-tional Taiwan University and Intel Corporation under GrantsNSC99-2911-I-002-001, 99R70600, and 10R80800.ReferencesBarbosa, L., and Feng, J.
2010.
Robust Sentiment Detec-tion on Twitter from Biased and Noisy Data.
In Pro-ceedings of International Conference on ComputationalLinguistics (COLING?10), 36?44.Bermingham, A., and Smeaton, A. F. 2010.
ClassifyingSentiment in Microblogs: is Brevity an Advantage?
InProceedings of ACM International Conference on In-formation and Knowledge Management (CIKM?10),1183?1186.Chen, M. Y.; Lin, H. N.; Shih, C. A.; Hsu, Y. C.; Hsu, P.Y.
; and Hewitt, M. 2008.
Music Theory for ComputerMusicians.
Delmar.Hsieh, S. K. 2010.
Classifying Mood in Plurks.
In Proceed-ings of Conference on Computational Linguistics andSpeech Processing (ROCLING 2010), 172?183.Davidov, D.; Tsur, O.; and Rappoport, A.
2010.
EnhancedSentiment Learning Using Twitter Hashtags and Smi-leys.
In Proceedings of International Conference onComputational Linguistics (COLING?10), 241?249.Go, A.; Bhayani, R.; and Huang, L. 2009.
Twitter Senti-ment Classification using Distant Supervision.
TechnicalReport, Stanford University.Hua, X. S.; Lu, L.; and Zhang, H. J.
2004.
Photo2Video -A System for Automatically Converting PhotographicSeries into Video.
In Proceedings of ACM InternationalConference on Multimedia (MM?04), 708?715.Ishizuka, K., and Onisawa, T. 2006.
Generation of Varia-tions on Theme Music Based on Impressions of StoryScenes.
In Proceedings of ACM International Confer-ence on Game Research and Development, 129?136.Kaminskas, M. 2009.
Matching Information Content withMusic.
In Proceedings of ACM International Confer-ence on Recommendation System (RecSys?09), 405?408.Lewin, J. S., and Pribula, A.
2009.
Extracting Emotionfrom Twitter.
Technical Report, Stanford University.Li, C. T., and Shan, M. K. 2007.
Emotion-based Impres-sionism Slideshow with Automatic Music Accompani-ment.
In Proceedings of ACM International Conferenceon Multimedia (MM?07), 839?842.Li, S.; Zheng, L.; Ren, X.; and Cheng, X.
2009.
EmotionMining Research on Micro-blog.
In Proceedings ofIEEE Symposium on Web Society, 71?75.Pak, A., and Paroubek, P. 2010.
Twitter Based System:Using Twitter for Disambiguating Sentiment Ambigu-ous Adjectives.
In Proceedings of International Work-shop on Semantic Evaluation, (ACL?10), 436?439.Pak, A., and Paroubek, P. 2010.
Twitter as a Corpus forSentiment Analysis and Opinion Mining.
In Proceedingsof International Conference on Language Resources andEvaluation (LREC?10), 1320?1326.Prasad, S. 2010.
Micro-blogging Sentiment Analysis UsingBayesian Classification Methods.
Technical Report,Stanford University.Riley, C. 2009.
Emotional Classification of Twitter Mes-sages.
Technical Report, UC Berkeley.Russell, J.
A.
1980.
Circumplex Model of Affect.
Journalof Personality and Social Psychology, 39(6):1161?1178.Strapparava, C., and Valitutti, A.
2004.
Wordnet-affect: anAffective extension of wordnet.
In Proceedings of Inter-national Conference on Language Resources and Evalu-ation, 1083?1086.Sun, Y. T.; Chen, C. L.; Liu, C. C.; Liu, C. L.; and Soo, V.W.
2010.
Sentiment Classification of Short ChineseSentences.
In Proceedings of Conference on Computa-tional Linguistics and Speech Processing(ROCLING?10), 184?198.Yang, C.; Lin, K. H. Y.; and Chen, H. H. 2007.
EmotionClassification Using Web Blog Corpora.
In Proceedingsof IEEE/WIC/ACM International Conference on WebIntelligence (WI?07), 275?278.37
