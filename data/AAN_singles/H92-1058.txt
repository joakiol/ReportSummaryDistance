PHONETIC CLASSIFICATION ONWIDE-BAND AND TELEPHONE QUALITY SPEECHBenjamin ChigierSpeech Technology GroupArtificial Intelligence LaboratoryNYNEX Science and TechnologyWhite Plains, NY, 10604, U.S.A.1.
ABSTRACTBenchmarking the performance for telephone-network-basedspeech recognition systems i hampered bytwo factors: lack ofstandardized databases for telephone twork speech, and insuffi-cient understanding of the impact of the telephone twork on rec-ognition systems.
The N-TIMIT database was used in theexperiments described in this paper in order to "calibrate" theeffect of the telephone twork on phonetic classification algo-fithrns.
Phonetic classification algorithms have been developed forwide-band and telephone quality speech, and were tested on sub-sets of the TIMIT and N-TIMIT databases.
The classifierdescribed inthis paper provides accuracy of 75% on wide-bandTIM1T data nd 66.5% on telephone quality N-TIMIT data.
Over-all the telephone twork seems to increase the error ate by a fac-tor of 1.3.2.
INTRODUCTIONResearchers typically make use of standardized databasesin order to benchmark the performance ofspeech recogni-tion/understanding systems between and within laborato-ries.
Comparisons between laboratories are important inorder to benchmark the progress of the field in general.Comparisons within a laboratory are important to bench-mark progress as a function of the research-and-develop-ment cycle.
Benchmarking phonetic lassificationalgorithms for telephone-network-based sp ech recogni-tion/understanding systems poses two problems.
First, thereis no commonly-accepted standard atabase for evaluatingphonetic lassification for telephone quality speech.
As aresult, few if any inter-laboratory comparisons have beenmade.
Second, the telephone network presents peech rec-ognition/understanding systems with a band-limited, noisy,and in some cases distorted speech signal.
While we wouldlike to benchmark the performance of recognition systemsintended for network speech against hat of systemsintended for wide-band speech, we do not have adequatequantification fthe impact of the telephone network's sig-nal degradation on the performance ofphonetic lassifica-tion algorithms.
Therefore, we do not know whether theperformance ofa telephone-speech classification algorithmis limited by characteristics ofthe algorithm(s) or by char-actedstics ofthe test utterances themselves.Both problems noted above could be addressed given astandardized database inwhich the speech data is presentedin two forms: speech with wide-band characteristics andthe same speech data with telephone network characteris-tics.
As reported in Jankowski et al \[1\], the N-TIMIT data-base was created for this purpose.
The N-TIMIT databaseis identical to TIMIT \[2\] except hat the former has beentransmitted over the telephone network.
Figure 1 shows asample spectrogram ofa TIMIT and N-TIMIT utterance.The N-TIMIT versions were recorded over many differenttransmission paths in order to get a representative sampleof the range of telephone network conditions.
This datapresents a platform to "calibrate" the impact of the tele-phone network on the performance ofphonetic lassifica-tion algorithms.The telephone network affects the speech signal it carriesin many ways.
Chigier and Spitz\[3\] discussed the possibleeffects of source characteristics (how the speech is pro-duced) and transmission characteristics (the environment iwhich the speech is produced, including ambient noise lev-els and the characteristics of the channel through which thespeech is recorded).
Some of the more obvious changes aredue to band limitation (the signal is band-passed betweenapproximately 300 Hz and 3400 Hz), addition of noise(both switching and line noise), and crosstalk.
The goal ofthis experiment is to quantify the combined effects of sig-nal changes due to telephone transmission characteristicson phonetic lassification and to present the performance ofa classifier under development.
In doing this we hope toprovide a model for inter-laboratory and intra-laboratorybenchmarking of telephone-based vs. wide-band algo-rithms.3.
EXPERIMENTAL DESIGNThe wide-band speech data used in this experiment consistsof a subset of utterances from the TIMIT database\[2\].
In2916261 Wave fo rm 2500 -- iO000L ILL IL I  ?
l \ [1  L L L L L L L L L L - I  LL I  ?
- : : - .
kk l I i LLL  .
.
.
.- -4875. .
.
.
.
.
.
?
"p~,T~rr ,  .
.
.
.
, , r r  .
.
.
.
-?
?
.
.. .~ .~ ~.~: ?
: .
.
~ : ;  ~ X. .
.
.
.
.
: ~ -7 ~ : :  ..... ~.~:.
:,~ :: ~ ~:-~",~;i~% '~~ "#~.
.
?
.
; , .
.
~ .
.
.
.
, ' .
~2,~.~%~ .
.
- : .
.~  ..~.,~ , .
- ,' ".~ .
'," ~.
t .
. "
"".~.." " ,~  .
.
.
.
.  '
:.
~ :~, ,  '1~ ,~.
,  ~ :~.
?
: ~ .
, ~ ~ '~": .
.
.
.
.
.
.
.
~t  ~.
l t~  .~ " \  L~:* i  "?~ ~.
l l l~ f f ' l F~g~ :I~'~P3~ .
.
~-; ~( ".I ?..i ' ..
."'
..
.
: .
:~ i. .
.
.
:~ ' i . '
:  ~ .
. "
' .
.
.
- -~>~:"  L '% ?
2 s-.
.
: .~ , .
~ ":~'.~ ~':~ , '~ '  -"~ ' : .
;  ,~ ; : : :~  : ; :~ :-.
, : 6 .
-?
.
: ; . '
.
.
.
.
, " .
.
,~ .
.
L , .~  ; ,~: .
.
,.
~ .ix l = I - -y  I r t  IdeXlJhl ix I v I b~l  Ib l  x I an  I ~ I ~ I hh  I ~hi l l l t l l l l l | l l l l l l t t l l l l l t l l l l l | l l l l l l l l l l  I I I I I I I l | l l t l l l l l l l l l l  I I I I I J l l l  I I I 1 ~ 1 1 \ [ 1 1 1 1 1 1  | l l l l l l l0.2  0 .3  0 .4  O:-i O.
0 .7  0 .8  0 .9  l .O  I .
iFigure 1: Spectrograms ofan utterance inTIMIT (above) and N-TIMIT (below).order to investigate the effects of telephone network trans-mission characteristics, the same subset of the TIMIT utter-ances used by Lee\[5\], and their N-TIMIT counterparts,were selected.
Specifically, the test set consisted of threesentences selected at random from the Brown Corpus\[4\]("si" utterances), and five sentences that provide a widecoverage of phoneme pairs ("sx" utterances), all for each of20 speakers.
This resulted in 6016 phonetic segments in160 unique sentences tobe classified into one of 39 catego-ries, also defined by Lee.
The "si" and "sx" utterances forthe remaining 610 speakers were used to train the classifica-tion system.product between the spectral slice and each of the 40 fre-quency responses of Seneff's auditory model\[6\].
This issimilar to passing the signal through a bank of critical-bandfilters.5.
CLASSIFICATIONA full-covariance gaussian classifier was then used to clas-sify each of the incoming segments into one of the 39 pho-nemes.
The gaussian classifier used 56 context-independentmodels based on a uni-gram model for the phonemes.4.
SIGNAL PROCESSING 5.1.
Feature ExtractionIdentical signal processing was performed on TIMIT andN-TIMIT.
The speech signals were sampled at 16 kHz andpre-emphasized.
We have developed a new signal represen-tation: bark auditory spectral coefficients (BASC).
TheBASC was obtained by filtering the FFT representationwith the filters of Seneff's auditory model\[6\].
Specifically, a128-point FFT was performed with a 28-ms Hanning win-dow every 5 ms.
The window size of 28 ms was empiricallydetermined tobe the best for this task given this classifica-tion system.
Each spectral slice, produced by the FFT, wasdown-sampled to 40 coefficients by computing the dotEach segment was divided in time into three equal parts.Forty coefficients were averaged across each third, resultingin 120 features for each phoneme.
The average spectral dif-ference was computed with its center at the begin boundaryand then calculated again with its center at the end bound-ary.
This spectral difference measure was computed foreach spectral coefficient ( here are 40 spectral coefficients)around each boundary in a segment.
Therefore this gave atotal of 80 spectral difference features.
In calculating thespectral average, the frames further away from the center ofthe boundary were weighted more heavily than the frames292close to the boundary.
This weighting scheme is similar tothat proposed by Rabiner et al \[7\].
Let S\[f,c\] be the value ofthe spectral representation at framef and spectral coeffi-cient c. Thus, the spectral difference coefficient at a seg-ment boundary, sb (begin or end boundary), AS\[sb,c\] isdefined as:N 1 Eq.
1: AS\[sb, c\] = R { ~ w(S\[(sb-w),c\]-S\[(sb+w),c\]) }w=|where 2N is the number of frames in the overall window,and w is the weighting factor.A pilot study was conducted to determine whether weightedaverages provide better classification performance than tra-ditional unweighted (the special case of w = 1 in Eq.
1)averages using the current classification system.
Theweighted versions lightly outperformed the unweightedaverages when testing on the cross-validation set describedabove.Another pilot study was designed to determine the optimalnumber of frames to use when computing the weightedaverages.
The number of frames included was systemati-cally varied from 0 to 10 (0 _< N _< 10 in Eq 1), both preced-ing and following the boundary, which resulted in aweighted average difference for each coefficient.
(Note thatfor N = 0 frames, no difference information is denved).
Theoptimal number of frames to include in the weighted aver-age was found to be 7, which provided the highest classifi-cation score on the cross-validation set.The average spectral distance calculations result in 40 fea-tures at the begin boundary and 40 features at the endboundary.
These were combined with the 120 featuresderived for each segment described above.
Duration andmaximum zero crossing count were added to the pool offeatures, resulting in 202 features that were passed on to theclassification system.5.2.
Feature SelectionPrincipal component analysis was used to reduce the num-ber of input dimensions to the classifiers.
The principalcomponents were ranked in decreasing order according toamount of variance accounted for in the original data (i.e.,based on the eigenvalues).
The final set of principal compo-nents used was determined empirically by adding one pnn-cipal component at a time to the classifier, training theclassifier, and then evaluating performance on the cross-validation set.
Finally, the set of pnncipal components hatproduced the best performance on the cross-validation setwas used to train the classifier on the entire training set.This procedure was carried out separately for the N-TIMITand the TIMIT database.
The resulting two classifiers wereevaluated on their respective t st sets.Ranking the pnncipal components according to the amountof variance they account for may not reflect how well theydiscriminate between classes.
Therefore, another procedurewas also evaluated to determine which of the principal com-ponents have the most discriminating power.
This proce-dure was a stepwise add-on procedure based on adding theprincipal component that improves the performance of theclassifier the most on the cross-validation set.
This rankingof the pnncipal components was determined by first train-ing a classifier on all 202 principal components.
Anotherclassifier was then created by taking the features from theinitial classifier, one at a time, and testing on the cross-vali-dation set.
The pnncipal component that performed the bestwas next used with the remaining features one at a time, andnow the pair of features that gave the best performance wasused with the remaining features.
This procedure was car-ried out by incrementally adding pnncipal components othe classifier based on their ability to improve performance.This procedure isnot an optimal procedure, but it is compu-tationally feasible (the optimal procedure would requiretesting 2202 (approximately 6.4x106?)
classifiers).6.
RESULTSThe eigenvectors are ordered according to the amount ofvariance that they account for in the original feature space;we can therefore draw a plot of the percentage of the totalvariance the pnncipal components account for of the origi-nal data as the number of pnncipal components increases.Figure 2 displays the number of pnncipal components in thesystem vs. the percentage of the total variance that isaccounted for by those principal components.
In N-TIMIT!/ / !
'1I'0 I'5 2'0Number of Principal ComponentsFigure 2: Number of principal components u ed vs. thepercentage ofvariance accounted for by the principalcomponents.293information i  the spectrum above 3400 Hz is small (due tothe bandpass characteristics of the telephone network) andso the variance of the features that represent this informa-tion is small.
Consequently fewer principal components areneeded to account for the variability of these features.
Thiscan be seen in Figure 2, where the N-TIMIT curve is higherthan the TIMIT curve.
A larger percentage of the varianceis accounted for in N-TIMIT than in TIMIT for the samenumber of eigenvectors.Figure 3 is a plot of TIMIT error rate and N-TIMIT errorrate on the cross-validation set.
It is interesting to note thatafter the top 10 principal components have been used, themean value of the ratio of N-TIMIT error rate to TIMITerror rate is 1.3, with a standard eviation of only 0.019.The error rate with 10 principal components i  39.6% and48.1% for TIMIT and N-TIMIT respectively and goesdown to a minimum of 25.8% and 34.1% for TIMIT and N-TIMIT respectively on the cross-validation set.
The numberof principal components discovered to give the best classifi-cation performance on the cross-validation set was 58 forthe TIMIT classifier and 65 for the N-TIMIT classifier.
Theimprovements in classification accuracy, however are verysmall after approximately 35 principal components havebeen included.O0C: 0>,tOoC0 ?~.~, I N'TIIdlT Err?r RatelError Rate I??
"*'** '.,.??...?..????.,?
?,..,...o?...H ???
?...,?..../0 60Number of Principal ComponentsFigure 3: Number of principal components used vs.error rates for TIMIT and N-TIMIT classifiers on thecross-validation set.The two procedures for ranking the principal componentswere compared.
The first procedure ranked the principalcomponents according to the variance they accounted for;the second ranked them according to their discriminativepower.
No difference in classification accuracy was foundbetween these two procedures.
This finding concurs withBrown\[8\]; The performance of his system when a largenumber of principal components was used was the same aswhen he used discriminative analysis.The first-choice accuracies of the TIMIT and N-TIMITclassifiers on the test set are 74.8% and 66.5% respectively.Error rates of the two classifiers on the test set appear inTable 1.
As on the cross-validation set, the phonetic lassifi-cation error rate on the test set is also increased by a factorof 1.3 by the telephone network.
In order to determinewhether TIM1T and N-TIMIT classification accuracy differsignificantly, a McNemar test of symmetry was conducted.The results of this analysis revealed significant differencesbetween TIMIT and N-TIMIT classifier performance (p <0.01).First Choice Top 2 Choices Top 3 Choice Database Error Rate Error Rate Error RateTIMIT i 25.2% 11.6% 6.4%iN-TIMIT 33.5% 17.5% 11.4%Table 1: Error rotes on test seLA McNemar test of symmetry was also conducted sepa-rately on each of the 39 phonemes to determine which pho-nemes accounted for the significant differences.
The resultsof this analysis revealed a significant effect of database on13 of the 39 phonemes (p < 0.01).
These phonemes areshown in Table 2.
The percentage of N-TIMIT phonemesDifference in Difference in Phoneme% correct # correctf I 29 25Ig 28 14k 27 45s 22 55hh 20 13m 18 24r 17 34p 17 19z 15 221 13 29er 12 23n 8 30el 3 38Table 2: Phonemes that are significantly different(p < 0.01) between TIMIT and N-TIMIT.correctly classified were subtracted from the percentage ofTIMIT phonemes correctly classified.
Results are presentedin decreasing order.
For example, the accuracy on the pho-294neme,/f/, is 29% higher on TIMIT than on N-TIMIT.
Alarge number of these errors are predictable based on theacoustic haracteristics of the segments and their sensitivity,to band-passing ornoise.
A spectrogram ofthe same TIMITJ and N-TIMIT utterance isshown in Figure 1.
This utterancewas chosen because it highlights everal of the phonemesthat are classified significantly differently in TIMIT and N-TIMIT.
Many of the classification errors are explainablefrom the spectrogram.
The ffication for/s/, for example, isavisible and salient cue in the TIMIT utterance, but is nearlynon-existent in the telephone quality N-TIMIT version.7.
CONCLUSIONSIn developing the TIMIT and N-TIMIT classifiersdescribed in this experiment, three findings emerged forimproving classification performance:1.
A Hanning window size of 28 ms was determined tobe the best for this task;2.
Weighted average spectral differences outperformedunweighted averages;3.
The number of frames to include in the weightedaverage spectral difference was found to be 7.The advantage ofweighted spectral differences i supportedby earlier esults reported by Rabiner et al \[7\].
It remains tobe seen whether the characteristics determined in this set-ting will transfer well to other ecognition tasks.The performance of the TIMIT classifier (75%) comparesfavorably  to results  reported by other research-ers\[5,9,10,11\].
Results indicate that the telephone network,in general, increases the phonetic lassification error rate bya factor of 1.3.
This correction factor may be useful in ourattempts to benchmark the performance of wide-band vs.network based recognition systems.
Furthermore, this studysets a first benchmark on the N-TIMIT database.
We hopeto encourage others to evaluate their systems on this data-base and in so doing follow the model established by ourcolleagues working on wide-band speech.4.5.6.7.8.9.10.11.Kucera, H., and Francis, W. N., "Computational analysis ofpresent day American English", Brown University Press,Providence, RI, 1967.Lee, K,.
Hon, H., "Speaker independent phone recognitionusing hidden Markov models".
Carnegie-MeUon Univer-sity, Computer Science Dept, Ref Number CMU-CS-88-121 March 1988.Seneff, S., "A Joint Synchrony/Mean Rate Model of Audbtory Speech Processing", Journal of Phonetics, Vol.
16,No.l, pp 55-76, 1988Rabiner, L. R., Wilpen, J. G., and Soong, E K., "High per-formance connected digit recognition, using hiddenMarkov models.
", ICASSP, 1988, pp.
119-122.Brown, E E, "The acoustic-modeling problem in auto-mmic speech recognition", Ph.D. Thesis, Carnegie-MellonUniversity, March 1988.Zue, V, Glass, J., Phillips, M., and Seneff, S., "AcousticSegmentation a d Phonetic Classification i the SUMMITSystem", ICASSP, 1989, pp.
389-392.Leung, H. C., Glass, J. R., Phillips, M. S., and Zue, V. W.,"Detection and Classification ofPhonemes Using Context-Independent Error Back-Propagation", ICSLP, 1990, pp.1061-1064.Digalakis, V., Rohlicek, J. R., and OstendorL M., "Adynamical system approach to continuous speech recogni-tion", DARPA, Speech and Natural Language Workshop,Feb 1991, pp 253-257.1.2.3.REFERENCESJankowski, C., Kalyanswamy, A. Basson, S., and Spitz, J.,"N-TIMIT: A phonetically balanced, continuous speech,telephone bandwidth speech database", ICASSP, 1990, pp.109-112.Fisher, W., Doddington, G., and Goudie-Marshall, K."The DARPA speech recognition database: specificationsand status", DARPA Workshop on Speech Recognition, Feb1986, pp.
93-99.Chigier, B., and Spitz, J., "Are laboratory databases appro-priam for tzaining and testing telephone bandwidth speechrecognizers?
", ICSLP, 1990, pp.
1017-1020.295
