Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 74?81,New York, June 2006. c?2006 Association for Computational LinguisticsExploiting Domain Structure for Named Entity RecognitionJing Jiang and ChengXiang ZhaiDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{jiang4,czhai}@cs.uiuc.eduAbstractNamed Entity Recognition (NER) is afundamental task in text mining and nat-ural language understanding.
Current ap-proaches to NER (mostly based on super-vised learning) perform well on domainssimilar to the training domain, but theytend to adapt poorly to slightly differentdomains.
We present several strategiesfor exploiting the domain structure in thetraining data to learn a more robust namedentity recognizer that can perform well ona new domain.
First, we propose a sim-ple yet effective way to automatically rankfeatures based on their generalizabilitiesacross domains.
We then train a classifierwith strong emphasis on the most general-izable features.
This emphasis is imposedby putting a rank-based prior on a logis-tic regression model.
We further proposea domain-aware cross validation strategyto help choose an appropriate parameterfor the rank-based prior.
We evaluatedthe proposed method with a task of recog-nizing named entities (genes) in biologytext involving three species.
The exper-iment results show that the new domain-aware approach outperforms a state-of-the-art baseline method in adapting to newdomains, especially when there is a greatdifference between the new domain andthe training domain.1 IntroductionNamed Entity Recognition (NER) is the task ofidentifying and classifying phrases that denote cer-tain types of named entities (NEs), such as per-sons, organizations and locations in news articles,and genes, proteins and chemicals in biomedical lit-erature.
NER is a fundamental task in many naturallanguage processing applications, such as questionanswering, machine translation, text mining, and in-formation retrieval (Srihari and Li, 1999; Huang andVogel, 2002).Existing approaches to NER are mostly based onsupervised learning.
They can often achieve highaccuracy provided that a large annotated training setsimilar to the test data is available (Borthwick, 1999;Zhou and Su, 2002; Florian et al, 2003; Klein et al,2003; Finkel et al, 2005).
Unfortunately, when thetest data has some difference from the training data,these approaches tend to not perform well.
For ex-ample, Ciaramita and Altun (2005) reported a per-formance degradation of a named entity recognizertrained on CoNLL 2003 Reuters corpus, where theF1 measure dropped from 0.908 when tested on asimilar Reuters set to 0.643 when tested on a WallStreet Journal set.
The degradation can be expectedto be worse if the training data and the test data aremore different.The performance degradation indicates that exist-ing approaches adapt poorly to new domains.
Webelieve one reason for this poor adaptability is thatthese approaches have not considered the fact that,depending on the genre or domain of the text, theentities to be recognized may have different mor-74phological properties or occur in different contexts.Indeed, since most existing learning-based NER ap-proaches explore a large feature space, without regu-larization, a learned NE recognizer can easily overfitthe training domain.Domain overfitting is a serious problem in NERbecause we often need to tag entities in completelynew domains.
Given any new test domain, it is gen-erally quite expensive to obtain a large amount oflabeled entity examples in that domain.
As a result,in many real applications, we must train on data thatdo not fully resemble the test data.This problem is especially serious in recognizingentities, in particular gene names, from biomedicalliterature.
Gene names of one species can be quitedifferent from those of another species syntacticallydue to their different naming conventions.
For exam-ple, some biological species such as yeast use sym-bolic gene names like tL(CAA)G3, while some otherspecies such as fly use descriptive gene names likewingless.In this paper, we present several strategies for ex-ploiting the domain structure in the training data tolearn a more robust named entity recognizer that canperform well on a new domain.
Our work is mo-tivated by the fact that in many real applications,the training data available to us naturally falls intoseveral domains that are similar in some aspects butdifferent in others.
For example, in biomedical lit-erature, the training data can be naturally groupedby the biological species being discussed, while fornews articles, the training data can be divided bythe genre, the time, or the news agency of the arti-cles.
Our main idea is to exploit such domain struc-ture in the training data to identify generalizable fea-tures which, presumably, are more useful for rec-ognizing named entities in a new domain.
Indeed,named entities across different domains often sharecertain common features, and it is these commonfeatures that are suitable for adaptation to new do-mains; features that only work for a particular do-main would not be as useful as those working formultiple domains.
In biomedical literature, for ex-ample, surrounding words such as expression andencode are strong indicators of gene mentions, re-gardless of the specific biological species being dis-cussed, whereas species-specific name characteris-tics (e.g., prefix = ?-less?)
would clearly not gener-alize well, and may even hurt the performance on anew domain.
Similarly, in news articles, the part-of-speeches of surrounding words such as ?followed bya verb?
are more generalizable indicators of namementions than capitalization, which might be mis-leading if the genre of the new domain is different;an extreme case is when every letter in the new do-main is capitalized.Based on these intuitions, we regard a feature asgeneralizable if it is useful for NER in all trainingdomains, and propose a generalizability-based fea-ture ranking method, in which we first rank the fea-tures within each training domain, and then combinethe rankings to promote the features that are rankedhigh in all domains.
We further propose a rank-based prior on logistic regression models, whichputs more emphasis on the more generalizable fea-tures during the learning stage in a principled way.Finally, we present a domain-aware validation strat-egy for setting an appropriate parameter value forthe rank-based prior.
We evaluated our method ona biomedical literature data set with annotated genenames from three species, fly, mouse, and yeast, bytreating one species as the new domain and the othertwo as the training domains.
The experiment resultsshow that the proposed method outperforms a base-line method that represents the state-of-the-art NERtechniques.The rest of the paper is organized as follows: InSection 2, we introduce a feature ranking methodbased on the generalizability of features across do-mains.
In Section 3, we briefly introduce the logisticregression models for NER.
We then propose a rank-based prior on logistic regression models and de-scribe the domain-aware validation strategy in Sec-tion 4.
The experiment results are presented in Sec-tion 5.
Finally we discuss related work in Section 6and conclude our work in Section 7.2 Generalizability-Based Feature RankingWe take a commonly used approach and treat NERas a sequential tagging problem (Borthwick, 1999;Zhou and Su, 2002; Finkel et al, 2005).
Each tokenis assigned the tag I if it is part of an NE and the tagO otherwise.
Let x denote the feature vector for atoken, and let y denote the tag for x.
We first com-pute the probability p(y|x) for each token, using a75learned classifier.
We then apply Viterbi algorithmto assign the most likely tag sequence to a sequenceof tokens, i.e., a sentence.
The features we use fol-low the common practice in NER, including surfaceword features, orthographic features, POS tags, sub-strings, and contextual features in a local window ofsize 5 around the target token (Finkel et al, 2005).As in any learning problem, feature selectionmay affect the NER performance significantly.
In-deed, a very likely cause of the domain overfit-ting problem may be that the learned NE recog-nizer has picked up some non-generalizable fea-tures, which are not useful for a new domain.
Below,we present a generalizability-based feature rankingmethod, which favors more generalizable features.Formally, we assume that the training examplesare divided into m subsets T1, T2, .
.
.
, Tm, corre-sponding to m different domains D1, D2, .
.
.
, Dm.We further assume that the test set Tm+1 is froma new domain Dm+1, and this new domain sharessome common features of the m training domains.Note that these are reasonable assumptions that re-flect the situation in real problems.We use generalizability to denote the amount ofcontribution a feature can make to the classificationaccuracy on any domain.
Thus, a feature with highgeneralizability should be useful for classificationon any domain.
To identify the highly generalizablefeatures, we must then compare their contributionsto classification among different domains.Suppose in each individual domain, the featurescan be ranked by their contributions to the classifi-cation accuracy.
There are different feature rankingmethods based on different criteria.
Without loss ofgenerality, let us use rT : F ?
{1, 2, .
.
.
, |F |} todenote a ranking function that maps a feature f ?
Fto a rank rT (f) based on a set of training examplesT , where F is the set of all features, and the rank de-notes the position of the feature in the final rankedlist.
The smaller the rank rT (f) is, the more impor-tant the feature f is in the training set T .
For the mtraining domains, we thus have m ranking functionsrT1 , rT2 , .
.
.
, rTm .To identify the generalizable features across the mdifferent domains, we propose to combine the m in-dividual domain ranking functions in the followingway.
The idea is to give high ranks to features thatare useful in all training domains .
To achieve thisgoal, we first define a scoring function s : F ?
Ras follows:s(f) = mmini=11rTi(f) .
(1)We then rank the features in decreasing order of theirscores using the above scoring function.
This is es-sentially to rank features according to their maxi-mum rank maxi rTi(f) among the m domains.
Letfunction rgen return the rank of a feature in this com-bined, generalizability-based ranked list.The original ranking function rT used for indi-vidual domain feature ranking can use different cri-teria such as information gain or ?2 statistic (Yangand Pedersen, 1997).
In our experiments, we used aranking function based on the model parameters ofthe classifier, which we will explain in Section 5.2.Next, we need to incorporate this preference forgeneralizable features into the classifier.
Note thatbecause this generalizability-based feature rankingmethod is independent of the learning algorithm, itcan be applied on top of any classifier.
In this work,we choose the logistic regression classifier.
One wayto incorporate the feature ranking into the classifieris to select the top-k features, where k is chosen bycross validation.
There are two potential problemswith this hard feature selection approach.
First, oncek features are selected, they are treated equally dur-ing the learning stage, resulting in a loss of the pref-erence among these k features.
Second, this incre-mental feature selection approach does not considerthe correlation among features.
We propose an al-ternative way to incorporate the feature ranking intothe classifier, where the preference for generalizablefeatures is transformed into a non-uniform prior overthe feature parameters in the model.
This can be re-garded as a soft feature selection approach.3 Logistic Regression for NERIn binary logistic regression models, the probabilityof an observation x being classified as I isp(I|x,?)
= exp(?0 +?|F |i=1 ?ixi)1 + exp(?0 +?|F |i=1 ?ixi)(2)= exp(?
?
x?
)1 + exp(?
?
x?)
, (3)76where ?0 is the bias weight, ?i (1 ?
i ?
|F |)are the weights for the features, and x?
is the aug-mented feature vector with x0 = 1.
The weight vec-tor ?
can be learned from the training examples bya maximum likelihood estimator.
It is worth point-ing out that logistic regression has a close relationwith maximum entropy models.
Indeed, when thefeatures in a maximum entropy model are defined asconjunctions of a feature on observations only anda Kronecker delta of a class label, which is a com-mon practice in NER, the maximum entropy modelis equivalent to a logistic regression model (Finkelet al, 2005).
Thus the logistic regression method weuse for NER is essentially the same as the maximumentropy models used for NER in previous work.To avoid overfitting, a zero mean Gaussian prioron the weights is usually used (Chen and Rosenfeld,1999; Bender et al, 2003), and a maximum a poste-rior (MAP) estimator is used to maximize the poste-rior probability:??
= arg max?p(?
)N?j=1p(yj |xj,?
), (4)where yj is the true class label for xj, N is the num-ber of training examples, andp(?)
=|F |?i=11?2pi?2iexp(?
?2i2?2i).
(5)In previous work, ?i are set uniformly to the samevalue for all features, because there is in general noadditional prior knowledge about the features.4 Rank-Based PriorInstead of using the same ?i for all features, we pro-pose a rank-based non-uniform Gaussian prior onthe weights of the features so that more general-izable features get higher prior variances (i.e., lowprior strength) and features on the bottom of the listget low prior variances (i.e., high prior strength).Since the prior has a zero mean, such a prior wouldforce features on the bottom of the ranked list, whichhave the least generalizability, to have near-zeroweights, but allow more generalizable features to beassigned higher weights during the training stage.4.1 Transformation FunctionWe need to find a transformation function h :{1, 2, .
.
.
, |F |} ?
R+ so that we can set ?2i =h(rgen(fi)), where rgen(fi) is the rank of featurefi in the generalizability-based ranked feature list,as defined in Section 2.
We choose the followingh function because it has the desired properties asdescribed above:h(r) = ar1/b , (6)where a and b (a, b > 0) are parameters that controlthe degree of the confidence in the generalizability-based ranked feature list.
Note that a corresponds tothe prior variance assigned to the top-most feature inthe ranked list.
When b is small, the prior variancedrops rapidly as the rank r increases, giving only asmall number of top features high prior variances.When b is larger, there will be less discriminationamong the features.
When b approaches infinity, theprior becomes a uniform prior with the variance setto a for all features.
If we set a small threshold ?
onthe variance, then we can derive that at least m =(a?
)b features have a prior variance greater than ?
.Thus b is proportional to the logarithm of the numberof features that are assigned a variance greater thanthe threshold ?
when a is fixed.
Figure 1 shows theh function when a is set to 20 and b is set to a set ofdifferent values.05101520250  200  400  600  800  1000h(r)rb = 2b = 4b = 6b = ?Figure 1: Transformation Function h(r) = 20r1/b4.2 Parameter Setting using Domain-AwareValidationWe need to set the appropriate values for the param-eters a and b.
For parameter a, we use the following77simple strategy to obtain an estimation.
We first traina logistic regression model on all the training datausing a Gaussian prior with a fixed variance (set to1 in our experiments).
We then find the maximumweight?max = |F |maxi=1 |?i| (7)in this trained model.
Finally we set a = ?2max.
Ourreasoning is that since a is the variance of the priorfor the best feature, a is related to the ?permissiblerange?
of ?
for the best feature, and ?max gives us away for adjusting a according to the empirical rangeof ?i?s.As we pointed out in Section 4.1, when a is fixed,parameter b controls the number of top features thatare given a relatively high prior variance, and henceimplicitly controls the number of top features tochoose for the classifier to put the most weights on.To select an appropriate value of b, we can use aheld-out validation set to tune the parameter valueb.
Here we present a validation strategy that exploitsthe domain structure in the training data to set theparameter b for a new domain.
Note that in regularvalidation, both the training set and the validationset contain examples from all training domains.
Asa result, the average performance on the validationset may be dominated by domains in which the NEsare easy to classify.
Since our goal is to build a clas-sifier that performs well on new domains, we shouldpay more attention to hard domains that have lowerclassification accuracy.
We should therefore exam-ine the performance of the classifier on each trainingdomain individually in the validation stage to gainan insight into the appropriate value of b for a newdomain, which has an equal chance of being similarto any of the training domains.Our domain-aware validation strategy first findsthe optimal value of b for each training domain.
Foreach subset Ti of the training data belonging to do-main Di, we divide it into a training set T ti and a val-idation set T vi .
Then for each domain Di, we train aclassifier on the training sets of all domains, that is,we train on ?mj=1 T tj .
We then test the classifier onT vi .
We try a set of different values of b with a fixedvalue of a, and choose the optimal b that gives thebest performance on T vi .
Let this optimal value of bfor domain Di be bi.Given bi (1 ?
i ?
m), we can choose an appropri-ate value of bm+1 for an unknown test domain Dm+1based on the assumption that Dm+1 is a mixture ofall the training domains.
bm+1 is then chosen to bea weighted average of bi, (1 ?
i ?
m):bm+1 =m?i=1?ibi, (8)where ?i indicates how similar Dm+1 is to Di.
Inmany cases, the test domain Dm+1 is completelyunknown.
In this case, the best we can do is to set?i = 1/m for all i, that is, to assume that Dm+1 isan even mixture of all training domains.5 Empirical Evaluation5.1 Experimental SetupWe evaluated our domain-aware approach to NERon the problem of gene recognition in biomedicalliterature.
The data we used is from BioCreAtIvETask 1B (Hirschman et al, 2005).
We chose thisdata set because it contains three subsets of MED-LINE abstracts with gene names from three species(fly, mouse, and yeast), while no other existing an-notated NER data set has such explicit domain struc-ture.
The original BioCreAtIvE 1B data was notprovided with every gene annotated, but for each ab-stract, a list of genes that were mentioned in the ab-stract was given.
A gene synonym list was also givenfor each species.
We used a simple string matchingmethod with slight relaxation to tag the gene men-tions in the abstracts.
We took 7500 sentences fromeach species for our experiments, where half of thesentences contain gene mentions.
We further splitthe 7500 sentences of each species into two sets,5000 for training and 2500 for testing.We conducted three sets of experiments, eachcombining the 5000-sentence training data of twospecies as training data, and the 2500-sentence testdata of the third species as test data.
The 2500-sentence test data of the training species was usedfor validation.
We call these three sets of experi-ments F+M?Y, F+Y?M, and M+Y?F.we use FEX1 for feature extraction and BBR2 forlogistic regression in our experiments.1http://l2r.cs.uiuc.edu/ cogcomp/asoftware.php?skey=FEX2http://www.stat.rutgers.edu/ madigan/BBR/785.2 Comparison with Baseline MethodBecause the data set was generated by our automatictagging procedure using the given gene lists, there isno previously reported performance on this data setfor us to compare with.
Therefore, to see whetherusing the domain structure in the training data canreally help the adaptation to new domains, we com-pared our method with a state-of-the-art baselinemethod based on logistic regression.
It uses a Gaus-sian prior with zero mean and uniform variance onall model parameters.
It also employs 5-fold regularcross validation to pick the optimal variance for theprior.
Regular feature selection is also consideredin the baseline method, where the features are firstranked according to some criterion, and then crossvalidation is used to select the top-k features.
Wetested three popular regular feature ranking meth-ods: feature frequency (F), information gain (IG),and ?2 statistic (CHI).
These methods were dis-cussed in (Yang and Pedersen, 1997).
However, withany of the three feature ranking criteria, cross valida-tion showed that selecting all features gave the bestaverage validation performance.
Therefore, the bestbaseline method which we compare our method withuses all features.
We call the baseline method BL.In our method, the generalizability-based featureranking requires a first step of feature ranking withineach training domain.
While we could also use F,IG or CHI to rank features in each domain, to makeour method self-contained, we used the followingstrategy.
We first train a logistic regression modelon each domain using a zero-mean Gaussian priorwith variance set to 1.
Then, features are rankedin decreasing order of the absolute values of theirweights.
The rationale is that, in general, featureswith higher weights in the logistic regression modelare more important.
With this ranking within eachtraining domain, we then use the generalizability-based feature ranking method to combine the mdomain-specific rankings.
The obtained ranked fea-ture list is used to construct the rank-based prior,where the parameters a and b are set in the way asdiscussed in Section 4.2.
We call our method DOM.In Table 1, we show the precision, recall, and F1measures of our domain-aware method (DOM) andthe baseline method (BL) in all three sets of exper-iments.
We see that the domain-aware method out-performs the baseline method in all three cases whenF1 is used as the primary performance measure.
InF+Y?M and M+Y?F, both precision and recall arealso improved over the baseline method.Exp Method P R F1F+M?Y BL 0.557 0.466 0.508DOM 0.575 0.516 0.544F+Y?M BL 0.571 0.335 0.422DOM 0.582 0.381 0.461M+Y?F BL 0.583 0.097 0.166DOM 0.591 0.139 0.225Table 1: Comparison of the domain-aware methodand the baseline method, where in the domain-awaremethod, b = 0.5b1 + 0.5b2Note that the absolute performance shown in Ta-ble 1 is lower than the state-of-the-art performanceof gene recognition (Finkel et al, 2005).3 One rea-son is that we explicitly excluded the test domainfrom the training data, while most previous work ongene recognition was conducted on a test set drawnfrom the same collection as the training data.
An-other reason is that we used simple string match-ing to generate the data set, which introduced noiseto the data because gene names often have irregularlexical variants.5.3 Comparison with Regular Feature RankingMethods0.350.40.450.50.550.60.651  2  3  4  5  6  7  8  9  10F1bF+M?YDOMFIGCHIBLFigure 2: Comparison between regular feature rank-ing and generalizability-based feature ranking onF+M?Y3Our baseline method performed comparably to the state-of-the-art systems on the standard BioCreAtIvE 1A data.790.30.350.40.450.51  2  3  4  5  6  7  8  9  10F1bF+Y?MDOMFIGCHIBLFigure 3: Comparison between regular feature rank-ing and generalizability-based feature ranking onF+Y?M00.050.10.150.20.250.31  2  3  4  5  6  7  8  9  10F1bM+Y?FDOMFIGCHIBLFigure 4: Comparison between regular feature rank-ing and generalizability-based feature ranking onM+Y?FTo further understand how our method improvedthe performance, we compared the generalizability-based feature ranking method with the three regularfeature ranking methods, F, IG, and CHI, that wereused in the baseline method.
To make fair compar-ison, for the regular feature ranking methods, wealso used the rank-based prior transformation as de-scribed in Section 4 to incorporate the preference fortop-ranked features.
Figure 2, Figure 3 and Figure 4show the performance of different feature rankingmethods in the three sets of experiments as the pa-rameter b for the rank-based prior changes.
As wepointed out in Section 4, b is proportional to the log-arithm of the number of ?effective features?.From the figures, we clearly see that the curve forthe generalizability-based ranking method DOM isalways above the curves of the other methods, indi-cating that when the same amount of top features arebeing emphasized by the prior, the features selectedby DOM give better performance on a new domainthan the features selected by the other methods.
Thissuggests that the top-ranked features in DOM are in-deed more suitable for adaptation to new domainsthan the top features ranked by the other methods.The figures also show that the ranking methodDOM achieved better performance than the baselineover a wide range of b values, especially in F+Y?Mand M+Y?F, whereas for methods F, IG and CHI,the performance quickly converged to the baselineperformance as b increased.It is interesting to note the comparison between Fand IG (or CHI).
In general, when the test data issimilar to the training data, IG (or CHI) is advanta-geous over F (Yang and Pedersen, 1997).
However,in this case when the test domain is different fromthe training domains, F shows advantages for adap-tation.
A possible explanation is that frequent fea-tures are in general less likely to be domain-specific,and therefore feature frequency can also be used as acriterion to select generalizable features and to filterout domain-specific features, although it is still notas effective as the method we proposed.6 Related WorkThe NER problem has been extensively studied inthe NLP community.
Most existing work has fo-cused on supervised learning approaches, employ-ing models such as HMMs (Zhou and Su, 2002),MEMMs (Bender et al, 2003; Finkel et al, 2005),and CRFs (McCallum and Li, 2003).
Collins andSinger (1999) proposed an unsupervised method fornamed entity classification based on the idea of co-training.
Ando and Zhang (2005) proposed a semi-supervised learning method to exploit unlabeled datafor building more robust NER systems.
In all thesestudies, the evaluation is conducted on unlabeleddata similar to the labeled data.Recently there have been some studies on adapt-ing NER systems to new domains employing tech-niques such as active learning and semi-supervisedlearning (Shen et al, 2004; Mohit and Hwa, 2005),80or incorporating external lexical knowledge (Cia-ramita and Altun, 2005).
However, there has notbeen any study on exploiting the domain structurecontained in the training examples themselves tobuild generalizable NER systems.
We focus onthe domain structure in the training data to builda classifier that relies more on features generaliz-able across different domains to avoid overfitting thetraining domains.
As our method is orthogonal tomost of the aforementioned work, they can be com-bined to further improve the performance.7 Conclusion and Future WorkNamed entity recognition is an important problemthat can help many text mining and natural lan-guage processing tasks such as information extrac-tion and question answering.
Currently NER facesa poor domain adaptability problem when the testdata is not from the same domain as the trainingdata.
We present several strategies to exploit thedomain structure in the training data to improve theperformance of the learned NER classifier on a newdomain.
Our results show that the domain-awarestrategies we proposed improved the performanceover a baseline method that represents the state-of-the-art NER techniques.AcknowledgmentsThis work was in part supported by the NationalScience Foundation under award numbers 0425852,0347933, and 0428472.
We would like to thankBruce Schatz, Xin He, Qiaozhu Mei, Xu Ling, andsome other BeeSpace project members for usefuldiscussions.
We would like to thank Mark Sammonsfor his help with FEX.
We would also like to thankthe anonymous reviewers for their comments.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method for textchunking.
In Proceedings of ACL-2005.Oliver Bender, Franz Josef Och, and Hermann Ney.2003.
Maximum entropy models for named entityrecognition.
In Proceedings of CoNLL-2003.Andrew Borthwick.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. thesis,New York University.Stanley F. Chen and Ronald Rosenfeld.
1999.
A Gaus-sian prior for smoothing maximum entropy models.Technical Report CMU-CS-99-108, School of Com-puter Science, Carnegie Mellon University.Massimiliano Ciaramita and Yasemin Altun.
2005.Named-entity recognition in novel domains with ex-ternal lexical knowledge.
In Workshop on Advancesin Structured Learning for Text and Speech Processing(NIPS-2005).Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof EMNLP/VLC-1999.Jenny Finkel, Shipra Dingare, Christopher D. Manning,Malvina Nissim, Beatrice Alex, and Claire Grover.2005.
Exploring the boundaries: Gene and proteinidentification in biomedical text.
BMC Bioinformat-ics, 6(Suppl 1):S5.Radu Florian, Abe Ittycheriah, Hongyan Jing, and TongZhang.
2003.
Named entity recognition through clas-sifier combination.
In Proceedings of CoNLL-2003.Lynette Hirschman, Marc Colosimo, Alexander Morgan,and Alexander Yeh.
2005.
Overview of BioCreAtIvEtask 1B: normailized gene lists.
BMC Bioinformatics,6(Suppl 1):S11.Fei Huang and Stephan Vogel.
2002.
Improved namedentity translation and bilingual named entity extrac-tion.
In ICMI-2002.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recogni-tion with character-level models.
In Proceedings ofCoNLL-2003.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of CoNLL-2003.Behrang Mohit and Rebecca Hwa.
2005.
Syntax-basedsemi-supervised named entity tagging.
In Proceedingsof ACL-2005.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
2004.
Multi-criteria-based active learningfor named entity recognition.
In Proceedings of ACL-2004.Rohini Srihari and Wei Li.
1999.
Information extractionsupported question answering.
In TREC-8.Yiming Yang and Jan O. Pedersen.
1997.
A comparativestudy on feature selection in text categorization.
InProceedings of ICML-1997.Guodong Zhou and Jian Su.
2002.
Named entity recog-nition using an HMM-based chunk tagger.
In Proceed-ings of ACL-2002.81
