Proceedings of the ACL 2007 Demo and Poster Sessions, pages 117?120,Prague, June 2007. c?2007 Association for Computational LinguisticsLearning to Rank Definitions to Generate Quizzes for InteractiveInformation PresentationRyuichiro Higashinaka and Kohji Dohsaka and Hideki IsozakiNTT Communication Science Laboratories, NTT Corporation2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan{rh,dohsaka,isozaki}@cslab.kecl.ntt.co.jpAbstractThis paper proposes the idea of ranking def-initions of a person (a set of biographi-cal facts) to automatically generate ?Whois this??
quizzes.
The definitions are or-dered according to how difficult they makeit to name the person.
Such ranking wouldenable users to interactively learn about aperson through dialogue with a system withimproved understanding and lasting motiva-tion, which is useful for educational sys-tems.
In our approach, we train a rankerthat learns from data the appropriate rankingof definitions based on features that encodethe importance of keywords in a definitionas well as its content.
Experimental resultsshow that our approach is significantly betterin ranking definitions than baselines that useconventional information retrieval measuressuch as tf*idf and pointwisemutual informa-tion (PMI).1 IntroductionAppropriate ranking of sentences is important, asnoted in sentence ordering tasks (Lapata, 2003), ineffectively delivering content.
Whether the task isto convey news texts or definitions, the objective isto make it easier for users to understand the content.However, just conveying it in an encyclopedia-likeor temporal order may not be the best solution, con-sidering that interaction between a system and a userimproves understanding (Sugiyama et al, 1999) andthat the cognitive load in receiving information is be-lieved to correlate with memory fixation (Craik andLockhart, 1972).In this paper, we discuss the idea of ranking defi-nitions as a way to present people?s biographical in-formation to users, and propose ranking definitionsto automatically generate a ?Who is this??
quiz.Here, we use the term ?definitions of a person?
tomean a short series of biographical facts (See Fig.
1).The definitions are ordered according to how diffi-cult they make it to name the person.
The rankingalso enables users to easily come up with answercandidates.
The definitions are presented to usersone by one as hints until users give the correct name(See Fig.
2).
Although the interaction would taketime, we could expect improved understanding ofpeople?s biographical information by users throughtheir deliberation and the long lasting motivation af-forded by the entertaining nature of quizzes, whichis important in tutorial tasks (Baylor and Ryu, 2003).Previous work on definition ranking has usedmeasures such as tf*idf (Xu et al, 2004) or rankingmodels trained to encode the likelihood of a defini-tion being good (Xu et al, 2005).
However, suchmeasures/models may not be suitable for quiz-styleranking.
For example, a definition having a strongco-occurrence with a person may not be an easy hintwhen it is about a very minor detail.
Certain de-scriptions, such as a person?s birthplace, would haveto come early so that users can easily start guessingwho the person is.
In our approach, we train a rankerthat learns from data the appropriate ranking of def-initions.
Note that we only focus on the ranking ofdefinitions and not on the interaction with users inthis paper.
We also assume that the definitions to beranked are given.Section 2 describes the task of ranking definitions,and Section 3 describes our approach.
Section 4 de-scribes our collection of ranking data and the rank-ing model training using the ranking support vectormachine (SVM), and Section 5 presents the evalu-ation results.
Section 6 summarizes and mentionsfuture work.2 Ranking Definitions for QuizzesFigure 1 shows a list of definitions of NatsumeSoseki, a famous Japanese novelist, in their originalranking at the encyclopedic website goo (http://dic-tionary.goo.ne.jp/) and in the quiz-style ranking weaim to achieve.
Such a ranking would realize a dia-logue like that in Fig.
2.
At the end of the dialogue,the user would be able to associate the person andthe definitions better, and it is expected that somenew facts could be learned about that person.117Original Ranking:1.
Novelist and scholar of British literature.2.
Real name: Kinnosuke.3.
Born in Ushigome, Edo.4.
Graduated from the University of Tokyo.5.
Master of early-modern literature along with Mori Ogai.6.
After the success of ?I Am a Cat?, quit all teaching jobs and joinedAsahi Shimbun.7.
Published masterpieces in Asahi Shimbun.8.
Familiar with Haiku, Chinese poetry, and calligraphy.9.
Works include ?Botchan?, ?Sanshiro?, etc.
?Quiz-style Ranking:1.
Graduated from the University of Tokyo.2.
Born in Ushigome, Edo.3.
Novelist and scholar of British literature.4.
Familiar with Haiku, Chinese poetry, and calligraphy.5.
Published masterpieces in Asahi Shimbun.6.
Real name: Kinnosuke.7.
Master of early-modern literature along with Mori Ogai.8.
After the success of ?I Am a Cat?, quit all teaching jobs and joinedAsahi Shimbun.9.
Works include ?Botchan?, ?Sanshiro?, etc.Figure 1: List of definitions of Natsume Soseki, afamous Japanese novelist, in their original ranking inthe encyclopedia and in the quiz-style ranking.
Thedefinitions were translated by the authors.Ranking definitions is closely related to defini-tional question answering and sentence orderingin multi-document summarization.
In definitionalquestion answering, measures related to informationretrieval (IR), such as tf*idf or pointwise mutual in-formation (PMI), have been used to rank sentencesor information nuggets (Xu et al, 2004; Sun et al,2005).
Such measures are used under the assump-tion that outstanding/co-occurring keywords about adefiniendum characterize that definiendum.
How-ever, this assumptionmay not be appropriate in quiz-style ranking; most content words in the definitionsare already important in the IR sense, and strong co-occurrence may not guarantee high ranks for hintsto be presented later because the hint can be too spe-cific.
An approach to creating a ranking model ofdefinitions in a supervised manner using machinelearning techniques has been reported (Xu et al,2005).
However, the model is only used to distin-guish definitions from non-definitions on the basisof features related mainly to linguistic styles.In multi-document summarization, the focus hasbeen mainly on creating cohesive texts.
(Lapata,2003) uses the probability of words in adjacent sen-tences as constraints to maximize the coherence ofall sentence-pairs in texts.
Although we acknowl-edge that having cohesive definitions is important,since we are not creating a single text and the dia-logue that we aim to achieve would involve frequentuser/system interaction (Fig.
2), we do not deal withthe coherence of definitions in this paper. S1 Who is this?
First hint: Graduated from theUniversity of Tokyo.U1 Yoshida Shigeru?S2 No, not even close!
Second hint: Born inUshigome, Edo.U2 I don?t know.S3 OK. Third hint: Novelist and scholar ofBritish literature.U3 Murakami Haruki?S4 Close!
Fourth hint: Familiar with Haiku,Chinese poetry, and calligraphy.U4 Mori Ogai?S5 Very close!
Fifth hint: Published master-pieces in Asahi Shimbun.U5 Natsume Soseki?S6 That?s right! Figure 2: Example dialogue based on the quiz-styleranking of definitions.
S stands for a system utter-ance and U for a user utterance.3 ApproachSince it is difficult to know in advance what char-acteristics are important for quiz-style ranking, welearn the appropriate ranking of definitions fromdata.
The approach is the same as that of (Xu et al,2005) in that we adopt a machine learning approachfor definition ranking, but is different in that what islearned is a quiz-style ranking of sentences that arealready known to be good definitions.First, we collect ranking data.
For this purpose,we turn to existing encyclopedias for concise biogra-phies.
Then, we annotate the ranking.
Secondly, wedevise a set of features for a definition.
Since theexistence of keywords that have high scores in IR-related measures may suggest easy hints, we incor-porate the scores of IR-related measures as features(IR-related features).Certain words tend to appear before or after oth-ers in a biographical document to convey particularinformation about people (e.g., words describing oc-cupations at the beginning; those describing worksat the end, etc.)
Therefore, we use word positionswithin the biography of the person in question asfeatures (positional features).
Biographies can befound in online resources, such as biography.com(http://www.biography.com/) and Wikipedia.
In ad-dition, to focus on the particular content of the def-inition, we use bag-of-words (BOW) features, to-gether with semantic features (e.g., semantic cate-gories in Nihongo Goi-Taikei (Ikehara et al, 1997)or word senses in WordNet) to complement thesparseness of BOW features.
We describe the fea-tures we created in Section 4.2.
Finally, we createa ranking model using a preference learning algo-118rithm, such as the ranking SVM (Joachims, 2002),which learns ranking by reducing the pairwise rank-ing error.4 Experiment4.1 Data CollectionWe collected biographies (in Japanese) from the gooencyclopedia.
We first mined Wikipedia to calcu-late the PageRankTMof people using the hyper-linkstructure.
After sorting them in descending order bythe PageRank score, we extracted the top-150 peo-ple for whom we could find an entry in the goo en-cyclopedia.
Then, 11 annotators annotated rankingsfor each of the 150 people individually.
The annota-tors were instructed to rank the definitions assumingthat they were creating a ?who is this??
quiz; i.e.,to place the definition that is the most characteris-tic of the person in question at the end.
The meanof the Kendall?s coefficients of concordance for the150 people was sufficiently high at 0.76 with a stan-dard deviation of 0.13.
Finally, taking the means ofranks given to each definition, we merged the indi-vidual rankings to create the reference rankings.
Anexample of a reference ranking is the bottom one inFig.
1.
There are 958 definition sentences in all, witheach person having approximately 6?7 definitions.4.2 Deriving FeaturesWe derived our IR-related features based onMainichi newspaper articles (1991?2004) andWikipedia articles.
We used these two differentsources to take into account the difference in theimportance of terms depending on the text.
Wealso used sentences, sections (for Wikipedia arti-cles only) and documents as units to calculate doc-ument frequency, which resulted in the creation offive frequency tables: (i) Mainichi-Document, (ii)Mainichi-Sentence, (iii) Wikipedia-Document, (iv)Wikipedia-Section, and (v) Wikipedia-Sentence.Using the five frequency tables, we calculated, foreach content word (nouns, verbs, adjectives, and un-known words) in the definition, (1) frequency (thenumber of documents where the word is found), (2)relative frequency (frequency divided by the maxi-mum number of documents), (3) co-occurrence fre-quency (the number of documents where both theword and the person?s name are found), (4) rela-tive co-occurrence frequency, and (5) PMI.
Then, wetook the minimum, maximum, and mean values of(1)?
(5) for all content words in the definition as fea-tures, deriving 75 (5 ?
5 ?
3) features.
Then, usingthe Wikipedia article (called an entry) for the personin question, we calculated (1)?
(4) within the entry,and calculated tf*idf scores of words in the defini-tion using the term frequency in the entry.
Again, bytaking the minimum, maximum, and mean values of(1)?
(4) and tf*idf, we yielded 15 (5 ?
3) features,for a total of 90 (75 + 15) IR-related features.Positional features were derived also using theWikipedia entry.
For each word in the definition, wecalculated (a) the number of times the word appearsin the entry, (b) the minimum position of the word inthe entry, (c) its maximum position, (d) its mean po-sition, and (e) the standard deviation of the positions.Note that positions are either ordinal or relative; i.e.,the relative position is calculated by dividing the or-dinal position by the total number of words in theentry.
Then, we took the minimum, maximum, andmean values of (a)?
(e) for all content words in thedefinition as features, deriving 30 (5 ?
2 (ordinal orrelative positions)?
3) features.For the BOW features, we first parsed all ourdefinitions with CaboCha (a Japanese morphologi-cal/dependency parser, http://chasen.org/?taku/soft-ware/cabocha/) and extracted all content words tomake binary features representing the existence ofeach content word.
There are 2,156 BOW featuresin our data.As for the semantic features, we used the seman-tic categories in NihongoGoi-Taikei.
Since there are2,715 semantic categories, we created 2,715 featuresrepresenting the existence of each semantic categoryin the definition.
Semantic categories were assignedto words in the definition by a morphological ana-lyzer that comes with ALT/J-E, a Japanese-Englishmachine translation system (Ikehara et al, 1991).In total, we have 4,991 features to represent eachdefinition.
We calculated all feature values for alldefinitions in our data to be used for the learning.4.3 Training Ranking ModelsUsing the reference ranking data, we trained a rank-ing model using the ranking SVM (Joachims, 2002)(with a linear kernel) that minimizes the pairwiseranking error among the definitions of each person.5 EvaluationTo evaluate the performance of the ranking model,following (Xu et al, 2004; Sun et al, 2005), wecompared it with baselines that use only the scoresof IR-related and positional features for ranking, i.e.,sorting.
Table 1 shows the performance of the rank-ing model (by the leave-one-out method, predictingthe ranking of definitions of a person by other peo-119Rank Description Ranking Error1 Proposed ranking model 0.1852 Wikipedia-Sentence-PMI-max 0.2993 Wikipedia-Section-PMI-max 0.3094 Wikipedia-Document-PMI-max 0.3125 Mainichi-Sentence-PMI-max 0.3186 Mainichi-Document-PMI-max 0.3257 Mainichi-Sentence-relative-co-occurrence-max 0.3388 Wikipedia-Entry-ordinal-Min-max 0.3389 Wikipedia-Sentence-relative-co-occurrence-max 0.33910 Wikipedia-Entry-relative-Min-max 0.34011 Wikipedia-Entry-ordinal-Mean-mean 0.342Table 1: Performance of the proposed rankingmodeland that of 10 best-performing baselines.ple?s rankings) and that of the 10 best-performingbaselines.
The ranking error is pairwise ranking er-ror; i.e., the rate of misordered pairs.
A descrip-tive name is given for each baseline.
For example,Wikipedia-Sentence-PMI-max means that we usedthe maximum PMI values of content words in thedefinition calculated from Wikipedia, with sentenceas the unit for obtaining frequencies.Our ranking model outperforms all of the base-lines.
McNemar?s test showed that the difference be-tween the proposed model and the best-performingbaseline is significant (p<0.00001).
The results alsoshow that PMI is more effective in quiz-style rank-ing than any other measure.
The fact that max is im-portant probably means that the mere existence of aword that has a high PMI score is enough to raise theranking of a hint.
It is also interesting thatWikipediagives better ranking, which is probably because peo-ple?s names and related keywords are close to eachother in such descriptive texts.Analyzing the ranking model trained by the rank-ing SVM allows us to calculate the weights given tothe features (Hirao et al, 2002).
Table 2 shows thetop-10 features in weights in absolute figures whenall samples were used for training.
It can be seenthat high PMI values and words/semantic categoriesrelated to government or creation lead to easy hints,whereas semantic categories, such as birth and oth-ers (corresponding to the person in ?a person fromTokyo?
), lead to early hints.
This supports our in-tuitive notion that birthplaces should be presentedearly for users to start thinking about a person.6 Summary and Future WorkThis paper proposed ranking definitions of a personto automatically generate a ?Who is this??
quiz.Using reference ranking data that we created man-ually, we trained a ranking model using a rankingSVM based on features that encode the importanceof keywords in a definition as well as its content.Rank Feature Name Weight1 Wikipedia-Sentence-PMI-max 0.7232 SemCat:33 (others/someone) -0.5593 SemCat:186 (creator) 0.4854 BOW:bakufu (feudal government) 0.4515 SemCat:163 (sovereign/ruler/monarch) 0.4226 Wikipedia-Document-PMI-max 0.4097 SemCat:2391 (birth) -0.4048 Wikipedia-Section-PMI-max 0.4029 SemCat:2595 (unit; e.g., numeral classifier) 0.37410 SemCat:2606 (plural; e.g., plural form) -0.368Table 2: Weights of features learned for ranking def-initions by the ranking SVM.
SemCat denotes it isa semantic-category feature with its semantic cate-gory ID followed by the description of the categoryin parentheses.
BOW denotes a BOW feature.Experimental results show that our ranking modelsignificantly outperforms baselines that use singleIR-related and positional measures for ranking.
Weare currently in the process of building a dialoguesystem that uses the quiz-style ranking for definitionpresentation.
We are planning to examine how thedifferent rankings affect the understanding and mo-tivation of users.ReferencesAmy Baylor and Jeeheon Ryu.
2003.
Does the presence ofimage and animation enhance pedagogical agent persona?Journal of Educational Computing Research, 28(4):373?395.Fergus I. M. Craik and Robert S. Lockhart.
1972.
Levels ofprocessing: A framework for memory research.
Journal ofVerbal Learning and Verbal Behavior, 11:671?684.Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Mat-sumoto.
2002.
Extracting important sentences with supportvector machines.
In Proc.
19th COLING, pages 342?348.Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and HiromiNakaiwa.
1991.
Toward an MT system without pre-editing?Effects of new methods in ALT-J/E?.
In Proc.
Third Ma-chine Translation Summit: MT Summit III, pages 101?106.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei ?
AJapanese Lexicon.
Iwanami Shoten.Thorsten Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proc.
KDD, pages 133?142.Mirella Lapata.
2003.
Probabilistic text structuring: Exper-iments with sentence ordering.
In Proc.
41st ACL, pages545?552.Akira Sugiyama, Kohji Dohsaka, and Takeshi Kawabata.
1999.A method for conveying the contents of written texts by spo-ken dialogue.
In Proc.
PACLING, pages 54?66.Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng Chua,and Min-Yen Kan. 2005.
Using syntactic and semantic rela-tion analysis in question answering.
In Proc.
TREC.Jinxi Xu, Ralph Weischedel, and Ana Licuanan.
2004.
Eval-uation of an extraction-based approach to answering defini-tional questions.
In Proc.
SIGIR, pages 418?424.Jun Xu, Yunbo Cao, Hang Li, and Min Zhao.
2005.
Rank-ing definitions with supervised learning methods.
In Proc.WWW, pages 811?819.120
