Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 82?90,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Mining of Lexical Variants from Noisy TextStephan Gouws?, Dirk Hovy and Donald Metzlerstephan@ml.sun.ac.za, {dirkh, metzler}@isi.eduUSC Information Sciences InstituteMarina del Rey, CA90292, USAAbstractThe amount of data produced in user-generated content continues to grow at a stag-gering rate.
However, the text found in thesemedia can deviate wildly from the standardrules of orthography, syntax and even seman-tics and present significant problems to down-stream applications which make use of thisnoisy data.
In this paper we present a novelunsupervised method for extracting domain-specific lexical variants given a large volumeof text.
We demonstrate the utility of thismethod by applying it to normalize text mes-sages found in the online social media service,Twitter, into their most likely standard Englishversions.
Our method yields a 20% reductionin word error rate over an existing state-of-the-art approach.1 IntroductionThe amount of data produced in user-generated con-tent, e.g.
in online social media, and from machine-generated sources such as optical character recog-nition (OCR) and automatic speech recognition(ASR), surpasses that found in more traditional me-dia by orders of magnitude and continues to growat a staggering rate.
However, the text found inthese media can deviate wildly from the standardrules of orthography, syntax and even semantics andpresent significant problems to downstream applica-tions which make use of this ?noisy?
data.
In social?This work was done while the first author was a visitingstudent at ISI from the MIH Media Lab at Stellenbosch Univer-sity, South Africa.media this noise might result from the need for so-cial identity, simple spelling errors due to high in-put cost associated with the device (e.g.
typing ona mobile phone), space constraints imposed by thespecific medium or even a user?s location (Gouws etal., 2011).
In machine-generated texts, noise mightresult from imperfect inputs, imperfect conversionalgorithms, or various degrees of each.Recently, several works have looked at the pro-cess of normalizing these ?noisy?
types of text intomore standard English, or in other words, to convertthe various forms of idiosyncratic spelling and writ-ing errors found in these media into what would nor-mally be considered standard English orthography.Many of these works rely on supervised methodswhich share the common burden of requiring train-ing data in the form of noisy input and clean outputpairs.
The problem with developing large amountsof annotated training data is that it is costly and re-quires annotators with sufficient expertise.
However,the volume of data that is available in these mediamakes this a suitable domain for applying semi- andeven fully unsupervised methods.One interesting observation is that these noisyout-of-vocabulary (OOV) words are typicallyformed through some semi-deterministic processwhich doesn?t render them completely indiscernibleat a lexical level from the original words they aremeant to represent.
We therefore refer to these OOVtokens as lexical variants of the clean in-vocabulary(IV) tokens they are derived from.
For instance,in social media ?2morrow?
?2morow?
and ?2mrw?still share at least some lexical resemblance with?tomorrow?, due to the fact that it is mainly the82Figure 1: A plot of the OOV distribution found in Twit-ter.
Also indicated is the potential for using (OOV,most-likely-IV) training pairs found on this curve for eitherexception dictionary entries (the most frequent pairs),or for learning lexical transformations (the long tail).The threshold between the two (vertical bar) is domain-specific.result of a phonetic transliteration procedure.
Also,?computer?
and ?conpu7er?
share strong lexicaloverlap, and might be the result of noise in the OCRprocess.As with many aspects of NLP, the distribution ofthese OOV tokens resemble a power law distribution(see Figure 1 for the OOV distribution in Twitter).Thus, some words are commonly converted to someOOV representation (e.g.
domain-specific abbrevia-tions in social media, or words which are commonlyincorrectly detected in OCR) and these account formost of the errors, with the rest making up the longtail.
If one could somehow automatically extract alist of all the domain-specific OOV tokens found ina collection of texts, along with the most likely cleanword (or words) each represents, then this could playa key role in for instance normalizing individualmessages.
Very frequent (noisy, clean) pairs at thehead of the distribution could be used for extractingcommon domain-specific abbreviations, and word-pairs in the long tail may be used as input to learn-ing algorithms for automatically learning the typesof transformations found in these media, as shownin Figure 1.For example, taking Twitter as our target domain,examples for learning common exception pairs mayinclude ?gf ???girlfriend?.
For learning types of lex-ical transformations, one might learn from ?think-ing???thinkin?
and ?walking???walkin?
that ?ng?could go to ?n?
(known as ?g-clipping?
).In this paper we present a novel unsupervisedmethod for extracting an approximation to such adomain-specific list of (noisy, clean) pairs, givenonly a large volume of representative text.
We fur-thermore demonstrate the utility of this method byapplying it to normalize text messages found in theonline social media service, Twitter, into their mostlikely standard English versions.The primary contributions of this paper are:?
We present an unsupervised method that mines(noisy, clean) pairs and requires only largeamounts of domain-specific noisy data?
We demonstrate the utility of this method by in-corporating it into a standard method for noisytext normalization, which results in a signifi-cant reduction in the word error rate comparedto the original method.2 Training Pair MiningGiven a large corpus of noisy text, our challenge is toautomatically mine pairs of domain-specific lexicalvariants that can be used as training data for a va-riety of natural language processing tasks.
The keychallenge is how to develop an effective approachthat is both domain-specific and robust to noisy cor-pora.
Our proposed approach requires nothing morethan a large ?common English?
corpus (e.g., a largenewswire corpus) and a large corpus of domain text(e.g., a large corpus of Twitter data, a query log,OCR output, etc.).
Using these two sources of ev-idence, the approach mines domain-specific lexicalvariants in a fully unsupervised manner.Before describing the details of our approach, wefirst describe the characteristics that we would likethe mined lexical variants to have.
First, the variantsshould be semantically related to each other.
Pairsof words that are lexically similar, but semanticallyunrelated are not of particular interest since suchpairs can be found using basic edit distance-basedapproaches.
Second, the variants should be domain-specific.
Variants that capture common English lexi-cal variations (e.g., ?running?
and ?run?)
can be cap-tured using standard normalization procedures, such83Figure 2: Flow chart illustrating our procedure for miningpairs of lexical variants.as stemming.
Instead, we are interested in identify-ing domain-specific variations (e.g., ?u?
and ?you?in the SMS and Twitter domains) that cannot eas-ily be handled by existing approaches.
Finally, thevariants should be lexically similar, by definition.Hence, ideal variants will be domain-specific, lex-ically similar, and semantically related.To mine such variants we synthesize ideas fromnatural language processing and large-scale textmining to derive a novel mining procedure.
Our pro-cedure can be divided into three atomic steps.
Firstwe identify semantically similar pairs, then we filterout common English variants, and finally we rescorethe resulting list based on lexical similarity (see Fig-ure 2).
The remainder of this section describes thecomplete details of each of these steps.2.1 Identifying Semantically Similar PairsThe first step of our mining procedure harvests se-mantically similar pairs of terms from both the com-mon English corpus and the domain corpus.
Thereare many different ways to measure semantic relat-edness.
In this work, we use distributional similar-ity as our measure of semantic similarity.
However,since we are taking a fully unsupervised approach,we do not know a priori which pairs of terms maybe related to each other.
Hence, we must computethe semantic similarity between all possible pairs ofterms within the lexicon.
To solve this computa-tionally challenging task, we use a large-scale all-pairs distributional similarity approach similar to theone originally proposed by Pasca and Dienes (Pascaand Dienes, 2005).
Our implementation, whichmakes use of Hadoop?s MapReduce distributed pro-gramming paradigm, can efficiently compute all-pairs distributional similarity over very large corpora(e.g., the Twitter pairs we use later were mined froma corpus of half a billion Twitter messages).Using a similar strategy as Pasca and Dienes, wedefine term contexts as the bigrams that appear tothe left and to the right of a given word (Pasca andDienes, 2005).
Following standard practice, the con-textual vectors are weighted according to pointwisemutual information and the similarity between thevectors is computed using the cosine similarity met-ric (Lin and Pantel, 2001; Bhagat and Ravichandran,2008).
It is important to note that there are manyother possible ways to compute distributional andsemantic similarity, and that just about any approachcan be used within our framework.
The approachused here was chosen because we had an existingimplementation.
Indeed, other approaches may bemore apt for other data sets and tasks.This approach is applied to both the common En-glish corpus and the domain corpus.
This yields twosets of semantically (distributionally) similar wordpairs that will ultimately be used to distill unsuper-vised lexical variants.2.2 Filtering Common English VariantsGiven these two sets of semantically similar wordpairs, the next step in our procedure is designed toidentify the domain-specific pairs by filtering out thecommon English variants.
The procedure that wefollow is very simple, yet highly effective.
Giventhe semantically similar word pairs harvested fromthe domain corpus, we eliminate all of the pairs thatare also found in the semantically similar commonEnglish pairs.Any type of ?common English?
corpus can beused for this purpose, depending on the task.
How-ever, we found that a large corpus of newswire ar-ticles tends to work well.
Most of the semanti-cally similar word pairs harvested from such a cor-pus are common lexical variants and synonyms.
Byeliminating these common variants from the har-vested domain corpus pairs, we are left with onlythe domain-specific semantically similar word pairs.2.3 Lexical Similarity-Based Re-orderingThe first step of our mining procedure identifiedsemantically similar term pairs using distributionalsimilarity, while the second identified those thatwere domain-specific by filtering out common En-glish variants.
The third, and final, step of our pro-cedure re-orders the output of the second step to ac-count for lexical similarity.For each word pair (from the second step of ourprocedure), we compute two scores: 1) a seman-84tic similarity score, and 2) a lexical similarity score.The final score of the pair is then simply the prod-uct of the two scores.
In this work, we use thecosine similarity score as our semantic similarityscore, since it is already computed during the firststep of our procedure.In the social media domain, as in the mobile tex-ting domain, compressed writing schemes typicallyinvolve deleting characters or replacing one or morecharacters with some other characters.
For example,users might delete vowels (?tomorrow???tmrrw?
),or replace ?ph?
with its phonetic equivalent ?f ?,as in ?phone???fone?.
We make use of a subse-quence similarity function (Lodhi et al, 2002) whichcan still capture the structural overlap (in the formof string subsequences) between the remaining un-changed letters in the noisy word and the originalclean word from which it was derived.
In this workwe use a subsequence length of 2, but as with theother steps in our procedure, this one is purpose-fully defined in a general way.
Any semantic sim-ilarity score, lexical similarity score, and combina-tion function can be used in practice.The output of the entire procedure is a scored listof word pairs that are semantically related, domain-specific, and lexically similar, thereby exhibiting thecharacteristics that we initially defined as important.We treat these (scored) pairs as pseudo training datathat has been derived in a fully unsupervised manner.We anticipate that these pairs will serve as powerfultraining data for a variety of tasks, such as noisy textnormalization, which we will return to in Section 3.2.4 Example and Error AnalysisAs an illustrative example of this procedure in prac-tice, Table 1 shows the actual output of our systemfor each step of the mining procedure.
To generatethis example, we used a corpus of 2GB of Englishnews articles as our ?common English?
corpus anda corpus of approximately 500 million Twitter mes-sages as our domain corpus.
In this way, our goalis to identify Twitter-specific lexical variants, whichwe will use in the next section to normalize noisyTwitter messages.Column (A) of the table shows that our distribu-tional similarity approach is capable of identifyinga variety of semantically similar terms in the Twit-ter corpus.
However, the list contains a large num-Rank PrecisionP@50 0.90P@100 0.88Table 2: Precision at 50 and 100 of the induced exceptiondictionary.ber of common English variants that are not spe-cific to Twitter.
Column (B) shows the outcome ofeliminating all of the pairs that were found in thenewswire corpus.
Many of the common pairs havebeen eliminated and the list now contains mostlyTwitter-specific variants.
Finally, Column (C) showsthe result of re-ordering the domain-specific pairs toaccount for lexical similarity.In our specific case, the output of step 1 yieldeda list of roughly 3.3M potential word variants.
Fil-tering out common English variants reduced this toabout 314K pairs.
In order to estimate the quality ofthe list we computed the precision at 50 and at 100for which the results are shown in Table 2.
Further-more, we find that up to position 500 the pairs arestill of reasonable quality.
Thereafter, the number oferrors start to increase noticeably.
In particular, wefind that the most common types of errors are1.
Number-related: e.g.
?30?
and ?30pm?
(due toincorrect tokenization), or ?5800?
and ?5530?;2.
Lemma-related: e.g.
?incorrect?
and ?incor-rectly?
; and3.
Negations: e.g.
?could?
and ?couldnt?.Performance can thus be improved by makinguse of better tokenization, lemmatizing words, fil-tering out common negations and filtering out pairsof numbers.Still, the resulting pairs satisfy all of our de-sired qualities rather well, and hence we hypothesizewould serve as useful training data for a number ofdifferent Twitter-related natural language processingtasks.
Indeed, we will now describe one such possi-ble application and empirically validate the utility ofthe automatically mined pairs.85(A) (B) (C)i?
you u?
you ur?
yourmy?
the seeking?
seeks wit?
withu?
you 2?
to to?
toois?
was lost?
won goin?
goinga?
the q?
que kno?
knowi?
we f*ck?
hell about?
boutmy?
your feat?
ft wat?
whatand?
but bday?
birthday jus?
justseeking?
seeks ff?
followfriday talkin?
talkingme?
you yang?
yg gettin?
getting2?
to wit?
with doin?
doingam?
was a?
my so?
sooare?
were are?
r you?
yourlost?
won amazing?
awesome dnt?
donthe?
she til?
till bday?
birthdayq?
que fav?
favorite nothin?
nothingit?
that mostly?
partly people?
pplf*ck?
hell northbound?
southbound lil?
littlecan?
could hung?
toned sayin?
sayingim?
its love?
miss so?
soooTable 1: Column (A) shows the highest weighted distributionally similar terms harvested from a large Twitter corpus.Column (B) shows which pairs from (A) remain after filtering out distributionally similar word pairs mined from alarge news corpus.
Column (C) shows the effect of reordering the pairs from (B) using a string similarity kernel.3 Deriving A Common ExceptionDictionary for Text Normalization as aUse Case for Mining Lexical VariantsAs discussed in Section 1, these training pairs mayaid methods which attempt to normalize noisy textby translating from the ill-formed text into stan-dard English.
Since the OOV distribution in noisytext mostly resemble a power law distribution (seeFigure 1), one may use the highest scoring train-ing pairs to induce ?exception dictionaries?
(lists of(noisy word)?
(most likely clean word)) of the mostcommon domain-specific abbreviations found in thetext.We will demonstrate the utility of our derivedpairs in one specific use case, namely inducing adomain-specific exception dictionary to augment avanilla normalization method.
We leave the sec-ond proposed use-case, namely using pairs in thelong tail for learning transformation rules, for futurework.We evaluate the first use case in Section 4.3.1 Baseline Normalization MethodWe make use of a competitive heuristic text nor-malization method over Twitter data as a baseline,and compare its accuracy to an augmented methodwhich makes use of an automatically induced excep-tion dictionary (using the method described in Sec-tion 2) as a first step, before resorting to the samebaseline method as a ?back-off?
for words not foundin the dictionary.As we point out in Section 5, there are variousmetaphors within which the noisy text normalizationproblem has been approached.
In general, however,the problem of noisy text normalization may be ap-proached by using a three step process (Gouws et al,2011):1.
In the out-of-vocabulary (OOV) detectionstep, we detect unknown words which are can-didates for normalization2.
In the candidate selection step, we find theweighted lists of most likely candidates (froma list of in-vocabulary (IV) words) for the OOVwords and group them into a confusion set.
The86confusion sets are then appended to one anotherto create a confusion- network or lattice3.
Finally, in the decoding step, we use a lan-guage model to rescore the confusion network,and then find the most likely posterior path(Viterbi path) through this network.The words at each node in the resulting posteriorViterbi path represents the words of the hypothe-sized original clean sentence.In this work, we reimplement the method de-scribed in Contractor (2010) as our baseline method.We next describe the details of this method in thecontext of the framework presented above.
See(Gouws et al, 2011) for more details.OOV DETECTION is a crucial part of the nor-malizaton process, since false-positives will resultin undesirable attempts to ?correct?
IV words, hencebringing down the method?s accuracy.
We imple-ment OOV detection as a simple lexicon-lookup pro-cedure, with heuristics for handling specific out-of-vocabulary-but-valid tokens such as hash tags and@usernames.CANDIDATE SELECTION involves comparingan unknown OOV word to a list of words whichare deemed in-vocabulary, and producing a top-Kranked list with candidate words and their estimatedprobabilities of relevance as output.
This process re-quires a function with which to compute the simi-larity or alternatively, distance, between two words.More traditional string-similarity functions like thesimple Lehvenshtein string edit distance do not faretoo well in this domain.We implement the IBM-similarity (Contractor etal., 2010) which employs a slightly more advancedsimilarity function.
It finds the length of the longestcommon subsequence (LCS) between two strings s1and s2, normalized by the edit distance (ED) be-tween the consonants in each string (referred to asthe ?consonant skeleton?
(CS)), thussim(s1, s2) =LCS(s1, s2)ED(CS(s1),CS(s2))Finally, the DECODING step takes an input wordlattice (lattice of concatenated, weighted confusionsets), and produces a new lattice by incorporatingthe probabilities from an n-gram language modelwith the prior probabilities in the lattice to produce areranked posterior lattice.
The most likely (Viterbi)path through this lattice represents the decoded cleanoutput.
We use SRI-LM (Stolcke, 2002) for this.3.2 Augmenting the Baseline: Our MethodIn order to demonstrate the utility of the mined lex-ical variant pairs, we first construct a (noisy, clean)lookup table from the mined pairs.
We (arbitrarily)use the 50 mined pairs with the highest overall com-bined score (see Section 2.3) for the exception dic-tionary.
For each pair, we map the OOV term (noisyand typically shorter) to the IV term (clean and usu-ally longer).
The exception lookup list is then usedto augment the baseline method (see Section 3.1) inthe following way: When the method encounters anew word, it first checks to see if the word is in theexception dictionary.
If it is, we normalize to thevalue in the dictionary.
If it is not, we pass the ill-formed word to the baseline method to proceed asnormal.4 Evaluation4.1 DatasetWe make use of the Twitter dataset discussed inHan (2011).
It consists of a random sampling of 549English tweets, annotated by three independent an-notators.
All OOV words were pre-identified and theannotators were requested to determine the standardform (gold standard) for each ill-formed word.4.2 Evaluation MetricsIn this study, we are interested in measuring thequality of our mined training pairs by evaluating itsutility on an external task: Using the training pairsto induce a (noisy?clean) exception dictionary toaugment the working of a standard noisy text nor-malization system.
Hence, our focus is entirely onthe accuracy of the candidate selection procedure asdefined in Section 3.1.
We compute this accuracyin terms of the word error rate (WER), defined asthe number of token substitutions, insertions or dele-tions one has to make to turn the system output intothe gold standard, normalized by the total number oftokens in the output.
In order to remove the possi-ble bias introduced by our very basic OOV-detection87Method WER % ChangeNaive baseline 10.7% ?IBM-baseline 7.8% ?27.1%Our method 5.6% ?47.7%Table 3: Word error rate (WER, lower is better) resultsof our method against a naive baseline and the muchstronger IBM-baseline (Contractor et al, 2010).
We alsoshow the relative change in WER for our method and theIBM-baseline compared to the naive baseline.mechanism, we evaluate the output of all systemsonly on the oracle pairs.
Oracle pairs are defined asthe (input,system-output,gold) pairs where input andgold do not match.
In other words, we remove thepossible confounding impact of imperfect OOV de-tection on the accuracy of the normalization processby assuming a perfect OOV-detection step.4.3 Discussion of ResultsThe results of our experiments are displayed in Ta-ble 3.
It is important to note that the focus is noton achieving the best WER compared to other sys-tems (although we achieve very competitive scores),but to evaluate the added utility of integrating anexception dictionary which is based purely on themined (noisy, clean) pairs with an already competi-tive baseline method.The ?naive baseline?
shows the results if we makeno changes to the input tokens for all oracle pairs.Therefore it reflects the total level of errors that arepresent in the corpus.The IBM-method is seen to reduce the amount oferrors by a substantial 27.1%.
However, the aug-mented method results in a further 20.6% reductionin errors, for a total reduction of 47.7% of all er-rors in the dataset, compared to the IBM-baseline?s27.1%.Since we replace matches in the dictionary indis-criminately, and since the dictionary comprise thosepairs that typically occur most frequently in the cor-pus from which they were mined, it is important tonote that if these pairs are of poor quality, then theirsheer frequency will drive the overall system accu-racy down.
Therefore, the accuracy of these pairsare strongly reflected in the WER performance ofthe augmented method.Noisy Clean % Oracle Pairsu you 8.7n and 1.4ppl people 1da the 1w with 0.7cuz because 0.5y why 0.5yu you 0.5lil little 0.5dat that 0.5wat what 0.4tha the 0.4kno know 0.4r are 0.4Table 4: Error analysis for all (noisy, clean) normaliza-tions missed by the vanilla IBM-baseline method, but in-cluded in the top-50 pairs used for constructing the ex-ception dictionary.
We also show the percentage of alloracle pairs that are corrected by including each pair inan exception dictionary.Table 4 shows the errors missed by the IBM-baseline, but contained in the mined exception dic-tionary.
We also show each pair?s frequency of oc-currence in the oracle pairs (hence its contributiontowards lowering WER).5 Related workTo the best of our knowledge, we are the first to ad-dress the problem of mining pairs of lexical variantsfrom noisy text in an unsupervised and purely sta-tistical manner that does not require aligned noisyand clean messages.
To obtain aligned clean andnoisy text without annotated data implies the useof some normalizing method first.
Yvon (2010)presents one such approach, where they generate ex-ception dictionaries from their finite-state system?snormalized output.
However, their method is stilltrained on annotated training pairs, and hence su-pervised.
A related direction is ?transliteration min-ing?
(Jiampojamarn et al, 2010) which aims to au-tomatically obtain bilingual lists of names written indifferent scripts.
They also employ string-similaritymeasures to find similar string pairs written in differ-ent scripts.
However, their input data is constrained88to Wikipedia articles written in different languages,whereas we impose no constrains on our input data,and merely require a large collection thereof.Noisy text normalization, on the other hand, hasrecently received a lot of focus.
Most works con-strue the problem in the metaphors of either ma-chine translation (MT) (Bangalore et al, 2002;Aw et al, 2006; Kaufmann and Kalita, 2010),spelling correction (Choudhury et al, 2007; Cookand Stevenson, 2009), or automated speech recog-nition (ASR) (Kobus et al, 2008).
For our evalua-tion, we developed an implementation of Contrac-tor (2010) which works on the same general ap-proach as Han (2011).6 Conclusions and Future WorkThe ability to automatically extract lexical variantsfrom large noisy corpora has many practical appli-cations, including noisy text normalization, queryspelling suggestion, fixing OCR errors, and so on.This paper developed a novel methodology for au-tomatically mining such pairs from a large domain-specific corpus.
The approach makes use of distri-butional similarity for measuring semantic similar-ity, a novel approach for filtering common Englishpairs by comparing against pairs mined from a largenews corpus, and a substring similarity measure forre-ordering the pairs according to their lexical simi-larity.To demonstrate the utility of the method, we usedautomatically mined pairs to construct an unsuper-vised exception dictionary, that was used in con-junction with a string similarity measure, to forma highly effective hybrid noisy text normalizationtechnique.
By exploiting the properties of the powerlaw distribution, the exception dictionary can suc-cessfully correct a large number of cases, while theheuristic string similarity-based approach handledmany of the less common test cases from the tail ofthe distribution.
The hybrid approach showed sub-stantial reductions in WER (around 20%) versus thestring similarity approach, hence validating our pro-posed approach.For future work we are interested in exploiting the(noisy, clean) pairs contained in the long tail as inputto learning algorithms for acquiring domain-specificlexical transformations.AcknowledgmentsStephan Gouws would like to thank MIH HoldingsLtd.
for financial support during the course of thiswork.ReferencesA.T.
Aw, M. Zhang, J. Xiao, and J. Su.
2006.
A phrase-based statistical model for SMS text normalization.
InProceedings of the COLING/ACL on Main conferenceposter sessions, pages 33?40.
Association for Compu-tational Linguistics.S.
Bangalore, V. Murdock, and G. Riccardi.
2002.Bootstrapping bilingual data using consensus transla-tion for a multilingual instant messaging system.
InProceedings of the 19th International Conference onComputational Linguistics Volume 1, pages 1?7.
As-sociation for Computational Linguistics.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL-08: HLT, pages 674?682, Columbus, Ohio, June.
Association for Computa-tional Linguistics.M.
Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,and A. Basu.
2007.
Investigation and modeling of thestructure of texting language.
International Journal onDocument Analysis and Recognition, 10(3):157?174.D.
Contractor, T.A.
Faruquie, and L.V.
Subramaniam.2010.
Unsupervised cleansing of noisy text.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 189?196.Association for Computational Linguistics.P.
Cook and S. Stevenson.
2009.
An unsupervised modelfor text message normalization.
In Proceedings of theWorkshop on Computational Approaches to Linguis-tic Creativity, pages 71?78.
Association for Computa-tional Linguistics.S.
Gouws, D. Metzler, C. Cai, and E. Hovy.
2011.
Con-textual Bearing on Linguistic Variation in Social Me-dia.
In Proceedings of the ACL-11 Workshop on Lan-guage in Social Media.
Association for ComputationalLinguistics.Bo Han and Timothy Baldwin.
2011.
Lexical Normal-isation of Short Text Messages: Makn Sens a #twit-ter.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies.
Association for Compu-tational Linguistics.S.
Jiampojamarn, K. Dwyer, S. Bergsma, A. Bhargava,Q.
Dou, M.Y.
Kim, and G. Kondrak.
2010.
Translit-eration generation and mining with limited training89resources.
In Proceedings of the 2010 Named Enti-ties Workshop, pages 39?47.
Association for Compu-tational Linguistics.M.
Kaufmann and J. Kalita.
2010.
Syntactic Normaliza-tion of Twitter Messages.
In International Conferenceon Natural Language Processing, Kharagpur, India.C.
Kobus, F. Yvon, and G. Damnati.
2008.
Normal-izing SMS: are two metaphors better than one?
InProceedings of the 22nd International Conference onComputational Linguistics-Volume 1, pages 441?448.Association for Computational Linguistics.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question-answering.
Nat.
Lang.
Eng.,7:343?360, December.H.
Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,and C. Watkins.
2002.
Text classification using stringkernels.
The Journal of Machine Learning Research,2:419?444.Marius Pasca and Pter Dienes.
2005.
Aligning needlesin a haystack: Paraphrase acquisition across the web.In Robert Dale, Kam-Fai Wong, Jian Su, and Oi YeeKwong, editors, Natural Language Processing IJC-NLP 2005, volume 3651 of Lecture Notes in ComputerScience, pages 119?130.
Springer Berlin / Heidelberg.A.
Stolcke.
2002.
SRILM-an extensible language mod-eling toolkit.
In Proceedings of the International Con-ference on Spoken Language Processing, volume 2,pages 901?904.
Citeseer.F.
Yvon.
2010.
Rewriting the orthography of sms mes-sages.
Journal of Natural Language Engineering,16(02):133?159.90
