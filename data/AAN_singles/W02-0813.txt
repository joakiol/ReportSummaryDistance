Combining Contextual Features for Word Sense DisambiguationHoa Trang Dang and Martha PalmerDepartment of Computer and Information SciencesUniversity of PennsylvaniaPhiladelphia, PA, USA, 19104{htd,mpalmer}linc.cis.upenn.eduAbstractIn this paper we present a maximum en-tropy Word Sense Disambiguation systemwe developed which performs competi-tively on SENSEVAL-2 test data for En-glish verbs.
We demonstrate that usingricher linguistic contextual features sig-nificantly improves tagging accuracy, andcompare the system?s performance withhuman annotator performance in lightof both fine-grained and coarse-grainedsense distinctions made by the sense in-ventory.1 IntroductionHighly ambiguous words pose continuing problemsfor Natural Language Processing (NLP) applica-tions.
They can lead to irrelevant document re-trieval in IR systems, and inaccurate translations inMachine Translation systems (Palmer et al, 2000).While homonyms like bank are fairly tractable, pol-ysemous words like run, with related but subtly dis-tinct meanings, present the greatest hurdle for WordSense Disambiguation (WSD).
SENSEVAL-1 andSENSEVAL-2 have attempted to provide a frame-work for evaluating automatic systems by creatingcorpora tagged with fixed sense inventories, whichalso enables the training of supervised WSD sys-tems.In this paper we describe a maximum entropyWSD system that combines information from manydifferent sources, using as much linguistic knowl-edge as can be gathered automatically by currentNLP tools.
Maximum entropy models have beenapplied to a wide range of classification tasks inNLP (Ratnaparkhi, 1998).
Our maximum entropysystem performed competitively with the best per-forming systems on the English verb lexical sampletask in SENSEVAL-1 and SENSEVAL-2.
We com-pared the system performance with human annota-tor performance in light of both fine-grained andcoarse-grained sense distinctions made by WordNetin SENSEVAL-2, and found that many of the sys-tem?s errors on fine-grained senses stemmed fromthe same sources that caused disagreements betweenhuman annotators.
These differences were par-tially resolved by backing off to more coarse-grainedsense-groups, which are sometimes necessary wheneven human annotators cannot make the fine-grainedsense distinctions specified in the dictionary.2 Related WorkWhile it is possible to build an automatic sense tag-ger using only the dictionary definitions, the mostaccurate systems tend to take advantage of super-vised learning.
The system with the highest overallperformance in SENSEVAL-1 used Yarowsky?s hier-archical decision lists (Yarowsky, 2000); while thereis a large set of potential features, only a small num-ber is actually used to determine the sense of anygiven instance of a word.
Chodorow, Leacock andMiller (Chodorow et al, 2000) also achieved highaccuracy using naive bayesian models for WSD,combining sets of linguistically impoverished fea-tures that were classified as either topical or local.Topical features consisted of a bag of open-classwords in a wide window covering the entire con-text provided; local features were words and parts ofspeech within a small window or at particular offsetsJuly 2002, pp.
88-94.
Association for Computational Linguistics.Disambiguation: Recent Successes and Future Directions, Philadelphia,Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sensefrom the target word.
The system was configured touse only local, only topical, or both local and topicalfeatures for each word, depending on which configu-ration produced the best result on a held-out portionof the training data.Previous experiments (Ng and Lee, 1996) haveexplored the relative contribution of different knowl-edge sources to WSD and have concluded that collo-cational information is more important than syntac-tic information.
Additionally, Pedersen (Pedersen,2001; Pedersen, 2000) has pursued the approachof using simple word bigrams and other linguisti-cally impoverished feature sets for sense tagging, toestablish upper bounds on the accuracy of featuresets that do not impose substantial pre-processingrequirements.
In contrast, we wish to demonstratethat such pre-processing significantly improves ac-curacy for sense-tagging English verbs, because webelieve that they allow us to extract a set of featuresthat more closely parallels the information humansuse for sense disambiguation.3 System DescriptionWe developed an automatic WSD system that usesa maximum entropy framework to combine linguis-tic contextual features from corpus instances of eachverb to be tagged.
Under the maximum entropyframework (Berger et al, 1996), evidence from dif-ferent features can be combined with no assump-tions of feature independence.
The automatic tag-ger estimates the conditional probability that a wordhas sense x given that it occurs in context y, wherey is a conjunction of features.
The estimated proba-bility is derived from feature weights which are de-termined automatically from training data so as toproduce a probability distribution that has maximumentropy, under the constraint that it is consistent withobserved evidence.In order to extract the linguistic features neces-sary for the model, all sentences were first automat-ically part-of-speech-tagged using a maximum en-tropy tagger (Ratnaparkhi, 1998) and parsed usingthe Collins parser (Collins, 1997).
In addition, anautomatic named entity tagger (Bikel et al, 1997)was run on the sentences to map proper nouns to asmall set of semantic classes.
Following work byChodorow, Leacock and Miller, we divided the pos-sible model features into topical and local contex-tual features.
Topical features looked for the pres-ence of keywords occurring anywhere in the sen-tence and any surrounding sentences provided ascontext (usually one or two sentences).
The setof 200-300 keywords is specific to each lemma tobe disambiguated, and is determined automaticallyfrom training data so as to minimize the entropy ofthe probability of the senses conditioned on the key-word.The local features for a verb w in a particular sen-tence tend to look only within the smallest clausecontaining w. They include collocational featuresrequiring no linguistic preprocessing beyond part-of-speech tagging (1), syntactic features that capturerelations between the verb and its complements (2-4), and semantic features that incorporate informa-tion about noun classes for objects (5-6):1. the word w, the part of speech of w, and wordsat positions -2, -1, +1, +2, relative to w2.
whether or not the sentence is passive3.
whether there is a subject, direct object, indi-rect object, or clausal complement (a comple-ment whose node label is S in the parse tree)4. the words (if any) in the positions of subject,direct object, indirect object, particle, preposi-tional complement (and its object)5. a Named Entity tag (PERSON, ORGANIZA-TION, LOCATION) for proper nouns appear-ing in (4)6.
WordNet synsets and hypernyms for the nounsappearing in (4)1This set of local features relies on access to syntac-tic structure as well as semantic class information,and represents our move towards using richer syn-tactic and semantic knowledge sources to model hu-man performance.1Nouns were not disambiguated in any way, and all possiblesynsets and hypernyms for the noun were included.
No separatedisambiguation of noun complements was done because, givenenough data, the maximum entropy model should assign highweights to the correct semantic classes of the correct noun senseif they represent defining selectional restrictions.4 EvaluationIn this section we describe the system performanceon the verbs from SENSEVAL-1 and SENSEVAL-2.The system was built after SENSEVAL-1 but beforeSENSEVAL-2.2SENSEVAL-1 SENSEVAL-1 used a DARPA-styleevaluation format where the participants were pro-vided with hand-annotated training data and testdata.
The lexical inventory used was the Hector lex-icon, developed jointly by DEC and Oxford Univer-sity Press (Kilgarriff and Rosenzweig, 2000).
Byallowing for discussion and revision of confusinglexical entries during tagging, before the final testdata was tagged, inter-annotator agreement of over90% was eventually achieved.
However, the Hectorlexicon was very small and under proprietary con-straints, making it an unsuitable candidate for ap-plications requiring a large-scale, publicly-availabledictionary.SENSEVAL-2 The subsequent SENSEVAL-2 exer-cise used a pre-release version of WordNet1.7 whichis much larger than Hector and is more widely usedin NLP applications.
The average training set sizefor verbs was only about half of that provided inSENSEVAL-1, while the average polysemy of eachverb was higher3.
Smaller training sets and theuse of a large-scale, publicly available dictionary ar-guably make SENSEVAL-2 a more indicative evalu-ation of WSD systems in the current NLP environ-ment than SENSEVAL-1.
The role of sense groupswas also explored as a way to address the pop-ular criticism that WordNet senses are too vagueand fine-grained.
During the data preparation forSENSEVAL-2, previous WordNet groupings of theverbs were carefully re-examined, and specific se-mantic criteria were manually associated with eachgroup.
This occasionally resulted in minor revisionsof the original groupings (Fellbaum et al, 2001).This manual method of creating a more coarse-grained sense inventory from WordNet contrastswith automatic methods that rely on existing se-2The system did not compete officially in SENSEVAL-2 be-cause it was developed by people who were involved in coordi-nating the English verbs lexical sample task.3The average number of senses per verb in the training datawas 11.6 using the Hector dictionary in SENSEVAL-1, and 15.6using WordNet1.7 in SENSEVAL-2.mantic links in WordNet (Mihalcea and Moldovan,2001), which can produce divergent dictionaries.Our system performs competitively with thebest performing systems in SENSEVAL-1 andSENSEVAL-2.
Measuring accuracy as the recallscore (which is equal to precision in our case be-cause the system assigns a tag to every instance), wecompare the system?s coarse-grained scores usingthe revised groupings versus random groupings, anddemonstrate the coherence and utility of the group-ings in reconciling apparent tagging disagreements.4.1 SENSEVAL-1 ResultsThe maximum entropy WSD system?s perfor-mance on the verbs from the evaluation data forSENSEVAL-1 (Kilgarriff and Rosenzweig, 2000) ri-valed that of the best-performing systems.
Table 1shows the performance of variants of the system us-ing different subsets of possible features.
In additionto experimenting with different combinations of lo-cal/topical features, we attempted to undo passiviza-tion transformations to recover underlying subjectsand objects.
This was expected to increase the accu-racy with which verb arguments could be identified,helping in cases where selectional restrictions on ar-guments played an important role in differentiatingbetween senses.The best overall variant of the system for verbsdid not use WordNet class features, but includedtopical keywords and passivization transformation,giving an average verb accuracy of 72.3%.
Thisfalls between Chodorow, Leacock, and Miller?s ac-curacy of 71.0%, and Yarowsky?s 73.4% (74.3%post-workshop).
If only the best combination of fea-ture sets for each verb is used, then the maximum en-tropy models achieve 73.7% accuracy.
Even thoughour system used only the training data provided andnone of the information from the dictionary itself,it was still competitive with the top performing sys-tems which also made use of the dictionary to iden-tify multi-word constructions.
As we show later,using this additional piece of information improvesperformance substantially.In addition to the SENSEVAL-1 verbs, we ran thesystem on the SENSEVAL-1 data for shake, whichcontains both nouns and verbs.
The system sim-ply excluded verb complement features wheneverthe part-of-speech tagger indicated that the wordtask lex lex+topic lex+trans+topic wn wn+topic wn+trans+topicamaze 0.957 0.928 0.942 0.957 0.899 0.913bet-v 0.709 0.667 0.667 0.718 0.650 0.650bother 0.866 0.852 0.847 0.837 0.828 0.823bury 0.468 0.502 0.517 0.572 0.537 0.532calculate 0.867 0.902 0.904 0.862 0.881 0.872consume 0.481 0.492 0.508 0.454 0.503 0.454derive 0.682 0.682 0.691 0.659 0.664 0.696float-v 0.437 0.441 0.445 0.406 0.445 0.432invade 0.560 0.522 0.531 0.580 0.551 0.536promise-v 0.906 0.902 0.902 0.888 0.893 0.893sack-v 0.972 0.972 0.972 0.966 0.966 0.966scrap-v 0.812 0.866 0.871 0.796 0.876 0.882seize 0.653 0.741 0.745 0.660 0.691 0.703verbs 0.705 0.718 0.723 0.703 0.711 0.709shake-p 0.744 0.725 0.742 0.767 0.770 0.758Table 1: Accuracy of different variants of maximum entropy models on SENSEVAL-1 verbs.
Only local in-formation was used, unless indicated by ?+topic,?
in which case the topical keyword features were includedin the model; ?wn?
indicates that WordNet class features were used, while ?lex?
indicates only lexical andnamed entity tag features were used for the noun complements; ?+trans?
indicates that an attempt was madeto undo passivization transformations.to be sense-tagged was not a verb.
Even on thismix of nouns and verbs, the system performedwell compared with the best system for shake fromSENSEVAL-1, which had an accuracy of 76.5% onthe same task.4.2 SENSEVAL-2 ResultsWe also tested the WSD system on the verbs fromthe English lexical sample task for SENSEVAL-2.In contrast to SENSEVAL-1, senses involving multi-word constructions could be directly identified fromthe sense tags themselves (through the WordNetsense keys that were used as sense tags), and thehead word and satellites of multi-word construc-tions were explicitly marked in the training and testdata.
This additional annotation made it much eas-ier for our system to incorporate information aboutthe satellites, without having to look at the dictio-nary (whose format may vary from one task to an-other).
The best-performing systems on the Englishverb lexical sample task (including our own) filteredout possible senses based on the marked satellites,and this improved performance.Table 2 shows the performance of the system us-ing different subsets of features.
While we found lit-tle improvement from transforming passivized sen-tences into a more canonical form to recover under-lying arguments, there is a clear improvement in per-formance as richer linguistic information is incorpo-rated in the model.
Adding topical keywords alsohelped.Incorporating topical keywords as well as col-locational, syntactic, and semantic local features,our system achieved 59.6% and 69.0% accuracyusing fine-grained and coarse-grained scoring, re-spectively.
This is in comparison to the next best-performing system, which had fine- and coarse-grained scores of 57.6% and 67.2% (Palmer et al,2001).
Here we see the benefit from including a filterthat only considered phrasal senses whenever therewere satellites of multi-word constructions markedin the test data; had we not included this filter, ourfine- and coarse-grained scores would have beenonly 56.9% and 66.1%.Table 3 shows a breakdown of the number ofsenses and groups for each verb, the fine-grainedaccuracy of the top three official SENSEVAL-2 sys-tems, fine- and coarse-grained accuracy of our maxi-Feature Type (local only) Accuracy Feature Type (local and topical) Accuracycollocation 47.6 collocation 49.8+ syntax 54.9 + syntax 57.1+ syntax + transform 55.1 + syntax + transform 57.3+ syntax + semantics 58.3 + syntax + semantics 59.6+ syntax + semantics + transform 58.9 + syntax + semantics + transform 59.5Table 2: Accuracy of maximum entropy system using different subsets of features for SENSEVAL-2 verbs.Verb Senses Groups SMULS JHU KUNLP MX MX-c ITA ITA-cbegin 8 8 87.5 71.4 81.4 83.2 83.2 81.2 81.4call 23 16 40.9 43.9 48.5 47.0 63.6 69.3 89.2carry 27 17 39.4 51.5 45.5 37.9 48.5 60.7 75.3collaborate 2 2 90.0 90.0 90.0 90.0 90.0 75.0 75.0develop 15 5 36.2 42.0 42.0 49.3 68.1 67.8 85.2draw 32 20 31.7 41.5 34.1 36.6 51.2 76.7 82.5dress 14 8 57.6 59.3 71.2 61.0 89.8 86.5 100.0drift 9 6 59.4 53.1 53.1 43.8 43.8 50.0 50.0drive 15 10 52.4 42.9 54.8 59.5 78.6 58.8 71.7face 7 4 81.7 80.6 82.8 81.7 90.3 78.6 97.4ferret 1 1 100.0 100.0 100.0 100.0 100.0 100.0 100.0find 17 10 29.4 26.5 27.9 27.9 39.7 44.3 56.9keep 27 22 44.8 55.2 44.8 56.7 58.2 79.1 80.1leave 14 10 47.0 51.5 50.0 62.1 66.7 67.2 80.5live 10 8 67.2 59.7 59.7 68.7 70.1 79.7 87.2match 8 4 40.5 52.4 52.4 47.6 69.0 56.5 82.6play 25 18 50.0 45.5 37.9 50.0 51.5 * *pull 33 28 48.3 55.0 45.0 53.3 68.3 68.1 72.2replace 4 2 44.4 57.8 55.6 62.2 93.3 65.9 100.0see 21 13 37.7 42.0 39.1 47.8 55.1 70.9 75.5serve 12 7 49.0 54.9 68.6 68.6 72.5 90.8 93.2strike 26 21 38.9 48.1 40.7 33.3 44.4 76.2 90.5train 9 4 41.3 54.0 58.7 57.1 69.8 28.8 55.0treat 6 5 63.6 56.8 56.8 56.8 63.6 96.9 97.5turn 43 31 35.8 44.8 37.3 44.8 56.7 74.2 89.4use 7 4 72.4 72.4 65.8 65.8 78.9 74.3 89.4wander 4 2 74.0 78.0 82.0 82.0 90.0 65.0 90.0wash 13 10 66.7 58.3 83.3 75.0 75.0 87.5 90.6work 21 14 43.3 45.0 45.0 41.7 56.7 * *TOTAL 15.6 10.7 56.3 56.6 57.6 59.6 69.0 71.3 82.0Table 3: Number of senses and sense groups in training data for each SENSEVAL-2 verb; fine-grainedaccuracy of top three competitors (JHU, SMULS, KUNLP) in SENSEVAL-2 English verbs lexical sampletask; fine-grained (MX) and coarse-grained accuracy (MX-c) of maximum entropy system; inter-taggeragreement for fine-grained senses (ITA) and sense groups (ITA-c).
*No inter-tagger agreement figures wereavailable for ?play?
and ?work?.mum entropy system, and human inter-tagger agree-ment on fine-grained and coarse-grained senses.Overall, coarse-grained evaluation using the groupsimproved the system?s score by about 10%.
Thisis consistent with the improvement we found ininter-tagger agreement for groups over fine-grainedsenses (82% instead of 71%).
As a base-line, to en-sure that the improvement did not come simply fromthe lower number of tag choices for each verb, wecreated random groups.
Each verb had the samenumber of groups, but with the senses distributedrandomly.
We found that these random groups pro-vided almost no benefit to the inter-annotator agree-ment figures (74% instead of 71%), confirming thegreater coherence of the manual groupings.4.3 Analysis of errorsWe found that the grouped senses for call substan-tially improved performance over evaluating withrespect to fine-grained senses; the system achieved63.6% accuracy with coarse-grained scoring usingthe groups, as compared to 47.0% accuracy withfine-grained scoring.
When evaluated against thefine-grained senses, the system got 35 instanceswrong, but 11 of the ?incorrect?
instances weretagged with senses that were actually in the samegroup as the correct sense.
This group of senses dif-fers from others in the ability to take a small clauseas a complement, which is modeled as a feature inour system.
Here we see that the system benefitsfrom using syntactic features that are linguisticallyricher than the features that have been used in thepast.29% of errors made by the tagger on develop weredue to confusing Sense 1 and Sense 2, which are inthe same group.
The two senses describe transitiveverbs that create new entities, characterized as either?products, or mental or artistic creations: CREATE(Sense 1)?
or ?a new theory of evolution: CREATEBY MENTAL ACT (Sense 2).?
Instances of Sense 1that were tagged as Sense 2 by the system included:Researchers said they have developed a genetic en-gineering technique for creating hybrid plants fora number of key crops; William Gates and PaulAllen developed an early language-housekeeper sys-tem for PCs.
Conversely, the following instances ofSense 2 were tagged as Sense 1 by the tagger: A Pur-due University team hopes to develop ways to mag-netically induce cardiac muscle contractions; KobeSteel Ltd. adopted Soviet casting technology used ituntil it developed its own system.
Based on the directobject of develop, the automatic tagger was hard-pressed to differentiate between developing a tech-nique/system (Sense 1) and developing a way/system(Sense 2).Analysis of inter-annotator disagreement betweentwo human annotators doing double-blind taggingrevealed similar confusion between these two sensesof develop; 25% of the human annotator disagree-ments on develop involved determining which ofthese two senses should be applied to phrases likedevelop a better way to introduce crystallographytechniques.
These instances that were difficult forthe automatic WSD system, were also difficult forhuman annotators to differentiate consistently.These different senses are clearly related, but therelation is not reflected in their hypernyms, whichemphasize the differences in what is being high-lighted by each sense, rather than the similarities.Methods of evaluation that automatically back offfrom synset to hypernyms (Lin, 1997) would failto credit the system for ?mistagging?
an instancewith a closely related sense.
Manually created sensegroups, on the other hand, can capture broader, moreunderspecified senses which are not explicitly listedand which do not participate in any of the WordNetsemantic relations.5 ConclusionWe have demonstrated that our approach to disam-biguating verb senses using maximum entropy mod-els to combine as many linguistic knowledge sourcesas possible, yields state-of-the-art performance forEnglish.
This may be a language-dependent feature,as other experiments indicate that additional linguis-tic pre-processing does not necessarily improve tag-ging accuracy for languages like Chinese (Dang etal., 2002).In examining the instances that proved trouble-some to both the human taggers and the automaticsystem, we found errors that were tied to subtlesense distinctions which were reconciled by back-ing off to the more coarse-grained sense groups.Achieving higher inter-annotator agreement is nec-essary in order to provide consistent training datafor supervised WSD systems.
Lexicographers havelong recognized that many natural occurrences ofpolysemous words are embedded in underspecifiedcontexts and could correspond to more than one spe-cific sense.
Annotators need the option of selecting,as an alternative to an explicit sense, either a groupof specific senses or a single, broader sense, wherespecific meaning nuances are subsumed.
Sensegrouping, already present in a limited way in Word-Net?s verb component, can be guided and enhancedby the analysis of inter-annotator disagreements andthe development of explicit sense distinction criteriathat such an analysis provides.6 AcknowledgmentsThis work has been supported by National Sci-ence Foundation Grants, NSF-9800658 and NSF-9910603, and DARPA grant N66001-00-1-8915 atthe University of Pennsylvania.
The authors wouldalso like to thank the anonymous reviewers for theirvaluable comments.ReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1).Daniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: A high-performance learning name-finder.
In Proceedings ofthe Fifth Conference on Applied Natural LanguageProcessing, Washington, DC.Martin Chodorow, Claudia Leacock, and George A.Miller.
2000.
A topical/local classifier for word senseidentification.
Computers and the Humanities, 34(1-2), April.
Special Issue on SENSEVAL.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Computa-tional Linguistics, Madrid, Spain, July.Hoa Trang Dang, Ching yi Chia, Martha Palmer, and Fu-Dong Chiou.
2002.
Simple features for chinese wordsense disambiguation.
In Proceedings of Coling-02,Taipei, Taiwain.Christiane Fellbaum, Martha Palmer, Hoa Trang Dang,Lauren Delfs, and Susanne Wolf.
2001.
Manual andautomatic semantic annotation with WordNet.
In Pro-ceedings of the Workshop on WordNet and Other Lex-ical Resources, Pittsburgh, PA.A.
Kilgarriff and J. Rosenzweig.
2000.
Framework andresults for English SENSEVAL.
Computers and theHumanities, 34(1-2), April.
Special Issue on SENSE-VAL.Dekang Lin.
1997.
Using syntactic dependency as localcontext to resolve word sense ambiguity.
In Proceed-ings of the 35th Annual Meeting of the ACL, Madrid,Spain.Rada Mihalcea and Dan I. Moldovan.
2001.
Automaticgeneration of a coarse grained WordNet.
In Proceed-ings of the Workshop on WordNet and Other LexicalResources, Pittsburgh, PA.Hwee Tou Ng and Hian Beng Lee.
1996.
Integrat-ing multiple knowledge sources to disambiguate wordsense: An exemplar-based approach.
In Proceedingsof the 34th Annual Meeting of the Association forComputational Linguistics, Santa Cruz, CA, June.M.
Palmer, Chunghye Han, Fei Xia, Dania Egedi, andJoseph Rosenzweig.
2000.
Constraining lexical selec-tion across languages using tags.
In Anne Abeille andOwen Rambow, editors, Tree Adjoining Grammars:formal, computational and linguistic aspects.
CSLI,Palo Alto, CA.Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-ren Delfs, and Hoa Trang Dang.
2001.
Englishtasks: All-words and verb lexical sample.
In Proceed-ings of SENSEVAL-2: Second International Workshopon Evaluating Word Sense Disambiguation Systems,Toulouse, France, July.Ted Pedersen.
2000.
A simple approach to building en-sembles of naive bayesian classifiers for word sensedisambiguation.
In Proceedings of the 1st Meetingof the North American Chapter of the Association forComputational Linguistics, Seattle, WA.Ted Pedersen.
2001.
A decision tree of bigrams is anaccurate predictor of word sense.
In Proceedings ofthe 2nd Meeting of the North American Chapter of theAssociation for Computational Linguistics, Pittsburgh,PA.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania.David Yarowsky.
2000.
Hierarchical decision lists forword sense disambiguation.
Computers and the Hu-manities, 34(1-2), April.
Special Issue on SENSE-VAL.
