The Automatic Component of the LINGSTATMachine-Aided Translation System*Jonathan Yamron, James Cant, Anne Demerits, Tailco Dietzel, and Yoshilco ltoDragon Systems, Inc., 320 Newda Street, Newton, MA 02160ABSTRACTWe present he newest implementation f the LINGSTATmachine-aided translation system.
The moat signiflcsatchange from earlier versions i  a new set of modules that pro-duce a draft translation of the document for the user to referto or modify.
This paper describes these modules, with spe-cial emphasis on an automatically trained lexicalized gram-mar used in the parsing module.
Some preHminary resultsfrom the January 1994 ARPA evaluation are reported.1.
INTRODUCTIONLINGSTAT is an interactive machine-aided translationsystem designed to increase the productivity of a trans-lator.
It is aimed both at experienced users whose goalis high quality tr~nAlation, and inexperienced users withlittle knowledge of the source whose goal is simply to ex-tract information from foreign language text.
(For an in-troduction to the basic structure of LINGSTAT, see \[1\].
)The first problem to be studied is Japanese to Englishtranslation with an emphasis on text from the domainof mergers and acquisitions, although recent evaluationshave included general newspaper text as well.
Work isalso progressing on a Spanish to English system.
Theapproach described below represents he current state ofthe Japanese system, and will be applied with minimalchanges to Spanish.Due to the special difficulties presented by the Japanesewriting system, previous versions of LINGSTAT havefocused on developing tools for the lexical analysis ofJapanese (such as tokenization ofthe Japanese characterstream, morphological nalysis, and kat~l~a transliter-ation), and on providing the user access to lexical infor-mation (such as pronunciations, glosses, and definitions)via online lookup tools.
In addition, asimple parser wasincorporated to identify modifying phrases.
No trans-lation of the document was provided.
Instead, the userused the results of the above analyses and the onlinetools to construct a translation.In the newest version of LINGSTAT, the user is pro-*This work was sponsored by the Advanced Research ProjectsAgency under contract number J-FBI-91-239.vided with a draft translation of the source document.For a source language similar to English, a starting pointfor such a draft might be a word-for-word translation,but because Japanese word order and sentence structureare so different from English, a more general frameworkhas been constructed.
The translation process in LING-STAT consists of the following steps:?
Tokenization and morphological nalysis?
Parsing?
Rearrangement of the source into English order?
Annotation and selection of glosses via an Englishlanguage modelThese modules are described in Section 2 below.
Sec-tion 3 gives some preliminary results from the January1994 evaluation, and Section 4 discusses some plans forfuture improvements.2.
IMPLEMENTATIONTokenization/de-inflectionIn LINGSTAT, "tokenization" refers to the process ofbreaking a source document into a sequence of rootwords tagged, if necessary, with inflection information.For most languages, the tokenizer is basically an en-gine that oversees the de-inflection of source words intoroot forms.
For languages like Japanese, written withoutspaces, the tokenizer also has the job of segmenting thesource.To segment Japanese, the LINGSTAT tokenizer uses aprobabilistic dynamic programming algorithm to breakup the character stream into the sequence of words thatmaximizes the product of word unigram probabilities, assupplied from a list of 300,000 words.
Inflected forms arerecognized uring tokenization by a de-inflector module.This module has a language-independent engine drivenby a language-specific de-inflection table.
(More detailson the function of these components can be found in \[1\].
)There have been two improvements in the tokenizer/de-163inflector module in the newer versions of the system,made possible by the introduction of part of speech in-formation into the word list.
The first is an extra checkon the validity of suggested e-inflections by demand-ing consistency between the inflection and the part ofspeech of the proposed root.
This has cleanly eliminateda number of spurious de-inflections that were previouslyhandled in a more ad-hoc fashion.
The second improve-ment, motivated more by plans to move on to Spanish, isto stop the tokenizer from attempting to uniquely spec-ify the de-inflection path (and now, part of speech) foreach token it finds.
As an example of the problem thisaddresses, consider the two de-inflections of the Spanishword ayudas:ayadas.-+ eyed= (help, aid)ayudas .-~ ayudar (to help, to aid)The original tokeniser made a choice between the nounand verb de-inflection based on the unigram frequencyof the root.
The new tokenizer still finds all allowedpossibilities, but now simply passes them to the parser,which is better equipped to resolve the ambiguity.Pars ingThe parser in LINGSTAT has two roles.
In the inter-active component, information about modifying phrasesis extracted from the parse and presented to the user asan aid to understanding the structure of each Japanesesentence.
In the automatic omponent, he parse is thebasis for the rearrangement of he Japanese sentence intoEnglish.Because it is a long-term goal to have a system that canhe quickly adapted to new domains and languages, ahigh priority is placed on developing parsing techniquesthat are capable of extracting some information auto-matically through training on new sources of text, thusminimizing the amount of human effort.
In the cur-rent system, this has led to a two-stage parsing process.The first stage implements a coarse probabilistic context-free grammar of a few hundred human-supplied rulesacting on parts of speech.
Because of this coarseness,some parsing ambiguities remain to be resolved by thesecond-stage parser, which implements a simple, lexical-ized, probabilistic ontext-free grammar trained on wordco-occurrences in unlabeled Japanese sentences withouthuman input.Context-fzee parser.
The first.stage parse is done us-ing a standard probabilistic ontext-free grammar actingon about 50 parts of speech.
Any ambiguities in part ofspeech assignments or de-inflection paths passed by thetokenizer/de-inflector are resolved based on the prob-ability of possible parses.
The grammar is allowed tocontain unitary and null productions, which impme anordering on the summation over rules that takes placeduring training; because there are currently only a fewhundred rules, this ordering is checked by hand.
Thegrammar can be trained with either the Inside-Outside\[2\] or Viterbi \[3\] algorithm.It is essential that the parser eturn a parse, even a badone, for subsequent processing.
Therefore special, low-probability '~unk" rules have been introduced to handleunanticipated constructions.
These junk rules affect hegeneration of terminal symbols and take the followingform: for each rule in which a non-terminal generates aparticular terminal, a rule is added permitting the samenon-terminal togenerate any other terminal with a smallprobability.
This allows the grammar to force the termi-ned string into a sequence that has a recognizable parse,but at a high enough cost such that any parse withoutsuch coersion will be favored.
One advantage of this ap-proach is that the grammar can compensate for missingor mislabeled ata.
Consider the fragmentthedet largeadv dogno=nin which the adjective large has been mislabeled as anadverb.
The junk rule permits the grammar to change itspart of speech to something more appropriate providedno other sensible parse can be found.In principle, the probability of invoking the junk rulecould be trained with the other ules in the grammar (theexample above suggests that it might be advantageousto do so).
Currently this is not being done, based on theobservation that an invocation of the junk rule is morelikely an indication of a deficiency in the grammar thana useful correction to the data.I..~'~,~\];~ed parser.
The grammar implemented bythecontext-free parser is not fine enough to properly resolvecertain kinds of ambiguity, such as the correct attach-ment of prepositional phrases or noun modifiers.
Theseattachment problems are handled by a second parser,which does a top-down rescoring of certain probabilitiescomputed in the first stage.
Currently this rescoringis used to fine-tune attachments of particle phrases inJapanese sentences.The second parser makes use of a second probabilisticgrammar, one whose basic elements are the words them-selves, and whose data consist of the probabilities of eachword in the vocabulary to be generated in the contextof any other word.
Like a bigram language model, theseprobabilities can be trained on word co-occurrences inunlabeled sentences, but unlike bigrams, the grammarcan learn about associations between words in a sentenceregardless of their separation.164This very simple ~ntext-free grammar can be describedas follows.
To each word in the vocabulary we associatea terminal symbol to (the word itseff) and a non-terminalsymbol A~.
The grammar consists of the following twokinds of rules:Awe --~ A~= to2 A~=A~ , (la)A=, --~ ?
,  (lb)where ~ represents he null production.
In addition, weintroduce a sentence start symbol A0 with the produc-tionAo --* A ,  toA.
.
(2)The probability of invoking a particular ule dependsonly on the word associated with the generating non-terminal and the terminal word in the production.
Theprobabilities for (la) and (lb) can therefore be writtenP(tol ~ to2) and p(wl -+ ~b), respectively, and these sat-isfyPCtot --* ?)
+ ~"~P(tot --' to2) = 1.iV2For the start symbol, the probabilities are p(0--* to) andsatisfyEp(0 - - ,  to) ---- 1 .UJThere is no null production for the start symbol.Roughly speaking, this grammar generates a sentence inthe following manner.
The start symbol first generatessome word in the sentence.
This word then generatessome number of words to its left and right, which inturn generate other words to their left and right.
Fromthe form of the grammar it can be deduced that thesegenerations are "local," in the sense that if tot generatesto2 on its right, w2 is not allowed to generate any wordto the left of tot (and similarly for tot generating to2 onits left).
The process continues in a cascading fashionuntil the whole sentence has been generated.
The fertil-ity of a particular word to (i.e., the number of words itwill typically generate) is determined by the probabilityp(to--~ ~b), as can be seen from exeanining the produc-tions (1): a non-terminal Aw will continue to producewords through rule (la) via tail recursion until rule (lb)is invoked.Although this grammar has the same type and numberof parameters a  a bigrarn model, here they have a verydifferent interpretation: they measure the probability ofone word to generate another anywhere in the sentence,subject only to the constraints imposed by the genera-tion process described above.
Thus an association be-tween two words that might typically appear together,such as .fast and car, will be recognized even if anotherword might occasionally intervene, such as red.
Anotherfeature is that words with the most predictive power ina sentence tend to generate words with less predictivepower, which has the consequence that words like ~etend to generate no words at all.
This is an improve-ment over a bigram model in which the is required toselect a succeeding word from a distribution that is es-sentially fiat across a large portion of the vocabulary.This grammar shares the appealing feature of n-grammodels that its parameters can be trained on unlabeledtext (consisting of whole sentences).
In this case, how-ever, the training procedure is iteratiw a modificationof the Inside-Outside algorithm that is of order N 4 inthe sentence length, t The iteration starts from a fiatdistribution, with co-occurrences of words within sen-tences leading to enhanced probabilities for some wordsto generate others.The N 4 algorithm actuary applies to a slightly different(but generatively equivalent) grammar than the one de-fined by rules (1) and (2).
To implement this algorithm,we first replace rule (la) byA== ~ A==toaA==A't (ton to the left of tot) ,Awl ~ A==A==to2A== (w2 to the right of tot),where the probability of both rules is the same and givenby ~tot -~ to3).
The only ditference between this sadrule (la) is that when A= generates multiple words tothe right of to, they are generated right to left instead ofleft to right.As an example of how the N 4 dependence arises, considerthe inside calculation for this model.
For a sentencetot...  WN, the quantities of interest for the inside passare the probabilities l (Aw, --~ w j .
.
.
wi - t )  for j < i andI (A , ,  --~ toi+1...toj) for j > i.
These may be calculatedrecursively by the following formulae:i -1  i - Ik f j  I lkx I(Aw.
wk_l) I(A=, wh?l .
.
.
wi)---, wt+  .
.
.
(3a)j b-1p( , ,  ---.
to, )k=i+l l=ix I(Aw, -"~ wk+l .
.
,  wj)l(Awh ~ wa+l.., w~-l)x I(A~, -~ wi+, .
.
,  wa) , (3b)where the "negative length" string wi.
.
.
wi-i is under-stood to represent the null production ~.
The recursionXThe authors would llke to thank Joshua Goodman for devel-oping the N t procedure, a notable improve=nent over previousimplementations.165is initialized byI(a=, --.
~) = pCw~ -~ ~).The above computations involve a double sum and aretherefore of order N 2, and there are order N = probabil-ities I(wi ~ wj .
.
.wi-1) and I(A=, ~ w~+l .
.
.wi),  fora total of N 4.
(For the Viterbi calculation, one simplyselects the largest contribution from the right hand sideof equations (3a) and (3b) instead of doing the doublesum.
)It is important to note that despite the N 4 behavior, thisgrammar is in general faster than context-free parsing,which is computationally of order N s. This is becausethe compute time for context-free parsing also includesa factor proportional to the number of rules in the gram-mar, which even in simple cases can be in the hundreds.There is no such factor in the computation for this lex-icalized grammar--it is effectively replaced by anotherpower of N, which is much smaller.To see how the probabilities p(wl .--~ w2) converge, thismodel was run through ten iterations of training on ap-proximately 100,000 sentences of ten words or less fromthe English half of the Canadian Hansard corpus.
Someexamples of these probabilities follow:the U..91 @ .52 ~,.26 S..14 agreement=ari#s.44.09 agreement.08 and.08 general.08 onAs expected, the trains strongly to generate the null sym-bol ~b.
The token U. has a strong tendency to generateS.
for obvious reasons; that it also generates agreementis a consequence of the frequent discussion in the corpusof the U. S. free trade agreement.
This is an example ofhow the model will find associations between separatedwords that even a trigram model will not see.
The distri-bution associated with tariffs arises from parliamentarydebate on the general agreement on tariffs and trade.The simple grammar described above can be consideredthe starting point for a class of more complex models.One obvious extension is to train the probability distri-butious for generating to the left and right separately.This corresponds to implementing the greanmarA~ ---* A t; w A R AL t; ~,, 2 =,~=,  , Aw, - -~b,  (4a)A R ....4, AR  - -L  = AR R e,l ~w~.~w=w2~w= , A=t ---, ~ .
(4b)Training this grammar on the same text as the originalmodel yields the left probabilities:the U..96 ~ .80.10 theta~#s.35.14 agreement.12 general.12 onAgain, the tends to generate a null.
Like mat  nouns,U.
has learned to generate a the to its left, and the leftdistribution for tariffs includes only those words foundtypically on its left.
The right probabilities for the samewords  are"the U. tariffs.90 ~ .3~ ~ .52 ?.37 $.
.18 and.19 agreement .17 trade.07 freeThese are also consistent with the results from the orig-inal model.Rear rangementThe next step in LINGSTAT's translation method isa transfer of the parse of each Japanese sentence intoa corresponding English parse, giving an English wordordering.
This is accomplished through the use ofEnglish rewrite rules encoded in the Japanese gram-mar.
Through this encoding, each non-terminal in theJapanese grammar corresponds to a non-terminal in animplied English granm~r.
The rewrite process just con-slats of taking the Japanese parse and expanding in thisEnglish grammar.
As this expansion proceeds, Japaneseconstructs that are not translated (certain particles, forexample) are removed, and tokens for English constructsnot represented in the Japanese (such as articles) are in-troduced.Annotat ion / language mode lThe Japanese words in the reordered sentenced are anno-tated with (possibly several) candidate English glosses,supplied from an electronic dictionary compiled fromvarious sources.
Numbers are translated irectly, andkat.akana tokens (which are usually borrowed foreignwords) are transliterated into English.
Tokens intro-duced in the rearrangement s ep are also glossed; thetoken indicating an English article is multiply glossed asthe, a, an, and null (which expands to an empty word).Inflected Japanese words are glossed by first glossing theroot, then applying an English version of the Japanese166inflection to each candidate.
This is made difficult by thepoor correspondence b tween Japanese and English in-flections: English is inflected for person and number, forexample, while in Japanese there are inflections for suchconstructions as the causative, which require non-localchanges in the corresponding English.
Japanese inflec-tious also often consist of multiple steps, which meansthat the English inflections must be compounded.
Forexample, to inflect he verb to wa& into the past desider-ative involves the two step transformation,to walk --~ to want to walk --, wanted to walk.This procedure can produce some unusual results whenthe number of inflection steps is greater than two.The final step in the translation process is to apply anEnglish language model to select the best gloss fromamong the many candidates for each word.
In the cur-rent system this is done with a trigram model, whichmakes the choices that maximize the average probabil-ity per word.
The trigram model used was trained onWall Street Journal and so has a business bias, partiallyreflecting the bias of the evaluation texts.3.
RESULTSThe January 1994 AItPA machine translation evalua-tion has recently been completed.
In this test, Dragonused the same translators as in the May 1993 evaluationand provided them with essentially the same interfaceand online tools.
The difference in this evaluation wasthat the translators were also provided an antomaticallygenerated English translation of the Japanese documentas a first draft.
Manual and machine-assisted ranslationtimes were measured, and the automatic output was alsosubmitted for separate valuation.Preliminary timing results how a speedup by a factor of2.4 in machine-assisted vs. manual translation.
Becausewe were using the May 1993 translators, this result maybe compared to the May 1993 result; it is essentiallyunchanged.
This suggests that the draft translation wasof no significant help to the translators in this evaluation,probably because the quality of automatic output is nothigh enough to be relied upon.A quality measurement of the automatic output is notyet available, but we offer one example of a sample trans-lation from the current system.
For the following cor-rectly glossed Japanese sentence,(Schroder) WA , (Mitsubishl Trust and BankingCo,~o.,tio.)
(to) C*a,n~ ~o,n~.v) No (s~t) NO(4.9~) NO (sell) KOTO WO (decided)LINGSTAT producedwaazuhaimu dtumodaa oJ the America ineeJtmentbank decided to adl oM 4.9~ o!
the shares ol thesame company to Mitsubishi Trust and BanldngCorporationEven this simple sentence demonstrates the large amountof rearrangement necessary to render the Japanese intoEnglish.
This effort is not without errors; a correct rans-lation shows that the word meaning same companu wasmishandled, as was the modifier of Wertheim Schroder:The American investment bank Wertheim Schroderhaa decided to sell 4.9~ o~ its ,reck to the MitJubishiTrust and Banking CorporationThis sentence is less complex than is typical in aJapanese newspaper article, and therefore LINGSTAT'sperformance in this case is not representative.4.
FUTURE PLANSThe steps that have the most effect on the quality of thefinal output translation (at least for Japanese) are thep~ser and gloss selection modules.
The parser in partic-ular is crucial, since it initiates a global rearrangementof the sentence into a sensible English order--a parsingmistake will often render a sentence unintelligible.The improvements contemplated for the parsing mod-ule include more hand work on the coarse context-freegrammar to provide more accurate parses, and a gen-eral speedup to allow more extensive training.
A fasterparser would also allow the merging of the two grammarsso that they could be trained simultaneously.
Attemptsto do this have so far resulted in an unacceptable increasein training and parsing time due to the complexity of thealgorithm.The language model used to select glosses in the finaltranslation step must be improved to have more globalcontrol.
Common mistakes made by the current modelinclude inconsistent glossing of a recurring word and vir-tually no notion of topic or domain (except on businesssubjects).
Both of these problems are the result of us-ing a language model, trigrams, that uses such restrictedcontext.The newest version of the system must be ported toSpanish for the next evaluation, scheduled for June.
Thiswill require improvements to the Spanish dictionary andde-inflector, an update of the Spanish grammar f om theolder Spanish system, a lexicalized grammar trained onSpanish text, and Spanish rewrite rules.
We intend touse the parallel Spanish-English component of the UNdata to provide gloss information.167REFERENCES[l] J. Yamron, J. Baker, Paul Bamberg, Hukon Cheva-lier, Taiko Dietzel, John Elder, Frank Kampmuu, MarkM~ndel, Linda Muganaro, Todd Margolis, ud  Eliza-beth Steele, LINGSTAT: An interactive, Machine.AidedTranslation System, Proceedings of the ARPA HumanTechnology Workshop, March 1993.
[2] J.K. Baker, Trainable Grammara for Speech Recognition,Speech Communication Papers for the 9Tth Meeting ofthe Acoustical Society of America (D.H. Klatt ud  J.J.Wolf, eds.
), pp.
547-550, 1979.
[3] F. Jelinek, J.D.
Lat'erty, and R.L.
Mercer, Baai?Methods of Probabiliati?
Contcz~-~lee~ Grammars, inSpeech Recognition and Underatanding: Recent Advan-ceJ, Trenda, and Applications, P. Ldace ud R. De Mori,eds., SprlngeroVedag, Series F: Computer and SystemsSciences, voL 75, 1992.168
