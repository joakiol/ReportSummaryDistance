Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 96?103,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsHandling Sparsity for Verb Noun MWE Token ClassificationMona T. DiabCenter for Computational Learning SystemsColumbia Universitymdiab@ccls.columbia.eduMadhav KrishnaComputer Science DepartmentColumbia Universitymadhkrish@gmail.comAbstractWe address the problem of classifyingmultiword expression tokens in runningtext.
We focus our study on Verb-NounConstructions (VNC) that vary in their id-iomaticity depending on context.
VNCtokens are classified as either idiomaticor literal.
Our approach hinges uponthe assumption that a literal VNC willhave more in common with its componentwords than an idiomatic one.
Commonal-ity is measured by contextual overlap.
Tothis end, we set out to explore differentcontextual variations and different similar-ity measures handling the sparsity in thepossible contexts via four different param-eter variations.
Our approach yields stateof the art performance with an overall ac-curacy of 75.54% on a TEST data set.1 IntroductionA Multi-Word Expression (MWE), for our pur-poses, can be defined as a multi-word unit thatrefers to a single concept, for example - kick thebucket, spill the beans, make a decision, etc.
AnMWE typically has an idiosyncratic meaning thatis more or different than the meaning of its compo-nent words.
An MWE meaning is transparent, i.e.predictable, in as much as the component wordsin the expression relay the meaning portended bythe speaker compositionally.
Accordingly, MWEsvary in their degree of meaning compositionality;compositionality is correlated with the level of id-iomaticity.
An MWE is compositional if the mean-ing of an MWE as a unit can be predicted from themeaning of its component words such as in makea decision meaning to decide.
If we conceive ofidiomaticity as being a continuum, the more id-iomatic an expression, the less transparent and themore non-compositional it is.MWEs are pervasive in natural language, espe-cially in web based texts and speech genres.
Iden-tifying MWEs and understanding their meaning isessential to language understanding, hence theyare of crucial importance for any Natural Lan-guage Processing (NLP) applications that aim athandling robust language meaning and use.To date, most research has addressed the prob-lem of MWE type classification for VNC expres-sions in English (Melamed, 1997; Lin, 1999;Baldwin et al, 2003; na Villada Moiro?n andTiedemann, 2006; Fazly and Stevenson, 2007;Van de Cruys and Villada Moiro?n, 2007; Mc-Carthy et al, 2007), not token classification.
Forexample: he spilt the beans over the kitchencounter is most likely a literal usage.
This is givenaway by the use of the prepositional phrase overthe kitchen counter, since it is plausable that beanscould have literally been spilt on a location such asa kitchen counter.
Most previous research wouldclassify spilt the beans as idiomatic irrespective ofusage.
A recent study by (Cook et al, 2008) of60 idiom MWE types concluded that almost halfof them had clear literal meaning and over 40% oftheir usages in text were actually literal.
Thus, itwould be important for an NLP application suchas machine translation, for example, when given anew MWE token, to be able to determine whetherit is used idiomatically or not.In this paper, we address the problem of MWEclassification for verb-noun (VNC) token con-structions in running text.
We investigate the bi-nary classification of an unseen VNC token ex-pression as being either Idiomatic (IDM) or Lit-eral (LIT).
An IDM expression is certainly anMWE, however, the converse is not necessarilytrue.
We handle the problem of sparsity for MWEclassification by exploring different vector spacefeatures: various vector similarity metrics, andmore linguistically oriented feature sets.
We eval-uate our results against a standard data set from thestudy by (Cook et al, 2007).
We achieve state ofthe art performance in classifying VNC tokens aseither literal (F-measure: F?1=0.64) or idiomatic(F?1=0.82), corresponding to an overall accuracyof 75.54%.This paper is organized as follows: In Section962 we describe our understanding of the variousclasses of MWEs in general.
Section 3 is a sum-mary of previous related research.
Section 4 de-scribes our approach.
In Section 5 we present thedetails of our experiments.
We discuss the resultsin Section 6.
Finally, we conclude in Section 7.2 Multi-word ExpressionsMWEs are typically not productive, though theyallow for inflectional variation (Sag et al, 2002).They have been conventionalized due to persistentuse.
MWEs can be classified based on their se-mantic types as follows.
Idiomatic: This categoryincludes expressions that are semantically non-compositional, fixed expressions such as kingdomcome, ad hoc, non-fixed expressions such as breaknew ground, speak of the devil.
Semi-idiomatic:This class includes expressions that seem seman-tically non-compositional, yet their semantics aremore or less transparent.
This category consistsof Light Verb Constructions (LVC) such as makea living and Verb Particle Constructions (VPC)such as write-up, call-up.
Non-Idiomatic: Thiscategory includes expressions that are semanti-cally compositional such as prime minister, propernouns such as New York Yankees.3 Previous Related WorkSeveral researchers have addressed the problem ofMWE classification (Baldwin et al, 2003; Katzand Giesbrecht, 2006; Schone and Juraksfy, 2001;Hashimoto et al, 2006; Hashimoto and Kawahara,2008).
The majority of the proposed research hasbeen using unsupervised approaches and have ad-dressed the problem of MWE type classificationirrespective of usage in context.
Only, the workby Hashimoto et al (2006) and Hashimoto andKawahara (2008) addressed token classification inJapanese using supervised learning.The most comparable work to ours is the re-search by (Cook et al, 2007) and (Fazly andStevenson, 2007).
On the other hand, (Cook etal., 2007) develop an unsupervised technique thatclassifies a VNC expression as idiomatic or literal.They examine if the similarity between the con-text vector of the MWE, in this case the VNC,and that of its idiomatic usage is higher than thesimilarity between its context vector and that ofits literal usage.
They define the vector dimen-sions in terms of the co-occurrence frequencies of1000 most frequent content bearing words (nouns,verbs, adjectives, adverbs and determiners) in thecorpus.
A context vector for a VNC expressionis defined in terms of the words in the sentencein which it occurs.
They employ the cosine mea-sure to estimate similarity between contextual vec-tors.
They assume that every instance of an ex-pression occurring in a certain canonical syntacticform is idiomatic, otherwise it is literal.
This as-sumption holds for many cases of idiomatic usagesince many of them are conventionalized, howeverin cases such as spilt the beans on the counter top,the expression would be misclassified as idiomaticsince it does occur in the canonical form thoughthe meaning in this case is literal.
Their workis similar to this paper in that they explore theVNC expressions at the token level.
Their methodachieves an accuracy of 52.7% on a data set con-taining expression tokens used mostly in their lit-eral sense, whereas it yields an accuracy of 82.3%on a data set in which most usages are idiomatic.Further, they report that a classifier that predictsthe idiomatic label if an expression (token) occursin a canonical form achieves an accuracy of 53.4%on the former data set (where the majority of theMWEs occur in their literal sense) and 84.7% onthe latter data set (where the majority of the MWEinstances are idiomatic).
This indicates that these?canonical?
forms can still be used literally.
Theyreport an overall system performance accuracy of72.4%.1(Fazly and Stevenson, 2007) correlate compo-sitionality with idiomaticity.
They measure com-positionality as a combination of two similarityvalues: firstly, similar to (Katz and Giesbrecht,2006), the similarity (cosine similarity) betweenthe context of a VNC and the contexts of its con-stituent words; secondly, the similarity between anexpression?s context and that of a verb that is mor-phologically related to the noun in the expression,for instance, decide for make a decision.
Contextcontext(t) of an expression or a word, t, is de-fined as a vector of the frequencies of nouns co-occurring with t within a window of ?5 words.The resulting compositionality measure yields anF?=1=0.51 on identifying literal expressions andF?=1=0.42 on identifying idiomatic expressions.However their results are not comparable to ourssince it is type-based study.1We note that the use of accuracy as a measure for thiswork is not the most appropriate since accuracy is a measureof error rather than correctness, hence we report F-measurein addition to accuracy.974 Our ApproachRecognizing the significance of contextual infor-mation in MWE token classification, we explorethe space of contextual modeling for the task ofclassifying the token instances of VNC expres-sions into literal versus idiomatic expressions.
In-spired by works of (Katz and Giesbrecht, 2006;Fazly and Stevenson, 2007), our approach is tocompare the context vector of a VNC with thecomposed vector of the verb and noun (V-N) com-ponent units of the VNC when they occur in iso-lation of each other (i.e., not as a VNC).
For ex-ample, in the case of the MWE kick the bucket, wecompare the contexts of the instances of the VNCkick the bucket against the combined contexts forthe verb (V) kick, independent of the noun bucket,and the contexts for the noun (N) bucket, indepen-dent of the verb kick.
The intuition is that if thereis a high similarity between the VNC and the com-bined V and N (namely, the V-N vector) contextsthen the VNC token is compositional, hence a lit-eral instance of the MWE, otherwise the VNC to-ken is idiomatic.Previous work, (Fazly and Stevenson, 2007),restricted context to within the boundaries of thesentences in which the tokens of interest oc-curred.
We take a cue from that work but de-fine ?context(t)?
as a vector with dimensions asall word types occurring in the same sentence ast, where t is a verb type corresponding to the Vin the VNC, noun type corresponding to N in theVNC, or VNC expression instance.
Moreover, ourdefinition of context includes all nouns, verbs, ad-jectives and adverbs occurring in the same para-graph as t. This broader notion of context shouldhelp reduce sparseness effects, simply by enrich-ing the vector with more contextual information.Further, we realize the importance of some closedclass words occurring in the vicinity of t. (Cooket al, 2007) report the importance of determin-ers in identifying idiomaticity.
Prepositions tooshould be informative of idiomaticity (or literal us-age) as illustrated above in spill the beans on thekitchen counter.
Hence, we include determinersand prepositions occurring in the same sentence ast.
The composed V-N contextual vector combinesthe co-occurrence of the verb type (aggregation ofall the verb token instances in the whole corpus)as well as the noun type with this predefined setof dimensions.
The VNC contextual vector is thatfor a specific instance of a VNC expression.Our objective is to find the best experimentalsettings that could yield the most accurate classifi-cation of VNC expression tokens taking into con-sideration the sparsity problem.
To that end, weexplore the space of possible parameter variationon the vectors representing our tokens of interest(VNC, V, or N).
We experiment with five differentparameter settings:Context-Extent The definition of context isbroad or narrow described as follows.
BothContextBroad and ContextNarrow comprise allthe open class or content words (nouns, verbs, ad-jectives and adverbs), determiners, and preposi-tions in the sentence containing the token.
More-over, ContextBroad, additionally, includes thecontent words from the paragraph in which thetoken occurs.Dimension This is a pruning parameter on thewords included from the Context Extent.
The in-tuition is that salient words should have a big-ger impact on the calculation of the vector sim-ilarity.
This parameter is varied in three ways:DimensionNoThresh includes all the words thatco-occur with the token under consideration inthe specified context extent; DimensionFreqsets a threshold on the co-occurrence frequencyfor the words to include in the dimensionsthereby reducing the dimensionality of the vectors.DimensionRatio is inspired by the utility of thetf-idf measure in information retrieval, we devisea threshold scheme that takes into considerationthe salience of the word in context as a function ofits relative frequency.
Hence the raw frequenciesof the words in context are converted to a ratio oftwo probabilities as per the following equation.ratio = p(word|context)p(word) =freq(word in context)freq(context)freq(word in corpus)N (1)where N is the number of words (tokens) inthe corpus and freq(context) is the number ofcontexts for a specific token of interest occurs.The numerator of the ratio is the probability thatthe word occurs in a particular context.
The de-nominator is the probability of occurrence of theword in the corpus.
Here, more weight is placedon words that are frequent in a certain context butrarer in the entire corpus.
In case of the V and Ncontexts, a suitable threshold, which is indepen-98dent of data size, is determined on this ratio in or-der to prune context words.The latter two pruning techniques,DimensionFreq and DimensionRatio, arenot performed for a VNC token?s context, hence,all the words in the VNC token?s contextualwindow are included.
These thresholding meth-ods are only applied to V-N composed vectorsobtained from the combination of the verb andnoun vectors.Context-Content This parameter had two set-tings: words as they occur in the cor-pus, Context ?
ContentWords; or some ofthe words are collapsed into named entities,Context ?
ContentWords+NER.
Context ?ContentWords+NER attempts to perform dimen-sionality reduction and sparsity reduction by col-lapsing named entities.
The intuition is that ifwe reduce the dimensions in semantically salientways we will not adversely affect performance.We employ BBN?s IdentiFinder Named EntityRecognition (NER) System2.
The NER system re-duces all proper names, months, days, dates andtimes to NE tags.
NER tagging is done on the cor-pus before the context vectors are extracted.
Forour purposes, it is not important that John kickedthe bucket on Friday in New York City ?
neitherthe specific actor of the action, nor the place whereis occurs is of relevance.
The sentence PERSONkicked the bucket on DAY in PLACE conveys thesame amount of information.
IdentiFinder identi-fies 24 NE types.
We deem 5 of these inaccuratebased on our observation, and exclude them.
Weretain 19 NE types: Animal, Contact Information,Disease, Event, Facility, Game, Language, Loca-tion (merged with Geo-political Entity), Nation-ality, Organization, Person, Product, Date, Time,Quantity, Cardinal, Money, Ordinal and Percent-age.
The written-text portion of the BNC contains6.4M named entities in 5M sentences (at least oneNE per sentence).
The average number of wordsper NE is 2.56, the average number of words persentence is 18.36.
Thus, we estimate that by us-ing NER, we reduce vector dimensionality by atleast 14% without introducing the negative effectsof sparsity.V-N Combination In order to create a singlevector from the units of a VNC expression, weneed to combine the vectors pertaining to the verb2http://www.bbn.com/technology/identifindertype (V) and the noun type (N).
After combin-ing the word types in the vector dimensions, weneed to handle their co-occurrence frequency val-ues.
Hence we have two methods: addition wherewe simply add the frequencies in the cases ofthe shared dimensions which amounts to a unionwhere the co-occurrence frequencies are added;or multiplication which amounts to an inter-section of the vector dimensions where the co-occurrence frequencies are multiplied, hence giv-ing more weight to the shared dimensions than inthe addition case.
In a study by (Mitchell and La-pata, 2008) on a sentence similarity task, a multi-plicative combination model performs better thanthe additive one.Similarity Measures We experiment with sev-eral standard similarity measures: Cosine Similar-ity, Overlap similarity, Dice Coefficient and Jac-card Index as defined in (Manning and Schu?tze,1999).
A context vector is converted to a set byusing the dimensions of the vector as members ofthe set.5 Experiments and Results5.1 DataWe use the British National Corpus (BNC),3which contains 100M words, because it draws itstext from a wide variety of domains and the ex-isting gold standard data sets are derived from it.The BNC contains multiple genres including writ-ten text and transcribed speech.
We only experi-ment with the written-text portion.
We syntacti-cally parse the corpus with the Minipar4 parser inorder to identify all VNC expression tokens in thecorpus.
We exploit the lemmatized version of thetext in order to reduce dimensionality and sparse-ness.
The standard data used in (Cook et al, 2007)(henceforth CFS07) is derived from a set compris-ing 2920 unique VNC-Token expressions drawnfrom the whole BNC.
In this set, VNC token ex-pressions are manually annotated as idiomatic, lit-eral or unknown.For our purposes, we discard 127 of the 2920token gold standard data set either because theyare derived from the speech transcription por-tion of the BNC, or because Minipar could notparse them.
Similar to the CFS07 set, we ex-clude expressions labeled unknown or pertaining3http://www.natcorp.ox.ac.uk/4http://www.cs.ualberta.ca/ lindek/minipar.htm99to the skewed data set as deemed by the annota-tors.
Therefore, our resulting data set comprises1125 VNC token expressions (CFS07 has 1180).We then split them into a development (DEV) setand a test (TEST) set.
The DEV set comprises564 token expressions corresponding to 346 id-iomatic (IDM) expressions and 218 literal (LIT)ones (CFS07 dev has 573).
The TEST set com-prises 561 token expressions corresponding to 356IDM expression tokens and 205 LIT ones (CFS07test has 607).
There is a complete overlap in typesbetween our DEV and CFS07?s dev set and ourTEST and CFS07?s test set.
They each comprise14 VNC type expressions with no overlap in typebetween the TEST and DEV sets.
We divide thetokens between the DEV and TEST maintainingthe same proportions of IDM to LIT as recom-mended in CFS07: DEV is 61.5% and TEST is63.7%.5.2 Experimental Set-upWe vary four of the experimental parameters:Context-Extent {sentence only narrow (N), sen-tence + paragraph broad(B)}, Context-Content{Words (W), Words+NER (NE)}, Dimension {nothreshold (nT), frequency (F), ratio (R)}, and V-N compositionality {Additive (A), Multiplicative(M)}.
We present the results for all similarity mea-sures.
The thresholds (for DimensionFreq andDimensionRatio) are tuned on all the similaritymeasures collectively.
It is observed that the per-formance of all the measures improved/worsenedtogether, illustrating the same trends in perfor-mance, over the various settings of the thresholdsevaluated on the DEV data set.
Based on tuningon the DEV set, we empirically set the value ofthe threshold on F to be 188 and for R to be 175across all experimental conditions.
We present re-sults here for 10 experimental conditions based onthe four experimental parameters: {nT-A-W-N,nT-M-W-N, F-A-W-N, F-M-W-N, R-A-W-N, R-M-W-N, R-A-W-B, R-M-W-B, R-A-NE-B, R-M-NE-B}.
For instance, R-A-W-N, the Dimen-sion parameter is set to the Ratio DimensionRatio(R), the V-N compositionality mode is addition(A), and the Context-Content is set to Context ?ContentWords (W), and, Context-Extent is set toContextNarrow (N).5.3 ResultsWe use F?=1 (F-measure) as the harmonic meanbetween Precision and Recall, as well as accu-racy to report the results.
We report the resultsseparately for the two classes IDM and LIT onthe DEV and TEST data set for all four similar-ity measures.6 DiscussionAs shown in Table 2, we obtain the best classifi-cation accuracy of 75.54% (R-A-NE-B) on TESTusing the Overlap similarity measure, with F?=1values for the IDM and LIT classes being 0.82and 0.64, respectively.
These results are generallycomparable to state-of-the-art results obtained byCFS07 who report an overall system accuracy of72.4% on their test set.
Hence, we improve overstate-of-the-art results by 3% absolute.In the DEV set, the highest results (F-measuresfor IDM and LIT, as well as accuracy scores) areobtained for all conditions consistently using theOverlap similarity measure.
We also note that ourapproach tends to fare better overall in classifyingIDM than LIT.
The best performance is obtainedin experimental setting R-A-NE-B at 78.53% ac-curacy corresponding to an IDM classification F-measure of 0.83 and LIT classification F-measureof 0.71.In the TEST set, we note that Overlap simi-larity yields the highest overall results, howeverinconsistently across all the experimental condi-tions.
The highest scores are yielded by the sameexperimental condition R-A-NE-B.
In fact, com-parable to previous work, the Cosine similaritymeasure significantly outperforms the other sim-ilarity measures when the Dimension parameter isset to no threshold (nT) and with a set threshold onfrequency (F).
However, Cosine is outperformedby Overlap when we apply a threshold to the RatioDimension.
It is worth noting that across all exper-imental conditions (except in one case, nT-A-W-N using Overlap similarity), IDM F-measures areconsistently higher than LIT F-measures, suggest-ing that our approach is more reliable in detectingidiomatic VNC MWE rather than not.The overall results strongly suggest that us-ing intelligent dimensionality reduction, such asa threshold on the ratio, significantly outperformsno thresholding (nT) and simple frequency thresh-olding (F) comparing across different similaritymeasures and all experimental conditions.
Recallthat R was employed to maintain the salient sig-nals in the context and exclude those that are irrel-evant.100Experiment Dice Coefficient Jaccard Index Overlap CosineF-measure Acc.
% F-measure Acc.
% F-measure Acc.
% F-measure Acc.
%IDM LIT IDM LIT IDM LIT IDM LITnT-A-W-N 0.45 0.44 44.39 0.47 0.43 44.92 0.50 0.56 53.30 0.49 0.42 45.63nT-M-W-N 0.48 0.46 46.88 0.48 0.46 46.88 0.58 0.57 57.78 0.46 0.47 46.52F-A-W-N 0.47 0.47 46.70 0.47 0.47 46.70 0.58 0.53 55.62 0.50 0.50 50.09F-M-W-N 0.48 0.49 48.31 0.48 0.49 48.31 0.58 0.57 57.40 0.54 0.50 52.05R-A-W-N 0.79 0.62 72.73 0.79 0.62 72.73 0.79 0.63 73.44 0.79 0.62 72.73R-M-W-N 0.76 0.06 62.21 0.76 0.06 62.21 0.77 0.06 62.39 0.77 0.06 62.39R-A-W-B 0.59 0.57 58.11 0.59 0.57 58.11 0.80 0.72 76.47 0.67 0.65 65.78R-M-W-B 0.67 0.63 65.06 0.67 0.63 65.06 0.80 0.71 76.65 0.71 0.66 68.81R-A-NE-B 0.58 0.58 58.14 0.58 0.58 58.14 0.83 0.71 78.53 0.70 0.64 67.08R-M-NE-B 0.63 0.63 62.79 0.63 0.63 62.79 0.76 0.69 73.17 0.73 0.67 70.13Table 1: Evaluation on of different experimental conditions on DEVExperiment Dice Coefficient Jaccard Index Overlap CosineF-measure Acc.
% F-measure Acc.
% F-measure Acc.
% F-measure Acc.
%IDM LIT IDM LIT IDM LIT IDM LITnT-A-W-N 0.58 0.48 53.50 0.62 0.49 56.37 0.43 0.50 46.32 0.63 0.48 56.37nT-M-W-N 0.58 0.46 52.60 0.53 0.48 50.45 0.53 0.50 51.71 0.55 0.51 52.78F-A-W-N 0.60 0.48 55.12 0.60 0.48 55.12 0.46 0.36 41.47 0.60 0.46 54.04F-M-W-N 0.56 0.48 52.07 0.56 0.48 52.07 0.49 0.45 47.04 0.62 0.49 56.19R-A-W-N 0.81 0.57 73.61 0.81 0.57 73.61 0.82 0.57 74.51 0.81 0.57 73.61R-M-W-N 0.78 0.09 64.99 0.78 0.09 64.99 0.78 0.08 64.81 0.78 0.08 64.81R-A-W-B 0.69 0.57 64.11 0.62 0.56 59.11 0.78 0.66 73.04 0.68 0.60 64.64R-M-W-B 0.64 0.60 61.79 0.64 0.60 61.79 0.78 0.64 72.86 0.69 0.62 65.89R-A-NE-B 0.61 0.56 58.45 0.61 0.56 58.45 0.82 0.64 75.54 0.68 0.58 63.37R-M-NE-B 0.59 0.58 58.63 0.59 0.58 58.63 0.76 0.65 71.40 0.69 0.61 65.29Table 2: Evaluation of different experimental conditions on TESTThe results suggest some interaction betweenthe vector combination method, A or M, and theDimensionality pruning parameters.
Experimen-tal conditions that apply the multiplicative compo-sitionality on the component vectors V and N yieldhigher results in the nT and F conditions across allthe similarity measures.
Yet once we apply R di-mensionality pruning, we see that the additive vec-tor combination, A parameter setting, yields bet-ter results.
This indicated that the M conditionalready prunes too much in addition to the R di-mensionality hence leading to slightly lower per-formance.For both DEV and TEST, we note that the R pa-rameter settings coupled with the A parameter set-ting.
For DEV, we observe that the results yieldedfrom the Broad context extent, contextual sentenceand surrounding paragraph, yield higher resultsthan those obtained from the narrow N, contextsentence only, across M and A conditions.
Thistrend is not consistent with the results on the TESTdata set.
R-A-W-N, outperforms R-A-W-B, how-ever, R-M-W-B outperforms R-M-W-N.We would like to point out that R-M-W-N hasvery low values for the LIT F-measure, this is at-tributed to the use of a unified R threshold valueof 175.
We experimented with different optimalthresholds for R depending on the parameter set-ting combination and we discovered that for R-M-W-N, the fine-tuned optimal threshold shouldhave been 27 as tuned on the DEV set, yieldingLIT F-measures of 0.68 and 0.63, for DEV andTEST, respectively.
Hence when using the uni-fied value of 175, more of the compositional vec-tors components of V+N are pruned away leadingto similarity values between the V+N vector andthe VNC vector of 0 (across all similarity mea-sures).
Accordingly, most of the expressions are101mis-classified as IDM.The best results overall are yielded from the NEconditions.
This result strongly suggests that usingclass based linguistic information and novel waysto keep the relevant tokens in the vectors such asR yields better MWE classification.Qualitatively, we note the best results are ob-tained on the following VNCs from the TESTset in the Overlap similarity measure for the R-A-W-B experimental setting (percentage of to-kens classified correctly): make hay(94%), makemark(88%), pull punch (86%), have word(81%),blow whistle (80%), hit wall (79%), hold fire(73%).
While we note the highest performanceon the following VNCs in the corresponding R-A-NE-B experimental setting: make hay(88%), makemark(87%), pull punch (91%), have word(85%),blow whistle (84%), hold fire (82%).
We observethat both conditions performed the worse on to-kens from the following VNCs lose thread, makehit.
Especially, make hit is problematic since itmostly a literal expression, yet in the gold stan-dard set we see it marked inconsistently.
For in-stance, the literal sentence He bowled it himselfand Wilfred Rhodes made the winning hit whilethe following annotates make hit as idiomatic: Itwas the TV show Saturday Night Live which orig-inally made Martin a huge hit in the States.We also note the difference in performance inthe hard cases of VNCs that are relatively trans-parent, only the R-A-W-B and R-A-NE-B exper-imental conditions were able to classify them cor-rectly with high F-measures as either IDM or LIT,namely: have word, hit wall, make mark.
For R-A-W-B, the yielded accuracies are 81%, 79% and88% respectively, and for R-A-NE-B, the accura-cies are 85%, 65%, and 87%, respectively.
How-ever, in the nT-A-W-N condition have word isclassified incorrectly 82% of the time and in F-A-W-N it is classified incorrectly 85% of the time.Make mark is classified incorrectly 77% of thetime, make hay (77%) and hit wall (57%) in theF-A-W-N experimental setting.
This may be at-tributed to the use of the Broader context, or theuse of R in the other more accurate experimentalsettings.7 ConclusionIn this study, we explored a set of features thatcontribute to VNC token expression binary clas-sification.
We applied dimensionality reductionheuristics inspired by information retrieval (tf-idflike ratio measure) and linguistics (named-entityrecogniiton).
These contributions improve signif-icantly over experimental conditions that do notmanipulate context and dimensions.
Our systemachieves state-of-the-art performance on a set thatis very close to a standard data set.
Different fromprevious studies, we classify VNC token expres-sions in context.
We include function words inmodeling the VNC token contexts as well as usingthe whole paragraph in which it occurs as context.Moreover we empirically show that the Overlapsimilarity measure is a better measure to use forMWE classification.8 AcknowledgementThe first author was partially funded by DARPAGALE and MADCAT projects.
The authors wouldlike to acknowledge the useful comments by threeanonymous reviewers who helped in making thispublication more concise and better presented.ReferencesTimothy Baldwin, Collin Bannard, Takakki Tanaka,and Dominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL 2003 workshop on Multiwordexpressions, pages 89?96, Morristown, NJ, USA.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2007.
Pulling their weight: Exploiting syntacticforms for the automatic identification of idiomaticexpressions in context.
In Proceedings of the Work-shop on A Broader Perspective on Multiword Ex-pressions, pages 41?48, Prague, Czech Republic,June.
Association for Computational Linguistics.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2008.
The VNC-Tokens Dataset.
In Proceedings ofthe LREC Workshop on Towards a Shared Task forMultiword Expressions (MWE 2008), Marrakech,Morocco, June.Afsaneh Fazly and Suzanne Stevenson.
2007.
Dis-tinguishing subtypes of multiword expressions us-ing linguistically-motivated statistical measures.
InProceedings of the Workshop on A Broader Perspec-tive on Multiword Expressions, pages 9?16, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Chikara Hashimoto and Daisuke Kawahara.
2008.Construction of an idiom corpus and its applica-tion to idiom identification based on WSD incor-porating idiom-specific features.
In Proceedings ofthe 2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 992?1001, Hon-102olulu, Hawaii, October.
Association for Computa-tional Linguistics.Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.2006.
Japanese idiom recognition: Drawing a linebetween literal and idiomatic meanings.
In Proceed-ings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 353?360, Sydney, Australia,July.
Association for Computational Linguistics.Graham Katz and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.
InProceedings of the Workshop on Multiword Expres-sions: Identifying and Exploiting Underlying Prop-erties, pages 12?19, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL-99,pages 317?324, Univeristy of Maryland, CollegePark, Maryland, USA.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press, June.Diana McCarthy, Sriram Venkatapathy, and AravindJoshi.
2007.
Detecting compositionality of verb-object combinations using selectional preferences.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 369?379, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Dan I. Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Pro-ceedings of the 2nd Conference on Empirical Meth-ods in Natural Language Processing (EMNLP?97),pages 97?108, Providence, RI, USA, August.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof ACL-08: HLT, pages 236?244, Columbus, Ohio,June.
Association for Computational Linguistics.Bego na Villada Moiro?n and Jo?rg Tiedemann.
2006.Identifying idiomatic expressions using automaticword-alignment.
In Proceedings of the EACL-06Workshop on Multiword Expressions in a Multilin-gual Context, pages 33?40, Morristown, NJ, USA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.Copestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for nlp.
In Pro-ceedings of the Third International Conference onComputational Linguistics and Intelligent Text Pro-cessing, pages 1?15, London, UK.
Springer-Verlag.Patrick Schone and Daniel Juraksfy.
2001.
Isknowledge-free induction of multiword unit dictio-nary headwords a solved problem?
In Proceedingsof Empirical Methods in Natural Language Process-ing, pages 100?108, Pittsburg, PA, USA.Tim Van de Cruys and Begon?a Villada Moiro?n.
2007.Semantics-based multiword expression extraction.In Proceedings of the Workshop on A Broader Per-spective on Multiword Expressions, pages 25?32,Prague, Czech Republic, June.
Association for Com-putational Linguistics.103
