Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 95?100,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsEmpty element recovery by spinal parser operationsKatsuhiko Hayashi and Masaaki NagataNTT Communication Science Laboratories, NTT Corporation2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan{hayashi.katsuhiko, nagata.masaaki}@lab.ntt.co.jpAbstractThis paper presents a spinal parsing al-gorithm that can jointly detect empty ele-ments.
This method achieves state-of-the-art performance on English and Japaneseempty element recovery problems.1 IntroductionEmpty categories, which are used in Penn Tree-bank style annotations to represent complex syn-tactic phenomena like constituent movement anddiscontinuous constituents, provide important in-formation for understanding the semantic structureof sentences.
Previous studies attempt empty ele-ment recovery by casting it as linear tagging (Di-enes and Dubey, 2003), PCFG parsing (Schmid,2006; Cai et al, 2011) or post-processing of syn-tactic parsing (Johnson, 2002; Gabbard et al,2006).
To the best of our knowledge, the resultsreported by (Cai et al, 2011) are the best yet re-ported, so we pursue a method that uses syntacticparsing to jointly solve the empty element recov-ery problem.Our proposal uses the spinal Tree AdjoiningGrammar (TAG) formalism of (Carreras et al,2008).
The spinal TAG has a set of elementarytrees, called spines, each consisting of a lexicalanchor with a series of unary projections.
Fig-ure 1 displays (a) a head-annotated constituent treeand (b) spines extracted from the tree.
This pa-per presents a transition-based algorithm togetherwith several operations to combine spines for con-structing full parse trees with empty elements.Compared with the PCFG parsing approaches, oneadvantage of our method is its flexible featurerepresentations, which allow the incorporation ofconstituency-, dependency- and spine-based fea-tures.
Of particular interest, the motivation forour spinal TAG-based approach comes from the.(a).S..VP-H..VP..NP..SBAR..S..VP-H..VP..ADVP.*T*-H.*e*..NP.NN-H.business..VB-H.get..TO-H.to..NP.*-H.*e*..WHADVP-H.0-H.*e*..NP-H..NN-H.way.DT.any..VB-H.find..MD-H.must..NP.PRP-H.We.(b).NP.PRP.We.S.VP.MD.must.VP.VB.find.DT.any.NP.NN.way.SBAR.WHADVP.0.*e*.NP.*.*e*.S.VP.TO.to.VP.VB.get.NP.NN.business.ADVP.*T*.
*e*Figure 1: (a) an example of a constituent tree withhead annotations denoted by -H; (b) spinal ele-mentary trees extracted from the parse tree (a).intuition that features extracted from spines canbe expected to be useful for empty element recov-ery in the same way as constituency-based verticalhigher-order conjunctive features are used in re-cent post-processing methods (Xiang et al, 2013;Takeno et al, 2015).
Experiments on English andJapanese datasets empirically show that our sys-tem outperforms existing alternatives.2 Spinal Tree Adjoining GrammarsWe define here the spinal TAG G = (N,PT,T,LS)where N is a set of nonterminal symbols, PT isa set of pre-terminal symbols (or part-of-speechtags), T is a set of terminal symbols (or words),and LS is a set of lexical spines.
Each spine, s,has the form n0?
n1?
???
?
nk?1?
nk(k ?
N)which satisfies the conditions:?
n0?
T and n1?
PT ,?
?i ?
[2,k], ni?
N.The height of spine s is ht(s) = k+1 and for someposition i ?
[0,k], the label at i is s(i) = ni.
Tak-95.(a).DT.any.NP@2.NN@1.way?.NP@2..NN@1.way..DT.any.(b).NP@2..NN@1.way..DT.any.SBAR..S..
.
...WHADVP.0.*e*?.NP@2..SBAR..S..
.
...WHADVP.0.*e*..NP@2..NN@1.way.DT.any.(c).NP.*.*e*.S@3..VP..
.
...VP@2.TO@1.to?.S@3..VP.. .
...VP@2.TO@1.to.NP.*.*e*.(d).SBAR@3.WHADVP@2.0@1.*e*.S..VP..
.
...VP.TO.to.NP.*.*e*?.SBAR@3..S..VP..
.
...VP.TO.to.NP.*.*e*..WHADVP@2.0@1.
*e*Figure 2: An example of parser operations: (a)sister adjunction left (b) regular adjunction right(c) insert left (d) combine right.ing the leftmost spine s = We ?
PRP ?
NP inFigure 1 (b), ht(s) = 3 and s(1) = PRP.The spinal TAG uses two operations, sister andregular adjunctions, to combine spines.
Both ad-junctions also have left and right types.
Fig-ures 2 (a) and (b) show examples of sister adjunc-tion left and regular adjunction right operations.We use @# to illustrate node position on a spine,explicitly.
After a regular adjunction, the result-ing tree has an additional node level which has acopy of its original node at position @x, while asister adjunction simply inserts a spine into somenode of another spine.
If adjunction left (or right)inserts spine s1into some node at @x on spine s2,we call s2the head spine of s1and s1the left (orright) child spine of s21.
This paper denotes sisteradjunction left and right as s1?
?xs2, s2?
?xs1, reg-ular adjunction left and right as s1?
?xs2, s2?
?xs1,respectively.3 Arc-Standard Shift-Reduce SpinalTAG ParsingThere are three algorithms for spinal TAG parsing,(1) Eisner-Satta CKY (Carreras et al, 2008), (2)arc-eager shift-reduce (Ballesteros and Carreras,2015) and (3) arc-standard shift-reduce (Hayashiet al, 2016) algorithms.
This paper uses the arc-1After adjunctions, the result forms a phrase consisting ofseveral spines.
If a phrasal spine is also used in adjunctionoperations as Figure 2 (b), we treat it as a lexical spine byreferring to its head spine.standard shift-reduce algorithm since it provides amore simple implementation.A transition system for spinal TAG parsing isthe tuple S = (C,T, I,Ct), where C is a set of con-figurations, T is a set of transitions, which are par-tial functions t : C ?
C, I is a total initializationfunction mapping each input string to a uniqueconfiguration, and Ct?C is a set of terminal con-figurations.
A configuration is the tuple (?,?
,A)where ?
is a stack of stack elements, ?
is a bufferof elements from an input, and A is a set of parseroperations.
A stack element s is a pair (s, j)wheres is a spine and j is a node index of s. We refer tos and j of s as s.s and s. j, respectively.Let x = ?w1/t1, .
.
.
,wn/tn?
(?i ?
[1,n], wi?
Tand ti?
PT ) be a pos-tagged input sentence.
Thearc-standard transition system by Hayashi et al(2016) can be defined as follows: its initializationfunction is I(x) = ([], [w1/t1, .
.
.
,wn/tn], /0), its setof terminal configurations is Ct= ([], [],A), and ithas the following transitions:1. for each s ?
LS with s(0) = wiand s(1) = ti,a shift transition of the form (?,wi/ti|?
,A) ?(?|s1,?
,A) where s1= (s,2)2;2-3. for each j with s1.
j ?
j < ht(s1.s), a sisteradjunction left transition of the form(?
|s2|s1,?
,A) ?
(?
|s?1,?
,A?
{s2.s ?
?js1.s})and a regular adjunction left transition of theform(?
|s2|s1,?
,A) ?
(?
|s?1,?
,A?
{s2.s ?
?js1.s})where s?1= (s1.s, j);4-5. for each j with s2.
j ?
j < ht(s2.s), a sisteradjunction right transition of the form(?
|s2|s1,?
,A) ?
(?
|s?1,?
,A?
{s2.s ?
?js1.s})and a regular adjunction right transition of theform(?
|s2|s1,?
,A) ?
(?
|s?1,?
,A?
{s2.s ?
?js1.s})where s?1= (s2.s, j);6. a finish transition of the form ([s], [],A) ?
([], [],A).2To construct a full parse tree from A, our actual imple-mentation attaches index i to spine s after shift transition.96.S..VP-H..SBAR..S-H.*T*-H.*e*..0.*e*..VBZ-H.says..NP.PRP-H.he.,.,.S..VP-H..NP.DT-H.all..RB.not.VBZ-H.
?s..NP.DT-H.ThatFigure 3: A phrasal empty spine shown on theshaded region.To reduce search errors, Hayashi et al (2016) em-ployed beam search with Dynamic Programmingof (Huang and Sagae, 2010).
For experiments, wealso use this technique and discriminative model-ing of (Hayashi et al, 2016).4 Empty Element Recovery4.1 Spinal TAG with Empty ElementsIn this paper, we redefine the spinal TAG as G =(N,PT,T,LS,*e*,ET,ES), where *e* is a specialword, ET is a set of empty categories, and ES isa set of empty spines.
An empty spine s = n0?n1?
???
?
nk?1?
nk(k ?
N) has the same formas lexical spines, but n0= *e* and n1?
ET .
Theheight and label definitions are also the same asthose of lexical spines.
For example, the rightmostspine s = *e* ?
*T* ?
ADVP in Figure 1 (b) isan empty spine with ht(s) = 3 and s(1) = *T*.This paper extends empty spines to allow theuse of phrasal constituents that consist of onlyempty elements, as a single spine.
A phrasalempty spine is a tuple (t,h), where t is a sequenceof (phrasal) empty spines specifying some sisteradjunctions between these spines and h is a headspine in t. The phrasal empty spine in Figure 3consists of two empty spines *e* ?
0 and *e*?
*T* ?
S ?
SBAR, where a sister adjunctionleft is performed at the SBAR node of the latterspine, which is a head spine in the phrase.
To ap-ply parser operations to a phrasal empty spine, weuse its head spine rather than itself.
This paper de-fines the height and label of a phrasal empty spineas those of its head spine.To recover empty elements, this paper intro-duces two additional operations, insert and com-bine, both of which have left and right types.
Fig-ures 2 (c) and (d) show insert left and combineright operations.
These operations are similar tosister adjunctions in that the former simply insertssome phrasal empty spine into some node of an-other spine and the latter also inserts a spine intosome node of a phrasal empty spine.4.2 New TransitionsTo handle empty spines in parsing process, we addthe following five transitions to the arc-standardtransition system of (Hayashi et al, 2016):7-8. for each s ?
ES and each j with s1.
j ?
j <ht(s1.s), an insert left transition of the form(?
|s1,?
,A) ?
(?
|s?1,?
,A?
{s ?
?js1.s})and an insert right transition of the form(?
|s1,?
,A) ?
(?
|s?1,?
,A?
{s1.s ?
?js})where s?1= (s1.s, j);9-10. for each s?ES and each j with 2?
j< ht(s),a combine left transition of the form(?
|s1,?
,A) ?
(?
|s?1,?
,A?
{s1.s ?
?js})and a combine right transition of the form(?
|s1,?
,A) ?
(?
|s?1,?
,A?
{s ?
?js1.s})where s?1= (s, j);11. an idle transition of the form (?
|s1,?
,A) ?(?
|s1,?
,A);Like unary and idle rules in shift-reduce CFGparsing (Zhu et al, 2013), our current system pro-hibits > b consecutive actions consisting of onlyinsert, combine and idle operations.
Given aninput sentence with length n, after performing nshift, n?
1 adjunction, b ?
(2n?
1) {insert, com-bine or idle} actions, the system triggers the finishaction and terminates.
For training, we make ora-cle derivations using the stack-shortest strategy.5 Related WorkTo realize empty element recovery, other lexical-ized TAG formalisms (Chen and Shanker, 2004;Shen et al, 2008) attach some or all empty el-ements directly to surface word lexicons.
Ourframework, however, uses spinal TAG parser op-erations as they provide more efficient parsing andmore compact sets of lexicons.
It is remarkablethat this paper is the first study to present a shift-reduce spinal TAG parsing algorithm to recoverempty elements.Recent work has shown that empty element re-covery can be effectively solved in conjunction97Tagger Lattice ProposedM O M O M O Gold*ICH* 2 5 2 2 31 43 78*RNR* 0 3 0 4 4 5 6*EXP* 10 12 0 0 19 26 30Table 2: Result Analysis: M denotes the numberof matches of system outputs (O) with the gold.00.511.5210  20  30  40  50  60  70parsing time(secs)sentence lengthH16BerkC11PropFigure 4: Scatter plot of parsing time against sen-tence length, comparing with Hayashi16, Berkeleyand Cai11 parsers.with parsing (Schmid, 2006; Cai et al, 2011).Schmid (2006) annotated a constituent tree withslash features to recover a direct path from a fillernode to its trace.
Cai et al (2011) successfully in-tegrated empty element recovery into lattice pars-ing for latent PCFGs.
Compared with PCFG pars-ing, the spinal TAG parser provides a more flexiblefeature representation.6 Experiments6.1 Experiments on the English PennTreebankWe used the Wall Street Journal (WSJ) part of theEnglish Penn Treebank: Sections 02?21 were usedfor training, Section 22 for development, and Sec-tion 23 for testing.
We annotated trees with headsby treep (Chiang and Bikel, 2002)3with the appli-cation of Collins?s head rules.
The 78524 lexicaland 115 phrasal empty spine types were obtainedfrom the training data4.
The set of phrasal emptyspines covered all phrasal empty spines extractedfrom the development data.We used the Stanford part-of-speech tagger totag development and test data.
To train the pro-posed parsing model, we used the violation?fixing3http://www3.nd.edu/?dchiang/software/treep/treep.html4Excluding words from lexical spines, there were 1080lexical spine types.Typed-empty (t,i,i) All BracketsP R F1 P R F1Rule 57.4 50.5 53.7 ?
?
?Takeno15 60.4 50.6 55.1 ?
?
?Tagger 63.1 34.7 44.8 72.9 68.6 70.7Lattice 64.1 52.2 57.5 73.7 70.6 72.1Proposed 65.3 57.6 61.2 74.3 72.8 73.6Table 3: Results on the Japanese Keyaki Treebank.perceptron algorithm (Huang et al, 2012).
Fortraining and testing, we set beam size to 16 andmax count b, introduced in Section 4.2, to 2.
Forcomparison with other systems in our environ-ment, we also implemented two systems:?
Lattice is a method by Cai et al (2011).
Wealso used blatt5, which is an extension ofthe Berkeley parser, to parse word lattices inwhich the special word *e* is encoded as de-scribed in (Cai et al, 2011).?
Tagger decides whether some empty cate-gory is inserted at the front of a word or not,with regularized logistic regression.
To sim-plify point-wise linear tagging, we combinedempty categories, those that appeared in thesame position of a sentence, into a single cat-egory: thus the original 10 empty types in-creased to 63.Table 1 shows final results on Section 23.
Toevaluate the accuracy of empty element recov-ery, we calculated precision, recall and F1 scoresfor (1) Labeled Empty Bracket (X/t,i,i), (2) La-beled Empty Element (t,i,i), and (3) All Brack-ets, where X ?
NT , t ?
ET and i is a posi-tion of the empty element, using eevalb6.
Theresults clearly show that our proposed methodsignificantly outperforms the other systems.
Ta-ble 2 shows the main reason for the improvementachieved by our method.
The *ICH*, *RNR* and*EXP* empty types are used to show the relationbetween non-adjacent constituents, caused by syn-tactic phenomena like Extraposition and Conjunc-tion.
Our method captures such complex relationsbetter with the help of the syntactic feature rich-ness.Table 1 reports the scores for non-empty brack-ets to examine whether the joint method improvesthe standard PARSEVAL scores.
While the Lattice5http://www.cs.bgu.ac.il/?yoavg/software/blatt/6http://www3.nd.edu/?dchiang/software/eevalb.py98Johnson (X/t,i,i) Typed-empty (t,i,i) All Brackets Non-empty BracketsP R F1 P R F1 P R F1 P R F1Schmid06 ?
?
?
87.9 83.0 85.4 ?
?
?
?
?
?Cai11 90.1 79.5 84.5 92.3 80.9 86.2 90.1 88.5 89.3 ?
?
?Tagger 89.7 69.3 78.1 90.7 70.1 79.0 87.8 85.5 86.7 87.8 86.8 87.3Lattice (Cai11) 89.8 79.2 84.2 91.4 80.6 85.7 90.2 88.7 89.5 90.2 89.5 89.8Proposed 90.3 81.7 85.8 91.8 83.2 87.3 90.8 89.7 90.3 90.8 90.3 90.6Berkeley ?
?
?
?
?
?
?
?
?
89.9 90.3 90.1Hayashi16 ?
?
?
?
?
?
?
?
?
90.9 90.4 90.7Table 1: Results on the English Penn Treebank (Section 23): to calculate the scores for Tagger, weobtained a parse tree by supplying the 1-best Tagger output with the Berkeley parser trained on Sections02-21 including empty elements (using the option ?-useGoldPOS?
).method was less accurate than the vanilla Berke-ley parser, the performance of our method couldbe maintained with little loss in parsing accuracy.Figure 4 shows the parse time in seconds for eachtest sentence and that our empty element recoveryparser works in reasonable time.6.2 Experiments on the Japanese KeyakiTreebankFinally, to show that our method works well onother languages, we conduct experiments on theJapanese Keyaki Treebank (Butler et al, 2012).For this data, we modified blatt to keep functionlabels And, in order to consider segmentation er-rors, we also modified eevalb to calculate not wordbut character span in a sentence.
We follow the ex-periments in (Takeno et al, 2015) and show the re-sults in Table 3.
Our method significantly outper-forms the state-of-the-art post-processing methodin Japanese.7 Conclusion and Future WorkUsing spinal parsing for the joint recovery ofempty elements achieves state-of-the-art perfor-mance in standard English and Japanese datasets.We plan to extend our work to recover trace-fillerand frame semantic structures using the PropBankdata.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their valuable comments and sug-gestions to improve the quality of the paper.
Thiswork was supported in part by JSPS KAKENHIGrant Number 26730126.ReferencesM.
Ballesteros and X. Carreras.
2015.
Transition-based spinal parsing.
In Proc.
of CoNLL.A.
Butler, Z. Hong, T. Hotta, R. Otomo, K. Yoshimoto,and Z. Zhou.
2012.
Keyaki treebank: phrase struc-ture with functional information for japanese.
InProc.
of Text Annotation Workshop.S.
Cai, D. Chiang, and Y. Goldberg.
2011.
Language-independent parsing with empty elements.
In Proc.of ACL-HLT, pages 212?216.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG,dynamic programming, and the perceptron for effi-cient, feature-rich parsing.
In Proc.
of CoNLL, pages9?16.J.
Chen and V. K. Shanker.
2004.
Automated extrac-tion of tags from the penn treebank.
In New develop-ments in parsing technology, pages 73?89.
Springer.D.
Chiang and D. M. Bikel.
2002.
Recovering la-tent information in treebanks.
In Proc.
of COLING,pages 1?7.P.
Dienes and A. Dubey.
2003.
Deep syntactic pro-cessing by combining shallow methods.
In Proc.
ofACL, pages 431?438.R.
Gabbard, M. Marcus, and S. Kulick.
2006.
Fullyparsing the penn treebank.
In Proc.
of NAACL-HLT,pages 184?191.K.
Hayashi, J. Suzuki, and M. Nagata.
2016.
Shift-reduce spinal tag parsing with dynamic program-ming.
Transactions of the Japanese Society for Ar-tificial Intelligence, 31(2).L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proc.
of ACL,pages 1077?1086.L.
Huang, S. Fayong, and Y. Guo.
2012.
Structuredperceptron with inexact search.
In Proc.
of NAACL,pages 142?151.M.
Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proc.
of ACL, pages 136?143.99H.
Schmid.
2006.
Trace prediction and recovery withunlexicalized pcfgs and slash features.
In Proc.
ofCOLING-ACL, pages 177?184.L.
Shen, L. Champollion, and A. K. Joshi.
2008.
Ltag-spinal and the treebank.
Language Resources andEvaluation, 42(1):1?19.S.
Takeno, M. Nagata, and K. Yamamoto.
2015.Empty category detection using path features anddistributed case frames.
In Proc.
of EMNLP, pages1335?1340.B.
Xiang, X. Luo, and B. Zhou.
2013.
Enlistingthe ghost: modeling empty categories for machinetranslation.
In Proc.
of ACL, pages 822?831.M.
Zhu, Y. Zhang, W. Chen, M. Zhang, and J. Zhu.2013.
Fast and accurate shift-reduce constituentparsing.
In Proc.
of ACL, pages 434?443.100
