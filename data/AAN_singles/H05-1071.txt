Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 563?570, Vancouver, October 2005. c?2005 Association for Computational LinguisticsKnowItNow: Fast, Scalable Information Extraction from the WebMichael J. Cafarella, Doug Downey, Stephen Soderland, Oren EtzioniDepartment of Computer Science and EngineeringUniversity of WashingtonSeattle, WA 98195-2350{mjc,ddowney,soderlan,etzioni}@cs.washington.eduAbstractNumerous NLP applications rely onsearch-engine queries, both to ex-tract information from and to com-pute statistics over the Web corpus.But search engines often limit thenumber of available queries.
As aresult, query-intensive NLP applica-tions such as Information Extraction(IE) distribute their query load overseveral days, making IE a slow, off-line process.This paper introduces a novel archi-tecture for IE that obviates queries tocommercial search engines.
The ar-chitecture is embodied in a systemcalled KNOWITNOW that performshigh-precision IE in minutes insteadof days.
We compare KNOWITNOWexperimentally with the previously-published KNOWITALL system, andquantify the tradeoff between re-call and speed.
KNOWITNOW?s ex-traction rate is two to three ordersof magnitude higher than KNOW-ITALL?s.1 Background and MotivationNumerous modern NLP applications use the Web as theircorpus and rely on queries to commercial search enginesto support their computation (Turney, 2001; Etzioni et al,2005; Brill et al, 2001).
Search engines are extremelyhelpful for several linguistic tasks, such as computing us-age statistics or finding a subset of web documents to an-alyze in depth; however, these engines were not designedas building blocks for NLP applications.
As a result,the applications are forced to issue literally millions ofqueries to search engines, which limits the speed, scope,and scalability of the applications.
Further, the applica-tions must often then fetch some web documents, whichat scale can be very time-consuming.In response to heavy programmatic search engine use,Google has created the ?Google API?
to shunt program-matic queries away from Google.com and has placed hardquotas on the number of daily queries a program can is-sue to the API.
Other search engines have also introducedmechanisms to limit programmatic queries, forcing ap-plications to introduce ?courtesy waits?
between queriesand to limit the number of queries they issue.To understand these efficiency problems in more detail,consider the KNOWITALL information extraction sys-tem (Etzioni et al, 2005).
KNOWITALL has a generate-and-test architecture that extracts information in twostages.
First, KNOWITALL utilizes a small set of domain-independent extraction patterns to generate candidatefacts (cf.
(Hearst, 1992)).
For example, the generic pat-tern ?NP1 such as NPList2?
indicates that the head ofeach simple noun phrase (NP) in NPList2 is a member ofthe class named in NP1.
By instantiating the pattern forclass City, KNOWITALL extracts three candidate citiesfrom the sentence: ?We provide tours to cities such asParis, London, and Berlin.?
Note that it must also fetcheach document that contains a potential candidate.Next, extending the PMI-IR algorithm (Turney, 2001),KNOWITALL automatically tests the plausibility of thecandidate facts it extracts using pointwise mutual in-formation (PMI) statistics computed from search-enginehit counts.
For example, to assess the likelihood that?Yakima?
is a city, KNOWITALL will compute the PMIbetween Yakima and a set of k discriminator phrases thattend to have high mutual information with city names(e.g., the simple phrase ?city?).
Thus, KNOWITALL re-quires at least k search-engine queries for every candidateextraction it assesses.Due to KNOWITALL?s dependence on search-enginequeries, large-scale experiments utilizing KNOWITALLtake days and even weeks to complete, which makes re-search using KNOWITALL slow and cumbersome.
Pri-vate access to Google-scale infrastructure would provide563sufficient access to search queries, but at prohibitive cost,and the problem of fetching documents (even if from acached copy) would remain (as we discuss in Section2.1).
Is there a feasible alternative Web-based IE system?If so, what size Web index and how many machines arerequired to achieve reasonable levels of precision/recall?What would the architecture of this IE system look like,and how fast would it run?To address these questions, this paper introduces anovel architecture for web information extraction.
Itconsists of two components that supplant the generate-and-test mechanisms in KNOWITALL.
To generate ex-tractions rapidly we utilize our own specialized searchengine, called the Bindings Engine (or BE), which ef-ficiently returns bindings in response to variabilizedqueries.
For example, in response to the query ?Citiessuch as ProperNoun(Head(?NounPhrase?
))?, BE willreturn a list of proper nouns likely to be city names.
Toassess these extractions, we use URNS, a combinatorialmodel, which estimates the probability that each extrac-tion is correct without using any additional search enginequeries.1 For further efficiency, we introduce an approx-imation to URNS, based on frequency of extractions?
oc-currence in the output of BE, and show that it achievescomparable precision/recall to URNS.Our contributions are as follows:1.
We present a novel architecture for Information Ex-traction (IE), embodied in the KNOWITNOW sys-tem, which does not depend on Web search-enginequeries.2.
We demonstrate experimentally that KNOWITNOWis the first system able to extract tens of thousandsof facts from the Web in minutes instead of days.3.
We show that KNOWITNOW?s extraction rate is twoto three orders of magnitude greater than KNOW-ITALL?s, but this increased efficiency comes at thecost of reduced recall.
We quantify this tradeoff forKNOWITNOW?s 60,000,000 page index and extrap-olate how the tradeoff would change with larger in-dices.Our recent work has described the BE search enginein detail (Cafarella and Etzioni, 2005), and also analyzedthe URNS model?s ability to compute accurate probabilityestimates for extractions (Downey et al, 2005).
However,this is the first paper to investigate the composition ofthese components to create a fast IE system, and to com-pare it experimentally to KNOWITALL in terms of time,1In contrast, PMI-IR, which is built into KNOWITALL, re-quires multiple search engine queries to assess each potentialextraction.recall, precision, and extraction rate.
The frequency-based approximation to URNS and the demonstration ofits success are also new.The remainder of the paper is organized as follows.Section 2 provides an overview of BE?s design.
Sec-tion 3 describes the URNS model and introduces an ef-ficient approximation to URNS that achieves similar pre-cision/recall.
Section 4 presents experimental results.
Weconclude with related and future work in Sections 5 and6.2 The Bindings EngineThis section explains how relying on standard search en-gines leads to a bottleneck for NLP applications, and pro-vides a brief overview of the Bindings Engine (BE)?oursolution to this problem.
A comprehensive description ofBE appears in (Cafarella and Etzioni, 2005).Standard search engines are computationally expen-sive for IE and other NLP tasks.
IE systems issue multiplequeries, downloading all pages that potentially match anextraction rule, and performing expensive processing oneach page.
For example, such systems operate roughly asfollows on the query (?cities such as ?NounPhrase??):1.
Perform a traditional search engine query to findall URLs containing the non-variable terms (e.g.,?cities such as?)2.
For each such URL:(a) obtain the document contents,(b) find the searched-for terms (?cities such as?)
inthe document text,(c) run the noun phrase recognizer to determinewhether text following ?cities such as?
satisfiesthe linguistic type requirement,(d) and if so, return the stringWe can divide the algorithm into two stages: obtainingthe list of URLs from a search engine, and then process-ing them to find the ?NounPhrase?
bindings.
Each stageposes its own scalability and speed challenges.
The firststage makes a query to a commercial search engine; whilethe number of available queries may be limited, a singleone executes relatively quickly.
The second stage fetchesa large number of documents, each fetch likely resultingin a random disk seek; this stage executes slowly.
Nat-urally, this disk access is slow regardless of whether ithappens on a locally-cached copy or on a remote doc-ument server.
The observation that the second stage isslow, even if it is executed locally, is important becauseit shows that merely operating a ?private?
search enginedoes not solve the problem (see Section 2.1).The Bindings Engine supports queries contain-ing typed variables (such as NounPhrase) and564string-processing functions (such as ?head(X)?
or?ProperNoun(X)?)
as well as standard query terms.
BEprocesses a variable by returning every possible stringin the corpus that has a matching type, and that can besubstituted for the variable and still satisfy the user?squery.
If there are multiple variables in a query, then allof them must simultaneously have valid substitutions.
(So, for example, the query ?<NounPhrase> is locatedin <NounPhrase>?
only returns strings when nounphrases are found on both sides of ?is located in?.)
Wecall a string that meets these requirements a binding forthe variable in question.
These queries, and the bindingsthey elicit, can usefully serve as part of an informationextraction system or other common NLP tasks (such asgathering usage statistics).
Figure 1 illustrates some ofthe queries that BE can handle.president Bush <Verb>cities such as ProperNoun(Head(<NounPhrase>))<NounPhrase> is the CEO of <NounPhrase>Figure 1: Examples of queries that can be handled byBE.
Queries that include typed variables and string-processing functions allow NLP tasks to be done ef-ficiently without downloading the original documentduring query processing.BE?s novel neighborhood index enables it to processthese queries with O(k) random disk seeks and O(k) se-rial disk reads, where k is the number of non-variableterms in its query.
As a result, BE can yield orders ofmagnitude speedup as shown in the asymptotic analysislater in this section.
The neighborhood index is an aug-mented inverted index structure.
For each term in the cor-pus, the index keeps a list of documents in which the termappears and a list of positions where the term occurs, justas in a standard inverted index (Baeza-Yates and Ribeiro-Neto, 1999).
In addition, the neighborhood index keepsa list of left-hand and right-hand neighbors at each posi-tion.
These are adjacent text strings that satisfy a recog-nizer for one of the target types, such as NounPhrase.As with a standard inverted index, a term?s list is pro-cessed from start to finish, and can be kept on disk as acontiguous piece.
The relevant string for a variable bind-ing is included directly in the index, so there is no needto fetch the source document (thus causing a disk seek).Expensive processing such as part-of-speech tagging orshallow syntactic parsing is performed only once, whilebuilding the index, and is not needed at query time.
Itis important to note that simply preprocessing the corpusand placing the results in a database would not avoid diskseeks, as we would still have to explicitly fetch these re-sults.
The run-time efficiency of the neighborhood indexQuery Time Index SpaceBE O(k) O(N)Standard engine O(k + B) O(N)Table 1: BE yields considerable savings in query timeover a standard search engine.
k is the number of con-crete terms in the query, B is the number of variablebindings found in the corpus, and N is the number ofdocuments in the corpus.
N and B are typically ex-tremely large, while k is small.comes from integrating the results of corpus processingwith the inverted index (which determines which of thoseresults are relevant).The neighborhood index avoids the need to return tothe original corpus, but it can consume a large amountof disk space, as parts of the corpus text are folded intothe index several times.
To conserve space, we performsimple dictionary-lookup compression of strings in theindex.
The storage penalty will, of course, depend on theexact number of different types added to the index.
In ourexperiments, we created a useful IE system with a smallnumber of types (including NounPhrase) and found thatthe neighborhood index increased disk space only fourtimes that of a standard inverted index.Asymptotic Analysis:In our asymptotic analysis of BE?s behavior, we countquery time as a function of the number of random diskseeks, since these seeks dominate all other processingtasks.
Index space is simply the number of bytes neededto store the index (not including the corpus itself).Table 1 shows that BE requires only O(k) random diskseeks to process queries with an arbitrary number of vari-ables whereas a standard engine takes O(k + B), wherek is the number of concrete query terms, and B is thenumber of bindings found in a corpus of N documents.Thus, BE?s performance is the same as that of a standardsearch engine for queries containing only concrete terms.For variabilized queries, N may be in the billions and Bwill tend to grow with N .
In our experiments, eliminatingthe B term from our query processing time has resultedin speedups of two to three orders of magnitude over astandard search engine.
The speedup is at the price of asmall constant multiplier to index size.2.1 DiscussionWhile BE has some attractive properties for NLP compu-tations, is it necessary?
Could fast, large-scale informa-tion extraction be achieved merely by operating a ?pri-vate?
search engine?The release of open-source search engines such asNutch2, coupled with the dropping price of CPUs and2http://lucene.apache.org/nutch/5658.160.06012345678910BE NutchElapsedminutesFigure 2: Average time to return the relevant bindingsin response to a set of queries was 0.06 CPU minutesfor BE, compared to 8.16 CPU minutes for the com-parable processing on Nutch.
This is a 134-fold speedup.
The CPU resources, network, and index size werethe same for both systems.disks, makes it feasible for NLP researchers to operatetheir own large-scale search engines.
For example, Tur-ney operates a search engine with a terabyte-sized indexof Web pages, running on a local eight-machine Beowulfcluster (Turney, 2004).
Private search engines have twoadvantages.
First, there is no query quota or need for?courtesy waits?
between queries.
Second, since the en-gine is local, network latency is minimal.However, to support IE, we must also execute the sec-ond stage of the algorithm (see the beginning of this sec-tion).
In this stage, each document that matches a queryhas to be retrieved from an arbitrary location on a disk.3Thus, the number of random disk seeks scales linearlywith the number of documents retrieved.
Moreover, manyNLP applications require the extraction of strings match-ing particular syntactic or semantic types from each page.The lack of linguistic data in the search engine?s indexmeans that many pages are fetched only to be discardedas irrelevant.To quantify the speedup due to BE, we compared it to astandard search index built on the open-source Nutch en-gine.
All of our Nutch and BE experiments were carriedout on the same corpus of 60 million Web pages and wererun on a cluster of 23 dual-Xeon machines, each with twolocal 140 Gb disks and 4 Gb of RAM.
We set al config-uration values to be exactly the same for both Nutch andBE.
BE gave a 134-fold speed up on average query pro-cessing time when compared to the same queries with theNutch index, as shown in Figure 2.3Moving the disk head to an arbitrary location on the diskis a mechanical operation that takes about 5 milliseconds onaverage.3 The URNS ModelTo realize the speedup from BE, KNOWITNOW must alsoavoid issuing search engine queries to validate the cor-rectness of each extraction, as required by PMI compu-tation.
We have developed a probabilistic model obviat-ing search-engine queries for assessment.
The intuitionbehind this model is that correct instances of a class orrelation are likely to be extracted repeatedly, while ran-dom errors by an IE system tend to have low frequencyfor each distinct incorrect extraction.Our probabilistic model, which we call URNS, takes theform of a classic ?balls-and-urns?
model from combina-torics.
We think of IE abstractly as a generative processthat maps text to extractions.
Each extraction is modeledas a labeled ball in an urn.
A label represents either aninstance of the target class or relation, or represents anerror.
The information extraction process is modeled asrepeated draws from the urn, with replacement.Formally, the parameters that characterize an urn are:?
C ?
the set of unique target labels; |C| is the numberof unique target labels in the urn.?
E ?
the set of unique error labels; |E| is the numberof unique error labels in the urn.?
num(b) ?
the function giving the number of ballslabeled by b where b ?
C ?
E. num(B) is themulti-set giving the number of balls for each labelb ?
B.The goal of an IE system is to discern which of thelabels it extracts are in fact elements of C, based on re-peated draws from the urn.
Thus, the central question weare investigating is: given that a particular label x wasextracted k times in a set of n draws from the urn, whatis the probability that x ?
C?
We can express the prob-ability that an element extracted k of n times is of thetarget relation as follows.P (x ?
C|x appears k times in n draws) =?r?num(C)( rs )k(1 ?
rs )n?k?r?
?num(C?E)( r?s )k(1 ?
r?s )n?k(1)where s is the total number of balls in the urn, and thesum is taken over possible repetition rates r.A few numerical examples illustrate the behavior ofthis equation.
Let |C| = |E| = 2, 000 and assumefor simplicity that all labels are repeated on the samenumber of balls (num(ci) = RC for all ci ?
C, andnum(ei) = RE for all ei ?
E).
Assume that the ex-traction rules have precision p = 0.9, which means thatRC = 9 ?
RE ?
target balls are nine times as commonin the urn as error balls.
Now, for k = 3 and n = 10, 000we have P (x ?
C) = 93.0%.
Thus, we see that a smallnumber of repetitions can yield high confidence in an ex-traction.
However, when the sample size increases so that566n = 20, 000, and the other parameters are unchanged,then P (x ?
C) drops to 19.6%.
On the other hand, ifC balls repeat much more frequently than E balls, sayRC = 90?RE (with |E| set to 20,000, so that p remainsunchanged), then P (x ?
C) rises to 99.9%.The above examples enable us to illustrate the advan-tages of URNS over the noisy-or model used in previouswork.
The noisy-or model assumes that each extraction isan independent assertion that the extracted label is ?true,?an assertion that is correct a fraction p of the time.
Thenoisy-or model assigns the following probability to ex-tractions:Pnoisy?or(x ?
C|x appears k times) = 1 ?
(1 ?
p)kTherefore, the noisy-or model will assign the sameprobability ?
99.9% ?
in all three of the above exam-ples, although this is only correct in the case for whichn = 10, 000 and RC = 90?RE .
As the other two exam-ples show, for different sample sizes or repetition rates,the noisy-or model can be highly inaccurate.
This is notsurprising given that the noisy-or model ignores the sam-ple size and the repetition rates.URNS uses an EM algorithm to estimate its parameters,and currently the algorithm takes roughly three minutesto terminate.4 Fortunately, we determined experimen-tally that we can approximate URNS?s precision and recallusing a far simpler frequency-based assessment method.This is true because good precision and recall merely re-quire an appropriate ordering of the extractions for eachrelation, and not accurate probabilities for each extrac-tion.
For unary relations, we use the simple approxima-tion that items extracted more often are more likely tobe true, and order the extractions from most to least ex-tracted.
For binary relations like CapitalOf(X,y),in which we extract several different candidate capitals yfor each known country X, we use a smoothed frequencyestimate to order the extractions.
Let freq(R(X, y)) de-note the number of times that the binary relation R(X, y)is extracted; we define:smoothed freq(R(X, y)) = freq(R(X, y))maxy?
freq(R(X, y?))
+ 1We found that sorting by smoothed frequency (in de-scending order) performed better than simply sorting byfreq for relations R(X, y) in which different known X val-ues may have widely varying Web presence.Unlike URNS, our frequency-based assessment doesnot yield accurate probabilities to associate with each ex-traction, but for the purpose of returning a ranked list ofhigh-quality extractions it is comparable to URNS (see4This code has not been optimized at all.
We believe thatwe can easily reduce its running time to less than a minute onaverage, and perhaps substantially more.0.750.80.850.90.9510 50 100 150 200 250Correct ExtractionsPrecisionKnowItNow-freq KnowItNow-URNSKnowItAll-PMIFigure 3: Country: KNOWITALL maintains some-what higher precision than KNOWITNOW throughoutthe recall-precision curve.Figures 3 through 6), and it has the advantage of beingmuch faster.
Thus, in the experiments reported on below,we use frequency-based assessment as part of KNOWIT-NOW.4 Experimental ResultsThis section contrasts the performance of KNOWITNOWand KNOWITALL experimentally.
Before considering theexperiments in detail, we note that a key advantage ofKNOWITNOW is that it does not make any queries to Websearch engines.
As a result, KNOWITNOW?s scale is notlimited by a query quota, though it is limited by the sizeof its index.We report on the following metrics:?
Recall: how many distinct extractions does eachsystem return at high precision?5?
Time: how long did each system take to produceand rank its extractions??
Extraction Rate: how many distinct high-qualityextractions does the system return per minute?
Theextraction rate is simply recall divided by time.We contrast KNOWITALL and KNOWITNOW?s preci-sion/recall curves in Figures 3 through 6.
We com-pared KNOWITNOW with KNOWITALL on four rela-tions: Corp, Country, CeoOf(Corp,Ceo), andCapitalOf(Country,City).
The unary relationswere chosen to examine the difference between a relationwith a small number of correct instances (Country) andone with a large number of extractions (Corp).
The bi-nary relations were chosen to cover both functional rela-tions (CapitalOf) and set-valued relations (CeoOf?we treat former CEOs as correct instances of the relation).5Since we cannot compute ?true recall?
for most relationson the Web, the paper uses the term ?recall?
to refer to the sizeof the set of facts extracted.5670.750.80.850.90.9510 50 100 150 200Correct ExtractionsPrecisionKnowItNow-freq KnowItNow-URNSKnowItAll-PMIFigure 4: CapitalOf: KNOWITNOW does nearly aswell as KNOWITALL, but has more difficulty thanKNOWITALL with sparse data for capitals of more ob-scure countries.For the two unary relations, both systems created ex-traction rules from eight generic patterns.
These are hy-ponym patterns like ?NP1 {,} such as NPList2?
or ?NP2{,} and other NP1?, which extract members of NPList2or NP2 as instances of NP1.
For the binary relations,the systems instantiated rules from four generic patterns.These are patterns for a generic ?of?
relation.
They are?NP1 , rel of NP2?, ?NP1 the rel of NP2?, ?rel of NP2, NP1?, and ?NP2 rel NP1?.
When rel is instantiated forCeoOf, these patterns become ?NP1 , CEO of NP2?
andso forth.Both KNOWITNOW and KNOWITALL merge extrac-tions with slight variants in the name, such as those dif-fering only in punctuation or whitespace, or in the pres-ence or absence of a corporate designator.
For binaryextractions, CEOs with the same last name and samecompany were also merged.
Both systems rely on theOpenNlp maximum-entropy part-of-speech tagger andchunker (Ratnaparkhi, 1996), but KNOWITALL appliesthem to pages downloaded from the Web based on the re-sults of Google queries, whereas KNOWITNOW appliesthem once to crawled and indexed pages.6 Overall, eachof the above elements of KNOWITALL and KNOWIT-NOW are the same to allow for controlled experiments.Whereas KNOWITNOW runs a small number of vari-abilized queries (one for each extraction pattern, foreach relation), KNOWITALL requires a stopping crite-rion.
Otherwise, KNOWITALL will continue to queryGoogle and download URLs found in its result pages overmany days and even weeks.
We allowed a total of 6 daysof search time for KNOWITALL, allocating more searchfor the relations that continued to be most productive.
ForCeoOf KNOWITNOW returned all pairs of Corp,Ceo6Our time measurements for KNOWITALL are not affectedby the tagging and chunking time because it is dominatedby time required to query Google, waiting a second betweenqueries.0.750.80.850.90.9510 5,000 10,000 15,000 20,000 25,000Correct ExtractionsPrecisionKnowItNow-freq KnowItNow-URNSKnowItAll-PMIFigure 5: Corp: KNOWITALL?s PMI assessment main-tains high precision.
KNOWITNOW has low recall upto precision 0.85, then catches up with KNOWITALL.in its corpus; KNOWITALL searched for CEOs of a ran-dom selection of 10% of the corporations it found, andwe projected the total extractions and search effort for allcorporations.
For CapitalOf, both KNOWITNOW andKNOWITALL looked for capitals of a set of 195 coun-tries.Table 2 shows the number of queries, search time, dis-tinct correct extractions at precision 0.8, and extractionrate for each relation.
Search time for KNOWITNOW ismeasured in seconds and search time for KNOWITALLis measured in hours.
The number of extractions perminute counts the distinct correct extractions.
Since welimit KNOWITALL to one Google query per second, thetime for KNOWITALL is proportional to the number ofqueries.
KNOWITNOW?s extraction rate is from 275 to4,707 times that of KNOWITALL at this level of preci-sion.While the number of distinct correct extractions fromKNOWITNOW at precision 0.8 is roughly comparable tothat of 6 days search effort from KNOWITALL, the sit-uation is different at precision 0.9.
KNOWITALL?s PMIassessor is able to maintain higher precision than KNOW-ITNOW?s frequency-based assessor.
The number of cor-rect corporations for KNOWITNOW drops from 23,128 atprecision 0.8 to 1,116 at precision 0.9.
KNOWITALL isable to identify 17,620 correct corporations at precision0.9.
Even with the drop in recall, KNOWITNOW?s ex-traction rate is still 305 times higher than KNOWITALL?s.The reason for KNOWITNOW?s difficulty at precision 0.9is due to extraction errors that occur with high frequency,particularly generic references to companies (?the Selleris a corporation ...?, ?corporations such as Banks?, etc.
)and truncation of certain company names by the extrac-tion rules.
The more expensive PMI-based assessmentwas not fooled by these systematic extraction errors.Figures 3 through 6 show the recall-precision curvesfor KNOWITNOW with URNS assessment, KNOWIT-NOW with the simpler frequency-based assessment, and568Google Queries Time Extractions Extractions per minuteNOW ALL NOW (sec) ALL (hrs) NOW ALL NOW ALL ratioCorp 0 (16) 201,878 42 56.1 23,128 23,617 33,040 7.02 4,707Country 0 (16) 35,480 42 9.9 161 203 230 0.34 672CeoOf 0 (6) 263,646 51 73.2 2,402 5,823 2,836 1.33 2,132CapitalOf 0 (6) 17,216 55 4.8 169 192 184 0.67 275Table 2: Comparison of KNOWITNOW with KNOWITALL for four relations, showing number of Google queries(local BE queries in parentheses), search time, correct extractions at precision 0.8, and extraction rate (thenumber of correct extractions at precision 0.8 per minute of search).
Overall, KNOWITNOW took a total ofslightly over 3 minutes as compared to a total of 6 days of search for KNOWITALL.0.750.80.850.90.9510 2,000 4,000 6,000Correct ExtractionsPrecisionKnowItNow-freq KnowitNow-URNSKnowItAll-PMIFigure 6: CeoOf: KNOWITNOW has difficulty dis-tinguishing low frequency correct extractions fromnoise.
KNOWITALL is able to cope with the sparsedata more effectively.KNOWITALL with PMI-based assessment.
For each ofthe four relations, PMI is able to maintain a higher pre-cision than either frequency-based or URNS assessment.URNS and frequency-based assessment give roughly thesame levels of precision.For the relations with a small number of correct in-stances, Country and CapitalOf, KNOWITNOW isable to identify 70-80% as many instances as KNOW-ITALL at precision 0.9.
In contrast, Corp and CeoOfhave a huge number of correct instances and a long tailof low frequency extractions that KNOWITNOW has dif-ficulty distinguishing from noise.
Over one fourth ofthe corporations found by KNOWITALL had Google hitcounts less than 10,500, a sparseness problem that wasexacerbated by KNOWITNOW?s limited index size.Figure 7 shows projected recall from larger KNOW-ITNOW indices, fitting a sigmoid curve to the recallfrom index size of 10M, 20M, up to 60M pages.
Thecurve was fitted using logistic regression, and is restrictedto asymptote at the level reported for Google-basedKNOWITALL for each relation.
We report re-call at precision 0.9 for capitals of 195 coun-tries and CEOs of a random selection of thetop 5,000 corporations as ranked by PMI.Recall is defined as the percent of countries with a00.20.40.60.810 100 200 300 400KnowItNow index size (millions)Recall at 0.9precisionKnowItNow CeoOfGoogle CeoOfKnowItNow CapitalOfGoogle CapitalOfFigure 7: Projections of recall (at precision 0.9) as afunction of KNOWITNOW index size.
At 400 millionpages, KNOWITNOW?s recall rapidly approaches therecall achieved by KNOWITALL using roughly 300,000Google queries.correct capital or the number of correct CEOs divided bythe number of corporations.The curve for CeoOf is rising steeply enough that a400 million page KNOWITNOW index may approach thesame level of recall yielded by KNOWITALL when it uses300,000 Google queries.
As shown in Table 2, KNOW-ITALL takes slightly more than three days to generatethese results.
KNOWITNOW would operate over a cor-pus 6.7 times its current one, but the number of requiredrandom disk seeks (and the asymptotic run time analy-sis) would remain the same.
We thus expect that with alarger corpus we can construct a KNOWITNOW systemthat reproduces KNOWITALL levels of precision and re-call while still executing in the order of a few minutes.5 Related WorkThere has been very little work published on how to makeNLP computations such as PMI-IR and IE fast for largecorpora.
Indeed, extraction rate is not a metric typicallyused to evaluate IE systems, but we believe it is an im-portant metric if IE is to scale.Hobbs et al point out the advantage of fast textprocessing for rapid system development (Hobbs et al,1992).
They could test each change to system parameters569and domain-specific patterns on a large sample of docu-ments, having moved from a system that took 36 hours toprocess 100 documents to FASTUS, which took only 11minutes.
This allowed them to develop one of the highestperforming MUC-4 systems in only one month.While there has been extensive work in the IR andWeb communities on improvements to the standard in-verted index scheme, there has been little work on effi-cient large-scale search to support natural language ap-plications.
One exception is Resnik?s Linguist?s SearchEngine (Elkiss and Resnik, 2004), a tool for searchinglarge corpora of parse trees.
There is little published in-formation about its indexing system, but the user man-ual suggests its corpus is a combination of indexed sen-tences and user-specific document collections driven bythe user?s AltaVista queries.
In contrast, the BE systemhas a single index, constructed just once, that serves allqueries.
There is no published performance data avail-able for Resnik?s system.6 Conclusions and Future DirectionsIn previous work, statistical NLP computation over largecorpora has been a slow, offline process, as in KNOW-ITALL (Etzioni et al, 2005) and also in PMI-IR appli-cations such as sentiment classification (Turney, 2002).Technology trends, and open source search engines suchas Nutch, have made it feasible to create ?private?
searchengines that index large collections of documents; but asshown in Figure 2, firing large numbers of queries at pri-vate search engines is still slow.This paper described a novel and practical approachtowards substantially speeding up IE.
We describedKNOWITNOW, which extracts thousands of facts in min-utes instead of days.
Furthermore, we sketched URNS,a probabilistic model that both obviates the need forsearch-engine queries and outputs more accurate prob-abilities than PMI-IR.
Finally, we introduced a simple,efficient approximation to URNS, whose probability esti-mates are not as good, but which has comparable preci-sion/recall to URNS, making it an appropriate assessor forKNOWITNOW.The speed and massively improved extraction rate ofKNOWITNOW come at the cost of reduced recall.
Wequantified this tradeoff in Table 2, and also argued that asKNOWITNOW?s index size increases from 60 million to400 million pages, KNOWITNOW would achieve in min-utes the same precision/recall that takes KNOWITALLdays to obtain.
Of course, a hybrid approach is possi-ble where KNOWITNOW has, say, a 100 million pageindex and, when necessary, augments its results with alimited number of queries to Google.
Investigating theextraction-rate/recall tradeoff in such a hybrid system isa natural next step.While our experiments have used the Web corpus, ourapproach transfers readily to other large corpora; exper-imentation with other corpora is another topic for futurework.
In conclusion, we believe that our techniques trans-form IE from a slow, offline process to an online one.They could open the door to a new class of interactive IEapplications, of which KNOWITNOW is merely the first.7 AcknowledgmentsThis research was supported in part by NSF grant IIS-0312988, DARPA contract NBCHD030010, ONR grantN00014-02-1-0324, and gifts from Google and the Tur-ing Center.ReferencesR.
Baeza-Yates and B. Ribeiro-Neto.
1999.
Modern Informa-tion Retrieval.
Addison Wesley.E.
Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng.
2001.Data-intensive question answering.
In Procs.
of Text RE-trieval Conference (TREC-10), pages 393?400.M.
Cafarella and O. Etzioni.
2005.
A Search Engine for Nat-ural Language Applications.
In Procs.
of the 14th Interna-tional World Wide Web Conference (WWW 2005).D.
Downey, O. Etzioni, and S. Soderland.
2005.
A ProbabilisticModel of Redundancy in Information Extraction.
In Procs.of the 19th International Joint Conference on Artificial Intel-ligence (IJCAI 2005).E.
Elkiss and P. Resnik, 2004.
The Linguist?s Search EngineUser?s Guide.
University of Maryland.O.
Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.
2005.
Un-supervised named-entity extraction from the web: An exper-imental study.
Artificial Intelligence, 165(1):91?134.M.
Hearst.
1992.
Automatic Acquisition of Hyponyms fromLarge Text Corpora.
In Procs.
of the 14th InternationalConference on Computational Linguistics, pages 539?545,Nantes, France.J.R.
Hobbs, D. Appelt, M. Tyson, J.
Bear, and D. Israel.
1992.Description of the FASTUS system used for MUC-4.
InProcs.
of the Fourth Message Understanding Conference,pages 268?275.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tag-ger.
In Procs.
of the Empirical Methods in Natural LanguageProcessing Conference, Univ.
of Pennsylvania.P.
D. Turney.
2001.
Mining the Web for Synonyms: PMI-IRversus LSA on TOEFL.
In Procs.
of the Twelfth EuropeanConference on Machine Learning (ECML-2001), pages 491?502, Freiburg, Germany.P.
D. Turney.
2002.
Thumbs up or thumbs down?
semanticorientation applied to unsupervised classification of reviews.In Procs.
of the 40th Annual Meeting of the Association forComputational Linguistics (ACL?02), pages 417?424.P.
D. Turney, 2004.
Waterloo MultiText System.
Institute forInformation Technology, Nat?l Research Council of Canada.570
