INTEGRATED PROCESSING PRODUCESROBUST UNDERSTANDINGMal lo ry  Se l f  r idgeDepartment of Electrical Engineering and Computer ScienceThe University of ConnecticutStorrs, Connecticut 06268Natural language interfaces to computers must deal with wide variation in real-world input.
Thispaper proposes that, in order to handle real-world input robustly, a natural anguage interface shouldbe constructed in accord with principles of integrated processing: processing syntax and semantics atthe same time, processing syntax and semantics using the same mechanisms, and processing languageand memory using the same mechanisms.
This paper describes an experimental natural language inter-face constructed according to these principles which displays the desired robustness.
The success ofthis interface suggests that future real-world interfaces could achieve robustness by performing inte-grated processing.1 INTRODUCTIONNatural language interfaces to computers must deal withwide variation in real-word input.
Since real-world inputis often missing words and contains variant syntax, auseful natural language interface must understand suchinput.
Unfortunately, the technology needed to providethis robustness i not fully mature, and there is uncertain-ty as to the directions in which such maturity lies.Part of this uncertainty centers around the relationshipbetween syntax, semantics, and world knowledge innatural language processing.
One theoretical position,the integrated processing hypothesis (Schank 1981),describes an approach to computer modelling of humanlanguage processing, based on the idea that these threesorts of knowledge must be applied together and interac-tively.
Since human language understanding is robust, acomputer model of human language processing based onthis position should also be robust.This paper describes a research project designed toexplore this conjecture, and describes a robust naturallanguage interface, called MURPHY, which embodies theintegrated processing hypothesis.
It argues that integratedprocessing yields robustness and that this hypothesistherefore represents a promising approach to theconstruction of robust natural language interfaces.MURPHY has been developed within a limited domain,and questions remain about the generality of its tech-niques.
Nonetheless, its performance within this domainsuggests that generalization to a richer and more realisticdomain is possible.This paper first defines the term robust as it will beused here, and introduces the MURPHY system.
Second,it considers previous work on the problems of robustness.Third, it describes the integrated processing hypothesisand the motivation for the research strategy adoptedhere.
Fourth, it describes the MURPHY system and itsperformance in detail, and then argues that this perform-ance derives from its implementation of the integratedprocessing hypothesis.
Finally, it suggests that the inte-grated processing hypothesis is indeed a promisingapproach to the construction of robust natural anguageinterfaces, and that MURPHY represents a successful firststep.2 ROBUST UNDERSTANDING2.1 WHAT IS ROBUSTNESS?The broadest possible definition of robusmess on the partof a natural anguage interface would involve a languageability equal to or greater than that of a human; clearlythis is too ambitious.
Instead, the term robust will be usedhere to refer to a particular subset of human languageabilities.
This subset includes first the ability to under-stand utterances that are missing various words, and thatcontain words out of their grammatically preferred order.Copyright1986 bythe Association for Computational Linguistics.
Permission tocopy without fee all or part of this material isgranted provided thatthe copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
To copyotherwise, orto republish, requires a fee and/or specific permission.0362-613X/86/020089-106503.00Computational Linguistics, Volume 12, Number 2, April-June 1986 89Mallory Self ridge Integrated Processing Produces Robust UnderstandingSecond, if an input is initially misunderstood, a robustnatural anguage interface should converge on the correctunderstanding by continuing to infer the next most likelymeaning based on the input and context.
Finally, a robustnatural language interface must be able to use correctionsprovided by the user to direct the inference of the nextmost likely meaning.
Since the output of the understand-ing process in such an interface must be a representationof the meaning of the input, such an interface would thusbe limited primarily by its semantic knowledge of thedomain of discourse; it could not understand an utter-anee if the meaning of that utterance was not represent-able within its knowledge.
Otherwise, it would eventuallyunderstand.
These aspects of robustness are consideredin more detail below.There are a number of situations in which an utterancecan be missing words.
First, users usually omit wordswhose meanings can be inferred from the general contextby the listener.
Second, the user can be deliberatelyemploying ellipsis, intending the listener to complete hisunderstanding using concepts drawn from conversationalhistory.
Third, the utterance can contain unknown words,which are "missing" as far as understanding isconcerned.
Finally, since no one has demonstrated aperfect echnique for predicting which words are or arenot going to be missing, a robust natural anguage under-stander must be prepared to understand utterances witharbitrary words omitted, albeit perhaps taking longer toconverge in difficult cases.In addition to missing words, one cannot guaranteethat the input will be syntactically well formed.
Rather, itwill sometimes be ill formed in various unpredictableways.
This paper will be concerned only with thesimplest ype of variant syntax, consisting of words posi-tioned incorrectly within the utterance.
Note that themispositioning can result in an input that is meaningful,even though this meaning is not what was intended.Further, the words can be correctly positioned withrespect to the unintended utterance, even though theyare mispositioned with respect o the intended utterance.This issue is considered in greater detail ater.Under some conditions, however, any naturallanguage understander will misunderstand, since alterna-tive interpretations may be equally preferable.
A robustinterface should address this problem by verifying itsunderstanding with the user, and being prepared to"guess again" if it proved incorrect.
Such an interfaceshould be prepared to generate first its most likely inter-pretation, then its next most likely, and so on, until nofurther possibilities exist.
This would guarantee that theinterface will eventually understand the utterance, assum-ing, again, that the meaning of the utterance is within itsdomain of expertise.
Note that the capacity to systemat-ically exhaust he possible meanings of an utterance isbeyond the ability of human understanders.
A human canproduce many interpretations, but cannot be guaranteedto generate all possible interpretations due to factorssuch as memory limitations.
Notwithstanding the currentimpossibility of equalling or exceeding the overalllanguage ability of humans, a robust understandershould, if possible, exceed human performance in thisparticular regard, if possible.This ability to infer the next most likely meaning ofthe input is important but incomplete.
If the listenerincorrectly infers the meaning of the speaker's utterance,the speaker does not usually say "no" and wait for thelistener to infer a second meaning.
Rather, the speakerusually supplies a correction.
It is thus appropriate that arobust natural language interface not only be able toconjecture the next most likely interpretation but alsothat it be able to use corrections from the user when theyare supplied.A robust understander should also display a number ofother desirable characteristics beyond those discussedabove.
For example, it must be able to handle words withmultiple meanings.
Further, it should be able to under-stand despite false starts, irrelevant interjections, andunknown words.
Third, it should provide spellingcorrection and related support.
Fourth, it should have amixed-initiative conversational bility.
Finally, it shouldbe able to learn new word meaning and syntax.
Suchcharacteristics are beyond the scope of this paper.However, section 8 argues that MURPHY possesses someof these characteristics a well.Thus, for the purposes of this paper, a robust under-stander is one that can be guaranteed to eventuallyunderstand input utterances despite arbitrary missing andout-of-order words, both with and without corrections,and including ellipsis, on the basis of semantics andsyntax, domain knowledge, and context.2.2 A ROBUST UNDERSTANDERIn order to address the problem of robust understanding,the MURPHY system was developed.
MURPHY operatesin conjunction with a robot assembly system (Engelberg1983; Engelberg, Levas, and Selfridge 1984; Levas1983; Levas and Selfridge 1984; Selfridge 1983).
BothMURPHY and the robot assembly system are written inFranz Lisp on a VAX- l l /780 .
MURPHY allows a userto question and direct the robot assembly system usingnatural anguage.
Natural anguage can be used to speci-fy low-level image operations, ask high-level questionsabout the relationships between objects in the image,describe the appearance of unknown objects for futureuse by the system, and teach the manipulator how toperform new assembly tasks.The following example illustrates MURPHY's robust-ness.
It concerns the recognition and assembly of threecomponents of a simple electric switch - the base, thecontact, and the post - into the completed switch.MURPHY knows the meaning of each word in this exam-ple, and has appropriate domain knowledge.
Specifically,it knows that each component is a physical object that ispart of the switch.
Further, it has geometric models ofeach object required for planning robot motions.
Finally,90 Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Seffridge Integrated Processing Produces Robust Understandingit has knowledge about how the switch base and contactappear in the image.
In order to demonstrate MURPHY'sability to learn new visual knowledge, however, theknowledge of how a switch post appears in the image hasbeen removed, and the knowledge of how to translate thegeometric model of the post into appearance informationhas also been removed.
As will be seen, this is why thesystem can talk about the switch post without being ableto visually recognize it, and allows MURPHY's abilities tobe easily demonstrated.
The user's inputs are shown inlower case, MURPHY's responses in upper case, andparenthetical notes have been added to describe systemoperation.
(1) Display the current image workspace.DO YOU MEAN "Display the current workspaceimage"?>Yes.OK.
(The system displays the image.
)(2) >Threshold the image at fifty, and then applyboundary following for regions larger than thirty-five pixels.DO YOU MEAN "Threshold the current workspaceimage at fifty and then apply boundary followingfor regions larger than thirtyfive pixels in thecurrent workspace image"?>YesOK.
(The system displays the operations occurring.
)(3) >What is in the image?A SWITCH BASE AND A SWITCH CONTACT.
(The base and contact are outlined and labeled.
)(4) >Where is the switch?DO YOU MEAN "Where is the switch post"?>No, contact.THE SWITCH CONTACT IS NEXT TO THE SWITCHBASE.
(5) >Is there a switch post?WHAT DOES A SWITCH POST LOOK LIKE?
(6) >A metal cylinder two inches long.DO YOU MEAN "A switch post is a metal cylindertwo inches long"?>Yes.THERE IS A SWITCH POST NEXT TO THE SWITCHCONTACT.
(The post is outlined and labeled.
)(7) >Make a switch.OK(High-level planning drives the two manipulatorsto assemble the base, post, and contact into acompleted switch.
)In this example, MURPHY enabled a user to interactwith the robot assembly system to assemble the electricswitch.
The user first specified certain low-level image-processing operations, and then asked a high-level ques-tion that prompted the system to perform additionalbottom-up and top-down recognition processing in orderto generate an answer.
The user then asked for additionalinformation, which was supplied.
Next, the user askedabout the existence of an unreported object.
The systemdisplayed mixed-initiative capabilities by responding witha query about the appearance of that object.
Instead ofsupplying a description of the object's appearance, theuser answered with a description of the object's shape.The system then inferred the appearance of the object,reexamined the low-level image description, found theobject, and answered the user's original question.
Finally,the user commanded the system to assemble the partsinto a complete switch, which it did.Of specific importance to this paper, however, is thefact that in this interaction MURPHY displayed exactlythe kind of robustness described in section 2.
In (1) thewords image and workspace were reversed.
MURPHYunderstood correctly despite this variant syntax.
In (4),the word contact is missing.
MURPHY first infers that themissing word was post, but is corrected by the user andtold that the missing word was contact.
An additionalexample of MURPHY understanding despite missingwords appears in (2).
In (5), the system displays mixed-initiative in response to the user's question "Is there aswitch post in the current workspace image?".
It answersthis question with another question, "What does a switchpost look like?".
In (6), the user's reply is missing severalwords, including the primary frame-supplying word.MURPHY infers that the user meant "A switch post is ametal cylinder two inches long".
This demonstrates theability to infer the missing frame-supplying word is, andthe ability to use conversational history to understand theelliptical reference to "A switch post".
In these casesMURPHY meets the criteria established in section 2.1.Within the robot assembly domain MURPHY currentlyknows about fifty words and five phrases, and aboutseventy concepts of fifteen different ypes.
In addition, ithas been briefly tested in domains other than robotassembly.
During a typical interaction, MURPHY usuallyresponds within five or ten seconds.
Occasionally it takesmore than a minute on long test sentences.
Its responsetime is acceptable for an initial implementation designedto address theoretical questions, and strongly suggeststhat with tuning MURPHY could provide responses in realtime almost always when in realistic situations.3 PRIOR RESEARCH ON ROnUSTNESSPrior research has addressed the problem of robustunderstanding from a number of different perspectives.Hayes and Mouradian (1981) apply a grammar to utter-ances flexibly enough to interpret a variety of grammat-ical deviations.
This is done using a bottom-up, patternmatching parser that employs parse suspension andcontinuation to the arcs of an ATN (Woods 1970).Besides optional pattern elements, flexibility is achievedby relaxing consistency constraints and allowing out-of-order matches.
Kwasny and Sondheimer (1981) extendHayes and Mouradian's approach by recording theComputational Linguistics, Volume 12, Number 2, April-June 1986 91Mallory Selfridge Integrated Processing Produces Robust Understandingnature of the grammatical deviations.
Their parser appliesgrammar relaxation techniques to arcs of an ATN.
When-ever an arc of the normative grammar, specifying thestructure of a well-formed utterance, cannot be trav-ersed, a deviance note is created and the arc is traversedanyhow.
This note records how the utterance deviatesfrom the expected grammatical form, and allows parsingto proceed in the presence of variant syntax.
Addi-tionally, feature relaxation techniques allow an inappro-priate word to stand in place of a correct one.Both these works suffer from many of the same prob-lems.
Although the intent of both is to focus on a specificsubset of the overall problem, it is difficult to verify thesuccess of either approach without a semantic compo-nent.
Second, neither can infer a next interpretation if itsinitial conjecture was incorrect.
Third, neither can handlearbitrary missing words.
Finally, the ability of either tohandle variant syntax is limited: they can handle vari-ations on only a subset of the total classes of syntax theirparser handles.More recently, Weischedel and Sondheimer (1983)describe work that significantly extends ome of the ideasreported by Kwasny and Sondheimer (1981).
Theirextension involves the use of meta-mles to deal with ill-formed input.
These meta-rules are intended to recognizean instance of ill-formedness and prescribe actions thatmay provide understanding.
Although the approach ofusing meta-rules appears to handle well-formedness andappears worthwhile, two characteristics distinguish themeta-rule approach from the present research.
First,Weischedel and Sondheimer address themselves only tothe problem of processing ill-formed input, and leave forlater research the problem of integrating this approachwith techniques for handling other aspects of robustness.Second, and more important, their approach to handlingill-formed input differs computationally from the parsingmechanism it overlays, while the research reported hereexplores the processing of ill-formed input using the samemechanism as that used for well-formed input.Hayes and Carbonell (1981) report research closer tothat described in this paper.
Their work combined anumber of different approaches within two differentexperimental parsers.
CASPAR combined a search for asemantic case frame with a linear pattern matcher tobuild a representation of the meaning of the input.DYPAR combined a context-free semantic grammar, apartial pattern matcher, and equivalence transformationsfor building a representation f the meaning of the utter-ance.
\[Note that the DYPAR program described by Hayesand Carbonell (1981) is entirely different from theDYPAR program described by Dyer (1982).\] While bothappear to incorporate promising techniques, neitherCASPAR nor DYPAR displays a high degree of robust-ness.
While CASPAR can handle?
unexpected and unrecognizable interjections in theinput,?
missing case markers,?
out-of-order cases, and?
ambiguous cases,it cannot?
understand if the word whose meaning builds theprirnary semantic ase frame is missing,?
guess again,?
handle ellipsis, or?
understand utterances with arbitrary out-of-orderwords and missing words.DYPAR seems similarly limited.
Although it is embeddedin an interesting database management system, thedegree of robustness it displays is not clear.Carbonell and Hayes (1983) describe researchextending that reported by Hayes and Carbonell (1981).They describe a number of "recovery strategies" that canenable understanding to proceed in the presence of whatare termed "extragrammaticalities", andwhich are testedusing CASPAR, DYPAR, and a parser called DYPAR-II.Although much of the approach taken is similar to thatdescribed here, much of it represents an alternativeapproach to solving similar problems that focuses on anumber of difference processing mechanisms rather thanon a single, integrated mechanism as described in thefollowing section.
Furthermore, no single programappears to use all the strategies described by the paper,and thus the utility of the strategies taken over-all is diffi-cult to assess.
Finally, none of these programs appearcapable of continuing to generate alternative interpreta-tions of an input until confirmed by the user; they are notguaranteed to eventually understand the input.Understanders built within other paradigms have alsodisplayed various degrees of robustness.
What might betermed "semantics-oriented" understanders havedisplayed high performance understanding, producingfrom an utterance a representation f the meaning of thatutterance.
For example, ELI and SAM (Riesbeck andSchank 1976, Cullingford 1978), CA (Birnbaum andSelfridge 1981), and ACE (Cullingford, Krueger,Selfridge, and Bienkowski 1981) are similar in spirit toMURPHY, in that each attempts to combine word mean-ings into a representation f the meaning of the utteranceas a whole, and then allow later memory processingaccess to this understanding.
However, these programscan best be thought of as demonstrating the power ofmemory-based understanding while failing to fully exploitthe potential of integrated processing.
Each are relative-ly intolerant of missing words and variant syntax,although each has various abilities in these respects.Other approaches, uch as the NOMAD system (Granger1984), and those reported by Wilks (1976) and Fass andWilks (1983), are also related to the approach taken inthis paper.
However, these systems differ significantly intheir approach to robustness.
For example, while theNOMAD system does present alternative interpretationsof an imperfectly understood input, and does employsyntactic knowledge and world knowledge simultaneouslyduring understanding, it is not guaranteed to eventuallyarrive at the intended meaning of an input (given92 Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Selfridge Integrated Processing Produces Robust UnderstandingNOMAD's domain, this would be difficult in any eventbecause input utterances do not originate with the user)and its language processing and memory processing donot appear to employ the same mechanism.
Similarly,systems described by Wilks (1976) and Fass and Wilks(1984), while similar in their use of preferences toMURPHY, are not guaranteed to always eventuallyunderstand and employ different mechanisms forlanguage and memory processing.Finally, it is important o consider high-performanceknowledge-based understanding mechanisms, such asdescribed by Dyer (1982) and Lebowitz (1980).
Theseprograms demonstrate impressive understanding abilitiesin the domains of understanding complex stories aboutinterpersonal relationships and news stories about terror-ism, respectively.
They convincingly demonstrate thepower of high-level memory processing in difficult under-standing tasks.
However, neither has concentrated onthe question of robustness as the term is being used inthis paper.
A final answer to the question of robustnesswill certainly incorporate such high-performance memoryprocessing.Each of the systems described in this section hascertain robust aspects, but each leaves something to bedesired.
While it is possible to imagine extending some ofthese systems to remove various limitations, it is impossi-ble to judge the success of such extensions in the absenceof actual implementations; no evaluation can be made onthe basis of such hypothetical extensions.
Thus, no previ-ous research as developed a natural anguage understan-der that is robust in all the ways being addressed here.4 THE INTEGRATED PROCESSING HYPOTHESISIn order to build a robust natural anguage interface, onemust specify the relationships between syntax andsemantics, and between language understanding andmemory processing, because actual construction of aninterface requires a commitment to specific relationships.Schank and Birnbaum (1981) address these issues inproposing the integrated processing hypothesis.
General-izing from their discussion, these issues can be Summa-rized by the following three questions:Is syntax processed prior to semantics, orare syntax and semantics processed at the same time?Is syntax processed separately from semantics, orare they processed together by the same process?Are language processing and memory processingdifferent processes, orare they fundamentally the same process?As discussed by Schank and Birnbaum (1981), thereare roughly two polar positions on these issues.
One posi-tion might be called the "separatist" position, while theother can be termed the "integrated" position.
Each posi-tion can be characterized by its answer to these threequestions.
The first question concerns the temporalrelationship between semantic and syntactic processingduring understanding.
The separatist position suggeststhat a syntactic analysis of an utterance is performedprior to any semantic analysis, and that its output is asyntactic description of the utterance.
This output is thenpassed to the semantic analysis process.
In opposition tothis view is the integrated perspective.
This proposes thatsyntactic analysis is carried out at the same time assemantic analysis.
Thus the temporal order betweensyntactic analysis and semantic analysis in language proc-essing is a matter of disagreement, and must beaddressed when constructing a robust natural languageinterface.The second question concerns the nature of the mech-anisms that process yntax and semantics.
The separatistview suggests that the mechanism that constructs asyntactic description of an utterance is a different mech-anism from that which builds a representation for themeaning of the utterance.
That is, this view suggests thatsyntactic analysis operates according to a different algo-rithm than semantic analysis.
The integrated view, onthe other hand, proposes that syntax and semantics areprocessed by the same mechanism.
This mechanism oper-ates equally well both on syntactic information andsemantic information.
These two positions are thus quitedifferent, and constructing a robust natural languageinterface requires a choice.The third question concerns the relationship betweenlanguage processing and memory processing.
The separa-tist position is that language processing is a special,specific function, largely unconnected from memoryprocesses.
In this view, memory is thought o be a rela-tively passive entity, with little active processing.
Theintegrated position, however, holds a different view ofthe role of memory in language processing.
It suggeststhat language processing is primarily a memory-basedprocess, and further takes the position that languageprocessing and memory processing are the same process.This question is of particular importance because arobust interface will presumably have to employ memoryprocessing of some sort.Note that an intermediate position between the inte-grated and separatist positions is possible.
One can holdthe integrated position with respect o one or two ques-tions and the separatist position with respect o the rest.For example, Bobrow and Webber (1980) describe anatural anguage interface in which syntax and semanticsare processed in a logically simultaneous, intermingledfashion, yet in which syntax and semantics are processedby different mechanisms and in which language andmemory processing is performed by different mech-anisms.
Nonetheless, the distinction represented by thetwo positions is useful.Schank and Birnbaum's integrated processing hypoth-esis is basically the hypothesis that the integrated posi-tion correctly characterizes human processing, and canbe summarized by the following:Syntax and semantics are processed at the same time.Computational Linguistics, Volume 12, Number 2, April-June 1986 93Mallory Selfridge Integrated Processing Produces Robust UnderstandingSyntax and semantics are processed by the same process.Language processing is fundamentally the same asmemory processing.Sehank and Birnbaum present a detailed argument tojustify the integrated processing hypothesis, but itsprimary impact here is its consequences as a model ofhuman understanding.
That is, if the integrated process-ing hypothesis in fact describes human processing, andsince humans are robust language processors, then oneway to build a robust natural language interface is toincorporate the integrated processing hypothesis into anatural language interface.
This conjecture might betermed the "integrated processing produces robustunderstanding conjecture", or the IPPRU conjecture.It is important to understand what this conjecture doesnot say.
The IPPRU conjecture does not claim thatembodying the integrated processing hypothesis i neces-sary to produce robust understanding, only that it is oneapproach that does work.
Although establishing thenecessity of the integrated processing hypothesis torobust understanding would be desirable, this is not with-in the scope of this paper.
Rather, the research reportedhere concerns a first step to the later establishment ofnecessity.
Neither the Integrated Processing Hypothesisnor the IPPRU conjecture claim that syntactic knowledgeis the same as semantic knowledge.
While syntax is proc-essed at the same time as semantics, and by the samemechanism, this paper proposes a different breakdownbetween syntax and semantics.
This breakdown is know-ledge-based instead of processing-based.
That is, thedifference between syntax and semantics in this view liesin the specific knowledge ach represents rather than theorder or processing mechanisms of each.Evaluating the IPPRU conjecture involves certainquestions.
How best can the integrated processinghypothesis be embodied in a program?
What should thedomain of that program be?
How can its performance beevaluated?
In order to address the question of embodyingthe integrated processing hypothesis within a program, animportant distinction must be made between?
a program that may have several modules but whichembodies the integrated processing hypothesis by virtueof the algorithms it employs and the manner in which itmanipulates its data, and?
a program that not only embodies the integrated proc-essing hypothesis but which also is itself integrated, inthe sense of being non-modular.Ideally, the integrated processing hypothesis hould beembodied in a program that is actually integrated as well.However, the construction of such a fully integratedprogram is a lengthy process and requires a number ofdifficult design decisions.
In order to gain information onwhich these design decisions can be made, the MURPHYsystem was developed as a rapid prototype, which,although not fully integrated, oes embody the integratedprocessing hypothesis.
Thus, MURPHY's performancedoes bear directly on the IPPRU conjecture, even thoughMURPHY itself is not fully integrated.The second question concerns the domain withinwhich a natural anguage program operates.
Ideally, thedomain will be both large and realistic.
However, sinceeffort on a large and realistic domain must be justified byhigh performance within a limited domain, it is appropri-ate to experiment with such a limited domain as a neces-sary first step to a large and realistic domain.
This is theapproach taken by others within this area of research(e.g.
Hayes and Mouradian 1981.
Kwasny andSondheimer 1981, Hayes and Carbonell 1981, Dyer1982, Lebowitz 1980).
The limited semantic domainchosen for this research is that of small-scale roboticassembly in a laboratory context.
This domain is appro-priate because it is a subset of a potentially useful real-world domain and because it provides a measure ofunderstanding - the degree to which the system success-fully carries out commands, answers questions, andremembers and uses declaratives.The third question concerns the criteria for evaluatingthe research.
Under what conditions will it be considereda success?
There appear to be basically two: First, doesit in fact perform robustly within its domain?
That is,does it fulfill the requirements described in section 2?Second, does it provide insight as to how its techniquesmight be applicable both to an expansion of the existingdomain and to other domains?
That is, are its limitationsclear, are the areas in which research is needed apparent,and is there suggestive vidence that such additionalresearch would be successful?
Positive answers to boththese questions would suggest hat this research shouldbe considered successful.5 MURPHY's  ARCHITECTUREThis section describes the MURPHY system in detail.MURPHY is composed of four major componentprograms:?
A natural anguage analyzer (NLA), which accesses adictionary of words and phrases to perform low-levelunderstanding of the words in the utterance;?
An inferencer (Robust Back End, or RBE), whichcompletes understanding using conversational history,context, and a body of domain knowledge;?
A conversational control program (CCON), whichperforms inference on the input meanings of the user'sutterances and provides a mixed-initiative conversa-tional ability, and which allows MURPHY to interactwith the robot assembly system;?
A natural language generator (Conceptual Generator,or CGEN), which accepts concepts and expresses themin English.A user's utterance to MURPHY is analyzed by NLA asfar as possible.
NLA then passes its understanding of theutterance to RBE, which completes the understandingprocess using domain knowledge and the conversationalhistory.
RBE then verifies its understanding with the user,94 Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Seffridge Integrated Processing Produces Robust Understandingand if incorrect it infers the next most likely meaning,and so on until it either infers the intended meaning orexhausts the possibilities.
If the latter, control is returnedto NLA to produce its next most likely understanding ofthe utterance, which again is passed to RBE for inference.When the intended meaning is confirmed by the user, it ispassed to CCON, which uses test-action rules to infer aresponse to the utterance.
Some responses involvequeries or answers directed to the user, some involveinternal inferences, and some direct calls to the robotassembly system.
Throughout, CGEN is used when need-ed to generate natural anguage responses.Almost every component of MURPHY is relevant othe question of robust understanding; NLA, RBE, CCON,the dictionary, context, and domain knowledge all haveimportant roles.
This section first describes each in turn,and then describes how these components embody theintegrated processing hypothesis.5.1 REPRESENTING DOMAIN KNOWLEDGE AND CONTEXTMURPHY's domain knowledge consists of a set of seman-tic primitives appropriate to the domain, represented inConceptual Dependency format (Schank 1975~ Schankand Abelson 1977; although MURPHY could be imple-mented with any of a wide variety of knowledge repre-sentation formalisms generally similar to ConceptualDependency).
Each semantic primitive, henceforthcalled a CD, consists of a header followed by a set oflabelled slots.
Together, the header and labelled slotscomprise a CD frame.
CDs can be combined with oneanother by placing one CD into a slot of another, accord-ing to certain restrictions: each CD has certain properties,and each slot in a CD can only accept other CDs withcertain properties.
For example, (requires human) speci-fies that the slot filler is required to have the property(human).
In addition, certain properties are preferable,but not essential.
For example, (prefers small) specifiesthat a filler that has the property (small) is preferable toone which does not; however, slot filling can still proceedeven if the preferred property is absent.
Note that thisnotation for restrictions on what CD can fill a slot inwhat other CD is not intended to be fully adequate butonly to satisfy current needs.
In a real-world system theuse of simple concept attributes would prove insufficient.A real world domain requires complex reasoning todetermine if a concept should be combined with anotherconcept.
However, it seems reasonable to conjecturethat a rich system can be built around the idea that whensuch reasoning is completed its result will be an assertionsimilar to the current attributes.
Thus, the currentapproach does not rule out the use of complex reasoning,and is upwardly compatible with such reasoning.
Howev-er, the current limited domain requires only the existingsimple predicates.
Thus, the primary definition of a CDconsists of the frame definition, the properties of the CD,and the restrictions that specify which kinds of CDs canfill each slot.
For example, the following shows a deft-nition for the CD that refers to the switch contact,object 1:define-conceptframe-header:isa:frame:slot-restrictions:objectlphysicalobject(objectl partof (nil) ref (nil))partof requires physicalobjectref requires determinerIn addition to the information captured by definitionsof this type, MURPHY's knowledge of the various CDpredicates i also represented in a more distributed fash-ion throughout the system.
For example, MURPHY alsoknows objectl as a three-dimensional object in space,represented geometrically as a number of points definingthe vertices of a planar solid.
This representation is usedby robot path planning and collision avoidance software,and is an essential part of the meaning of objectl.
Theother kind of additional knowledge MURPHY possessesabout its meaning representation is possible inferenceswithin the conversational control.
Each conversationalcontrol rule matches ome configuration of CDs, in orderto implement inferences which may be drawn from thepresence of a certain CD.
Each rule thus encodes addi-tional knowledge of a CD.5.2 REPRESENTING LANGUAGE KNOWLEDGEMURPHY's knowledge of words is contained in a diction-ary.
A word definition consists of the word's meaningand its syntax.
The meaning is a single CD or a complexof nested CDs from domain knowledge.
That is,MURPHY's word meanings are pointers into its know-ledge of the world.
Each word meaning may have severalempty slots.
For each empty slot, the word definitionincludes syntactic knowledge about where in the utter-ance a slot filler is expected to be.
This syntactic know-ledge is expressed as a set of independent syntacticfeatures.
These features are formed from the positionalpredicates PRECEDES and FOLLOWS, which are appliedto the short term memory around which NLA's process-ing focuses.
This short term memory contains, in order,the input words, their meanings, and the slots thesemeanings fill in other meanings.
PRECEDES andFOLLOWS relate the position in the input of a potentialslot filler to either the meaning containing the slot, a fillerof another slot in that word's meaning, or a lexical func-tion word.
To represent the knowledge that the filler isfound following the meaning containing the slot, theword definition includes the predicate "follows parent"indexed under that slot.
Similarly, the predicate"precedes (slot object)" represents the knowledge thatthe filler is found preceding the filler of the (slot name)slot, and "follows (fw (function word))" represents theknowledge that the filler is found following the functionword (function word).
Several predicates are used tocompletely describe the position of a filler in an utter-ance.
Thus, each slot in a word meaning has associatedwith it a collection of features describing where in theComputational Linguistics, Volume 12, Number 2, April-June 1986 95Mallory Seffridge Integrated Processing Produces Robust Understandingutterance a filler is expected to be.
For example, in thedefinition of the word contact (as in the switch contact)there is a CD that represents its meaning, and a collectionof syntactic features specifying where the fillers of theempty slots are expected to be.define-wordword: contactmeaning: (objectl partof (nil) ref (nil))syntax: partof-filler precedes parentfollows re f-fillerref-filler precedes parentprecedes partof-fillerMost likely, this representation of syntax cannot hopeto encompass an entire natural anguage.
It is used herebecause it is powerful enough to describe the syntax ofthe natural anguage capabilities of interest.
However, ithas demonstrated reasonable xpressiveness in a numberof different applications (Birnbaum and Selfridge 1981,Cullingford, Krueger, Selfridge, and Bienkowski 1981;Selfridge 1980, Selfridge 1981a; Selfridge 1981b;Selfridge 1982) and thus its use here is not entirely adhoe.5.3 THE NLA PROGRAMWhen the user types an utterance to MURPHY, the utter-ance is first processed by the NLA program.
NLA is adescendant of the CA program (Birnbaum and Selfridge1981), and also uses concepts derived from Wilks(1976).
Its role in the understanding process is to createas complete a CD representation f an utterance's mean-ing as possible using only the meanings of the words inthe utterance.
NLA's processing centers around a short-term memory called the C-LIST.
During analysis, themeaning of each input word is placed on the C-LIST(currently, NLA is limited to words with only a singlemeaning; it cannot disambiguate among multiple wordssenses).
The syntactic and semantic features associatedwith slots in the meanings of each word on the C-LISTare then checked to see if the meaning of any otherwords on the C-LIST can fill any of them.
If so, the CDthat most satisfies the syntactic and semantic featuresassociated with a particular slot is placed in the slot.
Thisprocess is repeated for each CD on the C-LIST.
Whencompleted, what remains on the C-LIST is one or moreCDs constructed by combining the meanings of the wordsin the utterance.
The CD or CDs represent as muchunderstanding of the utterance as could be achieved byexamining only the meanings of the utterance words.More formally, NLA's basic algorithm is as follows:(1) Place the CD meaning of each utterance word orphrase on the C-LIST.
(2) For each empty slot in each CD on the C-LIST,collect the syntactic and semantic features associ-ated with that slot.
(3) Search the C-LIST, and retrieve all the CDs thatsatisfy that slot's semantic requirements.
These CDsare candidate slot fillers.
(4) Order the candidates by preference value (thenumber of semantic preferences and syntacticfeatures a candidate satisfies).
(5) Examine the candidate with the highest preferencevalue.
If the CD is not marked "used", then fill thecurrent slot with it and mark it "used".
(6) If the candidate is marked "used" and its prefer-ence value for the slot it already fills is higher thanthe current preference value, then reject it andexamine the candidate with the next highest prefer-ence value.
(7) If the candidate is marked "used" and its prefer-ence value for the slot it already fills is lower thanthe preference value associated with that other slot,then remove it from the other slot and fill the"current slot with it, and recursively call NLA to refillthe other slot.In addition to the above algorithm, NLA performsadditional work.
This additional work is needed to allowNLA to produce a next most likely interpretation of theinput.
It does this by keeping track of all the candidatesfor each slot and their preference values, and by main-taining a list of rejected interpretations.
When calledupon to generate the next most likely interpretation, itdoes so by finding the most preferred interpretation thatdoes not appear on the list of rejected interpretations.
Inthis way, NLA can provide all possible interpretations ofthe input (in conjunction with the RBE program,described in the following subsection), ranked accordingto likelihood.
It should be noted that the technique ofkeeping track of rejected interpretations i crude yetfunctional; future work will address the question ofimproving this implementation.Since section 5.6 argues that both NLA and RBE useessentially the same mechanism, an abstract descriptionof each is important.
A number of alternativedescriptions exist; viewing NLA's understanding processas tree search is best for the current purposes.
The interi-or nodes of this tree represent partial understandings ofthe input, while leaves represent complete under-standings.
That is, interior nodes are CDs that containempty slots, while leaves are CDs that do incorporateevery word meaning.
The root of this tree is a start node,whose descendants are the CDs from the utterance thathave empty slots.
Given that the search process has acurrent node, generating the descendants of that nodeinvolves choosing an empty slot, retrieving its tests, usingthose tests to retrieve from the C-LIST all possible candi-date fillers, creating a copy of the CD at the current nodefor each candidate and filling the slot with a candidatefiller, and finally building a new node for each such CD.The CD at each new node will thus have one less emptyslot.
The new current node is then chosen to be theunvisited node whose filler had the highest preferencevalue for that slot.
Search proceeds until a leaf is reachedin which all possible slots have been filled from CDs onthe C-LIST.
Since NLA first chooses slot fillers that best96 Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Sell'ridge Integrated Processing Produces Robust Understandingsatisfy the syntax and semantics associated with a slot, itsprocessing strategy implements a kind of local best-firstsearch (Nilsson 1971).The following example describes part of NLA's proc-essing of the sentence put the post on the base.
In order toillustrate preference, this example focuses on filling theVAL slot in the meaning of on, in the middle of process-ing the input.
Of the two candidate fillers available atthis point for the VAL slot (the meanings of post andbase) the meaning of base is currently being used as thefiller of the OBJECT slot.
However, preference overridesthis prior slot filling, moves the meaning of base to theVAL slot, and finds the next best filler for the object slot.The example begins with the state of the C-LIST at thatpoint, then shows the selection of the filler of the VALslot, and finally shows the selection of the next-best fillerfor the object slot.C-LIST: (PTRANS ACTOR (NIL)OB JECT (PHYS-OBJ  TYPE (BASE)PART-OF  (NIL)REF (NIL))TO (TOP VAL (NIL)))(REF) -- used(PHYS-OBJ  TYPE (POST) PART-OF  (NIL)(TOP VAL (NIL)) -- used(REF)(PHYS-OBJ  TYPE (BASE) PART-OF  (NIL)REF (DEF))REF (NIL)) -- usedEXAMINING:  (TOP VAL  (NIL))CHECKING VAL SLOT: requ i res  phys -ob jfo l lows "on", "put", ob jec t - f i l l e rVAL CANDIDATES:  (PHYS-OBJ  TYPE (POST) PART-OF  (NIL) REF (NIL))p re ference  va lue  I: fo l low "put",(PHYS-OBJ  TYPE (BASE) PART-OF  (NIL) REF (NIL))p re ference  va lue  2: fo l lows "on", "put"PREFERRED FILLER: (PHYS-OBJ  TYPE (BASE) PART-OF  (NIL) REF (NIL))REMOVING F ILLER OF OBJECT SLOTRE-CHECKING OBJECT SLOT:OBJECT CANDIDATES:PREFERRED FILLER:requ i res  phys -ob jfo l lows "put", p recedes  to - f i l l e r(PHYS-OBJ  TYPE (POST) PART-OF  (NIL) REF (NIL))p re ference  va lue  2: fo l lows "put"p recedes  to - f i l l e r(PHYS-OBJ  TYPE (BASE) PART-OF  (NIL) REF (NIL))p re ference  va lue  I: fo l lows  "put"(PHYS-OBJ  TYPE (POST) PART-OF  (NIL) REF (NIL))C-LIST: (PTRANS ACTOR (NIL)OB JECT (PHYS-OBJ  TYPE (POST)PART-OF  (NIL)REF (NIL))TO (TOP VAL  (PHYS-OBJ  TYPE (BASE)PART-OF  (NIL)REF (NIL))))(REF) -- used(PHYS-OBJ  TYPE(TOP VAL (NIL))(REF)(PHYS-OBJ  TYPE(POST) PART-OF  (NIL) REF (NIL)) -- used-- used(BASE) PART-OF  (NIL) REF (NIL)) -- usedThe understanding process is complete when theremaining slots in the meaning of base have been exam-ined and the REF slot filled with the meaning of thesecond the.
At this point, all the "used" concepts areremoved from the C-LIST; those concepts remainingconstitute NLA's best understanding of the input:C-LIST : (PTRANS ACTOR (NIL)OB JECT (PHYS-OBJ  TYPE (POST)PART-OF  (NIL)REF (DEE))TO (TOP VAL (PHYS-OBJ  TYPE (BASE)PART-OF  (NIL)REF (DEF))))Computational Linguistics, Volume 12, Number 2, April-June 1986 97Mallory Self ridge Integrated Processing Produces Robust UnderstandingAt this point, NLA has produced its best understand-ing of the input.
Since this understanding contains emptyslots, it would require additional processing by RBE.
IfNLA were asked to produce its next best interpretations,it would reprocess the input for an alternative meaning,and fill the OBJECT slot with the meaning of base and theVAL slot with the meaning of post.5.4 THE RBE PROGRAMWhen NLA has concluded processing an input, it may nothave been fully understood.
If the input was missingwords, the C-LIST can contain CDs with unfilled slotsand several CDs that have not been combined into asingle CD.
In these cases, RBE is called to completeunderstanding.
When RBE must fill empty slots in a CD,it infers fillers from domain knowledge.
When RBE mustcombine several CDs into a single CD, it searches domainknowledge for a CD whose empty slots could be filled bythem.
Once it has filled all the empty slots with potential-ly correct fillers, and has combined all the uncombinedCDs into one, the result is a complete understanding ofthe meaning of the utterance.
Before passing this mean-ing to the conversational control, RBE verifies its under-standing is the one intended by the user by generating itin English.
If the user disagrees with the interpretation,RBE searches domain knowledge further, produces thenext most likely interpretation, and so on until it eitherinfers the intended meaning or has exhausted all possibil-ities.
In general, RBE prefers to fill a slot with a C-LISTelement whenever possible.
This is because the userpresumably included a word in an utterance because itsmeaning was intended to contribute to the meaning ofthat utterance.
If no C-LIST elements are appropriate slotfillers, RBE searches corrections, conversational history,and domain knowledge.
Thus RBE will infer concepts inthe order of reasonable likelihood.RBE's search process could not be guaranteed toterminate if RBE were as just described.
Under someconditions, it searches context and domain knowledge fora CD with empty slots, intending to fill them with theresults of partial understanding.
That is, RBE must infer aCD with a slot that can be filled by a CD currently on theC-LIST.
Unfortunately, this process can proceed indefi-nitely: how does RBE know when to stop inferringincreasingly inclusive CDs?
The function of RBE's infer-ence provides an answer: RBE stops when it has inferreda CD to which CCON can respond.
Since RBE continuesthe inference process until it fills all the slots in a CD towhich CCON can respond, these CDs serve as goals toRBE.
Consequently, they are termed goal concepts.
Now,there are several possible implementations that allowCCON to communicate goal-concepts to RBE.
Thecurrent implementation is the simplest: annotate goalconcepts as such, and have RBE's search be top-downbeginning with a goal concept.
If the first goal conceptchosen proves incorrect, RBE uses the next one, and soon, until it finds the correct one.
Future work will exploremore sophisticated search strategies.RBE's algorithm focuses around a data structure calledthe NODE-list, initially empty, and is given below:(1) If there is no goal concept on the C-LIST, collectfrom conversational history and domain knowledgeall goal concepts and place them on NODE-list.Otherwise, place all goal concepts from the C-LISTon NODE-list.
(2) For each empty slot within the first NODE-listelement, retrieve from context and domain know-ledge all the candidate fillers that satisfy that slot'ssemantic requirements.
Order all candidates fromuser's corrections before those from conversationalhistory, and order all those from conversationalhistory before those obtained from domain know-ledge.
Within each group, order candidates accord-ing to the number of semantic preferences theysatisfy.
(3) For each candidate, create a copy of the top ofNODE-list and place that candidate into the slot ofthe copy, and place the resulting CD back ontoNODE-list.
(4) If the CD at the beginning of NODE-list has emptyslots, go to (2).
If the CD has no empty slots, sendit to CGEN to query the user whether it is correct ornot.
(5) If the user confirms the CD, send it to CCON forresponse.
If not, remove the CD from NODE-listand go to (2).Just as NLA's processing can be characterized as treesearch, RBE's operation can be described the same way.That is, RBE takes the best understanding provided byNLA as the root node, and descendants are nodes atwhich CDs have additional slots filled.
Leaf nodes arethose that have no empty slots at all.
Generating thedescendants of a node involves?
choosing an empty slot;?
retrieving its semantic requirements and preferences;?
retrieving all possible candidates from domain know-ledge, conversational history, and user corrections thatsatisfy the requirements;?
creating a copy of the CD at the current node for eachcandidate and filling the slot with a candidate filler;and, finally,?
building a new node for each such CD.The CD at each new node will thus have a formerlyempty slot filled.
The filler will often have empty slots ofits own, and these will be filled in their turn.
Infiniteregress is stopped at an arbitrary fixed level to assuretermination.
(An alternate approach would be breadth-first, and would require no such arbitrary limitation.)
Thenew current node is then chosen to be the unvisited nodewhose filler had the highest preference value for that slot.Search proceeds until a leaf is reached at which there areno empty slots at all.
At this point, the understandingrepresented by the CD at that leaf is generated in naturallanguage, and the user is asked to verify the understand-ing.
If it was correct, then the search is over.
If not, then98 Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Selfridge Integrated Processing Produces Robust UnderstandingRBE backs up and continues the search at the point atwhich it was halted.
This process continues until RBE hassearched the entire tree of possibilities.If the intended meaning has still not been inferred,then NLA is called to generate its next most likely inter-pretation of the words in the input, and RBE begins againto infer fillers for empty slots in this CD.
Thus, RBEcontinues the search for a complete understanding begunby NLA, using essentially the same local best-first search-ing process to do so; if its search fails, it returns controlto NLA to generate its next most plausible interpretationof the input, and RBE again continues the search.
Thus,the best-first search processes of NLA and RBE togetherperform a unified best-first search.
This unified searchprocess exhaustively searches the space of possiblemeanings of the input utterance, ordered by likelihood asdescribed, and will eventually find any finite CD (Nilsson1971).
Since any intended meaning is assumed to berepresented by a finite CD, the search performed by NLAand RBE is guaranteed to eventually infer the intended,meaning.The following example illustrates RBE's processing onthe understood meaning of put the post on the base.
RBEbegins when it receives NLA's best understanding of thissentence and places it on NODE-list, as shown below.NODE-l ist:  (PTRANS ACTOR (NIL)OBJECT (PHYS-OBJ TYPE (POST)PART-OF (NIL)REF (DEF))TO (TOP VAL (PHYS-OBJ TYPE (BASE)PART-OF (NIL)REF (DEF))))RBE removes the first CD in NODE-list - the oneshown above - and notes that it has an empty ACTORslot.
It retrieves the semantic requirements and prefer-ences for the ACTOR slot, which specify that the filler isrequired to be an animate being.
Since, in this example,there are assumed to be no corrections or conversationalhistory, RBE searches only domain knowledge for such afiller.
It retrieves the concepts for itself and that of theuser, creates copies of the CD with these as ACTORfillers, and pushes them onto NODE-list.
The copycontaining the concept of MURPHY itself is on the frontof NODE-list because it was found first in domain know-ledge; this order reflects the knowledge that MURPHY ismore likely to be the intended actor.
The new NODE-listis shown below:NODE-l ist:  (PTRANS ACTOR (MURPHY)OBJECT (PHYS-OBJ TYPE (POST)PART-OF (NIL)REF (DEF))TO (TOP VAL (PHYS-OBJ TYPE (BASE)PART-OF (NIL)REF (DEF))))(PTRANS ACTOR (USER)OBJECT (PHYS-OBJ TYPE (POST)PART-OF (NIL)REF (DEF))TO (TOP VAL (PHYS-OBJ TYPE (BASE)PART-OF (NIL)REF (DEE))))The cycle continues when RBE again removes the firstCD and again examines it for empty slots.
The nextempty slot it finds is the first PART-OF slot.
The onlypossible filler from domain knowledge is the CD(COMPOUND-OBJ TYPE (SWITCH)).
This is used to fillthe first PART-OF slot, and the resulting CD is pushedback on NODE-list.
During the next cycle, the just-modi-fied CD is removed from the top of NODE-list and thesecond PART-OF slot is found to be empty, and is like-wise filled with (COMPOUND-OBJ TYPE (SWITCH)).
Theresulting NODE-list is shown below:NODE-l ist:  (PTRANS ACTOROBJECTTO(MURPHY)(PHYS-OBJ TYPE (POST)PART-OF (COMPOUND-OBJTYPE (SWITCH))REF (DEF))(TOP VAL (PHYS-OBJ TYPE (BASE)PART-OF (COMPOUND-OBJTYPE (SWITCH))REF (DEF))))Computational Linguistics, Volume 12, Number 2, April-Jane 1986 99Mallory Selfridge Integrated Processing Produces Robust Understanding(PTRANS ACTOR (USER)OBJECT (PHYS-OBJ TYPE (POST)PART-OF (NIL)REF (DEF))TO (TOP VAL (PHYS-OBJ TYPE (BASE)PART-OF (NIL)REF (DEE))))At this point the first CD on NODE-list has no emptyslots, and furthermore is a known goal concept.
RBE callsthe generator to ask the user if this was his intendedmeaning.
If the user verifies this as his intended meaning,RBE has successfully completed the understanding proc-ess.
If not, then RBE removes this CD from NODE-listand proceeds to cycle further.
In the case describedabove, this would amount to RBE inferring that possiblythe intended filler of the ACTOR slot was the user ratherthan MURPHY itself.When RBE has successfully verified its understandingwith the user, it passes this completed understanding tothe CCON program, which will, in turn, infer a responseto the input.
In addition, RBE also updates the conversa-tional history with the concepts that comprise thecompleted understanding.
Since RBE uses candidatesfrom conversational history before those from domainknowledge, it uses potentially, more relevant conceptsbefore less relevant ones.
In particular, RBE can infergoal concepts as a function of context.
Given an identi-cal input utterance, MURPHY will understand it one wayin one conversational context, and another way in anoth-er context, since it can obtain a goal concept fromconversational history.5.5 THE CCON PROGRAMThe CCON program is invoked when NLA and RBE haveunderstood the input to the user's satisfaction.
It uses aset of "if-then" conversational rules to respond to thatunderstanding.
The complete CD representing the mean-ing of the input is placed on a stack, and the rules arechecked to see if the test of any matches the CD on topof the stack.
If so, the action of that rule is executed.
Theaction can send commands to the robot system, add CDinferences to the stack, query MURPHY's knowledgebase, and ask the user questions by sending concepts toCGEN to be generated.
Thus, CCON is partly a rule-based system, partly a problem-reduction problem solver,and partly a knowledge-based inference engine.
Thefollowing algorithm describes its operation:(1) Begin when a goal concept is placed on the stack.
(2) Find the first rule whose test matches the concepton the top of the stack; if the stack is empty, returncontrol to NLA to seek another user input.
(3) Pop the stack and execute the action of that rule.
(4) Go to 2.Although CCON does no significant searching as far asthe understanding process is concerned, it is consistent tonote that it can as well be seen as performing a searchprocess.
Since the action of a rule can place concepts onthe stack, CCON can in fact traverse a tree of concepts,and thus perform essentially the same process as NLAand RBE.5.6 HOW MURPHY EMBODIES THE INTEGRATED PROCESS-ING HYPOTHESISIt is important now to consider the way in whichMURPHY embodies the integrated processing hypothesis.Three questions must therefore be addressed.
First, howdoes MURPHY process syntax and semantics imultane-ously?
Second, how are syntax and semantics processedby the same mechanism?
Third, how is language proc-essing fundamentally the same as memory processing?These questions are addressed in turn.To understand how MURPHY processes yntax andsemantics imultaneously, consider how NLA processesinput.
When NLA is understanding the input as well aspossible, it is filling slots in one word meaning withanother word meaning.
To determine the degree to whicha filler is appropriate for a slot, NLA performs a set oftests on the potential filler.
These tests derive from theslot, and include semantic requirements, emantic prefer-ences, and syntactic features.
As described earlier, NLAfirst collects all the tests, and then evaluates them togeth-er.
No distinction is made between semantic and syntac-tic tests during the evaluation process, and no test'soutput depends on the result of any other test.
Since theyare independent, he tests are logically simultaneous, andthus NLA processes syntax and semantics imultaneously.It could be argued, however, that since RBE usessemantic information too, and does so after syntax hasbeen used by NLA, this later use of semantics constitutesa departure from the simultaneity of syntactic andsemantic processing.
However, at this point languageknowledge is not being used, since fillers cannot befound from the C-LIST.
Thus it is not appropriate toconsider syntactic knowledge, since nothing but inputcould usefully be processed syntactically.
Furthermore,since RBE sometimes returns control to NLA, syntacticprocessing can be seen as interspersed with semantic andmemory processing, and thus embodying a form of simul-taneity of syntactic processing and semantic processing.Nonetheless, unifying NLA and RBE to provide forcomplete simultaneity would be desirable, and remainsfor future research.To understand how syntax and semantics are proc-essed by the same mechanism, consider again NLA'sprocessing of the input.
When syntax and semantics arebeing processed by NLA, not only are they being proc-essed simultaneously but the same mechanism withinNLA is also performing the processing.
This mechanism isthe one that takes each feature test in turn, regardless of|00  Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Seffridge Integrated Processing Produces Robust Understandingwhether it is a semantic requirement, semantic prefer-ence, or syntactic feature, and evaluates it with respect oa candidate filler to determinate whether to fill the slotwith that filler or not.
Thus NLA applies the same mech-anism to evaluate syntax as semantics.
Furthermore, RBEuses this same mechanism when inferring CDs fromcontext and domain knowledge.
It retrieves candidateslot fillers based on the degree to which they satisfysemantic requirements and preferences, using exactly thesame mechanism as NLA uses to evaluate candidate slotfillers from the C-LIST.
It is this mechanism, used byboth NLA and RBE, that processes both syntax andsemantics in the same manner.To understand how language processing is fundamen-tally the same as memory processing within MURPHY,compare the algorithms being executed by NLA and RBE.Within MURPHY, NLA performs "language processing"while RBE performs "memory processing".
As describedabove, each uses essentially the same form of best-firstsearch, and together they combine to form a singleunified best-first search.
Further, each are manipulatingthe same CDs in the same way, and control passes backand forth between them.
NLA chooses to fill a slot in aCD with a candidate filler CD if that candidate filler satis-fies the most syntactic features and semantic require-ments and preferences of the CDs in the C-LIST.
RBEchooses to fill a slot in a CD from candidate fillers drawnfrom conversational history and domain knowledge ifthat candidate filler satisfies the most semantic require-ments and preferences of the CDs retrieved.
Both NLAand RBE determine the most likely filler for a slot, andhence ultimately the most likely interpretation of theinput, by choosing the filler that satisfies the greatestnumber of syntactic and semantic features and prefer-ences.
Thus, both language and memory processing arecarried out by the same mechanism of search, evaluationof features, requirements and preferences, and choice ofthe CD satisfying the greatest number.6 ROBUST UNDERSTANDINGIt is important to analyze the relationship betweenMURPHY's performance and its implementation of theintegrated processing hypothesis, and to assess thatperformance overall.
This section will consider therelationship between each aspect of robustness and eachcomponent of the integrated processing hypothesis, andwill also discuss two general measures of performance.6.1 UNDERSTANDING INPUT WITH VARIANT SYNTAXTo describe how MURPHY understands input with vari-ant syntax, consider again interaction (1) from the exam-ple of section 2.
In this interaction, the user typed Displaythe current image workspace, in which image and work-space are reversed.
To understand this, NLA combinesthe meanings of the words in the utterance together, aswell as possible, by evaluating the semantic and syntacticfeatures of each slot in a word's meaning with respect othe remaining meanings, which are candidate slot fillers,and choosing as a filler the meaning that satisfies themost features.
Even though in the user's utterance theword workspace was out of position, and hence not allfeatures were true, its meaning still satisfied the mostfeatures, and was hence chosen as the filler of theSOURCE slot.
At this point, NLA has produced a singleCD containing no empty slots.
This CD is verified to be agoal concept, and understanding is complete.Now, in interaction (1) a filler was out of positionwith respect to the meaning that contained the slot.Consider the following more complex example notcontained in the example on page 91.> The display current workspace image.DO YOU MEAN "Display the current workspace image"?> Yes.OK (The system displays the image.
)In the user's utterance the word the is out of position.However, the system correctly understands that themeaning of the is intended to fill the REF slot in themeaning of image, and demonstrates it knowledge of thecorrect position in its query to the user.
Correct under-standing occurs because although not all of the syntacticand semantic features associated with the REF slot aretrue, the meaning of the is nonetheless the best filleravailable.
Syntactic features are retrieved from the defi-nition of image, and from the definition of display as well,since the meaning of image is understood to fill theOBJECT slot of the meaning of display and MURPHYpropagates syntax downward from parent to filler.
Thesefeatures are then evaluated with respect o the meaningof the.
Although not all syntactic features are true,MURPHY fills the REF slot with the meaning of the as thebest available.
These features and their evaluations areshown here:origin of featuremeaning of imagedefinition of imagedefinition of displayfeature valueref requires ref-spec Tref-filler precedes parent Tref-filler precedes subject-filler Tref-filler precedes time-filler Tref-filler follows meaning of display FEach of the examples considered above assumes thatthe utterance containing the out-of-order words does nothave an alternate interpretation for which the words arenot out of order.
For example, in Display the currentimage workspace, it was assumed that workspace was notsomething that could be displayed.
If it was, then theutterance makes sense as it is, and MURPHY would haveto decide which interpretation was correct, the one inwhich it was the workspace being displayed or the one inwhich the image is to be displayed.
As far as MURPHY isconcerned, there are two distinct situations in which anutterance with variant syntax can have two differentmeanings.
MURPHY can handle one, while the other isbeyond its current capabilities.
The first is one in whichthere are two empty slots, each of which can accept theComputational Linguistics, Volume 12, Number 2, April-June 1986 101Mallory Seffridge Integrated Processing Produces Robust Understandingother's filler.
In this situation, MURPHY will assume thatthe most preferred interpretation is correct, even if theuser mispositioned words and actually intended the other.If the user objects, however, MURPHY will infer thealternate interpretation as the next most preferred mean-ing.
The second situation arises when the word that is outof position has multiple meanings, and while it is out ofposition with respect o its intended meaning, it is in acorrect position with respect o an alternate meaning.
Forexample, workspace in the above example has two differ-ent meanings, one in which it describes the contents ofimage, and one a spatial area which can itself bedisplayed.
MURPHY cannot handle this second type ofinput since it cannot disambiguate multiple word senses.However, current research is directed toward includingthe ability to handle arbitrary numbers of word senses, asdemonstrated by Dawson (1984), and future researchmust address this problem.It is important o describe how MURPHY's ability tounderstand espite variant syntax derives from the inte-grated processing hypothesis.
Syntactic knowledge isused by NLA during slot filling to help decide which CDon the C-LIST should fill a given slot.
The syntacticfeatures for a slot are grouped with the semantic prefer-ences and semantic requirements for that slot, and candi-date fillers are scored to see which features, preferences,and requirements each satisfies.
The slot is filled with thecandidate that (a) satisfies the most with the highestscore and (b) is not already filling another slot with ahigher score.
This means that even though certainfeatures or preferences may not be satisfied by a candi-date, if that candidate nonetheless has the highest score itwill be chosen as the filler.
In particular, it means that afiller can be chosen even though some of the syntacticfeatures that describe its intended position are false, aswould be the case if the filler was out of position.
In thiscase, the combination of the remaining true syntacticfeatures, the semantic preferences, and the semanticrestrictions upply enough information to determine thecorrect meaning of the utterance ven though the word isout of position.
Thus, the ability to understand input withvariant syntax is a direct consequence of MURPHY'sprocessing syntax and semantics simultaneously and of itsprocessing syntax and semantics with the same mech-anism.6.2 UNDERSTANDING INPUT WITH MISSING WORDSIncomplete input prevents NLA from combining theC-LIST elements into a single CD and causes the C-LISTelements to have unfilled slots.
When this occurs, RBEcompletes the understanding process by inferring addi-tional concepts.
To understand this inference process,consider the following example, similar to (2) in theexample on page 91.> Threshold the current image at fifty.DO YOU MEAN "threshold the current workspace imageat fifty"?> Yes.OK(The system displays the operation occurring.
)In this example, the user's utterance was missing theword workspace.
NLA understands this utterance as wellas possible, but fails to find a filler for the SOURCE slotin the meaning of image.
RBE is called to infer a filler.First, the semantic requirements of the SOURCE slot arecollected: the filler is required to be a possible imagesource.
Then, RBE first searches the C-LIST for conceptswhose attributes match this requirement.
Not finding anyon the C-LIST, RBE searches domain knowledge, andfinds the meaning of workspace.
This candidate isinserted into the empty slot, and the overall meaning isverified by the user.The previous paragraph described the case in whichthe meanings of the missing words were slot fillers inframes supplied by other words contained in the utter-ance.
What if one or more of those frame-supplyingwords were missing?
For example, suppose the utterancedid not contain the main concept word.
In this case,NLA's partial understanding of the input will be in theform of several uncombined CDs.
In this case, RBE mustdo more than merely fill empty slots.
It must infer anentire additional CD which itself has empty slots that canbe filled by the various CDs produced by NLA, as well asinfer fillers for any remaining empty slots.
This entireadditional CD is inferred from context and domain know-ledge.
For example, consider interaction (6) from theexample of section 2.
In this interaction, MURPHY askedthe user What does a switch post look like?, and the userresponded A metal cylinder two inches long.
The user'sresponse is thus missing several words, including themain frame-building word is.
NLA's best understanding istwo isolated CDs representing the meaning of the phrasesa metal cylinder, and two inches long.
In order to under-stand the utterance as a whole, RBE must infer the mean-ing of the word is, which has slots for the meanings ofthese phrases, and must also fill a third slot in the mean-ing of is from domain knowledge with the meaning of thephrase a switch post.
Note specifically that it is the mean-ing of the word is that is the main concept of the utter-ance, yet is was missing from the utterance.
MURPHYdoes infer the meaning of is, fills two of its three emptyslots with the meanings of the phrases understood byNLA, and fills the third with the meaning of A switch post.The user verifies this understanding, and thus MURPHYhas understood espite a missing frame-building word.The final example of understanding utterances that aremissing words is the case in which the inferred slot filleritself has empty slots.
For example, suppose the user'sinput is merely the utterance Display, as in the followingexample:102 Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Self ridge Integrated Processing Produces Robust Understanding> Display.DO YOU MEAN "Display the current workspace image"?> Yes.OK(The system displays the image.
)Now, the meaning of display has one empty slot, thatwhich is to be displayed, and MURPHY must infer thisfiller.
However, suppose that the inferred filler is themeaning of image.
In this case, there are additional slotsto fill, namely the REF, TIME, and SOURCE slots, beforethe utterance will be completely understood.
In order tounderstand the utterance Display, NLA first understandsas well as possible.
This results merely in the meaning ofdisplay, which is passed to RBE.
RBE then searchescontext and domain knowledge for fillers of the IMAGEslot.
It finds the meaning of image, which itself has emptyREF, TIME, and SOURCE slots.
RBE searches contextand domain knowledge for fillers for these slots, andfinds the meanings of the, current, and workspace.
Theuser verifies these inferences, and understanding hasbeen successful.There is a second case, however, in which the fillers ofthe to-be-inferred CD have been supplied in the utter-ance.
For example, consider the following:> Display the current workspace.DO YOU MEAN "Display the current workspace image"?> Yes.OK(The system displays the image.
)In the user's input, the meaning of display has anempty IMAGE slot, which is to be filled by the inferredmeaning of image, as before.
The meaning of image, inturn, has empty REF, TIME, and SOURCE slots.
SinceRBE checks C-LIST elements before checking domainmemory, it finds the fillers for these slots in the meaningsof the input words current and workspace without search-ing domain knowledge.How does this performance derive from the integratedprocessing hypothesis?
Consider the case in which theutterance is missing one or more words.
In this case,NLA will give RBE a CD containing one or more emptyslots to be filled.
RBE searches context and domainknowledge for appropriate fillers for each empty slot, andfor empty slots in those fillers, and so on, until it builds aCD with no empty slots.
If the user confirms the CD, RBEis done.
If not, RBE resumes earching to infer the nextmost likely set of fillers.
Since RBE evaluates the suit-ability of fillers for slots in the same way NLA does, it isthe fact that language processing is fundamentally thesame as memory processing that allows MURPHY tounderstand espite missing words.
Moreover, this identityallows MURPHY to understand utterances in which themissing word's meaning contains a slot that must be filledby the meaning of a word that was present in the input.Since, presumably, meanings derived from input wordsshould be used before meanings not so derived, RBEmerely searches the C-LIST before searching context anddomain knowledge, and gives preference to candidatesfound there.
Thus processing language and memory usingthe same mechanism enables MURPHY to infer themeanings of missing words yet give preference to mean-ings derived from input words.6.3 UNDERSTANDING ELLIPSESUnderstanding ellipses requires that knowledge of theconversation be established, from which MURPHY caninfer what was omitted.
MURPHY maintains a conversa-tional history by decomposing the meaning of each inputinto its component parts and appending them to domainknowledge.
Thus conversational history is the initial partof domain knowledge, with the meanings of the mostrecent inputs first.
Thus when RBE searches domainknowledge to complete understanding of an incompleteinput, it first finds concepts derived from the meanings ofthe most recent user inputs.
If any of these concepts areappropriate for understanding, then they will be part ofthe most likely interpretation of the input.
Consider thefollowing example:> Where is the switch contact?THE SWITCH CONTACT IS NEXT TO THE SWITCH BASE> The switch post?DO YOU MEAN "Where is the switch post?
"> Yes.After MURPHY answers the user's first question, itadds the meanings of where and is to conversationalhistory (as well as the rest of the component meanings ofthe question).
When the user subsequently asks Theswitch postL NLA first understands this phrase inisolation.
The resulting meaning is not a goal concept, soRBE retrieves goal concepts from conversational history,finds the meaning of is previously placed there, and fillsits OBJECT slot with the meaning of The switch post.
Itthen examines the result to find additional empty slots,finds the VAL slot, and fills it with the meaning of wherealso retrieved from conversational history.
MURPHYunderstood an elliptical input, and has used its conversa-tional history to do so more rapidly than it would haveotherwise.MURPHY's ability to understand ellipses derives fromthe fact that it processes language and memory using thesame mechanism.
When the user's input is elliptical,NLA's understanding is incomplete.
In order to fill emptyslots and combine uncombined CDs, RBE searchesdomain knowledge.
Since the first part of domain know-ledge contains conversational history, RBE will searchconversational history as it attempts to complete theunderstanding.
Thus, if the utterance is elliptic, theappropriate concepts from conversational history will befound by RBE during its normal search.
Since this searchis the same process as that used by NLA and by RBE as itsearches domain knowledge proper, MURPHY's ability tounderstand elliptic utterances derives from the fact that itprocesses language and memory using the same mech-anism.Computational Linguistics, Volume 12, Number 2, April-June 1986 103Mallory Seffridge Integrated Processing Produces Robust Understanding6.4 CORRECTABILITYMURPHY's ability to enable the user to correct incorrectinferences operates very similarly to its ability to under-stand elliptical utterances.
This ability is needed whenMURPHY performs inferences in response to an incom-plete input.
Instead of merely saying "no" to MURPHYwhen it asks if its interpretation is correct, the user canprovide a correction.
MURPHY invokes NLA to under-stand the correction, and then adds its understood mean-ing to domain knowledge as if it were conversationalhistory.
Then, when RBE is searching further to infer thenext possible interpretation for the user's original utter-ance, the first possibility found will be the meaning of thecorrection.
Assuming the correction was indeed correct,RBE's next inference will contain the correction, the userwill verify it, and understanding will be complete.For example, suppose the user intended to type Whereis the switch contact?
but omitted the word contact andtyped instead Where is the switch?
Since the switch hasnot been assembled yet, NLA does not understand thisutterance as referring to the switch as a whole.
Rather, itunderstands only that the user is asking for the locationof something, and does not know what role the meaningof switch plays in the utterance.
Therefore RBE mustinfer the object whose location is being asked, and musttry to use the meaning of switch to fill a slot in thatinferred object's CD.
It happens that in this case it wouldfirst find the meaning of post, then of base, and finally ofcontact, and use the meaning of switch to fill the PARTOFslot in each:> Where is the switch?DO YOU MEAN "Where is the switch post"?> NoDO YOU MEAN "Where is the switch base"?> NoDO YOU MEAN "Where is the switch contact"?> Yes.THE SWITCH CONTACT IS NEXT TO THE SWITCHBASE.Thus MURPHY eventually understands, but conjecturestwo incorrect meanings before conjecturing the correctone.
However, if the user offers a correction, RBE'ssearch is concluded more quickly, as shown below.> Where is the switch?DO YOU MEAN "Where is the switch post"?> No, contact.THE SWITCH CONTACT IS NEXT TO THE SWITCHBASE.MURPHY's ability to generate a series of possibleinput meanings ordered by likelihood is a consequence ofthe fact that the processing performed by NLA and RBEis a best-first search through a tree of semantic struc-tures.
Leaves of the tree are possible meanings of theinput; since processing is best-first, MURPHY producesthe most likely meaning first.
However, if the first mean-ing is incorrect, the search can be resumed and the nextmost likely meaning obtained.
Further, the fact that theappropriate data structures are maintained uring searchallows the understood meanings of corrections to beinserted when available, and yields a biased search infavor of the corrections if the first conjecture was incor-rect.
Since MURPHY's ability to perform best-first under-standing results from the fact that NLA and RBEcooperate to perform best-first search, and since this is aconsequence of the fact that both implement he samealgorithm, MURPHY's ability stems from its use of thesame mechanism for both language and memory process-ing, and hence from the integrated processing hypothesis.6.5 OVERALL PERFORMANCEMURPHY can be evaluated with respect to two furthermeasures of performance.
The first is the number ofincorrect understandings MURPHY must generate beforeinferring the intended meaning.
This depends on thenumber of concepts that must be inferred to completeunderstanding, the number of candidate fillers for thoseslots that exist in domain knowledge, and the number ofalternative assignments of fillers to slots within thosemeanings deriving from the input words.
The product ofthese parameters i  the number of possible meanings of agiven input, and the order in which these are generated isa function of the order of concepts in conversationalhistory and domain knowledge and an ordering imposedby semantic preferences during search.
Since theintended meaning is one of the possible meanings andhence has a position in this order, the number of incor-rect meanings which must be generated are those that liebefore the correct meaning in this order.
In practiceMURPHY usually generates from zero to three incorrectmeanings before inferring the correct meaning; generallythe constraints imposed by the knowledge representationrenders tractable the combinatorially explosive numberof possibilities.
(Scaling the approach to richer environ-ments will require much more sophisticated models ofcontext - not the focus of the research reported here.)
Ifa correction is given, of course, then the very next mean-ing is usually correct.The second measure of performance is the question ofwhether MURPHY really understands all cases of missingwords, ellipses, and out-of-order words.
AlthoughMURPHY has not been tested on every possible input, itsunderstanding mechanisms implement a best-first searchmechanism (Nilsson 1971) that exhaustively searches thespace of possible meanings of an input utterance in anorder determined by the meanings of input words,conversational history, and domain knowledge asdescribed earlier.
Thus, since "understand" means"eventually understand" in this paper, and since thesearch is exhaustive, MURPHY will understand any input,including all cases of missing words, ellipses, and out-of-order words, as long as that meaning is representablewithin the CDs MURPHY knows about,Indeed, MURPHYwill attempt o make sense of deliberate nonsense, and ifit can do so using the concepts in the input, conversa-104 Computational Linguistics, Volume 12, Number 2, April-June 1986Mallory Selfridge Integrated Processing Produces Robust Understandingtional history and domain knowledge, and if the userconfirms its interpretation at some point, it will succeed.Given this processing strategy, however, what preventsMURPHY from generating almost any conceivable CD inits attempt o correct errors, and thus generating wildlyunreasonable guesses?
In fact, wildly unreasonableguesses are not ruled out at all; MURPHY must be able togenerate such guesses in case they represent the intendedmeaning.
However, since MURPHY generates possibleunderstandings according to preference, it will onlygenerate wildly unreasonable guesses if the user hasfailed to verify all of the more reasonable guesses, inwhich case such a guess has actually become the mostreasonable remaining.6.6 ROBUST UNDERSTANDINGThis section has described MURPHY's performance oninputs with variant syntax, missing words, ellipsis, withand without corrections.
It has further considered itsresponse time, the number of incorrect guesses it makes,and the degree to which it eventually understands allcases of such inputs.
It appears that MURPHY displaysthe characteristics of robustness described in section 2.7 INTEGRATED PROCESSING PRODUCESROBUST UNDERSTANDINGThis paper has described research based on the conjec-ture that, since the integrated processing hypothesis is amodel of how people understand language, and sincepeople are known to understand robustly, then a naturallanguage interface incorporating the integrated process-ing hypothesis hould prove robust.
It appears that thisapproach has validity, since MURPHY is indeed robust,and this robustness derives from its embodiment of theintegrated processing hypothesis.
Specifically, MURPHYappears to be robust in the areas described in section 2.It successfully understands utterances that are missingwords and have variant syntax, both without and withcorrections.
This performance thus supports the IPPRUconjecture.However, MURPHY is not fully robust in the broadestsense.
There remain other characteristics of real-worldinput which have not been considered here, such as falsestarts, unknown words, irrelevant interjections, andlearning new words.
In fact, however, MURPHY canalready understand input with some of these character-istics, and extensions to those remaining appear possible.Selfridge (1980, 1981a, 1981b, 1982) describes theCHILD program which models child language learning.CHILD readily understands utterances that containunknown words, and it readily learns new word meaningand syntax.
Since MURPHY and CHILD use the sameunderstanding and inference programs, MURPHY canalso understand input with unknown words and can learnnew word meanings and syntax.
Understanding falsestarts and irrelevant interjections are really the sameproblem, and it appears that MURPHY can easily under-stand despite them.
To see this, recall the basic slot-fill-ing mechanism MURPHY uses, and consider how thesearch for candidate fillers is carded out: those CDs thatfail to satisfy the semantic requirements are not consid-ered further.
Since in all likelihood the meaning of falsestarts and interjections will fail to satisfy any require-ments, most of the time they will be ignored and under-standing will proceed as if they were not present.
Inthose rare cases in which the meaning of a false start orinterjection can incorrectly, but plausibly, be incorpo-rated into the understood meaning of the utterance, theuser will fail to verify it and MURPHY will generateanother, eventually correct, understanding.
MURPHY'srobustness thus extends considerably beyond thatreported in this paper, and it appears to represent apromising approach for future work.Such future work will concentrate in several specificareas.
First, of course, is the complete unification of NLAand RBE, including as much interaction as possible withCCON.
This represents the step from merely embodyingthe integrated processing hypothesis to actually beingintegrated.
In addition, the ability to handle multipleword senses is critical; Dawson (1984) has extended thepreference algorithms described here to handle multipleword senses, and it remains only to incorporate them inMURPHY.
Further, MURPHY's representation of syntaxis probably inadequate for utterances of significantlygreater complexity than those currently handled.
Thisrepresentation will either have to be extended, or a newrepresentation developed.
Its representation f semanticsis similarly weak.
The technique of characterizingconcepts by whether they satisfy semantic requirementsand preferences i  too simple, and should be extended toinclude complex reasoning about concepts.
Finally,MURPHY's search algorithms require improvement.While they will certainly work in realistically largedomains, they will probably prove unreasonably slow toinfer the intended meaning.
Indeed, while humanlanguage processing is integrated (Schank and Birnbaum1981), humans are not capable of generating a completesequence of possible meanings in order of likelihood(Kolodner 1980).
Rather, humans employ high-levelreasoning and inference within a rich and highly struc-tured memory, and usually infer the intended meaningquickly.
It would be desirable to integrate MURPHY witha system employing such a memory (Dyer 1982, Lebow-itz 1980) in order to improve its understanding and infer-ence mechanisms.Since MURPHY is robust in the desired ways, and itslimitations appear clear, it appears that further explora-tions of the role of the integrated processing hypothesisin robust natural anguage interfaces hould prove fruit-ful.AKNOWLEDGEMENTRoger Schank originated the paradigm within which theresearch presented here took place; thanks to LarryComputational Linguistics, Volume 12, Number 2, April-June 1986 105Mallory Seffridge Integrated Processing Produces Robust UnderstandingBirnbaum for extensive discussions of the issues of inte-grated processing and for critically reading an earlierdraft; thanks to Howard Blair for critically reading anearlier draft.REFERENCESBirnbaum, L. and Selfridge, M. 1981 Conceptual Analysis.
In:Schank, R. and Riesbeck, C.K., Eds., Inside Computer Understand-ing: Five Programs plus Miniatures.
Lawrence Erlbaum Associates,Hillsdale, New Jersey, USA: 318-353.Bobrow, R.J. and Webber, B.L.
1980 Knowledge Representation forSyntactic/Semantic Processing.
Proceedings of the First AnnualConference of the AAAL Stanford University, California: 316-323.CarboneU, J.G.
and Hayes, P.J.
1983 Recovery Strategies for ParsingExtragrammatieal Language.
American Journal of ComputationalLinguistics 9(3-4): 123-146Cullingford, R. 1978 Script Application: Computer Understanding OfNewspaper Stories.
Research Report No.
116, Department ofComputer Science, Yale University, New Haven, Connecticut.Cullingford, R.; Krueger, M.; Selfridge, M.; and Bienkowski, M. 1981Automated Explanations as a Component of a CAD System.
IEEETransactions on Systems, Man and Cybernetics.
SMC-12.Dawson, B.
1984 Preference Analysis with Arbitrary Numbers ofWord Senses.
MS Thesis, Department of Electrical Engineering andComputer Science, University of Connecticut, Storrs, Connecticut.Dyer, M. 1982 In-Depth Understanding: A Computer Model Of Inte-grated Processing For Narrative Comprehension.
Research ReportNo.
219, Department of Computer Science, Yale University, NewHaven, Connecticut.Engeiberg, J.
1983 Integrated Processing Produces Robust Under-standing.
MS Thesis, Department of Electrical Engineering andComputer Science, University of Connecticut, Storrs, Connecticut.Engelberg, J.; Levas, A.; and Selfridge, M. 1984 A Natural LanguageInterface to a Robot Assembly System.
Proceedings of the IEEEInternational Conference on Robotics.
Atlanta, Georgia: 400-403.Fass, D. and Wilks, Y.
1983 Preference Semantics, Ill-formedness, andMetaphor.
American Journal of Computational Linguistics.
9(3-4):178-187.Granger, R.H. 1984 The NOMAD System: Expectation-BasedDetection and Correction of Errors During Understanding ofSyntactically and Semantically Ill-formed Text.
American Journal ofComputational Linguistics 9(3-4): 188-198.Hayes, P.J.
and Carbonell, J.G.
1981 Multi-Strategy Parsing and itsRole in Robust Machine Communications.
CMU-CS-81-118,Carnegie':Melion U iversity, Pittsburgh, Pennsylvania.Hayes, P.J.
and Mouradian, G.V.
1981 Flexible Parsing.
AmericanJournal of Computational Linguistics 7(4): 232-242.Kolodner, J.
1980 Retrieval and Organizational Strategies in Concep-tual Memory: A Computer Model.
Research Report No.
187, YaleUniversity, Dept.
of Computer Science, New Haven, Connecticut.Kwasny, S.C. and Sondheimer, N.K.
1981 Relaxation Techniques forParsing Grammatically Ill-Formed Input in Natural LanguageUnderstanding Systems.
American Journal of Computational Linguis-tics, 7(2): 99-108.Lebowitz, M. 1980 Generalization and Memory in an IntegratedUnderstanding System.
Research Report No.
186, Department ofComputer Science, Yale University, New Haven, Connecticut.Levas, A.
1983 Teaching Robots Assembly Plans By Example.
MSThesis, Department of Electrical Engineering and ComputerScience, University of Connecticut, Storrs, Connecticut.Levas, A. and Selfridge, M. 1984 A User-Friendly High-level RobotTeaching System.
Proceedings of the IEEE International Conference onRobotics.
Atlanta, Georgia: 413-416.Nilsson, Nils.
1971 Problem-Solving Methods in Artificial Intelligence.McGraw-Hill, New York.Riesbeek, C. and Schank, R. 1976 Comprehension by Computer:Expectation-Based Analysis of Sentences in Context.
ResearchReport No.
78, Department of Computer Science, Yale University,New Haven, Connecticut.Schank, R. 1975 Conceptual Information Processing.
Elsevier-NorthHolland, New York.Schank, R. and Abelson, R. 1977 Scripts, Plans, Goals and Understand-ing: An Inquiry into Human Knowledge Structures.
Lawrence Erl-baum Assoc., Hillsdale, New Jersey.Schank, R. and Birnbaum, L. 1981 Memory, Meaning, and Syntax.Research Report No.
189, Department of Computer Science, YaleUniversity, New Haven, Connecticut.Selfridge, M. 1980 A Process Model of Language Acquisition.
Ph.DDissertation, Research Report No.
172, Department of ComputerScience, Yale University, New Haven, Connecticut.Selfridge, M. 1981a Why Do Children Say "Goed"?
A ComputerModel of Child Generation.
Proceedings of the Third Annual Meetingof the Cognitive Science Society.
Berkeley, California: 131-133.Selfridge, M. 1981b A Computer Model of Language Acquisition.Proceedings of the 7th International Joint Conference on ArtificialIntelligence, Vancouver, British Columbia: 92-96.Selfridge, M. 1982 Why Do Children Misunderstand ReversiblePassives?
The CHILD Program Learns to Understand PassiveSentences.
Proceedings of the Third Annual Conference of the AAALPittsburgh, Pennsylvania: 251-254.Selfridge, M. 1983 Natural Language Interfaces to Image AnalysisSystems.
Proceedings of Trends and Applications, 1983.
Silver Spring,Maryland: 248-251.Weischedel, R.M.
and Sondheimer, N.K.
1983 Meta-rules as a Basisfor Processing Ill-Formed Output.
American Journal of Computa-tional Linguistics.
9(3-4): 161-177Wiiks, Y.
1976 Parsing English 1I.
In Wilks, Y. and Charniak, E., Eds.,Computational Semantics.
North-Holland Publishing Co., New York,New York: 155-184.Woods, W.A.
1970 Transition Network Grammars for NaturalLanguage Analysis.
Communications of the ACM 13(10): 591-606.106 Computational Linguistics, Volume 12, Number 2, April-June 1986
