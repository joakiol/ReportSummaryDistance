Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162?173,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsRecall-Oriented Learning of Named Entities in Arabic WikipediaBehrang Mohit?
Nathan Schneider?
Rishav Bhowmick?
Kemal Oflazer?
Noah A. Smith?School of Computer Science, Carnegie Mellon University?P.O.
Box 24866, Doha, Qatar ?Pittsburgh, PA 15213, USA{behrang@,nschneid@cs.,rishavb@qatar.,ko@cs.,nasmith@cs.
}cmu.eduAbstractWe consider the problem of NER in ArabicWikipedia, a semisupervised domain adap-tation setting for which we have no labeledtraining data in the target domain.
To fa-cilitate evaluation, we obtain annotationsfor articles in four topical groups, allow-ing annotators to identify domain-specificentity types in addition to standard cate-gories.
Standard supervised learning onnewswire text leads to poor target-domainrecall.
We train a sequence model and showthat a simple modification to the onlinelearner?a loss function encouraging it to?arrogantly?
favor recall over precision?substantially improves recall and F1.
Wethen adapt our model with self-trainingon unlabeled target-domain data; enforc-ing the same recall-oriented bias in the self-training stage yields marginal gains.11 IntroductionThis paper considers named entity recognition(NER) in text that is different from most past re-search on NER.
Specifically, we consider ArabicWikipedia articles with diverse topics beyond thecommonly-used news domain.
These data chal-lenge past approaches in two ways:First, Arabic is a morphologically rich lan-guage (Habash, 2010).
Named entities are ref-erenced using complex syntactic constructions(cf.
English NEs, which are primarily sequencesof proper nouns).
The Arabic script suppressesmost vowels, increasing lexical ambiguity, andlacks capitalization, a key clue for English NER.Second, much research has focused on the useof news text for system building and evaluation.Wikipedia articles are not news, belonging insteadto a wide range of domains that are not clearly1The annotated dataset and a supplementary documentwith additional details of this work can be found at:http://www.ark.cs.cmu.edu/AQMARdelineated.
One hallmark of this divergence be-tween Wikipedia and the news domain is a dif-ference in the distributions of named entities.
In-deed, the classic named entity types (person, or-ganization, location) may not be the most apt forarticles in other domains (e.g., scientific or socialtopics).
On the other hand, Wikipedia is a largedataset, inviting semisupervised approaches.In this paper, we describe advances on the prob-lem of NER in Arabic Wikipedia.
The techniquesare general and make use of well-understoodbuilding blocks.
Our contributions are:?
A small corpus of articles annotated in a newscheme that provides more freedom for annota-tors to adapt NE analysis to new domains;?
An ?arrogant?
learning approach designed toboost recall in supervised training as well asself-training; and?
An empirical evaluation of this technique as ap-plied to a well-established discriminative NERmodel and feature set.Experiments show consistent gains on the chal-lenging problem of identifying named entities inArabic Wikipedia text.2 Arabic Wikipedia NE AnnotationMost of the effort in NER has been fo-cused around a small set of domains andgeneral-purpose entity classes relevant to thosedomains?especially the categories PER(SON),ORG(ANIZATION), and LOC(ATION) (POL),which are highly prominent in news text.
Ara-bic is no exception: the publicly available NERcorpora?ACE (Walker et al 2006), ANER (Be-najiba et al 2008), and OntoNotes (Hovy et al2006)?all are in the news domain.2 However,2OntoNotes contains news-related text.
ACE includessome text from blogs.
In addition to the POL classes, bothcorpora include additional NE classes such as facility, event,product, vehicle, etc.
These entities are infrequent and maynot be comprehensive enough to cover the larger set of pos-162History Science Sports Technologydev: Damascus Atom Rau?l Gonza?les LinuxImam Hussein Shrine Nuclear power Real Madrid Solaristest: Crusades Enrico Fermi 2004 Summer Olympics ComputerIslamic Golden Age Light Christiano Ronaldo Computer SoftwareIslamic History Periodic Table Football InternetIbn Tolun Mosque Physics Portugal football team Richard StallmanUmmaya Mosque Muhammad al-Razi FIFA World Cup X Window SystemClaudio Filippone (PER) 	??J.
?J?
?KX???
; Linux (SOFTWARE) ??JJ?
; SpanishLeague (CHAMPIONSHIPS) ?GAJ.
?B@ ?P?Y?
@; proton (PARTICLE) 	??K?QK.
; nuclearradiation (GENERIC-MISC) ???J?
@ ?A?
?B@; Real Zaragoza (ORG)???Q???
?AKPTable 1: Translated titlesof Arabic Wikipedia arti-cles in our developmentand test sets, and someNEs with standard andarticle-specific classes.Additionally, Prussia andAmman were reservedfor training annotators,and Gulf War for esti-mating inter-annotatoragreement.appropriate entity classes will vary widely by do-main; occurrence rates for entity classes are quitedifferent in news text vs. Wikipedia, for instance(Balasuriya et al 2009).
This is abundantlyclear in technical and scientific discourse, wheremuch of the terminology is domain-specific, but itholds elsewhere.
Non-POL entities in the historydomain, for instance, include important events(wars, famines) and cultural movements (roman-ticism).
Ignoring such domain-critical entitieslikely limits the usefulness of the NE analysis.Recognizing this limitation, some work onNER has sought to codify more robust invento-ries of general-purpose entity types (Sekine et al2002; Weischedel and Brunstein, 2005; Grouinet al 2011) or to enumerate domain-specifictypes (Settles, 2004; Yao et al 2003).
Coarse,general-purpose categories have also been usedfor semantic tagging of nouns and verbs (Cia-ramita and Johnson, 2003).
Yet as the numberof classes or domains grows, rigorously docu-menting and organizing the classes?even for asingle language?requires intensive effort.
Ide-ally, an NER system would refine the traditionalclasses (Hovy et al 2011) or identify new entityclasses when they arise in new domains, adaptingto new data.
For this reason, we believe it is valu-able to consider NER systems that identify (butdo not necessarily label) entity mentions, and alsoto consider annotation schemes that allow annota-tors more freedom in defining entity classes.Our aim in creating an annotated dataset is toprovide a testbed for evaluation of new NER mod-els.
We will use these data as development andsible NEs (Sekine et al 2002).
Nezda et al(2006) anno-tated and evaluated an Arabic NE corpus with an extendedset of 18 classes (including temporal and numeric entities);this corpus has not been released publicly.testing examples, but not as training data.
In ?4we will discuss our semisupervised approach tolearning, which leverages ACE and ANER dataas an annotated training corpus.2.1 Annotation StrategyWe conducted a small annotation project on Ara-bic Wikipedia articles.
Two college-educated na-tive Arabic speakers annotated about 3,000 sen-tences from 31 articles.
We identified four top-ical areas of interest?history, technology, sci-ence, and sports?and browsed these topics un-til we had found 31 articles that we deemed sat-isfactory on the basis of length (at least 1,000words), cross-lingual linkages (associated articlesin English, German, and Chinese3), and subjec-tive judgments of quality.
The list of these arti-cles along with sample NEs are presented in ta-ble 1.
These articles were then preprocessed toextract main article text (eliminating tables, lists,info-boxes, captions, etc.)
for annotation.Our approach follows ACE guidelines (LDC,2005) in identifying NE boundaries and choos-ing POL tags.
In addition to this traditional formof annotation, annotators were encouraged to ar-ticulate one to three salient, article-specific en-tity categories per article.
For example, namesof particles (e.g., proton) are highly salient in theAtom article.
Annotators were asked to read theentire article first, and then to decide which non-traditional classes of entities would be importantin the context of article.
In some cases, annotatorsreported using heuristics (such as being proper3These three languages have the most articles onWikipedia.
Associated articles here are those that have beenmanually hyperlinked from the Arabic page as cross-lingualcorrespondences.
They are not translations, but if the associ-ations are accurate, these articles should be topically similarto the Arabic page that links to them.163Token position agreement rate 92.6% Cohen?s ?
: 0.86Token agreement rate 88.3% Cohen?s ?
: 0.86Token F1 between annotators 91.0%Entity boundary match F1 94.0%Entity category match F1 87.4%Table 2: Inter-annotator agreement measurements.nouns or having an English translation which isconventionally capitalized) to help guide their de-termination of non-canonical entities and entityclasses.
Annotators produced written descriptionsof their classes, including example instances.This scheme was chosen for its flexibility: incontrast to a scenario with a fixed ontology, anno-tators required minimal training beyond the POLconventions, and did not have to worry aboutdelineating custom categories precisely enoughthat they would extend straightforwardly to othertopics or domains.
Of course, we expect inter-annotator variability to be greater for these open-ended classification criteria.2.2 Annotation Quality EvaluationDuring annotation, two articles (Prussia and Am-man) were reserved for training annotators onthe task.
Once they were accustomed to anno-tation, both independently annotated a third ar-ticle.
We used this 4,750-word article (Gulf War,?JK AJ?
@ i.
J?m?
'@ H. Qk) to measure inter-annotatoragreement.
Table 2 provides scores for token-level agreement measures and entity-level F1 be-tween the two annotated versions of the article.4These measures indicate strong agreement forlocating and categorizing NEs both at the tokenand chunk levels.
Closer examination of agree-ment scores shows that PER and MIS classes havethe lowest rates of agreement.
That the mis-cellaneous class, used for infrequent or article-specific NEs, receives poor agreement is unsur-prising.
The low agreement on the PER classseems to be due to the use of titles and descriptiveterms in personal names.
Despite explicit guide-lines to exclude the titles, annotators disagreed onthe inclusion of descriptors that disambiguate theNE (e.g., the father in H.B@ ??K.
h. Qk.
: GeorgeBush, the father).4The position and boundary measures ignore the distinc-tions between the POLM classes.
To avoid artificial inflationof the token and token position agreement rates, we excludethe 81% of tokens tagged by both annotators as not belong-ing to an entity.History: Gulf War, Prussia, Damascus, CrusadesWAR CONFLICT ?
?
?Science: Atom, Periodic tableTHEORY ?
CHEMICAL ?
?NAME ROMAN ?
PARTICLE ?
?Sports: Football, Rau?l Gonza?lesSPORT ?
CHAMPIONSHIP ?AWARD ?
NAME ROMAN ?Technology: Computer, Richard StallmanCOMPUTER VARIETY ?
SOFTWARE ?COMPONENT ?Table 3: Custom NE categories suggested by one orboth annotators for 10 articles.
Article titles are trans-lated from Arabic.
?
indicates that both annotators vol-unteered a category for an article; ?
indicates that onlyone annotator suggested the category.
Annotators werenot given a predetermined set of possible categories;rather, category matches between annotators were de-termined by post hoc analysis.
NAME ROMAN indi-cates an NE rendered in Roman characters.2.3 Validating Category IntuitionsTo investigate the variability between annotatorswith respect to custom category intuitions, weasked our two annotators to independently read10 of the articles in the data (scattered across ourfour focus domains) and suggest up to 3 customcategories for each.
We assigned short names tothese suggestions, seen in table 3.
In 13 cases,both annotators suggested a category for an articlethat was essentially the same (?
); three such cat-egories spanned multiple articles.
In three casesa category was suggested by only one annotator(?
).5 Thus, we see that our annotators were gen-erally, but not entirely, consistent with each otherin their creation of custom categories.
Further, al-most all of our article-specific categories corre-spond to classes in the extended NE taxonomy of(Sekine et al 2002), which speaks to the reason-ableness of both sets of categories?and by exten-sion, our open-ended annotation process.Our annotation of named entities outside of thetraditional POL classes creates a useful resourcefor entity detection and recognition in new do-mains.
Even the ability to detect non-canonicaltypes of NEs should help applications such as QAand MT (Toral et al 2005; Babych and Hart-ley, 2003).
Possible avenues for future workinclude annotating and projecting non-canonical5When it came to tagging NEs, one of the two annota-tors was assigned to each article.
Custom categories onlysuggested by the other annotator were ignored.164NEs from English articles to their Arabic coun-terparts (Hassan et al 2007), automatically clus-tering non-canonical types of entities into article-specific or cross-article classes (cf.
Frietag, 2004),or using non-canonical classes to improve the(author-specified) article categories in Wikipedia.Hereafter, we merge all article-specific cate-gories with the generic MIS category.
The pro-portion of entity mentions that are tagged as MIS,while varying to a large extent by document, isa major indication of the gulf between the newsdata (<10%) and the Wikipedia data (53% for thedevelopment set, 37% for the test set).Below, we aim to develop entity detection mod-els that generalize beyond the traditional POL en-tities.
We do not address here the challenges ofautomatically classifying entities or inferring non-canonical groupings.3 DataTable 4 summarizes the various corpora used inthis work.6 Our NE-annotated Wikipedia sub-corpus, described above, consists of several Ara-bic Wikipedia articles from four focus domains.7We do not use these for supervised training data;they serve only as development and test data.
Alarger set of Arabic Wikipedia articles, selectedon the basis of quality heuristics, serves as unla-beled data for semisupervised learning.Our out-of-domain labeled NE data is drawnfrom the ANER (Benajiba et al 2007) andACE-2005 (Walker et al 2006) newswire cor-pora.
Entity types in this data are POL cate-gories (PER, ORG, LOC) and MIS.
Portions of theACE corpus were held out as development andtest data; the remainder is used in training.4 ModelsOur starting point for statistical NER is a feature-based linear model over sequences, trained usingthe structured perceptron (Collins, 2002).8In addition to lexical and morphological9 fea-6Additional details appear in the supplement.7We downloaded a snapshot of Arabic Wikipedia(http://ar.wikipedia.org) on 8/29/2009 and pre-processed the articles to extract main body text and metadatausing the mwlib package for Python (PediaPress, 2010).8A more leisurely discussion of the structured percep-tron and its connection to empirical risk minimization canbe found in the supplementary document.9We obtain morphological analyses from the MADA tool(Habash and Rambow, 2005; Roth et al 2008).Training words NEsACE+ANER 212,839 15,796Wikipedia (unlabeled, 397 docs) 1,110,546 ?DevelopmentACE 7,776 638Wikipedia (4 domains, 8 docs) 21,203 2,073TestACE 7,789 621Wikipedia (4 domains, 20 docs) 52,650 3,781Table 4: Number of words (entity mentions) in data sets.tures known to work well for Arabic NER (Be-najiba et al 2008; Abdul-Hamid and Darwish,2010), we incorporate some additional featuresenabled by Wikipedia.
We do not employ agazetteer, as the construction of a broad-domaingazetteer is a significant undertaking orthogo-nal to the challenges of a new text domain likeWikipedia.10 A descriptive list of our features isavailable in the supplementary document.We use a first-order structured perceptron; noneof our features consider more than a pair of con-secutive BIO labels at a time.
The model enforcesthe constraint that NE sequences must begin withB (so the bigram ?O, I?
is disallowed).Training this model on ACE and ANER dataachieves performance comparable to the state ofthe art (F1-measure11 above 69%), but fares muchworse on our Wikipedia test set (F1-measurearound 47%); details are given in ?5.4.1 Recall-Oriented PerceptronBy augmenting the perceptron?s online updatewith a cost function term, we can incorporate atask-dependent notion of error into the objective,as with structured SVMs (Taskar et al 2004;Tsochantaridis et al 2005).
Let c(y,y?)
denotea measure of error when y is the correct label se-quence but y?
is predicted.
For observed sequencex and feature weights (model parameters) w, thestructured hinge loss is `hinge(x,y,w) =maxy?(w>g(x,y?)
+ c(y,y?
))?w>g(x,y)(1)The maximization problem inside the parenthesesis known as cost-augmented decoding.
If c fac-10A gazetteer ought to yield further improvements in linewith previous findings in NER (Ratinov and Roth, 2009).11Though optimizing NER systems for F1 has been calledinto question (Manning, 2006), no alternative metric hasachieved widespread acceptance in the community.165tors similarly to the feature function g(x,y), thenwe can increase penalties for y that have morelocal mistakes.
This raises the learner?s aware-ness about how it will be evaluated.
Incorporat-ing cost-augmented decoding into the perceptronleads to this decoding step:y?
?
arg maxy?(w>g(x,y?)
+ c(y,y?
)), (2)which amounts to performing stochastic subgradi-ent ascent on an objective function with the Eq.
1loss (Ratliff et al 2006).In this framework, cost functions can be for-mulated to distinguish between different types oferrors made during training.
For a tag sequencey = ?y1, y2, .
.
.
, yM ?, Gimpel and Smith (2010b)define word-local cost functions that differentlypenalize precision errors (i.e., yi = O ?
y?i 6= Ofor the ith word), recall errors (yi 6= O?
y?i = O),and entity class/position errors (other cases whereyi 6= y?i).
As will be shown below, a key problemin cross-domain NER is poor recall, so we willpenalize recall errors more severely:c(y,y?)
=M?i=1??
?0 if yi = y?i?
if yi 6= O ?
y?i = O1 otherwise(3)for a penalty parameter ?
> 1.
We call our learnerthe ?recall-oriented?
perceptron (ROP).We note that Minkov et al(2006) similarly ex-plored the recall vs. precision tradeoff in NER.Their technique was to directly tune the weightof a single feature?the feature marking O (non-entity tokens); a lower weight for this feature willincur a greater penalty for predicting O. Belowwe demonstrate that our method, which is lesscoarse, is more successful in our setting.12In our experiments we will show that injecting?arrogance?
into the learner via the recall-orientedloss function substantially improves recall, espe-cially for non-POL entities (?5.3).4.2 Self-Training and SemisupervisedLearningAs we will show experimentally, the differencesbetween news text and Wikipedia text call for do-main adaptation.
In the case of Arabic Wikipedia,12The distinction between the techniques is that our costfunction adjusts the whole model in order to perform betterat recall on the training data.Input: labeled data ??x(n),y(n)?
?Nn=1; unlabeleddata ?x?
(j)?Jj=1; supervised learner L;number of iterations T ?Output: ww?
L(??x(n),y(n)?
?Nn=1)for t = 1 to T ?
dofor j = 1 to J doy?
(j) ?
arg maxy w>g(x?(j),y)w?
L(??x(n),y(n)?
?Nn=1 ?
??x?
(j), y?(j)?
?Jj=1)Algorithm 1: Self-training.there is no available labeled training data.
Yetthe available unlabeled data is vast, so we turn tosemisupervised learning.Here we adapt self-training, a simple tech-nique that leverages a supervised learner (like theperceptron) to perform semisupervised learning(Clark et al 2003; Mihalcea, 2004; McCloskyet al 2006).
In our version, a model is trainedon the labeled data, then used to label the un-labeled target data.
We iterate between trainingon the hypothetically-labeled target data plus theoriginal labeled set, and relabeling the target data;see Algorithm 1.
Before self-training, we removesentences hypothesized not to contain any namedentity mentions, which we found avoids furtherencouragement of the model toward low recall.5 ExperimentsWe investigate two questions in the context ofNER for Arabic Wikipedia:?
Loss function: Does integrating a cost func-tion into our learning algorithm, as we havedone in the recall-oriented perceptron (?4.1),improve recall and overall performance onWikipedia data??
Semisupervised learning for domain adap-tation: Can our models benefit from largeamounts of unlabeled Wikipedia data, in addi-tion to the (out-of-domain) labeled data?
Weexperiment with a self-training phase followingthe fully supervised learning phase.We report experiments for the possible combi-nations of the above ideas.
These are summarizedin table 5.
Note that the recall-oriented percep-tron can be used for the supervised learning phase,for the self-training phase, or both.
This leaves uswith the following combinations:?
reg/none (baseline): regular supervised learner.?
ROP/none: recall-oriented supervised learner.166Figure 1: Tuning the recall-oriented cost parame-ter for different learning settings.
We optimizedfor development set F1, choosing penalty ?
= 200for recall-oriented supervised learning (in the plot,ROP/*?this is regardless of whether a stage ofself-training will follow); ?
= 100 for recall-oriented self-training following recall-oriented su-pervised learning (ROP/ROP); and ?
= 3200 forrecall-oriented self-training following regular super-vised learning (reg/ROP).?
reg/reg: standard self-training setup.?
ROP/reg: recall-oriented supervised learner, fol-lowed by standard self-training.?
reg/ROP: regular supervised model as the initial la-beler for recall-oriented self-training.?
ROP/ROP (the ?double ROP?
condition): recall-oriented supervised model as the initial labeler forrecall-oriented self-training.
Note that the twoROPs can use different cost parameters.For evaluating our models we consider thenamed entity detection task, i.e., recognizingwhich spans of words constitute entities.
Thisis measured by per-entity precision, recall, andF1.13 To measure statistical significance of differ-ences between models we use Gimpel and Smith?s(2010) implementation of the paired bootstrap re-sampler of (Koehn, 2004), taking 10,000 samplesfor each comparison.5.1 BaselineOur baseline is the perceptron, trained on thePOL entity boundaries in the ACE+ANER cor-pus (reg/none).14 Development data was used toselect the number of iterations (10).
We per-formed 3-fold cross-validation on the ACE dataand found wide variance in the in-domain entitydetection performance of this model:P R F1fold 1 70.43 63.08 66.55fold 2 87.48 81.13 84.18fold 3 65.09 51.13 57.27average 74.33 65.11 69.33(Fold 1 corresponds to the ACE test set describedin table 4.)
We also trained the model to performPOL detection and classification, achieving nearlyidentical results in the 3-way cross-validation ofACE data.
From these data we conclude that our13Only entity spans that exactly match the gold spans arecounted as correct.
We calculated these scores with theconlleval.pl script from the CoNLL 2003 shared task.14In keeping with prior work, we ignore non-POL cate-gories for the ACE evaluation.baseline is on par with the state of the art for Ara-bic NER on ACE news text (Abdul-Hamid andDarwish, 2010).15Here is the performance of the baseline entitydetection model on our 20-article test set:16P R F1technology 60.42 20.26 30.35science 64.96 25.73 36.86history 63.09 35.58 45.50sports 71.66 59.94 65.28overall 66.30 35.91 46.59Unsurprisingly, performance on Wikipedia datavaries widely across article domains and is muchlower than in-domain performance.
Precisionscores fall between 60% and 72% for all domains,but recall in most cases is far worse.
Miscella-neous class recall, in particular, suffers badly (un-der 10%)?which partially accounts for the poorrecall in science and technology articles (theyhave by far the highest proportion of MIS entities).5.2 Self-TrainingFollowing Clark et al(2003), we applied self-training as described in Algorithm 1, with theperceptron as the supervised learner.
Our unla-beled data consists of 397 Arabic Wikipedia ar-ticles (1 million words) selected at random fromall articles exceeding a simple length threshold(1,000 words); see table 4.
We used only one iter-ation (T ?
= 1), as experiments on developmentdata showed no benefit from additional rounds.Several rounds of self-training hurt performance,15Abdul-Hamid and Darwish report as their best result amacroaveraged F1-score of 76.
As they do not specify whichdata they used for their held-out test set, we cannot performa direct comparison.
However, our feature set is nearly asuperset of their best feature set, and their result lies wellwithin the range of results seen in our cross-validation folds.16Our Wikipedia evaluations use models trained onPOLM entity boundaries in ACE.
Per-domain and overallscores are microaverages across articles.167SELF-TRAININGSUPERVISED none reg ROPreg 66.3 35.9 46.59 66.7 35.6 46.41 59.2 40.3 47.97ROP 60.9 44.7 51.59 59.8 46.2 52.11 58.0 47.4 52.16Table 5: Entity detection precision, recall, and F1 for each learning setting, microaveraged across the 24 articlesin our Wikipedia test set.
Rows differ in the supervised learning condition on the ACE+ANER data (regularvs.
recall-oriented perceptron).
Columns indicate whether this supervised learning phase was followed by self-training on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training.baselineentities words recallPER 1081 1743 49.95ORG 286 637 23.92LOC 1019 1413 61.43MIS 1395 2176 9.30overall 3781 5969 35.91Figure 2: Recall improve-ment over baseline in the testset by gold NER category,counts for those categories inthe data, and recall scores forour baseline model.
Markersin the plot indicate differentexperimental settings corre-sponding to cells in table 5.an effect attested in earlier research (Curran et al2007) and sometimes known as ?semantic drift.
?Results are shown in table 5.
We find that stan-dard self-training (the middle column) has verylittle impact on performance.17 Why is this thecase?
We venture that poor baseline recall and thedomain variability within Wikipedia are to blame.5.3 Recall-Oriented LearningThe recall-oriented bias can be introduced in ei-ther or both of the stages of our semisupervisedlearning framework: in the supervised learn-ing phase, modifying the objective of our base-line (?5.1); and within the self-training algorithm(?5.2).18 As noted in ?4.1, the aim of this ap-proach is to discourage recall errors (false nega-tives), which are the chief difficulty for the newstext?trained model in the new domain.
We se-lected the value of the false positive penalty forcost-augmented decoding, ?, using the develop-ment data (figure 1).The results in table 5 demonstrate improve-ments due to the recall-oriented bias in bothstages of learning.19 When used in the super-17In neither case does regular self-training produce a sig-nificantly different F1 score than no self-training.18Standard Viterbi decoding was used to label the datawithin the self-training algorithm; note that cost-augmenteddecoding only makes sense in learning, not as a predictiontechnique, since it deliberately introduces errors relative to acorrect output that must be provided.19In terms of F1, the worst of the 3 models with the ROPsupervised learner significantly outperforms the best modelwith the regular supervised learner (p < 0.005).
The im-vised phase (bottom left cell), the recall gainsare substantial?nearly 9% over the baseline.
In-tegrating this bias within self-training (last col-umn of the table) produces a more modest im-provement (less than 3%) relative to the base-line.
In both cases, the improvements to recallmore than compensate for the amount of degra-dation to precision.
This trend is robust: wher-ever the recall-oriented perceptron is added, weobserve improvements in both recall and F1.
Per-haps surprisingly, these gains are somewhat addi-tive: using the ROP in both learning phases givesa small (though not always significant) gain overalternatives (standard supervised perceptron, noself-training, or self-training with a standard per-ceptron).
In fact, when the standard supervisedlearner is used, recall-oriented self-training suc-ceeds despite the ineffectiveness of standard self-training.Performance breakdowns by (gold) class, fig-ure 2, and domain, figure 3, further attest to therobustness of the overall results.
The most dra-matic gains are in miscellaneous class recall?each form of the recall bias produces an improve-ment, and using this bias in both the supervisedand self-training phases is clearly most success-ful for miscellaneous entities.
Correspondingly,the technology and science domains (in which thisclass dominates?83% and 61% of mentions, ver-provements due to self-training are marginal, however: ROPself-training produces a significant gain only following reg-ular supervised learning (p < 0.05).168Figure 3: Supervisedlearner precision vs.recall as evaluatedon Wikipedia testdata in differenttopical domains.
Theregular perceptron(baseline model) iscontrasted with ROP.No self-training isapplied.sus 6% and 12% for history and sports, respec-tively) receive the biggest boost.
Still, the gapsbetween domains are not entirely removed.Most improvements relate to the reduction offalse negatives, which fall into three groups:(a) entities occurring infrequently or partiallyin the labeled training data (e.g.
uranium);(b) domain-specific entities sharing lexical or con-textual features with the POL entities (e.g.
Linux,titanium); and (c) words with Latin characters,common in the science and technology domains.
(a) and (b) are mostly transliterations into Arabic.An alternative?and simpler?approach tocontrolling the precision-recall tradeoff is theMinkov et al(2006) strategy of tuning a singlefeature weight subsequent to learning (see ?4.1above).
We performed an oracle experiment todetermine how this compares to recall-orientedlearning in our setting.
An oracle trained withthe method of Minkov et aloutperforms the threemodels in table 5 that use the regular perceptronfor the supervised phase of learning, but under-performs the supervised ROP conditions.20Overall, we find that incorporating the recall-oriented bias in learning is fruitful for adapting toWikipedia because the gains in recall outpace thedamage to precision.6 DiscussionTo our knowledge, this work is the first sugges-tion that substantively modifying the supervisedlearning criterion in a resource-rich domain canreap benefits in subsequent semisupervised appli-cation in a new domain.
Past work has looked20Tuning the O feature weight to optimize for F1 on ourtest set, we found that oracle precision would be 66.2, recallwould be 39.0, and F1 would be 49.1.
The F1 score of ourbest model is nearly 3 points higher than the Minkov et alstyle oracle, and over 4 points higher than the non-oracleversion where the development set is used for tuning.at regularization (Chelba and Acero, 2006) andfeature design (Daume?
III, 2007); we alter theloss function.
Not surprisingly, the double-ROPapproach harms performance on the original do-main (on ACE data, we achieve 55.41% F1, farbelow the standard perceptron).
Yet we observethat models can be prepared for adaptation evenbefore a learner is exposed a new domain, sacri-ficing performance in the original domain.The recall-oriented bias is not merely encour-aging the learner to identify entities already seenin training.
As recall increases, so does the num-ber of new entity types recovered by the model:of the 2,070 NE types in the test data that werenever seen in training, only 450 were ever foundby the baseline, versus 588 in the reg/ROP condi-tion, 632 in the ROP/none condition, and 717 inthe double-ROP condition.We note finally that our method is a simpleextension to the standard structured perceptron;cost-augmented inference is often no more ex-pensive than traditional inference, and the algo-rithmic change is equivalent to adding one addi-tional feature.
Our recall-oriented cost functionis parameterized by a single value, ?
; recall ishighly sensitive to the choice of this value (fig-ure 1 shows how we tuned it on developmentdata), and thus we anticipate that, in general, suchtuning will be essential to leveraging the benefitsof arrogance.7 Related WorkOur approach draws on insights from work inthe areas of NER, domain adaptation, NLP withWikipedia, and semisupervised learning.
As allare broad areas of research, we highlight only themost relevant contributions here.Research in Arabic NER has been focused oncompiling and optimizing the gazetteers and fea-169ture sets for standard sequential modeling algo-rithms (Benajiba et al 2008; Farber et al 2008;Shaalan and Raza, 2008; Abdul-Hamid and Dar-wish, 2010).
We make use of features identi-fied in this prior work to construct a strong base-line system.
We are unaware of any Arabic NERwork that has addressed diverse text domains likeWikipedia.
Both the English and Arabic ver-sions of Wikipedia have been used, however, asresources in service of traditional NER (Kazamaand Torisawa, 2007; Benajiba et al 2008).
Attiaet al(2010) heuristically induce a mapping be-tween Arabic Wikipedia and Arabic WordNet toconstruct Arabic NE gazetteers.Balasuriya et al(2009) highlight the substan-tial divergence between entities appearing in En-glish Wikipedia versus traditional corpora, andthe effects of this divergence on NER perfor-mance.
There is evidence that models trainedon Wikipedia data generalize and perform wellon corpora with narrower domains.
Nothmanet al(2009) and Balasuriya et al(2009) showthat NER models trained on both automaticallyand manually annotated Wikipedia corpora per-form reasonably well on news corpora.
The re-verse scenario does not hold for models trainedon news text, a result we also observe in ArabicNER.
Other work has gone beyond the entity de-tection problem: Florian et al(2004) addition-ally predict within-document entity coreferencefor Arabic, Chinese, and English ACE text, whileCucerzan (2007) aims to resolve every mentiondetected in English Wikipedia pages to a canoni-cal article devoted to the entity in question.The domain and topic diversity of NEs has beenstudied in the framework of domain adaptationresearch.
A group of these methods use self-training and select the most informative featuresand training instances to adapt a source domainlearner to the new target domain.
Wu et al(2009)bootstrap the NER leaner with a subset of unla-beled instances that bridge the source and targetdomains.
Jiang and Zhai (2006) and Daume?
III(2007) make use of some labeled target-domaindata to tune or augment the features of the sourcemodel towards the target domain.
Here, in con-trast, we use labeled target-domain data only fortuning and evaluation.
Another important dis-tinction is that domain variation in this priorwork is restricted to topically-related corpora (e.g.newswire vs. broadcast news), whereas in ourwork, major topical differences distinguish thetraining and test corpora?and consequently, theirsalient NE classes.
In these respects our NERsetting is closer to that of Florian et al(2010),who recognize English entities in noisy text, (Sur-deanu et al 2011), which concerns informationextraction in a topically distinct target domain,and (Dalton et al 2011), which addresses EnglishNER in noisy and topically divergent text.Self-training (Clark et al 2003; Mihalcea,2004; McClosky et al 2006) is widely usedin NLP and has inspired related techniques thatlearn from automatically labeled data (Liang etal., 2008; Petrov et al 2010).
Our self-trainingprocedure differs from some others in that we useall of the automatically labeled examples, ratherthan filtering them based on a confidence score.Cost functions have been used in non-structured classification settings to penalize cer-tain types of errors more than others (Chan andStolfo, 1998; Domingos, 1999; Kiddon and Brun,2011).
The goal of optimizing our structured NERmodel for recall is quite similar to the scenario ex-plored by Minkov et al(2006), as noted above.8 ConclusionWe explored the problem of learning an NERmodel suited to domains for which no labeledtraining data are available.
A loss function to en-courage recall over precision during superviseddiscriminative learning substantially improves re-call and overall entity detection performance, es-pecially when combined with a semisupervisedlearning regimen incorporating the same bias.We have also developed a small corpus of Ara-bic Wikipedia articles via a flexible entity an-notation scheme spanning four topical domains(publicly available at http://www.ark.cs.cmu.edu/AQMAR).AcknowledgmentsWe thank Mariem Fekih Zguir and Reham Al Tamimefor assistance with annotation, Michael Heilman forhis tagger implementation, and Nizar Habash and col-leagues for the MADA toolkit.
We thank members ofthe ARK group at CMU, Hal Daume?, and anonymousreviewers for their valuable suggestions.
This publica-tion was made possible by grant NPRP-08-485-1-083from the Qatar National Research Fund (a member ofthe Qatar Foundation).
The statements made hereinare solely the responsibility of the authors.170ReferencesAhmed Abdul-Hamid and Kareem Darwish.
2010.Simplified feature set for Arabic named entityrecognition.
In Proceedings of the 2010 Named En-tities Workshop, pages 110?115, Uppsala, Sweden,July.
Association for Computational Linguistics.Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-ica Monachini, and Josef van Genabith.
2010.An automatically built named entity lexicon forArabic.
In Nicoletta Calzolari, Khalid Choukri,Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-lios Piperidis, Mike Rosner, and Daniel Tapias, ed-itors, Proceedings of the Seventh Conference onInternational Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European Lan-guage Resources Association (ELRA).Bogdan Babych and Anthony Hartley.
2003.
Im-proving machine translation quality with automaticnamed entity recognition.
In Proceedings of the 7thInternational EAMT Workshop on MT and OtherLanguage Technology Tools, EAMT ?03.Dominic Balasuriya, Nicky Ringland, Joel Nothman,Tara Murphy, and James R. Curran.
2009.
Namedentity recognition in Wikipedia.
In Proceedingsof the 2009 Workshop on The People?s Web MeetsNLP: Collaboratively Constructed Semantic Re-sources, pages 10?18, Suntec, Singapore, August.Association for Computational Linguistics.Yassine Benajiba, Paolo Rosso, and Jose?
MiguelBened??Ruiz.
2007.
ANERsys: an Arabic namedentity recognition system based on maximum en-tropy.
In Alexander Gelbukh, editor, Proceedingsof CICLing, pages 143?153, Mexico City, Mexio.Springer.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2008.Arabic named entity recognition using optimizedfeature sets.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 284?293, Honolulu, Hawaii, Oc-tober.
Association for Computational Linguistics.Philip K. Chan and Salvatore J. Stolfo.
1998.
To-ward scalable learning with non-uniform class andcost distributions: a case study in credit card frauddetection.
In Proceedings of the Fourth Interna-tional Conference on Knowledge Discovery andData Mining, pages 164?168, New York City, NewYork, USA, August.
AAAI Press.Ciprian Chelba and Alex Acero.
2006.
Adaptation ofmaximum entropy capitalizer: Little data can helpa lot.
Computer Speech and Language, 20(4):382?399.Massimiliano Ciaramita and Mark Johnson.
2003.
Su-persense tagging of unknown nouns in WordNet.
InProceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing, pages168?175.Stephen Clark, James Curran, and Miles Osborne.2003.
Bootstrapping POS-taggers using unlabelleddata.
In Walter Daelemans and Miles Osborne,editors, Proceedings of the Seventh Conference onNatural Language Learning at HLT-NAACL 2003,pages 49?55.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 1?8, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Silviu Cucerzan.
2007.
Large-scale named entitydisambiguation based on Wikipedia data.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 708?716, Prague, Czech Republic,June.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with MutualExclusion Bootstrapping.
In Proceedings of PA-CLING, 2007.Jeffrey Dalton, James Allan, and David A. Smith.2011.
Passage retrieval for incorporating globalevidence in sequence labeling.
In Proceedings ofthe 20th ACM International Conference on Infor-mation and Knowledge Management (CIKM ?11),pages 355?364, Glasgow, Scotland, UK, October.ACM.Hal Daume?
III.
2007.
Frustratingly easy domainadaptation.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 256?263, Prague, Czech Republic,June.
Association for Computational Linguistics.Pedro Domingos.
1999.
MetaCost: a general methodfor making classifiers cost-sensitive.
Proceedingsof the Fifth ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining,pages 155?164.Benjamin Farber, Dayne Freitag, Nizar Habash, andOwen Rambow.
2008.
Improving NER in Arabicusing a morphological tagger.
In Nicoletta Calzo-lari, Khalid Choukri, Bente Maegaard, Joseph Mar-iani, Jan Odjik, Stelios Piperidis, and Daniel Tapias,editors, Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC?08), pages2509?2514, Marrakech, Morocco, May.
EuropeanLanguage Resources Association (ELRA).Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, and Salim Roukos.
2004.
Astatistical model for multilingual entity detectionand tracking.
In Susan Dumais, Daniel Marcu,and Salim Roukos, editors, Proceedings of the Hu-man Language Technology Conference of the North171American Chapter of the Association for Compu-tational Linguistics: HLT-NAACL 2004, page 18,Boston, Massachusetts, USA, May.
Association forComputational Linguistics.Radu Florian, John Pitrelli, Salim Roukos, and ImedZitouni.
2010.
Improving mention detection ro-bustness to noisy input.
In Proceedings of EMNLP2010, pages 335?345, Cambridge, MA, October.Association for Computational Linguistics.Dayne Freitag.
2004.
Trained named entity recog-nition using distributional clusters.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP2004, pages 262?269, Barcelona, Spain, July.
As-sociation for Computational Linguistics.Kevin Gimpel and Noah A. Smith.
2010a.
Softmax-margin CRFs: Training log-linear models with lossfunctions.
In Proceedings of the Human LanguageTechnologies Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 733?736, Los Angeles, California,USA, June.Kevin Gimpel and Noah A. Smith.
2010b.Softmax-margin training for structured log-linear models.
Technical Report CMU-LTI-10-008, Carnegie Mellon University.
http://www.lti.cs.cmu.edu/research/reports/2010/cmulti10008.pdf.Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,Karn Fort, Olivier Galibert, and Ludovic Quin-tard.
2011.
Proposal for an extension of tradi-tional named entities: from guidelines to evaluation,an overview.
In Proceedings of the 5th Linguis-tic Annotation Workshop, pages 92?100, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Nizar Habash and Owen Rambow.
2005.
Arabic to-kenization, part-of-speech tagging and morpholog-ical disambiguation in one fell swoop.
In Proceed-ings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages573?580, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan and Claypool Pub-lishers.Ahmed Hassan, Haytham Fahmy, and Hany Hassan.2007.
Improving named entity translation by ex-ploiting comparable and parallel corpora.
In Pro-ceedings of the Conference on Recent Advancesin Natural Language Processing (RANLP ?07),Borovets, Bulgaria.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: the 90% solution.
In Proceedings ofthe Human Language Technology Conference ofthe NAACL (HLT-NAACL), pages 57?60, New YorkCity, USA, June.
Association for ComputationalLinguistics.Dirk Hovy, Chunliang Zhang, Eduard Hovy, andAnselmo Peas.
2011.
Unsupervised discovery ofdomain-specific knowledge from text.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 1466?1475, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Jing Jiang and ChengXiang Zhai.
2006.
Exploit-ing domain structure for named entity recognition.In Proceedings of the Human Language Technol-ogy Conference of the NAACL (HLT-NAACL), pages74?81, New York City, USA, June.
Association forComputational Linguistics.Jun?ichi Kazama and Kentaro Torisawa.
2007.Exploiting Wikipedia as external knowledge fornamed entity recognition.
In Proceedings ofthe 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 698?707, Prague, Czech Republic,June.
Association for Computational Linguistics.Chloe Kiddon and Yuriy Brun.
2011.
That?s whatshe said: double entendre identification.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 89?94, Portland, Ore-gon, USA, June.
Association for ComputationalLinguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.LDC.
2005.
ACE (Automatic Content Extraction)Arabic annotation guidelines for entities, version5.3.3.
Linguistic Data Consortium, Philadelphia.Percy Liang, Hal Daume?
III, and Dan Klein.
2008.Structure compilation: trading structure for fea-tures.
In Proceedings of the 25th International Con-ference on Machine Learning (ICML), pages 592?599, Helsinki, Finland.Chris Manning.
2006.
Doing named entity recogni-tion?
Don?t optimize for F1.
http://nlpers.blogspot.com/2006/08/doing-named-entity-recognition-dont.html.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the Human Language TechnologyConference of the NAACL, Main Conference, pages152?159, New York City, USA, June.
Associationfor Computational Linguistics.Rada Mihalcea.
2004.
Co-training and self-trainingfor word sense disambiguation.
In HLT-NAACL2004 Workshop: Eighth Conference on Computa-tional Natural Language Learning (CoNLL-2004),Boston, Massachusetts, USA.172Einat Minkov, Richard Wang, Anthony Tomasic, andWilliam Cohen.
2006.
NER systems that suit user?spreferences: adjusting the recall-precision trade-offfor entity extraction.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Companion Volume: Short Papers, pages 93?96,New York City, USA, June.
Association for Com-putational Linguistics.Luke Nezda, Andrew Hickl, John Lehmann, and Sar-mad Fayyaz.
2006.
What in the world is a Shahab?Wide coverage named entity recognition for Arabic.In Proccedings of LREC, pages 41?46.Joel Nothman, Tara Murphy, and James R. Curran.2009.
Analysing Wikipedia and gold-standard cor-pora for NER training.
In Proceedings of the 12thConference of the European Chapter of the Associ-ation for Computational Linguistics (EACL 2009),pages 612?620, Athens, Greece, March.
Associa-tion for Computational Linguistics.PediaPress.
2010. mwlib.
http://code.pediapress.com/wiki/wiki/mwlib.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for accurate de-terministic question parsing.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 705?713, Cambridge,MA, October.
Association for Computational Lin-guistics.Lev Ratinov and Dan Roth.
2009.
Design chal-lenges and misconceptions in named entity recog-nition.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL-2009), pages 147?155, Boulder, Colorado,June.
Association for Computational Linguistics.Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.Zinkevich.
2006.
Subgradient methods for maxi-mum margin structured learning.
In ICML Work-shop on Learning in Structured Output Spaces,Pittsburgh, Pennsylvania, USA.Ryan Roth, Owen Rambow, Nizar Habash, MonaDiab, and Cynthia Rudin.
2008.
Arabic morpho-logical tagging, diacritization, and lemmatizationusing lexeme models and feature ranking.
In Pro-ceedings of ACL-08: HLT, pages 117?120, Colum-bus, Ohio, June.
Association for ComputationalLinguistics.Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.2002.
Extended named entity hierarchy.
In Pro-ceedings of LREC.Burr Settles.
2004.
Biomedical named entity recogni-tion using conditional random fields and rich featuresets.
In Nigel Collier, Patrick Ruch, and AdelineNazarenko, editors, COLING 2004 InternationalJoint workshop on Natural Language Processing inBiomedicine and its Applications (NLPBA/BioNLP)2004, pages 107?110, Geneva, Switzerland, Au-gust.
COLING.Khaled Shaalan and Hafsa Raza.
2008.
Arabicnamed entity recognition from diverse text types.
InAdvances in Natural Language Processing, pages440?451.
Springer.Mihai Surdeanu, David McClosky, Mason R. Smith,Andrey Gusev, and Christopher D. Manning.
2011.Customizing an information extraction system toa new domain.
In Proceedings of the ACL 2011Workshop on Relational Models of Semantics, Port-land, Oregon, USA, June.
Association for Compu-tational Linguistics.Ben Taskar, Carlos Guestrin, and Daphne Koller.2004.
Max-margin Markov networks.
In SebastianThrun, Lawrence Saul, and Bernhard Scho?lkopf,editors, Advances in Neural Information ProcessingSystems 16.
MIT Press.Antonio Toral, Elisa Noguera, Fernando Llopis, andRafael Mun?oz.
2005.
Improving question an-swering using named entity recognition.
Natu-ral Language Processing and Information Systems,3513/2005:181?191.Ioannis Tsochantaridis, Thorsten Joachims, ThomasHofmann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent outputvariables.
Journal of Machine Learning Research,6:1453?1484, September.Christopher Walker, Stephanie Strassel, Julie Medero,and Kazuaki Maeda.
2006.
ACE 2005 multi-lingual training corpus.
LDC2006T06, LinguisticData Consortium, Philadelphia.Ralph Weischedel and Ada Brunstein.
2005.BBN pronoun coreference and entity type cor-pus.
LDC2005T33, Linguistic Data Consortium,Philadelphia.Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.2009.
Domain adaptive bootstrapping for namedentity recognition.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1523?1532, Singapore, August.Association for Computational Linguistics.Tianfang Yao, Wei Ding, and Gregor Erbach.
2003.CHINERS: a Chinese named entity recognition sys-tem for the sports domain.
In Proceedings of theSecond SIGHAN Workshop on Chinese LanguageProcessing, pages 55?62, Sapporo, Japan, July.
As-sociation for Computational Linguistics.173
