What?s in a name?The automatic recognition of metonymical location names.Yves PeirsmanQuantitative Lexicology and Variational LinguisticsUniversity of Leuven, Belgiumyves.peirsman@arts.kuleuven.beAbstractThe correct identification of metonymiesis not normally a problem for most peo-ple.
For computers, things are different,however.
In Natural Language Processing,metonymy recognition is therefore usu-ally addressed with complex algorithmsthat rely on hundreds of labelled train-ing examples.
This paper investigates twoapproaches to metonymy recognition thatdispense with this complexity, albeit indifferent ways.
The first, an unsuper-vised approach to Word Sense Discrimi-nation, does not require any labelled train-ing instances.
The second, Memory-BasedLearning, replaces the complexity of cur-rent algorithms by a ?lazy?
learning phase.While the first approach is often able toidentify a metonymical and a literal clus-ter in the data, it is the second in particularthat produces state-of-the-art results.1 IntroductionIn the last few years, metonymy has emerged asan important focus of research in many areas oflinguistics.
In Cognitive Linguistics, it is often de-fined as ?a cognitive process in which one con-ceptual entity, the vehicle, provides mental accessto another conceptual entity, the target, within thesame domain, or idealized cognitive model (ICM)?
(Ko?vecses, 2002, p.145).
In example (1), for in-stance, China and Taiwan provide mental accessto the governments of the respective countries:(1) China has always threatened to use forceif Taiwan declared independence.
(BNC)This paper is concerned with algorithms that au-tomatically recognize such metonymical countrynames.
These are extremely relevant in NaturalLanguage Processing, since any system that auto-matically builds semantic representations of utter-ances needs to be able to recognize and interpretmetonymical words.Early approaches to metonymy recognition,such as Pustejovsky?s (1995), identified a word asmetonymical when it violated certain selectionalrestrictions.
Indeed, in example (1), China andTaiwan both violate the restriction that threatenand declare require an animate subject, and thushave to be interpreted metonymically.
This viewis present in the psycholinguistic literature, too.Some authors argue that a figurative interpreta-tion of a word typically comes about when all lit-eral interpretations fail; see Gibbs (1994) for anoverview.
This failure is often due to the violationof selectional restrictions.However, in psycholinguistics as well as incomputational linguistics, this approach has lostmuch of its appeal.
It has become clear to re-searchers in both fields that many metonymiesdo not violate any restrictions at all.
In to likeShakespeare, for instance, there is no explicit lin-guistic trigger for the metonymical interpretationof Shakespeare.
Rather, it is our world knowl-edge that pre-empts a literal reading of the au-thor?s name.
Examples like this one demonstratethat metonymy recognition should not be based onrigid rules, but rather, on information about the se-mantic class of the target word and the semanticand grammatical context in which it occurs.
Inpsycholinguistics, this insight (among others) hasgiven rise to theories claiming that a figurative in-terpretation does not follow the failure of a literalone, but that both processes occur in parallel (Fris-son and Pickering, 1999).
In computational lin-guistics, it has led to the development of statisti-25cal, corpus-based approaches to metonymy recog-nition.This view was first put into computational prac-tice by Markert and Nissim (2002a).
Their keyto success was the realization that metonymyrecognition is a sub-problem of Word SenseDisambiguation (WSD).
They found that mostmetonymies in the same semantic class belong toone of a limited number of metonymical patternsthat can be defined a priori.
The task of metonymyrecognition thus consists of the automatic assign-ment of one of these readings to a target word.Since all words in the same semantic class mayundergo the same semantic shifts, there only hasto be one classifier per class (and not per word, asin classic WSD).In this paper I will be concerned with theautomatic identification of metonymical locationnames.
More particularly, I will test two newapproaches to metonymy recognition on the basisof Markert and Nissim?s (2002b) corpora of 1,000mixed country names and 1,000 instances ofthe country name Hungary.1 The most impor-tant metonymical patterns in these corpora areplace-for-people, place-for-eventand place-for-product.
In addition, thereis a label mixed for examples that have tworeadings, and othermet for examples that donot belong to any of the pre-defined metonymicalpatterns.On the mixed country data, Nissim and Mark-ert?s (2003) classifiers achieved an accuracy of87%.
This was the result of a combination ofboth grammatical and semantic information.
Theirgrammatical information included the function ofa target word and its head.
The semantic informa-tion, in the form of Dekang Lin?s (1998) thesaurusof semantically similar words, allowed the classi-fier to search the training set for instances whosehead was similar, and not just identical, to that ofa test instance.Markert and Nissim?s (2002a) and Nissim andMarkert?s (2003) study is the only one to approachmetonymy recognition from a data-driven, statisti-cal perspective.
However, it also has a number ofdisadvantages.
First, it requires the annotation ofa large number of training and test instances.
Thiscompromises its possible application to a wide va-riety of metonymical patterns across a large num-1This data is publicly available and can be downloadedfrom http://homepages.inf.ed.ac.uk/mnissim/mascara.ber of semantic categories.
Second, its algorithmsare rather complex.
In the training phase, theycalculate smoothed probabilities on the basis ofa large annotated training corpus and in the testphase, they iteratively search through a thesaurusof semantically similar words.
This leads to thequestion if this complexity is indeed necessary inmetonymy recognition.This paper investigates two approaches thateach tackle one of these problems.
The unsuper-vised algorithm in section 2 has the intuitive ap-peal of not requiring any annotated training in-stances.
I will show that it is nevertheless oftenable to distinguish between two data clusters thatcorrelate with the two target readings.
In section 3,I will again take recourse to a supervised learn-ing method, but one that explicitly incorporatesa much simpler learning phase than its competi-tors in the literature ?
Memory-Based Learning.
Iwill demonstrate that this algorithm of ?lazy learn-ing?
gives state-of-the-art results in metonymyrecognition.
Moreover, although their psychologi-cal validity is not a focus of the present investiga-tion, the two studied algorithms have clear links tomodels of human behaviour.2 An unsupervised approach tometonymy recognition2.1 BackgroundUnsupervised machine learning algorithms donot need any labelled training examples.
In-stead, the machine itself has to try and groupthe training instances into a pre-defined numberof clusters, which ideally correspond to the im-plicit target labels.
The approach studied hereis Schu?tze?s (1998) Word Sense Discrimination,which uses second-order co-occurrence in order toidentify clusters of senses.Schu?tze?s (1998) algorithm first maps all wordsin the training corpus onto word vectors, whichcontain frequency information about the word?sfirst-order co-occurrents.
It then builds a vectorrepresentation for each of the contexts of the targetby adding up the word vectors of the words in thiscontext.
These second-order context vectors getclustered (often after some form of dimensionalityreduction), and each of the clusters is assumed tocorrespond to one of the senses of the target.
Theclassification of a test word, finally, proceeds byassigning it to the cluster whose centroid lies near-est to its context vector.
Schu?tze showed that, with26about 8,000 training instances on average, this al-gorithm obtains very promising results.This unsupervised algorithm is not just attrac-tive from a computational point of view; it is alsorelated to human behaviour.
First, it was inspiredby Miller and Charles?
(1991) observation that hu-mans rely on contextual similarity in order to de-termine semantic similarity.
Schu?tze (1998) there-fore hypothesized that there must be a correla-tion between contextual similarity and word mean-ing as well: ?a sense is a group of contextuallysimilar occurrences of a word?
(Schu?tze, 1998,p.99).
Second, this algorithm lies at the basis ofLatent Semantic Analysis (LSA).
Although thepsycholinguistic merits of LSA are an object ofdebate, its performance in several language taskscompares well to that of humans (Landauer andDumais, 1997).
Let us therefore investigate if it isable to tackle metonymy recognition as well.Schu?tze?s (1998) approach has been imple-mented in the SenseClusters program (Purandareand Pedersen, 2004)2, which also incorporatessome interesting variations on and extensions tothe original algorithm.
First, Purandare and Ped-ersen (2004) defend the use of bigram features in-stead of simple word features.
Bigrams are ?or-dered pairs of words that co-occur within five po-sitions of each other?
(Purandare and Pedersen,2004, p.2) and will be used throughout this pa-per.
Second, they also found that the hybrid algo-rithm of Repeated Bisections performs better thanSchu?tze?s (1998) clustering algorithm ?
at leastfor sparse data ?
so I will use it here, too.
Finally,as with all word sense discrimination techniques,evaluation proceeds indirectly: SenseClusters au-tomatically finds the alignment of senses and clus-ters that leads to the fewest misclassifications ?the confusion matrix that maximizes the diagonalsum.2.2 ExperimentsOn the basis of Markert and Nissim?s locationcorpora, I tested if unsupervised learning can beapplied to metonymy recognition.
60% of theinstances were used as training data, 40% as testdata, and the number of pre-defined clusters wasset to two.
The experiments were designed withfive specific research questions in mind:2This software package is freely available and can bedownloaded from http://senseclusters.sourceforge.net.?
Does unsupervised clustering work betterwith one-word sets?Since the unsupervised WSD approach stud-ied here uses lexical features only, I antici-pated it to work better with the Hungary datathan with the mixed country set.
After all, wecan expect one word to have fewer typical co-occurrences than an entire semantic class, soits contexts may be easier to cluster.?
Should a stoplist be used?Unsupervised clustering on the basis of co-occurrences usually ignores a number ofwords that are thought to be uninforma-tive about the reading of the target.
Exam-ples of such words are prepositions and ex-tremely frequent verbs (be, give, go, .
.
.
).
Inmetonymy recognition, however, these wordsmay be much more useful than in classicWSD.
If a location name occurs in a preposi-tional phrase with in, for instance, it is prob-ably used literally.
Similarly, verbs such asgive and go determine the interpretation of apossibly metonymical word in contexts likegive sth.
to a country (metonymical) and goto a country (literal).
Stoplists may thereforebe less useful in metonymy recognition.?
Are smaller context windows better thanlarge ones?Markert and Nissim (2002a) discovered that,with co-occurrence features, the reduction ofwindow sizes from 10 to about 3 led to a rad-ical improvement in precision (from 25% toabove 50%) and recall (from 4% to above20%).
Schu?tze?s (1998) original algorithm,however, used context windows of 25 wordson either side of the target.?
Does Singular Value Decomposition resultin better performance?3Schu?tze (1998) found that his algorithm per-forms better with SVD than without.
SVD issaid to abstract away from word dimensions,and to discover topical dimensions instead.This helps tackle vocabulary issues such assynonymy and polysemy, and moreover ad-dresses data sparseness.
However, as Mark-ert and Nissim (2002a) argue, the sense dis-tinctions between the literal and metonymi-cal meanings of a word are not of a topical3With SVD, I set the number of dimensions to 300, as inPurandare & Pedersen (2004).27+LL, +SVD +LL, -SVD -LL, +SVD -LL, -SVDcontext Acc F Acc F Acc F Acc F20 62.70 37.84** 73.78 11.01 60.54 28.43 54.72 26.2415 55.95 34.54* 51.08 34.18 55.68 30.51 56.49 27.1512 58.92 38.71** 60.54 39.67** 65.14 18.87 66.22 18.3010 55.68 36.92** 59.46 41.41** 61.35 20.99 65.95 20.257 54.32 35.25** 66.76 32.79** 59.73 26.60 65.14 25.435 66.76 38.81** 52.97 33.08 54.32 28.09 67.03 29.073 58.38 37.40** 70.54 14.17 61.62 37.17** 61.62 36.04**Table 1: Results on the mixed country data of four algorithms with varying context sizes and without astoplist.+LL : statistical feature selection-LL : frequency-based feature selection+SVD : dimensionality reduction with SVD-SVD : no dimensionality reduction** : F-score is significantly better than random assignment of data to clusters (p < 0.05)* : difference between F-score and random assignment approaches significance (p < 0.10)nature.
Word dimensions may thus lead tobetter performance.?
Should features be selected on the basis ofa statistical test?4Purandare and Pedersen (2004) used a log-likelihood test to select their features, prob-ably because of the intuition that ?candidatewords whose occurrence depends on whetherthe ambiguous word occurs will be indica-tive of one of the senses of the ambiguousword and hence useful for disambiguation?
(Schu?tze, 1998, p.102).
Schu?tze, in con-trast, found that statistical selection is outper-formed by frequency-based selection whenSVD is not used.Like Nissim and Markert (2003), I used fourmeasures to evaluate the experimental results: pre-cision, recall and F-score for the metonymical cat-egory, and overall accuracy.
They are defined inthe following way:?
Overall accuracy is the total number of in-stances that is classified correctly.?
Precision for the metonymical category is thepercentage of metonymical labels that theclassifier assigns correctly.?
Recall for the metonymical category is thepercentage of metonymies that the classifierrecognizes.4I again followed Purandare & Pedersen (2004) by select-ing bigrams with a log-likelihood score of 3.841 or more.?
F-score is the harmonic mean between preci-sion and recall:F = 2?
P ?RP +R(2)Let us use the confidence matrix below to illustratethese measures:LIT METLIT 208 86MET 37 39If the rows represent the correct labels and thecolumns the labels returned by the classifier, weget the following results:Acc = 208 + 39208 + 86 + 37 + 39 = 66.76%(3)P = 3939 + 86 = 31.20%(4)R = 3939 + 37 = 51.32%(5)F = 2?
31.20%?
51.32%31.20% + 51.32% = 38.81%(6)In engineering terms, a WSD system is only use-ful when its accuracy beats the so-called majoritybaseline.
This is the accuracy of a system that sim-ply gives the same, most frequent, label to all testinstances.
Such a classifier reaches an accuracy of79.46% on the test corpus of mixed country namesand of 77.35% on the test corpus with instances ofHungary.28+LL, +SVD +LL, -SVD -LL, +SVD -LL, -SVDcontext Acc F Acc F Acc F Acc F20 58.52 35.06* 73.28 14.63 57.51 32.39 57.00 34.7515 54.96 36.10* 60.05 33.19 54.20 34.31 57.25 35.38*12 53.18 38.67** 54.71 32.06 57.76 34.65 54.96 36.10*10 55.47 34.46 55.47 33.96 56.23 32.81 55.22 32.317 51.91 35.93 51.91 24.70 51.91 35.93 65.90 33.00**5 54.20 21.74 67.18 36.45** 63.87 35.45** 63.36 28.713 65.14 33.82** 64.89 35.51** 57.00 35.25* 59.80 36.80**Table 2: Results on the Hungary data of four algorithms with varying context sizes and without a stoplist.+LL, +SVD +LL, -SVD -LL, +SVD -LL, -SVDcontext Acc F Acc F Acc F Acc F20 67.51 15.89 67.94 16.00 57.76 36.64** 61.07 35.98**15 70.74 10.85 70.23 12.03 58.52 36.58** 64.89 34.91**12 66.92 30.11* 72.01 11.29 64.89 35.51** 64.89 34.29**10 63.87 29.00 61.83 27.88 63.87 29.00 63.87 29.007 67.18 27.93 62.60 27.59 64.38 29.29 64.38 29.295 67.18 29.51 67.43 29.67* 67.18 29.51 66.16 28.113 68.70 30.51** 68.70 30.51** 68.19 28.57 68.19 28.57Table 3: Results on the Hungary data of four algorithms with varying context sizes and with a stoplist.2.3 Experimental resultsCompared to this majority baseline, the results ofthe unsupervised approach fall below the mark.None of the accuracy values in tables 1, 2 and 3lies above this baseline.
With baselines of almost80%, however, this result comes as no surprise.Moreover, the classifier?s failure to beat the ma-jority baseline does not necessarily mean that it isunable to identify a ?metonymical?
and a ?literal?cluster in the data.
This ability should be inves-tigated with a ?2-test instead, which helps us de-termine if there is a correlation between a test in-stance?s cluster on the one hand and its label onthe other.
If we compare the results with this ?2-baseline, it emerges that in many cases, the iden-tified clusters indeed significantly correlate withthe reading of the target words.
The default (+LL+SVD) algorithm, for instance, typically identifiesa metonymical and a literal cluster in the mixedcountry data (table 1).
It also becomes clear thatthe best algorithms are not those with the highestaccuracy values.
After all, an accuracy close to thebaseline often results from the identification of onehuge ?literal?
cluster that covers most metonymiesas well.Let us now evaluate the algorithms with respectto the five research questions I mentioned above.First, a comparison between the results on themixed country data in table 1 and the Hungarydata in table 2 shows that the former are more con-sistent than the latter.
The (+LL +SVD) algorithmin particular is very successful on the country data.There is thus no sign of the anticipated difficultywith sets of mixed target words.Second, when the algorithm is applied to the setof mixed country names, it should not use a sto-plist.
Not a single time did the resulting clusterscorrelate significantly with the target labels ?
theresults were therefore not included here.
A possi-ble reason may be that the useful co-occurrencesin this data tend to be words on the stoplist, but itshould be studied more carefully if this is indeedthe case.On the Hungary data, the use of a stoplist has adifferent effect.
Overall success rate remains moreor less the same (although F-scores with a stoplistare slightly lower on average), but the results dis-play a different pattern.
Broadly speaking, a sto-plist is most beneficial when feature selection pro-ceeds on the basis of frequency and when largecontexts are used.
Smaller contexts are more suc-cessful without a stoplist.
There is a logic to this:as I observed above, stoplist words may be infor-mative about the reading of a possibly metonymi-cal word, but their usefulness increases when theyare closer to the target.
If go occurs within threewords of a country name, it may point towardsa literal reading; if it occurs within a context oftwenty words, it is less likely to do so.
This ex-plains why stoplists work best in combination withbigger contexts.Overall, the influence of context is hard to de-termine.
Small windows of three words on either29side of the target are generally most successful, butthe context size that should be chosen depends onother characteristics of the algorithm.
The sameis true for dimensionality reduction and statisti-cal feature selection.
In general, the anticipatednegative effects of dimensionality reduction werenot observed, and frequency-based feature selec-tion clearly benefited algorithms with a stoplist onthe Hungary data.
However, the algorithms shouldbe applied to more data sets in order to investigatethe precise effect of these factors.In short, although the investigated unsupervisedalgorithms never beat the majority baseline forMarkert and Nissim?s (2002b) data, they are oftenable to identify two clusters of data that correlatewith the two possible readings.
This is true forthe set with one target word as well as for the setwith mixed country names.
In general, the algo-rithms that incorporate both statistical feature se-lection and Singular Value Decomposition lead tothe best results, except for the Hungary data whenno stoplist is used.
In this last case, statistical fea-ture selection is best dropped and a large contextwindow should be chosen.3 Memory-based metonymy recognition3.1 BackgroundMemory-Based Learning (MBL), which is imple-mented in the TiMBL classifier (Daelemans et al,2004)5 rests on the hypothesis that people inter-pret new examples of a phenomenon by comparingthem to ?stored representations of earlier experi-ences?
(Daelemans et al, 2004, p.19).
It is thusrelated to Case-Based reasoning, which holds that?
[r]eference to previous similar situations is oftennecessary to deal with the complexities of novelsituations?
(Kolodner, 1993, p.5).
As a result ofthis learning hypothesis, an MBL classifier such asTiMBL eschews the formulation of complex rulesor the computation of probabilities during its train-ing phase.
Instead it remembers all training vec-tors and gives a test vector the most frequent labelof the most similar training vectors.TiMBL implements a number of MBL algo-rithms.
In my experiments, the so-called IB1-IGalgorithm (Daelemans and Van den Bosch, 1992)proved most successful.
It computes the distancebetween two vectors X and Y by adding up the5This software package is freely available and can bedownloaded from http://ilk.uvt.nl/software.html.weighted distances ?
between their correspondingfeature values, as in equation (7):?
(X,Y ) =n?i=1wi?
(xi, yi)(7)By default, TiMBL determines the weights for eachfeature on the basis of the feature?s InformationGain (the increase in information that the knowl-edge of that feature?s value brings with it) and thenumber of values that the feature can have.
Theprecise equations are discussed in Daelemans etal.
(2004) and need not concern us any furtherhere.3.2 ExperimentsI again applied this IB1-IG algorithm to Mark-ert and Nissim?s (2002b) location corpora.
In or-der to make my results as comparable as possi-ble to Markert and Nissim?s (2002a) and Nissimand Markert?s (2003), I made two changes in theevaluation process.
First, evaluation was now per-formed with 10-fold cross-validation.
Second, inthe calculation of accuracy, I made a distinctionbetween the several metonymical labels, so that amisclassification within the metonymical categorywas penalized as well.I conducted two rounds of experiments.
Thefirst used only grammatical features: the grammat-ical function of the word (subj, obj, iobj, pp, gen,premod, passive subj, other), its head, the pres-ence of a second head, and the second head (ifpresent).
Such features can be expected to iden-tify metonymies with a high precision, but sincemetonymies may have a wide variety of heads,performance will likely suffer from data sparse-ness (Nissim and Markert, 2003).
I therefore con-ducted a second round of experiments, in which Iadded semantic information to the feature sets, inthe form of the WordNet hypernym synsets of thehead?s first sense.WordNet is a machine-readable lexical databasethat, among other things, structures English verbs,nouns and adjectives in a hierarchy of so-called?synonym sets?
or synsets (Fellbaum, 1998).
Eachword belongs to such a group of synonyms, andeach synset ?is related to its immediately moregeneral and more specific synsets via direct hyper-nym and hyponym relations?
(Jurafsky and Mar-tin, 2000, p.605).
Fear, for instance, belongs tothe synset fear, fearfulness, fright, which has emo-tion as its most immediate, and psychological fea-30Acc P R FTiMBL 86.6% 80.2% 49.5% 61.2%N&M 87.0% 81.4% 51.0% 62.7%Table 4: Results for the mixed country data.TiMBL: TiMBL?s resultsN&M: Nissim and Markert?s (2003) resultsture as its highest hypernym.
This tree structureof synsets thus corresponds to a hierarchy of se-mantic classes that can be used to add semanticknowledge to a metonymy recognition system.My experiments investigated a few constella-tions of semantic features.
The simplest of theseused the highest hypernym synset of the head?sfirst sense as an extra feature.
A second approachadded to the feature vector the head?s highest hy-pernym synsets, with a maximum of ten.
If thehead did not have 10 hypernyms, its own synsetwould fill the remaining features.
The result ofthis last approach is that the MBL classifier firstlooks for heads within the same synset as the testhead.
If it does not find a word that shares all hy-pernyms with the test instance, it gradually climbsthe synset hierarchy until it finds the training in-stances that share as many hypernyms as possi-ble.
Obviously, this approach is able to make morefine-grained semantic distinctions than the previ-ous one.3.3 Experimental resultsThe experiments with grammatical informationshowed that TiMBL is able to replicate Nissim andMarkert?s (2003) results.
The obtained accuracyand F-scores for the mixed country names in ta-ble 4 are almost identical to Nissim and Mark-ert?s figures.
The results for the Hungary data intable 5 lie slightly lower, but again mirror Nis-sim and Markert?s figures closely (Katja Markert,personal communication).
This is all the morepromising since my results were reached withoutany semantic information.
Remember that Nis-sim and Markert?s algorithm, in contrast, usedDekang Lin?s (1998) clusters of semantically sim-ilar words in order to deal with data sparseness.Memory-Based Learning does not appear to needthis semantic information to arrive at state-of-the-art performance.
Instead, it tackles possible datasparseness by its automatic back-off to the gram-matical role if the target?s head is not found amongthe training data.Acc P R F84.7% 80.4% 51.9% 63.1%Table 5: Results for the Hungary data.Of course, the grammatical role of a targetword is often not sufficient for determining its lit-eral or metonymical status.
Therefore my secondround of experiments investigated if performancecan still be improved by the addition of seman-tic information.
This does not appear to be thecase.
Although F-scores for the metonymical cat-egory tended to increase slightly (as a result ofhigher recall values), the system?s accuracy hardlychanged.
In order to check if this was due tothe automatic selection of the head?s first Word-Net sense, I manually disambiguated all heads inthe data.
This showed that the first WordNet sensewas indeed often incorrect, but the selection of thecorrect sense did not improve performance.
Thereason for the failure of WordNet information togive higher results must thus be found elsewhere.A first possible explanation is the mismatch be-tween WordNet?s synsets and our semantic labels.Many synsets cover such a wide variety of wordsthat they allow for several readings of the target,while others are too specific to make generaliza-tion possible.
A second possible explanation is thepredominance of prepositional heads in the data,for which extra semantic information is useless.In short, the experiments above demonstrateconvincingly that Memory-Based Learning is asimple but robust approach to metonymy recog-nition.
This simplicity is a major asset, and isin stark contrast to the competing approaches tometonymy recognition in the literature.
It shouldbe studied, however, if there are other features thatcan further increase the classifier?s performance.Attachment information is one such source of in-formation that certainly deserves further attention.4 ConclusionsThis paper has investigated two computational ap-proaches to metonymy recognition that both intheir own way are less complex than their com-petitors in the literature.
The unsupervised algo-rithm in section 2 does not need any labelled train-ing data; the supervised algorithm of Memory-Based Learning incorporates an extremely simplelearning phase.
Both approaches moreover havea clear relation to models of human behaviour.31Schu?tze?s (1998) approach is related to LSA, amodel whose output correlates with human perfor-mance on a number of language tasks.
Memory-Based Learning is akin to Case-Based Reasoning,which holds that people approach a problem bycomparing it to similar instances in their memory.Rather than presenting a psycholinguistic cri-tique of these approaches, this paper has investi-gated their ability to recognize metonymical loca-tion names.
Not surprisingly, it was shown thatthe unsupervised approach is not yet a good basisfor a robust metonymy recognition system.
Nev-ertheless, it was often able to distinguish two clus-ters in the data that correlate with the literal andmetonymical readings.
It is striking that this isalso the case for a set of mixed target words fromthe same category ?
a type of data set that, tomy knowledge, this algorithm had not yet been ap-plied to.
Memory-Based Learning, finally, provedto be a reliable way of recognizing metonymi-cal words.
Although this approach is much sim-pler than many competing algorithms, it producedstate-of-the-art results, even without semantic in-formation.AcknowledgementsI would like to thank Mirella Lapata, Dirk Geer-aerts and Dirk Speelman for their feedback on thisproject.
I am also very grateful to Katja Markertand Malvina Nissim for their helpful informationabout their research.ReferencesW.
Daelemans and A.
Van den Bosch.
1992.
Generali-sation performance of backpropagation learning on asyllabification task.
In M. F. J. Drossaers and A. Nij-holt, editors, Proceedings of TWLT3: Connection-ism and Natural Language Processing, pages 27?37,Enschede, The Netherlands.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2004.
TiMBL: Tilburg Memory-Based Learner.
Technical report, Induction ofLinguistic Knowledge, Computational Linguistics,Tilburg University.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
Cambridge, MA: MIT Press.S.
Frisson and M. J. Pickering.
1999.
The processingof metonymy: Evidence from eye movements.
Jour-nal of Experimental Psychology: Learning, Memoryand Cognition, 25:1366?1383.R.
W. Jr. Gibbs.
1994.
The Poetics of Mind.
Figura-tive Thought, Language and Understanding.
Cam-bridge: Cambridge University Press.D.
Jurafsky and J. H. Martin.
2000.
Speech and Lan-guage Processing.
Upper Saddle River, NJ: PrenticeHall.J.
Kolodner.
1993.
Case-Based Reasoning.
San Ma-teo, CA: Morgan Kaufmann Publishers.Z.
Ko?vecses.
2002.
Metaphor: A Practical Introduc-tion.
Oxford: Oxford University Press.T.
K. Landauer and S. T. Dumais.
1997.
A solution toPlato?s problem: The latent semantic analysis theoryof the acquisition, induction, and representation ofknowledge.
Psychological Review, 104:211?240.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the International Con-ference on Machine Learning, Madison, USA.K.
Markert and M. Nissim.
2002a.
Metonymy res-olution as a classification task.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002), Philadelphia,USA.K.
Markert and M. Nissim.
2002b.
Towards a cor-pus annotated for metonymies: the case of locationnames.
In Proceedings of the Third InternationalConference on Language Resources and Evaluation(LREC 2002), Las Palmas, Spain.G.
A. Miller and W. G. Charles.
1991.
Contextual cor-relates of semantic similarity.
Language and Cogni-tive Processes, 6(1):1?28.M.
Nissim and K. Markert.
2003.
Syntactic featuresand word similarity for supervised metonymy res-olution.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics(ACL-03), Sapporo, Japan.A.
Purandare and T. Pedersen.
2004.
Word sensediscrimination by clustering contexts in vector andsimilarity spaces.
In Proceedings of the Confer-ence on Computational Natural Language Learning,Boston, USA.J.
Pustejovsky.
1995.
The Generative Lexicon.
Cam-bridge, MA: MIT Press.H.
Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?124.32
