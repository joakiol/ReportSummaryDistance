Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 91?99,Baltimore, Maryland, USA, 26 June 2014. c?2014 Association for Computational LinguisticsData Warehouse, Bronze, Gold, STEC, SoftwareDoug CooperCenter for Research in Computational Linguisticsdoug.cooper.thailand@gmail.comAbstractWe are building an analytical data warehousefor linguistic data ?
primarily lexicons andphonological data ?
for languages in theAsia-Pacific region.
This paper briefly out-lines the project, making the point that theneed for improved technology for endangeredand low-density language data extends wellbeyond completion of fieldwork.
We suggestthat shared task evaluation challenges(STECs) are an appropriate model to followfor creating this technology, and that stockingdata warehouses with clean bronze-standarddata and baseline tools ?
no mean task ?
is aneffective way to elicit the broad collaborationfrom linguists and computer scientists neededto create the gold-standard data that STECsrequire.1 IntroductionThe call for this workshop mentions the first stepof the language documentation process, pointingout that the promise of new technology in docu-menting endangered languages remains unful-filled, particularly in the context of modern re-cording technologies.But lack of tools extends far beyond this firststep.
It encompasses the accessibility of datalong since gathered and (usually, but not always)published, as well as applications for the data byits most voracious consumer:  the study of com-parative and historical linguistics.We encounter these problems daily in prelimi-nary development of data and software resourcesfor a planned Asia-Pacific Linguistic Data Ware-house.
Briefly, our initial focus is on five phyla(~2,000 languages):  Austroasiatic, Austronesian,Hmong-Mien, Kra-Dai, and Sino-Tibetan, whichform a Southeast Asian convergence area, andindividually extend well into China, India, theHimalayas, and the Pacific.
Data for languagesof Australia and New Guinea will follow.Not all of these languages are endangered, butmany are; not all are low-density, but most are.Our data are preferentially drawn from the sortof lexicography gathered for comparative pur-poses (ideally 2,500 items per language), and thephonological, semantic, and phylogenetic datathat can be found for, or inferred from, them.These are the only kind of data for which we arelikely to find near-complete language representa-tion.
We include smaller lexicons when neces-sary, and intra-language dialect surveys whenavailable.
All available metadata are incorpo-rated, including typological and phonotactic fea-tures, (phylogenetic) character sets, geo-physicaland demographic data, details of lexicon cover-age, extent, or quality, and bibliographic orsource data.Such data are not always easily found.
Theirdelivery packages ?
primarily books and journals?
may be discoverable via bibliographic meta-data, but details of the datasets themselves arenot.
As a result, traditional bibliographic docu-mentation, accessed via portals like OLAC(Simons and Bird, 2000) and Glottolog (Nord-hoff and Hammarstr?m, 2011), tends to have lowrecall and precision in regard to data resourcediscovery.Our experience in acquiring and performingmethodical data audits of  large quantities ofpublished and unpublished materials reveals setsof lexical, grammatical, phonological, corpus,and other materials that are regular enough inform, and extensive enough in content, to com-prise aggregable linguistic data supersets for theAsia-Pacific region.These ongoing data audits take a three-tieredapproach, separately documenting texts (to en-able source recovery), their abstract data content(to enable high-recall resource discovery), andany concrete, transcribed data instances (to en-able high-precision data aggregation).Discovery and aggregation only open the door.Many datasets are hand-crafted for a researcher?sspecific needs and interests, even if they fall intolarger research categories.
Yet far from havingreliable algorithms for central concerns (such asproto-language reconstruction, or subgrouping oflinguistic phyla in family trees or networks) thefield has not yet had to grapple with basic prob-lems ?
such as normalizing phonological tran-scription or gloss semantics, or accurately as-sembling large-scale cognate sets ?
that will be91presented by datasets that include millions ofdata items for thousands of languages, and manymore thousands of dialectal variants.The central issue we face is the gap between:?
the results of published and unpublishedfieldwork, and?
their usability in downstream research andreference applications.In some cases this gap is painfully obvious ?as in the backlog of carefully elicited wordlistsstill awaiting phonetic transcription.
In others,the gap becomes evident when we begin to as-semble large comparable datasets from publisheddata; deceptively difficult, and never accom-plished for collections broader than a single lan-guage family, or larger than about 200 words perlanguage.
Such tasks are still basically handwork; often requiring the specialized knowledgeof the field researcher.1.1 Data life cycle:  anticipate or participateWe see the need for tools as part of a new sort ofdata life cycle management that extends the con-cerns of content, format, discovery, access, cita-tion, preservation, and rights as usually articu-lated, notably in Bird and Simons (2003).Simply put, producing publishable or ?cor-rect?
results is not sufficient to guarantee thedownstream usability of data.
Rather, data mustundergo a series of transformations as it travelsfrom one research specialty to the next.
We hopethere will be an increasing expectation that thedata producer either anticipate or participate inthis process.At one end of the cycle, this often requiressmall, specialized datasets of the sort needed tosupport software development for tasks likeautomated transcription or phonemic analysis ?still open problems in the context of under-resourced languages.At the other, building massive datasets that aresuitable for improving and extending quantitativecomparative linguistic applications ?
or discover-ing the scales at which different methods mightbe most useful ?
has not been a priority for thelinguistics community:  if a few representativeitems demonstrate a relationship or support areconstruction convincingly, then exhaustivecoverage does not make the argument stronger.We face a classic resource deadlock.
High-quality ?last-user?
datasets are not constructedbecause traditional methods are too expensiveand time-consuming.
However, tools for refin-ing ?first-producer?
data on an industrial scaleare not built because the high-quality datasetsneeded to validate them do not exist.
Develop-ment of computational methods for problems likesubgrouping tends to focus on a small number ofavailable datasets, while their results are criti-cized for precisely this.2 STECs and gold-standard dataLog jams in natural language processing arenothing new.
A shared task evaluation chal-lenge (STEC) presents an open challenge to thefield in the context of evaluating performance ona specific task.
Originally developed in the con-text of the TIPSTER Text Program (which initi-ated the long-running MUC and TREC confer-ence series) as discussed in Belz and Kilgarriff(2006), see also Hirschmann (1998) ?Over thepast twenty years, virtually every field of re-search in human language technology (HLT) hasintroduced STECS.
?The STEC is the culmination of a series of ef-forts intended to focus and advance progress byasking such questions as:?
what problems need to be solved in orderto advance the field?
Where are we tryingto go, and what is standing in our way??
what kinds of necessary data are not gener-ally available?
What kinds of datasets aretoo difficult for individual researchers tocreate??
what kind of functional decomposition intosimpler goals will help demonstrates andmeasure progress in quantitative and quali-tative terms?Both data, and evaluation metrics, are madeavailable well before the STEC, which is oftenheld in conjunction with a major conference.The task is typically initiated by the release of adataset; results are submitted by some deadline,and the results of evaluation are announced be-fore or at the conference.The terms gold-standard and more recently,silver-standard (for machine-generated sets) areused to describe datasets created for use inSTECs and NLP applications.
These can bethought of as being ?correct answers?
for quanti-tative evaluation (Kilgarriff 1998).Gold-standard datasets are built to enablecomparable evaluation of alternative algorithmsor implementations.
Frequently, part of the setwill be publicly released in advance to serve astraining data, while part of it is held back to pro-vide test data (and is released at a later date).92Gold-standard datasets reflect the state of theart in an area, such as the specification of wordsenses, delineation of word boundaries, orevaluation of message sentiment, for which theremay not be any purely objective ground truth.We can reasonably expect to allow alternativeformulations of gold-standard sets in areas inwhich the state of the art may be uncertain, evenin the eyes of experts.
And we can anticipateincreased critical scrutiny of previously acceptedjudgments as more base data and better investi-gative tools become available; see e.g.
Round(2013, 2014).2.1 STECs for low-density languagesIn our opinion, all of the reasons for whichSTECs are devised and gold-standard datasetsdefined apply equally to the low-density lan-guage problems we touched on in Section 1.These include:?
normalization and syllabification of tran-scribed data,?
phonetic transcription of audio and ortho-graphic data,?
morphemic analysis of transcribed data,?
extraction of a phonemic analysis fromphonetic data,?
identification of internal cognates and/orderivationally related forms, as well asloan-word identification and stratification,?
automated reglossing / translation (to astandardized gloss set) of glosses and/ordefinitions.?
automated inference of phylogenetic sub-grouping.?
automated generation of proto-forms,All are characterized by the same requirementfor human judgment in processing, and lack ofabsolute certainty as to outcomes.The critical difference is that (as far as weknow) STECs in NLP invariably focus on high-density languages for which both data and exper-tise are readily available.
In contrast, low-density languages ?
which presumably includesthe entire range of endangered languages ?
areby their nature specialty realms, for which exper-tise, even within a single phylum, is often widelydispersed.Thus, the problem we face in creating success-ful STECs for documentary linguistics is notsimply a matter of thinking up tasks, and relyingon in-house expertise to develop gold-standarddatasets.
Rather, advancing development ofcomputational tools requires participation from alarge community of independently working lin-guists as well.3 Cast bronze to net goldOur approach to achieving this begins by layingthe groundwork for collaboration between:?
computer scientists who recognize the needfor better data, and will join the challengeof solving practical problems in buildingmassive, comparable datasets, and?
linguists willing to help create and validatethe gold-standard reference sets and train-ing data needed to establish quality metricsfor improving software tools.We think this collaboration is best motivatedin the old-fashioned way:  reduce participants?vapor   no data could be located (useful when documenting data availability by ISO code)water untranscribed audio recording onlypaper   print/image/PDF data are in hand, but not transcribed or extractedtin   raw e-orthography and definitions (as in typical documentary dictionaries)copper   raw e-forms and glosses (as in purpose-collected comparative lexicons; e.g.
Holle lists)bronzeclean electronic data and metadata, ready for hand or machine processing,naive normalization of forms and glosses, cognate sets partially specified,capable of demonstrating preliminary data warehouse functionality(Software:  baseline vanilla algorithms)silver  machine-normalized or grouped data, not yet verified by humans (Software:  better than baseline)gold   human-verified/accepted, machine-usable comparable datasets (Software:  (best) able to produce gold-standard results)Table 1.
Data quality standards re lexicons, cognate sets, reconstructions, and subgrouping, with par-allels to software tools.
Silver- and gold-standard are the only terms commonly used in this context.93startup costs, flatten their learning curves, high-light expected outcomes that will advance col-laborators?
self-interests, and help provide thedata, tools, and/or metrics that collaborators willneed to seek funding themselves.This in itself as a long-term effort ?
easily 5?8years for our region, with optimal funding ?whose thrust can be summarized as cast bronzeto net gold (see Table 1).Locating data, and bringing it to the minimalstate required for computer applications requiresa massive amount of work.
Consider just thediscovery aspect, for which the data audit men-tioned earlier entails an ongoing, two-prongedeffort.On one hand, we identify potential data con-tent by acquiring as much published and unpub-lished print material as possible, including com-plete journal runs, monograph series, informallypublished ?gray literature,?
extensive sets of un-published field notes, and regular publicationbacklists (notably, a half-century of works fromPacific Linguistics, which will be added to ouron-line repository later this year).1On the other, we systematically work throughthe complete ISO 639-3 inventory (as a proxy forthe on-the ground truth, and as a means of help-ing to perfect the standard, as well as identifyingdocumentary shortfalls that might be short-listedfor fieldwork) of our region, attempting to find atleast lexical content for every language.Overall, our summary project developmentplan has four steps, which relate to content andscale, and determined our choice of a regionalfocus ?
for which we could take responsibility ?rather than either working at greater depth on asingle phylum, or attempting to build a globalframework, and then relying primarily on outsidecontributors.First, define an area that is broad enough to beof wide linguistic interest, and able to supply arange of control and alternative test conditionsfor both traditional and computational methods.Even allowing for typological variation that maybe found in individual phyla, we think this usu-ally requires a regional perspective.1 For New Guinea, this required a special sub-projectdubbed INGA, dedicated to tracking down ?invisible?New Guinea archives held in libraries and file cabi-nets around the world!
As implied, when possible wenegotiate rights to scan and make all materials freelyavailable in an on-line repository, and will begin toregister DOI names (when appropriate) for texts anddata this year.Second, locate and prepare raw data of suffi-cient breadth and depth.
We think that aimingfor blanket rather than selective coverage is ap-propriate ?
it enables the broadest range of re-search agendas by reflecting the natural state ofhuman migration and constant language contact.Third, establish research goals that capture theinterest of both fields ?
documentary / compara-tive / historical linguistics and computer science.This extends the argument for complete regionalcoverage, especially in convergence areas.
But italso argues for limiting scope to an area in whichit is realistically possible to actively recruit in-volvement, conference by conference.Finally, we need to lower barriers to participa-tion  We think we can do this by providing aframework that allows data owners to take ad-vantage of existing software tools, and whichprovides software developers with easily custom-ized data test beds ?
the analytical data ware-house.4 The data warehouseA data warehouse is an integrated collection ofdatabases that incorporates tools for sampling,analyzing, and visualizing query results.
Unlikerepository databases intended for storage andretrieval of prepared values (perhaps for off-lineprocessing), data warehouses assume that datafiltering, transformation, and analysis are essen-tial to satisfying every query.
In the context ofcomparative lexicons, such tasks are well beyondthe scope of existing virtual research environ-ments such as WebLicht (Hinrichs et al 2010)and TextGrid (Neuroth et al 2011), which focusprimarily on text corpora.Because sampling filters allow selection ofhomogeneous or representative subsamples, wecan be as inclusive as possible in regard to dataacquisition.
We are not talking about data qual-ity; rather (working within our overall criterionof comparative lexical data) we want to avoidexcluding sets because of concerns about datasetsize or content disparity, or over-representationof dialect survey data.Many operations we wish to perform on orwith data involve open research questions.
Al-though users may perceive the warehouse asproviding access to tools, we intend to present itto tool developers as a tunable test bed of datathat does not require them to deal with data man-agement, as well as a means of using, and en-couraging development of, open-source toolkitssuch as the pioneering work of Kleiweg (2009)94and List and Moran (2013).
We return to theidea of plug-and-play operations on lexicons inSection 6.The warehouse also helps provide added valueto potential data contributors.
Even if software isfreely available, preparing data or setting uptools can impose substantial, even insurmount-able, burdens on data creators, particularly inregions in which cooperation between linguistsand computer scientists is less common than inthe US or Europe.4.1 Data warehouse query-flowIn our test warehouse implementation, function-ality is divided as follows:?
filter: define a search universe based on phy-logenetic or phonotactic properties, geo-physical or proximal location, lexicon char-acteristics, or other data or metadata features.?
frame: specify data and/or metadata to bereturned, e.g.
specific aspects of the formand/or gloss, or metadata details that mightbe useful for correlation testing.?
analyze: extract phone inventories, calculatefunctional load, investigate lexical neighbor-hoods, cluster data by phonological similar-ity, etc.?
visualize: provide alternatives to tables asappropriate, e.g.
tree/graph/map layouts.?
recycle: search within returned data, usefaceting to extend searches, or let the visu-alization serve as a chooser for a new search.For brevity we discuss just one feature:  filter-ing.
This lets the search universe be defined inas much detail as possible, and is partly commonsense:  our overall data universe is decidedlylumpy due to the decision to include small sam-ples (some <100 items) when necessary, and dia-lect surveys (perhaps with only minor differencesbetween doculects) when possible.It is also intended to take advantage of thelarge quantities of available metadata, whether itis explicit / external ?
that is, related to the lan-guage or doculect, or implicit / internal, i.e.
canbe derived from individual datasets or samples.Such metadata includes proposed phylogeneticrelations, typological features, geophysical anddemographic data, characteristics of lexiconcomposition, extent, or quality, bibliographic orsource data, and phonological properties of thedoculect itself.
Some of this metadata may bereturned with individual items as part of the dataframe.Filter targets may be specified if appropriate.For example, a filter might limit a search to lan-guages that contain sesquisyllables, or insteadrequire that returned items be sesquisyllabic.4.2 An example query and resultFigure 1 shows the result of a relatively simplewarehouse query (using our unreleased explora-tory implementation):  a geo-constrained phy-logenetic tree for Trans-New Guinea languages.Tree topology follows Ethnologue 16 (Lewis,2009) as provided by the MultiTree project(Aristar and Ratliff, 2005); other analyses areFigure 1  A geo-constrained phylogenetic tree (analysis by Ethnologue via MultiTree).
This clustertree keeps low-level group nodes near their daughters, but raises the root nodes.
Dialects are green,languages yellow, and groups blue95readily specified.
In this example dialects (fromthe same sources) are arranged in a circular pat-tern around the ISO 639-3 hub language (and,again, other analyses could be used instead).
Thesame filtering and visualization routines are usedin a different manner in Figure 2, which showswords for ?bone?
in Austronesian languages asprovided by ABVD (Greenhill et al, 2008).5 Data comparability and reusabilityWe will finish the discussion of data warehouseswith a quick look at data comparability and re-use.
Comparability or equivalence of datasetscan be looked at in two ways?
at the content level, e.g.
to ensure that thesame systems of transcription and glossingare used for all datasets, and?
at the structural level, in identifying datasetsof comparable complexity, structure, oravailable detail.At the content level, normalization of formsand glosses is the critical transformation in thejourney to gold-standard quality.
We will brieflydescribe our systems for normalization, Meta-gloss and Metaphon, because they are ripe forcomputational assistance.
The discussion endswith a quick introduction to Etyset, the frame-work we intend to use to describe and distributestructured datasets, such as those that incorporatesubgroup and cognate detail.5.1 Gloss, Metagloss, EtyglossIn most of our applications, a gloss is semanticannotation provided by the wordlist author inorder to index phonological forms.
Unfortu-nately, these may be elicitation terms rather thanglosses (green?
?grue.?
blue?
?grue?
), or localvernacular rather than common or scientificterms for flora and fauna.
Phrasing varies wildly,and proper reading may depend on having the listcontext available (short/tall, short/long).
Trans-lation may be lossy (strew or scatter as nouns)due to differences in grammaticalization or lexi-fication.
All of these undermine comparability.We have begun to define an intermediate,standardized metagloss layer to express the au-thor?s intent (if discernable).
A third layer, theetygloss, will help account for semantic shift inlabeling cognate groups; i.e.
glossing emptyplaceholders for proto-language reconstructions.In the simple case all three layers are identical.Metagloss provides a controlled vocabularyfor re-annotating or translating existing lexiconglosses; it foregrounds the critical design linkbetween glossing and searching.
We map this toWordNet senses, creating a low-overhead tool forword-sense disambiguation and facet generation.The Metagloss controlled vocabulary can beextended; it uses attributes to specify predictablerelationships (sheep:male:castrated for wether)and solve lexicalization problems that arise ingloss translation (e.g.
n@strew is the noun formof strew).
Additionally, it allows definition oflightweight ontologies; relations between Meta-glosses that clarify semantic relations and im-prove search fallback performance.5.2 Phon, Metaphon, EtyphonPhonological forms present similarly difficultsearch problems; these go beyond easily fixednotational convention.
For example, absence ofmarked syllable boundaries can make phonologi-cal searches difficult when we are interested inthe phoneme?s role (such as pre-nasalization)rather than its sign (/n/, /m/ etc.
).Figure 2  A search for ?bone?
in ABVD Austronesian data (again, relations by Ethnologue via Mul-tiTree), constrained to locations in Indonesia, and projected onto a map96The same holds true for other context-sensitive symbols (e.g.
?h?
as /h/, /h/, or as a pre-pended indicator of unvoiced phonemes).
Agreater problem arises from parsimonious nota-tions that rely on commentary to clarify unwrit-ten content, e.g.
predictable vowel insertion ?these must be made explicit.We define an intermediate layer of standard-ized notation called metaphon:  a conventionalnotation that allows consistent search, whileclearly documenting (and minimizing, in com-parison to wild-card searches) the scope of anyunavoidable approximation.
A third layer, theetyphon, allows temporary specification of a(possibly sub-lexical) phonemic rendition priorto any formal reconstruction.Metaphon, like metagloss, is intimately tied tosearch functionality.
Normalized transcriptionenables consistent extraction of phonological andphonotactic data.
It lets the search universe berestricted to languages (or items) that have par-ticular phonemes or features.
This dynamic,data-driven process lets us weigh relative signifi-cance ?
frequency, salience, functional load ?
offeatures in sets that are themselves results drawnfrom a restricted search universe; e.g.
to considerthe functional load of tones in sesquisyllables.5.3 Structural comparability:  EtySetThe discussion thus far has focused on the formand quality of data items.
We are equally con-cerned with what might be called structural com-parability of data sets, because this determine theapproach we take to systematic description, dis-semination, and re-use of cognate sets, phyloge-netic trees, or sets of proto-form reconstructions.This has nothing to do with tagging or inter-change standards, which can be handled withborrowed schemes designed for similar purposes,e.g.
Newick notation (Felsenstein, 1986) or suc-cessors (Nakhleh, 2003).
Rather, we requirenomenclature that might be used to describe theircontents, or to enable identification of sets ofcomparable complexity, structure, or detail.We think such comparison is crucial to helpresearch in quantitative historical linguisticsmove beyond its current state, which many lin-guists view as interesting but nevertheless ad hocexperimentation.
In other words, we would liketo see computational approaches to cognate iden-tification, subgrouping, and proto-language re-construction be developed and tested in envi-ronments for which the controlled variable islinguistic typology, with as many other factors aspossible held equal.Similarly, we would like to be able to varystarting conditions.
For example Bouchard-C?t?et al (2013) report on a computational approachto reconstruction given (assumed) prior knowl-edge of subgrouping in Austronesian.
However,any one or two variables from amongst cognategrouping, reconstruction, and phylogenetic sub-grouping may be used to test approaches to infer-ring or generating the third.We refer to cognate sets, phylogenetic trees,and reconstructed proto-forms as etysets.
Thekey terms of our working descriptive nomencla-ture are outlined in Table 2.Etysets may be bare (links only), or supportedby reconstructed forms or semantics; note thatthe phylogenetic analyses provided by Eth-nologue, Glottolog, or MultiTree may be repre-sent with bare etysets.
An internal cognate etysethas depth (number of internal sets) and size(number of forms in each set).
A regular cognateetyset has depth (the number of sets / implicitnumber of root proto-forms) and breadth (thenumber of lects represented in each cognate set).For example a  bare cognate etyset of Bah-naric, breadth Eth:80% / depth MSEA:90%depth includes data from 32 (of 40. according tothe Ethnologue analysis) Bahnaric languages,and at least 450 of the 500-odd terms in theMSEA (SIL 2002) elicitation list.
Cognategroupings are provided, but not reconstructionsor etyglosses.breadth number of nodes or leaves at any level of a phylogenetic tree.depth number of branch levels supplied.degree branchy-ness ?
the number of branches / degree of diversity at a given node.density a joint measure of breadth, depth, and degree.size # of cited or reconstructed forms associated with a leaf or branch node.coverage describes the extent of an etyset in terms of a fixed reference inventory.phylogenetic etyset described in term of breadth, depth, degree, and size.documented node includes metadata for approximate time depth and geographic location.cognate etyset may be internal or regular, and contains internal or regular cognate sets.Table 2.
Outline of the EtySet descriptive vocabulary.976 Operations on lexiconsWe end with a brief note about computationaltasks for and by a data warehouse that is:?
stocked primarily with lexical, phonological,and phylogenetic data and relevant metadata,?
intended to support research in comparativeand historical linguistics.These fall under the general heading of opera-tions on lexicons.
We do not draw a strict divid-ing line between software employed to preparedata for use in a warehouse, and software used bythe warehouse.
We do exclude operations whoseimplementation is likely to be closely tied to aparticular database implementation.All would benefit from being implemented asplug-and-play functions, requiring some, but notexcessive, programmer effort.
This:?
allows head-to-head comparison of alterna-tive algorithms, implementations, or inter-pretations of how measurements or actionsshould be carried out,?
allows encapsulation and offloading of com-putationally expensive algorithms; this is animportant issue for some quantitative or sta-tistical comparative methods, and?
encourages re-use of code in building new,alternative platforms for linguistic research.We assume that all of these can be specified interms of functionality, required data inputs, andexpected data outputs, sticking to a Unix-likemodel in which data can be minimally formattedplain-text streams which, with the assistance oftabs, parentheses, and newlines, can be inter-preted as bags, lists, vectors, matrices, trees, andthe like.
Higher-level streams (JSON, XML,RDF, HTML) are also reasonable outputs.For brevity?s sake, we limit examples to op-erations on phonological forms.
We could easilylist similar sets of operations ?
some straightfor-ward, some not ?
on morphology, semantics,alternatives for visualization, cognate identifica-tion, phylogenetic subgrouping, proto-form gen-eration, and the like.Operations on phonological strings / listsConversion and markup of transcription?
between standardized and/or special-purposenotations,?
to novel notations, e.g.
gestural scores,?
unambiguous conversion of notation fromhistorical (e.g.
Americanist) to IPA,?
potentially ambiguous normalization (e.g.interpretation of /h/),?
phonetic to phonemic conversion,?
marking of syllable boundaries,?
marking of syllable-internal features (e.g.onset, nucleus, coda),?
marking of morpheme boundaries.Extraction / recognition of phonological features?
sonority sequence tagging.?
extraction/recognition of phones, phonation,co-articulatory, suprasegmental features,?
count/extraction of phone/feature n-grams,?
extraction or identification of arbitrary collo-cational features (e.g.
sesquisyllable+tone),Calculation of distance/similarity measures be-tween strings, lists, and vectors?
weighted and unweighted edit distances,?
substring matching measures,?
vector cosine distance,?
phonologically based distance/similarity,?
language-internal distance/similarity,?
information content distance/similarity.Clustering?
subgrouping list contents,?
?sounds like...?
search (for very large sets).Neighborhood measures?
generation of phonological neighborhoods,?
identification of neighbors,?
calculation of neighborhood size, density,clustering coefficients.Load measures?
calculation of functional load of phonemes,features, collocations,?
calculation of salience of phonemes, fea-tures, collocations,?
use in pseudo-word generation.7 ConclusionThe call for this workshop foregrounds develop-ment of software to aid in initial documentationof endangered languages, seeks models for col-lection and management of endangered-languagedata, and means of encouraging productive inter-action between documentary linguists and com-puter scientists.98We suggest that these same needs exist alldown the line, encompassing low-resource lan-guages in general, documentation long-sincecompleted, and analytical applications far re-moved from fieldwork settings.
We propose thataddressing them in downstream environments,such as data warehouses and STECs, may be aneffective way to meet our common ?preeminentgrand challenge:?
integration of linguistic theo-ries and analyses, relying on massive scaling upof datasets and new computational methods, asarticulated by Bender and Good (2010).ReferencesAnthony Aristar and Martha Ratliff.
2005.
MultiTree:A digital library of language relationships.
Insti-tute for Language Information and Technology:Ypsilanti, MI.
http://multitree.org.Anja Belz and Adam Kilgarriff.
2006.
Shared-taskevaluations in HLT: Lessons for NLG.
In Proceed-ings of INLG-2006.Emily Bender and Jeff Good.
2010.
A Grand Chal-lenge for Linguistics:  Scaling Up and IntegratingModels.
White paper contributed to NSF SBE 2020initiative.
http://www.nsf.gov/sbe/sbe_2020/-2020_pdfs/Bender_Emily_81.pdfSteven Bird and Gary Simons.
2003.
Seven Dimen-sions of Portability for Language Documentationand Description.
Language 79:2003, 557-5822.Alexandre Bouchard-C?t?,  David Hall,  Thomas L.Griffiths and  Dan Klein.
2013.
Automated recon-struction of ancient languages using probabilisticmodels of sound change.
Proceedings of the Na-tional Academy of Sciences.
http://-www.pnas.org/content/110/11/4224Joseph Felsenstein.
1986.
The newick tree format.http://evolution.genetics.washington.edu/phylip/newicktree.htmlSimon Greenhill, Robert Blust, and Russell D.. Gray.2008.
The Austronesian Basic Vocabulary Data-base: From Bioinformatics to Lexomics.
Evolu-tionary Bioinformatics, 4:271-283. http://-language.psy.auckland.ac.nz/austronesianErhard W. Hinrichs, Marie Hinrichs and Thomas Zas-trow.
2010.
WebLicht: Web-Based LRT Servicesfor German.
In: Proceedings of the ACL 2010 Sys-tem Demonstrations.
pages 25?29.Lynette Hirschman.
1998.
The evolution of evalua-tion: Lessons from the Message UnderstandingConferences.
Computer Speech and Language,12:283?285.Adam Kilgarriff.
1998.
Gold Standard Datasets forEvaluating Word Sense Disambiguation Programs.Computer Speech and Language, 12 (3) Special Is-sue on Evaluation of Speech and Language Tech-nology, edited by Robert Gaizauskas.
453-472.http://www.kilgarriff.co.uk/Publications/1998-K-CompSL.pdf  For TREC see http://trec.nist.gov.The TIPSTER site has been preserved here:http://www.nist.gov/itl/div894/894.02/related_projects/tipster/Peter Kleiweg.
2006.
RuG/L04 Software for dialecto-metrics and cartography.
Rijksuniversiteit Gronin-gen. Faculteit der Letteren.
http://www.let.rug.nl-/kleiweg/L04/M.
Paul Lewis.
2009.
Ethnologue: Languages of theWorld, Sixteenth Edition.
SIL International, Dal-las, Texas.Johann-Mattis List and Steven Moran.
2013.
AnOpen-Source Toolkit for Quantitative HistoricalLinguistics.
Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics, 13?18, Sofia, Bulgaria.
http://-www.zora.uzh.ch/84667/1/P13-4003.pdfLuay Nakhleh, Daniel Miranker, and Francois Bar-bancon.
2003.
Requirements of Phylogenetic Data-bases.
In Third IEEE Symposium on BioInformat-ics and BioEngineering (BIBE?03).
141-148.
IEEEPress.
http://www.cs.rice.edu/~nakhleh/Papers/-bibe03_final.pdfHeike Neuroth, Felix Lohmeier, Kathleen MarieSmith: TextGrid.
Virtual Research Environment forthe Humanities.
In: The International Journal ofDigital Curation.
6, Nr.
2, 2011, S. 222?231.Sebastian Nordhoff and Harald Hammarstr?m.
2011.Glottolog/Langdoc: Defining dialects, languages,and language families as collections of resources.783 CEUR Workshop, Proceedings of the First In-ternational Workshop on Linked Science 2011http://iswc2011.semanticweb.org/fileadmin/iswc/-Papers/Workshops/LISC/nordhoff.pdfErich R. Round.
2013.
?Big data?
typology and lin-guistic phylogenetics: Design principles for validdatasets.
Presented 25 May 2013 at 21st Manches-ter Phonology Meeting.
Accessible viahttps://uq.academia.edu/ErichRound.Erich R. Round.
2014.
The performance of STRUC-TURE on linguistic datasets & ?researcher degreesof freedom?.
Presented 15 Jan 2014 at TaSil, Aar-hus, Denmark.
Accessible viahttps://uq.academia.edu/ErichRoundSIL Mainland Southeast Asia Group.
2002.
SoutheastAsia 436 Word List revised November 2002.http://msea-ling.info/digidata/495/b11824.pdfGary Simons and Steven Bird.
2000.
The seven pillarsof open language archiving:A vision statement.http://www.language-archives.org/docs/vision.-html.99
