Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 595?603,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEvaluating Multilanguage-Comparability of Subjectivity AnalysisSystemsJungi Kim, Jin-Ji Li and Jong-Hyeok LeeDivision of Electrical and Computer EngineeringPohang University of Science and Technology, Pohang, Republic of Korea{yangpa,ljj,jhlee}@postech.ac.krAbstractSubjectivity analysis is a rapidly grow-ing field of study.
Along with its ap-plications to various NLP tasks, muchwork have put efforts into multilingualsubjectivity learning from existing re-sources.
Multilingual subjectivity analy-sis requires language-independent crite-ria for comparable outcomes across lan-guages.
This paper proposes to mea-sure the multilanguage-comparability ofsubjectivity analysis tools, and providesmeaningful comparisons of multilingualsubjectivity analysis from various pointsof view.1 IntroductionThe field of NLP has seen a recent surge in theamount of research on subjectivity analysis.
Alongwith its applications to various NLP tasks, therehave been efforts made to extend the resourcesand tools created for the English language to otherlanguages.
These endeavors have been success-ful in constructing lexicons, annotated corpora,and tools for subjectivity analysis in multiple lan-guages.There are multilingual subjectivity analysis sys-tems available that have been built to monitor andanalyze various concerns and opinions on the In-ternet; among the better known are OASYS fromthe University of Maryland that analyzes opinionson topics from news article searches in multiplelanguages (Cesarano et al, 2007)1 and TextMap,an entity search engine developed by Stony BrookUniversity for sentiment analysis along with otherfunctionalities (Bautin et al, 2008).2 Though thesesystems currently rely on English analysis toolsand a machine translation (MT) technology to1http://oasys.umiacs.umd.edu/oasysnew/2http://www.textmap.com/translate other languages into English, up-to-dateresearch provides various ways to analyze subjec-tivity in multilingual environments.Given sentiment analysis systems in differ-ent languages, there are many situations whenthe analysis outcomes need to be multilanguage-comparable.
For example, it has been commonthese days for the Internet users across the worldto share their views and opinions on various top-ics including music, books, movies, and global af-fairs and incidents, and also multinational compa-nies such as Apple and Samsung need to analyzecustomer feedbacks for their products and servicesfrom many countries in different languages.
Gov-ernments may also be interested in monitoring ter-rorist web forums or its global reputation.
Sur-veying these opinions and sentiments in variouslanguages involves merging the analysis outcomesinto a single database, thereby objectively compar-ing the result across languages.If there exists an ideal subjectivity analy-sis system for each language, evaluating themultilanguage-comparability would be unneces-sary because the analysis in each language wouldcorrectly identify the exact meanings of all in-put texts regardless of the language.
However, thisrequirement is not fulfilled with current technol-ogy, thus the need for defining and measuring themultilanguage-comparability of subjectivity anal-ysis systems is evident.This paper proposes to evaluate themultilanguage-comparability of multilingualsubjectivity analysis systems.
We build a numberof subjectivity classifiers that distinguishes sub-jective texts from objective ones, and measurethe multilanguage-comparability according to ourproposed evaluation method.
Since subjectivityanalysis tools in languages other than English arenot readily available, we focus our experiments oncomparing different methods to build multilingualanalysis systems from the resources and systems595created for English.
These approaches enable us toextend a monolingual system to many languageswith a number of freely available NLP resourcesand tools.2 Related WorkMuch research have been put into developingmethods for multilingual subjectivity analysis re-cently.
With the high availability of subjectivity re-sources and tools in English, an easy and straight-forward approach would be to employ a machinetranslation (MT) system to translate input textsin target languages into English then carry outthe analyses using an existing subjectivity analy-sis tool (Kim and Hovy, 2006; Bautin et al, 2008;Banea et al, 2008).
Mihalcea et al (2007) andBanea et al (2008) proposed a number of ap-proaches exploiting a bilingual dictionary, a paral-lel corpus, and an MT system to port the resourcesand systems available in English to languages withlimited resources.For subjectivity lexicons translation, Mihalceaet al (2007) and Wan (2008) used the first sense ina bilingual dictionary, Kim and Hovy (2006) useda parallel corpus and a word alignment tool to ex-tract translation pairs, and Kim et al (2009) useda dictionary to translate and a link analysis algo-rithm to refine the matching intensity.To overcome the shortcomings of available re-sources and to take advantage of ensemble sys-tems, Wan (2008) and Wan (2009) explored meth-ods for developing a hybrid system for Chinese us-ing English and Chinese sentiment analyzers.
Ab-basi et al (2008) and Boiy and Moens (2009) havecreated manually annotated gold standards in tar-get languages and studied various feature selec-tion and learning techniques in machine learningapproaches to analyze sentiments in multilingualweb documents.For learning multilingual subjectivity, the lit-erature tentatively concludes that translating lex-icon is less dependable in terms of preserving sub-jectivity than corpus translation (Mihalcea et al,2007; Wan, 2008), and though corpus translationresults in modest performance degradation, it pro-vides a viable approach because no manual la-bor is required (Banea et al, 2008; Brooke et al,2009).Based on the observation that the performancesof subjectivity analysis systems in comparableexperimental settings for two languages differ,Texts with an identical negative sentiment:* The iPad could cannibalize the e-reader market.
* ?????
(iPad) ???
???
(e-reader market)????
?
??
(could cannibalize).Texts with different strengths of positive sentiments:* Samsung cell phones have excellent battery life.
* ??
(Samsung) ????
(cell phone) ????
(battery) ????
(somehow or other) ????
(last long).Figure 1: Examples of sentiments in multilingualtextBanea et al (2008) have attributed the variationsin the difficulty level of subjectivity learning tothe differences in language construction.
Bautin etal.
(2008)?s system analyzes the sentiment scoresof entities in multilingual news and blogs and ad-justed the sentiment scores using entity sentimentprobabilities of languages.3 Multilanguage-Comparability3.1 MotivationThe quality of a subjectivity analysis tool is mea-sured by its ability to distinguish subjectivity fromobjectivity and/or positive sentiments from nega-tive sentiments.
Additionally, a multilingual sub-jectivity analysis system is required to generateunbiased analysis results across languages; thesystem should base its outcome solely on the sub-jective meanings of input texts irrespective of thelanguage, and the equalities and inequalities ofsubjectivity labels and intensities must be usefulwithin and throughout the languages.Let us consider two cases where the pairs ofmultilingual inputs in English and Korean haveidentical and different subjectivity meanings (Fig-ure 1).
The first pair of texts carry a negative sen-timent about how the release of a new electronicsdevice might affect an emerging business market.When a multilanguage-comparable system is in-putted with such a pair, its output should appropri-ately reflect the negative sentiment, and be identi-cal for both texts.
The second pair of texts sharea similar positive sentiment about a mobile de-vice?s battery capacity but with different strengths.A good multilingual system must be able to iden-tify the positive sentiments and distinguish the dif-ferences in their intensities.However, these kinds of conditions cannot bemeasured with performance evaluations indepen-596dently carried out on each language; A systemwith a dissimilar ability to analyze subjective ex-pressions from one language to another may de-liver opposite labels or biased scores on texts withan identical subjective meaning, and vice versa,but still might produce similar performances onthe evaluation data.Macro evaluations on individual languages can-not provide any conclusions on the system?smultilanguage-comparability capability.
To mea-sure how much of a system?s judgment principlesare preserved across languages, an evaluation froma different perspective is necessary.3.2 Evaluation ApproachAn evaluation of multilanguage-comparabilitymay be done in two ways: measuring agreementsin the outcomes of a pair of multilingual texts withan identical subjective meaning, or measuring theconsistencies in the label and/or accordance in theorder of intensity of a pair of texts with differentsubjectivities.There are advantages and disadvantages to eachapproaches.
The first approach requires multi-lingual texts aligned at the level of specificity,for instance, document, sentence and phrase, thatthe subjectivity analysis system works.
Text cor-pora for MT evaluation such as newspapers,books, technical manuals, and government offi-cial records provide a wide variety of paralleltexts, typically at the sentence level.
Annotatingthese types of corpus can be efficient; as par-allel texts must have identical semantic mean-ings, subjectivity?related annotations for one lan-guage can be projected into other languages with-out much loss of accuracy.The latter approach accepts any pair of multi-lingual texts as long as they are annotated with la-bels and/or intensity.
In this case, evaluating the la-bel consistency of a multilingual system is only asdifficult as evaluating that of a monolingual sys-tem; we can produce all possible pairs of textsfrom test corpora annotated with labels for eachlanguage.
Evaluating with intensity is not easy forthe latter approach; if test corpora already existwith intensity annotations for both languages, nor-malizing the intensity scores to a comparable scaleis necessary (yet is uncertain unless every pair ischecked manually), otherwise every pair of mul-tilingual texts needs a manual annotation with itsrelative order of intensity.In this paper, we utilize the first approach be-cause it provides a more rational means; we canreasonably hypothesize that text translated into an-other language by a skilled translator carries anidentical semantic meaning and thereby conveysidentical subjectivity.
Therefore the required re-source is more easily attained in relatively inex-pensive ways.For evaluation, we measure the consistency inthe subjectivity labels and the correlation of sub-jectivity intensity scores of parallel texts.
Section5.1 describes the details of evaluation metrics.4 Multilingual Subjectivity SystemWe create a number of multilingual systems con-sisting of multiple subsystems each processing alanguage, where one system analyzes English, andthe other systems analyze the Korean, Chinese,and Japanese languages.
We try to reproduce a setof systems using diverse methods in order to com-pare the systems and find out which methods aremore suitable for multilanguage-comparability.4.1 Source Language SystemWe adopt the three systems described below as oursource language systems: a state-of-the-art sub-jectivity classifier, a corpus-based, and a lexicon-based systems.
The resources needed for devel-oping the systems or the system itself are readilyavailable for research purposes.
In addition, thesesystems cover the general spectrum of current ap-proaches to subjectivity analysis.State-of-the-art (S-SA): OpinionFinder is apublicly-available NLP tool for subjectivity analy-sis (Wiebe and Riloff, 2005; Wilson et al, 2005).3The software and its resources have been widelyused in the field of subjectivity analysis, and ithas been the de facto standard system againstwhich new systems are validated.
We use a high-coverage classifier from the OpinionFinder?s twosentence-level subjectivity classifiers.
This NaiveBayes classifier builds upon a corpus annotated bya high-precision classifier with the bootstrappingof the corpus and extraction patterns.
The classi-fier assesses a sentence?s subjectivity with a labeland a score for confidence in its judgment.Corpus-based (S-CB): The MPQA opinion cor-pus is a collection of 535 newspaper articles in En-glish annotated with opinions and private states at3http://www.cs.pitt.edu/mpqa/opinionfinderrelease/, ver-sion 1.5597the sub-sentence level (Wiebe et al, 2003).4 Weretrieve the sentence level subjectivity labels for11,111 sentences using the set of rules describedin (Wiebe and Riloff, 2005).
The corpus providesa relatively balanced corpus with 55% subjectivesentences.
We train an ML-based classifier us-ing the corpus.
Previous studies have found that,among several ML-based approaches, the SVMclassifier generally performs well in many subjec-tivity analysis tasks (Pang et al, 2002; Banea etal., 2008).We use SVMLight with its default configura-tions,5 inputted with a sentence represented as afeature vector of word unigrams and their countsin the sentence.
An SVM score (a margin or thedistance from a learned decision boundary) with apositive value predicts the input as being subjec-tive, and negative value as objective.Lexicon-based (S-LB): OpinionFinder contains alist of English subjectivity clue words with in-tensity labels (Wilson et al, 2005).
The lexiconis compiled from several manually and automati-cally built resources and contains 6885 unique en-tries.Riloff and Wiebe (2003) constructed a high-precision classifier for contiguous sentences us-ing the number of strong and weak subjectivewords in current and nearby sentences.
Unlike pre-vious work, we do not (or rather, cannot) main-tain assumptions about the proximity of input text.Using the lexicon, we build a simple and high-coverage rule-based subjectivity classifier.
Settingthe scores of strong and weak subjective words as1.0 and 0.5, we evaluate the subjectivity of a givensentence as the sum of subjectivity scores; abovea threshold, the input is subjective, and otherwiseobjective.
The threshold value is optimized for anF-measure using the MPQA corpus, and is set to1.0 throughout our experiments.4.2 Target Language SystemTo construct a target language system leveragingon available resources in the source language, weconsider three approaches from previous litera-ture:1. translating test sentences in target languageinto source language and inputting them into4http://www.cs.pitt.edu/mpqa/databaserelease/, version1.25http://svmlight.joachims.org/, version 6.02a source language system (Kim and Hovy,2006; Bautin et al, 2008; Banea et al, 2008)2. translating a source language training corpusinto target language and creating a corpus-based system in target language (Banea et al,2008)3. translating a subjectivity lexicon from sourcelanguage to target language and creating alexicon-based system in target language (Mi-halcea et al, 2007)Each approach has its advantages and disadvan-tages.
The advantage of the first approach is itssimple architecture, clear separation of subjectiv-ity and MT systems, and that it has only one sub-jectivity system, and is thus easier to maintain.Its disadvantage is that the time-consuming MThas to be executed for each text input.
In the sec-ond and third approaches, a subjectivity system inthe target language is constructed sharing corpora,rules, and/or features with the source languagesystem.
Later on, it may also include its own setof resources specifically engineered for the targetlanguage as a performance improvement.
How-ever, keeping the systems up-to-date would requireas much effort as the number of languages.
Allthree approaches use MT, and would suffer sig-nificantly if the translation results are poor.Using the first approach, we can easily adopt allthree source language systems;?
Target input translated into source, analyzedby source language system S-SA?
Target input translated into source, analyzedby source language system S-CB?
Target input translated into source, analyzedby source language system S-LBThe second and the third approaches are carriedout as follows:Corpus-based (T-CB): We translate the MPQAcorpus into the target languages sentence by sen-tence using a web-based service.6 Using the samemethod for S-CB, we train an SVM model foreach language with the translated training corpora.Lexicon-based (T-LB): This classifier is identi-cal to S-LB, where the English lexicon is replacedby one of the target languages.
We automaticallytranslate the lexicon using free bilingual dictionar-ies.7 First, the entries in the lexicon are looked6Google Translate (http://translate.google.com/)7quick english-korean, quick eng-zh CN, and JMDictfrom StarDict (http://stardict.sourceforge.net/) licensed underGPL and EDRDG.598Table 1: Agreement on subjectivity (S for subjec-tive, O objective) of 859 sentence chunks in Ko-rean between two annotators (An.
1 and An.
2).An.
2S O TotalAn.1 S 371 93 464O 23 372 395Total 394 465 859up in the dictionary, if they are found, we se-lect the first word in the first sense of the def-inition.
If the entry is not in the dictionary, welemmatize it,8 then repeat the search.
Our sim-ple approach produces moderate-sized lexicons(3,808, 3,980, 3,027 for Korean, Chinese, andJapanese) compared to Mihalcea et al (2007)?scomplicated translation approach (4,983 Roma-nian words).
The threshold values are optimizedusing the MPQA corpus translated into each tar-get language.95 Experiment5.1 Experimental SetupTest CorpusOur evaluation corpus consists of 50 parallelnewspaper articles from the Donga Daily NewsWebsite.10 The website provides news articles inKorean and their human translations in English,Japanese, and Chinese.
We selected articles thatcontain Editorial in its English title from a 30-day period.
Three human annotators who are flu-ent in the two languages manually annotated N-to-N sentence alignments for each language pairs(KR-EN, KR-CH, KR-JP).
By keeping only thesentence chunks whose Korean chunk appears inall language pairs, we were left with 859 sentencechunk pairs.The corpus was preprocessed with NLP toolsfor each language,11 and the Korean, Chinese, andJapanese texts were translated into English withthe same web-based service used to translate thetraining corpus in Section 4.2.Manual Annotation and Agreement Study8JWI (http://projects.csail.mit.edu/jwi/)9Korean 1.0, Chinese 1.0, and Japanese 0.510http://www.donga.com/11Stanford POS Tagger 1.5.1 and Stanford Chinese WordSegmenter 2008-05-21 (http://nlp.stanford.edu/software/),Chasen 2.4.4 (http://chasen-legacy.sourceforge.jp/), KoreanMorphological Analyzer (KoMA) (http://kle.postech.ac.kr/)Table 2: Agreement on projection of subjectivity(S for subjective, O objective) from Korean (KR)to English (EN) by one annotator.ENS O TotalKRS 458 6 464O 12 383 395Total 470 389 859To assess the performance of our subjectiv-ity analysis systems, the Korean sentence chunkswere manually annotated by two native speakersof Korean with Subjective and Objective labels(Table 1).
A proportion agreement of 0.86 and akappa value of 0.73 indicate a substantial agree-ment between the two annotators.
We set aside743 sentence chunks that both annotators agreedon for the automatic evaluation of subjectivityanalysis systems, thereby removing the borderlinecases, which are difficult even for humans to as-sess.
The corresponding sentence chunks for otherlanguages were extracted and tagged with labelsequivalent to Korean chunks.In addition, to verify how consistently the sub-jectivity of the original texts is projected to thetranslated, we carried out another manual annota-tion and agreement study with Korean and Englishsentence chunks (Table 2).Note that our cross-lingual agreement study issimilar to the one carried out by Mihalcea etal.
(2007), where two annotators labeled the sen-tence subjectivity of a parallel text in different lan-guages.
They reported that, similarly to monolin-gual annotations, most cases of disagreements onannotations are due to the differences in the anno-tators?
judgments on subjectivity, and the rest fromsubjective meanings lost in the translation processand figurative language such as irony.To avoid the role played by annotators?
pri-vate views from disagreements, the subjectivity ofsentence chunks in English were manually anno-tated by one of the annotators for the Korean text.Judged by the same annotator, we speculate thatthe disagreement in the annotation should accountonly for the inconsistency in the subjectivity pro-jection.
By proportion, the agreement between theannotation of Korean and English is 0.97, and thekappa is 0.96, suggesting an almost perfect agree-ment.
Only a small number of sentence chunkpairs have inconsistent labels; six chunks in Ko-599Texts swihanwseanwiadxcahh liwgcvm:giwc*nht*wsvnPoi???i???ubswgiwseazi?i???ulshx*csw-r:*xzi????i?
?ubslansn:zkoi.vcha (iwgaiua vnves zilshx*csw-iu)awbaanifvmwgipvca*i*nliSvcwgipvca*zishibvchansn:ibswgiwseakfanwseanwitvhwisniwc*nht*wsvnPoi???i??i??????uTnls*yhi#*w*i$vwvchzi%%&&????i???i???u%(%&&'lvtt*ci*mwvev)staiS*nvzi!
"#uxcahanwalzi$%&i'(?ulcabi*wwanwsvnzkoiTnls*yhi#*w*i$vwvchig*hixcvlm aliwgai%(%&&'lvtt*cihm) vex* wiS*nvkFigure 2: Excerpts from Donga Daily News withdiffering sentiments between parallel textsrean lost subjectivity in translation, and impliedsubjective meanings in twelve chunks were ex-pressed explicitly through interpretation.
Excerptsfrom our corpus show two such cases (Figure 2).Evaluation MetricsTo evaluate the multilanguage-comparability ofsubjectivity analysis systems, we measure 1) howconsistently the system assigns subjectivity labelsand 2) how closely numeric scores for systems?confidences correlate with regard to parallel textsin different languages.In particular, we use Cohen?s kappa coefficientfor the first and Pearson?s correlation coefficientfor the latter.
These widely used metrics provideuseful comparability measures for categorical andquantitative data.Both coefficients are scaled from ?1 to +1, in-dicating negative to positive correlations.
Kappameasures are corrected for chance, thereby yield-ing better measurements than agreement by pro-portion.
The characteristics of Pearson?s correla-tion coefficient that it measures linear relation-ships and is independent of change in origin, scale,and unit comply with our experiments.5.2 Subjectivity ClassificationOur multilingual subjectivity analysis systemswere evaluated on the test corpora described inSection 5.1 (Table 3).Due to the difference in testbeds, the perfor-mance of the state-of-the-art English system (S-SA) on our corpus is lower by about 10% rela-tively than the performance reported on the MPQAcorpus.12 However, it still performs sufficiently12precision, recall, and F-measure of 79.4, 70.6, and 74.7.well and provides the most balanced results amongthe three source language systems; The corpus-based system (S-CB) classifies with a high pre-cision, and the lexicon-based (S-LB) with a highrecall.
The source language systems (S-SA,-CB,-LB) lose a small percentage in precision when in-putted with translations, but the recalls are gener-ally on a par or even higher in the target languages.For the systems created from target language re-sources, Corpus-based systems (T-CB) generallyperform better than the ones with source languageresource (S-CB), and lexicon-based systems (T-LB) perform worse than (S-LB).
Similarly to sys-tems with source language resources, T-CB clas-sifies with a high precision and T-LB with a highrecall, but the gap is less.
Among the target lan-guages, Korean tends to have a higher precision,and Japanese a higher recall than other languagesin most systems.Overall, S-SA provides easy accessibility whenanalyzing both the source and the target languages,with a balanced precision and recall performance.Among the other approaches, only T-CB is bet-ter in all measures than S-SA, and S-LB performsbest on F-measure evaluations.5.3 Multilanguage-ComparabilityThe evaluation results on multilanguage-comparability are presented in Table 4.
Thesubjectivity analysis systems are evaluated withall language pairs with kappa and Pearson?scorrelation coefficients.
Kappa and Pearson?scorrelation values are consistent with each other;Pearson?s correlation between the two evaluationmeasures is 0.91.We observe a distinct contrast in performancesbetween corpus-based systems (S-CB and T-CB)and lexicon-based systems (S-LB and T-LB); Allcorpus-based systems show moderate agreementswhile agreements on lexicon-based systems areonly fair.Within corpus-based systems, S-CB performsbetter with language pairs that include English,and T-CB performs better with language pairs ofthe target languages.For lexicon-based systems, systems in the tar-get languages (T-LB) performs the worst withonly slight to fair agreements between languages.Lexicon-based systems and state-of-the-art sys-tems in the source language (S-LB and S-SA) re-sult in average performances.600Table 3: Performance of subjectivity analysis with precision (P), recall (R), and F-measure (F).
S-SA,-CB,-LB systems in Korean, Chinese, Japanese indicate English analysis systems inputted with transla-tions of the target languages into English.English Korean Chinese JapaneseP R F P R F P R F P R FS-SA 71.1 63.5 67.1 70.7 61.1 65.6 67.3 68.8 68.0 69.1 67.5 68.3S-CB 74.4 53.9 62.5 74.5 52.2 61.4 71.1 63.3 67.0 72.9 65.3 68.9S-LB 62.5 87.7 73.0 62.9 87.7 73.3 59.9 91.5 72.4 61.8 94.1 74.6T-CB 72.4 67.5 69.8 75.0 66.2 70.3 72.5 70.3 71.4T-LB 59.4 71.0 64.7 58.4 82.3 68.2 56.9 92.4 70.4Table 4: Performance of multilanguage-comparability: kappa coefficient (?)
for measuring comparabilityof classification labels and Pearson?s correlation coefficient (?)
for classification scores for English (EN),Korean (KR), Chinese (CH), and Japanese (JP).
Evaluations of T-CB,-LB for language pairs includingEnglish are carried out with results from S-CB,-LB for English and T-CB,-LB for target languages.S-SA S-CB S-LB T-CB T-LB?
?
?
?
?
?
?
?
?
?EN & KR 0.41 0.55 0.45 0.60 0.37 0.59 0.42 0.60 0.25 0.41EN & CH 0.39 0.54 0.41 0.62 0.33 0.52 0.39 0.57 0.22 0.38EN & JP 0.39 0.53 0.43 0.65 0.30 0.59 0.40 0.59 0.15 0.33KR & CH 0.36 0.54 0.39 0.59 0.28 0.57 0.46 0.64 0.23 0.37KR & JP 0.37 0.60 0.44 0.69 0.50 0.69 0.63 0.76 0.18 0.38CH & JP 0.37 0.53 0.49 0.66 0.29 0.57 0.46 0.63 0.22 0.46Average 0.38 0.55 0.44 0.64 0.35 0.59 0.46 0.63 0.21 0.39-100-50050100-100 -50 0 50 100(a) S-SA-4-3-2-101234-4 -3 -2 -1 0 1 2 3 4(b) S-CB-10-50510-10 -5 0 5 10(c) S-LB-4-3-2-101234-4 -3 -2 -1 0 1 2 3 4(d) T-CB-10-50510-10 -5 0 5 10(e) T-LBFigure 3: Scatter plots of English (x-axis) and Korean (y-axis) subjectivity scores from state-of-the-art(S-SA), corpus-based (S-CB), and lexicon-based (S-LB) systems of the source language, and corpus-based with translated corpora (T-CB), and lexicon-based with translated lexicon (T-LB) systems.
Slantedlines in figures are best-fit lines through the origins.601Figure 3 shows scatter plots of subjectivityscores of our English and Korean test corpora eval-uated on different systems; the data points on thefirst and the third quadrants are occurrences of la-bel agreements, and the second and the fourth aredisagreements.
Linearly scattered data points aremore correlated regardless of the slope.Figure 3a shows a moderate correlation for mul-tilingual results from the state-of-the-art system(S-SA).
Agreements on objective instances areclustered together while agreements on subjectiveinstances are diffused over a wide region.Agreements between the source languagecorpus-based system (S-CB) and the corpus-basedsystem trained with translated resources (T-CB)are more distinctively correlated than the resultsfor other pairs of systems (Figures 3b and 3d).
Wenotice that S-CB seems to have a lower number ofoutliers than T-CB, but slightly more diffusive.Lexicon-based systems (S-LB, T-LB) gener-ate noticeably uncorrelated scores (Figures 3c and3e).
We observe that the results from the Englishsystem with translated inputs (S-LB) is more cor-related than those from systems with translatedlexicons (T-LB), and that analysis results fromboth systems are biased toward subjective scores.6 DiscussionWhich approach is most suitable for multilingualsubjectivity analysis?In our experiments, the corpus-based sys-tems trained on corpora translated from Englishto the target languages (T-CB) perform wellfor subjectivity classification and multilanguage-comparability measures on the whole.
However,the methods we employed to expand the languageswere naively carried out without much considera-tions for optimization.
Further adjustments couldimprove the other systems for both classificationand multilanguage-comparability performances.Is there a correlation between classification per-formance and multilanguage-comparability?Lexicon-based systems in the source language(S-LB) have good overall classification perfor-mances, especially on recall and F-measures.However, these systems performs worse onmultilanguage-comparability than other systemswith poorer classification performances.
Intriguedby the observation, we tried to measure whichcriteria for classification performance influencesmultilanguage-comparability.
We again employedPearson?s correlation metrics to measure the corre-lations of precision (P), recall (R), and F-measures(F) to kappa (?)
and Pearson?s correlation (?)
val-ues.Specifically, we measure the correlations be-tween the sums of P, the sums of R, and thesums of F to ?
and ?
for all pairs of systems.13The correlations of P with ?
and ?
are 0.78and 0.68, R ?0.38 and ?0.28, and F ?0.20and ?0.05.
These numbers strongly suggest thatmultilanguage-comparability correlates with theprecisions of classifiers.However, we cannot always expect a high-precision multilingual subjectivity classifier to bemultilanguage-comparable as well.
For example,the S-SA system has a much higher precisionthan S-LB consistently over all languages, buttheir multilanguage-comparability performancesdiffered only by small amounts.7 ConclusionMultilanguage-comparability is an analysis sys-tem?s ability to retain its decision criteria acrossdifferent languages.
We implemented a number ofpreviously proposed approaches to learning mul-tilingual subjectivity, and evaluated the systemson multilanguage-comparability as well as clas-sification performance.
Our experimental resultsprovide meaningful comparisons of the multilin-gual subjectivity analysis systems across variousaspects.Also, we developed a multilingual subjectivityevaluation corpus from a parallel text, and studiedinter-annotator, inter-language agreements on sub-jectivity, and observed persistent subjectivity pro-jections from one language to another from a par-allel text.For future work, we aim extend this work toconstructing a multilingual sentiment analysis sys-tem and evaluate it with multilingual datasetssuch as product reviews collected from differentcountries.
We also plan to resolve the lexicon-based classifiers?
classification bias towards sub-jective meanings with a list of objective words(Esuli and Sebastiani, 2006) and their multilin-gual expansion (Kim et al, 2009), and evaluatethe multilanguage-comparability of systems con-structed with resources from different sources.13Pairs of values such as 71.1 + 70.7 and 0.41 for preci-sions and Kappa of S-SA for English and Korean.602AcknowledgementWe thank the anonymous reviewers for valuablecomments and helpful suggestions.
This work issupported in part by Basic Science Research Pro-gram through the National Research Foundationof Korea (NRF) funded by the Ministry of Edu-cation, Science and Technology (MEST) (2009-0075211), and in part by the BK 21 project in2010.ReferencesAhmed Abbasi, Hsinchun Chen, and Arab Salem.2008.
Sentiment analysis in multiple languages:Feature selection for opinion classification in webforums.
ACM Transactions on Information Systems,26(3):1?34.Carmen Banea, Rada Mihalcea, Janyce Wiebe, andSamer Hassan.
2008.
Multilingual subjectivityanalysis using machine translation.
In EMNLP ?08:Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 127?135, Morristown, NJ, USA.Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.2008.
International sentiment analysis for news andblogs.
In Proceedings of the International Confer-ence on Weblogs and Social Media (ICWSM).Erik Boiy and Marie-Francine Moens.
2009.
Amachine learning approach to sentiment analysisin multlingual Web texts.
Information Retrieval,12:526?558.Julian Brooke, Milan Tofiloski, and Maite Taboada.2009.
Cross-linguistic sentiment analysis: From en-glish to spanish.
In Proceedings of RANLP 2009,Borovets, Bulgaria.Carmine Cesarano, Antonio Picariello, Diego Refor-giato, and V.S.
Subrahmanian.
2007.
The oasys 2.0opinion analysis system: A demo.
In Proceedings ofthe International Conference on Weblogs and SocialMedia (ICWSM).Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5th Con-ference on Language Resources and Evaluation(LREC?06), pages 417?422, Geneva, IT.Soo-Min Kim and Eduard Hovy.
2006.
Identifyingand analyzing judgment opinions.
In Proceedingsof the Human Language Technology Conference ofthe NAACL (HLT/NAACL?06), pages 200?207, NewYork, USA.Jungi Kim, Hun-Young Jung, Sang-Hyob Nam, YehaLee, and Jong-Hyeok Lee.
2009.
Found in trans-lation: Conveying subjectivity of a lexicon of onelanguage into another using a bilingual dictionaryand a link analysis algorithm.
In ICCPOL ?09: Pro-ceedings of the 22nd International Conference onComputer Processing of Oriental Languages.
Lan-guage Technology for the Knowledge-based Econ-omy, pages 112?121, Berlin, Heidelberg.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2007.
Learning multilingual subjective language viacross-lingual projections.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics (ACL?07), pages 976?983, Prague, CZ.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 79?86.Ellen Riloff and Janyce Wiebe.
2003.
Learning ex-traction patterns for subjective expressions.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP).Xiaojun Wan.
2008.
Using bilingual knowledge andensemble techniques for unsupervised Chinese sen-timent analysis.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 553?561, Honolulu, Hawaii, Oc-tober.
Association for Computational Linguistics.Xiaojun Wan.
2009.
Co-training for cross-lingual sen-timent classification.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Nat-ural Language Processing of the AFNLP, pages235?243, Suntec, Singapore, August.
Associationfor Computational Linguistics.Janyce Wiebe and Ellen Riloff.
2005.
Creating subjec-tive and objective sentence classifiers from unanno-tated texts.
In Proceedings of the 6th InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CICLing-2005), pages 486?497, Mexico City, Mexico.Janyce Wiebe, E. Breck, Christopher Buckley, ClaireCardie, P. Davis, B. Fraser, Diane Litman, D. Pierce,Ellen Riloff, Theresa Wilson, D. Day, and MarkMaybury.
2003.
Recognizing and organizing opin-ions expressed in the world press.
In Proceedingsof the 2003 AAAI Spring Symposium on New Direc-tions in Question Answering.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing(HLT-EMNLP?05), pages 347?354, Vancouver, CA.603
