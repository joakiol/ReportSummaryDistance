Improving Language Model Size Reduction using Better PruningCriteriaJianfeng GaoMicrosoft Research, AsiaBeijing, 100080, Chinajfgao@microsoft.comMin Zhang1State Key Lab of Intelligent Tech & Sys.Computer Science & Technology Dept.Tsinghua University, China1 This work was done while Zhang was working at Microsoft Research Asia as a visiting student.AbstractReducing language model (LM) size is acritical issue when applying a LM torealistic applications which have memoryconstraints.
In this paper, three measuresare studied for the purpose of LMpruning.
They are probability, rank, andentropy.
We evaluated the performance ofthe three pruning criteria in a realapplication of Chinese text input in termsof character error rate (CER).
We firstpresent an empirical comparison, showingthat rank performs the best in most cases.We also show that the high-performanceof rank lies in its strong correlation witherror rate.
We then present a novelmethod of combining two criteria inmodel pruning.
Experimental resultsshow that the combined criterionconsistently leads to smaller models thanthe models pruned using either of thecriteria separately, at the same CER.1 IntroductionBackoff n-gram models for applications such aslarge vocabulary speech recognition are typicallytrained on very large text corpora.
Anuncompressed LM is usually too large for practicaluse since all realistic applications have memoryconstraints.
Therefore, LM pruning techniques areused to produce the smallest model while keepingthe performance loss as small as possible.Research on backoff n-gram model pruning hasbeen focused on the development of the pruningcriterion, which is used to estimate the performanceloss of the pruned model.
The traditional countcutoff method (Jelinek, 1990) used a pruningcriterion based on absolute frequency while recentresearch has shown that better pruning criteria canbe developed based on more sophisticated measuressuch as perplexity.In this paper, we study three measures forpruning backoff n-gram models.
They areprobability, rank and entropy.
We evaluated theperformance of the three pruning criteria in a realapplication of Chinese text input (Gao et al, 2002)through CER.
We first present an empiricalcomparison, showing that rank performs the best inmost cases.
We also show that the high-performanceof rank lies in its strong correlation with error rate.We then present a novel method of combining twopruning criteria in model pruning.
Our results showthat the combined criterion consistently leads tosmaller models than the models pruned using eitherof the criteria separately.
In particular, thecombination of rank and entropy achieves thesmallest models at a given CER.The rest of the paper is structured as follows:Section 2 discusses briefly the related work onbackoff n-gram pruning.
Section 3 describes indetail several pruning criteria.
Section 4 presents anempirical comparison of pruning criteria using aChinese text input system.
Section 5 proposes ourmethod of combining two criteria in model pruning.Section 6 presents conclusions and our future work.2 Related WorkN-gram models predict the next word given theprevious n-1 words by estimating the conditionalprobability P(wn|w1?wn-1).
In practice, n is usuallyset to 2 (bigram), or 3 (trigram).
For simplicity, werestrict our discussion to bigrams P(wn| wn-1), but ourapproaches can be extended to any n-gram.The bigram probabilities are estimated from thetraining data by maximum likelihood estimation(MLE).
However, the intrinsic problem of MLE isComputational Linguistics (ACL), Philadelphia, July 2002, pp.
176-182.Proceedings of the 40th Annual Meeting of the Association forthat of data sparseness: MLE leads to zero-valueprobabilities for unseen bigrams.
To deal with thisproblem, Katz (1987) proposed a backoff scheme.He estimates the probability of an unseen bigram byutilizing unigram estimates as follows???
>=????
otherwisewPwwwcwwPwwPiiiiiidii )()(0),()|()|(1111 ?
, (1)where c(wi-1wi) is the frequency of word pair (wi-1wi)in the training data, Pd represents the Good-Turingdiscounted estimate for seen word pairs, and ?
(wi-1)is a normalization factor.Due to the memory limitation in realisticapplications, only a finite set of word pairs haveconditional probability P(wi|wi-1) explicitlyrepresented in the model.
The remaining word pairsare assigned a probability by backoff (i.e.
unigramestimates).
The goal of bigram pruning is to removeuncommon explicit bigram estimates P(wi|wi-1) fromthe model to reduce the number of parameters whileminimizing the performance loss.The research on backoff n-gram model pruningcan be formulated as the definition of the pruningcriterion, which is used to estimate the performanceloss of the pruned model.
Given the pruningcriterion, a simple thresholding algorithm forpruning bigram models can be described as follows:1.
Select a threshold ?.2.
Compute the performance loss due topruning each bigram individually using thepruning criterion.3.
Remove all bigrams with performance lossless than ?.4.
Re-compute backoff weights.Figure 1: Thresholding algorithm for bigrampruningThe algorithm in Figure 1 together with severalpruning criteria has been studied previously(Seymore and Rosenfeld, 1996; Stolcke, 1998; Gaoand Lee, 2000; etc).
A comparative study of thesetechniques is presented in (Goodman and Gao,2000).In this paper, three pruning criteria will bestudied: probability, rank, and entropy.
Probabilityserves as the baseline pruning criterion.
It is derivedfrom perplexity which has been widely used as a LMevaluation measure.
Rank and entropy have beenpreviously used as a metric for LM evaluation in(Clarkson and Robinson, 2001).
In the current paper,these two measures will be studied for the purpose ofbackoff n-gram model pruning.
In the next section,we will describe how pruning criteria are developedusing these two measures.3 Pruning CriteriaIn this section, we describe the three pruning criteriawe evaluated.
They are derived from LM evaluationmeasures including perplexity, rank, and entropy.The goal of the pruning criterion is to estimate theperformance loss due to pruning each bigramindividually.
Therefore, we represent the pruningcriterion as a loss function, denoted by LF below.3.1 ProbabilityThe probability pruning criterion is derived fromperplexity.
The perplexity is defined as?= = ?
?Niii wwPNPP 1 1)|(log12  (2)where N is the size of the test data.
The perplexitycan be roughly interpreted as the expected branchingfactor of the test document when presented to theLM.
It is expected that lower perplexities arecorrelated with lower error rates.The method of pruning bigram models usingprobability can be described as follows: all bigramsthat change perplexity by less than a threshold areremoved from the model.
In this study, we assumethat the change in model perplexity of the LM can beexpressed in terms of a weighted difference of thelog probability estimate before and after pruning abigram.
The loss function of probability LFprobability,is then defined as)]|(log)|(')[log( 111 ???
??
iiiiii wwPwwPwwP , (3)where P(.|.)
denotes the conditional probabilitiesassigned by the original model, P?(.|.)
denotes theprobabilities in the pruned model, and P(wi-1 wi) is asmoothed probability estimate in the original model.We notice that LFprobability of Equation (3) is verysimilar to that proposed by Seymore and Rosenfeld(1996), where the loss function is)]|(log)|(')[log( 111 ???
??
iiiiii wwPwwPwwN .Here N(wi-1wi) is the discounted frequency thatbigram wi-1wi was observed in training.
N(wi-1wi) isconceptually identical to P(wi-1 wi) in Equation (3).From Equations (2) and (3), we can see that lowerLFprobability is strongly correlated with lowerperplexity.
However, we found that LFprobability issuboptimal as a pruning criterion, evaluated on CERin our experiments.
We assume that it is largely dueto the deficiency of perplexity as a LM performancemeasure.Although perplexity is widely used due to itssimplicity and efficiency, recent researches showthat its correlation with error rate is not as strong asonce thought.
Clarkson and Robinson (2001)analyzed the reason behind it and concluded that thecalculation of perplexity is based solely on theprobabilities of words contained within the test text,so it disregards the probabilities of alternativewords, which will be competing with the correctword (referred to as target word below) within thedecoder (e.g.
in a speech recognition system).Therefore, they used other measures such as rankand entropy for LM evaluation.
These measures arebased on the probability distribution over the wholevocabulary.
That is, if the test text is w1n, thenperplexity is based on the values of P(wi |wi-1), andthe new measures will be based on the values ofP(w|wi-1) for all w in the vocabulary.
Since thesemeasures take into account the probabilitydistribution over all competing words (including thetarget word) within the decoder, they are, hopefully,better correlated with error rate, and expected toevaluate LMs more precisely than perplexity.3.2 RankThe rank of the target word w is defined as theword?s position in an ordered list of the bigramprobabilities P(w|wi-1) where w?V, and V is thevocabulary.
Thus the most likely word (within thedecoder at a certain time point) has the rank of one,and the least likely has rank |V|, where |V| is thevocabulary size.We propose to use rank for pruning as follows: allbigrams that change rank by less than a thresholdafter pruning are removed from the model.
Thecorresponding loss function LFrank is defined as?????
?+?1)}|(log])|(){log[( 111ii wwiiiiii wwRkwwRwwp (4)where R(.|.)
denotes the rank of the observed bigramP(wi|wi-1) in the list of bigram probabilities P(w|wi-1)where w?V, before pruning, R?(.|.)
is the new rankof it after pruning, and the summation is over allword pairs (wi-1wi).
k is a constant to assure that0)|(log])|(log[ 11 ??+?
??
iiii wwRkwwR .
k is set to0.1 in our experiments.3.3 EntropyGiven a bigram model, the entropy H of theprobability distribution over the vocabulary V isgenerally given by?
?= =Vj ijiji wwPwwPwH 1 )|(log)|()( .We propose to use entropy for pruning as follows:all bigrams that change entropy by less than athreshold after pruning are removed from the model.The corresponding loss function LFentropy is definedas?
???
= ?
?Ni ii wHwHN 1 11 ))()((1  (5)where H is the entropy before pruning given historywi-1, H?
is the new entropy after pruning, and N is thesize of the test data.The entropy-based pruning is conceptuallysimilar to the pruning method proposed in (Stolcke,1998).
Stolcke used the Kullback-Leibler divergencebetween the pruned and un-pruned modelprobability distribution in a given context over theentire vocabulary.
In particular, the increase inrelative entropy from pruning a bigram is computedby?????
?
?ii wwiiiiii wwPwwPwwP1)]|(log)|(')[log( 111 ,where the summation is over all word pairs (wi-1wi).4 Empirical ComparisonWe evaluated the pruning criteria introduced in theprevious section on a realistic application, Chinesetext input.
In this application, a string of Pinyin(phonetic alphabet) is converted into Chinesecharacters, which is the standard way of inputtingtext on Chinese computers.
This is a similar problemto speech recognition except that it does not includeacoustic ambiguity.
We measure performance interms of character error rate (CER), which is thenumber of characters wrongly converted from thePinyin string divided by the number of characters inthe correct transcript.
The role of the languagemodel is, for all possible word strings that match thetyped Pinyin string, to select the word string with thehighest language model probability.The training data we used is a balanced corpus ofapproximately 26 million characters from variousdomains of text such as newspapers, novels,manuals, etc.
The test data consists of half a millioncharacters that have been proofread and balancedamong domain, style and time.The back-off bigram models we generated in thisstudy are character-based models.
That is, thetraining and test corpora are not word-segmented.As a result, the lexicon we used contains 7871 singleChinese characters only.
While word-based n-grammodels are widely applied, we used character-basedmodels for two reasons.
First, pilot experimentsshow that the results of word-based andcharacter-based models are qualitatively verysimilar.
More importantly, because we need to builda very large number of models in our experiments asshown below, character-based models are muchmore efficient, both for training and for decoding.We used the absolute discount smoothing methodfor model training.None of the pruning techniques we consider areloss-less.
Therefore, whenever we compare pruningcriteria, we do so by comparing the size reduction ofthe pruning criteria at the same CER.Figure 2 shows how the CER varies with thebigram numbers in the models.
For comparison, wealso include in Figure 2 the results using count cutoffpruning.
We can see that CER decreases as we keepmore and more bigrams in the model.
A steepercurve indicates a better pruning criterion.The main result to notice here is that therank-based pruning achieves consistently the bestperformance among all of them over a wide range ofCER values, producing models that are at 55-85% ofthe size of the probability-based pruned models withthe same CER.
An example of the detailedcomparison results is shown in Table 1, where theCER is 13.8% and the value of cutoff is 1.
The lastcolumn of Table 1 shows the relative model sizeswith respect to the probability-based pruned modelwith the CER 13.8%.Another interesting result is the goodperformance of count cutoff, which is almostoverlapping with probability-based pruning at largermodel sizes 2 .
The entropy-based pruningunfortunately, achieved the worst performance.13.613.713.813.914.014.13.E+05 4.E+05 5.E+05 6.E+05 7.E+05 8.E+05 9.E+05# of bigrams in the modelaverageerrorraterankprobentropycount cutoffFigure 2: Comparison of pruning criteriaTable 1: LM size comparison at CER 13.8%criterion # of bigram size (MB) % of probprobability 774483 6.1 100.0%cutoff (=1) 707088 5.6 91.8%entropy 1167699 9.3 152.5%rank 512339 4.1 67.2%2 The result is consistent with that reported in (Goodmanand Gao, 2000), where an explanation was offered.We assume that the superior performance ofrank-based pruning lies in the fact that rank (actingas a LM evaluation measure) has better correlationwith CER.
Clarkson and Robinson (2001) estimatedthe correlation between LM evaluation measuresand word error rate in a speech recognition system.The related part of their results to our study areshown in Table 2, where r is the Pearsonproduct-moment correlation coefficient, rs is theSpearman rank-order correlation coefficient, and Tis the Kendall rank-order correlation coefficient.Table 2: Correlation of LM evaluation measureswith word error rates (Clarkson and Robinson,2001)r rs TMean log rank 0.967 0.957 0.846Perplexity 0.955 0.955 0.840Mean entropy -0.799 -0.792 -0.602Table 2 indicates that the mean log rank (i.e.related to the pruning criterion of rank we used) hasthe best correlation with word error rate, followed bythe perplexity (i.e.
related to the pruning criterion ofprobability we used) and the mean entropy (i.e.related to the pruning criterion of entropy we used),which support our test results.
We can conclude thatthe LM evaluation measures which are bettercorrelated with error rate lead to better pruningcriteria.5 Combining Two CriteriaWe now investigate methods of combining pruningcriteria described above.
We begin by examining theoverlap of the bigrams pruned by two differentcriteria to investigate which might usefully becombined.
Then the thresholding pruning algorithmdescribed in Figure 1 is modified so as to make useof two pruning criteria simultaneously.
The problemhere is how to find the optimal settings of thepruning threshold pair (each for one pruningcriterion) for different model sizes.
We show how anoptimal function which defines the optimal settingsof the threshold pairs is efficiently established usingour techniques.5.1 OverlapFrom the abovementioned three pruning criteria, weinvestigated the overlap of the bigrams pruned by apair of criteria.
There are three criteria pairs.
Theoverlap results are shown in Figure 3.We can see that the percentage of the number ofbigrams pruned by both criteria seems to increase asthe model size decreases, but all criterion-pairs haveoverlaps much lower than 100%.
In particular, wefind that the average overlap between probabilityand entropy is approximately 71%, which is thebiggest among the three pairs.
The pruning methodbased on the criteria of rank and entropy has thesmallest average overlap of 63.6%.
The resultssuggest that we might be able to obtainimprovements by combining these two criteria forbigram pruning since the information provided bythese criteria is, in some sense, complementary.0.E+002.E+054.E+056.E+058.E+051.E+060.E+00 2.E+05 4.E+05 6.E+05 8.E+05 1.E+06# of pruned bigrams# of overlaped bigramsprob+rankprob+entropyrank+entropy100% overlapFigure 3: Overlap of selected bigrams betweencriterion pairs5.2 Pruning by two criteriaIn order to prune a bigram model based on twocriteria simultaneously, we modified thethresholding pruning algorithm described in Figure1.
Let lfi be the value of the performance lossestimated by the loss function LFi, ?i be thethreshold defined by the pruning criterion Ci.
Themodified thresholding pruning algorithm can bedescribed as follows:1.
Select a setting of threshold pair (?1?2)2.
Compute the values of performance loss lf1and lf2 due to pruning each bigramindividually using the two pruning criteriaC1 and C2, respectively.3.
Remove all bigrams with performance losslf1 less than ?1, and lf2 less than ?2.4.
Re-compute backoff weights.Figure 4: Modified thresholding algorithm forbigram pruningNow, the remaining problem is how to find theoptimal settings of the pruning threshold pair fordifferent model sizes.
This seems to be a verytedious task since for each model size, a largenumber of settings (?1?2) have to be tried for findingthe optimal ones.
Therefore, we convert the problemto the following one: How to find an optimalfunction ?2=f(?1) by which the optimal threshold ?2is defined for each threshold ?1.
The function can belearned by pilot experiments described below.
Giventwo thresholds ?1 and ?2 of pruning criteria C1 andC2, we try a large number of values of ?1, ?2, andbuild a large number of models pruned using thealgorithm described in Figure 4.
For each modelsize, we find an optimal setting of the thresholdsetting (?1?2) which results in a pruned model withthe lowest CER.
Finally, all these optimal thresholdsettings serve as the sample data, from which theoptimal function can be learned.
We found that inpilot experiments, a relatively small set of samplesettings is enough to generate the function which isclose enough to the optimal one.
This allows us torelatively quickly search through what wouldotherwise be an overwhelmingly large search space.5.3 ResultsWe used the same training data described in Section4 for bigram model training.
We divided the test setdescribed in Section 4 into two non-overlappedsubsets.
We performed testing on one subsetcontaining 80% of the test set.
We performedoptimal function learning using the remaining 20%of the test set (referred to as held-out data below).Take the combination of rank and entropy as anexample.
An uncompressed bigram model was firstbuilt using all training data.
We then built a verylarge number of pruned bigram models usingdifferent threshold setting (?
rank ?entropy), where thevalues ?
rank, ?entropy ?
[3E-12, 3E-6].
By evaluatingpruned models on the held-out data, optimal settingscan be found.
Some sample settings are shown inTable 3.Table 3: Sample optimal parameter settings forcombination of criteria based on rank and entropy# bigrams ?
rank ?entropy137987 8.00E-07 8.00E-09196809 3.00E-07 8.00E-09200294 3.00E-07 5.00E-09274434 3.00E-07 5.00E-10304619 8.00E-08 8.00E-09394300 5.00E-08 3.00E-10443695 3.00E-08 3.00E-10570907 8.00E-09 3.00E-09669051 5.00E-09 5.00E-10890664 5.00E-11 3.00E-10892214 5.00E-12 3.00E-10892257 3.00E-12 3.00E-10In experiments, we found that a linear regressionmodel of Equation (6) is powerful enough to learn afunction which is close enough to the optimal one.21 )log()log( ????
+?= rankentropy  (6)Here ?1 and ?2 are coefficients estimated from thesample settings.
Optimal functions of the other twothreshold-pair settings (?
rank?probability) and (?probability?entropy) are obtained similarly.
They areshown in Table 4.Table 4.
Optimal functions5.6)log(3.0)log( +?= rankentropy ?
?2.6)log( =yprobabilit?
, for any rank?5.3)log(7.0)log( +?= yprobabilitentropy ?
?In Figure 5, we present the results using modelspruned with all three threshold-pairs defined by thefunctions in Table 4.
As we expected, in all threecases, using a combination of two pruning criteriaachieves consistently better performance than usingeither of the criteria separately.
In particular, usingthe combination of rank and entropy, we obtainedthe best models over a wide large of CER values.
Itcorresponds to a significant size reduction of15-54% over the probability-based LM pruning atthe same CER.
An example of the detailedcomparison results is shown in Table 5.Table 5: LM size comparison at CER 13.8%Criterion # of bigram size (MB) % of probProb 1036627 8.2 100.0%Entropy 1291000 10.2 124.4%Rank 643411 5.1 62.2%Prob + entropy 542124 4.28 52.2%Prob + rank 579115 4.57 55.7%rank + entropy 538252 4.25 51.9%There are two reasons for the superiorperformance of the combination of rank and entropy.First, the rank-based pruning achieves very goodperformance as described in Section 4.
Second, asshown in Section 5.1, there is a relatively smalloverlap between the bigrams chosen by these twopruning criteria, thus big improvement can beachieved through the combination.6 ConclusionThe research on backoff n-gram pruning has beenfocused on the development of the pruning criterion,which is used to estimate the performance loss of thepruned model.This paper explores several pruning criteria forbackoff n-gram model size reduction.
Besides thewidely used probability, two new pruning criteriahave been developed based on rank and entropy.
Wehave performed an empirical comparison of thesepruning criteria.
We also presented a thresholdingalgorithm for model pruning, in which two pruningcriteria can be used simultaneously.
Finally, wedescribed our techniques of finding the optimalsetting of the threshold pair given a specific modelsize.We have shown several interesting results.
Theyinclude the confirmation of the estimation that themeasures which are better correlated with CER forLM evaluation leads to better pruning criteria.
Ourexperiments show that rank, which has the bestcorrelation with CER, achieves the best performancewhen there is only one criterion used in bigrammodel pruning.
We then show empirically that theoverlap of the bigrams pruned by different criteria isrelatively low.
This indicates that we might obtainimprovements through a combination of two criteriafor bigram pruning since the information providedby these criteria is complementary.
This hypothesisis confirmed by our experiments.
Results show thatusing two pruning criteria simultaneously achieves13.613.713.813.914.014.114.23.E+05 5.E+05 7.E+05 9.E+05 1.E+06# of bigrams in the modelaverageerrorraterankprobentropyrank+probrank+entropyprob+entropyFigure 5: Comparison of combined pruningcriterion performancebetter bigram models than using either of the criteriaseparately.
In particular, the combination of rankand entropy achieves the smallest bigram models atthe same CER.For our future work, more experiments will beperformed on other language models such asword-based bigram and trigram for Chinese andEnglish.
More pruning criteria and theircombinations will be investigated as well.AcknowledgementsThe authors wish to thank Ashley Chang, JoshuaGoodman, Chang-Ning Huang, Hang Li, HisamiSuzuki and Ming Zhou for suggestions andcomments on a preliminary draft of this paper.Thanks also to three anonymous reviews forvaluable and insightful comments.ReferencesClarkson, P. and Robinson, T. (2001), Improvedlanguage modeling through better languagemodel evaluation measures, Computer Speechand  Language, 15:39-53, 2001.Gao, J. and Lee K.F (2000).
Distribution-basedpruning of backoff language models, 38th Annualmeetings of the Association for ComputationalLinguistics (ACL?00), HongKong, 2000.Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002).Toward a unified approach to statistical languagemodeling for Chinese.
ACM Transactions onAsian Language Information Processing, Vol.
1,No.
1, pp 3-33.
Draft available fromhttp://www.research.microsoft.com/~jfgaoGoodman, J. and Gao, J.
(2000) Language modelsize reduction by pruning and clustering,ICSLP-2000, International Conference onSpoken Language Processing, Beijing, October16-20, 2000.Jelinek, F. (1990).
Self-organized languagemodeling for speech recognition.
In Readings inSpeech Recognition, A. Waibel and K. F.
Lee,eds., Morgan-Kaufmann, San Mateo, CA, pp.450-506.Katz, S. M., (1987).
Estimation of probabilities fromsparse data for other language component of aspeech recognizer.
IEEE transactions onAcoustics, Speech and Signal Processing,35(3):400-401, 1987.Rosenfeld, R. (1996).
A maximum entropy approachto adaptive statistical language modeling.Computer, Speech and Language, vol.
10, pp.187-- 228, 1996.Seymore, K., and Rosenfeld, R. (1996).
Scalablebackoff language models.
Proc.
ICSLP, Vol.
1.,pp.232-235, Philadelphia, 1996Stolcke, A.
(1998).
Entropy-based Pruning ofBackoff Language Models.
Proc.
DARPA NewsTranscription and Understanding Workshop,1998, pp.
270-274, Lansdowne, VA.
