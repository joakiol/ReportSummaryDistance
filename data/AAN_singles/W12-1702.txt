In: R. Levy & D. Reitter (Eds.
), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 11?20,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsSemi-supervised learning for automatic conceptual property extractionColin KellyComputer LaboratoryUniversity of CambridgeCambridge, CB3 0FD, UKcolin.kelly@cl.cam.ac.ukBarry DevereuxCentre for Speech,Language, and the BrainUniversity of CambridgeCambridge, CB2 3EB, UKbarry@csl.psychol.cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of CambridgeCambridge, CB3 0FD, UKanna.korhonen@cl.cam.ac.ukAbstractFor a given concrete noun concept, humansare usually able to cite properties (e.g., ele-phant is animal, car has wheels) of that con-cept; cognitive psychologists have theorisedthat such properties are fundamental to un-derstanding the abstract mental representationof concepts in the brain.
Consequently, theability to automatically extract such propertieswould be of enormous benefit to the field ofexperimental psychology.
This paper investi-gates the use of semi-supervised learning andsupport vector machines to automatically ex-tract concept-relation-feature triples from twolarge corpora (Wikipedia and UKWAC) forconcrete noun concepts.
Previous approacheshave relied on manually-generated rules andhand-crafted resources such as WordNet; ourmethod requires neither yet achieves bet-ter performance than these prior approaches,measured both by comparison with a propertynorm-derived gold standard as well as directhuman evaluation.
Our technique performsparticularly well on extracting features rele-vant to a given concept, and suggests a numberof promising areas for future focus.1 IntroductionThe representation of concrete concepts (e.g., car,banana, spanner) in the human brain has long beenan important area of investigation for cognitive psy-chologists.
Recent theories of this mental repre-sentation have proposed a componential, property-based and distributed model of conceptual knowl-edge (e.g., Farah and McClelland (1991), Randall etal.
(2004), Tyler et al (2000)).In order to empirically test these cognitive the-ories, researchers have moved towards employingreal-world knowledge in their experiments.
Thisknowledge has usually been procured from human-derived lists of properties taken from property norm-ing studies (Garrard et al, 2001; McRae et al,2005).
In such studies, human participants areasked to describe and note properties of a givenconcept (e.g., has shell for turtle).
Synonymousresponses are grouped together as a single prop-erty and those meeting a certain minimum response-frequency threshold are taken as valid properties.The most wide-ranging study to date was that con-ducted by McRae et al (2005): some sample prop-erties from this set are in Table 1.As others have noted (Murphy, 2002; McRae etal., 2005), property norming studies are prone to anumber of deficiencies.
One such weakness is theincongruity of shared properties across even highly-related concepts: human respondents exhibit a lackof consistency when listing properties that are com-mon to many similar concepts.
For example, whilehas legs is listed as a property of crocodile in theMcRae norms, it is absent as a property of alliga-tor.
A related issue is the non-comprehensive natureof the generated norms ?
although they may coverthe most salient properties for a given concept, theyare unlikely to comprise all of a concept?s properties(e.g., has heart does not appear as a property of anyof the 92 animal concepts).Our research aims to use NLP techniques to cre-ate a system able to emulate the output of suchstudies, and overcome some of the aforementionedweaknesses.
Our proposed system begins by search-ing dependency-parsed corpora for those sentencescontaining concept and feature terms which arealso found in a McRae norm-derived training setof properties.
For these sentences, the systemgenerates grammatical relation/part-of-speech struc-tural attributes and applies support vector machines(SVMs) to learn sets of attributes likely to indicatethe instantiation of a property in a sentence.
These11turtle bowlhas a shell 25 is round 19lays eggs 16 used for eating 12swims 15 used for soup 11is green 14 used for food 11lives in water 14 used for liquids 10is slow 13 used for eating cereal 10an animal 11 made of plastic 8walks 10 used for holding things 7walks slowly 10 is curved 7has 4 legs 9 found in kitchens 7Table 1: Top ten properties from McRae norms with pro-duction frequencies for turtle and bowl.learned patterns of salient attributes are finally ap-plied to a corpus to derive new properties for unseenconcepts.Our task is a challenging one: the properties weseek are extremely diverse in their form.
They rangefrom the simple (e.g., banana is yellow) to the com-plex (e.g., bayonet found at the end of a gun).
Al-though the properties can broadly be divided intoa number of categories (encyclopedic, taxonomic,functional, etc) there is not a great deal of regular-ity in the nature of the properties a given noun willlikely possess: it is highly concept-dependent.Furthermore, we hope to derive these propertiesfrom corpora, with the assumption that these prop-erties will manifest themselves therein.
Indeed, An-drews et al (2005) discuss a theory of human knowl-edge which relies on a combination of both dis-tributional (i.e., derived from spoken and writtenlanguage) and experiential data (i.e., that derivedfrom our interactions with the real world), claimingthat the necessary contribution of each data-type fora comprehensive human semantic representation isnon-trivial.
Finally, there are difficulties associatedwith evaluating our system?s output directly againsta set of human-generated property norms: we dis-cuss these in further detail later.Given their provenance, the properties found inproperty norms are free-form.
To simplify our taskwe apply a more rigid representation to the proper-ties we already have and to those we aim to seek.
Wedelineate each property into a concept relation fea-ture triple (see Section 2.2) and our task becomesone of finding valid relation feature pairs given a par-ticular concept.
This recoding renders our task morewell-defined and makes evaluation of our methodreptile1NNSinclude2VBPspecies0INof3INturtle5NNdobjfive1DTncmod ncmoddobjmarine0NNPncmod ncsubjFigure 1: C&C-derived GR-POS graph for the sentenceMarine reptiles include five species of turtle.more comparable to previous and related work.Having framed our task in this way, there is anobvious parallel with relation extraction: both ne-cessitate the selection/classification of relationshipsbetween individual entities (in our case, betweenconcept and feature).
Hearst (1992) was the firstto propose a pattern-based approach to this task us-ing lexico-syntactic patterns to automatically extracthyponyms and this technique has frequently beenused for ontology learning.
For example, Pantel andPennacchiotti (2008) linked instantiations of a set ofsemantic relations into existing semantic ontologiesand Davidov et al (2007) employed seed conceptsfrom a given semantic class to discover relationsshared by concepts in that class.Our task is more complex than classic relation ex-traction for two main reasons: 1) the relations whichwe aim to extract are not limited to a small set ofjust a few well-defined relations (e.g., is-a and part-of) nor to the relations of a specific semantic class(e.g., capital-is for countries).
Indeed the relationscan be as many and diverse as the concepts them-selves (e.g., each concept could possess a uniqueand distinguishing relation and feature).
2) We areattempting to simultaneously extract two pieces ofinformation: features of the concept and those fea-tures?
defining relationship with the concept, butonly those relations and features which would beclassified as ?common-sense?, something which iseasy for humans to recognise but difficult (if not im-possible) to describe rigorously or formally.There has recently been work on the automatic ex-12traction of binary relations that scale to a web cor-pus, for example the ReVerb (Etzioni et al, 2011)and WOE (Wu and Weld, 2010) systems.
Thesesystems are designed to extract legitimate relationsfrom a given sentence.
In contrast, our aim is to cap-ture more general relationships which are ?common-sense?
; just because an extracted relation is correctin a given context does not automatically make ittrue in general.
Previous reasoned approaches to ourtask have taken their lead from Hearst and her suc-cessors, employing manually-created rulesets to ex-tract such properties from corpora (e.g., Baroni et al(2009), Devereux et al (2010), and our comparisonsystem (Kelly et al, 2010)).
Baroni et al extract re-lational information in the form of ?type-sketches?,which give an approximate, implicit description ofthe relationship whereas we are aiming to extractexplicit relations between the target concept and itscorresponding features.
Devereux et al and Kellyet al have attempted this, but both employ WordNet(Fellbaum, 1998) to extract semantic relatedness in-formation.We use semi-supervised learning as it offers aflexible technique of harnessing small amounts oflabelled data to derive information from unlabelleddatasets/corpora and allows us to guide the extrac-tion towards our desired ?common-sense?
output.We chose SVMs as they have been used for a va-riety of tasks in NLP (e.g., Joachims et al (1998),Gime?nez and Marquez (2004)).
We will demon-strate that our system?s performance exceeds that ofKelly et al (2010) and Etzioni et al (2011).
It is, asfar as we are aware, the first work to employ semi-supervised learning for this task.2 MethodWe will use SVMs to learn lexico-syntactic pat-terns in our corpora corresponding to known prop-erties in order to find new ones.
Training an SVMrequires a labelled training set.
To generate thisset we harness our already-known concepts/features(and their relationships) from the McRae norms tofind instantiations of said relationships within ourcorpora.
We use parsed sentence information fromour corpora to create a set of attributes describingeach relationship, our learning patterns.
In doingso, we are assuming that across sentences in ourcorpora containing a concept/feature pair found inthe McRae norms, there will be a set of consistentlexico-syntactic patterns which indicate the same re-lationship as that linking the pair in the norms.Thus we iterate over our chosen corpora, parsingeach concept-containing sentence to yield grammat-ical relation (GR) and part-of-speech (POS) infor-mation from which we can create a GR-POS graphrelating the two.
Then for each triple, we find any/allpaths through the graph which link the concept to itsfeature and use the corresponding relation to labelthis path.
We collect descriptive information aboutthe path in the form of attributes describing it (e.g.,path nodes, labels, length) to create a training patternspecific to that concept relation feature triple andsentence.
It is these lists of attributes (and their rela-tion labels) which we employ as the labelled trainingset and as input for our SVM.2.1 CorporaWe employ two corpora for our experiments:Wikipedia and the UKWAC corpus (Ferraresi et al,2008).
These are both publicly available and web-based: the former a source of encyclopedic infor-mation and the latter a source of general text.
OurWikipedia corpus is based on a Sep 2009 versionof English-language Wikipedia and contains around1.84 million articles (>1bn words).
Our UKWACcorpus is an English-language corpus (>2bn words)obtained by crawling the .uk internet domain.2.2 Training dataOur experiments use a British-English version ofthe McRae norms (see Taylor et al (2011) for de-tails).
We needed to recode the free-form McRaeproperties into relation-classes and features whichwould be usable for our learning algorithm.
Aswe will be matching the features from these prop-erties with individual words in the training corpusit was essential that the features we generated con-tained only one lemmatised word.
In contrast, therelations were merely labels for the relationship de-scribed (they did not need to occur in the sentenceswe were training from) and therefore needed onlyto be single-string relations.
This allowed preposi-tional verbs as distinct relations, something whichhas not been attempted in previous work yet can besemantically significant (e.g., the relations used-in,used-for and used-by have dissimilar meanings).We applied the following sequential multi-step13process to our set of free-form properties to distillthem to triples of the form concept relation feature,where relation can be a multi-word string and featureis a single word:1.
Translation of implicit properties to their correct re-lations (e.g., pig an animal ?
pig is an animal).2.
Removal of indefinite and definite articles.3.
Behavioural properties become ?does?
properties(e.g., turtle beh eats ?
turtle does eats).4.
Negative properties given their own relation classes(e.g., turkey does cannot fly ?
turkey doesnt fly).5.
All numbers are translated to named cardinals (e.g.,spider has 8 legs ?
spider has eight legs).6.
Some of the norms already contained synonymousterms: these were split into separate triples for eachsynonym (e.g., pepper tastes hot/spicy ?
peppertastes hot and pepper tastes spicy).7.
Prepositional verbs were translated to one-word,hyphenated strings (e.g., made of ?
made-of ).8.
Properties with present participles as the penulti-mate word were split into one including the verb asthe feature and one including it in the relation (e.g.,envelope used for sending letters ?
envelope used-for-sending letters and envelope used-for sending).9.
Any remaining multi-word properties were splitwith the first term after the concept acting as therelation (e.g., bull has ring in its nose ?
bull hasring, bull has in, bull has its and bull has nose).10.
All remaining stop-words were removed; propertiesending in stop-words (e.g., bull has in and bull hasits) were removed completely.This yielded 7,518 property-triples with 254 distinctrelations and an average of 14.7 triples per concept.2.3 ParsingWe parsed both corpora using the C&C parser(Clark and Curran, 2007) as we employ both GRand POS information in our learning method.
To ac-celerate this stage, we process only sentences con-taining a form (e.g., singular/plural) of one of ourtraining/testing concepts.
We lemmatise each wordusing the WordNet NLTK lemmatiser (Bird, 2006).Parsing our corpora yields around 10Gb and 12Gbof data for UKWAC and Wikipedia respectively.The C&C dependency parse output contains, fora given sentence, a set of GRs forming an acyclicgraph whose nodes correspond to words from thesentence, with each node also labelled with the POSof that word.
Thus the GR-POS graph interrelates alllexical, POS and GR information for the entire sen-tence.
It is therefore possible to construct a GR-POSgraph rooted at our target term (the concept in ques-tion), with POS-labelled words as nodes, and edgeslabelled with GRs linking the nodes to one another.An example graph can be seen in Figure 1.2.4 Support vector machinesWe use SVMs (Cortes and Vapnik, 1995) for ourexperiments as they have been widely used in NLPand their properties are well-understood, showinggood performance on classification tasks (Meyer etal., 2003).
In their canonical form, SVMs are non-probabilistic binary linear classifiers which take a setof input data and predict, for each given input, whichof two possible classes it corresponds to.There are more than two possible relation-labelsto learn for our input patterns, so ours is a multi-classclassification task.
For our experiments we use theSVM Light Multiclass (v. 2.20) software (Joachims,1999) which applies the fixed-point SVM algorithmdescribed by Crammer and Singer (2002) to solvemulti-class problem instances.
Joachims?
softwarehas been widely used to implement SVMs (Vinok-ourov et al, 2003; Godbole et al, 2002).2.5 Attribute selectionPrevious techniques for our task have made use oflexical, syntactic and semantic information.
We aredeliberately avoiding the use of manually-createdsemantic resources, so we rely only on lexical andsyntactic attributes for our learning stage (i.e., theGR-POS paths described earlier).A table of all the categories of attributes we ex-tract for each GR-POS path are in Table 2.4, togetherwith attributes from the path linking turtle and reptilein our example sentence (see Figure 1).We ran our experiments with two vector-typeswhich we call our ?verb-augmented?
and our ?non-augmented?
vector-types.
The sets are identical ex-cept the verb-augmented vector-type will also con-tain an additional attribute category containing anattribute for every instance of a relation verb (i.e.,a verb which is found in our training set of relations,e.g., become, cause, taste, use, have and so on) inthe lexical path.
We do this to ascertain whether thisadditional verb-information might be more informa-tive to our system when learning relations (whichtend to be composed of verbs).14Attribute category Example attribute(s)GR path-length LENlemmatised anchor node LEM=turtlePOS of anchor node POS=NNGR path labels GR1=dobjRfrom anchor GR2=ncmodR(indexed) GR3=dobjRGR4=ncsubjNGR path labels GR1=ncsubjRfrom target GR2=dobjN(indexed) GR3=ncmodNGR4=dobjNPOS of path nodes POS1=INfrom anchor POS2=NNS(indexed) POS3=VBPPOS4=NNSPOS of path nodes POS1=NNSfrom target POS2=VBP(indexed) POS3=NNSPOS4=INlemmatised path nodes LEM=include(bag of words) LEM=speciesLEM=ofPOS of all path nodes POS=IN(set) POS=NNSPOS=VBPRelation verbs N/AGR path labels GR=dobjR(set) GR=ncmodNGR=ncsubjNlemmatised target node LEM=reptilePOS of target node POS=NNSTable 2: An example vector for an instance of therelation-label is.
The attributes are distinguished fromone another by their attribute category.
Relation verbsonly appear in the verb-augmented vector-type and nosuch verbs appear in our example sentence, so this cat-egory of attribute is empty.
All attributes in the table willreceive the value 1.0 except the LEN attribute which willhave the value 0.2 (the reciprocal of the path length, 5).We considered allocating a ?no-rel?
relation la-bel to those sets of attributes corresponding to pathsthrough the GR-POS graph which did not link theconcept to a feature found in our training data;however our initial experiments indicated the SVMmodel would assign every pattern we tested to the?no-rel?
relation.
Therefore we used only positiveinstances in our training pattern data.We cycle through all training concepts/features,finding sentences containing both.
For each suchsentence, our system generates the attributes fromthe GR-POS path linking the concept to the fea-ture (the linking-path) to create a pattern for thatpair, in the form of a relation-labelled vector con-taining real-valued attributes.
The system assigns1.0 to all attributes occurring in a given pathand the LEN value receives the reciprocal of thepath-length.1 Each linking-path is collected into arelation-labelled, sparse vector in this manner.
Inthe larger UKWAC corpus this corresponds to over29 million unique attributes across all found linking-paths (this figure corresponds to the dimensionalityof our vectors).
We then pass all vectors to the learn-ing module2 of SVM Light to generate a learnedmodel across all training concepts.2.6 Extracting candidate patternsHaving trained our model, we must now find po-tential features and relations for our test conceptsin our corpora.
We again only examine sentenceswhich contain at least one of our test concepts.
Fur-thermore, to avoid a combinatorial explosion of pos-sible paths rooted at those concepts we only permitas candidates those paths whose anchor node is asingular or plural noun and whose target node is ei-ther a singular/plural noun or adjective.
This filter-ing corresponds to choosing patterns containing oneof the three most frequent anchor node POS tags(NN, NNS and NNP) and target node POS tags (NN,JJ and NNS) found during our training stage.
Thesecandidate patterns constitute 92.6% and 87.7% ofall the vectors, respectively, from our training setof patterns (on the UKWAC corpus).
This patternpre-selection allows us to immediately ignore pathswhich, despite being rooted at a test concept, are un-likely to contain property norm-like information.2.7 Generating and ranking triplesWe next classified our test concepts?
candidatepatterns using the learned model.
SVM Light as-signs each pattern a relation-class from the trainingset and outputs the values of the decision functionsfrom the learned model when applied to that par-ticular pattern.
The sign of these values indicatesthe binary decision function choice, and their mag-nitude acts as a measure of confidence.
We wantedthose vectors which the model was most confidentin across all decision functions, so we took the sumof the absolute values of the decision values to gen-erate a pattern score for each vector/relation-label.1All other possible attributes are assigned the value 0.0.2Using a regularisation parameter (C) value of 1.0 and de-fault parameters otherwise.15Vector-type Corpus ?LL ?PMI ?SVM Prec.
Recall FIgnoring relation.Non-augmentedWikipedia 0.3 0.00 1.00 0.2214 0.3197 0.2564UKWAC 0.10 0.05 0.60 0.2279 0.3330 0.2664UKWAC-Wikipedia 0.35 0.00 0.75 0.2422 0.3533 0.2829Verb-augmentedWikipedia 0.20 0.00 0.65 0.2217 0.3202 0.2568UKWAC 0.30 0.00 0.95 0.2326 0.3400 0.2720UKWAC-Wikipedia 0.40 0.05 1.00 0.2444 0.3577 0.2859With relation.Non-augmentedWikipedia 0.05 0.00 1.00 0.1199 0.1732 0.1394UKWAC 0.05 0.00 1.00 0.1126 0.1633 0.1312UKWAC-Wikipedia 0.05 0.00 0.65 0.1241 0.1808 0.1449Verb-augmentedWikipedia 0.05 0.00 1.00 0.1215 0.1747 0.1410UKWAC 0.05 0.00 1.00 0.1190 0.1724 0.1387UKWAC-Wikipedia 0.05 0.00 0.70 0.1281 0.1860 0.1494Table 3: Parameter estimation both with and without relation, using our augmented and non-augmented vector-typesand across our two corpora and the combined corpora set.From these patterns we derived an output set oftriples where the concept and feature of a triple cor-responded to the anchor and target nodes of its pat-tern and the relation corresponded to the pattern?srelation-label.
Identical triples from differing pat-terns had their pattern scores summed to give a final?SVM score?
for that triple.2.8 Calculating triple scoresA brief qualitative evaluation of our system?s out-put indicates that although the higher-ranked (bySVM score) features and relations were, for the mostpart, quite sensible, there were some obvious outputerrors (e.g., non-dictionary strings or verbs appear-ing as features).
Therefore we restricted our fea-tures to those which appear as nouns or adjectives inWordNet and excluded features containing an NLTK(Bird, 2006) corpus stop-word.
Despite these exclu-sions, some general (and therefore less informative)relation/feature combinations (e.g., is good, is new)were still ranking highly.
To mitigate this, we ex-tract both log-likelihood (LL) and pointwise mutualinformation (PMI) scores for each concept/featurepair to assess the relative saliency of each extractedfeature, with a view to downweighting common butless interesting features.
To speed up this and laterstages, we calculate both statistics for the top 1,000triples extracted for each concept only.PMI was proposed by Church and Hanks (1990)to estimate word association.
We will use it to mea-sure the strength of association between a conceptand its feature.
We hope that emphasising concept-feature pairs with high mutual information will ren-der our triples more relevant/informative.We also employ the LL measure across our set ofconcept-feature pairs.
Proposed by Dunning (1993),LL is a measure of the distribution of linguistic phe-nomena in texts and has been used to contrast therelative corpus frequencies of words.
Our aim is tohighlight features which are particularly distinctivefor a given concept, and hence likely to be featuresof that concept alone.We calculate an overall score for a triple, t, by aweighted combination of the triple?s SVM, PMI andLL scores using the following formula:score(t) = ?PMI?PMI(t)+?LL?LL(t)+?SVM?SVM(t)where the PMI, SVM and LL scores are normalisedso they are in the range [0, 1].
The relative ?
weightsthus give an estimate of the three measures?
impor-tance relative to one another and allows us to gaugewhich combination of these scores is optimal.2.9 DatasetsWe also wanted to ascertain the extent to whichthe output from both our corpora could be combinedto improve results, balancing the encyclopedic butsomewhat specific nature of Wikipedia with the gen-erality and breadth of the UKWAC corpus.
We com-bined the output by summing individual SVM scoresof each triple from both corpora to yield a combinedSVM score.
PMI and LL scores were then calcu-lated as usual from this combined set of triples.3 Experimental Evaluation3.1 Evaluation methodologyWe employ ten-fold cross-validation to ascertainoptimal SVM, LL and PMI ?
parameters for our fi-nal system.
We exclude 44 concepts from our set of16Relation Prec.
Recall FKelly et al Without 0.1943 0.3896 0.2592With 0.1102 0.2210 0.1471ReVerb Without 0.1142 0.2258 0.1514With 0.0431 0.0864 0.0576Our method Without 0.2417 0.4847 0.3225With 0.1238 0.2493 0.1654Table 4: Our best scores on the ESSLLI set compared toKelly et al (2010) and the ReVerb system (Etzioni et al,2011).
Our results are from the verb-augmented vector-type, using the combined UKWAC-Wikipedia corpus andusing the ?
parameters highlighted in Table 3.510 to use in our final system testing and split theremaining 466 concepts randomly and evenly into10 folds.
We apply the training steps above to nineof the folds, generating predictions for the singleheld-out fold.
We repeat this for all ten folds, yield-ing relations and features with SVM, LL and PMIscores for our full set of 466 training concepts onthe UKWAC, Wikipedia and combined corpora.We varied the ?
values from our scoring equa-tion in the range [0,1] (interval 0.05) and com-pared the top twenty triples for each concept directlyagainst the held-out training set.
The best F-scoresand their corresponding ?
values (evaluating on fulltriples and concept-feature pairs alone) are in Ta-ble 3.
We can see that our best results employ theverb-augmented vector-type and the combined cor-pus, with a best F-score of 0.2859 when ignoringthe relation term and 0.1494 when including it in theevaluation.
The main difference between these tworesults is the relative contribution of the reweightingfactors: the SVM score is the most important over-all, but the LL and PMI scores come into play whenevaluating without the relation.
This could be ex-plained by the fact that the PMI and LL scores donot use any relation terms in their calculations.3.2 Quantitative evaluationThe unseen subset of the McRae norms is a setof human-generated common-sense properties withwhich our extracted properties can be compared.However, an issue with the McRae norms is thatsemantically identical properties can be representedby lexically different triples.
This problem was ac-knowledged by Baroni et al (2008) who createda synonym-expanded set of properties for 44 con-cepts (selected evenly across six semantic classes;the 44 concepts we excluded for testing) to par-Judge Judgeturtle A B bowl A Bis green c c is large p pis small c c used for food c cis species c c used for mixing c cis marine c c used for storing food c cused for sea r r used for storing soup r ris animal c c is ceramic c cis many p c is small p phas shell c c used for storing cereal r ris large c p used for storing spoon r ris reptile c c used for storing sugar p cTable 5: Our judges?
assessments of the correctness of thetop ten relation/feature pairs for two concepts extractedfrom our best system.tially solve it.
This expansion set comprises the con-cepts?
top ten properties from the McRae norms withsemi-automatically generated synonyms for each ofthe ten distinct features.
For example, the tripleturtle has shell was expanded to also include tur-tle has shield and turtle has carapace.We use the two best systems (i.e., including andexcluding the relation; highlighted in Table 3) togenerate two sets of top twenty output triples forour 44 concepts.
We then calculate precision, re-call and F-scores for each against our synonym-expanded set.3 Using this expanded set alows usto compare our work with that of Kelly et al (2010).We also compare with the top twenty output of theReverb system Etzioni et al (2011) using their pub-licly available relations derived from the ClueWeb09corpus, employing their normalized triples rankedby frequency.
All sets of results are in Table 4.
Wenote that even though Kelly et al optimised theiralgorithm on the ESSLLI set to yield a theoreticalbest-possible score?we are evaluating ?blind?
?ourperformance still shows an advance on theirs: theimprovement on both sets when comparing the pop-ulation of F-scores across all 44 concepts is statisti-cally significant at the 0.5% level.43.3 Human evaluationThe above does not quite offer the full picture:unlike the features, the relations are not synonym-expanded.
Furthermore, it is possible that there3We note that we are incorporating an upper bound for pre-cision of 0.500 by comparing with only the top ten properties.4Paired t-tests.
?With relation?
: t = 3.524, d.f.= 43, p =0.0010.
?Without relation?
: t = 3.503, d.f.= 43, p = 0.0011.17Relation A B ?
AgreementsWith c / p 146 161 0.7421 261 (87%)r / w 153 138Without c / p 226 235 0.5792 255 (85%)r / w 74 65Table 6: Inter-annotator agreement for our best system,both including and excluding the relation.are correct properties being generated which simplydon?t appear in the ESSLLI evaluation set.In order to address these concerns, we also per-formed a human evaluation on 15 of our concepts.5We asked two native English-speaking judges to de-cide whether a given triple was correct,6 plausible,7wrong but related,8 or wrong.9 We executed thehuman evaluation on our two best systems (as de-scribed above).
As there were shared triples andconcept-feature pairs across the two output sets,each triple and pair was evaluated only once.
Thejudges were aware of the purposes of the study butwere blind to the source sets.
Some example judge-ments are in Table 5.The agreement results across all 15 concepts to-gether with their ?
coefficients (Cohen, 1960) arein Table 6.
In our evaluation we conflated the cor-rect/plausible and wrong but related/wrong cate-gories (see also Kelly et al (2010) and Devereux etal.
(2010)).
We did this because of the subjective na-ture of the judgements, and because we are seekingproperties which are indeed correct or at least plausi-ble.
These results indicate that our system is extract-ing correct or plausible triples 51.1% of the time (ris-ing to 76.8% when considering features only).
Theyalso demonstrate a marked discrepancy between theresults for our two evaluations, reflecting the neces-sity of human evaluation when assessing our partic-ular task.4 DiscussionIn this paper we have shown that semi-supervisedlearning techniques can automatically learn lexico-5The 44 evaluation concepts had been separated into super-ordinate categories for unrelated psycholinguistic research andwe selected our 15 proportionally and at random from these su-perordinate categories.6A correct, valid, feature.7A triple which is plausible but only in a specific set of cir-cumstances or a feature which was correct but very general.8The triple is incorrect but there existed some sort of rela-tionship between the concept and relation and/or feature.9When the triple is simply wrong.syntactic patterns indicative of property norm-likerelations and features.
Using these patterns, oursystem can extract relevant and accurate propertiesfrom any parsed corpus and allows for multi-wordrelation labels, allowing greater semantic precision.As already mentioned, the work of Baroni etal.
(2009) is relevant to our own.
Their approachachieves a precision score of 0.239 on the top tenreturned features evaluated against the ESSLLI set:our best system offers precision of 0.370 on the sameevaluation.
Moreover, Baroni et al do not explicitlyderive relation terms.
We better the performance ofa comparable system (Kelly et al, 2010), even whenevaluating against an unseen set of concepts, and oursystem does not use manually-generated rules or se-mantic information.
Furthermore, human evaluationshows over half of our extracted properties are cor-rect/plausible.For future work, we have already mentioned thatwe are ignoring a large amount of potentially in-structive training data, specifically those GR-POSpaths in our corpus which don?t terminate on one ofour training features, as well as those paths throughsentences containing one of our concepts but noneof our training features.
It might therefore be worth-while investigating the use of this ?negative?
infor-mation.
Another potential avenue for explorationwould be the expansion of the learning vector-types.Although we already use a significant number oflearning attributes (an average of 37.9 per trainingpattern), we could include more: there may be addi-tional information not directly on the GR-POS pathlinking a concept and feature (e.g., nodes adjacentto said path) which might be indicative of their re-lationship.
We would also consider using active-learning, introducing a feedback loop and human-annotation to better distinguish between relationswhich our algorithm tends to classify incorrectly.For example, we could supplement input patterndata with disambiguating POS-GR graphs, drawinga distinction between valid and non-valid relations.Finally, our system could also be evaluated in thecontext of a psycholinguistic experiment.
For exam-ple, we could use our system output to predict con-cept similarity by using our extracted triples to cre-ate vector representations of each concept, calculat-ing the distance between those vectors and compar-ing these similarity ratings with human judgements.18AcknowledgementsThis research was supported by EPSRC grantEP/F030061/1.
We are grateful to McRae and col-leagues for making their norms publicly available,and to the anonymous reviewers for their helpful in-put.ReferencesM.
Andrews, G. Vigliocco, and D. Vinson.
2005.Integrating attributional and distributional informa-tion in a probabilistic model of meaning representa-tion.
In Timo Honkela et al, editor, Proceedings ofAKRR?05, International and Interdisciplinary Confer-ence on Adaptive Knowledge Representation and Rea-soning, pages 15?25, Espoo, Finland: Helsinki Uni-versity of Technology.M.
Baroni, S. Evert, and A. Lenci, editors.
2008.
ESSLLI2008 Workshop on Distributional Lexical Semantics.M.
Baroni, B. Murphy, Barbu E., and Poesio M. 2009.Strudel: A corpus-based semantic model based onproperties and types.
Cognitive Science, pages 1?33.S.
Bird.
2006.
NLTK: The natural language toolkit.
InProceedings of the COLING/ACL on Interactive pre-sentation sessions, pages 69?72.
Association for Com-putational Linguistics.K.W.
Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics, 16(1):22?29.S.
Clark and J.R. Curran.
2007.
Wide-coverage effi-cient statistical parsing with CCG and log-linear mod-els.
Computational Linguistics, 33(4):493?552.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and psychological measurement.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine learning, 20(3):273?297.K.
Crammer and Y.
Singer.
2002.
On the algorithmicimplementation of multiclass kernel-based vector ma-chines.
The Journal of Machine Learning Research,2:265?292.D.
Davidov, A. Rappoport, and M. Koppel.
2007.
Fullyunsupervised discovery of concept-specific relation-ships by web mining.
In Annual Meeting-AssociationFor Computational Linguistics, volume 45, page 232.B.
Devereux, N. Pilkington, T. Poibeau, and A. Korho-nen.
2010.
Towards unrestricted, large-scale acquisi-tion of feature-based conceptual representations fromcorpus data.
Research on Language & Computation,pages 1?34.T.
Dunning.
1993.
Accurate methods for the statistics ofsurprise and coincidence.
Computational linguistics,19(1):61?74.O.
Etzioni, A. Fader, J. Christensen, S. Soderland, andM.T.
Center.
2011.
Open information extraction: Thesecond generation.
In Twenty-Second InternationalJoint Conference on Artificial Intelligence.M.J.
Farah and J.L.
McClelland.
1991.
A computationalmodel of semantic memory impairment: Modalityspecificity and emergent category specificity.
Journalof Experimental Psychology: General, 120(4):339?357.C.
Fellbaum.
1998.
WordNet: An electronic lexicaldatabase.
The MIT press.A.
Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.2008.
Introducing and evaluating ukwac, a very largeweb-derived corpus of english.
In Proceedings of the4th Web as Corpus Workshop (WAC-4) Can we beatGoogle, pages 47?54.P.
Garrard, M.A.L.
Ralph, J.R. Hodges, and K. Patterson.2001.
Prototypicality, distinctiveness, and intercorre-lation: Analyses of the semantic attributes of livingand nonliving concepts.
Cognitive Neuropsychology,18(2):125?174.J.
Gime?nez and L. Marquez.
2004.
Svmtool: A gen-eral pos tagger generator based on support vector ma-chines.
In In Proceedings of the 4th InternationalConference on Language Resources and Evaluation.Citeseer.S.
Godbole, S. Sarawagi, and S. Chakrabarti.
2002.
Scal-ing multi-class support vector machines using inter-class confusion.
In Proceedings of the eighth ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 513?518.
ACM.M.A.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14thconference on Computational linguistics-Volume 2,pages 539?545.
Association for Computational Lin-guistics.T.
Joachims, C. Nedellec, and C. Rouveirol.
1998.
Textcategorization with support vector machines: learningwith many relevant.
In Machine Learning: ECML-98 10th European Conference on Machine Learning,Chemnitz, Germany, pages 137?142.
Springer.T.
Joachims.
1999.
Svmlight: Support vector machine.SVM-Light Support Vector Machine http://svmlight.joachims.
org/, University of Dortmund, 19.C.
Kelly, B. Devereux, and A. Korhonen.
2010.
Ac-quiring human-like feature-based conceptual represen-tations from corpora.
In First Workshop on Computa-tional Neurolinguistics, page 61.
Citeseer.K.
McRae, G.S.
Cree, M.S.
Seidenberg, and C. McNor-gan.
2005.
Semantic feature production norms fora large set of living and nonliving things.
Behav-ioral Research Methods, Instruments, and Computers,37:547?559.19D.
Meyer, F. Leisch, and K. Hornik.
2003.
The supportvector machine under test.
Neurocomputing, 55(1-2):169?186.G.
Murphy.
2002.
The big book of concepts.
The MITPress, Cambridge, MA.P.
Pantel and M. Pennacchiotti.
2008.
Automatically har-vesting and ontologizing semantic relations.
In Pro-ceeding of the 2008 conference on Ontology Learningand Population: Bridging the Gap between Text andKnowledge, pages 171?195.
IOS Press.B.
Randall, H.E.
Moss, J.M.
Rodd, M. Greer, and L.K.Tyler.
2004.
Distinctiveness and correlation in con-ceptual structure: Behavioral and computational stud-ies.
Journal of Experimental Psychology LearningMemory and Cognition, 30(2):393?406.K.I.
Taylor, B.J.
Devereux, K. Acres, B. Randall, andL.K.
Tyler.
2011.
Contrasting effects of feature-basedstatistics on the categorisation and basic-level identifi-cation of visual objects.
Cognition.L.K.
Tyler, H.E.
Moss, M.R.
Durrant-Peatfield, and J.P.Levy.
2000.
Conceptual structure and the structure ofconcepts: A distributed account of category-specificdeficits.
Brain and Language, 75(2):195?231.A.
Vinokourov, J. Shawe-Taylor, and N. Cristianini.2003.
Inferring a semantic representation of text viacross-language correlation analysis.
Advances in neu-ral information processing systems, 15:1473?1480.F.
Wu and D.S.
Weld.
2010.
Open information extractionusing wikipedia.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 118?127.
Association for ComputationalLinguistics.20
