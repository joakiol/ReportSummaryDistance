Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 822?828,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAre School-of-thought Words Characterizable?Xiaorui Jiang?1      Xiaoping Sun?2      Hai Zhuge??
?3*  ?
Key Lab of Intelligent Information Processing, Instituteof Computing Technology, CAS, Beijing, China?
Nanjing University of Posts and Telecommunications, Nanjing, China?
Aston University, Birmingham, UK  1xxiaoruijiang@gmail.com 2 sunxp@kg.ict.ac.cn 3 zhuge@ict.ac.cnAbstractSchool of thought analysis is an important yetnot-well-elaborated scientific knowledge dis-covery task.
This paper makes the first attemptat this problem.
We focus on one aspect of theproblem: do characteristic school-of-thoughtwords exist and whether they are characteriza-ble?
To answer these questions, we propose aprobabilistic generative School-Of-Thought(SOT) model to simulate the scientific author-ing process based on several assumptions.
SOTdefines a school of thought as a distribution oftopics and assumes that authors determine theschool of thought for each sentence beforechoosing words to deliver scientific ideas.
SOTdistinguishes between two types of school-of-thought words for either the general back-ground of a school of thought or the originalideas each paper contributes to its school ofthought.
Narrative and quantitative experi-ments show positive and promising results tothe questions raised above.1 IntroductionWith more powerful computational analysis tools,researchers are now devoting efforts to establisha ?science of better science?
by analyzing theecosystem of scientific discovery (Goth, 2012).Amongst this ambition, school of thought analy-sis has been identified an important fine-grainedscientific knowledge discovery task.
As men-tioned by Teufel (2010), it is important for anexperienced scientist to know which papers be-long to which school of thought (or technicalroute) through years of knowledge accumulation.Schools of thought typically emerge with theevolution of a research domain or scientific topic.Take reachability indexing for example, whichwe will repeatedly turn to later, there are twoschools of thought, the cover-based (since about1990) and hop-based (since the beginning of the2000s) methods.
Most of the following worksbelong to either school of thought and thus twostreams of innovative ideas emerge.
Figure 1 il-lustrates this situation.
Two chains of subsequen-tially published papers represent two schools ofthought of the reachability indexing domain.
Thetop chain of white double-line circles and thebottom chain of shaded circles represent the cov-er-based and hop-based streams respectively.However it is not easy to gain this knowledgeabout school of thought.
Current citation index-ing services are not very helpful for this kind ofknowledge discovery tasks.
As explained in Fig-ure 1, papers of different schools of thought citeeach other heavily and form a rather dense cita-tion graph.
An extreme example is p14, which cites more hop-based papers than its own schoolof thought.If the current citation indexing service can beequipped with school of thought knowledge, itwill help scientists, especially novice researchers,a lot in grasping the core ideas of a scientificdomain quickly and making their own way ofinnovation (Upham et al 2010).
School ofthought analysis is also useful for knowledgeFigure 1.
The citation graph of the reachabilityindexing domain (c.f.
the RE data set in Table 1).????????????????
* Corresponding author.822flow discovery (Zhuge, 2006; Zhuge, 2012),knowledge mapping (Chen, 2004; Herrera et al2010) and scientific paradigm summarization(Joang and Kan, 2010; Qazvinian et al 2013)etc.This paper makes the first attempts to unsu-pervised school of thought analysis.
Three mainaspects of school of thought analysis can be iden-tified: determining the number of schools ofthought, characterizing school-of-thought wordsand categorizing papers into one or severalschool(s) of thought (if applicable).
This paperfocuses on the second subproblem and leaves theother two as future work.
Particularly, we pur-pose to investigate whether characteristic school-of-thought words exist and whether they can beautomatically characterized.
To answer thesequestions, we propose the probabilistic genera-tive School-Of-Thought model (SOT for short)based on the following assumptions on the scien-tific authoring process.Assumption A1.
The co-occurrence patternsare useful for revealing which words and sen-tences are school-of-thought words and whichschools of thought they describe.
Take reachabil-ity indexing for example, hop-based papers try toget the ?optimum labeling?
by finding the?densest intermediate hops?
to encode reach-ability information captured by an intermediatedata structure called ?transitive closure con-tour?.
To accomplish this, they solve the ?dens-est subgraph problem?
on specifically created?bipartite?
graphs centered at ?hops?
by trans-forming the problem into an equivalent ?mini-mum set cover?
framework.
Thus, these bold-faced words often occur as hop-based school-of-thought words.
In cover-based methods, however,one or several ?spanning tree(s)?
are extractedand ?
(multiple) intervals?
are assigned to eachnode as reachability labels by ?pre-(order)?
and?post-order traversals?.
Meanwhile, graph the-ory terminologies like ?root?, ?child?
and ?an-cestor?
etc.
also frequently occur as cover-basedschool-of-thought words.Assumption A2.
Before writing a sentence todeliver their ideas, the authors need to determinewhich school of thought this sentence is to por-tray.
This is called the one-sot-per-sentence as-sumption, where ?sot?
abbreviates ?school ofthought?.
The one-sot-per-sentence assumptiondoes not mean that authors intentionally writethis way, but only simulates the outcome of thescientific paper organization.
Investigations intoscientific writing reveal that sentences of differ-ent schools of thought can occur anywhere andare often interleaved.
This is because authors of ascientific paper not only contribute to the schoolof thought they follow but also discuss differentschools of thought.
For example, in the Methodpart, the authors may turn to discuss another pa-per (possibly of a different school of thought) forcomparison.
This phenomenon also occurs fre-quently in the Results or Discussions section.Besides, citation sentences often acknowledgerelated works of different schools of thought.Assumption A3.
All the papers of a domaintalk about the general domain backgrounds.
Forexample, reachability indexing aims to build?compact indices?
for facilitating ?reachabilityqueries?
between ?source?
and ?target nodes?.Other background words include ?
(complete)transitive closure?, ?index size?
and ?reach?etc., as well as classical graph theory terminolo-gies like ?predecessors?
and ?successors?
etc.Assumption A4.
Besides contributing originalideas, papers of the same school of thought typi-cally need to follow some general strategies thatmake them fall into the same school of thought.For example, all the hop-based methods followthe general ideas of designing approximate algo-rithms for choosing good hops, while the originalideas of each paper lead to different labeling al-gorithms.
Scientific readers pay attention to theoriginal ideas of each paper as well as the gen-eral ideas of each school of thought.
This as-sumes that a word can be either a generality ororiginality word to deliver general and originalideas of a school of thought respectively.2 The School-of-Thought ModelFigure 2 shows the proposed SOT model.
SOTreflects all the assumptions made in Sect.
1.
Theplate notation follows Bishop (2006) where ashaded circle means an observed variable, in thiscontext word occurrence in text, a white circledenotes either a latent variable or a model pa-rameter, and a small solid dot represents a hyper-parameter of the corresponding model parameter.The generative scientific authoring process illus-trated in Figure 2 is elaborated as follows.Step 1.
School of thought assignment (A2).Figure 2.
The SOT Model823To simulate the one-sot-per-sentence assump-tion, we introduce a latent school-of-thought as-signment variable cd,s (1 ?
cd,s ?
C, where C is the number of schools of thought) for each sentences in paper d, dependent on which are topic as-signment and word occurrence variables.
As dif-ferent papers and their authors have different foci,flavors and writing styles, it is appropriate to as-sume that each paper d has its own Dirichlet dis-tribution of schools of thought ( )c cd Dir?
???
?
(refer to Heinrich (2008) for Dirichlet analysis oftexts).
cd,s is thus multinomially sampled from cd??
, that is, , ( )cd s dc Mutl ???
.Step 2.
Background word emission (A3).Before choosing a word wd,s,n to deliver scien-tific ideas, the authors first need to determinewhether this word describes domain backgroundsor depicts a specific school-of-thought.
This in-formation is indicated by the latent backgroundword indicator variable bd,s,n ( )bdBern ??
, where0 1( , )b b bd Beta?
?
??
is the probability of Bernoulli test.
bd,s,n = 1 means wd,s,n is a background word that is multinomially sampled from the Dirichletbackground word distribution ( )bg bgDir?
???
?
,i.e.
, , ( )bgd s nw Mutl ???
.Step 3.
Originality indicator assignment (A4).If bd,s,n = 0, wd,s,n is a school-of-thought word.
Then the authors need to determine whether wd,s,n talks about the general ideas of a certain schoolof thought (i.e.
a generality word when od,s,n = 0) or delivers original contributions to the specificschool of thought (i.e.
an originality word whenod,s,n = 1).
The latent originality indicator variable od,s,n is assigned in a similar way to bd,s,n.Step 4.
Topical word emission.SOT regards schools of thought and topics astwo different levels of semantic information.
Aschool of thought is modeled as a distribution oftopics discussed by the papers of a research do-main.
Each topic in turn is defined as a distribu-tion of the topical words.
Reflected in Figure 1,gc??
and oc??
are Dirichlet distributions of general-ity and originality topics respectively, with g?and o?
being the Dirichlet priors.
According tothe assignment of the originality indicator, thetopic td,s,n of the current token is multinomially selected from either gc??
(od,s,n = 0) or oc??
(od,s,n = 1).
After that, a word wd,s,n is multinomially emit-ted from the topical word distribution , ,d s ntpt??
,where ( )tp tpt Dir?
??
?
for each 1 ?
t ?
T.Gibbs sampling is used for SOT model infer-ence.
Considering the logic of presentation, it isdetailed in Appendix B.3 Experiments3.1 DatasetsLacking standard test benchmarks, we compiled7 data sets according to well-known recent sur-veys (see Appendix A).
Each data set consists ofseveral dozens of papers of the same domain.When constructing these data sets, the only placeof human intervention is the de-duplication step,which means typically only one of a number ofhighly duplicated references is kept in the dataset.
Different from previous studies reviewed inSect.
4, full texts but not abstracts are used.
Weextracted texts from the collected papers and re-moved tables, figures and sentences full of mathequations or unrecognizable symbols.
The statis-tics of the resulting data sets are listed in Table 1.The gold-standard number and the classificationof schools of thoughts reflect not only the view-points of the survey authors but also the consen-sus of the corresponding research communities.3.2 Qualitative ResultsThis section looks at the capabilities of SOT inlearning background and school-of-thought wordsusing the RE data set as an example.
Given theestimated model parameters, the distributions ofthe school-of-thought words of SOT can be cal-culated as weighted sums of topical word emis-sion probabilities ( ,tpt w?
for each word w) over all the topics (?t) and papers (?d), as in Eq.
(1).DATASETS NL W SNd(avg) CSCHOOLS OF THOUGHT(NUMBER OF PAPERS UNDER THIS SCHOOL OF THOUGHT)RE 18 54035 5300 294 2 Hop-Based (9), Cover-Based (9)NP 24 36227 3329 138 3 Mention-Pair Models (14), Entity-Mention Models (5), Ranking Models (5)PP 20 21941 2182 109 4 Using Single Monolingual Corpus (3), Using Monolingual Parallel Corpora (6), Using Monolingual Comparable Corpora (5), Using Bilingual Parallel Corpora (5)TE 34 55671 5335 156 2 Finite-State Transducer models (17), Synchronous Context-Free Grammar models (17)WA 18 19219 1807 100 3 Asymmetric Models (5), Symmetric Alignment Models (9), Supervised Learning for Alignment (4)DP 56 68384 6021 107 3 Transition-Based (20), Graph-Based (17), Grammar-Based (19)LR 44 77024 7395 168 3 Point-wise Approach (11), Pair-wise Approach (17), List-wise Approach (16)Notes: RE ?
REachability indexing; NP ?
Noun Phrase co-reference resolution; PP ?
ParaPhrase; TE ?
Translational Equivalence; WA ?Word Alignment; DP ?
Dependency Parsing; LR ?
Learning to Rank; W ?
number of words; S ?
number of sentences; C ?
gold-standardnumber of schools of thought; Nd ?
number of sentences in document d. Table 1.
Data Sets824, /,0/1 , ,( | , 0 /1)( , )( )d v o g o tpd c t t wd tvp w c oN d wN w ?
?
?=?
?= ?
??
??
?
(1)The first row of Table 2 lists the top-60 back-ground and school-of-thought words learned bySOT for the RE data set sorted in descending or-der of their probabilities column by column.
Thewords at the bottom are some of the remainingcharacteristic words together with their positionson the top-120 list.
In the experiments, T is set to20.
As the data sets are relative small, it is notappropriate to set T too large, otherwise most ofthe topics are meaningless or duplicate.
Eithercase will impose additive negative influences onthe usefulness of the model, for example whenapplied to schools of thought clustering in thenext section.
C is set to the gold-standard numberof schools of thought as in this study we aremainly interested in whether school-of-thoughtwords are characterizable.
The problems of iden-tifying the existence and number of schools ofthought are left to future work.
Other parametersettings follow Griffiths and Steyvers (2010).The learned word distributions are shown verymeaningful at the first glance.
They are furtherexplained as follows.For domain backgrounds, reachability index-ing is a classical problem of the graph database?domain?
which talks about the reachability be-tween the ?source?
and ?destination nodes?
ona ?graph?.
Reachability ?index?
or ?indices?aim at a ?reduction?
of the ?transitive closure?so as to make the ?required storage?
smaller.All current works preprocess the input graphs by?merging strongly connected components?into representative nodes to remove ?cycles?.We then give a deep investigation into thehop-based school-of-thought words (SoT-2).Cover-based ones conform well to the assump-tions in Sect.
1 too.
?2-hop?, ?3-hop?
and ?path-hop?
are three representative hop-based reacha-bility ?labeling schemes?
(a phrase preferredby hop-based papers).
Hop-based methods aim at?finding?
the ?optimum labeling?
with ?mini-mum cost?
and achieving a higher ?compres-sion ratio?
than cover-based methods.
To ac-complish this, hop-based methods define a?densest subgraph problem?
on a ?bipartite?graph, transform it to an equivalent ?set cover?problem, and then apply ?greedy?
algorithmsbased on several ?heuristics?
to find ?approxi-mate?
solutions.
The ?intermediate hops?
withthe highest ?density?
are found as labels andassigned to ?Lout?
and ?Lin?
of certain ?contour?
vertices.
?contour?
is used by hop-based meth-ods as a concise representation of the remainingto-be-encoded reachability information.The underlined bold italic words such as ?set?and ?cover?
are misleading (yet not necessarilyerroneous) words as both schools of thought usethem heavily, but in quite different contexts, forexample, a ?set?
of labels versus ?set cover?,and ?cover(s)?
partial reachability informationversus tree ?cover?.
To improve, one of our fu-ture works shall integrate multi-word expressionsor n-grams (Wallach, 2006) and syntactic analy-sis (Griffiths et al 2004) into the current model.BACKGROUND WORDS SCHOOL-OF-THOUGHT WORDS SOT-1 (COVER-BASED) SOT-2 (HOP-BASED)node   arc   figure   node reachable find 2-hop   problem   hopclosure   size   deleted   graph reach reachability vertex   tree   subgraphchain   lists   incremental   nodes size cover vertices  edges   proposedgraph   procedure predecessor   closure chains acyclic cover construction   largenodes   arcs   directed   tree graphs database graph   approach   path-hopcompressed update   edge   edges storage traversal algorithm indexing linlist   off-chain   systems   chain instance components size  contour spanningtransitive acyclic   connected   transitive intervals directed chain   processing smallersuccessor reduction techniques   non-tree spanning lists labeling   chain  optimalcompression relation   single   number segment reduction closure  pairs denseststorage   source   cycles   compressed order g. reachability compression decompositionchains   reach   updates   path connected addition transitive  reachable   dagrequired effort   depth   edge component technique graphs  property   pathsindex   obtained   materialize   index case degree time   figure   datanumber   component concatenation  list postorder gs number  path-tree   ratiodatabase path   presented   set strongly successors 3-hop   bipartite   nodescase   assignment added   interval original structure index   scheme   edgetechnique   predecessors original  successor ris single labels   density   findingdegree   addition   components   figure required paths query  queries   ranksuccessors indices   strongly   compression source arc set   reach   notedestination (65), determine (76), pair(77), resulting (84), merging (86),reached (87), store (96)root (67), pre- (85), topological (96), sub-tree (102), ancestor (105), child (106),multiple (113), preorder (117)lout (66), segment (68), minimum (69), in-termediate (77), greedy (87), faster (88),heuristics (92), approximate (120)Table 2.
The distributions of top-120 background and school-of-thought words.8253.3 Quantitative ResultsTo see the usefulness of school-of-thought words,we use the SOT model as a way to feature spacereduction for a more precise text representationin the school-of-thought clustering task.
A subsetof school-of-thought words whose accumulatedprobability exceeds a given threshold fsThr areused as the reduced feature vector.
Text is repre-sented in the vector space model weighted usingtf?idf.
K-means is used for clustering.
To obtain astable and reliable result, we choose 300 randomseeds as initial cluster centroids, run K-means300 times and, following the heuristic suggestionby Manning et al(2009), output the best cluster-ing by the minimum residual squared sum prin-ciple.
Two baselines are the ?RAW?
methodwithout dimension reduction and LDA-based(Blei et al 2003) feature selection.
Table 3 re-ports the F-measure values of different competi-tors.
In the parentheses are the correspondingthreshold values under which the reported clus-tering result is obtained.
The larger the thresholdvalue is, the less effective the method in dimen-sion reduction.Compared to the baselines, SOT has consist-ently the best clustering qualities.
When fsThr ?0.70, the feature space is reduced from severalthousand words to only a few hundreds.
LDA istypically better than RAW (except on LR) butless efficient in dimension reduction, e.g.
on WAand DP.
In the latter two cases, fsThr = 0.80 typ-ically means LDA is much less efficient in fea-ture reduction than SOT on these two data sets.DATA SETS F-MEASURE (?
= 2.0) RAW LDA (fsThr) SOT (fsThr)RE .7464 .7464 (.50) .7482 (.60)NP .4528 .6150 (.75) .6911 (.75)PP .3256 .4179 (.60) .6025 (.75)TE .2580 .5148(.60) .9405 (.40)WA .3125 .4569 (.80) .5519 (.60)DP .4787 .6762 (.80) .7155 (.50)LR .5413 .5276 (.95) .6583 (.75)Table 3.
School-of-thought clustering results4 Related WorkAn early work in semantic analysis of scientificarticles is Griffiths and Steyvers (2004) whichfocused on efficient browsing of large literaturecollections based on scientific topics.
Other re-lated researches include topic-based reviewerassignment (Mimno and McCallum, 2007), cita-tion influence estimation (Dietz et al 2007), re-search topic evolution (Hall et al 2008) and ex-pert finding (Tu et al 2010) etc.Another line of research is the joint modelingof topics and other types of semantic units suchas perspectives (Lin et al 2006), sentiment (Meiet al 2007) and opinions (Zhao et al 2010) etc.These works also took a multi-dimensional viewof document semantics.
The TAM model (Pauland Girju, 2010) might be the most relevant toSOT.
TAM simultaneously models aspects andtopics with different assumptions from SOT andit models purely on word level.Studies that introduce an explicit backgrounddistribution include Chemudugunta et al(2006),Haghighi and Vanderwende (2009), and Li et al(2010) etc.
Different from these works, SOT as-sumes that not only some ?meaningless?
general-purpose words but also more meaningful wordsabout the specific domain backgrounds can belearned.
What?s more these works all model on aword level.However, it is very useful to regard sentenceas the basic processing unit, for example in thetext scanning approach simulating human read-ing process by Xu and Zhuge (2013).
Indeed,sentence-level school of thought assignment iscrucial to SOT as it allows SOT to model the sci-entific authoring process.
There are also otherworks that model text semantics on different lev-els other than words or tokens, such as Wallach(2006) on n-grams and Titov and McDonald(2008) on words within multinomially sampledsliding windows.
The latter also distinguishesbetween different levels of topics, say global ver-sus local topics, while in SOT such discrimina-tion is generality versus originality topics.5 ConclusionThis paper proposes a probabilistic generativemodel SOT for characterizing school-of-thoughtwords.
In SOT, a school of thought is modeled asa distribution of topics, with the latter defined asa distribution of topical words.
School of thoughtassignment to each sentence is vital as it allowsSOT to simulate the scientific authoring processin which each sentence conveys a piece of ideacontributed to a certain school of thought as wellas the domain backgrounds.
Narrative and quan-titative analysis show that high-quality school-of-thought words can be captured by the proposedmodel.AcknowledgementsThis work is partially supported by National Sci-ence Foundation of China (No.
61075074 and No.61070183) and funding from Nanjing Universityof Posts and Telecommunications.
Special thanksgo to Prof. Jianmin Yao at Soochow Universityand Suzhou Scientific Service Center of Chinafor his advices and suggestions that help this pa-per finally come true.826ReferencesChemudugunta, C., Smyth P., and Steyvers, M. 2006.Modeling general ad specific aspects of docu-ments with a probabilistic topic model.
In Proc.NIPS?06.Bishop, C. M. 2006.
Patter Recognition and Machinelearning.
Ch.
8 Graphical Models.
Springer.Blei, D. M., Ng, A. Y., and Jordan, M. I.
2003.
Latentdirichlet alcation.
J. Mach.
Learn.
Res., 3: 993?1022.Chen, C. 2004.
Searching for intellectual turningpoints: Prograssive knowledge domain visualiza-tion.
Proc.
Natl.
Acad.
Sci., 101(suppl.
1): 5303?5310.Dietz, L., Bickel, S., and Scheffer, T. 2007.
Unsuper-vised prediction of citation influence.
In Proc.ICML?07, 233?240.Goth, G. 2012.
The science of better science.
Com-mun.
ACM, 55(2): 13?15.Griffiths, T., and Steyvers, M. 2004.
Finding scien-tific topics.
Proc.
Natl.
Acad.
Sci., 101 (suppl 1):5228?5235.Griffiths, T., Steyvers, M., Blei, D. M., and Tenen-baum, J.
B.
2004.
Integrating topics and syntax.In Proc.
NIPS?04.Haghighi, A., and Vanderwende, L. 2009.
Exploringcontent models for multi-document summariza-tion.
In Proc.
HLT-NAACL?09, 362?370.Hall, D., Jurafsky, D., and Manning, C. D. 2008.Studying the history of ideas using topic models.In Proc.
EMNLP?08, 363?371.Heinrich, G. 2008.
Parameter estimation for text anal-ysis.
Available atwww.arbylon.net/publications/text-est.pdf.Herrera, M., Roberts, D. C., and Gulbahce, N. 2010.Mapping the evolution of scientific fields.
PLoSONE, 5(5): e10355.Joang, C. D. V., and Kan, M.-Y.
(2010).
Towardsautomatic related work summarization.
In Proc.COLING 2010.Li, P., Jiang, J., and Wang, Y.
2010.
Generating tem-plates of entity summaries with an entity-aspectmodel and pattern mining.
In Proc.
ACL?10, 640?649.Lin, W., Wilson, T., Wiebe, J., and Hauptmann, A.2006.
Which side are you on?
Identifying per-spectives at the document and sentence levels.
InProc.
CoNLL?06, 109?116.Manning, C. D., Raghavan, P., and Sch?tze, H. 2009.Introduction to Information Retrieval.
Ch.
16.Flat Clustering.
Cambridge University Press.Mei, Q., Ling, X., Wondra, M., Su, H., and Zhai, C.2007.
Topic sentiment mixture: modeling facetsand opinions in weblogs.
In Proc.
WWW?07,171?180.Mimno, D., and McCallum, A.
2007.
Expertise mod-eling for matching papers with reviewers.
In Proc.SIGKDD?07, 500?509.Paul, M., and Girju, R. 2010.
A two-dimensional top-ic-aspect model for discovering multi-facetedtopics.
In Proc.
AAAI?10, 545?550.Qazvinian, V., Radev, D. R., Mohammad, S. M., Dorr,B., Zajic, D., Whidby, M., and Moon T. (2013).Generating extractive summaries of scientificparadigms.
J. Artif.
Intell.
Res., 46: 165?201.Teufel, S. 2010.
The Structure of Scientific Articles.CLSI Publications, Stanford, CA, USA.Titov, I., and McDonald R. 2008.
Modeling onlinereviews with multi-grain topic models.
In Proc.WWW?08, 111?120.Tu, Y., Johri, N., Roth, D., and Hockenmaier, J.
2010.Citation author topic model in expert search.
InProc.
COLING?10, 1265?1273.Upham, S. P., Rosenkopf, L., Ungar, L. H. 2010.
Po-sitioning knowledge: schools of thought and newknowledge creation.
Scientometrics, 83 (2): 555?581.Wallach, H. 2006.
Topic modeling: beyond bag-of-words.
In Proc.
ICML?06, 977?
984.Xu, B., and Zhuge, H. 2013.
A text scanning mecha-nism simulating human reading process, In Proc.IJCAI?13.Zhao, X., Jiang, J., Yan, H., and Li, X.
2010.
Jointlymodeling aspects and opinions with a MaxEnt-LDA hybrid.
In Proc.
EMNLP?10, 56?
65.Zhuge, H. 2006.
Discovery of knowledge flow in sci-ence.
Commun.
ACM, 49(5): 101-107.Zhuge, H. 2012.
The Knowledge Grid: TowardCyber-Physical Society (2nd edition).
World Sci-entific Publishing Company, Singapore.AppendicesA  Survey Papers for Building Data Sets[RE] Yu, P. X., and Cheng, J.
2010.
Managing andMining Graph Data, Ch.
6, 181?215.
Springer.
[NP] Ng, V. 2010.
Supervised noun phrase corefer-ence research: The first fifteen years.
In Proc.ACL?10, 1396?1141.
[PP] Madnani, N., and Dorr, B. J.
2010.
Generatingphrasal and sentential paraphrases: A survey ofdata-driven methods.
Comput.
Linguist., 36 (3):341?387.
[TE/WA] Lopez, A.
2008.
Statistical machine transla-tion.
ACM Comput.
Surv., 40(3), Article 8, 49pages.
[DP] K?bler, S., McDonald, R., and Nivre, J.
2009.Dependency parsing, Ch.
3?5, 21?78.
Morgan &Claypools Publishers.
[LR] Liu, T. Y.
2011.
Learning to rank for infor-mation retrieval, Ch.
2?4, 33?88.
Springer.B  Gibbs Sampling of the SOT ModelUsing collapsed Gibbs sampling (Griffiths andSteyvers, 2004), the latent variable c?
is infer-enced in Eq.
(B1).
In Eq.
(B1), , , , ( ,0, , )c b o tN c o t827is the number of words of topic t describing thecommon ideas (o = 0) or original ideas (o = 1) ofschool of thought c. The superscript ( , )d s?means that words in sentence s of paper d are notcounted.
( , ), ( , )d sd cN d c? )
counts the number of sen-tences in paper d describing school of thought cwith sentence s removed from consideration.
InEqs.
(B1)?
(B4), the symbol ?
means summationover the corresponding variable.
For example,, , , , , ,1, ,( ,0, , ) ( ,0, , )c b o t c b o tt TN c o N c o t=?
=?
?
(B5)Latent variables b?
, o?
and t?
are jointly sam-pled in Eqs.
(B2)?(B4).
( , , ), ( , )d s nd bN d b?
counts the number of background (b = 0) or school-of-thought (b = 1) words in document d withoutcounting the n-th token in sentence s.( , , ), (1, )d s nb vN v?
is the number of times vocabulary item v occurs as background word in the litera-ture collection without counting the n-th token insentence s of paper d. ( , , ), , ( ,0, )d s nd b oN d o?
is the number of words describing either common ideas(o = 0) or original ideas (o = 1) of some schoolof thought without considering the n-th token insentence s of paper d. ( , , ), , , ( ,0, , )d s nc b o tN c o t?
is the number of words of topic t in the literature col-lection describing either common ideas (o = 0) ororiginal ideas (o = 1) of school of thought cwithout counting the n-th token in sentence s ofpaper d. ( , , ), , (0, , )d s nb t vN t v?
is the number of school-of-thought words of topic t which is instantiatedby vocabulary item v in the literature collectionwithout counting the n-th token in sentence s ofpaper d.( , ), , , , , ,( , ), ( , )1 , , , , , ,, , , ,( , )1 , , ,( ( ,0,0, ) ) ( ( ,0,0, ) )( | , ) ( ( ,0,0, ) ) ( ( ,0,0, ) )( ( ,0,1, ) ) (( ( ,0,1, ) )g d s gT c b o t c b o td sd s d s g gt c b o t c b o toT c b o t c bd s ot c b o tN c t N c Tp c c c N c t N c TN c t NN c t?
??
??????=?=?
+ ?
?
+ ?= ?
??
+ ?
?
+ ??
+ ??
??
+???
?
( , ) ( , ), , ,( , ), , , ,( ,0,1, ) ) ( , )( ( ,0,1, ) ) ( , )d s o d s co t d co d s cc b o t d cc N d cN c N d C?
??
??
???
+ +??
?
+ ?
+ ?
(B1)( , , ) ( , , ), 1 ,, , , , ( , , ) ( , , ), 0 1 ,( ,1) (1, )( 1| , ) ( , ) (1, )d s n b d s n bgd b b vd s n d s n d s n b b d s n bgd b b vN d N vp b w v N d N V?
??
?
??
??
?+ += = ?
??
+ + ?
+ ??
(B2)( , , ) ( , , ) ( , , ), , , , , , , , ,( , , ) ( , , ), 0 , , 0( , , ) ( , , ), 0 1 , , 0 1( , , ), , ,( 0, 0, | , , , , , )( ,0) ( ,0,0)( , ) ( ,0, )(d s n d s n d s nd s n d s n d s n d s d s nd s n b d s n od b d b od s n b b d s n o od b d b od s nc b o tp b o t t c c b o t w vN d N dN d N dN?
??
?
?
??
?
??
??
?
?= = = = =+ +?
??
+ + ?
+ +??
??
?
( , , ), ,( , , ) ( , , ), , , , ,,0,0, ) (0, , )( ,0,0, ) (0, , )g d s n tpb t vd s n g d s n tpc b o t b t vc t N t vN c T N t V?
??
???
?+ +??
+ ?
?
+ ?
(B3)( , , ) ( , , ) ( , , ), , , , , , , , ,( , , ) ( , , ), 0 , , 1( , , ) ( , , ), 0 1 , , 0 1( , , ), , ,( 0, 1, | , , , , , )( ,0) ( ,0,1)( , ) ( ,0, )(d s n d s n d s nd s n d s n d s n d s d s nd s n b d s n od b d b od s n b b d s n o od b d b od s nc b o tp b o t t c c b o t w vN d N dN d N dN?
??
?
?
??
?
??
??
?
?= = = = =+ +?
??
+ + ?
+ +??
??
?
( , , ), ,( , , ) ( , , ), , , , ,,0,1, ) (0, , )( ,0,1, ) (0, , )o d s n tpb t vd s n o d s n tpc b o t b t vc t N t vN c T N t V?
??
???
?+ +??
+ ?
?
+ ?
(B4)Figure B1.
The SOT model inference.828
