Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247?256,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsLearning Discriminative Projections for Text Similarity MeasuresWen-tau Yih Kristina Toutanova John C. Platt Christopher MeekMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USA{scottyih,kristout,jplatt,meek}@microsoft.comAbstractTraditional text similarity measures considereach term similar only to itself and do notmodel semantic relatedness of terms.
We pro-pose a novel discriminative training methodthat projects the raw term vectors into a com-mon, low-dimensional vector space.
Our ap-proach operates by finding the optimal matrixto minimize the loss of the pre-selected sim-ilarity function (e.g., cosine) of the projectedvectors, and is able to efficiently handle alarge number of training examples in the high-dimensional space.
Evaluated on two very dif-ferent tasks, cross-lingual document retrievaland ad relevance measure, our method notonly outperforms existing state-of-the-art ap-proaches, but also achieves high accuracy atlow dimensions and is thus more efficient.1 IntroductionMeasures of text similarity have many applicationsand have been studied extensively in both the NLPand IR communities.
For example, a combinationof corpus and knowledge based methods have beeninvented for judging word similarity (Lin, 1998;Agirre et al, 2009).
Similarity derived from a large-scale Web corpus has been used for automaticallyextending lists of typed entities (Vyas and Pantel,2009).
Judging the degree of similarity betweendocuments is also fundamental to classical IR prob-lems such as document retrieval (Manning et al,2008).
In all these applications, the vector-basedsimilarity method is the most widely used.
Termvectors are first constructed to represent the origi-nal text objects, where each term is associated witha weight indicating its importance.
A pre-selectedfunction operating on these vectors, such as cosine,is used to output the final similarity score.
This ap-proach has not only proved to be effective, but is alsoefficient.
For instance, only the term vectors ratherthan the raw data need to be stored.
A pruned inverseindex can be built to support fast similarity search.However, the main weakness of this term-vectorrepresentation is that different but semantically re-lated terms are not matched and cannot influencethe final similarity score.
As an illustrative ex-ample, suppose the two compared term-vectorsare: {purchase:0.4, used:0.3, automobile:0.2} and{buy:0.3, pre-owned: 0.5, car: 0.4}.
Even thoughthe two vectors represent very similar concepts, theirsimilarity score will be 0, for functions like cosine,overlap or Jaccard.
Such an issue is more severein cross-lingual settings.
Because language vocab-ularies typically have little overlap, term-vector rep-resentations are completely inapplicable to measur-ing similarity between documents in different lan-guages.
The general strategy to handle this prob-lem is to map the raw representation to a commonconcept space, where extensive approaches havebeen proposed.
Existing methods roughly fall intothree categories.
Generative topic models like La-tent Dirichlet Allocation (LDA) (Blei et al, 2003)assume that the terms are sampled by probabil-ity distributions governed by hidden topics.
Lin-ear projection methods like Latent Semantic Anal-ysis (LSA) (Deerwester et al, 1990) learn a projec-tion matrix and map the original term-vectors to thedense low-dimensional space.
Finally, metric learn-ing approaches for high-dimensional spaces have247also been proposed (Davis and Dhillon, 2008).In this paper, we propose a new projection learn-ing framework, Similarity Learning via SiameseNeural Network (S2Net), to discriminatively learnthe concept vector representations of input text ob-jects.
Following the general Siamese neural networkarchitecture (Bromley et al, 1993), our approachtrains two identical networks concurrently.
The in-put layer corresponds to the original term vectorand the output layer is the projected concept vector.Model parameters (i.e., the weights on the edges)are equivalently the projection matrix.
Given pairsof raw term vectors and their labels (e.g., similar ornot), the model is trained by minimizing the loss ofthe similarity scores of the output vectors.
S2Netis closely related to the linear projection and met-ric learning approaches, but enjoys additional ad-vantages over existing methods.
While its modelform is identical to that of LSA, CCA and OPCA, itsobjective function can be easily designed to matchthe true evaluation metric of interest for the targettask, which leads to better performance.
Comparedto existing high-dimensional metric learning meth-ods, S2Net can learn from a much larger numberof labeled examples.
These two properties are cru-cial in helping S2Net outperform existing methods.For retrieving comparable cross-lingual documents,S2Net achieves higher accuracy than the best ap-proach (OPCA) at a much lower dimension of theconcept space (500 vs. 2,000).
In a monolingualsetting, where the task is to judge the relevance ofan ad landing page to a query, S2Net alo has thebest performance when compared to a number of ap-proaches, including the raw TFIDF cosine baseline.In the rest of the paper, we first survey someexisting work in Sec.
2, with an emphasis on ap-proaches included in our experimental comparison.We present our method in Sec.
3 and report on anextensive experimental study in Sec.
4.
Other re-lated work is discussed in Sec.
5 and finally Sec.
6concludes the paper.2 Previous WorkIn this section, we briefly review existing ap-proaches for mapping high-dimensional term-vectors to a low-dimensional concept space.2.1 Generative Topic ModelsProbabilistic Latent Semantic Analysis(PLSA) (Hofmann, 1999) assumes that eachdocument has a document-specific distribution ?over some finite number K of topics, where eachtoken in a document is independently generatedby first selecting a topic z from a multinomialdistribution MULTI(?
), and then sampling a wordtoken from the topic-specific word distributionfor the chosen topic MULTI(?z).
Latent DirichletAllocation (LDA) (Blei et al, 2003) generalizesPLSA to a proper generative model for documentsand places Dirichlet priors over the parameters?
and ?.
In the experiments in this paper, ourimplementation of PLSA is LDA with maximum aposteriori (MAP) inference, which was shown to becomparable to the current best Bayesian inferencemethods for LDA (Asuncion et al, 2009).Recently, these topic models have been general-ized to handle pairs or tuples of corresponding doc-uments, which could be translations in multiple lan-guages, or documents in the same language that areconsidered similar.
For instance, the Poly-lingualTopic Model (PLTM) (Mimno et al, 2009) is anextension to LDA that views documents in a tu-ple as having a shared topic vector ?.
Each of thedocuments in the tuple uses ?
to select the topicsz of tokens, but could use a different (language-specific) word-topic-distribution MULTI(?Lz ).
Twoadditional models, Joint PLSA (JPLSA) and Cou-pled PLSA (CPLSA) were introduced in (Platt et al,2010).
JPLSA is a close variant of PLTM when doc-uments of all languages share the same word-topicdistribution parameters, and MAP inference is per-formed instead of Bayesian.
CPLSA extends JPLSAby constraining paired documents to not only sharethe same prior topic distribution ?, but to also havesimilar fractions of tokens assigned to each topic.This constraint is enforced on expectation using pos-terior regularization (Ganchev et al, 2009).2.2 Linear Projection MethodsThe earliest method for projecting term vectors intoa low-dimensional concept space is Latent Seman-tic Analysis (LSA) (Deerwester et al, 1990).
LSAmodels all documents in a corpus using a n ?d document-term matrix D and performs singular248value decomposition (SVD) on D. The k biggestsingular values are then used to find the d ?
k pro-jection matrix.
Instead of SVD, LSA can be doneby applying eigen-decomposition on the correlationmatrix between terms C = DTD.
This is very sim-ilar to principal component analysis (PCA), where acovariance matrix between terms is used.
In prac-tice, term vectors are very sparse and their meansare close to 0.
Therefore, the correlation matrix is infact close to the covariance matrix.To model pairs of comparable documents,LSA/PCA has been extended in different ways.
Forinstance, Cross-language Latent Semantic Indexing(CL-LSI) (Dumais et al, 1997) applies LSA to con-catenated comparable documents from different lan-guages.
Oriented Principal Component Analysis(OPCA) (Diamantaras and Kung, 1996; Platt et al,2010) solves a generalized eigen problem by intro-ducing a noise covariance matrix to ensure that com-parable documents can be projected closely.
Canon-ical Correlation Analysis (CCA) (Vinokourov et al,2003) finds projections that maximize the cross-covariance between the projected vectors.2.3 Distance Metric LearningMeasuring the similarity between two vectors can beviewed as equivalent to measuring their distance, asthe cosine score has a bijection mapping to the Eu-clidean distance of unit vectors.
Most work on met-ric learning learns a Mahalanobis distance, whichgeneralizes the standard squared Euclidean distanceby modeling the similarity of elements in differentdimensions using a positive semi-definite matrix A.Given two vectors x and y, their squared Maha-lanobis distance is: dA = (x ?
y)TA(x ?
y).However, the computational complexity of learn-ing a general Mahalanobis matrix is at least O(n2),where n is the dimensionality of the input vectors.Therefore, such methods are not practical for highdimensional problems in the text domain.In order to tackle this issue, special metriclearning approaches for high-dimensional spaceshave been proposed.
For example, high dimen-sion low-rank (HDLR) metric learning (Davis andDhillon, 2008) constrains the form of A = UUT ,where U is similar to the regular projection ma-trix, and adapts information-theoretic metric learn-ing (ITML) (Davis et al, 2007) to learn U.sim(vp,vq)1tdtvp vqit1ckcjc'twtwFigure 1: Learning concept vectors.
The output layerconsists of a small number of concept nodes, where theweight of each node is a linear combination of all theoriginal term weights.3 Similarity Learning via Siamese NeuralNetwork (S2Net)Given pairs of documents with their labels, such asbinary or real-valued similarity scores, our goal isto construct a projection matrix that maps the corre-sponding term-vectors into a low-dimensional con-cept space such that similar documents are closewhen projected into this space.
We propose a sim-ilarity learning framework via Siamese neural net-work (S2Net) to learn the projection matrix directlyfrom labeled data.
In this section, we introduce itsmodel design and describe the training process.3.1 Model DesignThe network structure of S2Net consists of two lay-ers.
The input layer corresponds to the raw term vec-tor, where each node represents a term in the originalvocabulary and its associated value is determined bya term-weighting function such as TFIDF.
The out-put layer is the learned low-dimensional vector rep-resentation that captures relationships among terms.Similarly, each node of the output layer is an ele-ment in the new concept vector.
In this work, thefinal similarity score is calculated using the cosinefunction, which is the standard choice for documentsimilarity (Manning et al, 2008).
Our frameworkcan be easily extended to other similarity functionsas long as they are differentiable.The output of each concept node is a linear com-249bination of the weights of all the terms in the orig-inal term vector.
In other words, these two layersof nodes form a complete bipartite graph as shownin Fig.
1.
The output of a concept node cj is thusdefined as:tw?
(cj) =?ti?V?ij ?
tw(ti) (1)Notice that it is straightforward to add a non-linearactivation function (e.g., sigmoid) in Eq.
(1), whichcan potentially lead to better results.
However, inthe current design, the model form is exactly thesame as the low-rank projection matrix derived byPCA, OPCA or CCA, which facilitates comparisonto alternative projection methods.
Using concisematrix notation, let f be a raw d-by-1 term vector,A = [?ij ]d?k the projection matrix.
g = AT f isthus the k-by-1 projected concept vector.3.2 Loss Function and Training ProcedureFor a pair of term vectors fp and fq, their similar-ity score is defined by the cosine value of the corre-sponding concept vectors gp and gq according to theprojection matrix A.simA(fp, fq) =gTp gq||gp||||gq||,where gp = AT fp and gq = AT fq.
Let ypq bethe true label of this pair.
The loss function canbe as simple as the mean-squared error 12(ypq ?simA(fp, fq))2.
However, in many applications, thesimilarity scores are used to select the closest textobjects given the query.
For example, given a querydocument, we only need to have the comparabledocument in the target language ranked higher thanany other documents.
In this scenario, it is moreimportant for the similarity measure to yield a goodordering than to match the target similarity scores.Therefore, we use a pairwise learning setting by con-sidering a pair of similarity scores (i.e., from twovector pairs) in our learning objective.Consider two pairs of term vectors (fp1 , fq1) and(fp2 , fq2), where the first pair has higher similarity.Let ?
be the difference of their similarity scores.Namely, ?
= simA(fp1 , fq1)?
simA(fp2 , fq2).
Weuse the following logistic loss over ?, which upper-bounds the pairwise accuracy (i.e., 0-1 loss):L(?
;A) = log(1 + exp(???))
(2)Because of the cosine function, we add a scalingfactor ?
that magnifies ?
from [?2, 2] to a largerrange, which helps penalize more on the predictionerrors.
Empirically, the value of ?
makes no dif-ference as long as it is large enough1.
In the ex-periments, we set the value of ?
to 10.
Optimizingthe model parameters A can be done using gradi-ent based methods.
We derive the gradient of thewhole batch and apply the quasi-Newton optimiza-tion method L-BFGS (Nocedal and Wright, 2006)directly.
For a cleaner presentation, we detail thegradient derivation in Appendix A.
Given that theoptimization problem is not convex, initializing themodel from a good projection matrix often helps re-duce training time and may lead to convergence toa better local minimum.
Regularization can be doneby adding a term ?2 ||A ?
A0||2 in Eq.
(2), whichforces the learned model not to deviate too muchfrom the starting point (A0), or simply by early stop-ping.
Empirically we found that the latter is moreeffective and it is used in the experiments.4 ExperimentsWe compare S2Net experimentally with existing ap-proaches on two very different tasks: cross-lingualdocument retrieval and ad relevance measures.4.1 Comparable Document RetrievalWith the growth of multiple languages on the Web,there is an increasing demand of processing cross-lingual documents.
For instance, machine trans-lation (MT) systems can benefit from training onsentences extracted from parallel or comparabledocuments retrieved from the Web (Munteanu andMarcu, 2005).
Word-level translation lexicons canalso be learned from comparable documents (Fungand Yee, 1998; Rapp, 1999).
In this cross-lingualdocument retrieval task, given a query document inone language, the goal is to find the most similardocument from the corpus in another language.4.1.1 Data & SettingWe followed the comparable document retrievalsetting described in (Platt et al, 2010) and evalu-ated S2Net on the Wikipedia dataset used in that pa-per.
This data set consists of Wikipedia documents1Without the ?
parameter, the model still outperforms otherbaselines in our experiments, but with a much smaller gain.250in two languages, English and Spanish.
An articlein English is paired with a Spanish article if theyare identified as comparable across languages by theWikipedia community.
To conduct a fair compari-son, we use the same term vectors and data split as inthe previous study.
The numbers of document pairsin the training/development/testing sets are 43,380,8,675 and 8,675, respectively.
The dimensionalityof the raw term vectors is 20,000.The models are evaluated by using each Englishdocument as query against all documents in Span-ish and vice versa; the results from the two direc-tions are averaged.
Performance is evaluated by twometrics: the Top-1 accuracy, which tests whetherthe document with the highest similarity score is thetrue comparable document, and the Mean Recipro-cal Rank (MRR) of the true comparable.When training the S2Net model, all the compara-ble document pairs are treated as positive examplesand all other pairs are used as negative examples.Naively treating these 1.8 billion pairs (i.e., 433802)as independent examples would make the trainingvery inefficient.
Fortunately, most computation inderiving the batch gradient can be reused via com-pact matrix operations and training can still be doneefficiently.
We initialized the S2Net model using thematrix learned by OPCA, which gave us the best per-formance on the development set2.Our approach is compared with most methodsstudied in (Platt et al, 2010), including the best per-forming one.
For CL-LSI, OPCA, and CCA, we in-clude results from that work directly.
In addition, were-implemented and improved JPLSA and CPLSAby changing three settings: we used separate vocab-ularies for the two languages as in the Poly-lingualtopic model (Mimno et al, 2009), we performed 10EM iterations for folding-in instead of only one, andwe used the Jensen-Shannon distance instead of theL1 distance.
We also attempted to apply the HDLRalgorithm.
Because this algorithm does not scalewell as the number of training examples increases,we used 2,500 positive and 2,500 negative docu-ment pairs for training.
Unfortunately, among all the2S2Net outperforms OPCA when initialized from a randomor CL-LSI matrix, but with a smaller gain.
For example, whenthe number of dimensions is 1000, the MRR score of OPCAis 0.7660.
Starting from the CL-LSI and OPCA matrices, theMRR scores of S2Net are 0.7745 and 0.7855, respectively.Figure 2: Mean reciprocal rank versus dimension forWikipedia.
Results of OPCA, CCA and CL-LSI arefrom (Platt et al, 2010).hyper-parameter settings we tested, HDLR could notoutperform its initial model, which was the OPCAmatrix.
Therefore we omit these results.4.1.2 ResultsFig.
2 shows the MRR performance of all meth-ods on the development set, across different dimen-sionality settings of the concept space.
As can beobserved from the figure, higher dimensions usuallylead to better results.
In addition, S2Net consistentlyperforms better than all other methods across differ-ent dimensions.
The gap is especially large whenprojecting input vectors to a low-dimensional space,which is preferable for efficiency.
For instance, us-ing 500 dimensions, S2Net aleady performs as wellas OPCA with 2000 dimensions.Table 1 shows the averaged Top-1 accuracy andMRR scores of all methods on the test set, wherethe dimensionality for each method is optimized onthe development set (Fig.
2).
S2Net clearly outper-forms all other methods and the difference in termsof accuracy is statistically significant3.4.2 Ad RelevancePaid search advertising is the main revenue sourcethat supports modern commercial search engines.To ensure satisfactory user experience, it is impor-tant to provide both relevant ads and regular search3We use the unpaired t-test with Bonferroni correction andthe difference is considered statistically significant when the p-value is less than 0.01.251Algorithm Dimension Accuracy MRRS2Net 2000 0.7447 0.7973OPCA 2000 0.7255 0.7734CCA 1500 0.6894 0.7378CPLSA 1000 0.6329 0.6842JPLSA 1000 0.6079 0.6604CL-LSI 5000 0.5302 0.6130Table 1: Test results for comparable document retrievalin Wikipedia.
Results of OPCA, CCA and CL-LSI arefrom (Platt et al, 2010).results.
Previous work on ad relevance focuses onconstructing appropriate term-vectors to representqueries and ad-text (Broder et al, 2008; Choi et al,2010).
In this section, we extend the work in (Yihand Jiang, 2010) and show how S2Net can exploitannotated query?ad pairs to improve the vector rep-resentation in this monolingual setting.4.2.1 Data & TasksThe ad relevance dataset we used consists of12,481 unique queries randomly sampled from thelogs of the Bing search engine.
For each query, anumber of top ranked ads are selected, which resultsin a total number of 567,744 query-ad pairs in thedataset.
Each query-ad pair is manually labeled assame, subset, superset or disjoint.
In our experi-ment, when the task is a binary classification prob-lem, pairs labeled as same, subset, or superset areconsidered relevant, and pairs labeled as disjoint areconsidered irrelevant.
When pairwise comparisonsare needed in either training or evaluation, the rele-vance order is same > subset = superset > disjoint.The dataset is split into training (40%), validation(30%) and test (30%) sets by queries.Because a query string usually contains only a fewwords and thus provides very little content, we ap-plied the same web relevance feedback techniqueused in (Broder et al, 2008) to create ?pseudo-documents?
to represent queries.
Each query in ourdata set was first issued to the search engine.
Theresult page with up to 100 snippets was used as thepseudo-document to create the raw term vectors.
Onthe ad side, we used the ad landing pages insteadof the short ad-text.
Our vocabulary set contains29,854 words and is determined using a documentfrequency table derived from a large collection ofWeb documents.
Only words with counts larger thana pre-selected threshold are retained.How the data is used in training depends on themodel.
For S2Net, we constructed preference pairsin the following way.
For the same query, each rel-evant ad is paired with a less relevant ad.
The lossfunction from Eq.
(2) encourages achieving a highersimilarity score for the more relevant ad.
For HDLR,we used a sample of 5,000 training pairs of queriesand ads, as it was not able to scale to more train-ing examples.
For OPCA, CCA, PLSA and JPLSA,we constructed a parallel corpus using only rele-vant pairs of queries and ads, as the negative exam-ples (irrelevant pairs of queries and ads) cannot beused by these models.
Finally, PCA and PLSA learnthe models from all training queries and documentswithout using any relevance information.We tested S2Net and other methods in two differ-ent application scenarios.
The first is to use the adrelevance measure as an ad filter.
When the similar-ity score between a query and an ad is below a pre-selected decision threshold, this ad is considered ir-relevant to the query and will be filtered.
Evaluationmetrics used for this scenario are the ROC analysisand the area under the curve (AUC).
The second oneis the ranking scenario, where the ads are selectedand ranked by their relevance scores.
In this sce-nario, the performance is evaluated by the standardranking metric, Normalized Discounted CumulativeGain (NDCG) (Jarvelin and Kekalainen, 2000).4.2.2 ResultsWe first compare different methods in their AUCand NDCG scores.
TFIDF is the basic term vec-tor representation with the TFIDF weighting (tf ?log(N/df)).
It is used as our baseline and also asthe raw input for S2Net, HDLR and other linear pro-jection methods.
Based on the results on the devel-opment set, we found that PCA performs better thanOPCA and CCA.
Therefore, we initialized the mod-els of S2Net and HDLR using the PCA matrix.
Ta-ble 2 summarizes results on the test set.
All models,except TFIDF, use 1000 dimensions and their bestconfiguration settings selected on the validation set.TFIDF is a very strong baseline on this monolin-gual ad relevance dataset.
Among all the methodswe tested, at dimension 1000, only S2Net outper-forms the raw TFIDF cosine measure in every eval-uation metric, and the difference is statistically sig-252AUC NDCG@1 NDCG@3 NDCG@5S2Net 0.892 0.855 0.883 0.901TFIDF 0.861 0.825 0.854 0.876HDLR 0.855 0.826 0.856 0.877CPLSA 0.853 0.845 0.872 0.890PCA 0.848 0.815 0.847 0.870OPCA 0.844 0.817 0.850 0.872JPLSA 0.840 0.838 0.864 0.883CCA 0.836 0.820 0.852 0.874PLSA 0.835 0.831 0.860 0.879Table 2: The AUC and NDCG scores of the cosine sim-ilarity scores on different vector representations.
The di-mension for all models except TFIDF is 1000.0.3 0.4 0.5 0.6 0.7 0.8 0.90.05  0.1  0.15  0.2  0.25True-Positive Rate False-Positive RateThe ROC Curves S2NetTFIDFHDLRCPLSAFigure 3: The ROC curves of S2Net, TFIDF, HDLR andCPLSA when the similarity scores are used as ad filters.nificant4.
In contrast, both CPLSA and HDLR havehigher NDCG scores but lower AUC values, andOPCA/CCA perform roughly the same as PCA.When the cosine scores of these vector represen-tations are used as ad filters, their ROC curves (fo-cusing on the low false-positive region) are shownin Fig.
3.
It can be clearly observed that the similar-ity score computed based on vectors derived fromS2Net indeed has better quality, compared to theraw TFIDF representation.
Unfortunately, other ap-proaches perform worse than TFIDF and their per-formance in the low false-positive region is consis-tent with the AUC scores.Although ideally we would like the dimensional-ity of the projected concept vectors to be as small4For AUC, we randomly split the data into 50 subsets andran a paired-t test between the corresponding AUC scores.
ForNDCG, we compared the DCG scores per query of the com-pared models using the paired-t test.
The difference is consid-ered statistically significant when the p-value is less than 0.01.as possible for efficient processing, the quality ofthe concept vector representation usually degradesas well.
It is thus interesting to know the best trade-off point between these two variables.
Table 3 showsthe AUC and NDCG scores of S2Net at different di-mensions, as well as the results achieved by TFIDFand PCA, HDLR and CPLSA at 1000 dimensions.As can be seen, S2Net surpasses TFIDF in AUC atdimension 300 and keeps improving as the dimen-sionality increases.
Its NDCG scores are also con-sistently higher across all dimensions.4.3 DiscussionIt is encouraging to find that S2Net achieves strongperformance in two very different tasks, given thatit is a conceptually simple model.
Its empirical suc-cess can be attributed to two factors.
First, it is flex-ible in choosing the loss function and constructingtraining examples and is thus able to optimize themodel directly for the target task.
Second, it canbe trained on a large number of examples.
For ex-ample, HDLR can only use a few thousand exam-ples and is not able to learn a matrix better than itsinitial model for the task of cross-lingual documentretrieval.
The fact that linear projection methodslike OPCA/CCA and generative topic models likeJPLSA/CPLSA cannot use negative examples moreeffectively also limits their potential.In terms of scalability, we found that methodsbased on eigen decomposition, such as PCA, OPCAand CCA, take the least training time.
The complex-ity is decided by the size of the covariance matrix,which is quadratic in the number of dimensions.
Ona regular eight-core server, it takes roughly 2 to 3hours to train the projection matrix in both experi-ments.
The training time of S2Net scales roughlylinearly to the number of dimensions and trainingexamples.
In each iteration, performing the projec-tion takes the most time in gradient derivation, andthe complexity is O(mnk), where m is the num-ber of distinct term-vectors, n is the largest numberof non-zero elements in the sparse term-vectors andk is the dimensionality of the concept space.
Forcross-lingual document retrieval, when k = 1000,each iteration takes roughly 48 minutes and about 80iterations are required to convergence.
Fortunately,the gradient computation is easily parallelizable andfurther speed-up can be achieved using a cluster.253TFIDF HDLR CPLSA PCA S2Net100 S2Net300 S2Net500 S2Net750 S2Net1000AUC 0.861 0.855 0.853 0.848 0.855 0.879 0.880 0.888 0.892NDCG@1 0.825 0.826 0.845 0.815 0.843 0.852 0.856 0.860 0.855NDCG@3 0.854 0.856 0.872 0.847 0.871 0.879 0.881 0.884 0.883NDCG@5 0.876 0.877 0.890 0.870 0.890 0.897 0.899 0.902 0.901Table 3: The AUC and NDCG scores of S2Net at different dimensions.
PCA, HDLR & CPLSA (at dimension 1000)along with the raw TFIDF representation are used for reference.5 Related WorkAlthough the high-level design of S2Net follows theSiamese architecture (Bromley et al, 1993; Chopraet al, 2005), the network construction, loss func-tion and training process of S2Net are all differ-ent compared to previous work.
For example, tar-geting the application of face verification, Chopraet al (2005) used a convolutional network and de-signed a contrastive loss function for optimizing aEucliden distance metric.
In contrast, the networkof S2Net is equivalent to a linear projection ma-trix and has a pairwise loss function.
In terms ofthe learning framework, S2Net is closely related toseveral neural network based approaches, includingautoencoders (Hinton and Salakhutdinov, 2006) andfinding low-dimensional word representations (Col-lobert and Weston, 2008; Turian et al, 2010).
Archi-tecturally, S2Net is also similar to RankNet (Burgeset al, 2005), which can be viewed as a Siamese neu-ral network that learns a ranking function.The strategy that S2Net takes to learn from la-beled pairs of documents can be analogous to thework of distance metric learning.
Although highdimensionality is not a problem to algorithms likeHDLR, it suffers from a different scalability issue.As we have observed in our experiments, the al-gorithm can only handle a small number of simi-larity/dissimilarity constraints (i.e., the labeled ex-amples), and is not able to use a large number ofexamples to learn a better model.
Empirically, wealso found that HDLR is very sensitive to the hyper-parameter settings and its performance can vary sub-stantially from iteration to iteration.Other than the applications presented in this pa-per, concept vectors have shown useful in traditionalIR tasks.
For instance, Egozi et al (2008) use ex-plicit semantic analysis to improve the retrieval re-call by leveraging Wikipedia.
In a companion pa-per, we also demonstrated that various topic mod-els including S2Net can enhance the ranking func-tion (Gao et al, 2011).
For text categorization, simi-larity between terms is often encoded as kernel func-tions embedded in the learning algorithms, and thusincrease the classification accuracy.
Representativeapproaches include latent semantic kernels (Cris-tianini et al, 2002), which learns an LSA-based ker-nel function from a document collection, and workthat computes term-similarity based on the linguis-tic knowledge provided by WordNet (Basili et al,2005; Bloehdorn and Moschitti, 2007).6 ConclusionsIn this paper, we presented S2Net, a discrimina-tive approach for learning a projection matrix thatmaps raw term-vectors to a low-dimensional space.Our learning method directly optimizes the modelso that the cosine score of the projected vectors canbecome a reliable similarity measure.
The strengthof this model design has been shown empirically intwo very different tasks.
For cross-lingual documentretrieval, S2Net significantly outperforms OPCA,which is the best prior approach.
For ad selectionand filtering, S2Net alo outperforms all methods wecompared it with and is the only technique that beatsthe raw TFIDF vectors in both AUC and NDCG.The success of S2Net is truly encouraging, andwe would like to explore different directions to fur-ther enhance the model in the future.
For instance, itwill be interesting to extend the model to learn non-linear transformations.
In addition, since the pairs oftext objects being compared often come from differ-ent distributions (e.g., English documents vs. Span-ish documents or queries vs. pages), learning twodifferent matrices instead of one could increase themodel expressivity.
Finally, we would like to applyS2Net to more text similarity tasks, such as wordsimilarity and entity recognition and discovery.254ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland WordNet-based approaches.
In Proceedings ofHLT-NAACL, pages 19?27, June.Arthur Asuncion, Max Welling, Padhraic Smyth, andYee Whye Teh.
2009.
On smoothing and inferencefor topic models.
In UAI.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005.
Effective use of WordNet semantics viakernel-based learning.
In CoNLL.David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent dirichlet alocation.
Jour-nal of Machine Learning Research, 3:993?1022.Stephan Bloehdorn and Alessandro Moschitti.
2007.Combined syntactic and semantic kernels for text clas-sification.
In ECIR, pages 307?318.Andrei Z. Broder, Peter Ciccolo, Marcus Fontoura,Evgeniy Gabrilovich, Vanja Josifovski, and LanceRiedel.
2008.
Search advertising using web relevancefeedback.
In CIKM, pages 1013?1022.Jane Bromley, James W. Bentz, Le?on Bottou, IsabelleGuyon, Yann LeCun, Cliff Moore, Eduard Sa?ckinger,and Roopak Shah.
1993.
Signature verification us-ing a ?Siamese?
time delay neural network.
Interna-tional Journal Pattern Recognition and Artificial Intel-ligence, 7(4):669?688.Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.2005.
Learning to rank using gradient descent.
InICML.Y.
Choi, M. Fontoura, E. Gabrilovich, V. Josifovski,M.
Mediano, and B. Pang.
2010.
Using landing pagesfor sponsored search ad selection.
In WWW.Sumit Chopra, Raia Hadsell, and Yann LeCun.
2005.Learning a similarity metric discriminatively, with ap-plication to face verification.
In Proceedings of CVPR-2005, pages 539?546.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neuralnetworks with multitask learning.
In ICML.Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.2002.
Latent semantic kernels.
Journal of IntelligentInformation Systems, 18(2?3):127?152.Jason V. Davis and Inderjit S. Dhillon.
2008.
Struc-tured metric learning for high dimensional problems.In KDD, pages 195?203.Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, andInderjit S. Dhillon.
2007.
Information-theoretic met-ric learning.
In ICML.Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1990.
In-dexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Konstantinos I. Diamantaras and S.Y.
Kung.
1996.
Prin-cipal Component Neural Networks: Theory and Appli-cations.
Wiley-Interscience.Susan T. Dumais, Todd A. Letsche, Michael L. Littman,and Thomas K. Landauer.
1997.
Automatic cross-linguistic information retrieval using latent seman-tic indexing.
In AAAI-97 Spring Symposium Series:Cross-Language Text and Speech Retrieval.Ofer Egozi, Evgeniy Gabrilovich, and Shaul Markovitch.2008.
Concept-based feature generation and selectionfor information retrieval.
In AAAI.Pascale Fung and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compara-ble texts.
In Proceedings of COLING-ACL.Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, andBen Taskar.
2009.
Posterior regularization for struc-tured latent variable models.
Technical Report MS-CIS-09-16, University of Pennsylvania.Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.2011.
Clickthrough-based latent semantic models forweb search.
In SIGIR.G.
E. Hinton and R. R. Salakhutdinov.
2006.
Reducingthe dimensionality of data with neural networks.
Sci-ence, 313(5786):504?507, July.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In SIGIR ?99, pages 50?57.K.
Jarvelin and J. Kekalainen.
2000.
Ir evaluation meth-ods for retrieving highly relevant documents.
In SI-GIR, pages 41?48.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proc.
of COLING-ACL 98.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Pres.David Mimno, Hanna W. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In EMNLP.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31:477?504.Jorge Nocedal and Stephen Wright.
2006.
NumericalOptimization.
Springer, 2nd edition.John Platt, Kristina Toutanova, and Wen-tau Yih.
2010.Translingual document representations from discrimi-native projections.
In EMNLP.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and German cor-pora.
In Proceedings of the ACL, pages 519?526.255Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In ACL.Alexei Vinokourov, John Shawe-taylor, and Nello Cris-tianini.
2003.
Inferring a semantic representation oftext via cross-language correlation analysis.
In NIPS-15.Vishnu Vyas and Patrick Pantel.
2009.
Semi-automaticentity set refinement.
In NAACL ?09, pages 290?298.Wen-tau Yih and Ning Jiang.
2010.
Similarity modelsfor ad relevance measures.
In MLOAD - NIPS 2010Workshop on online advertising.Appendix A. Gradient DerivationThe gradient of the loss function in Eq.
(2) can bederived as follows.?L(?,A)?A=?
?1 + exp(???)???A???A=?
?AsimA(fp1 , fq1)??
?AsimA(fp2 , fq2)?
?AsimA(fp, fq) =?
?Acos(gp,gq),where gp = AT fp and gq = AT fq are the projectedconcept vectors of fq and fq.
The gradient of thecosine score can be further derived in the followingsteps.cos(gp,gq) =gTp gq?gp??gq?
?AgTp gq = (?AAT fp)gq + (?AAT fq)gp= fpgTq + fqgTp?A1?gp?= ?A(gTp gp)?
12= ?12(gTp gp)?
32?A(gTp gp)= ?
(gTp gp)?
32 fpgTp?A1?gq?= ?
(gTq gq)?
32 fqgTqLet a, b, c be gTp gq, 1/?gp?
and 1/?gq?, respec-tively.
?AgTp gq?gp?
?gq?= ?
abc3fqgTq ?
acb3fpgTp+ bc(fpgTq + fqgTp )256
