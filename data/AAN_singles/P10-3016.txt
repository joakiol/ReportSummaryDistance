Proceedings of the ACL 2010 Student Research Workshop, pages 91?96,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsAdapting Self-training for Semantic Role LabelingRasoul Samad Zadeh KaljahiFCSIT, University of Malaya50406, Kuala Lumpur, Malaysia.rsk7945@perdana.um.edu.myAbstractSupervised semantic role labeling (SRL) sys-tems trained on hand-crafted annotated corpo-ra have recently achieved state-of-the-art per-formance.
However, creating such corpora istedious and costly, with the resulting corporanot sufficiently representative of the language.This paper describes a part of an ongoing workon applying bootstrapping methods to SRL todeal with this problem.
Previous work showsthat, due to the complexity of SRL, this task isnot straight forward.
One major difficulty isthe propagation of classification noise into thesuccessive iterations.
We address this problemby employing balancing and preselection me-thods for self-training, as a bootstrapping algo-rithm.
The proposed methods could achieveimprovement over the base line, which do notuse these methods.1 IntroductionSemantic role labeling has been an active re-search field of computational linguistics since itsintroduction by Gildea and Jurafsky (2002).
Itreveals the event structure encoded in the sen-tence, which is useful for other NLP tasks or ap-plications such as information extraction, ques-tion answering, and machine translation (Surdea-nu et al, 2003).
Several CoNLL shared tasks(Carreras and Marquez, 2005; Surdeanu et al,2008) dedicated to semantic role labeling affirmthe increasing attention to this field.One important supportive factor of studyingsupervised statistical SRL has been the existenceof hand-annotated semantic corpora for trainingSRL systems.
FrameNet (Baker et al, 1998) wasthe first such resource, which made the emer-gence of this research field possible by the se-minal work of Gildea and Jurafsky (2002).
How-ever, this corpus only exemplifies the semanticrole assignment by selecting some illustrativeexamples for annotation.
This questions its suita-bility for statistical learning.
Propbank wasstarted by Kingsbury and Palmer (2002) aimingat developing a more representative resource ofEnglish, appropriate for statistical SRL study.Propbank has been used as the learningframework by the majority of SRL work andcompetitions like CoNLL shared tasks.
However,it only covers the newswire text from a specificgenre and also deals only with verb predicates.All state-of-the-art SRL systems show a dra-matic drop in performance when tested on a newtext domain (Punyakanok et al, 2008).
Thisevince the infeasibility of building a comprehen-sive hand-crafted corpus of natural language use-ful for training a robust semantic role labeler.A possible relief for this problem is the utilityof semi-supervised learning methods along withthe existence of huge amount of natural languagetext available at a low cost.
Semi-supervised me-thods compensate the scarcity of labeled data byutilizing an additional and much larger amountof unlabeled data via a variety of algorithms.Self-training (Yarowsky, 1995) is a semi-supervised algorithm which has been well stu-died in the NLP area and gained promising re-sult.
It iteratively extend its training set by labe-ling the unlabeled data using a base classifiertrained on the labeled data.
Although the algo-rithm is theoretically straightforward, it involvesa large number of parameters, highly influencedby the specifications of the underlying task.
Thusto achieve the best-performing parameter set oreven to investigate the usefulness of these algo-rithms for a learning task such as SRL, a tho-rough experiment is required.
This work investi-gates its application to the SRL problem.2 Related WorkThe algorithm proposed by Yarowsky (1995) forthe problem of word sense disambiguation hasbeen cited as the origination of self-training.
Inthat work, he bootstrapped a ruleset from a91small number of seed words extracted froman online dictionary using a corpus of unan-notated English text and gained a compara-ble accuracy to fully supervised approaches.Subsequently, several studies applied the algo-rithm to other domains of NLP.
Reference reso-lution (Ng and Cardie 2003), POS tagging (Clarket al, 2003), and parsing (McClosky et al, 2006)were shown to be benefited from self-training.These studies show that the performance of self-training is tied with its several parameters andthe specifications of the underlying task.In SRL field, He and Gildea (2006) used self-training to address the problem of unseen frameswhen using FrameNet as the underlying trainingcorpus.
They generalized FrameNet frame ele-ments to 15 thematic roles to control the com-plexity of the process.
The improvement gainedby the progress of self-training was small andinconsistent.
They reported that the NULL label(non-argument) had often dominated other labelsin the examples added to the training set.Lee et al (2007) attacked another SRL learn-ing problem using self-training.
Using Propbankinstead of FrameNet, they aimed at increasingthe performance of supervised SRL system byexploiting a large amount of unlabeled data(about 7 times more than labeled data).
The algo-rithm variation was similar to that of He and Gil-dea (2006), but it only dealt with core argumentsof the Propbank.
They achieved a minor im-provement too and credited it to the relativelypoor performance of their base classifier and theinsufficiency of the unlabeled data.3 SRL SystemTo have enough control over entire the systemand thus a flexible experimental framework, wedeveloped our own SRL system instead of usinga third-party system.
The system works withPropBank-style annotation and is described here.Syntactic Formalism: A Penn Treebank con-stituent-based approach for SRL is taken.
Syn-tactic parse trees are produced by the rerankingparser of Charniak and Johnson (2005).Architecture: A two-stage pipeline architec-ture is used, where in the first stage less-probableargument candidates (samples) in the parse treeare pruned, and in the next stage, final argumentsare identified and assigned a semantic role.However, for unlabeled data, a preprocessingstage identifies the verb predicates based on thePOS tag assigned by the parser.
The joint argu-ment identification and classification is chosen todecrease the complexity of self-training process.Features: Features are listed in table 1.
Wetried to avoid features like named entity tags toless depend on extra annotation.
Features markedwith * are used in addition to common featuresin the literature, due to their impact on the per-formance in feature selection process.Classifier: We chose a Maximum Entropyclassifier for its efficient training time and alsoits built-in multi-classification capability.
More-over, the probability score that it assigns to labelsis useful in selection process in self-training.
TheMaxent Toolkit1 was used for this purpose.1http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htmlFeature Name DescriptionPhrase Type Phrase type of the constitu-entPosition+PredicateVoiceConcatenation of constitu-ent position relative to verband verb voicePredicate Lemma  Lemma of the predicatePredicate POS POS tag of the predicatePath Tree path of non-terminalsfrom predicate to constitu-entHead WordLemmaLemma of the head wordof the constituentContent WordLemmaLemma of the contentword of the constituentHead Word POS POS tag of the head wordof the constituentContent Word POS POS tag of the head wordof the constituentGoverning Category The first VP or S ancestorof a NP constituentPredicateSubcategorizationRule expanding the predi-cate's parentConstituentSubcategorization *Rule expanding the consti-tuent's parentClause+VP+NPCount in PathNumber of clauses, NPsand VPs in the pathConstituent andPredicate DistanceNumber of words betweenconstituent and predicateCompound VerbIdentifierVerb predicate structuretype: simple, compound, ordiscontinuous compoundHead Word Loca-tion in Constituent *Location of head word in-side the constituent basedon the number of words inits right and leftTable 1: Features924 Self-training4.1 The AlgorithmWhile the general theme of the self-training algo-rithm is almost identical in different implementa-tions, variations of it are developed based on thecharacteristics of the task in hand, mainly by cus-tomizing several involved parameters.
Figure 1shows the algorithm with highlighted parameters.The size of seed labeled data set L and unla-beled data U, and their ratio are the fundamentalparameters in any semi-supervised learning.
Thedata used in this work is explained in section 5.1.In addition to performance, efficiency of theclassifier (C) is important for self-training, whichis computationally expensive.
Our classifier is acompromise between performance and efficien-cy.
Table 2 shows its performance compared tothe state-of-the-art (Punyakanok et al 2008)when trained on the whole labeled training set.Stop criterion (S) can be set to a pre-determined number of iterations, finishing all ofthe unlabeled data, or convergence of the processin terms of improvement.
We use the second op-tion for all experiments here.In each iteration, one can label entire theunlabeled data or only a portion of it.
In the lattercase, a number of unlaleled examples (p) areselected and loaded into a pool (P).
The selectioncan be based on a specific strategy, known aspreselection (Abney, 2008) or simply doneaccording to the original order of the unlabeleddata.
We investigate preselection in this work.After labeling the p unlabeled data, trainingset is augmented by adding the newly labeleddata.
Two main parameters are involved in thisstep: selection of labeled examples to be added totraining set and addition of them to that set.Selection is the crucial point of self-training,in which the propagation of labeling noise intoupcoming iterations is the major concern.
Onecan select all of labeled examples, but usuallyonly a number of them (n), known as growthsize, based on a quality measure is selected.
Thismeasure is often the confidence score assignedby the classifier.
To prevent poor labelingsdiminishing the quality of training set, athreshold (t) is set on this confidence score.Selection is also influenced by other factors, oneof which being the balance between selectedlabels, which is explored in this study andexplained in detail in the section 4.3.The selected labeled examples can be retainedin unlabeled set to be labeled again in nextiterations (delibility) or moved so that they arelabeled only once (indelibility).
We choose thesecond approach here.4.2 PreselectionWhile using a pool can improve the efficiency ofthe self-training process, there can be two othermotivations behind it, concerned with the per-formance of the process.One idea is that when all data is labeled, sincethe growth size is often much smaller than thelabeled size, a uniform set of examples preferredby the classifier is chosen in each iteration.
Thisleads to a biased classifier like the one discussedin previous section.
Limiting the labeling size toa pool and at the same time (pre)selecting diver-gence examples into it can remedy the problem.The other motivation is originated from thefact that the base classifier is relatively weak dueto small seed size, thus its predictions, as themeasure of confidence in selection process, maynot be reliable.
Preselecting a set of unlabeledexamples more probable to be correctly labeledby the classifier in initial steps seems to be a use-ful strategy against this fact.We examine both ideas here, by a random pre-selection for the first case and a measure of sim-plicity for the second case.
Random preselectionis built into our system, since we use randomized1- Add the seed example set L to currentlyempty training set T.2- Train the base classifier C with trainingset T.3- Iterate the following steps until the stopcriterion S is met.a- Select p examples from U into poolP.b- Label pool P with classifier Cc- Select n labeled examples with thehighest confidence score whose scoremeets a certain threshold t and add totraining set T.d- Retrain the classifier C with newtraining set.Figure 1: Self-training AlgorithmWSJ Test Brown TestP R F1 P R F1Cur 77.43 68.15 72.50 69.14 57.01 62.49Pun 82.28 76.78 79.44 73.38 62.93 67.75Table 2: Performances of the current system (Cur)and the state-of-the-art (Punyakanok et al, 2008)93training data.
As the measure of simplicity, wepropose the number of samples extracted fromeach sentence; that is we sort unlabeled sen-tences in ascending order based on the number ofsamples and load the pool from the beginning.4.3 Selection BalancingMost of the previous self-training problems in-volve a binary classification.
Semantic role labe-ling is a multi-class classification problem withan unbalanced distribution of classes in a giventext.
For example, the frequency of A1 as themost frequent role in CoNLL training set is84,917, while the frequency of 21 roles is lessthan 20.
The situation becomes worse when thedominant label NULL (for non-arguments) isadded for argument identification purpose in ajoint architecture.
This biases the classifiers to-wards the frequent classes, and the impact ismagnified as self-training proceeds.In previous work, although they used a re-duced set of roles (yet not balanced), He andGildea (2006) and Lee et al (2007), did not dis-criminate between roles when selecting high-confidence labeled samples.
The former studyreports that the majority of labels assigned tosamples were NULL and argument labels ap-peared only in last iterations.To attack this problem, we propose a naturalway of balancing, in which instead of labelingand selection based on argument samples, weperform a sentence-based selection and labeling.The idea is that argument roles are distributedover the sentences.
As the measure for selectinga labeled sentence, the average of the probabili-ties assigned by the classifier to all argumentsamples extracted from the sentence is used.5 Experiments and ResultsIn these experiments, we target two main prob-lems addressed by semi-supervised methods: theperformance of the algorithm in exploiting unla-beled data when labeled data is scarce and thedomain-generalizability of the algorithm by us-ing an out-of-domain unlabeled data.We use the CoNLL 2005 shared task data andsetting for testing and evaluation purpose.
Theevaluation metrics include precision, recall, andtheir harmonic mean, F1.5.1 The DataThe labeled data are selected from Propbankcorpus prepared for CoNLL 2005 shared task.Our learning curve experiments on varying sizeof labeled data shows that the steepest increase inF1 is achieved by 1/10th of CoNLL training data.Therefore, for training a base classifier as high-performance as possible, while simulating thelabeled data scarcity with a reasonably smallamount of it, 4000 sentence are selected random-ly from the total 39,832 training sentences asseed data (L).
These sentences contain 71,400argument samples covering 38 semantic roles outof 52 roles present in the total training set.We use one unlabeled training set (U) for in-domain and another for out-of-domain experi-ments.
The former is the remaining portion ofCoNLL training data and contains 35,832 sen-tences (698,567 samples).
The out-of-domain setwas extracted from Open American NationalCorpus 2  (OANC), a 14-million words multi-genre corpus of American English.
The wholecorpus was preprocessed to prune some proble-matic sentences.
We also excluded the biomedsection due to its large size to retain the domainbalance of the data.
Finally, 304,711 sentenceswith the length between 3 and 100 were parsedby the syntactic parser.
Out of these, 35,832 sen-tences were randomly selected for the experi-ments reported here (832,795 samples).Two points are worth noting about the resultsin advance.
First, we do not exclude the argu-ment roles not present in seed data when evaluat-ing the results.
Second, we observed that ourpredicate-identification method is not reliable,since it is solely based on POS tags assigned byparser which is error-prone.
Experiments withgold predicates confirmed this conclusion.5.2 The Effect of Balanced SelectionFigures 2 and 3 depict the results of using unba-lanced and balanced selection with WSJ andOANC data respectively.
To be comparable withprevious work (He and Gildea, 2006), the growthsize (n) for unbalanced method is 7000 samplesand for balanced method is 350 sentences, sinceeach sentence roughly contains 20 samples.
Aprobability threshold (t) of 0.70 is used for bothcases.
The F1 of base classifier, best-performedclassifier, and final classifier are marked.When trained on WSJ unlabeled set, the ba-lanced method outperforms the other in bothWSJ (68.53 vs. 67.96) and Brown test sets (59.62vs.
58.95).
A two-tail t-test based on differentrandom selection of training data confirms thestatistical significance of this improvement atp<=0.05 level.
Also, the self-training trend is2 http://www.americannationalcorpus.org/OANC94more promising with both test sets.
When trainedon OANC, the F1 degrades with both methods asself-training progress.
However, for both testsets, the best classifier is achieved by the ba-lanced selection (68.26 vs. 68.15 and 59.41 vs.58.68).
Moreover, balanced selection shows amore normal behavior, while the other degradesthe performance sharply in the last iterations(due to a swift drop of recall).Consistent with previous work, with unba-lanced selection, non-NULL-labeled unlabeledsamples are selected only after the middle of theprocess.
But, with the balanced method, selectionis more evenly distributed over the roles.A comparison between the results on Browntest set with each of unlabeled sets shows that in-domain data generalizes even better than out-of-domain data (59.62 vs. 59.41 and also note thetrend).
One apparent reason is that the classifiercannot accurately label the out-of-domain unla-beled data successively used for training.
Thelower quality of our out-of-domain data can beanother reason for this behavior.
Furthermore,the parser we used was trained on WSJ, so it ne-gatively affected the OANC parses and conse-quently its SRL results.5.3 The Effect of PreselectionFigures 4 and 5 show the results of using poolwith random and simplicity-based preselectionwith WSJ and OANC data respectively.
The poolsize (p) is 2000, and growth size (n) is 1000 sen-tences.
The probability threshold (t) used is 0.5.Comparing these figures with the previousfigures shows that preselection improves the self-training trend, so that more unlabeled data canstill be useful.
This observation was consistentwith various random selection of training data.Between the two strategies, simplicity-basedmethod outperforms the random method in bothself-training trend and best classifier F1 (68.45vs.
68.25 and 59.77 vs. 59.3 with WSJ and 68.33vs.
68 with OANC), though the t-test shows thatthe F1 difference is not significant at p<=0.05.This improvement does not apply to the case ofusing OANC data when tested with Brown dataFigure 2: Balanced (B) and Unbalanced (U) Selectionwith WSJ Unlabeled Data67.96 67.7767.9568.53 68.158.9557.9958.5859.6259.09575961636567690 7000 14000 21000 28000 35000F1Number?of?Unlabeled?SentencesWSJ?test?
(U) WSJ?test?(B)Brown?test?
(U) Brown?test?
(B)Figure 3: Balanced (B) and Unbalanced (U) Selectionwith OANC Unlabeled Data68.1565.7567.9568.2667.1458.6855.6458.5859.4158.4155575961636567690 7000 14000 21000 28000 35000F1Number?of?Unlabeled?SentencesWSJ?test?
(U) WSJ?test?(B)Brown?test?
(U) Brown?test?
(B)Figure 4: Random (R) and Simplicity (S) Pre-selectionwith WSJ Unlabeled Data68.25 68.1467.9568.45 68.4459.3 58.5558.5859.77 59.34575961636567690 5000 10000 15000 20000 25000 30000 35000F1Number?of?Unlabeled?SentencesWSJ?test?
(R) WSJ?test?(S)Brown?test?
(R) Brown?test?
(S)Figure 5: Random (R) and Simplicity (S) Pre-selectionwith OANC Unlabeled Data6867.3967.9568.3367.4559.38 59.1758.5859.2759.08575961636567690 5000 10000 15000 20000 25000 30000 35000F1Number?of?Unlabeled?SentencesWSJ?test?
(R) WSJ?test?(S)Brown?test?
(R) Brown?test?
(S)95(59.27 vs. 59.38), where, however,  the differ-ence is not statistically significant.
The sameconclusion to the section 5.2 can be made here.6 Conclusion and Future WorkThis work studies the application of self-trainingin learning semantic role labeling with the use ofunlabeled data.
We used a balancing method forselecting newly labeled examples for augmentingthe training set in each iteration of the self-training process.
The idea was to reduce the ef-fect of unbalanced distribution of semantic rolesin training data.
We also used a pool and ex-amined two preselection methods for loadingunlabeled data into it.These methods showed improvement in bothclassifier performance and self-training trend.However, using out-of-domain unlabeled data forincreasing the domain generalization ability ofthe system was not more useful than using in-domain data.
Among possible reasons are thelow quality of the used data and the poor parsesof the out-of-domain data.Another major factor that may affect the self-training behavior here is the poor performance ofthe base classifier compared to the state-of-the-art (see Table 2), which exploits more compli-cated SRL architecture.
Due to high computa-tional cost of self-training approach, bootstrap-ping experiments with such complex SRL ap-proaches are difficult and time-consuming.Moreover, parameter tuning process showsthat other parameters such as pool-size, growthnumber and probability threshold are very effec-tive.
Therefore, more comprehensive parametertuning experiments than what was done here isrequired and may yield better results.We are currently planning to port this settingto co-training, another bootstrapping algorithm.One direction for future work can be adapting thearchitecture of the SRL system to better matchwith the bootstrapping process.
Another directioncan be adapting bootstrapping parameters to fitthe semantic role labeling complexity.ReferencesAbney, S. 2008.
Semisupervised Learning for Compu-tational Linguistics.
Chapman and Hall, London.Baker, F., Fillmore, C. and Lowe, J.
1998.
The Berke-ley FrameNet project.
In Proceedings of COLING-ACL, pages 86-90.Charniak, E. and Johnson, M. 2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.In Proceedings of the 43rd Annual Meeting of theACL, pages 173-180.Carreras, X. and Marquez, L. 2005.
Introduction tothe CoNLL-2005 shared task: Semantic role labe-ling.
In Proceedings of the 9th Conference on Nat-ural Language Learning (CoNLL), pages.
152-164.Clark S., Curran, R. J. and Osborne M. 2003.
Boot-strapping POS taggers using Unlabeled Data.
InProceedings of the 7th Conference on NaturalLanguage Learning At HLT-NAACL 2003, pages49-55.Gildea, D. and Jurafsky, D. 2002.
Automatic labelingof semantic roles.
CL, 28(3):245-288.He, S. and Gildea, H. 2006.
Self-training and Co-training for Semantic Role Labeling: Primary Re-port.
TR 891, University of Colorado at BoulderKingsbury, P. and Palmer, M. 2002.
From Treebankto PropBank.
In Proceedings of the 3rd Interna-tional Conference on Language Resources andEvaluation (LREC-2002).Lee, J., Song, Y. and Rim, H. 2007.
Investigation ofWeakly Supervised Learning for Semantic RoleLabeling.
In Proceedings of the Sixth internationalConference on Advanced Language Processingand Web information Technology (ALPIT 2007),pages 165-170.McClosky, D., Charniak, E., and Johnson, M. 2006.Effective self-training for parsing.
In Proceedingsof the Main Conference on Human LanguageTechnology Conference of the North AmericanChapter of the ACL, pages 152-159.Ng, V. and Cardie, C. 2003.
Weakly supervised natu-ral language learning without redundant views.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the ACL on Human Lan-guage Technology, pages 94-101.Punyakanok, V., Roth, D. and Yi, W. 2008.
The Im-portance of Syntactic Parsing and Inference in Se-mantic Role Labeling.
CL, 34(2):257-287.Surdeanu, M., Harabagiu, S., Williams, J. and Aar-seth, P. 2003.
Using predicate argument structuresfor information extraction.
In Proceedings of the41st Annual Meeting of the ACL, pages 8-15.Surdeanu, M., Johansson, R., Meyers, A., Marquez,L.
and Nivre, J.
2008.
The CoNLL 2008 sharedtask on joint parsing of syntactic and semantic de-pendencies.
In Proceedings of the 12th Conferenceon Natural Language Learning (CoNLL), pages159-177.Yarowsky, E. 1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In pro-ceeding of the 33rd Annual Meeting of ACL, pages189-196.96
