Base Noun Phrase TranslationUsing Web Data and the EM AlgorithmYunbo CaoMicrosoft Research Asiai-yuncao@microsoft.comHang LiMicrosoft Research Asiahangli@microsoft.comAbstractWe consider here the problem of Base NounPhrase translation.
We propose a new methodto perform the task.
For a given Base NP, wefirst search its translation candidates from theweb.
We next determine the possibletranslation(s) from among the candidatesusing one of the two methods that we havedeveloped.
In one method, we employ anensemble of Na?ve Bayesian Classifiersconstructed with the EM Algorithm.
In theother method, we use TF-IDF vectors alsoconstructed with the EM Algorithm.Experimental results indicate that thecoverage and accuracy of our method aresignificantly better than those of the baselinemethods relying on existing technologies.1.
IntroductionWe address here the problem of Base NPtranslation, in which for a given Base NounPhrase in a source language (e.g., ?informationage?
in English), we are to find out its possibletranslation(s) in a target language (e.g., ??
in Chinese).We define a Base NP as a simple andnon-recursive noun phrase.
In many cases, BaseNPs represent holistic and non-divisible concepts,and thus accurate translation of them from onelanguage to another is extremely important inapplications like machine translation, crosslanguage information retrieval, and foreignlanguage writing assistance.In this paper, we propose a new method forBase NP translation, which contains two steps: (1)translation candidate collection, and (2)translation selection.
In translation candidatecollection, for a given Base NP in the sourcelanguage, we look for its translation candidates inthe target language.
To do so, we use aword-to-word translation dictionary and corpusdata in the target language on the web.
Intranslation selection, we determine the possibletranslation(s) from among the candidates.
We usenon-parallel corpus data in the two languages onthe web and employ one of the two methodswhich we have developed.
In the first method, weview the problem as that of classification andemploy an ensemble of Na?ve BayesianClassifiers constructed with the EM Algorithm.We will use ?EM-NBC-Ensemble?
to denote thismethod, hereafter.
In the second method, we viewthe problem as that of calculating similaritiesbetween context vectors and use TF-IDF vectorsalso constructed with the EM Algorithm.
We willuse ?EM-TF-IDF?
to denote this method.Experimental results indicate that our methodis very effective, and the coverage and top 3accuracy of translation at the final stage are91.4% and 79.8%, respectively.
The results aresignificantly better than those of the baselinemethods relying on existing technologies.
Thehigher performance of our method can beattributed to the enormity of the web data usedand the employment of the EM Algorithm.2.
Related Work2.1 Translation with Non-parallelCorporaA straightforward approach to word or phrasetranslation is to perform the task by using parallelbilingual corpora (e.g., Brown et al 1993).Parallel corpora are, however, difficult to obtainin practice.To deal with this difficulty, a number ofmethods have been proposed, which make use ofrelatively easily obtainable non-parallel corpora(e.g., Fung and Yee, 1998; Rapp, 1999; Diab andFinch, 2000).
Within these methods, it is usuallyassumed that a number of translation candidatesfor a word or phrase are given (or can be easilycollected) and the problem is focused ontranslation selection.All of the proposed methods manage to find outthe translation(s) of a given word or phrase, onthe basis of the linguistic phenomenon that thecontexts of a translation tend to be similar to thecontexts of the given word or phrase.
Fung andYee (1998), for example, proposed to representthe contexts of a word or phrase with areal-valued vector (e.g., a TF-IDF vector), inwhich one element corresponds to one word inthe contexts.
In translation selection, they selectthe translation candidates whose context vectorsare the closest to that of the given word or phrase.Since the context vector of the word or phraseto be translated corresponds to words in thesource language, while the context vector of atranslation candidate corresponds to words in thetarget language, and further the words in thesource language and those in the target languagehave a many-to-many relationship (i.e.,translation ambiguities), it is necessary toaccurately transform the context vector in thesource language to a context vector in the targetlanguage before distance calculation.The vector-transformation problem was not,however, well-resolved previously.
Fung andYee assumed that in a specific domain there isonly one-to-one mapping relationship betweenwords in the two languages.
The assumption isreasonable in a specific domain, but is too strict inthe general domain, in which we presume toperform translation here.
A straightforwardextension of Fung and Yee?s assumption to thegeneral domain is to restrict the many-to-manyrelationship to that of many-to-one mapping (orone-to-one mapping).
This approach, however,has a drawback of losing information in vectortransformation, as will be described.For other methods using non-parallel corpora,see also (Tanaka and Iwasaki, 1996; Kikui, 1999,Koehn and Kevin 2000; Sumita 2000; Nakagawa2001; Gao et al 2001).2.2 Translation Using Web DataWeb is an extremely rich source of data fornatural language processing, not only in terms ofdata size but also in terms of data type (e.g.,multilingual data, link data).
Recently, a newtrend arises in natural language processing, whichtries to bring some new breakthroughs to the fieldby effectively using web data (e.g., Brill et al2001).Nagata et al(2001), for example, proposed tocollect partial parallel corpus data on the web tocreate a translation dictionary.
They observedthat there are many partial parallel corporabetween English and Japanese on the web, andmost typically English translations of Japaneseterms (words or phrases) are parenthesized andinserted immediately after the Japanese terms indocuments written in Japanese.3.
Base Noun Phrase TranslationOur method for Base NP translation comprises oftwo steps: translation candidate collection andtranslation selection.
In translation candidatecollection, we look for translation candidates of agiven Base NP.
In translation selection, we findout possible translation(s) from the translationcandidates.In this paper, we confine ourselves totranslation of noun-noun pairs from English toChinese; our method, however, can be extendedto translations of other types of Base NPsbetween other language pairs.3.1 Translation Candidate CollectionWe use heuristics for translation candidatecollection.
Figure 1 illustrates the process ofcollecting Chinese translation candidates for anEnglish Base NP ?information age?
with theheuristics.1.
Input ?information age?;2.
Consult English-Chinese word translation dictionary:information ->age -> (how old somebody is) (historical era) (legal adult hood)3.
Compositionally create translation candidates inChinese:;;4.
Search the candidates on web sites in Chinese andobtain the document frequencies of them (i.e., numbersof documents containing them): 10000 10 05.
Output candidates having non-zero documentfrequencies and the document frequencies: 10000 10Figure 1.
Translation candidate collection3.2 Translation Selection --EM-NBC-EnsembleWe view the translation selection problem as thatof classification and employ EM-NBC-Ensembleto perform the task.
For the ease of explanation,we first describe the algorithm of using onlyEM-NBC and next extend it to that of usingEM-NBC-Ensemble.Basic AlgorithmLet e~ denote the Base NP to be translated and C~the set of its translation candidates (phrases).Suppose that kC =|~| .
Let c~ represent a randomvariable on C~ .
Let E denote a set of words inEnglish, and C a set of words in Chinese.Suppose that nCmE == ||and|| .
Let erepresent a random variable on E and c a randomvariable on C. Figure 2 describes the algorithm.Input: e~ , C~ , contexts containing e~ , contexts containing allCc ~~ ?
;1. create a frequency vector )),(,),(),(( 21 mefefef L),,1(, miEei L=?
using contexts containing e~ ;transforming the vector into )),(,),(),(( 21 nEEE cfcfcf L),,1(, niCci L=?
, using a translation dictionaryand the EM algorithm;2. for each ( Cc ~~ ?
){estimate with Maximum Likelihood Estimation the priorprobability )~(cP using contexts containing all Cc ~~ ?
;create a frequency vector )),(,),(),(( 21 ncfcfcf L),,1(, niCci L=?
using contexts containing c~ ;normalize the frequency vector , yielding),,1(,)),~|(,),~|(),~|(( 21 niCcccPccPccP in LL =?
;calculate the posterior probability )|~( DcP with EM-NBC(generally EM-NBC-Ensemble), where),,1(,)),(,),(),(( 21 niCccfcfcf inEEE LL =?=D3.
Sort Cc ~~ ?
in descending order of )|~( DcP ;Output: the top sorted resultsFigure 2.
Algorithm of EM-NBC-EnsembleContext InformationAs input data, we use ?contexts?
in English whichcontain the phrase to be translated.
We also usecontexts in Chinese which contain the translationcandidates.Here, a context containing a phrase is definedas the surrounding words within a window of apredetermined size, which window covers thephrase.
We can easily obtain the data bysearching for them on the web.
Actually, thecontexts containing the candidates are obtained atthe same time when we conduct translationcandidate collection (Step 4 in Figure 1).EM AlgorithmWe define a relation between E and Cas CER ??
, which represents the links in atranslation dictionary.
We further define}),(|{ Rceec ?=?
.At Step 1, we assume that all the instances in))(),..,(),(( 21 mefefef are independently generatedaccording to the distribution defined as:?
?=CccePcPeP )|()()( (1)We estimate the parameters of the distribution byusing the Expectation and Maximization (EM)Algorithm (Dempster et al, 1977).Initially, we set for all Cc ?||1)(CcP = ,?????????
?=ccceecePif,0if,||1)|(Next, we estimate the parameters by iterativelyupdating them, until they converge (cf., Figure 3).Finally, we calculate )(cf E for all Cc ?
as:?
?=EeE efcPcf )()()( (2)In this way, we can transform the frequencyvector in English ))(),..,(),(( 21 mefefef into a vectorin Chinese ))(),..,(),(( 21 nEEE cfcfcf=D .Prior Probability EstimationAt Step 2, we approximately estimate the priorprobability )~(cP by using the documentfrequencies of the translation candidates.
Thedata are obtained when we conduct candidatecollection (Step 4 in Figure 1).??????????
?EeEeCcecPefecPefcePecPefcPcePcPcePcPecP)|()()|()()|()|()()(StepM)|()()|()()|(StepEFigure 3.
EM AlgorithmEM-NBCAt Step 2, we use an EM-based Na?veBayesian Classifier (EM-NBC) to select thecandidates c~ whose posterior probabilities arethe largest:?????
?+= ????
)~|(log)()~(logmaxarg)|~(maxarg~~~~ccPcfcPcPCcECcCcD(3)Equation (3) is based on Bayes?
rule and theassumption that the data in D are independentlygenerated from CcccP ?
),~|( .In our implementation, we use an equivalent????????
???)~|(log)()~(logminarg~~ccPcfcPCcECc?
(4)where 1??
is an additional parameter used toemphasize the prior information.
If we ignore thefirst term in Equation (4), then the use of oneEM-NBC turns out to select the candidate whosefrequency vector is the closest to the transformedvector D in terms of KL divergence (cf., Coverand Tomas 1991).EM-NBC-EnsembleTo further improve performance, we use anensemble (i.e., a linear combination) ofEM-NBCs (EM-NBC-Ensemble), while theclassifiers are constructed on the basis of the datain different contexts with different window sizes.More specifically, we calculatewhere s),1,(i, L=iD denotes the data in differentcontexts.3.3 Translation Selection -- EM-TF-IDFWe view the translation selection problem as thatof calculating similarities between contextvectors and use as context vectors TF-IDFvectors constructed with the EM Algorithm.Figure 4 describes the algorithm in which we usethe same notations as those inEM-NBC-Ensemble.The idf value of a Chinese word c is calculatedin advance and as)/)(log()( Fcdfcidf ?= (6)where )cdf( denotes the document frequency ofc and F the total document frequency.Input: e~ , C~ , contexts containing e~ , contexts containingall Cc ~~ ?
, Cc),cidf( ?
;1. create a frequency vector )),(,),(),(( 21 mefefef L),,1(, miEei L=?
using contexts containing e~ ;transforming the vector into21 )),c(f,),c(f),c(f( nEEE L),,1(, niCci L=?
, using a translation dictionary andthe EM algorithm;create a TF-IDF vector11 )),cidf())c(f,),cidf()c(f( nnEE L=A ),,1(, niCci L=?2.
for each ( Cc ~~ ?
){create a frequency vector )),(,),(),(( 21 ncfcfcf L),,1(, niCci L=?
using contexts containing c~ ;create a TF-IDF vector11 ))cidf())c(f,),cidf()c(f( nnL=B ),,1(, niCc i L=?
;calculate ),cos()c~tfidf( BA= ; }3.
Sort Cc ~~ ?
in descending order of )c~tfidf( ;Output: the top sorted resultsFigure 4.
Algorithm of EM-TF-IDF3.4 Advantage of Using EM AlgorithmThe uses of EM-NBC-Ensemble and EM-TF-IDFcan be viewed as extensions of existing methodsfor word or phrase translation using non-parallelcorpora.
Particularly, the use of the EMAlgorithm can help to accurately transform afrequency vector from one language to another.Suppose that we are to determine if ??
is a translation of ?information age?
(actuallyit is).
The frequency vectors of context words for?information age?
and ??
are given in Aand D in Figure 5, respectively.
If for eachEnglish word we only retain the link connectingto the Chinese translation with the largestfrequency (a link represented as a solid line) toestablish a many-to-one mapping and transformvector A from English to Chinese, we obtainvector B.
It turns out, however, that vector B isquite different from vector D, although theyshould be similar to each other.
We will refer tothis method as ?Major Translation?
hereafter.With EM, vector A in Figure 5 is transformedinto vector C, which is much closer to vector D,as expected.
Specifically, EM can split thefrequency of a word in English and distributethem into its translations in Chinese in atheoretically sound way (cf., the distributedfrequencies of ?internet?).
Note that if we assumea many-to-one (or one-to-one) mapping?==siicPscP1)|~(1)|~( DD (5)relationship, then the use of EM turns out to beequivalent to that of Major Translation.3.5 CombinationIn order to further boost the performance oftranslation, we propose to also use the translationmethod proposed in Nagata et al Specifically, wecombine our method with that of Nagata et albyusing a back-off strategy.Figure 6 illustrates the process of collectingChinese translation candidates for an EnglishBase NP ?information asymmetry?
with Nagata etal?s method.In the combination of the two methods, we firstuse Nagata et als method to perform translation;if we cannot find translations, we next use ourmethod.
We will denote this strategy ?Back-off?.4.
Experimental ResultsWe conducted experiments on translation of theBase NPs from English to Chinese.We extracted Base NPs (noun-noun pairs) fromthe Encarta 1 English corpus using the tooldeveloped by Xun et al(2000).
There were about1 http://encarta.msn.com/Default.asp3000 Base NPs extracted.
In the experiments, weused the HIT English-Chinese word translationdictionary2 .
The dictionary contains about 76000Chinese words, 60000 English words, and118000 translation links.
As a web search engine,we used Google (http://www.google.com).Five translation experts evaluated thetranslation results by judging whether or not theywere acceptable.
The evaluations reported beloware all based on their judgements.4.1 Basic ExperimentIn the experiment, we randomly selected 1000Base NPs from the 3000 Base NPs.
We next usedour method to perform translation on the 1000phrases.
In translation selection, we employedEM-NBC-Ensemble and EM-TF-IDF.Table 1.
Best translation result for each methodAccuracy (%)Top 1 Top 3Coverage(%)EM-NBC-Ensemble 61.7 80.3Prior 57.6 77.6MT-NBC-Ensemble 59.9 78.1EM-KL-Ensemble 45.9 72.3EM-NBC 60.8 78.9EM-TF-IDF 61.9 80.8MT-TF-IDF 58.2 77.6EM-TF 55.8 77.889.9Table 1 shows the results in terms of coverageand top n accuracy.
Here, coverage is defined asthe percentage of phrases which have translationsselected, while top n accuracy is defined as thepercentage of phrases whose selected top ntranslations include correct translations.For EM-NBC-Ensemble, we set the ?
!in (4) tobe 5 on the basis of our preliminary experimentalresults.
For EM-TF-IDF, we used the non-webdata described in Section 4.4 to estimate idfvalues of words.
We used contexts with windowsizes of ?1, ?3, ?5, ?7, ?9, ?11.2 The dictionary is created by the Harbin Institute of Technology.A B C DFigure 5.
Example of frequency vector transformation1.
Input ?information asymmetry?;2.
Search the English Base NP on web sites in Chineseand obtain documents as follows (i.e., using partial parallelcorpora):	 !"#$%&'()*&"#+information asymmetry,3.
Find the most frequently occurring Chinese phrasesimmediately before the brackets containing the EnglishBase NP, using a suffix tree;4.
Output the Chinese phrases and their documentfrequencies:"#+ 5"#-.
5Figure 6.
Nagata et als method          !
" !"
Figure 7.
Translation resultsFigure 7 shows the results ofEM-NBC-Ensemble and EM-TF-IDF, in whichfor EM-NBC-Ensemble ?window size?
denotesthat of the largest within an ensemble.
Table 1summarizes the best results for each of them.?Prior?
and ?MT-TF-IDF?
are actuallybaseline methods relying on the existingtechnologies.
In Prior, we select candidateswhose prior probabilities are the largest,equivalently, document frequencies obtained intranslation candidate collection are the largest.
InMT-TF-IDF, we use TF-IDF vectors transformedwith Major Translation.Our experimental results indicate that bothEM-NBC-Ensemble and EM-TF-IDFsignificantly outperform Prior and MT-TF-IDF,when appropriate window sizes are chosen.
Thep-values of the sign tests are 0.00056 and 0.00133for EM-NBC-Ensemble, 0.00002 and 0.00901for EM-TF-IDF, respectively.We next removed each of the key componentsof EM-NBC-Ensemble and used the remainingcomponents as a variant of it to performtranslation selection.
The key components are (1)distance calculation by KL divergence (2) EM, (3)prior probability, and (4) ensemble.
The variants,thus, respectively make use of (1) the baselinemethod ?Prior?, (2) an ensemble of Na?veBayesian Classifiers based on Major Translation(MT-NBC-Ensemble), (3) an ensemble ofEM-based KL divergence calculations(EM-KL-Ensemble), and (4) EM-NBC.
Figure 7and Table 1 show the results.
We see thatEM-NBC-Ensemble outperforms all of thevariants, indicating that all the componentswithin EM-NBC-Ensemble play positive roles.We removed each of the key components ofEM-TF-IDF and used the remaining componentsas a variant of it to perform translation selection.The key components are (1) idf value and (2) EM.The variants, thus, respectively make use of (1)EM-based frequency vectors (EM-TF), (2) thebaseline method MT-TF-IDF.
Figure 7 and Table1 show the results.
We see that EM-TF-IDFoutperforms both variants, indicating that all ofthe components within EM-TF-IDF are needed.Comparing the results betweenMT-NBC-Ensemble and EM-NBC-Ensembleand the results between MT-TF-IDF andEM-TF-IDF, we see that the uses of the EMAlgorithm can indeed help to improve translationaccuracies.Table 2.
Sample of translation outputsBase NP Translationcalcium ionadventure tale	lung canceraircraft carrier *adult literacy **Table 2 shows translations of five Base NPs asoutput by EM-NBC-Ensemble, in which thetranslations marked with * were judged incorrectby human experts.
We analyzed the reasons forincorrect translations and found that the incorrecttranslations were due to: (1) no existence ofdictionary entry (19%), (2) non-compositionaltranslation (13%), (3) ranking error (68%).4.2 Our Method vs. Nagata et als MethodTable 3.
Translation resultsAccuracy (%)Top 1 Top 3Coverage (%)Our Method 61.7 80.3 89.9Nagata et als 72.0 76.0 10.5We next used Nagata et als method to performtranslation.
From Table 3, we can see that theaccuracy of Nagata et als method is higher thanthat of our method, but the coverage of it is lower.The results indicate that our proposed Back-offstrategy for translation is justifiable.4.3 CombinationIn the experiment, we tested the Back-off strategy,Table 4 shows the results.
The Back-off strategyTable 4.
Translation resultsAccuracy%Top 1 Top 3Coverage%Back-off (Ensemble) 62.9 79.7Back-off (TF-IDF) 62.2 79.8 91.4helps to further improve the results whetherEM-NBC-Ensemble or EM-TF-IDF is used.4.4 Web Data vs. Non-web DataTo test the effectiveness of the use of web data,we conducted another experiment in which weperformed translation by using non-web data.The data comprised of the Wall Street Journalcorpus in English (1987-1992, 500MB) and thePeople?s Daily corpus in Chinese (1982-1998,700MB).
We followed the Back-off strategy as inSection 4.3 to translate the 1000 Base NPs.Table 5.
Translation resultsAccuracy%DataTop 1 Top 3Coverage%Web (EM-NBC-Ensemble) 62.9 79.7 91.4Non-web (EM-NBC-Ensemble) 56.9 74.7 79.3Web (EM-IF-IDF) 62.2 79.8 91.4Non-web (EM-TF-IDF) 51.5 71.4 78.5The results in Table 5 show that the use of webdata can yield better results than non-use of it,although the sizes of the non-web data we usedwere considerably large in practice.
For Nagata etal?s method, we found that it was almostimpossible to find partial-parallel corpora in thenon-web data.5.
ConclusionsThis paper has proposed a new and effectivemethod for Base NP translation by using webdata and the EM Algorithm.
Experimental resultsshow that it outperforms the baseline methodsbased on existing techniques, mainly due to theemployment of EM.
Experimental results alsoshow that the use of web data is more effectivethan non-use of it.Future work includes further applying theproposed method to the translation of other typesof Base NPs and between other language pairs.AcknowledgementsWe thank Ming Zhou, Chang-Ning Huang,Jianfeng Gao, and Ashley Chang for manyhelpful discussions on this research project.
Wealso acknowledge Shenjie Li for help withprogram coding.ReferencesBrill E., Lin J., Banko M., Dumais S. and Ng A.
(2001)Data-Intensive Question Answering.
In Proc.
ofTREC '2001.Brown P.F., Della Pietra, S.A., Della Pietra V.J., andMercer, R.L.
(1993) The mathematics of StatisticalMachine Translation: Parameter Estimation.Computational Linguistics 19(2), pp.263--11.Cover T. and Thomas J.
(1991) Elements ofInformation Theory, Wiley.Dempster A. P, Laird N. M. and Rubin D. B.
(1977)Maximum likelihood from incomplete data via theEM algorithm.
J. Roy.
Stat.
Soc.
B 39:1--38.Diab M. and Finch S. (2000) A statistical word-leveltranslation model for comparable corpora.
In Proc.of RIAO.Fung P. and Yee L.Y.
(1998) An IR approach fortranslation new words from nonparallel,comparable texts.
In Proc.
of COLING-ACL '1998,pp 414--20.Gao J. F., Nie J. Y., Xun E. D., Zhang J., Zhou M. andHuang C. N. (2001) Improving Query Translationfor Cross-Language Information Retrieval UsingStatistical Models.
In Proc.
of SIGIR '2001.Kikui G. (1999) Resolving translation ambiguity usingnon-parallel bilingual corpora.
In Proc.
of ACL'1999 Workshop, Unsupervised Learning in NLP.Koehn P. and Knight K.(2000) Estimating wordtranslation probabilities from unrelatedmonolingual corpora using the EM algorithm.
InProc.
of AAAI '2000.Nagata M., Saito T., and Suzuki K. (2001) Using theWeb as a bilingual dictionary.
In Proc.
of ACL'2001DD-MT Workshop.Nakagawa H. (2001) Disambiguation of single nountranslations extracted from bilingual comparablecorpora.
In Terminology 7:1.Pederson T.(2000) A Simple Approach to BuildingEnsembles of Na?ve Bayesian Classifiers for WordSense Disambiguation.
In Proc.
of NAACL '2000.Rapp R. (1999) Automatic identification of wordtranslations from unrelated English and Germancorpora.
In Proc.
of ACL'1999.Sumita E.(2000) Lexical transfer using a vector-spacemodel.
In Proc.
of ACL '2000.Tanaka K. and Iwasaki H. (1996) Extraction ofLexical Translation from non-aligned corpora.
InProc.
of COLING '1996Xun E.D., Huang C.N.
and Zhou M. (2000) A UnifiedStatistical Model for the Identification of EnglishBaseNP.
In Proc.
of ACL '2000.
