Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 349?357,Beijing, August 2010EMDC: A Semi-supervised Approach for Word AlignmentQin GaoLanguage Technologies InstituteCarnegie Mellon Universityqing@cs.cmu.eduFrancisco GuzmanCentro de Sistemas InteligentesTecnolo?gico de Monterreyguzmanhe@gmail.comStephan VogelLanguage Technologies InstituteCarnegie Mellon Universitystephan.vogel@cs.cmu.eduAbstractThis paper proposes a novel semi-supervised word alignment techniquecalled EMDC that integrates discrimina-tive and generative methods.
A discrim-inative aligner is used to find high preci-sion partial alignments that serve as con-straints for a generative aligner whichimplements a constrained version of theEM algorithm.
Experiments on small-sizeChinese and Arabic tasks show consistentimprovements on AER.
We also experi-mented with moderate-size Chinese ma-chine translation tasks and got an aver-age of 0.5 point improvement on BLEUscores across five standard NIST test setsand four other test sets.1 IntroductionWord alignment is a crucial component in sta-tistical machine translation (SMT).
From a Ma-chine Learning perspective, the models for wordalignment can be roughly categorized as gener-ative models and discriminative models.
Thewidely used word alignment tool, i.e.
GIZA++(Och and Ney, 2003), implements the well-knownIBM models (Brown et al, 1993) and the HMMmodel (Vogel et al, 1996), which are genera-tive models.
For language pairs such as Chinese-English, the word alignment quality is often un-satisfactory.
There has been increasing interest onusing manual alignments in word alignment tasks,which has resulted in several discriminative mod-els.
Ittycheriah and Roukos (2005) proposed touse only manual alignment links in a maximumentropy model, which is considered supervised.Also, a number of semi-supervised word align-ers have been proposed (Taskar et al, 2005; Liuet al, 2005; Moore, 2005; Blunsom and Cohn,2006; Niehues and Vogel, 2008).
These methodsuse held-out manual alignments to tune weightsfor discriminative models, while using the modelparameters, model scores or alignment links fromunsupervised word aligners as features.
Callison-Burch et.
al.
(2004) proposed a method to interpo-late the parameters estimated by sentence-alignedand word-aligned corpus.
Also, there are recentattempts to combine multiple alignment sourcesusing alignment confidence measures so as to im-prove the alignment quality (Huang, 2009).In this paper, the question we address iswhether we can jointly improve discriminativemodels and generative models by feeding the in-formation we get from the discriminative alignerback into the generative aligner.
Examples ofthis line of research include Model 6 (Och andNey, 2003) and the EMD training approach pro-posed by Fraser and Marcu (2006) and its ex-tension called LEAF aligner (Fraser and Marcu,2007).
These approaches use labeled data to tuneadditional parameters to weight different compo-nents of the IBM models such as the lexical trans-lation model, the distortion model and the fertilitymodel.
These methods are proven to be effectivein improving the quality of alignments.
However,the discriminative training in these methods is re-stricted in using the model components of gener-ative models, in other words, incorporating newfeatures is difficult.Instead of using discriminative training meth-ods to tune the weights of generative models,in this paper we propose to use a discrimina-tive word aligner to produce reliable constraintsfor the EM algorithm.
We call this new train-ing scheme EMDC (Expectation-Maximization-Discrimination-Constraint).
The methodologycan be viewed as a variation of bootstrapping.
Itenables the generative models to interact with dis-criminative models at the data level instead of themodel level.
Furthermore, with a discriminative349word aligner that uses generative word aligner?soutput as features, we create a feedback loop thatcan iteratively improve the quality of both align-ers.
The major contributions of this paper are: 1)The EMDC training scheme, which ties the gen-erative and discriminative aligners together andenables future research on integrating other dis-criminative aligners.
2) An extended generativealigner based on GIZA++ that allows to performconstrained EM training.In Section 2, we present the EMDC trainingscheme.
Section 3 provides details of the con-strained EM algorithm.
In Section 4, we intro-duce the discriminative aligner and link filtering.Section 5 provides the experiment set-up and theresults.
Section 6 concludes the paper.2 EMDC Training SchemeThe EMDC training scheme consists ofthree parts, namely EM, Discrimination, andConstraints.
As illustrated in Figure 1, a largeunlabeled training set is first aligned with a gen-erative aligner (GIZA++ for the purpose of thispaper).
The generative aligner outputs the modelparameters and the Viterbi alignments for bothsource-to-target and target-to-source directions.Afterwards, a discriminative aligner (we use theone described in (Niehues and Vogel, 2008)),takes the lexical translation model, fertility modeland Viterbi alignments from both directions asfeatures, and is tuned to optimize the AER on asmall manually aligned tuning set.
Afterwards,the alignment links generated by the discrimina-tive aligner are filtered according to their likeli-hood, resulting in a subset of links that has highprecision and low recall.
The next step is to putthese high precision alignment links back into thegenerative aligner as constraints.
A conventionalgenerative word aligner does not support this typeof constraints.
Thus we developed a constrainedEM algorithm that can use the links from a partialalignment as constraints and estimate the modelparameters by marginalizing likelihoods.After the constrained EM training is performed,we repeat the procedure and put the updated gen-erative models and Viterbi alignment back into thediscriminative aligner.
We can either fix the num-ber of iterations, or stop the procedure when thegain on AER of a small held-out test set drops be-Figure 1: Illustration of EMDC training schemelow a threshold.The key components for the system are:1.
A generative aligner that can make use of re-liable alignment links as constraints and im-prove the models/alignments.2.
A discriminative aligner that outputs con-fidence scores for alignment links, whichallows to obtain high-precision-low-recallalignments.While in this paper we derive the reliable linksby filtering the alignment generated by a discrimi-native aligner, such partial alignments may be ob-tained from other sources as well: manual align-ments, specific named entity aligner, noun-phrasealigner, etc.As we mentioned in Section 1, the discrimina-tive aligner is not restricted to use features param-eters of generative models and Viterbi alignments.However, including the features from generativemodels is required for iterative training, becausethe improvement on the quality of these featurescan in turn improve the discriminative aligner.
Inour experiments, the discriminative aligner makesheavy use of the Viterbi alignment and the modelparameters from the generative aligner.
Nonethe-less, one can easily replace the discriminativealigner or add new features to it without modify-ing the training scheme.
The open-ended prop-erty of the training scheme makes it a promisingmethod to integrate different aligners.In the next two sections, we will describe thekey components of this framework in detail.3 Constrained EM algorithmIn this section we will briefly introduce the con-strained EM algorithm we used in the experiment,350further details of the algorithm can be found in(Gao et al, 2010).The IBM Models (Brown et al, 1993) are aseries of generative models for word alignment.GIZA++ (Och and Ney, 2003), the most widelyused implementation of IBM models and HMM(Vogel et al, 1996), employs EM algorithm to es-timate the model parameters.
For simpler modelssuch as Model 1 and Model 2, it is possible toobtain sufficient statistics from all possible align-ments in the E-step.
However, for fertility-basedmodels such as Models 3, 4, and 5, enumeratingall possible alignments is NP-complete.
To over-come this limitation, GIZA++ adopts a greedyhill-climbing algorithm, which uses simpler mod-els such as HMM or Model 2 to generate a ?centeralignment?
and then tries to find better alignmentsamong its neighbors.
The neighbors of an align-ment aJ1 = [a1, a2, ?
?
?
, aJ ] with aj ?
[0, I] aredefined as alignments that can be generated fromaJ1 by one of the following two operators:1.
The move operator m[i,j], that changes aj :=i, i.e.
arbitrarily sets word fj in the targetsentence to align to the word ei in source sen-tence;2.
The swap operator s[j1,j2] that exchanges aj1and aj2 .The algorithm will update the center alignmentas long as a better alignment can be found, andfinally outputs a local optimal alignment.
Theneighbor alignments of the final center alignmentare then used in collecting the counts for the M-Step.
Och and Ney (2003) proposed a fast imple-mentation of the hill-climbing algorithm that em-ploys two matrices, i.e.
Moving MatrixMI?J andSwapping Matrix SJ?J .
Each cell of the matricesstores the value of likelihood difference after ap-plying the corresponding operator.We define a partial alignment constraint of asentence pair (fJ1 , eI1) as a set of links: ?JI ={(i, j)|0 ?
i < I, 0 ?
j < J}.
Given a set ofconstraints, an alignment aJ1 = [a1, a2, ?
?
?
, aj ]on the sentence pair fJ1 , eI1, the translation proba-bility of Pr(fJ1 |eI1) will be zero if the alignmentis inconsistent with the constraints.
Constraints(0, j) or (i, 0) are used to explicitly represent thatword fj or ei is aligned to the empty word.Under the assumptions of the IBM models,there are two situations that aJ1 is inconsistent with?JI :1.
Target word misalignment: The IBM mod-els assume that one target word can only bealigned to one source word.
Therefore, if thetarget word fj aligns to a source word ei,while the constraint ?JI suggests fj should bealigned to ei?
, the alignment violates the con-straint and thus is considered inconsistent.2.
Source word to empty word misalignment: ifa source word is aligned to the empty word,it cannot be aligned to any concrete targetword.However, the partial alignments, which allown-to-n alignments, may already violate the 1-to-nalignment restriction of the IBM models.
In thesecases, we relax the condition in situation 1 that ifthe alignment link aj?
is consistent with any oneof the conflicting target-to-source constraints, itwill be considered consistent.
Also, we arbitrarilyassign the source word to empty word constraintshigher priorities than other constraints, becauseunlike situation 1, it does not have the problemof conflicting with other constraints.3.1 Constrained hill-climbing algorithmTo ensure that resulting center alignment beconsistent with the constraints, we need to splitthe hill-climbing algorithm into two stages: 1) op-timize towards the constraints and 2) optimize to-wards the optimal alignment under the constraints.From a seed alignment, we first move the align-ment towards the constraints by choosing a moveor swap operator that:1. produces the alignment that has the highestlikelihood among alignments generated byother operators,2.
eliminates at least one inconsistent link.We iteratively update the alignment until noother inconsistent link can be removed.
The algo-rithm implies that we force the seed alignment tobe closer to the constraints while trying to find thebest consistent alignment.
Figure 2 demonstratesthe idea, given the constraints shown in (a), andthe seed alignment shown as solid links in (b), we3512005???
?thesummerof2005Manual Alignment Link(a)2005???
?thesummerOf2005Seed Alignment Consistent Alignment Center Alignment(b)                    (c)2005???
?thesummerof2005Figure 2: Illustration of Algorithm 1move the inconsistent link to the dashed link by amove operation.After we find the consistent alignment, we pro-ceed to optimize towards the optimal alignmentunder the constraints.
The algorithm sets the valueof the cells in moving/swapping matrices to nega-tive if the corresponding operators will lead to aninconsistent alignment.
The moving matrix needsto be processed only once, whereas the swappingmatrix needs to be updated every iteration, sinceonce the alignment is updated, the possible viola-tions will also change.If a source word ei is aligned to the empty word,we set Mi,j = ?1,?j.
The swapping matrix doesnot need to be modified in this case because theswapping operator will not introduce new links.Because the cells that can lead to violations areset to negative, the operators will never be pickedwhen updating the center alignments.
This en-sures the consistency of the final center alignment.3.2 Count CollectionAfter finding the center alignment, we need tocollect counts from neighbor alignments so thatthe M-step can normalize the counts to producethe model parameters for the next step.
In thisstage, we want to make sure all the inconsistentalignments in the neighbor set of the center align-ment be ruled out from the sufficient statistics, i.e.have zero probability.
Similar to the constrainedhill climbing algorithm, we can manipulate themoving/swapping matrices to effectively excludeinconsistent alignments.
Since the original countcollection algorithm depends only on moving andswapping matrices, we just need to bypass all thecells which hold negative values, i.e.
represent in-consistent alignments.We can also view the algorithm as forcingthe posteriors of inconsistent alignments to zero,and therefore increase the posteriors of consistentalignments.
When no constraint is given, the algo-rithm falls back to conventional EM, and when allthe alignments are known, the algorithm becomesfully supervised.
And if the alignment qualitycan be improved if high-precision partial align-ment links is given as constraints.
In (Gao et al,2010) we experimented with using a dictionary togenerate such constraints, and in (Gao and Vogel,2010) we experimented with manual word align-ments from Mechanical Turk.
And in this paperwe try to use an alternative method that uses a dis-criminative aligner and link filtering to generatesuch constraints.4 Discriminative Aligner and LinkFilteringWe employ the CRF-based discriminative wordaligner described in (Niehues and Vogel, 2008).The aligner can use a variety of knowledgesources as features, such as: the fertility and lex-ical translation model parameters from GIZA++,the Viterbi alignment from both source-to-targetand target-to-source directions.
It can also makeuse of first-order features which model the depen-dency between different links, the Parts-of-Speechtagging features, the word form similarity featureand the phrase features.
In this paper we use allthe features mentioned above except the POS andphrase features.The aligner is trained using a belief-propagation (BP) algorithm, and can be optimizedto maximize likelihood or directly optimize to-wards AER on a tuning set.
The aligner outputsconfidence scores for alignment links, whichallows us to control the precision and recallrate of the resulting alignment.
Guzman et al(2009) experimented with different alignmentsproduced by adjusting the filtering threshold forthe alignment links and showed that they couldget high-precision-low-recall alignments by hav-ing a higher threshold.
Therefore, we replicatedthe confidence filtering procedures to producethe partial alignment constraints.
Afterwardswe iterate by putting the partial alignments backto the constrained word alignment algorithmdescribed in section 3.Although the discriminative aligner performswell in supplying high precision constraints, itdoes not model the null alignment explicitly.352Num.
ofSentencesNum.
of Words Num.
ofLinksSource TargetCh-En 21,863 424,683 524,882 687,247Ar-En 29,876 630,101 821,938 830,349Table 1: Corpus statistics of the manual alignedcorporaThreshold P R AERCh-En0.6 71.30 58.12 35.960.7 75.24 54.03 37.110.8 85.66 44.19 41.700.9 93.70 37.95 45.98Ar-En0.6 72.35 59.87 34.480.7 77.55 55.58 35.250.8 80.07 50.89 37.770.9 83.74 44.16 42.17Table 2: The qualities of the constraintsHence we are currently not able to provide sourceword to empty word alignment constraints whichhave been proven to be effective in improving thealignment quality in (Gao et al, 2010).
Due tospace limitation, please refer to: (Niehues and Vo-gel, 2008; Guzman et al, 2009) for further detailsof the aligner and link filtering, respectively.5 ExperimentsTo validate the proposed training scheme, weperformed two sets of experiments.
First of all,we experimented with a small manually alignedcorpus to evaluate the ability of the algorithm toimprove the AER.
The experiment was performedon Chinese to English and Arabic to English tasks.Secondly, we experimented with a moderate sizecorpus and performed translation tasks to observethe effects in translation quality.5.1 Effects on AERIn order to measure the effects of EMDC inalignment quality, we experimented with Chinese-English and Arabic-English manually aligned cor-pora.
The statistics of these sets are shown in Ta-ble 1.
We split the data into two fragments, thefirst 100 sentences (Set A) and the remaining (SetB).
We trained generative IBM models using theSet B, and tuned the discriminative aligner usingthe Set A.
We evaluated the AER on Set B, but inany of the training steps the manual alignments ofSet B were not used.In each iteration of EDMC, we load the modelparameters from the previous step and continuetraining using the new constraints.
Therefore, it isimportant to compare the performance of contin-uous training against an unconstrained baseline,because variation in alignment quality could beattributed to either the effect of more training it-erations or to the effect of semi-supervised train-ing scheme.
In Figures 3 and 4 we show thealignment quality for each iteration.
Iteration 0 isthe baseline, which comes from standard GIZA++training1.
The grey dash curves represent uncon-strained Model 4 training, and the curves withstart, circle, cross and diamond markers are con-strained EM alignments with 0.6, 0.7, 0.8 and0.9 filtering thresholds respectively.
As we cansee from the results, when comparing only themono-directional trainings, the alignment quali-ties improve over the unconstrained training in allthe metrics (precision, recall and AER).
From Ta-ble 2, we observe that the quality of discrimina-tive aligner also improved.
Nonetheless, whenwe consider the heuristically symmetrized align-ment2, we observe mixed results.
For instance,for the Chinese-English case we observe that AERimproves over iterations, but this is the result ofa increasingly higher recall rate in detriment ofprecision.
Ayan and Dorr (2006) pointed outthat grow-diag-final symmetrization tends to out-put alignments with high recall and low precision.However this does not fully explain the tendencywe observed between iterations.
The character-istics of the alignment modified by EDMC thatlead to larger improvements in mono-directionaltrainings but a precision drop with symmetrizationheuristics needs to be addressed in future work.Another observation is how the filtering thresh-olds affect the results.
As we can see in Table 3,for Chinese to English word alignment, the largestgain on the alignment quality is observed whenthe threshold was set to 0.8, while for Arabic toEnglish, the threshold of 0.7 or 0.6 works better.Table 2 shows the precision, recall, and AER ofthe constraint links used in the constrained EM al-1We run 5, 5, 3, 3 iterations of Model 1, HMM, Model 3and Model 4 respectively.2We used grow-diag-final-and3530 2 4 6 860626466%Precision0 2 4 6 8505254565860Recall0 2 4 6 83840424446AERUnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9(a) Arabic-English0 2 4 6 859606162%Precision0 2 4 6 86466687072Recall0 2 4 6 833343536373839 AER(b) English-Arabic0 2 4 6 860.56161.56262.563%Precision0 2 4 6 866687072 Recall0 2 4 6 8323334353637AER(c) Heuristically-symmetrizedFigure 3: Alignment qualities of each iteration for Arabic-English word alignment task.
The grey dashcurves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamondmarkers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.Source-Target Target-Source Heuristic DiscriminativeP R AER P R AER P R AER P R AERChBL 68.22 46.88 44.43 65.35 55.05 40.25 69.15 57.47 37.23 67.45 59.77 36.62NC +0.73 +0.71 -0.74 +1.14 +1.14 -1.15 +0.06 +1.07 -0.66 +0.15 +0.64 -0.420.6 +2.17 +2.28 -2.32 +1.17 +2.51 -1.97 -0.64 +2.65 -1.27 -0.39 +1.89 -0.870.7 +2.57 +2.32 -2.48 +1.94 +2.34 -2.19 -0.34 +2.30 -1.20 -0.28 +1.60 -0.760.8 +3.78 +3.27 -3.55 +2.94 +3.32 -3.18 -0.52 +3.32 -1.70 +0.69 +0.14 -0.890.9 +0.98 +1.13 -1.11 +1.48 +1.85 -1.71 -0.55 +1.94 -0.90 -0.58 +1.45 -0.54ArBL 58.41 50.42 45.88 59.08 64.84 38.17 60.35 66.99 36.50 68.93 63.94 33.66NC +2.98 +2.92 -2.96 +1.40 +2.06 -1.70 +0.97 +2.14 -1.49 -0.87 +2.37 -0.830.6 +6.69 +8.02 -7.47 +3.45 +6.70 -4.90 +2.62 +4.71 -3.55 +0.58 -0.55 +0.030.7 +8.38 +7.93 -8.16 +3.65 +5.26 -4.38 +2.83 +4.70 -3.67 +2.46 -0.42 -0.880.8 +6.48 +6.27 -6.39 +2.18 +3.54 -2.80 +1.81 +3.81 -2.70 +1.67 +2.30 -2.010.9 +4.02 +4.07 -4.07 +1.70 +3.10 -2.33 +0.62 +3.82 -2.03 +1.33 +2.70 -2.06Table 3: Improvement on word alignment quality on small corpus after 8 iterations.
BL stands forbaseline, and NC represents unconstrained Model 4 training, and 0.9, 0.8, 0.7, 0.6 are the thresholdsused in alignment link filtering.gorithm, the numbers are averaged across all iter-ations, the actual numbers of each iteration onlyhave small differences.
Although one might ex-pect that the quality of resulting alignment fromconstrained EM be proportional to the quality ofconstraints, from the numbers in Table 2 and 3,we are not able to induce a clear relationship be-tween them, and it could be language- or corpus-dependent.
However, in practice we nonethelessuse a held-out test set to tune this parameter.
The3540 2 4 6 869707172%Precision0 2 4 6 8464748495051Recall0 2 4 6 8404142434445AERUnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9(a) Chinese-English0 2 4 6 865.56666.56767.568%Precision0 2 4 6 85556575859Recall0 2 4 6 83738394041AER(b) English-Chinese0 2 4 6 868.668.86969.2%Precision0 2 4 6 85758596061Recall0 2 4 6 835363738 AER(c) Heuristically-symmetrizedFigure 4: Alignment qualities of each iteration for Chinese-English word alignment task.
The grey dashcurves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamondmarkers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.Ch-En En-Ch Heuristic DiscriminativeP R AER P R AER P R AER P R AERBL 73.51 50.14 40.38 68.82 57.66 37.31 72.98 60.23 34.01 72.10 61.63 33.55NC 73.23 50.38 40.30 68.30 58.00 37.27 72.39 60.99 33.80 72.07 61.81 33.450.8 76.27 52.90 37.53 70.26 60.26 35.11 72.75 63.49 32.19 72.64 63.29 32.35Table 4: Improvement on word alignment quality on moderate-size corpus, where BL and NC representsbaseline and non-constrained Model 4 trainingrelationship between quality of constraints andalignment results is an interesting topic for futureresearch.5.2 Effects on translation qualityIn this experiment we run the whole machinetranslation pipeline and evaluate the system onBLEU score.
We used the corpus LDC2006G05which contains 25 million words as training set,the same discriminative tuning set as previouslyused (100 sentence pairs) and the remaining21,763 sentence pairs from the hand-aligned cor-pus of the previous experiment are held-out testset for alignment qualities.
A 4-gram languagemodel trained from English GigaWord V1 and V2corpus was used.
The AER scores on the held-out test set are also provided for every iteration.Based on the observation in last experiment, weadopt the filtering threshold of 0.8.Similar to previous experiment, the heuristi-cally symmetrized alignments have lower preci-sions than their EMDC counterparts, however thegaps are smaller as shown in Table 4.
We observe2.85 and 2.21 absolute AER reduction on two di-rections, after symmetrization the gain on AERis 1.82.
Continuing Model 4 training appears tohave minimal effect on AER, and the improve-355I M NIST GALEmt06 mt02 mt03 mt04 mt05 mt08 ain db-nw db-wb dd-nw dd-wb aia0 G 31.00 31.80 29.89 32.63 29.33 24.24 26.92 24.48 28.44 24.261 D 30.65 31.60 30.04 32.89 29.34 24.52 0.12 27.43 24.72 28.32 24.30 0.14G 31.35 31.91 30.35 32.75 29.40 24.16 0.15 27.39 24.50 28.22 24.60 0.152 D 31.61 32.31 30.40 33.06 29.49 24.11 0.33 28.17 24.42 28.58 24.36 0.34G 31.14 31.94 30.42 32.86 29.49 24.15 0.20 27.31 24.51 27.50 24.02 0.033 D 31.29 32.39 30.28 33.19 29.60 24.41 0.43 27.64 25.32 28.55 24.71 0.47G 30.94 31.95 30.15 32.71 29.38 24.22 0.12 27.63 24.61 28.80 25.05 0.294 D 30.80 32.04 30.51 33.24 29.49 24.61 0.46 27.61 25.27 28.72 24.98 0.53G 30.68 31.81 30.33 33.05 29.28 24.41 0.26 27.20 24.79 28.43 24.50 0.245 D 30.93 31.89 29.96 32.89 29.37 24.50 0.17 27.75 24.50 29.05 24.90 0.33G 31.16 32.28 30.72 33.30 29.83 24.30 0.51 27.32 25.05 28.60 25.44 0.54Table 5: Improvement on translation alignment quality on moderate-size corpus, The column ain showsthe average improvement of BLEU scores for all NIST test sets (excluding the tuning set MT06), andcolumn aia is the average improvement on all unseen test sets.
The column M indicates the alignmentsource, G means the alignment comes from generative aligner, and D means discriminative alignerrespectively.
The number of iterations is shown in column I.ment mainly comes from the constraints.In the experiment, we use the Moses toolkit toextract phrases, tune parameters and decode.
Weuse the NIST MT06 test set as the tuning set,NIST MT02-05 and MT08 as unseen test sets.We also include results for four additional unseentest sets used in GALE evaluations: DEV07-Devnewswire part (dd-nw, 278 sentences) and We-blog part (dd-wb, 345 sentences), Dev07-Blindnewswire part (db-nw, 276 sentences and Weblogpart (db-wb, 312 sentences).
Table 5 presents theaverage improvement on BLEU scores in each it-eration.
As we can see from the results, in all iter-ations we got improvement on BLEU scores, andthe largest gain we have gotten is on the fifth it-eration, which has 0.51 average improvement onfive NIST test sets, and 0.54 average improvementacross all nine test sets.6 ConclusionIn this paper we presented a novel trainingscheme for word alignment task called EMDC.We also presented an extension of GIZA++ thatcan perform constrained EM training.
By inte-grating it with a CRF-based discriminative wordaligner and alignment link filtering, we can im-prove the alignment quality of both aligners itera-tively.
We experimented with small-size Chinese-English and Arabic English and moderate-sizeChinese-English word alignment tasks, and ob-served in all four mono-directional alignmentsmore than 3% absolute reduction on AER, withthe largest improvement being 8.16% absolute onArabic-to-English comparing to the baseline, and5.90% comparing to Model 4 training with thesame numbers of iterations.
On a moderate-sizeChinese-to-English tasks we also evaluated theimpact of the improved alignment on translationquality across nine test sets.
The 2% absoluteAER reduction resulted in 0.5 average improve-ment on BLEU score.Observations on the results raise several inter-esting questions for future research, such as 1)What is the relationship between the precision ofthe constraints and the quality of resulting align-ments after iterations, 2) The effect of using dif-ferent discriminative aligners, 3) Using alignersthat explicitly model empty words and null align-ments to provide additional constraints.
We willcontinue exploration on these directions.The extended GIZA++ is released to the re-search community as a branch of MGIZA++ (Gaoand Vogel, 2008), which is available online3.AcknowledgementThis work is supported by NSF CluE Project(NSF 08-560) and DARPA GALE project.3Accessible on Source Forge, with the URL:http://sourceforge.net/projects/mgizapp/356ReferencesAyan, Necip Fazil and Bonnie J. Dorr.
2006.
Goingbeyond aer: an extensive analysis of word align-ments and their impact on mt.
In Proceedingsof the 21st International Conference on Compu-tational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 9?16.Blunsom, Phil and Trevor Cohn.
2006.
Discrimina-tive word alignment with conditional random fields.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 65?72.Brown, Peter F., Vincent J.Della Pietra, StephenA.
Della Pietra, and Robert.
L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
In Computational Linguistics,volume 19(2), pages 263?331.Callison-Burch, C., D. Talbot, and M. Osborne.2004.
Statistical machine translation with word-and sentence-aligned parallel corpora.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, pages 175?183.Fraser, Alexander and Daniel Marcu.
2006.
Semi-supervised training for statistical word alignment.In ACL-44: Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics, pages 769?776.Fraser, Alexander and Daniel Marcu.
2007.
Get-ting the structure right for word alignment: LEAF.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 51?60.Gao, Qin and Stephan Vogel.
2008.
Parallel imple-mentations of word alignment tool.
In Proceedingsof the ACL 2008 Software Engineering, Testing, andQuality Assurance Workshop, pages 49?57.Gao, Qin and Stephan Vogel.
2010.
Consensus ver-sus expertise : A case study of word alignment withmechanical turk.
In NAACL 2010 Workshop on Cre-ating Speech and Language Data With MechanicalTurk, pages 30?34.Gao, Qin, Nguyen Bach, and Stephan Vogel.
2010.A semi-supervised word alignment algorithm withpartial manual alignments.
In In Proceedings ofthe ACL 2010 joint Fifth Workshop on StatisticalMachine Translation and Metrics MATR (ACL-2010WMT).Guzman, Francisco, Qin Gao, and Stephan Vogel.2009.
Reassessment of the role of phrase extrac-tion in pbsmt.
In The twelfth Machine TranslationSummit.Huang, Fei.
2009.
Confidence measure for wordalignment.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 932?940.Ittycheriah, Abraham and Salim Roukos.
2005.
Amaximum entropy word aligner for arabic-englishmachine translation.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Process-ing, pages 89?96.Liu, Yang, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 459?466.Moore, Robert C. 2005.
A discriminative frame-work for bilingual word alignment.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 81?88.Niehues, Jan. and Stephan.
Vogel.
2008.
Discrimina-tive word alignment via alignment matrix modeling.In Proceedings of the Third Workshop on StatisticalMachine Translation, pages 18?25.Och, Franz Joseph and Hermann Ney.
2003.
Asystematic comparison of various statistical align-ment models.
In Computational Linguistics, vol-ume 1:29, pages 19?51.Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 73?80.Vogel, Stephan, Hermann Ney, and Christoph Till-mann.
1996.
HMM based word alignment in statis-tical machine translation.
In Proceedings of 16th In-ternational Conference on Computational Linguis-tics), pages 836?841.357
