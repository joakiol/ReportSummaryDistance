Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 147?156,Coling 2014, Dublin, Ireland, August 24 2014.Building a Semantic Transparency Dataset of Chinese NominalCompounds: A Practice of Crowdsourcing MethodologyShichang Wang, Chu-Ren Huang, Yao Yao, Angel ChanDepartment of Chinese and Bilingual StudiesThe Hong Kong Polytechnic UniversityHung Hom, Kowloon, Hong Kongshi-chang.wang@connect.polyu.hk{churen.huang, y.yao, angel.ws.chan}@polyu.edu.hkAbstractThis paper describes the work which aimed to create a semantic transparency dataset of Chi-nese nominal compounds (SemTransCNC 1.0) by crowdsourcing methodology.
We firstly se-lected about 1,200 Chinese nominal compounds from a lexicon of modern Chinese and the SinicaCorpus.
Then through a series of crowdsourcing experiments conducted on the Crowdflowerplatform, we successfully collected both overall semantic transparency and constituent semantictransparency data for each of them.
According to our evaluation, the data quality is good.
Thiswork filled a gap in Chinese language resources and also practiced and explored the crowdsourc-ing methodology for linguistic experiment and language resource construction.1 IntroductionThe meaning of ????
(m?hu, horse-tiger, ?careless?)
has nearly nothing to do with neither ???
(m?,?horse?)
nor ???
(h?, ?tiger?).
However the meaning of ????
(d?ol?, road-way, ?road?)
is basicallyequal to ???
(d?o, ?road?)
or ???
(l?, ?way?).
And there are intermediate cases too, for instance, ????
(ji?ngh?, river-lake, ?all corners of the country?
), its meaning is not equal to ???
(ji?ng, ?river?
)plus ???
(h?, ?lake?
), but clear relatedness between them can be observed.
This phenomenon is calledsemantic transparency of compounds.
We distinguish between overall semantic transparency (OST) andconstituent semantic transparency (CST).
The semantic transparency of a compound, i.e., the overall se-mantic transparency, is the extent to which the compound retains its literal meaning in its actual meaning.The semantic transparency of a constituent of a compound, i.e., the constituent semantic transparency, isthe extent to which the constituent retains its meaning in the actual meaning of the compound.
Semanticsimilarity between the literal meaning and the actual meaning of a compound can be used to estimate theoverall semantic transparency of a compound, for the more the literal meaning is retained in the actualmeaning, the more similar they are.
The same technique can be used to estimate constituent semantictransparency.
Semantic transparency can be quantified; if we assign 0 to ?fully opaque?
and assign 1 to?fully transparent?, then semantic transparency can be quantified as a closed interval [0, 1].The quantitative analysis of semantic transparency must be supported by semantic transparencydatasets.
In previous semantic transparency related studies on Chinese compounds, some researcherscreated some datasets to support their own studies.
But this kind of datasets are usually relatively smalland restrictive, so cannot be used widely, for example, (???
and?
?, 2001; Myers et al., 2004;??
?, 2008; Mok, 2009), etc.
Some datasets, although large enough and can be used in other studies, arenot publicly accessible, for example, (???
and??
?, 1999;??
and??
?, 2005), etc.
A largeand publicly accessible semantic transparency dataset of Chinese compounds is still a gap in Chineselanguage resources.Crowdsourcing, as an emergingmethod of data collection and resource construction (Snow et al., 2008;Callison-Burch and Dredze, 2010; Munro et al., 2010; Schnoebelen and Kuperman, 2010; Gurevych andZesch, 2013; Wang et al., 2013) and an emerging method of behavioral experiment (Paolacci et al., 2010;This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/147Berinsky et al., 2011; Mason and Suri, 2012; Rand, 2012; Crump et al., 2013), is attracting more andmoreattention from the field of language study and language computing.
As a method of data collection andresource construction, it has the advantages of high speed and low cost, etc.
It can use redundancy to filterout noise in order to improve data quality; if used properly, it can produce expert-level data.
As a methodof experiment, besides the above advantages, it also has the following ones, (1) it is easier to obtain largesamples, because the amount of potential participants is huge; (2) the diversity of participants is good,because the participants are from different places and have different backgrounds; (3) crowdsourcingenvironments are usually anonymous, so it is easier to collect certain sensitive data.2 Method2.1 Compound SelectionWe use the following criteria to select compounds, (1) they are disyllabic nominal compounds; (2) eachof them has the structure NN, AN, or VN; (3) they are composed of free morphemes; (4) they havemid-range word frequencies; and (5) they are used in both Mainland China and Taiwan.
And we selectcompounds according to the following procedure:(1) Extract monosyllabic nouns, adjectives and verbs mainly according to ?The Dictionary of Con-temporary Chinese (the 6th edition)?
(??????,?
6?
), and thus we get three sets, a) the set ofmonosyllabic nouns, N; b) the set of monosyllabic adjectives, A; and c) the set of monosyllabic verbs, V.(2) Extract the words of the structure NN, AN, or VN 1 from the ?Lexicon of Common Words inContemporary Chinese?
(????????).
In this step, NN means both morphemes of the wordappear in the set N; AN means the first morpheme appears in the set A and the second appears in the setN; VN means the first morpheme appears in the set V and the second appears in the set N. After this step,we get ?word list 1?.
(3) Extract the words which have mid-range frequencies 2 from the Sinica Corpus 4.0 (Chen et al.,1996).
These words are represented in traditional Chinese characters.
We convert them into simplifiedChinese characters and only reserve the words which also appear in ?word list 1?.
After this step, we get?word list 2?.
(4) Manually verify ?word list 2?
to generate the final list.
Things need to be verified include thefollowing aspects.
(a) Because in ?word list 2?
word structures are judged automatically, there are manyerrors, so we have to verify the correctness of the word structure judgments.
(b) We have to make surethat the morphemes of each word are free morphemes.
(c) We also need to delete some proper nouns.The words we selected appear in both Sinica Corpus 4.0 and ?Lexicon of Common Words in Contem-porary Chinese?.
Since there is no completely reliable criterion to identify Chinese word, appearing intwo lexicons ensures their word identity.
This also ensures that they are used in both Mainland China andTaiwan, and further means they are quite possible to be shared in other Chinese language communities,for example Hong Kong, Macau, and Singapore, etc.According to above criteria and procedure, we selected a total of 1,176 words.
664 (56.46%) of themhave the structure NN; 322 (27.38%) have the structure AN; and 190 (16.16%) have the structure VN.2.2 Experimental DesignNormally, a crowdsourcing experiment should be reasonably small in size.
We randomly divide these1,176 words into 21 groups, Gi(i = 1, 2, 3, ..., 21); each group has 56 words.1See???
and???
(1998), and Huang (1998) for relevant statistics.2We use cumulative frequency feature to determine mid-range frequency.
Sort the word frequency list of Sinica Corpus4.0 descendingly; then calculate cumulative frequency word by word until each word corresponds with a cumulative frequencyvalue; finally, plot a curve on a coordinate plane whose x-axis represents the ranks of words in the sorted list, and the y-axisrepresents cumulative frequency values.
Very apparently, this curve can be divided into three successive phases; the wordswithin each phase have similar word frequency features.
According to this, we identify three word frequency categories, 5,163high-frequency words (frequency range: [182, 581823], cumulative frequency range: [0%, 80%]), 19,803 mid-range frequencywords (frequency range: [23, 181], cumulative frequency range: (80%, 93%]), and 177,496 low-frequency words (frequencyrange: [1, 22], cumulative frequency range: (93%, 100%]).
Sinica Corpus 4.0 contains about 11.2 million word tokens.148QuestionnairesWe collect overall semantic transparency (OST) and constituent semantic transparency (CST) data ofthese words.
In order to avoid interaction, we designed two kinds of questionnaires to collect OST dataand CST data respectively.
SoGi(i = 1, 2, 3, ..., 21) has two questionnaires, one OST questionnaire forOST data collection and one CST questionnaire for CST data collection.
Besides titles and instructions,each questionnaire has 3 sections.
Section 1 is used to collect identity information includes gender, age,education and location.
Section 2 contains four very simple questions about the Chinese language; thefirst two questions are open-ended Chinese character identification questions, the third question is a close-ended homophonic character identification question, and the fourth one is a close-ended antonymouscharacter identification question; different questionnaires use different questions.
Section 3 contains thequestions for semantic transparency data collection.
Suppose AB is a disyllabic nominal compound, weuse the following question to collect its OST rating scores: ?How is the sum of the meanings of A andB similar to the meaning of AB??
And use the following two questions to collect its CST rating scoresof its two constituents: ?How is the meaning of A when it is used alone similar to its meaning in AB?
?and ?How is the meaning of B when it is used alone similar to its meaning in AB??.
7-point scales areused in section 3; 1 means ?not similar at all?
and 7 means ?almost the same?.In order to evaluate the data received in the experiments, we embedded some evaluation devices in thequestionnaires.
We mainly evaluated intra-group and inter-group consistency; and if the data have goodintra-group and inter-group consistency, we can believe that the data quality is good.
In each group wechoose two words and make them appear twice, we call them intra-group repeated words and we can usethem to evaluate the intra-group consistency.
We insert into each group two same extra words, w1???
?, w2???
?, to evaluate the inter-group consistency.Quality Control MeasuresOn a crowdsourcing platform like Crowdflower, the participants are anonymous, they may try to cheatand submit invalid data, and they may come from different countries and speak different languages ratherthan the required one.
There may be spammers who continuously submit invalid data at very high speedand they may even bypass the quality control measures to cheat for money.
In order to ensure that theparticipants are native Chinese speakers and to improve data quality, we use the following measures, (1)a participant must correctly answer the first two Chinese character identification questions in the section2s of the questionnaires, and he/she must correctly answer at least one of the last two questions in thesesection 2s; (2) If a participant do not satisfy the above conditions, he/she will not see Section 3s; (3) eachword stimulus in section 3s has an option which allows the participants to skip it in case he/she does notrecognize that word; (4) all the questions in the questionnaires must be answered except the ones whichallow to be skipped and are explicitly claimed to be skipped; (5) we wrote a monitor program to detectand resist spammers automatically; (6) after the experiment is finished, we will analyze the data and filterout invalid data, and we will discuss this in detail in section 3.2.3 Experimental Platform and ProcedureWe choose Crowdflower as our experimental platform, because according to our previous experiments,it is a feasible crowdsourcing platform to collect Chinese language data.
We create one task for eachquestionnaire on the platform; there are 21 groups of word and each group has one OST questionnaireand one CST questionnaire, so there are a total of 42 tasksT osti, Tcsti(i = 1, 2, 3, ..., 21).
We publish these42 tasks successively, and for each task we create a monitor program to detect and resist spammers.
Allof these tasks use the following parameters: (1) each task will collect 90 responses; (2) we pay 0.15USDfor each response of OST questionnaire and pay 0.25USD for each response of CST questionnaire; (3)each worker account of Crowdflower can only submit one response for each questionnaire and each IPaddress can only submit one response for each questionnaire; (4) we only allow the workers from thefollowing regions (according to IP addresses) to submit data: Mainland China, Hong Kong, Macau,Taiwan, Singapore, Malaysia, USA, UK, Canada, Australia, Germany, France, Italy, New Zealand, andIndonesia; and we can dynamically disable or enable certain regions on demand in order to ensure bothdata quality and quantity.1493 Data Refinement and Result CalculationTheOST dataset produced by theOST taskT osti(i = 1, 2, 3, ..., 21) isDosti.
The CST dataset produced bythe CST task T cstiis Dcsti.
Each dataset contains 90 responses.
Because of the nature of crowdsourcingenvironment, there are many invalid responses in each dataset; so firstly we need to filter them out inorder to refine the data.
A response is invalid if (1) its completion time is less than 135 seconds (forOST responses); its completion time is less than 250 seconds (for CST responses) 3; or (2) it failed tocorrectly answer the first two questions of section 2s of the questionnaires; or (3) it wrongly answeredthe last two questions of section 2s of the questionnaires; or (4) it skipped one or more words in section3s of the questionnaires; or (5) it used less than two numbers on the 7-point scales in section 3s of thequestionnaires.
The statistics of valid response are shown in Table 1.The OST dataset Dosti(i = 1, 2, 3, ..., 21) contains nivalid responses; it means word w in the OSTdataset of the ith group has niOST rating scores; the arithmetic mean of these niOST rating scores isthe OST result of word w. The CST results of the two constituents of word w are calculated using thesame algorithm.OST CSTGin % n %G154 60 59 65.56G260 66.67 59 65.56G355 61.11 60 66.67G459 65.56 59 65.56G550 55.56 55 61.11G655 61.11 52 57.78G753 58.89 53 58.89G860 66.67 50 55.56G948 53.33 52 57.78G1057 63.33 62 68.89G1146 51.11 56 62.22G1248 53.33 58 64.44G1351 56.67 52 57.78G1450 55.56 50 55.56G1552 57.78 52 57.78G1657 63.33 56 62.22G1750 55.56 46 50.55G1851 56.67 53 58.89G1950 55.56 49 54.44G2050 55.56 47 52.22G2150 55.56 50 55.56Max 60 66.67 62 68.89Min 46 51.11 46 50.55Median 51.5 57.22 53 58.89Mean 52.67 58.52 53.81 59.76SD 4.09 4.55 4.49 5.04Table 1: The Amount of Valid Response in the OST and CST Datasets of Each Group4 EvaluationThree kinds of evaluation measures are used, (1) the intra-group consistency of the OST and CST results,(2) the inter-group consistency of the OST and CST results, and (3) the correlation between the OST andCST results.3Each OST questionnaire has about 70 questions, and each CST questionnaire has about 130; in an OST or CST question-naire, almost all the questions are the same except the stimuli words and can be instantly answered by intuition; note that aparticipant can take part in as many as 42 tasks; according to our test, if a participant is familiar with the tasks, he/she cananswer each question in less than 2 seconds (less than 1 second to identify the stimulus word and another less than 1 secondto rate it) without difficulty.
70 ?
2 = 140 seconds, the expected time should be less than this, so we use 135 seconds asthe temporal threshold for valid OST responses.
The calculation of the temporal threshold for valid CST responses is similar,130?
2 = 260 seconds, the expected time should be less than this, so we use 250 seconds.1504.1 Intra-group ConsistencyIn each group Gi(i = 1, 2, 3, ..., 21), we selected two words wi,1, wi,2(intra-group repeated words) andmade them appear twice between which there is enough distance; we can calculate the difference valuesbetween the results of the two appearances of these words.Intra-group Consistency of OST ResultsThere are 21 groups and in each group there are two intra-group repeated words, so there are a total of 42such words.
Each intra-group repeated word appears twice, so we can obtain two OST results r1, r2.
Thedifference value between the two results, d = |r1?
r2|, of each intra-group repeated word is calculated,so there are 42 difference values.
Among them, the maximum value is 0.29; the minimum value is 0;the median is 0.1; their mean is 0.11; and their standard deviation is 0.08; all of these values are low andindicate that these OST datasets have good intra-group consistency (see Table 2).Intra-group Consistency of CST ResultsEach intra-group repeated word has two constituents, c1, c2, so each constituent gets two CST results, i.e.,rc1,1, rc1,2and rc2,1, rc2,2.
We calculate the difference values for the two constituents, d1= |rc1,1?rc1,2|and d2= |rc2,1?
rc2,2|, and get 42 difference values of the first constituents and 42 difference valuesof the second constituents.
Among the difference values of the first constituents, the maximum valueis 0.27; the minimum value is 0; the median is 0.09; their mean is 0.1, and their standard deviation is0.07; all of these values are low, this indicates that the CST results of the first constituents in the CSTdatasets of the 21 groups have good intra-group consistency.
Among the difference values of the secondconstituents, the maximum value is 0.36; the minimum value is 0; the median is 0.07; their mean is 0.09,and their standard deviation is 0.09; all of these values are low; this indicates that the CST results of thesecond constituents in the CST datasets of the 21 groups have good intra-group consistency (see Table3).
So these 21 CST datasets have good intra-group consistency.4.2 Inter-group ConsistencyWe inserted two inter-group repeated words, w1???
?, w2???
?, into all of these 21 groups Gi(i =1, 2, 3, ..., 21); we can evaluate the inter-group consistency by comparing their semantic transparencyrating results in different groups.
Since w1, w2appear in all OST and CST questionnaires of 21 groups,we can obtain (1) 21 OST results of w1, (2) 21 OST results of w2, (3) 21 CST results of each of the twoconstituents w1,c1, w1,c2of w1, and (4) 21 CST results of each of the two constituents w2,c1, w2,c2of w2.Standard deviation can be used to measure difference, for example, the standard deviation of the 21 OSTresults of w1is 0.2; this value is small and indicates high consistency; because these 21 results are fromthe OST datasets of 21 groups respectively, so we can say that these 21 OST datasets have good inter-group consistency.
The standard deviation of the 21 OST results of w2is 0.14; the standard deviation of21 CST results of the first constituent of w1is 0.2, and that of the second is 0.18; the standard deviationof 21 CST results of the first constituent of w2is 0.15, and that of the second is 0.2; all of these valuesare small and all of them indicate good inter-group consistency (see Table 4).4.3 Correlation between OST and CST ResultsEach compound in the datasets has two constituents; both constituents affect the OST of the compound,but neither of them can solely determine the OST of the compound.
So the mean of the two CST valuesof a compound is a fairly good estimation of its OST value.
Therefore, if the datasets are reliable, in eachgroup, we should observe strong correlation between the OST results and their corresponding means ofthe CST results.
For each group, we calculate three Pearson product-moment correlation coefficients (r);r1is the r between the OST results and their corresponding CST results of the first constituents; r2isthe r between the OST results and their corresponding CST results of the second constituents; and r3isthe r between the OST results and their corresponding means of the CST results.
The r3values of the 21groups are all greater than 0.9 which indicates very strong correlation; among them, the maximum valueis 0.96; the minimum value is 0.91; and their mean is 0.94 (SD = 0.02); the r1and r2values are also151Giwi,1/2r1r2dG1??
5.26 5.26 0??
3.57 3.61 0.04G2??
5.63 5.75 0.12??
2.68 2.9 0.22G3??
5.67 5.58 0.09??
3.51 3.62 0.11G4??
5.31 5.32 0.02??
3.19 3.02 0.17G5??
5.36 5.32 0.04??
3.12 3.3 0.18G6??
5.53 5.4 0.13??
5.25 4.96 0.29G7??
5.25 5.23 0.02??
4.19 4.11 0.08G8??
5.48 5.33 0.15??
3.2 3.37 0.17G9??
5.19 5.19 0??
3.69 3.75 0.06G10??
5.49 5.63 0.14??
3.46 3.54 0.09G11??
5.48 5.39 0.09??
3.26 3.24 0.02G12??
5.19 5.4 0.21??
3.6 3.54 0.06G13??
5.47 5.39 0.08??
3.37 3.41 0.04G14??
5.54 5.52 0.02??
3.46 3.56 0.1G15??
5.54 5.37 0.17??
3.29 3.56 0.27G16??
5.49 5.53 0.04??
3.82 4.07 0.25G17??
5.2 5.38 0.18??
3.76 3.76 0G18??
5.31 5.18 0.14??
3.41 3.25 0.16G19??
5.22 5.28 0.06??
4.04 3.88 0.16G20??
5.28 5.18 0.1??
4.04 3.84 0.2G21??
5.06 5.02 0.04??
3.8 4 0.2Max 0.29Min 0Median 0.1Mean 0.11SD 0.08Table 2: The Intra-group Consistency of the OST Results of Each Groupreasonably high (see Table 5)4.
The results support the reliability of these datasets.5 Merging and NormalizationThe evaluation results show that the collected data are generally reliable and have relatively high intra-group and inter-group consistency which further indicate that these datasets share similar scale and arebasically comparable, so we can merge the 21 OST datasets into one big OST dataset Dostand mergethe 21 CST datasets into one big CST dataset Dcst.
When we merge these datasets, we delete all theextra words which are used to evaluate the inter-group consistency; for the repeated words which are4After merging and normalization (see Section 5), we calculated these three correlation coefficients betweenDostandDcst,the results are r1= 0.68, r2= 0.68, r3= 0.87.152c1c2Giwi,1/2rc1,1rc1,2d1rc2,1rc2,2d2G1??
3.83 4.05 0.22 5.49 5.42 0.07??
2.88 3.03 0.15 3.92 3.92 0G2??
5.12 5.22 0.1 5.24 5.1 0.14??
4.27 4.27 0 2.19 2.51 0.32G3??
5.12 5.08 0.03 5.35 5.4 0.05??
2.92 2.95 0.03 3.22 3.42 0.2G4??
4.51 4.34 0.17 5.56 5.27 0.29??
2.39 2.49 0.1 4.22 4.12 0.1G5??
4.75 4.64 0.11 5.09 5.15 0.05??
2.29 2.4 0.11 4.67 4.76 0.09G6??
5.4 5.23 0.17 5.35 5.4 0.06??
5.08 5.02 0.06 5.38 5.46 0.08G7??
4.7 4.83 0.13 5.13 5.13 0??
3.85 3.94 0.09 4.45 4.57 0.11G8??
5.06 4.88 0.18 5.28 5.3 0.02??
3.24 3.14 0.1 3.36 3.16 0.2G9??
5 4.98 0.02 5 4.98 0.02??
3.63 3.71 0.08 3.71 3.83 0.12G10??
4.53 4.6 0.06 5.37 5.39 0.02??
3.13 3.21 0.08 3.15 3.16 0.02G11??
4.45 4.55 0.11 5.36 5.55 0.2??
3.8 3.79 0.02 2.64 3 0.36G12??
4.69 4.52 0.17 4.97 4.9 0.07??
3.03 3.21 0.17 3.28 3.4 0.12G13??
4.15 4.19 0.04 5.15 5.27 0.12??
2.52 2.79 0.27 3.44 3.42 0.02G14??
4.42 4.36 0.06 5.14 5.12 0.02??
3.56 3.5 0.06 3.08 3.06 0.02G15??
5.08 5.02 0.06 5.06 5.13 0.08??
3.21 3 0.21 3.46 3.5 0.04G16??
4.34 4.34 0 5.11 5.09 0.02??
3.8 3.63 0.18 3.32 3.38 0.05G17??
4.76 4.72 0.04 4.74 4.87 0.13??
3.93 3.96 0.02 3.89 3.87 0.02G18??
4.26 4.32 0.06 4.77 4.7 0.08??
3.4 3.36 0.04 2.74 2.68 0.06G19??
4.63 4.61 0.02 4.57 4.49 0.08??
3.55 3.29 0.27 3.53 3.41 0.12G20??
4.98 4.91 0.06 5.15 5.17 0.02??
2.94 2.96 0.02 4.7 4.45 0.26G21??
4.68 4.56 0.12 5 4.98 0.02??
3.68 3.88 0.2 3.66 3.6 0.06Max 0.27 0.36Min 0 0Median 0.09 0.07Mean 0.1 0.09SD 0.07 0.09Table 3: The Intra-group Consistency of the CST Results of Each Groupused to evaluate the intra-group consistency, the final result of each of them is the mean of its two results.According to our definition, the range of semantic transparency value is [0, 1], but the experimental resultsare obtained using 7-point scales, so we need to normalize these results in order to map them to the range[0, 1].
The normalized OST and CST results will be merged into Dostand Dcstrespectively.
Assumethat, in the dataset Dost, the OST result of the ith (i = 1, 2, 3, ..., 1176) word is Swi, and the normalizedresult is S?wi, then,S?wi=Swi?
16153OST CSTGiw1w2w1,c1w1,c2w2,c1w2,c2G12.94 5.52 2.85 2.97 4.56 5.56G23.6 5.55 3.15 3.2 4.92 5.75G33.51 5.64 3.17 3.23 4.75 5.58G43.81 5.68 3.53 3.59 4.58 5.42G53.74 5.46 3.38 3.56 4.64 5.55G63.65 5.55 3.63 3.56 4.85 5.65G73.58 5.51 3.47 3.58 4.75 5.23G83.22 5.53 3.4 3.36 4.8 5.48G93.31 5.15 3.48 3.52 4.69 5.42G103.58 5.53 3.42 3.34 4.69 5.27G113.7 5.67 3.46 3.32 4.52 5.36G123.33 5.71 3.19 3.28 4.41 5.14G133.47 5.78 3.58 3.56 4.73 5.38G143.48 5.58 2.94 2.94 4.42 5.3G153.4 5.42 3.42 3.27 4.62 5.1G163.47 5.56 3.34 3.25 4.59 5.16G173.6 5.56 3.3 3.26 4.5 5.17G183.67 5.67 3.36 3.34 4.47 5G193.28 5.56 3.2 3.29 4.37 5.18G203.56 5.48 3.21 3.36 4.72 5.34G213.62 5.32 3.2 3.28 4.5 5.24Max 3.81 5.78 3.63 3.59 4.92 5.75Min 2.94 5.15 2.85 2.94 4.37 5Median 3.56 5.55 3.36 3.32 4.62 5.34Mean 3.5 5.54 3.32 3.34 4.62 5.35SD 0.2 0.14 0.2 0.18 0.15 0.2Table 4: The Inter-group Consistency of the OST and CST ResultsAnd assume that, in the datasetDcst, the CST result of the jth (j = 1, 2) constituent of the ith word isSci,j, and the normalized result is S?ci,j, then,S?ci,j=Sci,j?
166 DistributionInfluenced by outliers and perhaps other factors, the OST and CST results cannot cover the whole rangeof the scale [0, 1]; both ends shrink towards the central point 0.5, and the shrinkage of each end is about0.2; nevertheless, the results can still assign proper ranks of semantic transparency to the compounds andtheir constituents which are generally consistent with our intuitions.
Among the normalized OST results,the maximum is 0.81; the minimum is 0.28; the median is 0.63; and their mean is 0.62 (SD = 0.09).Among the normalized CST results of the first constituents (C1.CST results), the maximum is 0.77; theminimum is 0.19; the median is 0.57; and their mean is 0.56 (SD = 0.09).
And among the normalizedCST results of the second constituents (C2.CST results), the maximum is 0.79; the minimum is 0.22; themedian is 0.6; and their mean is 0.58 (SD = 0.1).
The distributions of OST, C1.CST, and C2.CST resultsare similar; all of them are negatively skewed (see Figure 1), and their estimated skewnesses are ?0.66,?0.77, and ?0.63 respectively.
These distributions exhibit that more compounds and their constituentsin our datasets have relatively high semantic transparency values.7 ConclusionThis work created a dataset of semantic transparency of Chinese nominal compounds (SemTransCNC1.0), which filled a gap in Chinese language resources.
It contains the overall and constituent semantictransparency data of about 1,200 Chinese disyllabic nominal compounds and can support semantic trans-parency related studies of Chinese compounds, for example, theoretical, statistical, psycholinguistic, and154Gir1r2r3G10.68 0.68 0.91G20.72 0.72 0.93G30.76 0.78 0.96G40.76 0.77 0.96G50.75 0.56 0.95G60.63 0.72 0.91G70.83 0.78 0.94G80.76 0.77 0.96G90.68 0.81 0.95G100.84 0.83 0.95G110.78 0.71 0.91G120.72 0.77 0.95G130.85 0.86 0.96G140.69 0.85 0.95G150.68 0.82 0.95G160.82 0.85 0.95G170.79 0.83 0.94G180.81 0.86 0.96G190.76 0.8 0.95G200.76 0.75 0.94G210.73 0.86 0.96Max 0.85 0.86 0.96Min 0.63 0.56 0.91Median 0.76 0.78 0.95Mean 0.75 0.78 0.94SD 0.06 0.07 0.02Table 5: The Correlation Coefficients between the OST and CST ResultsNormalized OST ResultsFrequency0.0 0.2 0.4 0.6 0.8 1.00100200300Normalized C1.CST ResultsFrequency0.0 0.2 0.4 0.6 0.8 1.00100200300Normalized C2.CST ResultsFrequency0.0 0.2 0.4 0.6 0.8 1.00100200300Figure 1: The Distributions of the Normalized OST and CST Resultscomputational studies, etc.
And this work was also a successful practice of crowdsourcing method for lin-guistic experiment and language resource construction.
Large scale language data collection experimentswhich require large amount of participants are usually very difficult to conduct in laboratories using thetraditional paradigm.
Crowdsourcing method enabled us to finish the data collection task within rela-tively short period of time and relatively low budget (1,000USD); during the process of the experiment,we needed not to organize and communicate with the participants, it saved a lot of time and energy.
Theparticipants are from all over the world, so it is better than traditional laboratory method in the aspectof participant diversity.
The data collected have very good intra-group and inter-group consistency, theOST and CST data highly correlate with each other as expected, and the results are consistent with ourintuitions: all of these indicate good data quality.
The methods of questionnaire design, quality control,data refinement, evaluation, emerging, and normalization can be used in crowdsourcing practices of thesame kind.155AcknowledgementsThe work described in this paper was supported by grants from the Research Grants Council of the HongKong Special Administrative Region, China (Project No.
544011 & 543512).ReferencesAdam J Berinsky, Gregory A Huber, and Gabriel S Lenz.
2011.
Using mechanical turk as a subject recruitmenttool for experimental research.
Submitted for review.Chris Callison-Burch and Mark Dredze.
2010.
Creating speech and language data with amazon?s mechanicalturk.
In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?sMechanical Turk, pages 1?12.
Association for Computational Linguistics.Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and Hui-Li Hsu.
1996.
Sinica Corpus: Design Methodologyfor Balanced Corpora.
In B.-S. Park and J.B. Kim, editors, Proceeding of the 11th Pacific Asia Conference onLanguage, Information and Computation, pages 167?176.
Seoul:Kyung Hee University.Matthew JC Crump, John V McDonnell, and Todd M Gureckis.
2013.
Evaluating amazon?s mechanical turk as atool for experimental behavioral research.
PloS one, 8(3):e57410.Iryna Gurevych and Torsten Zesch.
2013.
Collective intelligence and language resources: introduction to thespecial issue on collaboratively constructed language resources.
Language Resources and Evaluation, 47(1):1?7.Shuanfan Huang.
1998.
Chinese as a headless language in compounding morphology.
New approaches to Chineseword formation: Morphology, phonology and the lexicon in modern and ancient Chinese, pages 261?284.Winter Mason and Siddharth Suri.
2012.
Conducting behavioral research on amazon?s mechanical turk.
Behaviorresearch methods, 44(1):1?23.Leh Woon Mok.
2009.
Word-superiority effect as a function of semantic transparency of chinese bimorphemiccompound words.
Language and Cognitive Processes, 24(7-8):1039?1081.Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, TylerSchnoebelen, and Harry Tily.
2010.
Crowdsourcing and language studies: the new generation of linguistic data.In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?sMechanical Turk, pages 122?130.
Association for Computational Linguistics.James Myers, Bruce Derwing, and Gary Libben.
2004.
The effect of priming direction on reading chinese com-pounds.
Mental Lexicon Working Papers, 1:69?86.Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis.
2010.
Running experiments on amazon mechanicalturk.
Judgment and Decision making, 5(5):411?419.David G Rand.
2012.
The promise of mechanical turk: How online labor markets can help theorists run behavioralexperiments.
Journal of theoretical biology, 299:172?179.Tyler Schnoebelen and Victor Kuperman.
2010.
Using amazon mechanical turk for linguistic research.
Psi-hologija, 43(4):441?464.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural language tasks.
In Proceedings of the conference on empiricalmethods in natural language processing, pages 254?263.
Association for Computational Linguistics.AoboWang, CongDuyVuHoang, andMin-YenKan.
2013.
Perspectives on crowdsourcing annotations for naturallanguage processing.
Language Resources and Evaluation, 47:9?31.???.
2008.
????????????????????.
?????
?, 1:82?90.???
and??.
2001.
??????????????????.
?????
?, 1:53?59.???
and???.
1999.
?????????,??????????.
???
?, 31(3):266?273.???
and???.
1998.
?????????????????.
?????
?, 2(1):13.??
and???.
2005.
????????????????????.
???
?, 28(6):1358?1360.156
