Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558?566,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsEvaluating the Inferential Utility of Lexical-Semantic ResourcesShachar Mirkin, Ido Dagan, Eyal ShnarchComputer Science Department, Bar-Ilan UniversityRamat-Gan 52900, Israel{mirkins,dagan,shey}@cs.biu.ac.ilAbstractLexical-semantic resources are used ex-tensively for applied semantic inference,yet a clear quantitative picture of theircurrent utility and limitations is largelymissing.
We propose system- andapplication-independent evaluation andanalysis methodologies for resources?
per-formance, and systematically apply themto seven prominent resources.
Our find-ings identify the currently limited recall ofavailable resources, and indicate the po-tential to improve performance by exam-ining non-standard relation types and bydistilling the output of distributional meth-ods.
Further, our results stress the needto include auxiliary information regardingthe lexical and logical contexts in whicha lexical inference is valid, as well as itsprior validity likelihood.1 IntroductionLexical information plays a major role in seman-tic inference, as the meaning of one term is of-ten inferred form another.
Lexical-semantic re-sources, which provide the needed knowledge forlexical inference, are commonly utilized by ap-plied inference systems (Giampiccolo et al, 2007)and applications such as Information Retrieval andQuestion Answering (Shah and Croft, 2004; Pascaand Harabagiu, 2001).
Beyond WordNet (Fell-baum, 1998), a wide range of resources has beendeveloped and utilized, including extensions toWordNet (Moldovan and Rus, 2001; Snow et al,2006) and resources based on automatic distri-butional similarity methods (Lin, 1998; Panteland Lin, 2002).
Recently, Wikipedia is emerg-ing as a source for extracting semantic relation-ships (Suchanek et al, 2007; Kazama and Tori-sawa, 2007).As of today, only a partial comparative pictureis available regarding the actual utility and limi-tations of available resources for lexical-semanticinference.
Works that do provide quantitativeinformation regarding resources utility have fo-cused on few particular resources (Kouylekov andMagnini, 2006; Roth and Sammons, 2007) andevaluated their impact on a specific system.
Mostoften, works which utilized lexical resources donot provide information about their isolated con-tribution; rather, they only report overall per-formance for systems in which lexical resourcesserve as components.Our paper provides a step towards clarify-ing this picture.
We propose a system- andapplication-independent evaluation methodologythat isolates resources?
performance, and sys-tematically apply it to seven prominent lexical-semantic resources.
The evaluation and analysismethodology is specified within the Textual En-tailment framework, which has become popular inrecent years for modeling practical semantic infer-ence in a generic manner (Dagan and Glickman,2004).
To that end, we assume certain definitionsthat extend the textual entailment paradigm to thelexical level.The findings of our work provide useful insightsand suggested directions for two research com-munities: developers of applied inference systemsand researchers addressing lexical acquisition andresource construction.
Beyond the quantitativemapping of resources?
performance, our analysispoints at issues concerning their effective utiliza-tion and major characteristics.
Even more impor-tantly, the results highlight current gaps in exist-ing resources and point at directions towards fill-ing them.
We show that the coverage of mostresources is quite limited, where a substantialpart of recall is attributable to semantic relationsthat are typically not available to inference sys-tems.
Notably, distributional acquisition methods558are shown to provide many useful relationshipswhich are missing from other resources, but theseare embedded amongst many irrelevant ones.
Ad-ditionally, the results highlight the need to rep-resent and inference over various aspects of con-textual information, which affect the applicabilityof lexical inferences.
We suggest that these gapsshould be addressed by future research.2 Sub-sentential Textual EntailmentTextual entailment captures the relation between atext t and a textual statement (termed hypothesis)h, such that a person reading t would infer that his most likely correct (Dagan et al, 2005).The entailment relation has been defined insofarin terms of truth values, assuming that h is a com-plete sentence (proposition).
However, there aremajor aspects of inference that apply to the sub-sentential level.
First, in certain applications, thetarget hypotheses are often sub-sentential.
For ex-ample, search queries in IR, which play the hy-pothesis role from an entailment perspective, typ-ically consist of a single term, like drug legaliza-tion.
Such sub-sentential hypotheses are not re-garded naturally in terms of truth values and there-fore do not fit well within the scope of the textualentailment definition.
Second, many entailmentmodels apply a compositional process, throughwhich they try to infer each sub-part of the hy-pothesis from some parts of the text (Giampiccoloet al, 2007).Although inferences over sub-sentential ele-ments are being applied in practice, so far thereare no standard definitions for entailment at sub-sentential levels.
To that end, and as a prerequisiteof our evaluation methodology and our analysis,we first establish two relevant definitions for sub-sentential entailment relations: (a) entailment of asub-sentential hypothesis by a text, and (b) entail-ment of one lexical element by another.2.1 Entailment of Sub-sentential HypothesesWe first seek a definition that would capture theentailment relationship between a text and a sub-sentential hypothesis.
A similar goal was ad-dressed in (Glickman et al, 2006), who definedthe notion of lexical reference to model the factthat in order to entail a hypothesis, the text hasto entail each non-compositional lexical elementwithin it.
We suggest that a slight adaptation oftheir definition is suitable to capture the notion ofentailment for any sub-sentential hypotheses, in-cluding compositional ones:Definition 1 A sub-sentential hypothesis h is en-tailed by a text t if there is an explicit or impliedreference in t to a possible meaning of h.For example, the sentence ?crude steel outputis likely to fall in 2000?
entails the sub-sententialhypotheses production, steel production and steeloutput decrease.Glickman et al, achieving good inter-annotatoragreement, empirically found that almost all non-compositional terms in an entailed sentential hy-pothesis are indeed referenced in the entailing text.This finding suggests that the above definition isconsistent with the original definition of textualentailment for sentential hypotheses and can thusmodel compositional entailment inferences.We use this definition in our annotation method-ology described in Section 3.2.2 Entailment between Lexical ElementsIn the majority of cases, the reference to an?atomic?
(non-compositional) lexical element e inh stems from a particular lexical element e?
in t,as in the example above where the word outputimplies the meaning of production.To identify this relationship, an entailment sys-tem needs a knowledge resource that would spec-ify that the meaning of e?
implies the meaning ofe, at least in some contexts.
We thus suggest thefollowing definition to capture this relationship be-tween e?
and e:Definition 2 A lexical element e?
entails anotherlexical element e, denoted e?
?e, if there existsome natural (non-anecdotal) texts containing e?which entail e, such that the reference to the mean-ing of e can be implied solely from the meaning ofe?
in the text.
(Entailment of e by a text follows Definition 1).We refer to this relation in this paper as lexicalentailment1, and call e?
?
e a lexical entailmentrule.
e?
is referred to as the rule?s left hand side(LHS) and e as its right hand side (RHS).Currently there are no knowledge resources de-signed specifically for lexical entailment model-ing.
Hence, the types of relationships they cap-ture do not fully coincide with entailment infer-ence needs.
Thus, the definition suggests a spec-ification for the rules that should be provided by1Section 6 discusses other definitions of lexical entailment559a lexical entailment resource, following an oper-ative rationale: a rule e?
?
e should be includedin an entailment knowledge resource if it would beneeded, as part of a compositional process, to inferthe meaning of e from some natural texts.
Basedon this definition, we perform an analysis of the re-lationships included in lexical-semantic resources,as described in Section 5.A rule need not apply in all contexts, as longas it is appropriate for some texts.
Two contex-tual aspects affect rule applicability.
First is the?lexical context?
specifying the meanings of thetext?s words.
A rules is applicable in a certain con-text only when the intended sense of its LHS termmatches the sense of that term in the text.
For ex-ample, the application of the rule lay ?
produce isvalid only in contexts where the producer is poul-try and the products are eggs.
This is a well knownissue observed, for instance, by Voorhees (1994).A second contextual factor requiring validationis the ?logical context?.
The logical context de-termines the monotonicity of the LHS and is in-duced by logical operators such as negation and(explicit or implicit) quantifiers.
For example, therule mammal ?
whale may not be valid in mostcases, but is applicable in universally quantifiedtexts like ?mammals are warm-blooded?.
This is-sue has been rarely addressed in applied inferencesystems (de Marneffe et al, 2006).
The abovementioned rules both comply with Definition 2and should therefore be included in a lexical en-tailment resource.3 Evaluating Entailment ResourcesOur evaluation goal is to assess the utility oflexical-semantic resources as sources for entail-ment rules.
An inference system applies a rule byinferring the rule?s RHS from texts that match itsLHS.
Thus, the utility of a resource depends on theperformance of its rule applications rather than onthe proportion of correct rules it contains.
A rule,whether correct or incorrect, has insignificant ef-fect on the resource?s utility if it rarely matchestexts in real application settings.
Additionally,correct rules might produce incorrect applicationswhen applied in inappropriate contexts.
There-fore, we use an instance-based evaluation method-ology, which simulates rule applications by col-lecting texts that contain rules?
LHS and manuallyassessing the correctness of their applications.Systems typically handle lexical context eitherimplicitly or explicitly.
Implicit context valida-tion occurs when the different terms of a compos-ite hypothesis disambiguate each other.
For exam-ple, the rule waterside ?
bank is unlikely to beapplied when trying to infer the hypothesis bankloans, since texts that match waterside are unlikelyto contain also the meaning of loan.
Explicit meth-ods, such as word-sense disambiguation or sensematching, validate each rule application accordingto the broader context in the text.
Few systemsalso address logical context validation by handlingquantifiers and negation.
As we aim for a system-independent comparison of resources, and explicitapproaches are not standardized yet within infer-ence systems, our evaluation uses only implicitcontext validation.3.1 Evaluation MethodologyFigure 1: Evaluation methodology flow chartThe input for our evaluation methodology is alexical-semantic resource R, which contains lex-ical entailment rules.
We evaluate R?s utility bytesting how useful it is for inferring a sample oftest hypotheses H from a corpus.
Each hypothesisin H contains more than one lexical element in or-der to provide implicit context validation for ruleapplications, e.g.
h: water pollution.We next describe the steps of our evaluationmethodology, as illustrated in Figure 1.
We referto the examples in the figure when needed:1) Fetch rules: For each h ?
H and eachlexical element e ?
h (e.g.
water), we fetch allrules e?
?
e in R that might be applied to entail e(e.g.
lake ?
water).2) Generate intermediate hypotheses h?
:For each rule r: e?
?
e, we generate an intermedi-ate hypothesis h?
by replacing e in h with e?
(e.g.560h?1: lake pollution).
From a text t entailing h?, hcan be further entailed by the single application ofr.
We thus simulate the process by which an en-tailment system would infer h from t using r.3) Retrieve matching texts: For each h?
weretrieve from a corpus all texts that contain thelemmatized words of h?
(not necessarily as a sin-gle phrase).
These texts may entail h?.
We dis-card texts that also match h since entailing h fromthem might not require the application of any rulefrom the evaluated resource.
In our example, theretrieved texts contain lake and pollution but donot contain water.4) Annotation: A sample of the retrieved textsis presented to human annotators.
The annotatorsare asked to answer the following two questionsfor each text, simulating the typical inference pro-cess of an entailment system:a) Does t entail h??
If t does not entail h?then the text would not provide a useful examplefor the application of r. For instance, t1 (in Fig-ure 1) does not entail h?1 and thus we cannot de-duce h from it by applying the rule r. Such textsare discarded from further evaluation.b) Does t entail h?
If t is annotated as en-tailing h?, an entailment system would then inferh from h?
by applying r. If h is not entailed fromt even though h?
is, the rule application is consid-ered invalid.
For instance, t2 does not entail h eventhough it entails h?2.
Indeed, the application of r2:*soil ?
water 2, from which h?2 was constructed,yields incorrect inference.
If the answer is ?yes?,as in the case of t3, the application of r for t isconsidered valid.The above process yields a sample of annotatedrule applications for each test hypothesis, fromwhich we can measure resources performance, asdescribed in Section 5.4 Experimental Setting4.1 Dataset and AnnotationCurrent available state-of-the-art lexical-semanticresources mainly deal with nouns.
Therefore, weused nominal hypotheses for our experiment3.We chose TREC 1-8 (excluding 4) as our testcorpus and randomly sampled 25 ad-hoc queriesof two-word compounds as our hypotheses.
Wedid not use longer hypotheses to ensure that2The asterisk marks an incorrect rule.3We suggest that the definitions and methodologies can beapplied for other parts of speech as well.enough texts containing the intermediate hypothe-ses are found in the corpus.
For annotation sim-plicity, we retrieved single sentences as our texts.For each rule applied for an hypothesis h, wesampled 10 sentences from the sentences retrievedfor that rule.
As a baseline, we also sampled 10sentences for each original hypothesis h in whichboth words of h are found.
In total, 1550 uniquesentences were sampled and annotated by two an-notators.To assess the validity of our evaluation method-ology, the annotators first judged a sample of 220sentences.
The Kappa scores for inter-annotatoragreement were 0.74 and 0.64 for judging h?
andh, respectively.
These figures correspond to sub-stantial agreement (Landis and Koch, 1997) andare comparable with related semantic annotations(Szpektor et al, 2007; Bhagat et al, 2007).4.2 Lexical-Semantic ResourcesWe evaluated the following resources:WordNet (WNd): There is no clear agreementregarding which set of WordNet relations is use-ful for entailment inference.
We therefore took aconservative approach using only synonymy andhyponymy rules, which typically comply with thelexical entailment relation and are commonly usedby textual entailment systems, e.g.
(Herrera et al,2005; Bos and Markert, 2006).
Given a term e,we created a rule e?
?
e for each e?
amongst thesynonyms or direct hyponyms for all senses of ein WordNet 3.0.Snow (Snow30k): Snow et al (2006) pre-sented a probabilistic model for taxonomy induc-tion which considers as features paths in parsetrees between related taxonomy nodes.
They showthat the best performing taxonomy was the oneadding 30,000 hyponyms to WordNet.
We createdan entailment rule for each new hyponym added toWordNet by their algorithm4.LCC?s extended WordNet (XWN?
): In(Moldovan and Rus, 2001) WordNet glosses weretransformed into logical form axioms.
From thisrepresentation we created a rule e?
?
e for each e?in the gloss which was tagged as referring to thesame entity as e.CBC: A knowledgebase of labeled clusters gen-erated by the statistical clustering and labeling al-gorithms in (Pantel and Lin, 2002; Pantel and4Available at http://ai.stanford.edu/?
rion/swn561Ravichandran, 2004)5.
Given a cluster label e, anentailment rule e?
?
e is created for each membere?
of the cluster.Lin Dependency Similarity (Lin-dep): Adistributional word similarity resource based onsyntactic-dependency features (Lin, 1998).
Givena term e and its list of similar terms, we constructfor each e?
in the list the rule e?
?
e. This resourcewas previously used in textual entailment engines,e.g.
(Roth and Sammons, 2007).Lin Proximity Similarity (Lin-prox): Aknowledgebase of terms with their cooccurrence-based distributionally similar terms.
Rules are cre-ated from this resource as from the previous one6.Wikipedia first sentence (WikiFS): Kazamaand Torisawa (2007) used Wikipedia as an exter-nal knowledge to improve Named Entity Recog-nition.
Using the first step of their algorithm, weextracted from the first sentence of each page anoun that appears in a is-a pattern referring to thetitle.
For each such pair we constructed a rule title?
noun (e.g.
Michelle Pfeiffer ?
actress).The above resources represent various meth-ods for detecting semantic relatedness betweenwords: Manually and semi-automatically con-structed (WNd and XWN?, respectively), automat-ically constructed based on a lexical-syntactic pat-tern (WikiFS), distributional methods (Lin-dep andLin-prox) and combinations of pattern-based anddistributional methods (CBC and Snow30k).5 Results and AnalysisThe results and analysis described in this sectionreveal new aspects concerning the utility of re-sources for lexical entailment, and experimentallyquantify several intuitively-accepted notions re-garding these resources and the lexical entailmentrelation.
Overall, our findings highlight where ef-forts in developing future resources and inferencesystems should be invested.5.1 Resources PerformanceEach resource was evaluated using two measures -Precision and Recall-share, macro averaged overall hypotheses.
The results achieved for each re-source are summarized in Table 1.5Kindly provided to us by Patrick Pantel.6Lin?s resources were downloaded from:http://www.cs.ualberta.ca/?
lindek/demos.htmResource Precision (%) Recall-share (%)Snow30k 56 8WNd 55 24XWN?
51 9WikiFS 45 7CBC 33 9Lin-dep 28 45Lin-prox 24 36Table 1: Lexical resources performance5.1.1 PrecisionThe Precision of a resource R is the percentage ofvalid rule applications for the resource.
It is esti-mated by the percentage of texts entailing h fromthose that entail h?
: countR(entailing h=yes)countR(entailing h?=yes) .Not surprisingly, resources such as WNd, XWN?or WikiFS achieved relatively high precisionscores, due to their accurate construction meth-ods.
In contrast, Lin?s distributional resources arenot designed to include lexical entailment relation-ships.
They provide pairs of contextually simi-lar words, of which many have non-entailing rela-tionships, such as co-hyponyms7 (e.g.
*doctor ?journalist) or topically-related words, such as *ra-diotherapy ?
outpatient.
Hence their relativelylow precision.One visible outcome is the large gap betweenthe perceived high accuracy of resources con-structed by accurate methods, most notably WNd,and their performance in practice.
This findingemphasizes the need for instance-based evalua-tions, which capture the ?real?
contribution of aresource.
To better understand the reasons forthis gap we further assessed the three factorsthat contribute to incorrect applications: incorrectrules, lexical context and logical context (see Sec-tion 2.2).
This analysis is presented in Table 2.From Table 2 we see that the gap for accurateresources is mainly caused by applications of cor-rect rules in inappropriate contexts.
More inter-estingly, the information in the table allows us toasses the lexical ?context-sensitivity?
of resources.When considering only the COR-LEX rules to re-calculate resources precision, we find that Lin-depachieves precision of 71% ( 15%15%+6% ), while WNdyields only 56% ( 55%55%+44% ).
This result indicatesthat correct Lin-dep rules are less sensitive to lexi-cal context, meaning that their prior likelihoods to7a.k.a.
sister terms or coordinate terms562(%)Invalid Rule Applications Valid Rule ApplicationsINCOR COR-LOG COR-LEX Total INCOR COR-LOG COR-LEX Total (P)WNd 1 0 44 45 0 0 55 55WikiFS 13 0 42 55 3 0 42 45XWN?
19 0 30 49 0 0 51 51Snow30k 23 0 21 44 0 0 56 56CBC 51 12 4 67 14 0 19 33Lin-prox 59 4 13 76 8 3 13 24Lin-dep 61 5 6 72 9 4 15 28Table 2: The distribution of invalid and valid rule applications by rule types: incorrect rules (INCOR), correct rules requiring?logical context?
validation (COR-LOG), and correct rules requiring ?lexical context?
matching (COR-LEX).
The numbers of eachresource?s valid applications add up to the resource?s precision.be correct are higher.
This is explained by the factthat Lin-dep?s rules are calculated across multiplecontexts and therefore capture the more frequentusages of words.
WordNet, on the other hand, in-cludes many anecdotal rules whose application israre, and thus is very sensitive to context.
Simi-larly, WikiFS turns out to be very context-sensitive.This resource contains many rules for polysemousproper nouns that are scarce in their proper nounsense, e.g.
Captive ?
computer game.
Snow30k,when applied with the same calculation, reaches73%, which explains how it achieved a compara-ble result to WNd, even though it contains manyincorrect rules in comparison to WNd.5.1.2 RecallAbsolute recall cannot be measured since the totalnumber of texts in the corpus that entail each hy-pothesis is unknown.
Instead, we measure recall-share, the contribution of each resource to recallrelative to matching only the words of the origi-nal hypothesis without any rules.
We denote byyield(h) the number of texts that match h directlyand are annotated as entailing h. This figure is es-timated by the number of sampled texts annotatedas entailing h multiplied by the sampling propor-tion.
In the same fashion, for each resource R,we estimate the number of texts entailing h ob-tained through entailment rules of the resource R,denoted yieldR(h).
Recall-share of R for h is theproportion of the yield obtained by the resource?srules relative to the overall yield with and withoutthe rules from R: yieldR(h)yield(h)+yieldR(h) .From Table 1 we see that along with their rela-tively low precision, Lin?s resources?
recall greatlysurpasses that of any other resource, includingWordNet8.
The rest of the resources are even infe-8A preliminary experiment we conducted showed that re-rior to WNd in that respect, indicating their limitedutility for inference systems.As expected, synonyms and hyponyms in Word-Net contributed a noticeable portion to recall in allresources.
Additional correct rules correspond tohyponyms and synonyms missing from WordNet,many of them proper names and some slang ex-pressions.
These rules were mainly provided byWikiFS and Snow30k, significantly supplementingWordNet, whose HasInstance relation is quite par-tial.
However, there are other interesting types ofentailment relations contributing to recall.
Theseare discussed in Sections 5.2 and 5.3.
Examplesfor various rule types are found in Table 3.5.1.3 Valid Applications of Incorrect RulesWe observed that many entailing sentences wereretrieved by inherently incorrect rules in the distri-butional resources.
Analysis of these rules revealsthey were matched in entailing texts when the LHShas noticeable statistical correlation with anotherterm in the text that does entail the RHS.
For ex-ample, for the hypothesis wildlife extinction, therule *species ?
extinction yielded valid applica-tions in contexts about threatened or endangeredspecies.
Has the resource included a rule betweenthe entailing term in the text and the RHS, theentailing text would have been matched withoutneeding the incorrect rule.These correlations accounted for nearly a thirdof Lin resources?
recall.
Nonetheless, in princi-ple, we suggest that such rules, which do not con-form with Definition 2, should not be included in alexical entailment resource, since they also causeinvalid rule applications, while the entailing textsthey retrieve will hopefully be matched by addi-call does not dramatically improve when using the entire hy-ponymy subtree from WordNet.563Type Correct RulesHYPO Shevardnadze ?
official Snow30kANT efficacy ?
ineffectiveness Lin-depHOLO government ?
official Lin-proxHYPER arms ?
gun Lin-prox?
childbirth ?
motherhood Lin-dep?
mortgage ?
bank Lin-prox?
Captive ?
computer WikiFS?
negligence ?
failure CBC?
beatification ?
pope XWN?Type Incorrect RulesCO-HYP alcohol ?
cigarette CBC?
radiotherapy ?
outpatient Lin-dep?
teen-ager ?
gun Snow30k?
basic ?
paper WikiFS?
species ?
extinction Lin-proxTable 3: Examples of lexical resources rules by types.HYPO: hyponymy, HYPER: hypernymy (class entailment ofits members), HOLO: holonymy, ANT: antonymy, CO-HYP: co-hyponymy.
The non-categorized relations do not correspondto any WordNet relation.tional correct rules in a more comprehensive re-source.5.2 Non-standard Entailment RelationsAn important finding of our analysis is that someless standard entailment relationships have a con-siderable impact on recall (see Table 3).
Theserules, which comply with Definition 2 but donot conform to any WordNet relation type, weremainly contributed by Lin?s distributional re-sources and to a smaller degree are also includedin XWN?.
In Lin-dep, for example, they accountedfor approximately a third of the recall.Among the finer grained relations we identi-fied in this set are topical entailment (e.g.
IBMas the company entailing the topic computers),consequential relationships (pregnancy?mother-hood) and an entailment of inherent arguments bya predicate, or of essential participants by a sce-nario description, e.g.
beatification ?
pope.
Acomprehensive typology of these relationships re-quires further investigation, as well as the identi-fication and development of additional resourcesfrom which they can be extracted.As opposed to hyponymy and synonymy rules,these rules are typically non-substitutable, i.e.
theRHS of the rule is unlikely to have the exact samerole in the text as the LHS.
Many inference sys-tems perform rule-based transformations, substi-tuting the LHS by the RHS.
This finding suggeststhat different methods may be required to utilizesuch rules for inference.5.3 Logical ContextWordNet relations other than synonyms and hy-ponyms, including antonyms, holonyms and hy-pernyms (see Table 3), contributed a noticeableshare of valid rule applications for some resources.Following common practice, these relations aremissing by construction from the other resources.As shown in Table 2 (COR-LOG columns), suchrelations accounted for a seventh of Lin-dep?svalid rule applications, as much as was the con-tribution of hyponyms and synonyms to this re-source?s recall.
Yet, using these rules resulted withmore erroneous applications than correct ones.
Asdiscussed in Section 2.2, the rules induced bythese relations do conform with our lexical entail-ment definition.
However, a valid application ofthese rules requires certain logical conditions tooccur, which is not the common case.
We thussuggest that such rules are included in lexical en-tailment resources, as long as they are markedproperly by their types, allowing inference sys-tems to utilize them only when appropriate mech-anisms for handling logical context are in place.5.4 Rules PriorsIn Section 5.1.1 we observed that some resourcesare highly sensitive to context.
Hence, when con-sidering the validity of a rule?s application, twofactors should be regarded: the actual context inwhich the rule is to be applied, as well as the rule?sprior likelihood to be valid in an arbitrary con-text.
Somewhat indicative, yet mostly indirect, in-formation about rules?
priors is contained in someresources.
This includes sense ranks in WordNet,SemCor statistics (Miller et al, 1993), and similar-ity scores and rankings in Lin?s resources.
Infer-ence systems often incorporated this information,typically as top-k or threshold-based filters (Pan-tel and Lin, 2003; Roth and Sammons, 2007).
Byempirically assessing the effect of several such fil-ters in our setting, we found that this type of datais indeed informative in the sense that precisionincreases as the threshold rises.
Yet, no specificfilters were found to improve results in terms ofF1 score (where recall is measured relatively tothe yield of the unfiltered resource) due to a sig-nificant drop in relative recall.
For example, Lin-564prox loses more than 40% of its recall when onlythe top-50 rules for each hypothesis are exploited,and using only the first sense of WNd costs the re-source over 60% of its recall.
We thus suggest abetter strategy might be to combine the prior in-formation with context matching scores in orderto obtain overall likelihood scores for rule appli-cations, as in (Szpektor et al, 2008).
Furthermore,resources should include explicit information re-garding the prior likelihoods of of their rules.5.5 Operative ConclusionsOur findings highlight the currently limited re-call of available resources for lexical inference.The higher recall of Lin?s resources indicatesthat many more entailment relationships can beacquired, particularly when considering distribu-tional evidence.
Yet, available distributional ac-quisition methods are not geared for lexical entail-ment.
This suggests the need to develop acqui-sition methods for dedicated and more extensiveknowledge resources that would subsume the cor-rect rules found by current distributional methods.Furthermore, substantially better recall may be ob-tained by acquiring non-standard lexical entail-ment relationships, as discussed in Section 5.2, forwhich a comprehensive typology is still needed.At the same time, transformation-based inferencesystems would need to handle these kinds of rules,which are usually non-substitutable.
Our resultsalso quantify and stress earlier findings regardingthe severe degradation in precision when rules areapplied in inappropriate contexts.
This highlightsthe need for resources to provide explicit informa-tion about the suitable lexical and logical contextsin which an entailment rule is applicable.
In par-allel, methods should be developed to utilize suchcontextual information within inference systems.Additional auxiliary information needed in lexicalresources is the prior likelihood for a given rule tobe correct in an arbitrary context.6 Related WorkSeveral prior works defined lexical entailment.WordNet?s lexical entailment is a relationship be-tween verbs only, defined for propositions (Fell-baum, 1998).
Geffet and Dagan (2004) definedsubstitutable lexical entailment as a relation be-tween substitutable terms.
We find this definitiontoo restrictive as non-substitutable rules may alsobe useful for entailment inference.
Examples arebreastfeeding ?
baby and hospital ?
medical.Hence, Definition 2 is more broadly applicable fordefining the desired contents of lexical entailmentresources.
We empirically observed that the rulessatisfying their definition are a proper subset ofthe rules covered by our definition.
Dagan andGlickman (2004) referred to entailment at the sub-sentential level by assigning truth values to sub-propositional text fragments through their existen-tial meaning.
We find this criterion too permissive.For instance, the existence of country implies theexistence of its flag.
Yet, the meaning of flag istypically not implied by country.Previous works assessing rule application viahuman annotation include (Pantel et al, 2007;Szpektor et al, 2007), which evaluate acquisitionmethods for lexical-syntactic rules.
They posed anadditional question to the annotators asking themto filter out invalid contexts.
In our methodologyimplicit context matching for the full hypothesiswas applied instead.
Other related instance-basedevaluations (Giuliano and Gliozzo, 2007; Connorand Roth, 2007) performed lexical substitutions,but did not handle the non-substitutable cases.7 ConclusionsThis paper provides several methodological andempirical contributions.
We presented a novelevaluation methodology for the utility of lexical-semantic resources for semantic inference.
To thatend we proposed definitions for entailment at sub-sentential levels, addressing a gap in the textualentailment framework.
Our evaluation and analy-sis provide a first quantitative comparative assess-ment of the isolated utility of a range of prominentpotential resources for entailment rules.
We haveshown various factors affecting rule applicabilityand resources performance, while providing oper-ative suggestions to address them in future infer-ence systems and resources.AcknowledgmentsThe authors would like to thank Naomi Frankeland Iddo Greental for their excellent annotationwork, as well as Roy Bar-Haim and Idan Szpektorfor helpful discussion and advice.
This work waspartially supported by the Negev Consortium ofthe Israeli Ministry of Industry, Trade and Labor,the PASCAL-2 Network of Excellence of the Eu-ropean Community FP7-ICT-2007-1-216886 andthe Israel Science Foundation grant 1095/05.565ReferencesRahul Bhagat, Patrick Pantel, and Eduard Hovy.
2007.LEDIR: An unsupervised algorithm for learning di-rectionality of inference rules.
In Proceedings ofEMNLP-CoNLL.J.
Bos and K. Markert.
2006.
When logical infer-ence helps determining textual entailment (and whenit doesn?t).
In Proceedings of the Second PASCALRTE Challenge.Michael Connor and Dan Roth.
2007.
Context sensi-tive paraphrasing with a global unsupervised classi-fier.
In Proceedings of ECML.Ido Dagan and Oren Glickman.
2004.
Probabilistictextual entailment: Generic applied modeling of lan-guage variability.
In PASCAL Workshop on Learn-ing Methods for Text Understanding and Mining.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailmentchallenge.
In Joaquin Quinonero Candela, Ido Da-gan, Bernardo Magnini, and Florence d?Alche?
Buc,editors, MLCW, Lecture Notes in Computer Science.Marie-Catherine de Marneffe, Bill MacCartney, TrondGrenager, Daniel Cer, Anna Rafferty, and Christo-pher D. Manning.
2006.
Learning to distinguishvalid textual entailments.
In Proceedings of the Sec-ond PASCAL RTE Challenge.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, andCommunication).
The MIT Press.Maayan Geffet and Ido Dagan.
2004.
Feature vectorquality and distributional similarity.
In Proceedingsof COLING.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recogniz-ing textual entailment challenge.
In Proceedings ofACL-WTEP Workshop.Claudio Giuliano and Alfio Gliozzo.
2007.
Instancebased lexical entailment for ontology population.
InProceedings of EMNLP-CoNLL.Oren Glickman, Eyal Shnarch, and Ido Dagan.
2006.Lexical reference: a semantic matching subtask.
InProceedings of EMNLP.Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo.2005.
Textual entailment recognition based on de-pendency analysis and wordnet.
In Proceedings ofthe First PASCAL RTE Challenge.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Ex-ploiting Wikipedia as external knowledge for namedentity recognition.
In Proceedings of EMNLP-CoNLL.Milen Kouylekov and Bernardo Magnini.
2006.
Build-ing a large-scale repository of textual entailmentrules.
In Proceedings of LREC.J.
R. Landis and G. G. Koch.
1997.
The measurementsof observer agreement for categorical data.
In Bio-metrics, pages 33:159?174.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProceedings of HLT.Dan Moldovan and Vasile Rus.
2001.
Logic formtransformation of wordnet and its applicability toquestion answering.
In Proceedings of ACL.Patrick Pantel and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of ACMSIGKDD.Patrick Pantel and Dekang Lin.
2003.
Automaticallydiscovering word senses.
In Proceedings of NAACL.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In Proceedingsof HLT-NAACL.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
In Pro-ceedings of HLT.Marius Pasca and Sanda M. Harabagiu.
2001.
The in-formative role of wordnet in open-domain questionanswering.
In Proceedings of NAACL Workshop onWordNet and Other Lexical Resources.Dan Roth and Mark Sammons.
2007.
Semantic andlogical inference model for textual entailment.
InProceedings of ACL-WTEP Workshop.Chirag Shah and Bruce W. Croft.
2004.
Evaluatinghigh accuracy retrieval techniques.
In Proceedingsof SIGIR.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of COLING-ACL.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A core of semantic knowl-edge - unifying wordnet and wikipedia.
In Proceed-ings of WWW.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acqui-sition.
In Proceedings of ACL.Idan Szpektor, Ido Dagan, Roy Bar-Haim, and JacobGoldberger.
2008.
Contextual preferences.
In Pro-ceedings of ACL.Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of SIGIR.566
