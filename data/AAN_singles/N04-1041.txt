Automatically Labeling Semantic ClassesPatrick Pantel and Deepak RavichandranInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA  90292{pantel,ravichan}@isi.eduAbstractSystems that automatically discover semanticclasses have emerged in part to address thelimitations of broad-coverage lexical re-sources such as WordNet and Cyc.
The cur-rent state of the art discovers many semanticclasses but fails to label their concepts.
Wepropose an algorithm labeling semanticclasses and for leveraging them to extract is-arelationships using a top-down approach.1 IntroductionThe natural language literature is rich in theories of se-mantics (Barwise and Perry 1985; Schank and Abelson1977).
However, WordNet (Miller 1990) and Cyc (Le-nat 1995) aside, the community has had little success inactually building large semantic repositories.
Suchbroad-coverage lexical resources are extremely useful inapplications such as word sense disambiguation (Lea-cock, Chodorow and Miller 1998) and question answer-ing (Pasca and Harabagiu 2001).Current manually constructed ontologies such asWordNet and Cyc have important limitations.
First, theyoften contain rare senses.
For example, WordNet in-cludes a rare sense of computer that means ?the personwho computes?.
Using WordNet to expand queries to aninformation retrieval system, the expansion of computerwill include words like estimator and reckoner.
Also,the words dog, computer and company all have a sensethat is a hyponym of person.
Such rare senses make itdifficult for a coreference resolution system to useWordNet to enforce the constraint that personal pro-nouns (e.g.
he or she) must refer to a person.
The secondproblem with these lexicons is that they miss many do-main specific senses.
For example, WordNet misses theuser-interface-object sense of the word dialog (as oftenused in software manuals).
WordNet alo contains avery poor coverage of proper nouns.There is a need for (semi-) automatic approaches tobuilding and extending ontologies as well as for validat-ing the structure and content of existing ones.
With theadvent of the Web, we have access to enormousamounts of text.
The future of ontology growing lies inleveraging this data by harvesting it for concepts andsemantic relationships.
Moreover, once such knowledgeis discovered, mechanisms must be in place to enrichcurrent ontologies with this new knowledge.To address some of the coverage and specificityproblems in WordNet and Cyc, Pantel and Lin (2002)proposed and algorithm, called CBC, for automaticallyextracting semantic classes.
Their classes consist ofclustered instances like the three shown below:(A) multiple sclerosis, diabetes,osteoporosis, cardiovascular disease,Parkinson's, rheumatoid arthritis, heartdisease, asthma, cancer, hypertension,lupus, high blood pressure, arthritis,emphysema, epilepsy, cystic fibrosis,leukemia, hemophilia, Alzheimer, myeloma,glaucoma, schizophrenia, ...(B) Mike Richter, Tommy Salo, JohnVanbiesbrouck, Curtis Joseph, Chris Osgood,Steve Shields, Tom Barrasso, Guy Hebert,Arturs Irbe, Byron Dafoe, Patrick Roy, BillRanford, Ed Belfour, Grant Fuhr, DominikHasek, Martin Brodeur, Mike Vernon, RonTugnutt, Sean Burke, Zach Thornton, JocelynThibault, Kevin Hartman, Felix Potvin, ...(C) pink, red, turquoise, blue, purple,green, yellow, beige, orange, taupe, white,lavender, fuchsia, brown, gray, black,mauve, royal blue, violet, chartreuse,teal, gold, burgundy, lilac, crimson,garnet, coral, grey, silver, olive green,cobalt blue, scarlet, tan, amber, ...A limitation of these concepts is that CBC does notdiscover their actual names.
That is, CBC discovers asemantic class of Canadian provinces such as Manitoba,Alberta, and Ontario, but stops short of labeling theconcept as Canadian Provinces.
Some applications suchas question answering would benefit from class labels.For example, given the concept list (B) and a labelgoalie/goaltender, a QA system could look for answersto the question ?Which goaltender won the most HartTrophys??
in the concept.In this paper, we propose an algorithm for automati-cally inducing names for semantic classes and for find-ing instance/concept (is-a) relationships.
Using conceptsignatures (templates describing the prototypical syntac-tic behavior of instances of a concept), we extract con-cept names by searching for simple syntactic patternssuch as ?concept apposition-of instance?.
Searchingconcept signatures is more robust than searching thesyntactic features of individual instances since manyinstances suffer from sparse features or multiple senses.Once labels are assigned to concepts, we can extracta hyponym relationship between each instance of a con-cept and its label.
For example, once our system labelslist (C) as color, we may extract relationships such as:pink is a color, red is a color, turquoise is a color, etc.Our results show that of the 159,000 hyponyms we ex-tract using this simple method, 68% are correct.
Of the65,000 proper name hyponyms we discover, 81.5% arecorrect.The remainder of this paper is organized as follows.In the next section, we review previous algorithms forextracting semantic classes and hyponym relationships.Section 3 describes our algorithm for labeling conceptsand for extracting hyponym relationships.
Experimentalresults are presented in Section 4 and finally, we con-clude with a discussion and future work.2 Previous WorkThere have been several approaches to automaticallydiscovering lexico-semantic information from text(Hearst 1992; Riloff and Shepherd 1997; Riloff andJones 1999; Berland and Charniak 1999; Pantel and Lin2002; Fleischman et al 2003; Girju et al 2003).
Oneapproach constructs automatic thesauri by computingthe similarity between words based on their distributionin a corpus (Hindle 1990; Lin 1998).
The output ofthese programs is a ranked list of similar words to eachword.
For example, Lin?s approach outputs the follow-ing top-20 similar words of orange:(D) peach, grapefruit, yellow, lemon, pink,avocado, tangerine, banana, purple, SantaAna, strawberry, tomato, red, pineapple,pear, Apricot, apple, green, citrus, mangoA common problem of such lists is that they do notdiscriminate between the senses of polysemous words.For example, in (D), the color and fruit senses of orangeare mixed up.Lin and Pantel (2001) proposed a clustering algo-rithm, UNICON, which generates similar lists butdiscriminates between senses of words.
Later, Panteland Lin (2002) improved the precision and recall ofUNICON clusters with CBC (Clustering by Commit-tee).
Using sets of representative elements called com-mittees, CBC discovers cluster centroids thatunambiguously describe the members of a possibleclass.
The algorithm initially discovers committees thatare well scattered in the similarity space.
It then pro-ceeds by assigning elements to their most similar com-mittees.
After assigning an element to a cluster, CBCremoves their overlapping features from the elementbefore assigning it to another cluster.
This allows CBCto discover the less frequent senses of a word and toavoid discovering duplicate senses.CBC discovered both the color sense of orange, asshown in list (C) of Section 1, and the fruit sense shownbelow:(E) peach, pear, apricot, strawberry, ba-nana, mango, melon, apple, pineapple,cherry, plum, lemon, grapefruit, orange,berry, raspberry, blueberry, kiwi, ...There have also been several approaches to discov-ering hyponym (is-a) relationships from text.
Hearst(1992) used seven lexico-syntactic patterns, for example?such NP as {NP,}*{(or|and)} NP?
and ?NP {, NP}*{,}or other NP?.
Berland and Charniak (1999) used similarpattern-based techniques and other heuristics to extractmeronymy (part-whole) relations.
They reported anaccuracy of about 55% precision on a corpus of 100,000words.
Girju, Badulescu and Moldovan (2003)improved upon this work by using a machine learningfilter.
Mann (2002) and Fleischman et al (2003) usedpart of speech patterns to extract a subset of hyponymrelations involing proper nouns.3 Labeling ClassesThe research discussed above on discovering hyponymrelationships all take a bottom up approach.
That is,they use patterns to independently discover semanticrelationships of words.
However, for infrequent words,these patterns do not match or, worse yet, generate in-correct relationships.Ours is a top down approach.
We make use of co-occurrence statistics of semantic classes discovered byalgorithms like CBC to label their concepts.
Hyponymrelationships may then be extracted easily: one hypo-nym per instance/concept label pair.
For example, if welabeled concept (A) from Section 1 with disease, thenwe could extract is-a relationships such as: diabetes is adisease, cancer is a disease, and lupus is a disease.
Aconcept instance such as lupus is assigned a hypernymdisease not because it necessarily occurs in any particu-lar syntactic relationship with disease, but because itbelongs to the class of instances that does.The input to our labeling algorithm is a list of se-mantic classes, in the form of clusters of words, whichmay be generated from any source.
In our experiments,we used the clustering outputs of CBC (Pantel and Lin2002).
The output of the system is a ranked list of con-cept names for each semantic class.In the first phase of the algorithm, we extract featurevectors for each word that occurs in a semantic class.Phase II then uses these features to compute grammati-cal signatures of concepts using the CBC algorithm.Finally, we use simple syntactic patterns to discoverclass names from each class?
signature.
Below, we de-scribe these phases in detail.3.1 Phase IWe represent each word (concept instance) by a featurevector.
Each feature corresponds to a context in whichthe word occurs.
For example, ?catch __?
is a verb-object context.
If the word wave occurred in this con-text, then the context is a feature of wave.We first construct a frequency count vector C(e) =(ce1, ce2, ?, cem), where m is the total number of featuresand cef is the frequency count of feature f occurring inword e. Here, cef is the number of times word e occurredin a grammatical context f. For example, if the wordwave occurred 217 times as the object of the verb catch,then the feature vector for wave will have value 217 forits ?object-of catch?
feature.
In Section 4.1, we describehow we obtain these features.We then construct a mutual information vectorMI(e) = (mie1, mie2, ?, miem) for each word e, where miefis the pointwise mutual information between word e andfeature f, which is defined as:NcNcNcef mjejniifefmi??
== ?=11log  (1)where n is the number of words and N = ?
?= =nimjijc1 1is thetotal frequency count of all features of all words.Mutual information is commonly used to measurethe association strength between two words (Church andHanks 1989).
A well-known problem is that mutualinformation is biased towards infrequent ele-ments/features.
We therefore multiply mief with the fol-lowing discounting factor:1,min,min11111+????????????????
?+ ???
?====mjjfnieimjjfnieiefefcccccc  (2)3.2 Phase IIFollowing (Pantel and Lin 2002), we construct a com-mittee for each semantic class.
A committee is a set ofrepresentative elements that unambiguously describe themembers of a possible class.For each class c, we construct a matrix containingthe similarity between each pair of words ei and ej in cusing the cosine coefficient of their mutual informationvectors (Salton and McGill 1983):( ) ????
?=ffeffeffefejijijimimimimieesim22,  (3)For each word e, we then cluster its most similar in-stances using group-average clustering (Han and Kam-ber 2001) and we store as a candidate committee thehighest scoring cluster c'  according to the followingmetric:| c'| ?
avgsim(c') (4)where |c'| is the number of elements in c' and avgsim(c')is the average pairwise similarity between words in c'.The assumption is that the best representative for a con-cept is a large set of very similar instances.
The commit-tee for class c is then the highest scoring candidatecommittee containing only words from c. For example,below are the committee members discovered for thesemantic classes (A), (B), and (C) from Section 1:1) cardiovascular disease, diabetes,multiple sclerosis, osteoporosis,Parkinson's, rheumatoid arthritis2) Curtis Joseph, John Vanbiesbrouck, MikeRichter, Tommy Salo3) blue, pink, red, yellow3.3 Phase IIIBy averaging the feature vectors of the committeemembers of a particular semantic class, we obtain agrammatical template, or signature, for that class.
Forexample, Figure 1 shows an excerpt of the grammaticalsignature for concept (B) in Section 1.
The vector isobtained by averaging the feature vectors for the wordsCurtis Joseph, John Vanbiesbrouck, Mike Richter, andTommy Salo (the committee of this concept).
The?-V:subj:N:sprawl?
feature indicates a subject-verb re-lationship between the concept and the verb sprawlwhile ?N:appo:N:goaltender?
indicates an appositionrelationship between the concept and the noun goal-tender.
The (-) in a relationship means that the righthand side of the relationship is the head (e.g.
sprawl isthe head of the subject-verb relationship).
The two col-umns of numbers indicate the frequency and mutualinformation score for each feature respectively.In order to discover the characteristics of humannaming conventions, we manually named 50 conceptsdiscovered by CBC.
For each concept, we extracted therelationships between the concept committee and theassigned label.
We then added the mutual informationscores for each extracted relationship among the 50concepts.
The top-4 highest scoring relationships are:?
Apposition (N:appo:N)e.g.
... Oracle, a company knownfor its progressive employmentpolicies, ...?
Nominal subject (-N:subj:N)e.g.
... Apple was a hot young com-pany, with Steve Jobs in charge.?
Such as (-N:such as:N)e.g.
... companies such as IBM mustbe weary ...?
Like (-N:like:N)e.g.
... companies like Sun Micro-systems do no shy away from suchchallenges, ...To name a class, we simply search for these syntac-tic relationships in the signature of a concept.
We sumup the mutual information scores for each term that oc-curs in these relationships with a committee of a class.The highest scoring term is the name of the class.
Forexample, the top-5 scoring terms that occurred in theserelationships with the signature of the concept repre-sented by the committee {Curtis Joseph, JohnVanbiesbrouck, Mike Richter, Tommy Salo} are:1)      goalie 40.372)      goaltender 33.643)      goalkeeper 19.224)      player 14.555)      backup 9.40The numbers are the total mutual information scoresof each name in the four syntactic relationships.4 EvaluationIn this section, we present an evaluation of the classlabeling algorithm and of the hyponym relationshipsdiscovered by our system.4.1 Experimental SetupWe used Minipar (Lin 1994), a broad coverage parser,to parse 3GB of newspaper text from the Aquaint(TREC-9) collection.
We collected the frequency countsof the grammatical relationships (contexts) output byMinipar and used them to compute the pointwise mutualinformation vectors described in Section 3.1.We used the 1432 noun clusters extracted by CBC1as the list of concepts to name.
For each concept, wethen used our algorithm described in Section 3 to extractthe top-20 names for each concept.1 Available at http://www.isi.edu/~pantel/demos.htm{Curtis Joseph, John Vanbiesbrouck,Mike Richter, Tommy Salo}-N:gen:Npad 57 11.19backup 29 9.95crease 7 9.69glove 52 9.57stick 20 9.15shutout 17 8.80-N:conj:NHasek 15 12.36Martin Brodeur 12 12.26Belfour 13 12.22Patrick Roy 10 11.90Dominik Hasek 7 11.20Roy 6 10.01-V:subj:Nsprawl 11 6.69misplay 6 6.55smother 10 6.54skate 28 6.43turn back 10 6.28stop 453 6.19N:appo:Ngoaltender 449 10.79goalie 1641 10.76netminder 57 10.39goalkeeper 487 9.69N:conj:NMartin Brodeur 11 12.49Dominik Hasek 11 12.33Ed Belfour 10 12.04Curtis Joseph 7 11.46Tom Barrasso 5 10.85Byron Dafoe 5 10.80Chris Osgood 4 10.25Figure 1.
Excerpt of the grammatical signature for thegoalie/goaltender concept.4.2 Labeling PrecisionOut of the 1432 noun concepts, we were unable to name21 (1.5%) of them.
This occurs when a concept?s com-mittee members do not occur in any of the four syntacticrelationships described in Section 0.
We performed amanual evaluation of the remaining 1411 concepts.We randomly selected 125 concepts and their top-5highest ranking names according to our algorithm.
Ta-ble 1 shows the first 10 randomly selected concepts(each concept is represented by three of its committeemembers).For each concept, we added to the list of names ahuman generated name (obtained from an annotatorlooking at only the concept instances).
We also ap-pended concept names extracted from WordNet.
Foreach concept that contains at least five instances in theWordNet hierarchy, we named the concept with themost frequent common ancestor of each pair of in-stances.
Up to five names were generated by WordNetfor each concept.
Because of the low coverage of propernouns in WordNet, only 33 of the 125 concepts weevaluated had WordNet generated labels.We presented to three human judges the 125 ran-domly selected concepts together with the system, hu-man, and WordNet generated names randomly ordered.That way, there was no way for a judge to know thesource of a label nor the system?s ranking of the labels.For each name, we asked the judges to assign a score ofcorrect, partially correct, or incorrect.
We then com-puted the mean reciprocal rank (MRR) of the system,human, and WordNet labels.
For each concept, a nam-ing scheme receives a score of 1 / M where M is therank of the first name judged correct.
Table 2 shows theresults.
Table 3 shows similar results for a more lenientevaluation where M is the rank of the first name judgedcorrect or partially correct.Our system achieved an overall MRR score of77.1%.
We performed much better than the baselineWordNet (19.9%) because of the lack of coverage(mostly proper nouns) in the hierarchy.
For the 33 con-cepts that WordNet named, it achieved a score of 75.3%and a lenient score of 82.7%, which is high consideringthe simple algorithm we used to extract labels usingWordNet.The Kappa statistic (Siegel and Castellan Jr. 1988)measures the agreements between a set of judges?
as-sessments correcting for chance agreements:( ) ( )( )EPEPAPK ?
?=1(5)where P(A) is the probability of agreement between thejudges and P(E) is the probability that the judges agreeTable 1.
Labels assigned to 10 randomly selected concepts (each represented by three committee members.CBC CONCEPT HUMAN LABEL WORDNET LABELS SYSTEM LABELS (RANKED)BMG, EMI, Sony record label none label / company / album /machine / studioPreakness Stakes, Preakness, BelmontStakeshorse race none race / event / run / victory /startOlympia Snowe, Susan Collins, JamesJeffordsUS senator none republican / senator / chair-man / supporter / conservativeEldoret, Kisumu, Mombasa African city none city / port / cut off / town /southeastBronze Star, Silver Star, Purple Heart medal decoration / laurelwreath / medal / medal-lion / palmdistinction / set / honor / sym-bolMike Richter, Tommy Salo, JohnVanbiesbrouckNHL goalie none goalie / goaltender / goal-keeper / player / backupDodoma, Mwanza, Mbeya African city none facilitator / townfresco, wall painting, Mural art painting / picture painting / world / piece / floor/ symbolQinghua University, Fudan University,Beijing Universityuniversity none university / institution / stock-holder / college / schoolFederal Bureau of Investigation, DrugEnforcement Administration, FBIgovernmental depart-mentlaw enforcement agency agency / police / investigation/ department / FBIby chance on an assessment.
An experiment with K ?0.8 is generally viewed as reliable and 0.67 < K < 0.8allows tentative conclusions.
The Kappa statistic for ourexperiment is K = 0.72.The human labeling is at a disadvantage since onlyone label was generated per concept.
Therefore, thehuman scores either 1 or 0 for each concept.
Our sys-tem?s highest ranking name was correct 72% of thetime.
Table 4 shows the percentage of semantic classeswith a correct label in the top 1-5 ranks returned by oursystem.Overall, 41.8% of the top-5 names extracted by oursystem were judged correct.
The overall accuracy forthe top-4, top-3, top-2, and top-1 names are 44.4%,48.8%, 58.5%, and 72% respectively.
Hence, the nameranking of our algorithm is effective.4.3 Hyponym PrecisionThe 1432 CBC concepts contain 18,000 unique words.For each concept to which a word belongs, we extractedup to 3 hyponyms, one for each of the top-3 labels forthe concept.
The result was 159,000 hyponym relation-ships.
24 are shown in the Appendix.Two judges annotated two random samples of 100relationships: one from all 159,000 hyponyms and onefrom the subset of 65,000 proper nouns.
For each in-stance, the judges were asked to decide whether thehyponym relationship was correct, partially correct orincorrect.
Table 5 shows the results.
The strict measurecounts a score of 1 for each correctly judged instanceand 0 otherwise.
The lenient measure also gives a scoreof 0.5 for each instance judged partially correct.Many of the CBC concepts contain noise.
For ex-ample, the wine cluster:Zinfandel, merlot, Pinot noir, Chardonnay,Cabernet Sauvignon, cabernet, riesling,Sauvignon blanc, Chenin blanc, sangiovese,syrah, Grape, Chianti ...contains some incorrect instances such as grape, appe-lation, and milk chocolate.
Each of these instances willgenerate incorrect hyponyms such as grape is wine andmilk chocolate is wine.
This hyponym extraction taskwould likely serve well for evaluating the accuracy oflists of semantic classes.Table 5 shows that the hyponyms involving propernouns are much more reliable than common nouns.Since WordNet contains poor coverage of proper nouns,these relationships could be useful to enrich it.4.4 RecallSemantic extraction tasks are notoriously difficult toevaluate for recall.
To approximate recall, we conductedtwo question answering (QA) tasks: answeringdefinition questions and performing QA informationretrieval.Table 2.
MRR scores for the human evaluation of naming 125random concepts.JUDGE HUMANLABELSWordNetLabelsSystemLabels1 100% 18.1% 74.4%2 91.2% 20.0% 78.1%3 89.6% 21.6% 78.8%Combined 93.6% 19.9% 77.1%Table 3.
Lenient MRR scores for the human evaluation ofnaming 125 random concepts.JUDGE HUMANLABELSWordNetLabelsSystemLabels1 100% 22.8% 85.0%2 96.0% 20.8% 86.5%3 92.0% 21.8% 85.2%Combined 96.0% 21.8% 85.6%Table 4.
Percentage of concepts with a correct name in thetop-5 ranks returned by our system.JUDGE TOP-1 TOP-2 TOP-3 TOP-4 TOP-51 68.8% 75.2% 78.4% 83.2% 84.0%2 73.6% 80.0% 81.6% 83.2% 84.8%3 73.6% 80.0% 82.4% 84.0% 88.8%Combined 72.0% 78.4% 80.8% 83.5% 85.6%Table 5.
Accuracy of 159,000 extracted hyponyms and a sub-set of 65,000 proper noun hyponyms.JUDGE All Nouns Proper NounsStrict Lenient Strict Lenient1 62.0% 68.0% 79.0% 82.0%2 74.0% 76.5% 84.0% 85.5%Combined 68.0% 72.2% 81.5% 83.8%Definition QuestionsWe chose the 50 definition questions that appeared inthe QA track of TREC2003 (Voorhees, 2003).
For ex-ample: ?Who is Aaron Copland??
and ?What is theKama Sutra??
For each question we looked for at mostfive corresponding concepts in our hyponym list.
Forexample, for Aaron Copland, we found the followinghypernyms: composer, music, and gift.
We comparedour system with the concepts in WordNet and Fleisch-man et al?s instance/concept relations (Fleischman et al2003).
Table 6 shows the percentage of correct answersin the top-1 and top-5 returned answers from each sys-tem.
All systems seem to have similar performance onthe top-1 answers, but our system has many more an-swers in the top-5.
This shows that our system has com-paratively higher recall for this task.Information (Passage) RetrievalPassage retrieval is used in QA to supply relevant in-formation to an answer pinpointing module.
The higherthe performance of the passage retrieval module, thehigher will be the performance of the answer pinpoint-ing module.The passage retrieval module can make use of thehyponym relationships that are discovered by our sys-tem.
Given a question such as ?What color ?
?, the like-lihood of a correct answer being present in a retrievedpassage is greatly increased if we know the set of allpossible colors and index them in the document collec-tion appropriately.We used the hyponym relations learned by our sys-tem to perform semantic indexing on a QA passage re-trieval task.
We selected the 179 questions from the QAtrack of TREC-2003 that had an explicit semantic an-swer type (e.g.
?What band was Jerry Garcia with?
?and ?What color is the top stripe on the U.S.
flag??
).For each expected semantic answer type correspondingto a given question (e.g.
band and color), we indexedthe entire TREC-2002 IR collection with our system?shyponyms.We compared the passages returned by the passageretrieval module with and without the semantic index-ing.
We counted how many of the 179 questions had acorrect answer returned in the top-1 and top-100 pas-sages.
Table 7 shows the results.Our system shows small gains in the performance ofthe IR output.
In the top-1 category, the performanceimproved by 20%.
This may lead to better answer selec-tions.5 Conclusions and Future WorkCurrent state of the art concept discovery algorithmsgenerate lists of instances of semantic classes but stopshort of labeling the classes with concept names.
Classlabels would serve useful in applications such as ques-tion answering to map a question concept into a seman-tic class and then search for answers within that class.We propose here an algorithm for automatically label-ing concepts that searches for syntactic patterns within agrammatical template for a class.
Of the 1432 noun con-cepts discovered by CBC, our system labelled 98.5% ofthem with an MRR score of 77.1% in a human evalua-tion.Hyponym relationships were then easily extracted,one for each instance/concept label pair.
We extracted159,000 hyponyms and achieved a precision of 68%.
Ona subset of 65,000 proper names, our performance was81.5%.This work forms an important attempt to buildinglarge-scale semantic knowledge bases.
Without beingable to automatically name a cluster and extract hypo-nym/hypernym relationships, the utility of automaticallygenerated clusters or manually compiled lists of terms islimited.
Of course, it is a serious open question howmany names each cluster (concept) should have, andhow good each name is.
Our method begins to addressthis thorny issue by quantifying the name assigned to aclass and by simultaneously assigning a number that canbe interpreted to reflect the strength of membership ofeach element to the class.
This is potentially a signifi-cant step away from traditional all-or-nothing seman-tic/ontology representations to a concept representationTable 6.
Percentage of correct answers in the Top-1 andTop-5 returned answers on 50 definition questions.SYSTEM Top-1 Top-5Strict Lenient Strict LenientWordNet 38% 38% 38% 38%Fleischman 36% 40% 42% 44%Our System 36% 44% 60% 62%Table 7.
Percentage of questions where the passage retrievalmodule returns a correct answer in the Top-1 and Top-100ranked passages (with and without semantic indexing).CORRECT TOP-1 Correct Top-100With semanticindexing43 / 179 134 / 179Without semanticindexing36 / 179 131 / 179scheme that is more nuanced and admits multiple namesand graded set memberships.AcknowledgementsThe authors wish to thank the reviewers for their helpfulcomments.
This research was partly supported by NSFgrant #EIA-0205111.ReferencesBarwise, J. and Perry, J.
1985.
Semantic innocence and un-compromising situations.
In: Martinich, A. P.
(ed.)
ThePhilosophy of Language.
New York: Oxford UniversityPress.
pp.
401?413.Berland, M. and E. Charniak, 1999.
Finding parts in very largecorpora.
In ACL-1999.
pp.
57?64.
College Park, MD.Church, K. and Hanks, P. 1989.
Word association norms, mu-tual information, and lexicography.
In Proceedings of ACL-89.
pp.
76?83.
Vancouver, Canada.Fleischman, M.; Hovy, E.; and Echihabi, A.
2003.
Offlinestrategies for online question answering: Answering ques-tions before they are asked.
In Proceedings of ACL-03.
pp.1?7.
Sapporo, Japan.Girju, R.; Badulescu, A.; and Moldovan, D. 2003.
Learningsemantic constraints for the automatic discovery of part-whole relations.
In Proceedings of HLT/NAACL-03.
pp.80?87.
Edmonton, Canada.Han, J. and Kamber, M. 2001.
Data Mining ?
Concepts andTechniques.
Morgan Kaufmann.Hearst, M. 1992.
Automatic acquisition of hyponyms fromlarge text corpora.
In COLING-92.
pp.
539?545.
Nantes,France.Hindle, D. 1990.
Noun classification from predicate-argumentstructures.
In Proceedings of ACL-90.
pp.
268?275.
Pitts-burgh, PA.Leacock, C.; Chodorow, M.; and Miller; G. A.
1998.
Usingcorpus statistics and WordNet relations for sense identifica-tion.
Computational Linguistics, 24(1):147?165.Lenat, D. 1995.
CYC: A large-scale investment in knowledgeinfrastructure.
Communications of the ACM, 38(11):33?38.Lin, D. 1994.
Principar - an efficient, broad-coverage, princi-ple-based parser.
Proceedings of COLING-94.
pp.
42?48.Kyoto, Japan.Lin, D. 1998.
Automatic retrieval and  clustering of similarwords.
In Proceedings of COLING/ACL-98.
pp.
768?774.Montreal, Canada.Lin, D. and Pantel, P. 2001.
Induction of semantic classesfrom natural language text.
In Proceedings of SIGKDD-01.pp.
317?322.
San Francisco, CA.Mann, G. S. 2002.
Fine-Grained Proper Noun Ontologiesfor Question Answering.
SemaNet?
02: Building andUsing Semantic Networks, Taipei, Taiwan.Miller, G. 1990.
WordNet: An online lexical database.
Inter-national Journal of Lexicography, 3(4).Pasca, M. and Harabagiu, S. 2001.
The informative role ofWordNet in Open-Domain Question Answering.
In Pro-ceedings of NAACL-01 Workshop on WordNet and OtherLexical Resources.
pp.
138?143.
Pittsburgh, PA.Pantel, P. and Lin, D. 2002.
Discovering Word Senses fromText.
In Proceedings of SIGKDD-02.
pp.
613?619.
Edmon-ton, Canada.Riloff, E. and Shepherd, J.
1997.
A corpus-based approach forbuilding semantic lexicons.
In Proceedings of EMNLP-1997.Riloff, E. and Jones, R. 1999.
Learning dictionaries for infor-mation extraction by multi-level bootstrapping.
In Proceed-ings of AAAI-99.
pp.
474?479.
Orlando, FL.Salton, G. and McGill, M. J.
1983.
Introduction to ModernInformation Retrieval.
McGraw HillSchank, R. and Abelson, R. 1977.
Scripts, Plans, Goals andUnderstanding: An Inquiry into Human Knowledge Struc-tures.
Lawrence Erlbaum Associates.Siegel, S. and Castellan Jr., N. J.
1988.
Nonparametric Statis-tics for the Behavioral Sciences.
McGraw-Hill.Voorhees, E. 2003.
Overview of the question answering track.To appear in Proceedings of TREC-12 Conference.
NIST,Gaithersburg, MD.Appendix.
Sample hyponyms discovered by our system.INSTANCE CONCEPT INSTANCE CONCEPTactor hero price support benefitAmeritrade brokerage republican politicianArthurRhodespitcher Royal AirForceforcebebop MUSIC Rwanda cityBuccaneer team Santa Ana cityCongressionalResearchServiceagency shot-blocker playerCuba country slavery issueDan Petrescu midfielder spa facilityHercules aircraft taxi vehicleMoscow city TerrenceMalickdirectorNokia COMPANY verbena treenominee candidate Wagner composer
