Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 53?60,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsDetecting Implicit Expressions of Sentiment in TextBased on Commonsense KnowledgeAlexandra Balahur, Jes?s M. Hermida, Andr?s MontoyoDepartment of Software and Computing SystemsUniversity of AlicanteApartado de correos 99, E-03080 Alicante, Spain{abalahur, jhermida, montoyo}@dlsi.ua.esAbstractSentiment analysis is one of the recent,highly dynamic fields in Natural LanguageProcessing.
Most existing approaches arebased on word-level analysis of texts andare able to detect only explicit expressionsof sentiment.
In this paper, we present anapproach towards automatically detectingemotions (as underlying components ofsentiment) from contexts in which no cluesof sentiment appear, based oncommonsense knowledge.
The resource webuilt towards this aim ?
EmotiNet - is aknowledge base of concepts withassociated affective value.
Preliminaryevaluations show that this approach isappropriate for the task of implicit emotiondetection, thus improving the performanceof sentiment detection and classification intext.1 IntroductionResearch in affect has a long established traditionin many sciences - linguistics, psychology, socio-psychology, cognitive science, pragmatics,marketing or communication science.
Recently,many closely related subtasks were developed alsoin the field of Natural Language Proceesing (NLP),such as emotion detection, subjectivity analysis,opinion mining to sentiment analysis, attitude andappraisal analysis or review mining (Pang and Lee,2008).Among these tasks, sentiment analysis aims atdetecting the expressions of sentiment in text andsubsequently classify them, according to theirpolarity (semantic orientation) among differentcategories (usually, among positive and negative).The problem is defined by Pang and Lee (2008) as?the binary classification task of labeling anopinionated document as expressing either anoverall positive or an overall negative.?
(Pang andLee, 2008)According to the Webster dictionary(http://www.merriam-webster.com/), sentiment suggestsa settled opinion reflective of one?s feelings, wherethe term feeling is defined as the conscioussubjective experience of emotion.
(Van den Bos,2006), ?a single component of emotion, denotingthe subjective experience process?
(Scherer, 2005).Most of the research performed in the field ofsentiment analysis has aimed at detecting explicitexpressions of sentiment (i.e.
situations wherespecific words or word combinations are found intexts).
Nevertheless, the expression of emotion ismost of the times not achieved through the use ofemotion-bearing words (Pennebaker et al, 2003),but indirectly, by presenting situations that basedon commonsense knowledge can be interpreted inan affective manner (Balahur and Montoyo, 2008;Balahur and Steinberger, 2009).In this paper, we present a method to build acommonsense knowledge base (EmotiNet)representing situations that trigger emotions.
Wedemonstrate that by using this resource, we are53able to detect emotion from textual contexts inwhich no explicit mention of affect is present.2 State of the ArtIn Artificial Intelligence (AI), the term affectivecomputing was first introduced by Picard (1995).Previous approaches to spot affect in text includethe use of models simulating human reactionsaccording to their needs and desires (Dyer, 1987),fuzzy logic (Subasic and Huettner, 2000), lexicalaffinity based on similarity of contexts ?
the basisfor the construction of WordNet Affect(Strapparava and Valitutti, 2004) or SentiWord-Net (Esuli and Sebastiani, 2005), detection ofaffective keywords (Riloff et al, 2003) andmachine learning using term frequency (Pang etal., 2002; Riloff and Wiebe, 2003), or termdiscrimination (Danisman and Alpkocak, 2008).Other proposed methods include the creation ofsyntactic patterns and rules for cause-effectmodeling (Mei Lee et al, 2009).
Significantlydifferent proposals for emotion detection in textare given in the work by (Liu et al 2003) and therecently proposed framework of sentic computing(Cambria et al, 2009), whose scope is to modelaffective reaction based on commonsenseknowledge.
For a survey on the affect models andtheir affective computing applications, see (Calvoand D?Mello, 2010).3 Motivation and ContributionThe tasks of emotion detection and sentimentanalysis have been approached by a large volumeof research in NLP .
Nevertheless, most of thisresearch has concentrated on developing methodsfor detecting only explicit mentions of sentiment intext.
Therefore, sentences such as ?I?m going to aparty?, which express an underlying emotion,cannot be classified by most of the existingapproaches.
A method to overcome this issue isproposed in by sentic computing (Cambria et al,2009) and by (Liu et al 2003), whose main idea isacquiring knowledge on the emotional effect ofdifferent concepts.
In this manner, the systemwould know that ?going to a party?
is somethingthat produces ?joy?.
However, more complexcontexts, such as ?I?m going to a party, although Ishould study for my exam.
?, where the emotionexpressed is most probably ?guilt?, cannot becorrectly detected and classified by presentsystems.In the light of these considerations, ourcontribution relies in proposing and implementinga framework for modeling affect based on theappraisal theories, which can support the automaticprocessing of texts to extract:?
The components of the situation presented(which we denote by ?action chains?)
andtheir relation (temporal, causal etc.)?
The elements on which the appraisal isdone in each action of the chain (agent,action, object);?
The appraisal criteria that canautomatically be determined from the text(modifiers of the action, actor, object ineach action chain);4 Modeling Affective Reaction UsingCommonsense KnowledgeOur main idea is that emotion can be expressed intext by presenting a sequence of actions (situationsin which different concepts appear), which, basedon commonsense knowledge and previousexperiences, trigger an emotional reaction.
Thisidea is linked to the Appraisal Theories, whichclaim that emotions are elicited and differentiatedon the basis of the subjective evaluation of thepersonal significance of a situation, object or event(De Rivera, 1977; Frijda, 1986; Johnson-Laird andOatley, 1989 ?
among others).
Viewed in a simplermanner, a situation is presented as a chain ofactions, each with an actor and an object; theappraisal depends on the temporal and causalrelationship between them, on the characteristics ofthe actors involved in the action and on the objectof the action.Given this insight, the general idea behind ourapproach is to model situations as chains of actionsand their corresponding emotional effect using anontological representation.
According to thedefinition provided by Studer et al (1998), anontology captures knowledge shared by acommunity that can be easily sharable with othercommunities.
These two characteristics areespecially relevant if we want the recall of ourapproach to be increased.
Knowledge managed inour approach has to be shared by a largecommunity and it also needs to be fed byheterogeneous sources of common knowledge to54avoid uncertainties.
However, specific assertionscan be introduced to account for the specificities ofindividuals or contexts.
In this manner, we canmodel the interaction of different events in thecontext in which they take place.5 Building a Knowledge Base forDetecting Implicit Expressions ofEmotionIn order to build a resource that is capable ofcapturing emotional reaction to real-worldsituations in which commonsense knowledge playsa significant role in the affective interpretation, weaim at representing chains of actions and theircorresponding emotional labels from severalsituations in such a way that we will be able toextract general patterns of appraisal.
Our approachdefines an action chain as a sequence of actionlinks, or simply actions that trigger an emotion onan actor.
Each specific action link can be describedwith a tuple (actor, action type, patient, emotionalreaction).In order to manage and store action chains, theapproach we propose defines a new knowledgebase, called EmotiNet, which aims to be a resourcefor detecting emotions in text, and a(semi)automatic, iterative process to build it, whichis based on existing knowledge from differentsources.
This process extracts the action chainsfrom a set of documents and adds them to the KB.Specifically, EmotiNet was built by following thenext steps:1.
The design of an ontology, which containsthe definitions of the main concepts of thedomain.2.
The extension and population of thisontology using the situations stored in theISEAR International Survey of EmotionalAntecedents and Reactions (ISEAR,http://www.unige.ch/fapse/emotion/databanks/isear.html) ?
(Scherer and Wallbott, 1997)database.3.
The expansion of the ontology usingexisting commonsense knowledge bases ?ConceptNet (Liu and Singh, 2004) andother resources ?
VerbOcean (Chklovskiand Pantel, 2004).5.1 Design of the OntologyAs mentioned before, the process of building thecore of the EmotiNet knowledge base (KB) ofaction chains started with the design of the coreontology, whose design process was specificallydivided in three stages:1.
Establishing the scope and purpose of theontology.
The EmotiNet ontology needs to captureand manage knowledge from three domains:kinship membership, emotions (and their relations)and actions (characteristics and relations betweenthem).2.
Reusing knowledge from existing ontologies.In a second stage, we searched for other ontologieson the Web containing concepts related to theknowledge cores we specified.
At the end of theprocess, we located two ontologies that are reusedin our ontological representation: the ReiActionontology (www.cs.umbc.edu/~lkagal1/rei/ontologies/ReiAction.owl), which represents actionsbetween entities in a general manner, and thefamily ontology (www.dlsi.ua.es/~jesusmhc/emotinet/family.owl), which contains knowledge aboutfamily members and the relations between them.angerfearsurprisejoysadnessshameguiltbasicEmotionbasicEmotionoppositeEmotionanticipationdisgusttrustoppositeEmotionoppositeEmotionoppositeEmotionoptimismhasEmotionvigilancehasHigherIntensityEmotionCompositeEmotionhasEmotionoppositeEmotionhasHigherIntensitybasicEmotionrdfs:subClassOfrdf:typeFigure 1.
Partial RDF graph of the Emotion Ontology.3.
Creating the final knowledge core from theontologies imported.
This third stage involved thedesign of the last remaining core, i.e.
emotion, andthe combination of the different knowledge sourcesinto a single ontology: EmotiNet.
In order todescribe the emotions and the way they relate andcompose, we employ Robert Plutchik?s wheel ofemotion (Plutchik, 2001) and Parrot?s tree-55structured list of emotions (Parrot, 2001).
Thesemodels contain an explicit modeling of therelations between the different emotions.
At theend of the design process, the knowledge coreincluded different types of relations betweenemotions and a collection of specific instances ofemotion (e.g.
anger, joy).
In the last step, thesethree cores were combined using new classes andrelations between the existing members of theseontologies (Fig.
2).EmotionPersonActionSimpleActionDomainActionFeelForgetArgueCrashemotionFelt?AgentObjectrdfs:subClassOfrdfs:subClassOfactortargetrdfs:subClassOfrdfs:subClassOfModifierisAffectedByrdfs:subClassOfimplyEmotionFigure 2.
Main concepts of EmotiNet.5.2 Extension and Population of the OntologyIn order to have a homogenous starting base, weselected from the 7667 examples in the ISEARdatabase only the 1081 cases that containeddescriptions of situations between family members.Subsequently, the examples were POS-taggedusing TreeTagger.
Within each emotion class, wethen computed the similarity of the examples withone another, using the implementation of the Leskdistance in Ted Pedersen?s Similarity Package.This score was used to split the examples in eachemotion class into six clusters using the Simple K-Means implementation in Weka.
The idea behindthis approach, confirmed by the output of theclusters, was to group examples that are similar, invocabulary and structure.
From this collection, wemanually selected a subset of 175 documents with25 expressions related to each of the emotions:anger, disgust, guilt, fear, sadness, joy and shame.The criteria for choosing this subset were thesimplicity of the sentences and the variety ofactions described.The next step was to extract the actions chainsdescribed in each of the examples.
For this, weemployed Semrol, the semantic role labeling (SRL)system introduced by Moreda et al (2007).
For thecore of knowledge in the EmotiNet KB, we need100% accurate information.
Therefore, wemanually extract the agent, the verb and the patient(the surface object of the verb) from the output ofSemrol.
For example, if we use the input sentence?I?m going to a family party because my motherobliges me to?, the system extracts two triples withthe main actors of the sentences: (I, go, familyparty) and (mother, oblige, me), related by thecausal adverb ?because?.Further on, we resolve the anaphoric expressionsautomatically, using a heuristic selection of thefamily member mentioned in the text that is closestto the anaphoric reference and whose properties(gender, number) are compatible with the ones ofthe reference.
The replacement of the references tothe speaker, e.g.
?I?, ?me?, ?myself?, is resolved bytaking into consideration the entities mentioned inthe sentence.
In case of ambiguity, we choose theyoungest, female member.
Following the lastexample, the subject of the action would beassigned to the daughter of the family and thetriples would be updated: (daughter, go,family_party) and (mother, oblige, daughter).Finally, the action links (triplets) are grouped andsorted in action chains.
This process of sorting isdetermined by the adverbial expressions thatappear within the sentence, which actually specifythe position of each action on a temporal line (e.g.?although?
?because?, ?when?).
We definedpattern rules according to which the actionsintroduced by these modifiers happen prior to orafter the current context.Using our combined emotion model as a reference,we manually assigned one of the seven most basicemotions, i.e.
anger, fear, disgust, shame, sadness,joy or guilt, or the neutral value to all the actionlinks obtained, thus generating 4-tuples (subject,action, object, emotion), e.g.
(daughter, go, familyparty, neutral) or (mother, oblige, daughter,disgust).Once we carried out these processes on the chosendocuments, we obtained 175 action chains (orderedlists of tuples).
In order to be included in theEmotiNet knowledge base, all their action linksneeded to be mapped to existing concepts orinstances within the KB.
When these did not exist,they were added to it.
We would like to highlightthat in EmotiNet, each tuple (actor, action, patient,emotion) extracted has its own representation as aninstance of the subclasses of Action.
Each in-stance56of Action is related to an instance of the class Feel,which represents the emotion felt in this action.Subsequently, these instances (action links) weregrouped in sequences of actions (class Sequence)ended by an instance of the class Feel, whichdetermine the final emotion felt by the mainactor(s) of the chain.In our example, we created two new classes Goand Oblige (subclasses of DomainAction) and twonew instances of them: instance act1 (?Go?,?daughter?, ?family_party?, ?Neutral?
); andinstance act2 (?Oblige?, ?mother?, ?daughter?,?Angry?).
The last action link already existedwithin EmotiNet from another chain so we reusedit: instance act3 (?Feel?, ?daughter?, ?anger?).
Thenext step consisted in grouping these instances intosequences by means of instances of the classSequence, which is a subclass of Action that canestablish the temporal order between two actions(which one occurred first).
Fig.
3 shows anexample of a RDF graph with the action chain ofour example.
We used Jena(http://jena.sourceforge.net/) and MySQL for themanagement and storage of EmotiNet on adatabase.hasChildfeel_anger_1go_1oblige_1sequence_1sequence_2emotionFeltactor actoractorAction Chaintargettargetangersecondsecondfirstfirstmother_f1daughter_f1disgustimpliesparty_1Figure 3.
RDF graph of an action chain.5.3 Ontology ExpansionIn order to extend the coverage of the resource, weexpanded the ontology with the actions andrelations from VerbOcean.
This process is essentialfor EmotiNet, since it adds new types of action andrelations between actions, which might not havebeen analyzed before, thus reducing the degree ofdependency between the resource and the initial setof examples.
In particular, 299 new actions wereautomatically included as subclasses ofDomainAction, which were directly related to anyof the actions of our ontology through three newrelations: can-result-in, happens-before andsimilar.6 Experiments and EvaluationThe evaluation of our approach consists in testingif by employing the model we built and theknowledge contained in the core of EmotiNet(which we denote by ?knowledge sets?
), we areable to detect the emotion expressed in newexamples pertaining to the categories in ISEAR.Therefore, we use a test set (marked with B) thatcontains 895 examples (ISEAR phrasescorresponding to the seven emotions modeled,from which core examples were removed).In order to assess the system performance on thetwo test sets, we followed the same process weused for building the core of EmotiNet, with theexception that the manual modeling of examplesinto tuples was replaced with the automaticextraction of (actor, verb, patient) triples from theoutput given by Semrol.
Subsequently, weeliminated the stopwords in the phrases containedin these three roles and performed a simple corefe-rence resolution.
Next, we ordered the actionspresented in the phrase, using the adverbs thatconnect the sentences, through the use of patterns(temporal, causal etc.).
The resulted action chainsfor each of the examples in the two test sets will beused in carrying different experiments:(1).
In the first approach, for each of the situationsin the test sets (represented now as action chains),we search the EmotiNet KB to encounter thesequences in which these actions in the chains areinvolved and their corresponding subjects.
As aresult of the search process, we obtain the emotionlabel corresponding to the new situation and thesubject of the emotion based on a weightingfunction.
This function takes into consideration thenumber of actions and the position in which theyappear in the sequence contained in EmotiNet.
Theissue in this first approach is that many of theexamples cannot be classified, as the knowledgethey contain is not present in the ontology.(2).
A subsequent approach aimed at surpassing theissues raised by the missing knowledge inEmotiNet.
In a first approximation, we aimed atintroducing extra knowledge from VerbOcean, byadding the verbs that were similar to the ones in57the core examples (represented in VerbOceanthrough the ?similar?
relation).
Subsequently, eachof the actions in the examples to be classified thatwas not already contained in EmotiNet, was soughtin VerbOcean.
In case one of the similar actionswas already contained in the KB, the actions wereconsidered equivalent.
Further on, each action wasassociated with an emotion, using the ConceptNetrelations and concepts (HasSubevent, Causes,ConceptuallyRelatedTo, HasPrerequisite).
Finally,new examples were matched against chains ofactions containing the same emotions, in the sameorder.
While more complete than the firstapproximation, this approach was also affected bylack of knowledge about the emotional content ofactions.
To overcome this issue, we proposed twoheuristics:(2a) In the first one, actions on which no affectinformation was available, were sought in withinthe examples already introduced in the EmotiNetand were assigned the most frequent class ofemotion labeling them.
The corresponding resultsare marked with A2a and B2a, respectively.
(2b) In the second approximation, we used themost frequent emotion associated to the knownlinks of a chain, whose individual emotions wereobtained from ConceptNet.
In this case, the coreof action chains is not involved in the process.
Thecorresponding results are marked with A2b andB2b.We performed the steps described on test set B.The results are shown in Table 1 (results onclassified examples) and Table 2 (results on allexamples).EmotionCorrect Total AccuracyB1 B2aB2bB1 B2aB2bB1 B2a B2bdisgust 16 16 21 44 42 4036.3638.0952.50shame 25 25 26 70 78 7335.7132.0535.62anger 31 47 57105115 12129.5240.8647.11fear 35 34 37 58 65 6060.3452.3061.67sadness 46 45 41111123 12541.4436.5832.80joy 13 16 18 25 29 35 5255.1751.43guilt 59 68 64158165 17137.3441.2137.43Total 225 251 264571617 62539.4040.6842.24Table 1.
Results of the emotion detection usingEmotiNet on classified examples in test set BEmotion Correct Total RecallB1 B2a B2b B1 B1 B2a B2bDisgust 16 16 21 59 27.11 27.11 35.59Shame 25 25 26 91 27.47 27.47 28.57Anger 31 47 57 145 21.37 32.41 39.31Fear 35 34 37 85 60.34 52.30 61.67Sadness 46 45 41 267 17.22 16.85 15.36Joy 13 16 18 50 26 32 36.00Guilt 59 68 64 198 29.79 34.34 32.32Total 225 251 264 895 25.13 28.04 29.50Baseline 126 126 126 895 14.0.7 14.07 14.07Table 2.
Results of the emotion detection usingEmotiNet on all test examples in test set B7 Discussion and conclusionsFrom the results in Table 1 and 2, we can concludethat the approach is valid and represents a methodthat is appropriate for the detection of emotionsfrom contexts where no affect-related words arepresent.
Nonetheless, much remains to be done tofully exploit the capabilities of EmotiNet.
Weshowed that the approach has a high degree offlexibility, i.e.
new information can be easilyintroduced from existing common-senseknowledge bases, such as ConceptNet, mainly dueto its internal structure and degree of granularity.The error analysis we performed shed some lighton the causes of error of the system.
The firstfinding is that extracting only the action, verb andpatient semantic roles is not sufficient.
There areother roles, such as the modifiers, which changethe overall emotion in the text.
Therefore, suchmodifiers should be included as attributes of theconcepts identified in the roles.
A further source oferrors was that lack of knowledge on specificactions.
Thus, the results of our approach can bepractically limited by the structure, expressivityand degree of granularity of the importedresources.
Therefore, to obtain the final, extendedversion of EmotiNet we should analyze theinteractions between the core and the importedresources and among these re-sources as well.Finally, other errors were produced by NLPprocesses and propagated at various steps of theprocessing chain (e.g.
SRL, coreferenceresolution).
Some of these errors cannot beeliminated; however, others can be partially solvedby using alternative NLP tools.Future work aims at extending the model byadding affective properties to the concepts58included, so that more of the appraisal criteria canbe introduced in the model, testing new methods toassign affective value to the concepts and addingnew knowledge from sources such as CYC.AcknowledgmentsThis paper has been supported by the SpanishMinistry of Science and Innovation (grant no.TIN2009-13391-C04-01), by the Spanish Ministryof Education under the FPU Program (AP2007-03076), and by the Valencian Ministry ofEducation (grant no.
PROMETEO/2009/119 andACOMP/ 2010/288).ReferencesA.
Balahur and A. Montoyo.
2008.
Applying a CultureDependent Emotion Triggers Database for TextValence and Emotion Classification, proceedings ofthe AISB 2008 Convention ?Communication,Interaction and Social Intelligence?.A.
Balahur and R. Steinberger.
2009.
RethinkingOpinion Mining in Newspaper Articles: from Theoryto Practice and Back, proceedings of the first work-shop on Opinion Mining and Sentiment Analysis(WOMSA 2009).A.
Esuli and F. Sebastiani.
2005.
Determining thesemantic orientation of terms through gloss analysis?,proceedings of CIKM 2005.B.
Pang and L. Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends inInformation Retrieval, Vol 2, Nr.
1-2, 2008.B.
Pang, L. Lee and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques, proceedings of EMNLP-02.C.
Strapparava and R. Mihalcea.
2007.
Semeval 2007task 14: Affective text, proceedings of ACL 2007.E.
Cambria, A. Hussain, C. Havasi and C. Eckl.
2009.Affective Space: Blending Common Sense andAffective Knowledge to Perform EmotiveReasoning, proceedings of the 1st Workshop onOpinion Mining and Sentiment Analysis (WOMSA).E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions, proceedings of the2003 Conference on Empirical Methods in NaturalLanguage Processing.E.
Riloff, J. Wiebe and T. Wilson.
2003.
Learningsubjective nouns using extraction patternbootstrapping.
In Proceedings of the Conference onNatural Language Learning (CoNLL) 2003, pp.25-32, Edmonton, Canada.G.
Van den Bos.
2006.
APA Dictionary of Psychology.Washington, DC: American PsychologicalAssociation.H.
Liu and P. Singh.
2004.
ConceptNet: A PracticalCommonsense Reasoning Toolkit, BT TechnologyJournal, Volume 22, Kluwer Academic Publishers.H.
Liu, H. Lieberman and T. Selker.
2003.
A Model ofTextual Affect Sensing Using Real-World Know-ledge, proceedings of IUI 2003.J.
De Rivera.
1977.
A structural theory of the emotions,Psychological Issues, 10 (4), Monograph 40.J.
W. Pennebaker, M. R. Mehl and K. Niederhoffer.2003.
Psychological aspects of natural language use:Our words, our selves, Annual Review of Psychology54, 547-577.K.
Scherer and H. Wallbott.
1997.
The ISEARQuestionnaire and Codebook, Geneva Emotion Re-search Group.K.
Scherer, K. 2005.
What are emotions?
and how canthey be measured?
Social Science Information, 3(44),695-729.M.
Dyer.
1987.
Emotions and their computations: threecomputer models, Cognition and Emotion, 1, 323-347.N.
Frijda.
1986.
The emotions, Cambridge UniversityPress.P.
Moreda, B. Navarro and M. Palomar.
2007.
Corpus-based semantic role approach in informationretrieval, Data Knowl.
Eng.
(DKE) 61(3):467-483.P.
N. Johnson-Laird and K. Oatley.
1989.
The languageof emotions: An analysis of a semantic field,Cognition and Emotion, 3, 81-123.P.
Subasic and A. Huettner.
2000.
Affect Analysis oftext using fuzzy semantic typing, IEEE Trasactionson Fuzzy System, 9, 483-496.R.
A. Calvo and S. D?Mello.
2010.
Affect Detection: AnInterdisciplinary Review of Models, Methods andTheir Applications, IEEE Transactions on AffectiveComputing, Vol.
1, No.
1, Jan.-Jun.R.
Picard.
1995.
Affective computing, Technical re-port, MIT Media Laboratory.R.
Plutchik.
2001.
The Nature of Emotions.
AmericanScientist.
89, 344.R.
Studer, R. V. Benjamins and D. Fensel.
1998.Knowledge engineering: Principles and methods,Data & Knowledge Engineering, 25(1-2):161?197.59S.
Y. Mei Lee, Y. Chen and C.-R. Huang.
2009.
CauseEvent Representations of Happiness and Surprise,proceedings of PACLIC 2009.T.
Chklovski and P. Pantel.
2004.
VerbOcean: Miningthe Web for Fine-Grained Semantic Verb Relations?,proceedings of EMNLP-04.T.
Danisman and A. Alpkocak.
2008.
Feeler: EmotionClassification of Text Using Vector Space Model,proceedings of the AISB 2008 Convention, ?Com-munication, Interaction and Social Intelligence?.W.
Parrott.
2001.
Emotions in Social Psychology,Psychology Press, Philadelphia.60
