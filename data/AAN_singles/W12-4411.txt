Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 71?75,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsTransliteration Experiments on Chinese and ArabicGrzegorz Kondrak, Xingkai Li and Mohammad SalamehDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada, T6G 2E8{gkondrak,xingkai,msalameh}@ualberta.caAbstractWe report the results of our transliteration ex-periments with language-specific adaptationsin the context of two language pairs: Englishto Chinese, and Arabic to English.
In particu-lar, we investigate a syllable-based Pinyin in-termediate representation for Chinese, and aletter mapping for Arabic.1 IntroductionTransliteration transforms an orthographic form ofa word in one writing script into an orthographicform of the same word in another writing script.
Theproblem is challenging because the relationship be-tween the source and target representations is oftenambiguous.
The process is further complicated byrestrictions in the target phonological system.DIRECTL+ (Jiampojamarn et al, 2010a) is anonline discriminative training system that incorpo-rates joint n-gram features and many-to-many align-ments, which are generated by M2M-ALIGNER (Ji-ampojamarn et al, 2007).
Our team employed vari-ants of DIRECTL+ in the previous editions of theShared Task on Transliteration (Jiampojamarn et al,2009; Jiampojamarn et al, 2010b; Bhargava et al,2011).
Recently, Bhargava and Kondrak (2012)show significant improvement in accuracy for theEnglish-to-Japanese task by leveraging supplemen-tal transliterations from other scripts.In this edition of the Shared Task on Translitera-tion, we experiment with language-specific adapta-tions for the EnCh and ArEn data sets.
The struc-ture of the paper is as follows.
In Section 2, weprovide details about the system parameters used inM2M-ALIGNER and DIRECTL+.
Section 3 pro-vides details of our strategies adopted in the EnChtask, which incorporate Chinese-specific knowledgeand system combination algorithm.
In Section 4 weelaborate on the difficulty of Arabic name transliter-ation and propose a letter mapping scheme.
In Sec-tion 5 we present the official test results.2 Base SystemWe run DIRECTL+ with all of the features describedin (Jiampojamarn et al, 2010a).
System parameterswere determined during development.
For the EnChexperiments, we set the context feature size to 5, thetransition feature size to 2, and the joint n-gram fea-ture size to 6.
For the ArEn experiments, we usedthe same settings, except that we set the joint n-gramfeature size to 5.The M2M-ALIGNER parameters were set as fol-lows.
For the English-Pinyin alignment, the maxi-mum substring length was 1 on the English side, and2 on the Pinyin side, with empty substrings (nulls)allowed only on the Pinyin side.
For ArEn, the max-imum substring length was 2 for both sides.3 English to ChineseIn this section, we introduce the strategies for im-proving DIRECTL+ performance on the EnCh task,including the use of Chinese Pinyin for preprocess-ing, and the combination of different models.3.1 Data preprocessing and cleaningIn general, the preprocessing is limited to remov-ing letter case distinctions in English names, and re-71placing every non-initial letter x with ks.
However,we observed that the provided development set con-tains a number of entries (about 3%) that containmultiple English words on the source side, but nocorresponding separators on the target side, whereasno such entries occur in the training or testing set.Since this discrepancy between sets may cause prob-lems for alignment and generation, we separatedthe multi-word entries into individual words (usingwhitespace and apostrophes as delimiters) and man-ually selected proper transliteration targets for them.We also removed individual words that have no cor-responding transliterations on the target side.
Thecleaned development set contains 2483 entries.3.2 Alignment via PinyinFollowing Jiampojamarn et al (2009; 2010b), weutilize Pinyin as an intermediate representation ofChinese characters during M2M alignment with theobjective of improving its quality.
Pinyin is theformally-adopted Romanization system for Stan-dard Mandarin for the mapping of Chinese charac-ters to Roman alphabet.
It uses the 26 letters of theEnglish alphabet except for the letter v, with the ad-dition of the letter u?.
Every Chinese character can berepresented by a sequence of Pinyin letters accord-ing to the way it is pronounced.
Numerous freelyavailable online tools exist for facilitating Chinese-Pinyin conversion1 .In our experiments, the original Chinese charac-ters from the target side of the training set are con-verted to Pinyin before M2M alignment.
A smallpart of them (about 50 out of approximately 500distinct Chinese characters in the Shared Task data)have multiple pronunciations, and can thus be rep-resented by different Pinyin sequences.
For thosecharacters we manually select the pronunciationsthat are normally used for names.After the alignment between English and Pinyinrepresentation has been generated by M2M-ALIGNER, we use it to derive the alignment betweenEnglish and Chinese characters, which is then usedfor training DIRECTL+.
This preprocessing step re-sults in a more accurate alignment as it substantiallyreduces the number of target symbols from around500 distinct Chinese characters to 26 Pinyin letters.1For instance, http://www.chinesetopinyin.comOur approach is to utilize Pinyin only in the align-ment phase, and converts it back to Chinese charac-ters before the training phase.
We do not incorporatePinyin into the generation phase in order to avoidproblems involved in converting the transliterationresults from Pinyin back to Chinese characters.
Forexample, a Pinyin subsequence may have multipleChinese character mappings because of the fact thatmany Chinese characters have the same Pinyin rep-resentation.
In addition, it is not always clear how topartition the Pinyin sequence into substrings corre-sponding to individual Chinese characters.The choice of the appropriate Chinese charactersequence is the problem further complicating theconversion from Pinyin.
We experimented with a tri-gram language model trained on the target Chineseside of the training set for the purpose of identify-ing the correct transliteration result.
However, thisapproach yielded low accuracy on the developmentset.
In contrast, the strategy of using Pinyin only forthe alignment introduces no ambiguity because weknow the mapping between Pinyin sequences andthe target Chinese side of the training set.3.3 Syllabic PinyinThe Pinyin sequences representing the pronuncia-tions of Chinese characters should not be interpretedas combinations of individual letters.
Rather, a Man-darin phonetic syllable (the pronunciation of oneChinese character) is composed of an optional on-set (?initial?)
followed by an obligatory rhyme (?fi-nal?).
The rhyme itself is composed of an obligatorynucleus followed by an optional coda.
Phonetically,the onset contains a single consonant, the nucleuscontains a vowel or a diphthong, and the coda con-tains a single consonant ([r], [n] or [N]).
Both the on-set and the rhyme can be represented by either a sin-gle letter or sequence of two or three letters.
It is theinitials and finals listed in Table 1 rather than Pinyinletters that are the phonemic units of Pinyin for Stan-dard Mandarin.
The pronunciation of a multi-letterinitial/final is often different from the pronunciationof the sequence of its individual letters.
Treatingconverted Pinyin as a sequence of separate lettersmay result in an incorrect phonetic transcription.In this paper, we further experiment with encod-ing the converted sequences of Pinyin letters as thesequences of initials and finals for M2M alignment.72Initialsb p m f d t n lg k h j q x zh chsh r z c s y wFinalsa o e i u u?
ai eiui ao ou iu ie u?e er anen in un u?n ang eng ing ongTable 1: The initials and finals in Chinese Pinyin.Although the size of the alphabet increases from 26letters to 47 initials and finals, the original Chinesepronunciation is represented more precisely.
We re-fer to the new model which is trained on Pinyininitials and finals as PINYIN-SYL, and to the pre-viously proposed model which is trained on Pinyinletters as PINYIN-LET.3.4 System combinationThe combination of models based on differentprinciples may lead to improved prediction accu-racy.
We adopt the simple voting algorithm forsystem combination proposed by Jiampojamarn etal.
(2009), with minor modifications.
Since herewe combine only two systems (PINYIN-LET andPINYIN-SYL), the algorithm becomes even simpler.We first rank the participating models according totheir overall top-1 accuracy2 on the development set.Note that the n-best list produced by DIRECTL+may contain multiple copies of the same outputwhich differ only in the implied input-output align-ment.
We allow such duplicates to contribute to thevoting tally.
The top-1 prediction is selected fromthe set of top-1 predictions produced by the partic-ipating models, with ties broken by voting and thepreference for the highest-ranking system.
For con-structing n-best candidate lists, we order the candi-date transliterations according to the highest rankassigned by either of the systems, with ties againbroken by voting and the preference for the highest-ranking system.
We refer to this combined model asCOMBINED.Table 2 shows the results of the three discussedapproaches trained on the original training set, and2Word accuracy in top-1 evaluates only the top translitera-tion candidate produced by a transliteration system.System top-1 F-scorePINYIN-LET 0.296 0.679PINYIN-SYL 0.302 0.681COMBINED 0.304 0.682Table 2: Development results on EnCh.tested on the cleaned development set.
PINYIN-SYLperforms slightly better than PINYIN-LET, whichhints at the advantage of using Pinyin initials and fi-nals over Pinyin letters as the intermediate represen-tation during the alignment.
The combination of thetwo models produces a marginally higher F-score3.The likely reason for the limited gain is the strongsimilarity of the two combined models.
We exper-imented with adding a third model that is traineddirectly on the original Chinese characters withoutusing Pinyin as the intermediate representation, butits accuracy was lower, and the accuracy of the re-sulting combined model was below PINYIN-SYL.4 Arabic to EnglishArabic script has 36 letters and 9 diacritics.
Amongthese letters, the letters Alif and Yaa can be repre-sented in different forms (@@ @@ and ??
,respectively).
The ArEn data set contains Arabicnames without diacritics, which adds ambiguity tothe transliteration task.
When transliterated, suchdiacritics would appear as an English vowel.
Forexample, it is difficult to tell whether the correcttransliteration of the two-letter name l .'.
is Baj, Bujor Bij because of the lacking vowel diacritic.
Also,some Arabic consonants are transliterated into dou-ble English consonant because of the Shadda dia-critic.
Finally, some letters might have a differentpronunciation (or none) when they occur at the endof the Arabic word.
For example, the final letter ?is pronounced differently in ?
?
?
?
@ (Atamana) and?
G Am.'.
(Bagani).In the transliterations provided in the ArEndataset, the different forms of Alif, the Hamza let-ter (Z), and the Ain letter (?)
are sometimes renderedas an apostrophe.
In order to reduce the ambigu-ity, we devised a mapping shown in Table 3.
The3The mean F-score measures how different, on average, thetop transliteration candidate is from its closest reference.73Arabic English@@@ Alif forms, ?
Taa Marbouta a?
Sahd, ?
Seen s?
Dahd, X Dal d?
Tah, H Taa tTable 3: The mapping of Arabic letters to their Englishequivalents.mapping reduces sets of Arabic letters that have thesame corresponding English letter to a single higher-frequency symbol.
For example, both 	?
and X char-acters tend to correspond to the letter d in English,so we replace all occurrences of the former with thelatter.
We refer to this variant as LETTER-MAP, asopposed to NO-MAP, which is the baseline systemwith no additional mapping.Arabic compound names may be separated byspace in their Arabic form or when transliterated.We treated the space similar to any alphabetic char-acter.
Also, any punctuation characters such as theapostrophe and hyphen on the English side are alsotreated as an alphabetic character.System top-1 F-scoreNO-MAP 0.529 0.926LETTER-MAP 0.519 0.925Table 4: Development results on ArEn.Table 4 shows our results on the original devel-opment set (2588 names).
For these experiments,we split the original training set into a new train-ing (25114 names) and development (2064 names)sets.
The results indicate that the additional map-ping actually decreases the overall accuracy with re-spect to the baseline.
It seems that the mapping de-creases the amount of information available to DI-RECTL+, without sufficiently reducing the ambigu-ity.
This confirms the previous findings that manu-ally crafted rules for transliteration are generally in-effective (Karimi et al, 2011).5 Final resultsTable 5 shows our results as provided by the SharedTask organizers.
For the EnCh task submission, weTask System top-1 F-scoreEnCh PINYIN-LET 0.324 0.668PINYIN-SYL 0.325 0.673COMBINED 0.325 0.672ArEn NO-MAP 0.583 0.933Table 5: Official test results.trained the PINYIN-LET and PINYIN-SYL modelson the set that includes both the original training setand the cleaned development set.
The output of theCOMBINED system was designated as our PrimaryRun.
The final results generally agree with our de-velopment results presented in Section 3, but the per-formance differences between models are smaller.For the ArEn task, we decided not to submit the out-put of the LETTER-MAP version because of the neg-ative outcome of our development experiment.According to the top-1 measure, our primary sys-tem was ranked second on the English-to-Chinesetask, and third on the Arabic-to-English task.
In bothcases, we were within 0.5% of the best top-1 result.In addition, in both cases, we obtained the best re-sults among the primary systems according to theF-score measure.6 ConclusionIn this paper, we described our submission to theNEWS 2012 Shared Task on Machine Translitera-tion.
In the EnCh task, our focus was on gener-ating better alignment by employing Pinyin as theintermediate representation.
A more coarse-grainedrepresentation that uses Pinyin initials and finals ap-pears to be a step in the right direction.
In the ArEntask, we found that reducing the number of distinctArabic characters does not improve the accuracy ofthe base system.AcknowledgmentsWe wish to thank Sittichai Jiampojamarn for helpwith DIRECTL+.
This research was supportedby the Natural Sciences and Engineering ResearchCouncil of Canada.74ReferencesAditya Bhargava and Grzegorz Kondrak.
2012.
Leverag-ing supplemental representations for sequential trans-duction.
In Proceedings of the NAACL-HLT.Aditya Bhargava, Bradley Hauer, and Grzegorz Kon-drak.
2011.
Leveraging transliterations from multi-ple languages.
In Proceedings of the 3rd Named En-tities Workshop (NEWS 2011), pages 36?40, ChiangMai, Thailand, November.
Asian Federation of Natu-ral Language Processing.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden markov models to letter-to-phoneme con-version.
In Human Language Technologies 2007: TheConference of the North American Chapter of the As-sociation for Computational Linguistics; Proceedingsof the Main Conference, pages 372?379, Rochester,New York, April.
Association for Computational Lin-guistics.Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,Kenneth Dwyer, and Grzegorz Kondrak.
2009.
Di-rectl: a language independent approach to translitera-tion.
In Proceedings of the 2009 Named Entities Work-shop: Shared Task on Transliteration (NEWS 2009),pages 28?31, Suntec, Singapore, August.
Associationfor Computational Linguistics.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2010a.
Integrating joint n-gram featuresinto a discriminative training framework.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 697?700, Los An-geles, California, June.
Association for ComputationalLinguistics.Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,Aditya Bhargava, Qing Dou, Mi-Young Kim, andGrzegorz Kondrak.
2010b.
Transliteration generationand mining with limited training resources.
In Pro-ceedings of the 2010 Named Entities Workshop, pages39?47, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2011.Machine transliteration survey.
ACM Comput.
Surv.,43(3):17:1?17:46, April.75
