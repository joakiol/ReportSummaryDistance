Improving the Accuracy of Subcategorizations Acquired from CorporaNaoki YoshinagaDepartment of Computer Science,University of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033yoshinag@is.s.u-tokyo.ac.jpAbstractThis paper presents a method of improv-ing the accuracy of subcategorizationframes (SCFs) acquired from corpora toaugment existing lexicon resources.
Iestimate a confidence value of each SCFusing corpus-based statistics, and thenperform clustering of SCF confidence-value vectors for words to capture co-occurrence tendency among SCFs in thelexicon.
I apply my method to SCFsacquired from corpora using lexiconsof two large-scale lexicalized grammars.The resulting SCFs achieve higher pre-cision and recall compared to SCFs ob-tained by naive frequency cut-off.1 IntroductionRecently, a variety of methods have been proposedfor acquisition of subcategorization frames (SCFs)from corpora (surveyed in (Korhonen, 2002)).One interesting possibility is to use these tech-niques to improve the coverage of existing large-scale lexicon resources such as lexicons of lexi-calized grammars.
However, there has been littlework on evaluating the impact of acquired SCFswith the exception of (Carroll and Fang, 2004).The problem when we integrate acquired SCFsinto existing lexicalized grammars is lower qual-ity of the acquired SCFs, since they are acquiredin an unsupervised manner, rather than being man-ually coded.
If we attempt to compensate for thepoor precision by being less strict in filtering outless likely SCFs, then we will end up with a largernumber of noisy lexical entries, which is problem-atic for parsing with lexicalized grammars (Sarkaret al, 2000).
We thus need some method of select-ing the most reliable set of SCFs from the systemoutput as demonstrated in (Korhonen, 2002).In this paper, I present a method of improvingthe accuracy of SCFs acquired from corpora in or-der to augment existing lexicon resources.
I firstestimate a confidence value that a word can haveeach SCF, using corpus-based statistics.
To cap-ture latent co-occurrence tendency among SCFsin the target lexicon, I next perform clustering ofSCF confidence-value vectors of words in the ac-quired lexicon and the target lexicon.
Since eachcentroid value of the obtained clusters indicateswhether the words in that cluster have each SCF,we can eliminate SCFs acquired in error and pre-dict possible SCFs according to the centroids.I applied my method to SCFs acquired froma corpus of newsgroup posting about mobilephones (Carroll and Fang, 2004), using theXTAG English grammar (XTAG Research Group,2001) and the LinGO English Resource Grammar(ERG) (Copestake, 2002).
I then compared theresulting SCFs with SCFs obtained by naive fre-quency cut-off to observe the effects of clustering.2 Background2.1 SCF Acquisition for LexicalizedGrammarsI start by acquiring SCFs for a lexicalized gram-mar from corpora by the method described in (Car-roll and Fang, 2004).#S(EPATTERN :TARGET |yield|:SUBCAT (VSUBCAT NP):CLASSES ((24 51 161) 5293):RELIABILITY 0:FREQSCORE 0.26861903:FREQCNT 1 :TLTL (VV0):SLTL ((|route| NN1)):OLT1L ((|result| NN2)):OLT2L NIL:OLT3L NIL :LRL 0))Figure 1: An acquired SCF for a verb ?yield?In their study, they first acquire fine-grainedSCFs using the unsupervised method proposed byBriscoe and Carroll (1997) and Korhonen (2002).Figure 1 shows an example of one acquired SCFentry for a verb ?yield.?
Each SCF entry hasseveral fields about the observed SCF.
I explainhere only its portion related to this study.
TheTARGET field is a word stem, the first number inthe CLASSES field indicates an SCF type, and theFREQCNT field shows how often words derivablefrom the word stem appeared with the SCF type inthe training corpus.
The obtained SCFs comprisethe total 163 SCF types which are originally basedon the SCFs in the ANLT (Boguraev and Briscoe,1987) and COMLEX (Grishman et al, 1994) dic-tionaries.
In this example, the SCF type 24 corre-sponds to an SCF of transitive verb.They then obtain SCFs for the target lexicalizedgrammar (the LinGO ERG (Copestake, 2002) intheir study) using a handcrafted translation mapfrom these 163 types to the SCF types in the targetgrammar.
They reported that they could achievea coverage improvement of 4.5% but that aver-age parse time was doubled.
This is because theydid not use any filtering method for the acquiredSCFs to suppress an increase of the lexical ambi-guity.
We definitely need some method to controlthe quality of the acquired SCFs.Their method is extendable to any lexicalizedgrammars, if we could have a translation map fromthese 163 types to the SCF types in the grammar.2.2 Clustering of Verb SCF DistributionsThere is some related work on clustering ofverbs according to their SCF probability distri-butions (Schulte im Walde and Brew, 2002; Ko-rhonen et al, 2003).
Schulte im Walde and(true) probability distribution00.10.20.30.40.50.60.70.80.91NP None NP_to-PP NP_PP PPsubcategorization frameprobabilityapplyrecognitionthresholdFigure 2: SCF probability distributions for applyBrew (2002) used the k-Means (Forgy, 1965) al-gorithm to cluster SCF distributions for monose-mous verbs while Korhonen et al (2003) appliedother clustering methods to cluster polysemic SCFdata.
These studies aim at obtaining verb seman-tic classes, which are closely related to syntacticbehavior of argument selection (Levin, 1993).Korhonen (2002) made use of SCF distributionsfor representative verbs in Levin?s verb classes toobtain accurate back-off estimates for all the verbsin the classes.
In this study, I assume that thereare classes whose element words have identicalSCF types.
I then obtain these classes by clus-tering acquired SCFs, using information availablein the target lexicon, and directly use the obtainedclasses to eliminate implausible SCFs.3 Method3.1 Estimation of Confidence Values for SCFsI first create an SCF confidence-value vector vi foreach word wi, an object for clustering.
Each el-ement vi j in vi represents a confidence value ofSCF s j for a word wi, which expresses how strongthe evidence is that the word wi has SCF s j. Notethat a confidence value con fi j is not a probabilitythat a word wi appears with SCF s j but a proba-bility of existence of SCF s j for the word wi.
Inthis study, I assume that a word wi appears witheach SCF s j with a certain (non-zero) probabil-ity ?i j(= p(si j|wi)> 0 where ?
j ?i j = 1), but onlySCFs whose probabilities exceed a certain thresh-old are recognized in the lexicon.
I hereafter callthis threshold recognition threshold.
Figure 2 de-picts a probability distribution of SCF for apply.In this context, I can regard a confidence value ofeach SCF as a probability that the probability ofthat SCF exceeds the recognition threshold.One intuitive way to estimate a confidence valueis to assume an observed probability, i.e., relativefrequency, is equal to a probability ?i j of SCF s jfor a word wi (?i j = f reqi j/?
j f reqi j where f reqi jis a frequency that a word wi appears with SCF s jin corpora).
When the relative frequency of s j fora word wi exceeds the recognition threshold, itsconfidence value con fi j is set to 1, and otherwisecon fi j is set to 0.
However, an observed probabil-ity is unreliable for infrequent words.
Moreover,when we want to encode confidence values of re-liable SCFs in the target grammar, we cannot dis-tinguish the confidence values of those SCFs withconfidence values of acquired SCFs.The other promising way to estimate a confi-dence value, which I adopt in this study, is to as-sume a probability ?i j as a stochastic variable inthe context of Bayesian statistics (Gelman et al,1995).
In this context, a posteriori distribution ofthe probability ?i j of an SCF s j for a word wi isgiven by:p(?i j|D) =P(?i j)P(D|?i j)P(D)=P(?i j)P(D|?i j)?
10 P(?i j)P(D|?i j)d?i j, (1)where P(?i j) is a priori distribution, and D is thedata we have observed.
Since every occurrenceof SCFs in the data D is independent with eachother, the data D can be regarded as Bernoulli tri-als.
When we observe the data D that a word wiappears n times in total and x(?
n) times with SCFs j,1 its conditional distribution is represented bybinominal distribution:P(D|?i j) =(nx)?
xi j(1?
?i j)(n?x).
(2)To calculate this a posteriori distribution, I needto define the a priori distribution P(?i j).
The ques-tion is which probability distribution of ?i j canappropriately reflects prior knowledge.
In otherwords, it should encode knowledge we use to es-timate SCFs for unknown words.
I simply deter-mine it from distributions of observed probabilityvalues of s j for words seen in corpora2 by using1The values of FREQCNT is used to obtain n and x.2I estimated a priori distribution separately for each typeof SCF from words that appeared more than 50 times in thetraining corpus in the following experiments.a method described in (Tsuruoka and Chikayama,2001).
In their study, they assume a priori distri-bution as the beta distribution defined as:p(?i j|?
,? )
=?
?
?1i j (1?
?i j)??1B(?
,? )
, (3)where B(?
,? )
= ?
10 ?
?
?1i j (1 ?
?i j)?
?1d?i j. Thevalue of ?
and ?
is determined by moment esti-mation.3 By substituting Equations 2 and 3 intoEquation 1, I finally obtain the a posteriori distri-bution p(?i j|D) as:p(?i j|?
,?
,D)= c ??
x+?
?1i j (1?
?i j)n?x+?
?1,(4)where c =(nx)/(B(?
,?
)?
10 P(?i j)P(D|?i j)d?i j).When I regard the recognition threshold as t, Ican calculate a confidence value con fi j that a wordwi can have s j by integrating the a posteriori dis-tribution p(?i j|D) from the threshold t to 1:con fi j =?
1tc ??
x+?
?1i j (1?
?i j)n?x+?
?1d?i j.
(5)By using this confidence value, I represent an SCFconfidence-value vector vi for a word wi in the ac-quired SCF lexicon (vi j = con fi j).In order to combine SCF confidence-value vec-tors for words acquired from corpora and those forwords in the lexicon of the target grammar, I alsorepresent an SCF confidence-value vector v?i for aword w?i in the target grammar by:v?i j ={1?
?
w?i has s j in the lexicon?
otherwise, (6)where ?
expresses an unreliability of the lexicon.In this study, I trust the lexicon as much as possibleby setting ?
to the machine epsilon.3.2 Clustering of SCF Confidence-ValueVectorsI next present a clustering algorithm of wordsaccording to their SCF confidence-value vectors.Given k initial representative vectors called cen-troids, my algorithm iteratively updates clusters byassigning each data object to its closest centroid3The expectation and variance of the beta distribution aremade equal to those of the observed probability values.Input: a set of SCF confidence-valuevectors V= {v1,v2, .
.
.
,vn} ?
Rma distance function d : Rm ?Zm ?
Ra function to compute a centroid?
: {v j1 ,v j2 , .
.
.
,v jl }?
Zminitial centroids C= {c1,c2, .
.
.
,ck} ?
ZmOutput: a set of clusters {Cj}while cluster members are not stable doforeach cluster CjCj = {vi |?cl ,d(vi,c j)?
d(vi,cl)} (1)end foreachforeach clusters Cjc j = ?
(Cj) (2)end foreachend whilereturn {Cj}Figure 3: Clustering algorithm for SCFconfidence-value vectorsand recomputing centroids until cluster membersbecome stable, as depicted in Figure 3.Although this algorithm is roughly based on thek-Means algorithm, it is different from k-Means inimportant respects.
I assume the elements of thecentroids of the clusters as a discrete value of 0 or1 because I want to obtain clusters whose elementwords have the exactly same set of SCFs.I then derive a distance function d to calculatea probability that a data object vi should have anSCF set represented by a centroid cm as follows:d(vi,cm) = ?cm j=1vi j ?
?cm j=0(1?
vi j).
(7)By using this function, I can determine the closestcluster as argmaxCmd(vi,cm) ((1) in Figure 3).After every assignment, I calculate a next cen-troid cm of each cluster Cm ((2) in Figure 3) bycomparing a probability that the words in the clus-ter have an SCF s j and a probability that the wordsin the cluster do not have the SCF s j as follows:cm j =??
?1 when ?vi?Cmvi j > ?vi?Cm(1?
vi j)0 otherwise.
(8)I next address the way to determine the num-ber of clusters and initial centroids.
In this study,I assume that the most of the possible set of SCFsfor words are included in the lexicon of the tar-get grammar,4 and make use of the existing sets of4When the lexicon is less accurate, I can determine thenumber of clusters using other algorithms (Hamerly, 2003).SCFs for the words in the lexicon to determine thenumber of clusters and initial centroids.
I first ex-tract SCF confidence-value vectors from the lexi-con of the grammar.
By eliminating duplicationsfrom them and regarding ?
= 0 in Equation 6, I ob-tain initial centroids cm.
I then initialize the num-ber of clusters k to the number of cm.I finally update the acquired SCFs using the ob-tained clusters and the confidence values of SCFsin this order.
I call the following procedure cen-troid cut-off t when the confidence values are es-timated under the recognition threshold t. Sincethe value cm j of a centroid cm in a cluster Cm rep-resents whether the words in the cluster can haveSCF s j, I first obtain SCFs by collecting SCF s jfor a word wi ?
Cm when cm j is 1.
I then elimi-nate implausible SCFs s j for wi from the resultingSCFs according to their confidence values con fi j.In the following, I compare centroid cut-offwith frequency cut-off and confidence cut-off t,which use relative frequencies and confidence val-ues calculated under the recognition threshold t,respectively.
Note that these cut-offs use onlycorpus-based statistics to eliminate SCFs.4 ExperimentsI applied my method to SCFs acquired from135,902 sentences of mobile phone newsgrouppostings archived by Google.com, which is thesame data used in (Carroll and Fang, 2004).
Thenumber of acquired SCFs was 14,783 for 3,864word stems, while the number of SCF types inthe data was 97.
I then translated the 163 SCFtypes into the SCF types of the XTAG Englishgrammar (XTAG Research Group, 2001) and theLinGO ERG (Copestake, 2002)5 using translationmappings built by Ted Briscoe and Dan Flickingerfrom 23 of the SCF types into 13 (out of 57 possi-ble) XTAG SCF types, and 129 into 54 (out of 216possible) ERG SCF types.To evaluate my method, I split each lexicon ofthe two grammars into the training SCFs and thetesting SCFs.
The words in the testing SCFs wereincluded in the acquired SCFs.
When I applymy method to the acquired SCFs using the train-ing SCFs and evaluate the resulting SCFs with the5I used the same version of the LinGO ERG as (Carrolland Fang, 2004) (1.4; April 2003) but the map is updated.00.20.40.60.810  0.2  0.4  0.6  0.8  1RecallPrecisionAB C DA: frequency cut-offB: confidence cut-off 0.01C: confidence cut-off 0.03D: confidence cut-off 0.0500.20.40.60.810  0.2  0.4  0.6  0.8  1RecallPrecisionABCDA: frequency cut-offB: confidence cut-off 0.01C: confidence cut-off 0.03D: confidence cut-off 0.05XTAG ERGFigure 4: Precision and recall of the resulting SCFs using confidence cut-offs and frequency cut-off: theXTAG English grammar (left) the LinGO ERG (right)00.20.40.60.810  0.2  0.4  0.6  0.8  1RecallPrecisionABCDA: frequency cut-offB: centroid cut-off* 0.05C: centroid cut-off 0.05D: confidence cut-off 0.0500.20.40.60.810  0.2  0.4  0.6  0.8  1RecallPrecisionA BCDA: frequency cut-offB: centroid cut-off* 0.05C: centroid cut-off 0.05D: confidence cut-off 0.05XTAG ERGFigure 5: Precision and recall of the resulting SCFs using confidence cut-off and centroid cut-off: theXTAG English grammar (left) the LinGO ERG (right)testing SCFs, we can estimate to what extent mymethod can preserve reliable SCFs for words un-known to the grammar.6 The XTAG lexicon wassplit into 9,437 SCFs for 8,399 word stems astraining and 423 SCFs for 280 word stems as test-ing, while the ERG lexicon was split into 1,608SCFs for 1,062 word stems as training and 292SCFs for 179 word stems as testing.
I extractedSCF confidence-value vectors from the trainingSCFs and the acquired SCFs for the words in thetesting SCFs.
The number of the resulting dataobjects was 8,679 for XTAG and 1,241 for ERG.The number of initial centroids7 extracted fromthe training SCFs was 49 for XTAG and 53 forERG.
I then performed clustering of 8,679 dataobjects into 49 clusters and 1,241 data objects into6I here assume that the existing SCFs for the words in thelexicon is more reliable than the other SCFs for those words.7I used the vectors that appeared for more than one word.53 clusters, and then evaluated the resulting SCFsby comparing them to the testing SCFs.I first compare confidence cut-off with fre-quency cut-off to observe the effects of Bayesianestimation.
Figure 4 shows precision and recallof the SCFs obtained using frequency cut-off andconfidence cut-off 0.01, 0.03, and 0.05 by varyingthreshold for the confidence values and the relativefrequencies from 0 to 1.8 The graph indicates thatthe confidence cut-offs achieved higher recall thanthe frequency cut-off, thanks to the a priori distri-butions.
When we compare the three confidencecut-offs, we can improve precision using higherrecognition thresholds while we can improve re-call using lower recognition thresholds.
This isquite consistent with our expectations.8 Precision=Correct SCFs for the words in the resulting SCFsAll SCFs for the words in the resulting SCFsRecall = Correct SCFs for the words in the resulting SCFsAll SCFs for the words in the test SCFsI then compare centroid cut-off with confidencecut-off to observe the effects of clustering.
Fig-ure 5 shows precision and recall of the resultingSCFs using centroid cut-off 0.05 and the confi-dence cut-off 0.05 by varying the threshold for theconfidence values.
In order to show the effectsof the use of the training SCFs, I also performedclustering of SCF confidence-value vectors in theacquired SCFs with random initialization (k = 49(for XTAG) and 53 (for ERG); centroid cut-off0.05*).
The graph shows that clustering is mean-ingful only when we make use of the reliable SCFsin the manually-coded lexicon.
The centroid cut-off using the lexicon of the grammar boosted pre-cision compared to the confidence cut-off.The difference between the effects of mymethod on XTAG and ERG would be due to thefiner-grained SCF types of ERG.
This resultedin lower precision of the acquired SCFs for ERG,which prevented us from distinguishing infrequent(correct) SCFs from SCFs acquired in error.
How-ever, since unusual SCFs tend to be included in thelexicon, we will be able to have accurate clustersfor unknown words with smaller SCF variations aswe achieved in the experiments with XTAG.5 Concluding Remarks and Future WorkIn this paper, I presented a method to improvethe quality of SCFs acquired from corpora usingexisting lexicon resources.
I applied my methodto SCFs acquired from corpora using lexicons ofthe XTAG English grammar and the LinGO ERG,and have shown that it can eliminate implausibleSCFs, preserving more reliable SCFs.In the future, I need to evaluate the quality ofthe resulting SCFs by manual analysis and by us-ing the extended lexicons to improve parsing.
Iwill investigate other clustering methods such ashierarchical clustering, and use other informationfor clustering such as semantic preference of argu-ments of SCFs to have more accurate clusters.AcknowledgmentsI thank Yoshimasa Tsuruoka and Takuya Mat-suzaki for their advice on probabilistic modeling,Alex Fang for his help in using the acquired SCFs,and Anna Korhonen for her insightful suggestionson evaluation.
I am also grateful to Jun?ichi Tsujii,Yusuke Miyao, John Carroll and the anonymousreviewers for their valuable comments.
This workwas supported in part by JSPS Research Fellow-ships for Young Scientists and in part by CREST,JST (Japan Science and Technology Agency).ReferencesB.
Boguraev and T. Briscoe.
1987.
Large lexicons for naturallanguage processing: utilising the grammar coding systemof LDOCE.
Computational Linguistics, 13(4):203?218.T.
Briscoe and J. Carroll.
1997.
Automatic extraction ofsubcategorization from corpora.
In Proc.
the fifth ANLP,pages 356?363.J.
Carroll and A. C. Fang.
2004.
The automatic acquisitionof verb subcategorizations and their impact on the perfor-mance of an HPSG parser.
In Proc.
the first ijc-NLP, pages107?114.A.
Copestake.
2002.
Implementing typed feature structuregrammars.
CSLI publications.E.
W. Forgy.
1965.
Cluster analysis of multivariate data: Ef-ficiency vs. interpretability of classifications.
Biometrics,21:768?780.A.
Gelman, J.
B. Carlin, H. S. Stern, and D. B. Rubin, editors.1995.
Bayesian Data Analysis.
Chapman and Hall.R.
Grishman, C. Macleod, and A. Meyers.
1994.
Comlexsyntax: Building a computational lexicon.
In Proc.
the15th COLING, pages 268?272.G.
Hamerly.
2003.
Learning structure and concepts in datathrough data clustering.
Ph.D. thesis, University of Cali-fornia, San Diego.A.
Korhonen, Y. Krymolowski, and Z. Marx.
2003.
Clus-tering polysemic subcategorization frame distributions se-mantically.
In Proc.
the 41st ACL, pages 64?71.A.
Korhonen.
2002.
Subcategorization Acquisition.
Ph.D.thesis, University of Cambridge.B.
Levin.
1993.
English Verb Classes and Alternations.Chicago University Press.A.
Sarkar, F. Xia, and A. K. Joshi.
2000.
Some experimentson indicators of parsing complexity for lexicalized gram-mars.
In Proc.
the 18th COLING workshop, pages 37?42.S.
Schulte im Walde and C. Brew.
2002.
Inducing Germansemantic verb classes from purely syntactic subcategorisa-tion information.
In Proc.
the 41st ACL, pages 223?230.Y.
Tsuruoka and T. Chikayama.
2001.
Estimating reliabilityof contextual evidences in decision-list classifiers underBayesian learning.
In Proc.
the sixth NLPRS, pages 701?707.XTAG Research Group.
2001.
A Lexicalized Tree Adjoin-ing Grammar for English.
Technical Report IRCS-01-03,IRCS, University of Pennsylvania.
