Direct Maximization of Average Precision by Hill-Climbing, with aComparison to a Maximum Entropy ApproachWilliam Morgan and Warren Greiff and John HendersonThe MITRE Corporation202 Burlington Road MS K325Bedford, MA 01730{wmorgan, greiff, jhndrsn}@mitre.orgAbstractWe describe an algorithm for choosing termweights to maximize average precision.
Thealgorithm performs successive exhaustivesearches through single directions in weightspace.
It makes use of a novel technique forconsidering all possible values of average pre-cision that arise in searching for a maximum ina given direction.
We apply the algorithm andcompare this algorithm to a maximum entropyapproach.1 IntroductionThis paper presents an algorithm for searching termweight space by directly hill-climbing on average pre-cision.
Given a query and a topic?that is, given a setof terms, and a set of documents, some of which aremarked ?relevant?
?the algorithm chooses weights thatmaximize the average precision of the document set whensorted by the sum of the weighted terms.
We show thatthis algorithm, when used in the larger context of finding?optimal?
queries, performs similar to a maximum en-tropy approach, which does not climb directly on averageprecision.This work is part of a larger research program on thestudy of optimal queries.
Optimal queries, for our pur-poses, are queries that best distinguish relevant from non-relevant documents for a corpus drawn from some larger(theoretical) population of documents.
Although bothperformance on the training data and generalization abil-ity are components of optimal queries, in this paper wefocus only on the former.2 MotivationOur initial approach to the study of optimal queries em-ployed a conditional maximum entropy model.
Thismodel exhibited some problematic behavior, which mo-tivated the development of the weight search algorithmdescribed here.The maximum entropy model is used as follows.
It isgiven a set of relevant and non-relevant documents and avector of terms (the query).
For any document, the modelpredicts the probability of relevance for that documentbased on the Okapi term frequency (tf ) scores (Robertsonand Walker, 1994) for the query terms within it.
Queriesare developed by starting with the best possible one-termquery and adding individual terms from a candidate setchosen according to a mutual information criterion.
Aseach term is added, the model coefficients are set to max-imize the probability of the empirical data (the documentset plus relevance judgments), as described in Section 4.Treating the model coefficients as term weights yieldsa weighted query.
This query produces a retrieval statusvalue (RSV) for each document that is a monotonicallyincreasing function of the probability of relevance, in ac-cord with the probability ranking principle (Robertson,1977).
We can then calculate the average precision of thedocument set as ordered by these RSVs.As each additional query term represents another de-gree of freedom, one would expect model performance toimprove at each step.
However, we noted that the additionof a new term would occasionally result in a decrease inaverage precision?despite the fact that the model couldhave chosen a zero weight for the newly added term.Figure 1 shows an example of this phenomenon for oneTREC topic.This is the result of what might be called ?metric di-vergence?.
While we use average precision to evaluatethe queries, the maximum entropy model maximizes thelikelihood of the training data.
These two metrics occa-sionally disagree in their evaluation of particular weightvectors.
In particular, maximum entropy modeling mayfavor increasing the estimation of documents lower in theranking at the expense of accuracy in the prediction ofhighly ranked documents.
This can increase training datalikelihood yet have a detrimental effect on average preci-sion.The metric divergence problem led us to consider an al-ternative approach for setting term weights which wouldhill-climb on average precision directly.
In particular, wewere interested in evaluating the results produced by themaximum entropy approach?how much was the maxi-mization of likelihood affecting the ultimate performanceas measured by average precision?
The algorithm de-scribed in the following section was developed to thisend.3 The Weight Search AlgorithmThe general behavior of the weight search algorithm issimilar to the maximum entropy modeling described inSection 2?given a document corpus and a term vector,it seeks to maximize average precision by choosing aweight vector that orders the documents optimally.
Un-like the maximum entropy approach, the weight searchalgorithm hill-climbs directly on average precision.The core of the algorithm is an exhaustive search of asingle direction in weight space.
Although each directionis continuous and unbounded, we show that the searchcan be performed with a finite amount of computation.This technique arises from a natural geometric interpreta-tion of changes in document ordering and how they affectaverage precision.At the top level, the algorithm operates by cyclingthrough different directions in weight space, performingan exhaustive search for a maximum in each direction,until convergence is reached.
Although a global maxi-mum is found in each direction, the algorithm relies on agreedy assumption of unimodality and, as with the max-imum entropy model, is not guaranteed to find a globalmaximum in the multi-dimensional space.3.1 FrameworkThis section formalizes the notion of weight space andwhat it means to search for maximum average precisionwithin it.Queries in information retrieval can be treated as vec-tors of terms t1, t2, ?
?
?
, tN .
Each term is, as the namesuggests, an individual word or phrase that might oc-cur in the document corpus.
Every term t i has a weight?i determining its ?importance?
relative to the otherterms of the query.
These weights form a weight vec-tor ?
= ?
?1 ?2 ?
?
?
?N ?.
Further, given a documentcorpus ?, for each document dj ?
?
we have a ?valuevector?
?j = ?
?j1 ?j2 ?
?
?
?jN ?, where each ?value?
?ji ?
< gives some measure of term ti within documentdj?typically the frequency of occurrence or a functionthereof.
In the case of the standard tf-idf formula, ?
jiis the term frequency and ?i the inverse document fre-quency.If the document corpus and set of terms is held fixed,the average precision calculation can be considered afunction f : <N ?
[0, 1] mapping ?
to a single aver-age precision value.
Finding the weight vectors in this5 10 15 200.20.40.60.81.00.20.40.60.81.0number of termsaverageprecisionFigure 1: Average precision by query size as generatedby the maximum entropy model for TREC topic 307.context is then the familiar problem of finding maxima inan N -dimensional landscape.3.2 Powell?s algorithmOne general approach to this problem of searching amulti-dimensional space is to decompose the probleminto a series of iterated searches along single directionswithin the space.
Perhaps the most basic technique, cred-ited to Powell, is simply a round-robin-style iterationalong a set of unchanging direction vectors, until conver-gence is reached (Press et al, 1992, pp.
412-420).
Thisis the approach used in this study.Formally, the procedure is as follows.
You are givena set of direction vectors ?1, ?2, ?
?
?
, ?N and a startingpoint pi0.
First move pi0 to the maximum along ?1 andcall this pi1, i.e.
pi1 = pi0 + ?1?1 for some scalar ?1.Next move pi1 to the maximum along ?2 and call thispi2, and so on, until the final point piN .
Finally, replacepi0 with piN and repeat the entire process, starting againwith ?1.
Do this until some convergence criterion is met.This procedure has no guaranteed rate of convergence,although more sophisticated versions of Powell?s algo-rithm do.
In practice this has not been a problem.3.3 Exhaustively searching a single directionPowell?s algorithm can make use of any one-dimensionalsearch technique.
Rather than applying a completely gen-eral hill-climbing search, however, in the case where doc-ument scores are calculated by a linear equation on theterms, i.e.
?j =N?i=1?i?ji = ?
?
?jas they are in the tf-idf formula, we can exhaustivelysearch in a single direction of the weight space in an effi-cient manner.
This potentially yields better solutions andpotentially converges more quickly than a general hill-climbing heuristic.scaledocument scoreaabbecc?1?2ffd dFigure 2: Sample plot of ?
versus ?
for a given direction.The insight behind the algorithm is as follows.
Givena direction ?
in weight space and a starting point pi, thescore of each document is a linear function of the scale ?along ?
from pi:?j = ?
?
?j= (pi + ??)
?
?j= pi ?
?j + ?
(?
?
?j) .i.e.
document di?s score, plotted against ?, is a line withslope ?
?
?i and y-intercept pi ?
?j .Consider the graph of lines for all documents, such asthe example in Figure 2.
Each vertical slice of the graph,at some point ?
on the x axis, represents the order of thedocuments when ?
= ?
; specifically, the order of thedocuments is given by the order of the intersections ofthe lines with the vertical line at x = ?.Now consider the set of intersections of the documentlines.
Given two documents dr and ds, their intersection,if it exists, lies at point ?rs = (?xrs, ?yrs) where?xrs =pi ?
(?s ?
?r)?
?
(?r ?
?s), and?yrs = pi ?
?r + ?xrs (?
?
?r)(Note that this is undefined if ?
?
?r = ?
?
?s, i.e., if thedocument lines are parallel.
)Let ?
be the set of all such document intersectionpoints for a given direction, document set and term vec-tor.
Note that more than two lines may intersect at thesame point, and that two intersections may share the samex component while having different y components.Now consider the set ?x, defined as the projection of?
onto the x axis, i.e.
?x = {?
| ?
?
?
?
s.t.
?x = ?
}.The points in ?x represent precisely those values of ?where two or more documents are tied in score.
There-fore, the document ordering changes at and only at thesepoints of intersection; in other words, the points in ?xpartition the range of ?
into at most M(M ?1)/2+1 re-gions, where M is the total number of documents.
Withina given region, document ordering is invariant and henceaverage precision is constant.
As we can calculate theboundaries of, and the document ordering and averageprecision within, each region, we now have a way of find-ing the maximum across the entire space by evaluatinga finite number of regions.
Each of the O(M 2) regionsrequires an O(M log M) sort, yielding a total computa-tional bound of O(M 3 log M).In fact, we can further reduce the computation by ex-ploiting the fact that the change in document ordering be-tween any two regions is known and is typically small.The weight search algorithm functions in this manner.
Itsorts the documents completely to determine the order-ing in the left-most region.
Then, it traverses the regionsfrom left to right and updates the document ordering ineach, which does not require a sort.
Average precisioncan be incrementally updated based on the document or-dering changes.
This reduces the computational bound toO(M2 log M), the requirement for the initial sort of theO(M2) intersection points.4 Experiment SetupIn order to compare the results of the weight search al-gorithm to those of the maximum entropy model, we em-ployed the same experiment setup.
We ran on 15 topics,which were manually selected from the TREC 6, 7, and8 collections (Voorhees and Harman, 2000), with the ob-jective of creating a representative subset.
The documentsets were divided into randomly selected training, valida-tion and test ?splits?, comprising 25%, 25%, and 50%,respectively, of the complete set.For each query, a set of candidate terms was selectedbased on mutual information between (binary) term oc-currence and document relevance.
From this set, termswere chosen individually to be included in the query,and coefficients for all terms were calculated using L-BFGS, a quasi-Newton unconstrained optimization algo-rithm (Zhu et al, 1994).For experimenting with the weight search algorithm,we investigated queries of length 1 through 20 for eachtopic, so each topic involved 20 experiments.
The firstterm weight was fixed at 1.0.
The single-term queriesdid not require a weight search, as the weight of a singleterm does not affect the average precision score.
For theremaining 19 experiments for each topic, the directionvectors ?
were chosen such that the algorithm searcheda single term weight at a time.
For example, a query with5 10 15 200.20.40.60.81.00.20.40.60.81.0number of termsaverageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision0.20.40.60.81.0averageprecision301302307330332347352375383384388391407425439Figure 3: Average precision versus query size for theweight search algorithm.
Each line represents a topic.i terms used the i ?
1 directions?i,1 = ?0 1 0 0 ?
?
?
0?,?i,2 = ?0 0 1 0 ?
?
?
0?,...
?i,i?1 = ?0 0 0 0 ?
?
?
1?.The two-term query for a topic started the search from thepoint pi2,0 = ?1 0?, and each successive experiment forthat topic was initialized with the starting point pi0 equalto the final point in the previous iteration, concatenatedwith a 0.
The ?value vectors?
?j used in all experimentswere Okapi tf scores.5 ResultsThe average precision scores obtained by the maximumentropy and weight search algorithm experiments arelisted in Table 1.
The ?Best AP?
and ?No.
Terms?columns describe the query size at which average preci-sion was best and the score at that point.
These columnsshow that the maximum entropy approach performs justas well as the average precision hill-climber, and in somecases actually performs slightly better.
This suggests thatthe metric divergence as seen in Figure 1 did not prohibitthe maximum entropy approach from maximizing aver-age precision in the course of maximizing likelihood.The ?5 term AP?
column compares the performanceof the algorithms on smaller queries.
The weight searchalgorithm shows a slight advantage over the maximumentropy model on 10 of the 15 topics and equal perfor-mance on the others, but definitive conclusions are diffi-cult at this stage.Figure 3 shows the average precision achieved by theweight search algorithm, for all 20 query sizes and forall 15 topics.
Unlike the maximum entropy results,the algorithm is guaranteed to yield monotonically non-decreasing scores.Topic 5 term AP Best AP No.
TermsWS ME WS ME WS ME301 0.68 0.67 0.90 0.90 >20 >20302 0.88 0.86 1.00 1.00 10 10307 0.57 0.56 0.98 0.89 >20 >20330 0.65 0.61 1.00 1.00 10 10332 0.74 0.72 0.99 1.00 >20 18347 0.78 0.78 1.00 1.00 17 14352 0.55 0.55 0.94 0.93 >20 >20375 0.92 0.92 1.00 1.00 9 9383 0.89 0.89 1.00 1.00 9 9384 0.77 0.73 1.00 1.00 8 8388 0.82 0.80 1.00 1.00 7 7391 0.64 0.63 0.99 0.98 >20 >20407 0.83 0.83 1.00 1.00 9 9425 0.75 0.73 1.00 1.00 12 12439 0.53 0.51 1.00 1.00 17 16Table 1: Average precision achieved for weight search(WS) and maximum entropy (ME) algorithms.6 ConclusionsWe developed an algorithm for exhaustively searching acontinuous and unbounded direction in term weight spacein O(M2 log M) time.
Initial results suggest that themaximum entropy approach performs as well as this al-gorithm, which hill-climbs directly on average precision,allaying our concerns that the metric divergence exhib-ited by the maximum entropy approach is a problem forstudying optimal queries.ReferencesWilliam H. Press, Brian P. Flannery, Saul A. Teukolsky,and William T. Vetterling.
1992.
Numerical Recipesin C: The Art of Scientic Computing.
Cambridge Uni-versity Press, second edition.S.
E. Robertson and S. Walker.
1994.
Some simple effec-tive approximations to the 2-Poisson model for proba-bilistic weighted retrieval.
In W. Bruce Croft and C. J.van Rijsbergen, editors, Proc.
17th SIGIR Conferenceon Information Retrieval.S.
E. Robertson.
1977.
The probability ranking principlein IR.
Journal of Documentation, 33:294?304.E.
M. Voorhees and D. K. Harman.
2000.
Overview ofthe eighth Text REtrieval Conference (TREC-8).
InE.
M. Voorhees and D. K. Harman, editors, The EighthText REtreival Conference (TREC-8).
NIST SpecialPublication 500-246.C.
Zhu, R. Byrd, P. Lu, and J. Nocedal.
1994.
LBFGS-B:Fortran subroutines for large-scale bound constrainedoptimization.
Technical Report NAM-11, EECS De-partment, Northwestern University.
