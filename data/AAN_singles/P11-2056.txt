Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 323?328,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsModels and Training for Unsupervised Preposition Sense DisambiguationDirk Hovy and Ashish Vaswani and Stephen Tratz andDavid Chiang and Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Marina del Rey, CA 90292{dirkh,avaswani,stratz,chiang,hovy}@isi.eduAbstractWe present a preliminary study on unsu-pervised preposition sense disambiguation(PSD), comparing different models and train-ing techniques (EM, MAP-EM with L0 norm,Bayesian inference using Gibbs sampling).
Toour knowledge, this is the first attempt at un-supervised preposition sense disambiguation.Our best accuracy reaches 56%, a significantimprovement (at p <.001) of 16% over themost-frequent-sense baseline.1 IntroductionReliable disambiguation of words plays an impor-tant role in many NLP applications.
Prepositionsare ubiquitous?they account for more than 10% ofthe 1.16m words in the Brown corpus?and highlyambiguous.
The Preposition Project (Litkowski andHargraves, 2005) lists an average of 9.76 sensesfor each of the 34 most frequent English preposi-tions, while nouns usually have around two (Word-Net nouns average about 1.2 senses, 2.7 if monose-mous nouns are excluded (Fellbaum, 1998)).
Dis-ambiguating prepositions is thus a challenging andinteresting task in itself (as exemplified by the Sem-Eval 2007 task, (Litkowski and Hargraves, 2007)),and holds promise for NLP applications such asInformation Extraction or Machine Translation.1Given a sentence such as the following:In the morning, he shopped in Romewe ultimately want to be able to annotate it as1See (Chan et al, 2007) for how using WSD can help MT.in/TEMPORAL the morning/TIME he/PERSONshopped/SOCIAL in/LOCATIVERome/LOCATIONHere, the preposition in has two distinct meanings,namely a temporal and a locative one.
These mean-ings are context-dependent.
Ultimately, we wantto disambiguate prepositions not by and for them-selves, but in the context of sequential semantic la-beling.
This should also improve disambiguation ofthe words linked by the prepositions (here, morn-ing, shopped, and Rome).
We propose using un-supervised methods in order to leverage unlabeleddata, since, to our knowledge, there are no annotateddata sets that include both preposition and argumentsenses.
In this paper, we present our unsupervisedframework and show results for preposition disam-biguation.
We hope to present results for the jointdisambiguation of preposition and arguments in afuture paper.The results from this work can be incorporatedinto a number of NLP problems, such as seman-tic tagging, which tries to assign not only syntac-tic, but also semantic categories to unlabeled text.Knowledge about semantic constraints of preposi-tional constructions would not only provide betterlabel accuracy, but also aid in resolving preposi-tional attachment problems.
Learning by Readingapproaches (Mulkar-Mehta et al, 2010) also cru-cially depend on unsupervised techniques as theones described here for textual enrichment.Our contributions are:?
we present the first unsupervised prepositionsense disambiguation (PSD) system323?
we compare the effectiveness of various modelsand unsupervised training methods?
we present ways to extend this work to prepo-sitional arguments2 PreliminariesA preposition p acts as a link between two words, hand o.
The head word h (a noun, adjective, or verb)governs the preposition.
In our example above, thehead word is shopped.
The object of the preposi-tional phrase (usually a noun) is denoted o, in ourexample morning and Rome.
We will refer to h ando collectively as the prepositional arguments.
Thetriple h, p, o forms a syntactically and semanticallyconstrained structure.
This structure is reflected independency parses as a common construction.
Inour example sentence above, the respective struc-tures would be shopped in morning and shopped inRome.
The senses of each element are denoted by abarred letter, i.e., p?
denotes the preposition sense, h?denotes the sense of the head word, and o?
the senseof the object.3 DataWe use the data set for the SemEval 2007 PSDtask, which consists of a training (16k) and a testset (8k) of sentences with sense-annotated preposi-tions following the sense inventory of The Preposi-tion Project, TPP (Litkowski and Hargraves, 2005).It defines senses for each of the 34 most frequentprepositions.
There are on average 9.76 senses perpreposition.
This corpus was chosen as a startingpoint for our study since it allows a comparison withthe original SemEval task.
We plan to use largeramounts of additional training data.We used an in-house dependency parser to extractthe prepositional constructions from the data (e.g.,?shop/VB in/IN Rome/NNP?).
Pronouns and num-bers are collapsed into ?PRO?
and ?NUM?, respec-tively.In order to constrain the argument senses, we con-struct a dictionary that lists for each word all thepossible lexicographer senses according to Word-Net.
The set of lexicographer senses (45) is a higherlevel abstraction which is sufficiently coarse to allowfor a good generalization.
Unknown words are as-sumed to have all possible senses applicable to theirrespective word class (i.e.
all noun senses for wordslabeled as nouns, etc).4 Graphical Modelph op?h?
o?h op?h?
o?h op?h?
o?a)b)c)Figure 1: Graphical Models.
a) 1st order HMM.
b)variant used in experiments (one model/preposition,thus no conditioning on p).
c) incorporates furtherconstraints on variablesAs shown by Hovy et al (2010), prepositionsenses can be accurately disambiguated using onlythe head word and object of the PP.
We exploit thisproperty of prepositional constructions to representthe constraints between h, p, and o in a graphicalmodel.
We define a good model as one that reason-ably constrains the choices, but is still tractable interms of the number of parameters being estimated.As a starting point, we choose the standard first-order Hidden Markov Model as depicted in Figure1a.
Since we train a separate model for each preposi-tion, we can omit all arcs to p. This results in model1b.
The joint distribution over the network can thusbe written asPp(h, o, h?, p?, o?)
= P (h?)
?
P (h|h?)
?
(1)P (p?|h?)
?
P (o?|p?)
?
P (o|o?
)We want to incorporate as much information aspossible into the model to constrain the choices.
InFigure 1c, we condition p?
on both h?
and o?, to reflectthe fact that prepositions act as links and determine324their sense mainly through context.
In order to con-strain the object sense o?, we condition on h?, similarto a second-order HMM.
The actual object o is con-ditioned on both p?
and o?.
The joint distribution isequal toPp(h, o, h?, p?, o?)
= P (h?)
?
P (h|h?)
?
(2)P (o?|h?)
?
P (p?|h?, o?)
?
P (o|o?, p?
)Though we would like to also condition the prepo-sition sense p?
on the head word h (i.e., an arc be-tween them in 1c) in order to capture idioms andfixed phrases, this would increase the number of pa-rameters prohibitively.5 TrainingThe training method largely determines how well theresulting model explains the data.
Ideally, the sensedistribution found by the model matches the realone.
Since most linguistic distributions are Zipfian,we want a training method that encourages sparsityin the model.We briefly introduce different unsupervised train-ing methods and discuss their respective advantagesand disadvantages.
Unless specified otherwise, weinitialized all models uniformly, and trained until theperplexity rate stopped increasing or a predefinednumber of iterations was reached.
Note that MAP-EM and Bayesian Inference require tuning of somehyper-parameters on held-out data, and are thus notfully unsupervised.5.1 EMWe use the EM algorithm (Dempster et al, 1977) asa baseline.
It is relatively easy to implement with ex-isting toolkits like Carmel (Graehl, 1997).
However,EM has a tendency to assume equal importance foreach parameter.
It thus prefers ?general?
solutions,assigning part of the probability mass to unlikelystates (Johnson, 2007).
We ran EM on each modelfor 100 iterations, or until the perplexity stopped de-creasing below a threshold of 10?6.5.2 EM with Smoothing and RestartsIn addition to the baseline, we ran 100 restarts withrandom initialization and smoothed the fractionalcounts by adding 0.1 before normalizing (Eisner,2002).
Smoothing helps to prevent overfitting.
Re-peated random restarts help escape unfavorable ini-tializations that lead to local maxima.
Carmel pro-vides options for both smoothing and restarts.5.3 MAP-EM with L0 NormSince we want to encourage sparsity in our mod-els, we use the MDL-inspired technique intro-duced by Vaswani et al (2010).
Here, the goalis to increase the data likelihood while keepingthe number of parameters small.
The authors usea smoothed L0 prior, which encourages probabil-ities to go down to 0.
The prior involves hyper-parameters ?, which rewards sparsity, and ?, whichcontrols how close the approximation is to the trueL0 norm.2 We perform a grid search to tune thehyper-parameters of the smoothed L0 prior for ac-curacy on the preposition against, since it has amedium number of senses and instances.
For HMM,we set ?trans =100.0, ?trans =0.005, ?emit =1.0,?emit =0.75.
The subscripts trans and emit de-note the transition and emission parameters.
Forour model, we set ?trans =70.0, ?trans =0.05,?emit =110.0, ?emit =0.0025.
The latter resultedin the best accuracy we achieved.5.4 Bayesian InferenceInstead of EM, we can use Bayesian inference withGibbs sampling and Dirichlet priors (also known asthe Chinese Restaurant Process, CRP).
We followthe approach of Chiang et al (2010), running Gibbssampling for 10,000 iterations, with a burn-in pe-riod of 5,000, and carry out automatic run selec-tion over 10 random restarts.3 Again, we tuned thehyper-parameters of our Dirichlet priors for accu-racy via a grid search over the model for the prepo-sition against.
For both models, we set the concen-tration parameter ?trans to 0.001, and ?emit to 0.1.This encourages sparsity in the model and allows fora more nuanced explanation of the data by shiftingprobability mass to the few prominent classes.2For more details, the reader is referred to Vaswani et al(2010).3Due to time and space constraints, we did not run the 1000restarts used in Chiang et al (2010).325result tablePage 1HMM0.40 (0.40)0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53)0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49)baseline Vanilla EMEM, smoothed,100 randomrestartsMAP-EM +smoothed L0normCRP, 10 randomrestartsour modelTable 1: Accuracy over all prepositions w. different models and training.
Best accuracy: MAP-EM+smoothed L0 norm on our model.
Italics denote significant improvement over baseline at p <.001.Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters)6 ResultsGiven a sequence h, p, o, we want to find the se-quence of senses h?, p?, o?
that maximizes the jointprobability.
Since unsupervised methods use theprovided labels indiscriminately, we have to map theresulting predictions to the gold labels.
The pre-dicted label sequence h?, p?, o?
generated by the modelvia Viterbi decoding can then be compared to thetrue key.
We use many-to-1 mapping as describedby Johnson (2007) and used in other unsupervisedtasks (Berg-Kirkpatrick et al, 2010), where eachpredicted sense is mapped to the gold label it mostfrequently occurs with in the test data.
Success ismeasured by the percentage of accurate predictions.Here, we only evaluate p?.The results presented in Table 1 were obtainedon the SemEval test set.
We report results bothwith and without against, since we tuned the hyper-parameters of two training methods on this preposi-tion.
To test for significance, we use a two-tailedt-test, comparing the number of correctly labeledprepositions.
As a baseline, we simply label all wordtypes with the same sense, i.e., each preposition to-ken is labeled with its respective name.
When usingmany-to-1 accuracy, this technique is equivalent to amost-frequent-sense baseline.Vanilla EM does not improve significantly overthe baseline with either model, all other methodsdo.
Adding smoothing and random restarts increasesthe gain considerably, illustrating how importantthese techniques are for unsupervised training.
Wenote that EM performs better with the less complexHMM.CRP is somewhat surprisingly roughly equivalentto EM with smoothing and random restarts.
Accu-racy might improve with more restarts.MAP-EM with L0 normalization produces thebest result (56%), significantly outperforming thebaseline at p < .001.
With more parameters (9.7kvs.
3.7k), which allow for a better modeling ofthe data, L0 normalization helps by zeroing out in-frequent ones.
However, the difference betweenour complex model and the best HMM (EM withsmoothing and random restarts, 55%) is not signifi-cant.The best (supervised) system in the SemEval task(Ye and Baldwin, 2007) reached 69% accuracy.
Thebest current supervised system we are aware of(Hovy et al, 2010) reaches 84.8%.7 Related WorkThe semantics of prepositions were topic of a specialissue of Computational Linguistics (Baldwin et al,2009).
Preposition sense disambiguation was one ofthe SemEval 2007 tasks (Litkowski and Hargraves,2007), and was subsequently explored in a numberof papers using supervised approaches: O?Hara andWiebe (2009) present a supervised preposition sensedisambiguation approach which explores differentsettings; Tratz and Hovy (2009), Hovy et al (2010)make explicit use of the arguments for prepositionsense disambiguation, using various features.
Wediffer from these approaches by using unsupervisedmethods and including argument labeling.The constraints of prepositional constructionshave been explored by Rudzicz and Mokhov (2003)and O?Hara and Wiebe (2003) to annotate the se-mantic role of complete PPs with FrameNet andPenn Treebank categories.
Ye and Baldwin (2006)explore the constraints of prepositional phrases for326semantic role labeling.
We plan to use the con-straints for argument disambiguation.8 Conclusion and Future WorkWe evaluate the influence of two different models (torepresent constraints) and three unsupervised train-ing methods (to achieve sparse sense distributions)on PSD.
Using MAP-EM with L0 norm on ourmodel, we achieve an accuracy of 56%.
This is asignificant improvement (at p <.001) over the base-line and vanilla EM.
We hope to shorten the gap tosupervised systems with more unlabeled data.
Wealso plan on training our models with EM with fea-tures (Berg-Kirkpatrick et al, 2010).The advantage of our approach is that the modelscan be used to infer the senses of the prepositionalarguments as well as the preposition.
We are cur-rently annotating the data to produce a test set withAmazon?s Mechanical Turk, in order to measure la-bel accuracy for the preposition arguments.AcknowledgementsWe would like to thank Steve DeNeefe, JonathanGraehl, Victoria Fossum, and Kevin Knight, as wellas the anonymous reviewers for helpful commentson how to improve the paper.
We would also liketo thank Morgan from Curious Palate for letting uswrite there.
Research supported in part by Air ForceContract FA8750-09-C-0172 under the DARPA Ma-chine Reading Program and by DARPA under con-tract DOI-NBC N10AP20031.ReferencesTim Baldwin, Valia Kordoni, and Aline Villavicencio.2009.
Prepositions in applications: A survey and in-troduction to the special issue.
Computational Lin-guistics, 35(2):119?149.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless Unsu-pervised Learning with Features.
In North AmericanChapter of the Association for Computational Linguis-tics.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Annual Meeting ?
AssociationFor Computational Linguistics, volume 45, pages 33?40.David Chiang, Jonathan Graehl, Kevin Knight, AdamPauls, and Sujith Ravi.
2010.
Bayesian inferencefor Finite-State transducers.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 447?455.
Association forComputational Linguistics.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety.
Series B (Methodological), 39(1):1?38.Jason Eisner.
2002.
An interactive spreadsheet for teach-ing the forward-backward algorithm.
In Proceed-ings of the ACL-02 Workshop on Effective tools andmethodologies for teaching natural language process-ing and computational linguistics-Volume 1, pages 10?18.
Association for Computational Linguistics.Christiane Fellbaum.
1998.
WordNet: an electronic lexi-cal database.
MIT Press USA.Jonathan Graehl.
1997.
Carmel Finite-state Toolkit.ISI/USC.Dirk Hovy, Stephen Tratz, and Eduard Hovy.
2010.What?s in a Preposition?
Dimensions of Sense Dis-ambiguation for an Interesting Word Class.
In Coling2010: Posters, pages 454?462, Beijing, China, Au-gust.
Coling 2010 Organizing Committee.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 296?305.Ken Litkowski and Orin Hargraves.
2005.
The prepo-sition project.
ACL-SIGSEM Workshop on ?The Lin-guistic Dimensions of Prepositions and Their Use inComputational Linguistic Formalisms and Applica-tions?, pages 171?179.Ken Litkowski and Orin Hargraves.
2007.
SemEval-2007 Task 06: Word-Sense Disambiguation of Prepo-sitions.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, EduardHovy, Bernardo Magnini, and Christopher Manning,editors.
2010.
Proceedings of the NAACL HLT2010 First International Workshop on Formalisms andMethodology for Learning by Reading.
Associationfor Computational Linguistics, Los Angeles, Califor-nia, June.Tom O?Hara and Janyce Wiebe.
2003.
Preposi-tion semantic classification via Penn Treebank andFrameNet.
In Proceedings of CoNLL, pages 79?86.Tom O?Hara and Janyce Wiebe.
2009.
Exploiting se-mantic role resources for preposition disambiguation.Computational Linguistics, 35(2):151?184.327Frank Rudzicz and Serguei A. Mokhov.
2003.
Towardsa heuristic categorization of prepositional phrases inenglish with wordnet.
Technical report, CornellUniversity, arxiv1.library.cornell.edu/abs/1002.1095-?context=cs.Stephen Tratz and Dirk Hovy.
2009.
Disambiguation ofPreposition Sense Using Linguistically Motivated Fea-tures.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, Companion Volume: Student Research Work-shop and Doctoral Consortium, pages 96?100, Boul-der, Colorado, June.
Association for ComputationalLinguistics.Ashish Vaswani, Adam Pauls, and David Chiang.
2010.Efficient optimization of an MDL-inspired objectivefunction for unsupervised part-of-speech tagging.
InProceedings of the ACL 2010 Conference Short Pa-pers, pages 209?214.
Association for ComputationalLinguistics.Patrick Ye and Tim Baldwin.
2006.
Semantic role la-beling of prepositional phrases.
ACM Transactionson Asian Language Information Processing (TALIP),5(3):228?244.Patrick Ye and Timothy Baldwin.
2007.
MELB-YB:Preposition Sense Disambiguation Using Rich Seman-tic Features.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval-2007),Prague, Czech Republic.328
