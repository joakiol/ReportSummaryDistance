Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 55?59,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPredicting Attrition Along the Way: The UIUC ModelBussaba Amnueypornsakul, Suma Bhat and Phakpoom ChinprutthiwongUniversity of Illinois,Urbana-Champaign, USA{amnueyp1,spbhat2,chinpru2}@illinois.eduAbstractDiscussion forum and clickstream are two primarydata streams that enable mining of student behav-ior in a massively open online course.
A student?sparticipation in the discussion forum gives directaccess to the opinions and concerns of the student.However, the low participation (5-10%) in discus-sion forums, prompts the modeling of user behav-ior based on clickstream information.
Here westudy a predictive model for learner attrition on agiven week using information mined just from theclickstream.
Features that are related to the quizattempt/submission and those that capture inter-action with various course components are foundto be reasonable predictors of attrition in a givenweek.1 IntroductionAs an emerging area that promises new hori-zons in the landscape resulting from the mergerof technology and pedagogy massively open on-line courses (MOOCs) offer unprecedented av-enues for analyzing many aspects of learning atscales not imagine before.
The concept though inits incipient stages offers a fertile ground for an-alyzing learner characteristics that span demogra-phies, learning styles, and motivating factors.
Atthe same time, their asynchronous and impersonalapproach to learning and teaching, gives rise toseveral challenges, one of which is student reten-tion.In the absence of a personal communication be-tween the teacher and the student in such a sce-nario, it becomes imperative to be able to under-stand class dynamics based on the course logs thatare available.
This serves the efforts of the in-structor to better attend to the needs of the classat large.
One such analysis is to be able to predictif a student will drop out or continue his/her par-ticipation in the course which is the shared task ofthe EMNLP 2014 Workshop on Modeling LargeScale Social Interaction in Massively Open OnlineCourses (Rose and Siemens, 2014).Our approach is to model student attrition as be-ing a function of interaction with various coursecomponents.2 Related WorksThe task of predicting student behavior has beenthe topic of several recent studies.
In this contextcourse logs have been analyzed with an effort topredict students?
behavior.
The available studiescan be classified based on the type of course datathat has been used for the analysis as those us-ing discussion forum data and those using click-stream data.Studies using only discussion forum to under-stand user-behavior rely only on available discus-sion forum posts as their source of information.
Inthis context, in (Ros?e et al., 2014) it was observedthat students?
forum activity in the first week canreasonably predict the likelihood of users drop-ping out.
Taking a sentiment analysis approach,Wen et al.
(Wen et al., 2014b) observed a corre-lation between user sentiments expressed via fo-rum posts and their chance of dropping out.
Mo-tivation being a crucial aspect for a successful on-line learning experience, (Wen et al., 2014a) em-ploys computational linguistic models to measurelearner motivation and cognitive engagement fromthe text of forum posts and observe that participa-tion in discussion forums is a strong indicator ofstudent commitment.Even though discussion forum serves as a richsource of information that offers insights intomany aspects of student behavior, it has been ob-served that a very small percentage of students(5-10%) actually participate in the discussion fo-rum.
As an alternate data trace of student inter-action with the course material, the clickstream55data of users contains a wider range of informa-tion affording other perspectives of student behav-ior.
This is the theme of studies such as (Guoand Reinecke, 2014), which is focused on the nav-igation behavior of various demographic groups,(Kizilcec et al., 2013) which seeks to understandhow students engage with the course, (Rameshet al., 2014), that attempts to understand studentdisengagement and their learning patterns towardsminimizing dropout rate and (Stephens-Martinezet al., 2014) which seeks to model motivations ofusers by mining clickstream data.In this study, the task is to predict if a userwill stay in the course or drop out using infor-mation from forum posts and clickstream infor-mation.
Our approach is to use only clickstreaminformation and is motivated by key insights suchas interaction with the various course componentsand quiz attempt/submission.3 DataData from one MOOC with approximately 30Kstudents was distributed as training data.
Thisincluded discussion post information and click-stream information of the students with com-pletely anonymized user ids.
Of this a subset of6583 users was considered the held-out dataset onwhich we report the performance of the model.3.1 Preprocessing StageSince participants (posters) in the discussion fo-rum constitute a very small minority of the usersin a course (between 5-10% as observed in priorstudies), we mine the clickstream information forcourse-interaction.
From the clickstream we ex-tract the following information to indicate involve-ment in the course.?
Total watch time: From the video view infor-mation the amount of time watched is calcu-lated by taking the summation of the differ-ence between the time of the last event a userinteracts with a video and the initial time auser starts the same video.
If a user is idlefor longer than 50 minutes, we add the differ-ence between the current time before the usergoes idle and the time the user initially inter-acts with the video to the total time.
The newinitial time will be after the user goes activeagain.
Then we repeat the process until thereis no more viewing action in the clickstreamfor that user.?
Number of quiz attempts;?
Number of quiz submissions;?
Number of times a user visits the discussionforum;?
Number of times a user posts: The numberof times a user posts in a forum is counted.This count includes whether the user starts athread, posts, or comments.?
Action sequence: We define an action se-quence of a given user as being the sequenceof course-related activity in a given week fora given user.
It captures the user?s interac-tion with the various components of a coursein chronological order, such as seeking infor-mation on the course-wiki, watching a lecturevideo, posting in the discussion forum.
Theactivities are, p = forum post, a = quiz at-tempt, s = quiz submit, l = lecture page view,d = lecture download, f = forum view, w =wiki page visited, t = learning tool page vis-ited, o = play video.
As an example, the ac-tion sequence of a user wwaaws in a givenweek indicates that the user began the course-activity with a visit to the course wiki, fol-lowed by another visit to the wiki, then at-tempted the quiz two successive times and fi-nally submitted the quiz.Each of the items listed above, captures impor-tant aspects of interaction with the course serv-ing as an index of attrition; the more a user inter-acts with the course in a given week, the less thechances are of dropping out in that week.21%?32%?47%?
Drop?Inac?ve?Ac?ve?Figure 1: Percentage of each type of users56An exploratory analysis of the data reveals thatthere are three classes of users based on their in-teraction with the course components as revealedby the clickstream activity.
More specifically, withrespect to the length of their action sequence, the3 classes are:1.
Active: This class is the majority class rep-resented by 47% of the users in the course.The users actively interact with more thanone component of the course and their enroll-ment status shows that they did not drop.2.
Drop: This is the class represented by a rel-ative minority of the users (21%).
The usershardly interact with the course and from theirenrollment status they have dropped.3.
Inactive: This class of students, representedby 32% of the course, shares commonalitieswith the first two classes.
Whereas their en-rollment status indicates that they have notdropped (similar to the Active group), theirclickstream information shows that their levelof course activity is similar to that of theDrop class (as evidenced by the length oftheir action sequence.
We define a user to beinactive if the action sequence is less than 2and the user is still enrolled in the course.The distribution of the three classes of users in thetraining data is shown in Figure 1.
This key ob-servation of the presence of three classes of usersprompts us to consider three models to predict userattrition on any given week since we only predictwhether a user dropped or not.1.
Mode 1 (Mod1): Inactive users are modeledto be users that dropped because of their sim-ilar activity pattern;2.
Mode 2 (Mod2): Inactive users are modeledas Active users because they did not formallydrop out;3.
Mode 3 (Mod3): Inactive users are modeledas Drop with a probability of 0.5 and Activewith a probability of 0.5.
This is because theyshare status attributes with Active and inter-action attributes with Drop.4 FeaturesWe use two classes of features to represent user-behavior in a course and summarize them as fol-lows.?
Quiz related: The features in this class are:whether a user submitted the quiz (binary),whether a user attempted the quiz (binary),whether a user attempted but did not submitthe quiz (binary).
The intuition behind thisset of features is that in general quiz-relatedactivity denotes a more committed studentwith a higher level of involvement with thecourse.
This set is also intended to capturethree levels of commitment, ranging fromonly an attempt at the lowest level, attempt-ing but not submitting at a medium level, tosubmitting the quiz being the highest level.?
Activity related: The features in this categoryare derived from the action sequence of theuser during that week and they are:1.
Length of the action sequence (nu-meric);2.
The number of times each activity (p, a,s, l, d, f, w, o, or t) occurred (numeric);3.
The number of wiki page visits/length ofthe action sequence (numeric).The features essentially capture the degree ofinvolvement as a whole and the extent of in-teraction with each component.5 Experiments5.1 ModelsWe consider two input data distributions of thetraining data: a) a specific case, where the inac-tive users are excluded.
In this case, the model istrained only on users that are either active or thosethat have dropped.
b) a general case, where theinactive users are included as is.
In both cases, thetesting data has the inactive users included, but areeither modeled as Mode 1, 2 or 3.
This results in6 models {specific, general} x {Mode1, Mode2,Mode3}.We train an SVM for each model and ob-serve that an rbf kernel achieves the best accuracyamong the kernel choices.
We use the scikit imple-mentation of SVM (Pedregosa et al., 2011).
Theparameter ?
was tuned to maximize accuracy via 5fold cross validation on the entire training set.
Weobserve that the performance of Mode 3 was muchlower than that of Modes 1 and 2 and thus excludeit from the results.The tuned models were finally evaluated for ac-curacy, precision, recall, F-measure and Cohen?s57?
on the held-out dataset.5.2 Experimental ResultsMode 1 Mode 2Specific General Specific GeneralBaseline 46.42% 46.42% 78.66% 78.66%Accuracy 91.31% 85.34% 78.48% 78.56%Table 1: Accuracy of the models after parametertuning.We compare the accuracy of the tuned mod-els with a simple baseline which classifies a user,who, during a given week, submits the quiz andhas an action sequence length more than 1 asone who will not drop.
The baseline accuracy is46.42% for Mode 1 and 78.66% for Mode 2.
Weobserve that modeling the inactive user as one whodrops performs significantly better than the base-line, whereas modeling the inactive user as onewho stays, does not improve the baseline.
Thisis summarized in Table 1.Of these models we chose two of the best per-forming models and evaluate them on the held-out data.
The chosen models were: Model 1 =(specific,Mode1) and Model 2 = (general,Mode2).The resulting tuned Model 1 (inactive = drop) had?
= 0.1 and Model 2 (inactive = stay) had a?
= 0.3 and C as the default value.Model 1 Model 2Accuracy 50.98% 80.40%Cohen?s Kappa -0.06 0.065P 0.167 0.482R 0.371 0.058F 0.228 0.104Table 2: Accuracy, Cohen?s kappa, Precision (P),Recall (R) and F-measure (F) scores for the mod-els on the held-out data.The performance (accuracy, Cohen?s ?, preci-sion, recall and F-measure scores of the two mod-els on the held-out data are shown in Table 2.
Thefinal model submitted for evaluation on the test setis Model 2.
It was more general since its trainingdata included the inactive users as well.
However,the skew in the data distribution is even larger forthis model.We highlight some important observationsbased on the result.?
Model 2, which is trained to be more gen-eral and has the inactive users included, butoperates in Mode 2 (regards inactive usersas active) has a better accuracy compared toModel 1, which is trained by excluding theinactive users, but operates in Mode 1 (re-gards inactive users as drop).?
In terms of the ?
score, Model 2 shows someagreement, but Model 1 shows no agreement.?
The increased accuracy of Model 2 comes atthe expense of reduced recall.
This suggeststhat Model 2 has more false negatives com-pared to Model 1 on the held-out set.?
Even with reduced recall, Model 2 is moreprecise than Model 1.
This implies thatModel 1 tends to infer a larger fraction offalse positives compared to Model 2.6 Discussion6.1 Data ImbalanceThe impact of class imbalance on the SVM clas-sifier is well-known to result in the majority classbeing well represented compared to the minorityclass (Longadge and Dongre, 2013).
In our mod-eling with different input data distributions as inthe specific case (Model 1), where we exclude in-active users, the data imbalance could have signif-icantly affected the performance.
This is because,the class of active users is more than double thesize of the class of users who dropped.Our attempt to counter the effect of the minor-ity class by oversampling, resulted in no improve-ment in performance.
In future explorations, otherefforts to counter the data imbalance may be help-ful.6.2 Parameter tuningThe models studied here were tuned to maximizeaccuracy.
In the future, models that are tuned tomaximize Cohen?s ?
may be worth exploring.6.3 Ablation AnalysisQuiz Related Activity RelatedModel 1 80.48% 50.95%Model 2 80.48% 80.41%Table 3: Accuracy and kappa scores for the mod-els by removing the corresponding set of features.Table 3 summarizes the results of the ablationstudy conducted for each model by removing eachclass of features.
For Model 1, the activity-relatedfeatures constitute the most important set of fea-tures as seen by the drop in accuracy resultingfrom its omission.
For Model 2, however, bothsets of features have nearly the same effect.58ReferencesPhilip J. Guo and Katharina Reinecke.
2014.
Demo-graphic differences in how students navigate throughmoocs.
In Proceedings of the First ACM Conferenceon Learning @ Scale Conference, L@S ?14, pages21?30, New York, NY, USA.
ACM.Ren?e F. Kizilcec, Chris Piech, and Emily Schnei-der.
2013.
Deconstructing disengagement: Analyz-ing learner subpopulations in massive open onlinecourses.
In Proceedings of the Third InternationalConference on Learning Analytics and Knowledge,LAK ?13, pages 170?179, New York, NY, USA.ACM.R.
Longadge and S. Dongre.
2013.
Class ImbalanceProblem in Data Mining Review.
ArXiv e-prints,May.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Arti Ramesh, Dan Goldwasser, Bert Huang, HalDaume III, and Lise Getoor.
2014.
Uncovering hid-den engagement patterns for predicting learner per-formance in moocs.
In ACM Conference on Learn-ing at Scale, Annual Conference Series.
ACM, ACMPress.Carolyn Rose and George Siemens.
2014.
Shared taskon prediction of dropout over time in massively openonline courses.
In Proceedings of the 2014 Empiri-cal Methods in Natural Language Processing Work-shop on Modeling Large Scale Social Interaction inMassively Open Online Courses.Carolyn Penstein Ros?e, Ryan Carlson, Diyi Yang,Miaomiao Wen, Lauren Resnick, Pam Goldman,and Jennifer Sherer.
2014.
Social factors that con-tribute to attrition in moocs.
In Proceedings of theFirst ACM Conference on Learning @ Scale Con-ference, L@S ?14, pages 197?198, New York, NY,USA.
ACM.Kristin Stephens-Martinez, Marti A. Hearst, and Ar-mando Fox.
2014.
Monitoring moocs: Which infor-mation sources do instructors value?
In Proceedingsof the First ACM Conference on Learning @ ScaleConference, L@S ?14, pages 79?88, New York, NY,USA.
ACM.Miaomiao Wen, Diyi Yang, and Carolyn PensteinRos?e.
2014a.
Linguistic reflections of studentengagement in massive open online courses.
InICWSM.Miaomiao Wen, Diyi Yang, and Carolyn PensteinRos?e.
2014b.
Sentiment analysis in mooc discus-sion forums: What does it tell us?
In the 7th Inter-national Conference on Educational Data Mining.59
