Back Transliteration from Japanese to EnglishUsing Target English ContextIsao Goto?, Naoto Kato?
?, Terumasa Ehara??
?, and Hideki Tanaka?
?NHK Science and TechnicalResearch Laboratories1-11-10 Kinuta, Setagaya,Tokyo, 157-8510, Japangoto.i-es@nhk.or.jptanaka.h-ja@nhk.or.jp?
?ATR Spoken Language Trans-lation Research Laboratories2-2-2 Hikaridai, KeihannaScience City, Kyoto, 619-0288,Japannaoto.kato@atr.jp??
?Tokyo University ofScience, Suwa5000-1, Toyohira, Chino,Nagano, 391-0292, Japaneharate@rs.suwa.tus.ac.jpAbstractThis paper proposes a method of automaticback transliteration of proper nouns, in whicha Japanese transliterated-word is restored tothe original English word.
The English wordsare created from a sequence of letters; thusour method can create new English words thatare not registered in dictionaries or Englishword lists.
When a katakana character is con-verted into English letters, there are variouscandidates of alphabetic characters.
To ensureadequate conversion, the proposed methoduses a target English context to calculate theprobability of an English character or stringcorresponding to a Japanese katakana charac-ter or string.
We confirmed the effectivenessof using the target English context by an ex-periment of personal-name back translitera-tion.1 IntroductionIn transliteration, a word in one language is con-verted into a character string of another languageexpressing how it is pronounced.
In the case oftransliteration into Japanese, special characterscalled katakana are used to show how a word ispronounced.
For example, a personal name andits transliterated word are shown below.Cunningham         ?????
(ka ni n ga mu)[Transliteration]Here, the italic alphabets are romanized Japanesekatakana characters.New transliterated words such as personalnames or technical terms in katakana are not al-ways listed in dictionaries.
It would be useful forcross-language information retrieval if thesewords could be automatically restored to theoriginal English words.Back transliteration is the process of restoringtransliterated words to the original English words.Here is a problem of back transliteration.?
?????????
(English word) (ku ra cchi fi ?
ru do)[Back transliteration]There are many ambiguities to restoring atransliterated katakana word to its original Eng-lish word.
For example, should "a" in "ku ra cchifi ?
ru do" be converted into the English letter of"a" or "u" or some other letter or string?
Tryingto resolve the ambiguity is a difficult problem,which means that back transliteration to the cor-rect English word is also difficult.Using the pronunciation of a dictionary or lim-iting output English words to a particular Englishword list prepared in advance can simplify theproblem of back transliteration.
However, thesemethods cannot produce a new English word thatis not registered in a dictionary or an Englishword list.
Transliterated words are mainly propernouns and technical terms, and such words areoften not registered.
Thus, a back transliterationframework for creating new words would be veryuseful.A number of back transliteration methods forselecting English words from an English pronun-ciation dictionary have been proposed.
They in-clude Japanese-to-English (Knight and Graehl,1998) 1 , Arabic-to-English (Stalls and Knight,1 Their English letter-to-sound WFST does not convert Eng-lish words that are not registered in a pronunciation diction-ary.1998), and Korean-to-English (Lin and Chen,2002).There are also methods that select Englishwords from an English word list, e.g., Japanese-to-English (Fujii and Ishikawa, 2001) and Chi-nese-to-English (Chen et al, 1998).Moreover, there are back transliteration meth-ods capable of generating new words, there aresome methods for back transliteration from Ko-rean to English (Jeong et al, 1999; Kang andChoi, 2000).These previous works did not take the targetEnglish context into account for calculating theplausibility of matching target characters with thesource characters.This paper presents a method of taking the tar-get English context into account to generate anEnglish word from a Japanese katakana word.Our character-based method can produce newEnglish words that are not listed in the learningcorpus.This paper is organized as follows.
Section 2describes our method.
Section 3 describes theexperimental set-up and results.
Section 4 dis-cusses the performance of our method based onthe experimental results.
Section 5 concludes ourresearch.2 Proposed Method2.1 Advantage of using English contextFirst we explain the difficulty of back translitera-tion without a pronunciation dictionary.
Next, weclarify the reason for the difficulty.
Finally, weclarify the effect using English context in backtransliteration.In back transliteration, an English letter orstring is chosen to correspond to a katakana char-acter or string.
However, this decision is difficult.For example, there are cases that an English letter"u" corresponds to "a" of katakana, and there arecases that the same English letter "u" does notcorrespond to the same "a" of katakana.
"u" inCunningham corresponds to "a" in katakana and"u" in Bush does not correspond to "a" in kata-kana.
It is difficult to resolve this ambiguitywithout the pronunciation registered in a diction-ary.The difference in correspondence mainlycomes from the difference of the letters aroundthe English letter "u."
The correspondence of anEnglish letter or string to a katakana character orstring varies depending on the surrounding char-acters, i.e., on its English context.Thus, our back transliteration method uses thetarget English context to calculate the probabilityof English letters corresponding to a katakanacharacter or string.2.2 Notation and conversion-candidatelatticeWe formulate the word conversion process as aunit conversion process for treating new words.Here, the unit is one or more characters that forma part of characters of the word.A katakana word, K, is expressed by equation2.1 with "^" and "$" added to its start and end,respectively.10 0 1 1...mmk k k k++= =K  (2.1)0 ^k = , 1 $mk + =  (2.2)where jk  is the j-th character in the katakanaword, and m is the number of characters exceptfor "^" and "$" and 10mk +  is a character stringfrom 0k  to 1mk + .We use katakana units constructed of one ormore katakana characters.
We denote a katakanaunit as ku.
For any ku, many English units, eu,could be corresponded as conversion-candidates.The ku's and eu's are generated using a learningcorpus in which bilingual words are separatedinto units and every ku unit is related an eu unit.
{ }EL  denotes the lattice of all eu's correspond-ing to ku's covering a Japanese word.
Every eu isa node of the lattice and each node is connectedwith next nodes.
{ }EL  has a lattice structure start-ing from "^" and ending at "$."
Figure 1 shows anexample of { }EL  corresponding to a katakanaword "?????????
(ki ru shu shu ta in)."
In the figure, each circle represents one eu.A character string linking individual characterunits in the paths 1 2( , ,.., )d qp p p p?
between "^"and "$" in { }EL  becomes a conversion candidate,where q is the number of paths between "^" and"$" in { }EL .We get English word candidates by joining eu'sfrom "^" to "$" in { }EL .
We select a certain path,pd, in { }EL .
The number of character unitscchi?(ki)chicickicykkekhikikiekiikyquichchechouchusscschschush?
(ru) ??
(shu) ??
(shu) ?
(ta) ?
(i) ?(n)tatadtagteterthatitottatuehiijiyyehyimmonmpnnengnghninnnnnentnwt??(tai)lldleleslewllllelluloulurrcrdrergroorourrrrertlusheshususyszchchechouchusscschschushsheshususysztajtayteytitietytye?
?
??
?
??
?
?
?
?
??
?
??
?
??
?
??
?
?^ $Figure 1: Example of lattice { }EL  of conversion candidates units.except for "^" and "$" in pd is expressed as ( )dn p .The character units in pd are numbered from startto end.The English word, E, resulting from the con-version of a katakana word, K, for pd is expressedas follows:10 0 1 1..mmk k k k++= =K( ) 10 0 1 ( ) 1..ddn pn p++= ku = ku ku ku , (2.3)( ) 10 0 1 ( ) 1..ddl pl pe e e e++= =E( ) 10 0 1 ( ) 1..ddn pn p++= eu = eu eu eu , (2.4)0 0 0 0 ^k e= = = =ku eu ,1 ( ) 1 ( ) 1 ( ) 1 $d d dm l p n p n pk e+ + + += = = =ku eu ,  (2.5)where ej is the j-th character in the English word.
( )dl p  is the number of characters except for "^"and "$" in the English word.
( ) 10 dn p +eu  for each pdin { }EL  in equation 2.4 becomes the candidateEnglish word.
( ) 10 dn p +ku  in equation 2.3 shows thesequence of katakana units.2.3 Probability models using target Eng-lish contextTo determine the corresponding English word fora katakana word, the following equation 2.6 mustbe calculated:?
arg max ( | )PEE = E K .
(2.6)Here, E?
represents an output result.To use the English context for calculating thematching of an English unit with a katakana unit,the above equation is transformed into Equation2.7 by using Bayes?
theorem.?
arg max ( ) ( | )P P=EE E K E  (2.7)Equation 2.7 contains a translation model inwhich an English word is a condition and kata-kana is a result.The word in the translation model ( | )P K E  inEquation 2.7 is broken down into character unitsby using equations 2.3 and 2.4.
{}( ) 1 ( ) 10 0( ) 1 ( ) 10 0( ) 1 ( ) 10 0( ) 1 ( ) 10 0( ) 1 ( ) 1 ( ) 10 0 0?
arg max ( )( , , | )arg max ( )( | , , )( | , ) ( | )d dn p n pd dd dn p n pd dd d dn p n pn p n pn p n p n pPPPPP P+ ++ ++ ++ ++ + +=?=???
??
?Eeu kuEeu kuE EK ku eu EEK ku eu Eku eu E eu E(2.8)( ) 10dn p +eu  includes information of E. K is onlyaffected by ( ) 10dn p +ku .
Thus equation 2.8 can berewritten as follows:( ) 1 ( ) 10 0( ) 10( ) 1 ( ) 1 ( ) 10 0 0 .?
argmax ( )( | )( | ) ( | )dn p n pd dd d dn pn p n p n pPPP P+ +++ + +??????=???
?Eeu kuE EK kuku eu eu E(2.9)( ) 10( | )dn pP +K ku  is 1 when the string of K and( ) 10dn p +ku  is the same, and the strings of the( ) 10dn p +ku  of all paths in the lattice and the stringof the K is the same.
Thus, ( ) 10( | )dn pP +K ku  is al-ways 1.We approximate the sum of paths by selectingthe maximum path.
( ) 1 ( ) 10 0( ) 10?
arg max ( ) ( | )( | )d ddn p n pn pP PP+ ++?
?EE E ku eueu E(2.10)We show an instance of each probabilitymodel with a concrete value as follows:( )(^Crutchfield$)PPE ,( ) 10( | )(^ | ^ / )( ) ( / / / / / )dn pPPku ra cchi fi ru do ku ra cchi fi ru do+?
?K ku????????
?$ ?/?/??/??
?/?/?/$ ,( ) 1 ( ) 10 0( | )(^ / | ^ / C/ru/tch/fie/l/d / $)( / / / / / )d dn p n pPPku ra cchi fi ru do+ +?ku eu?/?/??/??
?/?/?/$ ,( ) 10( | )(^ / C/ru/tch/fie/ld / $ | ^Crutchfield$)dn pPP+eu E .We broke down the language model ( )P E  inequation 2.10 into letters.
( ) 111( | )( )dl pjj j ajP e eP+??=?
?E  (2.11)Here, a is a constant.
Equation 2.11 is an (a+1)-gram model of English letters.Next, we approximate the translation model( ) 1 ( ) 10 0( | )d dn p n pP + +ku eu  and the chunking model( ) 10( | )dn pP +eu E .
For this, we use our previouslyproposed approximation technique (Goto et al,2003).
The outline of the technique is shown asfollows.
( ) 1 ( ) 10 0( | )d dn p n pP + +ku eu  is approximated by reduc-ing the condition.
( ) 1 ( ) 10 0( ) 1( ) 110 01( | )( | , )d dddn p n pn pn piiiPP+ +++?== ?ku euku ku eu( ) 1( ) 1( ) ( ) 11( | , , )dn pstart ii start i b i end iiP e e+??
+=?
?
ku eu(2.12)where start(i) is the first position of the i-th char-acter unit eui, while end(i) is the last position ofthe i-th character unit eui; and b is a constant.Equation 2.12 takes English context ( ) 1( )start istart i be??
and( ) 1end ie +  into account.Next, the chunking model ( ) 10( | )dn pP +eu E  istransformed.
All chunking patterns of ( ) 10 dl pe +=Einto ( ) 10 dn p +eu  are denoted by each l(pd)+1 pointbetween l(pd)+2 characters that serve or do notserve as delimiters.
eu0 and ( ) 1dn p +eu  are deter-mined in advance.
l(pd)-1 points remain ambigu-ous.
We represent the value that is delimiter or isnon-delimiter between ej and ej+1 by zj.
We callthe zj delimiter distinction.
{ delimiternon-delimiterjz =  (2.13)Here, we show an example of English units us-ing zj.
(e1 e2 e3 e4  e5  e6 e7 e8 e9  e10 e11)C r u t c h f i e l d(z1  z2  z3  z4  z5  z6 z7  z8  z9  z10)/ / / /1  0  1  0  0  1  0  0  1  1English:Values of zj:/In this example, a delimiter of zj is represented by1 and a non-delimiter is represented by 0.The chunking model is transformed into aprocessing per character by using zj.
And we re-duce the condition.
( ) 10( ) 1 ( ) 10 0( ) 1( ) 110 01( | )( | )( | , )dd dddn pl p l pl pl pjjjPP z eP z z e+?
+?+?=== ?eu E( ) 11 111( | , )dl pj jj j c j cjP z z e??
+?
?
?=?
?
(2.14)The conditional information of the English1jj ce+?
is as many as c characters and 1 characterbefore and after zj, respectively.
The conditionalinformation of 1 1jj cz??
?
is as many as c+1 delimiterdistinctions before zj.By using equation 2.11, 2.12, and 2.14, equa-tion 2.10 becomes as follows:( ) 111( )( ) 1( ) ( ) 11( ) 11 111?
arg max ( | )( | , , )( | , ).dddl pjj j ajn pstart ii start i b i end iil pj jj j c j cjP e eP e eP z z e+??=??
+=??
+?
?
?=?????
?EEku eu(2.15)Equation 2.15 is the equation of our backtransliteration method.2.4 Beam search solution for contextsensitive grammarEquation 2.15 includes context-sensitive gram-mar.
As such, it can not be carried out efficiently.In decoding from the head of a word to the tail,eend(i)+1 in equation 2.15 becomes context-sensitive.
Thus we try to get approximate resultsby using a beam search solution.
To get the re-sults, we use dynamic programming.
Every nodeof eu in the lattice keeps the N-best results evalu-ated by using a letter of eend(i)+1 that gives themaximum probability in the next letters.
Whenthe results of next node are evaluated for select-ing the N-best, the accurate probabilities from theprevious nodes are used.2.5 Learning probability models basedon the maximum entropy methodThe probability models are learned based on themaximum entropy method.
This makes it possi-ble to prevent data sparseness relating to themodel as well as to efficiently utilize many con-ditions, such as context, simultaneously.
We usethe Gaussian Prior (Chen and Rosenfeld, 1999)smoothing method for the language model.
Weuse one Gaussian variance.
We use the value ofthe Gaussian variance that minimizes the testset's perplexity.The feature functions of the models based onthe maximum entropy method are defined ascombinations of letters.
In addition, we usevowel, consonant, and semi-vowel classes for thetranslation model.
We manually define the com-binations of the letter positions such as ej and ej-1.The feature functions consist of the letter combi-nations that meet the combinations of the letterpositions and are observed at least once in thelearning data.2.6 Corpus for learningA Japanese-English word list aligned by unit wasused for learning the translation model and thechunking model and for generating the lattice ofconversion candidates.
The alignment was doneby semi-automatically.
A romanized katakanacharacter usually corresponds to one or severalEnglish letters or strings.
For example, a roman-ized katakana character "k" usually correspondsto an English letter "c," "k," "ch," or "q."
Withsuch heuristic rules, the Japanese-English wordcorpus could be aligned by unit and the align-ment errors were corrected manually.3 Experiment3.1 Learning data and test dataWe conducted an experiment on back translitera-tion using English personal names.
The learningdata used in the experiment are described below.The Dictionary of Western Names of 80,000People2 was used as the source of the Japanese-English word corpus.
We chose the names in al-phabet from A to Z and their corresponding kata-kana.
The number of distinct words was 39,830for English words and 39,562 for katakana words.The number of English-katakana pairs was83,0573.
We related the alphabet and katakanacharacter units in those words by using themethod described in section 2.6.
We then usedthe corpus to make the translation and the chunk-ing models and to generate a lattice of conversioncandidates.The learning of the language model was car-ried out using a word list that was created bymerging two word lists: an American personal-2 Published by Nichigai Associates in Japan in 1994.3  This corpus includes many identical English-katakanaword pairs.name list4, and English head words of the Dic-tionary of Western Names of 80,000 people.
TheAmerican name list contains frequency informa-tion for each name; we also used the frequencydata for the learning of the language model.
Atest set for evaluating the value of the Gaussianvariance was created using the American namelist.
The list was split 9:1, and we used the largerdata for learning and the smaller data for evaluat-ing the parameter value.The test data is as follows.
The test data con-tained 333 katakana name words of AmericanCabinet officials, and other high-ranking officials,as well as high-ranking governmental officials ofCanada, the United Kingdom, Australia, andNew Zealand (listed in the World Yearbook 2002published by Kyodo News in Japan).
The Englishname words that were listed along with the corre-sponding katakana names were used as answerwords.
Words that included characters other thanthe letters A to Z were excluded from the testdata.
Family names and First names were notdistinguished.3.2 Experimental modelsWe used the following methods to test the indi-vidual effects of each factor of our method.?
Method AUsed a model that did not take English contextinto account.
The plausibility is expressed as fol-lows:( )1?
arg max ( | )dn pi iiP== ?EE eu ku .
(3.1)?
Method BUsed our language model and a translation modelthat did not consider English context.
The con-stant a = 3 in the language model.
The plausibil-ity is expressed as follows:( ) 1 ( )131 1?
arg max ( | ) ( | )d dl p n pjj j i ij iP e e P+?
?= == ?
?EE ku eu .(3.2)?
Method CApplied our chunking model to method B, with c= 3 in the chunking model.
The plausibility isexpressed as follows:4  Prepared from the 1990 Census conducted by the U.S.Department of Commerce.
Available athttp://www.census.gov/genealogy/names/ .
The list includes91,910 distinct words.
( ) 1 ( )131 1?
arg max ( | ) ( | )d dl p n pjj j i ij iP e e P+?
?= == ?
?EE ku eu( ) 11 44 31( | , ).dl pj jj j jjP z z e??
+?
?=?
?
(3.3)?
Method DUsed our translation model that considered Eng-lish context, but not the chunking model.
b = 3 inthe translation model.
The plausibility is ex-pressed as follows:( )( ) 1131( )( ) 1( ) 3 ( ) 11?
arg max ( | )| , , .ddl pjj jjn pstart ii start i i end iiP e eP e e+??=??
+==??
?EEku eu(3.4)?
Method EUsed our language model, translation model, andchunking model.
The plausibility is expressed asfollows:( )( ) 1131( )( ) 1( ) 3 ( ) 11( ) 11 44 31?
arg max ( | )| , ,( | , ).dddl pjj jjn pstart ii start i i end iil pj jj j jjP e eP e eP z z e+??=??
+=??
+?
?==????
?EEku eu(3.5)3.3 ResultsTable 1 shows the results of the experiment5 onback transliteration from Japanese katakana toEnglish.
The conversion was determined to besuccessful if the generated English word agreedperfectly with the English word in the test data.Table 2 shows examples of back transliteratedwords.Method A B C D ETop 1 23.7 57.4 61.6 63.1 66.4Top 2 34.8 69.1 72.4 71.8 74.2Top 3 42.9 73.6 76.6 75.4 79.3Top 5 54.1 77.5 79.9 80.8 83.5Top 10 63.4 82.0 85.3 86.5 87.7Table 1: Ratio (%) of including the answer wordin high-ranking words.5 For model D and E, we used N=50 for the beam searchsolution.
In addition, we kept paths that represented parts ofwords existing in the learning data.Japanese katakana(romanized katakana)Created English???????
(a shu ku ro fu to)Ashcroft?????????
(ki ru shu shu ta i n)Kirschstein?????
(su pe n sa -)Spencer????
(pa u e ru)Powell?????
(pu ri n shi pi)PrincipiTable 2: Example of English words produced.4 DiscussionThe correct-match ratio of the method E for thefirst-ranked words was 66%.
Its correct-matchratio for words up to the 10th rank was 87%.Regarding the top 1 ranked words, method Bthat used a language model increase the ratio 33-points from method A that did not use a languagemodel.
This demonstrates the effectiveness of thelanguage model.Also for the top 1 ranked words, method Cwhich adopted the chunking model increase theratio 4-points from method B that did not adoptthe chunking model in the top 1 ranked words.This indicates the effectiveness of the chunkingmodel.Method D that used a translation model takingEnglish context into account had a ratio 5-pointshigher in top 1 ranked words than that of methodB that used a translation model not taking Eng-lish context into account.
This demonstrates theeffectiveness of the language model.Method E gave the best ratio.
Its ratio for thetop 1 ranked word was 42-points higher thanmethod A's.These results demonstrate the effectiveness ofusing English context for back transliteration.5 ConclusionThis paper described a method for Japanese toEnglish back transliteration.
Unlike conventionalmethods, our method uses a target English con-text to calculate the plausibility of matching be-tween English and katakana.
Our method cantreat English words that do not exist in learningdata.
We confirmed the effectiveness of ourmethod in an experiment using personal names.We will apply this technique to cross-languageinformation retrieval.ReferencesHsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding,and Shih-Chung Tsai.
1998.
Proper Name Transla-tion in Cross-Language Information Retrieval.
36thAnnual Meeting of the Association for Computa-tional Linguistics and 17th International Conferenceon Computational Linguistics, pp.232-236.Stanley F. Chen, Ronald Rosenfeld.
1999.
A GaussianPrior for Smoothing Maximum Entropy Models.Technical Report CMU-CS-99-108, Carnegie Mel-lon University.Bonnie Glover Stalls and Kevin Knight.
1998.
Trans-lating Names and Technical Terms in Arabic Text.COLING/ACL Workshop on Computational Ap-proaches to Semitic Languages.Isao Goto, Naoto Kato, Noriyoshi Uratani, and Teru-masa Ehara.
2003.
Transliteration ConsideringContext Information based on the Maximum En-tropy Method.
Machine Translation Summit IX,pp.125-132.Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung Lee,and Key-Sun Choi.
1999.
Automatic Identificationand Back-Transliteration of Foreign Words for In-formation Retrieval.
Information Processing andManagement, Vol.35, No.4, pp.523-540.Byung-Ju Kang and Key-Sun Choi.
2000.
AutomaticTransliteration and Back-Transliteration by Deci-sion Tree Learning.
International Conference onLanguage Resources and Evaluation.
pp.1135-1411.Kevin Knight and Jonathan Graehl.
1998.
MachineTransliteration.
Computational Linguistics, Vol.24,No.4, pp.599-612.Wei-Hao Lin and Hsin-Hsi Chen.
2002.
BackwardMachine Transliteration by Learning PhoneticSimilarity.
6th Conference on Natural LanguageLearning, pp.139-145.Atsushi Fujii and Tetsuya Ishikawa.
2001.
Japa-nese/English Cross-Language Information Re-trieval: Exploration of Query Translation andTransliteration.
Computers and the Humanities,Vol.35, No.4, pp.389-420.
