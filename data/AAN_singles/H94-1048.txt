A Maximum Entropy Model for Prepositional Phrase AttachmentAdwait Ratnaparkhi, Jeff  Reynar,* and Salim RoukosIBM Research  D iv is ionThomas  J. Watson  Research  CenterYork town Heights ,  NY  105981.
IntroductionA parser for natural language must often choose between twoor more equally grammatical parses for the same sentence.Often the correct parse can be determined from the lexicalproperties of certain key words or from the context in whichthe sentence occurs.
For example in the sentence,In July, the Environmental Protection Agency imposed agrad-ual ban on virtually all uses of asbestos.the prepositional phrase on virtually all uses of asbestos canattach to either the noun phrase a gradual ban, yielding\[vP imposed \[JvP a gradual ban \[pp on virtually all uses ofasbestos \]\] \],or the verb phrase imposed, yielding\[vP imposed \[uP a gradual ban \]\[iop on virtually all uses offasbestos \]\].For this example, a human annotator's attachment decision,which for our purposes is the "correct" attachment, is to thenoun phrase.
We present in this paper methods for con-structing statistical models for computing the probability ofattachment decisions.
These models could be then integratedinto scoring the probability of an overall parse.
We presentour methods in the context of prepositional phrase (PP) at-tachment.Earlier work \[11 \] on PP-attachment for verb phrases (whetherthe PP attaches to the preceding noun phrase or to the verbphrase) used statistics on co-occurences of two bigrams: themain verb (V) and preposition (P) bigram and the main nounin the object noun phrase (N1) and preposition bigram.
Inthis paper, we explore the use of more features to help inmodeling the distribution of the binary PP-attachment deci-sion.
We also describe a search procedure for selecting a"good" subset of features from a much larger pool of featuresfor PP-attachment.
Obviously, the feature search cannot be* Jeff Reynar, f rom University of  Pennsylvania, worked on this project asa summer  student at I .B.M.guaranteed tobe optimal but appears experimentally to yielda good subset of features as judged by the accuracy rate inmaking the PP-attachment decisons.
These search strategiescan be applied to other attachment decisions.We use data from two treebanks: the IBM-Lancaster Treebankof Computer Manuals and the University of PennsylvaniaWSJ treebank.
We extract he verb phrases which include PPphrases either attached to the verb or to an object noun phrase.Then our model assigns a probability to either of the possibleattachments.
We consider models of the exponential familythat are derived using the Maximum Entropy Principle \[1\].We begin by an overview of ME models, then we describeour feature selection method and a method for constructinga larger pool of features from an exisiting set, and then givesome of our results and conclusions.2.
Maximum Entropy ModelingThe Maximum Entropy model \[1\] produces aprobability dis-tribution for the PP-attachment decision using only informa-tion from the verb phrase in which the attachment occurs.We denote the partially parsed verb phrase, i.e., the verbphrase without the attachment decision, as a history h, andthe conditional probability of an attachment asp(dlh), whered 6 .\[0, 1} and corresponds to a noun or verb attachment(respectively).
The probability model depends on certainfeatures of the whole event (h, d) denoted by fi(h, d).
Anexample of a binary-valued feature function is the indicatorfunction that a particular (V, P)  bigram occured along withthe attachment decision being V, i.e.
fprint,on(h, d) is oneif and only if the main verb of h is "print", the prepositionis "on", and d is "V".
As discussed in \[6\], the ME principleleads to a model for p(dlh ) which maximizes the training datalog-likelihood,a) log p(dlh),h,dwhere ~(h, w) is the empirical distribution of the training set,and where p(dlh ) itself is an exponential model:250p(dlh) =k11 eX'Y'(h'd)i=01 kYI e~'f'(h"0d=0 i=04.
Head Noun of the Object of the Preposition (N2)For example, questions on the history "imposed a gradualban on virtually all uses of asbestos", can only ask about hefollowing four words:At the maximum of the training data log-likelihood, the modelhas the property that its k parameters, namely the At's, satisfyk constraints on the expected values of feature functions,where the ith constraint is,EmA = #.f~imposed ban  on usesThe notion of a "head" word here corresponds loosely to thenotion of a lexical head.
We use a small set of rules, calleda Tree Head Table, to obtain the head word of a constituent\[12\].We allow two types of binary-valued questions:The model expected value is,Emf~ = ~(h)p(d lh ) f i (h ,  d)h,d1.
Questions about the presence of any n-gram (n _< 4)of the four head words, e.g., a bigram maybe {V ==' ' i s '  ' ,  P == ' ' o f '  ' }.
Features comprised solelyof questions on words are denoted as "word" features.and the training data expected value, also called the desiredvalue, is$f ,  = d)f,(h, d)h,dThe values of these k parameters can be obtained by one ofmany iterative algorithms.
For example, one can use the Gen-eralized Iterative Scaling algorithm of Darroch and Ratcliff\[3\].
As one increases the number of features, the achievablemaximum of the training data likelihood increases.
We de-scribe in Section 3 a method for determining a reliable set offeatures.3.
FeaturesFeature functions allow us to use informative characteristicsof the training set in estimating p(dlh).
A feature is definedas follows:-.~(h,d) d~_f ~'1, i f fd=OandVq6 Q~,q(h)= 1O, otherwise.
I.where Q~ is a set of binary-valued questions about h. Werestrict he questions in any Q~ ask only about he followingfour head words:I.
Head Verb (V)2.
Head Noun (N1)3.
Head Preposition (P).
Questions that involve the class membership of a headword.
we use a binary hierarchy of classes derived bymutual information clustering which we describe below.Given a binary class hierarchy, we can associate a bitstring with every word in the vocabulary.
Then, byquerying the value of certain bit positions we can con-stmct binary questions.
For example, we can ask whetherabout a bit position for any of the four head words, e.g.,Bi t  5 of P repos i t ion  == i.
We discuss be-low a richer set of these questions.
Features comprisedsolely of questions about class bits are denoted as "class"features, and features containing questions about bothclass bits and words are denoted as "mixed" features 1.Before discussing, feature selection and construction, wegive a brief overview of the mutual information clusteringof words.Mutual Information Bits Mutual information clustering, asdescribed in \[10\], creates a a class "tree" for a given vocab-ulary.
Initially, we take the C most frequent words (usually1000) and assign each one to its own class.
We then take the(C + 1)st word, assign it to its own class, and merge the pairof classes that minimize the loss of average mutual informa-tion.
This repeats until all the words in the vocabulary havebeen exhausted.
We then take our C classes, and use the samealgorithm to merge classes that minimize the loss of mutualinformation, until one class remains.
If we trace the order inwhich words and classes are merged, we can form a binarytree whose leaves consists of words and whose root is theclass which spans the entire vocabulary.
Consequently, weuniquely identify each word by its path from the root, which1 See Table 7 for examples of  features251can be represented by a string of binary digits.
If a path lengtof a word is less than the maximum depth, we pad the bottorof the path with O's (dummy left branches), so that all wordare represented by an equally long bitstring.
"Class" featurequery the value of bits, and hence examine the path of thword in the mutual information tree.Special Features In addition to the types of features described above, we employ two special features in the MImodel, the: Complement and the Null feature.
The Complement, defined asfcomr,(h,d) dJ {1,0, otherwise.ifffi(h'd)=0'Vfi 6.,%4will fire on a pair (h, d) when no other fi in the model applie,,The Initial feature is simplyclef I'1, i f fd=O fn~zz(h, d) = ~, 0, otherwiseand causes the ME model to match the a pr  i o r  i probabilityof seeing an N-attachment.3.1.
Feature  SearchThe search problem here is to find an optimal set of featuresA4 for use in the ME model.
We begin with a search space 79of putative features, and use a feature ranking criterion whichincrementally selects the features in .A4, and also incremen-tally expands the search space 79.Initially 79 consists of all 1, 2, 3 and 4-gram word features ofthe four headwords that occur in the training histories 2, and4all possible unigram class features 3.
We obtain E (~) = 15k=lword features from each training history, and, assuming eachword is assigned m bits, a total of 2m * 4 unigram classfeatures, e.g., there are 2m features per word: B i t  1 o fVerb  == O, B i t  1 o f  Verb  == 1 .
.
.
.
.B i t  m o f  Verb  == 0, B i t  m o f  Verb  ==IThe feature search then proceeds as follows:1.
Initialize 79 as described above, initialize A,4 to containcomplement and null feature2.
Select the best feature from 79 using Delta-Likelihoodrank3.
Add it to .A42With a certain f requency cut-off, usual ly 3 to 53 Also with a certain f requency cut-off0.850.80.750.70.650.60.550.5'PERFORMANCE: Wall St. Journal' io io .
.
.
.
.
20 1 O0 120 140 160 180 200Figure 1" Performance of Maximum Entropy Model on WallSt.
Journal Data4.
Train Maximum Entropy Model, using features in .A45.
Grow 79 based on last feature selected6.
repeat from (2)If we measure the training entropy and test entropy after theaddition of each feature, the training entropy will monotoni-cally decrease while the test entropy will eventually reach aminimum (due to overtraining).
Test set performance usuallypeaks at the test entropy minimum ( see Fig.
1 & 2 ).Delta-Likelihood At step (2) in the search, we rank all fea-tures in 7 9 by estimating their potential contribution to thelog-likelihood of the training set.
Let q be the conditionalprobability distribution of the model with the features cur-rently in A,4.
Then for each f~ 6 79, we compute, by estimat-ing only ~,  the probability distribution p that results when fiis added to the ME model:p(dlh) =q(dlh)e~J,(h, d)1E q(wlh) e~'J'(h''?)'t,.'
=0We then compute the increase in (log) likelihood with the newmodel:6L, = ~IS(h,  w)lnp(wlh ) - ~e~(h, w)lnq(wlh )h,w h,'wand choose the feature with the highest 6L.
Features redun-dmlt or correlated to those features already in .A.4 will produce25210.90.~0.70.60.5ENTROPY: Wall St. JournalTraining0.4 20 A dO dO .
.
.
.
100 120 140 160 180 200Figure 2: Entropy of Maximum Entropy Model on Wall St.Journal Dataa zero or negligible 6L, and will therefore be outranked bygenuinely informative features.
The chosen feature is addedto M and used in the ME Model.3.2.
Growth of Putative Feature SetAt step (5) in the search we expand the space 7 ~ of putativefeatures based on the feature last selected from 72 for additionto M.  Given an n-gram feature f~ (i.e., of type "word","class" or"mixed") that was last added to M,  we create 2m.4new n + 1-gram features which ask questions about class bitsin addition to the questions asked in fi.
E.g., let fi(h, d)constrain d = 0 and constrain h with the questions v ==' ' imposed '  ' , P == ' ' on '  ' Then, given fi(h,d),the 2m new features generated for just the Head Noun are thefollowing:V == ' ' imposed ' ' ,  P == ~'on ' ' ,B i t  1 fo r  Noun == 0V == ' ' imposed ' ' ,  P == ' 'on ' ' ,B i t  1 fo r  Noun == 1V == ' ' imposed ' ' ,  P ==B i t  m fo r  Noun == 0' 'on' 'V == ' ' imposed ' ' ,  P == ' 'on ' ' ,B i t  m fo r  Noun == 1We construct he remaining 6m features imilarly from theremaining 3head words.
We skip the construction of featuresComputer Manuals Wall St. JournalTraining Events 8264 20801Test Events 943 3097Table 1: Size of Datacontaining questions that are inconsistent or redundant withthose word or class questions in fi.The newly created features are then added to P,  and competefor selection in the next Delta-Likelihood ranking process.This method allows the introduction of complex features onword classes while keeping the search space manageable; "Pgrows linearly with .M.4.
Resu l tsWe applied the Maximum Entropy model to sentences fromtwo corpora, the I.B.M.
Computer Manuals Data, annotated byUniv.
of Lancaster, and the Wall St. Journal Data, annotatedby Univ.
of Penn.
The size of the training sets, test sets, andthe results are shown in Tables 1 & 2.The experiments in Table 2 differ in the following manner:"Words Only" The search space P begins with all possiblen-gram word features with n being 1, 2, 3,or 4;this feature set does not grow during the featuresearch.
"Classes Only" The search space P begins with only un-igram class features, and grows by dynamicallycontructing class n-gram questions as describedearlier.
"Word and Classes" The search space P begins with allpossible n-gram word features and unigram classfeatures, and grows by adding class questions (asdescribed earlier).The results in Table 2 are achieved in the neighborhood ofabout 200 features.
As can be seen in Figure 1, performanceimproves quickly as features are added and improves rathervery slowly after the 60-th feature.
The performance is fairlyclose for the various feature sets when a sufficient number offeatures are added.
We also compared these results to a deci-sion tree grown on the same 4 head-word events.
The sameExperiment Computer Manuals Wall St. JournalWords Only 82.2% 77.7%Classes Only 84.5% 79.1%Words and Classes 84.1% 81.6%Table 2: Performance of ME Model on Test Events253Domain PerformanceComputer Manuals 79.5%Wall St. Journal 77.7%Table 3: Decision Tree Performancemutual intbrmation bits were used for growing the decisiontrees.
Table 3 gives the results on the same training and testdata.
The \]VIE models are slightly better than the decision treemodels.For comparison, we obtained the PP-attachment performancesof 3 treebanking experts on a set of 300 randomly selected testevents from the WSJ corpus.
In the first trial, they were givenonly the four head words to make the attachment decision,and in the next, they were given the headwords along with thesentence in which they occurred.
Figure 3 shows an exampleof the head words test a.
The results of the treebankers andthe performance of the ME model on that same set are shownin Table 5.
We also identified the set of 274 events on whichtreebankers, given the sentence, unanimously agreed.
Wedefined this to be the truth set.
We show in Table 6 theagreement on PP-attachment of the original WSJ treebankparses with this consensus set, the average performance of the3 human experts with head words only, and the ME model.The WSJ treebank indicates the accuracy rate of our trainingdata, the human performance indicates how much informationis in the headwords, and the ME model is still a good 124 the key is N,V,N,N,V, N,N,N,N,V,V,N,V,N,N,N,V,N,Vpercentage points behind.Selection Order Feature(1) Preposition == "of"(2) Bit 2 of Head Noun == 0(3) Preposition is "to"(4) Bit 12 of Head Noun == 1(9) Head Noun == "million", Preposition == "in"(30) Preposition == "to", Bit 8 of Object == 1(47) Preposition == "in", Object == "months"Table 4: Examples of Features Chosen for Wall St. JournalDataAverage Human(head words only) ~ 88.2%Average Human(with whole sentence) 93.2%ME Model 78.0%Table 5: Average Performance of Human & ME Model on300 Events of WSJ Data# Events % WSJ TB Human ME Modelin Consensus Performance Performance Performance274 95.7% 92.5% 80.7%Table 6: Human and ME model performance on consensusset for WSJreport mi l l l ion  in chargesreport mi l l l ion  for quarterref lect ing sett lement of contractscarr ied all but onewere in jur ies among workershad damage to bui ld ingbe damage to someuses var ia t ion  of designc i ted example of d istr ictleads Pepsi in sharetrai ls Pepsi in salesr isk conf l ict  w i th  U.S.r isk conf l ict  over p lanoppose seat ing as delegatesave some of plantsintroduced vers ions of carslowered bids in ant ic ipat ionoversees trading on Nasdaqgained 1 to 19Figure 3: Sample of 4 head words for PP-attachmentWe also obtained the performances of 3 non-experts on aset of 200 randomly selected test events from the ComputerManuals corpus.
In this trial, the participants made attachmentdecisions given only the four head words.
The results areshown in Table 7.5.
ConclusionThe Maximum Entropy model predicts prepositional phraseattachment 10 percentage points less accurately than a tree-banker, but it performs comparably toa non-expert, assumingthat only only the head words of the history are available inboth cases.
The biggest improvements to the ME model willcome from better utilization of classes, and a larger history.Currently, the use of the mutual information class bits givesus a few percentage points in performance, but the ME modelshould gain more from other word classing schemes whichare better tuned to the PP-attachment problem.
A scheme inwhich the word classes are built from the observed attach-ment preferences of words ought to outperform the mutualinformation clustering method, which uses only word bigramdistributions\[10\].254I Average Human I 77-3% \]ME Model 83.5%Table 7: Average Performance of Human & ME Model on200 Events of Computer Manuals DataSecondly, the ME model does not use information containedin the rest of the sentence, although it is apparently usefulin predicting the attachment, as evidenced by a 5% averagegain in the treebankers' accuracy.
Any implementation f thismodel using the rest of the sentence would require featureson other words, and perhaps features on the sentence's parsetree structure, coupled with an efficient incremental search.Such improvements should boost the performance of themodel to that of treebankers.
Already, the ME model out-performs a decision tree confronted with the same task.
Wehope to use Maximum Entropy to predict other linguistic phe-nomena that hinder the performance of most natural anguageparsers.References1.
Jaynes, E. T., "Information Theory and Statistical Mechanics."Phys.
Rev.
106, pp.
620-630, 1957.2.
Kullback, S., Information Theory in Statistics.
Wiley, NewYork, 1959.3.
Darroch, J.N.
and Ratcliff, D., "Generalized Iterative Scalingfor Log-Linear Models", The Annals of Mathematical Statis-tics, Vol.
43, pp 1470-1480, 1972.4.
Delia Pietra,S., Della Pietra, V., Mercer, R. L., Roukos, S.,"Adaptive Language Modeling Using Minimum DiscriminantEstimation," Proceedings oflCASSP-92, pp.
1-633-636, SanFrancisco, March 1992.5.
Brown, P., Delia Pietra, S., Della Pietra, V., Mercer, R., Nadas,A., and Roukos, S., "Maximum Entropy Methods and TheirApplications to Maximum Likelihood Parameter Estimation ofConditional Exponential Models," A forthcoming IBM techni-cal report.6.
Berger, A., Della Pietra, S.A., and Della Pietra, V.J.. Maxi-mum Entropy Methods in Machine Translation.
manuscript inpreparation.7.
Black, E., Garside, R., and Leech, G., 1993.
Statistically-driven Computer Grammars of English: The IBM/LancasterApproach.
Rodopi.
Atlanta, Georgia.8.
Black, E., Jelinek, F., Lafferty, J., Magerman, D. M., Mercer,R., and Roukos, S., 1993.
Towards History-based Grammars:Using Richer Models for Probabilistic Parsing.
In Proceed-ings of the Association for Computational Linguistics, 1993.Columbus, Ohio.9.
Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C.J., 1984.
Classification and Regression Trees.
Wadsworth andBrooks.
Pacific Grove, California.10.
Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C.,and Mercer, R. L. Class-based n-gram Models of NaturalLanguage.
In Proceedings of the IBM Natural Language 1TL,March, 1990.
Paris, France.11.
Hindle, D. and Rooth, M. 1990.
Structural Ambiguity and Lex-ical Relations.
In Proceedings ofthe June 1990 DARPA Speechand Natural Language Workshop.
Hidden Valley, Pennsylva-nia.12.
Magerman, D., 1994.
Natural Language Parsing as StatisticalPattern Recognition.
Ph.D. dissertation, Stanford University,California.255
