Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 125?130,Prague, June 2007. c?2007 Association for Computational LinguisticsHypothesis Transformation and Semantic Variability Rules Used inRecognizing Textual EntailmentAdrian Iftene?Al.
I. Cuza?
University, Faculty ofComputer Science, Iasi, Romaniaadiftene@info.uaic.roAlexandra Balahur-Dobrescu?Al.
I. Cuza?
University, Faculty ofComputer Science, Iasi, Romaniaabalahur@info.uaic.roAbstractBased on the core approach of the tree editdistance algorithm, the system central mod-ule is designed to target the scope of TE ?semantic variability.
The main idea is totransform the hypothesis making use of ex-tensive semantic knowledge from sourceslike DIRT, WordNet, Wikipedia, acronymsdatabase.
Additionally, we built a system toacquire the extra background knowledgeneeded and applied complex grammar rulesfor rephrasing in English.1 IntroductionMany NLP applications need to recognize whenthe meaning of one text can be expressed by, orinferred from, another text.
Information Retrieval(IR), Question Answering (QA), Information Ex-traction (IE), Text Summarization (SUM) are ex-amples of applications that need to assess such asemantic relationship between text segments.
Tex-tual Entailment Recognition (RTE) (Dagan et al,2006) has recently been proposed as an applicationindependent task to capture such inferences.This year our textual entailment system partici-pated for the first time in the RTE1 competition.Next chapters present its main parts, the detailedresults obtained and some possible future im-provements.2 System descriptionThe process requires an initial pre-processing, fol-lowed by the execution of a core module whichuses the output of the first phase and obtains in theend the answers for all pairs.
Figure 1 shows how1http://www.pascal-network.org/Challenges/RTE3/the pre-processing is realized with the MINIPAR(Lin, 1998) and LingPipe2 modules which providethe input for the core module.
This one uses fourdatabases: DIRT, Acronyms, Background knowl-edge and WordNet.Figure 1: System architectureThe system architecture is based on a peer-to-peer networks design, in which neighboring com-puters collaborate in order to obtain the global fit-ness for every text-hypothesis pair.
Eventually,based on the computed score, we decide for whichpairs we have entailment.
This type of architecturewas used in order to increase the computationspeed.3 Initial pre-processingThe first step splits the initial file into pairs of filesfor text and hypothesis.
All these files are then sentto the LingPipe module in order to find the Namedentities.2http://www.alias-i.com/lingpipe/InitialdataDIRTMiniparmoduleDependencytrees for(T, H) pairsLingPipemoduleNamedentities for(T, H) pairsFinalresultCoreModule3CoreModule2CoreModule1AcronymsBackgroundknowledgeWordnetP2PComputersWikipedia125In parallel, we transform with MINIPAR boththe text and the hypothesis into dependency trees.Figure 2 shows the output associated with the sen-tence: ?Le Beau Serge was directed by Chabrol.
?.Figure 2: MINIPAR output ?
dependency treeFor every node from the MINIPAR output, weconsider a stamp called entity with three main fea-tures: the node lemma, the father lemma and theedge label (which represents the relation betweenwords) (like in Figure 3).Figure 3: Entity componentsUsing this stamp, we can easily distinguish be-tween nodes of the trees, even if these have thesame lemma and the same father.
In the examplefrom Figure 1, for the ?son?
nodes we have twoentities (Le_Beau_Serge, direct, s) and(Le_Beau_Serge, direct, obj).4 The hypothesis tree transformationPresently, the core of our approach is based on atree edit distance algorithm applied on the depend-ency trees of both the text and the hypothesis(Kouylekov, Magnini 2005).
If the distance (i.e.
thecost of the editing operations) among the two treesis below a certain threshold, empirically estimatedon the training data, then we assign an entailmentrelation between the two texts.The main goal is to map every entity in the de-pendency tree associated with the hypothesis(called from now on hypothesis tree) to an entity inthe dependency tree associated with the text (calledfrom now on text tree).For every mapping we calculate a local fitnessvalue which indicates the appropriateness betweenentities.
Subsequently, the global fitness is calcu-lated from these partial values.For every node (refers to the word contained inthe node) which can be mapped directly to a nodefrom the text tree, we consider the local fitnessvalue to be 1.
When we cannot map one word ofthe hypothesis to one node from the text, we havethe following possibilities:?
If the word is a verb in the hypothesis tree, weuse the DIRT resource (Lin and Pantel, 2001)in order to transform the hypothesis tree into anequivalent one, with the same nodes except theverb.
Our aim in performing this transforma-tion is to find a new value for the verb whichcan be better mapped in the text tree.?
If the word is marked as named entity by Ling-Pipe, we try to use an acronyms?
database3 or ifthe word is a number we try to obtain informa-tion related to it from the background knowl-edge.
In the event that even after theseoperations we cannot map the word from thehypothesis tree to one node from the text tree,no fitness values are computed for this caseand we decide the final result: No entailment.?
Else, we use WordNet (Fellbaum, 1998) tolook up synonyms for this word and try to mapthem to nodes from the text tree.Following this procedure, for every transforma-tion with DIRT or WordNet, we consider for localfitness the similarity value indicated by these re-sources.
If after all checks, one node from the hy-pothesis tree cannot be mapped, some penalty isinserted in the value of the node local fitness.4.1 The DIRT resourceFor the verbs in the MINIPAR output, we extracttemplates with DIRT- like format.
For the sampleoutput in Figure 2, where we have a single verb?direct?, we obtain the following list of ?full?
tem-plates:N:s:V<direct>V:by:N and N:obj:V<direct>V:by:N. To this list we add a list of ?partial?
tem-plates: N:s:V<direct>V:, :V<direct>V:by:N,:V<direct>V:by:N, and N:obj:V<direct>V:.In the same way, we build a list with templatesfor the verbs in the text tree.
With these two listswe perform a search in the DIRT database and ex-tract the ?best?
trimming, considering the templatetype (full or partial) and the DIRT score.According to the search results, we have the fol-lowing situations:3http://www.acronym-guide.comdirect (V)Le_Beau_Serge (N) be (be) ChabrolLe_Beau_Serge (N)Le (U) Beau (U)sbe byobjlex-modnode lemmaedge labelfather lemmalex-mod126a) left ?
left relations similarityThis case is described by the following two tem-plates for the hypothesis and the text:relation1 HypothesisVerb relation2relation1 TextVerb relation3This is the most frequent case, in which a verb isreplaced by one of its synonyms or equivalent ex-pressionsThe transformation of the hypothesis tree is donein two steps:1.
Replace the relation2 with relation3,2.
Replace the verb from the hypothesis withthe corresponding verb from the text.
(seeFigure 4).Figure 4: Left-left relation similarityb) right ?
right relations similarity: the sameidea from the previous case.c) left ?
right relations similarityThis case can be described by the following twotemplates for the hypothesis and the text:relation1 HypothesisVerb relation2relation3 TextVerb relation1The transformation of the hypothesis tree is:1.
Replace the relation2 with relation3,2.
Replace the verb from the hypothesis withthe corresponding verb from the text.3.
Rotate the subtrees accordingly: left sub-tree will be right subtree and vice-versaright subtree will become left-subtree (as itcan be observed in Figure 5).Figure 5: Left-right relation similarityThis case appears for pair 161 with the verb ?at-tack?
:T: ?The demonstrators, convoked by the solidaritywith Latin America committee, verbally attackedSalvadoran President Alfredo Cristiani.
?H: ?President Alfredo Cristiani was attacked bydemonstrators.
?In this case, for the text we have the templateN:subj:V<attack>V:obj:N, and for the hypothesisthe template N:obj:V<attack>V:by:N. Using DIRT,hypothesis H is transformed into:H?
: Demonstrators attacked President AlfredoCristiani.Under this new form, H is easier comparable to T.d) right ?
left relations similarity: the sameidea from the previous caseFor every node transformed with DIRT, we con-sider its local fitness as being the similarity valueindicated by DIRT.4.2 Extended WordNetFor non-verbs nodes from the hypothesis tree, if inthe text tree we do not have nodes with the samelemma, we search for their synonyms in the ex-tended WordNet4.
For every synonym, we check tosee if it appears in the text tree, and select the map-ping with the best value according to the valuesfrom Extended WordNet.
Subsequently, we changethe word from the hypothesis tree with the wordfrom WordNet and also its fitness with its indicatedsimilarity value.
For example, the relation between?relative?
and ?niece?
is accomplished with a scoreof 0.078652.4http://xwn.hlt.utdallas.edu/downloads.htmlHypothesisVerbrelation1 relation2TextVerbrelation3 relation1LeftSubtreeRightSubtreeRightSubtreeLeftSubtreeHypothesisVerbrelation1relation2TextVerbrelation1 relation3LeftSubtreeRightSubtreeRightSubtreeLeftSubtree1274.3 AcronymsThe acronyms?
database helps our program findrelations between the acronym and its meaning:?US - United States?, and ?EU - European Union?.We change the word with the corresponding ex-pression from this database.
Since the meaning isthe same, the local fitness is considered maximum,i.e.
1.4.4 Background KnowledgeSome information cannot be deduced from the al-ready used databases and thus we require addi-tional means of gathering extra information of theform:Argentine [is] ArgentinaNetherlands [is] Holland2 [is] twoLos Angeles [in] CaliforniaChinese [in] ChinaTable 1: Background knowledgeBackground knowledge was built semi-automatically, for the named entities (NEs) and fornumbers from the hypothesis without correspon-dence in the text.
For these NEs, we used a moduleto extract from Wikipedia5 snippets with informa-tion related to them.
Subsequently, we use this filewith snippets and some previously set patterns ofrelations between NEs, with the goal to identify aknown relation between the NE for which we havea problem and another NE.If such a relation is found, we save it to an out-put file.
Usually, not all relations are correct, butthose that are will help us at the next run.Our patterns identify two kinds of relations be-tween words:?
?is?, when the module extracts information ofthe form: ?Argentine Republic?
(Spanish: 'Re-publica Argentina', IPA)?
or when explanationsabout the word are given in brackets, or whenthe extracted information contains one verbused to define something, like ?is?, ?define?,?represent?
: '2' ('two') is a number.?
?in?
when information is of the form: 'Chinese'refers to anything pertaining to China or in theform Los Angeles County, California, etc.5http://en.wikipedia.org/wiki/Main_PageIn this case, the local fitness for the node is set tothe maximum value for the [is]-type relations, andit receives some penalties for the [in]-type relation.5 Determination of entailmentAfter transforming the hypothesis tree, we calcu-late a global fitness score using the extended localfitness value for every node from the hypothesis -which is calculated as sum of the following values:1. local fitness obtained after the tree trans-formation and node mapping,2.
parent fitness after parent mapping,3.
mapping of the node edge label from thehypothesis tree onto the text tree,4.
node position (left, right) towards its fatherin the hypothesis and position of the map-ping nodes from the text.After calculating this extended local fitness score,the system computes a total fitness for all the nodesin the hypothesis tree and a negation value associ-ated to the hypothesis tree.
Tests have shown thatout of these parameters, some are more important(the parameter at 1.)
and some less (the parameterat 3.).
Below you can observe an example of howthe calculations for 3 and 4 are performed and whatthe negation rules are.5.1 Edge label mappingAfter the process of mapping between nodes, wecheck how edge labels from the hypothesis tree aremapped onto the text tree.
Thus, having two adja-cent nodes in the hypothesis, which are linked byan edge with a certain label, we search on the pathbetween the nodes?
mappings in the text tree thislabel.
(see Figure 6)Figure 6: Entity mappingText treenodemappingfathermappingedge labelmappingHypothesis tree128It is possible that more nodes until the label of theedge linking the nodes in the hypothesis exist, or itis possible that this label is not even found on thispath.
According to the distance or to the case inwhich the label is missing, we insert some penaltiesin the extended local fitness.5.2 Node positionAfter mapping the nodes, one of the two followingpossible situations may be encountered:?
The position of the node towards its father andthe position of the mapping node towards itsfather?s mapping are the same (left-left orright-right).
In this case, the extended local fit-ness is incremented.?
The positions are different (left-right or right-left) and in this case a penalty is applied ac-cordingly.5.3 Negation rulesFor every verb from the hypothesis we consider aBoolean value which indicates whether the verbhas a negation or not, or, equivalently, if it is re-lated to a verb or adverb ?diminishing?
its sense ornot.
Consequently, we check in its tree on its de-scending branches to see whether one or more ofthe following words are to be found (pure form ofnegation or modal verb in indicative or conditionalform): ?not, may, might, cannot, should, could,etc.?.
For each of these words we successively ne-gate the initial truth value of the verb, which bydefault is ?false?.
The final value depends on thenumber of such words.Since the mapping is done for all verbs in thetext and hypothesis, regardless of their originalform in the snippet, we also focused on studyingthe impact of the original form of the verb on itsoverall meaning within the text.
Infinitives can beidentified when preceded by the particle ?to?.
Ob-serving this behavior, one complex rule for nega-tion was built for the particle ?to?
when it precedesa verb.
In this case, the sense of the infinitive isstrongly influenced by the active verb, adverb ornoun before the particle ?to?, as follows: if it isbeing preceded by a verb like ?allow, impose, gal-vanize?
or their synonyms, or adjective like ?nec-essary, compulsory, free?
or their synonyms ornoun like ?attempt?, ?trial?
and their synonyms, themeaning of the verb in infinitive form is stressedupon and becomes ?certain?.
For all other cases,the particle ?to?
diminish the certainty of the actionexpressed in the infinitive-form verb.
Based on thesynonyms database with the English thesaurus6, webuilt two separate lists ?
one of ?certainty stressing(preserving)?
?
?positive?
and one of ?certaintydiminishing?
?
?negative?
words.
Some examplesof these words are ?probably?, ?likely?
?
from thelist of ?negative?
words and ?certainly?, ?abso-lutely?
?
from the list of ?positive?
words.5.4 Global fitness calculationWe calculate for every node from the hypothesistree the value of the extended local fitness, and af-terwards consider the normalized value relative tothe number of nodes from the hypothesis tree.
Wedenote this result by TF (total fitness):rNodesNumbeHypothesiscalFitnessExtendedLoTF Hnodenode?=After calculating this value, we compute a valueNV (the negation value) indicating the number ofverbs with the same value of negation, using thefollowing formula:rOfVerbsTotalNumberVerbsNumbePositiveNV _=where the Positive_VerbsNumber is the number ofnon-negated  verbs from the hypothesis using thenegation rules, and TotalNumberOfVerbs is thetotal number of verbs from the hypothesis.Because the maximum value for the extendedfitness is 4, the complementary value of the TF is4-TF and the formula for the global fitness used is:)4(*)1(* TFNVTFNVessGlobalFitn ?
?+=For pair 518 we have the following:Initial entity NodeFitnessExtendedlocal fitness(the, company, det) 1 3.125(French, company, nn) 1 3.125(railway, company, nn) 1 3.125(company, call, s) 1 2.5(be, call, be) 1 4(call, -, -) 0.096 3.048(company, call, obj) 1 1.125(SNCF, call, desc) 1 2.625Table 2: Entities extended fitness6http://thesaurus.reference.com/129TF = (3.125 + 3.125 + 3.125 + 2.5 + 4 + 3.048 +1.125 + 2.625)/8 = 22.673/8 = 2.834NV = 1/1 = 1GlobalFitness = 1*2.834+(1?1)*(4-2.834) = 2.834Using the development data, we establish athreshold value of 2.06.
Thus, pair 518 will havethe answer ?yes?.6 ResultsOur system has a different behavior on differentexisting tasks, with higher results on Question An-swering (0.87) and lower results on InformationExtraction (0.57).
We submitted two runs for oursystem, with different parameters used in calculat-ing the extended local fitness.
However, the resultsare almost the same (see Table 3).IE IR QA SUM GlobalRun01 0.57 0.69 0.87 0.635 0.6913Run02 0.57 0.685 0.865 0.645 0.6913Table 3: Test resultsTo be able to see each component?s relevance, thesystem was run in turn with each component re-moved.
The results in the table below show that thesystem part verifying the NEs is the most impor-tant.System Description Precision RelevanceWithout DIRT 0.6876 0.54 %Without WordNet 0.6800 1.63 %Without Acronyms 0.6838  1.08 %Without BK 0.6775 2.00 %Without Negations 0.6763 2.17 %Without NEs 0.5758 16.71 %Table 4: Components relevance7 ConclusionsThe system?s core algorithm is based on the treeedit distance approach, however, focused on trans-forming the hypothesis.
It presently uses wide-spread syntactic analysis tools like Minipar, lexicalresources like WordNet and LingPipe for NamedEntities recognition and semantic resources likeDIRT.
The system?s originality resides firstly increating a part-of and equivalence ontology usingan extraction module for Wikipedia data on NEs(the background knowledge), secondly in using adistinct database of acronyms from different do-mains, thirdly acquiring a set of important contextinfluencing terms and creating a semantic equiva-lence set of rules based on English rephrasing con-cepts and last, but not least, on the technical side,using a distributed architecture for time perform-ance enhancement.The approach unveiled some issues related to thedependency to parsing tools, for example separat-ing the verb and the preposition in the case ofphrasal verbs, resulting in the change of meaning.Another issue was identifying expressions thatchange context nuances, which we denoted by?positive?
or ?negative?
words.
Although we ap-plied rules for them, we still require analysis todetermine their accurate quantification.For the future, our first concern is to search for amethod to establish more precise values for penal-ties, in order to obtain lower values for pairs withNo entailment.
Furthermore, we will develop a newmethod to determine the multiplication coefficientsfor the parameters in the extended local fitness andthe global threshold.8 AcknowledgementsThe authors thank the members of the NLP groupin Iasi for their help and support at different stagesof the system development.
Special thanks go toDaniel Matei which was responsible for preparingall the input data.The work on this project is partially financed bySiemens VDO Iai and by the CEEX Rotel projectnumber 29.ReferencesDagan, I., Glickman, O., and Magnini, B.
2006.
ThePASCAL Recognising Textual Entailment Challenge.In Qui?onero-Candela et al, editors, MLCW 2005,LNAI Volume 3944, pages 177-190.
Springer-Verlag.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, Mass.Kouylekov, M. and Magnini, B.
2005.
Recognizing Tex-tual Entailment with Tree Edit Distance Algorithms.In Proceedings of the First Challenge Workshop Rec-ognising Textual Entailment, Pages 17-20, 25?28April, 2005, Southampton, U.K.Lin, D. 1998.
Dependency-based Evaluation ofMINIPAR.
In Workshop on the Evaluation of ParsingSystems, Granada, Spain, May, 1998.Lin, D., and Pantel, P. 2001.
DIRT - Discovery of Infer-ence Rules from Text.
In Proceedings of ACM Con-ference on Knowledge Discovery and Data Mining(KDD-01).
pp.
323-328.
San Francisco, CA.130
