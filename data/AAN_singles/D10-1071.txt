Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 725?735,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA New Approach to Lexical Disambiguation of Arabic TextRushin ShahCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213, USArnshah@cs.cmu.eduParamveer S. Dhillon, Mark Liberman,Dean Foster, Mohamed Maamouriand Lyle UngarUniversity of Pennsylvania3451 Walnut StreetPhiladelphia, PA 19104, USA{dhillon|myl|ungar}@cis.upenn.edu,foster@wharton.upenn.edu,maamouri@ldc.upenn.eduAbstractWe describe a model for the lexical analy-sis of Arabic text, using the lists of alterna-tives supplied by a broad-coverage morpho-logical analyzer, SAMA, which include sta-ble lemma IDs that correspond to combina-tions of broad word sense categories and POStags.
We break down each of the hundredsof thousands of possible lexical labels intoits constituent elements, including lemma IDand part-of-speech.
Features are computedfor each lexical token based on its local anddocument-level context and used in a novel,simple, and highly efficient two-stage super-vised machine learning algorithm that over-comes the extreme sparsity of label distribu-tion in the training data.
The resulting systemachieves accuracy of 90.6% for its first choice,and 96.2% for its top two choices, in selectingamong the alternatives provided by the SAMAlexical analyzer.
We have successfully usedthis system in applications such as an onlinereading helper for intermediate learners of theArabic language, and a tool for improving theproductivity of Arabic Treebank annotators.1 Background and MotivationThis paper presents a methodology for generatinghigh quality lexical analysis of highly inflected lan-guages, and demonstrates excellent performance ap-plying our approach to Arabic.
Lexical analysis ofthe written form of a language involves resolving,explicitly or implicitly, several different kinds of am-biguities.
Unfortunately, the usual ways of talkingabout this process are also ambiguous, and our gen-eral approach to the problem, though not unprece-dented, has uncommon aspects.
Therefore, in orderto avoid confusion, we begin by describing how wedefine the problem.In an inflected language with an alphabetic writ-ing system, a central issue is how to interpret stringsof characters as forms of words.
For example, theEnglish letter-string ?winds?
will normally be in-terpreted in one of four different ways, all fourof which involve the sequence of two formativeswind+s.
The stem ?wind?
might be analyzed as (1) anoun meaning something like ?air in motion?, pro-nounced [wInd] , which we can associate with an ar-bitrary but stable identifier like wind n1; (2) a verbwind v1 derived from that noun, and pronounced thesame way; (3) a verb wind v2 meaning somethinglike ?
(cause to) twist?, pronounced [waInd]; or (4)a noun wind n2 derived from that verb, and pro-nounced the same way.
Each of these ?lemmas?, ordictionary entries, will have several distinguishablesenses, which we may also wish to associate withstable identifiers.
The affix ?-s?
might be analyzedas the plural inflection, if the stem is a noun; or asthe third-person singular inflection, if the stem is averb.We see this analysis as conceptually divided intofour parts: 1) Morphological analysis, which rec-ognizes that the letter-string ?winds?
might be (per-haps among other things) wind/N + s/PLURAL orwind/V + s/3SING; 2) Morphological disambigua-tion, which involves deciding, for example, that inthe phrase ?the four winds?, ?winds?
is probably aplural noun, i.e.
wind/N + s/PLURAL; 3) Lemmaanalysis, which involves recognizing that the stemwind in ?winds?
might be any of the four lem-mas listed above ?
perhaps with a further listing ofsenses or other sub-entries for each of them; and 4)Lemma disambiguation, deciding, for example, that725the phrase ?the four winds?
probably involves thelemma wind n1.Confusingly, the standard word-analysis tasks incomputational linguistics involve various combina-tions of pieces of these logically-distinguished op-erations.
Thus, ?part of speech (POS) tagging?
ismainly what we?ve called ?morphological disam-biguation?, except that it doesn?t necessarily requireidentifying the specific stems and affixes involved.In some cases, it also may require a small amount of?lemma disambiguation?, for example to distinguisha proper noun from a common noun.
?Sense disam-biguation?
is basically a form of what we?ve called?lemma disambiguation?, except that the sense dis-ambiguation task may assume that the part of speechis known, and may break down lexical identity morefinely than our system happens to do.
?Lemmatiza-tion?
generally refers to a radically simplified formof ?lemma analysis?
and ?lemma disambiguation?,where the goal is simply to collapse different in-flected forms of any similarly-spelled stems, so thatthe strings ?wind?, ?winds?, ?winded?, ?winding?
willall be treated as instances of the same thing, withoutin fact making any attempt to determine the identityof ?lemmas?
in the traditional sense of dictionaryentries.Linguists use the term morphology to include allaspects of lexical analysis under discussion here.But in most computational applications, ?morpho-logical analysis?
does not include the disambigua-tion of lemmas, because most morphological ana-lyzers do not reference a set of stable lemma IDs.So for the purposes of this paper, we will continue todiscuss lemma analysis and disambiguation as con-ceptually distinct from morphological analysis anddisambiguation, although, in fact, our system dis-ambiguates both of these aspects of lexical analysisat the same time.The lexical analysis of textual character-stringsis a more complex and consequential problem inArabic than it is in English, for several reasons.First, Arabic inflectional morphology is more com-plex than English inflectional morphology is.
Wherean English verb has five basic forms, for example,an Arabic verb in principle may have dozens.
Sec-ond, the Arabic orthographic system writes elementssuch as prepositions, articles, and possessive pro-nouns without setting them off by spaces, roughlyas if the English phrase ?in a way?
were written ?in-away?.
This leads to an enormous increase in thenumber of distinct ?orthographic words?, and a sub-stantial increase in ambiguity.
Third, short vowelsare normally omitted in Arabic text, roughly as ifEnglish ?in a way?
were written ?nway?.As a result, a whitespace/punctuation-delimitedletter-string in Arabic text typically has many morealternative analyses than a comparable Englishletter-string does, and these analyses have manymore parts, drawn from a much larger vocabulary ofform-classes.
While an English ?tagger?
can spec-ify the morphosyntactic status of a word by choos-ing from a few dozen tags, an equivalent level ofdetail in Arabic would require thousands of alterna-tives.
Similarly, the number of lemmas that mightplay a role in a given letter-sequence is generallymuch larger in Arabic than in English.We start our labeling of Arabic text with the alter-native analyses provided by SAMA v. 3.1, the Stan-dard Arabic Morphological Analyzer (Maamouri etal., 2009).
SAMA is an updated version of the ear-lier Buckwalter analyzers (Buckwalter, 2004), witha number of significant differences in analysis tomake it compatible with the LDC Arabic Treebank3-v3.2 (Maamouri et al, 2004).
The input to SAMAis an Arabic orthographic word (a string of lettersdelimited by whitespace or punctuation), and theoutput of SAMA is a set of alternative analyses, asshown in Table 1.
For a typical word, SAMA pro-duces approximately a dozen alternative analyses,but for certain highly ambiguous words it can pro-duce hundreds of alternatives.The SAMA analyzer has good coverage; for typ-ical texts, the correct analysis of an orthographicword can be found somewhere in SAMA?s list ofalternatives about 95% of the time.
However, thisbroad coverage comes at a cost; the list of analyticalternatives must include a long Zipfian tail of rareor contextually-implausible analyses, which collec-tively are correct often enough to make a large con-tribution to the coverage statistics.
Furthermore,SAMA?s long lists of alternative analyses are notevaluated or ordered in terms of overall or contex-tual plausibility.
This makes the results less usefulin most practical applications.Our goal is to rank these alternative analyses sothat the correct answer is as near to the top of the list726Token Lemma Vocalization Segmentation Morphology GlossyHlm Halam-u 1 yaHolumu ya + Holum +uIV3MS + IV + IV-SUFF MOOD:Ihe / it + dream + [ind.
]yHlm Halam-u 1 yaHoluma ya + Holum +aIV3MS + IV + IV-SUFF MOOD:She / it + dream + [sub.
]yHlm Halum-u 1 yaHolumo ya + Holum +oIV3MS + IV + IV-SUFF MOOD:Jhe / it + be gentle + [jus.
]qbl qabil-a 1 qabila qabil + a PV + PV-SUFF SUBJ:3MSaccept/receive/approve +he/it [verb]qbl qabol 1 qabol qabol NOUN BeforeTable 1: Partial output of SAMA for yHlm and qbl.
On average, every token produces more than 10 such analysesas possible.
Despite some risk of confusion, we?llrefer to SAMA?s list of alternative analyses for anorthographic word as potential labels for that word.And despite a greater risk of confusion, we?ll refer tothe assignment of probabilities to the set of SAMAlabels for a particular Arabic word in a particulartextual context as tagging, by analogy to the oper-ation of a stochastic part-of-speech tagger, whichsimilarly assigns probabilities to the set of labelsavailable for a word in textual context.Although our algorithms have been developed forthe particular case of Arabic and the particular setof lexical-analysis labels produced by SAMA, theyshould be applicable without modification to the setsof labels produced by any broad-coverage lexicalanalyzer for the orthographic words of any highly-inflected language.In choosing our approach, we have been moti-vated by two specific applications.
One applica-tion aims to help learners of Arabic in reading text,by offering a choice of English glosses with asso-ciated Arabic morphological analyses and vocaliza-tions.
SAMA?s excellent coverage is an importantbasis for this help; but SAMA?s long, unranked listof alternative analyses for a particular letter-string,where many analyses may involve rare words or al-ternatives that are completely implausible in the con-text, will be confusing at best for a learner.
It ismuch more helpful for the list to be ranked so thatthe correct answer is almost always near the top, andis usually one of the top two or three alternatives.In our second application, this same sort of rank-ing is also helpful for the linguistically expert nativespeakers who do Arabic Treebank analysis.
Theseannotators understand the text without difficulty, butfind it time-consuming and fatiguing to scan a longlist of rare or contextually-implausible alternativesfor the correct SAMA output.
Their work is fasterand more accurate if they start with a list that isranked accurately in order of contextual plausibility.Other applications are also possible, such as vo-calization of Arabic text for text-to-speech synthe-sis, or lexical analysis for Arabic parsing.
However,our initial goals have been to rank the list of SAMAoutputs for human users.We note in passing that the existence of set of sta-ble ?lemma IDs?
is an unusual feature of SAMA,which in our opinion ought to be emulated by ap-proaches to lexical analysis in other languages.
Thelack of such stable lemma IDs has helped to disguisethe fact that without lemma analysis and disam-biguation, morphological analyses and disambigua-tion is only a partial solution to the problem of lexi-cal analysis.In principle, it is obvious that lemma disambigua-tion and morphological disambiguation are mutuallybeneficial.
If we know the answer to one of the ques-tions, the other one is easier to answer.
However,these two tasks require rather different sets of con-textual features.
Lemma disambiguation is similarto the problem of word-sense disambiguation ?
onsome definitions, they are identical ?
and as a re-sult, it benefits from paragraph-level and document-level bag-of-words attributes that help to character-ize what the text is ?about?
and therefore which lem-mas are more likely to play a role in it.
In contrast,morphological disambiguation mainly depends onfeatures of nearby words, which help to character-727ize how inflected forms of these lemmas might fitinto local phrasal structures.2 Problem and MethodologyConsider a collection of tokens (observations), ti, re-ferred to by index i ?
{1, .
.
.
, n}, where each tokenis associated with a set of p features, xij , for the jthfeature, and a label, li, which is a combination ofa lemma and a morphological analysis.
We use in-dicator functions yik to indicate whether or not thekth label for the ith token is present.
We representthe complete set of features and labels for the en-tire training data using matrix notation as X and Y ,respectively.
Our goal is to predict the label l (orequivalently, the vector y for a given feature vectorx.A standard linear regression model of this prob-lem would bey = x?
+  (1)The standard linear regression estimate of ?
(ig-noring, for simplicity the fact that the ys are 0/1) is:??
= (XTtrainXtrain)?1XTtrainYtrain (2)where Ytrain is an n?h matrix containing 0s and1s indicating whether or not each of the h possiblelabels is the correct label (li) for each of the n tokensti, Xtrain is an n ?
p matrix of context features foreach of the n tokens, the coefficients ??
are p?
h.However, this is a large, sparse, multiple labelproblem, and the above formulation is neither statis-tically nor computationally efficient.
Each observa-tion (x,y) consists of thousands of features associ-ated with thousands of potential labels, almost all ofwhich are zero.
Worse, the matrix of coefficients ?,to be estimated is large (p?
h) and one should thususe some sort of transfer learning to share strengthacross the different labels.We present a novel principled and highly compu-tationally efficient method of estimating this multi-label model.
We use a two stage procedure, firstusing a subset (Xtrain1, Ytrain1) of training datato give a fast approximate estimate of ?
; we thenuse a second smaller subset of the training data(Xtrain2, Ytrain2,) to ?correct?
these estimates in away that we will show can be viewed as a spe-cialized shrinkage.
Our first stage estimation ap-proximates ?, but avoids the expensive computa-tion of (XTtrainXtrain)?1.
Our second stage corrects(shrinks) these initial estimates in a manner special-ized to this problem.
The second stage takes ad-vantage of the fact that we only need to considerthose candidate labels produced by SAMA.
Thus,only dozens of the thousands of possible labels areconsidered for each token.We now present our algorithm.
We start with acorpus D of documents d of labeled Arabic text.
Asdescribed above, each token, ti is associated with aset of features characterizing its context, computedfrom the other words in the same document, and a la-bel, li = (lemmai,morphologyi), which is a combi-nation of a lemma and a morphological analysis.
Asdescribed below, we introduce a novel factorizationof the morphology into 15 different components.Our estimation algorithm, shown in Algorithm 1,has two stages.
We partition the training corpus intotwo subsets, one of which (Xtrain1) is used to es-timate the coefficients ?s and the other of which(Xtrain2) is used to optimally ?shrink?
these coeffi-cient estimates to reduce variance and prevent over-fitting due to data sparsity.For the first stage of our estimation procedure, wesimplify the estimate of the (?)
matrix (Equation 2)to avoid the inversion of the very high dimensional(p?p) matrix (XTX) by approximating (XTX) byits diagonal, Var(X), the inverse of which is trivialto compute; i.e.
we estimate ?
using??
= Var(Xtrain1)?1XTtrain1Ytrain1 (3)For the second stage, we assume that the coeffi-cients for each feature can be shrunk differently, butthat coefficients for each feature should be shrunkthe same regardless of what label they are predict-ing.
Thus, for a given observation we predict:g?ik =p?j=1wj ?
?jkxij (4)where the weightswj indicate how much to shrinkeach of the p features.In practice, we fold the variance of each of the jfeatures into the weight, giving a slightly modifiedequation:g?ik =p?j=1?j?
?jkxij (5)728where ??
= XTtrain1Ytrain1 is just a matrix of thecounts of how often each context feature shows upwith each label in the first training set.
The vec-tor ?, which we will estimate by regression, is justthe shrinkage weightsw rescaled by the feature vari-ance.Note that the formation here is different from thefirst stage.
Instead of having each observation bea token, we now let each observation be a (token,label) pair, but only include those labels that wereoutput by SAMA.
For a given token ti and poten-tial label lk, our goal is to approximate the indica-tor function g(i, k), which is 1 if the kth label oftoken ti is present, and 0 otherwise.
We find candi-date labels using a morphological analyzer (namelySAMA), which returns a set of possible candidatelabels, say C(t), for each Arabic token t. Our pre-dicted label for ti is then argmaxk?C(ti)g(i, k).The regression model for learning the weights ?jin the second stage thus has a row for each labelg(i, k) associated with a SAMA candidate for eachtoken i = ntrain1+1 .
.
.
ntrain2 in the second train-ing set.
The value of g(i, k) is predicted as a func-tion of the feature vector zijk = ?
?jkxij .The shrinkage coefficients, ?j , could be estimatedfrom theory, using a version of James-Stein shrink-age (James and Stein, 1961), but in practice, superiorresults are obtained by estimating them empirically.Since there are only p of them (unlike the p ?
h ?s),a relatively small training set is sufficient.
We foundthat regression-SVMs work slightly better than lin-ear regression and significantly better than standardclassification SVMs for this problem.Prediction is then done in the obvious way by tak-ing the tokens in a test corpusDtest, generating con-text features and candidate SAMA labels for eachtoken ti, and selected the candidate label with thehighest score g?
(i, k) that we set out to learn.
Moreformally, The model parameters ??
and ?
producedby the algorithm allow one to estimate the mostlikely label for a new token ti out of a set of can-didate labels C(ti) usingkpred = argmaxk?C(ti)p?j=1?j?
?jkxij (6)The most expensive part of the procedure is es-timating ?
?, which requires for each token in cor-Algorithm 1 Training algorithm.Input: A training corpusDtrain of n observations(Xtrain, Ytrain)PartitionDtrain into two sets,D1 andD2, of sizesntrain1 and ntrain2 = n?
ntrain1 observations// Using D1, estimate ???
?jk =?ntrain1i=1 xijyik for the jth feature and kthlabel// Using D2, estimate ?j// Generate new ?features?
Z and the true labelsg(i, k) for each of the SAMA candidate labels foreach of the tokens in D2zijk = ?
?jkxij for i in i = ntrain1 + 1 .
.
.
ntrain2Estimate ?j for the above (feature,label) pairs(zijk, g(i, k)) using Regression SVMsOutput: ?
and ?
?pus D1, (a subset of D), finding the co-occurrencefrequencies of each label element (a lemma, or apart of the morphological segmentation) with thetarget token and jointly with the token and withother tokens or characters in the context of the to-ken of interest.
For example, given an Arabic to-ken, ?yHlm?, we count what fraction of the timeit is associated with each lemma (e.g.
Halam-u 1), count(lemma=Halam-u 1, token=yHlm) andeach segment (e.g.
?ya?
), count(segment=ya, to-ken=yHlm).
(Of course, most tokens never show upwith most lemmas or segments; this is not a prob-lem.)
We also find the base rates of the componentsof the labels (e.g., count(lemma=Halam-u 1), andwhat fraction of the time the label shows up in vari-ous contexts, e.g.
count(lemma=Halam-u 1, previ-ous token = yHlm).
We describe these features inmore detail below.3 Features and Labels used for TrainingOur approach to tagging Arabic differs from conven-tional approaches in the two-part shrinkage-basedmethod used, and in the choice of both features andlabels used in our model.
For features, we studyboth local context variables, as described above, anddocument-level word frequencies.
For the labels, thekey question is what labels are included and howthey are factored.
Standard ?taggers?
work by doingan n-way classification of all the alternatives, whichis not feasible here due to the thousands of possi-729ble labels.
Standard approaches such as ConditionalRandom Fields (CRFs) are intractable with so manylabels.
Moreover, few if any taggers do any lemmadisambiguation; that is partly because one must startwith some standard inventory of lemmas, which arenot available for most languages, perhaps becausethe importance of lemma disambiguation has beenunderestimated.We make a couple of innovations to deal withthese issues.
First, we perform lemma disambigua-tion in addition to ?tagging?.
As mentioned above,lemmas and morphological information are not in-dependent; the choice of lemma often influencesmorphology and vice versa.
For example, Table 1contains two analyses for the word qbl.
For the firstanalysis, where the lemma is qabil-a 1 and the glossis accept/receive/approve + he/it [verb], the word isa verb.
However, for the second analysis, where thelemma is qabol 1 and the gloss is before, the wordis a noun.Simultaneous lemma disambiguation and taggingintroduces additional complexity: An analysis ofATB and SAMA shows that there are approximately2,200 possible morphological analyses (?tags?)
and40,000 possible lemmas; even accounting for thefact that most combinations of lemmas and morpho-logical analyses don?t occur, the size of the labelspace is still in the order of tens of thousands.
Todeal with data sparsity, our second innovation is tofactor the labels.
We factor each label l into a set of16 label elements (LEs).
These include lemmas, aswell as morphological elements such as basic part-of-speech, suffix, gender, number, mood, etc.
Theseare explained in detail below.
Thus, since each la-bel l is a set of 15 categorical variables, each y inthe first learning stage is actually a vector with 16nonzero components and thousands of zeros.
Sincewe do simultaneous estimation of the entire set oflabel elements, the value g(i, k) being predicted inthe second learning phase is 1 if the entire label setis correct, and zero otherwise.
We do not learn sep-arate models for each label.3.1 Label Elements (LEs)The fact that there are tens of thousands of possiblelabels presents the problem of extreme sparsity oflabel distribution in the training data.
We find that amodel that estimates coefficients ??
to predict a sin-LE Descriptionlemma Lemmapre1 Closer prefixpre2 Farther prefixdet Determinerpos Basic POSdpos Additional data on basic possuf Suffixperpos Person (basic pos)numpos Number (basic pos)genpos Gender (basic pos)persuf Person (suffix)numsuf Number (suffix)gensuf Gender (suffix)mood Mood of verbpron Pronoun suffixTable 2: Label Elements (LEs).
Examples of additionaldata on basic POS include whether a noun is proper orcommon, whether a verb is transitive or not, etc.
Boththe basic POS and its suffix may have person, gender andnumber data.gle label (a label being in the Cartesian product ofthe set of label elements) yields poor performance.Therefore, as just mentioned, we factor each labell into a set of label elements (LEs), and learn thecorrelations ??
between features and label elements,rather than features and entire label sets.
This re-duces, but does not come close to eliminating, theproblem sparsity.
A complete list of these LEs andtheir possible values is detailed in Table 2.3.2 Features3.2.1 Local Context FeaturesWe take (t, l) pairs from D2, and for each suchpair generate features Z based on co-occurrencestatistics ??
in D1, as mentioned in Algorithm 2.These statistics include unigram co-occurrence fre-quencies of each label with the target token and bi-gram co-occurrence of the label with the token andwith other tokens or characters in the context of thetarget token.
We define them formally in Table 3.Let Zbaseline denote the set of all such basic featuresbased on the local context statistics of the target to-ken, namely the words and letters preceding and fol-lowing it.
We will use this set to create a baselinemodel.730Statistic DescriptionFreq countD1(t, l)PrevWord countD1(t, l, t?1)NextWord countD1(t, l, t+1)PreviLetter countD1(t, l, first letter(t?1))NextiLetter countD1(t, l, first letter(t+1)PrevfLetter countD1(t, l, last letter(t?1)NextfLetter countD1(t, l, last letter(t+1)Table 3: Co-occurrence statistics ??.
We use these togenerate feature sets for our regression SVMs.For each label element (LE) e, we define a set offeatures Ze similar to Zbaseline; these features arebased on co-occurrence frequencies of the particularLE e, not the entire label l.Finally, we define an aggregate feature set Zaggras follows:Zaggr = Zbaseline?
{Ze} (7)where e ?
{lemma, pre1, pre2, det, pos, dpos,suf, perpos, numpos, genpos, persuf, numsuf, gensuf,mood, pron}.3.2.2 Document Level FeaturesWhen trying to predict the lemma, it is useful toinclude not just the words and characters immedi-ately adjacent to the target token, but also the all thewords in the document.
These words capture the?topic?
of the document, and help to disambiguatedifferent lemmas, which tend to be used or not usedbased on the topic being discussed, similarly to theway that word sense disambiguation systems in En-glish sometimes use the ?bag of words?
the docu-ment to disambiguate, for example a ?bank?
for de-positing money from a ?bank?
of a river.
More pre-cisely, we augment the features for each target tokenwith the counts of each word in the document (the?term frequency?
tf) in which the token occurs witha given label.Zfull = Zaggr?Ztf (8)This setZfull is our final feature set.
We useZfullto train an SVM model Mfull; this is our final pre-dictive model.3.3 Corpora used for Training and TestingWe use three modules of the Penn Arabic Tree-bank (ATB) (Maamouri et al, 2004), namely ATB1,ATB2 and ATB3 as our corpus of labeled Ara-bic text, D. Each ATB module is a collectionof newswire data from a particular agency.
ATB1uses the Associated Press as a source, ATB2 usesUmmah, and ATB3 uses Annahar.
D contains a totalof 1,835 documents, accounting for approximately350,000 words.
We construct the training and test-ing setsDtrain andDtest fromD using 10-fold crossvalidation, and we constructD1 andD2 fromDtrainby randomly performing a 9:1 split.As mentioned earlier, we use the SAMA mor-phological analyzer to obtain candidate labels C(t)for each token t while training and testing an SVMmodel on D2 and Dtest respectively.
A sample out-put of SAMA is shown in Table 1.
To improve cov-erage, we also add to C(t) all the labels l seen for tin D1.
We find that doing so improves coverage to98%.
This is an upper bound on the accuracy of ourmodel.C(t) = SAMA(t)?
{l|(t, l) ?
D1} (9)4 ResultsWe use two metrics of accuracy: A1, which mea-sures the percentage of tokens for which the modelassigns the highest score to the correct label or LEvalue (or E1= 100?A1, the corresponding percent-age error), and A2, which measures the percentageof tokens for which the correct label or LE valueis one of the two highest ranked choices returnedby the model (or E2 = 100 ?
A2).
We test ourmodelMfull onDtest and achieve A1 and A2 scoresof 90.6% and 96.2% respectively.
The accuracyachieved by our Mfull model is, to the best of ourknowledge, higher than prior approaches have beenable to achieve so far for the problem of combinedmorphological and lemma disambiguation.
This isall the more impressive considering that the upperbound on accuracy for our model is 98% because,as described above, our set of candidate labels is in-complete.In order to analyze how well different LEs can bepredicted, we train an SVM model Me for each LEe using the feature set Ze, and test all such models731on Dtest.
The results for all the LEs are reported inthe form of error percentages E1 and E2 in Table 4.Model E1 E2 Model E1 E2Mlemma 11.1 4.9 Mpre1 1.9 1.4Mpre2 0.2 0 Mdet 0.7 0.1Mpos 23.4 4.0 Mdpos 10.3 1.9Msuf 7.6 2.5 Mperpos 3.0 0.1Mnumpos 3.2 0.2 Mgenpos 1.8 0.1Mpersuf 3.2 0.1 Mnumsuf 8.2 0.5Mgensuf 11.6 0.4 Mmood 1.6 1.4Mpron 1.8 0.6 Mcase 14.7 5.9Mfull 9.4 3.8 - - -Table 4: Results of Me for each LE e. Note: The resultsreported are 10 fold cross validation test accuracies andno parameters have been tuned on them.A comparison of the results for Mfull with theresults for Mlemma and Mpos is particularly infor-mative.
We see that Mfull is able to achieve a sub-stantially lower E1 error score (9.4%) than Mlemma(11.1%) and Mpos (23.4%); in other words, we findthat our full model is able to predict lemmas and ba-sic parts-of-speech more accurately than the individ-ual models for each of these elements.We examine the effect of varying the size of D2,i.e.
the number of SVM training instances, on theperformance of Mfull on Dtest, and find that withincreasing sizes of D2, E1 reduces only slightlyfrom 9.5% to 9.4%, and shows no improvementthereafter.
We also find that the use of document-level features in Mlemma reduces E1 and E2 per-centages for Mlemma by 5.7% and 3.2% respec-tively.4.1 Comparison to Alternate Approaches4.1.1 Structured Prediction ModelsPreliminary experiments showed that knowing thepredicted labels (lemma + morphology) of the sur-rounding words can slightly improve the predic-tive accuracy of our model.
To further investi-gate this effect, we tried running experiments us-ing different structured models, namely CRF (Con-ditional Random Fields) (Lafferty et al, 2001),(Structured) MIRA (Margin Infused Relaxation Al-gorithm) (Crammer et al, 2006) and StructuredPerceptron (Collins, 2002).
We used linear chainCRFs as implemented in MALLET Toolbox (Mc-Callum, 2001) and for Structured MIRA and Per-ceptron we used their implementations from EDLINToolbox (Ganchev and Georgiev, 2009).
However,given the vast label space of our problem, runningthese methods proved infeasible.
The time complex-ity of these methods scales badly with the number oflabels; It took a week to train a linear chain CRFfor only ?
50 labels and though MIRA and Per-ceptron are online algorithms, they also become in-tractable beyond a few hundred labels.
Since ourlabel space contains combinations of lemmas andmorphologies, so even after factoring, the dimensionof the label space is in the order of thousands.We also tried a na?
?ve version (two-pass approxi-mation) of these structured models.
In addition tothe features in Zfull, we include the predicted la-bels for the tokens preceding and following the tar-get token as features.
This new model is not onlyslow to train, but also achieves only slightly lowererror rates (1.2% lower E1 and 1.0% lower E2) thanMfull.
This provides an upper bound on the bene-fit of using the more complex structured models, andsuggests that given their computational demands our(unstructured) model Mfull is a better choice.4.1.2 MADA(Habash and Rambow, 2005) perform morpho-logical disambiguation using a morphological ana-lyzer.
(Roth et al, 2008) augment this with lemmadisambiguation; they call their system MADA.
Ourwork differs from theirs in a number of respects.Firstly, they don?t use the two step regression proce-dure that we use.
Secondly, they use only ?unigram?features.
Also, they do not learn a single model froma feature set based on labels and LEs; instead, theycombine models for individual elements by usingweighted agreement.
We trained and tested MADAv2.32 using its full feature set on the same DtrainandDtest.
We should point out that this is not an ex-act comparison, since MADA uses the older Buck-walter morphological analyzer.14.1.3 Other AlternativesUnfactored Labels: To illustrate the benefit ob-tained by breaking down each label l into1A new version of MADA was released very close to thesubmission deadline for this conference.732LEs, we contrast the performance of our Mfullmodel to an SVM model Mbaseline trained us-ing only the feature set Zbaseline, which onlycontains features based on entire labels, thosebased on individual LEs.Independent lemma and morphology prediction:Another alternative approach is to pre-dict lemmas and morphological analysesseparately.
We construct a feature setZlemma?
= Zfull ?
Zlemma and train an SVMmodel Mlemma?
using this feature set.
Labelsare then predicted by simply combining theresults predicted independently by Mlemmaand Mlemma?
.
Let Mind denote this approach.Unigram Features: Finally, we also consider acontext-less approach, i.e.
using only ?uni-gram?
features for labels as well as LEs.
Wecall this feature set Zuni, and the correspond-ing SVM model Muni.The results of these various models, along withthose of Mfull are summarized in Table 5.
We seethatMfull has roughly half the error rate of the state-of-the-art MADA system.Model E1 E2Mbaseline 13.6 9.1Mind 18.7 6.0Muni 11.6 6.4Mcheat 8.2 2.8MADA 16.9 12.6Mfull 9.4 3.8Table 5: Percent error rates of alternative approaches.Note: The results reported are 10 fold cross validationtest accuracies and no parameters have been tuned onthem.
We used same train-test splits for all the datasets.5 Related Work(Hajic, 2000) show that for highly inflectionallanguages, the use of a morphological analyzerimproves accuracy of disambiguation.
(Diab etal., 2004) perform tokenization, POS taggingand base phrase chunking using an SVM basedlearner.
(Ahmed and Nu?rnberger, 2008) performword-sense disambiguation using a Naive Bayesianmodel and rely on parallel corpora and match-ing schemes instead of a morphological ana-lyzer.
(Kulick, 2010) perform simultaneous tok-enization and part-of-speech tagging for Arabic byseparating closed and open-class items and focus-ing on the likelihood of possible stems of open-class words.
(Mohamed and Ku?bler, 2010) presenta hybrid method between word-based and segment-based POS tagging for Arabic and report good re-sults.
(Toutanova and Cherry, 2009) perform jointlemmatization and part-of-speech tagging for En-glish, Bulgarian, Czech and Slovene, but they donot use the two step estimation-shrinkage model de-scribed in this paper; nor do they factor labels.
Theidea of joint lemmatization and part-of-speech tag-ging has also been discussed in the context of Hun-garian in (Kornai, 1994).A substantial amount of relevant work has beendone previously for Hebrew.
(Adler and Elhadad,2006) perform Hebrew morphological disambigua-tion using an unsupervised morpheme-based HMM,but they report lower scores than those achieved byour model.
Moreover, their analysis doesn?t includelemma IDs, which is a novelty of our model.
(Gold-berg et al, 2008) extend the work of (Adler and El-hadad, 2006) by using an EM algorithm, and achievean accuracy of 88% for full morphological analy-sis, but again, this does not include lemma IDs.
Tothe best of our knowledge, there is no existing re-search for Hebrew that does what we did for Arabic,namely to use simultaneous lemma and morpholog-ical disambiguation to improve both.
(Dinur et al,2009) show that prepositions and function words canbe accurately segmented using unsupervised meth-ods.
However, by using this method as a preprocess-ing step, we would lose the power of a simultaneoussolution for these problems.
Our method is closer instyle to a CRF, giving much of the accuracy gains ofsimultaneous solution, while being about 4 orders ofmagnitude easier to train.We believe that our use of factored labels is novelfor the problem of simultaneous lemma and mor-phological disambiguation; however, (Smith et al,2005) and (Hatori et al, 2008) have previouslymade use of features based on parts of labels inCRF models for morphological disambiguation andword-sense disambiguation respectively.
Also, wenote that there is a similarity between our two-stage733machine learning approach and log-linear models inmachine translation that break the data in two parts,estimating log-probabilities of generative modelsfrom one part, and discriminatively re-weighting themodels using the second part.6 ConclusionsWe introduced a new approach to accurately predictlabels consisting of both lemmas and morphologi-cal analyses for Arabic text.
We obtained an accu-racy of over 90% ?
substantially higher than currentstate-of-the-art systems.
Key to our success is thefactoring of labels into lemma and a large set of mor-phosyntactic elements, and the use of an algorithmthat computes a simple initial estimate of the coef-ficient relating each contextual feature to each la-bel element (simply by counting co-occurrence) andthen regularizes these features by shrinking each ofthe coefficients for each feature by an amount deter-mined by supervised learning using only the candi-date label sets produced by SAMA.We also showed that using features of word n-grams is preferable to using features of only individ-ual tokens of data.
Finally, we showed that a modelusing a full feature set based on labels as well asfactored components of labels, which we call labelelements (LEs) works better than a model createdby combining individual models for each LE.
Webelieve that the approach we have used to create ourmodel can be successfully applied not just to Arabicbut also to other languages such as Turkish, Hungar-ian and Finnish that have highly inflectional mor-phology.
The current accuracy of of our model, get-ting the correct answer among the top two choices96.2% of the time is high enough to be highly use-ful for tasks such as aiding the manual annotationof Arabic text; a more complete automation wouldrequire that accuracy for the single top choice.AcknowledgmentsWe woud like to thank everyone at the Linguis-tic Data Consortium, especially Christopher Cieri,David Graff, Seth Kulick, Ann Bies, Wajdi Za-ghouani and Basma Bouziri for their help.
We alsowish to thank the anonymous reviewers for theircomments and suggestions.ReferencesMeni Adler and Michael Elhadad.
2006.
An Unsuper-vised Morpheme-Based HMM for Hebrew Morpho-logical Disambiguation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the Association forComputational Linguistics.Farag Ahmed and Andreas Nu?rnberger.
2008.
Ara-bic/English Word Translation Disambiguation usingParallel Corpora and Matching Schemes.
In Proceed-ings of EAMT?08, Hamburg, Germany.Tim Buckwalter.
2004.
Buckwalter Arabic Morphologi-cal Analyzer version 2.0.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In Proceedings ofEMNLP?02.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online Passive-Aggressive Algorithms.
Journal of Machine LearningResearch, 7:551?585.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004.Automatic Tagging of Arabic text: From Raw Text toBase Phrase Chunks.
In Proceedings of the 5th Meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics/Human LanguageTechnologies Conference (HLT-NAACL?04).Elad Dinur, Dmitry Davidov, and Ari Rappoport.
2009.Unsupervised Concept Discovery in Hebrew UsingSimple Unsupervised Word Prefix Segmentation forHebrew and Arabic.
In Proceedings of the EACL 2009Workshop on Computational Approaches to SemiticLanguages.Kuzman Ganchev and Georgi Georgiev.
2009.
Edlin:An Easy to Read Linear Learning Framework.
In Pro-ceedings of RANLP?09.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM Can Find Pretty Good HMM POS-Taggers (WhenGiven a Good Start)*.
In Proceedings of ACL?08.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofACL?05, Ann Arbor, MI, USA.Jan Hajic.
2000.
Morphological Tagging: Data vs. Dic-tionaries.
In Proceedings of the 1st Meeting of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL?00).Jun Hatori, Yusuke Miyao, and Jun?ichi Tsujii.
2008.Word Sense Disambiguation for All Words using Tree-Structured Conditional Random Fields.
In Proceed-ings of COLing?08.W.
James and Charles Stein.
1961.
Estimation withQuadratic Loss.
In Proceedings of the Fourth Berkeley734Symposium on Mathematical Statistics and Probabil-ity, Volume 1.Andra?s Kornai.
1994.
On Hungarian morphology (Lin-guistica, Series A: Studia et Dissertationes 14).
Lin-guistics Institute of Hungarian Academy of Sciences,Budapest.Seth Kulick.
2010.
Simultaneous Tokenization and Part-of-Speech Tagging for Arabic without a Morphologi-cal Analyzer.
In Proceedings of ACL?10.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional Random Fields: Proba-bilistic Models for Segmenting and Labeling SequenceData.
In Proceedings of ICML?01, pages 282?289.Mohamed Maamouri, Ann Bies, and Tim Buckwalter.2004.
The Penn Arabic Treebank: Building a LargeScale Annotated Arabic Corpus.
In Proceedings ofNEMLAR Conference on Arabic Language Resourcesand Tools.Mohamed Maamouri, David Graff, Basma Bouziri, Son-dos Krouna, and Seth Kulick.
2009.
LDC StandardArabic Morphological Analyzer (SAMA) v. 3.0.Andrew McCallum, 2001.
MALLET: A Machine Learn-ing for Language Toolkit.
Software available athttp://mallet.cs.umass.edu.Emad Mohamed and Sandra Ku?bler.
2010.
Arabic Partof Speech Tagging.
In Proceedings of LREC?10.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic Morphological Tag-ging, Diacritization, and Lemmatization Using Lex-eme Models and Feature Ranking.
In Proceedings ofACL?08, Columbus, Ohio, USA.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-Based Morphological Disambiguationwith Random Fields*.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP).Kristina Toutanova and Colin Cherry.
2009.
A GlobalModel for Joint Lemmatization and Part-of-SpeechPrediction.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing, pages 486?494.735
