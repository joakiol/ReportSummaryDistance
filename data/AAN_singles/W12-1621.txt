Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 140?149,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsTowards Mediating Shared Perceptual Basis in Situated DialogueChangsong Liu, Rui Fang, Joyce Y. ChaiDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI, 48864{cliu,fangrui,jchai}@cse.msu.eduAbstractTo enable effective referential grounding insituated human robot dialogue, we have con-ducted an empirical study to investigate howconversation partners collaborate and medi-ate shared basis when they have mismatchedvisual perceptual capabilities.
In particu-lar, we have developed a graph-based repre-sentation to capture linguistic discourse andvisual discourse, and applied inexact graphmatching to ground references.
Our empiri-cal results have shown that, even when com-puter vision algorithms produce many errors(e.g.
84.7% of the objects in the environmentare mis-recognized), our approach can stillachieve 66% accuracy in referential ground-ing.
These results demonstrate that, due to itserror-tolerance nature, inexact graph matchingprovides a potential solution to mediate sharedperceptual basis for referential grounding insituated interaction.1 IntroductionTo support natural interaction between a human anda robot, technology enabling human robot dialoguehas become increasingly important.
Human robotdialogue often involves objects and their identitiesin the environment.
One critical problem is inter-pretation and grounding of references - a processto establish mutual understanding between conver-sation partners about intended references (Clark andWilkes-Gibbs, 1986).
The robot needs to identifyreferents in the environment that are specified by itshuman partner and the partner needs to recognizethat the intended referents are correctly understood.It is critical for the robot and its partner to quicklyand reliably reach the mutual acceptance of refer-ences before conversation can move forward.Despite recent progress (Scheutz et al, 2007b;Foster et al, 2008; Skubic et al, 2004; Kruijff et al,2007; Fransen et al, 2007), interpreting and ground-ing references remains a very challenging problem.In situated interaction, although a robot and its hu-man partner are co-present in a shared environment,they have significantly mismatched perceptual capa-bilities (e.g., recognizing objects in the surround-ings).
Their knowledge and representation of theshared world are significantly different.
When ashared perceptual basis is missing, grounding ref-erences to the environment will be difficult (Clark,1996).
Therefore, a foremost question is to under-stand how partners with mismatched perceptual ca-pabilities mediate shared basis to achieve referentialgrounding.To address this problem, we have conducted anempirical study to investigate how conversation part-ners collaborate and mediate shared basis when theyhave mismatched visual perceptual capabilities.
Inparticular, we have developed a graph-based rep-resentation to capture linguistic discourse and vi-sual discourse, and applied inexact graph matchingto ground references.
Our empirical results haveshown that, even when the perception of the envi-ronment by computer vision algorithms has a higherror rate (84.7% of the objects are mis-recognized),our approach can still correctly ground those mis-recognized objects with 66% accuracy.
The resultsdemonstrate that, due to its error-tolerance nature,inexact graph matching provides a potential solu-140tion to mediate shared perceptual basis for referen-tial grounding in situated interaction.In the following sections, we first describe an em-pirical study based on a virtual environment to ex-amine how partners mediate their mismatched visualperceptual basis.
We then provide details about ourgraph matching based approach and its evaluation.2 Related WorkThere has been an increasing number of publishedworks on situated language understanding(Scheutzet al, 2007a; Foster et al, 2008; Skubic et al,2004; Huwel and Wrede, 2006), focusing on inter-pretation of referents in a shared environment.
Dif-ferent approaches have been developed to resolvevisual referents.
Gorniak and Roy present an ap-proach that grounds referring expressions to visualobjects through semantic decomposition, using con-text free grammar that connect linguistic structureswith underlying visual properties (Gorniak and Roy,2004a).
Recently, they have extended this workby including action-affordances (Gorniak and Roy,2007).
This line of work has mainly focused ongrounding words to low-level visual properties.
Toincorporate situational awareness, incremental ap-proaches have been developed to prune interpreta-tions which do not have corresponding visual ref-erents in the environment (Scheutz et al, 2007a;Scheutz et al, 2007b; Brick and Scheutz, 2007).A recent work applies a bidirectional approach toconnect bottom-up incremental language processingto top-down constrains on possible interpretation ofreferents given situation awareness (Kruijff et al,2007).
Most of these previous works address utter-ance level processing.
Here, we are interested in ex-ploring how the mismatched perceptual capabilitiesinfluences the collaborative discourse, and develop-ing a graph-based framework for referential ground-ing with mismatched perceptions.3 Empirical StudyIt is very difficult to study the collaborative pro-cess between partners with mismatched perceptualcapabilities.
Subjects with truly mismatched per-ceptual capabilities are difficult to recruit, and thediscrepancy between capabilities is difficult to mea-sure and control.
The wizard-of-oz studies withFigure 1: Our experimental system.
Two partners collab-orate on an object naming task using this system.
Thedirector on the left side is shown an (synthesized) origi-nal image, while the matcher on the right side is shownan impoverished version of the original image.physical robots (e.g., as in (Green and Severin-son Eklundh, 2001; Shiomi et al, 2007; Kahn et al,2008)) are also insufficient since it is not clear whatshould be the underlying principles to guide the wiz-ard?s decisions and thus the perceived robot?s behav-iors (Steinfeld et al, 2009).
To address these prob-lems, motivated by the Map Task (Anderson et al,1991) and the recent encouraging results from vir-tual simulation in Human Robot Interaction (HRI)studies (Carpin et al, 2007; Chernova et al, 2010),we conducted an empirical study based on virtualsimulations of mismatched perceptual capabilities.3.1 Experimental System and TaskThe setup of our experimental system is shown inFigure 1.
In the experiment, two human partners(a director and a matcher) collaborate on an objectnaming task.
The mismatched perceptual capabili-ties between partners are simulated by different ver-sions of an image shown to them: the director looksat an original image, while the matcher looks at animpoverished version of the original image.The original image (the one on the left in Fig-ure 1) was created by randomly selecting images ofdaily-life items (office supplies, fruits, etc.)
froman image database and randomly positioning themonto a background.
To create the impoverished im-141age (the one on the right in Figure 1), we appliedstandard Computer Vision (CV) algorithms to pro-cess the original image and then create an abstractrepresentation based on the outputs from the CV al-gorithms.More specifically, the original image was fedinto a segmentation ?
feature extraction ?recognition pipeline of CV algorithms.
First, theOTSU algorithm (Otsu, 1975) was used for imagesegmentation.
Then visual features such as colorand shape were extracted from the segmented re-gions (Zhang and Lu, 2002).
Finally, object recogni-tion was done by searching the nearest neighbor (inthe shape-feature vector space) from a knowledgebase of ?known?
objects.
The impoverished imagewas then created based on the CV algorithms?
out-puts.
For example, if an object in the original imagewas recognized as a pear, an abstract illustration ofpear would be displayed in the impoverished imageat the same position.
Other features such like colorand size of the object were also extracted from theoriginal image and assigned to the illustration in theimpoverished image.In the naming task, the director?s goal is to com-municate the ?secret names?
of some randomly se-lected objects (i.e., target objects) in his/her image tothe matcher, so that the matcher would know whichobject has what name.
As shown in Figure 1, thosesecret names are displayed only on the director?sscreen but not the matcher?s.
Once the matcher be-lieves that he/she correctly acquires the name of antarget object, he/she will record the name by mouse-clicking on the target and repeating the name.
Atask is considered complete when the matcher hasrecorded the names of all the target objects.3.2 ExamplesConsistent with previous findings (Liu et al, 2011),our empirical study shows that human partners tendto combine object properties and spatial relations toconstruct their referring expressions.
In addition,our empirical study has further demonstrated howpartners manage to mediate their perceptual basisthrough collaborative discourse.
Here are two ex-amples from our data:Example 1.D1: the very top right hand corner, there is a red appleM: okD: and then to the left of that red apple on the top of thescreen is a red or black cherryM: okD: and then to the left of that is a brown kiwi fruitM: okD: and the, the red cherry is called Richard?
?
?
?
?
?Example 2.D: ok, um, so can we start in the top rightM: alright, um, the top right there are two rows of items,they are all circular or apple shapedD: ok, um, the item in the very top right corner does nothave a nameM: um, no nameM: um, to the left of thatD: yes, to the left of that is RichardM: ok, are there only three items in that rowD: yes, there are only threeM: ok, this is Richard?
?
?
?
?
?As shown in Example 1, the most commonly usedobject properties include object class, color, spatiallocation, and others such as size, length and shape.For the relations, the most common one is the pro-jective spatial relations (Liu et al, 2010), such asright, left, above, below.
Besides, as illustrated byExample 2, descriptions based on grouping of mul-tiple objects are also commonly used.
To mediatetheir shared basis, both the director and the matchermake extra effort to collaborate with each other.
Forinstance, in Example 1, the director applies install-ment (Clark and Wilkes-Gibbs, 1986) where he ut-ters noun phrases in episodes and the matcher ex-plicitly accepts each installment before the directormoves forward.
In Example 2, the matcher intendsto assist the grounding process by proactively pro-viding what he perceives about the environment.The data collected from our empirical study haveindicated that, to mediate a shared perceptual basisand ground references, a successful method shouldconsider the following issues: (1) It needs to capturethe dynamics of the linguistic discourse and iden-tify various relations among different referring ex-pressions throughout discourse.
(2) it needs to rep-resent the perceived visual features and topologicalrelations between visual objects in the visual dis-course.
(3) Because the perceived visual world by1D stands for Director and M for Matcher.142the matcher (who represents the lower-calibre arti-ficial agent) very often differs from the perceivedvisual world by the director (who represents thehigher-calibre human partner), reference resolutionwill need some approximation without enforcing acomplete satisfaction of constraints.
Based on theseconsiderations, we have developed a graph-basedapproach for referential grounding.
Next we givea detailed account on this approach.4 A Graph-based Approach to ReferentialGroundingIn the field of image analysis and pattern recogni-tion, Attributed Relational Graph (ARG) is a veryuseful data structure to represent an image (Tsai andFu, 1979; Sanfeliu and Fu, 1983).
In an ARG, theunderlying unlabeled graph represents the topologi-cal structure of the scene.
Then each node and edgeare labeled with a vector of attributes that representslocal features of a single node or the topological fea-tures between two nodes.
Based on the ARG rep-resentations, an inexact graph matching is to finda graph or a subgraph whose error-transformationcost with the already given graph is minimum (Es-hera and Fu, 1984).Motivated by the representation power of ARGand the error-correcting capability of inexact graphmatching, we developed a graph-based approach toaddress the referential grounding problem.
ARGand probabilistic graph matching have been pre-viously applied in multimodal reference resolu-tion (Chai et al, 2004a; Chai et al, 2004b) by in-tegrating speech and gestures.
Here, although weuse similar ARG representations, our algorithm isbased on inexact graph matching and our focus is onmediating shared perceptual basis.4.1 Graph RepresentationsFigure 2 illustrates the key elements and the processof our graph-based method.
The key elements of ourmethod are two ARG representations, one of whichis called the discourse graph and the other called thevision graph.The discourse graph captures the information ex-tracted from the linguistic discourse.2 To create thediscourse graph, the linguistic discourse first needs2Currently we only focus on the utterances from the director.Figure 2: An illustration of graph representations in ourmethod.
The discourse graph is created from formal se-mantic representations of the linguistic discourse; The vi-sion graph is created by applying CV algorithms on thecorresponding scene.
Given the two graphs, referentialgrounding is to construct a node-to-node mapping fromthe discourse graph to the vision graph.to be processed by NLP components, such as the se-mantic composition and discourse coreference res-olution components.
The output of the NLP com-ponents are usually in the form of some formal se-mantic representations, e.g.
in the form of first-orderlogic representations.
The discourse graph is thencreated based on the formal semantics, i.e.
eachnew discourse entity corresponds to a node in thegraph, one-arity predicates correspond to node at-tributes and two-arity predicates correspond to edgeattributes.
The vision graph, on the other hand, is arepresentation of the visual features extracted fromthe scene.
Each object detected by CV algorithmsis represented as a node in the vision graph, and theattributes of the node correspond to visual features,such as the color, size and position of the object.
Theedges between nodes represent their relations in thephysical space.Given the discourse graph and the vision graph,now we can formulate referential grounding as con-structing a node-to-node mapping from the dis-course graph to the vision graph, or in other words,a matching between the two graphs.
Note that, the143matching we encounter here is different from theoriginal graph matching problem that is often usedin the image analysis field.
The original version onlyconsiders matching between two graphs that havethe same type of values for each attribute.
But inthe case of referential grounding, all the attributes inthe discourse graph possess symbolic values sincethey come from formal semantic representations,whereas the attributes in the vision graph are oftennumeric values produced by CV algorithms.
Our so-lution is to introduce a set of symbol grounding func-tions, which bridges the heterogeneous attributes ofthe two graphs and makes general graph matchingalgorithms applicable to referential grounding.4.2 Inexact Graph MatchingWe formulate referential grounding as a graphmatching problem, which has extended the origi-nal graph matching approach used in image process-ing and pattern recognition filed (Tsai and Fu, 1979;Tsai and Fu, 1983; Eshera and Fu, 1984).First, we give the formal definition of an ARG,which is a doublet of the formG = (N,E)whereN The set of attributed-nodes of graph G,defined asN = {(i, a) |1 ?
i ?
|N | } .E The set of directed attributed-edges ofgraph G, defined asE = {(i, j, e) |1 ?
i, j ?
|N | } .
(i, a) ?
N Node i with a as its attribute vector,where a = [v1, v2, ?
?
?
, vK ] is a vectorof K attributes.
To simplify the nota-tion, We will denote a node as ai.
(i, j, e) ?
E The directed edge from node i to nodej with e as its attribute vector, wheree = [u1, u2, ?
?
?
, uL] is a vector of Lattributes.
We will denote an edge aseij .In an ARG, the value of a node/edge attributevk/ul can be symbolic, numeric, or as a vector ofnumeric values.
For example, if v1 is used to rep-resent the color feature of an object, then a possibleassignment could be v1 = [255, 0, 0], which is thergb color vector.Suppose we represent referring expressions fromthe linguistic discourse as a discourse graph G andobjects perceived from the environment as a vi-sion graph G?, referential grounding then becomesa graph matching problem: given G = (N,E) andG?
= (N ?, E?
), in whichN = {ai |1 ?
i ?
I } , E = {ei1i2 |1 ?
i1, i2 ?
I }N ?
= {aj ?
|1 ?
j ?
J } , E?
= {e?j1j2 |1 ?
j1, j2 ?
J }A matching between G and G?
is to find a one-to-one mapping between the nodes in N and the nodesin N ?.Note that it is not necessary for every node inN or N ?
to be mapped to a corresponding node inthe other graph.
If a node is not to be mapped toany node in the other graph, we describe it as be-ing mapped to ?, which denotes an abstract ?null?node.
To represent the matching result, we re-orderN and N ?
such that the first I ?/J ?
nodes in N /N ?
arethose which have been mapped to their correspond-ing nodes in the other graph, and the nodes afterthem are the unmatched nodes, i.e.
those matchedwith ?.
Then the matching result isM = M1 ?M2 ?M3= {(i, j) |1 ?
i ?
I ?, 1 ?
j ?
J ?
}?
{i |I ?
< i ?
I }?
{j |J ?
< j ?
J }Here M1 is a set of I ?
pairs of indices of matchednodes.
M2 and M3 are the sets of indices of all theunmatched nodes in N and N ?, respectively.
ThenM is what we call a matching between G and G?.It is an inexact matching in the sense that we allowbothG andG?
to have a subset of nodes, i.e.
M2 andM3, that are not matched with any node in the othergraph (Conte et al, 2004).
The cost of a matchingM is then defined asC (M) = C (M1) + C (M2) + C (M3)To complete the definition of C (M), we use M11to denote the set of all the first indices of the matchedpairs in M1, i.e.
M11 = {i |1 ?
i ?
I ?
}, and H =(NH , EH) is the subgraph of G that is induced bythe subset of nodes NH = {ai |i ?M11 }, then wehaveC (M1) =?ai?NHCN (ai, a?j) +?ei1i2?EHCE (ei1i2 , e?j1j2)C (M2) =?ai?
(N?NH)CN (ai,?)
+?ei1i2?
(E?EH)CE (ei1i2 ,?
)144in which CN (ai, a?j) is the cost of mapping aito a?j , CE (ei1i2 , e?j1j2) is the cost of mappingei1i2 to e?j1j2 , and CN (ai,?
)/CE (ei1i2 ,?)
is thecost of mapping ai/ei1i2 to the null node/edge.They are also called node/edge substitution cost andnode/edge insertion cost, respectively (Eshera andFu, 1984).
Note that, in our case we let C (M3) = 0since we have assumed that the size of G?
is biggerthan the size of G.Finally, the optimal matching between G and G?is the one with the minimum matching costM?
= arg minMC (M)which gives us the most feasible result of groundingthe entities in the discourse graph with the objects inthe vision graph.Given our formulation of referential grounding asa graph matching problem, the next question is howto find the optimal matching between two graphs.Unfortunately, such a problem belongs to the classof NP-complete (Conte et al, 2004).
In practice,techniques such as A?
search are commonly used toimprove the efficiency, e.g.
in (Tsai and Fu, 1979;Tsai and Fu, 1983).
But the memory requirementcan still be considerably large if the heuristic doesnot provide a close estimate of the future matchingcost (Conte et al, 2004).
In our current approach, weuse a simple beam search algorithm (Zhang, 1999)to retain the tractability.
Following the assumptionin (Eshera and Fu, 1984), we set the beam size ashJ2, where h is the current level of the search treeand J is the size of the bigger graph (in our caseG?
).4.3 Symbol Grounding FunctionsAs mentioned in Section 4.1, in referential ground-ing the discourse graph and the vision graph pos-sess different types of attribute values, therefore weintroduce a set of ?symbol grounding functions?,based on which node/edge substitution and insertioncosts can be formally defined.We start with node substitution cost to give a for-mal definition of symbol grounding functions.
Asdefined in the previous section, the node substitu-tion cost of mapping (substituting) node a with nodea?
is3CN (a, a?
)3For the ease of notation we have dropped the subscript of anode.Recall that in our definition of ARG, each nodeis represented by a vector of attributes, i.e.
a =[v1, v2, ?
?
?
, vK ] and a?
= [v?1, v?2, ?
?
?
, v?K ].
Thus,we define the node substitution cost asCN (a, a?)
=K?k=1?
ln fk (vk, v?k)in which fk (vk, v?k) = p (p ?
[0, 1]) is what we callthe symbol grounding function for the k-th attribute.More specifically, a symbol grounding functionfor the k-th attribute takes two input arguments,namely vk and v?k, which are the values of the k-thattribute from node a and a?
respectively.
The out-put of the function is a real number p in the rangeof [0, 1], which can be interpreted as a measurementof the compatibility between a symbol (or word) vkand a visual feature value v?k.Let L = {w1, w2, ?
?
?
, wZ ,UNK} be the set of allpossible symbolic values of vk, then fk (vk, v?k) canbe further decomposed asfk (vk, v?k) =????????????
?fk1 (v?k) if vk = w1;fk2 (v?k) if vk = w2;... ...fkZ (v?k) if vk = wZ ;?k if vk = UNK.Here the idea is that each value of vk may specify anunique function that determines the compatibility ofa visual feature value v?k.
For example, suppose thatwe are defining a symbol grounding function for theattribute of ?spatial location?, i.e.
where is an ob-ject located in the environment.
The symbolic valuev can be in the set of {Top,Bottom, ?
?
?
,UNK}, andthe visual feature value v?
is the x and y coordinates(in pixels) of the object?s center of mass in the im-age.
A grounding function for the symbol Top canbe defined as4fTop (v?)
= fTop (x, y) ={1?
y800 if y < 400;0 otherwise.Note that we have added a special symbol UNKto represent the ?unknown?
(or ?unspecified?)
valueof vk.
When the value of an attribute in the dis-course graph is unknown, i.e.
the speaker did notmention anything about a particular property, thegrounding function will simply return a predefined4Assume that the size of the image is 800?
800 pixels andthe left-top corner is the origin (0, 0)145Type of Error Numberof ObjectsNo Error 9 (5.1%)Recognition Error 150 (84.7%)Segmentation Error 18 (10.2%)Total 177Table 1: Types of errors among all the target (named)objects.
Recognition error: an object is incorrectly rec-ognized as another type of object, or an unknown type.Segmentation error: an object is missing, or merged withanother object.constant, which we denote as ?.
The node insertioncost CN (a,?)
is now defined as5CN (a,?)
=K?k=1?
ln ?kCurrently we set al the symbol grounding func-tions?
outputs for the unknown value (i.e.
the ?s) to?, which is an arbitrarily small real number (?
> 0).5 Empirical ResultsThree pairs of subjects participated in our experi-ment.
Each pair (one acted as the director and theother as the matcher) completed the naming task on8 randomly created images.
In total we collected 24dialogues with 177 target objects to be named.
Table1 summarizes the errors made by the CV algorithmswhen the 177 named objects from the original im-ages were processed and represented in the impover-ished images, as described in Section 3.1.
As shownin the table, only 5% of the objects were correctlyrepresented in the impoverished images.
The other95% of objects were either mis-recognized (about85%) or mis-segmented (10%).The evaluation of our approach is based onwhether the target objects are correctly groundedby the graph matching method.
To focus our cur-rent effort on the referential grounding aspect, weignored all the matchers?
contributions to the dia-logues.
Thus the discourse graphs were built basedon only the director?s utterances.
The formal se-mantics of each of the director?s valid utteranceswas manually annotated using the DRS (DiscourseRepresentation Structure) representation (Bird et al,2009).
The discourse graphs were then generated5The edge substitution/insertion cost is defined in the sameway as the node substitution/insertion cost.Accuracy/Detection RateType of Error Object-properties Object-propertiesOnly and RelationsNo Error 66.7% (6/9) 77.8% (7/9)Recognition Error 38.7% (58/150) 66% (99/150)Segmentation Error 33.3% (6/18) 44.4% (8/18)Overall 39.5% (70/177) 64.4% (114/177)Table 2: Referential grounding performance of ourmethod.
The accuracy/detection rates in the table wereobtained by comparing the results with annotated groundtruths.from the annotated formal semantics.
The visiongraphs were generated from the outputs of the CValgorithms.
The graph matching method was thenapplied to return a (sub-) optimal matching betweenthe two graphs.Table 2 shows the referential grounding perfor-mance of our method.
To better understand the ad-vantages of the graph-based approach, we have com-pared two settings.
In the first setting, only theobject-specific properties are considered for com-puting the comparability between a linguistic ex-pression and a visual object, and the relations be-tween objects are ignored.
This setting is similarto the baseline approach used in (Prasov and Chai,2008; Prasov and Chai, 2010).
In the second set-ting, the complete graph-based approach is applied,i.e.
both the object?s properties and the relations be-tween objects are considered.
As shown in Table 2,although the improvements of performance for theno-error objects and mis-segmented objects are notsignificant due to the small sample sizes, the perfor-mance for the mis-recognized objects is significantlyimproved by 27.3% (p < .001).
The improvementfor the overall performance is also significant (by24.9%, p < .001).
The comparison between twosettings have demonstrated the importance of rep-resenting and reasoning on relations between ob-jects in referential grounding, and the graph-basedapproach provides an ideal solution to capture rela-tions.In particular, even CV error rate is high (due to thesimple CV algorithms we used), our method is stillable to achieve 66% accuracy of grounding the mis-recognized objects.
Furthermore, when a referredobject is completely ?missing?
in the vision graph146due to segmentation error6, our method is capableto detect such discrepancy between linguistic inputand visual perception.
The results have shown that44.4% of those cases have been correctly detected.This is also a very important aspect since informa-tion about failures of grounding will allow the di-alogue manager and/or the vision system to adaptbetter strategies.6 DiscussionsThe work presented here only represents an initialstep in our on-going investigation towards mediat-ing shared perceptual basis in situated dialogue.
Itconsists of several simplifications which will be ad-dressed in our future work.First, the discourse graph is created only basedon contributions from the director, using manual an-notations of formal semantics of the discourse.
Asshown in the examples (Section 3.2), the collabora-tive discourse has rich dynamics reflecting partici-pants?
collaborative behaviors.
So our future workis to model these different discourse dynamics andtake them into account in the creation of the dis-course graph.
The discourse graph will be createdafter each contribution as the conversation unfolds.When utterances are automatically processed, se-mantics of these utterances often will not be ex-tracted correctly or completely as in their manualannotations.
Therefore, our future work will alsoexplore how to efficiently match hypothesized dis-course graphs (from automated semantic process-ing) with vision graphs.Second, our current symbol grounding functionsare very simple and intuitive.
Our future work willexplore more sophisticated models that have theoret-ical motivations (e.g., grounding spatial terms basedon the Attentional Vector Sum (AVS) model (Regierand Carlson, 2001)) and enable automated acquisi-tion of these functions (Roy, 2002; Gorniak and Roy,2004b).
In addition, we will explore context-basedsymbol grounding functions where context will beexplicitly modeled.
Grounding a linguistic term to avisual feature will be influenced by contextual fac-tors such as surroundings of the environment, the6For example, if the director refers to ?a white ball?
butCV algorithm fails to detect that object from the environment,then the node in the discourse graph representing ?a white ball?should not be mapped to anything in the vision graph.discourse history, the speaker?s individual prefer-ence, and so on.Lastly, as shown in our examples, the matcheralso contributes significantly to ground references.This appears to suggest that, in situated dialogue,lower-calibre partners (i.e., robot, and here thematcher) also make extra effort to ground refer-ences.
The underlying motivation could be theirurge to match what they perceive from the environ-ment to what they are told by their higher-calibrepartners (i.e., human).
This motivation can be poten-tially modeled as graph-matching and can be usedto guide the design of system responses.
We willexplore this idea in the future.7 ConclusionIn situated human robot dialogue, a robot and itshuman partners have significantly mismatched capa-bilities in perceiving the environment, which makesgrounding of references in the environment espe-cially difficult.
To address this challenge, this paperdescribes an empirical study investigating how hu-man partners mediate the mismatched perceptual ba-sis.
Based on this data, we developed a graph-basedapproach and formulate referential grounding as in-exact graph matching.
Although our current investi-gation has several simplifications, our initial empiri-cal results have shown the potential of this approachin mediating shared perceptual basis in situated dia-logue.AcknowledgmentsThis work was supported by Award #1050004 andAward #0957039 from National Science Founda-tion and Award #N00014-11-1-0410 from Office ofNaval Research.ReferencesA.H.
Anderson, M. Bader, E.G.
Bard, E. Boyle, G. Do-herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,J.
Miller, et al 1991.
The hcrc map task corpus.
Lan-guage and speech, 34(4):351?366.S.
Bird, E. Klein, and E. Loper.
2009.
Natural languageprocessing with Python.
O?Reilly Media.T.
Brick and M. Scheutz.
2007.
Incremental naturallanguage processing for hri.
In Proceeding of the147ACM/IEEE international conference on Human-RobotInteraction (HRI-07), pages 263?270.S.
Carpin, M. Lewis, J. Wang, S. Balakirsky, andC.
Scrapper.
2007.
USARSim: a robot simulator forresearch and education.
In Proceedings of the 2007IEEE Conference on Robotics and Automation.J.Y.
Chai, P. Hong, and M.X.
Zhou.
2004a.
A proba-bilistic approach to reference resolution in multimodaluser interfaces.
In Proceedings of the 9th internationalconference on Intelligent user interfaces, pages 70?77.ACM.J.Y.
Chai, P. Hong, M.X.
Zhou, and Z. Prasov.
2004b.Optimization in multimodal interpretation.
In Pro-ceedings of the 42nd Annual Meeting on Associationfor Computational Linguistics, page 1.
Association forComputational Linguistics.S.
Chernova, J. Orkin, and C. Breazeal.
2010.
Crowd-sourcing hri through online multiplayer games.
AAAISymposium on Dialogue with Robots.H.
H. Clark and D. Wilkes-Gibbs.
1986.
Referring as acollaborative process.
In Cognition, number 22, pages1?39.H.
H. Clark.
1996.
Using language.
Cambridge Univer-sity Press, Cambridge, UK.D.
Conte, P. Foggia, C. Sansone, and M. Vento.
2004.Thirty years of graph matching in pattern recognition.International journal of pattern recognition and artifi-cial intelligence, 18(3):265?298.M.
A. Eshera and K. S. Fu.
1984.
A graph distance mea-sure for image analysis.
IEEE transactions on systems,man, and cybernetics, 14(3):398?410.M.E.
Foster, E.G.
Bard, R.L.
Hill, M. Guhe, J. Oberlan-der, and A. Knoll.
2008.
Generating haptic- osten-sive referring expressions in cooperative, task-basedhuman-robot dialogue.
Proceedings of ACM/IEEEHuman-Robot Interaction.B.
Fransen, V. Morariu, E. Martinson, S. Blis-ard, M. Marge, S. Thomas, A. Schultz, andD.
Perzanowski.
2007.
Using vision, acoustics, andnatural language for disambiguation.
In Proceedingsof HRI07, pages 73?80.P.
Gorniak and D. Roy.
2004a.
Grounded semantic com-position for visual scenes.
In Journal of Artificial In-telligence Research, volume 21, pages 429?470.P.
Gorniak and D. Roy.
2004b.
Grounded semantic com-position for visual scenes.
J. Artif.
Intell.
Res.
(JAIR),21:429?470.P.
Gorniak and D. Roy.
2007.
Situated language under-standing as filtering perceived affordances.
In Cogni-tive Science, volume 31(2), pages 197?231.A.
Green and K. Severinson Eklundh.
2001.
Task-oriented dialogue for CERO: a user centered ap-proach.
In Proceedings of 10th IEEE internationalworkshop on robot and human interactive communi-cation, September.Sonja Huwel and Britta Wrede.
2006.
Situatedspeech understanding for robust multi-modal human-robot communication.
In Proceedings of the In-ternational Conference on Computational Linguistics(COLING/ACL).P.
Kahn, N. Greier, T. Kanda, H. Ishiguro, J. Ruckert,R.
Severson, and S. Kane.
2008.
Design patterns forsociality in human-robot interaction.
In Proceedingsof HRI, pages 97?104.Geert-Jan M. Kruijff, Pierre Lison, Trevor Benjamin,Henrik Jacobsson, and Nick Hawes.
2007.
Incremen-tal, multi-level processing for comprehending situateddialogue in human-robot interaction.
In Symposium onLanguage and Robots.C.
Liu, J. Walker, and J.Y.
Chai.
2010.
Disambiguatingframes of reference for spatial language understandingin situated dialogue.
In AAAI Fall Symposium on Dia-logue with Robots.C.
Liu, D. Kay, and J.Y.
Chai.
2011.
Awareness of part-ners eye gaze in situated referential grounding: An em-pirical study.
In 2nd Workshop on Eye Gaze in Intelli-gent Human Machine Interaction.N.
Otsu.
1975.
A threshold selection method from gray-level histograms.
Automatica, 11:285?296.Z.
Prasov and J.Y.
Chai.
2008.
What?s in a gaze?
: therole of eye-gaze in reference resolution in multimodalconversational interfaces.
In Proceedings of the 13thinternational conference on Intelligent user interfaces,pages 20?29.
ACM.Z.
Prasov and J.Y.
Chai.
2010.
Fusing eye gaze withspeech recognition hypotheses to resolve exophoricreferences in situated dialogue.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 471?481.
Associationfor Computational Linguistics.T.
Regier and L.A. Carlson.
2001.
Grounding spatial lan-guage in perception: an empirical and computationalinvestigation.
Journal of Experimental Psychology:General, 130(2):273.D.K.
Roy.
2002.
Learning visually grounded words andsyntax for a scene description task.
Computer Speech& Language, 16(3):353?385.A.
Sanfeliu and K. S. Fu.
1983.
A distance measurebetween attributed relational graphs for pattern recog-nition.
IEEE transactions on systems, man, and cyber-netics, 13(3):353?362.M.
Scheutz, P. Schermerhorn, J. Kramer, and D. Ander-son.
2007a.
First steps toward natural human-likeHRI.
In Autonomous Robots, volume 22.M.
Scheutz, P. Schermerhorn, J. Kramer, and D. Ander-son.
2007b.
Incremental natural language processingfor hri.
In Proceedings of HRI.148M.
Shiomi, T. Kanda, S. Koizumi, H. Ishiguro, andN.
hagita.
2007.
Group attention control for commu-nication robots with wizard of OZ approach.
In Pro-ceedings of HRI, pages 121?128.M.
Skubic, D. Perzanowski, S. Blisard, A. Schultz,W.
Adams, M. Bugajska, and D. Brock.
2004.
Spatiallanguage for human-robot dialogs.
IEEE Transactionson Systems, Man and Cybernetics, Part C, 34(2):154?167.A.
Steinfeld, O. C. Jenkins, and B. Scassellati.
2009.
Theoz of wizard: Simulating the human for interaction re-search.
In Proceedings of HRI, pages 101?107.W.H.
Tsai and K.S.
Fu.
1979.
Error-correcting isomor-phisms of attributed relational graphs for pattern anal-ysis.
Systems, Man and Cybernetics, IEEE Transac-tions on, 9(12):757?768.W.H.
Tsai and K.S.
Fu.
1983.
Subgraph error-correctingisomorphisms for syntactic pattern.
year: 1983,13:48?62.D.
Zhang and G. Lu.
2002.
An integrated approach toshape based image retrieval.
In Proc.
of 5th Asian con-ference on computer vision (ACCV), pages 652?657.W.
Zhang.
1999.
State-space search: Algorithms, com-plexity, extensions, and applications.
Springer-VerlagNew York Inc.149
