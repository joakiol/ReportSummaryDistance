Proceedings of the AHA!
Workshop on Information Discovery in Text, pages 1?6,Dublin, Ireland, August 23 2014.Application-Driven Relation Extraction with Limited Distant SupervisionAndreas VlachosComputer Science DepartmentUniversity College Londona.vlachos@cs.ucl.ac.ukStephen ClarkComputer LaboratoryUniversity of Cambridgesc609@cam.ac.ukAbstractRecent approaches to relation extraction following the distant supervision paradigm have focusedon exploiting large knowledge bases, from which they extract substantial amount of supervision.However, for many relations in real-world applications, there are few instances available to seedthe relation extraction process, and appropriate named entity recognizers which are necessary forpre-processing do not exist.
To overcome this issue, we learn entity filters jointly with relationextraction using imitation learning.
We evaluate our approach on architect names and buildingcompletion years, using only around 30 seed instances for each relation and show that the jointlylearned entity filters improved the performance by 30 and 7 points in average precision.1 IntroductionIn this paper we focus on relation extraction in the context of a real-world application.
The applicationis a dialog-based city tour guide, based in Edinburgh.
One of the features of the system is its pro-activenature, offering information which may be of interest to the user.
In order to be pro-active in this way,as well as answer users?
questions, the system requires a large amount of knowledge about the city.
Partof that knowledge is stored in a database, which is time-consuming and difficult to populate manually.Hence, we have explored the use of an automatic knowledge base population technique based on distantsupervision (Craven and Kumlien, 1999; Mintz et al., 2009).The attraction of this approach is that the only input required is a list of seed instances of the relation inquestion and a corpus of sentences expressing new instances of that relation.
However, existing studiestypically assume a large seed set, whereas in our application such sets are often not readily available, e.g.Mintz et al.
(2009) reported using 7K-140K seed instances per relation as input.
In this paper, the tworelations that we evaluate on are architect name and completion year of buildings.
These were chosenbecause they are highly relevant to our application, but also somewhat non-standard compared to theexisting literature; and crucially they do not come with a readily-available set of seed instances.Furthermore, previous approaches typically assume named entity recognition (NER) as a pre-processing step in order to construct the training and testing instances.
However, since these tools arenot tailored to the relations of interest, they introduce spurious entity matches that are harmful to per-formance as shown by Ling and Weld (2012) and Zhang et al.
(2013).
These authors ameliorated thisissue by learning fine-grained entity recognizers and filters using supervised learning.
The labeled dataused was extracted from the anchor text of entity mentions annotated in Wikipedia, however this is notpossible for entities not annotated in this resource.In this work, instead of relying on labeled data to construct entity filters, we learn them jointly with therelation extraction component.
For this purpose we use the imitation learning algorithm DAGGER (Rosset al., 2011), which can handle the dependencies between actions taken in a sequence, and use supervisionfor later actions to learn how to take actions earlier in the sequence.
We evaluate our approach usingaround 30 seed instances per relation and show that the jointly learned entity filters result in gains of 7and 30 points in average precision for the completion year and the architect name relations respectively.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1relation keywords: building, architectquestion answerAdvocates?
Library William PlayfairBute House Robert AdamDunstane House ?Craigiehall ?sentencesThe Advocates?
Library is currently located in a William Playfair-designed building.Bute House is unusual in Robert Adam?s design for Charlotte Squarein having a central front door.Dunstane House in Edinburgh was built in 1852 to the design ofarchitect William Playfair.The 16-room Dunstane House was originally built by the Ross familyas their private home in 1852.Dunstane House was designed by famous architect William Playfair.Craigiehall is a late-17th-century country house, which now serves asthe headquarters of the Second Division of the British Army.label question candidate sentencetraining instances+ Advocates?
Library William Playfair The Advocates?
Library.
.
.+ Bute House Robert Adam Bute House is unusual.
.
.- Bute House Charlotte Square Bute House is unusual.
.
.predicted instances- Dunstane House Edinburgh Dunstane House in.
.
.+ Dunstane House William Playfair Dunstane House in.
.
.+ Dunstane House Ross The 16-room Dunstane.
.
.+ Dunstane House William Playfair Dunstane House was.
.
.- Craigiehall Second Division Craigiehall is a .
.
.- Craigiehall British Army Craigiehall is a. .
.entity filterrelation extractorquestion answer scoreDunstane House William Playfair 2Ross 1CraigiehallWEBDISTANT SUPERVISIONTRAINPREDICTOUTPUTFigure 1: The stages of our proposed approach applied to the architect name relation.2 Approach overviewWe will use the architect-building relation as an example to give an overview of our approach, as shownin Figure 1.
The input to the system is a list of buildings, where for some we know the architect (theseeds), and the task is to find the architects for the remainder.
One difference with the standard setup forrelation extraction using distant supervision is that we assume a list of historical buildings instead of atailored NER system.
This is reasonable for the example, since such a list is relatively easy to acquire.In order to create training data, queries containing words from the seeds are sent to a search engine.Sentences from the returned pages are then processed to find examples which contain mentions of botha building and the corresponding architect.
Applying the distant supervision hypothesis, we assume thatsuch sentences are indeed expressing the desired relation, and these are positive examples.
While suchdata contains noise, it has been shown to be useful in practice (Yao et al., 2010; Hoffmann et al., 2011).At test time the input is the name of a historical building.
Now the web is searched to find examplesentences containing this name, and the classifier is applied to each sentence, returning either the nameof the architect, or none.
Note that different sentences could provide evidence for different architects;hence assuming only one architect for each building, a procedure is required to decide between thepossible answers (see Sec.
5).3 Entity Filtering for Relation ExtractionEach relation extraction instance consists of a sentence containing a question entity (e.g.
Bute House)and a candidate answer (e.g.
Robert Adam), and the task is to predict whether the answer and questionentity have the relation of interest.
The standard approach is to learn a binary classifier (possibly as partof a more complex model e.g.
Hoffmann et al.
(2011)) using features that describe each entity as wellas the lexico-syntactic relation between them in the sentence.
These commonly include the lexicalizeddependency path from the question entity to the candidate answer, as well as the lemmas on this path.
Inthis setup, NER assists by filtering the instances generated to those that contain appropriate recognizedentities and by providing features for them.However, since we do not assume NER in pre-processing, this task becomes harder in our setup,since the candidate answers are very often inappropriate for the relation at question.
A simple way2Algorithm 1: Learning with DAGGERInput: training set S, loss `, CSC learner CSCLOutput: Learned policy HN1 CSC Examples E = ?2 for i = 1 to N do3 for s in S do4 Predict y?1:T= Hi?1(s)5 for y?tin pi(s) do6 Extract features ?t= f(s, y?1:t?1)7 foreach possible action yjtdo8 Predict y?t+1:T= Hi?1(s; y?1:t?1, yjt)9 Assess cjt= `(y?1:t?1, yjt, y?t+1:T)10 Add (?t, ct) to E11 Learn Hi= CSCL(E)to incorporate NER-like information is to add the features that would have been used for NER to therelation extraction features and learn a classifier as above.
Such features are commonly extracted fromthe candidate answer itself as well as its context.
The former include the tokens of the answer, theirlemmas, whether the answer is capitalised, etc.
The latter include the words and bigrams precedingand following the answer, as well as syntactic dependencies between the words denoting the entity andsurrounding lemmas.However, while these features are likely to be useful, they also render learning relation extractionharder because they are not directly relevant to the task.
For example, the features describing the firsttraining instance of Fig.
1 would include that the token Playfair is part of the candidate answer and thatthe lemma design is part on the syntactic dependency path between the architect and the building, butonly the latter is crucial for the correct classification of this instance.
Thus, including the NER featuresabout the candidate answer can be misleading, especially since they tend to be less sparse than the relationextraction ones.Therefore we split the prediction into two binary classification stages: the first stage predicts whetherthe candidate answer is appropriate for the relation (entity filtering), and the second one whether thesentence expresses the relation between the answer and the question entity (relation extraction).
If theprediction for the first stage is negative, then the second stage is not reached.
However, we do not havelabels to train a classifier for the entity filtering stage since if an instance is negative this could be eitherdue to the candidate answer or to the relation expressed in the sentence.
We discuss how we overcomethis issue using the algorithm DAGGER (Ross et al., 2011) next.4 Imitation learning with DAGGERImitation learning algorithms such as DAGGER and SEARN (Daum?e III et al., 2009) have been appliedsuccessfully to a variety of structured prediction tasks (Vlachos, 2012; He et al., 2013) due to theirflexibility in incorporating features.
In this work we focus on the parameter-free version of DAGGERand highlight its ability to handle missing labels in the training data.
During training, DAGGER convertsthe problem of learning how to predict sequences of actions into cost sensitive classification (CSC)learning.
The dependencies between the actions are learned by appropriate generation of CSC examples.In our case, each instance is predicted by a sequence of two actions: an entity filtering action followed (ifpositive) by a relation extraction action.
The output is a learned policy, consisting of the binary classifiersfor entity filtering and relation extraction.Following Alg.
1, in each iteration DAGGER generates training examples using the previous learnedpolicy Hi?1to predict the instances (line 4).
For each action taken, the cost for each possible action isestimated by assuming that the action was taken; then the following actions for that instance are predicted3Recall-top Precision-top F-score-top Recall-all Precision-all F-score-allBase 0.28 0.28 0.28 0.9 0.1 0.181stage 0.52 0.71 0.6 0.67 0.68 0.6752stage 0.5 0.68 0.58 0.67 0.67 0.67Base 0.0 0.0 0.0 0.62 0.002 0.0041stage 0.15 0.26 0.19 0.23 0.17 0.22stage 0.26 0.65 0.37 0.3 0.55 0.39Table 1: Test set results for the 3 systems on year completed (top) and architect name (bottom).using Hi?1(line 8); and the complete sequence of actions is compared against the correct output usingthe loss function (line 9).
Since the latter is only applied to complete sequences, it does not need todecompose over individual actions.
We define the loss to be 0 when the relation extraction stage iscorrect and 1 otherwise.
Therefore we do not need to know the labels for entity filtering, but we learn aclassifier for it so that the relation extraction predictions are correct.
Finally, the CSC training examplesgenerated are added (line 10) and a new policy is learnt (line 11).Since the losses are either 0 or 1, the CSC learning task is equivalent to ordinary classification learning.To learn the binary classifiers for each stage we implemented the adaptive regularization of weights(AROW) algorithm (Crammer et al., 2009) which scales to large datasets and handles sparse feature setsby adjusting the learning rate for each feature.
In the first iteration, we do not have a learned policy, thuswe assume a naive entity filter that accepts all candidate answers and a relation extractor that predicts thecorrect label.5 ExperimentsThe relations used for evaluation are building-architect and building-completion year, for the reasonsgiven in Sec.
1.
For each of the 138 listed historical buildings in Wikipedia,1we found the correctanswers, resulting in 60 building-completion year and 68 building-architect pairs.
We split the data intotwo equal parts for training/development and testing.
We then collected relevant web pages queryingthe web as described in Sec.
2.
The queries were submitted to Bing via its Search API and the top300 results for each query were obtained.
We downloaded the corresponding pages and extracted theirtextual content with BoilerPipe (Kohlsch?utter et al., 2010).
We then processed the texts using the StanfordCoreNLP toolkit.2We tried to match the question entity with tokens in each of the sentences, allowingfor minor differences in tokenization, whitespace and capitalization.
If a sentence contained the questionentity and a candidate answer, we parsed it using the Klein and Manning (2002) parser.
The instancesgenerated were labeled using the distant supervision assumption, resulting in 974K and 4.5M labeledinstances for the completion year and the architect relation, respectively.We ran experiments with three systems; the jointly learned entity filtering-relation extraction approachusing imitation learning (henceforth 2stage), the one-stage classification approach using the features forboth entity filtering and relation extraction (henceforth 1stage), and a baseline that for each questionentity returns all candidate answers for the relation ranked by the number of times they appeared withthe question entity and ignoring all other information (henceforth Base).
Following four-fold cross-validations experiment on the development data, we used 12 iterations for learning with DAGGER.Each system returns a list of answers ranked according to the number of instances classified as positivefor that answer.
We used two evaluation modes.
The first considers only the top-ranked answer (top),whereas the second considers all answers returned until either the correct one is found or they are ex-hausted (all).
In all we define recall as the number of correct answers over the total number of questionentities, and precision as the chance of finding the correct answer while traversing those returned.Results by all models are reported for both relations in Table 1.
A first observation is that the architectname relation is substantially harder to extract since all models achieve worse scores than for the com-pletion year relation.
More specifically, Base achieves respectable scores in top mode in completion yearextraction, but it fails completely in architect name.
This is due to the existence of many other names1http://en.wikipedia.org/wiki/Category:Listed_buildings_in_Edinburgh2http://nlp.stanford.edu/software/corenlp.shtml400.20.40.60.810  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9Precision-allRecall-all1stage2stage00.20.40.60.810  0.1  0.2  0.3  0.4  0.5  0.6  0.7Precision-allRecall-all1stage2stageTable 2: Test set precision-recall curves in all mode for year completed (left) and architect name (right).that appear more frequently together with a building than that of its architect, while the completion yearis sometimes the number most frequently mentioned in the same sentence with the building.
In addition,Base achieves the maximum possible all recall by construction, since if there is a sentence containing thecorrect answer for a question entity it will be returned.
However this comes at a cost of low precision.Both the machine-learned models improve upon Base substantially on both datasets, with the 2stagemodel being substantially better in architect name extraction, especially in terms of precision.
In comple-tion year extraction the differences are smaller, with 1stage being slightly better.
These small differencesare expected since recognizing completion years is much easier than recognizing architect names, thuslearning a separate entity filtering model for them is less likely to be useful.
Nevertheless, inspectingthe weights learned by the 2stage model showed that some useful distinctions were learned, e.g.
beingpreceded by the word ?between?
as in ?built between 1849 and 1852?
renders a number less likely to be acompletion year.
Finally, we examined the quality of the learned models further by generating precision-recall curves for the all mode by adjusting the classification thresholds used by 1stage and 2stage.
Asshown in the plots of Table 2, 2stage achieves higher precision than 1stage at most recall levels for bothrelations, with the benefits being more pronounced in the architect name relation.
Summarizing thesecurves using average precision (Manning et al., 2008), the scores were 0.69 and 0.76 for the comple-tion year, and 0.21 and 0.51 for the architect, for the 1stage and the 2stage models respectively, thusconfirming the usefulness of separating the entity filtering features from relation extraction.6 DiscussionWhile all the buildings considered in our experiments have a dedicated Wikipedia page, only a few hada sentence mentioning them together with the correct answer in that resource.
Also, the architects whowere the correct answers did not always have a dedicated Wikipedia page.
Even though combininga search engine with distant supervision results in a highly imbalanced learning task, it increases thepotential coverage of our system.
In this process we rely on the keywords used in the queries in orderto find pages containing the entities intended rather than synonymous ones, e.g.
the keyword ?building?helps avoid extracting sentences mentioning saints instead of churches.
Nevertheless, building namessuch as churches named after saints were often ambiguous resulting in false positives.Bunescu and Mooney (2007) also used a small seed set and a search engine, but they collected sen-tences via queries containing both the question and the answer entities, thus (unreallistically) assumingknowledge of all the correct answers.
Instead we rely on simple heuristics to identify candidate answers.These heuristics are relation-dependent and different types of answers can be easily accommodated, e.g.in completed year relation they are single-token numbers.
Finally, the entity filters learned jointly withrelation extraction in our approach, while they perform a role similar to NER, they are learned so thatthey help avoid relation extraction errors and not to replace an actual NER system.7 ConclusionsOur application-based setting has placed novel demands on relation extraction system trained with distantsupervision, and in this paper we have shown that reasonable results can be obtained with only around30 seed examples without requiring NER for pre-processing.
Furthermore, we have demonstrated thatlearning entity filters and relation extraction jointly improves performance.5AcknowledgementsThe research reported was conducted while the first author was at the University of Cambridge andfunded by the European Community?s Seventh Framework Programme (FP7/2007-2013) under grantagreement no.
270019 (SPACEBOOK project www.spacebook-project.eu).ReferencesRazvan Bunescu and Raymond Mooney.
2007.
Learning to extract relations from the web using minimal su-pervision.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages576?583.Koby Crammer, Alex Kulesza, and Mark Dredze.
2009.
Adaptive regularization of weight vectors.
In Advancesin Neural Information Processing Systems 22, pages 414?422.Mark Craven and Johan Kumlien.
1999.
Constructing biological knowledge-bases by extracting information fromtext sources.
In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology,pages 77?86.Hal Daum?e III, John Langford, and Daniel Marcu.
2009.
Search-based structured prediction.
Machine Learning,75:297?325.He He, Hal Daum?e III, and Jason Eisner.
2013.
Dynamic feature selection for dependency parsing.
In Proceedingsof the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464, Seattle,October.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld.
2011.
Knowledge-basedweak supervision for information extraction of overlapping relations.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics, pages 541?550.Dan Klein and Chris Manning.
2002.
Fast exact inference with a factored model for natural language parsing.
InAdvances in Neural Information Processing Systems 15, pages 3?10.Christian Kohlsch?utter, Peter Fankhauser, and Wolfgang Nejdl.
2010.
Boilerplate detection using shallow textfeatures.
In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining, pages441?450.Xiao Ling and Daniel S. Weld.
2012.
Fine-grained entity recognition.
In Proceedings of the 26th Conference onArtificial Intelligence, pages 94?100.Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze.
2008.
Introduction to Information Retrieval.Cambridge University Press, New York, NY, USA.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009.
Distant supervision for relation extraction withoutlabeled data.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural Language Processing of the AFNLP, pages 1003?1011.St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011.
A reduction of imitation learning and structuredprediction to no-regret online learning.
In 14th International Conference on Artificial Intelligence and Statistics,pages 627?635.Andreas Vlachos.
2012.
An investigation of imitation learning algorithms for structured prediction.
Journal ofMachine Learning Research Workshop and Conference Proceedings, Proceedings of the 10th European Work-shop on Reinforcement Learning, 24:143?154.Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010.
Collective cross-document relation extraction with-out labelled data.
In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-ing, pages 1013?1023.Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zhifang Sui.
2013.
Towards accuratedistant supervision for relational facts extraction.
In Proceedings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Papers), pages 810?815, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.6
