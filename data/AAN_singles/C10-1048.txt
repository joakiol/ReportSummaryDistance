Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 421?429,Beijing, August 2010Detection of Simple Plagiarism in Computer Science PapersYaakov HaCohen-KernerDepartment of ComputerScience, Jerusalem College ofTechnology (Machon Lev)kerner@jct.ac.ilAharon TayebDepartment of ComputerScience, Jerusalem College ofTechnology (Machon Lev)aharontayeb@gmail.comNatan Ben-DrorDepartment of ComputerScience, Jerusalem College ofTechnology (Machon Lev)bd.natan@gmail.comAbstractPlagiarism is the use of the language andthoughts of another work and the repre-sentation of them as one's own originalwork.
Various levels of plagiarism existin many domains in general and in aca-demic papers in particular.
Therefore, di-verse efforts are taken to automaticallyidentify plagiarism.
In this research, wedeveloped software capable of simpleplagiarism detection.
We have built acorpus (C) containing 10,100 academicpapers in computer science written inEnglish and two test sets including pa-pers that were randomly chosen from C.A widespread variety of baseline me-thods has been developed to identifyidentical or similar papers.
Several me-thods are novel.
The experimental resultsand their analysis show interesting find-ings.
Some of the novel methods areamong the best predictive methods.1 IntroductionIn light of the explosion in the number of availa-ble documents, fast and accurate searching forplagiarism is becoming more needed.
Identifica-tion of identical and similar documents is becom-ing very important.Plagiarism is the use of the language andthoughts of another work and the representationof them as one's own original work (Wikipedia,2010; Library and Information Services, 2010).Plagiarism can be committed by "recycling" oth-er's work as well as by one?s own work (self-plagiarism).Various levels of plagiarism exist in manydomains in general and in academic papers inparticular.
In addition to the ethical problem,plagiarism in Academics can be illegal if copy-right of the previous publication has been trans-ferred to another entity.It is important to mention, that in many casessimilar papers are different versions of the samework, e.g., a technical report, a poster paper, aconference paper, a journal paper and a Ph.
D.dissertation.To avoid any kind of plagiarism, all sourceswhich were used in the completion of awork/research must be mentioned (Library andInformation Services, 2010).Over the last decade, various softwares havebeen built to automatically identify plagiarism(e.g., Collberg et al (2005), Sorokina et al(2006), and Keuskamp and Sliuzas (2007)).In this research, we developed such a system.This system is planned to deal with simple kindsof plagiarism, e.g., copying of sentences or partof sentences.
We have built a corpus that con-tains academic papers in computer science writ-ten in English.
Most of the papers are related tothe domain research of Natural LanguageProcessing (NLP) and are from the last ten years.The remainder of this paper is organized asfollows: Section 2 gives a background regardingplagiarism.
Section 3 overviews researches andsystems dealing with detection of plagiarism.Section 4 describes five groups of baseline me-thods, which have been implemented by us todetect plagiarism.
Section 5 presents the experi-ments that have been performed and their analy-sis.
Section 6 gives an illustrative example.
Sec-tion 7 concludes and proposes future directionsfor research.2 PlagiarismPlagiarism is defined in the 1995 Random HouseCompact Unabridged Dictionary as the "use orclose imitation of the language and thoughts ofanother author and the representation of them asone's own original work.
"421Self-plagiarism is the reuse of significant,identical, or nearly identical parts of one?s ownwork without citing the original work.
In addi-tion to the ethical issue, this phenomenon can beillegal if copyright of the previous work has beentransferred to another entity.
Usually, self-plagiarism is considered to be a serious ethicalproblem in cases where a publication needs tocontain an important portion of a new material,such as in academic papers (Wikipedia, 2010).On the other hand, it is common for research-ers to rephrase and republish their research, tai-loring it for different academic journals and con-ference articles, to disseminate their research tothe widest possible interested public.
However,these researchers must include in each publica-tion a meaningful or an important portion of anew material (Wikipedia, 2010).There are various classifications for levels ofplagiarism.
For instance, IEEE (2010) catego-rized plagiarism into five levels, or degrees, ofmisconduct, ranging from the most serious (Lev-el One) to the least serious (Level Five):Level One: The uncredited verbatim copyingof a full paper, or the verbatim copying of a ma-jor portion (greater than half of the original pa-per)Level Two: The uncredited verbatim copyingof a large portion (less than half of the originalpaper).Level Three: The uncredited verbatim copy-ing of individual elements (e.g., paragraphs, sen-tences, figures).Level Four: The uncredited improper paraph-rasing of pages or paragraphs.Level Five: The credited verbatim copying ofa major portion of a paper without clear delinea-tion (e.g., quotes or indents).Loui (2002) handled eight allegations of pla-giarism related to students' works.
Collberg et al(2005) proposes eight ranks of plagiarism.3 Related ResearchThere are two main attitudes concerning discov-ery of similar documents: ranking and finger-printing.
Ranking methods are derived from in-formation retrieval (IR) and are widely used inIR systems and Internet search engines.
Knownranking methods are the cosine measure, the in-ner product, and the normalized inner product.Hoad and Zobel (2003) extended the rankingfamily by defining identity measures, designedfor identification of co-derivative documents.Fingerprinting aims to compare between twodocuments based on their fingerprints.
Finger-print methods have been used by many previousresearches, e.g., Manber (1994).
Heintze (1996),Lyo et al (2001), Hoad and Zobel (2003), andShivakumar and Garcia-Molina (1996).3.1 Full FingerprintingGiven a document, a full fingerprint of thedocument consists of the set of all the possiblesequential substrings of length ?
in words (adefinition that is based on characters is also pos-sible).
There are N?
?+1 such substrings, whereN is the length of the document in words.
Thisfingerprinting selects overlapping sub-strings.For instance, if ?
is 3, this method selects the 3-word phrases that begin at position 0; 1; 2; etc.The size of ?
is known as the fingerprint granu-larity.
This variable can have a significant impactof the accuracy of fingerprinting (Shivakumarand Garcia-Molina, 1996).Comparing a document X to a document Ywhere X's size is |X| and if n is the number ofsubstrings common to both documents then n/|X|is the measure of how much of X is contained inY.3.2 Selective FingerprintingTo decrease the size of a full fingerprint, thereare various versions of selective fingerprints.The simplest kind of selective fingerprintingis the "All substrings selection" described inHoad and Zobel (2003).
This fingerprinting issimilar to the full fingerprinting, but it does notselect overlapping sub-strings.
Rather, it selectsall non-overlapping substrings of size ?
(inwords) from the document.
For example, if ?
is3, this strategy selects the 3-word phrases thatbegin at position 0; 3; 6; 9; etc.Heintze (1996) performed various experi-ments using a fixed number of fingerprints inde-pendent of the size of the document and a fixednumber of substrings of size ?
(in characters).The best results were achieved by 1,000 finger-prints with ?=50.
Another possibility is to workwith a fixed proportion of the substrings, so thatthe size of the selective fingerprint is propor-tional to the size of the document.
The main dis-422advantage of this possibility is space consump-tion.Hoad and Zobel (2003) suggested many addi-tional general types of selective fingerprinting,e.g., positional, frequency-based, and structure-based.3.3 Additional Similarity MeasuresSymmetricSimilarityMonostori1 et al (2002) defined a measurecalled SymmetricSimilarity as follows:SS(X, Y) = ?d(X) ?
d(Y)?/?d(X) + d(Y)?where X and Y are the two compared docu-ments, d(X) and d(Y) are the number of thefingerprints of X and Y, respectively, and?d(X)?d(Y)?
is the number of the commonfingerprints.S2 and S3Bernstein and Zobel (2004) defined severaladditional similarity measures, such as S2and S3:S2(X, Y) = ?d(X) ?
d(Y)?/min(?d(X)?,?d(Y)?
)S3(X, Y)= ?d(X) ?
d(Y)?/ (d(X) + d(Y))/2)where min(?d(X)?, ?d(Y)?)
is the minimalnumber of the fingerprints of X and Y, re-spectively, and d(X) + d(Y) is the averagenumber of the fingerprints of X and Y.Rarest-in-documentThe Rarest-in-Document method is one ofthe frequency-based methods defined byHoad and Zobel (2003).
This method choos-es the substrings that produce the rarest sub-strings with length of k words in the docu-ment.
This means that all of the substringsmust be calculated and sorted according totheir frequency in the document, and then therarest of them are selected.
The intuition isthat sub-strings, which are less common, aremore effective discriminators when compar-ing documents for similarity.Anchor methodsHoad and Zobel (2003) defined anchor me-thods.
These methods are based on specific,predefined strings (called anchors), in thetext of the document.
The anchors are chosento be common enough that there is at leastone in almost every document, but not socommon that the fingerprint becomes verylarge (Manber, 1994).Various anchors were used by Hoad and Zo-bel.
The anchors were randomly selected, butextremely common strings such as "th" and "it"were rejected.
The 35 2-character anchor methoddetects all of the documents that were consi-dered as similar by a human expert.Additional experiments have been applied toidentify the optimal size of an anchor.
Manber(1994) used 50-character anchors in a collectionof over 20,000 "readme" documents, identifying3,620 sets of identical files and 2,810 sets of sim-ilar files.
Shivakumar and Garcia-Molina (1996)achieved the best results with one-sentence anc-hors and Heintze (1996) achieved the best resultswith 1000-character anchors.4 Baseline Detection MethodsTo find whether there is a plagiarism, noveland old baseline methods have been imple-mented.
These methods can be divided intofive groups: full fingerprint methods, selec-tive fingerprint methods, anchor methods,word comparison methods, and combinationsof methods.Full fingerprint methodsAll the full fingerprint methods are defined foroverlapping substrings of length k in words fromthe beginning of the document.1.
FF(k) - Full Fingerprints of length k2.
SSF(k) - SymmetricSimilarity forFull fingerprints of length k3.
S2F(k) - S2 for Full fingerprints of length k4.
S3F(k) - S3 for Full fingerprints of length k5.
RDF(k) - Rarest-in-Document for Fullfingerprints of length k6.
CA -  Compare between the abstracts of thetwo documents using FF(3)Selective Fingerprint methodsIn this research, all the selective fingerprintmethods are selective by the sense of non-overlapping substrings of length k in wordsfrom the beginning of the document.7.
SF(k) -  Selective Fingerprints of length k4238.
SSS(k) - SymmetricSimilarity for Selectivefingerprints of length k9.
S2S(k) - S2 for Selective fingerprints oflength k10.
S3S(k) - S3 for Selective fingerprints oflength k11.
RDS(k) - Rarest-in-Document for Selectivefingerprints of length kAnchor methodsWe decided to work with seventy (N=70) 3-character anchors.
Based on these anchors wehave defined the following methods:12.
AFW -  Anchor First Words -  First 3-charcters from each one of the first N wordsin the tested document13.
AFS -  Anchor First Sentences -  First 3-charcters from each one of the first N sen-tences in the tested document14.
AF -  most Frequent Anchors -  N mostfrequent 3-charcter prefixes in the testeddocument15.
AR -  Rarest Anchors - N rarest frequent 3-charcter prefixes in the tested document16.
ALW -  Anchor Last Words -  First 3-charcters from each one of the last N wordsin the tested document17.
ALS -  Anchor Last Sentences -  First 3-charcters from each one of the last N sen-tences in the tested document Word compari-sons18.
CR - CompareReferences.
This methodcompares between the titles of the papers in-cluded in the references section of the twoexamined papers.Combinations of methods19.
CARA-   CompareAbstractReferencesAve-rage.
This method returns the average valueof CA and CR.20.
CARM -  CompareAbstractReferencesMin.This method returns the minimal value be-tween CA and CR.As mentioned above, Hoad and Zobel (2003)defined anchor methods based on the first/last Nsentences/words/3-charcter prefixes in the testeddocument.
As shown in Table 1 and in its analy-sis, the anchor methods are not successful, prob-ably because they use a small portion of data.Therefore, we decided to implement methodsdefined for the following portions of the paper:the first third (first), the middle third (middle),and the last third (end) of the paper according tothe number of the words in the discussed paper.All the first, middle and end methods use FF(3).These methods were combined with CA or CR.CA was not combined with the first methods be-cause the abstract is included in the first part ofthe paper.
CR was not combined with the lastmethods because the references are included inthe end part of the paper.21.
CAMA- CompareAbstractMiddleAve.
Thismethod returns the average value of CA andFF(3) computed for the middle parts of thetwo examined papers.22.
CAMM - CompareAbstractMiddleMin.This method returns the minimal value be-tween CA and FF(3) computed for the mid-dle parts of the two examined papers.23.
CAEA - CompareAbstractEndAverage.This method returns the average value of CAand FF(3) computed for the end parts of thetwo examined papers.24.
CAEM - CompareAbstractEndMin.
Thismethod returns the minimal value betweenCA and FF(3) computed for the end parts ofthe two examined papers.25.
CRFA -  CompareReferencesFirstAverage.This method returns the average value of CRand FF(3) computed for the first parts of thetwo examined papers.26.
CRFM - CompareReferencesFirstMin.
Thismethod returns the minimal value betweenbetween CR and FF(3) computed for the firstparts of the two examined papers.27.
CRMA - CompareReferencesMiddleAve-rage.
This method returns the average valueof CR and FF(3) computed for the middleparts of the two examined papers.28.
CRMM - CompareReferencesMiddleMin.This method returns the minimal value be-tween CR and FF(3) computed for the mid-dle parts of the two examined papers.To the best of our knowledge, we are the firstto implement methods that compare special andimportant sections in academic papers: abstractand references: CA and CR, and combinations ofthem.
In addition, we implemented new methodsdefined for the three thirds: the first (F) third, themiddle (M) third, and the last (E) third of thepaper.
These methods were combined with CAand CR in various variants.
All in total, we havedefined 12 new baseline methods.4245     Experimental Results5.1 DatasetAs mentioned above, the examined datasetincludes 10,100 academic papers in computerscience.
Most of the papers are related to NLPand are from the last ten years.
Most of thepapers were downloaded fromhttp://www.aclweb.org/anthology/.These documents include 52,909,234 wordsthat are contained in 3,722,766 sentences.
Eachdocument includes in average 5,262 words.
Theminimum and maximum number of words in adocument are 28,758 and 305, respectively.The original PDF files were downloadedusing IDM - Internet Download Manager(http://www.internetdownloadmanager.com/).Then we convert them to TXT files usingghostscript (http://pages.cs.wisc.edu/~ghost/).Many PDF files were not papers and many otherswere converted to gibberish files.
Therefore, theexamined corpus contains only 10,100 papers.5.2 Experiment ITable 1 presents the results of the 38 imple-mented methods regarding the corpus of 10,100documents.
The test set includes 100 papers thatwere randomly chosen from the examineddataset.
For each tested document, all the other10,099 documents were compared using the var-ious baseline methods.The IDN, VHS, HS, MS columns present thenumber of the document pairs that found as iden-tical, very high similar, high similar, and mediumsimilar to the 100 tested documents, respectively.The IDN, VHS, HS, MS levels were granted todocument pairs that got the following similarityvalues: 100%, [80%, 100%), [60%, 80%), and[40%, 60%), respectively.
However, similar pairof papers is not always a case of plagiarism, e.g.,in case where one paper cites the second one.The first left column indicates a simple ordin-al number.
The second left column indicates theserial number of the baseline method (Section 4)and the number in parentheses indicates thenumber of the chosen words (3 or 4) to be in-cluded in each substring.On the one hand, the anchor methods (# 12-17) tried on the interval of 70-500 anchors reporton relatively high numbers of suspicious docu-ment pairs, especially at the MS level.
Accordingto our expert, these high numbers are rather ex-aggerated.
The reason might be that such fixnumbers of anchors are not suitable for detectionof similar papers in various degrees of similarity.Table 1.
Results of the 38 implemented me-thods for 100 tested papers.# #(k) Method IDN VHS HS MS1 1(3) FF(3) 9 0 2 12 1(4) FF(4) 9 0 1 13 2(3) SSF(3) 0 0 0 94 2(4) SSF(4) 0 0 0 95 3(3) S2F(3) 9 0 2 26 3(4) S2F(4) 9 0 1 17 4(3) S3F(3) 0 0 9 08 4(4) S3F(4) 0 0 9 09 5(3) RDF(3) 1 5 1 310 5(4) RDF(4) 1 6 0 311 6 CA 9 0 1 012 7(3) SF(3) 9 0 0 113 7(4) SF(4) 9 0 0 114 8(3) SSS(3) 0 0 0 915 8(4) SSS(4) 0 0 0 916 9(3) S2S(3) 9 0 0 117 9(4) S2S(4) 9 0 0 118 10(3) S3S(3) 0 0 9 019 10(4) S3S(4) 0 0 9 020 11(3) RDS(3) 0 0 0 121 11(4) RDS(4) 0 0 0 022 12 AFW 4 6 18 277223 13 AFS 6 3 10 70824 14 AF 6 4 4 31325 15 AR 4 6 19 278926 16 ALW 4 6 9 50027 17 ALS 4 5 12 70428 18 CR 9 0 1 329 19 CARA 8 2 1 030 20 CARM 8 0 2 031 21 CAMA 9 0 1 032 22 CAMM 9 0 0 133 23 CAEA 9 0 1 034 24 CAEM 9 0 0 135 25 CRFA 8 0 3 036 26 CRFM 8 0 2 037 27 CRMA 8 0 3 038 28 CRMM 8 0 1 1425On the other hand, the SSF(k), S3F(k),S3S(k), RDF(k), and RDS(k) methods report onrelatively very low numbers of suspicious docu-ment pairs.
According to our expert, these num-bers are too low.
The reason for this findingmight be that these methods are quite stringentfor detection of similar document pairs.The full fingerprint methods: FF(k), S2F(k)and the selective fingerprint methods SF(k), andS2S(k) present very similar results, which arereasonable according to our expert.
Most of thesemethods report on 9 IDN, 0 VHS, 0-2 HS, and 1-2 MS document pairs.
The full fingerprint me-thods report on slightly more HS and MS docu-ment pairs.
According to our expert, these me-thods are regarded as the best.Our novel methods: CA and CR also reporton 9 IDN, 0 VHS, one HS, and 0 or 3 MS docu-ment pairs, respectively.
The sum (10-13) of theIDN, VHS, HS and MS document pairs found bythe best full and selective fingerprint methodsmentioned in the last paragraph is the same sumof the IDN, VHS, HS and MS document pairsfound by the CA and CR methods.
That is, theCA and CR are very close in their quailty to thebest methods.
However, the CA and the CR havea clear advantage on the other methods.
Theycheck a rather small portion of the papers, andtherfore their run time is much more smaller.On the one hand, CR seems to be better thanCA (and even the best selective fingerprint me-thods SF(k), and S2S(k)) because it reports onmore MS document pairs, which means that CRis closer in its quality to the best full fingerprintmethods.
On the other hand, according to ourexpert CA is better than CR, since CR has moredetection failures.The combinations of CA and/or CR and/orthe methods defined for the three thirds of thepapers report on results that are less or equalfrom the viewpoint of their quality to CA or CR.Several general conclusions can be drawnfrom the experimental results as follows:(1) There are 9 documents (in the examinedcorpus) that are identical to one of the 100 testedpapers.
According to our expert, each one ofthese documents is IDN to a different paper fromthe 100 tested papers.
This means that at least9% of our random tested papers have IDN filesin a corpus that contains 10, 099 files (for eachtest file).
(2) Several papers that have been found asIDN might be legal copies.
For example: (a) bymistake, the same paper might be stored twice atthe same conference website or (b) the paper,which is stored in its conference website mightalso be stored at its author's website.
(3) All the methods that run with two possiblevalues of k (3 or 4 words) present similar resultsfor the two values of k.(4) FF(3) found as better than FF(4).
FF(3)discovers 9 IDN papers, 2 HS papers, and 1 MSpaper.
These results were approved by a humanexpert.
FF(4) missed one paper.
One HS paperidentified by FF(3) was identified as MS byFF(4) and one MS paper identified by FF(3) wasidentified as less than MS by FF(4).
Moreover,also for other methods, variants with K=3 werebetter or equal to those with K=4.
The main rea-son for these findings might be that the variantswith K=4 check less substrings because thechecks are done for each sentence.
Substringsthat end at the sequential sentence are notchecked.
Therefore, it is likely that additionalequal substrings from the checked papers are notidentified.
(5) S2F(3) discovers one more MS papercompared to FF(3).
According to the human ex-pert, the similarity measure of this paper shouldbe less than MS.
Therefore, we decided to selectFF(3) as the best method.
(6) FF(3)'s run time is very high since itworks on overlapping substrings for the wholepapers.
(7) Our two novel methods: CA and CR areamong the best methods for identification of var-ious levels of plagiarism.
As mentioned before,CA was found as a better predictor.5.3 Selection of Methods and Experiment IISixteen methods out of the thirty-eight methodspresented in Table 1, were selected for additionalexperiments.
All the methods with k=4, the anc-hor methods, SSF, S3F, S3S, RDF, and RDS me-thods were omitted, due to their faulty results (asexplained above).
The remaining 16 methods(with k=3) are: FF, S2F, S2F, SF, S2S and all our12 baseline methods: CA, and CR- CRMM.Table 2 presents the results of these methodsregarding the corpus of 10,100 documents.
Sincewe selected less than half of the original methods426we allow ourselves to test 1,000 documents in-stead of 100.Table 2.
Results of the 16 selected methods for1,000 tested papers.Again, according to our expert, FF has beenfound as the best predictive method.
Surprising-ly, CA achieved the second best results with oneadditional VHS paper.
11 HS documents and 5MS documents have been identified by CA as byFF.
The meaning of this finding is that the ab-stracts in almost all the simple similar documentswere not significantly changed.
That is, the au-thors of the non-IDN documents did not investenough to change their abstracts.CR indentified 41 documents as identical.
Thereason for this is probably because 3 additionalpapers have the same reference section as in 3other tested papers, although these 3 documentpairs are different in other sections.
Furthermore,CR reports on relatively high number of suspi-cious document pairs, especially at the MS level.The meaning of this finding is that the referencesin many document pairs are not significantly dif-ferent although these documents have larger dif-ferences in other sections.
Consequently, combi-nations with CA achieved better results thancombinations with CR.A very important finding is that the run timeof FF was very expensive (one day, 3 hours and57.3 minutes) compared to the run time of CA (9hours and 16.7 minutes).
In other words, CAachieved almost the same results as FF but moreefficiently.5.4 An Error AnalysisThe selected methods presented in Table 2 wereanalyzed according to the results of FF.
Table 3shows the distributions of false true positives(TP), false positives (FP), true negatives (TN),and false negatives (FN), regarding the 10,099retrieved documents for the 1,000 tested docu-ment.The false positive rate is the proportion inpercents of positive test results (i.e., a plagiarismwas identified by a baseline function) that arereally negative values (i.e., the truth is that thereis no plagiarism).
The false negative rate is theproportion of negative test results that are reallypositive values.Table 3.
Distributions of the various possiblestatistical results.FF is the only method that detects all cases ofsimple plagiarism.
According to FF, there are0.534% true positives.
That is, 54 papers out of10,099 are suspected as plagiarized versions of# Method IDN VHS HS MS Timed:h:m1 FF 38 0 11 5 1:3:57.32 S2F 41 1 10 18 32:00.03 SF 37 1 1 6 31:12.24 S2 38 1 1 14 20:10.85 CA 38 1 11 5 09:16.76 CR 41 2 11 67 05:57.77 CARA 33 2 1 21 31:53.48 CARM 30 4 1 5 33:40.19 CAMA 38 0 5 6 11:26.510 CAMM 38 0 3 4 10:09.811 CAEA 38 0 6 7 10:42.112 CAEM 38 0 3 4 12:35.313 CRFA 32 1 3 25 54:20.714 CRFM 30 3 3 6 54:10.015 CRMA 33 2 3 25 58:52.216 CRMM 30 2 2 5 54:17.7# Method TP FP TN FN1 FF 0.534 0 99.465 02 S2F 0.524 0.168 99.296 0.0103 SF 0.425 0.019 99.445 0.1084 S2 0.435 0.099 99.366 0.0995 CA 0.534 0.010 99.455 06 CR 0.534 0.663 98.801 07 CARA 0.386 0.178 99.287 0.1488 CARM 0.356 0.039 99.425 0.1789 CAMA 0.475 0 99.465 0.05910 CAMM 0.445 0 99.465 0.08911 CAEA 0.485 0.020 99.445 0.04912 CAEM 0.445 0 99.465 0.08913 CRFA 0.396 0.207 99.257 0.13814 CRFM 0.376 0.039 99.425 0.15815 CRMA 0.405 0.217 99.247 0.12816 CRMM 0.366 0.020 99.445 0.16842754 papers of the 1,000 tested papers.
This findingfits the results of FF(3) in Table 2, where thereare 38 IDN, 11 HS, and 5 MS.CA, the second best method has 0% false po-sitives, and 0.01% false negatives, which meansthat CA identified one suspected plagiarized ver-sion that is really a non-plagiarized document.This finding is presented in Table 2, where CAidentified 55 suspected plagiarized documents,one more than FF.CR has 0% false positives, and 0.663% falsenegatives, which means that CR identified 67suspected plagiarized versions that are reallynon-plagiarized documents.
This finding is pre-sented in Table 2, where CR identified 121 sus-pected plagiarized documents, 67 more than FF.6 Illustrative ExampleDue to space limitations, we briefly present anillustrative example of comparison between acouple of papers found as HS (High Similar)according to FF(3), the best detection method.However, this is not a case of plagiarism, sincethe longer paper cited the shorter one as neededand there are differences in the submission lengthand quality.The tested paper (Snider and Diab, 2006A)contains 4 pages and it was published on June06.
The retrieved paper (Snider and Diab,2006B) contains 8 pages and it was published amonth later.
The title of the tested paper isidentical to the first eight words of the title of theretrieved paper.
The authors of both papers arethe same and their names appear in the sameorder.
Most of the abstracts are the same.
One ofthe main differences is the report of other results(probably updated results).A relatively big portion of the beginning ofthe Introduction section in both papers isidentical.
Very similar sentences are found at thebeginning of different sections (Section 2 in the4-page paper and Section 3 in the the 8-pagepaper).Many sentences or phrases from the rest ofthe papers are identical and some are very similar(e.g., addition of 'The' before "verbs areclassified" in the abstract of the retrieved paper.It is important to point that the authors in their8-page paper wrote "This paper is an extensionof our previous work in Snider and Diab (2006)".This sentence together with the detailedreference prove that the authors cite theirprevious work as required.Concerning the references in both papers, atthe first glance we found many differences be-tween the two papers.
The short paper containsonly 7 references while the larger paper contains14 references.
However, a second closer lookidentifies that 5 out of the 7 references in theshorter paper are found in the reference sectionof the larger paper.
Indeed, regarding the refer-ence sections we did not find HS; but we have toremember that the larger paper include 8 pagestwice than the shorter paper and therfore, morereferences could be included.7 Conclusions and Future WorkTo the best of our knowledge, we are the first toimplement the CA and CR methods that comparetwo basic and important sections in academicpapers: the abstract and references, respectively.In addition, we defined combinations of them.Furthermore, we implemented methods definedfor the three thirds of the paper.
These methodswere combined with CA or CR in various va-riants.
All in total, we have defined 12 new base-line methods.Especially CA and also CR are among thebest methods for identification of various levelsof plagiarism.
In contrast to the best full andselective fingerprint methods, CA and CR checka rather small portion of the papers, andtherefore, their run time is much more smaller.The success of CA and CR teaches us thatmost documents that are suspected as simpleplagiarized papers include abstracts andreferences, which have not been significantlychanged compared to other documents or viceversa.There is a continuous need for automaticdetection of plagiarism due to web influences,and advanced and more complex levels ofplagiarism.
Therefore, some possible futuredirections for research are: (1) Developing newkinds of selective fingerprint methods and newcombinations of methods to improve detection,(2) Applying this research to larger and/or othercorpora, and (3) Dealing with complex kinds ofplagiarism, e.g., the use of synonyms,paraphrases, and transpositions of activesentences to passive sentences and vice versa.428ReferencesBernstein, Y., and Zobel, J., 2004.
A Scalable Systemfor Identifying Co-Derivative Documents.
In Pro-ceedings of 11th International Conference onString Processing and Information Retrieval(SPIRE), vol.
3246, pp.
55-67.Bretag, T., and Carapiet, S., 2007.
A PreliminaryStudy to Identify the Extent of Self Plagiarism inAustralian Academic Research.
Plagiary, 2(5), pp.1-12.Collberg, C., Kobourov, S., Louie, J., and Slattery, T.,2005.
Self-Plagiarism in Computer Science.
Com-munications of the ACM, 48(4), pp.
88-94.Heintze, N., 1996.
Scalable Document Fingerprinting.In Proceedings of the USENIX Workshop on Elec-tronic Commerce, Oakland California.Hoad, T. C., and Zobel, J., 2003.
Methods for Identi-fying Versioned and Plagiarised Documents.
Jour-nal of the American Society for InformationScience and Technology, Vol 54(3), pp.
203-215.IEEE, 2010.
Introduction to the Guidelines for Han-dling Plagiarism Complaints.http://www.ieee.org/publications_standards/publications/rights/plagiarism.html.Keuskamp, D., and Sliuzas, R., 2007.
Plagiarism Pre-vention or Detection?
The Contribution of Text-Matching Software to Education about AcademicIntegrity.
Journal of Academic Language andLearning, Vol 1(1), pp.
91-99.Library and Information Services, 2010.
Cyprus Uni-versity of Technology in Scopus,http://www.cut.ac.cy/library/english/services/references_en.html#plagiarism.Loui, M. C., 2002.
Seven Ways to Plagiarize: Han-dling Real Allegations of Research Misconduct.Science and Engineering Ethics, 8, pp.
529-539.Lyon, C., Malcolm, J., and Dickerson, B., 2001.
De-tecting Short Passages of Similar Text in LargeDocument Collections.
In Proceedings of Confe-rence on Empirical Methods in Natural LanguageProcessing, pp.
118-125.Manber, U., 1994.
Finding Similar Files in a LargeFile System, In Proceedings of the USENIX Tech-nical Conference, pp.
1-10.Monostori1, K., Finkel, R., Zaslavsky, A., Hodasz,G., and Patke, M., 2002.
Comparison of OverlapDetection Techniques.
In Proceedings of the 2002International Conference on ComputationalScience, Lecture Notes in Computer Science, vol2329, pp.
51-60.Shivakumar, N., and Garcia-Molina, H., 1996.
Build-ing a Scalable and Accurate Copy Detection Me-chanism.
In Proceedings of the International Con-ference on Digital Libraries, pp.
160-168.Snider, N., and Diab, M., JUNE 2006A.
UnsupervisedInduction of Modern Standard Arabic VerbClasses.
In Proceedings of the Human LanguageTechnology Conference of the North AmericanChapter of the ACL, pp.
153- 156, June 2006.Snider, N., and Diab, M., JULY 2006B.
UnsupervisedInduction of Modern Standard Arabic Verb ClassesUsing Syntactic Frames and LSA.
In Proceedingsof the COLING/ACL 2006 Main Conference PosterSessions, pp.
795- 802.Sorokina, D., Gehrke, J., Warner, S., Ginsparg, P.,2006.
Plagiarism Detection in arXiv.
In Proceed-ings of Sixth International Conference on DataMining (ICDM), pp.
1070-1075.Wikipedia, 2010.
Plagiarism.http://en.wikipedia.org/wiki/Plagiarism.Witten, I. H., Moffat, A., and Bell, T. C., 1999.
Man-aging Gigabytes: Compressing and Indexing Doc-uments and Images.
Morgan Kaufmann, secondedition.429
