Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
248?256, Prague, June 2007. c?2007 Association for Computational LinguisticsInstance Based Lexical Entailment for Ontology PopulationClaudio Giuliano and Alfio GliozzoFBK-irst, Istituto per la Ricerca Scientifica e TecnologicaI-38050, Trento, ITALY{giuliano,gliozzo}@itc.itAbstractIn this paper we propose an instance basedmethod for lexical entailment and applyit to automatic ontology population fromtext.
The approach is fully unsupervised andbased on kernel methods.
We demonstratethe effectiveness of our technique largelysurpassing both the random and most fre-quent baselines and outperforming currentstate-of-the-art unsupervised approaches ona benchmark ontology available in the liter-ature.1 IntroductionTextual entailment is formally defined as a relation-ship between a coherent text T and a language ex-pression, the hypothesis H .
T is said to entail H ,denoted by T ?
H , if the meaning of H can be in-ferred from the meaning of T (Dagan et al, 2005;Dagan and Glickman., 2004).
Even though this no-tion has been recently proposed in the computationallinguistics literature, it has already attracted a greatattention due to the very high generality of its set-tings and to the indubitable usefulness of its (poten-tial) applications.In this paper, we concentrate on the problem oflexical entailment, a textual entailment subtask inwhich the system is asked to decide whether the sub-stitution of a particular word w with the word e in acoherent text Hw = H lwHr generates a sentenceHe = H leHr such that Hw ?
He, where H l andHr denote the left and the right context of w, re-spectively.
For example, given the word ?weapon?
asystem may substitute it with the synonym ?arm?, inorder to identify relevant texts that denote the soughtconcept using the latter term.
A particular case oflexical entailment is recognizing synonymy, whereboth Hw ?
He and He ?
Hw hold.In the literature, slight variations of this problemare also referred to as sense matching (Dagan et al,2006), lexical reference (Glickman et al, 2006a)and lexical substitution (Glickman et al, 2006b).They have been applied to a wide variety of tasks,such as semantic matching, subtitle generation andWord Sense Disambiguation (WSD).
Modeling lex-ical entailment is also a prerequisite to approach theSemEval-2007 lexical substitution task1, consistingof finding alternative words that can occur in givencontext.In this paper, we propose to apply an approach forlexical entailment to the ontology population task.The basic idea is that if a word entails another onein a given context then the former is an instance ora subclass of the latter.
This approach is intuitivelyappealing because lexical entailment is intrinsicallyan unsupervised task, therefore it does not requirelexical resources, seed examples or manually anno-tated data sets.
Unsupervised approaches are partic-ularly suited for ontology population, whose goal isto find instances of concepts from corpora, becauseboth corpus and the ontology sizes can scale up tomillions of documents and thousands of concepts,preventing us from applying supervised learning.
Inaddition, the top level part of the ontology (i.e., theTbox in the Description Logics terminology) is very1http://nlp.cs.swarthmore.edu/semeval/tasks/task10/description.shtml248often modified during the ontology engineering life-cycle, for example by introducing new concepts andrestructuring the subclass of hierarchy according tothe renewed application needs required by the evo-lution of the application domain.
It is evident thatto preserve the consistency between the Tbox andthe Abox (i.e., the set of instances and their rela-tions) in such a dynamic ontology engineering pro-cess, supervised approaches are clearly inadequate,as small changes in the TBox will be reflected intodramatic annotation effort to keep instances in theAbox aligned.The problem of populating a predefined ontol-ogy of concepts with novel instances implies a WSDtask, as the entities in texts are ambiguous with re-spect to the domain ontology.
For example, the en-tity Washington is both the name of a state and thename of a city.
In the ontology population settingstraditional WSD approaches cannot be directly ap-plied since entities are not reported into dictionar-ies, making the lexical entailment alternative moreviable.
In particular, we model the problem of on-tology population as the problem of recognizing foreach mention of an entity of a particular coarse-grained type (e.g., location) the fine-grained con-cept (e.g., lake or mountain) that can be substi-tuted in texts preserving the meaning.
For example,in the sentence ?the first man to climb the Everestwithout oxygen?, ?Everest?
can be substituted withthe word mountain preserving the meaning, whilethe sentence is meaningless when ?Everest?
is re-placed with the word lake.
Following the lexicalentailment approach, the ontology population taskis transformed into the problem of recognizing theterm from a fine-grained set of categories (e.g., city,country, river, lake and mountain) that can be substi-tuted in the contexts where the entity is mentioned(e.g., Everest in the example above).The main contributions of this paper are summa-rized as follows.
First, we propose a novel approachto lexical entailment, called Instance Based Lexi-cal Entailment (IBLE), that allows approaching theproblem as a classification task, in which a giventarget word (i.e., the entailing word) in a particu-lar context is judged to entail a different word takenfrom a (pre-defined) set of (possible) candidate en-tailed words (see Section 3).
Second, we exploit theIBLE approach to model the ontology populationtask as follows.
Given a set of candidate conceptsbelonging to generic ontological types (e.g., peo-ple or locations), and a set of pre-recognized men-tions of entities of these types in the corpus (e.g.,Newton, Ontario), we assign the entity to the classwhose lexicalization is more frequently entailed inthe corpus.
In particular, as training set to learnthe fine-grained category models, we use all the oc-currences of their corresponding expressions in thesame corpus (e.g., we collected all occurrences incontext of the word scientist to describe the conceptscientist).
Then, we apply the trained modelto classify the pre-recognized coarse-grained entitiesinto the fine-grained categories.Our approach is fully unsupervised as for trainingit only requires occurrences of the candidate entailedwords taken in their contexts.
Restricted to the on-tology population task, for each coarse-grained en-tity (e.g., location), the candidate entailed words arethe terms corresponding to the fine-grained classes(e.g., lake or mountain) and the entailing words arementions of entities (e.g., New York, Ontario) be-longing to the coarse-grained class, recognized byan entity tagger.Experiments show that our method for recog-nizing lexical entailment is effective for the on-tology population task, reporting improvementsover a state-of-the-art unsupervised technique basedon contextual similarity measures (Cimiano andVo?lker, 2005).
In addition, we also compared it toa supervised approach (Tanev and Magnini, 2006),that we regarded as an upper bound, obtaining com-parable results.2 The Ontology Population TaskPopulating concepts of a predefined ontology withinstances found in a corpus is a primary goal ofknowledge management systems.
As concepts inthe ontology are generally structured into hierar-chies belonging to a common ontological type (e.g.,people or locations), the problem of populating on-tologies can be solved hierarchically, firstly identi-fying instances in texts as belonging to the topmostconcepts, and then assigning them to a fine-grainedclass.
Supervised named entity recognition (NER)systems can be used for accomplishing the first step.State-of-the-art NER systems are characterized by249high accuracy, but they require a large amount oftraining data.
However, domain specific ontologiesgenerally contains many ?fine-grained?
categories(e.g., particular categories of people, such as writ-ers, scientists, and so on) and, as a consequence, su-pervised methods cannot be used because the anno-tation costs would become prohibitive.Therefore, in the literature, the fine-grained clas-sification task has been approached by adoptingweakly supervised (Tanev and Magnini, 2006; Fleis-chman and Hovy, 2002) or unsupervised methods(Cimiano and Vo?lker, 2005).
Tanev and Magnini(2006) proposed a weakly supervised method thatrequires as training data a list of terms without con-text for each class under consideration.
Such list canbe automatically acquired from existing ontologiesor other sources (i.e., database fields, web sites likeWikipedia, etc.)
since the approach imposes virtu-ally no restrictions on them.
Given a generic syntac-tically parsed corpus containing at least each train-ing entity twice, the algorithm learns, for each class,a feature vector describing the contexts where thoseentities occur.
Then it compares the new (unknown)entity with the so obtained feature vectors, assigningit to the most similar class.
Fleischman and Hovy(2002) approached the ontology population problemas a classification task, providing examples of in-stances in their context as training examples for theirrespective fine-grained categories.The aforementioned approaches are clearly inad-equate to recognize such fine-grained distinctions,as they would require a time consuming and costlyannotation process for each particular class, thatis clearly infeasible when the number of conceptsin the ontology scales up.
Therefore, most of thepresent research in ontology population is focus-ing on either unsupervised approaches (Cimianoand Vo?lker, 2005) or weakly supervised approaches(Tanev and Magnini, 2006).Unsupervised approaches are mostly based onterm similarity metrics.
Cimiano and Vo?lker (2005)assign a particular entity to the fine-grained classsuch that the contextual similarity is maximal amongthe set of fine-grained subclasses of a coarse-grainedcategory.
Contextual similarity has been measuredby adopting lexico-syntactic features provided by adependency parser, as proposed in (Lin, 1998).3 Instance Based Lexical EntailmentDagan et al (2006) adapted the classical supervisedWSD setting to approach the sense matching prob-lem (i.e., the binary lexical entailment problem ofdeciding whether a word, such as position, entailsa different word, such as job, in a given context)by defining a one-class learning algorithm based onsupport vector machines (SVM).
They train a one-class model for each entailed word (e.g., all the oc-currences of the word job in the corpus) and, then,apply it to classify all the occurrences of the entail-ing words (e.g., the word position), providing a bi-nary decision criterion2.
Similarly to the WSD case,examples are represented by feature vectors describ-ing their contexts, and then compared to the featurevectors describing the context of the target word.In this paper, we adopt a similar strategy to ap-proach a multi-class lexical entailment problem.The basic hypothesis is that if a word w entailse in a particular context (Hw ?
He), then someof the contexts T je in which e occurs in the train-ing corpus are similar to Hw.
Given a word wand an (exhaustive) set of candidate entailed wordsE = {e1, e2, .
.
.
, en}, to which we refer hereafterwith the expression ?substitution lexica?, our goal isto select the word ei ?
E that can be substituted tow in the context Hw generating a sentence He suchthat Hw ?
He.
In the multi-class setting, super-vised learning approaches can be used.
In particular,we can apply a one-versus-all learning methodology,in which each class ei is trained from both positive(i.e., all the occurrences of ei in the corpus) and neg-ative examples (i.e., all the occurrences of the wordsin the set {ej |j 6= i}).Our approach is clearly a simplification of themore general lexical entailment settings, wheregiven two generic words w and e, and a contextH = H lwHr, the system is asked to decide whetherw entails e or not.
In fact, the latter is a binaryclassification problem, while the former is easier asthe system is required to select ?the best?
optionamong the substitution lexicon.
Of course providingsuch set could be problematic in many cases (e.g.,it could be incomplete or simply not available for2This approach resembles the pseudo-words technique pro-posed to evaluate WSD algorithms at the earlier stages of theWSD studies (Gale et al, 1992), when large scale sense taggedcorpora were not available for training supervised algorithms.250many languages or rare words).
On the other hand,such a simplification is practically effective.
First ofall, it allows us to provide both positive and nega-tive examples, avoiding the use of one-class classi-fication algorithms that in practice perform poorly(Dagan et al, 2006).
Second, the large availabil-ity of manually constructed substitution lexica, suchas WordNet (Fellbaum, 1998), or the use of reposi-tories based on statistical word similarities, such asthe database constructed by Lin (1998), allows us tofind an adequate substitution lexicon for each targetword in most of the cases.For example, as shown in Table 1, the word jobhas different senses depending on its context, someof them entailing its direct hyponym position (e.g.,?looking for permanent job?
), others entailing theword task (e.g., ?the job of repairing?).
The prob-lem of deciding whether a particular instance of jobcan be replaced by position, and not by the wordplace, can be solved by looking for the most simi-lar contexts where either position or place occur inthe training data, and then selecting the class (i.e.,the entailed word) characterized by the most similarones, in an instance based style.
In the first example(see row 1), the word job is strongly associated tothe word position, because the contexts of the latterin the examples 1 and 2 are similar to the contextof the former, and not to the word task, whose con-texts (4, 5 and 6) are radically different.
On the otherhand, the second example (see row 2) of the wordjob is similar to the occurrences 4 and 5 of the wordtask, allowing its correct substitution.It is worthwhile to remark that, due to the ambi-guity of the entailed words (e.g., position could alsoentail either perspective or place), not every occur-rence of them should be taken into account, in orderto avoid misleading predictions caused by the irrele-vant senses.
Therefore, approaches based on a moreclassical contextual similarity technique (Lin, 1998;Dagan, 2000), where words are described ?globally?by context vectors, are doomed to fail.
We will pro-vide empirical evidence of this in the evaluation sec-tion.Choosing an appropriate similarity function forthe contexts of the words to be substituted is a pri-mary issue.
In this work, we exploited similar-ity functions already defined in the WSD literature,relying on the analogy between the lexical entail-ment and the WSD task.
The state-of-the-art super-vised WSD methodology, reporting the best resultsin most of the Senseval-3 lexical sample tasks in dif-ferent languages, is based on a combination of syn-tagmatic and domain kernels (Gliozzo et al, 2005)in a SVM classification framework.
Therefore, weadopted exactly the same strategy for our purposes.A great advantage of this methodology is that itis totally corpus based, as it does not require nei-ther the availability of lexical databases, nor the useof complex preprocessing steps such as parsing oranaphora resolution, allowing us to apply it on dif-ferent languages and domains once large corpora areavailable for training.
Therefore, we exploited ex-actly the same strategy to implement the IBLE clas-sifier required for our purposes, defining a kernelcomposed by n simple kernels, each representinga different aspect to be considered when estimatingcontextual similarity among word occurrences.
Infact, by using the closure properties of the kernelfunctions, it is possible to define the kernel combi-nation schema as follows3:KC(xi, xj) =n?l=1Kl(xi, xj)?Kl(xj , xj)Kl(xi, xi), (1)where Kl are valid kernel functions, measuring sim-ilarity between the objects xi and xj from differentperspectives4.One means to satisfy both the WSD and the lex-ical entailment requirements is to consider two dif-ferent aspects of similarity: domain aspects, mainlyrelated to the topic (i.e., the global context) of thetexts in which the word occurs, and syntagmatic as-pects, concerning the lexico-syntactic pattern in thelocal context.
Domain aspects are captured by thedomain kernel, described in Section 3.1, while syn-tagmatic aspects are taken into account by the syn-tagmatic kernel, presented in Section 3.2.3Some recent works (Zhao and Grishman, 2005; Gliozzoet al, 2005) empirically demostrate the effectiveness of com-bining kernels in this way, showing that the combined kernelalways improves the performance of the individual ones.
In ad-dition, this formulation allows evaluating the individual contri-bution of each information source.4An exhaustive discussion about kernel methods for NLPcan be found in (Shawe-Taylor and Cristianini, 2004).251Entailed job Trainingposition ... looking for permanent academic job in ... 1 ... from entry-level through permanent positions.2 My academic position ...3 ... put the lamp in the left position ...task The job of repairing 4 The task of setting up ...5 Repairing the engine is an hard task.6 ... task based evaluation.Table 1: IBLE example.3.1 The Domain Kernel(Magnini et al, 2002) claim that knowing the do-main of the text in which the word is located is a cru-cial information forWSD.
For example the (domain)polysemy among the Computer Science andthe Medicine senses of the word virus can besolved by simply considering the domain of the con-text in which it is located.
Domain aspects are alsocrucial in recognizing lexical entailment.
For exam-ple, the term virus entails software agent inthe Computer Science domain (e.g., ?The lap-top has been infected by a virus?
), while it entailsbacterium when located in the Medicine domain(e.g., ?HIV is a virus?).
As argued in (Magnini etal., 2002), domain aspects can be considered by an-alyzing the lexicon in a large context of the wordto be disambiguated, regardless of the actual wordorder.
We refer to (Gliozzo et al, 2005) for a de-tailed description of the domain kernel.
The sim-plest methodology to estimate the domain similar-ity among two texts is to represent them by meansof vectors in the Vector Space Model (VSM), andto exploit the cosine similarity.
The VSM is a k-dimensional space Rk, in which the text tj is rep-resented by means of the vector ~tj such that the ithcomponent of ~tj is the term frequency of the termwi in it.
The similarity between two texts in theVSM is estimated by computing the cosine betweenthem, providing the kernel function KV SM that canbe used as a basic tool to estimate domain similaritybetween texts5.5In (Gliozzo et al, 2005), in addition to the standard VSM,a domain kernel, exploiting external information acquired fromunlabeled data, has been also used to reduce the amount of (la-beled) training data.
Here, given that our approach is fully un-supervised, i.e., we can obtain as many examples as we need,we do not use the domain kernel.3.2 The Syntagmatic KernelSyntagmatic aspects are probably the most impor-tant evidence for recognizing lexical entailment.
Ingeneral, the strategy adopted to model syntagmaticrelations in WSD is to provide bigrams and trigramsof collocated words as features to describe local con-texts (Yarowsky, 1994).
The main drawback of thisapproach is that non contiguous or shifted colloca-tions cannot be identified, decreasing the general-ization power of the learning algorithm.
For ex-ample, suppose that the word job has to be disam-biguated into the sentence ?.
.
.
permanent academicjob in.
.
.
?, and that the occurrence ?We offer per-manent positions.
.
.
?
is provided for training.
Atraditional feature mapping would extract the con-text words w?1:academic, w?2:permanentto represent the former, and w?1:permanent,w?2:offer to index the latter.
Evidently such fea-tures will not match, leading the algorithm to a mis-classification.The syntagmatic kernel, proposed by Gliozzo etal.
(2005), is an attempt to solve this problem.
Itis based on a gap-weighted subsequences kernel(Shawe-Taylor and Cristianini, 2004).
In the spiritof kernel methods, this kernel is able to comparesequences directly in the input space, avoiding anyexplicit feature mapping.
To perform this opera-tion, it counts how many times a (non-contiguous)subsequence of symbols u of length n occurs inthe input string s, and penalizes non-contiguous oc-currences according to the number of the containedgaps.
To define our syntagmatic kernel, we adaptedthe generic definition of the sequence kernels to theproblem of recognizing collocations in local wordcontexts.
We refer to (Giuliano et al, 2006) for adetailed description of the syntagmatic kernel.2524 Lexical Entailment for OntologyPopulationIn this section, we apply the IBLE technique, de-scribed in Section 3, to recognize lexical entailmentfor ontology population.
To this aim, we cast ontol-ogy population as a lexical entailment task, wherethe fine-grained categories are the candidate entailedwords, and the named entities to be subcategorizedare the entailing words.
Below, we present the mainsteps of our algorithm in details.Step 1 By using a state-of-the-art supervised NERsystem, we recognize the named entities belongingto a set of coarse-grained categories (e.g., locationand people) of interest for the domain.Step 2 For all fine-grained categories belonging tothe same coarse-grained type, we extract from a do-main corpus all the occurrences of their lexicaliza-tions in context (e.g., for the category actor, weextract all contexts where the term actor occurs),and use them as input to train the IBLE classifier.
Inthis way, we obtain a multi-class classifier for eachontological type.
Then, we classify all the occur-rences of the named entities recognized in the firststep.
The output of this process is a list of taggednamed entities; where the elements of the list couldhave been classified into different fine-grained cat-egories even though they refer to the same phrase(e.g., the occurrences of the entity ?Jack London?could have been classified both as writer andactor, depending on the contexts where they oc-cur).Step 3 A distinct category is finally assigned to theentities referring to the same phrase in the list.
Thisis done on the basis of the tags that have been as-signed to all its occurrences during the previous step.To this purpose, we implemented a voting mecha-nism.
The basic idea is that an entity belongs to aspecific category if its occurrences entail a particu-lar superclass ?more often than expected by chance?,where the expectation is modeled on the basis of theoverall distribution of fine-grained category labels,assigned during the second step, in the corpus.
Thisintuition is formalized by applying a statistical reli-ability measure, that depends on the distribution ofpositive assignments for each class, defined by thefollowing formula:R(e, c) =P (c|e)?
?c?c, (2)where P (c|e) is estimated by the relative frequencyof the fine-grained class c among the different oc-currences of the entity e, ?c and ?c measure themean and the standard deviation of the distributionP (c|E), and E is an (unlabeled) training set of in-stances of the coarse-grained type classified by theIBLE algorithm.
Finally, each entity is assigned tothe category c?
such thatc?
= argmaxcR(e, c).
(3)5 EvaluationEvaluating a lexical entailment algorithm in itselfis rather complex.
Therefore, we performed a taskdriven evaluation of our system, measuring its use-fulness in an ontology population task, for whichevaluation benchmarks are available, allowing us tocompare our technique to existing state-of-the-artapproaches.As introduced in Section 4, the ontology popu-lation task can be modeled as a lexical entailmentproblem, in which the fine-grained classes are theentailed words and the named entities belonging tothe coarse-grained ontological type are the entailingwords.In the following, we first introduce the experimen-tal settings (Section 5.1).
Then we evaluate our tech-nique by comparing it to state-of-the-art unsuper-vised approaches for ontology population (Section5.2).5.1 Experimental SettingsFor all experiments, we adopted the evaluationbenchmark proposed in (Tanev and Magnini, 2006).It considers two high-level named entity cate-gories both having five fine-grained sub-classes (i.e.,mountain, lake, river, city, and countryas subtypes of LOCATION; statesman, writer,athlete, actor, and inventor are subtypes ofPERSON).
The authors usedWordNet andWikipediaas primary data sources for populating the evaluationontology.
In total, the ontology is populated with280 instances which were not ambiguous (with re-spect to the ontology) and appeared at least twice in253the English CLEF corpus6.
Even the evaluation taskis rather small and can be perceived as an artificialexperimental setting, it is the best available bench-mark we can use to compare our system to existingapproaches in the literature, as we are not aware ofother available resources.To perform NER we used CRFs (Lafferty et al,2001).
We trained a first-order CRF on the MUCdata set to annotate locations and people.
In ourexperiments, we used the implementation providedin MALLET (McCallum, 2002).
We used a stan-dard feature set inspired by the literature on textchunking and NER (Tjong Kim Sang and Buch-holz, 2000; Tjong Kim Sang and De Meulder, 2003;Tjong Kim Sang, 2002) to train a first-order CRFs.Each instance is represented by encoding all thefollowing families of features, all time-shifted by -2,-1,0,1,2: (a) the word itself, (b) the PoS tag ofthe token, (c) orthographic predicates, such as cap-italization, upper-case, numeric, single character,and punctuation, (d) gazetteers of locations, peoplenames and organizations, (e) character-n-gram pred-icates for 2 6 n 6 3.As an (unsupervised) training set for the fine-grained categories, we exploited all occurrences incontext of their corresponding terms we found inthe CLEF corpus (e.g., for the category actor weused all the occurrences of the term actor).
We didnot use any prior estimation of the class frequency,adopting a pure unsupervised approach.
Table 2lists the fine-grained concepts and the number ofthe training examples found for each of them in theCLEF corpus.As a reference for a comparison of the outcomesof this study, we used the results presented in (Tanevand Magnini, 2006) for the Class-Word and Class-Example approaches.
The Class-Word approach ex-ploits a similarity metric between terms and con-cepts based on the comparison of the contexts wherethey appear.
Details of this technique can be foundin (Cimiano and Vo?lker, 2005).
Tanev and Magnini(2006) proposed a variant of the Class-Word algo-rithm, called Class-Example, that relies on syntacticfeatures extracted from corpus and uses as an addi-tional input a set of training examples for each class.Overall, it required 1, 194 examples to accomplish6http://www.clef-campaign.orgthis task.All experiments were performed using the SVMpackage LIBSVM7 customized to embed our ownkernel.
In all the experiments, we used the defaultparameter setting.location personmountain 1681 statesman 119lake 730 writer 3436river 1411 athlete 642city 35000 actor 2356country 15037 inventor 105Table 2: Number of training examples for each class.5.2 ResultsTable 4 shows our results compared with two base-lines (i.e., random and most frequent, estimatedfrom the test data) and the two alternative ap-proaches for ontology population described in theprevious section.
Our system outperforms bothbaselines and largely surpasses the Class-Word un-supervised method.It is worthwhile to remark here that, being theIBLE algorithm fully unsupervised, improving themost frequent baseline is an excellent result, rarelyachieved in the literature on unsupervised methodsfor WSD (McCarthy et al, 2004).
In addition, oursystem is also competitive when compared to super-vised approaches, being it only 5 points lower thanthe Class-Example method, while it does not requireseed examples and syntactic parsing.
This charac-teristic makes our system flexible and adaptable todifferent languages and domains.System Micro F1 Macro F1RND Baseline 0.20 0.20Class-Word 0.42 0.33MF baseline 0.52 NAIBLE 0.57 0.47Class-Example 0.62 0.68Table 3: Comparison of different ontology popula-tion techniques.7http://www.csie.ntu.edu.tw/?cjlin/libsvm/254Finally, we performed a disaggregated evaluationof our system, assessing the performance for differ-ent ontological types and different concepts.
Re-sults show that our method performs better on largerfine-grained classes (i.e., writer and country),while the results on smaller categories are affectedby low recall, even if the predictions provided bythe system tends to be highly accurate.
Taking intoconsideration that our system is fully unsupervised,this behavior is highly desirable because it impliesthat it is somehow able to identify the predominantclass.
In addition the high precision on the smallerclasses can be explained by our instance based ap-proach.Person N Prec Rec F1Inventor 11 1 0.18 0.31Statesman 20 1.0 0.05 0.10Writer 88 0.61 0.89 0.72Actor 25 0.57 0.68 0.62Athlete 20 1 0.1 0.18Micro 164 0.61 0.61 0.61Macro 5 0.83 0.38 0.52Table 4: Performance of the IBLE approach on peo-ple.Location N Prec Rec F1City 23 0.35 0.26 0.30Country 40 0.61 0.70 0.65River 10 0.8 0.4 0.53Mountain 5 0.25 0.2 0.22Lake 4 0.2 0.5 0.29Micro 82 0.50 0.50 0.50Macro 5 0.44 0.41 0.42Table 5: Performance of the IBLE approach on lo-cations.6 Conclusions and Future WorkIn this paper, we presented a novel unsupervisedtechnique for recognizing lexical entailment in texts,namely instance based lexical entailment, and weexploited it to approach an ontology population task.The basic assumption is that if a word is entailedby another in a given context, then some of thecontexts of the entailed word should be similar tothat of the word to be disambiguated.
Our tech-nique is effective, as it largely surpasses both therandom and most frequent baselines.
In addition, itimproves over the state-of-the-art for unsupervisedapproaches, achieving performances close to the su-pervised rivaling techniques requiring hundreds ofexamples for each class.Ontology population is only one of the possibleapplications of lexical entailment.
For the future,we plan to apply our instance based approach to awide variety of tasks, e.g., lexical substitution, wordsense disambiguation and information retrieval.
Inaddition, we plan to exploit our lexical entailment asa subcomponent of a more complex system to rec-ognize textual entailment.
Finally, we are going toexplore more elaborated kernel functions to recog-nize lexical entailment and more efficient learningstrategies to apply our method to web-size corpora.AcknowledgmentsThe authors would like to thank Bernardo Magniniand Hristo Tanev for providing the benchmark,Ido Dagan for useful discussions and commentsregarding the connections between lexical entail-ment and ontology population, and Alberto Lavellifor his thorough review.
Claudio Giuliano is sup-ported by the X-Media project (http://www.x-media-project.org), sponsored by the Eu-ropean Commission as part of the Information So-ciety Technologies (IST) program under EC grantnumber IST-FP6-026978.
Alfio Gliozzo is sup-ported by the FIRB-Israel research project N.RBIN045PXH.ReferencesPhilipp Cimiano and Johanna Vo?lker.
2005.
Towardslarge-scale, open-domain and ontology-based namedentity classification.
In Proceedings of RANLP?05,pages 66?
166?172, Borovets, Bulgaria.I.
Dagan and O. Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
In Proceedings of the PASCAL Workshopon LearningMethods for Text Understanding andMin-ing, Grenoble.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL recognising textual entailment255challenge.
In Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment.Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein, and Carlo Strapparava.
2006.
Direct wordsense matching for lexical substitution.
In Proceed-ings ACL-2006, pages 449?456, Sydney, Australia,July.I.
Dagan.
2000.
Contextual word similarity.
In RobDale, Hermann Moisl, and Harold Somers, editors,Handbook of Natural Language Processing, chap-ter 19, pages 459?476.
Marcel Dekker Inc.C.
Fellbaum.
1998.
WordNet.
An Electronic LexicalDatabase.
MIT Press.Michael Fleischman and Eduard Hovy.
2002.
Finegrained classification of named entities.
In Proceed-ings of ACL-2002, pages 1?7, Morristown, NJ, USA.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
Work on statistical methods for wordsense disambiguation.
In R. Goldman et al, editor,Working Notes of the AAAI Fall Symposium on Prob-abilistic Approaches to Natural Language, pages 54?60.Claudio Giuliano, Alfio Massimiliano Gliozzo, and CarloStrapparava.
2006.
Syntagmatic kernels: a wordsense disambiguation case study.
In Proceedings ofthe EACL-2006 Workshop on Learning Structured In-formation in Natural Language Applications, Trento,Italy, 5-7 April.O.
Glickman, E. Shnarch, and I. Dagan.
2006a.
Lexicalreference: a semantic matching subtask.
In proceed-ings of EMNLP 2006.Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-gio, and Walter Daelemans.
2006b.
Investigating lexi-cal substitution scoring for subtitle generation.
In Pro-ceedings of CoNLL-2006.A.
Gliozzo, C. Giuliano, and C. Strapparava.
2005.
Do-main kernels for word sense disambiguation.
In Pro-ceedings of ACL-2005, pages 403?410, Ann Arbor,Michigan, June.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of ICML-2002, pages 282?289, WilliamsCollege, MA.
Morgan Kaufmann, San Francisco, CA.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of ACL-98, pages 768?774, Morristown, NJ, USA.B.
Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.2002.
The role of domain information in wordsense disambiguation.
Natural Language Engineer-ing, 8(4):359?373.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word sensesin untagged text.
In Proceedings of ACL-2004,Barcelona, Spain, July.J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.Hristo Tanev and Bernardo Magnini.
2006.
Weakly su-pervised approaches for ontology population.
In Pro-ceedings of EACL-2006, Trento, Italy.Erik Tjong Kim Sang and Sabine Buchholz.
2000.
In-troduction to the CoNLL-2000 shared task: Chunking.In Proceedings of CoNLL-2000, Lisbon, Portugal.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL-2003, pages 142?147, Edmonton, Canada.Erik F. Tjong Kim Sang.
2002.
Introduction tothe CoNLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of CoNLL-2002, pages 155?158, Taipei, Taiwan.D.
Yarowsky.
1994.
Decision lists for lexical ambiguityresolution: Application to accent restoration in Span-ish and French.
In Proceedings of ACL-94, pages 88?95, Las Cruces, New Mexico.Shubin Zhao and Ralph Grishman.
2005.
Extracting re-lations with integrated information using kernel meth-ods.
In Proceedings of ACL 2005, Ann Arbor, Michi-gan, June.256
