Data Col lect ion And Evaluat ionDavid S. Pal lettNat ional  Inst i tute of Standards and TechnologyABSTRACTThis session focussed on two inter-re lated issues: (I) per formanceassessment for spoken language systems and (2) exper ience to datein speech corpora col lect ion for these systems.
The sessioninc luded formal  p resentat ions  f rom representat ives  of SRIInternational,  MIT's Laboratory for Computer Science, BBN Systemsand Technologies Corporation, and Carnegie Mel lon Univers i ty 'sSchool of Computer Science.SESSION OVERVIEWMater ial  presented by Patti Price et al of SRI Internat ionaldescr ibed  col lect ion of more than 12 hours of human-humaninteract ive problem solving in the air travel p lanning domain.
SRIhas made use of this data to def ine an initial vocabulary  and todef ine an interface for this domain.
Recent efforts to conduct"Wizard" s imulat ions of human-system interact ions in this domainwere described.
Price noted that in the natura l ly  occurr ingdialogues it is rare that a database query occurs.
Rather, theuser wil l  state a plan (e.g., "I need to make a reservation") andthen provide, in small steps, the pieces of information necessaryfor the "agent" to help accompl ish the plan.
Breaking the dia logueinto small pieces of information, asking for frequent confirmation,and having the agent sometimes take an act ive role all seem to playa role in making the dialogue efficient.
These f indings suggest toPrice that both arguments of naturalness (a strong mot ivat ingfactor for the use of natural language in the first place) andeff ic iency argue for human-system interact ions that wil l  y ieldlarge numbers of sentences that are not database queries.
In viewof this, it was argued that performance assessment procedures mustgo beyond considerat ion of database query-answer pairs and includeother mechanisms for assessment, such as the template-based methodof the MUCK-2 approach.In the first of the two papers by the group at MIT's Laboratoryfor Computer Science, Zue et al descr ibe the col lect ion andpre l iminary analysis of a spontaneous speech corpus using as imulated human-system dialogue with the VOYAGER spoken languagesystem.
The Voyager system is made up of three components: (i) theSUMMIT speech recognit ion system which converts the speech into aset of word hypotheses, (2) the TINA natural  language component,which provides a l inguist ic interpretat ion and a parse tree that ist ranslated into a query language form, and (3) a modi f ied vers ionof the direct ion assistance program (developed by J im Davis of115MIT's Media Laboratory).
Spontaneous speech data were recordedfrom I00 subjects, and each subject was also recorded readingorthographic transcriptions of their spontaneous speech (minusfalse starts, hesitations and filled pauses).
In col lecting thespontaneous speech, a simulation was used that replaced the SUMMITspeech recognition component with a human typing the orthographictranscription of the spontaneous speech into the remainder of thesystem.
The speech data, consisting of nearly i0,000 utterances,were subsequently digit ized and transcribed.
Comparison ofcorresponding spontaneous and read speech data show that thespontaneous utterances \[i.e., sentences\] are longer than their readcounterparts, and that there is much more variabi l i ty in thespontaneous speech.
Pauses that are more frequent and longeraccount for much of the longer duration characterizing spontaneousspeech.
Non-speech vocalizations found in the spontaneous speechinclude mouth clicks, breath noise and fil led pauses such as "um","uh", or "ah".
False starts occurred in almost 4% of thespontaneous sentences.
The words following false starts in thespontaneous speech included "back ups" (to the same as the\[apparent\]  in tended word),  a d i f fe rent  word in the same\[syntactic\] category, a word from a new category, or a back up torepeat \[several\] words already uttered.
This study concludes thatthe process of data collection in this simulation was relativelystraightforward, and that incremental data col lection can "be quiteeffective for development of spoken language systems".In the second paper from Zue's group, a number of performanceassessment issues are raised.
It is suggested that spokenlanguage systems should be evaluated along several dimensions.
Thedimensions include (i) accuracy of the system and its variousmodules (e.g., phonetic, word and sentence accuracy as well asl ingu is t i c  and task completion accuracy), (2) coverage andhabitability, (3) flexibility, and (4) eff iciency (e.g.
taskcompletion time).
Zue et al note that evaluations of accuracyinevitably involve the use of references involving varying degreesof subjectivity.
At higher levels, system outputs may involve moreabst rac t  information, complicating the process of automaticcomparison with a reference output.
The prel iminary evaluation ofthe MIT Voyager system includes evaluation of the Summit speechrecognition component.
Using a 570 word lexicon and a word-pairgrammar with a test set perplexity of 22 to constrain the searchspace, word accuracies of approximately 86% and sentence accuraciesof 49% for sentences of about 8 words per sentence are reported.Analyses of natural language performance focussed on coverage interms of percentage of sentences that could be parsed andperplexity.
Overall system performance has been evaluated byseveral means, including a panel of naive users to judge theappropriateness of the responses of the system as well as thequeries made by the subjects.
Although data were avai lable onlyfor a small number of subjects, it appeared that "appropriate"responses together with "verbose, but otherwise correct" responses116accounted for approx imate ly  85% of the responses.
About  87% of theuser quer ies were judged reasonable.
The issue of e f f ic iency wasnot addressed, since the system under d iscuss ion operates in about12 t imes real time, prec luding real - t ime interact ive dialogues.The paper by Boisen, Ramshaw and Bates from BBN descr ibes "anautomatic, essent ia l ly  domain- independent"  means of evaluat ingspoken language systems that provide answers to quer ies from adatabase.
This proposal  was developed out of an unders tand ing  thatsome consensus has been achieved on a number of issues including:(i) "Common evaluat ion involves work ing on a common domain (ordomains).
A common corpus of development quer ies (in both spokenand t ranscr ibed form), and answers to those quer ies in somecanonical  format, are therefore required.
",  (2) "One basis forsystem evaluat ion wil l  be answers to quer ies from a commondatabase, perhaps in addit ion to other measures."
(3) "Automaticevaluat ion methods should be used whenever  they are feasible".
Theproposal  for evaluat ion on a DARPA common task has as a keycomponent a program des ignated a "Comparator" that comparescanonical  answers to the answers suppl ied by a spoken languagesystem.
Answers are to be expressed in the form of a "CommonAnswer Spec i f icat ion (CAS)", as descr ibed in the proposal.
Theproposed comparator  is a Common LISP program for compar ing systemoutput expressed in CAS format with canonical  answers.
Much as Zueet al note, Boisen et al note that "evaluat ion requires humanjudgement, and therefore the best we can expect from a program iscomparison, not evaluation".
BBN has prepared a small  corpus ofqueries and their  answers for the (proposed) "Common PersonnelDatabase" to i l lustrate the use of the CAS format and as a check onthe c lar i ty  and completeness of the CAS.
Finally, Boisen et alnote that the co l lect ion of any corpus "for SLS development  andtest ing wil l  be more useful  if it is easi ly  sub-d iv ided into easierand harder  cases", and they propose candidate categor izat ions,start ing from a default  case in which no extra-sentent ia l  contextis required, to the more d i f f icu l t  categor ies involv ing "local"extra-sentent ia l  reference, el l ips is  cases, non- local  referencesand \[even\] more complex cases.
It is argued that these pr inc ip lesof ca tegor i za t ion  shou ld  be fo l lowed in implement ing SLSevaluations.In BBN's second paper in this session, Derr and Schwartz descr ibedthe development of a new grammar that can be used in assess ing theperformance of speech recognit ion systems.
It is a "stat ist icalf i rst -order class grammar" that has been developed for twodi f ferent  task domains (the DARPA Resource Management  domain and a2000 word personnel  database domain).
Derr and Schwartz argue thatthe exist ing two grammatical  condit ions (the "no grammar" or nullgrammar and the word-pa i r  grammar cases) "suffer from severalinadequacies".
The null grammar provides "only a worst -caserecognit ion test point", whi le  the word-pa i r  grammar not onlyexcludes "many reasonable word sequences", but the use of the word-pair  grammar y ie lds such high recognit ion per formance that re l iablemeasurement  of system improvements (i.e.
s tat is t ica l ly  s igni f icantinferences of improvements) cannot be obtained wi thout  use of very117large development and evaluation test sets.
Given a prioriassignment of words to classes, the statistics of BBN's classgrammar were counted directly from training data by counting thenumber of transitions from each class to each other class.
Using99 classes for the lexicon of the DARPA Resource Management taskdomain, the class grammar provides a perplexity of approximately75 and recognition error rates that are in-between the results forthe word-pair and null grammars.
The increased error rate isnoteworthy not for the fact that recognition performance isdegraded per se, but for the fact that, using this grammar,incrementa l  improvements  may be shown to be statist ical lysignif icant using smaller test sets and less test and machinetime.
\[Following this presentation, it was observed thatincremental improvements could also be shown to be statist ical lysignificant with the null grammar, but without the apparentbenefit of higher word accuracies (i.e., the performance using theclass grammar is shown to be somewhat better than the worst-caseresults for the null grammar).
The critical issue is one ofde f in ing  the des iderata  for constraining grammars and therelationship of these grammars to those that are in some sense"natural" to the task sub-language.\] Further advantages outl inedfor this approach include the fact that it is readily adapted tonew task domains and that it is "tunable" (i.e., can be set toprovide varying perplexity) by varying the number of classes inthe grammar.The paper by Rudnicky et al from CMU presents results of a studyof a spoken language interface involving a complex problem-solving task.
A group of users was asked to perform 40 spreadsheettasks, each successive task being carried out in a differentmodality (speech or typing).
The voice spreadsheet consists of theUNIX-based spreadsheet program "SC" interfaced to a recognizerembodying the Sphinx speech recognition technology.
The CMU studyis noteworthy for the fact that, of the systems discussed in thissession, it is the only system to provide near-real timeinteractions without the intervention of a "Wizard".
For a taskvocabulary of 271 words \[and a constraining grammar with aperplexity of 52\], word accuracies of 92.7% to 94.9% were achievedfor spontaneous and read speech.
Analyses conducted by Rudnicky etal.
include discussions of semantic accuracy, grammatical i ty andlanguage habitability.
Spontaneous speech events are discussed inthree categor ies :  lexical ,  ext ra - lex ica l ,  and non-lexical.Detailed analyses also include "the time it takes to do things".Since the implementation used at CMU for these studies processedspeech in about 2 times real time, it is perhaps not surprisingthat total task time was greater for speech input than keyboard.However,  by accounting for processing "overhead" times andproposing a halving of the present error rate, Rudnicky et alestimate that task completion times for speech and keyboard shouldbe "equivalent".
Current efforts are directed toward achieving atrue real-time implementation and improving system accuracy.118
