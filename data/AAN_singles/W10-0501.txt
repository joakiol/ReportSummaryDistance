Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 1?2,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsThe ?Nays?
Have It: Exploring Effects of Sentiment in CollaborativeKnowledge SharingAblimit Aji , Eugene AgichteinMathematics & Computer Science DepartmentEmory University{aaji,eugene}@mathcs.emory.eduAbstractIn this paper we study what effects sentimenthave on the temporal dynamics of user inter-action and content generation in a knowledgesharing setting.
We try to identify how senti-ment influences interaction dynamics in termsof answer arrival, user ratings arrival, commu-nity agreement and content popularity.
Ourstudy suggests that ?Negativity Bias?
triggersmore community attention and consequentlymore content contribution.
Our findings pro-vide insight into how users interact in onlineknowledge sharing communities, and helpfulfor improving existing systems.1 IntroductionRecently, Collaborative Knowledge Sharing sites( orCQA sites), such as Naver and Yahoo!
Answershave exploded in popularity.
Already, for many in-formation needs, these sites are becoming valuablealternatives to search engines.
Previous studies iden-tified visibility as an important factor for contentpopularity and developed models in static settings.However, when users post social media content, theymight either explicitly or implicitly express theirpersonal attitudes or sentiment.
The following ex-ample illustrates a question with negative sentiment.Q :Obama keeps saying we need to sacrifice.What sacrifices has he and the gov made collec-tively and individually?A1: Our hard earned tax dollars.
17 ?, 2 ?A2: None and they never will.
18 ?, 2 ?Psychological studies (Smith et al, 2008) suggestthat our brain has ?Negativity Bias?
- that is, peopleautomatically devote more attention to negative in-formation than to positive information.
Thus, ourattitudes may be more heavily influenced by nega-tive opinions.
Our hypothesis is that this kind of hu-man cognitive bias would have measurable effectson how users respond to information need in CQAcommunities.
Our goal in this paper is to under-stand how question sentiment influence the dynam-ics of the user interactions in CQA - that is, to un-derstand how users respond to questions of differentsentiment, how question sentiment affects commu-nity agreement on best answer and question popu-larity.2 Sentiment InfluenceWhile (Aji et al, 2010) suggests that question cat-egory has a patent influence on interaction dynam-ics, we mainly focus on sentiment in this exploratorystudy, for the reason that sentiment is a high levelbut prominent facet in every piece of content.
Wefocused on how may sentiment effect the followingdimensions:?
Answer Arrival: Measured as number of an-swers arrived every minute.?
Vote Arrival: Measured as number of votes ar-rived per answer.?
Community Agreement: Mean Reciprocal Rank(MRR), computed by ranking the answers in or-der of decreasing ?Thumbs up?
ratings, and iden-tifying the rank of the actual ?best?
answer, as se-lected by the asker.MRR = 1|Q|N?i=11ranki(1)where ranki is the rank of the best answer amongthe answers submitted for question i.?
Answer Length, Question Length: We examinewhether questions with different sentiment exhibitvariations in question and answer length.?
Interest ?Stars?
: How many users marked ques-tion as interesting.3 Dataset DescriptionFor our study we tracked a total of approximately10,000 questions, sampled from 20 categories fromYahoo!
Answers.
Specifically, each new question inour tracking list crawled every five minutes until it?sclosed.
As a result, we obtained approximately 221million question-answer-feedback snapshots in to-tal.
Since labeling all the questions would be ex-pensive, we randomly selected 2000 questions fromthis dataset for human labeling.
We then utilized theAmazon Mechanical Turk Service1.
Five workerslabeled each question as either positive, negative orneutral; the ratings were filtered by using majorityopinion (at least 3 out of 5 labels).
Overall statisticsof this dataset are reported in Table 1.
The overallinter-rater agreement was 65%.Positive Negative Neutral Total379 173 548 1,100Table 1: Statistics of the Temporal dataset.4 Results and DiscussionFigure 1 reports answer arrival dynamics for ques-tion with varying sentiment.
Answers to negativequestions arrive substantially faster than answers topositive or neutral questions, whereas the differencebetween positive and neutral questions are minor.This strongly confirms the ?Negative Bias?
effect.Given the fact that questions stay in the categoryfront page relatively same amount of time wheretheir visibility contributes potential answers, on av-erage, negative sentiment questions managed to getmore answers than two other types of questions (4.3vs.
3.3 and 3.5).
It seems, sentiment expressed in aquestion contributes to the answer arrival more thanvisibility.0 0.51 1.52 2.53 3.54 4.51  10  100  1000Answerstime (in minutes)negative sentimentneutral sentimentpositive sentimentFigure 1: Cumulative answer arrivalFigure 2 reports rating arrival dynamics.
Interest-ingly, positive ratings arrive much faster to negativequestions, whereas positive and negative ratings ar-rive roughly at the same rate for positive and neutralquestions.
While this might be partially due to thefact that negative sentiment questions are more ?at-tention grabbing?
than other types of questions, weconjecture that this effect is caused by the selectionbias of the raters participating in negative questionthreads, who tend to support answers that strongly1http://www.mturk.com00.10.20.30.40.51  10  100  1000Ratingstime (in minutes)negative +neutral +positive +negative -neutral -positive -Figure 2: Cumulative user ratings arrivalagree (or strongly disagree) with the question asker.Surprisingly, community agreement(MRR) on theType MRR QLength ALength StarsNegative 0.47 78 49 0.25Positive 0.56 58 52 0.16Neutral 0.57 52 47 0.15Table 2: Agreement, Question length, Answer Lengthand Star count averaged over question typebest answer is lower for negative sentiment ques-tions.
On average, negative sentiment questionswere marked as interesting more than positive orneutral questions were marked as interesting.
Al-though this may sound counterintuitive, it is not sur-prising if we recall how the ?Negative Bias?
influ-ences user behavior and may increase implicit ?visi-bility?.
All the above mentioned differences are sta-tistically significant(t-test p = 0.05).In summary, our preliminary exploration indi-cates that sentiment may have a powerful effect onthe content contribution dynamics in collaborativequestion answering, and is a promising direction forfurther study of knowledge sharing communities.AcknowledgmentsWe thank HP Social Computing Labs for support.ReferencesAblimit Aji.
Eugene Agichtein.
2010.
DeconstructingInteraction Dynamics in Knowledge Sharing Commu-nities.
International Conference on Social Computing,Behavioral Modeling, & Prediction.Gabor Szabo.
Bernardo Huberman.
2008.
Predicting thepopularity of online content.
HP Labs Technical Re-port.Kristina Lerman.
2007.
Social Information Processingin Social News Aggregation.
IEEE Internet Comput-ing: Special Issue on Social Search.N.
Kyle Smith Jeff T. Larsen Tanya L. Chartrand JohnT.
Cacioppo 2006.
Affective Context Moderates theAttention Bias Toward Negative Information.
Journalof Personality and Social Psychology.2
