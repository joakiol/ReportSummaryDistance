Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 576?583,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsLearning to Extract Relations from the Webusing Minimal SupervisionRazvan C. BunescuDepartment of Computer SciencesUniversity of Texas at Austin1 University Station C0500Austin, TX 78712razvan@cs.utexas.eduRaymond J. MooneyDepartment of Computer SciencesUniversity of Texas at Austin1 University Station C0500Austin, TX 78712mooney@cs.utexas.eduAbstractWe present a new approach to relation ex-traction that requires only a handful of train-ing examples.
Given a few pairs of namedentities known to exhibit or not exhibit aparticular relation, bags of sentences con-taining the pairs are extracted from the web.We extend an existing relation extractionmethod to handle this weaker form of su-pervision, and present experimental resultsdemonstrating that our approach can reliablyextract relations from web documents.1 IntroductionA growing body of recent work in informationextraction has addressed the problem of relationextraction (RE), identifying relationships betweenentities stated in text, such as LivesIn(Person,Location) or EmployedBy(Person, Company).Supervised learning has been shown to be effectivefor RE (Zelenko et al, 2003; Culotta and Sorensen,2004; Bunescu and Mooney, 2006); however, anno-tating large corpora with examples of the relationsto be extracted is expensive and tedious.In this paper, we introduce a supervised learningapproach to RE that requires only a handful oftraining examples and uses the web as a corpus.Given a few pairs of well-known entities thatclearly exhibit or do not exhibit a particular re-lation, such as CorpAcquired(Google, YouTube)and not(CorpAcquired(Yahoo, Microsoft)), asearch engine is used to find sentences on the webthat mention both of the entities in each of the pairs.Although not all of the sentences for positive pairswill state the desired relationship, many of themwill.
Presumably, none of the sentences for negativepairs state the targeted relation.
Multiple instancelearning (MIL) is a machine learning frameworkthat exploits this sort of weak supervision, inwhich a positive bag is a set of instances which isguaranteed to contain at least one positive example,and a negative bag is a set of instances all of whichare negative.
MIL was originally introduced tosolve a problem in biochemistry (Dietterich etal., 1997); however, it has since been applied toproblems in other areas such as classifying imageregions in computer vision (Zhang et al, 2002), andtext categorization (Andrews et al, 2003; Ray andCraven, 2005).We have extended an existing approach to rela-tion extraction using support vector machines andstring kernels (Bunescu and Mooney, 2006) to han-dle this weaker form of MIL supervision.
This ap-proach can sometimes be misled by textual featurescorrelated with the specific entities in the few train-ing pairs provided.
Therefore, we also describe amethod for weighting features in order to focus onthose correlated with the target relation rather thanwith the individual entities.
We present experimen-tal results demonstrating that our approach is able toaccurately extract relations from the web by learningfrom such weak supervision.2 Problem DescriptionWe address the task of learning a relation extrac-tion system targeted to a fixed binary relationshipR.
The only supervision given to the learning algo-576rithm is a small set of pairs of named entities that areknown to belong (positive) or not belong (negative)to the given relationship.
Table 1 shows four posi-tive and two negative example pairs for the corpo-rate acquisition relationship.
For each pair, a bag ofsentences containing the two arguments can be ex-tracted from a corpus of text documents.
The corpusis assumed to be sufficiently large and diverse suchthat, if the pair is positive, it is highly likely that thecorresponding bag contains at least one sentence thatexplicitly asserts the relationship R between the twoarguments.
In Section 6 we describe a method forextracting bags of relevant sentences from the web.+/?
Arg a1 Arg a2+ Google YouTube+ Adobe Systems Macromedia+ Viacom DreamWorks+ Novartis Eon Labs?
Yahoo Microsoft?
Pfizer TevaTable 1: Corporate Acquisition Pairs.Using a limited set of entity pairs (e.g.
those inTable 1) and their associated bags as training data,the aim is to induce a relation extraction system thatcan reliably decide whether two entities mentionedin the same sentence exhibit the target relationshipor not.
In particular, when tested on the examplesentences from Figure 1, the system should classifyS1, S3,and S4 as positive, and S2 and S5 as negative.+/S1: Search engine giant Google has bought video-sharing website YouTube in a controversial $1.6 billiondeal.
?/S2: The companies will merge Google?s search ex-pertise with YouTube?s video expertise, pushing whatexecutives believe is a hot emerging market of videooffered over the Internet.+/S3: Google has acquired social media company,YouTube for $1.65 billion in a stock-for-stock transactionas announced by Google Inc. on October 9, 2006.+/S4: Drug giant Pfizer Inc. has reached an agreementto buy the private biotechnology firm Rinat NeuroscienceCorp., the companies announced Thursday.
?/S5: He has also received consulting fees from Al-pharma, Eli Lilly and Company, Pfizer, Wyeth Pharmaceu-ticals, Rinat Neuroscience, Elan Pharmaceuticals, and For-est Laboratories.Figure 1: Sentence examples.As formulated above, the learning task can beseen as an instance of multiple instance learning.However, there are important properties that set itapart from problems previously considered in MIL.The most distinguishing characteristic is that thenumber of bags is very small, while the average sizeof the bags is very large.3 Multiple Instance LearningSince its introduction by Dietterich (1997), an ex-tensive and quite diverse set of methods have beenproposed for solving the MIL problem.
For the taskof relation extraction, we consider only MIL meth-ods where the decision function can be expressed interms of kernels computed between bag instances.This choice was motivated by the comparativelyhigh accuracy obtained by kernel-based SVMs whenapplied to various natural language tasks, and in par-ticular to relation extraction.
Through the use of ker-nels, SVMs (Vapnik, 1998; Scho?lkopf and Smola,2002) can work efficiently with instances that im-plicitly belong to a high dimensional feature space.When used for classification, the decision functioncomputed by the learning algorithm is equivalent toa hyperplane in this feature space.
Overfitting isavoided in the SVM formulation by requiring thatpositive and negative training instances be maxi-mally separated by the decision hyperplane.Gartner et al (2002) adapted SVMs to the MILsetting using various multi-instance kernels.
Twoof these ?
the normalized set kernel, and the statis-tic kernel ?
have been experimentally compared toother methods by Ray and Craven (2005), with com-petitive results.
Alternatively, a simple approach toMIL is to transform it into a standard supervisedlearning problem by labeling all instances from pos-itive bags as positive.
An interesting outcome of thestudy conducted by Ray and Craven (2005) was that,despite the class noise in the resulting positive ex-amples, such a simple approach often obtains com-petitive results when compared against other moresophisticated MIL methods.We believe that an MIL method based on multi-instance kernels is not appropriate for trainingdatasets that contain just a few, very large bags.
Ina multi-instance kernel approach, only bags (andnot instances) are considered as training examples,577which means that the number of support vectors isgoing to be upper bounded by the number of train-ing bags.
Taking the bags from Table 1 as a sam-ple training set, the decision function is going to bespecified by at most seven parameters: the coeffi-cients for at most six support vectors, plus an op-tional bias parameter.
A hypothesis space character-ized by such a small number of parameters is likelyto have insufficient capacity.Based on these observations, we decided to trans-form the MIL problem into a standard supervisedproblem as described above.
The use of this ap-proach is further motivated by its simplicity and itsobserved competitive performance on very diversedatasets (Ray and Craven, 2005).
Let X be the setof bags used for training, Xp ?
X the set of posi-tive bags, and Xn ?
X the set of negative bags.
Forany instance x ?
X from a bag X ?
X , let ?
(x)be the (implicit) feature vector representation of x.Then the corresponding SVM optimization problemcan be formulated as in Figure 2:minimize:J(w, b, ?)
= 12?w?2 + CL(cp LnL ?p + cnLpL ?n)?p =?X?Xp?x?X?x?n =?X?Xn?x?X?xsubject to:w?
(x) + b ?
+1?
?x, ?x ?
X ?
Xpw?
(x) + b ?
?1 + ?x, ?x ?
X ?
Xn?x ?
0Figure 2: SVM Optimization Problem.The capacity control parameter C is normalizedby the total number of instances L = Lp + Ln =?X?Xp |X| +?X?Xn |X|, so that it remains in-dependent of the size of the dataset.
The additionalnon-negative parameter cp (cn = 1?cp) controls therelative influence that false negative vs. false posi-tive errors have on the value of the objective func-tion.
Because not all instances from positive bagsare real positive instances, it makes sense to havefalse negative errors be penalized less than false pos-itive errors (i.e.
cp < 0.5).In the dual formulation of the optimization prob-lem from Figure 2, bag instances appear only insidedot products of the form K(x1, x2) = ?(x1)?
(x2).The kernel K is instantiated to a subsequence ker-nel, as described in the next section.4 Relation Extraction KernelThe training bags consist of sentences extractedfrom online documents, using the methodology de-scribed in Section 6.
Parsing web documents inorder to obtain a syntactic analysis often gives un-reliable results ?
the type of narrative can varygreatly from one web document to another, and sen-tences with grammatical errors are frequent.
There-fore, for the initial experiments, we used a modi-fied version of the subsequence kernel of Bunescuand Mooney (2006), which does not require syn-tactic information.
This kernel computes the num-ber of common subsequences of tokens between twosentences.
The subsequences are constrained to be?anchored?
at the two entity names, and there isa maximum number of tokens that can appear ina sequence.
For example, a subsequence featurefor the sentence S1 in Figure 1 is s?
= ??e1?
.
.
.bought .
.
.
?e2?
.
.
.
in .
.
.
billion .
.
.
deal?, where?e1?
and ?e2?
are generic placeholders for the twoentity names.
The subsequence kernel induces afeature space where each dimension correspondsto a sequence of words.
Any such sequence thatmatches a subsequence of words in a sentence exam-ple is down-weighted as a function of the total lengthof the gaps between every two consecutive words.More exactly, let s = w1w2 .
.
.
wk be a sequence ofk words, and s?
= w1 g1 w2 g2 .
.
.
wk?1 gk?1 wk amatching subsequence in a relation example, wheregi stands for any sequence of words between wi andwi+1.
Then the sequence s will be represented in therelation example as a feature with weight computedas ?
(s) = ?g(s?).
The parameter ?
controls the mag-nitude of the gap penalty, where g(s?)
= ?i |gi| isthe total gap.Many relations, like the ones that we explore inthe experimental evaluation, cannot be expressedwithout using at least one content word.
We there-fore modified the kernel computation to optionallyignore subsequence patterns formed exclusively of578stop words and punctuation signs.
In Section 5.1,we introduce a new weighting scheme, wherein aweight is assigned to every token.
Correspondingly,every sequence feature will have an additional mul-tiplicative weight, computed as the product of theweights of all the tokens in the sequence.
The aimof this new weighting scheme, as detailed in the nextsection, is to eliminate the bias caused by the specialstructure of the relation extraction MIL problem.5 Two Types of BiasAs already hinted at the end of Section 2, there isone important property that distinguishes the cur-rent MIL setting for relation extraction from otherMIL problems: the training dataset contains veryfew bags, and each bag can be very large.
Con-sequently, an application of the learning model de-scribed in Sections 3 & 4 is bound to be affected bythe following two types of bias: [Type I Bias] By definition, all sentences insidea bag are constrained to contain the same two ar-guments.
Words that are semantically correlatedwith either of the two arguments are likely to oc-cur in many sentences.
For example, consider thesentences S1 and S2 from the bag associated with?Google?
and ?YouTube?
(as shown in Figure 1).They both contain the words ?search?
?
highly cor-related with ?Google?, and ?video?
?
highly corre-lated with ?YouTube?, and it is likely that a signifi-cant percentage of sentences in this bag contain oneof the two words (or both).
The two entities can bementioned in the same sentence for reasons otherthan the target relation R, and these noisy trainingsentences are likely to contain words that are corre-lated with the two entities, without any relationshipto R. A learning model where the features are basedon words, or word sequences, is going to give toomuch weight to words or combinations of words thatare correlated with either of individual arguments.This overweighting will adversely affect extractionperformance through an increased number of errors.A method for eliminating this type of bias is intro-duced in Section 5.1. [Type II Bias] While Type I bias is due to wordsthat are correlated with the arguments of a relationinstance, the Type II bias is caused by words thatare specific to the relation instance itself.
UsingFrameNet terminology (Baker et al, 1998), thesecorrespond to instantiated frame elements.
For ex-ample, the corporate acquisition frame can be seenas a subtype of the ?Getting?
frame in FrameNet.The core elements in this frame are the Recipi-ent (e.g.
Google) and the Theme (e.g.
YouTube),which for the acquisition relationship coincide withthe two arguments.
They do not contribute anybias, since they are replaced with the generic tags?e1?
and ?e2?
in all sentences from the bag.
Thereare however other frame elements ?
peripheral, orextra-thematic ?
that can be instantiated with thesame value in many sentences.
In Figure 1, for in-stance, sentence S3 contains two non-core frame ele-ments: the Means element (e.g ?in a stock-for-stocktransaction?)
and the Time element (e.g.
?on Oc-tober 9, 2006?).
Words from these elements, like?stock?, or ?October?, are likely to occur very oftenin the Google-YouTube bag, and because the train-ing dataset contains only a few other bags, subse-quence patterns containing these words will be giventoo much weight in the learned model.
This is prob-lematic, since these words can appear in many otherframes, and thus the learned model is likely to makeerrors.
Instead, we would like the model to fo-cus on words that trigger the target relationship (inFrameNet, these are the lexical units associated withthe target frame).5.1 A Solution for Type I BiasIn order to account for how strongly the words in asequence are correlated with either of the individualarguments of the relation, we modify the formula forthe sequence weight ?
(s) by factoring in a weight?
(w) for each word in the sequence, as illustrated inEquation 1.?
(s) = ?g(s?)
??w?s?
(w) (1)Given a predefined set of weights ?
(w), it is straight-forward to update the recursive computation ofthe subsequence kernel so that it reflects the newweighting scheme.If all the word weights are set to 1, then the newkernel is equivalent to the old one.
What we want,however, is a set of weights where words that arecorrelated with either of the two arguments are givenlower weights.
For any word, the decrease in weight579should reflect the degree of correlation between thatword and the two arguments.
Before showing theformula used for computing the word weights, wefirst introduce some notation:?
Let X ?
X be an arbitrary bag, and let X.a1and X.a2 be the two arguments associated withthe bag.?
Let C(X) be the size of the bag (i.e.
the num-ber of sentences in the bag), and C(X,w) thenumber of sentences in the bag X that containthe word w. Let P (w|X) = C(X,w)/C(X).?
Let P (w|X.a1 ?
X.a2) be the probability thatthe word w appears in a sentence due only tothe presence of X.a1 or X.a2, assuming X.a1and X.a2 are independent causes for w.The word weights are computed as follows:?
(w) = C(X,w)?
P (w|X.a1 ?X.a2) ?
C(X)C(X,w)= 1?
P (w|X.a1 ?X.a2)P (w|X) (2)The quantity P (w|X.a1 ?
X.a2) ?
C(X) representsthe expected number of sentences in which w wouldoccur, if the only causes were X.a1 or X.a2, inde-pendent of each other.
We want to discard this quan-tity from the total number of occurrences C(X,w),so that the effect of correlations with X.a1 or X.a2is eliminated.We still need to compute P (w|X.a1 ?X.a2).
Be-cause in the definition of P (w|X.a1 ?X.a2), the ar-guments X.a1 and X.a2 were considered indepen-dent causes, P (w|X.a1 ?
X.a2) can be computedwith the noisy-or operator (Pearl, 1986):P (?)
= 1?
(1?P (w|a1)) ?
(1?P (w|a2)) (3)= P (w|a1)+P (w|a2)?P (w|a1) ?
P (w|a2)The quantity P (w|a) represents the probability thatthe word w appears in a sentence due only to thepresence of a, and it could be estimated using countson a sufficiently large corpus.
For our experimen-tal evaluation, we used the following approxima-tion: given an argument a, a set of sentences con-taining a are extracted from web documents (de-tails in Section 6).
Then P (w|a) is simply approxi-mated with the ratio of the number of sentences con-taining w over the total number of sentences, i.e.P (w|a) = C(w, a)/C(a).
Because this may be anoverestimate (w may appear in a sentence contain-ing a due to causes other than a), and also becauseof data sparsity, the quantity ?
(w) may sometimesresult in a negative value ?
in these cases it is set to0, which is equivalent to ignoring the word w in allsubsequence patterns.6 MIL Relation Extraction DatasetsFor the purpose of evaluation, we created twodatasets: one for corporate acquisitions, as shownin Table 2, and one for the person-birthplace rela-tion, with the example pairs from Table 3.
In bothtables, the top part shows the training pairs, whilethe bottom part shows the test pairs.+/?
Arg a1 Arg a2 Size+ Google YouTube 1375+ Adobe Systems Macromedia 622+ Viacom DreamWorks 323+ Novartis Eon Labs 311?
Yahoo Microsoft 163?
Pfizer Teva 247+ Pfizer Rinat Neuroscience 50 (41)+ Yahoo Inktomi 433 (115)?
Google Apple 281?
Viacom NBC 231Table 2: Corporate Acquisition Pairs.+/?
Arg a1 Arg a2 Size+ Franz Kafka Prague 552+ Andre Agassi Las Vegas 386+ Charlie Chaplin London 292+ George Gershwin New York 260?
Luc Besson New York 74?
Wolfgang A. Mozart Vienna 288+ Luc Besson Paris 126 (6)+ Marie Antoinette Vienna 105 (39)?
Charlie Chaplin Hollywood 266?
George Gershwin London 104Table 3: Person-Birthplace Pairs.Given a pair of arguments (a1, a2), the corre-sponding bag of sentences is created as follows: A query string ?a1 ?
?
?
?
?
?
?
a2?
containingseven wildcard symbols between the two argumentsis submitted to Google.
The preferences are set tosearch only for pages written in English, with Safe-search turned on.
This type of query will match doc-uments where an occurrence of a1 is separated froman occurrence of a2 by at most seven content words.This is an approximation of our actual information580need: ?return all documents containing a1 and a2 inthe same sentence?. The returned documents (limited by Google tothe first 1000) are downloaded, and then the textis extracted using the HTML parser from the JavaSwing package.
Whenever possible, the appropriateHTML tags (e.g.
BR, DD, P, etc.)
are used as hardend-of-sentence indicators.
The text is further seg-mented into sentences with the OpenNLP1 package. Sentences that do not contain both arguments a1and a2 are discarded.
For every remaining sentence,we find the occurrences of a1 and a2 that are clos-est to each other, and create a relation example byreplacing a1 with ?e1?
and a2 with ?e2?.
All otheroccurrences of a1 and a2 are replaced with a nulltoken ignored by the subsequence kernel.The number of sentences in every bag is shown inthe last column of Tables 2 & 3.
Because Googlealso counts pages that are deemed too similar in thefirst 1000, some of the bags can be relatively small.As described in Section 5.1, the word-argumentcorrelations are modeled through the quantityP (w|a) = C(w, a)/C(a), estimated as the ratio be-tween the number of sentences containing w and a,and the number of sentences containing a. Thesecounts are computed over a bag of sentences con-taining a, which is created by querying Google forthe argument a, and then by processing the resultsas described above.7 Experimental EvaluationEach dataset is split into two sets of bags: onefor training and one for testing.
The test datasetwas purposefully made difficult by including neg-ative bags with arguments that during training wereused in positive bags, and vice-versa.
In order toevaluate the relation extraction performance at thesentence level, we manually annotated all instancesfrom the positive test bags.
The last column in Ta-bles 2 & 3 shows, between parentheses, how manyinstances from the positive test bags are real pos-itive instances.
The corporate acquisition test sethas a total of 995 instances, out of which 156 arepositive.
The person-birthplace test set has a totalof 601 instances, and only 45 of them are positive.Extrapolating from the test set distribution, the pos-1http://opennlp.sourceforge.netitive bags in the person-birthplace dataset are sig-nificantly sparser in real positive instances than thepositive bags in the corporate acquisition dataset.The subsequence kernel described in Section 4was used as a custom kernel for the LibSVM2 Javapackage.
When run with the default parameters,the results were extremely poor ?
too much weightwas given to the slack term in the objective func-tion.
Minimizing the regularization term is essen-tial in order to capture subsequence patterns sharedamong positive bags.
Therefore LibSVM was mod-ified to solve the optimization problem from Fig-ure 2, where the capacity parameter C is normal-ized by the size of the transformed dataset.
In thisnew formulation, C is set to its default value of 1.0?
changing it to other values did not result in signifi-cant improvement.
The trade-off between false pos-itive and false negative errors is controlled by theparameter cp.
When set to its default value of 0.5,false-negative errors and false positive errors havethe same impact on the objective function.
As ex-pected, setting cp to a smaller value (0.1) resultedin better performance.
Tests with even lower valuesdid not improve the results.We compare the following four systems: SSK?MIL: This corresponds to the MIL formu-lation from Section 3, with the original subsequencekernel described in Section 4. SSK?T1: This is the SSK?MIL system aug-mented with word weights, so that the Type I biasis reduced, as described in Section 5.1. BW-MIL: This is a bag-of-words kernel, inwhich the relation examples are classified based onthe unordered words contained in the sentence.
Thisbaseline shows the performance of a standard text-classification approach to the problem using a state-of-the art algorithm (SVM). SSK?SIL: This corresponds to the original sub-sequence kernel trained with traditional, single in-stance learning (SIL) supervision.
For evaluation,we train on the manually labeled instances from thetest bags.
We use a combination of one positive bagand one negative bag for training, while the othertwo bags are used for testing.
The results are aver-aged over all four possible combinations.
Note thatthe supervision provided to SSK?SIL requires sig-2http://www.csie.ntu.edu.tw/?cjlin/libsvm58101020304050607080901000  10  20  30  40  50  60  70  80  90  100Precision(%)Recall (%)SSK-T1SSK-MILBW-MIL01020304050607080901000  10  20  30  40  50  60  70  80  90  100Precision(%)Recall (%)SSK-T1SSK-MILBW-MIL(a) Corporate Acquisitions (b) Person-BirthplaceFigure 3: Precision-Recall graphs on the two datasets.nificantly more annotation effort, therefore, given asufficient amount of training examples, we expectthis system to perform at least as well as its MILcounterpart.In Figure 3, precision is plotted against recall byvarying a threshold on the value of the SVM deci-sion function.
To avoid clutter, we show only thegraphs for the first three systems.
In Table 4 weshow the area under the precision recall curves ofall four systems.
Overall, the learned relation extrac-tors are able to identify the relationship in novel sen-tences quite accurately and significantly out-performa bag-of-words baseline.
The new version of thesubsequence kernel SSK?T1 is significantly moreaccurate in the MIL setting than the original sub-sequence kernel SSK?MIL, and is also competitivewith SSK?SIL, which was trained using a reason-able amount of manually labeled sentence examples.Dataset SSK?MIL SSK?T1 BW?MIL SSK?SIL(a) CA 76.9% 81.1% 45.9% 80.4%(b) PB 72.5% 78.2% 69.2% 73.4%Table 4: Area Under Precision-Recall Curve.8 Future WorkAn interesting potential application of our approachis a web relation-extraction system similar to GoogleSets, in which the user provides only a handful ofpairs of entities known to exhibit or not to exhibita particular relation, and the system is used to findother pairs of entities exhibiting the same relation.Ideally, the user would only need to provide pos-itive pairs.
Sentences containing one of the rela-tion arguments could be extracted from the web, andlikely negative sentence examples automatically cre-ated by pairing this entity with other named enti-ties mentioned in the sentence.
In this scenario, thetraining set can contain both false positive and falsenegative noise.
One useful side effect is that TypeI bias is partially removed ?
some bias still remainsdue to combinations of at least two words, each cor-related with a different argument of the relation.We are also investigating methods for reducing TypeII bias, either by modifying the word weights, or byintegrating an appropriate measure of word distri-bution across positive bags directly in the objectivefunction for the MIL problem.
Alternatively, im-plicit negative evidence can be extracted from sen-tences in positive bags by exploiting the fact that, be-sides the two relation arguments, a sentence from apositive bag may contain other entity mentions.
Anypair of entities different from the relation pair is verylikely to be a negative example for that relation.
Thisis similar to the concept of negative neighborhoodsintroduced by Smith and Eisner (2005), and has thepotential of eliminating both Type I and Type II bias.9 Related WorkOne of the earliest IE methods designed to workwith a reduced amount of supervision is that ofHearst (1992), where a small set of seed patternsis used in a bootstrapping fashion to mine pairs of582hypernym-hyponym nouns.
Bootstrapping is actu-ally orthogonal to our method, which could be usedas the pattern learner in every bootstrapping itera-tion.
A more recent IE system that works by boot-strapping relation extraction patterns from the web isKNOWITALL (Etzioni et al, 2005).
For a given tar-get relation, supervision in KNOWITALL is providedas a rule template containing words that describe theclass of the arguments (e.g.
?company?
), and a smallset of seed extraction patterns (e.g.
?has acquired?
).In our approach, the type of supervision is different ?we ask only for pairs of entities known to exhibit thetarget relation or not.
Also, KNOWITALL requireslarge numbers of search engine queries in order tocollect and validate extraction patterns, therefore ex-periments can take weeks to complete.
Compara-tively, the approach presented in this paper requiresonly a small number of queries: one query per rela-tion pair, and one query for each relation argument.Craven and Kumlien (1999) create a noisy train-ing set for the subcellular-localization relation bymining Medline for sentences that contain tuplesextracted from relevant medical databases.
To ourknowledge, this is the first approach that is using a?weakly?
labeled dataset for relation extraction.
Theresulting bags however are very dense in positive ex-amples, and they are also many and small ?
conse-quently, the two types of bias are not likely to havesignificant impact on their system?s performance.10 ConclusionWe have presented a new approach to relation ex-traction that leverages the vast amount of informa-tion available on the web.
The new RE system istrained using only a handful of entity pairs known toexhibit and not exhibit the target relationship.
Wehave extended an existing relation extraction ker-nel to learn in this setting and to resolve problemscaused by the minimal supervision provided.
Exper-imental results demonstrate that the new approachcan reliably extract relations from web documents.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir helpful suggestions.
This work was supportedby grant IIS-0325116 from the NSF, and a gift fromGoogle Inc.ReferencesStuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann.2003.
Support vector machines for multiple-instance learn-ing.
In NIPS 15, pages 561?568, Vancouver, BC.
MIT Press.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998.The Berkeley FrameNet project.
In Proc.
of COLING?ACL?98, pages 86?90, San Francisco, CA.
Morgan KaufmannPublishers.Razvan C. Bunescu and Raymond J. Mooney.
2006.
Sub-sequence kernels for relation extraction.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, NIPS 18.M.
Craven and J. Kumlien.
1999.
Constructing biologi-cal knowledge bases by extracting information from textsources.
In Proc.
of ISMB?99, pages 77?86, Heidelberg,Germany.Aron Culotta and Jeffrey Sorensen.
2004.
Dependency treekernels for relation extraction.
In Proc.
of ACL?04, pages423?429, Barcelona, Spain, July.Thomas G. Dietterich, Richard H. Lathrop, and Tomas Lozano-Perez.
1997.
Solving the multiple instance problem withaxis-parallel rectangles.
Artificial Intelligence, 89(1-2):31?71.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-MariaPopescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,and Alexander Yates.
2005.
Unsupervised named-entity ex-traction from the web: an experimental study.
Artificial In-telligence, 165(1):91?134.T.
Gartner, P.A.
Flach, A. Kowalczyk, and A.J.
Smola.
2002.Multi-instance kernels.
In In Proc.
of ICML?02, pages 179?186, Sydney, Australia, July.
Morgan Kaufmann.M.
A. Hearst.
1992.
Automatic acquisition of hyponyms fromlarge text corpora.
In Proc.
of ACL?92, Nantes, France.Judea Pearl.
1986.
Fusion, propagation, and structuring in be-lief networks.
Artificial Intelligence, 29(3):241?288.Soumya Ray and Mark Craven.
2005.
Supervised versus mul-tiple instance learning: An empirical comparison.
In Proc.of ICML?05, pages 697?704, Bonn, Germany.Bernhard Scho?lkopf and Alexander J. Smola.
2002.
Learningwith kernels - support vector machines, regularization, opti-mization and beyond.
MIT Press, Cambridge, MA.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation: Train-ing log-linear models on unlabeled data.
In Proc.
of ACL?05,pages 354?362, Ann Arbor, Michigan.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.
JohnWiley & Sons.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernel meth-ods for relation extraction.
Journal of Machine LearningResearch, 3:1083?1106.Q.
Zhang, S. A. Goldman, W. Yu, and J. Fritts.
2002.
Content-based image retrieval using multiple-instance learning.
InProc.
of ICML?02, pages 682?689.583
