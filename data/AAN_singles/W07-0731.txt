Proceedings of the Second Workshop on Statistical Machine Translation, pages 216?219,Prague, June 2007. c?2007 Association for Computational LinguisticsThe Syntax Augmented MT (SAMT) System for the Shared Task in the 2007ACL Workshop on Statistical Machine TranslationAndreas Zollmann and Ashish Venugopal and Matthias Paulik and Stephan VogelSchool of Computer Science, Carnegie Mellon University, PittsburghinterACT Lab, University of Karlsruhe{ashishv,zollmann,paulik,vogel+}@cs.cmu.eduAbstractWe describe the CMU-UKA Syntax AugmentedMachine Translation system ?SAMT?
used for theshared task ?Machine Translation for European Lan-guages?
at the ACL 2007 Workshop on StatisticalMachine Translation.
Following an overview of syn-tax augmented machine translation, we describe pa-rameters for components in our open-source SAMTtoolkit that were used to generate translation resultsfor the Spanish to English in-domain track of theshared task and discuss relative performance againstour phrase-based submission.1 IntroductionAs Chiang (2005) and Koehn et al (2003) note,purely lexical ?phrase-based?
translation modelssuffer from sparse data effects when translating con-ceptual elements that span or skip across severalsource language words.
Phrase-based models alsorely on distance and lexical distortion models to rep-resent the reordering effects across language pairs.However, such models are typically applied overlimited source sentence ranges to prevent errors in-troduced by these models and to maintain efficientdecoding (Och and Ney, 2004).To address these concerns, hierarchically struc-tured models as in Chiang (2005) define weightedtransduction rules, interpretable as components ofa probabilistic synchronous grammar (Aho and Ull-man, 1969) that represent translation and reorderingoperations.
In this work, we describe results fromthe open-source Syntax Augmented Machine Trans-lation (SAMT) toolkit (Zollmann and Venugopal,2006) applied to the Spanish-to-English in-domaintranslation task of the ACL?07 workshop on statisti-cal machine translation.We begin by describing the probabilistic model oftranslation applied by the SAMT toolkit.
We thenpresent settings for the pipeline of SAMT tools thatwe used in our shared task submission.
Finally, wecompare our translation results to the CMU-UKAphrase-based SMT system and discuss relative per-formance.2 Synchronous Grammars for SMTProbabilistic synchronous context-free grammars(PSCFGs) are defined by a source terminal set(source vocabulary) TS , a target terminal set (targetvocabulary) TT , a shared nonterminal setN and pro-duction rules of the formX ?
?
?, ?,?, w?where following (Chiang, 2005)?
X ?
N is a nonterminal?
?
?
(N ?TS)?
: sequence of source nonterminalsand terminals?
?
?
(N ?
TT )?
: sequence of target nonterminalsand terminals?
the count #NT(?)
of nonterminal tokens in ?
isequal to the count #NT(?)
of nonterminal tokensin ?,?
?
: {1, .
.
.
,#NT(?)}
?
{1, .
.
.
,#NT(?)}
one-to-one mapping from nonterminal tokens in ?
tononterminal tokens in ??
w ?
[0,?)
: nonnegative real-valued weightChiang (2005) uses a single nonterminal cate-gory, Galley et al (2004) use syntactic constituentsfor the PSCFG nonterminal set, and Zollmann andVenugopal (2006) take advantage of CCG (Combi-natorial Categorical Grammar) (Steedman, 1999) in-spired ?slash?
and ?plus?
categories, focusing on tar-get (rather than source side) categories to generatewell formed translations.We now describe the identification and estima-tion of PSCFG rules from parallel sentence alignedcorpora under the framework proposed by Zollmannand Venugopal (2006).2162.1 Grammar InductionZollmann and Venugopal (2006) describe a processto generate a PSCFG given parallel sentence pairs?f, e?, a parse tree pi for each e, the maximum aposteriori word alignment a over ?f, e?, and phrasepairs Phrases(a) identified by any alignment-drivenphrase induction technique such as e.g.
(Och andNey, 2004).Each phrase in Phrases(a) (phrases identifiablefrom a) is first annotated with a syntactic categoryto produce initial rules.
If the target span of thephrase does not match a constituent in pi, heuristicsare used to assign categories that correspond to par-tial rewriting of the tree.
These heuristics first con-sider concatenation operations, forming categorieslike ?NP+VP?, and then resort to CCG style ?slash?categories like ?NP/NN?
giving preference to cate-gories found closer to the leaves of the tree.To illustrate this process, consider the followingFrench-English sentence pair and selected phrasepairs obtained by phrase induction on an automat-ically produced alignment a, and matching targetspans with pi.f = il ne va pase = he does not goPRP ?
il, heVB ?
va, goRB+VB ?
ne va pas, not goS ?
il ne va pas, he does not goThe alignment a with the associated target sideparse tree is shown in Fig.
1 in the alignment visual-ization style defined by Galley et al (2004).Following the Data-Oriented Parsing inspiredrule generalization technique proposed by Chiang(2005), one can now generalize each identifiedrule (initial or already partially generalized) N ?f1 .
.
.
fm/e1 .
.
.
en for which there is an initial ruleM ?
fi .
.
.
fu/ej .
.
.
ev where 1 ?
i < u ?
m and1 ?
j < v ?
n, to obtain a new ruleN ?
f1 .
.
.
fi?1Mkfu+1 .
.
.
fm/e1 .
.
.
ej?1Mkev+1 .
.
.
enwhere k is an index for the nonterminal M that in-dicates the one-to-one correspondence between thenew M tokens on the two sides (it is not in the spaceof word indices like i, j, u, v,m, n).
The initial ruleslisted above can be generalized to additionally ex-tract the following rules from f, e.S ?
PRP1 ne va pas , PRP1 does not goS ?
il ne VB1 pas , he does not VB1S ?
il RB+VB1, he does RB+VB1S ?
PRP1 RB+VB2, PRP1 does RB+VB2RB+VB ?
ne VB1 pas , not VB1Fig.
2 uses regions to identify the labeled, sourceand target side span for all initial rules extracted onour example sentence pair and parse.
Under this rep-resentation, generalization can be viewed as a pro-cess that selects a region, and proceeds to subtractout any sub-region to form a generalized rule.SqqqqqqqMMMMMMMNP VPqqqqqqqMMMMMMMPRN AUX RB VBhe does notqqqqqqqMMMMMMMgoqqqqqqqil ne va pasFigure 1: Alignment graph (word alignment and target parsetree) for a French-English sentence pair.il 1 ne 2 va 3 pas 4he 1does 2not 3go 4ffiffff9SRB+VBVBVPNP+AUXNPFigure 2: Spans of initial lexical phrases w.r.t.
f, e. Each phraseis labeled with a category derived from the tree in Fig.
1.2.2 DecodingGiven a source sentence f , the translation task undera PSCFG grammar can be expressed analogously tomonolingual parsing with a CFG.
We find the mostlikely derivation D with source-side f and read offthe English translation from this derivation:e?
= tgt(argmaxD:src(D)=fp(D))(1)where tgt(D) refers to the target terminals andsrc(D) to the source terminals generated by deriva-tion D.Our distribution p over derivations is defined by alog-linear model.
The probability of a derivation D217is defined in terms of the rules r that are used in D:p(D) =pLM (tgt(D))?LM?r?D?i ?i(r)?iZ(?
)(2)where ?i refers to features defined on each rule,pLM is a language model (LM) probability applied tothe target terminal symbols generated by the deriva-tion D, and Z(?)
is a normalization constant cho-sen such that the probabilities sum up to one.
Thecomputational challenges of this search task (com-pounded by the integration of the LM) are addressedin (Chiang, 2007; Venugopal et al, 2007).
Thefeature weights ?i are trained in concert with theLM weight via minimum error rate (MER) training(Och, 2003).We now describe the parameters for the SAMTimplementation of the model described above.3 SAMT ComponentsSAMT provides tools to perform grammar induc-tion ( ?extractrules?, ?filterrules?
), from bilingualphrase pairs and target language parse trees, as wellas translation (?FastTranslateChart?)
of source sen-tences given an induced grammar.3.1 extractrulesextractrules is the first step of the grammar induc-tion pipeline, where rules are identified based on theprocess described in section 2.1.
This tool works ona per sentence basis, considering phrases extractedfor the training sentence pair ?si, ti?
and the corre-sponding target parse tree pii.
extractrules outputsidentified rules for each input sentence pair, alongwith associated statistics that play a role in the esti-mation of the rule features ?.
These statistics takethe form of real-valued feature vectors for each ruleas well as summary information collected over thecorpus, such as the frequency of each nonterminalsymbol, or unique rule source sides encountered.For the shared task evaluation, we ran extrac-trules with the following extraction parametersettings to limit the scope and number of rulesextracted.
These settings produce the same initialphrase table as the CMU-UKA phrase based sys-tem.
We limit the source-side length of the phrasepairs considered as initial rules to 8 (parameterMaxSourceLength).
Further we set the max-imum number of source and target terminals perrule (MaxSource/MaxTargetWordCount)to 5 and 8 respectively with 2 of nonter-minal pairs (i.e., substitution sites) per rule(MaxSubstititionCount).
We limit thetotal number of symbols in each rule to 8(MaxSource/TargetSymbolCount) andrequire all rules to contain at least one source-sideterminal symbol (noAllowAbstractRules,noAllowRulesWithOnlyTargetTerminals)since this reduces decoding time considerably.
Ad-ditionally, we discard all rules that contain sourceword sequences that do not exist in the developmentand test sets provided for the shared task (parameter-r).3.2 filterrulesThis tool takes as input the rules identified by ex-tractrules, and associates each rule with a featurevector ?, representing multiple criteria by which thedecoding process can judge the quality of each ruleand, by extension, each derivation.
filterrules is alsoin charge of pruning the resulting PSCFG to ensuretractable decoding.?
contains both real and Boolean valued featuresfor each rule.
The following probabilistic featuresare generated by filterrules:?
p?
(r| lhs(X)) : Probability of a rule given its left-hand-side (?result?)
nonterminal?
p?
(r| src(r)) : Prob.
of a rule given its source side?
p?
(ul(src(r)),ul(tgt(r))|ul(src(r)) : Probabilityof the unlabeled source and target side of the rulegiven its unlabeled source side.Here, the function ul removes all syntactic la-bels from its arguments, but retains ordering nota-tion, producing relative frequencies similar to thoseused in purely hierarchical systems.
As in phrase-based translation model estimation, ?
also containstwo lexical weights (Koehn et al, 2003), countersfor number of target terminals generated.
?
alsoboolean features that describe rule types (i.e.
purelyterminal vs purely nonterminal).For the shared task submission, we pruned awayrules that share the same source side based onp?
(r| src(r)) (the source conditioned relative fre-quency).
We prune away a rule if this value isless that 0.5 times the one of the best performingrule (parameters BeamFactorLexicalRules,BeamFactorNonlexicalRules).3.3 FastTranslateChartThe FastTranslateChart decoder is a chart parserbased on the CYK+(Chappelier and Rajman, 1998)algorithm.
Translation experiments in this paperare performed with a 4-gram SRI language modeltrained on the target side of the corpus.
Fast-TranslateChart implements both methods of han-dling the LM intersection described in (Venugopalet al, 2007).
For this submission, we use the Cube-Pruning (Chiang, 2007) approach (the default set-ting).
LM and rule feature parameters ?
are trainedwith the included MER training tool.
Our prun-ing settings allow up to 200 chart items per cell218with left-hand side nonterminal ?
S?
(the reservedsentence spanning nonterminal), and 100 items percell for each other nonterminal.
Beam pruningbased on an (LM-scaled) additive beam of neg-lob probability 5 is used to prune the search fur-ther.
These pruning settings correspond to setting?PruningMap=0-100-5-@_S-200-5?.4 Empirical ResultsWe trained our system on the Spanish-English in-domain training data provided for the workshop.
Ini-tial data processing and normalizing is describedin the workshop paper for the CMU-UKA ISLphrase-based system.
NIST-BLEU scores are re-ported on the 2K sentence development ?dev06?
andtest ?test06?
corpora as per the workshop guide-lines (case sensitive, de-tokenized).
We compareour scores against the CMU-UKA ISL phrase-basedsubmission, a state-of-the art phrase-based SMTsystem with part-of-speech (POS) based word re-ordering (Paulik et al, 2007).4.1 Translation ResultsThe SAMT system achieves a BLEU score of32.48% on the ?dev06?
development corpus and32.15% on the unseen ?test06?
corpus.
This isslightly better than the score of the CMU-UKAphrase-based system, which achieves 32.20% and31.85% when trained and tuned under the same in-domain conditions.
1To understand why the syntax augmented ap-proach has limited additional impact on the Spanish-to-English task, we consider the impact of reorder-ing within our phrase-based system.
Table 1 showsthe impact of increasing reordering window length(Koehn et al, 2003) on translation quality for the?dev06?
data.2 Increasing the reordering windowpast 2 has minimal impact on translation quality,implying that most of the reordering effects acrossSpanish and English are well modeled at the local orphrase level.
The benefit of syntax-based systems tocapture long-distance reordering phenomena basedon syntactic structure seems to be of limited valuefor the Spanish to English translation task.5 ConclusionsIn this work, we briefly summarized the Syntax-augmented MT model, described how we trainedand ran our implementation of that model on1The CMU-UKA phrase-based workshop submission wastuned on out-of-domain data as well.2Variant of the CMU-UKA ISL phrase-based system with-out POS based reordering.
With POS-based reordering turnedon, additional window-based reordering even for window length1 had no improvement in NIST-BLEU.ReOrder 1 2 3 4 POS SAMTBLEU 31.98 32.24 32.30 32.26 32.20 32.48Table 1: Impact of phrase based reordering model settings com-pared to SAMT on the ?dev06?
corpus measured by NIST-BLEUthe MT?07 Spanish-to-English translation task.We compared SAMT translation results toa strong phrase-based system trained underthe same conditions.
Our system is availableopen-source under the GNU General Pub-lic License (GPL) and can be downloaded atwww.cs.cmu.edu/?zollmann/samtReferencesAlfred Aho and Jeffrey Ullman.
1969.
Syntax directedtranslations and the pushdown assembler.
Journal ofComputer and System Sciences.Jean-Cedric.
Chappelier and Martin Rajman.
1998.A generalized CYK algorithm for parsing stochasticCFG.
In Proc.
of Tabulation in Parsing and Deduction(TAPD?98), Paris, France.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL.David Chiang.
2007.
Hierarchical phrase based transla-tion.
Computational Linguistics.
To appear.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.of HLT/NAACL, Boston, Massachusetts.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT/NAACL, Edmonton,Canada.Franz Och and Hermann Ney.
2004.
The alignment tem-plate approach to statistical machine translation.
Com-put.
Linguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL, Sap-poro, Japan, July 6-7.Matthias Paulik, Kay Rottmann, Jan Niehues, SiljaHildebrand, and Stephan Vogel.
2007.
The ISLphrase-based MT system for the 2007 ACL work-shop on statistical MT.
In Proc.
of the Associationof Computational Linguistics Workshop on StatisticalMachine Translation.Mark Steedman.
1999.
Alternative quantifier scope inCCG.
In Proc.
of ACL, College Park, Maryland.Ashish Venugopal, Andreas Zollmann, and Stephan Vo-gel.
2007.
An efficient two-pass approach to syn-chronous CFG driven MT.
In Proc.
of HLT/NAACL,Rochester, NY.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProc.
of the Workshop on Statistical Machine Transla-tion, HLT/NAACL, New York, June.219
