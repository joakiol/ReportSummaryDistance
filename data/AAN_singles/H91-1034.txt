A Template Matcher for Robust NLInterpretat ionEric Jackson, Douglas Appelt, John Bear,Robert Moore, and Ann PodloznySRI International333 Ravenswood AvenueMenlo Park, CA 94025AbstractIn this paper, we describe the Template Matcher, a sys-tem built at SRI to provide robust natural-languageinterpretation in the Air Travel Information System(ATIS) domain.
The system appears to be robust toboth speech recognition errors and unanticipated or dif-ficult locutions used by speakers.
We explain the mo-tivation for the Template Matcher, describe in generalterms how it works in comparison with similar systems,and examine its performance.
We discuss some limita-tions of this approach, and sketch a plan for integratingthe Template Matcher with an analytic parser, which webelieve will combine the advantages of both.IntroductionOne of the conclusions SRI has drawn from working withthe ATIS common task data is that, even with a veryconstrained user task, there will always be unanticipatedexpressions and difficult constructions in the spoken lan-guage elicted by the task that will cause problems fora conventional, analytical approach to natural-languageprocessing.
However, it also seems that requests for onlya few types of information account for a very large pro-portion of the utterances produced by users performinga task like air travel planning.
This point is illustratedby some of the more difficult queries in the June 1990test set:discontinuity.
The third example would be straightfor-ward, except for the fact that the verb "servicing" hasbeen substituted for the more conventional "serving.
"Despite the difficult linguistic problems posed by thesequeries, the information they request is very simple---just fares, flights, and airlines for travel between a pairof specified cities.Consideration of examples uch as these has led us tomodify our approach to natural-language processing inspoken language systems.
The key modification to oursystem is the addition of a Template Matcher to pro-vide robust interpretation for the most common typesof requests in the task domain.
The Template Matcherachieves robustness in two ways: (1) it provides an inter-pretation when not all the words or constructions in anutterance have been accounted for, and (2) it providesa mechanism for trading-off the risk of wrong answerswith the degree of coverage.
These properties arise froma mechanism that assigns scores to interpretations, pe-nalizing interpretations that do not account for wordsin the utterance.
The bulk of this paper is devoted todescribing the Template Matcher and discussing its per-formance as a stand-alone system for interpretation ofnaturM-language queries for the ATIS task.
Later in thepaper we consider how such a module might best fit intoa complete system for spoken-language understanding.Give me a list of all airfares for round-trip tick-ets from Dallas to Boston flying on AmericanAirlines.Show me all the flights and their fares from SanFrancisco to Boston on June second.I need information on airlines servicing Bostonflying from Dallas.In the first example the phrase "flying on AmericanAirlines" apparently modifies "tickets," with the flightsthat the tickets are for apparently being the implied sub-ject of "flying."
The second example seems to containa discontinuous constituent, "flights .. from San Fran-cisco to Boston on June second," which is the antecedentof the pronoun "their" that occurs in the middle of theDescription of the SystemThe Template Matcher operates by trying to build "tem-plates" from information it finds in the sentence.
Basedon an analysis of the types of sentences observed in theATIS corpus, we devised four templates that accountfor most of the data: flight, fare, ground transportation,and meanings of codes and headings.
We have recentlyadded several new templates, including aircraft, city, air-line, and airport.
Templates consist of slots which theTemplate Matcher fills with information contained in theuser input.
Slots are filled by looking through the sen-tence for particular kinds of short phrases.
For example,"from" followed by an airport or city name will causethe "origin" slot to be filled with the appropriate name.The sentence190Show me all the United flights Boston to Dallasnonstop on the third of November leaving afterfour in the afternoon.would generate the following flight template:If i ight, \[stops ,nonstop\],\[airline ,UA\],\[origin, BOSTON\],\[destination,DALLAS\],\[departing_after, \[16001\],\[date, \[november, 3, current_year\] \] \]The template score is basically the percentage ofwordsin the sentence that contribute in some way to the build-ing of that template.
Given an input sentence, the Tem-plate Matcher constructs one template of each sort, andthe one with the best score is used to construct thedatabase query, provided its score is greater than a cer-tain "cut-off" parameter.
The cut-off parameter is whatpermits the risk trade-off mentioned above: the higherthe cut-off, the more conservative the system is in at-tempting to produce a response.
Words can contributeto a score in different ways: words that fill a slot (e.g.,"Boston") add to the score, words that help get a slotfilled (e.g.
"from") also add to the score.
Some wordsmay not contribute to the interpretation, but nonethe-less confirm the choice of a particular template (e.g.,"downtown" for the ground transportation template),and hence are added to the score for that template.Other words are ignored for the purposes of scoring (e.g.,"and, .
.
.
.
please, .
.
.
.
ok," and "show"), since they do nottend to confirm particular templates.In certain cases the Template Matcher may modify thebasic score of a template.
Each template has a set of keywords (or key phrases).
The presence of these words orphrases in a sentence is a strong indication that the asso-ciated template is the appropriate one for that sentence.For the flight template, the keywords include words like"flight," "fly," and "go"; for the fare template, wordsand phrases such as "how much," "fare," and "price"are examples; for the meaning template, examples in-clude "what is," "explain," and "define."
If none of atemplate's key words are present in a sentence then thattemplate's score is docked by a certain keyword punish-ment factor, which varies from template to template.
Inmost cases the lack of a keyword will prevent he asso-ciated template from scoring above the cut-off.There are two situations in which the TemplateMatcher will "abort" a given template, that is, give ita score of zero and cease processing it.
First, if the sys-tem tries to fill a slot in a certain template with twodifferent values, that template is aborted.
Since we haveno better than a fifty-fifty chance of guessing which is thecorrect filler, we are better off not attempting any an-swer.
Second, if a template has no slots filled, it will re-ceive a score of zero.
This restriction is relaxed when theTemplate Matcher is operating in "context-dependent"mode, where follow-up questions are expected.
A querylike "show me the fares," which would not fill any slots,would be much more likely as a follow-up question thanas a context-independent query.Comparison with Other SystemsSystems using the basic idea behind the TemplateMatcher go back as least as far as the SAM system atYale \[2\], and include the Phoenix system at CMU \[3, 4\]and the SCISOR system at General Electric \[5\] as re-cent examples.
There is also a degree of similarity to"case-frame"-based parsing methods \[6, 7\].
The maindistinction is that the slots in our templates are domain-specific concepts rather than general inguistic or con-ceptual cases.Of these precursors, the Phoenix system seems mostsimilar to the Template Matcher.
Like the TemplateMatcher, the Phoenix system has templates (which theycall "frames") with slots that get filled with informationfrom the sentence.
The scoring mechanisms of the twosystems are similar, but not identical.
For both, thebasic score of an interpretation is the number of wordsin the sentence that the interpretation accounts for.
Inthe Phoenix system, for a word in a sentence to countfor an interpretation's score, it must help fill some slot inthat interpretation's frame.
For the Template Matcher,the word will also count if it is an "ignore" or "confirm"word as discussed above.There are several other differences between the scoringmechanisms of the two systems: The Template Matcherpunishes templates that do not have a keyword presentin the sentence, and the Template Matcher requires thatat least one slot in a template be filled.
Also, the twosystems behave differently when an attempt is made tofill a single slot with two different fillers.
The TemplateMatcher will abort a template if this happens, whilethe Phoenix system will fill the slot with the second ofthe two possible fillers.
The latter approach will handlecertain types of false starts, but might be expected toyield more incorrect answers in other situations.
Finally,CMU is not currently using a cutoff to weed out bad in-terpretations, although given the existence of a scoringmechanism in their system, this is something they clearlycould do.ResultsAfter two weeks of development this system was testedon the June 1990 ATIS test set.
This was a fair test tothe extent that the implementor of the matching rou-tines and the templates themselves (Jackson) had notexamined the data from this test set prior to the eval-uation.
(Moore had noted, however, that the test setqueries seemed amenable to a template-matching ap-proach).
For various values of the cut-off parameter weobtained the results shown in the following table.191Cut-off Right Wrong No Answer0.000 55 13 220.833 42 4 441.000 37 2 51(These results were determined by visual inspection ofthe templates; the database retrieval code was not imple-mented at this point.)
The conclusion we drew from thistest is that a template-matching approach could quicklyyield results that were competitive with the some of thebetter results reported in the original June 1990 ATIStest.After completing the implementation of the systemand extensive development using the ATIS training data,we used the Template Matcher for the February 1991ATIS class A evaluation, in both the NL and SLS tests.The results as measured by NIST are shown below.Test Right Wrong No AnswerNL only 109 9 27SLS 96 11 38We used a cut-off of 0.8 for this evaluation, as we hadpreviously determined from training data that this valueshould come close to optimizing the number of right an-swers minus the number of wrong answers.The system for the SLS tests was a serial connectionof the version of SRI's DECIPHER system used in theATIS SPREC evaluation and the Template Matcher de-scribed above.
The answers reported in the SPREC eval-uation were edited to be in lexical SNOR format andrun through the Template Matcher exactly as in theNL tests.
It is interesting to note the relatively smalldegradation from the NL to the SLS results, despite a18.0 percent word error rate in the speech recognition;this seems to indicate the robustness of the TemplateMatcher to recognition errors.We had not planned to participate in the D1 evalua-tion, but at the request of NIST, we did those tests aswell, taking context into account by using the answer tothe first query in the D1 pair to restrict the databasesearch in answering the second query, the same tech-nique used in our ATIS demo system.
In addition, theTemplate Matcher was run in context-dependent modefor the second query of each D1 pair.
The results onthe second queries of the pairs as measured by NIST areshown in the table below.Test Right Wrong No AnswerNL only 22 3 13SLS 15 11 12We have not yet analyzed why there was a greater degra-dation in going from the NL to the SLS results in theD1 tests.Limitat ionsIn this section, we discuss some sentences that causeproblems for the Template Matcher that are not easilyresolvable.Show me flights returning from Dallas into SanFrancisco by ten P M.This sentence is a good example of the need for syn-tactic information.
The problem is that the TemplateMatcher cannot tell that the phrase "by ten P M" mod-ifies "returning," and thus constrains the arrival time.By default, it treats the "by" phrase as restricting thedeparture time, and thus misinterprets he query.What is an A fare?The problem here is that "A" is ambiguous; it maybe either the indefinite article or a fare class code.
Wehave been forced to leave the fare class code "A" out ofthe Template Matcher lexicon.
Adding it would do moreharm than good, for we would then misinterpret everyoccurence of the phrase "a fare" (with the indefinite ar-ticle), as in "Give me a fare from Boston to Dallas.
"Syntactic information could help resolve this ambiguity,as could speech information, since the determiner "a"and the letter "A" have different acoustic properties.List the fares for Delta flight eight oh sevenand Delta flight six twenty one from Dallas toDenver.Conjunctions of complex noun phrases are beyond thescope of the Template Matcher as it currently stands.The system could be modified to handle such phenom-ena, but an analytical grammar might be the more nat-ural tool for the job.Do you have to take a Y N flight only at night?This is an example of a sentence where all the wordscontribute to a certain template (the flight template, inthis case) and yet that template is not the correct one.A New Arch i tectureAs the examples in the previous section suggest, theTemplate Matcher by itself is probably not the ulti-mate solution to the problem of robust interpretation ofnatural-language queries.
We believe that the template-matching approach and an analytical parser-based ap-proach have complementary strengths and that an ap-proach that combines both of them is likely to be ulti-mately superior than either one alone.
We have thereforebegun developing a new architecture for language pro-cessing in spoken language systems that combines thetwo approaches.
Our basic strategy will be to use theanalysis produced by the parser whenever we can, butto fall back on the Template Matcher when the parser-based system fails to produce a complete analysis.
It isour conjecture, supported at least in part by the bestresults reported in the June 1990 ATIS evaluation, thatan analytical, parser-based approach can be designed sothat when it succeeds in providing a complete analysisof the input, that analysis has a very high probability192of being correct.
With the Template Matcher it seemsthat there will inevitably be a larger possibility for error,because it uses strictly less of the information availablein the utterance than a parser.
In particular, our Tem-plate Matcher can ignore words; it ignores order; and ithas almost no notion of structure.
By using the Tem-plate Marcher as a backup to the parser-based system,we eliminate the possibility of the Template Matcher get-ting a wrong interpretation of something that could besuccessfully analyzed by the parser.A second reason for running the Template Matcherafter the parser is to enable the Template Matcher touse partial results of parsing in its operation.
Our cur-rent Template Matcher uses only single words and fixedphrases as key words or slot fillers.
We are in the pro-cess of extending the Template Matcher so that it useswhole phrases that have been identified by the parserin attempting to analyze the entire utterance.
For ex-ample, we saw that the Template Matcher is unable toanalyze a phrase as complex as "returning from Dallasinto San Francisco by ten P M." Generalized to workfrom parsed phrases, the Template Matcher might beable to successfully interpret a complex utterance con-taining this phrase even if the entire utterance could notbe parsed.
Additionally, running the Template Matcheron parsed phrases hould cut down on the sheer numberof particular word patterns that have to be included inthe template specifications.The use of robust interpretation methods changes theway in which the constraints embodied in a grammarare viewed.
They must be treated as soft, rather thanhard, constraints.
This has significant implications forthe rest of a spoken language system.
If we want theparser to find grammatical fragments of the input thatmay be of use to the Template Matcher, then the parsingalgorithm we previously used, which imposed strong left-context constraints, is no longer appropriate.
We wantsomething closer to pure bottom-up parsing to find allthe phrases that the Template Matcher might use.
Wehave developed such a parser, whose details are outlinedin another paper for this workshop \[1\].Perhaps the most significant consequence of using ro-bust interpretation methods in a spoken language sys-tem, however, is that the failure to find a complete parsecan no longer be used as a hard constraint to reduce per-plexity for the speech recognizer.
An analytical grammarstill contains valuable information that should be used bythe recognizer, however.
We feel that one promising ap-proach to making use of this information is to extend theidea of a word-based statistical language model, such asa bi-gram model, to a phrase-based statistical languagemodel, e.g., a "bi-phrase" model.
The idea is simplyto estimate the probability of occurrence of a particulartype of phrase conditioned on the type of phrase thatprecedes it.
In making this work effectively, however, itis important o include some lexical information in thecategorization of phrases, usually information about thelexical head of the phrase.The ability of such a framework to capture long dis-tance constraints not captured by N-gram models is il-lustrated by an utterance such as "What airlines thatserve Boston fly 747s?"
If we want to predict the like-lihood of "fly" occuring in this context, the precedingword "Boston" gives us essentially no information.
If,however, we have identified "What airlines that serveBoston" as a noun phrase whose lexical head is "air-lines" then the likelihood of a verb whose lexical head is"fly" should be relatively high.The incorporation of a probabilistic element into thesystem raises a number of other interesting possibilities,including incorporation of probabilistic scoring based onobservations of likelihoods of particular templates forsentences in the corpus, of particular slots for each tem-plate, and of particular words for each slot; and the pos-sibility of using the Template Matcher itself as the basisof a statistical language model to guide recognition.SummaryIn sum, the Template Matcher represents a complemen-tary approach to traditional natural-language process-ing.
It has the virtues of robustness and broad coverageof many linguistic variants for requests for specific typesof information.
Although we have not discussed the issueof computational efficiency in this paper, the TemplateMatcher is noticably faster than a typical parser.
Theapproach also has the advantage of rapid developmenttime which should enhance portability to new domains.AcknowledgmentsThis research was supported by the Defense AdvancedResearch Projects Agency under Contract N00014-90-C-0085 with the Office of Naval Research.References\[1\] Moore, R.C., and Dowding, J., Efficient Bottom- UpParsing, Proceedings, Fourth DARPA Workshop onSpeech and Natural Language, February 1991.\[2\] Schank, R.C.
and Yale A.I.
Project, SAM--A StoryUnderstander, Research Report 43, Department ofComputer Science, Yale University, 1975.\[3\] Ward, W., Understanding Spontaneous Speech, Pro-ceedings, DARPA Speech and Natural LanguageWorkshop, February 1989.\[4\] Ward, W., The CMU Air Travel Information Ser-vice: Understanding Spontaneous Speech, Proceed-ings, DARPA Speech and Natural Language Work-shop, June 1990.\[5\] Rau, L.F., and Jacobs, P.S., Integrating Top-Downand Bottom-Up Strategies in a Text Processing Sys-tem, Proceedings, Second Conference on AppliedNatural Language Processing, Austin, Texas, 1988.193[6] Riesbeck, C., and Schank, R.C., Comprehensionby Computer: Expectation-Based Analysis of Sen-tences in Context, Research Report 78, Departmentof Computer Science, Yale University, 1976.
[7] Carbonell, J.G.
and Hayes, P.J., Recovery Strategiesfor Parsing Extragrammatical L nguage, TechnicalReport CMU-CS-84-107, Carnegie-Mellon Univer-sity Computer Science Technical Report, 1984.194
