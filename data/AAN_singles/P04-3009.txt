Wide Coverage Symbolic Surface RealizationCharles B. CallawayIstituto per la Ricerca Scientica e TecnologicaIstituto Trentino di Cultura, Italy (ITC-irst)callaway@itc.itAbstractRecent evaluation techniques applied to corpus-based systems have been introduced that canpredict quantitatively how well surface realizerswill generate unseen sentences in isolation.
Weintroduce a similar method for determining thecoverage on the Fuf/Surge symbolic surface re-alizer, report that its coverage and accuracy onthe Penn TreeBank is higher than that of a sim-ilar statistics-based generator, describe severalbenets that can be used in other areas of com-putational linguistics, and present an updatedversion of Surge for use in the NLG community.1 IntroductionSurface realization is the process of convertingthe semantic and syntactic representation of asentence or series of sentences into the text, orsurface form, of a particular language (Elhadad,1991; Bateman, 1995).
Most surface realiz-ers have been symbolic, grammar-based systemsusing syntactic linguistic theories like HPSG.These systems were often developed as eitherproof-of-concept implementations or to supportlarger end-to-end NLG systems which have pro-duced limited amounts of domain-specic texts.As such, determining the generic coverage ofa language has been substituted by the goal ofproducing the necessary syntactic coverage for aparticular project.
As described in (Langkilde-Geary, 2002), the result has been the use ofregression testing with hand-picked examplesrather than broad evaluations of linguistic com-petence.
Instead, large syntactically annotatedcorpora such as the Penn TreeBank (Marcus etal., 1993) have allowed statistically based sys-tems to produce large quantities of sentencesand then more objectively determine generationcoverage with automatic evaluation measures.We conducted a similar corpus-based exper-iment (Callaway, 2003) with the Fuf/Surgesymbolic surface realizer (Elhadad, 1991).
Wedescribe a direct comparison with HALogen(Langkilde-Geary, 2002) using Section 23 ofthe TreeBank, showing that the symbolic ap-proach improves upon the statistical system inboth coverage and accuracy.
We also presenta longitudinal comparison of two versions ofFuf/Surge showing a signicant improvementin its coverage and accuracy after new gram-mar and morphology rules were added.
Thisimproved version of Surge is available for usein the NLG community.2 Related Work in Wide CoverageGenerationVerifying wide coverage generation depends on(1) a large, well structured corpus, (2) a trans-formation algorithm that converts annotatedsentences into the surface realizer's expected in-put form, (3) the surface realizer itself, and (4)an automatic metric for determining the accu-racy of the generated sentences.
Large, wellstructured, syntactically marked corpora suchas the Penn TreeBank (Marcus et al, 1993) canprovide a source of example sentences, while au-tomatic metrics like simple string accuracy arecapable of giving a fast, rough estimate of qual-ity for individual sentences.Realization of text from corpora has been ap-proached in several ways.
In the case of Rat-naparkhi's generator foright information inthe air travel domain (Ratnaparkhi, 2000), thetransformation algorithm is trivial as the gen-erator uses the corpus itself (annotated with se-mantic information such as destination orightnumber) as input to a surface realizer with ann-gram model of the domain, along with a max-imum entropy probability model for selectingwhen to use which phrase.Fergus (Bangalore and Rambow, 2000) usedthe Penn TreeBank as a corpus, requiringa more substantial transformation algorithmsince it requires a lexical predicate-argumentstructure instead of the TreeBank's represen-tation.
The system uses an underlying tree-(S (NP-SBJ ((cat clause)(NP (JJ overall) (process ((type ascriptive) (tense past)))(NNS sales))) (participants(VP (VBD were) ((carrier ((cat common) (lex "sale") (number plural)(ADJP-PRD (describer ((cat adj) (lex "overall")))))(RB roughly) (attribute ((cat ap) (lex "flat")(JJ flat)))) (modifier ((cat adv) (lex "roughly")))))))Figure 1: A Penn TreeBank Sentence and Corresponding Surge Input Representationbased syntactic model to generate a set of pos-sible candidate realizations, and then choosesthe best candidate with a trigram model of theTreebank text.
An evaluation of three versionsof Fergus on randomly chosen Wall StreetJournal sentences of the TreeBank showed sim-ple string accuracy up to 58.9%.Finally, Langkilde's work on HALogen(Langkilde-Geary, 2002) uses a rewriting algo-rithm to convert the syntactically annotatedsentences from the TreeBank into a semantic in-put notation via rewrite rules.
The system usesthe transformed semantic input to create mil-lions of possible realizations (most of which aregrammatical but unwieldy) in a lattice struc-ture and then also uses n-grams to select themost probable as its output sentence.
Langk-ilde evaluated the system using the standardtrain-and-test methodology with Section 23 ofthe TreeBank as the unseen set.These systems represent a statistical ap-proach to wide coverage realization, turning toautomatic methods to evaluate coverage andquality based on corpus statistics.
However, asymbolic realizer can use the same evaluationtechnique if a method exists to transform thecorpus annotation into the realizer's input rep-resentation.
Thus symbolic realizers can alsouse the same types of evaluations employed bythe parsing and MT communities, allowing formeaningful comparisons of their performance onmetrics such as coverage and accuracy.3 The Penn TreeBankThe Penn TreeBank (Marcus et al, 1993) is alarge set of sentences bracketed for syntactic de-pendency and part of speech, covering almost 5million words of text.
The corpus is divided into24 sections, with each section having on average2000 sentences.
The representation of an exam-ple sentence is shown at the left of Figure 1.In general, many sentences contained in theTreeBank are not typical of those produced bycurrent NLG systems.
For instance, newspapertext requires extensive quoting for conveying di-alogue, special formatting for stock reports, andmethods for dealing with contractions.
Thesetypes of constructions are not available in cur-rent general purpose, rule-based generators: Direct and indirect quotations from re-porters' interviews (Callaway, 2001):\It's turning out to be a real blockbuster,"Mr. Sweig said. Incomplete quotations:Then retailers \will probably push themout altogether," he says. Simple lists of facts from stock reports:8 13/16% high, 8 1/2% low, 8 5/8% nearclosing bid, 8 3/4% oered. Both formal and informal language:You've either got a chair or you don't. A variety of punctuation mixed with text:$55,730,000 of school nancing bonds,1989 Series B (1987 resolution). Combinations of infrequent syntactic rules:Then how should we think about service? Irregular and rare words:\I was upset with Roger, I fumpered andschmumpered," says Mr. Peters.By adding rules for these phenomena, NLGrealizers can signicantly increase their cover-age.
For instance, approximately 15% of PennTreeBank sentences contain either direct, indi-rect or incomplete written dialogue.
Thus for anewspaper domain, excluding dialogue from thegrammar greatly limits potential coverage.
Fur-thermore, using a corpus for testing a surfacerealizer is akin to having a very large regressiontest set, with the added benet of being able torobustly generate real-world sentences.In order to compare a symbolic surface real-izer with its statistical counterparts, we testedan enhanced version of an o-the-shelf symbolicgeneration system, the Fuf/Surge (Elhadad,1991) surface realizer.
To obtain a meaningfulcomparison, we utilized the same approach asRealizer Sentences Coverage Matches Covered Matches Total Matches AccuracySurge 2.2 2416 48.1% 102 8.8% 4.2% 0.8542Surge+ 2416 98.9% 1474 61.7% 61.0% 0.9483Halogen 2416 83.3% 1157 57.5% 47.9% 0.9450Table 1: Comparing two Surge versions with HALogen [Langkilde 2002].HALogen, treating Section 23 of the Treebankas an unseen test set.
We created an analo-gous transformation algorithm (Callaway, 2003)to convert TreeBank sentences into the Surgerepresentation (Figure 1), which are then givento the symbolic surface realizer, allowing us tomeasure both coverage and accuracy.4 Coverage and Accuracy EvaluationOf the three statistical systems presented above,only (Langkilde-Geary, 2002) used a standard,recoverable method for replicating the gener-ation experiment.
Because of the sheer num-ber of sentences (2416), and to enable a directcomparison with HALogen, we similarly usedthe simple string accuracy (Doddington, 2002),where the smallest number of Adds, Deletions,and Insertions were used to calculate accuracy:1 - (A + D + I) / #Characters.Unlike typical statistical and machine learn-ing experiments, the grammar was \trained" byhand, though the evaluation of the resultingsentences was performed automatically.
Thisresulted in numerous generalized syntactic andmorphology rules being added to the Surgegrammar, as well as specialized rules pertain-ing to specic domain elements from the texts.Table 1 shows a comparative coverage andaccuracy analysis of three surface realizers onSection 23 of the Penn TreeBank: the originalSurge 2.2 distribution, our modied version ofSurge, and the HALogen system described in(Langkilde-Geary, 2002).
The surface realizersare measured in terms of: Coverage: The number of sentences forwhich the realizer returned a recognizablestring rather than failure or an error. Matches: The number of identical sen-tences (including punctuation/capitals). Percent of covered matches: How often therealizer returned a sentence match giventhat a sentence is produced. Percent of matches for all sentences: Ameasure of matches from all inputs, whichpenalizes systems that improve accuracyat the expense of coverage (Matches /2416, or Coverage * Covered Matches). Accuracy : The aggregate simple string ac-curacy score for all covered sentences (asopposed to the entire sentence set).The rst thing to note is the drastic improve-ment between the two versions of Surge.
Asthe analysis in Section 3 showed, studying theelements of a particular domain are very impor-tant in determining what parts of a grammarshould be improved.
For instance, the TreeBankcontains many constructions which are not han-dled by Surge 2.2, such as quotations, whichaccount for 15% of the sentences.
When Surge2.2 encounters a quotation, it fails to produce atext string, accounting for a large chunk of thesentences not covered (51.9% compared to 1.1%for our enhanced version of Surge).Additionally, a number of morphology en-hancements, such as contractions and punctua-tion placement contributed to the much higherpercentage of exact matches.
While some ofthese are domain-specic, many are broadergeneralizations which although useful, were notincluded in the original grammar because theywere not encountered in previous domains orarose only in complex sentences.On all four measures the enhanced version ofSurge performed much better than the statisti-cal approach to surface realization embodied inHALogen.
The accuracy measure is especiallysurprising given that statistical and machinelearning approaches employ maximization algo-rithms to ensure that grammar rules are chosento get the highest possible accuracy.
However,given that the dierence in accuracy from Surge2.2 is relatively small while its quality is obvi-ously poor, using such accuracy measures aloneis a bad way to compare surface realizers.Finally, the coverage dierence between theenhanced version of Surge and that of HALo-gen is especially striking.
Some explanationsmay be that statistical systems are not yet capa-ble of handling certain linguistic phenomena likelong-distance dependencies (due to n-gram ap-proaches), or given that statistical systems aretypically robust and very unlikely to produce nooutput, that there were problems in the trans-formation algorithm that converted individualsentence representations from the corpus.5 Additional BenetsThe evaluation approach presented here hasother advantages besides calculating the cover-age and accuracy of a grammar.
For instance,in realizers where linguists must add new lexicalresources by hand, such a system allows themto generate text by rst creating sample sen-tences in the more familiar TreeBank notation.Sentences could also be directly generated byfeeding an example text to a parser capable ofproducing TreeBank structures.
This would beespecially useful in new domains to quickly seewhat new specialized syntax they might need.Additionally, the transformation program canbe used as an error-checker to assist in anno-tating sentences in a new corpus.
Rules couldbe (and have been) added alongside the normaltransformation rules that detect when errors areencountered, categorize them, and make themavailable to the corpus creator for correction.This can extend beyond the syntax level, de-tecting even morphology errors such as incorrectverbs, typos, or dialect dierences.Finally, such an approach can help test pars-ing systems without the need for the time-consuming process of annotating corpora in therst place.
If a parser creates a TreeBank repre-sentation for a sentence, the generation systemcan then attempt to regenerate that same sen-tence automatically.
Exact matches are highlylikely to have been correctly parsed, and moretime can be spent locating and resolving parsesthat returned very low accuracy scores.6 Conclusions and Future WorkRecent statistical systems for generation havefocused on surface realizers, oering robust-ness, wide coverage, and domain- and language-independence given certain resources.
This pa-per represents the analogous eort for a sym-bolic generation system using an enhanced ver-sion of the Fuf/Surge systemic realizer.
Wepresented a grammatical coverage and accu-racy experiment showing the symbolic systemhad a much higher level of coverage of Englishand better accuracy as represented by the PennTreeBank.
The improved Surge grammar, ver-sion 2.4, will be made freely available to theNLG community.While we feel that both coverage and accu-racy could be improved even more, additionalgains would not imply a substantial improve-ment in the quality of the grammar itself.
Thereason is that most problems aecting accuracylie in transforming the TreeBank representationas opposed to the grammar, which has remainedrelatively stable.ReferencesS.
Bangalore and O. Rambow.
2000.
Exploitinga probabilistic hierarchical model for genera-tion.
In COLING{2000: Proceedings of the18th International Conference on Computa-tional Linguistics, Saarbruecken, Germany.John A. Bateman.
1995.
KPML: The KOMET-penman (multilingual) development environ-ment.
Technical Report Release 0.8, Insti-tut fur Integrierte Publikations- und Informa-tionssysteme (IPSI), GMD, Darmstadt.Charles Callaway.
2001.
A computational fea-ture analysis for multilingual character-to-character dialogue.
In Proceedings of the Sec-ond International Conference on IntelligentText Processing and Computational Linguis-tics, pages 251{264, Mexico City, Mexico.Charles B. Callaway.
2003.
Evaluating coveragefor large symbolic NLG grammars.
In Pro-ceedings of the Eighteenth International JointConference on Articial Intelligence, pages811{817, Acapulco, Mexico, August.George Doddington.
2002.
Automatic evalua-tion of machine translation quality using n-gram co-occurrence statistics.
In Proceedingsof the 2002 Conference on Human LanguageTechnology, San Diego, CA, March.Michael Elhadad.
1991.
FUF: The universalunier user manual version 5.0.
Technical Re-port CUCS-038-91, Dept.
of Computer Sci-ence, Columbia University.Irene Langkilde-Geary.
2002.
An empirical ver-ication of coverage and correctness for ageneral-purpose sentence generator.
In Sec-ond International Natural Language Genera-tion Conference, Harriman, NY, July.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus ofEnglish: The PennTreeBank.
ComputationalLinguistics, 26(2).Adwait Ratnaparkhi.
2000.
Trainable meth-ods for surface natural language generation.In Proceedings of the First North AmericanConference of the ACL, Seattle, WA, May.
