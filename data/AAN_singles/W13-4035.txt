Proceedings of the SIGDIAL 2013 Conference, pages 214?222,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsPOMDP-based dialogue manager adaptation to extended domainsM.
Gas?ic?, C. Breslin, M. Henderson, D. Kim, M. Szummer, B. Thomson, P. Tsiakoulis and S. YoungCambridge University Engineering Department{mg436,cb404,mh521,dk449,mos25,brmt2,pt344,sjy}@eng.cam.ac.ukAbstractExisting spoken dialogue systems are typ-ically designed to operate in a static andwell-defined domain, and are not wellsuited to tasks in which the concepts andvalues change dynamically.
To handle dy-namically changing domains, techniqueswill be needed to transfer and reuse ex-isting dialogue policies and rapidly adaptthem using a small number of dialogues inthe new domain.
As a first step in this di-rection, this paper addresses the problemof automatically extending a dialogue sys-tem to include a new previously unseenconcept (or slot) which can be then usedas a search constraint in an informationquery.
The paper shows that in the con-text of Gaussian process POMDP optimi-sation, a domain can be extended througha simple expansion of the kernel and thenrapidly adapted.
As well as being muchquicker, adaptation rather than retrainingfrom scratch is shown to avoid subjectingusers to unacceptably poor performanceduring the learning stage.1 IntroductionExisting spoken dialogue systems are typically de-signed to operate in a static and well-defined do-main, and are not well suited to tasks in whichthe concepts and values change dynamically.
Forexample, consider a spoken dialogue system in-stalled in a car, which is designed to provide in-formation about nearby hotels and restaurants.
Inthis case, not only will the data change as thecar moves around, but the concepts (or slots) thata user might wish to use to frame a query willalso change.
For example, a restaurant system de-signed to be used within cities might not have theconcept of ?al fresco?
dining and could not there-fore handle a query such as ?Find me a Frenchrestaurant where I can eat outside?.
In order tomake this possible, techniques will be needed toextend and adapt existing dialogue policies.Adaptation can be viewed as a process of im-proving action selection in a different condition tothe one in which the policy was originally trained.While adaptation has been extensively studied inspeech recognition (see an overview in (Gales andYoung, 2007)), in spoken dialogue systems it isstill relatively novel and covers a wide range ofpossible research topics (Litman and Pan, 1999;Litman and Pan, 2002; Georgila and Lemon, 2004;Janarthanam and Lemon, 2010).A recent trend in statistical dialogue modellinghas been to model dialogue as a partially ob-servable Markov decision process (POMDP).
Thisprovides increased robustness to errors in speechunderstanding and automatic dialogue policy op-timisation via reinforcement learning (Roy et al2000; Zhang et al 2001; Williams and Young,2007; Young et al 2010; Thomson and Young,2010).
A POMDP-based dialogue manager main-tains a distribution over every possible dialoguestate at every dialogue turn.
This is called thebelief state.
Based on that distribution the sys-tem chooses the action that gives the highest ex-pected reward, measured by the Q-function.
TheQ-function for a belief state and an action is theexpected cumulative reward that can be obtainedif that action is taken in that belief state.
The opti-misation typically requires O(105) to O(106) di-alogues, so is normally done in interaction with asimulated user (Jurc??
?c?ek et al 2011b).In reinforcement learning, policy adaptation hasbeen addressed in the context of transfer learn-ing (Taylor and Stone, 2009).
The core idea is toexploit expertise gained in one domain (source do-main) to improve learning in another domain (tar-get domain).
A number of techniques have beendeveloped but they have not been previously ap-plied to dialogue management.214Gaussian process (GP) based reinforcementlearning (Engel, 2005) has been recently appliedto POMDP dialogue policy optimisation in or-der to exploit the correlations between differentbelief states and thus reduce the number of dia-logues needed for the learning process (Gas?ic?
etal., 2010).An important feature of a Gaussian process isthat it can incorporate a prior mean and variancefor the function it estimates, in this case the Q-function.
Setting these appropriately can signif-icantly speed up the process of learning.
If themean or the variance are estimated in one envi-ronment, for example a particular user type or aparticular domain, they can be used as a prior foradaptation in a different environment, i.e.
anotheruser type or another domain.
A Gaussian processdoes not depend on the belief state but on the cor-relation between two belief states encoded by thekernel function.
Therefore, if one defines a kernelfunction for two belief states in one domain, thepolicy can be used in a different domain, providedthat the correlations between belief states follow asimilar pattern.This paper explores the problem of extending anexisting domain by introducing a previously un-seen slot.
Specifically, a simple restaurant systemis considered which allows a user to search forrestaurants based on food-type and area.
This do-main is then extended by introducing an additionalprice-range slot.
The policy is trained for the basictwo-slot domain and then reused in the extendeddomain by defining a modified kernel function andusing adaptation.
This strategy not only allows forthe knowledge of a previously trained policy to bereused but it also guards against poor performancein the early stages of learning.
This is particularlyuseful in a real-world situation where the adapta-tion is performed in direct interaction with users.In addition, a potential application of this tech-nique to reduce the number of training dialoguesis examined.
The domain is decomposed into aseries of simple domains and the policy is grad-ually adapted to the final domain with a smallernumber of dialogues than are normally needed fortraining.The rest of the paper is organised as follows.
InSection 2 the background on Gaussian processesin POMDP optimisation is given.
Then Section 3gives a description of the Bayesian Update of Di-alogue State dialogue manager, which is used asa test-bed for the experiments.
In Section 4, asimple method of kernel modification is describedwhich allows a policy trained in the basic domainto be used in an extended domain.
Methods offast adaptation are investigated in Section 5 andthis adaptation strategy is then tested via interac-tion with humans using the Amazon MechanicalTurk service in Section 6.
Finally, the use of re-peated adaptation to speed up the process of policyoptimisation by learning gradually from simple tomore complex domains is explored in Section 7,before presenting conclusions in Section 8.2 Gaussian processes in POMDPsThe role of a dialogue policy pi is to map each be-lief state b ?
B into an action a ?
A so as tomaximise the expected cumulative reward, a mea-sure of how good the dialogue is.The expected cumulative reward is defined bythe Q-function as:Q(b, a) = Epi( T??=t+1???t?1r?
|bt = b, at = a),(1)where r?
is the reward obtained at time ?
, T isthe dialogue length and ?
is the discount factor,0 < ?
?
1.
Optimising the Q-function is thenequivalent to optimising the policy pi.A Gaussian process (GP) is a non-parametricBayesian probabilistic model that can be usedfor function regression (Rasmussen and Williams,2005).
It is fully defined by a mean and a kernelfunction which defines prior function correlations.GP-Sarsa is an on-line RL algorithm that mod-els the Q-function as a Gaussian process (Engelet al 2005), Q(b, a) ?
GP (0, k((b, a), (b, a)))where the kernel k(?, ?)
is factored into separatekernels over the belief state and action spaceskC(b,b?
)kA(a, a?).
For a sequence of belief state-action pairs Bt = [(b0, a0), .
.
.
, (bt, at)]T visitedin a dialogue and the corresponding observed im-mediate rewards rt = [r1, .
.
.
, rt]T, the posteriorof the Q-function for any belief state-action pair(b, a) is defined by the following:215Q(b, a)|rt,Bt ?
N (Q(b, a), cov((b, a), (b, a))),Q(b, a) = kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1rt,cov((b, a), (b, a)) = k((b, a), (b, a))?
kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1Htkt(b, a)Ht =????
?1 ??
?
?
?
0 00 1 ?
?
?
0 0... .
.
.
.
.
.
... ...0 ?
?
?
0 1 ???????
,kt(b, a) = [k((b0, a0), (b, a)), .
.
.
, k((bt, at), (b, a))]T,Kt = [kt((b0, a0)), .
.
.
,kt((bt, at))](2)where Kt is the Gram matrix ?
the matrix of thekernel function values for visited points Bt, Ht isa linear operator that captures the reward looka-head from the Q-function (see Eq.
1) and ?2 isan additive noise parameter which controls howmuch variability in theQ-function estimate we ex-pect during the process of learning.If we assume that the Gaussian processplaces a prior mean on the Q-function,Q(b, a) ?
GP (m(b, a), k((b, a), (b, a)))then the posterior mean Q(b, a) is given by (Ras-mussen and Williams, 2005):Q(b, a) = m(b, a) + kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1(rt ?mt), (3)where mt = [m(b0, a0), .
.
.
,m(bt, at)]T. Theestimate of the variance is same as in Eq.
2.The Q-function posterior in Eqs.
2 and 3defines a Gaussian distribution for every be-lief state-action pair.
Thus, when a new be-lief state b is encountered, for each action a ?A, there is a Gaussian distribution Q(b, a) ?N (Q(b, a), cov((b, a), (b, a)))).
Sampling fromthese Gaussian distributions gives a set of Q-values for each action {Q(b, a) : a ?
A} fromwhich the action with the highest sampledQ-valuecan be selected:pi(b) = argmaxa{Q(b, a) : a ?
A} .
(4)In this way, the stochastic model of theQ-functionis effectively transformed into a stochastic policymodel, which can be optimised to maximise the re-ward (Geist and Pietquin, 2011; Gas?ic?
et al 2011;Gas?ic?
et al 2012).Due to the matrix inversion in Eq.
2, the compu-tational complexity of calculating the Q-functionposterior is O(t3), where t is the number of datapoints in Bt, and this poses a serious computa-tional problem.
The algorithm used here to ap-proximate the Gaussian process is the kernel spansparsification method described in (Engel, 2005).In this case, only a set of representative data pointsis retained ?
called the dictionary of visited points.3 BUDS dialogue managerThe Bayesian Update of Dialogue State (BUDS)dialogue manager is a POMDP-based dialoguemanager (Thomson and Young, 2010) which fac-torises the dialogue state into conditionally de-pendent elements.
These elements are arrangedinto a dynamic Bayesian network, which allowsfor their marginal probability distributions to beupdated during the dialogue.
Thus, the beliefstate of the BUDS dialogue manager consists ofthe marginal posterior probability distribution overhidden nodes in the Bayesian network.
The hiddennodes in the BUDS system consist of the historynodes and the goal nodes for each concept in thedialogue.
For instance in a restaurant informationdomain these include area, food-type, address.The history nodes define possible dialogue histo-ries for a particular concept, eg.
system-informed,user-requested.
The goal nodes define possiblevalues for a particular concept, eg.
Chinese, In-dian.
The role of the policy pi is then to map each216belief state into a summary action a from the sum-mary action space A.
Once a summary action isfound it is heuristically mapped into the masteraction that the system finally takes (Gas?ic?
et al2012).
The master actions are composed of dia-logue act type and list of slot value pairs.
There are15 dialogue act types in the BUDS system that fa-cilitate not only simple information providing sce-narios but also more complex dialogues where theuser can change their mind and ask for alterna-tives.To apply GP policy optimisation, a kernel func-tion must be defined on both the belief state spaceB and the action space A.
The kernel functionover the belief state b is constructed from the sumof individual kernels over the hidden node distri-butions, such that the kernel function of two cor-responding nodes is based on the expected likeli-hood kernel (Jebara et al 2004), which is also asimple linear inner product:kB(b,b?)
=?h?bh,b?h?, (5)where bh is the probability distribution encodedin the hth hidden node.
This kernel gives the ex-pectation of one belief state distribution under theother.For history nodes, the kernel is a simple innerproduct between the corresponding node distribu-tions.
While it is possible to calculate the kernelfunction for the goal nodes in the same way as forthe history nodes, in this case, the choice of sys-tem action, such as confirm or inform, does notdepend on the actual values.
It rather depends onthe shape of the distribution and, in particular, itdepends on the probability of the most likely valuecompared to the rest.
Therefore, to exploit the cor-relations further, the kernel over two goal nodesis calculated as the dot product of vectors, whereeach vector represents the corresponding distribu-tion sorted into order of probability.
The only ex-ceptions are the goal for the method node and thediscourse act node.
The former defines whetherthe user is searching for a venue by name or byconstraints and the latter defines which discourseact the user used, eg.
acknowledgement, thank you.Their kernels are calculated in the same way as forthe history nodes.For the action space kernel, the ?-kernel is useddefined by:kA(a, a?)
= ?a(a?).
(6)where ?a(a?)
= 1 iff a = a?.3.1 TopTable domainThe TopTable domain consists of restaurants inCambridge, UK automatically extracted from theTopTable web service (TopTable, 2012).
There areabout 150 restaurants and each restaurant has 7 at-tributes ?
slots.
This results in a belief space thatconsists of 25 concepts where each concept takesfrom 3 to 150 values and each value has a proba-bility in [0, 1].
The summary action space consistsof 16 summary actions.3.2 The agenda-based simulated userIn training and testing a simulated user was used.The agenda-based user simulator (Schatzmann,2008; Keizer et al 2010) factorises the user stateinto an agenda and a goal.
The goal ensuresthat the user simulator exhibits consistent, goal-directed behaviour.
The role of the agenda is toelicit the dialogue acts that are needed for the usersimulator to fulfil the goal.
In addition, an er-ror model adds confusions to the simulated userinput such that it resembles those found in realdata (Thomson et al 2012).
The length of the N-best list was set to 10 and the confusion rate wasset to 15% during training and testing.1 This errorrate means that 15% of time the true hypothesis isnot in the N-best list.
Intermediate experimenta-tion showed that these confusion rates are typicalof real data.The reward function was set to give a rewardof 20 for successful dialogues, zero otherwise.
Inaddition, 1 is deducted for each dialogue turn toencourage shorter dialogues.
The discount factor?
is set to 1 and the dialogue length is limited to30 turns.4 Extended domainsTransfer learning is a reinforcement learning tech-nique which address three problems:?
given a target domain, how to select themost appropriate source domain from a set ofsource domains,?
given a target and a source domain how tofind the relationship between them, and?
given a target and a source domain and therelationship between them, how to effectivelytransfer knowledge between them.1Except of course where the system is explicitly tested onvarying noise levels.217Here we assume that we are given a source anda target domain and that the relationship betweenthem is defined by mapping the kernel function.Knowledge transfer is then effected by adaptingthe source domain policy for use in the target do-main.
For the latter, two forms of adaptation areinvestigated: one simply continues to update theset of source data dictionary points with new dic-tionary points, the second uses the source domainposterior as a prior for the new target domain.In this case, the source is a basic restaurant do-main with slots name, area, food-type, phone, ad-dress, and postcode.
The extended target domainhas an additional price-range slot.
We are inter-ested primarily in training the policy on the ba-sic domain and testing it on the extended domain.However, since real applications may also requirea slot to be forgotten, we also investigate the re-verse where the policy is trained in the extendeddomain and tested on the basic domain.In order to enable the required cross domainportability, a kernel function defining the correla-tion between belief states from differing domainsis needed.
Since the extended domain has an ex-tra slot and thus extra hidden nodes, we need todefine the correlations between the extra hiddennodes and the hidden nodes in the belief state ofthe basic domain.
This can be performed in vari-ous ways, but the simplest approach is to specifywhich slot from the basic domain is most similarto the new slot in the extended domain and thenmatch their corresponding hidden nodes.
In thatway the belief state kernel function between twobelief states bB, bE for the basic B and the ex-tended E domain becomes:kB(bB,bE) =?h?B?bBh ,bEh?+?e/?B?bBl(e),bEe ?, (7)where h are the hidden nodes in the basic domain,e are the hidden nodes in the extended domain andfunction l : E?
B for each hidden node that doesnot exist in the basic domain finds its appropriatereplacement.
In the particular case studied here,the slot area is most similar to the new price-rangeslot since they both have a relatively small numberof values, about 5.
Hence, l(price-range)?
area.If the cardinality of the mapped slots differ, theshorter is padded with zeros though other forms ofnormalisation are clearly possible.The (summary) action space for the extendeddomain has more actions than the basic domain.For example, one action that exists in the extendeddomain and does not exist in the basic domain isrequest(price-range).
To define the kernel func-tion between these sets of actions, one can specifyfor each extra action in the extended domain itsmost similar action in the basic domain:kA(aB, aE) ={?aB(aE) aE ?
AB,?aB(L(aE)) aE /?
AB,(8)where function L : AE ?
AB for each actionthat does not exist in the basic domain finds itsreplacement action.Functions L and l are here defined manually.However, a simple but effective heuristic would beto find for each new slot in the extended domain, aslot in the basic domain with similar cardinality.Porting in the reverse direction from the ex-tended to the basic domain is easier since one cansimply disregard the extra hidden nodes and ac-tions in the kernel calculation.To experimentally examine the extent to whichthis method supports cross domain portability, wetrained policies for both domains until conver-gence, using 105 dialogues on the simulated user.We then cross tested them on the mismatching do-mains at varying user input error rates.
The resultsare given in Fig.
1.0 10 20 30 40 50ErrorRate2024681012Rewardbsc-trn&tstextd-trn&tstextd-trn&bsc-tstbsc-trn&extd-tstFigure 1: Cross testing policies trained on differ-ent domains.
bsc refers to the basic domain, extd isthe extended domain, trn is training and tst is test-ing.From the results it can be seen that the policytrained for the basic domain has a better perfor-mance than the policy trained on the extended do-main, when tested on the matching domain (com-218pare bsc-trn&tst with extd-trn&tst).
The extended do-main has more slots so it is more difficult for thesystem to fulfil the user request, especially in noisyconditions.
Secondly, the performance of the pol-icy trained on the extended domain and tested onthe basic domain is close to optimal (compare bsc-trn&tst with extd-trn&bsc-tst).
However, the pol-icy trained on the basic domain and tested on theextended domain has much worse performance(compare bsc-trn&extd-tst with extd-trn&tst).
It ishard for the policy to adequately extrapolate fromthe basic to the extended domain.
This differencein performance, however, motivates the need foradaptation and this is investigated in the next sec-tion.5 AdaptationAdaptation of a policy trained on one domain toanother can be performed in several ways.
Herewe examine two adaptation strategies similar tothe method described in (Taylor et al 2007),where every action-value for each state in the tar-get domain is initialised with learned source do-main values.The first strategy is to take the policy trained inthe source domain and simply continue training itin the target domain until convergence.
In Gaus-sian process reinforcement learning, this meansthat we assume a zero-mean prior on the Gaussianprocess for theQ-function and let the dictionary ofvisited points Bt from Eq.
2 consist of both pointsvisited in the source domain and the extended tar-get domain, making sure that the Gram matrixKt uses extended domain kernel function wherenecessary.
However, the estimate of the variancedecreases with the number of visited points (seeEq.
2).
The danger therefore when performingadaptation in this way is that the estimate of vari-ances obtained in the source domain will be verysmall since the policy has already been trained un-til convergence with a large number of dialogues.As a consequence, the rate of exploration definedby sampling in Eq.
4 will be reduced and thus leadto the subsequent optimisation in the new targetdomain falling prematurely into a local optimum.As an alternative, we propose another adapta-tion strategy.
The estimate of the posterior of themean for the Q-function, Q in Eq.
2, from the pol-icy trained on the basic domain can be taken to bethe prior of the mean when the policy is trained onthe extended domain as in Eq.
3.
More precisely, ifQbsc is the posterior mean of the policy trained onthe basic domain then mextd = Qbsc.
In this caseit is also important to make sure that the kernelfunction used to calculateQbsc is redefined for theextended domain where necessary.
The prior onthe variance is the original kernel function renor-malised:k((b, a), (b?, a?))?
k((b,a),(b?,a?))?k((b,a),(b,a))k((b?,a?),(b?,a?)).
(9)Given that the estimate of the mean provides rea-sonable performance, it is not necessary to placea flat prior on the variance of the Q-function andtherefore the kernel is normalised as in Eq.
9.When comparing adaptation strategies, we areinterested in two aspects of performance.
The firstis the performance of the policy during training.The second is how quickly the policy reaches theoptimal performance.
For that reason we adoptthe following evaluation scheme.
After every 100adaptation dialogues we test the partially opti-mised policy with 1000 simulated dialogues, dif-ferent to the ones used in adaptation.
These 1000dialogues are the same for every test point on thegraph.
The results are given in Fig.
2.0 200 400 600 800 1000 1200 1400 1600Training dialogues20151050510RewardPRIORADAPTTRAINbsc-trn&extd-tstextd-trn&tstFigure 2: Different adaptation strategiesThe lower horizontal line represents the perfor-mance of the policy trained on the basic sourcedomain and tested on the extended target domain.This is the baseline.
The upper horizontal linerepresents the policy trained until convergence onthe extended domain and also tested on the ex-tended domain.
This provides the gold standard.The adaptation strategy that takes both the meanand variance of the policy trained on the basic do-main and retrains the policy on the extended do-219main is denoted as ADAPT in Fig.
2.
The adap-tation strategy that uses the posterior mean of thepolicy trained on the source domain as the priormean for adaptation is denoted as PRIOR in Fig.
2.Finally, for comparison purposes we show the per-formance of the policy that is trained from scratchon the extended domain.
This is denoted as TRAINon the graph.
It can be seen that both adapta-tion strategies significantly reduce the number oftraining dialogues and, more importantly, main-tain the level of performance during adaptation.The adaptation strategy that places the prior on themean has slightly worse performance in the begin-ning but provides the best performance after 1500dialogues.
As already noted, this could be dueto overly confident variances in the ADAPT caseleading to a local optimum.6 Human experimentsIn order to adapt and evaluate policies with hu-mans, we used crowd-sourcing via the Ama-zon Mechanical Turk service in a set-up similarto (Jurc??
?c?ek et al 2011a; Gas?ic?
et al 2013).The BUDS dialogue manager was incorporatedin a live telephone-based spoken dialogue system.The Mechanical Turk users were assigned spe-cific tasks in the extended TopTable domain.
Theywere asked to find restaurants that have particu-lar features as defined by the given task.
To elicitmore complex dialogues, the users were some-times asked to find more than one restaurant, andin cases where such a restaurant did not exist theywere required to seek an alternative, for examplefind a Chinese restaurant instead of a Vietnameseone.
After each dialogue the users filled in a feed-back form indicating whether they judged the di-alogue to be successful or not.
Based on that bi-nary rating, the subjective success was calculatedas well as the average reward.
An objective rat-ing can also be obtained by comparing the systemoutputs with the predefined task.During policy adaptation, at the end of eachcall, users were asked to press 1 if they were satis-fied (i.e.
believed that they had been successful infulfilling the assigned task) and 0 otherwise.
Theobjective success was also calculated.
The dia-logue was then only used for adaptation if the userrating agreed with the objective measure of suc-cess as in (Gas?ic?
et al 2013).
The performancebased on user ratings during adaptation for bothadaptation strategies is given in Table 1.Table 1: Policy performance during adaptation#Diags Reward Success (%)ADAPT 251 11.7?
0.5 92.0?
1.7PRIOR 329 12.1?
0.4 96.7?
1.0We then evaluated four policies with real users:the policy trained on the basic domain, the pol-icy trained on the extended domain and the pol-icy adapted to the extended domain using the priorand the policy adapted to the extended domain viainteraction with real users using retraining.
Theresults are given in Table 2.Table 2: Human evaluation of four systems in theextended domain: trained in the basic domain,trained in the extended domain, trained in the ba-sic and adapted in the extended domain using bothADAPT and PRIOR methods.Training #Diags Reward Success(%)Basic 246 11.0?
0.5 91.9?
1.7Extended 250 12.1?
0.4 94.4?
1.5ADAPT 268 12.6?
0.4 94.4?
1.4PRIOR 252 12.4?
0.4 95.6?
1.3The results show two important features ofthese adaptation strategies.
The first is that it ispossible to adapt the policy from one domain toanother with a small number of dialogues.
Bothadaptation techniques achieve results statisticallyindistinguishable from the matched case where thepolicy was trained directly in the extended do-main.
The second important feature is that bothadaptation strategies guarantee a minimum levelof performance during training, which is betterthan the performance of the basic policy tested onthe extended domain.
This is particularly impor-tant when training with real users so that they arenot exposed to poor performance at any time dur-ing training.7 Application to fast learningThe above results show that transfer learningthrough policy adaptation can be relatively fast.Since complex domains can be decomposed into aseries of domains with gradually increasing com-plexity, an alternative to training a system to con-vergence starting from an uninformative prior is220to train a system in stages iteratively adapting tosuccessively more complex domains (Taylor andStone, 2009).We explored this idea by training the extendedsystem in three stages.
The first has only one slotthat the user can specify: food-type and additionalslots phone, address and postcode that can be re-quested (initial in Fig.
3).
The second has an ad-ditional area slot (intermediate in Fig.
3) and thefinal domain has a the price-range slot added (finalon the graph).A policy for each of these domains was traineduntil convergence and the average rewards of thesepolicies are the horizontal lines on Fig.
3.
In addi-tion, the following adaptation schedule was imple-mented.
An initial policy was trained from scratchfor the one-slot initial system using only 1500 dia-logues.
The resulting policy was then retrained forthe intermediate two-slot system using again just1500 dialogues.
Finally, the required three-slotsystem was trained using 1500 dialogues.
At eachstage the policy was tested every 100 training dia-logues, and the resulting performances are shownby the three graphs initial-train, intermediate-adaptand final-adapt in Fig.
3.
The policies were testedon the domains they are trained on or adapted to.It can be seen that after just 500 dialogues ofthe third stage (i.e.
after just 3500 dialogues in to-tal) the policy reaches optimal performance.
It hasbeen shown previously that Gaussian process re-inforcement learning for this task normally takes104 dialogues (Gas?ic?
et al 2012) so this schedulehalves the number of dialogues needed for train-ing.
Also it is important to note that when trainingfrom scratch the average reward is less than 5 for300 dialogues (see TRAIN in Fig.
2), in this casethat only happens for about 100 dialogues (seeinitial-train in Fig.
3).8 ConclusionsThis paper has investigated the problem of ex-tending a dialogue system to handle new previ-ously unseen concepts (i.e.
slots) using adapta-tion based transfer learning.
It has been shown thata GP kernel can be mapped to establish a relation-ship between a basic and an extended domain andthat GP-based adaptation can restore a system tooptimal performance within 200 to 300 adaptationdialogues.
A major advantage of this technique isthat it allows a minimum level of performance tobe guaranteed and hence guards against subject-0 200 400 600 800 1000 1200 1400 1600Training dialogues15105051015Rewardinitial-trainintermediate-adaptfinal-adaptintermediateinitialfinalFigure 3: Application of transfer learning to fasttraining.
The target is to achieve the performanceof the fully trained 3 slot system as shown by thelower horizontal line final.
This is achieved in threestages, with the target being achieved part waythrough the 3rd stage using just 3500 dialogues intotal.ing the user to poor performance during the earlystages of adaptation.Two methods of adaptation have been studied ?one based on augmenting the training points fromthe source domain with new points from the tar-get domain, and a second which treats the sourcepolicy as a prior for the target policy.
Results us-ing the prior method were consistently better.
In afurther experiment, it was also shown that startingwith a simple system and successively extendingand adapting it slot by slot, can achieve optimalperformance faster than one trained directly fromscratch.These results suggest that it should be feasi-ble to construct dialogue systems which can dy-namically update and extend their domains of dis-course automatically during direct conversationswith users.
However, further investigation ofmethods for learning the relationship between thenew and the old domains is needed.
Also, thescalability of these results to large-scale domainexpansion remains a topic for future work.AcknowledgmentsThis work was partly supported by PAR-LANCE (www.parlance-project.eu), an EU Sev-enth Framework Programme project (grant num-ber 287615).221ReferencesY Engel, S Mannor, and R Meir.
2005.
Reinforcementlearning with Gaussian processes.
In Proceedings ofICML.Y Engel.
2005.
Algorithms and Representations forReinforcement Learning.
PhD thesis, Hebrew Uni-versity.M Gales and S Young.
2007.
The application of hid-den Markov models in speech recognition.
Found.Trends Signal Process., 1:195?304.M Gas?ic?, F Jurc??
?c?ek, S Keizer, F Mairesse, J Schatz-mann, B Thomson, K Yu, and S Young.
2010.Gaussian Processes for Fast Policy Optimisation ofPOMDP-based Dialogue Managers.
In Proceedingsof SIGDIAL.M Gas?ic?, F Jurc??
?c?ek, B Thomson, K Yu, and S Young.2011.
On-line policy optimisation of spoken dia-logue systems via live interaction with human sub-jects.
In Proceedings of ASRU.M Gas?ic?, M Henderson, B Thomson, P Tsiakoulis, andS Young.
2012.
Policy optimisation of POMDP-based dialogue systems without state space com-pression.
In Proceedings of SLT.M Gas?ic?, C. Breslin, M. Henderson, Szummer M.,B Thomson, P. Tsiakoulis, and S Young.
2013.On-line policy optimisation of Bayesian DialogueSystems by human interaction.
In Proceedings ofICASSP.M Geist and O Pietquin.
2011.
Managing Uncertaintywithin the KTD Framework.
In Proceedings of theWorkshop on Active Learning and Experimental De-sign, Sardinia (Italy).K Georgila and O Lemon.
2004.
Adaptive multimodaldialogue management based on the information stateupdate approach.
In W3C Workshop on MultimodalInteraction.S Janarthanam and O Lemon.
2010.
Adaptive Re-ferring Expression Generation in Spoken DialogueSystems: Evaluation with Real Users.
In Proceed-ings of SIGDIAL.T Jebara, R Kondor, and A Howard.
2004.
Probabilityproduct kernels.
J. Mach.
Learn.
Res., 5:819?844,December.F Jurc??
?c?ek, S Keizer, M Gas?ic?, F Mairesse, B Thomson,K Yu, and S Young.
2011a.
Real user evaluation ofspoken dialogue systems using Amazon MechanicalTurk.
In Proceedings of Interspeech.F Jurc??
?c?ek, B Thomson, and S Young.
2011b.
Naturalactor and belief critic: Reinforcement algorithm forlearning parameters of dialogue systems modelled asPOMDPs.
ACM Transactions on Speech and Lan-guage Processing.S Keizer, M Gas?ic?, F Jurc??
?c?ek, F Mairesse, B Thomson,K Yu, and S Young.
2010.
Parameter estimationfor agenda-based user simulation.
In Proceedings ofSIGDIAL.DJ Litman and S Pan.
1999.
Empirically evaluatingan adaptable spoken dialogue system.
In Proceed-ings of the seventh international conference on Usermodelling.DJ Litman and S Pan.
2002.
Designing and evaluat-ing an adaptive spoken dialogue system.
User Mod-elling and User-Adapted Interaction, 12:111?137.CE Rasmussen and CKI Williams.
2005.
GaussianProcesses for Machine Learning.
MIT Press, Cam-bridge, Massachusetts.N Roy, J Pineau, and S Thrun.
2000.
Spoken dialoguemanagement using probabilistic reasoning.
In Pro-ceedings of ACL.J Schatzmann.
2008.
Statistical User and Error Mod-elling for Spoken Dialogue Systems.
Ph.D. thesis,University of Cambridge.ME Taylor and P Stone.
2009.
Transfer learning forreinforcement learning domains: A survey.
J. Mach.Learn.
Res., 10:1633?1685, December.ME Taylor, P Stone, and Y Liu.
2007.
Transfer learn-ing via inter-task mappings for temporal differencelearning.
J. Mach.
Learn.
Res., 8:2125?2167, De-cember.B Thomson and S Young.
2010.
Bayesian update ofdialogue state: A POMDP framework for spoken di-alogue systems.
Computer Speech and Language,24(4):562?588.B Thomson, M Gas?ic?, M Henderson, P Tsiakoulis, andS Young.
2012.
N-Best error simulation for trainingspoken dialogue systems.
In Proceedings of SLT.TopTable.
2012.
TopTable.
https://www.toptable.com.JD Williams and SJ Young.
2007.
Partially ObservableMarkov Decision Processes for Spoken Dialog Sys-tems.
Computer Speech and Language, 21(2):393?422.S Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-mann, B Thomson, and K Yu.
2010.
The Hid-den Information State model: A practical frame-work for POMDP-based spoken dialogue manage-ment.
Computer Speech and Language, 24(2):150?174.B Zhang, Q Cai, J Mao, E Chang, and B Guo.2001.
Spoken Dialogue Management as Planningand Acting under Uncertainty.
In Proceedings ofEurospeech.222
