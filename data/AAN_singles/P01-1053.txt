Automatic Detection of Syllable Boundaries Combining the Advantagesof Treebank and Bracketed Corpora TrainingKarin M?llerInstitut f?r Maschinelle SprachverarbeitungUniversity of StuttgartAzenbergstrasse 12D-70174 Stuttgart, Germanykarin.mueller@ims.uni-stuttgart.deAbstractAn approach to automatic detection ofsyllable boundaries is presented.
Wedemonstrate the use of several manu-ally constructed grammars trained witha novel algorithm combining the advan-tages of treebank and bracketed corporatraining.
We investigate the effect ofthe training corpus size on the perfor-mance of our system.
The evaluationshows that a hand-written grammar per-forms better on finding syllable bound-aries than does a treebank grammar.1 IntroductionIn this paper we present an approach to super-vised learning and automatic detection of sylla-ble boundaries.
The primary goal of the paperis to demonstrate that under certain conditionstreebank and bracketed corpora training can becombined by exploiting the advantages of the twomethods.
Treebank training provides a method ofunambiguous analyses whereas bracketed corporatraining has the advantage that linguistic knowl-edge can be used to write linguistically motivatedgrammars.In text-to-speech (TTS) systems, like those de-scribed in Sproat (1998), the correct pronuncia-tion of unknown or novel words is one of thebiggest problems.
In many TTS systems largepronunciation dictionaries are used.
However,the lexicons are finite and every natural languagehas productive word formation processes.
TheGerman language for example is known for itsextensive use of compounds.
A TTS systemneeds a module where the words converted fromgraphemes to phonemes are syllabified beforethey can be further processed to speech.
Theplacement of the correct syllable boundary is es-sential for the application of phonological rules(Kahn, 1976; Blevins, 1995).
Our approach of-fers a machine learning algorithm for predictingsyllable boundaries.Our method builds on two resources.
Thefirst resource is a series of context-free gram-mars (CFG) which are either constructed manu-ally or extracted automatically (in the case of thetreebank grammar) to predict syllable boundaries.The different grammars are described in section4.
The second resource is a novel algorithm thataims to combine the advantages of treebank andbracketed corpora training.
The obtained proba-bilistic context-free grammars are evaluated on atest corpus.
We also investigate the influence ofthe size of the training corpus on the performanceof our system.The evaluation shows that adding linguistic in-formation to the grammars increases the accuracyof our models.
For instance, we coded the knowl-edge that (i) consonants in the onset and coda arerestricted in their distribution, and (ii) the positioninside of the word plays an important role.
Fur-thermore, linguistically motivated grammars onlyneed a small size of training corpus to achievehigh accuracy and even out-perform the treebankgrammar trained on the largest training corpus.The remainder of the paper is organized as fol-lows.
Section 2 refers to treebank training.
Insection 3 we introduce the combination of tree-[fOnOnsetONucleusRCodCodaSyl ] [dOnOnset@NucleusSyl ] [ROnOnsetUNucleusNCodCodaSyl ]WordFigure 1: Example tree in the training phasebank and bracketed corpora training.
In section4 we describe the grammars and experiments forGerman data.
Section 5 is dedicated to evaluationand in section 6 we discuss our results.2 Treebank Training (TT) andBracketed Corpora Training (BCT)Treebank grammars are context-free grammars(CFG) that are directly read from production rulesof a hand-parsed treebank.
The probability ofeach rule is assigned by observing how often eachrule was used in the training corpus, yielding aprobabilistic context-free grammar.
In syntax it isa commonly used method, e.g.
Charniak (1996)extracted a treebank grammar from the Penn WallStreet Journal.
The advantages of treebank train-ing are the simple procedure, and the good resultswhich are due to the fact that for each word thatappears in the training corpus there is only onepossible analysis.
The disadvantage is that gram-mars which are read off a treebank are dependenton the quality of the treebank.
There is no free-dom of putting more information into the gram-mar.Bracketed Corpora Training introduced byPereira and Schabes (1992) employs a context-free grammar and a training corpus, which is par-tially tagged with brackets.
The probability of arule is inferred by an iterative training procedurewith an extended version of the inside-outside al-gorithm.
However, only those analyses are con-sidered that meet the tagged brackets (here sylla-ble brackets).
Usually the context-free grammarsgenerate more than one analysis.
BCT reducesthe large number of analyses.
We utilize a spe-cial case of BCT where the number of analyses isalways 1.TreebankTraining:Application:New Algorithm:GrammarTransformationAnalysis Grammar without BracketsTraining Grammar with Brackets(manually constructed)(extracted from CELEX)Input with BracketInput without BracketsFigure 2: The novel algorithm that we capitalizeon in this paper3 Combining the Advantages of TT andBCTOur method used for the experiments is basedon treebank training as well as bracketed corporatraining.
The main idea is that there are large pro-nunciation dictionaries that provide informationabout how words are transcribed and how theyare syllabified.
We want to exploit this linguis-tic knowledge that was put into these dictionar-ies.
For our experiments we employ a pronun-ciation dictionary, CELEX (Baayen et al (1993))that provides syllable boundaries, our so-calledtreebank.
We use the syllable boundaries asbrackets.
The advantage of BCT can be uti-lized: writing grammars using linguistic knowl-edge.
With our method a special case of BCT isapplied where the brackets in combination with amanually constructed grammar guarantee a singleanalysis in the training step with maximal linguis-tic information.Figure 2 depicts our new algorithm.
We man-ually construct different linguistically motivatedcontext-free grammars with brackets marking thesyllable boundaries.
We start with a simple gram-mar and continue to add more linguistic informa-tion to the advanced grammars.
The input of thegrammars is a bracketed corpus that was extractedfrom the pronunciation dictionary CELEX.
In atreebank training step we obtain a probabilisticcontext-free grammar (PCFG) by observing howoften each rule was used in the training corpus.The brackets of the input guarantee an unam-bigous analysis of each word.
Thus, we can applythe formula of treebank training given by (Char-(1.1) 0.1774 Word!
[ Syl ](1.2) 0.5107 Word!
[ Syl ] [ Syl ](1.3) 0.1997 Word!
[ Syl ] [ Syl ] [ Syl ](1.4) 0.4915 Syl!
Onset Nucleus Coda(1.5) 0.3591 Syl!
Onset Nucleus(1.6) 0.0716 Syl!
Nucleus Coda(1.7) 0.0776 Syl!
Nucleus(1.8) 0.9045 Onset!
On(1.9) 0.0918 Onset!
On On(1.10) 0.0036 Onset!
On On On(1.11) 0.0312 Nucleus!
O(1.12) 0.3286 Nucleus!
@(1.13) 0.0345 Nucleus!
U(1.14) 0.8295 Coda!
Cod(1.15) 0.1646 Coda!
Cod Cod(1.16) 0.0052 Coda!
Cod Cod Cod(1.17) 0.0472 On!
f(1.18) 0.0744 On!
d(1.19) 0.2087 Cod!
R(1.20) 0.0271 Cod!
NFigure 3: Grammar fragment after the trainingniak, 1996): if r is a rule, let jrj be the number oftimes r occurred in the parsed corpus and (r) bethe non-terminal that r expands, then the proba-bility assigned to r is given byp(r) =jrjPr02fr0j(r0)=(r)gjr0jWe then transform the PCFG by dropping thebrackets in the rules resulting in an analysisgrammar.
The bracketless analysis grammar isused for parsing the input without brackets; i.e.,the phoneme strings are parsed and the syllableboundaries are extracted from the most proba-ble parse.
We want to exemplify our method bymeans of a syllable structure grammar and an ex-emplary phoneme string.Grammar.
We experimented with a series ofgrammars, which are described in details in sec-tion 4.2.
In the following we will exemplify howthe algorithm works.
We chose the syllable struc-ture grammar, which divides a syllable into on-set, nucleus and coda.
The nucleus is obligatorywhich can be either a vowel or a diphtong.
Allphonemes of a syllable that are on the left-handside of the nucleus belong to the onset and thephonemes on the right-hand side pertain to thecoda.
The onset or the coda may be empty.
Thecontext-free grammar fragment in Figure 3 de-scribes a so called training grammar with brack-ets.We use the input word ?Forderung?
(claim)[fOR][d@][RUN] in the training step.
The unam-biguous analysis of the input word with the sylla-ble structure grammar is shown in Figure 1.Training.
In the next step we train the context-free training grammar.
Every grammar rule ap-pearing in the grammar obtains a probability de-pending on the frequency of appearance in thetraining corpus, yielding a PCFG.
A fragment 1of the syllable structure grammar is shown in Fig-ure 3 (with the recieved probabilities).Rules (1.1)-(1.3) show that German disyllabicwords are more probable than monosyllabic andtrisyllabic words in the training corpus of 389000words.
If we look at the syllable structure, then itis more common that a syllable consists of an on-set, nucleus, and coda than a syllable comprisingthe onset and nucleus; the least probable struc-ture are syllables with an empty onset, and syl-lables with empty onset and empty coda.
Rules(1.8)-(1.10) show that simple onsets are preferredover complex ones, which is also true for codas.Furthermore, the voiced stop [d] is more likelyto appear in the onset than the voiceless fricative[f].
Rules (1.19)-(1.20) show the Coda consonantswith descending probability: [R], [N].Grammar transformation.
In a furtherstep we transform the obtained PCFG by drop-ping all syllable boundaries (brackets).
Rules(1.4)-(1.20) do not change in the fragment ofthe syllable structure grammar.
However, therules (1.1)-(1.3) of the analysis grammar areaffected by the transformation, e.g.
the rule(1.2.)
Word !
[ Syl ] [ Syl ] would be transformedto (1.2.?)
Word !
Syl Syl, dropping the bracketsPredicting syllable boundaries.
Our systemis now able to predict syllable boundaries with thetransformed PCFG and a parser.
The input of thesystem is a phoneme string without brackets.
Thephoneme string [fORd@RUN] (claim) gets thefollowing possible syllabifications according tothe syllable structure grammar: [fO][Rd@R][UN],[fO][Rd@][RUN], [fOR][d@R][UN], [fOR][d@][RUN],[fORd][@R][UN] and [fORd][@][RUN] .The final step is to choose the most probableanalysis.
The subsequent tree depicts the mostprobable analysis: [fOR][d@][RUN], which isalso the correct analysis with the overall wordprobability of 0.5114.
The probability of one1The grammar was trained on 389000 wordsanalysis is defined as the product of the prob-abilities of the grammar rules appearing in theanalysis normalized by the sum of all analysisprobabilities of the given word.
The category?Syl?
shows which phonemes belong to thesyllable, it indicates the beginning and the end ofa syllable.
The syllable boundaries can be readoff the tree: [fOR][d@][RUN].fOnOnsetONucleusRCodCodaSyldOnOnset@NucleusSylROnOnsetUNucleusNCodCodaSylWord (0.51146)4 ExperimentsWe experimented with a series of grammars: thefirst grammar, a treebank grammar, was automat-ically read from the corpus, which describes a syl-lable consisting of a phoneme sequence.
Thereare no intermediate levels between the syllableand the phonemes.
The second grammar is aphoneme grammar where only the number ofphonemes is important.
The third grammar is aconsonant-vowel grammar with the linguistic in-formation that there are consonants and vowels.The fourth grammar, a syllable structure gram-mar is enriched with the information that the con-sonant in the onset and coda are subject to certainrestrictions.
The last grammar is a positional syl-lable structure grammar which expresses that theconsonants of the onset and coda are restricted ac-cording to the position inside of a word (e.g, ini-tial, medial, final or monosyllabic).
These gram-mars were trained on different sizes of corporaand then evaluated.
In the following we first intro-duce the training procedure and then describe thegrammars in details.
In section 5 the evaluationof the system is described.4.1 Training procedureWe use a part of a German newspaper corpus, theStuttgarter Zeitung, consisting of 3 million wordswhich are divided into 9/10 training and 1/10 testcorpus.
In a first step, we look up the words andtheir syllabification in a pronunciation dictionary.The words not appearing in the dictionary are dis-carded.
Furthermore we want to examine the in-fluence of the size of the training corpus on theresults of the evaluation.
Therefore, we split thetraining corpus into 9 corpora, where the size ofthe corpora increases logarithmically from 4500to 2.1 million words.
These samples of wordsserve as input to the training procedure.In a treebank training step we observe for eachrule in the training grammar how often it is usedfor the training corpus.
The grammar rules withtheir probabilities are transformed into the anal-ysis grammar by discarding the syllable bound-aries.
The grammar is then used for predictingsyllable boundaries in the test corpus.4.2 Description of grammarsTreebank grammar.
We started with an au-tomatically generated treebank grammar.
Thegrammar rules were read from a lexicon.
Thenumber of lexical entries ranged from 250 itemsto 64000 items.
The grammars obtained startwith 460 rules for the smallest training corpus,increasing to 6300 rules for the largest trainingcorpus.
The grammar describes that words arecomposed of syllables which consist of a stringof phonemes or a single phoneme.
The followingtable shows the frequencies of some of the rulesof the analysis grammar that are required toanalyze the word [fORd@RUN] (claim):(3.1) 0.1329 Word !
Syl Syl Syl(3.2) 0.0012 Syl !
f O R(3.3) 0.0075 Syl !
d @(3.4) 0.0050 Syl !
d @ R(3.5) 0.0020 Syl !
R U N(3.6) 0.0002 Syl !
U NRule (3.1) describes a word that branches tothree syllables.
The rules (3.2)-(3.6) depict thatthe syllables comprise different phoneme strings.For example, the word ?Forderung?
(claim) canresult in the following two analyses:f O RSyld @SylR U NSylWord (0.9153)f O RSyld @ RSylU NSylWord (0.0846)The right tree receives the overall probability of(0.0846) and the left tree (0.9153), which meansthat the word [fORd@RUN] would be syllabified:[fOR] [d@] [RUN] (which is the correct analysis).Phoneme grammar.
A second grammar isautomatically generated where an abstract levelis introduced.
Every input phoneme is taggedwith the phoneme label: P. A syllable consistsof a phoneme sequence, which means that thenumber of phonemes and syllables is the decisivefactor for calculating the probability of a wordsegmentation (into syllables).
The followingtable shows a fragment of the analysis grammarwith the rule frequencies.
The grammar consistsof 33 rules.
(4.1) 0.4423 Word !
Syl Syl Syl(4.2) 0.1506 Syl !
P P(4.3) 0.2231 Syl !
P P P(4.4) 0.0175 P !
f(4.5) 0.0175 P !
O(4.6) 0.0175 P !
RRule (4.1) describes a three-syllabic word.The second and third rule describe that athree-phonemic syllable is preferred over two-phonemic syllables.
Rules (4.4)-(4.6) show thatP is re-written by the phonemes: [f], [O], and [R].The word ?Forderung?
can be analyzed with thetraining grammar as follows (two examples outof 4375 possible analyses):fPOPRPSyldP@PSylRPUPNPSylWord (0.2031)fPSylOPRPSyldP@PRPUPNPSylWord (0.0006)Consonant-vowel grammar.
In comparisonwith the phoneme grammar, the consonant-vowel (CV) grammar describes a syllable as aconsonant-vowel-consonant (CVC) sequence(Clements and Keyser, 1983).
The linguisticknowledge that a syllable must contain a vowel isadded to the CV grammar, which consists of 31rules.
(5.1) 0.1608 Word !
Syl(5.2) 0.3363 Word !
Syl Syl Syl(5.3) 0.3385 Syl !
C V(5.4) 0.4048 Syl !
C V C(5.5) 0.0370 C !
f(5.6) 0.0370 C !
R(5.7) 0.0333 V !
O(5.8) 0.0333 V !
@Rule (5.1) shows that a three-syllabic wordis more likely to appear than a mono-syllabicword (rule (5.2)).
A CVC sequence is moreprobable than an open CV syllable.
The rules(5.5)-(5.8) depict some consonants and vowelsand their probability.
The word ?Forderung?can be analyzed as follows (two examples out ofseven possible analyses):fCOVRCSyldC@VSylRCUVNCSylWord (0.6864)fCOVRCSyldC@VRCSylUVNCSylWord (0.2166)The correct analysis (left tree) is more probablethan the wrong one (right tree).Syllable structure grammar.
We added to theCV grammar the information that there is an on-set, a nucleus and a coda.
This means that the con-sonants in the onset and in the coda are assigneddifferent weights.
The grammar comprises 1025rules.
The grammar and an example tree was al-ready introduced in section 3.Positional syllable structure grammar.
Fur-ther linguistic knowledge is added to the syllablestructure grammar.
The grammar differentiatebetween monosyllabic words, syllables that occurin inital, medial, and final position.
Furthermorethe syllable structure is defined recursively.Another difference to the simpler grammarversions is that the syllable is devided into onsetand rhyme.
It is common wisdom that there arerestrictions inside the onset and the coda, whichare the topic of phonotactics.
These restrictionsare language specific; e.g., the phoneme sequence[ld] is quite frequent in English codas but itnever appears in English onsets.
Thus the featureposition of the phonemes in the onset and in thecoda is coded in the grammar, that means forexample that an onset cluster consisting of 3phonemes are ordered by their position inside ofthe cluster, and their position inside of the word,e.g.
On.ini.1 (first onset consonant in an initialsyllable), On.ini.2, On.ini.3.
A fragment of theanalysis grammar is shown in the following table:(6.1) 0.3076 Word!
Syl.one(6.2) 0.6923 Word!
Syl.ini Syl(6.3) 0.3662 Syl!
Syl.fin(6.4) 0.7190 Syl.one!Onset.one Rhyme.one(6.5) 0.0219 Onset.one!
On.one.1 On.one.2(6.6) 0.0215 On.ini.1!
f(6.7) 0.0689 Nucleus.ini!
O(6.8) 0.3088 Coda.ini!
Cod.ini.1(6.9) 0.0464 Cod.ini.1!
RRule (6.1) shows a monosyllabic word con-sisting of one syllable.
The second and thirdrules describe a bisyllabic word comprising aninitial and a final syllable.
The monosyllabicfeature ?one?
is inherited to the daughter nodes,here to the onset, nucleus and coda in rule (6.4).Rule (6.5) depicts an onset that branches intotwo onset parts in a monosyllabic word.
Thenumbers represents the position inside the onset.The subsequent rule displays the phoneme [f] ofan initial onset.
In rule (6.7) the nucleus of aninitial syllable consists of the phoneme [O].
Rule(6.8) means that the initial coda only comprisesone consonant, which is re-written by rule (6.9)to a mono-phonemic coda which consists ofthe phoneme [R].
The first of the followingtwo trees recieves a higher overall probababilitythan the second one.
The correct analysis ofthe transcribed word /claim/ [fORd@RUN]can be extracted from the most probable tree:[fOR][d@][RUN].
Note, all other analyses of[fORd@RUN] are very unlikely to occur.fOn.ini.1Onset.iniONuc.iniNucleus.iniRCod.ini.1Coda.iniSyl.inidOn.med.1Onset.med@Nuc.medNucleus.medSyl.medROn.fin.1Onset.finUNuc.finNucleus.finNCod.fin.1Coda.finSyl.finSylSylWord (0.9165)fOn.ini.1Onset.iniONuc.iniNucleus.iniRCod.ini.1Coda.iniSyl.inidOn.med.1Onset.med@Nuc.medNucleus.medRCod.med.1Coda.medSyl.medUNuc.finNucleus.finNCod.fin.1Coda.finSyl.finSylSylWord (0.0834)5 EvaluationWe split our corpus into a 9/10 training and a 1/10test corpus resulting in an evaluation (test) corpusconsisting of 242047 words.Our test corpus is available on the World WideWeb2.
There are two different features that char-acterize our test corpus: (i) the number of un-known words in the test corpus, (ii) and the num-ber of words with a certain number of syllables.The proportion of the unknown words is depictedin Figure 4.
The percentage of unknown wordsis almost 100% for the smallest training corpus,decreasing to about 5% for the largest trainingcorpus.
The ?slow?
decrease of the number ofunknown words of the test corpus is due to boththe high amount of test data (242047 items) andthe ?slightly?
growing size of the training cor-pus.
If the training corpus increases, the num-ber of words that have not been seen before (un-known) in the test corpus decreases.
Figure 4shows the distribution of the number of syllablesin the test corpus ranked by the number of sylla-bles, which is a decreasing function.
Almost 50%of the test corpus consists of monosyllabic words.If the number of syllables increases, the numberof words decreases.The test corpus without syllable boundaries,is processed by a parser (Schmid (2000)) andthe probabilistic context-free grammars sustain-ing the most probable parse (Viterbi parse) ofeach word.
We compare the results of the parsingstep with our test corpus (annotated with sylla-ble boundaries) and compute the accuracy.
If theparser correctly predicts all syllable boundaries of2http://www.ims.uni-stuttgart.de/phonetik/eval-syl01020304050607080901004500 9600 15000 33000 77800 182000 389000 1031000 2120000percentageofunknown wordssize of training corpusproportion of unknown words in the test corpus: 242047 tokens"unknown.tok"0200004000060000800001000001200001-syl 2-syls 3-syls 4-syls 5-syls 6-syls 7-syls 8-syls 9-syls 10-syls 11-syls 12-syls 13-sylsnumber of wordsnumber of syllablesproportion of number of syllables in the test corpus"syls"Figure 4: Unknown words in the test corpus(left); number of syllables in the test corpus (right)grammars word accuracytreebank 94.89phoneme 64.44CV 93.52syl structure 94.77pos.
syl structure 96.49Figure 5: Best accuracy values of the series ofgrammarsa word, the accuracy increases.
We measure theso called word accuracy.The accuracy curves of all grammars are shownin Figure 6.
Comparing the treebank gram-mar and the simplest linguistic grammar we seethat the accuracy curve of the treebank grammarmonotonically increases, whereas the phonemegrammar has almost constant accuracy values(63%).
The figure also shows that the simplestgrammar is better than the treebank grammar un-til the treebank grammar is trained with a cor-pus size of 77.800.
The accuracy of both gram-mars is about 65% at that point.
When the corpussize exceeds 77800, the performance of the tree-bank grammar is better than the simplest linguis-tic grammar.
The best treebank grammar reachesa accuracy of 94.89%.
The low accuracy ratesof the treebank grammar trained on small corporaare due to the high number of syllables that havenot been seen in the training procedure.
Figure 6shows that the CV grammar, the syllable struc-ture grammar and the positional syllable structuregrammar outperform the treebank grammar by atleast 6% with the second largest training corpusof about 1 million words.
When the corpus size isdoubled, the accuracy of the treebank grammar isstill 1.5% below the positional syllable structuregrammar.Moreover, the positional syllable structuregrammar only needs a corpus size of 9600 to out-perform the treebank grammar.
Figure 5 is a sum-mary of the best results of the different grammarson different corpora sizes.6 DiscussionWe presented an approach to supervised learn-ing and automatic detection of syllable bound-aries, combining the advantages of treebank andbracketed corpora training.
The method exploitsthe advantages of BCT by using the brackets ofa pronunciation dictionary resulting in an unam-bigous analysis.
Furthermore, a manually con-structed linguistic grammar admit the use of max-imal linguistic knowledge.
Moreover, the advan-tage of TT is exploited: a simple estimation pro-cedure, and a definite analysis of a given phonemestring.
Our approach yields high word accu-racy with linguistically motivated grammars us-ing small training corpora, in comparison with thetreebank grammar.
The more linguistic knowl-edge is added to the grammar, the higher the accu-racy of the grammar is.
The best model recieved a96.4% word accuracy rate (which is a harder cri-terion than syllable accuracy).Comparison of the performance with othersystems is difficult: (i) hardly any quantita-tive syllabification performance data is availablefor German; (ii) comparisons across languagesare hard to interpret; (iii) comparisons acrossdifferent approaches require cautious interpreta-tions.
Nevertheless we want to refer to sev-1020304050607080901004500 9600 15000 33000 77800 182000 389000 1031000 2120000precisionsize of training corpusoverall precision"treebank.tok""phonemes.tok""C_V.tok""syl_structure.tok""ling_pos.tok""ling_pos_stress.tok"808590951004500 9600 15000 33000 77800 182000 389000 1031000 2120000precisionsize of training corpusoverall precision"treebank.tok""phonemes.tok""C_V.tok""syl_structure.tok""ling_pos.tok""ling_pos_stress.tok"Figure 6: Evaluation of all grammars (left), zoom in (right)eral approaches that examined the syllabificationtask.
The most direct point of comparison is themethod presented by M?ller (to appear 2001).
Inone of her experiments, the standard probabil-ity model was applied to a syllabification task,yielding about 89.9% accuracy.
However, syl-lable boundary accuracy is measured and notword accuracy.
Van den Bosch (1997) investi-gated the syllabification task with five induc-tive learning algorithms.
He reported a gener-alisation error for words of 2.22% on Englishdata.
However, in German (as well as Dutchand Scandinavian languages) compounding byconcatenating word forms is an extremely pro-ductive process.
Thus, the syllabification taskis much more difficult in German than in En-glish.
Daelemans and van den Bosch (1992) re-port a 96% accuracy on finding syllable bound-aries for Dutch with a backpropagation learningalgorithm.
Vroomen et al (1998) report a sylla-ble boundary accuracy of 92.6% by measuring thesonority profile of syllables.
Future work is to ap-ply our method to a variety of other languages.ReferencesHarald R. Baayen, Richard Piepenbrock, and H. vanRijn.
1993.
The CELEX lexical database?Dutch,English, German.
(Release 1)[CD-ROM].
Philadel-phia, PA: Linguistic Data Consortium, Univ.
Penn-sylvania.Juliette Blevins.
1995.
The Syllable in PhonologicalTheory.
In John A. Goldsmith, editor, Handbook ofPhonological Theory, pages 206?244, Blackwell,Cambridge MA.Eugene Charniak.
1996.
Tree-bank grammars.
InProceedings of the Thirteenth National Conferenceon Artificial Intelligence, AAAI Press/MIT Press,Menlo Park.George N Clements and Samuel Jay Keyser.
1983.CV Phonology.
A Generative Theory of the Syllable.MIT Press, Cambridge, MA.Walter Daelemans and Antal van den Bosch.
1992.Generalization performance of backpropagationlearning on a syllabification task.
In M.F.J.Drossaers and A Nijholt, editors, Proceedings ofTWLT3: Connectionism and Natural LanguageProcessing, pages 27?37, University of Twente.Daniel Kahn.
1976.
Syllable-based Generalizationsin English Phonology.
Ph.D. thesis, MassatchusettsInstitute of Technology, MIT.Karin M?ller.
to appear 2001.
Probabilistic context-free grammars for syllabification and grapheme-to-phoneme conversion.
In Proc.
of the Conference onEmpirical Methods in Natural Language Process-ing, Pittsburgh, PA.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30th Annual Meeting ofthe Association for Computational Linguistics.Helmut Schmid.
2000.
LoPar.
Design and Implemen-tation.
[http://www.ims.uni-stuttgart.de/projekte/gramotron/SOFTWARE/LoPar-en.html].Richard Sproat, editor.
1998.
Multilingual Text-to-Speech Synthesis: The Bell Labs Approach.
KluwerAcademic, Dordrecht.Antal Van den Bosch.
1997.
Learning to PronounceWritten Words: A Study in Inductive LanguageLearning.
Ph.D. thesis, Univ.
Maastricht, Maas-tricht, The Netherlands.Jean Vroomen, Antal van den Bosch, and Beatricede Gelder.
1998.
A Connectionist Model for Boot-strap Learning of Syllabic Structure.
13:2/3:193?220.
