Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 254?259,Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational LinguisticsCombining Domain Adaptation Approaches for Medical Text Transla-tionLongyue Wang, Yi Lu, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco OliveiraNatural Language Processing & Portuguese-Chinese Machine Translation Laboratory,Department of Computer and Information Science,University of Macau, Macau, Chinavincentwang0229@hotmail.com,{mb25435, derekfw, lidiasc, mb25433, olifran}@umac.moAbstractThis paper explores a number of simpleand effective techniques to adapt statisti-cal machine translation (SMT) systems inthe medical domain.
Comparative exper-iments are conducted on large corpora forsix language pairs.
We not only compareeach adapted system with the baseline,but also combine them to further improvethe domain-specific systems.
Finally, weattend the WMT2014 medical summarysentence translation constrained task andour systems achieve the best BLEUscores for Czech-English, English-German, French-English language pairsand the second best BLEU scores for re-minding pairs.1.
IntroductionThis paper presents the experiments conductedby the NLP2CT Laboratory at the University ofMacau for WMT2014 medical sentence transla-tion task on six language pairs: Czech-English(cs-en), French-English (fr-en), German-English(de-en) and the reverse direction pairs, i.e., en-cs,en-fr and en-de.By comparing the medical text with commontext, we discovered some interesting phenomenain medical genre.
We apply domain-specifictechniques in data pre-processing, languagemodel adaptation, translation model adaptation,numeric and hyphenated words translation.Compared to the baseline systems (detailed inSection 2 & 3), the results of each method showreasonable gains.
We combine individual ap-proach to further improve the performance of oursystems.
To validate the robustness and lan-guage-independency of individual and combinedsystems, we conduct experiments on the officialtraining data (detailed in Section 3) in all six lan-guage pairs.
We anticipate the numeric compari-son (BLEU scores) on these individual and com-bined domain adaptation approaches that couldbe valuable for others on building a real-life do-main-specific system.The reminder of this paper is organized as fol-lows.
In Section 2, we detail the configurationsof our experiments as well as the baseline sys-tems.
Section 3 presents the specific pre-processing for medical data.
In Section 4 and 5,we describe the language model (LM) and trans-lation model (TM) adaptation, respectively.
Be-sides, the techniques for numeric and hyphenatedwords translation are reported in Section 6 and 7.Finally, the performance of design systems andthe official results are reported in Section 8.2.
Experimental SetupAll available training data from both WMT2014standard translation task1 (general-domain data)and medical translation task 2  (in-domain data)are used in this study.
The official medical sum-mary development sets (dev) are used for tuningand evaluating all the comparative systems.
Theofficial medical summary test sets (test) are onlyused in our final submitted systems.The experiments were carried out with theMoses 1.03 (Koehn et al., 2007).
The translationand the re-ordering model utilizes the ?grow-diag-final?
symmetrized word-to-word align-ments created with MGIZA++4 (Och and Ney,1 http://www.statmt.org/wmt14/translation-task.html.2 http://www.statmt.org/wmt14/medical-task/.3 http://www.statmt.org/moses/.4 http://www.kyloo.net/software/doku.php/mgiza:overview.2542003; Gao and Vogel, 2008) and the trainingscripts from Moses.
A 5-gram LM was trainedusing the SRILM toolkit5 (Stolcke et al., 2002),exploiting improved modified Kneser-Neysmoothing, and quantizing both probabilities andback-off weights.
For the log-linear model train-ing, we take the minimum-error-rate training(MERT) method as described in (Och, 2003).3.
Task Oriented Pre-processingA careful pre-processing on training data is sig-nificant for building a real-life SMT system.
Inaddition to the general data preparing steps usedfor constructing the baseline system, we intro-duce some extra steps to pre-process the trainingdata.The first step is to remove the duplicate sen-tences.
In data-driven methods, the more fre-quent a term occurs, the higher probability it bi-ases.
Duplicate data may lead to unpredicted be-havior during the decoding.
Therefore, we keeponly the distinct sentences in monolingual cor-pus.
By taking into account multiple translationsin parallel corpus, we remove the duplicate sen-tence pairs.
The second concern in pre-processing is symbol normalization.
Due to thenature of medical genre, symbols such as num-bers and punctuations are commonly-used to pre-sent chemical formula, measuring unit, terminol-ogy and expression.
Fig.
1 shows the examplesof this case.
These symbols are more frequent inmedical article than that in the common texts.Besides, the punctuations of apostrophe and sin-gle quotation are interchangeably used in Frenchtext, e.g.
?l?effet de l'inhibition?.
We unify it byreplacing with the apostrophe.
In addition, weobserve that some monolingual training subsets(e.g., Gene Regulation Event Corpus) containsentences of more than 3,000 words in length.
Toavoid the long sentences from harming the true-case model, we split them into sentences with asentence splitter6 (Rune et al., 2007) that is opti-mized for biomedical texts.
On the other hand,we consider the target system is intended forsummary translation, the sentences tend to beshort in length.
For instance, the average sen-tence lengths in development sets of cs, fr, deand en are around 15, 21, 17 and 18, respective-ly.
We remove sentence pairs which are morethan 80 words at length.
In order to that our ex-periments are reproducible, we give the detailed5 http://www.speech.sri.com/projects/srilm/.6 http://www.nactem.ac.uk/y-matsu/geniass/.statistics of task oriented pre-processed trainingdata in Table 2.1,25-OH47 to 80%10-20 ml/kgA&E departmentInfective endocarditis (IE)Figure 1.
Examples of the segments with sym-bols in medical texts.To validate the effectiveness of the pre-processing, we compare the SMT systemstrained on original data 7 (Baseline1) and task-oriented-processed data (Baseline2), respective-ly.
Table 1 shows the results of the baseline sys-tems.
We found all the Baseline2 systems outper-form the Baseline1 models, showing that the sys-tems can benefit from using the processed data.For cs-en and en-cs pairs, the BLEU scores im-prove quite a lot.
For other language pairs, thetranslation quality improves slightly.By analyzing the Baseline2 results (in Table 1)and the statistics of training corpora (in Table 2),we can further elaborate and explain the results.The en-cs system performs poorly, because ofthe short average length of training sentences, aswell as the limited size of in-domain parallel andmonolingual corpora.
On the other hand, the fr-en system achieves the best translation score, aswe have sufficient training data.
The translationquality of cs-en, en-fr, fr-en and de-en pairs ismuch higher than those in the other pairs.
Hence,Baseline2 will be used in the subsequent compar-isons with the proposed systems described inSection 4, 5, 6 and 7.Lang.
Pair Baseline1 Baseline2 Diff.en-cs 12.92 17.57 +4.65cs-en 20.85 31.29 +10.44en-fr 38.31 38.36 +0.05fr-en 44.27 44.36 +0.09en-de 17.81 18.01 +0.20de-en 32.34 32.50 +0.16Table 1: BLEU scores of two baseline systemstrained on original and processed corpora fordifferent language pairs.4.
Language Model AdaptationThe use of LMs (trained on large data) duringdecoding is aided by more efficient storage andinference (Heafield, 2011).
Therefore, we not7 Data are processed according to Moses baseline tutorial:http://www.statmt.org/moses/?n=Moses.Baseline.255Data Set Lang.
Sent.
Words Vocab.
Ave. Len.In-domainParallel Datacs/en 1,770,4219,373,482/10,605,222134,998/156,4025.29/5.99de/en 3,894,09952,211,730/58,544,6081,146,262/487,85013.41/15.03fr/en 4,579,53377,866,237/68,429,649495,856/556,58717.00/14.94General-domainParallel Datacs/en 12,426,374180,349,215/183,841,8051,614,023/1,661,83014.51/14.79de/en 4,421,961106,001,775/112,294,4141,912,953/919,04623.97/25.39fr/en 36,342,5301,131,027,766/953,644,9803,149,336/3,324,48131.12/26.24In-domainMono.
Datacs 106,548 1,779,677 150,672 16.70fr 1,424,539 53,839,928 644,484 37.79de 2,222,502 53,840,304 1,415,202 24.23en 7,802,610 199430649 1,709,594 25.56General-domainMono.
Datacs 33,408,340 567,174,266 3,431,946 16.98fr 30,850,165 780,965,861 2,142,470 25.31de 84,633,641 1,548,187,668 10,726,992 18.29en 85,254,788 2,033,096,800 4,488,816 23.85Table 2: Statistics summary of corpora after pre-processing.only use the in-domain training data, but also theselected pseudo in-domain data 8  from general-domain corpus to enhance the LMs (Toral, 2013;Rubino et al., 2013; Duh et al., 2013).
Firstly,each sentence s in general-domain monolingualcorpus is scored using the cross-entropy differ-ence method in (Moore and Lewis, 2010), whichis calculated as follows:( ) ( ) ( )I Gscore s H s H s?
?
(1)where H(s) is the length-normalized cross-entropy.
I and G are the in-domain and general-domain corpora, respectively.
G is a random sub-set (same size as the I) of the general-domaincorpus.
Then top N percentages of ranked datasentences are selected as a pseudo in-domainsubset to train an additional LM.
Finally, we lin-early interpolate the additional LM with in-domain LM.We use the top N% of ranked results, whereN={0, 25, 50, 75, 100} percentages of sentencesout of the general corpus.
Table 3 shows the ab-solute BLEU points for Baseline2 (N=0), whilethe LM adapted systems are listed with valuesrelative to the Baseline2.
The results indicate thatLM adaptation can gain a reasonable improve-ment if the LMs are trained on more relevantdata for each pair, instead of using the wholetraining data.
For different systems, their BLEU8 Axelrod et al.
(2011) names the selected data as pseudo in-domain data.
We adopt both terminologies in this paper.scores peak at different values of N. It gives thebest results for cs-en, en-fr and de-en pairs whenN=25, en-cs and en-de pairs when N=50, and fr-en pair when N=75.
Among them, en-cs and en-fr achieve the highest BLEU scores.
The reasonis that their original monolingual (in-domain)data for training the LMs are not sufficient.When introducing the extra pseudo in-domaindata, the systems improve the translation qualityby around 2 BLEU points.
While for cs-en, fr-enand de-en pairs, the gains are small.
However, itcan still achieve a significant improvement of0.60 up to 1.12 BLEU points.Lang.
N=0 N=25 N=50 N=75 N=100en-cs 17.57 +1.66 +2.08 +1.72 +2.04cs-en 31.29 +0.94 +0.60 +0.66 +0.47en-fr 38.36 +1.82 +1.66 +1.60 +0.08fr-en 44.36 +0.91 +1.09 +1.12 +0.92en-de 18.01 +0.57 +1.02 -4.48 -4.54de-en 32.50 +0.60 +0.50 +0.56 +0.38Table 3: BLEU scores of LM adapted systems.5.
Translation Model AdaptationAs shown in Table 2, general-domain parallelcorpora are around 1 to 7 times larger than thein-domain ones.
We suspect if general-domaincorpus is broad enough to cover some in-domainsentences.
To observe the domain-specificity ofgeneral-domain corpus, we firstly evaluate sys-tems trained on general-domain corpora.
In Ta-256ble 4, we show the BLEU scores of general-domain systems9 on translating the medical sen-tences.
The BLEU scores of the compared sys-tems are relative to the Baseline2 and the size ofthe used general-domain corpus is relative to thecorresponding in-domain one.
For en-cs, cs-en,en-fr and fr-en pairs, the general-domain parallelcorpora we used are 6 times larger than the orig-inal ones and we obtain the improved BLEUscores by 1.72 up to 3.96 points.
While for en-deand de-en pairs, the performance drops sharplydue to the limited training corpus we used.Hence we can draw a conclusion: the general-domain corpus is able to aid the domain-specifictranslation task if the general-domain data islarge and broad enough in content.Lang.
Pair BLEU Diff.
Corpusen-cs 21.53 +3.96+601.89%cs-en 33.01 +1.72en-fr 41.57 +3.21+693.59%fr-en 47.33 +2.97en-de 16.54 -1.47+13.63%de-en 27.35 -5.15Table 4: The BLEU scores of systems trained ongeneral-domain corpora.Taking into account the performance of gen-eral-domain system, we explore various data se-lection methods to derive the pseudo in-domainsentence pairs from general-domain parallel cor-pus for enhancing the TMs (Wang et al., 2013;Wang et al., 2014).
Firstly, sentence pair in cor-responding general-domain corpora is scored bythe modified Moore-Lewis (Axelrod et al.,2011), which is calculated as follows:?
?g g( ) ( ) ( )( ) ( )I src G srcI t t G t tscore s H s H sH s H s?
??
??
??
??
??
?
(2)which is similar to Eq.
(1) and the only differ-ence is that it considers the both the source (src)and target (tgt) sides of parallel corpora.
Thentop N percentage of ranked sentence pairs areselected as a pseudo in-domain subset to train anindividual translation model.
The additionalmodel is log-linearly interpolated with the in-domain model (Baseline2) using the multi-decoding method described in (Koehn andSchroeder, 2007).Similar to LM adaptation, we use the top N%of ranked results, where N={0, 25, 50, 75, 100}percentages of sentences out of the general cor-9  General-domain systems are trained only on genera-domain training corpora (i.e., parallel, monolingual).pus.
Table 5 shows the absolute BLEU points forBaseline2 (N=0), while for the TM adapted sys-tems we show the values relative to the Base-line2.
For different systems, their BLEU peak atdifferent N. For en-fr and en-de pairs, it gives thebest translation results at N=25.
Regarding cs-enand fr-en pairs, the optimal performance ispeaked at N=50.
While the best results for de-enand en-cs pairs are N=75 and N=100 respective-ly.
Besides, performance of TM adapted systemheavily depends on the size and (domain) broad-ness of the general-domain data.
For example,the improvements of en-de and de-en systems areslight due to the small general-domain corpora.While the quality of other systems improve about3 BLEU points, because of their large and broadgeneral-domain corpora.Lang.
N=0 N=25 N=50 N=75 N=100en-cs 17.57 +0.84 +1.53 +1.74 +2.55cs-en 31.29 +2.03 +3.12 +3.12 +2.24en-fr 38.36 +3.87 +3.66 +3.53 +2.88fr-en 44.36 +1.29 +3.36 +1.84 +1.65en-de 18.01 +0.02 -0.13 -0.07 0de-en 32.50 -0.12 +0.06 +0.31 +0.24Table 5: BLEU scores of TM adapted systems.6.
Numeric AdaptationAs stated in Section 3, numeric occurs frequentlyin medical texts.
However, numeric expression indates, time, measuring unit, chemical formula areoften sparse, which may lead to OOV problemsin phrasal translation and reordering.
Replacingthe sparse numbers with placeholders may pro-duce more reliable statistics for the MT models.Moses has support using placeholders in train-ing and decoding.
Firstly, we replace all thenumbers in monolingual and parallel trainingcorpus with a common symbol (a sample phraseis illustrated in Fig.
2).
Models are then trainedon these processed data.
We use the XMLmarkup translation method for decoding.Original: Vitamin D 1,25-OHReplaced: Vitamin D @num@, @num@-OHFigure 2.
Examples of placeholders.Table 6 shows the results on this number ad-aptation approach as well as the improvementscompared to the Baseline2.
The method im-proves the Baseline2 systems by 0.23 to 0.40BLEU scores.
Although the scores increaseslightly, we still believe this adaptation method issignificant for medical domain.
The WMT2014medical task only focuses on the summary of257medical text, which may contain fewer chemicalexpression in compared with the full article.
Asthe used of numerical instances increases, place-holder may play a more important role in domainadaptation.Lang.
Pair BLEU (Dev) Diff.en-cs 17.80 +0.23cs-en 31.52 +0.23en-fr 38.72 +0.36fr-en 44.69 +0.33en-de 18.41 +0.40de-en 32.88 +0.38Table 6: BLEU scores of numeric adapted sys-tems.7.
Hyphenated Word AdaptationMedical texts prefer a kind of compound words,hyphenated words, which is composed of morethan one word.
For instance, ?slow-growing?
and?easy-to-use?
are composed of words and linkedwith hyphens.
These hyphenated words occurquite frequently in medical texts.
We analyze thedevelopment sets of cs, fr, en and de respective-ly, and observe that there are approximately3.2%, 11.6%, 12.4% and 19.2% of sentences thatcontain one or more hyphenated words.
The highratio of such compound words results in Out-Of-Vocabulary words (OOV) 10 , and harms thephrasal translation and reordering.
However, anumber of those hyphenated words still havechance to be translated, although it is not precise-ly, when they are tokenized into individualwords.Algorithm: Alternative-translation MethodInput:1.
A sentence, s, with M hyphenated words2.
Translation lexiconRun:1.
For i = 1, 2, ?, M2.
Split the ith hyphenated word (Ci) intoPi3.
Translate  Pi into Ti4.
If (Ti are not OOVs):5.
Put alternative translation Ti in XML6.
Else: keep Ci unchangedOutput:Sentence, s?, embedded with alternativetranslations for all Ti.EndTable 7: Alternative-translation algorithm.10 Default tokenizer does not handle the hyphenated words.To resolve this problem, we present an alter-native-translation method in decoding.
Table 7shows the proposed algorithm.In the implementation, we apply XML markupto record the translation (terminology) for eachcompound word.
During the decoding, a hyphen-ated word delimited with markup will be re-placed with its corresponding translation.
Table 8shows the BLEU scores of adapted systems ap-plied to hyphenated translation.
This method iseffective for most language pairs.
While thetranslation systems for en-cs and cs-en do notbenefit from this adaptation, because the hy-phenated words ratio in the en and cs dev areasymmetric.
Thus, we only apply this method foren-fr, fr-en, de-en and en-de pairs.Lang.
Pair BLEU (Dev) Diff.en-cs 16.84 -0.73cs-en 31.23 -0.06en-fr 39.12 +0.76fr-en 45.02 +0.66en-de 18.64 +0.63de-en 33.01 +0.51Table 8: BLEU scores of hyphenated wordadapted systems.3.
Final Results and ConclusionsAccording to the performance of each individualdomain adaptation approach, we combined thecorresponding models for each language pair.
InTable 10, we show the BLEU scores and its in-crements (compared to the Baseline2) of com-bined systems in the second column.
The officialtest set is converted into the recased and deto-kenized SGML format.
The official results of oursubmissions are given in the last column of Table9.Lang.PairBLEU of Com-bined systemsOfficialBLEUen-cs 23.66 (+6.09) 22.60cs-en 38.05 (+6.76) 37.60en-fr 42.30 (+3.94) 41.20fr-en 48.25 (+3.89) 47.10en-de 21.14 (+3.13) 20.90de-en 36.03 (+3.53) 35.70Table 9: BLEU scores of the submitted systemsfor the medical translation task.This paper presents a set of experiments con-ducted on all available training data for six lan-guage pairs.
We explored various domain adap-tation approaches for adapting medical transla-258tion systems.
Compared with other methods, lan-guage model adaptation and translation modeladaptation are more effective.
Other adaptedtechniques are still necessary and important forbuilding a real-life system.
Although all individ-ual methods are not fully additive, combiningthem together can further boost the performanceof the overall domain-specific system.
We be-lieve these empirical approaches could be valua-ble for SMT development.AcknowledgmentsThe authors are grateful to the Science andTechnology Development Fund of Macau andthe Research Committee of the University ofMacau for the funding support for their research,under the Reference nos.
MYRG076 (Y1-L2)-FST13-WF and MYRG070 (Y1-L2)-FST12-CS.The authors also wish to thank the colleagues inCNGL, Dublin City University (DCU) for theirhelpful suggestion and guidance on related work.ReferenceAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain da-ta selection.
In Proceedings of EMNLP, pages 355-362.K.
Duh, G. Neubig, K. Sudoh, H. Tsukada.
2013.
Ad-aptation data selection using neural language mod-els: Experiments in machine translation.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics, pages, 678?683.Qin Gao and Stephan Vogel.
2008.
Parallel imple-mentations of word alignment tool.
Software Engi-neering, Testing, and Quality Assurance for Natu-ral Language Processing, pages 49-57.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of theSixth Workshop on Statistical Machine Transla-tion, pages 187-197.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the 2nd ACL Work-shop on Statistical Machine Translation, pages224-227.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Ber-toldi, Brooke Cowan, Wade Shen, Christine Moranet al.
2007.
Moses: open source toolkit for statisti-cal machine translation.
In Proceedings of ACL,pages 177-180.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of ACL: Short Papers, pages 220-224.S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji,Yusuke Miyao, Yuichiro Matsubayashi and Tomo-ko Ohta.
2007.
AKANE system: protein-protein in-teraction pairs in BioCreAtIvE2 challenge, PPI-IPSsubtask.
In Proceedings of the Second BioCreativeChallenge Evaluation Workshop, pages 209-212.Raphael Rubino, Antonio Toral, Santiago Cort?sVa?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty,and Qun Liu.
2013.
The CNGL-DCU-Prompsittranslation systems for WMT13.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 213-218.Andreas Stolcke and others.
2002.
SRILM-An exten-sible language modeling toolkit.
In Proceedings ofthe International Conference on Spoken LanguageProcessing, pages 901-904.Antonio Toral.
2013.
Hybrid selection of languagemodel training data using linguistic informationand perplexity.
In ACL Workshop on Hybrid Ma-chine Approaches to Translation.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignmentmodels.
Computational Linguistics, 29:19-51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Com-putational Linguistics, pages 160-167.Longyue Wang, Derek F. Wong, Lidia S. Chao, YiLu, and Junwen Xing.
2014 ?A Systematic Com-parison of Data Selection Criteria for SMT DomainAdaptation,?
The Scientific World Journal, vol.2014, Article ID 745485, 10 pages.Longyue Wang, Derek F. Wong, Lidia S. Chao, YiLu, Junwen Xing.
2013. iCPE: A Hybrid Data Se-lection Model for SMT Domain Adaptation.
Chi-nese Computational Linguistics and Natural Lan-guage Processing Based on Naturally AnnotatedBig Data.
Springer Berlin Heidelberg.
pages, 280-290.259
