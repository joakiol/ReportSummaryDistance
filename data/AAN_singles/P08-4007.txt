Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 24?27,Columbus, June 2008. c?2008 Association for Computational LinguisticsSIDE: The Summarization Integrated Development EnvironmentMoonyoung Kang, Sourish Chaudhuri, Mahesh Joshi, Carolyn P. Ros?Carnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213 USAmoonyoun,schaudhu,maheshj,cprose@cs.cmu.eduAbstractIn this type-II demo, we introduce SIDE1 (theSummarization Integrated Development Envi-ronment), an infrastructure that facilitatesconstruction of summaries tailored to theneeds of the user.
It aims to address the issuethat there is no such thing as the perfect sum-mary for all purposes.
Rather, the quality of asummary is subjective, task dependent, andpossibly specific to a user.
The SIDE frame-work allows users flexibility in determiningwhat they find more useful in a summary,both in terms of structure and content.
As aneducational tool, it has been successfully usertested by a class of 21 students in a graduatecourse on Summarization and Personal Infor-mation Management.1 IntroductionA wide range of summarization systems havebeen developed in the past 40 years, beginningwith early work in the Library sciences field.
Tothis day, a great deal of research in summarizationfocuses on alternative methods for selecting sub-sets of text segments based on a variety of forms ofrhetorical analysis and relevance rankings.
Never-theless, while there is much in common betweenapproaches used for summarization in a variety ofcontexts, each new summarization project tends toinclude a new system development effort, becausea general purpose, extensible framework for sum-1The working system can be downloaded fromhttp://www.cs.cmu.edu/~cprose/SIDE.html, and a videoof an example of SIDE use can be found athttp://ankara.lti.cs.cmu.edu/side/video.swf.This project is supported by ONR Cognitive and NeuralSciences Division, Grant number N000140510043marization has not been made available.
As an ex-ample, Teufel and Moens?
(2002) argue that thesummarization strategy for scientific articles mustbe different from news articles because the formerfocus on novelty of information, are much longerand very different in structure.A large proportion of summarization systems donot allow users to intervene in the summarizationprocess so that the form of the summary could betailored to the individual user?s needs (Mieskes, M.,M?ller, C., & Strube, M., 2007).
From the samedocument, many summaries can potentially begenerated, and the most preferable one for one userwill not, in general, be the same as what is pre-ferred by a different user.
The fact that users withsimilar backgrounds can have vastly differing in-formation needs is highlighted by Paice and Jones?
(1993) study where an informal sentence selectionexperiment had to be abandoned because the par-ticipants, who were agriculture experts, were tooinfluenced by their research interests to agree witheach other.
However, summarization systems tendto appear as black boxes from the user?s perspec-tive and the users cannot specify what they wouldwant in the summary.SIDE is motivated by the two scenarios men-tioned above - the absence of a common tool forgenerating summaries from different contexts, aswell as the fact that different users might have dif-ferent information needs from the same document.Bellotti (2005) discusses the problem of informa-tion overload in communication media such as e-mail and online discussion boards.
The rapidgrowth of weblogs, wikis and dedicated informa-tion sources makes the problem of informationoverload more acute.
It also means that summari-zation systems have the responsibility of takinginto account the kind of information that its userwould be interested in.With SIDE, we attempt to give the user a greatersay in deciding what kind of information and howmuch of it the user wants as part of his summary.24In the following sections, we elaborate on thefeatures of SIDE and its technical details.2 FunctionalityThe design of SIDE is aimed at allowing the useras much involvement at every stage of the sum-mary generation process as the user wishes.
SIDEallows the user to select a set of documents to trainthe system upon, and to decide what aspects ofinput documents should be detected and used formaking choices, particularly at the stage of select-ing a subset of segments to preserve from thesource documents.
The other key feature of thedevelopment environment is that it allows devel-opers to plug in custom modules using the PluginManager in the GUI.
In this way, advanced userscan extend the capabilities of SIDE for meetingtheir specific needs while still taking advantage ofthe existing, general purpose aspects of SIDE.The subsequent sub-sections discuss individualparts of system behavior in greater detail at a con-ceptual level.
Screen shots and more step by stepdiscussion of how to use the GUI are given withthe case study that outlines the demo script.2.1 FiltersTo train the system and create a model, the userhas to define a filter.
Defining a filter has 4 steps ?creating annotated files with user-defined annota-tions, choosing feature sets to train (unigrams, bi-grams etc), choosing evaluation metrics (WordToken Counter, TF-IDF) and choosing a classifierto train the system.Annotating Files: The GUI allows the user tocreate a set of unstructured documents.
The usercan create folders and import sets of documents orindividual documents.
The GUI allows the user toview the documents in their original form; alterna-tively, the user can add it to the filter and segmentit by sentence, paragraph, or by own definition.The user can define a set of annotations for eachfilter, and use those to annotate segments of the file.The system has sentence and paragraph segmentersbuilt into it.
The user can also define a segmenterand plug it in.Feature Sets: The feature set panel allows theuser to decide which features the user wants to usein training the model.
It is built on top of TagHel-per Tools (Donmez et al, 2005) and uses it to ex-tract the features chosen by the user.
The systemhas options for using unigrams, bigrams, Part-Of-Speech bigrams and punctuation built into it, andthe user can specify whether they wish to applystemming and/or stop word removal.
Like thesegmenters, if the user wants to use a specific fea-ture to train, the user can plug in the feature extrac-tor for the same through the GUI.Evaluation Metrics: The evaluation metric de-cides how to order the sentences that are chosen tobe part of the summary.
In keeping with the plug-in architecture of the system, the user can defineown metric and plug it into the system using thePlugin Manager.Classifier: The user can decide which classifierto train the model with.
This functionality is builton top of TagHelper Tools, which uses the Wekatoolkit (Witten & Frank, 2005) to give users a setof classifiers to choose from.
Once the system hasbeen trained, the user can see the training results ina panel which provides a performance summary -including the kappa scores computed through 10-fold cross validation and the confusion matrix, thesets of features extracted from the text, and thesettings that were used for training the model.The user can choose the model for classifyingsegments in the target document.
The user also canplug-in a machine learning algorithm to the systemif necessary.2.2 SummariesSummaries are defined by Recipes that specifywhat types of segments should be included in theresulting summary, and how a subset of the onesthat meet those requirements should be selectedand then arranged.
Earlier we discussed how filtersare defined.
One or more filters can be applied to atext so that each segment has one or more labels.These labels can then be used to index into a text.For example, a Recipe might specify using a logi-cal expression such that only a subset of segmentswhose labels meet some specified set of constraintsshould be selected.
The selected subset is then op-tionally ranked using a specified Evaluation metric.Finally, from this ranked list, some number orsome percentage of segments will then finally beselected to be included in the resulting summary.The segments are then optionally re-ordered to theoriginal document order before including them inthe summary, which is then displayed to the user.253 Case StudyThe following subsections describe an examplewhere the user starts with some unstructured doc-uments and uses the system to generate a specifica-tion for a summary, which can then be applied toother similar documents.We illustrate a script outline of our demo pres-entation.
The demo shows how simple it is to movethrough the steps of configuring SIDE for a type ofsummary that a user would like to be able to gen-erate.
In order to demonstrate this, we will lead theuser through an annotation task where we assigndialogue acts to turns in some tutoring dialogues.From this annotated data, we can generate summa-ries that pull out key actions of particular types.For example, perhaps we would like to look at allthe instructions that the tutor has given to a studentor all the questions the student has asked the tutor.The summarizing process consists of annotatingtraining documents to define filters, decidingwhich features to use along with what machinelearning algorithm to train the filters, training theactual filters, defining a summary in terms of thestructured annotation that is accomplished by thedefined filters, and finally, summarizing target filesusing the resulting configuration.
The purpose ofSIDE is to provide both an easy GUI interface forpeople who are not familiar with programming,and extensible, plug-and-play code for those whowant to program and change SIDE into a more so-phisticated and specialized type of summarizer.The demo will provide options for both novice us-ers primarily interested in working with SIDEthrough its GUI interface and for more experiencedusers who would like to work with the code.3.1 Using the GUIThe summarization process begins with loadingunstructured training and testing documents.
Next,filters are defined by adding training documents,segmenting each by choosing an automatic seg-menter, and assigning annotations to the segments.After a document is segmented, the segments areannotated with labels that classify segments usinga user-defined coding scheme (Figure 1).
Unanno-tated segments are later ignored during the trainingphase.
Next, a set of feature types, such as uni-grams, bigrams, part of speech bigrams, etc., areselected, which together will be used to build thefeature space that will be input to a selected ma-chine learning algorithms, or ensemble of algo-rithms.
In this example, ?Punctuation?
FeatureClass Extractor, which can distinguish interroga-tive sentence, is selected and for ?Evaluation Met-rics?, ?Word Token Counter?
is selected.
Now, wetrain this model with an appropriate machine learn-ing algorithm.
In this example, J48 which isFigure 1: The interface where segments are annotated.26BooleanExpressionTreeRankerLimiterFigure 2: The interface for defining how to build a summary from the annotated data.one of Weka?s (Witten & Frank, 2005) decisiontree learners is chosen as the learning algorithm.Users can explore different ensembles of machinelearning algorithms, compare performance over thetraining data using cross-validation, and select thebest performing one to use for summarization.Once one or more filters have been defined, wemust define how summaries are built from thestructured representation that is built by the filters.Figure 2 shows the main interface for doing this.Recipes consist of four parts, namely ?Selecting?,?Ranking?, ?Limiting?, ?Sequencing?.
Selection isdone using a boolean expression tree consisting of?and?, ?or?, and ?is?
nodes.
By doing selection, onlythose segments with proper annotations will beselected for inclusion in the resulting summary.Ranking is done by the Evaluation Metric selectedwhen defining the Recipe.
The size of a summarycan be limited by limiting the number of segmentsyou want in your summary.
Finally, the summarycan be reordered as you wish and displayed.4 Current DirectionsCurrently, most of the functionality in SIDE fo-cuses on the content selection problem.
We ac-knowledge that to move beyond extractive formsof summarization, additional functionality at thesummary generation stage is necessary.
Our cur-rent work focuses on addressing these issues.ReferencesBellotti, V., Ducheneaut, N., Howard, M., Smith, I., &Grinter, R. (2005).
Quality versus Quantity: E-MailCentric Task Management and Its Relation withOverload, Human-Computer Interaction, Volume 20,Donmez, P., Ros?, C. P., Stegmann, K., Weinberger, A.,and Fischer, F. (2005).
Supporting CSCL with Auto-matic Corpus Analysis Technology , Proceedings ofComputer Supported Collaborative Learning.Mieskes, M., M?ller, C., & Strube, M. (2007) Improv-ing extractive dialogue summarization by utilizinghuman feedback, Proceedings of the 25th IASTEDInternational Multi-Conference: artificial intelligenceand applications, p.627-632Paice, Chris D. & Jones, Paul A.
(1993) The identifica-tion of important concepts in highly structured tech-nical papers.
In Proceedings of the 16th ACM-SIGIRConference, pages 69?78Teufel, S. & Moens, M. (2002).
Summarizing ScientificArticles: Experiments with Relevance and RhetoricalStatus, Computational Linguistics, Vol 28, No.
1.Witten, Ian H.; Frank, Eibe (2005).
Data Mining: Prac-tical machine learning tools and techniques, 2nd Edi-tion.
Morgan Kaufmann, San Francisco.27
