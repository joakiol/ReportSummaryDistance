Maintaining the Forest andBurning out the Underbrush in XTAGChr i s t ine  Doran ,  Beth  Hockey ,  Ph i l ip  Hope ly ,  Joseph  Rosenzwe igAnoop Sarkar ,  B.
S r in ivas ,  Fei  X iaIRCS, University of PennsylvaniaPhi ladelphia,  PA 19104{cdoran, beth, phopely, j osephr, anoop, srini, fxia}@linc, cis.
upenn, eduAlex is  Nasr ,  Owen RambowCoGenTex, Inc.840 Hanshaw Road, Suite 11Ithaca, NY 14850{nasr, owen}~cogentex, comAbst rac tIn this paper we report on the recentadvancements and current status of theXTAG Project, housed at the University ofPennsylvania.
We discuss the current cov-erage of the system, as evaluated on theTSNLP English sentences, hierarchical or-ganization of the grammar, and the newand more portable implementation f theX-interface to the grammar and all of thesupporting tools in CLISP, which is freelyavailable.
We also present a methodologyfor specializing our grammar to a particu-lar domain, and give some results on thiseffort.1 Deve lopment  and  Cur rent  S ta tuso f  XTAG1.1 History of XTAGThe XTAG project has been ongoing at Penn insome form or another since 1988.
It began with atoy grammar run on LISP machines, and currentlyhas a large English grammar, small grammars inseveral other languages, a sophisticated X-windowsbased grammar development environment and nu-merous satellite tools.
Approximately 35 peoplehave worked extensively on the system, and at leastthat many have worked more peripherally.
Thus,while it is not a geographically distributed project, ithas been temporally distributed.
At any given time,there is no single person who is completely familiarwith all aspects of either the grammar or the toolkit.
As a result, careful documentation has provento be invaluable.
Historically, this has taken theform of distinct papers on individual components;this is still the case for the tools.
For the grammar,however, there is now a single document, available asa (frozen) technical report (XTAG-Group, 1995) ora constantly updated HTML document.
1 The techreport has been useful not only for the people work-ing on the project at Penn, but also for those outsideof Penn who are either interested in Tree AdjoiningGrammar specifically, or simply interested in seeinghow we handled some particular aspect of the gram-mar.1.2 Cur rent  s ta tus  o f  XTAGWorking with and developing a large grammar is achallenging process, and the importance of havinggood visualization tools cannot be over-emphasized.Currently the XTAG system has X-windows basedtools for viewing and updating the morphologicaland syntactic databases (Karp et al, 1992; Egediand Martin, 1994), and a sophisticated parsing andgrammar development interface.
This interface in-cludes a tree editor, the ability to vary parameters1 Both are freely available from the project's web page,at http :/ / www.cis.upenn.edu:80 /'xtag.30Component DetailsMorphological Consists of approximately 317,000 inflected items.Analyzer and Entries are indexed on the inflected form and return the root form, POS, andMorph Database inflectional information.
Database does not address derivational morphology.POS Tagger Wall Street Journal-trained trigram tagger (Church, 1988) extended to outputand Lex Prob N-best POS sequences (Soong and Huang, 1990).
Decreases the time to parseDatabase a sentence by an average of 93% .Syntactic More than 105,000 entries.Database Each entry consists of: the uninflected form of the word, its POS,the list of trees or tree-families associated with the word, and a list of featureequations that capture lexical idiosyncrasies.Tree Database 768 trees, divided into 54 tree families and 164 individual trees.Tree families represent subcategorization frames; the trees in a tree familywould be related to each other transformationally in a movement-basedapproach.X-Interface Menu-based facility for creating and modifying tree files.User controlled parser parameters: parser's tart category, enable/disable/retryon failure for POS tagger.Storage/retrieval f cilities for elementary and parsed trees.Graphical displays of tree and feature data structures.Hand combination of trees by adjunction or substitution forgrammar development.Ability to assign POS tag and/or Supertag before parsingTable 1: System Summaryin the parser, work with multiple grammars and/orparsers, and use metarules for more efficient treeediting and construction (Becker, 1994).
An inter-face for the lexical organization hierarchy is underdevelopment.The large grammar (database) version of XTAGhas recently been ported to CLISP, contemporarypublic-domain software, with the specific goal of per-mitting XTAG to run under the (public-domain)Linux operating system.
2 The public domain soft-ware suite as of this writing has been tested un-der SunOS 5.4 and Linux 1.2.13, 2.0.20 & 2.0.21on \[ntel-based platforms.
A user currently maydemo a live version of XTAG from a CD-ROMon Intel Linux without having to install or re-compile the suite; those interested can contactx tag- request~l inc .
?
is .
upenn, edu for further in-formation.
Development of an MS-DOS-loadabledemo CD-ROM version of the software suite usingLinux is underway, and a Maclinux version is alsoplanned .32 Linux is the largest working example of a distributedsoftware development project, and has been ported tomore machines than any other operating system.3Configurations for various memory sizes are beingdeveloped, but it is recommended that an Intel-baseduser running a demo have somewhere between 16 and 64megabytes of memory and at least a 586-level processorA snap-shot of the English grammar and parseris shown in Figure 1.
We also have a large Frenchgrammar (started at Penn and expanded at Paris7, by Anne Abeill~), and small grammars for Ko-rean, Chinese and Hindi.
The X-windows interfaceis completely modular and can be (and has been)used with any of these grammars.1.3 Grammar  CoverageTo evaluate the coverage of the English grammar, weran it on the Test Suites for Natural Language Pro-cessing (TSNLP) English corpus (Lehmann et al,1996).
The corpus is intended to be a systematiccollection of English grammatical phenomena, in-cluding complementation, agreement, modification,diathesis, modality, tense and aspect, sentence andclause types, coordination, and negation.
\[t con-tains 1409 grammatical sentences and phrases and3036 ungrammatical ones.Before parsing the TSNLP data, we made a fewtokenization changes: we changed contractions fromtwo tokens to one, downcased the first words of sen-tences, changed a pair of square brackets to paren-theses and changed quotes to pairs of opens andcloses.
There were 42 examples which we judgedungrammatical, and removed from the test corpus.for relatively decent operation speed.31Error Class %POS Tag 19.7%Missing item in lexicon 43.3%Missing tree 21.2%Feature clashes 3%Rest 12.8%ExampleShe adds to/V it , He noises/N him abroadused as an auxiliary V, calm NP downshould're, bet NP NP S, regard NP as AdjMy every firm, All moneyapprox, e.g.Table 2: Error analysis of TSNLP English corpusThese were sentences with conjoined subject pro-nouns, where one or both were accusative, e.g.
Herand him succeed.
Overall, we parsed 61.4% of the1367 remaining sentences and phrases.
The errorswere of various types, broken down in Table 2.The missing lexicon items are obviously the eas-iest of these to remedy.
This class also highlightedthe fact that our grammar is heavily slanted towardAmerican English - our grammar does not handledare or need as auxiliary verbs, and there were anumber of very British particle constructions, e.g.She misses him out.
The missing trees are slightlyharder to address, but the data obtained here is veryuseful in helping us fill gaps in our grammar.
We donot currently handle the class of modal+ 're contrac-tions at all, and this clearly ought to be remedied.The feature clashes are mostly in sequences of deter-miners, and would need to be looked at more closelyto see whether the changes needed to correct themwould do more harm than good.
One general prob-lem with the corpus is that, because it uses a veryrestricted lexicon, if there is one problematic lexicalitem it is likely to appear a large number of timesand cause a disproportionate amount of grief.
Usedto appears 33 times and we get al 33 wrong.
How-ever, it must be noted that the XTAG grammar hasanalyses for syntactic phenomena that were not rep-resented in the TSNLP test suite such as sententialsubjects and subordinating clauses among others.As noted by our reviewers, the TSNLP test suitein its current status is not intended as a ready maderepresentative s t of test data that can be used forcross system evaluation.
We are aware of this andwe present the results of our system performanceon TSNLP as another data point in our sequenceof grammar evaluation experiments.
The Englishgrammar has previously been evaluated on ATIS,Wall Street Journal and IBM-Manual data (Srinivaset al, 1996), and found to perform well in thesedomains.2 Grammar  Organ izat ionThe XTAG English grammar currently consists of768 tree templates, so grammar maintenance is nosmall task.
In general, lexicalizing a TAG createsredundancy because the same trees, modulo theiranchor labels, may be associated with many differ-ent lexical items.
We have eliminated this redun-dancy by storing only abstract ree templates withuninstantiated anchor labels, and instantiating lexi-calized trees on the fly, as words are encountered inthe input.
Another source of redundancy, however,is the reuse of tree substructures in many differenttree templates.
For example, most sentential treetemplates include a structural fragment correspond-ing to the phrase-structure ule S --+ NP VP.This redundancy poses a problem for grammarmaintenance and revision.
To consistently imple-ment a change in the grammar, all the relevant reescurrently must be edited individually, although wedo have an implementation of Becket's metarules(Becker, 1994) which allows us to automate this pro-cess to a great extent.
For instance, the addition of anew feature equation associated with the structuralfragment corresponding to S -~ NP VP would affectmost clausal trees in the grammar.
Crucially, onecan only manually verify that such an update doesnot conflict with any other principle already instan-tiated in the grammar.
As the grammar grows, thedifficulty of this task grows with it.Following the idea first proposed in (Vijay-Shankar and Schabes, 1992), we extend the idea ofabstraction over lexical anchors.
A tree templatewith an unspecified anchor label subsumes an en-tire class of lexically specified trees; similarly, wedefine "meta-templates',  or quasi-trees, which sub-sume classes of tree templates.
The quasi-trees arespecified by partial tree descriptions in a logicallanguage patterned after Rogers and Vijay-Shanker(Rogers and Vijay-Shankar, 1994); we call the par-tial descriptions blocks.
Since we are using a feature-based LTAG, our language has also been equippedwith descriptive predicates allowing us to specify atree's feature-structure equations, in addition to itsstructural characteristics.
Each block abstractly de-scribes all trees incorporating the partial structureit represents.An elementary tree template is expressed as a con-32Dea l t lNeDes~paonSut~e~ ht$ deciuctlvefeauee~Verb hal dee.Ju|tlvafetmle~Se1~ec~ il ~betimtim??bdw'irJ~c~p|on.
.
.
.
Sut~a iomm~y?
.o.
tvbe~ ?Impem0w I" .
\[ Suf ~nm his impeutive" \[ Verb Im ,mp~alveI "~r ' "  I :~'~ ' ' " "Figure 1: Tree are generated by combining partial tree descriptionjunction of blocks.
The blocks are organized as aninheritance lattice, so that descriptive redundancyis localized within an individual block.
Within thisdescription lattice, we isolate two sub-lattices whichform more or less independent dimensions: the sub-categorization sub-lattice and the sub-lattice of de-scriptions of "transformations" on base subcatego-rization frames, such as wh-question formation andimperative mood.
The subcategorization sub-latticeis further divided into four fairly orthogonal sub-parts: (1) the set of blocks describing the syntacticsubject, (2) those for the main anchor(s), (3) thosedescribing complements and (4) those for structurebelow a complement.Similar approaches have been pursued for a largeFrench LTAG by (Candito, 1996) and for the XTAGEnglish grammar by (Becket, 1994).
Following theideas set forth in (Vijay-Shankar and Schabes, 1992),Candito constructs a description hierarchy in muchthe same way as the present work, albeit for asmaller range of constructions than what exists inthe XTAG grammar.
Becker's meta-rules can alsobeen seen as partial descriptions, wherein the inputsand outputs of the meta-rules are sisters in a de-scription hierarchy and the parent is the commonstructure shared by both.
However, there is still re-dundancy across meta-rules whose inputs apply tothe same partial descriptions.
For instance, the sub-ject wh- extraction and subject relative metaruleswould be specified independently and both refer toan NP in subject position of a clause.2.1 Hierarchical Organization of theCurrent English GrammarWe use the hierarchy to build the tree templates forthe XTAG English grammar.
In maintaining thegrammar, however, only the abstract descriptionsneed ever be manipulated; the larger sets of treetemplates and actual trees which they subsume arecomputed eterministically from these high-level de-scriptions, as given in Figure 1.Consider, for example, the description of the rel-ative clause tree for transitive verbs which containsfour blocks: one specifying that its subject is ex-tracted, one that the subject is an NP, one thatthe main anchor is a verb, and one that the com-plement is an NP.
These blocks correspond to thequasi-trees (partially specified trees) shown in Fig-ure 2 and 3(1) and when combined will generate theelementary tree in Figure 3(2).
For the sake of sim-plicity, feature equations are not shown.
In thesefigures, solid lines and dashed lines denote the par-ent and dominance relations respectively; each nodehas a label, enclosed in parentheses, and at leastone name.
Multiple names for the same node areseparated by commas uch as VP, AnchorP in Fig-ure 2(2).
The arc in Figure 3(1) indicates that theprecedence order of V and AnchorP is unspecified.
(In small clauses, the main anchor is a preposition,adjective or noun, not a verb, so AnchorP and VPare not always the same node.
)Our lexical organization tool is implemented inProlog, and contains blocks which account for 85%33Root Root // ~ Subject VP,AnchorPSubject('NP') VP \[V,Anchor('V')AnchorPAnchor Complement('NP')(1) Subject_is_NP (2) Main_anchor is Verb (3)Complement is NPFigure 2: Subcategorization quasi-treesExtraetionRoot('NP')Root('S') / \S ubj?ct,ExtractionTrace('NP' ) VP('VP')I / ->"  V('V') AnchorPAnchor(1) quasi-tree for relative clauseExtractionRoot('NP')R~t( 'S ' )VP,~chorP('VP') S ubject.ExtractiotaTrace(' NP' ) / /V,Anchor('V') Complement('NP')g(2) tree generated from the four quasi-treesFigure 3: Quasi-tree for subject extraction in relative clause, and tree generated by combining it with the 3quasi-trees in Figure 2of the current English grammar.
By the time of theworkshop, the remainder of the grammar will also beimplemented.
There is also an interface to the Pro-log module, and a visualization tool for displayingportions of the description lattice.2.2 A tool for grammar  examinat ionBeing able to specify the grammar in a high-leveldescription language has obvious advantages formaintenance and updating of the grammar, in thatchanges need only be made in one place and areautomatically percolated appropriately throughoutthe grammar.
We expect to reap additional bene-fits from this approach when developing a grammarfor another language.
Beyond these issues of effi-ciency and consistency, this approach also gives usa unique perspective on the existing grammar as awhole.
Defining hierarchical blocks for the grammarboth necessitates and facilitates an examination ofthe linguistic assumptions that have been made withregard to feature specification and tree-family defini-tion.
This can be very useful for gaining a overviewof the theory that is being implemented and expos-ing gaps that have not yet been explained.
Becauseof the organic way in which the grammar was builtover the years, we have always uspected that theremight exist a fair amount of inconsistency eitherwithin the feature structures, or within the tree fam-ilies.
The effort in organizing the lexicon has so farturned up very few non-linguistically motivated in-consistencies, which is a gratifying validation of theconstraints imposed by the LTAG formalism.Our work in tree organization has allowed us tocharacterize three principal types of exceptions inthe XTAG English grammar: (1) a class of trees ismissing from the grammar, though this class wouldbe expected from allowing the descriptive blocksto combine freely (for example, a sentential sub-ject with a verb anchor and a PP complement);(2) within a class of trees, some member is miss-ing, though an analogous member is present in an-other class (extraction of the clausal complement ofa noun-anchored predicative); (3) one tree in a classcan be generated by combining quite general de-scriptions, but there is an exceptional piece of struc-ture or feature quation (the ergative alternation oftransitive verbs).
While these may sometimes re-flect known syntactic generalizations (e.g.
extrac-tion islands, as with the example in (2)), they mayalso reflect inconsistencies which have arisen over thelengthy time-course of grammar development andneed to be corrected.
As previously noted, the lat-ter have so far been quite limited in number andsignificance.Our approach makes it incumbent on us to seekprincipled explanations for these irregularities, incethey must be explicitly encoded in the description34hierarchy.
Without the description hierarchy, therewould be no need to reconcile these differences, incethey would be entirely independent pieces of a flatgrammar.3 Tailoring XTAG to the WeatherDomainWhile it is certainly interesting to develop a wide-coverage grammar for its own sake, it is clear thatfor any practical application the grammar will haveto be tailored to the particular domain.
Our overar-ching goal in building the English grammar was tomake it broad enough and general enough that tai-loring would be a matter of extracting the desiredsubset of the lexicon and/or the tree database.
Inthis section, we will discuss and evaluate various ap-proaches to specializing a large grammar, and thenwill discuss our effort at specializing the XTAG En-glish grammar for a weather-message domain.3.1 Genera l  ConsiderationsIn considering how one might specialize a grammar,we make the following basic assumptions: that asub-language exists; that it can be identified; thatthere is training data (usually unannotated) avail-able; that default mechanisms will be adequate forhandling over-specialization (since we know trainingdata will not perfectly reflect he genre) and that thesmaller grammar combined with defaults will still bemore efficient han the large grammar.Based on these assumptions, the first choice iswhether to do full parsing at all in the final ap-plication.
If the domain contains a large numberof fragments, it might be preferable to use a par-tial parsing approach, in which case development ofa sub-grammar will be less crucial.
Supertagging(Joshi and Srinivas, 1994) is one such approach; oncethe supertagger is trained for the domain, it couldbe used in place of the full parser.
If, however, itis determined that full parsing is practicable for thedomain, there are still a number of considerations inderiving the sub-grammar.In the ideal situation, there would already be acorrected parsed corpus (treebank), which can beused for crafting a sub-grammar for the domain.This is exceptionally unlikely, and in the more com-mon case, training data will have to be constructed,either manually or automatically.
In a lexicalizedgrammar like LTAG, this turns out to be quitemanageable, since there are distinct representationswhich encode syntactic structures.
We can use astatistical approach, such as supertagging, to makea first pass at assigning the correct structures to eachword, and then hand-correct them to derive the rele-vant set of structures.
In non-lexicalized grammars,this process would be much more difficult, becausethere is no straightforward way to associate struc-tures with lexical word and to identify the rules tobe eliminated.
If it is impossible to create trainingdata by any other method, the full grammar canbe applied and then the output corrected to createa treebank of the training data.
Needless to say,this is a tedious, time-consuming and computation-ally expensive task.
Alternatively, a domain expertcould provide a list of grammatical phenomena need-ing to be handled, and this list used to extract thesub-grammar.Once the training data has been processed byone of these methods, the sub-grammar is extractedbased on the elementary objects in the grammar e-quired to handle all of the syntactic phenomena iden-tified in the training set.
This could mean extractingprecisely the constructions used in the training set,or generalizing from them.
A lexical hierarchy suchas that described in Section 2 can be used for thisprocess, with generalization performed along eitherof the hierarchy dimensions.
The expansion couldbe done by general principles (add all trees of a cer-tain subcat frame if any are present), or could bedone based on performance of the sub-grammar onheld-out raining data.Most domains have a rich terminological vocab-ulary, which if not taken into account can causeprohibitive ambiguity in parsing and interpretation.Identifying and demarcating domain specific termi-nology is helpful for all of these approaches, ince theterms can then be treated as single tokens.
This canbeen done either manually or automatically (Daille,1994; Jacquemin and Royaut, 1994).Once the sub-grammar has been finalized, strate-gies for recovering from failure to parse should bedeveloped.
One simple strategy is to fall back to thelarge/whole grammar.
A more sophisticated strat-egy would be to back off using a lexical hierarchy inthe same way it was used for generalizing from thetraining set.3.2 Specializing to the Weather DomainThe domain we chose to test out these strategies wasweather eports, provided to us by CoGenTex3 Thesentences tend to be quite long (an average of 20 to-kens/sentence) and complex, and included a largeamount of domain specific terminology in additionto many geographical names.
To identify the domain4Thanks to the Contrastive Syntax Project, Linguis-tics Department of the University of Montreal, for theuse of their weather synopsis corpus.35specific terms, we are using a hand-collected list,but we are currently working with Beatrice Daille(Daille, 1994) to collect them automatically.
Col-lapsing these terms reduced the length of the testsentences by 22%.
Example 1 is illustrative of thetype of sentences and the terminology in this do-main.
We split the development data into a trainingset (99 sentences) and a test set (50 sentences).
(1) Skies were beginning to clear over \[westernNew-Brunswick\] and \[western Nova Scotia\]early this morning as \[drier air\] pushed intothe district from the west.We primarily pursued the full-parsing approach,but explored partial parsing to a more limited ex-tent as well.
Since we did not have access to parsedtraining data, we tried several of the approaches dis-cussed above for creating the small grammar.
Pars-ing with the full grammar was impractical and in-efficient.
We also attempted to parse the trainingsentences using a sub-grammar, created with the aidof a domain expert who identified relevant syntacticconstructions.
We used this information as input tothe lexical organization tool to extract a sub-latticeof the grammar hierarchy (along both the subcatand transformational dimensions).
However, initialexperiments suggest this first pass sub-grammar wasstill too large, and that more radical pruning of thelarge grammar would be required.The most effective strategy for us was to use thesupertagger to create an annotated training cor-pus.
The supertagger (which had been trained on200,000 words of correctly supertagged WSJ data)performed at about 87%.
We then manually cor-rected the erroneous supertags, and prepared a sub-grammar using the word/POS-tag/supertag riplesfrom the weather training corpus.
Using this sub-grammar, we set up the task to parse the 50 testsentences, backing off to the full grammar.
As of thetime of submission of this paper, we were still pars-ing these sentences.
Although the sentences whichcould be parsed by the sub-grammar were assigned aparse very quickly, overall, we did not see the antic-ipated speed up that we expected.
We suspect hatbacking off to the full grammar is not the best wayto go, and are working on ways to back off using thelexical inheritance hierarchy.There are a number of directions for future worksuggested by these initial experiments.
With regardto partial parsing, we retrained the supertagger onthe 100 training sentences (1416 tokens).
This su-pertagger performed at 78%, a considerable d creasefrom the WSJ-trained supertagger, but respectablegiven the small training set.
Some of the errors pro-duced by the WSJ-trained supertagger were idiosyn-cratic to the newswire domain, so we plan to explorestrategies for combining the information from theWSJ domain with the weather eport domain, anal-ogous to techniques used in the speech domain.Re ferencesBecker, T. 1994.
Patterns in metarules.
In Proceed-ings of the 3rd TAG+ Conference, Paris, France.Candito, Marie-Helene.
1996.
A principle-based hi-erarchical representation f LTAGs.
In Proceed-ings of COLING-96, Copenhagen, Denmark, Au-gust.Church, Kenneth Ward.
1988.
A Stochastic PartsProgram and Noun Phrase Parser for UnrestrictedText.
In 2nd Applied Natural Language ProcessingConference, Austin, Texas.Daille, Beatrice.
1994.
Study and Implementation fCombined Techniques for Automatic Extractionof Terminology.
In The Balancing Act Workshop:Combining Symbolic and Statistical Approaches toLanguage.Egedi, Dania and Patrick Martin.
1994.
AFreely Available Syntactic Lexicon for English.In Proceedings of the International Workshopon Shamble Natural Language Resources, Nara,Japan, August.Jacquemin, C. and J. Royaut.
1994.
Retriev-ing terms and their variants in a lexicalisedunification-based framework.
In Proceedings ofSIGIR94, pages 132-141.Joshi, Aravind K. and B. Srinivas.
1994.
Disam-biguation of Super Parts of Speech (or Supertags):Almost Parsing.
In Proceedings of the 17 th Inter-national Conference on Computational Linguis-tics (COLING '94), Kyoto, Japan, August.Karp, Daniel, Yves Schabes, Martin Zaidel, andDania Egedi.
1992.
A Freely Available WideCoverage Morphological Analyzer for English.In Proceedings of the 15 th International Con-ference on Computational Linguistics (COLING'92), Nantes, France, August.Lehmann, Sabine, Stephan Oepen, Sylvie Regnier-Prost, Klaus Netter, Veronika Lux, Judith Klein,Kirsten Falkedal, Frederik Fouvry, Dominique Es-tival, Eva Dauphin, Herv~ Compagnion, JudithBaur, Lorna Balkan, and Doug Arnold.
1996.TSNLP - -  Test Suites for Natural Language Pro-cessing.
In Proceedings of COLING 1996, Kopen-hagen.36Rogers, J. and Vijay-Shankar.
1994.
Obtaining treesfrom their descriptions: An application to treeadjoining grammars.
Computational Intelligence,10(4).S0ong, Frank K. and Eng-Fong Huang.
1990.
FastTree-Trellis Search for Finding the N-Best Sen-tence Hypothesis in Continuous Speech Recogni-tion.
Journal of Acoustic Society, AM., May.Srinivas, B., Christine Doran, Beth Ann Hockey, andAravind Joshi.
1996.
An approach to robust par-tial parsing and evaluation metrics.
In Proceedingsof the Workshop on Robust Parsing at EuropeanSummer School in Logic, Language and Informa-tion, Prague, August.Vijay-Shankar and Y. Schabes.
1992.
Sharing in lex-icalized tree adjoining rammar.
In Proceedings ofCOLING-92, Nantes, France, August.XTAG-Group, The.
1995.
A Lexicalized Tree Ad-joining Grammar for English.
Technical ReportIRCS 95-03, University of Pennsylvania.37
