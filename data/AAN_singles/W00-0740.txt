In: Proceedings of CoNLL-2000 and LLL-2000, pages 184-193, Lisbon, Portugal, 2000.Incorporating Linguistics Constraints intoInductive Logic ProgrammingJames Cussens Stephen PulmanDept.
of Computer  Science University of Cambridge Computer  LaboratoryUniversity of York New Museums Site, Pembroke StreetHeslington, York, YO10 5DD, UK Cambridge CB2 3QG, UKj c@cs.
york .
ac.
uk S tephen.
Pulman@c 1. cam.
ac.
ukAbst rac tWe report work on effectively incorporating lin-guistic knowledge into grammar induction.
Weuse a highly interactive bottom-up inductivelogic programming (ILP) algorithm to learn'missing' grammar rules from an :incompletegrammar.
Using linguistic constraints on, forexample, head features and gap threading, re-duces the search space to such an extent that,in the small-scale xperiments reported here,we can generate and store all candidate gram-mar rules together with information about theircoverage and linguistic properties.
This allowsan appealingly simple and controlled methodfor generating linguistically plausible grammarrules.
Starting from a base of highly spe-cific rules, we apply least general generalisationand inverse resolution to generate more generalrules.
Induced rules are ordered, for example bycoverage, for easy inspection by the user and atany point, the user can commit to a hypothe-sised rule and add it to the grammar.
Relatedwork in ILP and computational linguistics isdiscussed.1 IntroductionA major advantage of inductive logic program-ming is the ability to incorporate domain knowl-edge (background knowledge) into the inductiveprocess.
In ILP domain knowledge is usuallyencoded by (i) a set of definite clauses declar-ing rules and facts which are true (or assumedto be true) in the domain and (ii) extra-logicalconstraints on the hypothesis pace.
The ILPapproach thus allows a very direct and flexiblemethod of expressing domain knowledge.In this paper, we report on continuation ofthework described in (Cussens and Pulman, 2000),which attempts to maximise the effectiveness oflinguistic knowledge when inducing a grammar.We take an existing rammatical formalism (de-rived from the FraCaS Project (1996)) and ex-tend it with inductive capabilities, rather thanshoe-horning a grammar learning problem intoa form suitable for some particular ILP algo-rithm.
This has major practical benefits, sincethe required linguistic knowledge can be en-coded in a linguistically natural manner.
Asin all real applications of ILP most effort isrequired in 'getting the background knowledgeright'.
Being able to express this knowledge ina representation specifically developed to enablelinguists to write down a grammar makes thisstep easier and quicker.The paper is organised in a manner analo-gous to that of  our algorithm.
In Section 2, wedescribe how to generate naive grammar ulesdirectly from the chart produced uring a failedparse.
The essentials of this approach ave al-ready been described in (Cussens and Pulman,2000), but we briefly describe it here for com-pleteness and also because we have altered itsimplementation.
Section 3 describes the mostimportant step of the algorithm--the represen-tation and use of linguistic constraints at anearly stage in the inductive process.
Section 4describes the two generalisation operators cur-rently used in the search by way of an exam-ple.
Section 5 describes two further experimentsvery briefly.
Most of the various components ofour method have been investigated previouslyeither in the ILP or the computational linguis-tics literature: in Section 6 we discuss this re-lated work.
In Section 7, we assess the currentwork and point to future work.1842 Generat ing  na ive  ru lesThe first step in our algorithm can be describedas inductive chart parsing.
The details of inte-grating induction into chart parsing have beendescribed in (Cussens and Pulman, 2000), herewe give just a brief account.
This first step ofthe algorithm is the only one that has been re-tained from this previous work.
The basic ideais that, after a failed parse, we use abductionto find needed edges: which, if they existed,would allow a complete parse of the sentence.These are produced in a top-down manner start-ing with the initial need for a sigma edge span-ning the entire sentence.
If a need matches themother of a grammar ule and edges for all thedaughters bar one are in the chart, then themissing daughter edge is generated as a newneed.The process of generating naive rules is verysimple, and we will explain it by way of an ex-ample.
Suppose the vp_vp rood grammar ule,shown in Fig 1, has been artificially removedfrom a grammar.
The absence of this rule meansvp_vp_mod syn vp : \[gaps= \[A, B\] ,mor=C, aux=n\] ==>\[ vp : \[gaps = \[A, D\] , mor=C, aux=n\] ,mod: \[gaps= \[D, B\], of =or (s, vp), type=_\] \] .Figure 1: Missing grammar rule (human-readable representation)that, for example, the sentence All big compa-nies wrote a report quickly can not be parsed,since we can not get the needed VP wrote a re-port quickly from the found VP wrote a reportand the found MOD quickly.
The correspondingneeded and actual (complete) edges are given inFig 2.
A naive rule is constructed by putting aY.need (Sent, Cat, From, To).need(l, vp(\[ng,ng\],f(0,0,0,0,1,1,1,1,1),_), 3, 7).%edge (Sent, Id, Origin, From, To, Cat, .
.
)ThCat, From, To).edge(l,  39, vp v rip, 3, 6,vp(\[_A,_A\] ,f(O,O,O,O ....... l,l),n) .... ).edge(l, 19, quickly, 6, 7,mod(\[_B,_B\] ,f (0,0,0, I) ,f(0,1, i,i)),...).Figure 2: Needed and (abbreviated) actualedgesneeded edge on its LHS and other edges on theRHS which in this case gives us the naive rulein Fig 3.
In (Cussens and Pulman, 2000) onlyactual edges were allowed on the RHS of a naiverule, since this ensures that the naive rule suf-fices to allow a parse.
Recently, we have addedan option which allows needed edges to appearon the RHS, thus generating more naive rules.This amounts to conjecturing that the needededges should actually be there, but are missingfrom the set of actual edges because some othergrammar ule is missing: thus preventing theparser from producing them.
Since all naiverules are subsequently constrained and evalu-ated on the data, and then not added to thegrammar unless the user allows them, such boldconjectures can be retracted later on.
Fromcmp_synrule (rO,vp( \[ng,ng\] ,f (0,0,0,0,1, I, I, 1, i) ,_),vp( \[_A,_A\] ,f (0,0,0,0 ....... 1,1) ,n),mod(E_B,_B\] ,f (0,0,O, i) ,f (0,I, i,i))).Figure 3: Naive VP  -+ VP  MOD rule in com-piled forman ILP perspective, the construction of naiverules involves repeated applications of inverseresolution (Muggleton and Buntine, 1988) untilwe produce a clause which meets extra-logicalconstraints on vertex connectivity.
Abbreviat-ing, we produce vp(3,7)  : -  vp(3,6)  and thenvp(3,7)  :'- vp(3 ,6 ) ,mod(6 ,7) .
This is thenfollowed by variabilising the vertices to givevp(Vl,V3) :- vp(Vl,V2) ,mod(V2,V3).
Ex-actly the same procedure can be implementedby building a 'bottom-clause' using the Progolalgorithm.
We previously used P-Progol (nowcalled Aleph) to construct naive rules in thisway, but have since found it more convenient towrite our own code to do this.3 Us ing  l ingu is t i c  constraints3.1 Simple filter constraintsThe user never sees naive rules; most are fil-tered out as linguistically implausible and thosethat survive have generally become specialised.Our basic motto is: constrain early, constraintightly.
The aim is that no linguistically implau-sible rule is ever added to the set of candidaterules.
This allows an incremental approach toimplementing the constraints.
On observing alinguistically implausible rule in the candidate185set, we have to specify what makes it implau-sible and then express this as a constraint inProlog.
In this way, we build up a set of filterswhich get rid of linguistically implausible naiverules as soon as they are produced.Table 1 lists the constraints currently used.The Head features and Gap threading con-straints are discussed later.
RHS length simplylimits the number of constituents on the RHSof a rule to some small user-defined integer (inthe experiments described here it was equal to4).
LHS # RHS filters out rules with a sin-gle daughter which is the same category as themother.
Head OK filters out rules', where theLHS has a head category which is not found onthe RHS.
The last three constraints in Table 1act on the LHS of potential rules (i.e.
needs),filtering out, respectively, sigma categories, cat-egories which do not appear as the LHS of ex-isting rules (and so are probably lexical) and s(sentence) categories.Constraint Specialises Defined onHead features Yes CompiledGap threading Yes CompiledRHS length No CompiledLHS ~ RHS No CompiledHead OK No ReadableLHS not sigma No NeedsLHS not new No NeedsLHS not s No NeedsTable h Linguistic constraints3.2 Gap thread ing  and  head featureconstra intsGap-threading is a technique originating withPereira's 'extraposition grammars' (Pereira,1981).
It is an implementation technique com-monly used for dealing with movement phenom-ena in syntax, as illustrated by a Wh-questionlike What does Smith own _?, where the Wh-word is logically associated with the gap marked'2.There are three components to this type ofanalysis.
Firstly, one rule must introduce the'moved' constituent.
This rule also sets up anexpectation for a gap of the same type as themoved constituent elsewhere in the sentence.This expectation is coded as a set of features,or in our case, a single tuple-valued feature with'GapIn' and 'GapOut'  values.
By setting thevalue of the 'GapIn' feature to be that of (acopy of) the moved constituent, and GapOutto be some null marker (here, ng= nogap) wecan enforce that expectation.
Secondly, ruleswhich do not involve gaps directly pass the valueof the GapIn and GapOut values along theirdaughters (this is the 'threading' part) makingsure that the gap value is threaded everywherethat a gap is permitted to occur linguistically.Thirdly, there are rules which rewrite the typeof constituent which can be moved as the emptystring, discharging the 'gap' expectation.
Ex-ample rules of all three types are as follows:(?)
s: \[gap=(G,G)\] ->np: \[type=wh, agr=A, gap= (ng, ng) \]s : \[gap= (rip : \[type=wh, agr=A,gap= (ng, ng) \], ng) \](ii) vp: \[gap(In,0ut)\] ->v: \[\] np: \[gap=(In,Next)\]pp : \[gap= (Nxt, 0ut) \](iii) np: \[gap=(np: \[type=T,agr=A,gap= (ng,ng) \] ,ng), type=T, agr=A\] ->epsilonRule (i) introduces a fronted wh NP as sister toan S which must contain an associated NP gapagreeing in number etc.
Rule (ii) passes the gapfeature from the mother VP along the daugh-ters that can in principle contain a gap.
Rule(iii) rewrites an NP whose gap value indicatesthat a moved element precedes it as the emptystring.
Rules of these three types conspire toensure that a moved constituent is associatedwith exactly one gap.Constituents which cannot contain a gap as-sociated with a moved element outside the con-stituent identify the In and Out values of thegap feature, and so a usual NP rule mightbe of the form: np: \[gap(G,G)\] -> det: \[...\]n: \[...\] In a sentence containing no gaps thevalue of In and Out will be ng everywhere.Naive rules will not necessarily fall into oneof the three categories above, because the cate-gories that make up their components will havebeen instantiated in various possibly incompleteways.
Thus in Fig 3 the gaps values in themother are (ng,ng), and those in the daugh-ters are separately threaded (A,A) and (B,B).186We apply various checks and filters to candi-date rules to ensure that the logic of the gapfeature instantiations i  consistent with the lin-guistic principles embodied in the gap threadinganalysis.The gap threading logic is tested as follows.Firstly, rules are checked to see whether theymatch the general pattern of the three typesabove, gap-introduction, gap-threading, or gap-discharge rules.
Secondly, in each of the threecases, the values of the gap features are checkedto ensure they match the relevant schematic ex-amples above.The most frequently postulated type of rule isa gap threading rule.
The rule in Fig 3 has thegeneral shape of such a rule but the feature val-ues do not thread in the appropriate way andso it will be in effect unified with a templatethat makes this happen.
The effect here willactually be to instantiate all In and Out valuesto ng, thus specialising the rule.
Hypothesisedrules where the values are all variables will getthe In and Out values unified analogously tothe example threading rule (ii) above.
Hypoth-esised rules where the gap values are not vari-ables are checked to see that they are subsumedby the appropriate schema: thus all the differ-ent threading patterns in Fig 4 would be substi-tution instances of the pattern imposed by theexample threading rule (ii).
At the later gen-eralisation stage the correct variable threadingregime should be the only one consistent withall the observed instantiation patterns.Y.
\[ng/ng,ng/ng, /ng\] .
1\ [a l l ,  big, companies, wrote, a, report,  quickly\].%\[np/ng,np/ng,ng/ng\].
2\[what,dont,al l ,big,companies,read,with,a,machine\].~\[np/ng,np/np,np/ng\].
3\[what,dont,al l ,big,companies,read,a , report ,wi th\ ] .~\[np/np,np/np,np/np\].
4\[what,dont,al l ,big,companies,read,a,report ,quickly, f rom\] .Figure 4: Artificial dataset showing 4 differentpatterns of gap threadingOur constraints on head feature agreementare similar to the gap threading constraints.The specialised version of the naive rule in Fig 3is displayed in Fig 5.
Note that although therule in Fig 5 is not incorrect, it is overly specific,applying only to mor=pl, aux=n where there isno gap to thread.
We now consider how to gen-eralise rules.vp : \[gaps= \[ng: \[\] , ng: \[\] \] ,mor=pl, aux=n\] ==>\[vp: \[gaps= \[ng: \[\] ,ng: \[\] \] ,mor=pl, aux=n\],mod: \[gaps= \[ng: \[\] ,ng : \[\] \] , of=vp, type=n\] \]Figure 5: VP ~ VP MOD rule specialised tomeet head and gap constraints4 Genera l i sa t ion  operatorsIn this section, we show how to generate gram-mar rules by generalising overly specific rulesusing the VP -+ VP MOD running example.Our target is to generate the missing grammarrule displayed in Fig 1.
We will use the ar-tificial dataset given in Fig 4 which displays4 different patterns of gap threading.
Fromthe first three sentences we generate the ex-pected overly specific grammar ules which cor-respond to the three patterns of gap thread-ing.
These axe given, in abbreviated form, inFig 6.
We use least general generalisation (lgg)~Covers sentence lvp : \[gaps = \[ng, ng\] , mor=pl, aux=n\] ==>\[vp : \[gaps = \[ng,ng\] ,mor=pl, aux=n\] ,mod: \[gaps= \[ng, ng\], of=vp, type=n\] \]'/.Covers sentence 2vp : \[gaps= \[np ,ng\] ,mor=inf, aux=n\] ==>\[vp : \[gaps =\[np ,ng\] ,mor=inf, aux=n\] ,mod: \[gaps = \[ng,ng\] , of=or (nom, vp), type=n\] \]Y.Covers sentence 3vp : \[gaps =\[np, ng\] ,mor=inf, aux=n\] ==>\[vp : \[gaps = \[np,np\] ,mor=inf, aux=n\] ,mod: \[gaps =\[np, ng\], of=or (nom, vp), type=n\] \]Figure 6: Overly specific gap threading rules (inabbreviated form)as our basic generalisation perator.
This is im-plemented (for terms) in the Sicstus terms li-brary built-in term_subsumer/3.
Lgg operateson the compiled form of the rules (such as thecmp_synrule/3 unit clause displayed in Fig 5),187not the human-readable form as in Fig 6.
Thelgg of the first two rules produces the follow-ing rule (translated back into human-readableform):vp: \[gaps= \[_282, ng: \[\] \] ,  mor=or ( inf ,  p1), aux=n\]==>\[vp: \[gaps= \[_282,ng: \[\]\] ,mor=or(inf ,p1),aux=n\],rood: \[gaps= \[ng: \[\] ,ng: \[\] \] ,  of=or (nora, vp), type=n\]\]The lgg of this rule with the third is:vp : \[gaps= \[_ 286, ng : \[\] \] ,  mor=or ( inf ,  p l ) ,  aux=n\]\[vp : \[gaps= \[_286, _270\], mor=or ( in f ,  pl) ,  aux=n\],rood: \[gaps= \[_270,ng: \[\] \ ] ,  of=or (nom,vp), type=n\]\]This rule covers the first three sentences but isnot general enough to cope with the situationwhere the gap is not discharged on the motherVP- -a  pattern present in the fourth sentence.Unfortunately, the fourth sentence needs touse the missing rule twice to get a parse, andit is a fundamental limitation of our approachthat a missing rule can only be recovered froma failed parse if it is required only once.
Notethat to induce a rule we only need one sentencewhere the rule is needed oncc our assumptionis that in real (large) training datasets there willbe enough sentences for this to be true for anymissing grammar ule.Although this assumption seems reasonable,we have decided to experiment with a general-isation operator, which is helpful when the as-sumption does not hold true.
A rule with acontext-free skeleton of VP -+ VP MOD MODis generated from the fourth sentence.
This cor-responds to the two applications of the targetVP --+ VP MOD rule.
The rule we have, can bederived by having the target rule resolve on it-self.
It follows that we can inductively generatethe target rule from VP ---+ VP MOD MOD byimplementing a special inverse resolution oper-ator which produces the most specific clause C2from a clause C1, when C1 can be produced byC2 resolving with itself.
Applying this operatorto the VP ~ VP MOD MOD rule renders:vp : \[gaps= \[np, _342\] ,mor=inf, aux=n\] ==>\[vp: \[gaps= \[np ,np\] ,mor=inf, aux=n\] ,mod: \[gaps= \[np, _342\], of=or (nom, vp), type=n\] \]'Lggifying' this latest rule with the lgg of the3 other rules finally generates a grammar ulewith the correct gap threading, which we dis-play in Fig 7 as it appears to the user (with afew added line breaks).
However, this rule isnot general enough simply because our train-ing data is not general enough.
Adding in thesentences All big companies will write a reportquickly, All big companies have written a reportquickly and All big companies wrote a report in-credibly generates a more general version cover-ing these various cases.
However, there is stilla problem because our induced rule allows themodifier to be modifying either a nom or a vp(represented by the term f (0,_280,_280,1) inthe compiled form), where the correct rule al-lows the modifier to modify an s or a vp (repre-sented by the term f (0,0,._280,1) in the com-piled form).
This is because our constraints stillneed to be improved.\[ 7- display_rules.r158 vp ==> \[vp,mod\]vp : \[gaps= \[_384,_368\] ,mor=or ( inf ,  p l ) ,  aux=n\] ==>\[vp : \[gaps= \[_ 384, _ 366\], mor=or ( inf ,  p l ) ,  aux=n\],rood: \[gaps= \[_366, _368\], of=or (nora, vp), type=n\] \]cmp_synrule(r158,vp(\[_324,_322\],f (0 ,0,_316,_316,1,1,1,1,1) ,n) ,\ [vp(\[_324,_302\] , f (0,0,_316,_316,1,1,1,1,1),n),mod(\[_302,_322\], f(O,_280,_280,1),f(0,1,1,1))\])INF0: \[head_feature_status(good,\[mor/f(O,O,_316,_316,1,1,1,1,1),aux/n\]=\ [mor/ f (0,0,_316,_316,1,1,1,1,1) ,aux/n\]) ,gap_feature_status (gap_threading_rule), score (2) \]Covers: 4 sentences:\[4,3,2,1\]** Hit ENTER to continue, anything else to stop **Figure 7: Almost finding the missing grammarrule5 Two exper imentsOur experiments consist of (i) randomly gener-ating 50 sentences from a grammar, (ii) deletingsome grammar rules and (iii) seeing whether wecan recover the missing grammar rules using the50 sentences.
Our approach is interactive withthe user making the final decision on which hy-pothesised rules to add to the grammar.
Hy-pothesised rules are currently ordered by cover-age and presented to the user in that order.
In188our artificial experiments he earlier the missingrule is presented to the user the more successfulthe experiment.In the first experiment we deleted the VP --~VP MOD rule in Fig 1 and the rulenp_det_nom synnp : \[gaps= \[A, A\] ,mor=B,type=C, case=_\] ==>\[det : \[type=C ,mor=B\] ,nora: \[mor=B\] \].After generalisation f naive rules, the rule withthe largest cover wasnp: \[gaps= \[ng: \[\] ,ng: \[\]\] ,mor=or(pl,s3),type=_414, case=_415\] ==>\[det : \[type=or (n, q) ,mor=_405\],nom : \[mor=or (pl, s3) \] \]which is over-general since the morphology fea-ture of the determiner is not constrained toequal that of the mother.
However, the thirdmost general rule covered 24 sentences and was:np : \[gaps= \[ng : \[\] ,ng : \[\] \] ,mot=or (pl, s3),type=n, case=_442\] ==>\[det : \[type=n, mot=or (pl, s3) \] ,nora: \[mor=or (pl, s3) \] \]which does have agreement on morphology.Committing to this latter rule by asserting it asa grammar  rule, removing newly parsable sen-tences and re-generating rules produced a vp==> \[vp,mod\] rules which was more general interms of morphology than the one in Fig 7, butless general in terms of gap threading.
This justreflects the sensitivity of our learning strategyon the particular types of sentences in the train-ing data.In a second experiment, we deleted the rules:nom_nom_mod syn nom: \[mor=A\]==>\[nom: \[mot=A\] ,rood: \[gaps= \[ng: \[1 ,ng : \[\] \] , of=nora,type=or (n, q) \] \].vp_v_np syn vp : \[gaps=A,mor=B, aux=C\] ==>Iv : \[mor=B, aux=C, inv=n, subc= \[np: \[gaps=_,mor=_, type=_, case=_\] \] \] ,np: \[gaps=A, mot=_, type=or (n, q),case=nonsubj \] \] .s_aux_np_vp syns:\[gaps=A,mor=or(pl,or(sl,or(s2,s3))),type=or(n,q),inv=y\]==>\[v:\[mor=or(pl,or(sl,or(s2,s3))),aux=y,inv=y,subc=\[vp:\[gaps=_,mor=B,aux=_\]\]\],np:\ [gaps=\[ng: \ [ \ ] ,ng: \ [ \ ] \ ] ,mor=or(pl,or(sl,or(s2,s3))),type=or(n,q),case=subj\],vp:\[gaps=A,mor=B,aux=_\]\].Our algorithm failed to recover thes_aux np_vp rule but did find close ap-proximations to the other two rules:vp: \[gaps= \[_418, _420\],mor=or ( in f ,  or ( ing,  s3) ),  aux=n\] ==>\[v : \[mor=or ( in f ,  or ( ing,  s3) ) ,  aux=n, lay=n,subc= \[rip : \[gaps=_430 ,mor=_431,type=or (n ,q ) ,  case=nonsubj\] \] \ ] ,np: \[gaps= \[_418, _420\] ,mor=or (p l ,  s3),type=n, case=_407\] \]nora: \[mor=or (pl, s3)\] ==>\[nom: \[mor=or (pl, s3) \] ,rood: \[gaps = \[_339, _339\],of=or (nom,vp),type=or (n,q) \] \]6 Re la ted  workThe strong connections between proving andparsing axe well known (Shieber et al, 1995),so it is no surprise that we find related methodsin both ILP and computational linguistics.
InILP the notion of inducing clauses to fix a failedproof, which is the topic of Section 2, is very olddating from the seminal work of Shapiro (1983).In NLP, Mellish (1989) presents a method for re-pairing failed parses in a relatively efficient waybased on the fact that, after a failed parse, theinformation in the chart is sufficient for us to beable to determine what constituents would haveallowed the parse to go through if they had beenfound.6.1 Re la ted  work  in I LPThe use of abduction to repair proofs/paxseshas been extensively researched in ILP as hasthe importance of abduction for multiple pred-icate learning.
De Raedt (1992), for example,notes that "Roughly speaking, combining ab-duction with single predicate-leaxning leads tomultiple concept-leaxning".
This paper, whereabduction is used to learn, say, verb phrases andnoun phrases from examples of sentences i anexample of this.
Recent work in this vein in-cludes (Muggleton and Bryant, 2000) and thepapers in (Flach and Kakas, 2000).Amongst his work a particularly relevant pa-per for us is (Wirth, 1988).
Wirth's Learning189by Failure to Prove (LFP) approach finds miss-ing clauses by constructing partial proof trees(PPTs) and hence diagnosing the source of in-completeness.
A clause representing the PPTis constructed (called the resolvent of the PPT)as is an approximation to the resolvent of thecomplete proof tree.
Inverse resolution is thenapplied to these two clauses to derive the miss-ing clause.
Wirth explains his method by way ofa small context-free DCG completion problem.Our approach is similar to Wirth's in thedependence on abduction to locate the sourceof proof (i.e.
parse) failure.
Also both meth-ods use a meta interpreter to construct partialproofs.
In our case the meta-interpreter is thechart parser augmented with the generation ofneeds and the partial proof is represented by thechart augmented with the needs.
In Wirth'swork the resolvent of the PPT  represents thepartial proof and a more general purpose meta-interpreter is used.
(We conjecture that ourtabular representation has a better chance ofscaling up for real applications.)
Thirdly, bothmethods are interactive.
Translating his ap-proach to the language of this paper, Wirth asksthe user to verify that proposed needed atoms(our needed edges) are truly needed.
The useralso has to evaluate the final hypothesised rules.We prefer to have the user only perform thelatter task, but the advantage of Wirth's ap-proach is that the user can constrain the searchat an earlier stage.
Wirth defends an interac-tive approach on the grounds that "A systemthat learn\[s\] concepts or rules from looking atthe world is useless as long as the results are notverified because a user who feels responsible forhis knowledge base rarely use these concepts orrules".In contrast o (Cussens and Puhnan, 2000)we now search bottom-up for our rules.
This isbecause the rules we are searching for are nearthe bottom of the search space, and also becausebottom-up searching effects a more constrained,example-driven search.
Bottom-up search hasbeen used extensively in ILP.
For example, theGOLEM algorithm (Muggleton and Feng, 1990)used relative least general generalisation (rlgg).However, bottom-up search is rare in modernILP implementations.
This is primarily be-cause the clauses produced can be unmanage-ably large, particularly when generalisation isperformed relative to background knowledge, aswith rlgg.
Having grammar ules encoded asunit clauses alleviates this problem as does ourdecision to use lgg rather than rlgg.Zelle and Mooney (1996) provides a bridgebetween ILP and NLP inductive methods.Their CHILL algorithm is a specialised ILP sys-tem that learns control rules for a shift-reduceparser.
The connection with the approach pre-sented here (and that of Wirth) is that inter-mediate stages of a proof/parse are representedand then examined to find appropriate rules.
InCHILL these intermediate stages are states of ashift-reduce parser.6.2 Re la ted  work  in NLPMost work on grammar induction has takenplace using formalisms in which categoriesare atomic: context-free grammars, categorialgrammars, etc.
Few attempts have been madeat rule induction using a rich unification formal-ism.
Two lines of work that are exceptions tothis, and thus comparable to our own, are thatof Osborne and colleagues; and the work of theSICS group using SRI's Core Language Engineand similar systems.Osborne (1999) argues (correctly) that thehypothesis pace of grammars is sufficientlylarge that some form of bias is required.
Thecurrent paper is concerned with methods foreffecting what is known as declarative bias inthe machine learning literature, i.e.
hard con-straints that reduce the size of the hypothe-sis space.
Osborne, on the other hand, usesthe Minimum Description Length (MDL) prin-ciple to effect a preferential (soft) bias towardssmaller grammars.
His approach is incrementaland the induction of new rules is triggered byan unparsable sentence as follows:1.
Candidate rules are generated where thedaughters are edges in the chart after thefailed parse, and the mother is one ofthese daughters, possibly with its bar levelraised.2.
The sentence is parsed and for each success-ful parse, the set of candidate rules used inthat parse constitutes a model.3.
The 'best' model is found using a MinimumDescription Length approach and is addedto the existing grammar.190So Osborne, like us, uses the edges in thechart after a failed parse to form the daughtersof hypothesised rules.
The mothers, though, arenot found by abduction as in our case, also thereis no subsequent generalisation step.Unlike us Osborne induces a probabilisticgrammar.
When candidate rules are added,probabilities are renormalised and the n mostlikely parses are found.
If annotated ata isbeing used, models that produce parses incon-sistent with this data are rejected.
In (Os-borne, 1999), the DCG is mapped to a SCFGto compute probabilities, in very recent work astochastic attribute-value grammar is used (Os-borne, 2000).
Giving the increasing sophistica-tion of probabilistic linguistic models (for ex-ample, Collins (1997) has a statistical approachto learning gap-threading rules) a probabilisticextension of our work is attractive-- it  will beinteresting to see how far an integration of 'log-ical' and statistical can go.Thalmann and Samuelsson (1995) describe ascheme which combines robust parsing and ruleinduction for unification grammars.
They usean LR parser, whose states and actions are aug-mented so as to try to recover from situationsthat in a standard LR parser would result in anerror.
The usual actions of shift, reduce, andaccept are augmented byhypothes ised  shift: shift a new item on tothe stack even if no such action is specified inthat statehypothes ised  unary  reduce:  reduce asymbol Y as if there was a rule X -~ Y, wherethe value of X is not yet determined.hypothes ised  b inary  reduce:  reduce asymbols Y Z as if there was a rule X ~ Y Z,where the value of X is not yet determined.The value of the X symbol is determined bythe next possibilities for reduction.To illustrate, consider the grammar1 S -+ NP VP2 NP -+ Name3 VP --+ Viand a sentence ' John snores loudly'.Assume that all the words are known(though this is not necessary for theirmethod).
The sequence of events will be:Operation Stack1.
Shift2.
Reduce with 23.
Shift4.
Reduce with 35.
HShift6.
HReduce7.
Reduce with 1Name:johnNP\[Name:john\]NP\[Name:john\] Vi:snoresNP\[Name:john\] VP\[Vi:snores\]NP VP Adv:loudlyNP X\[VP Adv\]S\[NP \[VP VP Adv\]\]After stage 4 we could reduce with 1 but thiswould not lead to an accepting state.
Insteadwe perform a hypothesised shift at stage 5 fol-lowed by a hypothesised binary reduce with XVP Adv in stage 6.
Next we reduce withrule 1 which instantiates X to VP and we havea complete parse provided we hypothesise therule VP ~ VP Adv.Two more hypothesised actions are used toaccount for gap threading:hypothes ised  move:  put the current sym-bol on a separate movement stack (i.e.
hypoth-esise that this constituent has been fronted)hypothes ised  fill: move the top of themovement stack to to top of the main stackThese actions have costs associated withthem and a control regime so that the 'cheap-est' analysis will always be preferred.
An anal-ysis which uses none of the new actions will becost-free.
Unary reduction is more expensivethan binary reduction because the consequentunary rules may lead to cycles, and such rulesare often redundant.These actions hypothesise only the contextfree backbone of the rules.
Feature principlesanalogous to those we described above are used,along with hand editing, to get the final form ofthe hypothesised rule.
Presumably the infor-mation hypothesised by the move and fill oper-ations as to be translated somehow into the gapthreading notation which is also used by theirformalism.
No details are given of the results ofthis system, nor any empirical evaluation.This work shares many of the goals of theapproach we describe, in particular the use ofexplicit encoding of background knowledge offeature principles.
The main difference is thatthe technique they describe only hypothesisesthe context free backbone of the necessary rules,whereas in our approach the feature structuresare also hypothesised simultaneously.Asker et al (1992) also describe a methodfor inducing new lexical entries when extending191coverage of a unification grammar to a new do-main, a task which is also related to our work inthat they are using a full unification formalismand using partial analyses to constrain hypothe-ses.
Firstly, they use 'explanation based gener-alisation' to learn a set of sentence templatesfor those sentences in the new corpus that canbe successfully analysed.
This process essen-tially takes commonly occurring trees and 'flat-tens' them, abstracting over the content wordsin them.
Secondly they use these templates toanalyse those sentences from the new corpuswhich contain unknown words, treatimg the en-tries implied by the templates for these wordsas provisionally correct.
Finally these inferredentries are checked against a set of hand-coded'paradigm' entries, and when all the entries cor-responding to a paradigm have been found, anew canonical lexical entry for this word is cre-ated from the paradigm.Again, no results are evaluation are given, butit is clear that this method is likely to yield sim-ilar results to our own for inference of lexicalentries.7 Future  d i rec t ionsWe find our preliminary results encouraging be-cause (i) we usually get close to missing rules,(ii) the rules are fairly linguistically sophisti-cated, for example, involving gap threading and(iii) the burden on the user is l ight--by order-ing induced rules by their coverage, the user seesthe best rules first, and does not have to botherinspecting the mass of highly specialised rulesproduced.
The work is incomplete and ongo-ing, and we conclude by listing three importanttasks for the next phase of our work: where weintend to do thorough empirical testing on realdata.
(1) In (Cussens and Pulman, 2000), edgeswere re-used to speed up cover testing.
This isstill not working in the newer implementation.
(2) In real applications missing lexical items aremore significant han missing grammar ules.Although one can easily learn lexical items byencoding them as grammar ules it should bemore efficient o replace an unknown word by avariable, and then just see how it gets instan-tiated as we parse.
(3) In these small experi-ments we could get away with an appealinglysimple learning strategy: produce and store allnaive rules then produce and store all possiblelggs.
To scale up we will probably need to usea greedier approach.AcknowledgementsThanks to Christer Samuelsson and BjSrnGamb?ck for pointing us to relevant literature.Re ferencesLars Asker, BjSrn Gamb~ick, and Christer Samuels-son.
1992.
EBL2: An approach to automatic lex-ical acquisition.
In Proceedings of the l~th Inter-national Conference on Computational Linguis-tics, pages 1172-1176.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proc.
ACL'97.James Cussens and Stephen Pulman.
2000.
Ex-periments in inductve chart parsing.
In JamesCussens and Sago D~eroski, editors, LearningLanguage in Logic.
Springer.Luc De Raedt.
1992.
Interactive Theory Revision:An Inductive Logic Programming Approach.
Aca-demic Press, London.Peter A. Flach and Antonis C. Kakas, editors.
2000.Abduction and Induction: Essays on their Rela-tion and Integration, volume 18 of Applied LogicSeries.
Kluwer, Dordrecht.Chris Mellish.
1989.
Some chart based techniquesfor parsing ill-formed input.
In Proc PTth ACL,pages 102-109, Vancouver, BC.
ACL.Stephen Muggleton and Christopher Bryant.
2000.Theory completion using inverse entailment.
InJames Cussens and Alan Frisch, editors, Proceed-ings of the lOth International Conference on In-ductive Logic Programming, pages 130-146, Lon-don, August.
Springer.Stephen Muggleton and Wray Buntine.
1988.
Ma-chine invention of first-order predicates by invert-ing resolution.
In Proceedings of the Fifth Inter-national Conference on Machine Learning, pages339-352.
Kaufmann.Stephen Muggleton and Cao Feng.
1990.
Efficientinduction of logic programs.
In Proc.
of theFirst Conference on Algorithmic Learning The-ory, pages 473-491, Tokyo.Miles Osborne.
1999.
MDL-based DCG Inductionfor NP Identification.
In Miles Osborne and ErikTjong Kim Sang, editors, CoNLL99, pages 61-68,Bergen, Norway, June.
EACL.Miles Osborne.
2000.
Estimation of StochasticAttribute-Value Grammars using an InformativeSample.
In Coling 2000.F.C.N.
Pereira.
1981.
Extraposition grammars.Computational Linguistics, 7:243-256.FraCaS project.
1996.
Fracas: A frame-192work for computational semantics.http://www, cogsci,  ed.
ac.
uk/ - f racas/ .Ehud Shapiro.
1983.
Algorithmic Program Debug-ging.
MIT Press~ Cambridge.Stuart M. Shieber, Yves Schabes, and FernandoC.
N. Pereira.
1995.
Principles and implementa-tion of deductive parsing.
Journal o\] Logic Pro-gramming, 24(1-2):3-26.
Available at the Com-putation and Language e-print archive as cmp-lg/9404008.Lars Thalmann and Christer Samuelsson.
1995.
Auniform framework for grammar induction and ro-bust parsing.
In Proceedings o\] the 5th Scandina-vian Conference on Artificial Intelligence,, pages293-304.Ruediger Wirth.
1988.
Learning by failure to prove.In Derek Sleeman, editor, Proceedings of the 3rdEuropean Working Session on Learning, pages237-251, Glasgow, October.
Pitman.J.
M. Zelle and R. J. Mooney.
1996.
Learning toparse database queries using inductive logic pro-gramming.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence, Port-land, OR, August.193
