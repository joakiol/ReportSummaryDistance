Proceedings of NAACL HLT 2007, Companion Volume, pages 117?120,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsOn using Articulatory Features for Discriminative Speaker AdaptationFlorian MetzeDeutsche Telekom LaboratoriesBerlin; Germanyflorian.metze@telekom.deAbstractThis paper presents a way to performspeaker adaptation for automatic speechrecognition using the stream weights in amulti-stream setup, which included acous-tic models for ?Articulatory Features?such as ROUNDED or VOICED.
Wepresent supervised speaker adaptation ex-periments on a spontaneous speech taskand compare the above stream-based ap-proach to conventional approaches, inwhich the models, and not stream com-bination weights, are being adapted.
Inthe approach we present, stream weightsmodel the importance of features such asVOICED for word discrimination, whichoffers a descriptive interpretation of theadaptation parameters.1 IntroductionAlmost all approaches to automatic speech recogni-tion (ASR) using Hidden Markov Models (HMMs)to model the time dependency of speech are alsobased on phones, or context-dependent sub-phoneticunits derived from them, as the atomic unit of speechmodeling.
In phonetics, a phone is a shorthand no-tation for a certain configuration of underlying artic-ulatory features (AFs) (Chomsky and Halle, 1968):/p/ is for example defined as the unvoiced, bi-labialplosive, from which /b/ can be distinguished by itsVOICED attribute.
In this sense, instead of describ-ing speech as a single, sequential stream of sym-bols representing sounds, we can also look at speechas the result of a process involving several paral-lel streams of information, each of which describessome linguistic or articulatory property as being ei-ther absent or present.A multi-stream architecture is a relatively simpleapproach to combining several information sourcesin ASR, because it leaves the basic structure ofthe Hidden Markov Model and its computationalcomplexity intact.
Examples combining differentobservations are audio-visual speech recognition(Potamianos and Graf, 1998) and sub-band basedspeech processing (Janin et al, 1999).
The sameidea can also be used to combine different classi-fiers on the same observation.
In a multi-streamHMM setup, log-linear interpolation (Beyerlein,2000) can be derived as a framework to integrat-ing several independent acoustic models given asGaussian Mixture Models (GMMs) into the speechrecognition process: given a ?weight?
vector ?
={?0, ?1, ?
?
?
, ?M}, a word sequence W , and anacoustic observation o, the posterior probabilityp(W |o) one wants to optimize is written as:p(W |o) = C exp{M?i=0?i log pi(W |o)}C is a normalization constant, which can be ne-glected in practice, as long as normalization ?i?i =const is observed.
It is now possible to setp(W |o) ?
p(o|W ) (Beyerlein, 2000) and write aspeech recognizer?s acoustic model p(o|W ) in thisform, which in logarithmic representation reducesto a simple weighted sum of so-called ?scores?
foreach individual stream.
The ?i represent the ?im-117portance?
of the contribution of each individual in-formation source.Extending Kirchhoff?s (Kirchhoff, 1999) ap-proach, the log-likelihood score combinationmethod to AF-based ASR can be used to combineinformation from M different articulatory featureswhile at the same time retaining the ?standard?acoustic models as stream 0.
As an exampleusing M = 2, the acoustic score for /z/ wouldbe computed as a weighted sum of the scores fora (context-dependent sub-)phonetic model z, thescore for FRICATIVE and the score for VOICED,while the score for /s/ would be computed as aweighted sum of the scores for a (context-dependentsub-) phonetic model s, the score for FRICA-TIVE and the score for NON VOICED.
The freeparameters ?i can be global (G), or they can bemade state-dependent (SD) during the optimizationprocess, thus changing the importance of a featuregiven a specific phonetic context, as long as overallnormalization is observed.
(Metze, 2005) discussesthis stream setup in more detail.2 ExperimentsTo investigate the performance of the proposed AF-based model, we built acoustic models for 68 ar-ticulatory features on 32h of English SpontaneousScheduling Task ESST data from the Verbmobilproject (Wahlster, 2000), and integrated them withmatching phone-based acoustic models.For training robust baseline phone models, 32hfrom the ESST corpus were merged with 66h Broad-cast News ?96 data, for which manually annotatedspeaker labels are available.
The system is trainedusing 6 iterations of ML training and uses 4000 con-text dependent (CD) acoustic models (HMM states),32 Gaussians per model with diagonal covariancematrices and a global semi-tied covariance matrix(STC) in a 40-dimensional MFCC-based featurespace after LDA.
The characteristics of the trainingand test sets used in the following experiments aresummarized in Table 1.The ESST test vocabulary contains 9400 wordsincluding pronunciation variants (7100 base forms)while the language model perplexity is 43.5 with anout of vocabulary (OOV) rate of 1%.
The languagemodel is a tri-gram model trained on ESST dataData Set Train Test1825 ds2 xv2Duration 98h 2h25 1h26 0h59Utterances 39100 1825 1150 675Recordings 8681 58 32 26Speakers 423 16 9 7Table 1: Data sets used in this work: The ESST testset 1825 is the union of the development set ds2and the evaluation set xv2.containing manually annotated semantic classes formost proper names (persons, locations, numbers).Generally, systems run in less than 4 times real-timeon Pentium 4-class machines.
The baseline WordError Rate is reported as adaptation ?None?
in Ta-ble 2; the system parameters were optimized on theds2 data set.
As the stream weight estimation pro-cess can introduce a scaling factor for the acousticmodel, we verified that the baseline system can notbe improved by widening the beam or by readjust-ing the weight of the language model vs. the acous-tic model.
The baseline system can also not be im-proved significantly by varying the number of pa-rameters, either by increasing the number of Gaus-sians per codebook or by increasing the number ofcodebooks.2.1 MMI Training of Stream WeightsTo arrive at an optimal set of stream weights, weused the iterative update rules presented in (Metze,2005) to generate stream weights ?i using the Max-imum Mutual Information (MMI) criterion (Bahl etal., 1986).Results after one iteration of stream weight esti-mation on the 1825 and ds2 data sets using stepsize  = 4 ?
10?8, initial stream weight ?0i6=0 =3 ?
10?3, and lattice density d = 10 are shown inTable 2 in rows ?AF (G) on 1825?
and ?AF (G) onds2?
: As there are only 68 stream weights to es-timate, adaptation works only slightly better whenadapting and testing on the same corpus (?cheat-ing experiment?
: 22.6% vs. 22.8% word error rate(WER) on ds2).
There is no loss in WER (24.9%)on xv2 when adapting the weights on ds2 insteadof 1825, which has no overlap with xv2, so gen-eralization on unseen test data is good for global118stream weights, i.e.
weights which do not dependon state or context.2.2 Speaker-specific Stream WeightsThe ESST test 1825 set is suitable to test speaker-specific properties of articulatory features, because itcontains 16 speakers in 58 different recordings.
As1825 provides between 2 and 8 dialogs per speaker,it is possible to adapt the system to individual speak-ers in a ?round-robin?
or ?leave-one-out?
experi-ment, i.e.
to decode every test dialog with weightsadapted on all remaining dialogs from that speakerin the 1825 test set.
Using speaker-specific, butglobal (G), weights computed with the above set-tings, the resulting WER is 21.5% (row ?AF (G) onspeaker?
in Table 2).Training parameters were chosen to display im-provements after the first iteration of training with-out convergence in further iterations.
Consequently,training a second iteration of global (i.e.
contextindependent) weights does not improve the perfor-mance of the speaker adapted system.
In our ex-periments we reached best results when comput-ing state-dependent (SD) feature weights on top ofglobal weights using the experimentally determinedsmaller learning rate of SD = 0.2 ?
.
In this case,speaker and state dependent AF stream weights fur-ther reduce the word error rate to 19.8% (see bottomrow of Table 2).2.3 ML Model AdaptationWhen training speaker-dependent articulatory fea-ture weights in Section 2.2, we were effectively per-forming supervised speaker adaptation (on separateadaptation data) with articulatory feature weights.To compare the performance of AFs to other ap-proaches to speaker adaptation, we adapted thebaseline acoustic models to the test data usingsupervised maximum likelihood linear regression(MLLR) (Leggetter and Woodland, 1994) and con-strained MLLR, which is also known as ?feature-space adaptation?
(FSA) (Gales, 1997).The ESST data has very little channel variationso that the performance of models that were trainedon both ESST and BN data can be improved slightlyon ESST test dialogs by using FSA, while MLLRalready leads to over-specialization (Table 2, rows?FSA/ MLLR on ds2).
The results in Table 2Adaptation Test corpustype and corpus 1825 ds2 xv2None 25.0% 24.1% 26.1%FSA on ds2 22.5% 25.4%FSA on speaker 22.8% 21.6% 24.3%MLLR on ds2 16.3% 26.4%MLLR on speaker 20.9% 19.8% 22.4%MMI-MAP on ds2 14.4% 26.2%MMI-MAP on speaker 20.5% 19.5% 21.7%AF (G) on 1825 23.7% 22.8% 24.9%AF (G) on ds2 22.6% 24.9%AF (SD) on ds2 22.5% 26.5%AF (G) on speaker 21.5% 20.1% 23.6%AF (SD) on speaker 19.8% 18.6% 21.7%Table 2: Word error rates on the ESST test sets us-ing different kinds of adaptation.
See Table 1 for adescription of data sets.show that AF adaptation performs as well as FSA inthe case of supervised adaptation on the ds2 dataand better by about 1.3% absolute in the speakeradaptation case, despite using significantly less pa-rameters (69 for the AF case vs. 40*40=1.6k forthe FSA case).
While supervised FSA is equiva-lent to AF adaptation when adapting and decodingon the ds2 data in a ?cheating experiment?
for di-agnostic purposes (22.5% vs 22.6%, rows ?FSA/AF (G) on ds2?
of Table 2), supervised FSA onlyreaches a WER of 22.8% on 1825 when decod-ing every ESST dialog with acoustic models adaptedto the other dialogs available for this speaker (row?FSA on speaker?).
AF-based adaptation reaches21.5% for the global (G) case and 19.8% for thestate dependent (SD) case (last two rows).
The AF(SD) case has 68*4000=276k free parameters, butdecision-tree based tying using a minimum count re-duces these to 4.3k per speaker.
Per-speaker MLLRuses 4.7k parameters in the transformation matriceson average per speaker, but performs worse than AF-based adaptation by about 1% absolute.2.4 MMI Model AdaptationIn a non-stream setup, discriminative speaker adap-tation approaches have been published using condi-tional maximum likelihood linear regression (CM-LLR) (Gunawardana and Byrne, 2001) and MMI-119MAP (Povey et al, 2003).
In supervised adapta-tion experiments on the Switchboard corpus, whichare similar to the experiments presented in the pre-vious section, CMLLR reduced word error rateover the baseline, but failed to outperform conven-tional MLLR adaptation (Gunawardana and Byrne,2001), which was already tested in Section 2.3.
Wetherefore compared AF-based speaker adaptation toMMI-MAP as described in (Povey et al, 2003).The results are given in Table 2: using a com-parable number of parameters for adaptation as inthe previous section, AF-based adaptation performsslightly better than MMI-MAP (19.8% WER vs.20.5%; rows ?MMI-MAP/ AF (SD) on speaker?
).When testing on the adaptation data ds2 as a di-agnostic experiment, MMI-MAP as well as MLLRoutperform AF based adaptation, but the gains donot carry over to the validation set xv2, which weattribute to over-specialization of the acoustic mod-els (rows ?MLLR/ MMI-MAP/ AF (SD) on ds2).3 Summary and ConclusionThis paper presented a comparison between twoapproaches to discriminative speaker adaptation:speaker adaptation using articulatory features (AFs)in the multi-stream setup presented in (Metze, 2005)slightly outperformed model-based discriminativeapproaches to speaker adaptation (Gunawardana andByrne, 2001; Povey et al, 2003), however at thecost of having to evaluate additional codebooks inthe articulatory feature streams during decoding.
Inour experiments, we used 68 AFs, which requiresthe evaluation of 68 models for ?feature present?and 68 models for ?feature absent?
for each frameduring decoding, plus the computation necessary forstream combination.
In this setup however, the adap-tation parameters, which are given by the streamcombination weights, have an intuitive meaning, asthey model the importance of phonological featuressuch as VOICED or ROUNDED for word discrimina-tion for this particular speaker and phonetic context.Context-dependent stream weights can also modelfeature asynchrony to some extent, so that this ap-proach not only improves automatic speech recogni-tion, but might also be an interesting starting pointfor future work in speaker clustering, speaker iden-tification, or other applications in speech analysis.ReferencesLalit R. Bahl, Peter F. Brown, Peter V. de Souza, andRobert L. Mercer.
1986.
Maximum mutual informa-tion estimation of Hidden Markov Model parametersfor speech recognition.
In Proc.
ICASSP, volume 1,pages 49?52, Tokyo; Japan, May.
IEEE.Peter Beyerlein.
2000.
Diskriminative Modellkom-bination in Spracherkennungssystemen mit gro?emWortschatz.
Ph.D. thesis, Rheinisch-Westfa?lisch-Technische Hochschule Aachen (RWTH), October.
InGerman.Noam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
Harper and Row, New York; USA.Mark J. F. Gales.
1997.
Maximum likelihood lineartransformations for HMM-based speech recognition.Technical report, Cambridge University, Cambridge;UK, May.
CUED/F-INFENG/TR 291.Asela Gunawardana and William Byrne.
2001.
Discrim-inative speaker adaptation with conditional maximumlikelihood linear regression.
In Proc.
Eurospeech 2001- Scandinavia, Aalborg; Denmark, September.
ISCA.Adam Janin, Dan Ellis, and Nelson Morgan.
1999.Multi-stream speech recognition: Ready for primetime.
In Proc.
EuroSpeech 1999, Budapest; Hungary,September.
ISCA.Katrin Kirchhoff.
1999.
Robust Speech Recognition Us-ing Articulatory Information.
Ph.D. thesis, Technis-che Fakulta?t der Universita?t Bielefeld, Bielefeld; Ger-many, June.Chris J. Leggetter and Phil C. Woodland.
1994.
Speakeradaptation of HMMs using linear regression.
Techni-cal report, Cambridge University, England.Florian Metze.
2005.
Articulatory Features for Conver-sational Speech Recognition.
Ph.D. thesis, Fakulta?tfu?r Informatik der Universita?t Karlsruhe (TH), Karl-sruhe; Germany, December.Gerasimos Potamianos and Hans-Peter Graf.
1998.
Dis-criminative training of HMM stream exponents foraudio-visual speech recognition.
In Proc.
ICASSP1998, Seattle, WA; USA.
IEEE.Dan Povey, Mark J.F.
Gales, Do Y. Kim, and Phil C.Woodland.
2003.
MMI-MAP and MPE-MAP foracoustic model adaptation.
In Proc.
Eurospeech 2003,Geneva; Switzerland, September.
ISCA.Wolfgang Wahlster, editor.
2000.
Verbmobil: Founda-tions of Speech-to-Speech Translation.
Springer, Hei-delberg.120
