Taking Account of the User's View in 3D Multimodal InstructionDialogueYuk iko  I. Nakano and Ken j i  hnamura  and Hisash i  Ohara1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan{yukiko, i lnamura, ohara}@ntl;nly.isl.ntt.co.jpAbst ractWhile recent advancements in virtual reality technologyhave created a rich communication interface linking hu-mans and computers, there has beefl little work on build-ing dialogue systems for 3D virtual worlds.
This paperproposes a method for altering the instruction dialogueto match the user's view in a virtual enviromnent.
-\~reillustrate the method with the system MID-aD, which in-teractively instructs the user on dismantling some partsof a car.
First, in order to change the content of ~heinstruction dialogue to match the user's view, we extendthe refinement-driven plmming algorithm by using theuser's view as a l)lan constraint.
Second, to manage thedialogue smoothly, the systeln keeps track of the user'sviewpoint as part of the dialogue skate and uses thisinformation for coping with interruptive sul)dialogues.These mechanisms enable MID-3D to set instruction di-alogues in an incremental way; it takes account of theuser's view even when it changes frequently.1 I n t roduct ionIn a aD virtual enviromnent, we can freely walkthrough the virtual space and view three di-mensional objects from various angles.
A inul-tilnodal dialogue system for such a virtual en-vironment should ainl to realize conversationswhich are performed in the real world.
It wouldalso be very useflll for education, where it isnecessary to learn in near real-life situations.One of the most significant characteristics of3D virtual environments is that the user can se-lect her/his own view from whidi to observe thevirtual world.
Thus, the nmltimodal instruc-tion dialogue system should be able to set thecourse of the dialogue by considering the user'scurrent view.
However, previous works on nml-tilnodal presentation generation and instruc-tion dialogue generation (Wahlster et al, 1993;Moore, 1995; Cawsey, 1992) do not achieve thisgoal because they were not designed to hail-(lie dialogues pertbrmed in 3D virtual environ-ments .This paper proposes a method that ensuresthat the course of the dialogue matches theuser's view in the virtual environment.
Morespecificall> we focus on (1) how to select thecontents of the dialogue since it is essentialthat the instruction dialogue system form a se-quence of dialogue contents that is coherentand comprehensible, and (2) how to controlmixed-initiative instruction dialogues nloothly,especially how to manage interruptive subdia-logues.
These two problelns basically determinethe course of the dialogue.First, in order to decide the appropriate con-tent, we propose a content selection mechanismbased on plan-based multilnodal presentationgeneration (Andrd and Rist, 1993; Wahlster etal., 1993).
We extend this algorithm by usingthe user's view as a constraint in expanding theplan.
In addition, by employing tilt incremen-tal planning algorithm, the syst;em can adjustthe content o match the user's view during on-going conversations.Second, ill order to nlanage interruptive sub-dialogues, we propose a dialogue managementmechanism that takes account of the user'sview.
This mechanism maintains the user'sviewpoint as a dialogue state in addition to in-tentional and linguistic context (Rich and Sid-her, 1998).
It maintains the dialogue state as afocus stack of discourse segments and updatesit at each turn.
Tlms, it Call track the view-point information in an on-going dialogue.
Byusing this viewpoint inibrlnation in restartingthe dialogue after an interruptive subdialogue,the dialogue Inai~agement medmnism returnsthe user's viewpoint o that of the interruptedsegment.These two mechanisms work as a core dia-logue engine in MID-3D (Multimodal Instruc-tion Dialogue system for 3D virtual environ-ments).
They make it possible to set the in-struction dialogue in an increnlental ww while572Figure 1: Right angleFigure 2: l,efl; angleconsidering the user's view.
They also (mal)h'~MID-a1) to (:re~te coherent and mixe, d-initiative(liah)gues in virtual enviromuents.This paper is organized as lbllows.
In Sec-ti(m 2, we define the 1)rol)h;ms spc(:ifi(: 1;o 313multimoda\] (tiah)gne genera.tion.
Section 3 de-scribes rclat;ed works.
\ ]n S('x:l;ion 4, we pro-pose the MID-a1) architecture.
Sections 5 ;rod6 des(:ril)e the contenl; plmming meclm.nism a.ndthe dialogue manngement meclm.nism, a.nd showthey dynami(:ally decide coherent insl;rn(:t;ions,and control mixed-initial;ire diah)guc.s consider-ing the user's view.
V~/e also show a smnt)le di-:dogue in Section 7.2 Prob lemsIn a virtual emdromnent, the user can freelymove a.round the world and select her/his ownview.
r\['he systelll C&llllOt; predict where the userwill stand and what; s/he observes in the vir-tual environment.
This section describes twotypes of 1)roblems in generating instru(:tion dia-logues ibr such virtual enviromnents.
They arccaused l)y mismatches b(~,twe(;ll tile llSel'~S vi0,w-l)oint ;m(1 the sta.te of th(; dialogue.First, the syStelll shouM check whether theuser's view matches the tbcns of the next ex-change when the systen~ tries to ('hange COllllllll-ni('ative goals.
\]if a mismatch occurs, the systemshouhl choose the instru(:tion (li~dogue contentaccording to the user's view.
Figure 1 a,n(1 2 m:eexaml)les of observing a car's front suspensionfrom (liff(',r(mt, points of view.
In Figm'(', 1, theright; side of the steering system can 1)e seen,while Figure 2 shows the left side.
If the systemis not aware of the user's view, I;he system maytalk about the left; tie rod end even though theuser's view remains the right side (Figure 1).In such n (:ase, the system shouM chang(: its d(>scril)tion or ask the user to change her/his viewto |;11('.
left; side.
view (Figure 2) and r('.
(-Olmnen(:eits instruction hi)out this part.
Therefore, thesystem should be al)le to change the contentof the dialogue according 1;o the user's view.In order to ac(:omplish this, the system shoul(1lmve ;1. content selection nlechan.ism whi(:h in-crementally (let:ides i;h('~ content while ('he(:kingthe llSef~s (;llrrellt vi(!w.Second, t;here could 1)e a case in which 1;21(;user chang(~s 1;he, topi(: as well as the vie\vl)oilltas interrupl;ing the.
system's instru('t;ion, i n sucha case, the (tia.h)gue~ system shouhl kee l) track ofthe user's viewpoint as ~ 1)art of the dialoguestate nnd return to that viewpoint when resmn-ing the (lia.logu(?
after the interrupl;ing sul)(li-alogue.
Sul)l)ose that while the sys|;em is (',x-l)lnining tlm right; t)i(; rod end, th('.
user initiallylooks a,t the right side, (l"igure 1) hut then shiftsher/his view to the left (Figure 2) and asksabout the \]eft knu(-kle arm.
After finishing asub(lialogue about this arm, the syst(;nl triesto return to the dialogue al)out the interruptedtopic.
At this time, if the sysl;em resumed thedialogue using the current view (Figure 2), theview and the instruction would \])e(;olne mis-matched.
When resmning the interrupted i-alogue, it would be less (:onfllsing to the userif the system retm:ned to the user's prior view-l)oint rather than selecting n new o11o.
'\].
'he usermay be (:onfilsed if the dialogue is resulned butthe observed state looks different.\,Ve address the ~fl)ove problems.
In order to(:ope wit;h the first; problem, we present a con-tent selection mechanism that incrementally ex-pands the content plan of a multimodal dialoguewhile checking the user's view.
To solve thesecond 1)roblem, we present a. dialogue nmnage-merit me(:\]mnism l;hat keel)s t:ra(-k of the user'sviewpoint as a part of the diah)gue context and573.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-:~; '~ Operation l,uttonsFigure 3: The system architectureuses this intbrmation in resuming the dialogueafter interruptive subdialogues.3 Re la ted  workThere are many multimodal systems, such asnmltimedia presentation systems and animatedagents (Mwbury, 1993; Lester et al, 1997;Bares and Lester, 1997; Stone and Lester, 1996;Towns et al, 1998)~ all of which use 3D graph-ics and 3D animations.
In some of them (May-bury, 1993; Wahlster et al, 1993; Towns etal., 1998), planning is used in generating mul-timodal presentations including graphics andanimations.
They are similar to MID-aD inthat they use planning mechanisms in contentplanning.
However, in presentation systems,unlike dialogue systems, the user just watchesthe presentation without changing her/his view.Therefore, these studies are not concerned withdlanging the content of the discourse to matchthe user's view.In some studies of dialogue management(Rich and Sidner, 1998; Stent et M., 1999),the state of the dialogue is represented usingGrosz and Sidner's framework (Grosz and Sid-ner, 1986).
We also adopt this theory in our di-alogue management mechanism.
However, theydo not keep track of the user's viewpoint infor-mation as a part of the dialogue state becausethey were not concerned with dialogue manage-ment in virtual environments.Studies on pedagogical agents have goalscloser to ours.
In (Rickel and .\]ohnson, 1999),a pedagogical agent demonstrates the sequen-tial operation of complex machiuery and an-swers some follow up questions fl'on~ the stu-dent.
Lester et al (1999) proposes a life-like pedagogical agent that supports problem-solving activities.
Although these studies areconcerned with building interactive learning en-vironments using natural anguage, they do notdiscuss how to decide the course of on-going in-struction dialogues in an incremental nd coher-ent way.4 Overview of the SystemArch i tec tureThis section describes the architecture of MID-3D.
This system instructs users how to disman-tle the steering system of a cal'.
Tile systemsteps through the procedure and the user caninterrupt he system's instructions at any time.Figme 3 shows the architecture and a snapshotof the system.
The 3D virtual environment isviewed through an application window.
A 3Dmodel of a part of the car is provided and a frog-574like character is used as the pedagogical agent(Johnson et al, 2000).
The user herself/himselfCall also al)l)ear in the virtual enviromn(mt asan avatar.
The buttons to the right of the 3Dscre(m are operation 1)uttons tbr changillg theviewpoint.
By using these buttons, the user canfreely change her/his viewt)oint at any time.This system consists of five main modules:hll)Ut Analyzer, Domain Plan Reasoner, Con-tent Planner (CP), Sentence Planner, DialogueManager (DM), and Virtual Environment Con-troller.First of all, the user's inputs are interpretedthrough the Input Analyzer.
It receives tringsof characters from the voice recognizer andthe user's inputs ti'om the Virtual EnvironmentController.
It interl)rets these inputs, trans-forms them into a semantic reprcsentation~ andsends them to the DM.The DM, working as a dialogue managementmechanism, keeI)s track of the dialogue (:ontextincluding the user:s view and decides the, nextgoal (or a(:tion) of the system.
Ut)on receiv-ing an intmt from the user through the InputAnalyzer, the DM sends it to the l)omaill PlanReasoner (DPR) to get discourse goals for re-st)onding to the inlmt.
For example, if th(: userrequests ome instruction, the DI'I{ decides thesequence of steps that realizes the l)rocedure 1)yrefi~rring to domain knowh~dge.
Th(: 1354 (;henadds (;he discourse goals to the goal agenda.If the user does not sulmlit a ~lew (;ot)ie , theDM (:ontilmes to expand the, instruction plan1)y sending a goal in the goal agenda to (:lie CP.Details of the I)M are given in Section 6.After the goal is sent to the CP, it decides theapl)ropriate contents of instruction dialogue byeml)loying a refinement-driven hierar(:hi(:al lin-ear 1)lamfing technique.
When it; receives a goalfl'om the DM, it exl)ands the goal and returnsits sul)goal to the DM.
13y ret)eating this pro-cess, the dialogue contents are, gradually spec-ified.
Theretbre, the CP provides the scenariotbr the instruction 1)ased on the control 1)rovidedby the DM.
Details of the CP are provided inSection 5.The Sentence Plalmer generates urface, lin-guisti(: expressions coordinated with action(Kato et al, 1996).
The linguistic exl)ressionsarc.
output through a voice synthesizer.
Actions;/re realized through the Virtual EnviromnentController as 3D animation.For the Virtual Environment Controller, weuse HyCLASS (Kawanol)e et al, 1998), which<Operator 1>(:tleader:Iiftbcl:Constraints:Main-Acts:Subskliary-Acts<Operator 2>(:lleader:Effect:Conslraints:Main-Acts:Subsidiary-Acts(Inshuct-act N l l ?act MM)(BMB S 11 (Goal II (Done 11 ?act)))((KB (Obj ?act ?object))(Visible-p (Visible ?ol~iect t)))((Look S II)(Request S I I (Try It (action ?act)) NO-SYNC MM))((Describe- act S II ?act MM)(Reset S (actioll ?act))))(Instruct-act S 11 ?act MM)(BMB S 11 (Goal I1 (Done 11 '?act)))((KB (Obj ?act ?object))(Visiblc-p (Visible ?object oil)))((Look S ll)(Make-recognize S 11 (Object ?object) MM)(Rcqucst S 11 (Try I1 (action ?act)) NO-SYNC M M))((l)escribc-act S 11 ?act MM)(Reset S (action '?act))))Figure 4: Exanlt)les of Content Plan Operatorsis a 3D simulation-1)ased nvironment tbr edu-(:ational activities.
Several APls are providedtbr controlling HyCLASS.
By using these in-terfaces, the CP and the DM can discern theliser~s view and issue an action command in ()l'-der to challge the virtual (;nvironnmllt.
\?h(mHyCLASS receives an action command, it in-terprets the command and renders the 31) ani-mation corresponding to the action in real time.5 Se lec t ing  the  Conten(;  o fI ns t ruct ion  D ia logueIll this section, we introduce the CP and showhow the instruction dialogue is (leeided in allin(:renl(:ntal way to ma, tch the user's view.5.1 Content  P lannerIn MID-3D, the CP is (:ailed by the DM.
Wheala goal is put to the CP fl'(nn the DM, it; selects aplan operator fi)r achieving the goal, applies theol)erator to lind new subgoals, and returns themto l;he \])M. The sul)goals are then added to thegoal agenda maintained by the DM.
Theretbre,the CP provides the seenm:io tbr the instruc-tion dialogue to the DM and enables MID-3Dto output coherent instructions.
Moreover, theContent Planer emt)loys depth-first search witha retinement-drivell hierarchical linear plmmingalgorithm as in (Cmvsey, 1992).
The advantageof this method is that the t)lan is de, veloped in-crenmntally, and can be changed while the con-versation is in progress.
Thus, by aI)plying thisalgorithm to 3D dialogues, it be(-omes lmssibleto set instruction dialogue strategies that arecontingent on the user's view.5755.2 Considering the User's View inContent  Se lect ionIn order to decide the dialogue content accord-ing to tile user's view, we extend the descrip-tion of the content plan operator (Andrd andRist, 1993) by using the user's view as a con-straint in plan operator selection.
We also mod-ify the constraint checking flmctions of |;lie pre-vious planning algorithm such that HyCLASSis queried about the state of the virtual envi-ronment.Figure 4 shows examples of content plan op-erators.
Each operator consists of the nameof the operator (Header), the etfcct resultingfrom plan execution (Effect), the constraints forexecuting the plan (Constraints), the essentialsubgoals (Main-acts), and the optional subgoals(Subsidiary-acts).
As shown in {Operator 1.
)in Figure 4, we use the constraint (gisible-p(Visible ?object t)) to check whether theobject is visible fl'om tile user's viewpoint.Actually, the CP asks HyCLASS to examinewhether the object is in the student's field ofview.If an object is bound to the ?ob jec t  vari-able by rel~rring to the knowledge base, andthe object is visible to the user, (Operator 1)is selected.
As a result, two Main-Acts (look-ing at the, user and requesting to try to dothe action) and two Subsidiary-Acts (showinghow to do the action, then resetting the state)are set as subgoals and returned to the DM.In contrast, if l;he object is not visible to theuser, {Operator 2} is selected.
In this case, agoal for making the user i(tenti(y the object isadded to the Main-Acts; (Hake-recognize SH (Object ?object) MM).As shown al)ove, the user's view is consideredin deciding the instruction strategy.
In additionto the above example, the distance between thetarget object and the user as well as three di-mensional overlapping of objects, can also beconsidered as constraims related to the user'sview.Although the user's view is also considered inselecting locative expressions of objects in theSentence Planner in MID-3D, we do not discussthis issue here becanse surface generation is notthe tbcus of this paper.6 Manag ing  I n ter rupt iveSubd ia logueThe DM controls the other components ofMID-3D based on a discourse model that representsthe state of tile dialogue.
This section describesthe DM and shows how the user's view is usedin managing the instruction dialogue.6.1 Maintaining the  D iscourse  Mode lThe DM maintains a discourse model for track-ing the state of the dialogue.
The discoursemodel consists of the discourse goal agenda(agenda), focus stack, and dialogue history.
Theagenda is a list of goals that should be achievedthrough a dialogue between the user and thesystem.
If all the goals in the agenda re accom-plished, the instruction (tialogue finishes suc-cessflflly.
The focus stack is a sta& of discoursesegment frames (DSF).
Each DSF is a frmnestructure that stores the tbllowing inlbrmationas slot vMues:utterance content (UC): A list of utter-ance contents constructing a discourse segment.Physical actions are also regarded as uttcra.ncecontents (D;rguson and Allen, 1998).discourse purpose (1)19: The purt)ose of a dis-course segment.- 9oal state (GS): A state (or states) whi('hshouhl 1)e accomplished to achieve the discourselmrpose of the segment.In addition to these, we add the user's view-point slot to the DSF description in order totrack the user's viewl)oint information:user's vic.
'wpoint (UV): Current user's view-point, which is represented as the position andorientation of the camera.
The position consistsof x-, y-, and z-coordinates.
The orientationconsists of x-, y-, and z-angles of the ('amera.The basic algorithm of the DM is to repeat(a) th(; peribnning actions step and (1)) updat-ing the discourse model, until there is no un-satisfied goal in the agenda (~IYaum, 1994).
In1)ertbrming actions step, the DM decides whatto do next ill the current dialogue state, an(1then pertbnns the action.
When continuing thesystem explanation, the DM posts the first goalin the agenda to the CP.
If the user's responseis needed in the current state, the 1)M waits tbrthe nser's input.The other step in the DM algorith.m is to up-date the discourse model according to the statethat results from the actions pertbrmed by theuser as well as the actions peribrmed by the sys-tem.
Although we do not detail this step here,the tbllowing operations could be executed e-pending on the case.
if the current discoursepurpose is accomplished, the top level DSF ispopped and added to the dialogue history, q_/he576l I)SFI21DSFI2DSFI Jf J JUV: ((18, -20, -263) (0, 0.3 I, 0))UC: ((IJseJ~act (Ask where heal_r))I)P: (Response-to-user-act(Uscr-act (ask where bootr)))GS: ((Know 11 (About (l'lace_of boot_r)))...)UV: ((-38, -22, -259) (0, -0.33, 0))UC: ((System-act (lnl'(~rm S 11 (Show S (Actionrcmovc-tiemd end.I)) NO-SYNC I'R))DI': (I)cscribe-acl S l I rcmove-licrod end I))GS: ((Know 1I (llove-lo-do 11(action remove-tiered eml I)))...)Figure 5: Example of the state of a dialoguesystem then assunms that the user understandsthe instruction and adds the assumption to theuser model.
If a new discourse 1)urpose is in-troduced from the CP, the I)M creates a newDSF by setting the header of the selected planoperator in the discourse lmrpose slot mM theeffi~ct of the operator in the goal state slot.
TheDSF is then trashed to the tbcus stack.
If thecurrent discourse purpose is contimmd, the DMupdates the information of the top level DSF.6.2 Cons ider ing  the  User 's  V iew inCoping wi th  Interrupt iveSubd ia loguesThe main ditlbxence of the Dialogue Manager ofour system from the i)revious one is to maintainthe user's viewpoint information and use this inmanaging the dialogue.
When the DM updatesthe information of the current DSt i', it observesthe user's viewi~oint at  that petal; and renewsthe UV slot and it also adds the sema.nl;ic rep-resentation of utterance (or action) in the UCslot.
As a result, it becomes possible to updatethe user's viewpoint information at each turn,and to track the user's viewl)oint in an on-goingdialogue.By using this mechanism, the DM can copewith interruptive subdialognes.
In resmningfrom a subdialogue, the user may become con-timed if the dialogue is resumed but the ob-served state differs from what the user relllelll-hers.
In order to match the view to the re-sumed dialogtm, the I)M refers the UV slot ofthe top DSF and puts the users view ha& tothat point.
This ensures that the user experi-ences a smooth transition back to the previoustopic.
Figure 5 shows an example of the state ofa dialogue.
DSF12 represents a discourse seg-ment that describes how to remove the left tierod end.
DSF121 represents the user-initiatedinterrul)tive subdialogue about where the right\[14\]System: Theleft knuckle arm is removed like this.
(with the anilnation showing the left knucklearm coming off)\[ 15\]User: (After moving the viewpoint to Figure I andclicking the right knuckle ann)What is this?\[16\]System: This is the right knuckle arm.\[I 7\]Uscr: OK.\[18\]Systeln: Now, let's continue the explanation.
(with changing the view to the one in utterance \[ 14\])\[19\]System: Theleft knuckle arm is removed like this.
(with the animation showing the left knuckle armcoming off)\[20\]User: Yes.\[21\]System: Now, try removing the left knuckle arm.\[22\]User: (After clicking the left knuckle arm) Remove it.\[23\]System: Youhave succcssfitlly completed thc operation.\[24\]System: Thenext step is to remove the right ie rod cad.\[25\]System: Let's move to the right ie rod end.1126\] User: Ycs.\[27\]System: (Moving the view to the right)The right ie rod end is in fiont of you.\[28\] User: Yes, 1 see.Figure 6: Example of a dialogue with MII)-3Dboot is.
hmnediately before starting DSF\]21,the user's viewpoint in l)SF12 is ((-38, -22,-259) (0, -0.33, 0)).
After completing thesubdialogue \])y answering the user's question,DSF121 is l)opped and the system resmnesDSF12.
At this time, the \])M gets the view-point value of the top DSF (DSF12), alld (;Oltl-mands ItyCLASS to change the viewpoint tothat view, which is in this case ((-as, -22, -2,59)(0, -0.a3, 0)) ' The systeln then restarts theinterrupted ialogue.7 ExmnpleIn order to illustrate the behavior of MID-3D,an example is shown in Figure 6.
This is a partof an instruction dialogue on how to dismantlethe steering system of a car.
The current topicis removing the left knuckle arm.
In utterance\[14\], the system describes how to remove thispart in conjunction with an animation createdby HyCLASS.In \[15\], the user interrupted the system's in-struction and asked "What is this?"
by clickingthe right knuckle arm.
At this point, the user'sspeech input was interpreted in the Input An-~In the current system, it; is not 1)ossible to movethe camera to an arbitrary point because of the limi-tations of the virtual environment controller employed.Accordingly, this func|;ion is al)proximated by selectingthe nearest of several predetined viewpoints.577alyzer and a user initiative subdialogue startedby t)ushing another DSF onto the focus stack.In order to answer the question, the DM askedthe Domain Plan Reasoner how to answer theuser's question.
As a result, a discourse goal wasreturned to the DM and added to the agenda.The DM then sent the goal (Describe-name SH (object  knuckle_arm_r)) to the CP.
Thisgoal generated utterance \[16\].In system utterance \[18\], in order to resumethe dialogue, a recta-comment, "Now let's con-tinue the explanation", was generated and theviewpoint returned to the previous one in \[14\]as noted in the DSF.
After returning to the pre-vious view, the interrupted goal was re-planned.As a result, utterance \[19\] was generated.After completing this operation in \[23\],the next step, removing the right tie rodend, is started.
At this time, if theuser is viewing the left side (Figure 2) andthe system has the goal ( Ins t ruct -ac t  SH remove-tierod_end_r MR), (Operator 2} inFigure 4 is applied because the target object,right tie rod end, is not visible fi'om the user'sviewpoint.
Thus a goal of making the user viewthe right tie rod end is added as a subgoal andutterances \[24\] and \[25\] are generated.8 Discuss ionThis paper proposed a inethod tbr altering in-struction dialogues to match the user's view ina virtual enviromnent.
We described the Con-tent Planner which can incrementally decide co-herent instruction dialogue content to matchchanges in the user's view.
We also presentedthe Dialogue Manager, which can keep trackof the user's viewpoint in an on-going dialogueand use this intbrmation i resuming from inter-ruptive subdialogues.
These mechanisms allowto detect mismatches between the user's view-point and the topic at any point in the dialogue,and then to choose the instruction content anduser's viewpoint appropriately.
MID-3D, an ex-perimental system that uses these mechanisms,shows that the method we proposed is effectivein realizing instruction dialogues that suit theuser's view in virtual enviromnents.Re ferencesElisabeth Andr6 and Thmnas Rist.
1993.
The design ofil lustrated ocuments as a planning task.
In Mark T.Maybury, editor, Intelligent Multimedia Interfaces,pages 94-116.
AAAI Press / The MIT Press.Will iam H. Bares and James C. Lester.
1997.
Real-time generation of customized 3D animated explana-tions for knowledge-based learning environments.
InAAAI97, pages 347-354.Alison Cawsey.
1992.
Explanation and Interaction: TheComputer Generation of Expalanatory Dialogues.
The.MIT Press.George Ferguson and James F. Allen.
1998.
TRIPS:An integrated intelligent problem-solving assistant.In AAAI98, pages 567-572.Barbara J. Orosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175-204.W.
Lewis Johnson, Jeff W. Rickel, and James C. Lester.2000.
Animated pedagogical agents: Face-to-face in-teraction in interactive learning environments.
Inter-national Journal of Artificial InteUigencc in Educa-tion.Tsuneaki Kato, Ynkiko I. Nakano, Hideharu Nakajima,and Takaaki Hasegawa.
1996.
Interactive mnltimodalexplanations and their temporal coordination.
InECAI-96, pages 261-265.
John Willey and Sons Lim-ited.Akihisa Kawanobe, Susumn Kakuta, Hirofumi Touhei,and Katsumi Hosoya.
1998.
Preliminary reporton HyCLASS anthoring tool.
In ED-MEDIA/ED-TELECOM.James C. Lester, Jennifer L. Voerlnan, Stuart O. Towns,and Charles B. Callaway.
1997.
Cosmo: A lih;-likeanimated pedagogical gent witl, deictie believability.In IJCAI-97 Workshop, Animated Interface Agent.Jmnes C. Lester, Brian A.
Stone, and Gray D. Stelling.1999.
Lifelike pedagogical gents for mixed-initiativeproblem solving in constructivist learning environ-ments.
User Modeling and User-Adapted Interaction,9(1-2):1-44.Mark T. Maybury.
1993.
Planning multimedia explana-tion using communicative acts.
In Mark T. Maylmry,editor, Intelligent Multimedia Interfaces, pages 59 -74.AAAI Press / The MIT Press.Johamm D. Moore.
1995.
Participating in ExplanatoryDialogues: Interpreting and I~esponding to Questionsin Context.
MIT Press.Chm'les Rich and Candace L. Sidner.
1998.
COLLA-GEN: A collaboration manager for software interfhceagents.
User Modeling and User-Adapted Interaction,8:315-350.Jeff W. Rickel and W. Lewis Johnson.
1999.
Animatedagents for procedual training in virtual reality: Per-ception, cognition and motor control.
Applied Artifi-cial Intellifence, 13:343-392.Amanda Stent, John Dowding, Jean Mark Gawron, Eliz-abeth Owen Brat, and Robert Moore.
1999.
TheCommandTalk spoken dialogue systeln.
In AC'Lgg,pages 183-190.Brian A.
Stone and James C. Lester.
1996.
Dynami-cally sequencing an animated pedagogical agent.
InAAAI96, pages 424-431.Stuart G. Towns, Charles B. Callaway, and 3anles C.Lester.
1998.
Generating coordinated natural lan-guage and 3D animations for complex spatial expla-nations.
In AAAI98, pages 112-119.David R. Traum.
1994.
A Computational Theory ofGrounding in Natural Language Conversation.
Ph.D.thesis, University of Rochester.Wolfgang \Vahlster, Elisabcth Andr6, Wolfgang Fin-kler, Hans-Jiirgen Profitlieh, and Thomas Rist.
1993.Plan-based integration of natural anguage and graph-ics generation.
Artificial Intelligence, 63:387-427.578
