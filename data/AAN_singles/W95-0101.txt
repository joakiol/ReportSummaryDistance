Unsupervised Learning of Disambiguation Rules for Part ofSpeech TaggingEric BrilPDepartment of Computer ScienceJohns Hopkins Universitybrill@cs, j hu.
eduAbstractIn this paper we describe an unsupervised learning algorithm for automaticallytraining arule-based part of speech tagger without using a manually tagged corpus.
Wecompare this algorithm to the Baum-Welch algorithm, used for unsupervised training ofstochastic taggers.
Next, we show a method for combining unsupervised and supervisedrule-based training algorithms to create a highly accurate tagger using only a smallamount of manually tagged text.In t roduct ionThere has recently been a great deal of work exploring methods for automatically trainingpart of speech taggers, as an alternative to laboriously hand-crafting rules for tagging, aswas done in the past \[Klein and Simmons, 1963; Harris, 1962\].
Almost all of the work inthe area of automatically trained taggers has explored Markov-model based part of speechtagging \[Jelinek, 1985; Church, 1988; Derose, 1988; DeMarcken, 1990; Cutting et al, 1992;Kupiec, 1992; Charniak et al, 1993; Weischedel et al, 1993; Schutze and Singer, 1994; Linet al, 1994; Elworthy, 1994; Merialdo, 1995\].
2 For a Markov-model based tagger, trainingconsists of learning both lexical probabilities (P(word l tag) )  and contextual probabilities(P(tagi l tagi_ l  .
.
.
tagi-n)) .
Once trained, a sentence can be tagged by searching for the tagsequence that maximizes the product of lexical and contextual probabilities.The most accurate stochastic taggers use estimates of lexical and contextual proba-bilities extracted from large manually annotated corpora (eg.
\[Weischedel t al., 1993;Charniak et al, 1993\]).
It is possible to use unsupervised learning to train stochastictaggers without the need for a manually annotated corpus by using the Baum-Welch al-gorithm \[Baum, 1972; Jelinek, 1985; Cutting et al, 1992; Kupiec, 1992; Elworthy, 1994;Merialdo, 1995\].
This algorithm works by iteratively adjusting the lexical and contextualprobabilities to increase the overall probability of the training corpus.
If no prior knowledgeis available, probabilities are initially either assigned randomly or evenly distributed.
Al-though less accurate than the taggers built using manually annotated corpora, the fact thatthey can be trained using only a dictionary listing the allowable parts of speech for eachword and not needing a manually tagged corpus is a huge advantage in many situations.
Al-though a number of manually tagged corpora are available (eg.
\[Francis and Kucera, 1982;Marcus et al, 1993\]), training on a corpus of one type and then applying the tagger to acorpus of a different ype usually results in a tagger with low accuracy \[Weischedel t al.,1993\].
Therefore, if tagged text is needed in training, this would require manually tagging1This work was funded in part by NSF grant IRI-9502312.2Some other approaches to tagging are described in \[Hindle, 1989; Black et al, 1992\].text each time the tagger is to be apphed to a new language, and even when being appliedto a new type of text.In \[Brill, 1992; Brill, 1994\], a rule-based part of speech tagger is described which achieveshighly competitive performance compared to stochastic taggers, and captures the learnedknowledge in a set of simple deterministic rules instead of a large table of statistics.
Inaddition, the learned rules can be converted into a deterministic finite state transducer.Tagging with this finite state transducer requires n steps to tag a sequence of length n,independent of the number of rules, and results in a part of speech tagger ten times fasterthan the fastest stochastic tagger \[Roche and Schabes, 1995\].
One weakness of this rule-based tagger is that no unsupervised training algorithm has been presented for learningrules automatically without a manually annotated corpus.
In this paper we present such analgorithm.
We describe an algorithm for both unsupervised and weakly supervised trainingof a rule-based part of speech tagger, and compare the performance of this algorithm tothat of the Baum-Welch algorithm.Transformation-Based Error-Driven LearningThe rule-based tagger is based on a learning algorithm called transformation-based error-driven learning.
Transformation-based error-driven learning has been applied to a numberof natural language problems, including part of speech tagging, prepositional phrase at-tachment disambiguation, speech generation and syntactic parsing \[Brill, 1992; Brill, 1994;Ramshaw and Marcus, 1994; Roche and Schabes, 1995; Brill and Resnik, 1994; Huang etal., 1994; Brill, 1993a; Brill, 1993b\].
Figure 1 illustrates the learning process.
First, unan-notated text is passed through an initial-state annotator.
The initial-state annotator canrange in complexity from assigning random structure to assigning the output of a sophis-ticated manually created annotator.
Once text has been passed through the initial-stateannotator, it is then compared to the truth as specified in a manually annotated corpus, andtransformations are learned that can be applied to the output of the initial state annotatorto make it better resemble the truth.In all of the applications explored to date, the following greedy search is applied: at eachiteration of learning, the transformation is found whose application results in the highestscore; that transformation is then added to the ordered transformation list and the trainingcorpus is updated by applying the learned transformation.
To define a specific applicationof transformation-based learning, one must specify the following:1.
The initial state annotator.2.
The space of transformations the learner is allowed to examine.3.
The scoring function for comparing the corpus to the truth and choosing a transfor-mation.Once an ordered list of transformations is learned, new text can be annotated by firstapplying the initial state annotator to it and then applying each of the learned transforma-tions, in order.2UNANNOTATEDTEXTSTATENER RULESFigure 1: Transformation-Based Error-Driven Learning.T rans format ion -Based  Par t  o f  Speech  Tagg ingIn transformation-based part of speech tagging, 3 all words are initially tagged with theirmost likely tag, as indicated in the training corpus.
Below are some of the transformationtemplates used by the learner.
4Change tag a to tag b when:1.
The preceding (following) word is tagged z.2.
The preceding (following) word is w.3.
The word two before (after) is w.4.
One of the two preceding (following) words is tagged z.5.
The current word is w and the preceding (following) word is x.6.
The current word is w and the preceding (following) word is tagged z.The evaluation measure is simply tagging accuracy.
In each learning iteration, thesystem learns that transformation whose application results in the greatest reduction oferror.
5 Because the learning algorithm is data-driven, it only needs to consider a small3For a more detailed description of supervised transformation-based part of speech tagging, see \[Brill,1994\].4In \[Brill, 1994\], a total of 21 templates are used.5Note an important difference between Markov-mode\] based taggers and the transformation-based tagger:the former attempts to maximize the probability of a string, whereas the latter attempts to minimize thenumber of errors.3percentage of all possible transformations when searching for the best one.
An example ofa learned transformation is:Change the tag of a word from VERB to NOUN if the previous word is a DETER-MINER.If the word race occurs more frequently as a verb than as a noun in the training corpus,the initial state annotator will mistag this word as a verb in the sentence: The race wasvery excit ing.
The above transformation will correct his tagging error.It was shown in \[Brill, 1994\] that the transformation-based tagger achieves a high rateof tagging accuracy.
The transformation-based tagger captures its learned information ina set of simple rules, compared to the many thousands of opaque probabilities learned byMarkov-model based taggers.
6 Supervised training is feasible when one has access to alarge manually tagged training corpus from the same domain as that to which the trainedtagger will be applied.
We next explore unsupervised and weakly supervised training as apractical alternative when the necessary resources are not available for supervised training.Unsuperv ised  Learn ing  of  T rans format ionsIn supervised training, the corpus is used for scoring the outcome of applying transfor-mations, in order to find the best transformation i  each iteration of learning.
In orderto derive an unsupervised version of the learner, an objective function must be found fortraining that does not need a manually tagged corpus.We begin our exploration providing the training algorithm with a minimal amount ofinitial knowledge, namely knowing the allowable tags for each word, and nothing else.
7The relative likelihoods of tags for words is not known, nor is any information about whichtags are likely to appear in which contexts.
This would correspond to the knowledge thatcould be extracted from an on-line dictionary or through morphological nd distributionalanalysis.The unsupervised rule learning algorithm is based on the following simple idea.
Giventhe sentence:The can will be crushed.with no information beyond the dictionary entry for the word can, the best we can dois randomly guess between the possible tags for can in this context.
However, using anunannotated corpus and a dictionary, it could be discovered that of the words that appearafter The  in the corpus that have only one possible tag listed in the dictionary, nouns aremuch more common than verbs or modals.
From this the following rule could be learned:Change the tag of a word from (modal  OR noun OR verb)  to noun if the previousword is The.SThe transformation-based tagger is available through anonymous ftp to ftp.cs.jhu.edu in/pub/brill/Programs.Tin this paper we ignore the problem of unknown words: words appearing in the test set which did notappear in the training set.
We plan to explore ways of processing unknown words in future work, either byinitially assigning them all open-class tags, or devising an unsupervised version of the rule-based unknown.word tagger described in \[Brill, 1994\].4To fully define the learner, we must specify the three components of the learner: theinitial state annotator, the set of transformation templates, and the scoring criterion.In i t ia l  S ta te  Annotator  The unsupervised learner begins with an unannotated textcorpus, and a dictionary listing words and the allowable part of speech tags for each word.The tags are not listed in any particular order.
The initial state annotator tags each word inthe corpus with a list of all allowable tags.
Below is an example of the initial-state taggingof a sentence from the Penn Treebank \[Marcus et al, 1993\], where an underscore is to beread as or.
8Rival/ J J_NNP gangs/NNS have/VB_VBP turned/VBD_VBN cities/NNS into/IN combat/NN_VBzones/NNS ./.Transformation Templates The learner currently has four transformation templates.They are:Change the tag of a word from X to Y if:1.
The previous tag is T.2.
The previous word is W.3.
The next tag is T.4.
The next word is W.Transformations are used differently in the unsupervised learner than in the supervisedlearner.
Here, a transformation will reduce the uncertainty as to the correct tag of a word ina particular context, instead of changing one tag to another.
So all learned transformationswill have the form:Change the tag of a word from X to Y in context Cwhere X is a set of two or more part of speech tags, and Y is a single part of speech tag,such that Y E X.
Below we list some transformations that were actually learned by thesystem.Change the tag:From NN_VB_VBP to VBP if the previous tag is NNSFrom NN_VB to VB if the previous tag is MDFrom JJ_NNP to J J  if the following tag is NNS8JJ= Adjective, MD = Modal, NNP = Singular Proper Noun, NN = Singular or Mass Noun, POS =Possessive, VB = Verb, Base Form, VBD = Verb, Past Tense, VBN = Verb, Past Part., VBP = Verb,Non-3rd Person Sing.
Present.5Scoring Cr i ter ion When using supervised transformation-based learning to train a partof speech tagger, the scoring function is just the tagging accuracy that results from ap-plying a transformation.
With unsupervised learning, the learner does not have a goldstandard training corpus with which accuracy can be measured.
Instead, we can try to useinformation from the distribution of unambiguous words to find reliable disambiguatingcontexts.In each learning iteration, the score of a transformation is computed based on thecurrent agging of the training set.
Recall that this is completely unsupervised.
Initially,each word in the training set is tagged with all tags allowed for that word, as indicated inthe dictionary.
In later learning iterations, the training set is transformed as a result ofapplying previously learned transformations.
To score the transformation: Change the tagof a word from X to Y in context C, where Y E X, we do the following.
For each tag Z E X,Z ~ Y, computefreq(Y)/ freq( Z) ?
incontext( Z,C)where freq(Y) is the number of occurrences of words unambiguously tagged with tag Y inthe corpus, freq(Z) is the number of occurrences of words unambiguously tagged with tagZ in the corpus, and incontext(Z,C) is the number of times a word unambiguously taggedwith tag Z occurs in context C in the training corpus.
9Letn = argmaxz freq(Y)/freq(Z) * incontext(Z, C)Then the score for the transformation Change the tag of a word from X to Y in contextCis:incontext(Y, C) - freq(Y)/ freq( R) ?
incontext( R, C)A good transformation for removing the part of speech ambiguity of a word is one forwhich one of the possible tags appears much more frequently as measured by unambiguouslytagged words than all others in the context, after adjusting for the differences in relativefrequency between the different ags.
The objective function for this transformation mea-sures this by computing the difference between the number of unambiguous instances oftag Y in context C and the number of unambiguous instances of the most likely tag R incontext C, where R E X, R ~ Y, adjusting for relative frequency.
In each learning iteration,the learner searches for the transformation which maximizes this function.
Learning stopswhen no positive scoring transformations can be found.Unsupervised Learning: ResultsTo test the effectiveness of the above unsupervised learning algorithm, we ran a number ofexperiments using two different corpora and part of speech tag sets: the Penn TreebankWall Street Journal Corpus \[Marcus et al, 1993\] and the original Brown Corpus \[Francisand Kucera, 1982\].
First, a dictionary was created listing all possible tags for each word inthe corpus.
This means that the test set contains no unknown words.
We have set up theexperiments in this way to facilitate comparisons with results given in other papers, wherethe same was done.9An example of a context is: the previous tag is a determiner.8B8i i , ,4oo 6oo 80o loooTransformagon NumberFigure 2: Test Set Accuracy vs Transformation Number for the Penn Treebank Wall StreetJournal CorpusPenn Treebank Resu l tsIn this experiment, a training set of 120,000 words and a separate test set of 200,000 wordswere used.
We measure the accuracy of the tagger by comparing text tagged by the trainedtagger to the gold standard manually annotated corpus.
In the case where the tag of a wordis not fully disambiguated by the tagger, a single tag is randomly chosen from the possibletags, and this tag is then compared to the gold standard.
Initial state tagging accuracyon the training set is 90.7%.
After learning 1,151 transformations, training set accuracyincreases to 95.0%.
Initial state tagging accuracy on the test set is also 90.7%.
Accuracyincreases to 95.1% after applying the learned transformations.Figure 2 shows test set tagging accuracy as a function of transformation number.
Infigure 3, we plot the difference between training and test set accuracies after the apphcationof each transformation, i cluding a smoothed curve.
1?
Notice that there is no overtraining:the difference in accuracies on training and test set remain within a very narrow rangethroughout, with test set accuracy exceeding training set accuracy by a small margin.Overtraining did not occur when using the original Brown Corpus either.
When training astochastic tagger using the Baum-Welch algorithm, overtraining often does occur \[Meriaido,1995; Elworthy, 1994\], requiring an additional held-out raining corpus for determining anappropriate number of training iterations.1?The graphs are choppy because after each transformation s applied, correctness forwords not yet fullydisambiguated is judged after andomly selecting from the possible tags for that word.8o~9o.
*  %?
.
?
.
.?
, .
* ; ? "
?
, .2k  ?
?
?
.
?
?
.
?% ?
?
?g ?
?
??
* .
.
~ ?  "
.
?
?
?
?
?
~o ?
~ ?
?
oO ??
~ ?
?
?
?
.
oO $ ~  ?
?*~?
~,.~ ?~,?
l l k .
O .
.
?
?
?
.
2 ?
?
?
?
?
?~ * ?
?
?
?
?
?
?
?
?
?
% ??
?
?
?% ?e  ,~% .
?~ ?
?
?
?
?
?
??
.
?
?
?
~"  ~:  .
-o~ ?
, ?
o  '~  - ?
.
.~  -o .
-%o.?~.
- ?
~ ?o ?
?
o~ ?~Ooo ?
?% ??P?
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
_ ' .
- - .?
0".
: .
.
.
:, ~" .
.
.
.
.
.
.
.
.
o?
~i i i i0 6o0 800 1000Translorma~lon NumberFigure 3: Difference Between Training and Test Set Accuracies.Corpus Training Corpus Size (Words)Penn Treebank 120KBrown Corpus 120KBrown Corpus 350K% Correct95.195.696.0Table 1: Unsupervised Training: Test Set AccuracyBrown Corpus  Resu l tsIn this experiment, we also used a training set of 120,000 words and a separate test set of200,000 words.
Initial state tagging accuracy on the training set is 89.8%.
After learning1,729 transformations and applying them to the training set, accuracy increases to 95.6%.Initial state tagging accuracy on the test set is 89.9%, with accuracy increasing to 95.6%after applying the learned transformations.
Expanding the training set to 350,000 wordsand testing on the same test set, accuracy increases to 96.0%.
All unsupervised learningresults are summarized in table 1.Compar i son  Wi th  Other  Resu l tsIn \[Merialdo, 1995\], tagging experiments are described training a tagger using the Baum-Welch algorithm with a dictionary constructed as described above and an untagged corpus.Experiments were run on Associated Press articles which were manually tagged at theUniversity of Lancaster.
When training on one million words of text, test set accuracy8peaks at 86.6%.
In \[Elworthy, 1994\], similar experiments were run.
There, a peak accuracyof 92.0% was attained using the LOB corpus, n Using the Penn Treebank corpus, a peakaccuracy of 83.6% resulted.
These results are significantly lower than the results achievedusing unsupervised transformation-based learning.In \[Kupiec, 1992\] a novel twist to the Baum-Welch algorithm is presented, where insteadof having contextual probabilities for a tag following one or more previous tags, words arepooled into equivalence classes, where all words in an equivalence class have the same setof allowable part of speech assignments.
Using these equivalence classes greatly reducesthe number of parameters that need to be estimated.
Kupiec ran experiments using theoriginal Brown Corpus.
When training on 440,000 words, test set accuracy was 95.7%,excluding punctuation.
As shown above, test set accuracy using the transformation-basedalgorithm described in this paper gives an accuracy of 96.0% when trained on 350,000 words.Excluding punctuation, this accuracy is 95.6%.
Note that since the Baum-Welch algorithmfrequently overtrains, a tagged text would be necessary to figure out what training iterationgives peak performance.Weakly Supervised Rule LearningWe have explored a method of training a transformation-based tagger when no informationis known other than a list of possible tags for each word.
Next we explore weakly supervisedlearning, where a small amount of human intervention is permitted.
With Markov-modelbased taggers, there have been two different methods proposed for adding knowledge toa tagger trained using the Baum-Welch algorithm.
One method is to manually alter thetagging model, based on human error analysis.
This method is employed in \[Kupiec, 1992;Cutting et al, 1992\].
Another approach is to obtain the initial probabilities for the modeldirectly from a manually tagged corpus instead of using random or evenly distributed initialprobabilities, and then adjust these probabilities using the Baum-Welch algorithm and anuntagged corpus.
This approach is described in \[Merialdo, 1995; Elworthy, 1994\].A tagged corpus can also be used to improve the accuracy of unsupervised transformation-based learning.
A transformation-based ystem is a processor and not a classifier.
Beinga processor, it can be applied to the output of any initial state annotator.
As mentionedabove, in the supervised transformation-based tagger described in \[Brill, 1994\], each wordis initially tagged with its most likely tag.
Here, we use the trained unsupervised partof speech tagger as the initial state annotator for a supervised learner.
Transformationswill then be learned to fix errors made by the unsupervised learner.
As shown in figure4, unannotated text is first passed through the unsupervised initial-state annotator, whereeach word is assigned a list of all allowable tags.
The output of this tagger is then passedto the unsupervised learner, which learns an ordered list of transformations.
The initial-state annotator and learned unsupervised transformations are then applied to unannotatedtext, which is then input to the supervised learner, along with the corresponding manuallytagged corpus.
The supervised learner learns a second ordered list of transformations.Once the system is trained, fresh text is tagged by first passing it through the unsu-pervised initial state annotator, then applying each of the unsupervised transformations,in order, and then applying each of the supervised transformations, in order.The advantage of combining unsupervised and supervised learning over using supervisedn\[Elworthy, 1994\] quotes accuracy on ambiguous words, which we have converted to overall accuracy.I UNTAGGED \]TEXTINITL~L-STATE \[UNSUPERVISEDANNOTATOR: I / / TRANSFORMATIONSLEARNER % \[SUPERVISEDTEXTFigure 4: Combining Unsupervised and Supervised Learninglearning alone is that the combined approach allows us to utifize both tagged and untaggedtext in training.
Since manually tagged text is costly and time-consuming to generate, itis often the case that when there is a corpus of manually tagged text available there willalso be a much larger amount of untagged text available, a resource not utilized by purelysupervised training algorithms.One significant difference between this approach and that taken in using the Baum-Welch algorithm is that here the supervision influences the learner after unsupervisedtraining, whereas when using tagged text to bias the initial probabilities for Baum-Welchtraining, supervision influences the learner prior to unsupervised training.
The latter ap-proach has the potential weakness of unsupervised training erasing what was learned fromthe manually annotated corpus.
For example, in \[Merialdo, 1995\], extracting probabilityestimates from a 50,000 word manually tagged corpus gave a test set accuracy of 95.4%.After applying ten iterations of the Baum-Welch algorithm, accuracy dropped to 94.4%.Using the transformations learned in the above unsupervised training experiment runon the Penn Treebank, we apply these transformations to a separate training corpus.
Newsupervised transformations are then learned by comparing the tagged corpus that resultsfrom applying these transformations with the correct agging, as indicated in the manuallyannotated training corpus.In table 2, we show tagging accuracy on a separate test set using different sizes ofmanually annotated corpora.
In each case, a 120,000 word untagged corpus was used forinitial unsupervised training.
This table also gives results from supervised training usingthe annotated corpus, without any prior unsupervised training.
12 In all cases, the combinedtraining outperformed the purely supervised training at no added cost in terms of annotated12The purely supervised learning algorithm is the same as that described in \[Brill, 1994\], except there themost likely tag for every word in the dictionary isprovided to the learner.10% Correct % CorrectSupervised Training Using Unsupervised Not Using Unsup.Corpus Size (Words) Transformations Transformations0 95.1 90.8400 95.4 91.81200 95.5 92.94000 95.7 93.97600 95.8 94.610300 96.0 95.122300 96.3 95.544400 96.6 96.161400 96.7 96.388200 96.8 96.5Table 2: Unsupervised + Supervised vs. Purely Supervised Training.training text.ConclusionsIn this paper, we have presented a new algorithm for unsupervised training of a rule-basedpart of speech tagger.
The rule-based tagger trained using this algorithm significantlyoutperforms the traditional method of applying the Baum-Welch algorithm for unsupervisedtraining of a stochastic tagger, and achieves comparable performance toa class-based Baum-Welch training algorithm.
In addition, we have shown that by combining unsupervisedand supervised learning, we can obtain a tagger that significantly outperforms a taggertrained using purely supervised learning.
We are encouraged by these results, and expect animprovement in performance when the number of transformation templates provided to theunsupervised learner increases beyond the four currently used.
We have also demonstratedthat overtraining, a problem in Baum-Welch training, is not a problem in transformation-based learning.References\[Baum, 1972\] Baum, L. 1972.
An inequality and associated maximization technique instatistical estimation for probabilistic functions of a Markov process.
Inequalities 3:1-8.\[Black et al, 1992\] Black, E.; Jelinek, F.; Lafferty, J.; Mercer, R.; and Roukos, S. 1992.Decision tree models applied to the labeling of text with parts-of-speech.
In DarpaWorkshop 07, Speech and Natural Language.
Harriman, N.Y.\[Brill and Resnik, 1994\] Brill, E. and Resnik, P. 1994.
A transformation-based approachto prepositional phrase attachment disambiguation.
In Proceedings of the Fifteenth In-ternational Conference on Computational Linguistics (COLING-1994) , Kyoto, Japan.\]\]\[Brill, 1992\] Brill, E. 1992.
A simple rule-based part of speech tagger.
In Proceedings ofthe Third Conference on Applied Natural Language Processing, ACL, Trento, Italy.\[Brill, 1993a\] Brill, E. 1993a.
Automatic grammar induction and parsing free text: Atransformation-based approach.
In Proceedings of the 31st Meeting of the Association ofComputational Linguistics, Columbus, Oh.\[Brill, 1993b\] Brill, E. 1993b.
Transformation-based error-driven parsing.
In Proceedings ofthe Third International Workshop on Parsing Technologies, Tilburg, The Netherlands.\[Brill, 1994\] Brill, E. 1994.
Some advances in rule-based part of speech tagging.
In Pro-ceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), Seattle,Wa.\[Charniak et al, 1993\] Charniak, E.; Hendrickson, C.; Jacobson, N.; and Perkowitz, M.1993.
Equations for part of speech tagging.
In Proceedings of the Conference of theA merican Association for Artificial Intelligence (AAA 1-93).\[Church, 1988\] Church, K. 1988.
A stochastic parts program and noun phrase parser forunrestricted text.
In Proceedings of the Second Conference on Applied Natural LanguageProcessing, ACL.\[Cutting et al, 1992\] Cutting, D.; Kupiec, J.; Pedersen, J.; and Sibun, P. 1992.
A prac-tical part-of-speech tagger.
In Proceedings of the Third Conference on Applied NaturalLanguage Processing, A CL, Trento, Italy.\[DeMarcken, 1990\] DeMarcken, C. 1990.
Parsing the lob corpus.
In Proceedings of the 1990Conference of the Association for Computational Linguistics.\[Derose, 1988\] Derose, S. 1988.
Grammatical category disambiguation by statistical opti-mization.
Computational Linguistics 14.\[Elworthy, 1994\] Elworthy, D. 1994.
Does Baum-Welch re-estimation help taggers.
InProceedings of the Fourth Conference on Applied Natural Language Processing, A CL.\[Francis and Kucera, 1982\] Francis, W. and Kucera, H. 1982.
Frequency analysis of Englishusage: Lexicon and grammar.
Houghton Mifflin, Boston.\[Harris, 1962\] Harris, Z.
1962.
String Analysis of Language Structure.
Mouton and Co.,The Hague.\[Hindle, 1989\] Hindle, D. 1989.
Acquiring disambiguation rules from text.
In Proceedingsof the 27th Annual Meeting of the Association for Computational Linguistics.\[Huang et al,nunciationsternational1994\] Huang, C.; Son-Bell, M.; and Baggett, D. 1994.
Generation of pro-from orthographies u ing transformation-based error-driven learning.
In In-Conference on Speech and Language Processing (ICSLP), Yokohama, Japan.\[Jelinek, 1985\] Jelinek, F. 1985.
Self-Organized Language Modelling for Speech Recognition.Dordrecht.
In Impact of Processing Techniques on Communication, J. Skwirzinski, ed.12\[Klein and Simmons, 1963\] Klein, S. and Simmons, R. 1963.
A computational approach togrammatical coding of English words.
JACM 10.\[Kupiec, 1992\] Kupiec, J.
1992.
Robust part-of-speech tagging using a hidden Markovmodel.
Computer speech and language 6.\[Linet al, 1994\] Lin, Y.; Chiang, T.; and Su, K. 1994.
Automatic model refinement withan apphcation to tagging.
In Proceedings of the 15th International Conference on Com-putational Linguistics.\[Marcus et al, 1993\] Marcus, M.; Santorini, B.; and Marcinkiewicz, M. 1993.
Building alarge annotated corpus of English: the Penn Treebank.
Computational Linguistics 19(2).\[Merialdo, 1995\] Merialdo, B.
1995.
Tagging english text with a probabilistic model.
Com-putational Linguistics: To Appear.\[Ramshaw and Marcus, 1994\] Ramshaw, L. and Marcus, M. 1994.
Exploring the statis-tical derivation of transformational ru e sequences for part-of-speech tagging.
In TheBalancing Act: Proceedings of the A CL Workshop on Combining Symbolic and Statisti-cal Approaches to Language, New Mexico State University.\[Roche and Schabes, 1995\] Roche, E. and Schabes, Y.
1995.
Deterministic part of speechtagging with finite state transducers.
Computational Linguistics: To Appear.\[Schutze and Singer, 1994\] Schutze, H. and Singer, Y.
1994.
Part of speech tagging usinga variable memory Markov model.
In Proceedings of the Association for ComputationalLinguistics.\[Weischedel etal., 1993\] Weischedel, R.; Meteer, M.; Schwartz, R.; Ramshaw, L.; and Pal-mucci, J.
1993.
Coping with ambiguity and unknown words through probabilistic models.Computational Linguistics.13
