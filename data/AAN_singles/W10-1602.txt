Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,pages 8?14, Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsData-driven computational linguistics at FaMAF-UNC, ArgentinaLaura Alonso i Alemany and Gabriel Infante-LopezGrupo de Procesamiento de Lenguaje NaturalSeccio?n de Ciencias de la Computacio?nFacultad de Matema?tica, Astronom?
?a y F?
?sicaUniversidad Nacional de Co?rdobaCo?rdoba, Argentina{gabriel|alemany}@famaf.unc.edu.arAbstractThis paper provides a survey of some on-going research projects in computational lin-guistics within the group of Natural LanguageProcessing at the University of Co?rdoba, Ar-gentina.
We outline our future plans and spot-light some opportunities for collaboration.1 IntroductionIn this paper we present our group, describe itsmembers, research agenda, interests and possiblecollaboration opportunities.
The research agendaof the NLP group contains diverse lines of work.As a group, we have a special interest in produc-ing language technologies for our languages, at alevel comparable in performance with the state-of-the-art technology for English.
We are developingsuch technology by deeply understanding its under-ling models and either adapting them to our lan-guages or by creating new ones.In this paper we present only those related to Nat-ural Language Parsing and data-driven characterisa-tion of linguistic phenomena.
For both lines we pro-vide a small survey of our results so far, we describeour current research questions and we spotlight pos-sible opportunities of collaboration.The paper is organized as follows.
The follow-ing Section describes the group, its composition,projects and goals.
Section 3 briefly introduces theresearch agenda related to natural language pars-ing and structure finding.
Section 4 sketches thework on data-driven characterisation of linguisticphenomena in three main parts: semi-structured textmining, characterisation of verbal behaviour andmining of relations in biomedical text.
Finally, Sec-tion 5 presents outlines our overall vision for collab-oration with other researchers in the Americas.2 Description of the groupThe NLP Group1 is part of the Computer Sciencesection at the Facultad de Matema?tica, Astronom?
?ay F?
?sica, at the Universidad Nacional de Co?rdoba.The group was started in 2005, with two full time re-searchers who had just got their doctorate degree inAmsterdam and Barcelona.
Then, in 2009 and 2010three more full-time researchers joined the group,coming from the Universities of Geneva and Nancy.As of 2010, the group has 5 faculty researchers,4 PhD students and several undergraduate students.The computer science section has around 20 mem-bers ?
including the NLP group, faculty membersand PhD students.The faculty researchers are, by alphabetical order:?
Laura Alonso Alemany, working in text miningand data-driven systematization of language.?
Carlos Areces, investigating different reason-ing tasks and their applications in natural lan-guage processing.?
Luciana Benotti, investigates the addition ofpragmatic abilities into dialogue systems.?
Paula Estrella, working in Machine Transla-tion.?
Gabriel Infante-Lopez, working on NaturalLanguage Parsing and Structure Finding.1http://www.cs.famaf.unc.edu.ar/?pln/8One of the main aims of the group has been ed-ucation, both at undergraduate and graduate lev-els.
Computer Science is an under-developed areain Argentina, and Natural Language Processing evenmore so.
When the group was created, there werevery few NLP researchers in the country, and theyworked in isolation, with little connection to otherresearchers from neighbouring countries.
One ofthe strategic goals of our University and of the NLPgroup itself were to create a critical mass of re-searchers in NLP.
To that aim, we worked on in-corporating researchers to our group and establish-ing relations with other groups.
Researchers wereincorporated via special programmes from both theFaculty and the Argentinean Government to increasethe number of doctors in Computer Science in thescientific system in Argentina.Most of our efforts in the first years went to raiseawareness about the area and provide foundationaland advanced courses.
This policy lead to a signifi-cant number of graduation theses2 and to the incor-poration of various PhD students to our group.We taught several undergraduate and graduatecourses on various NLP topics at our own Univer-sity, at the University of R?
?o Cuarto, at the Univer-sity of Buenos Aires and at the Universidad de laRepu?blica (Uruguay), as well as crash courses at theSociety for Operative Investigations (SADIO) andat the Conferencia Latinoamericana de Informa?tica(CLEI 2008).
We also gave several talks at vari-ous universities in the country, and participated inlocal events, like JALIMI?05 (Jornadas Argentinasde Lingu??
?stica Informa?tica: Modelizacio?n e Inge-nier?
?a) or the Argentinean Symposium on ArtificialIntelligence.Since the beginning of its activities, the grouphas received funding for two major basic researchprojects, funded by the Argentinean Agency for theDevelopment of Science and Technology.
A thirdsuch project is pending approval.We have a special interest in establishing work-ing relations and strengthening the synergies withthe research community in NLP, both within SouthAmerica and the rest of the world.
We have had sci-entific and teaching exchanges with the NLP group2http://cs.famaf.unc.edu.ar/?pln/Investigacion/tesis_grado/tesis_grado.htmlin Montevideo, Uruguay.
From that collaboration,the Microbio project emerged3, bringing togetherresearchers on NLP from Chile, Brazil, Uruguay,France and Argentina.
This project was fundedby each country?s scientific institutions (MinCyT,in the case of Argentina) within STIC-AmSud4,a scientific-technological cooperation programmeaimed to promote and strengthen South America re-gional capacities and their cooperation with Francein the area of Information Technologies and Com-munication.
Within this project, we hosted the kick-off workshop on February 2008, with attendants rep-resenting all groups in the project.We have also had billateral international cooper-ation in some smaller projects.
Together with theCNR-INRIA in Rennes, France, we have worked ina project concerning the smallest grammar problem.We tackle the same problem, finding small gram-mars in two different domains: ADN sequencesand Natural Language sentences.
In collaborationwith several universities in Spain (UB, UOC, UPC,EHU/UPV), we have taken part in the major basicresearch programme KNOW5, aiming to aggregatemeaning, knowledge and reasoning to current infor-mation technologies.
This project has now receivedfunding to carry on a continuating project6.Moreover, we are putting forward some propos-als for further international collaboration.
Follow-ing the path opened by the Microbio project, weare working on a proposal to the Ecos Sud pro-gramme for joint collaboration with research teamsin France7.We are also working in strengthening relationswithin Argentinean NLP groups.
To that aim, we arecollaborating with the NLP group at the Universityof Buenos Aires in the organization of the Schoolon Computational Linguistics ELiC8, with severalgrants for students sponsored by NAACL.
We arealso putting forward a proposal for a workshop on3http://www.microbioamsud.net/4http://www.sticamsud.org/5KNOW project: http://ixa.si.ehu.es/know.6Representation of Semantic Knowledge, TIN2009-14715-C04-03 (Plan Nacional de I+D+i 2008-2011).7ECOS-SUD programme: http://www.mincyt.gov.ar/coopinter_archivos/bilateral/francia.htm.8ELiC school on Computational Linguistics: http://www.glyc.dc.uba.ar/elic2010/.9NLP to be co-located with the IBERAMIA confer-ence on Artificial Intelligence, to be held at Bah?
?aBlanca on November 2010.3 Natural Language Parsing andStructure Finding3.1 Unsupervised ParsingUnsupervised parsing of Natural Language Syntaxis a key technology for the development of lan-guage technology.
It is specially important for lan-guages that have either small treebanks or none atall.
Clearly, there is a big difference between pro-ducing or using a treebank for evaluation and pro-ducing or using them for training.
In the formercase, the size of the treebank can be significantlysmaller.
In our group, we have investigated differ-ent approaches to unsupervised learning of naturallanguage.
and we are currently following two dif-ferent lines, one that aims at characterizing the po-tential of a grammar formalism to learn a given tree-bank structure and a second that uses only regularautomata to learn syntax.Characterization of Structures In (Luque andInfante-Lopez, 2009) we present a rather unusualresult for language learning.
We show an upperbound for the performance of a class of languageswhen a grammar from that class is used to parsethe sentences in any given treebank.
The class oflanguages we studied is the defined by Unambigu-ous Non-Terminally Separated (UNTS) grammars(Clark, 2006).
UNTS grammars are interesting be-cause, first, they have nice learnability propertieslike PAC learnability (Clark, 2006), and, second,they are used as the background formalism that wonthe Omphalos competition (Clark, 2007).
Our strat-egy consists on characterizing all possible ways ofparsing all the sentences in a treebank using UNTSgrammars, then, we find the one that is closest to thetreebank.
We show that, in contrast to the results ob-tained for learning formal languages, UNTS are notcapable of producing structures that score as state-of-the-art models on the treebanks we experimentedwith.Our results are for a particular, very specific typeof grammar.
We are currently exploring how towiden our technique to provide upper bounds to amore general class of languages.
Our technique doesnot state how to actually produce a grammar thatperforms as well as the upper bound, but it can beuseful for determining how to transform the trainingmaterial to make upper bounds go up.
In particu-lar we have defined a generalization of UNTS gram-mars, called k-l-UNTS grammars, that transform aword w in the training material in a 3-uple ?
?,w, ?
?where ?
contains the k previous symbols to w and?
contains the l symbols following w. Intuitively, k-l-UNTS augments each word with a variable lengthcontext.
It turns out that the resulting class of lan-guages is more general than UNTS grammars: theyare PAC learnable, they can be learned with the samelearning algorithm as UNTS and, moreover, theirupper bound for performance is much higher thanfor UNTS.
Still, it might be the case that the exist-ing algorithm for finding UNTS is not the right onefor learning the structure of a treebank, it might bethe case that strings in the PTB have not been pro-duced by a k-l-UNTS grammar.
We are currentlyinvestigating how to produce an algorithm that fitsbetter the structure given in a treebank.Learning Structure Using Probabilistic Au-tomata DMV+CCM (Klein and Manning, 2004;Klein and Manning, 2002) is a probabilistic modelfor unsupervised parsing, that can be successfullytrained with the EM algorithm to achieve state ofthe art performance.
It is the combination of theConstituent-Context Model, that models unlabeledconstituent parsing, and the DependencyModel withValence, that models projective dependency parsing.On the other hand, CCM encodes the probability thata given string of POS tags is a constituent.
DMV ismore of our interest in this work, because it encodesa top-down generative process where the heads gen-erate their dependents to both directions until thereis a decision to stop, in a way that resembles suc-cessful supervised dependency models such as in(Collins, 1999).
The generation of dependents ofa head on a specific direction can be seen as an im-plicit probabilistic regular language generated by aprobabilistic deterministic finite automaton.Under this perspective, the DMV model is in factan algorithm for learning several automata at thesame time.
All automata have in common that theyhave the same number of states and the same num-ber of arcs between states, which is given by the def-10inition of the DMV model.
Automata differ in thatthey have different probabilities assigned to the tran-sitions.
The simple observation that DMV actuallysuppose a fixed structure for the automata it inducesmight explain its poor performance with freer orderlanguages like Spanish.
Using our own implementa-tion (see (Luque, 2009)) we have empirically testedthat DMV+CMVworks well in languages with strictword order, like English, but for other languageswith freer word order, like Spanish, DMV+CMVperformance decreases dramatically.
In order toimprove DMV+CCM performance for this type oflanguages, the structure of the automaton might bemodified, but since the DMV model has an ad hoclearning algorithm, a new parametric learning algo-rithm has to be defined.
We are currently investigat-ing different automaton structures for different lan-guages and we are also investigating not only theinduction of the parameters for fixed structure, butalso inducing the structure of the automata itself.3.2 Smallest Grammar and Compression forNatural LanguageThe smallest grammar problem has been widelystudied in the literature.
The aim of the problem isto find the smallest (smallest in the sense of numberof symbols that occur in the grammar) context freegrammar that produces only one given string.
Thesmallest grammar can be thought as a relaxation ofthe definition of Kolmogorov Complexity where thecomplexity is given by a context free grammar in-stead of a Turing machine.
It is believed that thesmallest grammar can be used both for computingoptimal compression codes and for finding meaning-ful patterns in strings.Moreover, since the procedure for finding thesmallest grammar is in fact a procedure that assignsa tree structure to a string, the smallest grammarproblem is, in fact, a particular case of unsupervisedparsing that has a very particular objective functionto be optimized.Since the search space is exponentially big, allexisting algorithms are in fact heuristics that lookfor a small grammar.
In (Carrascosa et al, 2010)we presented two algorithms that outperform all ex-isting heuristics.
We have produce and algorithmthat produces 10% smaller grammars for natural lan-guage strings and 1.5% smaller grammars for DNAsequences.Even more, we show evidence that it is possi-ble to find grammars that share approximately thesame small score but that have very little structurein common.
Moreover, the structure that is foundby the smallest grammar algorithm for the sentencesin PTB have little in common with the structure thatthe PTB defines for those sentences.Currently, we are trying to find answers to two dif-ferent questions.
First, is there a small piece of struc-ture that is common to all grammars having compa-rable sizes?
and second, can the grammars that arefound by our algorithms be used for improving com-pression algorithms?4 Data-driven characterisation oflinguistic phenomena4.1 Semi-structured text miningOne of our lines of research is to apply standard textmining techniques to unstructured text, mostly usergenerated content like that found in blogs, social net-works, short messaging services or advertisements.Our main corpus of study is constituted by classi-fied advertisements from a local newspaper, but oneof our lines of work within this project is to assessthe portability of methods and techniques to differ-ent genres.The goals we pursue are:creating corpora and related resources, and mak-ing them publicly available.
A corpus of news-paper advertisements and a corpus of short textmessages are underway.normalization of text bringing ortographic vari-ants of a word (mostly abbreviations) to acanonical form.
To do that, we apply machinelearning techniques to learn the parameters foredit distances, as in (Go?mez-Ballester et al,1997; Ristad and Yanilos, 1998; Bilenko andMooney, 2003; McCallum et al, 2005; Oncinaand Sebban, 2006).
We build upon previouswork on normalization by (Choudhury et al,2007; Okazaki et al, 2008; Cook and Steven-son, 2009; Stevenson et al, 2009).
Prelimi-nary results show a significant improvement oflearned distances over standard distances.11syntactic analysis applying a robust shallow pars-ing approach aimed to identify entities and theirmodifiers.ontology induction from very restricted domains,to aid generalization in the step of informationextraction.
We will be following the approachpresented in (Michelson and Knoblock, 2009).information extraction inducing templates fromcorpus using unsupervised and semi-supervised techniques, and using inducedtemplates to extract information to populatea relational database, as in (Michelson andKnoblock, 2006).data mining applying traditional knowledge dis-covery techniques on a relational database pop-ulated by the information extraction techniquesused in the previous item.This line of research has been funded for threeyears (2009-2012) by the Argentinean Ministry forScience and Technology, within the PAE project, asa PICT project (PAE-PICT-2007-02290).This project opens many opportunities for collab-oration.
The resulting corpora will be of use for lin-guistic studies.
The results of learning edit distancesto find abbreviations can also be used by linguists asan input to study the regularities found in this kindof genres, as proposed in (Alonso Alemany, 2010).We think that some joint work on learning stringedit distances would be very well integrated withinthis project.
We are also very interested in collabo-rations with researchers who have some experiencein NLP in similar genres, like short text messages orabbreviations in medical papers.Finally, interactions with data mining communi-ties, both academic and industrial, would surely bevery enriching for this project.4.2 Characterisation of verbal behaviourOne of our research interests is the empirical charac-terization of the subcategorization of lexical items,with a special interest on verbs.
This line of workhas been pursued mainly within the KNOW project,in collaboration with the UB-GRIAL group9.Besides the theoretical interest of describing thebehaviour of verbs based on corpus evidence, this9http://grial.uab.es/line has an applied aim, namely, enriching syntac-tic analyzers with subcategorization information, tohelp resolving structural ambiguities by using lexi-cal information.
We have focused on the behaviourof Spanish verbs, and implemented some of our find-ings as a lexicalized enhancement of the dependencygrammars used by Freeling10.
An evaluation of theimpact of this information on parsing accuracy is un-derway.We have applied clustering techniques to obtaina corpus-based characterization of the subcatego-rization behaviour of verbs (Alonso Alemany et al,2007; Castello?n et al, 2007).
We explored the be-haviour of the 250 most frequent verbs of Spanishon the SenSem corpus (Castello?n et al, 2006), man-ually annotated with the analysis of verbs at variouslinguistic levels (sense, aspect, voice, type of con-struction, arguments, role, function, etc.).
Apply-ing clustering techniques to the instances of verbs inthese corpus, we obtained coarse-grained classes ofverbs with the same subcategorization.
A classifierwas learned from considering clustered instances asclasses.
With this classifier, verbs in unseen sen-tences were assigned a subcategorization behaviour.Also with the aim of associating subcategoriza-tion information to verbs using evidence foundin corpora, we developed IRASubcat (Altamirano,2009).
IRASubcat11.
is a highly flexible system de-signed to gather information about the behaviour ofverbs from corpora annotated at any level, and inany language.
It identifies patterns of linguistic con-stituents that co-occur with verbs, detects optionalconstituents and performs hypothesis testing of theco-occurrence of verbs and patterns.We have also been working on connecting pred-icates in FrameNet and SenSem, using WordNetsynsets as an interlingua (Alonso Alemany et al,SEPLN).
We have found many dissimilarities be-tween FrameNet and SenSem, but have been ableto connect some of their predicates and enrich theseresources with information from each other.We are currently investigating the impact of dif-ferent kinds of information on the resolution of pp-attachment ambiguities in Spanish, using the AN-CORA corpus (Taule?
et al, 2006).
We are exploring10http://www.lsi.upc.edu/?nlp/freeling/11http://www.irasubcat.com.ar/12the utility of various WordNet-related information,like features extracted from the Top Concept Ontol-ogy, in combination with corpus-based information,like frequencies of occurrence and co-occurrence ofwords in corpus.The line of research of characterisation of verbalbehaviour presents many points for collaboration.In collaboration with linguists, the tools and meth-ods that we have explained here provide valuable in-formation for the description and systematization ofsubcategorization of verbs and other lexical pieces.It would be very interesting to see whether thesetechniques, that have been successfully applied toSpanish, apply to other languages or with differentresources.
We are also interested in bringing to-gether information from different resources or fromdifferent sources (corpora, dictionaries, task-specificlexica, etc.
), in order to achieve richer resources.We also have an interest for the study of hypothe-sis testing as applied to corpus-based computationallinguistics, to get some insight on the informationthat these techniques may provide to guide researchand validate results.4.3 Discovering relations between entititesAs a result of the Microbio project, we have devel-oped a module to detect relations between entitiesin biomedical text (Bruno, 2009).
This module hasbeen trained with the GENIA corpus (Kim et al,2008), obtaining good results (Alonso Alemany andBruno, 2009).
We have also explored different waysto overcome the data sparseness problem caused bythe small amount of manually annotated examplesthat are available in the GENIA corpus.
We haveused the corpus as the initial seed of a bootstrappingprocedure, generalized classes of relations via theGENIA ontology and generalized classes via clus-tering.
Of these three procedures, only generaliza-tion via an ontology produced good results.
How-ever, we have hopes that a more insightful charac-terization of the examples and smarter learning tech-niques (semi-supervised, active learning) will im-prove the results for these other lines.Since this area of NLP has ambitious goals, op-portunities for collaboration are very diverse.
Ingeneral, we would like to join efforts with other re-searchers to solve part of these complex problems,with a special focus in relations between entities andsemi-supervised techniques.5 Opportunities for CollaborationWe are looking for opportunities of collaborationwith other groups in the Americas, producing a syn-ergy between groups.
We believe that we can artic-ulate collaboration by identifying common interestsand writing joint proposals.
In Argentina there aresome agreements for billateral or multi-lateral col-laboration with other countries or specific institu-tions of research, which may provide a frameworkfor starting collaborations.We are looking for collaborations that promotethe exchange of members of the group, speciallygraduate students.
Our aim is to gain a level of col-laboration strong enough that would consider, forexample, co-supervision of PhD students.
Ideally,co-supervised students would spend half of theirtime in each group, tackle a problem that is commonfor both groups and work together with two super-visors.
The standard PhD scholarship in Argentina,provided by Conicet, allows such modality of doc-torate studies, as long as financial support for travelsand stays abroad is provided by the co-supervisingprogramme.
We believe that this kind of collabora-tion is one that builds very stable relations betweengroups, helps students learn different research id-iosyncrasies and devotes specific resources to main-tain the collaboration.ReferencesLaura Alonso Alemany and Santiago E. Bruno.
2009.Learning to learn biological relations from a smalltraining set.
In CiCLing, pages 418?429.Laura Alonso Alemany, Irene Castello?n, and NevenaTinkova Tincheva.
2007.
Obtaining coarse-grainedclasses of subcategorization patterns for spanish.
InRANLP?07.Laura Alonso Alemany, Irene Castello?n, Egoitz Laparra,and German Rigau.
SEPLN.
Evaluacio?n de me?todossemi-automa?ticos para la conexio?n entre FrameNet ySenSem.
In 2009.Laura Alonso Alemany.
2010.
Learning parametersfor an edit distance can learn us tendencies in user-generated content.
Invited talk at NLP in the So-cial Sciences, Instituto de Altos Estudios en Psicolo-gia y Ciencias Sociales, Buenos Aires, Argentina, May2010.13I.
Romina Altamirano.
2009.
Irasubcat: Un sistemapara adquisicio?n automa?tica de marcos de subcatego-rizacio?n de piezas le?xicas a partir de corpus.
Master?sthesis, Facultad de Matema?tica, Astronom?
?a y F?
?sica,Universidad Nacional de Co?rdoba, Argentina.Mikhail Bilenko and Raymond J. Mooney.
2003.
Adap-tive duplicate detection using learnable string simi-larity measures.
In Proceedings of the ninth ACMSIGKDD.Santiago E. Bruno.
2009.
Deteccio?n de relaciones entreentidades en textos de biomedicina.
Master?s thesis,Facultad de Matema?tica, Astronom?
?a y F?
?sica, Univer-sidad Nacional de Co?rdoba, Argentina.Rafael Carrascosa, Franc?ois Coste, Matthias Galle?, andGabriel Infante-Lopez.
2010.
Choosing Word Occur-rences for the Smallest Grammar Problem.
In Pro-ceedings of LATA 2010.
Springer.Irene Castello?n, Ana Ferna?ndez-Montraveta, Glo`riaVa?zquez, Laura Alonso, and Joanan Capilla.
2006.The SENSEM corpus: a corpus annotated at the syntac-tic and semantic level.
In 5th International Conferenceon Language Resources and Evaluation (LREC 2006).Irene Castello?n, Laura Alonso Alemany, and Nevena Tin-kova Tincheva.
2007.
A procedure to automaticallyenrich verbal lexica with subcategorization frames.
InProceedings of the Argentine Simposium on ArtificialIntelligence, ASAI?07.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structureof texting language.
Int.
J. Doc.
Anal.
Recognit.,10(3):157?174.Alexander Clark.
2006.
Pac-learning unambiguous ntslanguages.
In International Colloquium on Grammat-ical Inference, pages 59?71.Alexander Clark.
2007.
Learning deterministic contextfree grammars: the omphalos competition.
MachineLearning, 66(1):93?110.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania, PA.Paul Cook and Suzanne Stevenson.
2009.
An unsuper-vised model for text message normalization.
In Work-shop on Computational Approaches to Linguistic Cre-ativity.
NAACL HLT 2009.E.
Go?mez-Ballester, M. L. Mico?-Andre?s, J. Oncina,and M. L. Forcada-Zubizarreta.
1997.
An empir-ical method to improve edit-distance parameters fora nearest-neighbor-based classification task.
In VIISpanish Symposium on Pattern Recognition and ImageAnalysis, Barcelona, Spain.Jin D. Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008.Corpus annotation for mining biomedical events fromliterature.
BMC Bioinformatics, 9(1).Dan Klein and Christopher D. Manning.
2002.
A gener-ative constituent-context model for improved grammarinduction.
In ACL, pages 128?135.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proc.
of ACL 42.Franco Luque and Gabriel Infante-Lopez.
2009.
Upperbounds for unsupervised parsing with unambiguousnon-terminally.
In International Workshop Compu-tational Linguistic Aspects of Grammatical Inference.EACL, Greece.Franco M. Luque.
2009.
Implementation of theDMV+CCM parser.
http://www.cs.famaf.unc.edu.ar/?francolq/en/proyectos/dmvccm.AndrewMcCallum, Kedar Bellare, and Fernando Pereira.2005.
A conditional random field for discriminatively-trained finite-state string edit distance.
In Proceedingsof the Proceedings of the Twenty-First Conference An-nual Conference on Uncertainty in Artificial Intelli-gence (UAI-05), pages 388?395, Arlington, Virginia.AUAI Press.Matthew Michelson and Craig A. Knoblock.
2006.Phoebus: a system for extracting and integrating datafrom unstructured and ungrammatical sources.
InAAAI?06: proceedings of the 21st national conferenceon Artificial intelligence, pages 1947?1948.
AAAIPress.Matthew Michelson and Craig A. Knoblock.
2009.
Ex-ploiting background knowledge to build reference setsfor information extraction.
In Proceedings of the 21stInternational Joint Conference on Artific ial Intelli-gence (IJCAI-2009), Pasadena, CA.Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsujii.2008.
A discriminative alignment model for abbrevia-tion recognition.
In COLING ?08: Proceedings of the22nd International Conference on Computational Lin-guistics, pages 657?664, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Jose?
Oncina and Marc Sebban.
2006.
Learning stochas-tic edit distance: Application in handwritten characterrecognition.
Pattern Recognition, 39(9):1575?1587.E.
S. Ristad and P. N. Yanilos.
1998.
Learning string editdistance.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 20:522?532.Mark Stevenson, Yikun Guo, Abdulaziz Al Amri, andRobert Gaizauskas.
2009.
Disambiguation of biomed-ical abbreviations.
In BioNLP ?09: Proceedings of theWorkshop on BioNLP, pages 71?79, Morristown, NJ,USA.
Association for Computational Linguistics.M.
Taule?, M.A.
Mart?
?, and M. Recasens.
2006.
Ancora:Multilevel annotated corpora for catalan and spanish.In LREC?06.14
