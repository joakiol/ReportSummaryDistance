Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 249?255,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsAnnotating Relation Inference in Context via Question AnsweringOmer Levy Ido DaganComputer Science DepartmentBar-Ilan UniversityRamat-Gan, Israel{omerlevy,dagan}@cs.biu.ac.ilAbstractWe present a new annotation method forcollecting data on relation inference incontext.
We convert the inference task toone of simple factoid question answering,allowing us to easily scale up to 16,000high-quality examples.
Our method cor-rects a major bias in previous evaluations,making our dataset much more realistic.1 IntroductionRecognizing entailment between natural-languagerelations (predicates) is a key challenge in manysemantic tasks.
For instance, in question answer-ing (QA), it is often necessary to ?bridge the lex-ical chasm?
between the asker?s choice of wordsand those that appear in the answer text.
Relationinference can be notoriously difficult to automat-ically recognize because of semantic phenomenasuch as polysemy and metaphor:Q: Which drug treats headaches?A: Aspirin eliminates headaches.In this context, ?eliminates?
implies ?treats?
andthe answer is indeed ?aspirin?.
However, this ruledoes not always hold for other cases ?
?eliminatespatients?
has a very different meaning from ?treatspatients?.
Hence, context-sensitive methods arerequired to solve relation inference.Many methods have tried to address relationinference, from DIRT (Lin and Pantel, 2001)through Sherlock (Schoenmackers et al, 2010) tothe more recent work on PPDB (Pavlick et al,2015b) and RELLY (Grycner et al, 2015).
How-ever, the way these methods are evaluated remainslargely inconsistent.
Some papers that deal withphrasal inference in general (Beltagy et al, 2013;Pavlick et al, 2015a; Kruszewski et al, 2015) usean extrinsic task, such as a recent recognizing tex-tual entailment (RTE) benchmark (Marelli et al,2014).
By nature, extrinsic tasks incorporate a va-riety of linguistic phenomena, making it harder toanalyze the specific issues of relation inference.The vast majority of papers that do focus on re-lation inference perform some form of post-hocevaluation (Lin and Pantel, 2001; Szpektor et al,2007; Schoenmackers et al, 2010; Weisman etal., 2012; Lewis and Steedman, 2013; Riedel etal., 2013; Rockt?aschel et al, 2015; Grycner andWeikum, 2014; Grycner et al, 2015; Pavlick et al,2015b).
Typically, the proposed algorithm gen-erates several inference rules between two rela-tion templates, which are then evaluated manu-ally.
Some studies evaluate the rules out of con-text (is the rule ?X eliminates Y ??
?X treats Y ?true?
), while others apply them to textual data andevaluate the validity of the rule in context (given?aspirin eliminates headaches?, is ?aspirin treatsheadaches?
true?).
Not only are these post-hocevaluations oblivious to recall, their ?human in theloop?
approach makes them expensive and virtu-ally impossible to accurately replicate.Hence, there is a real need for pre-annotateddatasets for intrinsic evaluation of relation infer-ence in context.
Zeichner et al (2012) constructedsuch a dataset by applying DIRT-trained inferencerules to sampled texts, and then crowd-annotatingwhether each original text (premise) entails thetext generated from applying the inference rule(hypothesis).
However, this process is biased; byusing DIRT to generate examples, the dataset isinherently blind to the many cases where relationinference exists, but is not captured by DIRT.We present a new dataset for evaluating rela-tion inference in context, which is unbiased to-wards one method or another, and natural to anno-tate.
To create this dataset, we design a QA settingwhere annotators are presented with a single ques-249Figure 1: A screenshot from our annotation task.tion and several automatically-retrieved text frag-ments.
The annotators?
goal is to mark which ofthe text fragments provide a potential answer tothe question (see Figure 1).
Since the entities inthe text fragments are aligned with those in thequestion, this process implicitly annotates whichrelations entail the one in the question.
For exam-ple, in Figure 1, if ?
[US PRESIDENT] increasedtaxes?
provides an answer to ?Which US presidentraised taxes?
?, then ?increased?
implies ?raised?in that context.
Because this task is so easy to an-notate, we were able to scale up to 16,371 anno-tated examples (3,147 positive) with 91.3% preci-sion for only $375 via crowdsourcing.Finally, we evaluate a collection of existingmethods and common practices on our dataset,and observe that even the best combination ofmethods cannot recall more than 25% of the pos-itive examples without dipping below 80% preci-sion.
This places into perspective the huge amountof relevant cases of relation inference inherentlyignored by the bias in (Zeichner et al, 2012).Moreover, this result shows that while our anno-tation task is easy for humans, it is difficult forexisting algorithms, making it an appealing chal-lenge for future research on relation inference.Our code1and data2are publicly available.2 Relation Inference DatasetsTo the best of our knowledge, there are onlythree pre-annotated datasets for evaluating rela-tion inference in context.3Each example inthese datasets consists of two binary relations,premise and hypothesis, and a label indicat-1http://bitbucket.org/omerlevy/relation_inference_via_qa2http://u.cs.biu.ac.il/?nlp/resources/downloads/relation_inference_via_qa3It is worth noting the lexical substitution datasets (Mc-Carthy and Navigli, 2007; Biemann, 2013; Kremer et al,2014) also capture instances of relation inference.
However,they do not focus on relations and are limited to single-wordsubstitutions.
Furthermore, the annotators are tasked withgenerating substitutions, whereas we are interested in judg-ing (classifying) an existing substitution.ing whether the hypothesis is inferred from thepremise.
These relations are essentially Open IE(Banko et al, 2007) assertions, and can be repre-sented as (subject, relation, object) tuples.Berant et al (2011) annotated inferencebetween typed relations (?
[DRUG] eliminates[SYMPTOM]???
[DRUG] treats [SYMP-TOM]?
), restricting the definition of ?context?.They also used the non-standard type-systemfrom (Schoenmackers et al, 2010), which limitsthe dataset?s applicability to other corpora.
Levyet al (2014) annotated inference between in-stantiated relations sharing at least one argument(?aspirin eliminates headaches??
?drugs treatheadaches?).
While this format captures a morenatural notion of context, it also conflates thetask of relation inference with that of entityinference (?aspirin???drug?).
Both datasets wereannotated by experts.Zeichner et al (2012) annotated inference be-tween instantiated relations sharing both argu-ments:aspirin eliminates headaches?
aspirin treats headachesaspirin eliminates headaches9 aspirin murders headachesThis format provides a broad definition of contexton one hand, while isolating the task of relationinference.
In addition, methods that can be evalu-ated on this type of data, can also be directly em-bedded into downstream applications, motivatingsubsequent work to use it as a benchmark (Mela-mud et al, 2013; Abend et al, 2014; Lewis, 2014).We therefore create our own dataset in this format.The main drawback of Zeichner et al?s processis that it is biased towards a specific relation infer-ence method, DIRT (Lin and Pantel, 2001).
Essen-tially, Zeichner et al conducted a post-hoc eval-uation of DIRT and recorded the results.
Whiletheir approach does not suffer from the major dis-advantages of post-hoc evaluation ?
cost and ir-replicability ?
it ignores instances that do not be-have according to DIRT?s assumptions.
These in-visible examples amount to an enormous chunkof the inference performed when answering ques-tions, which are covered by our approach (see ?4).3 Collection & Annotation ProcessOur data collection and annotation process is de-signed to achieve two goals: (1) to efficiently sam-ple premise-hypothesis pairs in an unbiased man-250ner; (2) to allow for cheap, consistent, and scalableannotations based on an intuitive QA setting.3.1 Methodology OverviewWe start by collecting factoid questions.Each question is captured as a tupleq = (qtype, qrel, qarg), for example:Whichqtypefoodqrelis included inqargchocolate ?In addition to ?Which??
questions, this templatecaptures other WH-questions such as ?Who??
(qtype= person).We then collect a set of candidate answersfor each question q.
A candidate answer isalso represented as a tuple (aanswer, arel, aarg) or(aarg, arel, aanswer), for example:aargchocolatearelis made fromaanswerthe cocoa beanWe collect answer candidates according to thefollowing criteria:1. aarg= qarg2.
aansweris a type of qtype3.
arel6= qrelThese criteria isolate the task of relation inferencefrom additional inference tasks, because they en-sure that a?s arguments are entailing q?s.
In addi-tion, the first two criteria ensure that enough can-didate answers actually answer the question, whilethe third discards trivial cases.
In contrast to (Ze-ichner et al, 2012) and post-hoc evaluations, thesecriteria do not impose any bias on the relation pairarel, qrel.
Furthermore, we show in ?3.2 that botha and q are both independent naturally-occurringtexts, and are not machine-generated by applyinga specific set of inference rules.For each (a, q) pair, Mechanical Turk annota-tors are asked whether a provides an answer to q.This natural approach also enables batch annota-tion; for each question, several candidate answerscan be presented at once without shifting the anno-tator?s focus.
To make sure that the annotators donot use their world knowledge about aanswer, wemask it during the annotation phase and replace itwith qtype(see Figure 1 and ?3.3).Finally, we instantiate qtypewith aanswer, sothat each (a, q) pair fits Zeichner?s format: instan-tiated predicates sharing both arguments.3.2 Data CollectionWe automatically collected 30,703 pairs of ques-tions and candidate answers for annotation.
Ourprocess is largely inspired by (Fader et al, 2014).Questions We collected 573 questions by manu-ally converting questions from TREC (Voorheesand Tice, 2000), WikiAnswers (Fader et al, 2013),WebQuestions (Berant et al, 2013), to our ?Whichqtypeqrelqarg??
format.
Though many questionsdid fit our format, a large portion of them wereabout sports and celebrities, which were not appli-cable to our choice of corpus (Google books) andtaxonomy (WordNet).4Corpus QA requires some body of knowledgefrom which to retrieve candidate answers.
Wefollow Fader et al (2013; 2014), and use a col-lection of Open IE-style assertions (Banko et al,2007) as our knowledge base.
Specifically, weused hand-crafted syntactic rules5to extract over63 million unique subject-relation-object tripletsfrom Google?s Syntactic N-grams (Goldberg andOrwant, 2013).
The assertions may include multi-word phrases as relations or arguments, as illus-trated earlier.
This process yields some ungram-matical or out-of-context assertions, which arelater filtered during annotation (see ?3.3).Answer Candidates In ?3.1 we defined three cri-teria for matching an answer candidate to a ques-tion, which we now translate into a retrieval pro-cess.
We begin by retrieving all assertions whereone of the arguments (subject or object) is equalto qarg, ignoring stopwords and inflections.
Thematching argument is named aarg, while the other(non-matching) argument becomes aanswer.To implement the second criterion (aanswerisa type of qtype) we require a taxonomy T , aswell as a word-sense disambiguation (WSD) al-gorithm to match natural-language terms to enti-ties in T .
In this work, we employ WordNet?s hy-pernymy graph (Fellbaum, 1998) as T and Lesk(Lesk, 1986) for WSD (both via NLTK (Bird et al,2009)).
While automatic WSD is prone to someerrors, these cases are usually annotated as non-sensical in the final phase.Lastly, we remove instances where arel= qrel.64This is the only part in our process that might introducesome bias.
However, this bias is independent of existing re-lation inference methods such as DIRT.5See supplementary material for a detailed description.6Several additional filters were applied to prune non-grammatical assertions (see supplementary material).2513.3 Crowdsourced AnnotationMasking Answers We noticed that exposingaanswerto the annotator may skew the annota-tion; rather than annotating whether arelimpliesqrelin the given context, the annotator might an-notate whether aansweranswers q according to hergeneral knowledge.
For example:Q: Which country borders Ethiopia?A: Eritrea invaded Ethiopia.An annotator might be misled by knowing in ad-vance that Eritrea borders Ethiopia.
Although aninvasion typically requires land access, it does notimply a shared border, even in this context; ?Italyinvaded Ethiopia?
also appears in our corpus, butit is not true that ?Italy borders Ethiopia?.Effectively, what the annotator might be doingin this case is substituting qtype(?country?)
withaanswer(?Eritrea?)
and asking herself if the as-sertion (aanswer, qrel, qarg) is true (?Does Eritreaborder Ethiopia??).
As demonstrated, this ques-tion may have a different answer from the infer-ence question in which we are interested (?If acountry invaded Ethiopia, does that country bor-der Ethiopia??).
We therefore mask aanswerdur-ing annotation by replacing it with qtypeas a place-holder:A: [COUNTRY] invaded Ethiopia.This forces the annotator to ask herself whetherarelimplies qrelin this context, i.e.
does invadingEthiopia imply sharing a border with it?Labels Each annotator was given a single ques-tion with several matching candidate answers (20on average), and asked to mark each candidate an-swer with one of three labels:3 The sentence answers the question.7 The sentence does not answer the question.?
The sentence does not make sense,or is severely non-grammatical.Figure 1 shows several annotated examples.
Thethird annotation (?)
was useful in weeding outnoisy assertions (23% of candidate answers).Aggregation Overall, we created 1,500 question-naires,7spanning a total of 30,703 (a, q) pairs.Each questionnaire was annotated by 5 differ-7Each of our 573 questions had many candidate answers.These were split into smaller chunks (questionnaires) of lessthan 25 candidate answers each.ent people, and aggregated using the unanimous-up-to-one (at least 4/5) rule.
Examples that didnot exhibit this kind of inter-annotator agreementwere discarded, and so were examples which weredetermined as nonsensical/ungrammatical (anno-tated with ?).
After aggregating and filtering, wewere left with 3,147 positive (3) and 13,224 neg-ative (7) examples.8To evaluate this aggregation rule, we took a ran-dom subset of 32 questionnaires (594 (a, q) pairs)and annotated them ourselves (expert annotation).We then compared the aggregated crowdsourcedannotation on the same (a, q) pairs to our own.The crowdsourced annotation yielded 91.3% pre-cision on our expert annotations (i.e.
only 8.7%of the crowd-annotated positives were expert-annotated as negative), while recalling 86.2% ofexpert-annotated positives.4 Performance of Existing MethodsTo provide a baseline for future work, we testthe performance of two inference-rule resourcesand two methods of distributional inference on ourdataset, as well as a lemma-similarity baseline.94.1 BaselinesLemma Baseline We implemented a baseline thattakes into account four features from the premiserelation (arel) and the hypothesis relation (qrel) af-ter they have been lemmatized: (1) Does arelcon-tain all of qrel?s content words?
(2) Do the re-lations share a verb?
(3) Does the relations?
ac-tive/passive voice match their arguments?
align-ments?
(4) Do the relations agree on negation?The baseline will classify the example as positiveif all features are true.PPDB 2.0 We used the largest collection ofparaphrases (XXXL) from PPDB (Pavlick et al,2015b).
These paraphrases include argument slotsfor cases where word order changes (e.g.
pas-sive/active).Entailment Graph We used the publicly-available inference rules derived from Berant etal.
?s (2011) entailment graph.
These rules con-tain typed relations and can also be applied in acontext-sensitive manner.
However, ignoring the8This harsh filtering process is mainly a result of poor an-notator quality.
See supplementary material for a detailed de-scription of the steps we took to improve annotator quality.9To recreate the embeddings, see supplementary material.252types and applying the inference rules out of con-text worked better on our dataset, perhaps becauseBerant et al?s taxonomy was learned from a dif-ferent corpus.Relation Embeddings Similar to DIRT (Lin andPantel, 2001), we create vector representations forrelations, which are then used to measure relationsimilarity.
From the set of assertions extractedin ?3.2, we create a dataset of relation-argumentpairs, and use word2vecf (Levy and Goldberg,2014) to train the embeddings.
We also tried to usethe arguments?
embeddings to induce a context-sensitive measure of similarity, as suggested byMelamud et al (2015); however, this method didnot improve performance on our dataset.Word Embeddings Using Google?s SyntacticN-grams (Goldberg and Orwant, 2013), fromwhich candidate answers were extracted, wetrained dependency-based word embeddings withword2vecf (Levy and Goldberg, 2014).
We usedthe average word vector to represent multi-wordrelations, and cosine to measure their similarity.4.2 ResultsUnder the assumption that collections of inferencerules are more precision-oriented, we also try dif-ferent combinations of rule-based and embedding-based methods by first applying the rules and thencalculating the embedding-based similarity onlyon instances that were not identified as positiveby the rules.
Since the embeddings produce asimilarity score, not a classification, we plot allmethods?
performance on a single precision-recallcurve (Figure 2).All methods used the lemma baseline as a firststep to identify positive examples; without it, per-formance drops dramatically.
This is probablymore of a dataset artifact than an observation aboutthe baselines; just like we filtered examples wherearel6= qrel, we could have used a more aggressivepolicy and removed all pairs that share lemmas.It seems that most methods provide little valuebeyond the lemma baseline ?
the exception beingBerant et al?s (2011) entailment graph.
Unify-ing the entailment graph with PPDB (and, implic-itly, the lemma baseline) slightly improves perfor-mance, and provides a significantly better startingpoint for the method based on word embeddings.Even so, performance is still quite poor in absoluteterms, with less than 25% recall at 80% precision.Figure 2: The performance of existing methods on ourdataset.
All methods are run on top of the lemma baseline.All Rules is the union of PPDB and the entailment graph.Rules + W Embs is a combination of All Rules and our wordembeddings.4.3 The Ramifications of Low RecallThese results emphasize the huge false-negativerate of existing methods.
This suggests that a mas-sive amount of inference examples, which are nec-essary for answering questions, are inherently ig-nored in (Zeichner et al, 2012) and post-hoc eval-uations.
Our dataset remedies this bias, and posesa new challenge for future research on relation in-ference.AcknowledgementsThis work was supported by the German ResearchFoundation via the German-Israeli Project Coop-eration (grant DA 1600/1-1), the Israel ScienceFoundation grant 880/12, and by grants from theMAGNET program of the Israeli Office of theChief Scientist (OCS).References[Abend et al2014] Omri Abend, Shay B. Cohen, andMark Steedman.
2014.
Lexical inference overmulti-word predicates: A distributional approach.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 644?654, Baltimore, Mary-land, June.
Association for Computational Linguis-tics.
[Banko et al2007] Michele Banko, Michael J. Ca-farella, Stephen Soderland, Matthew Broadhead,and Oren Etzioni.
2007.
Open information extrac-tion from the web.
In IJCAI 2007, Proceedings ofthe 20th International Joint Conference on Artificial253Intelligence, Hyderabad, India, January 6-12, 2007,pages 2670?2676.
[Beltagy et al2013] Islam Beltagy, Cuong Chau,Gemma Boleda, Dan Garrette, Katrin Erk, andRaymond Mooney.
2013.
Montague meets markov:Deep semantics with probabilistic logical form.
InSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 1: Proceedingsof the Main Conference and the Shared Task:Semantic Textual Similarity, pages 11?21, Atlanta,Georgia, USA, June.
Association for ComputationalLinguistics.
[Berant et al2011] Jonathan Berant, Ido Dagan, and Ja-cob Goldberger.
2011.
Global learning of typedentailment rules.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages610?619, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.
[Berant et al2013] Jonathan Berant, Andrew Chou,Roy Frostig, and Percy Liang.
2013.
Semantic pars-ing on Freebase from question-answer pairs.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1533?1544, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.
[Biemann2013] Chris Biemann.
2013.
Creating asystem for lexical substitutions from scratch usingcrowdsourcing.
Language Resources and Evalua-tion, 47(1):97?122.
[Bird et al2009] Steven Bird, Ewan Klein, and EdwardLoper.
2009.
Natural Language Processing withPython.
O?Reilly Media.
[Fader et al2013] Anthony Fader, Luke Zettlemoyer,and Oren Etzioni.
2013.
Paraphrase-driven learningfor open question answering.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1608?1618, Sofia, Bulgaria, August.
Association forComputational Linguistics.
[Fader et al2014] Anthony Fader, Luke Zettlemoyer,and Oren Etzioni.
2014.
Open question answeringover curated and extracted knowledge bases.
In Pro-ceedings of the 20th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 1156?1165.
ACM.
[Fellbaum1998] Christiane Fellbaum.
1998.
WordNet.Wiley Online Library.
[Goldberg and Orwant2013] Yoav Goldberg and JonOrwant.
2013.
A dataset of syntactic-ngrams overtime from a very large corpus of english books.
InSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 1: Proceedings ofthe Main Conference and the Shared Task: SemanticTextual Similarity, pages 241?247, Atlanta, Georgia,USA, June.
Association for Computational Linguis-tics.
[Grycner and Weikum2014] Adam Grycner and Ger-hard Weikum.
2014.
Harpy: Hypernyms and align-ment of relational paraphrases.
In Proceedings ofCOLING 2014, the 25th International Conferenceon Computational Linguistics: Technical Papers,pages 2195?2204, Dublin, Ireland, August.
DublinCity University and Association for ComputationalLinguistics.
[Grycner et al2015] Adam Grycner, Gerhard Weikum,Jay Pujara, James Foulds, and Lise Getoor.
2015.Relly: Inferring hypernym relationships betweenrelational phrases.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 971?981, Lisbon, Portugal,September.
Association for Computational Linguis-tics.
[Kremer et al2014] Gerhard Kremer, Katrin Erk, Se-bastian Pad?o, and Stefan Thater.
2014.
What substi-tutes tell us - analysis of an ?all-words?
lexical sub-stitution corpus.
In Proceedings of the 14th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 540?549, Gothen-burg, Sweden, April.
Association for ComputationalLinguistics.
[Kruszewski et al2015] Germ?an Kruszewski, DenisPaperno, and Marco Baroni.
2015.
Deriv-ing boolean structures from distributional vectors.Transactions of the Association for ComputationalLinguistics, 3:375?388.
[Lesk1986] Michael Lesk.
1986.
Automatic sense dis-ambiguation using machine readable dictionaries:How to tell a pine cone from an ice cream cone.
InProceedings of the 5th Annual International Con-ference on Systems Documentation, pages 24?26.ACM.
[Levy and Goldberg2014] Omer Levy and Yoav Gold-berg.
2014.
Dependency-based word embeddings.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 302?308, Baltimore, Mary-land, June.
Association for Computational Linguis-tics.
[Levy et al2014] Omer Levy, Ido Dagan, and JacobGoldberger.
2014.
Focused entailment graphs foropen ie propositions.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning, pages 87?97, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.
[Lewis and Steedman2013] Mike Lewis and MarkSteedman.
2013.
Combining distributional andlogical semantics.
Transactions of the Associationfor Computational Linguistics, 1:179?192.
[Lewis2014] Mike Lewis.
2014.
Combined Distribu-tional and Logical Semantics.
Ph.D. thesis, Univer-sity of Edinburgh.254[Lin and Pantel2001] Dekang Lin and Patrick Pantel.2001.
Dirt: Discovery of inference rules from text.In Proceedings of the seventh ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, pages 323?328.
ACM.
[Marelli et al2014] Marco Marelli, Stefano Menini,Marco Baroni, Luisa Bentivogli, Raffaella bernardi,and Roberto Zamparelli.
2014.
A sick cure for theevaluation of compositional distributional semanticmodels.
In Nicoletta Calzolari, Khalid Choukri,Thierry Declerck, Hrafn Loftsson, Bente Maegaard,Joseph Mariani, Asuncion Moreno, Jan Odijk, andStelios Piperidis, editors, Proceedings of the NinthInternational Conference on Language Resourcesand Evaluation (LREC?14), pages 216?223, Reyk-javik, Iceland, May.
European Language ResourcesAssociation (ELRA).
ACL Anthology Identifier:L14-1314.
[McCarthy and Navigli2007] Diana McCarthy andRoberto Navigli.
2007.
Semeval-2007 task 10:English lexical substitution task.
In Proceedingsof the Fourth International Workshop on Seman-tic Evaluations (SemEval-2007), pages 48?53,Prague, Czech Republic, June.
Association forComputational Linguistics.
[Melamud et al2013] Oren Melamud, Jonathan Berant,Ido Dagan, Jacob Goldberger, and Idan Szpektor.2013.
A two level model for context sensitive infer-ence rules.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 1331?1340,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.
[Melamud et al2015] Oren Melamud, Omer Levy, andIdo Dagan.
2015.
A simple word embedding modelfor lexical substitution.
In Proceedings of the 1stWorkshop on Vector Space Modeling for NaturalLanguage Processing, pages 1?7, Denver, Colorado,June.
Association for Computational Linguistics.
[Pavlick et al2015a] Ellie Pavlick, Johan Bos, MalvinaNissim, Charley Beller, Benjamin Van Durme, andChris Callison-Burch.
2015a.
Adding semanticsto data-driven paraphrasing.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing (Vol-ume 1: Long Papers), pages 1512?1522, Beijing,China, July.
Association for Computational Linguis-tics.
[Pavlick et al2015b] Ellie Pavlick, Pushpendre Ras-togi, Juri Ganitkevitch, Benjamin Van Durme, andChris Callison-Burch.
2015b.
Ppdb 2.0: Bet-ter paraphrase ranking, fine-grained entailment re-lations, word embeddings, and style classification.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 2: Short Papers), pages425?430, Beijing, China, July.
Association for Com-putational Linguistics.
[Riedel et al2013] Sebastian Riedel, Limin Yao, An-drew McCallum, and Benjamin M. Marlin.
2013.Relation extraction with matrix factorization anduniversal schemas.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 74?84, Atlanta, Georgia,June.
Association for Computational Linguistics.
[Rockt?aschel et al2015] Tim Rockt?aschel, SameerSingh, and Sebastian Riedel.
2015.
Injectinglogical background knowledge into embeddingsfor relation extraction.
In Proceedings of the 2015Conference of the North American Chapter ofthe Association for Computational Linguistics:Human Language Technologies, pages 1119?1129,Denver, Colorado, May?June.
Association forComputational Linguistics.
[Schoenmackers et al2010] Stefan Schoenmackers,Jesse Davis, Oren Etzioni, and Daniel Weld.
2010.Learning first-order horn clauses from web text.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1088?1098, Cambridge, MA, October.
Associationfor Computational Linguistics.
[Szpektor et al2007] Idan Szpektor, Eyal Shnarch, andIdo Dagan.
2007.
Instance-based evaluation of en-tailment rule acquisition.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 456?463, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.
[Voorhees and Tice2000] Ellen M Voorhees andDawn M Tice.
2000.
Building a question answeringtest collection.
In Proceedings of the 23rd AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages200?207.
ACM.
[Weisman et al2012] Hila Weisman, Jonathan Berant,Idan Szpektor, and Ido Dagan.
2012.
Learningverb inference rules from linguistically-motivatedevidence.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 194?204, Jeju Island, Korea, July.Association for Computational Linguistics.
[Zeichner et al2012] Naomi Zeichner, Jonathan Be-rant, and Ido Dagan.
2012.
Crowdsourcinginference-rule evaluation.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages156?160, Jeju Island, Korea, July.
Association forComputational Linguistics.255
