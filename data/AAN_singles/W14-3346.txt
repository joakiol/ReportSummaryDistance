Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362?367,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsA Systematic Comparison of Smoothing Techniques for Sentence-LevelBLEUBoxing Chen and Colin CherryNational Research Council Canadafirst.last@nrc-cnrc.gc.caAbstractBLEU is the de facto standard machinetranslation (MT) evaluation metric.
How-ever, because BLEU computes a geo-metric mean of n-gram precisions, it of-ten correlates poorly with human judg-ment on the sentence-level.
There-fore, several smoothing techniques havebeen proposed.
This paper systemati-cally compares 7 smoothing techniquesfor sentence-level BLEU.
Three of themare first proposed in this paper, and theycorrelate better with human judgments onthe sentence-level than other smoothingtechniques.
Moreover, we also comparethe performance of using the 7 smoothingtechniques in statistical machine transla-tion tuning.1 IntroductionSince its invention, BLEU (Papineni et al., 2002)has been the most widely used metric for bothmachine translation (MT) evaluation and tuning.Many other metrics correlate better with humanjudgments of translation quality than BLEU, asshown in recent WMT Evaluation Task reports(Callison-Burch et al., 2011; Callison-Burch et al.,2012).
However, BLEU remains the de facto stan-dard evaluation and tuning metric.
This is proba-bly due to the following facts:1.
BLEU is language independent (except forword segmentation decisions).2.
BLEU can be computed quickly.
This is im-portant when choosing a tuning metric.3.
BLEU seems to be the best tuning metricfrom a quality point of view - i.e., modelstrained using BLEU obtain the highest scoresfrom humans and even from other metrics(Cer et al., 2010).One of the main criticisms of BLEU is that ithas a poor correlation with human judgments onthe sentence-level.
Because it computes a geomet-ric mean of n-gram precisions, if a higher ordern-gram precision (eg.
n = 4) of a sentence is0, then the BLEU score of the entire sentence is0, no matter how many 1-grams or 2-grams arematched.
Therefore, several smoothing techniquesfor sentence-level BLEU have been proposed (Linand Och, 2004; Gao and He, 2013).In this paper, we systematically compare 7smoothing techniques for sentence-level BLEU.Three of them are first proposed in this paper, andthey correlate better with human judgments on thesentence-level than other smoothing techniques onthe WMT metrics task.
Moreover, we comparethe performance of using the 7 smoothing tech-niques in statistical machine translation tuning onNIST Chinese-to-English and Arabic-to-Englishtasks.
We show that when tuning optimizes theexpected sum of these sentence-level metrics (asadvocated by Cherry and Foster (2012) and Gaoand He (2013) among others), all of these metricsperform similarly in terms of their ability to pro-duce strong BLEU scores on a held-out test set.2 BLEU and smoothing2.1 BLEUSuppose we have a translation T and its referenceR, BLEU is computed with precision P (N,T,R)and brevity penalty BP(T,R):BLEU(N,T,R) = P (N,T,R)?
BP(T,R) (1)where P (N,T,R) is the geometric mean of n-gram precisions:P (N,T,R) =(N?n=1pn)1N(2)362and where:pn=mnln(3)mnis the number of matched n-grams betweentranslation T and its referenceR, and lnis the totalnumber of n-grams in the translation T .
BLEU?sbrevity penalty punishes the score if the translationlength len(T ) is shorter than the reference lengthlen(R), using this equation:BP(T,R) = min(1.0, exp(1?len(R)len(T )))(4)2.2 Smoothing techniquesThe original BLEU was designed for thedocument-level; as such, it required no smooth-ing, as some sentence would have at least one 4-gram match.
We now describe 7 smoothing tech-niques that work better for sentence-level evalua-tion.
Suppose we consider matching n-grams forn = 1 .
.
.
N (typically, N = 4).
Let mnbe theoriginal match count, and m?nbe the modified n-gram match count.Smoothing 1: if the number of matched n-grams is 0, we use a small positive value ?
to re-place the 0 for n ranging from 1 toN .
The number?
is set empirically.m?n= ?, if mn= 0.
(5)Smoothing 2: this smoothing technique wasproposed in (Lin and Och, 2004).
It adds 1 to thematched n-gram count and the total n-gram countfor n ranging from 2 to N .m?n= mn+ 1, for n in 2 .
.
.
N, (6)l?n= ln+ 1, for n in 2 .
.
.
N. (7)Smoothing 3: this smoothing technique is im-plemented in the NIST official BLEU toolkitmteval-v13a.pl.1The algorithm is given below.
Itassigns a geometric sequence starting from 1/2 tothe n-grams with 0 matches.1.
invcnt = 12. for n in 1 to N3.
if mn= 04. invcnt = invcnt?
25. m?n= 1/invcnt6.
endif7.
endfor1available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/Smoothing 4: this smoothing technique is novelto this paper.
We modify Smoothing 3 to addressthe concern that shorter translations may have in-flated precision values due to having smaller de-nominators; therefore, we give them proportion-ally smaller smoothed counts.
Instead of scalinginvcnt with a fixed value of 2, we replace line 4 inSmoothing 3?s algorithm with Equation 8 below.invcnt = invcnt?Kln(len(T ))(8)It assigns larger values to invcnt for shorter sen-tences, resulting in a smaller smoothed count.
Kis set empirically.Smoothing 5: this smoothing technique is alsonovel to this paper.
It is inspired by the intuitionthat matched counts for similar values of n shouldbe similar.
To a calculate the n-gram matchedcount, it averages the n ?
1, n and n + 1 ?grammatched counts.
We define m?0= m1+ 1, andcalculate m?nfor n > 0 as follows:m?n=m?n?1+ mn+ mn+13(9)Smoothing 6: this smoothing technique wasproposed in (Gao and He, 2013).
It interpolatesthe maximum likelihood estimate of the precisionpnwith a prior estimate p0n.
The prior is estimatedby assuming that the ratio between pnand pn?1will be the same as that between pn?1and pn?2.Formally, the precisions of lower order n-grams,i.e., p1and p2, are not smoothed, while the pre-cisions of higher order n-grams, i.e.
n > 2, aresmoothed as follows:pn=mn+ ?p0nln+ ?
(10)where ?
is set empirically, and p0nis computed asp0n= pn?1?pn?1pn?2(11)Smoothing 7: this novel smoothing techniquecombines smoothing 4 and smoothing 5.
That is,we first compute a smoothed count for those 0matched n-gram counts using Smoothing 4, andthen take the average of three counts to set the fi-nal matched n-gram count as in Equation 9.3 ExperimentsWe carried out two series of experiments.
The7 smoothing techniques were first compared in363set year lang.
#system #seg.
pairdev 2008 xx-en 43 7,804test1 2012 xx-en 49 34,909test2 2013 xx-en 94 281,666test3 2012 en-xx 54 47,875test4 2013 en-xx 95 220,808Table 1: Statistics of the WMT dev and test sets.the metric task as evaluation metrics, then theywere compared as metrics for tuning SMT systemsto maximize the sum of expected sentence-levelBLEU scores.3.1 Evaluation taskWe first compare the correlations with humanjudgment for the 7 smoothing techniques onWMTdata; the development set (dev) is the WMT 2008all-to-English data; the test sets are theWMT 2012and WMT 2013 all-to-English, and English-to-allsubmissions.
The languages ?all?
(?xx?
in Ta-ble 1) include French, Spanish, German, Czechand Russian.
Table 1 summarizes the dev/test setstatistics.Following WMT 2013?s metric task (Mach?a?cekand Bojar, 2013), for the segment level, we useKendall?s rank correlation coefficient ?
to measurethe correlation with human judgment:?
=#concordant-pairs?#discordant-pairs#concordant-pairs + #discordant-pairs(12)We extract all pairwise comparisons where onesystem?s translation of a particular segment wasjudged to be better than the other system?s trans-lation, i.e., we removed all tied human judg-ments for a particular segment.
If two transla-tions for a particular segment are assigned thesame BLEU score, then the #concordant-pairsand #discordant-pairs both get a half count.
Inthis way, we can keep the number of total pairsconsistent for all different smoothing techniques.For the system-level, we used Spearman?s rankcorrelation coefficient ?
and Pearson?s correla-tion coefficient ?
to measure the correlation ofthe metric with human judgments of translation.If we compute document-level BLEU as usual,all 7 smoothing techniques actually get the sameresult, as document-level BLEU does not needsmoothing.
We therefore compute the document-level BLEU as the weighted average of sentence-level BLEU, with the weights being the referenceInto-Englishsmooth seg ?
sys ?
sys ?crp ?
0.720 0.8870 0.165 0.759 0.8871 0.224 0.760 0.8872 0.226 0.757 0.8873 0.224 0.760 0.8874 0.228 0.763 0.8875 0.234 0.765 0.8876 0.230 0.754 0.8877 0.236 0.766 0.887Table 2: Correlations with human judgment onWMT data for Into-English task.
Results are av-eraged on 4 test sets.
?crp?
is the origianl IBMcorpus-level BLEU.lengths:BLEUd=?Di=1len(Ri)BLEUi?Di=1len(Ri)(13)where BLEUiis the BLEU score of sentence i,and D is the size of the document in sentences.We first set the free parameters of each smooth-ing method by grid search to optimize thesentence-level score on the dev set.
We set ?
to 0.1for Smoothing 1; K = 5 for Smoothing 4; ?
= 5for Smoothing 6.Tables 2 and 3 report our results on the met-rics task.
We compared the 7 smoothing tech-niques described in Section 2.2 to a baseline withno smoothing (Smoothing 0).
All scores match n-grams n = 1 to 4.
Smoothing 3 is implementedin the standard official NIST evaluation toolkit(mteval-v13a.pl).
Results are averaged across the4 test sets.All smoothing techniques improved sentence-level correlations (? )
over no smoothing.
Smooth-ing method 7 got the best sentence-level results onboth the Into-English and Out-of-English tasks.On the system-level, our weighted average ofsentence-level BLEU scores (see Equation 13)achieved a better correlation with human judge-ment than the original IBM corpus-level BLEU.However, the choice of which smoothing tech-nique is used in the average did not make a verybig difference; in particular, the system-level rankcorrelation ?
did not change for 13 out of 14 cases.These methods help when comparing one hypoth-esis to another, but taken as a part of a larger aver-age, all seven methods assign relatively low scores364Out-of-Englishsmooth seg ?
sys ?
sys ?crp ?
0.712 0.7440 0.119 0.715 0.7441 0.178 0.722 0.7482 0.180 0.725 0.7443 0.178 0.724 0.7444 0.181 0.727 0.7445 0.184 0.731 0.7446 0.182 0.725 0.7447 0.187 0.734 0.744Table 3: Correlations with human judgment onWMT data for Out-of-English task.
Results areaveraged on 4 test sets.
?crp?
is the origianl IBMcorpus-level BLEU.to the cases that require smoothing, resulting insimilar system-level rankings.3.2 Tuning taskIn this section, we explore the various BLEUsmoothing methods in the context of SMT param-eter tuning, which is used to set the decoder?slinear model weights w. In particular, we usea tuning method that maximizes the sum of ex-pected sentence-level BLEU scores, which hasbeen shown to be a simple and effective methodfor tuning with large feature sets by both Cherryand Foster (2012) and Gao and He (2013), butwhich requires a smoothed sentence-level BLEUapproximation.
For a source sentence fi, the prob-ability of the kthtranslation hypothesis ekiis its ex-ponentiated and normalized model score:Pw(eki|fi) =exp(scorew(eki, fi))?k?exp(scorew(ek?i, fi))where k?ranges over all hypotheses in a K-bestlist.2We then use stochastic gradient descent(SGD) to minimize:?||w||2??i[len(Ri)?
EPw(BLEU(eki, fi))]Note that we scale the expectation by referencelength to place more emphasis on longer sen-tences.
We set the regularization parameter ?,which determines the trade-off between a high ex-pected BLEU and a small norm, to ?
= 10.Following Cherry and Foster (2012), we tunewith a MERT-like batch architecture: fixing a set2We useK = 100 in our experiments.corpus # segs # en tokChinese-Englishtrain 10.1M 283Mtune 1,506 161KMT06 1,664 189KMT08 1,357 164KArabic-Englishtrain 1,512K 47.8Mtune 1,664 202KMT08 1,360 205KMT09 1,313 187KTable 4: Statistics of the NIST Chinese-Englishand Arabic-English data.of K-best lists, optimizing, and then re-decodingthe entire dev set to K-best and aggregating withprevious lists to create a better K-best approxima-tion.
We repeat this outer loop 15 times.We carried out experiments in two different set-tings, both involving data from NIST Open MT2012.3The first setting is based on data from theChinese-to-English constrained track, comprisingabout 283 million English running words.
Thesecond setting uses NIST 2012 Arabic-to-Englishdata, but excludes the UN data.
There are about47.8 million English running words in these train-ing data.
The dev set (tune) for the Chinese-to-English task was taken from the NIST 2005 eval-uation set, augmented with some web-genre mate-rial reserved from other NIST corpora.
We test onthe evaluation sets from NIST 2006 and 2008.
Forthe Arabic-to-English task, we use the evaluationsets from NIST 2006, 2008, and 2009 as our devset and two test sets, respectively.
Table 4 summa-rizes the training, dev and test sets.Experiments were carried out with an in-house,state-of-the-art phrase-based system.
Each corpuswas word-aligned using IBM2, HMM, and IBM4models, and the phrase table was the union ofphrase pairs extracted from these separate align-ments, with a length limit of 7.
The translationmodel (TM) was smoothed in both directions withKneser-Ney smoothing (Chen et al., 2011).
Weuse the hierarchical lexicalized reordering model(RM) (Galley and Manning, 2008), with a dis-tortion limit of 7.
Other features include lexi-cal weighting in both directions, word count, adistance-based RM, a 4-gram LM trained on thetarget side of the parallel data, and a 6-gram En-3http://www.nist.gov/itl/iad/mig/openmt12.cfm365Tune std MT06 std MT08 std0 27.6 0.1 35.6 0.1 29.0 0.21 27.6 0.0 35.7 0.1 29.1 0.12 27.5 0.1 35.8 0.1 29.1 0.13 27.6 0.1 35.8 0.1 29.1 0.14 27.6 0.1 35.7 0.2 29.1 0.25 27.6 0.1 35.5 0.1 28.9 0.26 27.5 0.1 35.7 0.1 29.0 0.27 27.6 0.1 35.6 0.1 29.0 0.1Table 5: Chinese-to-English Results for the smallfeature set tuning task.
Results are averaged across5 replications; std is the standard deviation.glish Gigaword LM.We also conducted a set of experiments with amuch larger feature set.
This system used onlyGIZA++ for word alignment, increased the distor-tion limit from 7 to 9, and is trained on a high-quality subset of the parallel corpora used ear-lier.
Most importantly, it includes the full set ofsparse phrase-pair features used by both Hopkinsand May (2011) and Cherry and Foster (2012),which results in nearly 7,000 features.Our evaluation metric is the original IBMBLEU, which performs case-insensitive matchingof n-grams up to n = 4.
We perform randomreplications of parameter tuning, as suggested byClark et al.
(2011).
Each replication uses a differ-ent random seed to determine the order in whichSGD visits tuning sentences.
We test for signifi-cance using the MultEval tool,4which uses a strat-ified approximate randomization test to accountfor multiple replications.
We report results aver-aged across replications as well as standard devia-tions, which indicate optimizer stability.Results for the small feature set are shown inTables 5 and 6.
All 7 smoothing techniques, aswell as the no smoothing baseline, all yield verysimilar results on both Chinese and Arabic tasks.We did not find any two results to be significantlydifferent.
This is somewhat surprising, as othergroups have suggested that choosing an appropri-ate BLEU approximation is very important.
In-stead, our experiments indicate that the selectedBLEU smoothing method is not very important.The large-feature experiments were only con-ducted with the most promising methods accord-ing to correlation with human judgments:4available at https://github.com/jhclark/multevalTune std MT08 std MT09 std0 46.9 0.1 46.5 0.1 49.1 0.11 46.9 0.0 46.4 0.1 49.1 0.12 46.9 0.0 46.4 0.1 49.0 0.13 47.0 0.0 46.5 0.1 49.2 0.14 47.0 0.0 46.5 0.1 49.2 0.15 46.9 0.0 46.4 0.1 49.1 0.16 47.0 0.0 46.4 0.1 49.1 0.17 47.0 0.0 46.4 0.1 49.0 0.1Table 6: Arabic-to-English Results for the smallfeature set tuning task.
Results are averaged across5 replications; std is the standard deviation.Tune std MT06 std MT08 stdmira 29.9 0.1 38.0 0.1 31.0 0.10 29.5 0.1 37.9 0.1 31.4 0.32 29.6 0.3 38.0 0.2 31.1 0.24 29.9 0.2 38.1 0.1 31.2 0.26 29.7 0.1 37.9 0.2 31.0 0.27 29.7 0.2 38.0 0.2 31.2 0.1Table 7: Chinese-to-English Results for the largefeature set tuning task.
Results are averagedacross 5 replications; std is the standard deviation.Significant improvements over the no-smoothingbaseline (p ?
0.05) are marked in bold.0: No smoothing (baseline)2: Add 1 smoothing (Lin and Och, 2004)4: Length-scaled pseudo-counts (this paper)6: Interpolation with a precision prior (Gao andHe, 2013)7: Combining Smoothing 4 with the match in-terpolation of Smoothing 5 (this paper)The results of the large feature set experiments areshown in Table 7 for Chinese-to-English and Ta-ble 8 for Arabic-to-English.
For a sanity check, wecompared these results to tuning with our very sta-ble Batch k-best MIRA implementation (Cherryand Foster, 2012), listed as mira, which shows thatall of our expected BLEU tuners are behaving rea-sonably, if not better than expected.Comparing the various smoothing methods inthe large feature scenario, we are able to see signif-icant improvements over the no-smoothing base-line.
Notably, Method 7 achieves a significantimprovement over the no-smoothing baseline in 3out of 4 scenarios, more than any other method.Unfortunately, in the Chinese-English MT08 sce-nario, the no-smoothing baseline significantly out-366Tune std MT08 std MT09 stdmira 47.9 0.1 47.3 0.0 49.3 0.10 48.1 0.1 47.2 0.1 49.5 0.12 48.0 0.1 47.4 0.1 49.7 0.14 48.1 0.2 47.4 0.1 49.6 0.16 48.2 0.0 47.3 0.1 49.7 0.17 48.1 0.1 47.3 0.1 49.7 0.1Table 8: Arabic-to-English Results for the largefeature set tuning task.
Results are averagedacross 5 replications; std is the standard deviation.Significant improvements over the no-smoothingbaseline (p ?
0.05) are marked in bold.performs all smoothed BLEU methods, making itdifficult to draw any conclusions at all from theseexperiments.
We had hoped to see at least a clearimprovement in the tuning set, and one does seea nice progression as smoothing improves in theChinese-to-English scenario, but no correspond-ing pattern emerges for Arabic-to-English.4 ConclusionsIn this paper, we compared seven smoothingtechniques for sentence-level BLEU.
Three ofthem are newly proposed in this paper.
Thenew smoothing techniques got better sentence-level correlations with human judgment than othersmoothing techniques.
On the other hand, whenwe compare the techniques in the context of tun-ing, using a method that requires sentence-levelBLEU approximations, they all have similar per-formance.ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 22?64, Edinburgh, Scot-land, July.
Association for Computational Linguis-tics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Daniel Cer, Christopher D. Manning, and Daniel Juraf-sky.
2010.
The best lexical metric for phrase-basedstatistical mt system optimization.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 555?563, LosAngeles, California, June.
Association for Compu-tational Linguistics.Boxing Chen, Roland Kuhn, George Foster, andHoward Johnson.
2011.
Unpacking and transform-ing feature functions: New ways to smooth phrasetables.
In MT Summit 2011.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InNAACL 2012.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In ACL 2011.Michel Galley and C. D. Manning.
2008.
A simpleand effective hierarchical phrase reordering model.In EMNLP 2008, pages 848?856, Hawaii, October.Jianfeng Gao and Xiaodong He.
2013.
Training mrf-based phrase translation models using gradient as-cent.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 450?459, Atlanta, Georgia, June.Association for Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In EMNLP 2011.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
In Proceedings of the 42nd Meetingof the Association for Computational Linguistics(ACL?04), Main Volume, pages 605?612, Barcelona,Spain, July.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Results ofthe WMT13 metrics shared task.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 45?51, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318,Philadelphia, July.
ACL.367
