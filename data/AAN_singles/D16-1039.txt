Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 403?413,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning Term Embeddings for Taxonomic Relation Identification UsingDynamic Weighting Neural NetworkLuu Anh TuanInstitute for Infocomm Research, Singaporeat.luu@i2r.a-star.edu.sgYi TayNanyang Technological Universityytay2@e.ntu.edu.sgSiu Cheung HuiNanyang Technological Universityasschui@ntu.edu.sgSee Kiong NgInstitute for Infocomm Research, Singaporeskng@i2r.a-star.edu.sgAbstractTaxonomic relation identification aims to rec-ognize the ?is-a?
relation between two terms.Previous works on identifying taxonomic re-lations are mostly based on statistical and lin-guistic approaches, but the accuracy of theseapproaches is far from satisfactory.
In this pa-per, we propose a novel supervised learningapproach for identifying taxonomic relationsusing term embeddings.
For this purpose, wefirst design a dynamic weighting neural net-work to learn term embeddings based on notonly the hypernym and hyponym terms, butalso the contextual information between them.We then apply such embeddings as featuresto identify taxonomic relations using a super-vised method.
The experimental results showthat our proposed approach significantly out-performs other state-of-the-art methods by 9%to 13% in terms of accuracy for both generaland specific domain datasets.1 IntroductionTaxonomies which serve as the backbone of struc-tured knowledge are useful for many NLP applica-tions such as question answering (Harabagiu et al,2003) and document clustering (Fodeh et al, 2011).However, the hand-crafted, well-structured tax-onomies including WordNet (Miller, 1995), Open-Cyc (Matuszek et al, 2006) and Freebase (Bol-lacker et al, 2008) that are publicly available maynot be complete for new or specialized domains.
Itis also time-consuming and error prone to identifytaxonomic relations manually.
As such, methodsfor automatic identification of taxonomic relationsis highly desirable.The previous methods for identifying taxonomicrelations can be generally classified into two cate-gories: statistical and linguistic approaches.
The sta-tistical approaches rely on the idea that frequentlyco-occurring terms are likely to have taxonomic re-lationships.
While such approaches can result intaxonomies with relatively high coverage, they areusually heavily dependent on the choice of featuretypes, and suffer from low accuracy.
The linguis-tic approaches which are based on lexical-syntacticpatterns (e.g.
?A such as B?)
are simple and efficient.However, they usually suffer from low precision andcoverage because the identified patterns are unableto cover the wide range of complex linguistic struc-tures, and the ambiguity of natural language com-pounded by data sparsity makes these approachesless robust.Word embedding (Bengio et al, 2001), alsoknown as distributed word representation, whichrepresents words with high-dimensional and real-valued vectors, has been shown to be effective inexploring both linguistic and semantic relations be-tween words.
In recent years, word embedding hasbeen used quite extensively in NLP research, rang-ing from syntactic parsing (Socher et al, 2013a),machine translation (Zou et al, 2013) to senti-ment analysis (Socher et al, 2013b).
The cur-rent methods for learning word embeddings havefocused on learning the representations from wordco-occurrence so that similar words will have simi-lar embeddings.
However, using the co-occurrencebased similarity learning alone is not effective forthe purpose of identifying taxonomic relations.Recently, Yu et al (2015) proposed a super-403vised method to learn term embeddings based onpre-extracted taxonomic relation data.
However, thismethod is heavily dependent on the training data todiscover all taxonomic relations, i.e.
if a pair ofterms is not in the training set, it may become anegative example in the learning process, and willbe classified as a non-taxonomic relation.
The de-pendency on training data is a huge drawback of themethod as no source can guarantee that it can coverall possible taxonomic relations for learning.
More-over, the recent studies (Velardi et al, 2013; Levyet al, 2014; Tuan et al, 2015) showed that contex-tual information between hypernym and hyponym isan important indicator to detect taxonomic relations.However, the term embedding learning method pro-posed in (Yu et al, 2015) only learns through thepairwise relations of terms without considering thecontextual information between them.
Therefore,the resultant quality is not good in some specific do-main areas.In this paper, we propose a novel approach tolearn term embeddings based on dynamic weight-ing neural network to encode not only the informa-tion of hypernym and hyponym, but also the con-textual information between them for the purposeof taxonomic relation identification.
We then ap-ply the identified embeddings as features to find thepositive taxonomic relations using the supervisedmethod SVM.
The experimental results show thatour proposed term embedding learning approachoutperforms other state-of-the-art embedding learn-ing methods for identifying taxonomic relationswith much higher accuracy for both general and spe-cific domains.
In addition, another advantage ofour proposed approach is that it is able to general-ize from the training dataset the taxonomic relationproperties for unseen pairs.
Thus, it can recognizesome true taxonomic relations which are not evendefined in dictionary and training data.
For the restof this paper, we will discuss the proposed term em-bedding learning approach and its performance re-sults.2 Related workPrevious works on taxonomic relation identificationcan be roughly divided into two main approaches ofstatistical learning and linguistic pattern matching.Statistical learning methods include co-occurrenceanalysis (Lawrie and Croft, 2003), hierarchical la-tent Dirichlet alocation (LDA) (Blei et al, 2004;Petinot et al, 2011), clustering (Li et al, 2013), lin-guistic feature-based semantic distance learning (Yuet al, 2011), distributional representation (Roller etal., 2014; Weeds et al, 2014; Kruszewski et al,2015) and co-occurrence subnetwork mining (Wanget al, 2013).
Supervised statistical methods (Petinotet al, 2011) rely on hierarchical labels to learn thecorresponding terms for each label.
These methodsrequire labeled training data which is costly and notalways available in practice.
Unsupervised statis-tical methods (Pons-Porrata et al, 2007; Li et al,2013; Wang et al, 2013) are based on the idea thatterms that frequently co-occur may have taxonomicrelationships.
However, these methods generallyachieve low accuracies.Linguistic approaches rely on lexical-syntacticpatterns (Hearst, 1992) (e.g.
?A such as B?)
to cap-ture textual expressions of taxonomic relations, andmatch them with the given documents or Web in-formation to identify the relations between a termand its hypernyms (Kozareva and Hovy, 2010; Nav-igli et al, 2011; Wentao et al, 2012).
These pat-terns can be manually created (Kozareva and Hovy,2010; Wentao et al, 2012) or automatically identi-fied (Snow et al, 2004; Navigli et al, 2011).
Suchliguistic pattern matching methods can generallyachieve higher precision than the statistical methods,but they suffer from lower coverage.
To balance theprecision and recall, Zhu et al (2013) and Tuan etal.
(2014) have combined both unsupervised statis-tical and linguistic methods for finding taxonomicrelations.In recent years, there are a few studies on tax-onomic relation identification using word embed-dings such as the work of Tan et al (2015) and Fuet al (2014).
These studies are based on word em-beddings from the Word2Vec model (Mikolov et al,2013a), which is mainly optimized for the purposeof analogy detection using co-occurrence based sim-ilarity learning.
As such, these studies suffer frompoor performance on low accuracy for taxonomic re-lation identification.The approach that is closest to our work is the oneproposed by Yu et al (2015), which also learns termembeddings for the purpose of taxonomic relation404identification.
In the approach, a distance-marginneural network is proposed to learn term embed-dings based on the pre-extracted taxonomic relationsfrom the Probase database (Wentao et al, 2012).However, the neural network is trained using onlythe information of the term pairs (i.e.
hypernym andhyponym) without considering the contextual infor-mation between them, which has been shown to bean important indicator for identifying taxonomic re-lations from previous studies (Velardi et al, 2013;Levy et al, 2014; Tuan et al, 2014).
Moreover, ifa pair of terms is not contained in the training set,there is high possibility that it will become a nega-tive example in the learning process, and will likelybe recognized as a non-taxonomic relation.
The keyassumption behind the design of this approach is notalways true as no available dataset can possibly con-tain all taxonomic relations.3 MethodologyIn this section, we first propose an approach forlearning term embeddings based on hypernym, hy-ponym and the contextual information betweenthem.
We then discuss a supervised method for iden-tifying taxonomic relations based on the term em-beddings.3.1 Learning term embeddingsAs shown in Figure 1, there are three steps for learn-ing term embeddings: (i) extracting taxonomic rela-tions; (ii) extracting training triples; and (iii) trainingneural network.
First, we extract from WordNet altaxonomic relations as training data.
Then, we ex-tract from Wikipedia all sentences which contain atleast one pair of terms involved in a taxonomic rela-tion in the training data, and from that we identifythe triples of hypernym, hyponym and contextualwords between them.
Finally, using the extractedtriples as input, we propose a dynamic weightingneural network to learn term embeddings based onthe information of these triples.3.1.1 Extracting taxonomic relationsThis step aims to extract a set of taxonomic re-lations for training.
For this purpose, we use Word-Net hierarchies for extracting all (direct and indirect)taxonomic relations between noun terms in Word-Net.
However, based on our experience, the rela-Extracting taxonomicrelationsExtracting trainingtriplesTraining neural networkSet of taxonomicrelationsSet oftraining triplesTerm embeddingsFigure 1: Proposed approach for learning term embeddings.tions involving with top-level terms such as ?object?,?entity?
or ?whole?
are usually ambiguous and be-come noise for the learning purpose.
Therefore, weexclude from the training set al relations which in-volve with those top-level terms.
Note that we alsoexclude from training set al taxonomic relations thatare happened in the datasets used for testing in Sec-tion 4.1.
As a result, the total number of extractedtaxonomic relations is 236,058.3.1.2 Extracting training triplesThis step aims to extract the triples of hypernym,hyponym and the contextual words between them.These triples will serve as the inputs to the neuralnetwork for training.
In this research, we definecontextual words as all words located between thehypernym and hyponym in a sentence.
We use thelatest English Wikipedia corpus as the source for ex-tracting such triples.Using the set of taxonomic relations extractedfrom the first step as reference, we extract fromthe Wikipedia corpus all sentences which containat least two terms involved in a taxonomic relation.Specifically, for each sentence, we use the Stanfordparser (Manning et al, 2014) to parse it, and checkwhether there is any pair of terms which are nounsor noun phrases in the sentence having a taxonomicrelationship.
If yes, we extract the hypernym, hy-ponym and all words between them from the sen-405tence as a training triple.
In total, we have extracted15,499,173 training triples from Wikipedia.Here, we apply the Stanford parser rather thanmatching the terms directly in the sentence in orderto avoid term ambiguity as a term can serve for dif-ferent grammatical functions such as noun or verb.For example, consider the following sentence:?
Many supporters book tickets for the premiereof his new publication.The triple (?publication?, ?book?, ?tickets for the pre-miere of his new?)
may be incorrectly added to thetraining set due to the occurrence of the taxonomicpair (?publication?, ?book?
), even though the mean-ing of ?book?
in this sentence is not about the ?pub-lication?.3.1.3 Training neural networkContextual information is an important indicatorfor detecting taxonomic relations.
For example, inthe following two sentences:?
Dog is a type of animal which you can have asa pet.?
Animal such as dog is more sensitive to soundthan human.The occurrence of contextual words ?is a type of?and ?such as?
can be used to identify the taxo-nomic relation between ?dog?
and ?animal?
in thesentences.
Many works in the literature (Kozarevaand Hovy, 2010; Navigli et al, 2011; Wentao et al,2012) attempted to manually find these contextualpatterns, or automatically learn them.
However, dueto the wide range of complex linguistic structures,it is difficult to discover all possible contextual pat-terns between hypernyms and hyponyms in order todetect taxonomic relations effectively.In this paper, instead of explicitly discovering thecontextual patterns of taxonomic relations, we pro-pose a dynamic weighting neural network to encodethis information, together with the hypernym andhyponym, for learning term embeddings.
Specifi-cally, the target of the neural network is to predictthe hypernym term from the given hyponym termand contextual words.
The architecture of the pro-posed neural network is shown in Figure 2, whichconsists of three layers: input layer, hidden layer andoutput layer.In our setting, the vocabulary size is V , and thehidden layer size is N .
The nodes on adjacent lay-ers are fully connected.
Given a term/word t in thevocabulary, the input vector of t is encoded as aone-hot V -dimensional vector xt, i.e.
xt consistsof 0s in all elements except the element used touniquely identify t which is set as 1.
The weightsbetween the input layer and output layer are repre-sented by a V?N matrix W .
Each row of W is aN -dimensional vector representation vt of the asso-ciated word/term t of the input layer.Given a hyponym term hypo and k context wordsc1, c2, .., ck in the training triple, the output of hid-den layer h is calculated as:h = W> ?
12k (k ?
xhypo + xc1 + xc2 + ...+ xck)= 12k (k ?
vhypo + vc1 + vc2 + ...+ vck)(1)where vt is the vector representation of the inputword/term t.The weight of h in Equation (1) is calculated asthe average of the vector representation of hyponymterm and contextual words.
Therefore, this weightis not based on a fixed number of inputs.
Instead,it is dynamically updated based on the number ofcontextual words k in the current training triple, andthe hyponym term.
This model is called dynamicweighting neural network to reflect its dynamic na-ture.
Note that to calculate h, we also multiply thevector representation of hyponym by k to reduce thebias problem of high number of contextual words,so that the weight of the input vector of hyponym isbalanced with the total weight of contextual words.From the hidden layer to the output layer, thereis another weight N ?
V for the output matrix W ?.Each column of W ?
is a N -dimensional vector v?trepresenting the output vector of t. Using theseweights, we can compute an output score ut for eachterm/word t in the vocabulary:ut = v?t> ?
h (2)where v?t is the output vector of t.We then use soft-max, a log-linear classificationmodel, to obtain the posterior distribution of hyper-nym terms as follows:406:9 x 1:9 x 1:9 x 1:9 x 1:?1 x 92utput la\er+idden la\er,nput la\er9dimension1dimension9dimensionxK\poxcxcxckKFigure 2: The architecture of the proposed dynamic weighting neural network model.p(hype|hypo, c1, c2, .., ck)= euhype?Vi=1 eui= ev?>hype?
12k (k?vhypo+?kj=1 vcj )?Vi=1 ev?>i ?
12k (k?vhypo+?kj=1 vcj )(3)The objective function is then defined as:O = 1TT?t=1log(p(hypet|hypot, c1t, c2t, .., ckt))(4)where T is the number of training triples; hypet,hypot and cit are hypernym term, hyponym termand contextual words respectively in the trainingtriple t.After maximizing the log-likelihood objectivefunction in Equation (4) over the entire training setusing stochastic gradient descent, the term embed-dings are learned accordingly.3.2 Supervised taxonomic relationidentificationTo decide whether a term x is a hypernym of termy, we build a classifier that uses embedding vec-tors as features for taxonomic relation identification.Specifically, we use Support Vector Machine (SVM)(Cortes and Vapnik, 1995) for this purpose.
Givenan ordered pair (x, y), the input feature is the con-catenation of embedding vectors (vx,vy) of x and y.In addition, our term embedding learning approachhas the property that the embedding of hypernym isencoded based on not only the information of hy-ponym but also the information of contextual words.Therefore, we add one more feature to the input ofSVM, i.e.
the offset vector (vx ?
vy), to contain theinformation of all contextual words between x and y.In summary, the feature vector is a 3d dimensionalvector ?vx, vy, vx ?
vy?, where d is the dimensionof term embeddings.
As will be shown later in theexperimental results, the offset vector plays an im-portant role in the task of taxonomic relation identi-fication of our approach.4 ExperimentsWe conduct experiments to evaluate the perfor-mance of our term embedding learning approach onthe general domain areas as well as the specific do-main areas.
In performance evaluation, we compareour approach with two other state-of-the-art super-vised term embedding learning methods in Yu et al(2015) and the Word2Vec model (Mikolov et al,2013a).4074.1 DatasetsThere are five datasets used in the experiments.
Twodatasets, namely BLESS and ENTAILMENT, aregeneral domain datasets.
The other three datasets,namely Animal, Plant and Vehicle, are specific do-main datasets.?
BLESS (Baroni and Lenci, 2011) dataset: Itcovers 200 distinct, unambiguous concepts(terms); each of which is involved with otherterms, called relata, in some relations.
We ex-tract from BLESS 14,547 pairs of terms for thefollowing four types of relations: taxonomic re-lation, meronymy relation (a.k.a.
part-of rela-tion), coordinate relation (i.e.
two terms hav-ing the same hypernym), and random relation.From these pairs, we set taxonomic relations aspositive examples, while other relations formthe negative examples.?
ENTAILMENT dataset (Baroni et al, 2012):It consists of 2,770 pairs of terms, with equalnumber of positive and negative examples oftaxonomic relations.
Altogether, there are1,376 unique hyponyms and 1,016 unique hy-pernyms.?
Animal, Plant and Vehicle datasets (Velardi etal., 2013): They are taxonomies constructedbased on the dictionaries and data crawled fromthe Web for the corresponding domains.
Thepositive examples are created by extracting allpossible (direct and indirect) taxonomic rela-tions from the taxonomies.
The negative ex-amples are generated by randomly pairing twoterms which are not involved in any taxonomicrelation.The number of terms, positive examples and neg-ative examples extracted from the five datasets aresummarized in Table 1.Dataset # terms # positive # negativeBLESS 5229 1337 13210ENTAILMENT 2392 1385 1385Animal 659 4164 8471Plant 520 2266 4520Vehicle 117 283 586Table 1: Datasets used in the experiments.4.2 Comparison modelsIn the experiments, we use the following supervisedmodels for comparison:?
SVM+Our: This model uses SVM and the termembeddings obtained by our learning approach.The input is a 3d-dimensional vector ?vx, vy,vx?
vy?, where d is the dimension of term em-beddings, x and y are two terms used to checkwhether x is a hypernym of y or not, and vx, vyare the term embeddings of x and y respec-tively.?
SVM+Word2Vec: This model uses SVM andthe term embeddings obtained by applying theSkip-gram model (Mikolov et al, 2013a) onthe entire English Wikipedia corpus.
The in-put is also a 3d-dimensional vector as in theSVM+Our model.
Note that the results of theSkip-gram model are word embeddings.
So if aterm is a multiword term, its embedding is cal-culated as the average of all words in the term.?
SVM+Yu: This model uses SVM and theterm embeddings obtained by using Yu et al?smethod (2015).
According to the best settingstated in (Yu et al, 2015), the input is a 2d+1dimensional vector ?O(x), E(y), ?O(x)-E(y)?1?,where O(x), E(y) and ?O(x)-E(y)?1 are hy-ponym embedding of x, hypernym embeddingof y and 1-norm distance of the vector (O(x)-E(y)) respectively.Parameter settings.
The SVM in the three modelsis trained using a RBF kernel with ?= 0.03125 andpenalty term C = 8.0.
For term embedding learning,the vector?s dimension is set to 100.
The tuning ofthe dimension will be discussed in Section 4.6.4.3 Performance on general domain datasetsFor the general domain datasets, we have conductedtwo experiments to evaluate the performance of ourproposed approach.Experiment 1.
For the BLESS dataset, we hold outone concept for testing and train on the remaining199 concepts.
The hold-out concept and its rela-tum constitute the testing set, while the remaining199 concepts and their relatum constitute the train-ing set.
To further separate the training and test-ing sets, we exclude from the training set any pair408of terms that has one term appearing in the testingset.
We report the average accuracy across all con-cepts.
For the ENTAILMENT dataset, we use thesame evaluation method: hold out one hypernym fortesting and train on the remaining hypernyms, andwe also report the average accuracy across all hy-pernyms.
Furthermore, to evaluate the effect of theoffset vector to taxonomic relation identification, wedeploy a setting that removes the offset vector in thefeature vectors of SVM.
Specifically, for SVM+Ourand SVM+Word2Vec, the input vector is changedfrom ?vx, vy, vx ?
vy?
to ?vx, vy?.
We use the sub-script short to denote this setting.Model Dataset AccuracySVM+Yu BLESS 90.4%SVM+Word2Vecshort BLESS 83.8%SVM+Word2Vec BLESS 84.0%SVM+Ourshort BLESS 91.1%SVM+Our BLESS 93.6%SVM+Yu ENTAIL 87.5%SVM+Word2Vecshort ENTAIL 82.8%SVM+Word2Vec ENTAIL 83.3%SVM+Ourshort ENTAIL 88.2%SVM+Our ENTAIL 91.7%Table 2: Performance results for the BLESS and ENTAIL-MENT datasets.Table 2 shows the performance of the three su-pervised models in Experiment 1.
Our approachachieves significantly better performance than Yu?smethod and Word2Vec method in terms of accu-racy (t-test, p-value < 0.05) for both BLESS andENTAILMENT datasets.
Specifically, our approachimproves the average accuracy by 4% compared toYu?s method, and by 9% compared to the Word2Vecmethod.
The Word2Vec embeddings have the worstresult because it is based only on co-occurrencebased similarity, which is not effective for the clas-sifier to accurately recognize all the taxonomic re-lations.
Our approach performs better than Yu?smethod and it shows that our approach can learn em-beddings more effectively.
Our approach encodesnot only hypernym and hyponym terms but also thecontextual information between them, while Yu?smethod ignores the contextual information for tax-onomic relation identification.Moreover, from the experimental results ofSVM+Our and SVM+Ourshort, we can observe thatthe offset vector between hypernym and hyponym,which captures the contextual information, plays animportant role in our approach as it helps to improvethe performance in both datasets.
However, the off-set feature is not so important for the Word2Vecmodel.
The reason is that the Word2Vec model istargeted for the analogy task rather than taxonomicrelation identification.Experiment 2.
This experiment aims to evaluate thegeneralization capability of our extracted term em-beddings.
In the experiment, we train the classifieron the BLESS dataset, test it on the ENTAILMENTdataset and vice versa.
Similarly, we exclude fromthe training set any pair of terms that has one termappearing in the testing set.
The experimental resultsin Table 3 show that our term embedding learningapproach performs better than other methods in ac-curacy.
It also shows that the taxonomic propertiesidentified by our term embedding learning approachhave great generalization capability (i.e.
less depen-dent on the training set), and can be used genericallyfor representing taxonomic relations.Model Training Testing AccuracySVM+Yu BLESS ENTAIL 83.7%SVM+Word2Vecshort BLESS ENTAIL 76.5%SVM+Word2Vec BLESS ENTAIL 77.1%SVM+Ourshort BLESS ENTAIL 85.8%SVM+Our BLESS ENTAIL 89.4%SVM+Yu ENTAIL BLESS 87.1%SVM+Word2Vecshort ENTAIL BLESS 78.0%SVM+Word2Vec ENTAIL BLESS 78.9%SVM+Ourshort ENTAIL BLESS 87.1%SVM+Our ENTAIL BLESS 90.6%Table 3: Performance results for the general domain datasetswhen using one domain for training and another domain fortesting.4.4 Performance on specific domain datasetsSimilarly, for the specific domain datasets, we haveconducted two experiments to evaluate the perfor-mance of our proposed approach.Experiment 3.
For each of the Animal, Plant andVehicle datasets, we also hold out one term for test-ing and train on the remaining terms.
The posi-tive and negative examples which contain the hold-out term constitute the testing set, while other pos-itive and negative examples constitute the training409set.
We also exclude from the training set any pairof terms that has one term appearing in the test-ing set.
The experimental results are given in Ta-ble 4.
We can observe that not only for general do-main datasets but also for specific domain datasets,our term embedding learning approach has achievedsignificantly better performance than Yu?s methodand the Word2Vec method in terms of accuracy (t-test, p-value < 0.05).
Specifically, our approach im-proves the average accuracy by 22% compared toYu?s method, and by 9% compared to the Word2Vecmethod.Model Dataset AccuracySVM+Yu Animal 67.8%SVM+Word2Vec Animal 80.2%SVM+Our Animal 89.3%SVM+Yu Plant 65.7%SVM+Word2Vec Plant 81.5%SVM+Our Plant 92.1%SVM+Yu Vehicle 70.5%SVM+Word2Vec Vehicle 82.1%SVM+Our Vehicle 89.6%Table 4: Performance results for the Animal, Plant and Vehicledatasets.Another interesting point to observe is that the ac-curacy of Yu?s method drops significantly in spe-cific domain datasets (as shown in Table 4) whencompared to the general domain datasets (as shownin Table 2).
One possible explanation is the accu-racy of Yu?s method depends on the training data.As Yu?s method learns the embeddings using pre-extracted taxonomic relations from Probase, and if arelation does not exist in Probase, there is high pos-sibility that it becomes a negative example and berecognized as a non-taxonomic relation by the clas-sifier.
Therefore, the training data extracted fromProbase plays an important role in Yu?s method.For general domain datasets (BLESS and ENTAIL-MENT), there are about 75%-85% of taxonomic re-lations in these datasets found in Probase, whilethere are only about 25%-45% of relations in thespecific domains (i.e.
Animal, Plant and Vehicle)found in Probase.
Therefore, Yu?s method achievesbetter performance in general domain datasets thanthe specific ones.
Our approach, in contrast, less de-pends on the training relations.
Therefore, it canachieve high accuracy in both the general and spe-cific domain datasets.Experiment 4.
Similar to experiment 2, this ex-periment aims to evaluate the generalization capa-bility of our term embeddings.
In this experiment,for each of the Animal, Plant and Vehicle domains,we train the classifier using the positive and nega-tive examples in each domain and test the classifierin other domains.
The experimental results in Table5 show that our approach achieves the best perfor-mance compared to other state-of-the-art methodsfor all the datasets.
As also shown in Table 3, our ap-proach has achieved high accuracy for both generaland specific domain datasets, while in Yu?s method,there is a huge difference in accuracy between thesedomain datasets.Model Training Testing AccuracySVM+Yu Animal Plant 65.5%SVM+Word2Vec Animal Plant 82.4%SVM+Our Animal Plant 91.9%SVM+Yu Animal Vehicle 66.2%SVM+Word2Vec Animal Vehicle 81.3%SVM+Our Animal Vehicle 89.5%SVM+Yu Plant Animal 68.4%SVM+Word2Vec Plant Animal 81.8%SVM+Our Plant Animal 91.5%SVM+Yu Plant Vehicle 65.2%SVM+Word2Vec Plant Vehicle 81.0%SVM+Our Plant Vehicle 88.5%SVM+Yu Vehicle Animal 70.9%SVM+Word2Vec Vehicle Animal 79.7%SVM+Our Vehicle Animal 87.6%SVM+Yu Vehicle Plant 66.2%SVM+Word2Vec Vehicle Plant 78.7%SVM+Our Vehicle Plant 87.7%Table 5: Performance results for the specific domain datasetswhen using one domain for training and another domain fortesting.4.5 Empirical comparison with WordNetBy error analysis, we found that our results maycomplement WordNet.
For example, in the Animaldomain, our approach identifies ?wild sheep?
as ahyponym of ?sheep?, but in WordNet, they are sib-lings.
However, many references 1, 2 consider ?wildsheep?
as a species of ?sheep?.
Another such ex-ample is shown in the Plant domain, where our ap-1http://en.wikipedia.org/wiki/Ovis2http://www.bjornefabrikken.no/side/norwegian-sheep/410proach recognizes ?lily?
as a hyponym of ?floweringplant?, but WordNet places them in different sub-trees incorrectly 3.
Therefore, our results may helprestructure and even extend WordNet.Note that these taxonomic relations are not inour training set.
They are also not recognized bythe term embeddings obtained from the Word2Vecmethod and Yu et al?s method.
It again shows thatour term embedding learning approach has the capa-bility to identify taxonomic relations which are noteven defined in dictionary or training data.4.6 Tuning vector dimensionsWe also conduct experiments to learn term embed-dings from the general domain datasets with differ-ent dimensions (i.e.
50, 100, 150 and 300) using ourproposed approach.
We then use these embeddingsto evaluate the performance of taxonomic relationidentification based on training time and accuracy,and show the results in Table 6.
The experimentsare carried out on a PC with Intel(R) Xeon(R) CPUat 3.7GHz and 16GB RAM.Dimension Dataset Training time Accuracy50 BLESS 1825s 87.7%100 BLESS 2991s 89.4%150 BLESS 4025s 89.9%300 BLESS 7113s 90.0%50 ENTAIL 1825s 88.5%100 ENTAIL 2991s 90.6%150 ENTAIL 4025s 90.9%300 ENTAIL 7113s 90.9%Table 6: Performance results based on training time and accu-racy of the SVM+Our model using different vector dimensions.In general, when increasing the vector dimension,the accuracy of our term embedding learning ap-proach will be increased gradually.
More specifi-cally, the accuracy improves slightly when the di-mension is increased from 50 to 150.
But after that,increasing the dimension has very little effect on theaccuracy.
We observe that the vector dimension forlearning term embeddings can be set between 100 to150 to achieve the best performance, based on thetrade-off between accuracy and training time.3https://en.wikipedia.org/wiki/Lilium5 ConclusionIn this paper, we proposed a novel approach to learnterm embeddings using dynamic weighting neuralnetwork.
This model encodes not only the hyper-nym and hyponym terms, but also the contextual in-formation between them.
Therefore, the extractedterm embeddings have good generalization capabil-ity to identify unseen taxonomic relations which arenot even defined in dictionary and training data.
Theexperimental results show that our approach signifi-cantly outperforms other state-of-the-art methods interms of accuracy in identifying taxonomic relationidentification.ReferencesMarco Baroni and Alessandro Lenci.
2011.
How weblessed distributional semantic evaluation.
Proceed-ings of the GEMS 2011 Workshop on GEometricalModels of Natural Language Semantics, pages 1?10.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, andChung-chieh Shan.
2012.
Entailment above the wordlevel in distributional semantics.
Proceedings of the13th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 23?32.Yoshua Bengio, Rjean Ducharme, and Pascal Vincent.2001.
A Neural Probabilistic Language Model.
Pro-ceedings of the NIPS conference, pages 932?938.David M. Blei, Thomas L. Griffiths, Michael I. Jor-dan, and Joshua B. Tenenbaum.
2004.
Hierarchicaltopic models and the nested chinese restaurant process.Advances in Neural Information Processing Systems,pages 17?24.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
Proceedings of the ACM SIGMOD Inter-national Conference on Management of Data, pages1247?1250.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Samah Fodeh, Bill Punch, and Pang N. Tan.
2011.
Onontology-driven document clustering using core se-mantic features.
Knowledge and information systems,28(2):395?421.Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, HaifengWang, and Ting Liu.
2014.
Learning semantic hierar-chies via word embeddings.
Proceedings of the 52ndAnnual Meeting of the ACL, pages 1199?1209.Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.Pasca.
2003.
Open-domain textual question an-411swering techniques.
Natural Language Engineering,9(3):231?267.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
Proceedings of the14th Conference on Computational Linguistics, pages539?545.Zornitsa Kozareva and Eduard Hovy.
2010.
ASemi-supervised Method to Learn and Construct Tax-onomies Using the Web.
Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1110?1118.German Kruszewski, Denis Paperno, and Marco Baroni.2015.
Deriving boolean structures from distributionalvectors.
Transactions of the Association for Computa-tional Linguistics, 3:375?388.Dawn J. Lawrie and W. Bruce Croft.
2003.
Generatinghierarchical summaries for web searches.
Proceedingsof the 26th ACM SIGIR conference, pages 457?463.Omer Levy, Steffen Remus, Chris Biemann, Ido Dagan,and Israel Ramat-Gan.
2014.
Do supervised distribu-tional methods really learn lexical inference relations.Proceedings of the NAACL conference, pages 1390?1397.Baichuan Li, Jing Liu, Chin Y. Lin, Irwin King, andMichael R. Lyu.
2013.
A Hierarchical Entity-basedApproach to Structuralize User Generated Content inSocial Media: A Case of Yahoo!
Answers.
Proceed-ings of the EMNLP conference, pages 1521?1532.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The stanford corenlp natural language process-ing toolkit.
Proceedings of the 52nd Annual Meetingof the ACL, pages 55?60.Cynthia Matuszek, John Cabral, Michael J. Witbrock,and John DeOliveira.
2006.
An introduction to thesyntax and content of cyc.
Proceedings of the AAAISpring Symposium, pages 44?49.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word rep-resentations in vector space.
arXiv preprint arX-iv:1301.3781.George A. Miller.
1995.
WordNet: a Lexical Databasefor English.
Communications of the ACM, 38(11):39?41.Roberto Navigli, Paola Velardi, and Stefano Faralli.2011.
A Graph-based Algorithm for Inducing LexicalTaxonomies from Scratch.
Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence,pages 1872?1877.Yves Petinot, Kathleen McKeown, and Kapil Thadani.2011.
A hierarchical model of web summaries.
Pro-ceedings of the 49th Annual Meeting of the ACL, pages670?675.Aurora Pons-Porrata, Rafael Berlanga-Llavori, and JoseRuiz-Shulcloper.
2007.
Topic discovery based on textmining techniques.
Information processing & man-agement, 43(3):752?768.Stephen Roller, Katrin Erk, and Gemma Boleda.
2014.Inclusive yet selective: Supervised distributional hy-pernymy detection.
Proceedings of the COLING con-ference, pages 1025?1036.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
Advances in Neural Information Process-ing Systems 17.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013a.
Parsing with composi-tional vector grammars.
Proceedings of the 51st An-nual Meeting of the ACL, pages 932?937.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng, andChristopher Potts.
2013b.
Recursive deep models forsemantic compositionality over a sentiment treebank.Proceedings of the EMNLP conference, pages 1631?1642.Liling Tan, Rohit Gupta, and Josef van Genabith.
2015.Usaar-wlv: Hypernym generation with deep neuralnets.
Proceedings of the SemEval, pages 932?937.Luu A. Tuan, Jung J. Kim, and See K. Ng.
2014.
Tax-onomy Construction using Syntactic Contextual Evi-dence.
Proceedings of the EMNLP conference, pages810?819.Luu A. Tuan, Jung J. Kim, and See K. Ng.2015.
Incorporating Trustiness and Collective Syn-onym/Contrastive Evidence into Taxonomy Construc-tion.
Proceedings of the EMNLP conference, pages1013?1022.Paola Velardi, Stefano Faralli, and Roberto Navigli.2013.
Ontolearn reloaded: A graph-based algorithmfor taxonomy induction.
Computational Linguistics,39(3):665?707.Chi Wang, Marina Danilevsky, Nihit Desai, Yinan Zhang,Phuong Nguyen, Thrivikrama Taula, and Jiawei Han.2013.
A phrase mining framework for recursive con-struction of a topical hierarchy.
Proceedings of the19th ACM SIGKDD conference, pages 437?445.Julie Weeds, Daoud Clarke, Jeremy Reffin, David J Weir,and Bill Keller.
2014.
Learning to distinguish hyper-nyms and co-hyponyms.
Proceedings of the COLINGconference, pages 2249?2259.Wu Wentao, Li Hongsong, Wang Haixun, and Kenny.
Q.Zhu.
2012.
Probase: A probabilistic taxonomy fortext understanding.
Proceedings of the ACM SIGMODconference, pages 481?492.Jianxing Yu, Zheng-Jun Zha, Meng Wang, Kai Wang, andTat-Seng Chua.
2011.
Domain-assisted product as-412pect hierarchy generation: towards hierarchical orga-nization of unstructured consumer reviews.
Proceed-ings of the EMNLP conference, pages 140?150.Zheng Yu, Haixun Wang, Xuemin Lin, and Min Wang.2015.
Learning term embeddings for hypernymy iden-tification.
Proceedings of the 24th International JointConference on Artificial Intelligence, pages 1390?1397.Xingwei Zhu, Zhao Y. Ming, and Tat-Seng Chua.
2013.Topic hierarchy construction for the organization ofmulti-source user generated contents.
Proceedings ofthe 36th ACM SIGIR conference, pages 233?242.Will Y Zou, Richard Socher, Daniel M. Cer, and Christo-pher D. Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
Proceedings ofthe EMNLP conference, pages 1393?1398.413
