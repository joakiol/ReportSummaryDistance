Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1427?1432,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDetecting Compositionality of Multi-Word Expressions using NearestNeighbours in Vector Space ModelsDouwe KielaUniversity of CambridgeComputer Laboratorydouwe.kiela@cl.cam.ac.ukStephen ClarkUniversity of CambridgeComputer Laboratorystephen.clark@cl.cam.ac.ukAbstractWe present a novel unsupervised approach todetecting the compositionality of multi-wordexpressions.
We compute the compositional-ity of a phrase through substituting the con-stituent words with their ?neighbours?
in a se-mantic vector space and averaging over thedistance between the original phrase and thesubstituted neighbour phrases.
Several meth-ods of obtaining neighbours are presented.The results are compared to existing super-vised results and achieve state-of-the-art per-formance on a verb-object dataset of humancompositionality ratings.1 IntroductionMulti-word expressions (MWEs) are defined as ?id-iosyncratic interpretations that cross word bound-aries?
(Sag et al 2002).
They tend to have astandard syntactic structure but are often semanti-cally non-compositional; i.e.
their meaning is notfully determined by their syntactic structure and themeanings of their constituents.
A classic exampleis kick the bucket, which means to die rather than tohit a bucket with the foot.
These types of expres-sions account for a large proportion of day-to-daylanguage interactions (Schuler and Joshi, 2011) andpresent a significant problem for natural languageprocessing systems (Sag et al 2002).This paper presents a novel unsupervised ap-proach to detecting the compositionality of MWEs,specifically of verb-noun collocations.
The idea isthat we can recognize compositional phrases by sub-stituting related words for constituent words in thephrase: if the result of a substitution yields a mean-ingful phrase, its individual constituents are likely tocontribute toward the overall meaning of the phrase.Conversely, if a substitution yields a non-sensicalphrase, its constituents are likely to contribute lessor not at all to the overall meaning of the phrase.For the phrase eat her hat, for example, we mightconsider the following substituted phrases:1. consume her hat2.
eat her trousersBoth phrases are semantically anomalous, implyingthat eat hat is a highly non-compositional verb-nouncollocation.
Following a similar procedure for eatapple, however, would not lead to an anomaly: con-sume apple and eat pear are perfectly meaningful,leading us to believe that eat apple is compositional.In the context of distributional models, this ideacan be formalised in terms of vector spaces:the average distance between a phrasevector and its substituted phrase vectors isrelated to its compositionality.Since we are relying on the relative distances ofphrases in semantic space, we require a methodfor computing vectors for phrases.
We experi-mented with a number of composition operatorsfrom Mitchell and Lapata (2010), in order to com-pose constituent word vectors into phrase vectors.The relation between phrase vectors and substitutedphrase vectors is most pronounced in the case of1427pointwise multiplication, which has the effect ofplacing semantically anomalous phrases relativelyclose together in space (since the vectors for the con-stituent words have little in common), whereas thesemantically meaningful phrases are further apart.This implies that compositional phrases are less sim-ilar to their neighbours, which is to say that thegreater the average distance between a phrase vec-tor and its substituted phrase vectors, the greater itscompositionality.The contribution of this short focused research pa-per is a novel approach to detecting the composition-ality of multi-word expressions that makes full useof the ability of semantic vector space models to cal-culate distances between words and phrases.
Usingthis unsupervised approach, we achieve state-of-the-art performance in a direct comparison with existingsupervised methods.2 Dataset and VectorsThe verb-noun collocation dataset from Venkatapa-thy and Joshi (2005), which consists of 765 verb-object pairs with human compositionality ratings,was used for evaluation.
Venkatapathy & Joshi useda support vector machine (SVM) to obtain a Spear-man ?s correlation of 0.448.
They employed a va-riety of features ranging from frequency to LSA-derived similarity measures and used 10% of thedataset as training data with tenfold cross-validation.McCarthy et al(2007) used the same dataset and ex-panded on the original approach by adding WordNetand distributional prototypes to the SVM, achievinga ?s correlation of 0.454.The distributional vectors for our experimentswere constructed from the ukWaC corpus (Baroniet al 2009).
Vectors were obtained using a stan-dard window method (with a window size of 5) andthe 50,000 most frequent context words as features,with stopwords removed.
We also experimentedwith syntax-based co-occurrence features extractedfrom a dependency-parsed version of ukWaC, butin agreement with results obtained by Schulte imWalde et al(2013) for predicting compositional-ity in German, the window-based co-occurrencemethod produced better results.We tried several weighting schemes from the liter-ature, such as t-test (Curran, 2004), positive mutualinformation (Bullinaria and Levy, 2012) and the ra-tio of the probability of the context word given thetarget word1 to the context word?s overall probabil-ity (Mitchell and Lapata, 2010).
We found that atf-idf variant called LTU yielded the best results, de-fined as follows (Reed et al 2006):wij =(log(fij) + 1.0) log(Nnj )0.8 + 0.2?
|context word||avg context word|where fij is the number of times that the target wordand context word co-occur in the same window, njis the context word frequency, N is the total fre-quency and |context word| is the total number of oc-currences of a context word.
Distance is calculatedusing the standard cosine measure:dist(v1, v2) = 1?v1 ?
v2|v1||v2|where v1 and v2 are vectors in the semantic vectorspace model.3 Finding Neighbours and ComputingCompositionalityWe experimented with two different ways of obtain-ing neighbours for the constituent words in a phrase.Since vector space models lend themselves naturallyto similarity computations, one way to get neigh-bours is to take the k-most similar vectors from asimilarity matrix.
This approach is straightforward,but has some potential drawbacks: it assumes thatwe have a large number of vectors to select neigh-bours from, and becomes computationally expensivewhen the number of neighbours is increased.An alternative source for obtaining neighbours isthe lexical database WordNet (Fellbaum, 1998).
Wedefine neighbours as siblings in the hypernym hier-archy, so that the neighbours of a word can be foundby taking the hyponyms of its hypernyms.
Word-Net al allows us to extract only neighbours of thesame grammatical type (yielding noun neighboursfor nouns and verb neighbours for verbs, for exam-ple).
Since not every word has the same numberof neighbours in WordNet, we use only the first k1We use target word to refer to the word for which a vectoris being constructed.1428neighbours, which means that the neighbours haveto be ranked.
An obvious ranking method is to usethe frequency with which each neighbour co-occurswith the other constituent(s) of the same phrase.
Forexample, for all the WordNet neighbours of eat (forall senses of eat), we count the co-occurrences withhat in a given window size and rank them accord-ingly.
This ranking method also has the desirableside-effect of performing some word sense disam-biguation, at least in some cases.
For example, thehighly ranked neighbours of apple for eat apple arelikely to be items of food, and not (inedible) trees(apple is also a tree in WordNet).In order to obtain frequency-ranked neighbours,we used the ukWaC corpus with a window size of5.
One reason for having multiple neighbours is thatit allows us to correct for word sense disambigua-tion errors (as mentioned above), since averagingover results for several neighbours reduces the im-pact of including incorrect senses.
For example, thefirst 20 neighbours of eat, ranked by co-occurrencefrequency with all the objects of eat in the dataset,are:eat use consume drink sample smokeswallow spend break hit save afford burnpartake dine breakfast worry damage de-plete drugOne problem with the evaluation dataset is thatit does not solely consist of verb-noun pairs: 84phrases contain pronouns, while there are also sev-eral examples containing words that WordNet con-siders to be adjectives rather than nouns.
This prob-lem was mitigated by part-of-speech tagging thedataset.
As neighbours for pronouns (which are notincluded in WordNet), we used the other pronounspresent in the dataset.
For the remaining words,we included the part-of-speech when looking up theword in WordNet.3.1 Average distance compositionality scoreWe considered several different ways of construct-ing phrasal vectors.
We chose not to use the com-positional models of Baroni and Zamparelli (2010)and Socher et al(2011) because we believe that it isimportant that our methods are completely unsuper-vised and do not require any initial learning phase.Hence, we experimented with different ways of con-structing phrasal vectors according to Mitchell andLapata (2010) and found that pointwise multiplica-tion  worked best in our experiments.
Thus, wedefine the composed vector????
?eat hat as:??eat?
?hatWe can now compute a compositionality score sc byaveraging the distance between the original phrasevector and its substituted neighbour phrase vectorsvia the following formula:sc(????
?eat hat) =12k(k?i=1dist(??eat??hat,??eat???????
?neighbouri) +k?j=1dist(??eat??hat,????????neighbourj?
?hat))We also experimented with substituting only forthe noun or the verb, and in fact found that only tak-ing neighbours for the verb yields better results:sc(????
?eat hat) =1kk?j=1dist(??eat??hat,????????neighbourj?
?hat)To illustrate the method, consider the collocationstake breath and lend money.
The annotators as-signed these phrases a compositionality score of 1out of 6 and 6 out of 6, respectively, meaning that theformer is non-compositional and the latter is com-positional.
The distances between the first ten verb-substituted phrases and the original phrase, togetherwith the average distance, are shown in Table 1 andTable 2.Substituting the verb in the non-compositionalphrase yields semantically anomalous vectors,which leads to very small changes in the distancebetween it and the original phrase vector.
This is aresult of using pointwise multiplication, where over-lapping components are stressed: since the vectorsfor take and breath have little overlap outside of1429Neighbour Distget breath 0.049find breath 0.051use breath 0.050work breath 0.060hold breath 0.094run breath 0.079carry breath 0.076look breath 0.065play breath 0.071buy breath 0.100AvgDist 0.069Table 1: Example take breathNeighbour Distpay money 0.446put money 0.432bring money 0.405provide money 0.442owe money 0.559sell money 0.404cost money 0.482look money 0.425distribute money 0.544offer money 0.428AvgDist 0.457Table 2: Example lend moneythe idiomatic sense in take breath, its neighbour-substituted phrases also have little overlap, result-ing in a smaller change in distance upon substitu-tion.
Conversely, substituting the verb in the com-positional phrase yields meaningful vectors, puttingthem in locations in semantic vector space which aresufficiently far apart to distinguish them from thenon-compositional cases.4 ResultsResults are given for the two methods of obtainingneighbours: via frequency-ranked WordNet neigh-bours and via vector space neighbours.
The com-positionality score was computed by using only theverb, only the noun, or both constituent neighboursin the substituted phrase vectors.System ?sVenkatapathy and Joshi (2005) 0.447McCarthy et al(2007) 0.454AvgDist VSM neighbours-both 0.131AvgDist VSM neighbours-verb 0.420AvgDist VSM neighbours-noun 0.245AvgDist WN-ranked neighbours-both 0.165AvgDist WN-ranked neighbours-verb 0.461AvgDist WN-ranked neighbours-noun 0.169Table 3: Spearman ?s resultsThe results are compared with the scores reportedin Venkatapathy and Joshi (2005) and McCarthy etal.
(2007), which were achieved using SVMs with awide variety of features.
Values of 1 ?
k ?
20 weretried.
If a phrase has fewer than k neighbours be-cause not enough neighbours have been found to co-occur with the other constituent, we use all of them.The results for k = 20 are reported here becausethat gave the best overall score.
The dataset has aninter-annotator agreement of Kendall?s ?
of 0.61 anda Spearman ?s of 0.71 and all reported differencesin values are highly significant.
Table 3 gives theresults.Note that, even though the current approach is un-supervised (in terms of not having access to compo-sitionality ratings during training, although it doesrely on WordNet), it outperforms SVMs that requirean ensemble of complex feature sets (some of whichare also based on WordNet).It is interesting to observe that the state-of-the-artperformance is reached when only using the verb?sneighbours to compute substituted phrase vectors.One might initially expect this not to be the case,since e.g.
eat trousers, where the noun has beensubstituted, does not make a lot of sense either ?which we would expect to be informative for de-termining compositionality.
There are two possi-ble explanations for this, which might be at playsimultaneously: since our dataset consists of verb-object pairs, the verb constituent is always the headword of the phrase, and the dataset contains severalso-called ?light verbs?, which have little semanticcontent of their own.
Head words have been foundto have a higher impact on compositionality scoresfor compound nouns: Reddy et al(2011) weighted1430the contribution of individual constituents in such away that the modifier?s contribution is included butis weighted less highly than the head?s contribution,which led to an improvement in performance.
Ourresults might be improved by weighting the contri-bution of constituent words in a similar fashion, andby more closely examining the impact of light verbsfor the compositionality of a phrase.5 Related WorkThe past decade has seen extensive work on compu-tational and statistical methods in detecting the com-positionality of MWEs (Lin, 1999; Schone and Ju-rafsky, 2001; Katz and Giesbrecht, 2006; Sporlederand Li, 2009; Biemann and Giesbrecht, 2011).Many of these methods rely on distributional mod-els and vector space models (Schu?tze, 1993; Tur-ney and Pantel, 2010; Erk, 2012).
Work has beendone on different types of phrases, including workon particle verbs (McCarthy et al 2003; Bannardet al 2003), verb-noun collocations (Venkatapathyand Joshi, 2005; McCarthy et al 2007), adjective-noun combinations (Vecchi et al 2011) and noun-noun compounds (Reddy et al 2011), as well as onlanguages other than English (Schulte im Walde etal., 2013).
Recent developments in distributionalcompositional models (Widdows, 2008; Mitchelland Lapata, 2010; Baroni and Zamparelli, 2010; Co-ecke et al 2010; Socher et al 2011) have openedup a number of possibilities for constructing vectorsfor phrases, which have also been applied to com-positionality tests (Giesbrecht, 2009; Kochmar andBriscoe, 2013).This paper takes that work a step further: by con-structing phrase vectors and evaluating these vectorson a dataset of human compositionality ratings, weshow that existing compositional models allow us todetect compositionality of multi-word expressionsin a straightforward and intuitive manner.6 ConclusionWe have presented a novel unsupervised approachthat can be used to detect the compositionality ofmulti-word expressions.
Our results show that theunderlying intuition appears to be sound: substitut-ing neighbours may lead to meaningful or meaning-less phrases depending on whether or not the phraseis compositional.
This can be formalized in vec-tor space models to obtain compositionality scoresby computing the average distance to the originalphrase?s substituted neighbour phrases.
In this shortfocused research paper, we show that, depending onhow we obtain neighbours, we are able to achievea higher performance than that achieved by super-vised methods which rely on a complex feature setand support vector machines.AcknowledgmentsThis work has been supported by EPSRC grantEP/I037512/1.
The authors would like to thank Di-ana McCarthy for providing the dataset; and EdGrefenstette, Eva Maria Vecchi, Laura Rimell andTamara Polajnar and the anonymous reviewers fortheir helpful comments.ReferencesColin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics of verb-particles.
In Proceedings of the ACL 2003 Workshopon Multiword expressions: analysis, acquisition andtreatment, MWE 03.M.
Baroni and R. Zamparelli.
2010.
Nouns are vectors,adjectives are matrices: Representing adjective-nounconstructions in semantic space.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?10, pages 1183?1193.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Chris Biemann and Eugenie Giesbrecht.
2011.
Disco-11: Proceedings of the workshop on distributional se-mantics and compositionality.John A. Bullinaria and Joseph P. Levy.
2012.
ExtractingSemantic Representations from Word Co-occurrenceStatistics: Stop-lists, Stemming and SVD.
BehaviorResearch Methods, 44:890?907.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a compositionaldistributional model of meaning.
In J. van Bentham,M.
Moortgat, and W. Buszkowski, editors, LinguisticAnalysis (Lambek Festschrift), volume 36, pages 345?384.James Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.1431Katrin Erk.
2012.
Vector space models of word meaningand phrase meaning: A survey.
Language and Lin-guistics Compass, 6(10):635?653.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Eugenie Giesbrecht.
2009.
In search of semantic com-positionality in vector spaces.
In Sebastian Rudolph,Frithjof Dau, and SergeiO.
Kuznetsov, editors, Con-ceptual Structures: Leveraging Semantic Technolo-gies, volume 5662 of Lecture Notes in Computer Sci-ence, pages 173?184.
Springer Berlin Heidelberg.Graham Katz and Eugenie Giesbrecht.
2006.
Automaticidentification of non-compositional multi-word ex-pressions using latent semantic analysis.
In Proceed-ings of the Workshop on Multiword Expressions: Iden-tifying and Exploiting Underlying Properties, MWE?06, pages 12?19.Ekaterina Kochmar and Ted Briscoe.
2013.
CapturingAnomalies in the Choice of Content Words in Com-positional Distributional Semantic Space.
In RecentAdvances in Natural Language Processing.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of the 37thannual meeting of the Association for ComputationalLinguistics on Computational Linguistics, ACL ?99,pages 317?324.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a continuum of compositionality in phrasalverbs.
In Proceedings of the ACL 2003 workshopon Multiword expressions: analysis, acquisition andtreatment - Volume 18, MWE ?03, pages 73?80.Diana McCarthy, Sriram Venkatapathy, and AravindJoshi.
2007.
Detecting compositionality of verb-object combinations using selectional preferences.
InProceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 369?379.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011.
An empirical study on compositionality in com-pound nouns.
In Proceedings of The 5th Interna-tional Joint Conference on Natural Language Process-ing 2011 (IJCNLP 2011), Thailand.J.W.
Reed, Y. Jiao, T.E.
Potok, B.A.
Klump, M.T.
El-more, and A.R.
Hurson.
2006.
TF-ICF: A new termweighting scheme for clustering dynamic data streams.In Machine Learning and Applications, 2006.
ICMLA?06.
5th International Conference on, pages 258?263.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.Copestake, and Dan Flickinger.
2002.
Multiword ex-pressions: A Pain in the Neck for NLP.
In Proceed-ings of the Third International Conference on Com-putational Linguistics and Intelligent Text Processing,CICLing ?02, pages 1?15.Patrick Schone and Daniel Jurafsky.
2001.
Isknowledge-free induction of multiword unit dictionaryheadwords a solved problem?
In Proceedings ofEmpirical Methods in Natural Language Processing,EMNLP ?01.William Schuler and Aravind K. Joshi.
2011.
Tree-rewriting models of multi-word expressions.
In Pro-ceedings of the Workshop on Multiword Expressions:from Parsing and Generation to the Real World, MWE?11, pages 25?30.Sabine Schulte im Walde, Stefan Mu?ller, and StephenRoller.
2013.
Exploring Vector Space Models toPredict the Compositionality of German Noun-NounCompounds.
In Proceedings of the 2nd Joint Confer-ence on Lexical and Computational Semantics, pages255?265, Atlanta, GA.Hinrich Schu?tze.
1993.
Word space.
In Advances inNeural Information Processing Systems 5, pages 895?902.
Morgan Kaufmann.Richard Socher, Cliff Lin, Andrew Y. Ng, and Christo-pher D. Manning.
2011.
Parsing Natural Scenesand Natural Language with Recursive Neural Net-works.
In The 28th International Conference on Ma-chine Learning, ICML 2011.Caroline Sporleder and Linlin Li.
2009.
2009. unsuper-vised recognition of literal and non-literal use of id-iomatic expressions.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL, EACL?09.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: vector space models of semantics.J.
Artif.
Int.
Res., 37(1):141?188, January.Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-elli.
2011.
(linear) maps of the impossible: Capturingsemantic anomalies in distributional space.
In Pro-ceedings of the Workshop on Distributional Seman-tics and Compositionality, pages 1?9, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Sriram Venkatapathy and Aravind K. Joshi.
2005.
Mea-suring the relative compositionality of verb-noun (v-n)collocations by integrating features.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,HLT ?05, pages 899?906.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Second AAAI Sympo-sium on Quantum Interaction, Oxford.1432
