Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 788?793,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsPredicting Valence-Arousal Ratings of Words Using a WeightedGraph MethodLiang-Chih Yu1,3, Jin Wang2,3,4, K. Robert Lai2,3 and Xue-jie Zhang41Department of Information Management, Yuan Ze University, Taiwan2Department of Computer Science & Engineering, Yuan Ze University, Taiwan3Innovation Center for Big Data and Digital Convergence Yuan Ze University, Taiwan4School of Information Science and Engineering, Yunnan University, Yunnan, P.R.
ChinaContact: lcyu@saturn.yzu.edu.twAbstractCompared to the categorical approachthat represents affective states as severaldiscrete classes (e.g., positive and nega-tive), the dimensional approach repre-sents affective states as continuous nu-merical values on multiple dimensions,such as the valence-arousal (VA) space,thus allowing for more fine-grained sen-timent analysis.
In building dimensionalsentiment applications, affective lexiconswith valence-arousal ratings are usefulresources but are still very rare.
There-fore, this study proposes a weightedgraph model that considers both the rela-tions of multiple nodes and their similari-ties as weights to automatically deter-mine the VA ratings of affective words.Experiments on both English and Chi-nese affective lexicons show that theproposed method yielded a smaller errorrate on VA prediction than the linear re-gression, kernel method, and pagerankalgorithm used in previous studies.1 IntroductionThanks to the vigorous development of onlinesocial network services, anyone can now easilypublish and disseminate articles expressing theirthoughts and opinions.
Sentiment analysis thushas become a useful technique to automaticallyidentify affective information from texts (Pangand Lee, 2008; Calvo and D'Mello, 2010; Liu,2012; Feldman, 2013).
In sentiment analysis,representation of affective states is an essentialissue and can be generally divided into categori-cal and dimensional approaches.The categorical approach represents affectivestates as several discrete classes such as binary(positive and negative) and Ekman?s six basicemotions (e.g., anger, happiness, fear, sadness,disgust and surprise) (Ekman, 1992).
Based onthis representation, various techniques have beeninvestigated to develop useful applications suchas deceptive opinion spam detection (Li et al.,2014), aspect extraction (Mukherjee and Liu,2012), cross-lingual portability (Banea et al.,2013; Xu et al., 2015), personalized sentimentanalysis (Ren and Wu, 2013; Yu et al., 2009) andviewpoint identification (Qiu and Jiang, 2013).In addition to identifying sentiment classes, anextension has been made to further determinetheir sentiment strength in terms of a multi-pointscale (Taboada et al., 2011; Li et al., 2011; Yu etal., 2013; Wang and Ester, 2014).The dimensional approach has drawn consid-erable attention in recent years as it can provide amore fine-grained sentiment analysis.
It repre-sents affective states as continuous numericalvalues on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown inFigure 1.
The valence represents the degree ofpleasant and unpleasant (or positive and negative)feelings, and the arousal represents the degree ofexcitement and calm.
Based on such a two-dimensional representation, a common researchgoal is to determine the degrees of valence andarousal of given texts such that any affectivestate can be represented as a point in the VA co-ordinate plane.
To accomplish this goal, affectivelexicons with valence-arousal ratings are usefulresources but few exist.
Most existing applica-tions rely on a handcrafted lexicon ANEW (Af-788fective Norms for English Words) (Bradley,1999) which provides 1,034 English words withratings in the dimensions of pleasure, arousal anddominance to predict the VA ratings of short andlong texts (Paltoglou et al, 2013; Kim et al.,2010).
Accordingly, the automatic prediction ofVA ratings of affective words is a critical task inbuilding a VA lexicon.Few studies have sought to predict the VA rat-ing of words using regression-based methods(Wei et al., 2011; Malandrakis et al., 2011).
Thiskind of method usually starts from a set of wordswith labeled VA ratings (called seeds).
The VArating of an unseen word is then estimated fromsemantically similar seeds.
For instance, Wei etal.
(2011) trained a linear regression model foreach seed cluster, and then predicted the VA rat-ing of an unseen word using the model of thecluster to which the unseen word belongs.Malandrakis et al.
(2011) used a kernel functionto combine the similarity between seeds and un-seen words into a linear regression model.
In-stead of estimating VA ratings of words, anotherdirection is to determine the polarity (i.e., posi-tive and negative) of words by applying the labelpropagation (Rao and Ravichandran, 2009; Has-san et al., 2011) and pagerank (Esuli et al., 2007)on a graph.
Based on these methods, the polarityof an unseen word can be determined/rankedthrough its neighbor nodes (seeds).Although the pagerank algorithm has beenused for polarity ranking, it can still be extendedfor VA prediction.
Therefore, this study extendsthe idea of pagerank in two aspects.
First, weimplement pagerank for VA prediction by trans-forming ranking scores into VA ratings.
Second,whereas pagerank assigns an equal weight to theedges connected between an unseen word and itsneighbor nodes, we consider their similarities asweights to construct a weighted graph such thatneighbor nodes more similar to the unseen wordmay contribute more to estimate its VA ratings.That is, the proposed weighted graph model con-siders both the relations of multiple nodes andthe similarity weights among them.
In experi-ments, we evaluate the performance of the pro-posed method against the linear regression, ker-nel method, and pagerank algorithm on bothEnglish and Chinese affective lexicons for VAprediction.The rest of this paper is organized as follows.Section 2 describes the proposed weighted graphmodel.
Section 3 summarizes the comparativeresults of different methods for VA prediction.Conclusions are finally drawn in Section 4.2 Graph Model for VA PredictionBased on the theory of link analysis, the rela-tions between unseen words and seed words canbe considered as a graph, as shown in Figure 2.The valence-arousal ratings of each unseen wordcan then be predicted through the links connect-ed to the seed words to which it is similar usingtheir similarities as weights.
To measure the sim-ilarity between words (nodes), we use theword2vec toolkit (Mikolov et al., 2013) providedby Google (http://code.google.com/p/word2vec/).The formal definition of a graph model is de-scribed as follows.
Let G=(V, E) be an undirectedgraph, where V denotes a set of words and E de-notes a set of undirected edges.
Each edge e in Edenotes a relation between word vi and word vj inV (1?i, j?n, i?j), representing the similarity be-tween them.
For each node vi,( ) { | ( , ) }i j j iN v v v v E?
?
denotes the set of itsneighbor nodes, representing a set of words towhich it is similar.
The valence or arousal of vi,denoted as ivval or ivaro , can then be determinedby its neighbors, defined as( )( )( , )(1 ) ( , )jj ii ij ii j vv N vv vi jv N vSim v v valval val Sim v v?
?????
?
?
?
??
,(1)where ?
is a decay factor or a confidence levelfor computation (a constant between 0 and 1),which limits the effect of rank sinks to guaranteeconvergence to a unique vector.
Initially, the va-lence (or arousal) of each unseen word is as-signed to a random value that between 0 and 10.Later, it is iteratively updated using the followingformula,Figure 1.
Two-dimensional valence-arousalspace.7891( )1( )( 0)( , )(1 ) ( 0)( , )jj iij iiti j vv N vtvi jv N vtvRandomValue tSim v v valval val tSim v v?
????????
?
?
?
??????????
(2)where t denotes the t-th iteration.
It is worth not-ing that the valence (or arousal) of the seedwords is a constant in each iterative step.
Basedon this, the valence (or arousal) of each unseenword is propagated through the graph in multipleiterations until convergence.To improve the efficiency of the iterativecomputation, Eq.
(2) can be transformed into amatrix notation.
Suppose that the vectors,1 1( , , , )N Tv v vval val val?V ?
,1 1( , , , )N Tv v varo aro aro?A ?are the vectors of the valence-arousal rating of allwords (including seed words and unseen words).Matrix1 1 1 111( , ) ( , ) ( , )( , ) ( , ) ( , )( , ) ( , ) ( , )j Ni i j i NN N j N NSim v v Sim v v Sim v vSim v v Sim v v Sim v vSim v v Sim v v Sim v v??
??
??
??
??
??
??
??
?S?
??
?
??
??
?
??
?is the adjacency matrix of each words, whereSim(vi, vj) represents the similarity betweenwords i and j, where i, j=1, 2, ?, N, i ?
j.Given two other vectors  (1,1, ,1)T?I ?
and1 2( , , , )TNd d d?D ?
, where0iiiif noded if node?
???
?
?
?candseed ,?
is the previously mentioned decay factor.
Forvectors 1 2( , , , )TNa a a?A ?
and 1 2( , , , )TNb b b?B ?
,function ( )A,B?
and ( )A,B?
can be defined as1 1 2 2( ) ( , , , )TN Na b a b a b?
?
?
?A,B ??
,1 1 2 2( ) ( / , / , , / )TN Na b a b a b?A, B ??
.Then, Eq.
(2) can be turned into the followingmatrix format.1 1[( ) , ] [ , ( )]T Tt t t?
??
?
?V I - D V D SV ,S I?
?
?
,1 1[( ) , ] [ , ( )]T Tt t t?
??
?
?A I - D A D SA ,S I?
?
?
(3)Through the transformation of matrix multi-plication, the computation of VA prediction canconverge within only a few iterations.3 Experimental ResultsData.
This experiment used two affective lexi-cons with VA ratings: 1) ANEW which contains1,034 English affective words (Bradley, 1999)and 2) 162 Chinese affective words (CAW) tak-en from (Wei et al., 2011).
Both lexicons wereused for 5-fold cross-validation.
That is, for eachrun, 80% of the words in the lexicons were con-sidered as seeds and the remaining 20% wereused as unseen words.
The similarities betweenEnglish words and between Chinese words werecalculated using the word2vec toolkit trainedwith the respective English and Chinese wikicorpora (https://dumps.wikimedia.org/).Implementation Details.
Two regression-basedmethods were used for comparison:  linear re-gression (Wei et al., 2011) and the kernel method(Malandrakis et al., 2011), along with two graph-based methods: pagerank (Esuli et al., 2007) andthe proposed weighted graph model.
For bothregression-based methods, the similarities andVA ratings of the seed words were used for train-ing, and the VA ratings of an unseen word werepredicted by taking as input its similarity to theseeds.
In addition, for the kernel method, the lin-ear similarity function was chosen because ityielded top performance.
Both graph-basedmethods used an iterative procedure for VA pre-diction and required no training.
For pagerank,the iterative procedure was implemented usingthe algorithm presented in (Esuli et al., 2007),which estimates the VA ratings of an unseenword by assigning an equal weight to the edgesconnected to its neighbor seeds.
For the proposedmethod, the iterative procedure was implementedby considering the word similarity as weights.seed unseenunseenseedunseenseedseedsimilaritysimilaritysimilaritysimilarity similaritysimilaritysimilarityFigure 2.
Conceptual diagram of a weightedgraph model for VA prediction.790Evaluation Metrics.
The prediction perfor-mance was evaluated by examining the differ-ence between the predicted values of VA ratingsand the corresponding actual values in theANEW and CAW lexicons.
The evaluation met-rics included:?
Root mean square error (RMSE)?
?21ni iiRMSE A P n??
???
Mean absolute error (MAE)11 | |ni iiMAE A Pn ??
??
,?
Mean absolute percentage error (MAPE)11 100%n i ii iA PMAPE n A???
?
?where Ai is the actual value, Pi is the predictedvalue, and n is the number of test samples.
Alower MAPE, MAE or RMSE value indicatesmore accurate forecasting performance.Iterative Results of the Graph-based Methods.Figure 3 uses RMSE as an example to show theiterative results of the pagerank and proposedmethods.
The results show that the performanceof both methods stabilized after around 10 itera-tions, indicating its efficiency for VA prediction.Another observation is that the ultimate converg-ing result of each word is unrelated to the decayfactor and the initial random assignment.Comparative Results.
Table 1 compares theresults of the regression-based methods (LinearRegression and Kernel) and graph-based meth-ods (PageRank and Weighted Graph).
The per-formance of PageRank and Weighted Graph wastaken from results of the 50th iteration.
The re-sults show that both graph-based methods out-performed the regression-based methods for allmetrics.
For the graph-based methods, the pro-posed Weighted Graph yielded better MAPE per-formance than PageRank (around 4%), Kernel(around 8%) and Linear Regression (around 7%)on both the ANEW and CAW corpora.
Theweighted graph model achieved better perfor-mance because it predicted VA ratings by con-sidering both the relations of multiple nodes andthe weights between them.
For the regression-based methods, both Linear Regression and Ker-nel achieved similar results.
Another observationis that the arousal prediction error is greater thanthat for the valence prediction, indicating that thearousal dimension is more difficult to predict.Valence ANEW (English) CAW (Chinese) RMSE MAE MAPE(%) RMSE MAE MAPE(%)Weighted Graph 1.122 0.812 11.51 1.224 0.904 13.03PageRank 1.540 1.085 15.69 1.642 1.187 16.84Kernel 1.926 1.385 19.55 2.028 1.426 20.57Linear Regression 1.832 1.301 18.61 1.935 1.393 19.66Arousal ANEW (English) CAW (Chinese) RMSE MAE MAPE(%) RMSE MAE MAPE(%)Weighted Graph 1.203 0.894 12.24 1.311 0.966 13.37PageRank 1.627 1.149 16.48 1.735 1.238 17.51Kernel 2.007 1.419 20.27 2.118 1.434 21.44Linear Regression 1.912 1.382 19.33 2.020 1.421 20.46Table 1.
Comparative results of different methods in VA prediction.Figure 3.
Iterative results of the pagerank algo-rithm and weighted graph model.7914  ConclusionThis study presents a weighted graph model topredict valence-arousal ratings of words whichcan be used for lexicon augmentation in the va-lence and arousal dimensions.
Unlike the equalweight used in the traditional pagerank algorithm,the proposed method considers the similaritiesbetween words as weights such that the neighbornodes more similar to the unseen word may con-tribute more to VA prediction.
Experiments onboth English and Chinese affective lexiconsshow that the proposed method yielded a smallererror rate than the pagerank, kernel and linearregression methods.
Future work will focus onextending the VA prediction from the word-levelto the sentence- and document-levels.AcknowledgmentsThis work was supported by the Ministry of Sci-ence and Technology, Taiwan, ROC, underGrant No.
NSC102-2221-E-155-029-MY3.
Theauthors would like to thank the anonymous re-viewers and the area chairs for their constructivecomments.ReferenceCarmen Banea, Rada Mihalcea, and JanyceWiebe.
2013.
Porting multilingual subjectivityresources across languages.
IEEE Trans.
Af-fective Computing, 4(2):211-225.Margaret M. Bradley and Peter J. Lang.
1999.Affective norms for English words (ANEW):Instruction manual and affective ratings.Technical Report C-1, The Center for Re-search in Psychophysiology, University ofFlorida.Rafael A. Calvo and Sidney D'Mello.
2010.
Af-fect detection: An interdisciplinary review ofmodels, methods, and their applications.
IEEETrans.
Affective Computing, 1(1): 18-37.Paul Ekman.
1992.
An argument for basic emo-tions.
Cognition and Emotion, 6:169-200.Andrea Esuli and Fabrizio Sebastiani.
2007.
Pag-eranking wordnet synsets: An application toopinion mining.
In Proceedings of the 45thAnnual Meeting of the Association for Compu-tational Linguistics (ACL-07), pages 424-431.Ronen Feldman.
2013.
Techniques and applica-tions for sentiment analysis.
Communicationsof the ACM, 56(4):82-89.Ahmed Hassan, Amjad Abu-Jbara, Rahul Jha,Dragomir Radev.
2011.
Identifying the seman-tic orientation of foreign words.
In Proceed-ings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics (ACL-11),pages 592-597.Sunghwan Mac Kim, Alessandro Valitutti, andRafael A. Calvo.
2010.
Evaluation of unsuper-vised emotion models to textual affect recog-nition.
In Proc.
of Workshop on Computation-al Approaches to Analysis and Generation ofEmotion in Text, pages 62-70.Fangtao Li, Nathan Liu, Hongwei Jin, Kai Zhao,Qiang Yang, and Xiaoyan Zhu.
2011.
Incorpo-rating reviewer and product information forreview rating prediction.
In Proceedings of the22nd International Joint Conference on Artifi-cial Intelligence (IJCAI-11), pages 1820-1825.Jiwei Li, Myle Ott, Claire Cardie, and EduardHovy.
2014.
Towards a general rule for identi-fying deceptive opinion spam.
In Proceedingsof the 52nd Annual Meeting of the Associationfor Computational Linguistics (ACL-14), pag-es 1566-1576.Bing Liu.
2012.
Sentiment Analysis and OpinionMining.
Morgan & Claypool, Chicago, IL.Nikos Malandrakis, Alexandros Potamianos,Iosif Elias, and Shrikanth Narayanan.
2011.Kernel models for affective lexicon creation.In Proceedings of the 12th Annual Conferenceof the International Speech CommunicationAssociation (Interspeech-11), pages 2977-2980.Tomas Mikolov, Ilya Sutskever, Kai Chen, GregCorrado, and Jeffrey Dean.
2013.
Distributedrepresentations of words and phrases and theircompositionality.
In Advances in Neural In-formation Processing Systems 26 (NIPS-13),pages 3111-3119.Arjun Mukherjee and Bing Liu.
2012.
Aspectextraction through semi-supervised modeling.In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics(ACL-12), pages 339-348.Georgios Paltoglou, Mathias Theunis, ArvidKappas, and Mike Thelwall.
2013.
Predictingemotional responses to long informal text.IEEE Trans.
Affective Computing, 4(1):106-115.792Bo Pang and Lillian Lee.
2008.
Opinion miningand sentiment analysis.
Foundations andtrends in information retrieval, 2(1-2):1-135.Minghui Qiu and Jing Jiang.
2013.
A latent vari-able model for viewpoint discovery fromthreaded forum posts.
In Proceedings of the2013 Conference of the North AmericanChapter of the Association for ComputationalLinguistics: Human Language Technologies(NAACL/HLT-13), pages 1031-1040.Delip Rao and Deepak Ravichandran.
2009.Semi-supervised polarity lexicon induction.
InProceedings of the 12th Conference of the Eu-ropean Chapter of the Associationfor Computational Linguistics (EACL-09),pages 675?682.Fuji Ren and Ye Wu.
2013.
Predicting user-topicopinions in Twitter with social and topicalcontext.
IEEE Trans.
Affective Computing,4(4):412-424.James A. Russell.
1980.
A circumplex model ofaffect.
Journal of personality and social psy-chology, 39(6):1161.Maite Taboada, Julian Brooke, Milan Tofiloski,Kimberly Voll, and Manfred Stede.
2011.Lexicon-based methods for sentiment analysis.Computational Linguistics, 37(2):267-307.Hao Wang and Martin Ester.
2014.
A sentiment-aligned topic model for product aspect ratingprediction.
In Proceedings of the 2014 Con-ference on Empirical Methods in NaturalLanguage Processing (EMNLP-14), pages1192-1202.Wen-Li Wei, Chung-Hsien Wu, and Jen-ChunLin.
2011.
A regression approach to affectiverating of Chinese words from ANEW.
In Proc.of Affective Computing and Intelligent Interac-tion (ACII-11), pages 121-131.Ruifeng Xu, Lin Gui, Jun Xu, Qin Lu, and Kam-Fai Wong.
2015.
Cross lingual opinion holderextraction based on multi-kernel SVMs andtransfer learning.
World Wide Web, 18:299-316.Liang-Chih Yu, Chung-Hsien Wu, and Fong-LinJang.
2009.
Psychiatric document retrieval us-ing a discourse-aware model.
Artificial Intelli-gence, 173(7-8): 817-829.Liang-Chih Yu, Jheng-Long Wu, Pei-ChannChang, and Hsuan-Shou Chu.
2013.
Using acontextual entropy model to expand emotionwords and their intensity for the sentimentclassification of stock market news.Knowledge-based Systems.
41:89-97.793
