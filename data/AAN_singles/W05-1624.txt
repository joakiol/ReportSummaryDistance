An Experiment Setup for Collecting Data for Adaptive Output Planningin a Multimodal Dialogue SystemIvana Kruijff-Korbayova?, Nate Blaylock,Ciprian Gerstenberger, Verena RieserSaarland University, Saarbru?cken, Germanykorbay@coli.uni-sb.deTilman Becker, Michael Kai?er,Peter Poller, Jan SchehlDFKI, Saarbru?cken, Germanytilman.becker@dfki.deAbstractWe describe a Wizard-of-Oz experiment setup forthe collection of multimodal interaction data for aMusic Player application.
This setup was devel-oped and used to collect experimental data as partof a project aimed at building a flexible multimodaldialogue system which provides an interface to anMP3 player, combining speech and screen inputand output.
Besides the usual goal of WOZ datacollection to get realistic examples of the behav-ior and expectations of the users, an equally im-portant goal for us was to observe natural behaviorof multiple wizards in order to guide our systemdevelopment.
The wizards?
responses were there-fore not constrained by a script.
One of the chal-lenges we had to address was to allow the wizardsto produce varied screen output a in real time.
Oursetup includes a preliminary screen output planningmodule, which prepares several versions of possi-ble screen output.
The wizards were free to speak,and/or to select a screen output.1 IntroductionIn the larger context of the TALK project1 we are develop-ing a multimodal dialogue system for a Music Player appli-cation for in-car and in-home use, which should support nat-ural, flexible interaction and collaborative behavior.
The sys-tem functionalities include playback control, manipulation ofplaylists, and searching a large MP3 database.
We believethat in order to achieve this goal, the system needs to provideadvanced adaptive multimodal output.We are conducting Wizard-of-Oz experiments[Bernsen et al, 1998] in order to guide the developmentof our system.
On the one hand, the experiments shouldgive us data on how the potential users interact with suchan application.
But we also need data on the multimodalinteraction strategies that the system should employ toachieve the desired naturalness, flexibility and collaboration.We therefore need a setup where the wizard has freedom of1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-edge; www.talk-project.org) is funded by the EU as projectNo.
IST-507802 within the 6th Framework program.choice w.r.t.
their response and its realization through singleor multiple modalities.
This makes it different from previousmultimodal experiments, e.g., in the SmartKom project[Tu?rk, 2001], where the wizard(s) followed a strict script.But what we need is also different in several aspects fromtaking recordings of straight human-human interactions: thewizard does not hear the user?s input directly, but only gets atranscription, parts of which are sometimes randomly deleted(in order to approximate imperfect speech recognition);the user does not hear the wizard?s spoken output directlyeither, as the latter is transcribed and re-synthesized (toproduce system-like sounding output).
The interactionsshould thus more realistically approximate an interactionwith a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).The wizard should be able to present different screen out-puts in different context, depending on the search results andother aspects.
However, the wizard cannot design screens onthe fly, because that would take too long.
Therefore, we de-veloped a setup which includes modules that support the wiz-ard by providing automatically calculated screen output op-tions the wizard can select from if s/he want to present somescreen output.Outline In this paper we describe our experiment setup andthe first experiences with it.
In Section 2 we overview theresearch goals that our setup was designed to address.
Theactual setup is presented in detail in Section 3.
In Section 4we describe the collected data, and we summarize the lessonswe learnt on the basis of interviewing the experiment partici-pants.
We briefly discuss possible improvements of the setupand our future plans with the data in Section 5.2 Goals of the Multimodal ExperimentOur aim was to gather interactions where the wizard can com-bine spoken and visual feedback, namely, displaying (com-plete or partial) results of a database search, and the user canspeak or select on the screen.Multimodal Presentation Strategies The main aim was toidentify strategies for the screen output, and for the multi-modal output presentation.
In particular, we want to learnFigure 1: Multimodal Wizard-of-Oz data collection setup foran in-car music player application, using the Lane Changedriving simulator.
Top right: User, Top left: Wizard, Bottom:transcribers.when and what content is presented (i) verbally, (ii) graphi-cally or (iii) by some combination of both modes.
We expectthat when both modalities are used, they do not convey thesame content or use the same level of granularity.
These areimportant questions for multimodal fission and for turn plan-ning in each modality.We also plan to investigate how the presentation strategiesinfluence the responses of the user, in particular w.r.t.
whatfurther criteria the user specifies, and how she conveys them.Multimodal Clarification Strategies The experimentsshould also serve to identify potential strategies for multi-modal clarification behavior and investigate individual strat-egy performance.
The wizards?
behavior will give us an ini-tial model how to react when faced with several sources ofinterpretation uncertainty.
In particular we are interested inwhat medium the wizard chooses for the clarification request,what kind of grounding level he addresses, and what ?sever-ity?
he indicates.
2 In order to invoke clarification behaviorwe introduced uncertainties on several levels, for example,multiple matches in the database, lexical ambiguities (e.g., ti-tles that can be interpreted denoting a song or an album), anderrors on the acoustic level.
To simulate non-understandingon the acoustic level we corrupted some of the user utterancesby randomly deleting parts of them.3 Experiment SetupWe describe here some of the details of the experiment.
Theexperimental setup is shown schematically in Figure 1.
Thereare five people involved in each session of the experiment: anexperiment leader, two transcribers, a user and a wizard.The wizards play the role of an MP3 player applicationand are given access to a database of information (but notactual music) of more than 150,000 music albums (almost 12Severity describes the number of hypotheses indicated by thewizard: having no interpretation, an uncertain interpretation, or sev-eral ambiguous interpretations.Figure 2: Screenshot from the FreeDB-based database appli-cation, as seen by the wizard.
First-level of choice what todisplay.million songs), extracted from the FreeDB database.3 Fig-ure 2 shows an example screen shot of the music databaseas it is presented to the wizard.
Subjects are given a set ofpredefined tasks and are told to accomplish them by usingan MP3 player with a multimodal interface.
Tasks includeplaying songs/albums and building playlists, where the sub-ject is given varying amounts of information to help themfind/decide on which song to play or add to the playlist.
Ina part of the session the users also get a primary driving task,using a Lane Change driving simulator [Mattes, 2003].
Thisenabled us to test the viability of combining primary and sec-ondary task in our experiment setup.
We also aimed to gaininitial insight regarding the difference in interaction flow un-der such conditions, particularly with regard to multimodal-ity.The wizards can speak freely and display the search resultor the playlist on the screen.
The users can also speak as wellas make selections on the screen.The user?s utterances are immediately transcribed by a typ-ist and also recorded.
The transcription is then presented tothe wizard.4 We did this for two reasons: (1) To deprivethe wizards of information encoded in the intonation of utter-ances, because our system will not have access to it either.
(2)To be able to corrupt the user input in a controlled way, sim-ulating understanding problems at the acoustic level.
Unlike[Stuttle et al, 2004], who simulate automatic speech recogni-tion errors using phone-confusion models, we used a tool that?deletes?
parts of the transcribed utterances, replacing themby three dots.
Word deletion was triggered by the experimentleader.
The word deletion rate varied: 20% of the utterancesgot weakly and 20% strongly corrupted.
In 60% of the casesthe wizard saw the transcribed speech uncorrupted.The wizard?s utterances are also transcribed (and recorded)3Freely available at http://www.freedb.org4We were not able to use a real speech recognition system, be-cause we do not have one trained for this domain.
This is one of thepurposes the collected data will be used for.Figure 3: Screenshot from the display presentation tool offer-ing options for screen output to the wizard for second-levelof choice what to display an how.and presented to the user via a speech synthesizer.
There aretwo reasons for doing this: One is to maintain the illusion forthe subjects that they are actually interacting with a system,since it is known that there are differences between human-human and human-computer dialogue [Duran et al, 2001],and we want to elicit behavior in the latter condition; theother has to do with the fact that synthesized speech is imper-fect and sometimes difficult to understand, and we wanted toreproduce this condition.The transcription is also supported by a typing and spellingcorrection module to minimize speech synthesis errors andthus help maintain the illusion of a working system.Since it would be impossible for the wizard to constructlayouts for screen output on the fly, he gets support for histask from the WOZ system: When the wizard performs adatabase query, a graphical interface presents him a first levelof output alternatives, as shown in Figure 2.
The choices arefound (i) albums, (ii) songs, or (iii) artists.
For a second levelof choice, the system automatically computes four possiblescreens, as shown in Figure 3.
The wizard can chose one ofthe offered options to display to the user, or decide to clearthe user?s screen.
Otherwise, the user?s screen remains un-changed.
It is therefore up to the wizard to decide whetherto use speech only, display only, or to combine speech anddisplay.The types of screen output are (i) a simple text-messageconveying how many results were found, (ii) output of a listof just the names (of albums, songs or artists) with the cor-responding number of matches (for songs) or length (for al-bums), (iii) a table of the complete search results, and (iv) atable of the complete search results, but only displaying a sub-set of columns.
For each screen output type, the system usesheuristics based on the search to decide, e.g., which columnsshould be displayed.
These four screens are presented to thewizard in different quadrants on a monitor (cf.
Figure 3),allowing for selection with a simple mouse click.
The heuris-tics for the decision what to display implement preliminarystrategies we designed for our system.
We are aware that dueto the use of these heuristics, the wizard?s output realizationmay not be always ideal.
We have collected feedback fromboth the wizards and the users in order to evaluate whetherthe output options were satisfactory (cf.
Section 4 for moredetails).Technical Setup To keep our experimental system modu-lar and flexible we implemented it on the basis of the OpenAgent Architecture (OAA) [Martin et al, 1999], which is aframework for integrating a community of software agents ina distributed environment.
Each system module is encapsu-lated by an OAA wrapper to form an OAA agent, which isable to communicate with the OAA community.
The exper-imental system consists of 12 agents, all of them written inJava.
We made use of an OAA monitor agent which comeswith the current OAA distribution to trace all communicationevents within the system for logging purposes.The setup ran distributed over six PCs running differentversions of Windows and Linux.54 Collected Data and ExperienceThe SAMMIE-26 corpus collected in this experiment containsdata from 24 different subjects, who each participated in onesession with one of our six wizards.
Each subject worked onfour tasks, first two without driving and then two with driving.The duration was restricted to twice 15 minutes.
Tasks wereof two types: searching for a title either in the database or inan existing playlist, building a playlist satisfying a number ofconstraints.
Each of the two sets for each subject containedone task of each type.
The tasks again differed in how specificinformation was provided.
We aimed to keep the difficultylevel constant across users.
The interactions were carried outin German.7The data for each session consists of a video and audiorecording and a logfile.
Besides the transcriptions of the spo-ken utterances, a number of other features have been anno-tated automatically in the log files of the experiment, e.g.,the wizard?s database query and the number of found results,the type and form of the presentation screen chosen by thewizard, etc.
The gathered logging information for a singleexperiment session consists of the communication events inchronological order, each marked by a timestamp.
Based onthis information, we can recapitulate the number of turns andthe specific times that were necessary to accomplish a usertask.
We expect to use this data to analyze correlations be-5We would like to thank our colleagues from CLT Sprachtech-nologie http://www.clt-st.de/ for helping us to set up thelaboratory.6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-teraction Experiment.
We have so far conducted two series of data-collection experiments: SAMMIE-1 involved only spoken interaction(cf.
[Kruijff-Korbayova?
et al, 2005] for more details), SAMMIE-2 isthe multimodal experiment described in this paper.7However, most of the titles and artist names in the musicdatabase are in English.tween queries, numbers of results, and spoken and graphicalpresentation strategies.Whenever the wizard made a clarification request, theexperiment leader invoked a questionnaire window on thescreen, where the wizard had to classify his clarification re-quest according to the primary source of the understandingproblem.
At the end of each task, users were asked to whatextent they believed they accomplished their tasks and howsatisfied they were with the results.
Similar to methods usedby [Skantze, 2003] and [Williams and Young, 2004], we planto include subjective measures of task completion and cor-rectness of results in our evaluation matrix, as task descrip-tions can be interpreted differently by different users.Each subject was interviewed immediately after the ses-sion.
The wizards were interviewed once the whole experi-ment was over.
The interviews were carried out verbally, fol-lowing a prepared list of questions.
We present below someof the points gathered through these interviews.Wizard Interviews All 6 wizards rated the overall under-standing as good, i.e., that communication completed suc-cessfully.
However, they reported difficulties due to delays inutterance transmission in both directions, which caused un-necessary repetitions due to unintended turn overlap.There were differences in how different wizards rated andused the different screen output options: The table containingmost of the information about the queried song(s) or album(s)was rated best and shown most often by some wizards, whileothers thought it contained too much information and wouldnot be clear at first glance for the users and hence they usedit less or never.
The screen option containing the least infor-mation in tabular form, namely only a list of songs/albumswith their length, received complementary judgments: someof the wizards found it useless because it contained too littleinformation, and they thus did not use it, and others found itvery useful because it would not confuse the user by present-ing too much information, and they thus used it frequently.Finally, the screen containing a text message conveying onlythe number of matches, if any, has been hardly used by thewizards.
The differences in the wizards?
opinions about whatthe users would find useful or not clearly indicate the needfor evaluation of the usefulness of the different screen outputoptions in particular contexts from the users?
view point.When showing screen output, the most common patternused by the wizards was to tell the user what was shown (e.g.,I?ll show you the songs by Prince), and to display the screen.Some wizards adapted to the user?s requests: if asked to showsomething (e.g., Show me the songs by Prince), they wouldshow it without verbal comments; but if asked a question(e.g., What songs by Prince are there?
or What did you find?
),they would show the screen output and answer in speech.Concerning the adaptation of multimodal presentationstrategies w.r.t.
whether the user was driving or not, fourof the six wizards reported that they consciously used speechinstead of screen output if possible when the user was driving.The remaining two wizards did not adapt their strategy.On the whole, interviewing the wizards brought valuableinformation on presentation strategies and the use of modal-ities, but we expect to gain even more insight after the an-notation and evaluation of the collected data.
Besides ob-servations about the interaction with the users, the wizardsalso gave us various suggestions concerning the software usedin the experiment, e.g., the database interface (e.g., the pos-sibility to decide between strict search and search for par-tial matches, and fuzzy search looking for items with similarspelling when no hits are found), the screen options presenter(e.g., ordering of columns w.r.t.
their order in the database in-terface, the possibility to highlight some of the listed items),and the speech synthesis system.Subject Interviews In order to use the wizards?
behavior asa model for interaction design, we need to evaluate the wiz-ards?
strategies.
We used user satisfaction, task experience,and multi-modal feedback behavior as evaluation metrics.The 24 experimental subjects were all native speakers ofGerman with good English skills.
They were all students(equally spread across subject areas), half of them male andhalf female, and most of them were between 20 to 30 yearsold.In order to calculate user satisfaction, users were inter-viewed to evaluate the system?s performance with a user sat-isfaction survey.
The survey probed different aspects of theusers?
perception of their interaction with the system.
Weasked the users to evaluate a set of five core metrics on a5-point Likert scale.
We followed [Walker et al, 2002] def-inition of the overall user satisfaction as the sum of text-to-speech synthesis performance, task ease, user expertise, over-all difficulty and future use.
The mean for user satisfactionacross all dialogues was 15.0 (with a standard derivation of2.9).
8 A one-way ANOVA for user satisfaction between wiz-ards (df=5, F=1.52 p=0.05) shows no significant differenceacross wizards, meaning that the system performance wasjudged to be about equally good for all wizards.To measure task experience we elicited data on perceivedtask success and satisfaction on a 5-point Likert scale aftereach task was completed.
For all the subjects the final per-ceived task success was 4.4 and task satisfaction 3.9 acrossthe 4 tasks each subject had to complete.
For task successas well as for task satisfaction no significant variance acrosswizards was detected.Furthermore the subjects were asked about the employedmulti-modal presentation and clarification strategies.The clarification strategies employed by the wizardsseemed to be successful: From the subjects?
point of view,mutual understanding was very good and the few misunder-standings could be easily resolved.
Nevertheless, in the caseof disambiguation requests and when grounding an utterance,subjects ask for more display feedback.
It is interesting tonote that subjects judged understanding difficulties on higherlevels of interpretation (especially reference resolution prob-lems and problems with interpreting the intention) to be morecostly than problems on lower levels of understanding (likethe acoustic understanding).
For the clarification strategy this8[Walker et al, 2002] reported an average user satisfaction of16.2 for 9 Communicator systems.implies that the system should engage in clarification at thelowest level a error was detected.9Multi-modal presentation strategies were perceived to behelpful in general, having a mean of 3.1 on a 5-point Lik-ert scale.
However, the subjects reported that too much in-formation was being displayed especially for the tasks withdriving.
85.7% of the subjects reported that the screen out-put was sometimes distracting them.
76.2% of the sub-jects would prefer to more verbal feedback, especially whiledriving.
On a 3-point Likert scale subjects evaluated theamount of the information presented verbally to be aboutright (mean of 1.8), whereas they found the information pre-sented on the screen to be too much (mean of 2.3).
Stud-ies by [Bernsen and Dybkjaer, 2001] on the appropriatenessof using verbal vs. graphical feedback for in-car dialoguesindicate that the need for text output is very limited.
Somesubjects in that study, as well subjects in our study report thatthey would prefer to not have to use the display at all whiledriving.
On the other hand subjects in our study perceived thescreen output to be very helpful in less stressful driving situa-tions and when not driving (e.g.
for memory assistance, clari-fications etc.).
Especially when they want to verify whether acomplex task was finally completed (e.g.
building a playlist),they ask for a displayed proof.
For modality selection in in-car dialogues the driver?s mental workload on primary andsecondary task has to be carefully evaluated with respect to asituation model.With respect to multi-modality subjects also asked formore personalized data presentation.
We therefore need todevelop intelligent ways to reduce the amount of data beingdisplayed.
This could build on prior work on the generationof ?tailored?
responses in spoken dialogue according to a usermodel [Moore et al, 2004].The results for multi-modal feedback behavior showed nosignificant variations across wizards except for the generalhelpfulness of multi-modal strategies.
An ANOVA PlannedComparison of the wizard with the lowest mean against theother wizards showed that his behavior was significantlyworse.
It is interesting to note, that this wizard was usingthe display less than the others.
We might consider not to in-clude the 4 sessions with this wizard in our output generationmodel.We also tried to analyze in more detail how the wizards?presentation strategies influenced the results.
The optionwhich was chosen most of the time was to present a tablewith the search results (78.6%); to present a list was only cho-sen in 17.5% of the cases and text only 0.04%.
The wizards?choices varied significantly only for presenting the table op-tion.
The wizard who was rated lowest for multimodality wasusing the table option less, indicating that this option shouldbe used more often.
This is also supported by the fact that theshow table option is the only presentation strategy which ispositively correlated to how the user evaluated multimodality(Spearman?s r = 0.436*).
We also could find a 2-tailed corre-9Note that engaging at the lowest level just helps to save dialogue?costs?.
Other studies have shown that user satisfaction is higherfor strategies that would ?hide?
the understanding error by askingquestions on higher levels [Skantze, 2003], [Raux et al, 2005]lation between user satisfaction and multimodality judgment(Spearman?s r = 0.658**).
This indicates the importance ofgood multimodal presentation strategies for user satisfaction.Finally, the subjects were asked for own comments.
Theyliked to be able to provide vague information, e.g., ask for ?anoldie?, and were expecting collaborative suggestions.
Theyalso appreciated collaborative proposals based on inferencesmade from previous conversations.In sum, as the measures for user satisfaction, task experi-ence, and multi-modal feedback strategies, the subjects?
judg-ments show a positive trend.
The dialogue strategies em-ployed by most of the wizards seem to be a good startingpoint for building a baseline system.
Furthermore, the resultsindicate that intelligent multi-modal generation needs to beadaptive to user and situation models.5 Conclusions and Future StepsWe have presented an experiment setup that enables us togather multimodal interaction data aimed at studying not onlythe behavior of the users of the simulated system, but alsothat of the wizards.
In order to simulate a dialogue system in-teraction, the wizards were only shown transcriptions of theuser utterances, sometimes corrupted, to simulate automaticspeech recognition problems.
The wizard?s utterances werealso transcribed and presented to the user through a speechsynthesizer.
In order to make it possible for the wizards toproduce contextually varied screen output in real time, wehave included a screen output planning module which auto-matically calculated several screen output versions every timethe wizard ran a database query.
The wizards were free tospeak and/or display screen output.
The users were free tospeak or select on the screen.
In a part of each session, theuser was occupied by a primary driving task.The main challenge for an experiment setup as describedhere is the considerable delay between user input and wizardresponse.
This is due partly to the transcription and spellingcorrection step and partly due to the time it takes the wizard todecide on and enter a query to the database, then select a pre-sentation and in parallel speak to the user.
We have yet to ana-lyze the exact distribution of time needed for these tasks.
Sev-eral ways can be chosen to speed up the process.
Transcrip-tion can be eliminated either by using speech recognition anddealing with its errors, or instead applying signal processingsoftware, e.g., to filter out prosodic information from the userutterance and/or to transform the wizard?s utterance into syn-thetically sounding speech (e.g., using a vocoder).
Databasesearch can be sped up in a number of ways too, ranging fromallowing selection directly from the transcribed text to auto-matically preparing default searches by analyzing the user?sutterance.
Note, however, that the latter will most likely prej-udice the wizard to stick to the proposed search.We plan to annotate the corpus, most importantly w.r.t.wizard presentation strategies and context features relevantfor the choice between them.
We also plan to compare thepresentation strategies to the strategies in speech-only mode,for which we collected data in an earlier experiment (cf.[Kruijff-Korbayova?
et al, 2005]).For clarification strategies previous studies already showedthat the decision process needs to be highly dynamic by tak-ing into account various features such as interpretation uncer-tainties and local utility [Paek and Horvitz, 2000].
We planto use the wizard data to learn an initial multi-modal clarifi-cation policy and later on apply reinforcement learning meth-ods to the problem in order to account for long-term dialoguegoals, such as task success and user satisfaction.The screen output options used in the experiment will alsobe employed in the baseline system we are currently imple-menting.
The challenges involved there are to decide (i) whento produce screen output, (ii) what (and how) to display and(iii) what the corresponding speech output should be.
We willanalyze the corpus in order to determine what the suitablestrategies are.References[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and LailaDybkjaer.
Exploring natural interaction in the car.
InCLASS Workshop on Natural Interactivity and IntelligentInteractive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, andL.
Dybkj?r.
Designing Interactive Speech Systems ?From First Ideas to User Testing.
Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, LaurieDamianos, and Lynette Hirschman.
Comparing several as-pects of human-computer and human-human dialogues.
InProceedings of the 2nd SIGDIAL Workshop on Discourseand Dialogue, Aalborg, 1-2 September 2001, pages 48?57,2001.[Kruijff-Korbayova?
et al, 2005] Ivana Kruijff-Korbayova?,Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,Michael Kai?er, Peter Poler, Jan Schehl, and VerenaRieser.
Presentation strategies for flexible multimodalinteraction with a music player.
In Proceedings ofDIALOR?05 (The 9th workshop on the semantics andpragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.Moran.
The open agent architecture: A framework forbuilding distributed software systems.
Applied ArtificialIntelligence: An International Journal, 13(1?2):91?128,Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes.
The lane-change-task as a toolfor driver distraction evaluation.
In Proceedings of IGfA,2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,Oliver Lemon, and Michael White.
Generating tailored,comparative descriptions in spoken dialogue.
In Proceed-ings of the Seventeenth International Florida Artificial In-telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz.
Con-versation as action under uncertainty.
In Proceedings ofthe Sixteenth Conference on Uncertainty in Artificial In-telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-hus, Allan W. Black, and Maxine Eskenazi.
Let?s go pub-lic!
taking a spoken dialog system to the real world.
2005.
[Skantze, 2003] Gabriel Skantze.
Exploring human errorhandling strategies: Implications for spoken dialogue sys-tems.
In Proceedings of the ISCA Tutorial and ResearchWorkshop on Error Handling in Spoken Dialogue Systems,2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, andSteve Young.
A framework for dialogue data collectionwith a simulated asr channel.
In Proceedings of the IC-SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk.
The technical processing insmartkom data collection: a case study.
In Proceedingsof Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,A.
Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,J.
Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,G.
Sandersa, S. Seneff, D. Stallard, and S. Whittaker.Cross-site evaluation in darpa communicator: The june2000 data collection.
2002.
[Williams and Young, 2004] Jason D. Williams and SteveYoung.
Characterizing task-oriented dialog using a sim-ulated asr channel.
In Proceedings of the ICSLP, 2004.
