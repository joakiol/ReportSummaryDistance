Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82?90,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTaxonomy Learning Using Word Sense InductionIoannis P. KlapaftisDepartment of Computer ScienceThe University of YorkYork, UK, YO10 5DDgiannis@cs.york.ac.ukSuresh ManandharDepartment of Computer ScienceThe University of YorkYork, UK, YO10 5DDsuresh@cs.york.ac.ukAbstractTaxonomies are an important resource for avariety of Natural Language Processing (NLP)applications.
Despite this, the current state-of-the-art methods in taxonomy learning havedisregarded word polysemy, in effect, devel-oping taxonomies that conflate word senses.In this paper, we present an unsupervisedmethod that builds a taxonomy of senseslearned automatically from an unlabelled cor-pus.
Our evaluation on two WordNet-derivedtaxonomies shows that the learned taxonomiescapture a higher number of correct taxonomicrelations compared to those produced by tradi-tional distributional similarity approaches thatmerge senses by grouping the features of eachword into a single vector.1 IntroductionA concept or a sense, s, can be defined as the mean-ing of a word or a multiword expression.
A con-cept s can be linguistically realised by more than oneword while at the same time a wordw can be the lin-guistic realisation of more than one concept.
Givena set of concepts S, taxonomy learning is the task ofhierarchically classifying the elements in S in an au-tomatic manner.
For example, consider a set of con-cepts linguistically realised by the words/multiwordexpressions LAN, computer network, internet, mesh-work, gauze, snood.
Taxonomy learning methodsproduce taxonomies, such as the ones shown in Fig-ures 1 (a) and 1 (b).By observing Figure 1 (a), we can express IS-A statements, such as Internet IS-A Computer Net-work etc.
However, the same does not apply to theFigure 1: A labelled and an unlabelled concept taxonomytaxonomy in Figure 1 (b), since this taxonomy is notfully labelled.
Despite this, its hierarchical organ-isation clearly shows that the concepts are dividedinto groups, which are further subdivided into sub-groups and so forth, until we reach a level whereeach concept belongs to its own group.
Unlabelledtaxonomies are typically produced by agglomera-tive hierarchical clustering algorithms (King, 1967;Sneath and Sokal, 1973).The knowledge encoded in taxonomies can beutilised in a range of NLP applications.
For in-stance, taxonomies can be used in information re-trieval to expand a user query with semantically re-lated words or to enhance document representationby abstracting from plain words and adding concep-tual information (Cimiano, 2006).
WordNet?s (Fell-baum, 1998) taxonomic relations have also beenused in Word Sense Disambiguation (WSD) (Nav-igli and Velardi, 2004b).
In named entity recog-nition, methods relying on gazetteers could make82use of automatically acquired taxonomies (Cimiano,2006), while question answering systems have alsobenefited (Moldovan and Novischi, 2002).Despite the wide uses of taxonomies, the majorityof methods disregard or do not deal effectively withword polysemy, in effect, developing taxonomiesthat conflate the senses of words (see Section 2).In this work, we show that Word Sense Induction(WSI) can be effectively employed to address thislimitation of existing methods.We present a novel method that employs WSI togenerate the different senses of a set of target wordsfrom an unlabelled corpus and then produces a tax-onomy of senses using Hierarchical AgglomerativeClustering (HAC) (King, 1967; Sneath and Sokal,1973).
We evaluate our method on two WordNet-derived sub-taxonomies and show that our methodleads to the development of concept hierarchies thatcapture a higher number of correct taxonomic rela-tions in comparison to those generated by currentdistributional similarity approaches.2 Related workInitial research on taxonomy learning focused onidentifying in a given text lexico-syntactic patternsthat suggest hyponymy relations (Hearst, 1992).
Forinstance, the pattern NP0 such as NP1,.
.
.
,NPnsuggests that NP0 is a hypernym of NPi.
For ex-ample, given the phrase Fruits, such as oranges, ap-ples,..., the above pattern would suggest that fruitis a hypernym of orange and apple.
These pattern-based approaches operate at the word level by learn-ing lexical relations between words rather than be-tween senses of words.In the same spirit, other work attempted to exploitthe regularities of dictionary entries to identify hy-ponymy relations (Amsler, 1981).
For example inWordNet, WAN is defined as a computer networkthat spans .
.
.
.
Hence, one can easily induce thatWAN is a hyponym of computer network by assum-ing that the first noun phrase in the definition is a hy-pernym of the target word.
These approaches learnlexical relations at the sense level since dictionariesseparate the senses of a word.
However this wouldbe true if and only if the glosses of the dictionarieswere sense-annotated, which is not the case for themajority of electronic dictionaries (Cimiano, 2006).Another limitation is that taxonomies are built ac-cording to the sense distinctions present in dictio-naries and not according to the actual use of wordsin the corpus.The majority of taxonomy learning approachesare based on the distributional hypothesis (Harris,1968).
Typically, distributional similarity methods(Cimiano et al, 2004; Cimiano et al, 2005; Faureand Ne?dellec, 1998; Reinberger and Spyns, 2004;Caraballo, 1999) utilise syntactic dependencies suchas subject/verb, object/verb relations, conjunctiveand appositive constructions and others.
These de-pendencies are used to extract the features that serveas the dimensions of the vector space.
Each targetnoun is then represented as a vector of extracted fea-tures where the frequency of co-occurrence of thetarget noun with each feature is used to calculate theweight of that feature.
The constructed vectors arethe input to hierarchical clustering or formal conceptanalysis (Ganter and Wille, 1999) to produce a tax-onomy.
These approaches assume that a target nounis monosemous creating one vector of features foreach target noun.
This limitation can lead to a num-ber of problems.Firstly, the constructed taxonomies might be bi-ased towards the inclusion of taxonomic relation-ships between the most frequent senses of tar-get nouns, ignoring interesting taxonomic relationswhere less frequent senses are present.
For exam-ple, consider the word house.
Current distributionalsimilarity methods would possibly capture the hy-ponyms of its Most Frequent Sense (MFS1), how-ever ignoring the hyponyms of less frequent sensesof house, e.g.
casino, theater, etc.
Given that wordsenses typically follow a Zipf distribution, thesemethods construct vectors dominated by the MFS ofwords.
This bias significantly degrades the useful-ness of learned taxonomies.Secondly, given that distributional similarity ap-proaches rely on the computation of pairwise simi-larities between target words, merging their sensesto a single vector might lead to unreliable similarityestimates.
For example, merging the features of thedifferent senses of house could provide a lower sim-ilarity with its monosemous hyponym beach house,since only the first sense of house is related to beach1WordNet: A dwelling that serves as living quarters .
.
.83house.
This problem might lead both to inclusionof incorrect or loss of correct taxonomic relations.In our work, we aim to overcome these drawbacksby identifying the different senses with which targetwords appear in text and then building a hierarchyof the identified senses.Soft clustering approaches (Reinberger andSpyns, 2004; Reinberger et al, 2003) have also beenapplied to taxonomy learning to deal with polysemy.These methods associate each verb with a vector offeatures, where each feature is a noun appearing asa subject or object of that verb.
That way a noun canappear in different vectors, hence in different clus-ters during hierarchical clustering as a result of itspolysemy.
However, the underlying assumption isthat a verb is monosemous with respect to its associ-ated vector of nouns.
This assumption is not alwaysvalid and can cause the problems mentioned above.Other work in taxonomy learning exploits thehead/modifier relationships to create taxonomic re-lations (Buitelaar et al, 2004; Hwang, 1999;Sa?nchez and Moreno, 2005).
These relations areused to create: (1) a class (concept) for each head,and (2) subclasses by adding nominal or adjectivalmodifiers.
For example, credit card IS-A card.
Thecorresponding hyponymy relations are learned at thelexical level disregarding word polysemy.
Some ofthese approaches identified the problem of polysemyand applied sense disambiguation with respect toWordNet in order to capture the different senses of atarget term (Navigli and Velardi, 2004b; Navigli andVelardi, 2004a).
Specifically, the taxonomy built byexploiting head/modifiers relations was modified ac-cording to WordNet?s hyponymy relations betweensenses of disambiguated terms.
One important de-ficiency of using sense disambiguation is that dic-tionaries miss many domain-specific senses.
Addi-tionally, the fixed-list of senses paradigm prohibitslearning word senses according to their use in con-text.
The use of sense induction we propose in thispaper aims to overcome these limitations.3 MethodGiven a set of words W , a WSI method is appliedto each wi ?
W (Section 3.1).
The outcome of thefirst stage is a set of senses, S, where each swi ?
Sdenotes the i-th sense of word w ?
W .
This setFigure 2: WSI for network & LANof senses is the input to hierarchical clustering thatproduces a hierarchy of senses (Section 3.2).3.1 Word sense inductionWSI is the task of identifying the senses of a tar-get word in a given text.
Recent WSI methodswere evaluated under the framework of SemEval-2007 WSI task (SWSI) (Agirre and Soroa, 2007).The evaluation framework defines two types of as-sessment, i.e.
evaluation in: (1) a clustering and(2) a WSD setting.
Based on this evaluation, we se-lected the method of Klapaftis & Manandhar (2008)(henceforth referred to as KM) that achieves high F-score in both evaluation schemes as compared to thesystems participating in SWSI.
We briefly describeKM mentioning its parameters used in our evalua-tion (Section 4).
Figures 2 (a) and 2 (b) describe thedifferent steps for inducing the senses of the targetwords network and LAN.Corpus preprocessing: The input to KM is abase corpus bc, in which the target word w appearsin each paragraph.
In Figure 2 (a), the base cor-pus consists of the paragraphs A, B, C and D. Theaim of this stage is to capture nouns contextually84related to w. Initially, the target word is removedfrom bc, part-of-speech tagging is applied to eachparagraph, only nouns are kept and lemmatised.
Inthe next step, the distribution of each noun is com-pared to the distribution of the same noun in a ref-erence corpus2 using the log-likelihood ratio (G2)(Dunning, 1993).
Nouns with a G2 below a pre-specified threshold (parameter p1) are removed fromeach paragraph.
Figure 2 (a) shows the remainingnouns for each paragraph of bc.Graph creation & clustering: In the setting ofKM, a collocation is a juxtaposition of two nounswithin the same paragraph.
Thus, each noun is com-bined with any other noun yielding a total of(N2)collocations for a paragraph with N nouns.
Eachcollocation, cij , is assigned a weight that measuresthe relative frequency of two nouns co-occurring.This weight is the average of the conditional prob-abilities p(ni|nj) and p(nj |ni), where p(ni|nj) =f(cij)f(nj), f(cij) is the number of paragraphs nouns ni,nj co-occur and f(nj) is the number of paragraphsin which nj appears.
Collocations are filtered withrespect to their frequency (parameter p2) and weight(parameter p3).
Each retained collocation is rep-resented as a vertex.
Edges between vertices arepresent, if two collocations co-occur in one or moreparagraphs.
Figure 2 (a) shows that this process hasgenerated 24 collocations for the target word net-work.
On the top right of the figure we also observethe collocations associated with each paragraph.In the next step, a smoothing technique is appliedto discover new edges between vertices.
The weightapplied to each edge connecting vertices vi and vj(collocations cab, cde) is the maximum of their con-ditional probabilities (max(p(cab|cde), p(cde|cab))).Finally, the graph is clustered using Chinese whis-pers (Biemann, 2006).
The final output is a set ofsenses, each one represented by a set of contextuallyrelated collocations.
In Figure 2, we generated twosenses for network and one sense for LAN.3.2 Hierarchical clustering of sensesGiven the set of senses S, our task at this point is tohierarchically classify the senses using HAC.
Con-sider for example the words network and LAN, and2The British National Corpus, 2001, Distributed by OxfordUniversity Computing Services.Senses computer meshwork LANnetworkcomputer network 1 0.0 0.66meshwork 0.0 1 0.14LAN 0.66 0.14 1Table 1: Similarity matrix for HAC.Figure 3: WSI & HAC examplelet us assume that the WSI process has generatedthe senses in Figures 2 (a) and 2 (b).
HAC oper-ates by treating each sense as a singleton cluster andthen successively merging the most similar clustersaccording to a pre-defined similarity function.
Thisprocess iterates until all clusters have been mergedinto a single cluster taken to be the root.To calculate the pairwise similarities betweensenses we exploit the attributes that represent eachsense, i.e.
their collocations.
Let BC be the cor-pus resulting from the union of the base corpora ofall words in W .
In our example, BC would consistof the paragraphs, in which the words network andLAN appear, i.e.
A, B, ..., G. An induced sense tagsa paragraph, if one or more of its collocations ap-pear in that paragraph.
Thus, each induced sense isassociated with a set of paragraph labels that denotethe paragraphs tagged by that sense.
Figure 3 showsthe paragraph labels tagged by each sense of our ex-ample.
Finally, given two senses sai , sbi and theircorresponding sets of tagged paragraphs fai and fbi ,we use the Jaccard coefficient to calculate their sim-ilarity, i.e.
JC(sai , sbi) =|fai ?fbi ||fai ?fbi |, where skj denotesthe j-th sense of word k. The resulting similaritymatrix of our example is shown in Table 1.
Giventhat matrix, HAC would first group computer net-work and LAN as they have the highest similarity(Figure 3).
In the final iteration, the remaining twoclusters (Cluster 1 & meshwork) would be groupedto the root.An important parameter of HAC is the choiceof the technique for calculating cluster similarities.Note that as we move towards the higher levels of85the taxonomy clusters contain more than one sets oftagged paragraphs (Figure 3 - Cluster 1), hence thechoice of the similarity function is crucial.
We ex-periment with three techniques, i.e.
single-linkage,complete-linkage and average-linkage.
The first onedefines the similarity between two clusters as themaximum similarity among all the pairs of their cor-responding feature sets.
The second considers theminimum similarity among all the pairs, while thethird calculates the average similarity of all the pairs.4 EvaluationWe evaluate our method with respect to twoWordNet-derived sub-taxonomies (Section 4.3).
Forthat reason, it is necessary to map the induced sensesto WordNet before applying HAC.
Note that themapping process might map more than one inducedsenses to the same WordNet sense.
In that case,these induced senses are merged to a single onealong with their corresponding collocations.4.1 Mapping WSI clusters to WordNet sensesThe process of mapping the induced senses to Word-Net is straightforward.
Let w ?
W be a word withn senses in WordNet.
A WordNet sense i of w is de-noted bywswi , i = [1, n].
Let us also assume that theWSI method has produced m senses for w, whereeach sense j is denoted as swj , j = [1,m].
Each in-duced sense swj is associated with a set of featuresfwj as in the previous section.
These features are theparagraphs (paragraph labels) of BC tagged by swj .In the next step, each WordNet sense wswi is associ-ated with its WordNet signature gwi that contains thefollowing semantic features: hypernyms/hyponyms,meronyms/holonyms and synonyms of wswi .
Forexample, the signature of the fifth WordNet senseof network would contain internet, cyberspace andother semantically related words.
Table 2 shows par-tial signatures for each sense of network.The signature gwi is used to formalise the Word-Net sense wswi as a set of features qwi .
These fea-tures are the paragraphs (paragraph labels) of BCthat contain one or more of the aforementioned se-mantically related to wswi words that exist in gwi .Given an induced sense swj , a similarity score is cal-culated between swj and each WordNet sense of w.The maximum score determines the WordNet senseWordNet sense Semantically related words/phrases1 reticulum, RF, RAS2 communication system/equipment3 gauze, snood, tulle4 reseau, reticle, reticulation5 net, internet, cyberspaceTable 2: Semantically related words/phrases to networklabel that will be assigned to swj , i.e.
label(swj ) =argmaxi JC(fwj , qwi ), where JC is the Jaccard sim-ilarity coefficient.
In the example of Figure 2 (a),the computer network sense would be mapped to thefifth WordNet sense of network, since there is a sig-nificant overlap between the paragraphs tagged bythe induced and that WordNet sense.4.2 Evaluation measuresFor the purposes of this section we present one goldstandard taxonomy (Figure 1 (a)) and a second de-rived from our method (Figure 1 (b)).
The compari-son of these taxonomies is based on the semantic co-topy of a node, which has also been used in (Maed-che and Staab, 2002; Cimiano et al, 2005).
In par-ticular, the semantic cotopy of a node is defined asthe set of all its super- and subnodes excluding theroot and including that node.
For example, the se-mantic cotopy of computer network in Figure 1 (a)is {computer network, internet, LAN}.
There aretwo issues, which make the evaluation difficult.The first one is that HAC produces a taxonomy inwhich all internal nodes are unlabelled, as opposedto the gold standard taxonomy.
In Figure 1 (b), wehave manually labelled internal nodes with their IDsfor clarity.
For example, the semantic cotopy of thenode New Cluster 1 in Figure 1 (b) is {computer net-work, internet, LAN, New Cluster 1, New Cluster0}.
By comparing the cotopies of nodes computernetwork in Figure 1 (a) and New Cluster 1 in Fig-ure 1 (b), we observe that the automatic method hassuccessfully grouped all of the hypernyms and hy-ponyms of computer network under New Cluster 1.However, the corresponding cotopies are not iden-tical, because the cotopy of New Cluster 1 also in-cludes the labels produced by HAC.To deal with this problem, we use a version of se-mantic cotopy for nodes in the automatically learnedtaxonomy which excludes nodes that do not exist inWordNet.
That way the semantic cotopies of NewCluster 1 in Figure 1 (b) and computer network in86Figure 1 (a) will yield maximum similarity.The second issue is that the nodes that exist in thegold standard taxonomy are leaf nodes in the auto-matically learned taxonomy.
As a result, the seman-tic cotopy of LAN in Figure 1 (b) is {LAN} sinceall of its supernodes do not exist in WordNet.
Incontrast, the semantic cotopy of LAN in Figure 1(a) is {LAN, computer network}.
We observe thatthere is an overlap between the two cotopies derivedby the existence of the same concept in both tax-onomies, i.e.
LAN.
In fact, all of the leaf nodes ofa learned taxonomy will have a small overlap withthe corresponding concept in the gold standard.
Forthis problem, we observe that in our automaticallylearned taxonomies it does not make sense to cal-culate the semantic cotopy of leaf nodes.
On thecontrary, we need to evaluate the internal nodes thatgroup the leaf nodes.
Let us assume the followingnotation:TA = automatically learned taxonomy?i = node in a taxonomyC(TA) = internal nodes + leaf nodes of TAI(TA) = internal nodes of TATG = gold standard taxonomyC(TG) = internal nodes + leaf nodes of TGI(TG) = internal nodes of TGhyper(?i) = supernodes of ?i excluding the roothypo(?i) = subnodes of ?i including ?iFor ?i ?
I(TA), the semantic cotopy is defined as:SC ?
(?i) = (hyper(?i) ?
hypo(?i)) ?
C(TG)For ?i ?
C(TG), the semantic cotopy is defined as:SC ??
(?i) = (hyper(?i) ?
hypo(?i))P (?i, ?j) =|SC ?
(?i) ?
SC ??
(?j)||SC ?
(?i)|(1)R(?i, ?j) =|SC ?
(?i) ?
SC ??
(?j)||SC ??
(?j)|(2)F (?i, ?j) =2P (?i, ?j)R(?i, ?j)P (?i, ?j) +R(?i, ?j)(3)Precision, recall and harmonic mean of node ?i ?I(TA) with respect to node ?j ?
C(TG) are de-fined in Equations 1, 2 and 3.
The F-score, FS, ofnode ?i ?
I(TA) is the maximum F attained at any?j ?
C(TG) (FS(?i) = argmaxj F (?i, ?j)).
Fi-nally, the similarity TS of the entire taxonomy tothe gold standard taxonomy is the average of theF-scores of each ?i ?
I(TA) (Equation 4).
TheTS(TA, TG) in Figure 1 is 0.9.
All nodes of TAhave a perfect match, apart from New Cluster 0 andNew Cluster 2, which are matched against computernetwork and meshwork respectively, having a per-fect precision but a lower recall since the cotopiesof computer network and meshwork consist of threeconcepts.
The automatically learned taxonomy hastwo redundant clusters that decrease its similarity.TS(TA, TG) =1|I(TA)|?
?i?I(TA)FS(?i) (4)The similarity measure TS(TA, TG) provides thesimilarity of the automatically learned taxonomy tothe gold standard one, but it is not symmetric.
Cal-culating the taxonomic similarity one way might notprovide accurate results, in cases where TA missessenses of the gold standard.
This is due to thefact that we would only evaluate the internal nodesof TA, partially ignoring the fact that TA mighthave missed some parts of the gold standard taxon-omy.
For that reason, we also calculate TS(TG, TA)which provides the similarity of the gold standardtaxonomy to the automatically learned one.
Fi-nally, taxonomic similarities are combined to pro-duce their harmonic mean (Equation 5).TxSm(TA, TG) =2TS(TG, TA)TS(TA, TG)TS(TG, TA) + TS(TA, TG)(5)4.3 Evaluation datasets & settingThe first gold standard taxonomy is derived by ex-tracting from WordNet al the hyponyms of thesenses of the word network.
The extracted taxonomycontains 29 senses linguistically realized by 24 wordsets (one sense might be expressed with more thanone words), since network has 5 senses and reseauhas 2 senses in the gold standard taxonomy.
Notethat we have disregarded senses only expressed bymultiword expressions.
The average polysemy ofwords is around 1.7.
The second taxonomy is de-rived by extracting the concepts under the senses ofthe word speaker.
The speaker taxonomy contains52 senses linguistically realized by 50 word sets,since speaker has 3 senses included in the taxonomy.The average polysemy of words is around 1.58.To create our datasets3 we use the Yahoo!
searchapi4.
For each word w in each of the datasets, we is-3Available in http://www.cs.york.ac.uk/aig/projects/indect/taxlearn4http://developer.yahoo.com/search/ [Accessed:10/06/2009]87Parameter RangeG2 threshold (p1) 5,10Collocation frequency (p2) 4,6,8Collocation weight (p3) 0.1,0.2,0.3,0.4Table 3: Chosen parameters for the KM WSI method.sue a query to Yahoo!
that contains w and we down-load a maximum of 1000 pages.
In cases wherea particular sense is expressed by more than oneword, the query was formulated by including all thewords and putting the keyword OR between them.For each page we extracted fragments of text (para-graphs) that occur in <p> </p> html tags.
We ex-tracted 58956 and 78691 paragraphs for the networkand speaker dataset respectively.
The reason we ex-tracted on average less content for the second datasetwas that Yahoo!
provided a small number of resultsfor rare words such as alliterator, anecdotist, etc.Table 3 shows the parameter ranges for the WSImethod.
Our method is evaluated according to theseparameters.
Our first baseline is RAND, which per-forms a random hierarchical clustering of senses toproduce a binary tree.
In each iteration two clustersare randomly chosen and form a new cluster, untilwe end up with one cluster taken to be the root.
Theperformance of RAND is calculated by executing therandom algorithm 10 times and then averaging theresults.
The second baseline is the taxonomy mostfrequent sense baseline (TL MFS), in which we donot perform WSI.
Instead, given a parameter settingand a word w, all the collocations of w are groupedinto one vector, which will possibly be dominatedby collocations related to the MFS of w. WordNetmapping takes place and finally HAC with average-linkage is applied to create the taxonomy.4.4 Results & discussionFigures 4 (a) and 4 (b) show the performanceof HAC with single-linkage (HAC SNG), average-linkage (HAC AVG) and complete-linkage (HACCMP) against RAND for p1 = 5 and different com-binations of p2 and p3.
It is clear that HAC SNG andHAC AVG outperform RAND by very large marginsunder all parameter combinations.
In the networkdataset, both of them achieve their highest distancefrom RAND (27.84%) at p2 = 8 and p3 = 0.2.
In thespeaker dataset, their highest distance from RAND(20.97% and 19.63% respectively) is achieved atp2 = 4 and p3 = 0.1.
HAC CMP performs worsethan the other HAC versions, yet it clearly outper-forms RAND in all but one parameter combinations(p1 = 5, p2 = 6, p3 = 0.4) in the speaker dataset.Generally, for collocation weight equal to 0.4 theperformance of all HAC versions drops.
At thishigh collocation weight the WSI method produces alarger number of small clusters than in lower thresh-olds.
This issue negatively affects both the map-ping process and HAC.
For example in the speakerdataset, for p1 = 5, p2 = 8 and p3 = 0.1 our tax-onomies contained 86.54% of the gold standard tax-onomy senses.
Increasing the collocation weight to0.2 did not have any effect, but increasing the weightto 0.3 and then 0.4 led to 71.15% and 65.38% sensecoverage.
Overall, our conclusion is that all HACversions exploit the WSI method and learn usefulinformation better than chance.
The picture is thesame for p1 = 10.Figures 4 (c) and 4 (d) show the performance ofHAC versions against the TL MFS baseline in thesame parameter setting as above.
We observe thatboth HAC SNG and HAC AVG perform significantlybetter than TL MFS apart from p3 = 0.4, in whichcase all HAC versions perform worse.
In the networkdataset, the largest performance difference for HACSNG is 10.12% and for HAC AVG 9.9% at p2 = 6and p3 = 0.2.
In the speaker dataset, the largest per-formance difference for HAC SNG is 10.83% andfor HAC AVG 7.83% at p2 = 8 and p3 = 0.2.
HACCMP performs worse than TL MFS under most pa-rameter settings in both datasets.
The picture is thesame for p1 = 10.Overall, the analysis of the WSI-based taxonomylearning approach against TL MFS shows that HACSNG and HAC AVG perform better than TL MFSunder all parameter combinations for both datasets.The main reason for their superior performance isthat their learned taxonomies contain a higher num-ber of senses than TL MFS as a result of the senseinduction process.
This greater sense coverage leadsto the discovery of a higher number of correct taxo-nomic relations between senses than TL MFS, hencein a better performance.
To conclude, our resultsverify our hypothesis and suggest that the unsuper-vised learning of word senses contributes to produc-ing taxonomies with a higher similarity to the goldstandard ones than traditional distributional similar-ity methods.88Figure 4: Performance analysis of the proposed method for p1 = 5 and different combinations of p2 and p3.Despite that, our evaluation also shows that inmost cases HAC CMP is unable to exploit the in-duced senses and performs worse than TL MFS,HAC SNG and HAC AVG.
This result was not ex-pected, since HAC SNG employs a local criterion tomerge two clusters and does not consider the globalstructure of the clusters, in effect, being biased to-wards elongated clusters.
The observation of thegold standard taxonomies shows that they consistboth of cohyponym concepts which are expectedto be contextually related, but also of cohyponymswhich are not expected to appear in similar contexts.For example, someone would expect a high similar-ity between WAN, LAN, or between snood and tulle.However, the same does not apply for snood andcheesecloth or tulle and grillwork, because cheese-cloth and grillwork appear in significantly differentcontexts than snood and tulle.
Despite that, all ofthem are cohyponyms.
This issue is more prevalentin the speaker dataset, where concepts such as loud-speaker, tannoy, woofer are expected to be contex-tually related, while cohyponyms such as whisperer,lecturer and interviewer are not.
This means that thegold standard taxonomies include elongated clustersand explains the superior performance of HAC SNG.This issue is not affecting HAC AVG, but it has a sig-nificant effect on HAC CMP.
Generally, HAC CMPemploys a non-local criterion by considering the di-ameter of a candidate cluster.
This results in com-pact clusters with small diameters, as opposed toelongated ones.5 ConclusionWe presented an unsupervised method for taxonomylearning that employs WSI to identify the senses oftarget words and then builds a taxonomy of thesesenses using HAC.
We have shown that dealing withpolysemy by means of sense induction helps to de-velop taxonomies that capture a higher number ofcorrect taxonomic relations than traditional distribu-tional similarity methods, which associate each tar-get word with one vector of features, in effect, merg-ing its senses.AcknowledgementsThis work is supported by the European Commis-sion via the EU FP7 INDECT project, Grant No.218086, Research area: SEC-2007-1.2-01 Intelli-gent Urban Environment Observation System.89ReferencesE.
Agirre and A. Soroa.
2007.
SemEval-2007 Task02: Evaluating Word Sense Induction and Discrimi-nation Systems.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations, pages 7?12,Prague, Czech Republic.R.
A. Amsler.
1981.
A Taxonomy for English Nouns andVerbs.
In Proceedings of the 19th ACL Conference,pages 133?138, Stanford, California.C.
Biemann.
2006.
Chinese Whispers - An EfficientGraph Clustering Algorithm and its Application toNatural Language Processing Problems.
In Proceed-ings of TextGraphs, pages 73?80, New York,USA.P.
Buitelaar, D. Olejnik, and M. Sintek.
2004.
A Ptote?ge?Plug-in for Ontology Extraction from Text Based onLinguistic Analysis.
In Proceedings of the 1st Euro-pean Semantic Web Symposium, pages 31?44, Crete,Greece.
CEUR-WS.org.S.
A. Caraballo.
1999.
Automatic Construction of aHypernym-labeled Noun Hierarchy from Text.
In Pro-ceedings of the 37th ACL Conference, pages 120?126,College Park, Maryland.P.
Cimiano, A. Hotho, and S. Staab.
2004.
Compar-ing Conceptual, Divisive and Agglomerative Cluster-ing for Learning Taxonomies from Text.
In Proceed-ings of the 16th ECAI Conference, pages 435?439, Va-lencia, Spain.P.
Cimiano, A. Hotho, and S. Staab.
2005.
LearningConcept Hieararchies from Text Corpora Using For-mal Concept Analysis.
Journal of Artificial Intelli-gence Research, 24:305?339.P.
Cimiano.
2006.
Ontology Learning and Populationfrom Text: Algorithms, Evaluation and Applications.Springer-Verlag New York, Inc., Secaucus, NJ, USA.T.
Dunning.
1993.
Accurate Methods for the Statistics ofSurprise and Coincidence.
Computational Linguistics,19(1):61?74.D.
Faure and C. Ne?dellec.
1998.
A Corpus-based Con-ceptual Clustering Method for Verb Frames and On-tology Acquisition.
In LREC workshop on Adaptinglexical and corpus resources to sublanguages and ap-plications, pages 5?12, Granada, Spain.C.
Fellbaum.
1998.
Wordnet: An Electronic LexicalDatabase.
MIT Press, Cambridge, Massachusetts,USA.B.
Ganter and R. Wille.
1999.
Formal Concept Anal-ysis: Mathematical Foundations.
Springer-VerlagNew York, Inc., Secaucus, NJ, USA.
Translator-C.Franzke.Z.
Harris.
1968.
Mathematical Structures of Language.Wiley, New York, USA.M.
A. Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
In Proceedings ofthe 14th Coling Conference, pages 539?545, Nantes,France.C.
H. Hwang.
1999.
Incompletely and ImpreciselySpeaking: Using Dynamic Ontologies for Represent-ing and Retrieving Information.
In Proceedings ofthe 6th International Workshop on Knowledge Repre-sentation Meets Databases, pages 14?20, Linkoping,Sweden.
CEUR-WS.org.B.
King.
1967.
Step-wise Clustering Procedures.
Jour-nal of the American Statistical Association, 69:86?101.I.
P. Klapaftis and S. Manandhar.
2008.
Word Sense In-duction Using Graphs of Collocations.
In Proceedingsof the 18th ECAI Conference, pages 298?302, Patras,Greece.
IOS Press.A.
Maedche and S. Staab.
2002.
Measuring Similaritybetween Ontologies.
In Proceedings of the EuropeanConference on Knowledge Acquisition and Manage-ment (EKAW), pages 251?263, London,UK.
Springer-Verlag.D.
Moldovan and A. Novischi.
2002.
Lexical Chainsfor Question Answering.
In Proceedings of the 19thColing Conference, pages 1?7, Taipei, Taiwan.R.
Navigli and P. Velardi.
2004a.
Learning Domain On-tologies from Document Warehouses and Dedicatedweb Sites.
Computational Linguistics, 30(2):151?179.R.
Navigli and P. Velardi.
2004b.
Structural Semantic In-terconnection: a Knowledge-based Approach to WordSense Disambiguation.
In Proceedings of Senseval-3: Third International Workshop on the Evaluation ofSystems for the Semantic Analysis of Text, pages 179?182, Barcelona, Spain.M.L.
Reinberger and P. Spyns.
2004.
DiscoveringKnowledge in Texts for the Learning of Dogma-inspired Ontologies.
In Proceedings of the ECAIWorkshop on Ontology Learning and Population,pages 19?24, Valencia, Spain.M.
L. Reinberger, P. Spyns, W. Daelemans, and R. Meers-man.
2003.
Mining for Lexons: Applying Unsuper-vised Learning Methods to create ontology bases.
InCoopIS/DOA/ODBASE, pages 803?819.D.
Sa?nchez and A. Moreno.
2005.
Web-scale Taxon-omy Learning.
In Proceedings of the Workshop onLearning and Extending Ontologies by using MachineLearning methods, pages 53?60, Bonn, Germany.P.
H. A. Sneath and R. R. Sokal.
1973.
Numerical Taxon-omy, The Principles and Practice of Numerical Clas-sification.
W. H. Freeman, San Francisco, USA.90
