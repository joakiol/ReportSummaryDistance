Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1011?1021,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsThe Answer is at your Fingertips: Improving Passage Retrieval for WebQuestion Answering with Search Behavior DataMikhail Ageev?Moscow State Universitymageev@yandex.ruDmitry LagunEmory Universitydlagun@emory.eduEugene AgichteinEmory Universityeugene@mathcs.emory.eduAbstractPassage retrieval is a crucial first step of au-tomatic Question Answering (QA).
While ex-isting passage retrieval algorithms are effec-tive at selecting document passages most sim-ilar to the question, or those that containthe expected answer types, they do not takeinto account which parts of the document thesearchers actually found useful.
We propose,to the best of our knowledge, the first success-ful attempt to incorporate searcher examina-tion data into passage retrieval for question an-swering.
Specifically, we exploit detailed ex-amination data, such as mouse cursor move-ments and scrolling, to infer the parts of thedocument the searcher found interesting, andthen incorporate this signal into passage re-trieval for QA.
Our extensive experiments andanalysis demonstrate that our method signif-icantly improves passage retrieval, comparedto using textual features alone.
As an addi-tional contribution, we make available to theresearch community the code and the searchbehavior data used in this study, with the hopeof encouraging further research in this area.1 IntroductionAutomated Question Answering (QA), is an attrac-tive variation of search where the QA system auto-matically returns an answer to a user?s question, in-stead of a list of document results.
Passage retrievalis a first critical step of QA system, where candi-date passages are identified and scored as likely tocontain an answer.
While significant progress hasbeen made recently on incorporating syntactic andsemantic analysis for improving the QA system per-formance, this analysis is typically applied only onthe (limited) set of candidate passages retrieved.
Themain reason is that it is generally not practical toperform deep analysis on all documents in a largecollection, and not yet feasible for the Web at large.
?Work done at Emory University.In the web search setting, automated questionanswering presents additional challenges and op-portunities.
On the downside, the questions andqueries from real users are often not grammaticalor well-formed, differing from the questions usedin the traditional TREC Question Answering evalua-tions (Kelly and Lin, 2007; Sun et al 2005).
On theupside, by interacting with a search engine, the mil-lions of searchers implicitly provide additional cluesabout usefulness of documents, result ranking, andother aspects of the search process.
In this paper,we explore making use of the search behavior datato improve passage retrieval for automated QuestionAnswering on the web.Our basic observation is that when a user is at-tempting to answer a question, he or she will morecarefully examine the parts of the document thatcontain an answer.
This observation is intuitive,and is strongly supported by numerous eye track-ing studies (e.g., Buscher et al(2008) and Buscheret al(2009a)).
Based on this, we hypothesize thatthe passages containing the answers can be automat-ically identified from the naturalistic searcher behav-ior, and this prediction can be subsequently used toimprove passage ranking.
To the best of our knowl-edge, our work is the first to successfully incorporatesearcher examination into passage ranking for Ques-tion Answering.Our approach is primarily aimed at recurring (re-peated) questions, which comprise a large fractionof the search volume (while the exact statistics vary,over 50% of search queries are submitted by multi-ple users).
For such questions, a system would trackthe clicked result URLs, as well as the user interac-tions on the landing pages.
Then the system woulduse this information to present the improved resultsto new users who ask the same (or similar) ques-tion.
Intuitively, our method uses the same generalidea of result click data mining, used by the majorsearch engines to improve result ranking, but takes1011it a step further to exploit user interactions on the ac-tual landing pages.
A key point to emphasize is thatour approach exploits the natural browsing behaviorof the users, not requiring any additional effort fromthe searchers.Specifically, our contributions include:?
A novel approach to passage retrieval for ques-tion answering, that naturally integrates textualand behavioral evidence.?
A robust infrastructure for connecting fine-grained searcher behavior to precise page con-tents.?
Thorough experiments over hundreds of searchsessions and thousands of page views, demon-strating significant improvements to passageretrieval by harnessing the user?s page exami-nation data.Next we describe related work, to place our contri-bution in context.2 Related WorkOur work brings together two areas of research: pas-sage retrieval for question answering, and miningsearcher behavior data.Passage retrieval has long been recognized asthe first crucial step of automatic question answer-ing.
In some cases, passage retrieval can even serveas the final product of a Question Answering sys-tem (Clarke et al 2000).
As another example, re-dundancy in the retrieved passages has been used bythe AskMSR system (Brill et al 2002) to select an-swers.
Tellex et al(2003) report a thorough com-parison of passage retrieval methods for QA, up to2003.
Additional improvements have been achievedby using deeper analysis of the text.
For exam-ple, Cui et al(2005) exploited dependency relationsbetween the question terms, Aktolga et al(2011)incorporated syntactic structure and answer typing,while Harabagiu et al(2005) used semantic analy-sis at all stages of the question answering process.
Inthis paper, we pursue a complementary direction, byexploiting searcher examination behavior, with theassumption that human searchers can easily zoom inon relevant passages as part of normal searching.It has been previously recognized that searcher in-teractions could be valuable for question answering,and a task on Complex Interactive QA has been ranas part of TREC 2007 (Kelly and Lin, 2007).
Ourwork goes much further by considering not only ex-plicit interactions, but also the searcher examinationbehavior (i.e., detailed information on which textpassages were examined) ?
which, as we show, pro-vides additional valuable information for passage re-trieval.
Furthermore, it has been recognized that thequestions used in traditional TREC QA evaluationmay not be reflective of the ?real?
questions, posedby users (Bernardi and Kirschner, 2010).
Our paperuses a subset of the real questions posted by users onCommunity Question Answering (CQA) sites, andsearches and interactions from real users ?
whichmakes our task unique and more challenging thanthe previous settings.In particular, our work builds on the rich his-tory of using eye tracking technology to identifyareas of interest and attention, and to study read-ing behavior.
In the context of web search docu-ment examination, Buscher et al(2008) extractedsub-documents by tracking eye movements as im-plicit feedback and expanded search queries to im-prove the search result ranking.
Buscher et alalsostudied the prediction of salient Web page regionsusing eye-tracking (Buscher et al 2009a).
Thiswork, and others, have shown that user attention canhelp identify regions of documents of particular rel-evance or usefulness for the query.
While eye track-ing equipment limits the applicability of these find-ings to lab studies, these studies served as inspira-tion to our work to detect the inferred areas of in-terest.
Specifically, we use mouse cursor trackingas a natural proxy for user?s attention, to replace therequirement for eye tracking equipment.
As origi-nally reported by Rodden et al(2008), the authorsdiscovered the coordination between a user?s eyemovements and mouse movements when scanninga web search results page.
This work was furtherextended by Huang et al(2012) to predict the gazeposition from mouse cursor movement, with meanerror of about 150 px.
In summary, there is mount-ing evidence that the user?s attention in web searchcan be approximated using mouse cursor, scrolling,and other interaction data.
In particular, Hijikata(2004) proposed a method to extract text passagesof Web pages based on the user?s mouse activity andfound that extracted passages based on mouse ac-1012tivity such as text tracing, link pointing, link clickingand text selection enable more accurate extraction ofkey words of interest than using the whole text of thepage.
Recently, White and Buscher (2012) proposeda method that uses text selections as implicit feed-back for document ranking.
Most closely related tothis work is a contemporaneous effort on improv-ing web search result summaries, or snippets, byexploiting searcher behavior on the examined doc-uments, described by Ageev et al(2013).
How-ever, to the best of our knowledge, there has beenno prior work on modeling searcher interaction onresult documents to improve Question Answeringperformance, and in particular the passage retrievalstep.3 Problem Statement and ApproachThis section first states the problem we are address-ing more precisely.
Then, we describe the key partsof our approach (Section 3.2), and the required in-frastructure we had to develop to accomplish the re-quired data collection (Section 3.4).3.1 Problem StatementOur goal is to incorporate the searcher behavior (inparticular, page examination) into passage retrieval.That is, by analyzing the searcher behavior data, weaim to identify the parts of the page that contain rel-evant passages for answering a question.
Specifi-cally, given a question, a set of queries generatedby searchers attempting to answer this question, anda set of documents retrieved by a search engine foreach of the queries, our goal to retrieve a set of pas-sages that contain correct answers for the question.That is, our goal is to identify, from searcher be-havior, the passages in the documents most likely tocontain correct answers to a question, which couldthen be incorporated into a fully automated questionanswering system, or returned to the user directly,for example, by incorporating these passages intothe result abstracts or ?snippets?.3.2 ApproachOur approach accomplishes the goal above by in-corporating both textual and behavioral evidence.Specifically, we combine together traditional text-based passage retrieval features, and the inferreduser interest in specific parts of a document basedon searcher behavior.First, a passage score is obtained from the QA-SYS system (Ng and Kan, 2010), resulting in astrong text-only baseline that generates candidatepassages.
Separately, examination behavior data iscollected over the landing pages, using our logginginfrastructure described in the next section.
Then, abehavior model is trained to identify the passagesof interest to the user, based on user examinationdata (Section 4.2).
Finally, the behavior-based pre-diction of interest in each candidate passage is com-bined with the original (text-based) passage score,in order to generate the final behavior-biased pas-sage ranking (Section 4.3).
Note that by decouplingthe behavior modeling from the candidate genera-tion method, our approach can be used with anyother passage retrieval approach that provides scoresfor the candidate passages (that could be combinedwith the behavior scores for the final ranking step).While general and flexible, our approach makestwo key assumptions, resulting in potential limi-tations.
First, our approach is primarily targeted(and evaluated for) informational questions ?
thatis, questions for which the user expects to find ananswer in the text of the page.
For other questionclasses (e.g., opinion), passage retrieval might haveto be optimized differently.
We also assume thatthe user interactions on landing pages can be col-lected by a search engine or a third party.
This isnot far-fetched: already, browser plug-ins and tool-bars collect some form of user interactions on webpages, major organizations can (and sometimes do)use proxies, and common page widgets like bannerads and visit counters commonly inject JavaScript tomonitor basic user interactions ?
and can be easilyextended to collect the examination data describedin this paper.
The privacy and security of these meth-ods are beyond the scope of this paper, we merelypoint out that these behavior gathering tools, as-sumed by our approach, already exist and are al-ready widely deployed.
The interested reader canobtain an overview of the relevant privacy issuesand proposed solutions in references (Mayer andMitchell, 2012; Krishnamurthy and Wills, 2009).3.3 Acquiring Search Behavior DataOur infrastructure for acquiring search behavior wasdeveloped with two goals in mind: (1) to obtain be-havior data similar to real-world search, with theability to track fine-grained search behavior such as1013a mouse cursor movement (as there are no publiclyavailable data of this kind); (2) to create a controlledand clean ground truth set, to train our system andevaluate the effectiveness of our approach.To collect sufficient amount of search behaviordata, we adapted for our task the publicly availableUFindIt architecture, described in reference Ageevet al(2011).
The participants played several searchcontests, or ?games?, each consisting of 12 searchtasks (questions) to solve.
The stated goal of thegame was to submit the highest possible number ofcorrect answers within the allotted time.
After thesearcher decided that they found the answer, theywere instructed to type the answer together with thesupporting URL into the corresponding fields in thegame interface.
Each search session (for one ques-tion) was completed by either submitting an answeror clicking the ?skip question?
button to pass to thenext question.Participants were recruited through the AmazonMechanical Turk (MTurk) service.
As a first step,the workers had to solve a ReCaptcha puzzle toverify that they are human and not an automated?bot?.
A browser verification check was performedto confirm that the browser was compatible with ourJavaScript tracking code.
During the data postpro-cessing stage, we filtered out the users who did notanswer even the easy, trivial questions, as it indi-cated either poor understanding of the game rules,or an attempt to make a quick buck without effort.In order to capture all of the participants?
searchactions, they were instructed to use only our searchinterface (and not a separate browser window).
Thesearch interface performed the web searches usingthe public API of a popular web search engine, andshowed result pages to the users using the originalpage design, layout and stylesheets, so the user?ssearch experience is not affected.3.4 Page Examination Behavior LoggingA key part of our system is a mechanism for collect-ing searcher interactions on web pages, and tyingthem precisely to the page content at the word level.As the HTML page passed through the proxy, aJavaScript code is embedded to track the user?s inter-actions, including mouse movements and scrolling,as well as the properties of the visited page.
The be-havioral (interaction) events are logged by the searchinterface proxy and written to the server log.To connect the tracked mouse cursor positionsto exact text passages we employed the followingtrick.
After the HTML page is rendered in thebrowser window, our JavaScript code modifies thepage DOM tree so that each word is wrapped by aseparate DOM Element.
Then for each DOM El-ement, the window coordinates of that element areevaluated and saved in an Element?s attribute.
Theprocessed HTML page is then saved to the server byan asynchronous request.
The saved coordinates areupdated if the page layout is changed due to resizewindow event or AJAX action.As a result of this instrumentation, for each pagevisit we know the searcher?s intent (question), asearch engine query that the user issued, a URLand HTML page, the bounding boxes of each wordin the HTML text, and all of the searcher actions,e.g., mouse movement coordinates, mouse clicks,and scrolling.4 Behavior-Biased Passage RetrievalWe now present the details of our behavior-biasedpassage retrieval algorithm (BePR).
First, we de-scribe the text-only retrieval system.
Then, we in-troduce our method for inferring the most interestingor useful parts of the document from user behavior(Section 4.2).4.1 Text-Based Passage RetrievalWe adopt an open-source question answering frame-work QANUS (Ng and Kan, 2010) (versionv29Nov2012).
The QANUS distribution containsthe fully functional factoid QA system QA-SYS thatwe use as a baseline for our experiments.
QA-SYS implements many of the state-of-the-art ques-tion answering techniques, and is similar to a top-performing QA system from TREC (Sun et al2005).
The QA-SYS distribution is configured forprocessing documents and questions in TREC QAformat, and we adopted QA-SYS for answer extrac-tion from web documents.
QA-SYS takes a set ofdocuments and a question as an input, and processesthe input in three stages: (1) information sourcepreparation, (2) question processing, and (3) answerretrieval.In the first stage, the downloaded HTML pagesare pre-processed with Natural Language Tool Kit(NLTK, Bird (2006)).
Extracted text is divided intosentences using Punkt unsupervised sentence split-1014ter (Kiss and Strunk, 2006).
The QA-SYS performsPart of Speech tagging using Stanford POS tagger(Toutanova et al 2003), and Named Entity Recog-nition using Stanford NER (Finkel et al 2005), andthen builds a Lucene index over the set of inputdocuments.
In the second stage the QA-SYS per-forms POS tagging, NE recognition, and questiontype classification for an input question.To answer a question, QA-SYS creates a queryfrom the question, performs the search over the in-dexed text collection, and retrieves top 50 docu-ments.
Each document is split by sentences, andfor each sentence a QA-SYS Passage Retrieval Score(TextScore) is computed as a linear combinationof term frequency score, proximity score, and termcoverage score.
After that 40 passages with the high-est TextScore are retrieved, for each passage QA-SYS performs pattern based answer extraction basedon the identified expected answer type of the ques-tion.As the focus of this paper is to improve PassageRetrieval performance, we use the TextScore sen-tence ranking as a baseline, and improve on it byadding the new search behavioral features indicatingthe passage relevance, as described next.4.2 Inferring Relevant Passages from SearchBehaviorTo rank passages by their ?interestingness?
?
that is,to identify the passages that have been carefully ex-amined by the searcher, we use a learning-to-rankapproach, and apply regression algorithms to predictthe probability that a specific passage is interestingfor a user.
A passage is labeled as ?interesting?, ifthe user submitted an answer in the current session,and both the passage and the answer have at leastone common word, after stemming and stop-wordremoval.For each passage, a set of behavior features thatcould represent passage interestingness is created.To associate behavioral features with a given doc-ument passage, we match the sequence of behav-ior events and the set of bounding boxes for eachword and DOM Element of a page.
For efficiency,we build a spatial R-Tree index of these boundingboxes, which allows us to quickly find the matchingDOM Elements for each event.One key feature is the duration of the time in-terval when a mouse cursor was hovering over theFeature DescriptionMouseOverTime Time duration when the mousecursor was over the text passageMouseNearTime Time duration when the mousecursor was close to the textpassage in the window(x?
100px, y ?
70px)MouseOverEvents The number of mouse eventsduring MouseOverTimeMouseNearEvents The number of mouse eventsduring MouseNearTimeDispTime Time duration when the textpassage has been visible inthe browser window(depends on scrollbar position)DispMiddleTime Time duration when the textpassage was visible in the middlepart of the browser windowTable 1: Behavior features for text passagesspecific text passage, or very close to the passage.We also take a scrollbar and event count featuresfrom papers (Buscher et al 2009b), and (Guo andAgichtein, 2012) to detect evidence of ?reading?
vs.?skimming?
behavior, and adopt those features torepresent the behavior near the specific location ofa page.
The full set of our passage behavior featuresare reported in Table 1.To implement the passage ranker, we experi-mented with a variety of learning-to-rank (LTR) al-gorithms, and chose two implementations of Regres-sion Trees, due to their strong performance for gen-eral web search ranking tasks.
The first algorithmis Regression Tree (Friedman et al 2001), and thesecond is Gradient Boosting Regression Tree algo-rithm (Friedman, 2001).
They are named BePR-BTree, and BePR-GBM respectively.The dataset consists of a set of questions, with as-sociated search behavior data collected from all theusers who tried to find an answer to this question,the answers submitted by the users, and a set of val-idated answers.
These sets are divided into train-ing, validation, and test, so that the training and val-idation set URLs are disjoint, and the test set haveno intersection with training and validation set byURLs, questions, and users.
The training set is cre-ated from only those page visits where the documenttext has non-empty intersection with the user?s an-swer, and the answer is correct.
The trained regres-1015sion algorithm is applied to all page visits in the testset.
When the trained model is applied at test time,it has no information about the user?s intent, the cor-rect answer, or the current query, but rather uses onlythe behavioral features of the current page visit toidentify the ?interesting?
passages.The predicted probability of passage interesting-ness is averaged over all the users and page visits,and the resulting passage interestingness is then usedas the BScore of the passage.
Note that BScoreis defined for only visited pages; to incorporate theoverall clickthrough information (i.e., the fraction ofthe time a page was visited, indicating relevance),we introduce a generalized version, designated asBSscoreAll, defined as: ?
?CTR+(1??
)?BScore,where CTR is the clickthrough rate for the page,defined as the fraction of time the result was clickedfor all searches.
Intuitively, this version reduces theweight of the behavior score for the pages with in-sufficient behavior data by ?backing off?
to the doc-ument clickthrough rate, according to the parame-ter ?.
For the cases where only the visited pagesare considered (ignoring the searches when the pagewas not visited), ?
is set to 0, reverting the scoreto the original BScore definition.
The resultingbehavior-based passage score is then used as the ag-gregate value of searcher interest in the passage forthe combined passage retrieval step, described next.4.3 Combining Textual and BehavioralEvidenceThe final step in our approach is to combine thetext-based score TextScore(f) for a sentence (Sec-tion 4.1) with the interestingness score BScore(f)(Section 4.2), inferred from the examination data.
Inour current implementation we combine these scoresby linear combination:FScore(f) =?
?BScore(f)+ (1?
?)
?
TextScore(f)Other more sophisticated ways to combine textand behavior evidence are possible, such as jointlylearning over both text and behavior features.
How-ever, we chose to follow the simpler linear approachfor interpretability of the results (e.g., by varying the?
parameter).5 Data Collection and Experimental SetupThis section presents the methodology used for se-lecting the questions (Section 5.1), the correspond-ing search behavior data (Section 5.2), and the ex-perimental collections and metrics (Section 5.3).5.1 QuestionsThe search tasks were selected from communityquestion answering sites such as wiki.answers.comand Yahoo!
Answers by the researchers.
The cri-teria used were that the question should be clearlystated, had a clear answer, and that finding this an-swer was not a trivial task, that is, the answer wasnot retrieved simply by submitting the question ver-batim to Google, Bing, or Yahoo!
Search engines.Overall, 36 such questions were selected, posing (asit turned out) greatly varying levels of difficulty forparticipants.
These questions were randomly splitinto three game rounds of 12 questions each.5.2 Browsing Behavior DatasetThe search behavior data for each of the questionsabove was acquired as described in Section 3.3.
Atotal of 270 participants finished the game.
Af-ter filtering out users who did not follow the gamerules, we have 3047 search sessions performed by265 users.
Our data for these users consists of 7800queries, 3910 unique queries, 8574 SERP clicks on1544 distinct URLs.
For 5683 page visits (66%)and 883 distinct URLs the on-page behavioral datais collected.
For the rest 34% of page visits the be-havioral data were not collected due to conflicts be-tween our JavaScript tracking code and other codepresented on the page.
For each page view thereare about 400 atomic browsing events (mouse move-ments, scrolling, key pressing) on average.
All thesource and derived data are available at http://ir.mathcs.emory.edu/intent.The dataset is divided into training, validation,and test set in the following way.
The behaviordataset for the first game is divided randomly intoequal-sized training and validation sets that are dis-joint by URLs.
The training set was used to trainthe regression algorithm for predicting passage at-tractiveness, and the validation set was used to ex-plore the influence of behavior weight ?
on passageretrieval performance, and to select the parameter ?for using on a test set.
The validation set consistsof 254 different URLs spread over 11 questions, and1016for each of them there is a collected browsing be-havior.The test set consists of 441 URLs spread over 24questions, and the test set has no intersection withtraining and validation set by URLs, questions, andusers.5.3 Candidate Document Selection StrategiesThe first step for question answering is a selectionof a candidate document set.
In our settings, wemay select a subset of web documents in a differ-ent way.
We explore passage retrieval effectivenessusing three different strategies of document set se-lection.?
For each question select All documents thatare in top 10 documents returned by a searchengine for any query that was issued duringsearch for the specific question.
For our datasetthis gives around 500 candidate documents perquestion on average.?
For each question select only documents thatwere Clicked by a user.
This restricts a can-didate document set to set of most promisingdocuments.
For our dataset this gives around25 candidate documents per question on aver-age.?
For each pair of question and Relevant docu-ment apply passage retrieval to the specific doc-ument.
In this experiment we label a document?Relevant?
if a correct answer was extractedfrom it.
In a real-world scenario, while doc-ument relevance could be estimated by a va-riety of click-based methods, we address thechallenge of how to actually extract the cor-rect answer from the document, automatically,with the help of the natural behavior data.
Weperform this experiment to estimate the perfor-mance of passage retrieval for the case whenrelevant documents are known with high confi-dence.Evaluation Metrics: We evaluate passage retrievalperformance by standard Mean Reciprocal Rank(MRR), and Mean Average Precision (MAP) met-rics for top 20 retrieved sentences (Voorhees andTice, 1999).
We also evaluate ROUGE-1 met-ric (Lin, 2004) for the first retrieved passage.00.020.040.060.080.10.120.140 0.1 0.2 0.3 0.4 0.5ROUGEfragment scoreuser's answer ROUGE-1user's answer ROUGE-2all correct answers ROUGE-1all correct answers ROUGE-2Figure 1: The actual passage interestingness, measuredby intersection with user?s answer, vs. the passage rele-vance score BScore predicted from behavior data6 ResultsWe now present the empirical results.
First, we re-port the intermediate result of using behavior data toinfer the interesting (useful) passages in the docu-ment.
Then, we report the main results of the paperwhere the quality of the generated snippets with andwithout using behavior data is compared using hu-man judgments.6.1 Prediction of Passage InterestingnessThis experiment evaluates how well we can predictinteresting passages by observing a user?s on-pagebehavior.
We suppose that the passage is interestingif it is related to the answer for the question.
Foreach visited page, we collect the user?s answer (ifsubmitted), and all correct answers from all userswho answered this question.
Then, we comparethose answers to each text passage in the documentusing ROUGE metrics (Lin, 2004).Figure 1 shows the relationship between the in-terestingness of a passage and behavior score.
Thegraph shows that when the score is high (?
0.5),then average intersection between the passage anduser?s answer is much higher than those when thepassage score is low.
All ROUGE-N metrics sig-nificantly grow when the behavior score grows, al-though ROUGE-2 over all correct answers are al-ways very small (it grows from 0.003 to 0.007).ROUGE-1 is much greater than ROUGE-2 for highscores, as the interesting passage might contain use-ful information for the answer, but the user reformu-lates the obtained information and submits reformu-lated answer.
The ROUGE-N metrics for a user?sanswer are much greater than those for all correct1017Feature Feature ImportanceDispMiddleTime 0.51MouseOverTime 0.34DispTime 0.12MouseNearTime 0.02MouseOverEvents 0.01MouseNearEvents 0.01Table 2: Feature importance for behavioral features, asmeasured by Gini coefficientanswers, as other users might obtain valuable infor-mation from other documents, and some questionshave distinct correct answers.Behavior Feature Importance Analysis: To esti-mate relative importance of behavior features weevaluated the Gini importance index (Breiman,1996) for each behavior feature from the Table 1.The Table 2 shows that the most important featuresare the time duration when the text passage was vis-ible in the middle part of the scrolling window, andthe time duration when the mouse cursor was overthe text passage.
The first feature has been shownto be a good feature for re-ranking search resultsin reference (Buscher et al 2009b), and we haveshown that it is also useful for passage retrieval.
TheMouseOverTime feature has been previously shownto be correlated with examination time, measuredby eye-tracking experiments (Guo and Agichtein,2010), and it helps us detect local behavior in theneighborhood of a specific text passage.Analysis of Searcher Attention: In order to betterunderstand what characteristics of the textual pas-sages attract the searcher?s attention, we explored21 linguistic features for each sentence.
Our fea-tures were designed to estimate text readability, andthe overlap of a passage with the query that wasused to find the document.
We implemented thereadability features from (Kanungo and Orr, 2009),and query matching features from (Metzler and Ka-nungo, 2008).
Table 3 reports the top 10 featureswith the highest absolute value of the correlation co-efficient with passage interestingness scoreBScore.Interestingly, the most highly correlated features arerelated to readability, while query matching featuresare less important.Feature description corrNumber of distinct words in the passage 0.31Total number of words in the passage 0.28Number of letter ([a-zA-z]) characters 0.27Relative location of the passage in the document -0.25Number of unique words in the passage -0.24divided by total number of wordsNumber of punctuation characters -0.20Number of words with first letter capitalized -0.17Overlap of query terms expanded 0.15with synonyms and the passageAbsolute count of query terms 0.15matched in the passageAverage position of query term within the passage -0.14Table 3: Correlation of passage interestingness BScorewith linguistic properties of a sentenceFigure 2: MRR for passage retrieval for varying behav-ior weight ?
and interestingness prediction algorithmsBePR-DTree, and BePR-GBM6.2 Passage Retrieval with Behavior DataThis section reports the main results of the paper.First, we describe the parameter tuning, followed bythe main performance results.Parameter Tuning: To tune the passage retrievalperformance, we use the validation set to find theoptimal value for ?.
Figure 2 reports the passageretrieval MRR for varying ?, for two learning al-gorithms BePR-GBM and BePR-DTree.
The figureshows that both BePR-GBM and BePR-BTree im-prove over the QA-SYS baseline.
BePR-GBM algo-rithm achieves the best performance with ?
= 0.8,and also exhibits more robust behavior compared toBePR-BTree, so we use BePR-GBM with ?
= 0.8for the main experiments described next.
Similarly,using the training and validation sets, we optimized1018  	QA-SYSBePR  	QA-SYSBePR  	QA-SYSBePRFigure 3: Passage retrieval MRR (a), ROUGE1 (b), and MAP (c) for the BePR and QA-SYS systems, on the test set.the value of the clickthrough rate weight ?
= 0.05(used for the BScoreAll score) for the All documentset only (as for the Clicked and Relevant documentsets, ?
is always set to 0 by construction).Main retrieval results: We now compare the base-line algorithm for passage retrieval implemented inQA-SYS system and described in section 4.1 withthe BePR algorithm (section 4.2-4.3) that combinesthe textual passage score and the behavior score us-ing the ?
parameter for the relative weight of thebehavior evidence.Figure 3 reports the main results of the paper,namely the MRR, ROUGE-1@1 and MAP pas-sage retrieval metrics for the baseline QA-SYS al-gorithm, and BePR-GBM, on the test set.
As thefigure shows, BePR achieves higher performanceon all metrics, and for all document sets.
The im-provements are statistically significant (p < 0.01)for experiments with Clicked and Relevant docu-ment sets.
Not surprisingly, the improvements aresmallest when All documents are considered, as un-clicked documents do not provide any associated be-havior data.
As the results show, our simple back-offstrategy (using the document clickthrough rate withthe ?
parameter) is moderately successful, but couldbe further refined in the future.Finally, we illustrate how behavior features affectpassage ranking.
Let?s consider a question ?Howmany Swedes speak English as a percentage??.
Theperfect relevant page for this question is a Wikipediapage ?Languages of Sweden?.
A sentence ?Mainforeign language(s): English 89%, German 30%,French 11%.?
contains an answer to the question, butit has only a small intersection with question terms,and QA-SYS ranks this question in the 13th place.Other sentences that contain a country name, a num-ber, or have more terms that match the question areranked higher.
In contrast, as searchers examinedthis sentence carefully to find the answer, BePR isable to promote this sentence to the second place inthe ranking.7 Resources and DataAll the code and the collected data used in thisresearch are available at http://ir.mathcs.emory.edu/intent/.
The dataset contains theset of questions used for the experiments, and user?sbehavior: queries submitted by users to searchengine, result pages, visited URLs, downloadedlanding pages, on-page browsing behavior (mousemovements, scrollbar events, resize actions, clicks).By sharing our code and data, we hope to encouragefurther research in this area.8 Conclusions and Future WorkWe presented the first successful approach to incor-porating naturalistic searcher behavior data into pas-sage retrieval for question answering.
Specifically,we developed a robust method to infer searcher in-terest in specific parts of the document, which couldthen be combined with more traditional textual fea-tures used for passage retrieval.
Our results showsignificant improvements over a strong baseline, de-rived from a competitive Question Answering sys-tem.To implement the proposed method in a real-world search engine for Web QA, the proposed in-frastructure and/or the released data could be usedas a training set for the algorithm that predicts frag-ment interestingness from user behavior.
Such asystem would need to track document examinationdata.
This can already be done by incorporating ourreleased tracking code or a similar method into a1019browser toolbar, banner ad system, visit counters orother JavaScript widgets that already track user vis-its.
While we acknowledge user privacy as an im-portant concern, it is beyond the scope of this work.In the future, we plan to extend this work to moreprecisely pinpoint the answer location on a page,and consequently incorporate searcher behavior intosubsequent answer extraction and ranking stages ofquestion answering.
We also plan to further investi-gate the examination data to better understand howsearchers find correct (and incorrect) answers usingboth general web search engines and QA systems ?in order to inform and further improve query sug-gestion, result snippet generation, and result rankingalgorithms.AcknowledgmentsThis work was supported by the National ScienceFoundation grant IIS-1018321, the DARPA grantD11AP00269, the Yahoo!
Faculty Research En-gagement Program, and by the Russian Foundationfor Basic Research Grant 12-07-31225.ReferencesMikhail Ageev, Qi Guo, Dmitry Lagun, and EugeneAgichtein.
2011.
Find it if you can: a game for mod-eling different types of web search success using inter-action data.
In Proceedings of the 34th internationalACM SIGIR conference on Research and developmentin Information Retrieval, SIGIR ?11, pages 345?354,New York, NY, USA.
ACM.Mikhail Ageev, Dmitry Lagun, and Eugene Agichtein.2013.
Improving search result summaries by usingsearcher behavior data.
In Proceedings of the 36th in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval, SIGIR ?13.Elif Aktolga, James Allan, and David A. Smith.
2011.Passage reranking for question answering using syn-tactic structures and answer types.
In Proceedingsof the 33rd European conference on Advances in in-formation retrieval, ECIR?11, pages 617?628, Berlin,Heidelberg.
Springer-Verlag.Raffaella Bernardi and Manuel Kirschner.
2010.
Fromartificial questions to real user interaction logs: Realchallenges for interactive question answering systems.In Proceedings of Workshop on Web Logs and Ques-tion Answering, pages 8?15.Steven Bird.
2006.
Nltk: the natural language toolkit.
InProceedings of the COLING/ACL on Interactive pre-sentation sessions, COLING-ACL ?06, pages 69?72,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Leo Breiman.
1996.
Bagging predictors.
Mach.
Learn.,24(2):123?140.Eric Brill, Susan Dumais, and Michele Banko.
2002.
Ananalysis of the askmsr question-answering system.
InProc.
of ACL, EMNLP ?02, pages 257?264, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Georg Buscher, Andreas Dengel, and Ludger van Elst.2008.
Query expansion using gaze-based feedback onthe subdocument level.
In Proceedings of the 31stannual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?08, pages 387?394, New York, NY, USA.
ACM.Georg Buscher, Edward Cutrell, and Meredith RingelMorris.
2009a.
What do you see when you?re surf-ing?
: using eye tracking to predict salient regions ofweb pages.
In Proceedings of the SIGCHI Conferenceon Human Factors in Computing Systems, CHI ?09,pages 21?30.
ACM.Georg Buscher, Ludger van Elst, and Andreas Dengel.2009b.
Segment-level display time as implicit feed-back: a comparison to eye tracking.
In Proceedings ofthe 32nd international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?09, pages 67?74, New York, NY, USA.
ACM.Charles Clarke, Gordon Cormack, Derek Kisman, andThomas Lynam.
2000.
Question answering by pas-sage selection (multitext experiments for trec-9).
InProceedings of the Ninth Text REtrieval Conference(TREC-9).Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua.
2005.
Question answering passage re-trieval using dependency relations.
In Proceedingsof the 28th annual international ACM SIGIR confer-ence on Research and development in information re-trieval, SIGIR ?05, pages 400?407, New York, NY,USA.
ACM.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jerome Friedman, Trevor Hastie, and Robert Tibshirani.2001.
The elements of statistical learning, volume 1.Springer Series in Statistics.Jerome H. Friedman.
2001.
Greedy function approxi-mation: A gradient boosting machine.
The Annals ofStatistics, 29(5):pp.
1189?1232.Qi Guo and Eugene Agichtein.
2010.
Towards predictingweb searcher gaze position from mouse movements.In CHI ?10 Extended Abstracts on Human Factors inComputing Systems, CHI EA ?10, pages 3601?3606,New York, NY, USA.
ACM.1020Qi Guo and Eugene Agichtein.
2012.
Beyond dwelltime: estimating document relevance from cursormovements and other post-click searcher behavior.
InProceedings of the 21st international conference onWorld Wide Web, WWW ?12, pages 569?578, NewYork, NY, USA.
ACM.Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, Andrew Hickl, and Patrick Wang.2005.
Employing two question answering systems intrec-2005.
In Proceedings of the fourteenth text re-trieval conference.Yoshinori Hijikata.
2004.
Implicit user profiling for ondemand relevance feedback.
In Proceedings of the 9thinternational conference on Intelligent user interfaces,IUI ?04, pages 198?205, New York, NY, USA.
ACM.Jeff Huang, Ryen White, and Georg Buscher.
2012.
Usersee, user point: gaze and cursor alignment in websearch.
In Proceedings of the SIGCHI Conference onHuman Factors in Computing Systems, CHI ?12, pages1341?1350, New York, NY, USA.
ACM.Tapas Kanungo and David Orr.
2009.
Predicting thereadability of short web summaries.
In Proceedingsof the Second ACM International Conference on WebSearch and Data Mining, WSDM ?09, pages 202?211,New York, NY, USA.
ACM.Diane Kelly and Jimmy Lin.
2007.
Overview of the trec2006 ciqa task.
In ACM SIGIR Forum, volume 41,pages 107?116.
ACM.Tibor Kiss and Jan Strunk.
2006.
Unsupervised multilin-gual sentence boundary detection.
Comput.
Linguist.,32(4):485?525, December.Balachander Krishnamurthy and Craig Wills.
2009.
Pri-vacy diffusion on the web: a longitudinal perspective.In Proceedings of the 18th international conferenceon World wide web, WWW ?09, pages 541?550, NewYork, NY, USA.
ACM.Chin-Yew Lin.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
Barcelona, Spain, July.
As-sociation for Computational Linguistics.Jonathan R. Mayer and John C. Mitchell.
2012.
Third-party web tracking: Policy and technology.
In Pro-ceedings of the 2012 IEEE Symposium on Securityand Privacy, SP ?12, pages 413?427, Washington, DC,USA.
IEEE Computer Society.D.
Metzler and T. Kanungo.
2008.
Machine learned sen-tence selection strategies for query-biased summariza-tion.
In SIGIR Learning to Rank Workshop.Jun-Ping Ng and Min-Yen Kan. 2010.
Qanus:An open-source question-answering platformhttp://www.comp.nus.edu.sg/ junping/docs/qanus.pdf.Kerry Rodden, Xin Fu, Anne Aula, and Ian Spiro.
2008.Eye-mouse coordination patterns on web search re-sults pages.
In CHI ?08 Extended Abstracts on HumanFactors in Computing Systems, CHI EA ?08, pages2997?3002, New York, NY, USA.
ACM.Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-SengChua, and Min-Yen Kan. 2005.
Using syntactic andsemantic relation analysis in question answering.
InTREC.Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-des, and Gregory Marton.
2003.
Quantitative evalu-ation of passage retrieval algorithms for question an-swering.
In Proceedings of the 26th annual interna-tional ACM SIGIR conference on Research and devel-opment in informaion retrieval, SIGIR ?03, pages 41?47, New York, NY, USA.
ACM.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Ellen Voorhees and Dawn M Tice.
1999.
The trec-8question answering track evaluation.
In Proceedingsof The Eighth Text REtrieval Conference (TREC-8),http://trec.
nist.
gov/pubs/trec8/t8 proceedings.
html.Ryen W. White and Georg Buscher.
2012.
Text se-lections as implicit relevance feedback.
In Proceed-ings of the 35th international ACM SIGIR conferenceon Research and development in information retrieval,pages 1151?1152, New York, NY, USA.
ACM.1021
