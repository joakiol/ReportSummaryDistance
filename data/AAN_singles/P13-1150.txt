Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1527?1536,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUnsupervised Consonant-Vowel Prediction over Hundreds of LanguagesYoung-Bum Kim and Benjamin SnyderUniversity of Wisconsin-Madison{ybkim,bsnyder}@cs.wisc.eduAbstractIn this paper, we present a solution to oneaspect of the decipherment task: the pre-diction of consonants and vowels for anunknown language and alphabet.
Adopt-ing a classical Bayesian perspective, weperforms posterior inference over hun-dreds of languages, leveraging knowledgeof known languages and alphabets to un-cover general linguistic patterns of typo-logically coherent language clusters.
Weachieve average accuracy in the unsuper-vised consonant/vowel prediction task of99% across 503 languages.
We furthershow that our methodology can be usedto predict more fine-grained phonetic dis-tinctions.
On a three-way classificationtask between vowels, nasals, and non-nasal consonants, our model yields unsu-pervised accuracy of 89% across the sameset of languages.1 IntroductionOver the past centuries, dozens of lost languageshave been deciphered through the painstakingwork of scholars, often after decades of slowprogress and dead ends.
However, several impor-tant writing systems and languages remain unde-ciphered to this day.In this paper, we present a successful solutionto one aspect of the decipherment puzzle: auto-matically identifying basic phonetic properties ofletters in an unknown alphabetic writing system.Our key idea is to use knowledge of the phoneticregularities encoded in known language vocabu-laries to automatically build a universal probabilis-tic model to successfully decode new languages.Our approach adopts a classical Bayesian per-spective.
We assume that each language hasan unobserved set of parameters explaining itsobserved vocabulary.
We further assume thateach language-specific set of parameters was itselfdrawn from an unobserved common prior, sharedacross a cluster of typologically related languages.In turn, each cluster derives its parameters froma universal prior common to all language groups.This approach allows us to mix together data fromlanguages with various levels of observations andperform joint posterior inference over unobservedvariables of interest.At the bottom layer (see Figure 1), ourmodel assumes a language-specific data generat-ing HMM over words in the language vocabulary.Each word is modeled as an emitted sequence ofcharacters, depending on a corresponding Markovsequence of phonetic tags.
Since individual lettersare highly constrained in their range of phoneticvalues, we make the assumption of one-tag-per-observation-type (e.g.
a single letter is constrainedto be always a consonant or always a vowel acrossall words in a language).Going one layer up, we posit that the language-specific HMM parameters are themselves drawnfrom informative, non-symmetric distributionsrepresenting a typologically coherent languagegrouping.
By applying the model to a mix of lan-guages with observed and unobserved phonetic se-quences, the cluster-level distributions can be in-ferred and help guide prediction for unknown lan-guages and alphabets.We apply this approach to two small decipher-ment tasks:1. predicting whether individual characters inan unknown alphabet and language representvowels or consonants, and2.
predicting whether individual characters inan unknown alphabet and language representvowels, nasals, or non-nasal consonants.For both tasks, our approach yields considerable1527success.
We experiment with a data set consist-ing of vocabularies of 503 languages from aroundthe world, written in a mix of Latin, Cyrillic, andGreek alphabets.
In turn for each language, weconsider it and its alphabet ?unobserved?
?
wehide the graphic and phonetic properties of thesymbols ?
while treating the vocabularies of theremaining languages as fully observed with pho-netic tags on each of the letters.On average, over these 503 leave-one-language-out scenarios, our model predicts consonant/voweldistinctions with 99% accuracy.
In the more chal-lenging task of vowel/nasal/non-nasal prediction,our model achieves average accuracy over 89%.2 Related WorkThe most direct precedent to the present work isa section in Knight et al (2006) on universal pho-netic decipherment.
They build a trigram HMMwith three hidden states, corresponding to conso-nants, vowels, and spaces.
As in our model, indi-vidual characters are treated as the observed emis-sions of the hidden states.
In contrast to the presentwork, they allow letters to be emitted by multiplestates.Their experiments show that the HMM trainedwith EM successfully clusters Spanish letters intoconsonants and vowels.
They further design amore sophisticated finite-state model, based onlinguistic universals regarding syllable structureand sonority.
Experiments with the second modelindicate that it can distinguish sonorous conso-nants (such as n, m, l, r) from non-sonorous con-sonants in Spanish.
An advantage of the linguis-tically structured model is that its predictions donot require an additional mapping step from unin-terpreted hidden states to linguistic categories, asthey do with the HMM.Our model and experiments can be viewed ascomplementary to the work of Knight et al, whilealso extending it to hundreds of languages.
Weuse the simple HMM with EM as our baseline.
Inlieu of a linguistically designed model structure,we choose an empirical approach, allowing poste-rior inference over hundreds of known languagesto guide the model?s decisions for the unknownscript and language.In this sense, our model bears some similarityto the decipherment model of Snyder et al (2010),which used knowledge of a related language (He-brew) in an elaborate Bayesian framework to de-cipher the ancient language of Ugaritic.
While theaim of the present work is more modest (discover-ing very basic phonetic properties of letters) it isalso more widely applicable, as we don?t requireddetailed analysis of a known related language.Other recent work has employed a simi-lar perspective for tying learning across lan-guages.
Naseem et al (2009) use a non-parametricBayesian model over parallel text to jointly learnpart-of-speech taggers across 8 languages, whileCohen and Smith (2009) develop a shared logis-tic normal prior to couple multilingual learningeven in the absence of parallel text.
In simi-lar veins, Berg-Kirkpatrick and Klein (2010) de-velop hierarchically tied grammar priors over lan-guages within the same family, and Bouchard-C?t?
et al (2013) develop a probabilistic model ofsound change using data from 637 Austronesianlanguages.In our own previous work, we have developedthe idea that supervised knowledge of some num-ber of languages can help guide the unsupervisedinduction of linguistic structure, even in the ab-sence of parallel text (Kim et al, 2011; Kim andSnyder, 2012)1.
In the latter work we also tack-led the problem of unsupervised phonemic predic-tion for unknown languages by using textual reg-ularities of known languages.
However, we as-sumed that the target language was written in aknown (Latin) alphabet, greatly reducing the dif-ficulty of the prediction task.
In our present case,we assume no knowledge of any relationship be-tween the writing system of the target languageand known languages, other than that they are allalphabetic in nature.Finally, we note some similarities of our modelto some ideas proposed in other contexts.
Wemake the assumption that each observation type(letter) occurs with only one hidden state (con-sonant or vowel).
Similar constraints have beendeveloped for part-of-speech tagging (Lee et al,2010; Christodoulopoulos et al, 2011), and thepower of type-based sampling has been demon-strated, even in the absence of explicit model con-straints (Liang et al, 2010).3 ModelOur generative Bayesian model over the ob-served vocabularies of hundreds of languages is1We note that similar ideas were simultaneously proposedby other researchers (Cohen et al, 2011).15281529For example, the cluster Poisson parameter overvowel observation types might be ?
= 9 (indi-cating 9 vowel letters on average for the cluster),while the parameter over consonant observationtypes might be ?
= 20 (indicating 20 consonantletters on average).
These priors will be distinctfor each language cluster and serve to characterizeits general linguistic and typological properties.We pause at this point to review the Dirich-let distribution in more detail.
A k?dimensionalDirichlet with parameters ?1 ...?k defines a distri-bution over the k ?
1 simplex with the followingdensity:f(?1 ... ?k|?1 ... ?k) ??i?
?i?1iwhere ?i > 0, ?i > 0, and?i ?i = 1.
The Dirich-let serves as the conjugate prior for the Multino-mial, meaning that the posterior ?1...?k|X1...Xn isagain distributed as a Dirichlet (with updated pa-rameters).
It is instructive to reparameterize theDirichlet with k + 1 parameters:f(?1 ... ?k|?0, ?
?1 ...
?
?k) ??i??0?
?i?1iwhere ?0 = ?i ?i, and ?
?i = ?i/?0.
In thisparameterization, we have E[?i] = ??i.
In otherwords, the parameters ?
?i give the mean of the dis-tribution, and ?0 gives the precision of the dis-tribution.
For large ?0 ?
k, the distribution ishighly peaked around the mean (conversely, when?0 ?
k, the mean lies in a valley).Thus, the Dirichlet parameters of a languagecluster characterize both the average HMMs of in-dividual languages within the cluster, as well ashow much we expect the HMMs to vary fromthe mean.
In the case of emission distribu-tions, we assume symmetric Dirichlet priors?
i.e.one-parameter Dirichlets with densities given byf(?1 ...?k|?)
??
?(?
?1)i .
This assumption is nec-essary, as we have no way to identify charactersacross languages in the decipherment scenario,and even the number of consonants and vowels(and thus multinomial/Dirichlet dimensions) canvary across the languages of a cluster.
Thus, themean of these Dirichlets will always be a uniformemission distribution.
The single Dirichlet emis-sion parameter per cluster will specify whetherthis mean is on a peak (large ?)
or in a valley(small ?).
In other words, it will control the ex-pected sparsity of the resulting per-language emis-sion multinomials.In contrast, the transition Dirichlet parametersmay be asymmetric, and thus very specific andinformative.
For example, one cluster may havethe property that CCC consonant clusters are ex-ceedingly rare across all its languages.
This prop-erty would be expressed by a very small mean?
?CCC ?
1 but large precision ?0.
Later we shallsee examples of learned transition Dirichlet pa-rameters.3.3 Cluster GenerationThe generation of the cluster parameters (Algo-rithm 1) defines the highest layer of priors for ourmodel.
As Dirichlets lack a standard conjugateprior, we simply use uniform priors over the in-terval [0, 500].
For the cluster Poisson parameters,we use conjugate Gamma distributions with vaguepriors.34 InferenceIn this section we detail the inference proce-dure we followed to make predictions under ourmodel.
We run the procedure over data from503 languages, assuming that all languages butone have observed character and tag sequences:w1, w2, .
.
.
, t1, t2, .
.
.
Since each character type wis assumed to have a single tag category, this isequivalent to observing the character token se-quence along with a character-type-to-tag map-ping tw.
For the target language, we observe onlycharacter token sequence w1, w2, .
.
.We assume fixed and known parameter val-ues only at the cluster generation level.
Unob-served variables include (i) the cluster parameters?, ?, ?, (ii) the cluster assignments z, (iii) the per-language HMM parameters ?, ?
for all languages,and (iv) for the target language, the tag tokenst1, t2, .
.
.
?
or equivalently the character-type-to-tag mappings tw ?
along with the observationtype-counts Nt.4.1 Monte Carlo ApproximationOur goal in inference is to predict the most likelytag tw,?
for each character type w in our target lan-guage ?
according to the posterior:f (tw,?
|w, t??
)=?f (t?, z, ?, ?
|w, t??)
d?
(1)3(1,19) for consonants, (1,10) for vowels, (0.2, 15) fornasals, and (1,16) for non-nasal consonants.1530where ?
= (t?w,?, z, ?, ?
), w are the observedcharacter sequences for all languages, t??
are thecharacter-to-tag mappings for the observed lan-guages, z are the language-to-cluster assignments,and ?
and ?
are all the cluster-level transition andemission Dirichlet parameters.Sampling values (t?, z, ?, ?
)Nn=1 from the inte-grand in Equation 1 allows us to perform the stan-dard Monte Carlo approximation:f (tw,?
= t |w, t??)?
N?1N?n=1I (tw,?
= t in sample n) (2)To maximize the Monte Carlo posterior, we sim-ply take the most commonly sampled tag valuefor character type w in language ?.
Note thatwe leave out the language-level HMM parame-ters (?, ?)
as well as the cluster-level Poisson pa-rameters ?
from Equation 1 (and thus our samplespace), as we can analytically integrate them outin our sampling equations.4.2 Gibbs SamplingTo sample values (t?, z, ?, ?)
from their poste-rior (the integrand of Equation 1), we use Gibbssampling, a Monte Carlo technique that constructsa Markov chain over a high-dimensional samplespace by iteratively sampling each variable condi-tioned on the currently drawn sample values forthe others, starting from a random initialization.The Markov chain converges to an equilibriumdistribution which is in fact the desired joint den-sity (Geman and Geman, 1984).
We now sketchthe sampling equations for each of our sampledvariables.Sampling tw,?To sample the tag assignment to character w inlanguage ?, we need to compute:f (tw,?
|w, t?w,?, t?
?, z, ?, ?)
(3)?
f (w?, t?, N?
| ?k, ?k,Nk??)
(4)where N?
are the types-per-tag counts implied bythe mapping t?, k is the current cluster assignmentfor the target language (z?
= k), ?k and ?k are thecluster parameters, andNk??
are the types-per-tagcounts for all languages currently assigned to thecluster, other than language ?.Applying the chain rule along with our model?sconditional independence structure, we can furtherre-write Equation 4 as a product of three terms:f(N?|Nk??)
(5)f(t1, t2, .
.
.
|?k) (6)f(w1, w2, .
.
.
|N?, t1, t2, .
.
.
, ?k) (7)The first term is the posterior predictive distribu-tion for the Poisson-Gamma compound distribu-tion and is easy to derive.
The second term is thetag transition predictive distribution given Dirich-let hyperparameters, yielding a familiar Polya urnscheme form.
Removing terms that don?t dependon the tag assignment t?,w gives us:?t,t?(?k,t,t?
+ n(t, t?))[n?(t,t?)]?t(?t?
?k,t,t?
+ n(t))[n?
(t)]where n(t) and n(t, t?)
are, respectively, unigramand bigram tag counts excluding those containingcharacter w. Conversely, n?
(t) and n?
(t, t?)
are,respectively, unigram and bigram tag counts onlyincluding those containing character w. The no-tation a[n] denotes the ascending factorial: a(a +1) ?
?
?
(a+n?1).
Finally, we tackle the third term,Equation 7, corresponding to the predictive dis-tribution of emission observations given Dirichlethyperparameters.
Again, removing constant termsgives us:?[n(w)]k,t?t?
N?,t??[n(t?
)]k,t?where n(w) is the unigram count of character w,and n(t?)
is the unigram count of tag t, over allcharacters tokens (including w).Sampling ?k,t,t?To sample the Dirichlet hyperparameter for clusterk and transition t ?
t?, we need to compute:f(?k,t,t?
|t, z)?
f(t, z|?z,t,t?
)= f(tk|?z,t,t?
)where tk are the tag sequences for all languagescurrently assigned to cluster k. This term is a pre-dictive distribution of the multinomial-Dirichletcompound when the observations are grouped intomultiple multinomials all with the same prior.Rather than inefficiently computing a product ofPolya urn schemes (with many repeated ascending1531factorials with the same base), we group commonterms together and calculate:?j=1(?k,t,t?
+ k)n(j,k,t,t?)?j=1(?t??
?k,t,t??
+ k)n(j,k,t)where n(j, k, t) and n(j, k, t, t?)
are the numbersof languages currently assigned to cluster k whichhave more than j occurrences of unigram (t) andbigram (t, t?
), respectively.This gives us an efficient way to compute un-normalized posterior densities for ?.
However, weneed to sample from these distributions, not justcompute them.
To do so, we turn to slice sam-pling (Neal, 2003), a simple yet effective auxiliaryvariable scheme for sampling values from unnor-malized but otherwise computable densities.The key idea is to supplement the variablex, distributed according to unnormalized densityp?
(x), with a second variable u with joint densitydefined as p(x, u) ?
I(u < p?(x)).
It is easyto see that p?
(x) ?
?
p(x, u)du.
We then itera-tively sample u|x and x|u, both of which are dis-tributed uniformly across appropriately boundedintervals.
Our implementation follows the pseudo-code given in Mackay (2003).Sampling ?k,tTo sample the Dirichlet hyperparameter for clusterk and tag t we need to compute:f(?k,t|t,w, z,N)?
f(w|t, z, ?k,t,N)?
f(wk|tk, ?k,t,Nk)where, as before, tk are the tag sequences forlanguages assigned to cluster k, Nk are the tagobservation type-counts for languages assignedto the cluster, and likewise wk are the char-acter sequences of all languages in the cluster.Again, we have the predictive distribution ofthe multinomial-Dirichlet compound with multi-ple grouped observations.
We can apply the sametrick as above to group terms in the ascending fac-torials for efficient computation.
As before, weuse slice sampling for obtaining samples.Sampling z?Finally, we consider sampling the cluster assign-ment z?
for each language ?.
We calculate:f(z?
= k|w, t,N, z?
?, ?, ?)?
f(w?, t?, N?|?k, ?k,Nk??
)= f(N?|Nk??
)f(t?|?k)f(w?|t?, N?, ?k)The three terms correspond to (1) a standard pre-dictive distributions for the Poisson-gamma com-pound and (2) the standard predictive distribu-tions for the transition and emission multinomial-Dirichlet compounds.5 ExperimentsTo test our model, we apply it to a corpus of 503languages for two decipherment tasks.
In bothcases, we will assume no knowledge of our tar-get language or its writing system, other than thatit is alphabetic in nature.
At the same time, wewill assume basic phonetic knowledge of the writ-ing systems of the other 502 languages.
For ourfirst task, we will predict whether each charactertype is a consonant or a vowel.
In the second task,we further subdivide the consonants into two ma-jor categories: the nasal consonants, and the non-nasal consonants.
Nasal consonants are known tobe perceptually very salient and are unique in be-ing high frequency consonants in all known lan-guages.5.1 DataOur data is drawn from online electronic transla-tions of the Bible (http://www.bible.is,http://www.crosswire.org/index.jsp, and http://www.biblegateway.com).
We have identified translations covering503 distinct languages employing alphabeticwriting systems.
Most of these languages (476)use variants of the Latin alphabet, a few (26)use Cyrillic, and one uses the Greek alphabet.As Table 1 indicates, the languages cover a verydiverse set of families and geographic regions,with Niger-Congo languages being the largestrepresented family.4 Of these languages, 30 areeither language isolates, or sole members of theirlanguage family in our data set.For our experiments, we extracted unique wordtypes occurring at least 5 times from the down-loaded Bible texts.
We manually identified vowel,nasal, and non-nasal character types.
Since the let-ter ?y?
can frequently represent both a consonantand vowel, we exclude it from our evaluation.
Onaverage, the resulting vocabularies contain 2,388unique words, with 19 consonant characters, two2 nasal characters, and 9 vowels.
We include thedata as part of the paper.4In fact, the Niger-Congo grouping is often consideredthe largest language family in the world in terms of distinctmember languages.1532Language Family #langNiger-Congo 114Austronesian 67Oto-Manguean 41Indo-European 39Mayan 34Quechuan 17Afro-Asiatic 17Uto-Aztecan 16Altaic 16Trans-New Guinea 15Nilo-Saharan 14Sino-Tibetan 13Tucanoan 9Creole 8Chibchan 6Maipurean 5Tupian 5Nakh-Daghestanian 4Uralic 4Cariban 4Totonacan 4Mixe-Zoque 3Jivaroan 3Choco 3Guajiboan 2Huavean 2Austro-Asiatic 2Witotoan 2Jean 2Paezan 2Other 30Table 1: Language families in our data set.
TheOther category includes 9 language isolates and21 language family singletons.5.2 Baselines and Model VariantsAs our baseline, we consider the trigram HMMmodel of Knight et al (2006), trained with EM.
Inall experiments, we run 10 random restarts of EM,and pick the prediction with highest likelihood.We map the induced tags to the gold-standard tagcategories (1-1 mapping) in the way that maxi-mizes accuracy.We then consider three variants of our model.The simplest version, SYMM, disregards all in-formation from other languages, using simplesymmetric hyperparameters on the transition andemission Dirichlet priors (all hyperparameters setto 1).
This allows us to assess the performance ofModel Cons vs Vowel C vs V vs NAll EM 93.37 74.59SYMM 95.99 80.72MERGE 97.14 86.13CLUST 98.85 89.37Isolates EM 94.50 74.53SYMM 96.18 78.13MERGE 97.66 86.47CLUST 98.55 89.07Non-Latin EM 92.93 78.26SYMM 95.90 79.04MERGE 96.06 83.78CLUST 97.03 85.79Table 2: Average accuracy for EM baseline andmodel variants across 503 languages.
First panel:results on all languages.
Second panel: results for30 isolate and singleton languages.
Third panel:results for 27 non-Latin alphabet languages (Cyril-lic and Greek).
Standard Deviations across lan-guages are about 2%.our Gibbs sampling inference method for the type-based HMM, even in the absence of multilingualpriors.We next consider a variant of our model,MERGE, that assumes that all languages reside ina single cluster.
This allows knowledge from theother languages to affect our tag posteriors in ageneric, language-neutral way.Finally, we consider the full version of ourmodel, CLUST, with 20 language clusters.
By al-lowing for the division of languages into smallergroupings, we hope to learn more specific param-eters tailored for typologically coherent clusters oflanguages.6 ResultsThe results of our experiments are shown in Ta-ble 2.
In all cases, we report token-level accuracy(i.e.
frequent characters count more than infre-quent characters), and results are macro-averagedover the 503 languages.
Variance across languagesis quite low: the standard deviations are about 2percentage points.For the consonant vs. vowel prediction task,all tested models perform well.
Our baseline, theEM-based HMM, achieves 93.4% accuracy.
Sim-ply using our Gibbs sampler with symmetric priorsboosts the performance up to 96%.
Performance15331534Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classificationtask with four latent clusters.
Row gives starting state, column gives target state.
Size of red blobs areproportional to magnitude of corresponding hyperparameters.Language Family Portion #langs Ent.Indo-European0.38 26 2.260.24 41 3.190.21 38 3.77Quechuan 0.89 18 0.61Mayan 0.64 33 1.70Oto-Manguean 0.55 31 1.99Maipurean 0.25 8 2.75Tucanoan 0.2 45 3.98Uto-Aztecan 0.4 25 2.85Altaic 0.44 27 2.76Niger-Congo1 2 0.000.78 23 1.260.74 27 1.050.68 22 1.220.67 33 1.620.5 18 2.210.24 25 3.27Austronesian0.91 22 0.530.71 21 1.510.24 17 3.06Table 3: Plurality language families across 20clusters.
The columns indicate portion of lan-guages in the plurality family, number of lan-guages, and entropy over families.with a bigram HMM with four language clus-ters.
Examining just the first row, we see thatthe languages are partially grouped by their pref-erence for the initial tag of words.
All clus-ters favor languages which prefer initial conso-nants, though this preference is most weakly ex-pressed in cluster 3.
In contrast, both clusters2 and 4 have very dominant tendencies towardsconsonant-initial languages, but differ in the rel-ative weight given to languages preferring eithervowels or nasals initially.Finally, we examine the relationship betweenthe induced clusters and language families in Ta-ble 3, for the trigram consonant vs. vowel CLUSTmodel with 20 clusters.
We see that for abouthalf the clusters, there is a majority language fam-ily, most often Niger-Congo.
We also observedistinctive clusters devoted to Austronesian andQuechuan languages.
The largest two clusters arerather indistinct, without any single language fam-ily achieving more than 24% of the total.8 ConclusionIn this paper, we presented a successful solutionto one aspect of the decipherment task: the predic-tion of consonants and vowels for an unknown lan-guage and alphabet.
Adopting a classical Bayesianperspective, we develop a model that performsposterior inference over hundreds of languages,leveraging knowledge of known languages to un-cover general linguistic patterns of typologicallycoherent language clusters.
Using this model, weautomatically distinguish between consonant andvowel characters with nearly 99% accuracy across503 languages.
We further experimented on athree-way classification task involving nasal char-acters, achieving nearly 90% accuracy.Future work will take us in several new direc-tions: first, we would like to move beyond the as-sumption of an alphabetic writing system so thatwe can apply our method to undeciphered syllabicscripts such as Linear A.
We would also like toextend our methods to achieve finer-grained reso-lution of phonetic properties beyond nasals, con-sonants, and vowels.AcknowledgmentsThe authors thank the reviewers and acknowledge support bythe NSF (grant IIS-1116676) and a research gift fromGoogle.Any opinions, findings, or conclusions are those of the au-thors, and do not necessarily reflect the views of the NSF.1535ReferencesTaylor Berg-Kirkpatrick and Dan Klein.
2010.
Phylogeneticgrammar induction.
In Proceedings of the ACL, pages1288?1297.
Association for Computational Linguistics.Alexandre Bouchard-C?t?, David Hall, Thomas L Griffiths,and Dan Klein.
2013.
Automated reconstruction ofancient languages using probabilistic models of soundchange.
Proceedings of the National Academy of Sci-ences, 110(11):4224?4229.Christos Christodoulopoulos, Sharon Goldwater, and MarkSteedman.
2011.
A Bayesian mixture model for part-of-speech induction using multiple features.
In Proceed-ings of EMNLP, pages 638?647.
Association for Compu-tational Linguistics.Shay B Cohen and Noah A Smith.
2009.
Shared logistic nor-mal distributions for soft parameter tying in unsupervisedgrammar induction.
In Proceedings of NAACL, pages 74?82.
Association for Computational Linguistics.Shay B Cohen, Dipanjan Das, and Noah A Smith.
2011.
Un-supervised structure prediction with non-parallel multilin-gual guidance.
In Proceedings of EMNLP, pages 50?61.Association for Computational Linguistics.Stuart Geman and Donald Geman.
1984.
Stochastic relax-ation, Gibbs distributions, and the Bayesian restoration ofimages.
Pattern Analysis and Machine Intelligence, IEEETransactions on, (6):721?741.Young-Bum Kim and Benjamin Snyder.
2012.
Univer-sal grapheme-to-phoneme prediction over latin alphabets.In Proceedings of EMNLP, pages 332?343, Jeju Island,South Korea, July.
Association for Computational Lin-guistics.Young-Bum Kim, Jo?o V Gra?a, and Benjamin Snyder.2011.
Universal morphological analysis using structurednearest neighbor prediction.
In Proceedings of EMNLP,pages 322?332.
Association for Computational Linguis-tics.Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada.2006.
Unsupervised analysis for decipherment problems.In Proceedings of COLING/ACL, pages 499?506.
Associ-ation for Computational Linguistics.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010.Simple type-level unsupervised POS tagging.
In Proceed-ings of EMNLP, pages 853?861.
Association for Compu-tational Linguistics.Percy Liang, Michael I Jordan, and Dan Klein.
2010.
Type-based MCMC.
In Proceedings of NAACL, pages 573?581.Association for Computational Linguistics.David JC MacKay.
2003.
Information Theory, Inference andLearning Algorithms.
Cambridge University Press.Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, andRegina Barzilay.
2009.
Multilingual part-of-speech tag-ging: Two unsupervised approaches.
Journal of ArtificialIntelligence Research, 36(1):341?385.Radford M Neal.
2003.
Slice sampling.
Annals of statistics,31:705?741.Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010.A statistical model for lost language decipherment.
InProceedings of the ACL, pages 1048?1057.
Associationfor Computational Linguistics.1536
