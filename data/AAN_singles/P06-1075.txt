Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 593?600,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Effect of Translation Quality in MT-BasedCross-Language Information RetrievalJiang Zhu      Haifeng WangToshiba (China) Research and Development Center5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng DistrictBeijing, 100738, China{zhujiang, wanghaifeng}@rdc.toshiba.com.cnAbstractThis paper explores the relationship be-tween the translation quality and the re-trieval effectiveness in Machine Transla-tion (MT) based Cross-Language Infor-mation Retrieval (CLIR).
To obtain MTsystems of different translation quality,we degrade a rule-based MT system bydecreasing the size of the rule base andthe size of the dictionary.
We use the de-graded MT systems to translate queriesand submit the translated queries of vary-ing quality to the IR system.
Retrieval ef-fectiveness is found to correlate highlywith the translation quality of the queries.We further analyze the factors that affectthe retrieval effectiveness.
Title queriesare found to be preferred in MT-basedCLIR.
In addition, dictionary-based deg-radation is shown to have stronger impactthan rule-based degradation in MT-basedCLIR.1 IntroductionCross-Language Information Retrieval (CLIR)enables users to construct queries in one lan-guage and search the documents in another lan-guage.
CLIR requires that either the queries orthe documents be translated from a language intoanother, using available translation resources.Previous studies have concentrated on querytranslation because it is computationally less ex-pensive than document translation, which re-quires a lot of processing time and storage costs(Hull & Grefenstette, 1996).There are three kinds of methods to performquery translation, namely Machine Translation(MT) based methods, dictionary-based methodsand corpus-based methods.
Corresponding tothese methods, three types of translation re-sources are required: MT systems, bilingualwordlists and parallel or comparable corpora.CLIR effectiveness depends on both the designof the retrieval system and the quality of thetranslation resources that are used.In this paper, we explore the relationship be-tween the translation quality of the MT systemand the retrieval effectiveness.
The MT systeminvolved in this research is a rule-based English-to-Chinese MT (ECMT) system.
We degrade theMT system in two ways.
One is to degrade therule base of the system by progressively remov-ing rules from it.
The other is to degrade the dic-tionary by gradually removing word entries fromit.
In both methods, we observe successivechanges on translation quality of the MT system.We conduct query translation with the degradedMT systems and obtain translated queries ofvarying quality.
Then we submit the translatedqueries to the IR system and evaluate the per-formance.
Retrieval effectiveness is found to bestrongly influenced by the translation quality ofthe queries.
We further analyze the factors thataffect the retrieval effectiveness.
Title queries arefound to be preferred in MT-based query transla-tion.
In addition, the size of the dictionary isshown to have stronger impact on retrieval effec-tiveness than the size of the rule base in MT-based query translation.The remainder of this paper is organized asfollows.
In section 2, we briefly review relatedwork.
In section 3, we introduce two systemsinvolved in this research: the rule-based ECMTsystem and the KIDS IR system.
In section 4, wedescribe our experimental method.
Section 5 andsection 6 reports and discusses the experimentalresults.
Finally we present our conclusion andfuture work in section 7.5932 Related Work2.1 Effect of Translation ResourcesPrevious studies have explored the effect oftranslation resources such as bilingual wordlistsor parallel corpora on CLIR performance.Xu and Weischedel (2000) measured CLIRperformance as a function of bilingual dictionarysize.
Their English-Chinese CLIR experimentson TREC 5&6 Chinese collections showed thatthe initial retrieval performance increasedsharply with lexicon size but the performancewas not improved after the lexicon exceeded20,000 terms.
Demner-Fushman and Oard (2003)identified eight types of terms that affected re-trieval effectiveness in CLIR applicationsthrough their coverage by general-purpose bilin-gual term lists.
They reported results from anevaluation of the coverage of 35 bilingual termlists in news retrieval application.
Retrieval ef-fectiveness was found to be strongly influencedby term list size for lists that contain between3,000 and 30,000 unique terms per language.Franz et al (2001) investigated the CLIR per-formance as a function of training corpus size forthree different training corpora and observed ap-proximately logarithmically increased perform-ance with corpus size for all the three corpora.Kraaij (2001) compared three types of translationresources for bilingual retrieval based on querytranslation: a bilingual machine-readable diction-ary, a statistical dictionary based on a parallelweb corpus and the Babelfish MT service.
Hedrew a conclusion that the mean average preci-sion of a run was proportional to the lexical cov-erage.
McNamee and Mayfield (2002) examinedthe effectiveness of query expansion techniquesby using parallel corpora and bilingual wordlistsof varying quality.
They confirmed that retrievalperformance dropped off as the lexical coverageof translation resources decreased and the rela-tionship was approximately linear.Previous research mainly focused on studyingthe effectiveness of bilingual wordlists or parallelcorpora from two aspects: size and lexical cover-age.
Kraaij (2001) examined the effectiveness ofMT system, but also from the aspect of lexicalcoverage.
Why lack research on analyzing effectof translation quality of MT system on CLIRperformance?
The possible reason might be theproblem on how to control the translation qualityof the MT system as what has been done to bi-lingual wordlists or parallel corpora.
MT systemsare usually used as black boxes in CLIR applica-tions.
It is not very clear how to degrade MTsoftware because MT systems are usually opti-mized for grammatically correct sentences ratherthan word-by-word translation.2.2 MT-Based Query TranslationMT-based query translation is perhaps the moststraightforward approach to CLIR.
Comparedwith dictionary or corpus based methods, theadvantage of MT-based query translation lies inthat technologies integrated in MT systems, suchas syntactic and semantic analysis, could help toimprove the translation accuracy (Jones et al,1999).
However, in a very long time, fewer ex-periments with MT-based methods have beenreported than with dictionary-based methods orcorpus-based methods.
The main reasons include:(1) MT systems of high quality are not easy toobtain; (2) MT systems are not available forsome language pairs; (3) queries are usuallyshort or even terms, which limits the effective-ness of MT-based methods.
However, recent re-search work on CLIR shows a trend to adoptMT-based query translation.
At the fifth NTCIRworkshop, almost all the groups participating inBilingual CLIR and Multilingual CLIR tasksadopt the query translation method using MTsystems or machine-readable dictionaries (Ki-shida et al, 2005).
Recent research work alsoproves that MT-based query translation couldachieve comparable performance to other meth-ods (Kishida et al, 2005; Nunzio et al, 2005).Considering more and more MT systems are be-ing used in CLIR, it is of significance to care-fully analyze how the performance of MT systemmay influence the retrieval effectiveness.3 System Description3.1 The Rule-Based ECMT SystemThe MT system used in this research is a rule-based ECMT system.
The translation quality ofthis ECMT system is comparable to the bestcommercial ECMT systems.
The basis of thesystem is semantic transfer (Amano et al, 1989).Translation resources comprised in this systeminclude a large dictionary and a rule base.
Therule base consists of rules of different functionssuch as analysis, transfer and generation.3.2 KIDS IR SystemKIDS is an information retrieval engine that isbased on morphological analysis (Sakai et al,2003).
It employs the Okapi/BM25 term weight-ing scheme, as fully described in (Robertson &Walker, 1999; Robertson & Sparck Jones, 1997).594To focus our study on the relationship betweenMT performance and retrieval effectiveness, wedo not use techniques such as pseudo-relevancefeedback although they are available and areknown to improve IR performance.4 Experimental MethodTo obtain MT systems of varying quality, wedegrade the rule-based ECMT system by impair-ing the translation resources comprised in thesystem.
Then we use the degraded MT systemsto translate the queries and evaluate the transla-tion quality.
Next, we submit the translated que-ries to the KIDS system and evaluate the re-trieval performance.
Finally we calculate the cor-relation between the variation of translation qual-ity and the variation of retrieval effectiveness toanalyze the relationship between MT perform-ance and CLIR performance.4.1 Degradation of MT SystemIn this research, we degrade the MT system intwo ways.
One is rule-based degradation, whichis to decrease the size of the rule base by ran-domly removing rules from the rule base.
Forsake of simplicity, in this research we only con-sider transfer rules that are used for transferringthe source language to the target language andkeep other kinds of rules untouched.
That is, weonly consider the influence of transfer rules ontranslation quality1.
We first randomly divide therules into segments of equal size.
Then we re-move the segments from the rule base, one ateach time and obtain a group of degraded rulebases.
Afterwards, we use MT systems with thedegraded rule bases to translate the queries andget groups of translated queries, which are ofdifferent translation quality.The other is dictionary-based degradation,which is to decrease the size of the dictionary byrandomly removing a certain number of wordentries from the dictionary iteratively.
Functionwords are not removed from the dictionary.
Us-ing MT systems with the degraded dictionaries,we also obtain groups of translated queries ofdifferent translation quality.4.2 Evaluation of PerformanceWe measure the performance of the MT systemby translation quality and use NIST score as theevaluation measure (Doddington, 2002).
The1 In the following part of this paper, rules refer to transferrules unless explicitly stated.NIST scores reported in this paper are generatedby NIST scoring toolkit2.For retrieval performance, we use Mean Aver-age Precision (MAP) as the evaluation measure(Voorhees, 2003).
The MAP values reported inthis paper are generated by trec_eval toolkit 3 ,which is the standard tool used by TREC forevaluating an ad hoc retrieval run.5 Experiments5.1 DataThe experiments are conducted on the TREC5&6Chinese collection.
The collection consists ofdocument set, topic set and the relevance judg-ment file.The document set contains articles publishedin People's Daily from 1991 to 1993, and newsarticles released by the Xinhua News Agency in1994 and 1995.
It includes totally 164,789documents.
The topic set contains 54 topics.
Inthe relevance judgment file, a binary indicationof relevant (1) or non-relevant (0) is given.<top><num> Number: CH41<C-title> ??????????
?<E-title> Bridge and Tunnel Construction forthe Beijing-Kowloon Railroad<C-desc> Description:?????????????????
?<E-desc> Description:Beijing-Kowloon Railroad, bridge, tunnel,connection, very large bridge<C-narr> Narrative:????????????????????????????????
?<E-narr> Narrative:A relevant document discusses bridge andtunnel construction for the Beijing-KowloonRailroad, including location, constructionstatus, span or length.</top>Figure 1.
Example of TREC Topic5.2 Query Formulation & EvaluationFor each TREC topic, three fields are provided:title, description and narrative, both in Chineseand English, as shown in figure 1.
The title fieldis the statement of the topic.
The description2 The toolkit could be downloaded from:http://www.nist.gov/speech/tests/mt/resources/scoring.htm3 The toolkit could be downloaded from:http://trec.nist.gov/trec_eval/trec_eval.7.3.tar.gz595field lists some terms that describe the topic.
Thenarrative field provides a complete descriptionof document relevance for the assessors.
In ourexperiments, we use two kinds of queries: titlequeries (use only the title field) and desc queries(use only the description field).
We do not usenarrative field because it is the criteria used bythe assessors to judge whether a document isrelevant or not, so it usually contains quite anumber of unrelated words.Title queries are one-sentence queries.
Whenuse NIST scoring tool to evaluate the translationquality of the MT system, reference translationsof source language sentences are required.
NISTscoring tool supports multi references.
In ourexperiments, we introduce two reference transla-tions for each title query.
One is the Chinese title(C-title) in title field of the original TREC topic(reference translation 1); the other is the transla-tion of the title query given by a human transla-tor (reference translation 2).
This is to alleviatethe bias on translation evaluation introduced byonly one reference translation.
An example oftitle query and its reference translations areshown in figure 2.
Reference 1 is the Chinesetitle provided in original TREC topic.
Reference2 is the human translation of the query.
For thisquery, the translation output generated by theMT system is "???????????".
Ifonly use reference 1 as reference translation, thesystem output will not be regarded as a goodtranslation.
But in fact, it is a good translation forthe query.
Introducing reference 2 helps to alle-viate the unfair evaluation.Title Query: CH27<query>Robotics Research in China<reference 1>??????????
?<reference 2>???????
?Figure 2.
Example of Title QueryA desc query is not a sentence but a string ofterms that describes the topic.
The term in thedesc query is either a word, a phrase or a stringof words.
A desc query is not a proper input forthe MT system.
But the MT system still works.
Ittranslates the desc query term by term.
When theterm is a word or a phrase that exists in the dic-tionary, the MT system looks up the dictionaryand takes the first translation in the entry as thetranslation of the term without any further analy-sis.
When the term is a string of words such as"number(??)
of(?)
infections(??
)", the sys-tem translates the term into "????".
Besidesusing the Chinese description (C-desc) in thedescription field of the original TREC topic asthe reference translation of each desc query, wealso have the human translator give another ref-erence translation for each desc query.
Compari-son on the two references shows that they arevery similar to each other.
So in our final ex-periments, we use only one reference for eachdesc query, which is the Chinese description (C-desc) provided in the original TREC topic.
Anexample of desc query and its reference transla-tion is shown in figure 3.Desc Query: CH22<query>malaria, number of deaths, number of infections<reference>???????????
?Figure 3.
Example of Desc Query5.3 RunsPrevious studies (Kwok, 1997; Nie et al, 2000)proved that using words and n-grams indexesleads to comparable performance for Chinese IR.So in our experiments, we use bi-grams as indexunits.We conduct following runs to analyze the rela-tionship between MT performance and CLIRperformance:?
rule-title: MT-based title query transla-tion with degraded rule base?
rule-desc: MT-based desc query transla-tion with degraded rule base?
dic-title: MT-based title query translationwith degraded dictionary?
dic-desc: MT-based desc query transla-tion with degraded dictionaryFor baseline comparison, we conduct Chinesemonolingual runs with title queries and desc que-ries.5.4 Monolingual PerformanceThe results of Chinese monolingual runs areshown in Table 1.Run MAPtitle-cn1 0.3143title-cn2 0.3001desc-cn 0.3514Table 1.
Monolingual Results596title-cn1: use reference translation 1 of each ti-tle query as Chinese querytitle-cn2: use reference translation 2 of each ti-tle query as Chinese querydesc-cn: use reference translation of each descquery as Chinese queryAmong all the three monolingual runs, desc-cnachieves the best performance.
Title-cn1achieves better performance than title-cn2, whichindicates directly using Chinese title as Chinesequery performs better than using human transla-tion of title query as Chinese query.5.5 Results on Rule-Based DegradationThere are totally 27,000 transfer rules in the rulebase.
We use all these transfer rules in the ex-periment of rule-based degradation.
The 27,000rules are randomly divided into 36 segments,each of which contains 750 rules.
To degrade therule base, we start with no degradation, then weremove one segment at each time, up to a com-plete degradation with all segments removed.With each of the segment removed from the rulebase, the MT system based on the degraded rulebase produces a group of translations for the in-put queries.
The completely degraded systemwith all segments removed could produce agroup of rough translations for the input queries.Figure 4 and figure 5 show the experimentalresults on title queries (rule-title) and desc que-ries (rule-desc) respectively.Figure 4(a) shows the changes of translationquality of the degraded MT systems on title que-ries.
From the result, we observe a successivechange on MT performance.
The fewer rules, theworse translation quality achieves.
The NISTscore varies from 7.3548 at no degradation to5.9155 at complete degradation.
Figure 4(b)shows the changes of retrieval performance byusing the translations generated by the degradedMT systems as queries.
The MAP varies from0.3126 at no degradation to 0.2810 at completedegradation.
Comparison on figure 4(a) and 4(b)indicates similar variations between translationquality and retrieval performance.
The better thetranslation quality, the better the retrieval per-formance is.Figure 5(a) shows the changes of translationquality of the degraded MT systems on desc que-ries.
Figure 5(b) shows the correspondingchanges of retrieval performance.
We observe asimilar relationship between MT performanceand retrieval performance as to the results based5.80006.00006.20006.40006.60006.80007.00007.20007.40007.60000 4 8 12 16 20 24 28 32 36 40MT System with Degraded Rule BaseNISTScoreFigure 4(a).
MT Performance on Rule-basedDegradation with Title Query4.84004.86004.88004.90004.92004.94004.96004.98005.00005.02005.04000 4 8 12 16 20 24 28 32 36 40MT System with Degraded Rule BaseNISTScoreFigure 5(a).
MT Performance on Rule-basedDegradation with Desc Query0.28000.28500.29000.29500.30000.30500.31000.31500.32000 4 8 12 16 20 24 28 32 36 40MT System with Degraded Rule BaseMAPFigure 4(b).
Retrieval Effectiveness on Rule-based Degradation with Title Query0.27500.27700.27900.28100.28300.28500.28700.28900 4 8 12 16 20 24 28 32 36 40MT System with Degraded Rule BaseMAPFigure 5(b).
Retrieval Effectiveness on Rule-based Degradation with Desc Query5975.80006.00006.20006.40006.60006.80007.00007.20007.40007.60000 4 8 12 16 20 24 28 32 36 40MT System with Degraded DcitionaryNISTScoreFigure 6(a).
MT Performance on Dictionary-based Degradation with Title Query4.40004.50004.60004.70004.80004.90005.00005.10000 4 8 12 16 20 24 28 32 36 40MT System with Degraded DictionaryNISTScoreFigure 7(a).
MT Performance on Dictionary-based Degradation with Desc Query0.18000.20000.22000.24000.26000.28000.30000.32000 4 8 12 16 20 24 28 32 36 40MT System with Degraded DcitionaryMAPFigure 6(b).
Retrieval Effectiveness on Diction-ary-based Degradation with Title Query0.24000.24500.25000.25500.26000.26500.27000.27500.28000.28500.29000 4 8 12 16 20 24 28 32 36 40MT System with Degraded DictionaryMAPFigure 7(b).
Retrieval Effectiveness on Diction-ary-based Degradation with Desc Queryon title queries.
The NIST score varies from5.0297 at no degradation to 4.8497 at completedegradation.
The MAP varies from 0.2877 at nodegradation to 0.2759 at complete degradation.5.6 Results on Dictionary-Based Degrada-tionThe dictionary contains 169,000 word entries.
Tomake the results on dictionary-based degradationcomparable to the results on rule-based degrada-tion, we degrade the dictionary so that the varia-tion interval on translation quality is similar tothat of the rule-based degradation.
We randomlyselect 43,200 word entries for degradation.
Theseword entries do not include function words.
Weequally split these word entries into 36 segments.Then we remove one segment from the diction-ary at each time until all the segments are re-moved and obtain 36 degraded dictionaries.
Weuse the MT systems with the degraded dictionar-ies to translate the queries and observe thechanges on translation quality and retrieval per-formance.
The experimental results on title que-ries (dic-title) and desc queries (dic-desc) areshown in figure 6 and figure 7 respectively.From the results, we also observe a similar rela-tionship between translation quality and retrievalperformance as what we have observed in therule-based degradation.
For both title queries anddesc queries, the larger the dictionary size, thebetter the NIST score and MAP is.
For title que-ries, the NIST score varies from 7.3548 at nodegradation to 6.0067 at complete degradation.The MAP varies from 0.3126 at no degradationto 0.1894 at complete degradation.
For desc que-ries, the NIST score varies from 5.0297 at nodegradation to 4.4879 at complete degradation.The MAP varies from 0.2877 at no degradationto 0.2471 at complete degradation.5.7 Summary of the ResultsHere we summarize the results of the four runs inTable 2.Run NIST Score MAPtitle queriesNo degradation 7.3548 0.3126Complete: rule-title 5.9155 0.2810Complete: dic-title 6.0067 0.1894desc queriesNo degradation 5.0297 0.2877Complete: rule-desc 4.8497 0.2759Complete: dic-desc 4.4879 0.2471Table 2.
Summary of Runs5986 DiscussionBased on our observations, we analyze the corre-lations between NIST scores and MAPs, as listedin Table 3.
In general, there is a strong correla-tion between translation quality and retrieval ef-fectiveness.
The correlations are above 95% forall of the four runs, which means in general, abetter performance on MT will lead to a betterperformance on retrieval.Run Correlationrule-title 0.9728rule-desc 0.9500dic-title 0.9521dic-desc 0.9582Table 3.
Correlation Between Translation Qual-ity & Retrieval Effectiveness6.1 Impacts of Query FormatFor Chinese monolingual runs, retrieval based ondesc queries achieves better performance thanthe runs based on title queries.
This is because adesc query consists of terms that relate to thetopic, i.e., all the terms in a desc query are pre-cise query terms.
But a title query is a sentence,which usually introduces words that are unre-lated to the topic.Results on bilingual retrieval are just contraryto monolingual ones.
Title queries perform betterthan desc queries.
Moreover, MAP at no degra-dation for title queries is 0.3126, which is about99.46% of the performance of monolingual runtitle-cn1, and outperforms the performance oftitle-cn2 run.
But MAP at no degradation fordesc queries is 0.2877, which is just 81.87% ofthe performance of the monolingual run desc-cn.Comparison on the results shows that the MTsystem performs better on title queries than ondesc queries.
This is reasonable because descqueries are strings of terms, however the MTsystem is optimized for grammatically correctsentences rather than word-by-word translation.Considering the correlation between translationquality and retrieval effectiveness, it is rationalthat title queries achieve better results on re-trieval than desc queries.6.2 Impacts of Rules and DictionaryTable 4 shows the fall of NIST score and MAP atcomplete degradation compared with NIST scoreand MAP achieved at no degradation.Comparison on the results of title queriesshows that similar variation of translation qualityleads to quite different variation on retrieval ef-fectiveness.
For rule-title run, 19.57% reductionin translation quality results in 10.11% reductionin retrieval effectiveness.
But for dic-title run,18.33% reduction in translation quality results in39.41% reduction in retrieval effectiveness.
Thisindicates that retrieval effectiveness is more sen-sitive to the size of the dictionary than to the sizeof the rule base for title queries.
Why dictionary-based degradation has stronger impact on re-trieval effectiveness than rule-based degradation?This is because retrieval systems are typicallymore tolerant of syntactic than semantic transla-tion errors (Fluhr, 1997).
Therefore althoughsyntactic errors caused by the degradation of therule base result in a decrease of translation qual-ity, they have smaller impacts on retrieval effec-tiveness than the word translation errors causedby the degradation of dictionary.For desc queries, there is no big difference be-tween dictionary-based degradation and rule-based degradation.
This is because the MT sys-tem translates the desc queries term by term, sodegradation of rule base mainly results in wordtranslation errors instead of syntactic errors.Thus, degradation of dictionary and rule base hassimilar effect on retrieval effectiveness.Run NIST Score Fall MAP Falltitle queriesrule-title 19.57% 10.11%dic-title 18.33% 39.41%desc queriesrule-desc 3.58% 4.10%dic-desc 10.77% 14.11%Table 4.
Fall on Translation Quality & RetrievalEffectiveness7 Conclusion and Future WorkIn this paper, we investigated the effect of trans-lation quality in MT-based CLIR.
Our studyshowed that the performance of MT system andIR system correlates highly with each other.
Wefurther analyzed two main factors in MT-basedCLIR.
One factor is the query format.
We con-cluded that title queries are preferred for MT-based CLIR because MT system is usually opti-mized for translating sentences rather than words.The other factor is the translation resources com-prised in the MT system.
Our observationshowed that the size of the dictionary has astronger effect on retrieval effectiveness than thesize of the rule base in MT-based CLIR.
There-fore in order to improve the retrieval effective-ness of a MT-based CLIR application, it is more599effective to develop a larger dictionary than todevelop more rules.
This introduces another in-teresting question relating to MT-based CLIR.That is how CLIR can benefit further from MT.Directly using the translations generated by theMT system may not be the best choice for the IRsystem.
There are rich features generated duringthe translation procedure.
Will such features behelpful to CLIR?
This question is what we wouldlike to answer in our future work.ReferencesShin-ya Amano, Hideki Hirakawa, Hirosysu Nogami,and Akira Kumano.
1989.
The Toshiba MachineTranslation System.
Future Computing System,2(3):227-246.Dina Demner-Fushman, and Douglas W. Oard.
2003.The Effect of Bilingual Term List Size on Diction-ary-Based Cross-Language Information Retrieval.In Proc.
of the 36th Hawaii International Confer-ence on System Sciences (HICSS-36), pages 108-117.George Doddington.
2002.
Automatic Evaluation ofMachine Translation Quality Using N-gram Co-occurrence Statistics.
In Proc.
of the Second Inter-national Conference on Human Language Tech-nology (HLT-2002), pages 138-145.Christian Fluhr.
1997.
Multilingual Information Re-trieval.
In Ronald A Cole, Joseph Mariani, HansUszkoreit, Annie Zaenen, and Victor Zue (Eds.
),Survey of the State of the Art in Human LanguageTechnology, pages 261-266, Cambridge UniversityPress, New York.Martin Franz, J. Scott McCarley, Todd Ward, andWei-Jing Zhu.
2001.
Quantifying the Utility ofParallel Corpora.
In Proc.
of the 24th Annual ACMConference on Research and Development in In-formation Retrieval (SIGIR-2001), pages 398-399.David A.
Hull and Gregory Grefenstette.
1996.
Que-rying Across Languages: A Dictionary-Based Ap-proach to Multilingual Information Retrieval.
InProc.
of the 19th Annual ACM Conference on Re-search and Development in Information Retrieval(SIGIR-1996), pages 49-57.Gareth Jones, Tetsuya Sakai, Nigel Collier, AkiraKumano and Kazuo Sumita.
1999.
Exploring theUse of Machine Translation Resources for English-Japanese Cross-Language Infromation Retrieval.
InProc.
of MT Summit VII Workshop on MachineTranslation for Cross Language Information Re-trieval, pages 15-22.Kazuaki Kishida, Kuang-hua Chen, Sukhoon Lee,Kazuko Kuriyama, Noriko Kando, Hsin-Hsi Chen,and Sung Hyon Myaeng.
2005.
Overview of CLIRTask at the Fifth NTCIR Workshop.
In Proc.
of theNTCIR-5 Workshop Meeting, pages 1-38.Wessel Kraaij.
2001.
TNO at CLEF-2001: ComparingTranslation Resources.
In Proc.
of the CLEF-2001Workshop, pages 78-93.Kui-Lam Kwok.
1997.
Comparing Representation inChinese Information Retrieval.
In Proc.
of the 20thAnnual ACM Conference on Research and Devel-opment in Information Retrieval (SIGIR-1997),pages 34-41.Paul McNamee and James Mayfield.
2002.
Compar-ing Cross-Language Query Expansion Techniquesby Degrading Translation Resources.
In Proc.
ofthe 25th Annual ACM Conference on Research andDevelopment in Information Retrieval (SIGIR-2002), pages 159-166.Jian-Yun Nie, Jianfeng Gao, Jian Zhang, and MingZhou.
2000.
On the Use of Words and N-grams forChinese Information Retrieval.
In Proc.
of the FifthInternational Workshop on Information Retrievalwith Asian Languages (IRAL-2000), pages 141-148.Giorgio M. Di Nunzio, Nicola Ferro, Gareth J. F.Jones, and Carol Peters.
2005.
CLEF 2005: AdHoc Track Overview.
In C. Peters (Ed.
), WorkingNotes for the CLEF 2005 Workshop.Stephen E. Robertson and Stephen Walker.
1999.Okapi/Keenbow at TREC-8.
In Proc.
of the EighthText Retrieval Conference (TREC-8), pages 151-162.Stephen E. Robertson and Karen Sparck Jones.
1997.Simple, Proven Approaches to Text Retrieval.Technical Report 356, Computer Laboratory, Uni-versity of Cambridge, United Kingdom.Tetsuya Sakai, Makoto Koyama, Masaru Suzuki, andToshihiko Manabe.
2003.
Toshiba KIDS atNTCIR-3: Japanese and English-Japanese IR.
InProc.
of the Third NTCIR Workshop on Researchin Information Retrieval, Automatic Text Summari-zation and Question Answering (NTCIR-3), pages51-58.Ellen M. Voorhees.
2003.
Overview of TREC 2003.In Proc.
of the Twelfth Text Retrieval Conference(TREC 2003), pages 1-13.Jinxi Xu and Ralph Weischedel.
2000.
Cross-lingualInformation Retrieval Using Hidden Markov Mod-els.
In Proc.
of the 2000 Joint SIGDAT Conferenceon Empirical Methods in Natural Language Proc-essing and Very Large Corpora (EMNLP/VLC-2000), pages 95-103.600
