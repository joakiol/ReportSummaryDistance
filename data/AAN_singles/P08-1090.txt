Proceedings of ACL-08: HLT, pages 789?797,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUnsupervised Learning of Narrative Event ChainsNathanael Chambers and Dan JurafskyDepartment of Computer ScienceStanford UniversityStanford, CA 94305{natec,jurafsky}@stanford.eduAbstractHand-coded scripts were used in the 1970-80sas knowledge backbones that enabled infer-ence and other NLP tasks requiring deep se-mantic knowledge.
We propose unsupervisedinduction of similar schemata called narrativeevent chains from raw newswire text.A narrative event chain is a partially orderedset of events related by a common protago-nist.
We describe a three step process to learn-ing narrative event chains.
The first uses unsu-pervised distributional methods to learn narra-tive relations between events sharing corefer-ring arguments.
The second applies a tempo-ral classifier to partially order the connectedevents.
Finally, the third prunes and clustersself-contained chains from the space of events.We introduce two evaluations: the narrativecloze to evaluate event relatedness, and an or-der coherence task to evaluate narrative order.We show a 36% improvement over baselinefor narrative prediction and 25% for temporalcoherence.1 IntroductionThis paper induces a new representation of struc-tured knowledge called narrative event chains (ornarrative chains).
Narrative chains are partially or-dered sets of events centered around a common pro-tagonist.
They are related to structured sequences ofparticipants and events that have been called scripts(Schank and Abelson, 1977) or Fillmorean frames.These participants and events can be filled in andinstantiated in a particular text situation to draw in-ferences.
Chains focus on a single actor to facili-tate learning, and thus this paper addresses the threetasks of chain induction: narrative event induction,temporal ordering of events and structured selection(pruning the event space into discrete sets).Learning these prototypical schematic sequencesof events is important for rich understanding of text.Scripts were central to natural language understand-ing research in the 1970s and 1980s for proposedtasks such as summarization, coreference resolu-tion and question answering.
For example, Schankand Abelson (1977) proposed that understandingtext about restaurants required knowledge about theRestaurant Script, including the participants (Cus-tomer, Waiter, Cook, Tables, etc.
), the events consti-tuting the script (entering, sitting down, asking formenus, etc.
), and the various preconditions, order-ing, and results of each of the constituent actions.Consider these two distinct narrative chains.accused X W joinedX claimed W servedX argued W oversawdismissed X W resignedIt would be useful for question answering or tex-tual entailment to know that ?X denied ?
is also alikely event in the left chain, while ?
replaces W?temporally follows the right.
Narrative chains (suchas Firing of Employee or Executive Resigns) offerthe structure and power to directly infer these newsubevents by providing critical background knowl-edge.
In part due to its complexity, automatic in-duction has not been addressed since the early non-statistical work of Mooney and DeJong (1985).The first step to narrative induction uses an entity-based model for learning narrative relations by fol-789lowing a protagonist.
As a narrative progressesthrough a series of events, each event is character-ized by the grammatical role played by the protag-onist, and by the protagonist?s shared connection tosurrounding events.
Our algorithm is an unsuper-vised distributional learning approach that uses core-ferring arguments as evidence of a narrative relation.We show, using a new evaluation task called narra-tive cloze, that our protagonist-based method leadsto better induction than a verb-only approach.The next step is to order events in the same nar-rative chain.
We apply work in the area of temporalclassification to create partial orders of our learnedevents.
We show, using a coherence-based evalua-tion of temporal ordering, that our partial orders leadto better coherence judgements of real narrative in-stances extracted from documents.Finally, the space of narrative events and temporalorders is clustered and pruned to create discrete setsof narrative chains.2 Previous WorkWhile previous work hasn?t focused specifically onlearning narratives1, our work draws from two linesof research in summarization and anaphora resolu-tion.
In summarization, topic signatures are a setof terms indicative of a topic (Lin and Hovy, 2000).They are extracted from hand-sorted (by topic) setsof documents using log-likelihood ratios.
Theseterms can capture some narrative relations, but themodel requires topic-sorted training data.Bean and Riloff (2004) proposed the use ofcaseframe networks as a kind of contextual roleknoweldge for anaphora resolution.
A case-frame is a verb/event and a semantic role (e.g.<patient> kidnapped).
Caseframe networks are re-lations between caseframes that may represent syn-onymy (<patient> kidnapped and <patient> ab-ducted) or related events (<patient> kidnapped and<patient> released).
Bean and Riloff learn thesenetworks from two topic-specific texts and applythem to the problem of anaphora resolution.
Ourwork can be seen as an attempt to generalize the in-tuition of caseframes (finding an entire set of events1We analyzed FrameNet (Baker et al, 1998) for insight, butfound that very few of the frames are event sequences of thetype characterizing narratives and scripts.rather than just pairs of related frames) and apply itto a different task (finding a coherent structured nar-rative in non-topic-specific text).More recently, Brody (2007) proposed an ap-proach similar to caseframes that discovers high-level relatedness between verbs by grouping verbsthat share the same lexical items in subject/objectpositions.
He calls these shared arguments anchors.Brody learns pairwise relations between clusters ofrelated verbs, similar to the results with caseframes.A human evaluation of these pairs shows an im-provement over baseline.
This and previous case-frame work lend credence to learning relations fromverbs with common arguments.We also draw from lexical chains (Morris andHirst, 1991), indicators of text coherence from wordoverlap/similarity.
We use a related notion of protag-onist overlap to motivate narrative chain learning.Work on semantic similarity learning such asChklovski and Pantel (2004) also automaticallylearns relations between verbs.
We use similar dis-tributional scoring metrics, but differ with our useof a protagonist as the indicator of relatedness.
Wealso use typed dependencies and the entire space ofevents for similarity judgements, rather than onlypairwise lexical decisions.Finally, Fujiki et al (2003) investigated script ac-quisition by extracting the 41 most frequent pairs ofevents from the first paragraph of newswire articles,using the assumption that the paragraph?s textual or-der follows temporal order.
Our model, by contrast,learns entire event chains, uses more sophisticatedprobabilistic measures, and uses temporal orderingmodels instead of relying on document order.3 The Narrative Chain Model3.1 DefinitionOur model is inspired by Centering (Grosz et al,1995) and other entity-based models of coherence(Barzilay and Lapata, 2005) in which an entity is infocus through a sequence of sentences.
We proposeto use this same intuition to induce narrative chains.We assume that although a narrative has severalparticipants, there is a central actor who character-izes a narrative chain: the protagonist.
Narrativechains are thus structured by the protagonist?s gram-matical roles in the events.
In addition, narrative790events are ordered by some theory of time.
This pa-per describes a partial ordering with the before (nooverlap) relation.Our task, therefore, is to learn events that consti-tute narrative chains.
Formally, a narrative chainis a partially ordered set of narrative events thatshare a common actor.
A narrative event is a tu-ple of an event (most simply a verb) and its par-ticipants, represented as typed dependencies.
Sincewe are focusing on a single actor in this study, anarrative event is thus a tuple of the event and thetyped dependency of the protagonist: (event, depen-dency).
A narrative chain is a set of narrative events{e1, e2, ..., en}, where n is the size of the chain, anda relation B(ei, ej) that is true if narrative event eioccurs strictly before ej in time.3.2 The ProtagonistThe notion of a protagonist motivates our approachto narrative learning.
We make the following as-sumption of narrative coherence: verbs sharingcoreferring arguments are semantically connectedby virtue of narrative discourse structure.
A singledocument may contain more than one narrative (ortopic), but the narrative assumption states that a se-ries of argument-sharing verbs is more likely to par-ticipate in a narrative chain than those not sharing.In addition, the narrative approach captures gram-matical constraints on narrative coherence.
Simpledistributional learning might discover that the verbpush is related to the verb fall, but narrative learningcan capture additional facts about the participants,specifically, that the object or patient of the push isthe subject or agent of the fall.Each focused protagonist chain offers one per-spective on a narrative, similar to the multiple per-spectives on a commercial transaction event offeredby buy and sell.3.3 Partial OrderingA narrative chain, by definition, includes a partialordering of events.
Early work on scripts includedordering constraints with more complex precondi-tions and side effects on the sequence of events.
Thispaper presents work toward a partial ordering andleaves logical constraints as future work.
We focuson the before relation, but the model does not pre-clude advanced theories of temporal order.4 Learning Narrative RelationsOur first model learns basic information about anarrative chain: the protagonist and the constituentsubevents, although not their ordering.
For this weneed a metric for the relation between an event anda narrative chain.Pairwise relations between events are first ex-tracted unsupervised.
A distributional score basedon how often two events share grammatical argu-ments (using pointwise mutual information) is usedto create this pairwise relation.
Finally, a global nar-rative score is built such that all events in the chainprovide feedback on the event in question (whetherfor inclusion or for decisions of inference).Given a list of observed verb/dependency counts,we approximate the pointwise mutual information(PMI) by:pmi(e(w, d), e(v, g)) = logP (e(w, d), e(v, g))P (e(w, d))P (e(v, g))(1)where e(w, d) is the verb/dependency pair w and d(e.g.
e(push,subject)).
The numerator is defined by:P (e(w, d), e(v, g)) =C(e(w, d), e(v, g))?x,y?d,f C(e(x, d), e(y, f))(2)where C(e(x, d), e(y, f)) is the number of times thetwo events e(x, d) and e(y, f) had a coreferring en-tity filling the values of the dependencies d and f .We also adopt the ?discount score?
to penalize lowoccuring words (Pantel and Ravichandran, 2004).Given the debate over appropriate metrics for dis-tributional learning, we also experimented with thet-test.
Our experiments found that PMI outperformsthe t-test on this task by itself and when interpolatedtogether using various mixture weights.Once pairwise relation scores are calculated, aglobal narrative score can then be built such that allevents provide feedback on the event in question.For instance, given all narrative events in a docu-ment, we can find the next most likely event to occurby maximizing:maxj:0<j<mn?i=0pmi(ei, fj) (3)where n is the number of events in our chain andei is the ith event.
m is the number of events f inour training corpus.
A ranked list of guesses can bebuilt from this summation and we hypothesize that791Known events:(pleaded subj), (admits subj), (convicted obj)Likely Events:sentenced obj 0.89 indicted obj 0.74paroled obj 0.76 fined obj 0.73fired obj 0.75 denied subj 0.73Figure 1: Three narrative events and the six most likelyevents to include in the same chain.the more events in our chain, the more informed ourranked output.
An example of a chain with 3 eventsand the top 6 ranked guesses is given in figure 1.4.1 Evaluation Metric: Narrative ClozeThe cloze task (Taylor, 1953) is used to evaluate asystem (or human) for language proficiency by re-moving a random word from a sentence and havingthe system attempt to fill in the blank (e.g.
I forgotto the waitress for the good service).
Depend-ing on the type of word removed, the test can evalu-ate syntactic knowledge as well as semantic.
Deyes(1984) proposed an extended task, discourse cloze,to evaluate discourse knowledge (removing phrasesthat are recoverable from knowledge of discourse re-lations like contrast and consequence).We present a new cloze task that requires narra-tive knowledge to solve, the narrative cloze.
Thenarrative cloze is a sequence of narrative events in adocument from which one event has been removed.The task is to predict the missing verb and typed de-pendency.
Take this example text about Americanfootball with McCann as the protagonist:1.
McCann threw two interceptions early.2.
Toledo pulled McCann aside and told him he?d start.3.
McCann quickly completed his first two passes.These clauses are represented in the narrative modelas five events: (threw subject), (pulled object),(told object), (start subject), (completed subject).These verb/dependency events make up a narrativecloze model.
We could remove (threw subject) anduse the remaining four events to rank this missingevent.
Removing a single such pair to be filled in au-tomatically allows us to evaluate a system?s knowl-edge of narrative relations and coherence.
We do notclaim this cloze task to be solvable even by humans,New York Times Editorialoccupied subj brought subj rejecting subjprojects subj met subj appeared subjoffered subj voted pp for offer subjthinks subjFigure 2: One of the 69 test documents, containing 10narrative events.
The protagonist is President Bush.but rather assert it as a comparative measure to eval-uate narrative knowledge.4.2 Narrative Cloze ExperimentWe use years 1994-2004 (1,007,227 documents) ofthe Gigaword Corpus (Graff, 2002) for training2.We parse the text into typed dependency graphswith the Stanford Parser (de Marneffe et al, 2006)3,recording all verbs with subject, object, or preposi-tional typed dependencies.
We use the OpenNLP4coreference engine to resolve the entity mentions.For each document, the verb pairs that share core-ferring entities are recorded with their dependencytypes.
Particles are included with the verb.We used 10 news stories from the 1994 sectionof the corpus for development.
The stories werehand chosen to represent a range of topics such asbusiness, sports, politics, and obituaries.
We used69 news stories from the 2001 (year selected ran-domly) section of the corpus for testing (also re-moved from training).
The test set documents wererandomly chosen and not preselected for a range oftopics.
From each document, the entity involvedin the most events was selected as the protagonist.For this evaluation, we only look at verbs.
Allverb clauses involving the protagonist are manu-ally extracted and translated into the narrative events(verb,dependency).
Exceptions that are not includedare verbs in headlines, quotations (typically not partof a narrative), ?be?
properties (e.g.
john is happy),modifying verbs (e.g.
hurried to leave, only leave isused), and multiple instances of one event.The original test set included 100 documents, but2The document count does not include duplicate news sto-ries.
We found up to 18% of the corpus are duplications, mostlyAP reprints.
We automatically found these by matching the firsttwo paragraphs of each document, removing exact matches.3http://nlp.stanford.edu/software/lex-parser.shtml4http://opennlp.sourceforge.net792those without a narrative chain at least five events inlength were removed, leaving 69 documents.
Mostof the removed documents were not stories, but gen-res such as interviews and cooking recipes.
An ex-ample of an extracted chain is shown in figure 2.We evalute with Narrative Cloze using leave-one-out cross validation, removing one event and usingthe rest to generate a ranked list of guesses.
The testdataset produces 740 cloze tests (69 narratives with740 events).
After generating our ranked guesses,the position of the correct event is averaged over all740 tests for the final score.
We penalize unseenevents by setting their ranked position to the lengthof the guess list (ranging from 2k to 15k).Figure 1 is an example of a ranked guess list for ashort chain of three events.
If the original documentcontained (fired obj), this cloze test would score 3.4.2.1 BaselineWe want to measure the utility of the protago-nist and the narrative coherence assumption, so ourbaseline learns relatedness strictly based upon verbco-occurence.
The PMI is then defined as betweenall occurrences of two verbs in the same document.This baseline evaluation is verb only, as dependen-cies require a protagonist to fill them.After initial evaluations, the baseline was per-forming very poorly due to the huge amount of datainvolved in counting all possible verb pairs (using aprotagonist vastly reduces the number).
We exper-imented with various count cutoffs to remove rareoccurring pairs of verbs.
The final results use a base-line where all pairs occurring less than 10 times inthe training data are removed.Since the verb-only baseline does not use typeddependencies, our narrative model cannot directlycompare to this abstracted approach.
We thus mod-ified the narrative model to ignore typed dependen-cies, but still count events with shared arguments.Thus, we calculate the PMI across verbs that sharearguments.
This approach is called Protagonist.The full narrative model that includes the grammat-ical dependencies is called Typed Deps.4.2.2 ResultsExperiments with varying sizes of training dataare presented in figure 3.
Each ranked list ofcandidate verbs for the missing event in Base-1995 1996 1997 1998 1999 2000 2001 2002 2003 2004050010001500200025003000Training Data from 1994?XRankedPositionNarrative Cloze TestBaseline Protagonist Typed DepsFigure 3: Results with varying sizes of training data.
Year2003 is not explicitly shown because it has an unusuallysmall number of documents compared to other years.line/Protagonist contained approximately 9 thou-sand candidates.
Of the 740 cloze tests, 714 of theremoved events were present in their respective listof guesses.
This is encouraging as only 3.5% of theevents are unseen (or do not meet cutoff thresholds).When all training data is used (1994-2004), theaverage ranked position is 1826 for Baseline and1160 for Protagonist (1 being most confident).
TheBaseline performs better at first (years 1994-5), butas more data is seen, the Baseline worsens whilethe Protagonist improves.
This verb-only narrativemodel shows a 36.5% improvement over the base-line trained on all years.
Results from the full TypedDeps model, not comparable to the baseline, paral-lel the Protagonist results, improving as more data isseen (average ranked position of 1908 with all thetraining data).
We also ran the experiment with-out OpenNLP coreference, and instead used exactand substring matching for coreference resolution.This showed a 5.7% decrease in the verb-only re-sults.
These results show that a protagonist greatlyassists in narrative judgements.5 Ordering Narrative EventsThe model proposed in the previous section is de-signed to learn the major subevents in a narrativechain, but not how these events are ordered.
In thissection we extend the model to learn a partial tem-poral ordering of the events.793There are a number of algorithms for determiningthe temporal relationship between two events (Maniet al, 2006; Lapata and Lascarides, 2006; Cham-bers et al, 2007), many of them trained on the Time-Bank Corpus (Pustejovsky et al, 2003) which codesevents and their temporal relationships.
The cur-rently highest performing of these on raw data is themodel of temporal labeling described in our previ-ous work (Chambers et al, 2007).
Other approacheshave depended on hand tagged features.Chambers et al (2007) shows 59.4% accuracy onthe classification task for six possible relations be-tween pairs of events: before, immediately-before,included-by, simultaneous, begins and ends.
We fo-cus on the before relation because the others areless relevant to our immediate task.
We combineimmediately-before with before, and merge the otherfour relations into an other category.
At the binarytask of determining if one event is before or other,we achieve 72.1% accuracy on Timebank.The above approach is a two-stage machine learn-ing architecture.
In the first stage, the model usessupervised machine learning to label temporal at-tributes of events, including tense, grammatical as-pect, and aspectual class.
This first stage classi-fier relies on features such as neighboring part ofspeech tags, neighboring auxiliaries and modals, andWordNet synsets.
We use SVMs (Chambers et al(2007) uses Naive Bayes) and see minor perfor-mance boosts on Timebank.
These imperfect clas-sifications, combined with other linguistic features,are then used in a second stage to classify the tem-poral relationship between two events.
Other fea-tures include event-event syntactic properties suchas the syntactic dominance relations between thetwo events, as well as new bigram features of tense,aspect and class (e.g.
?present past?
if the first eventis in the present, and the second past), and whetherthe events occur in the same or different sentences.5.1 Training a Temporal ClassifierWe use the entire Timebank Corpus as super-vised training data, condensing the before andimmediately-before relations into one before rela-tion.
The remaining relations are merged into other.The vast majority of potential event pairs in Time-bank are unlabeled.
These are often none relations(events that have no explicit relation) or as is of-ten the case, overlap relations where the two eventshave no Timebank-defined ordering but overlap intime.
Even worse, many events do have an order-ing, but they were not tagged by the human annota-tors.
This could be due to the overwhelming task oftemporal annotation, or simply because some eventorderings are deemed more important than others inunderstanding the document.
We consider all un-tagged relations as other, and experiment with in-cluding none, half, and all of them in training.Taking a cue from Mani et al (2006), we alsoincreased Timebank?s size by applying transitivityrules to the hand labeled data.
The following is anexample of the applied transitive rule:if run BEFORE fall and fall BEFORE injuredthen run BEFORE injuredThis increases the number of relations from 37519to 45619.
Perhaps more importantly for our task,of all the added relations, the before relation isadded the most.
We experimented with original vs.expanded Timebank and found the expanded per-formed slightly worse.
The decline may be due topoor transitivity additions, as several Timebank doc-uments contain inconsistent labelings.
All reportedresults are from training without transitivity.5.2 Temporal Classifier in Narrative ChainsWe classify the Gigaword Corpus in two stages,once for the temporal features on each event (tense,grammatical aspect, aspectual class), and once be-tween all pairs of events that share arguments.
Thisallows us to classify the before/other relations be-tween all potential narrative events.The first stage is trained on Timebank, and thesecond is trained using the approach describedabove, varying the size of the none training rela-tions.
Each pair of events in a gigaword documentthat share a coreferring argument is treated as a sepa-rate ordering classification task.
We count the result-ing number of labeled before relations between eachverb/dependency pair.
Processing the entire corpusproduces a database of event pair counts where con-fidence of two generic events A and B can be mea-sured by comparing how many before labels havebeen seen versus their inverted order B and A5.5Note that we train with the before relation, and so transpos-ing two events is similar to classifying the after relation.7945.3 Temporal EvaluationWe want to evaluate temporal order at the narrativelevel, across all events within a chain.
We envisionnarrative chains being used for tasks of coherence,among other things, and so it is desired to evaluatetemporal decisions within a coherence framework.Along these lines, our test set uses actual narrativechains from documents, hand labeled for a partialordering.
We evaluate coherence of these true chainsagainst a random ordering.
The task is thus decidingwhich of the two chains is most coherent, the orig-inal or the random (baseline 50%)?
We generatedup to 300 random orderings for each test document,averaging the accuracy across all.Our evaluation data is the same 69 documentsused in the test set for learning narrative relations.The chain from each document is hand identifiedand labeled for a partial ordering using only the be-fore relation.
Ordering was done by the authors andall attempts were made to include every before re-lation that exists in the document, or that could bededuced through transitivity rules.
Figure 4 showsan example and its full reversal, although the evalu-ation uses random orderings.
Each edge is a distinctbefore relation and is used in the judgement score.The coherence score for a partially ordered nar-rative chain is the sum of all the relations that ourclassified corpus agrees with, weighted by how cer-tain we are.
If the gigaword classifications disagree,a weighted negative score is given.
Confidence isbased on a logarithm scale of the difference betweenthe counts of before and after classifications.
For-mally, the score is calculated as the following:?E:x,y??????
?log(D(x, y)) if x?y and B(x, y) > B(y, x)?log(D(x, y)) if x?y and B(y, x) > B(x, y)?log(D(x, y)) if !x?y & !y?x & D(x, y) > 00 otherwisewhere E is the set of all event pairs, B(i, j) is howmany times we classified events i and j as before inGigaword, and D(i, j) = |B(i, j) ?
B(j, i)|.
Therelation i?j indicates that i is temporally before j.5.4 ResultsOut approach gives higher scores to orders that co-incide with the pairwise orderings classified in ourgigaword training data.
The results are shown in fig-ure 5.
Of the 69 chains, 6 did not have any orderedevents and were removed from the evaluation.
WeFigure 4: A narrative chain and its reverse order.All ?
6 ?
10correct 8086 75% 7603 78% 6307 89%incorrect 1738 1493 619tie 931 627 160Figure 5: Results for choosing the correct ordered chain.(?
10) means there were at least 10 pairs of orderedevents in the chain.generated (up to) 300 random orderings for each ofthe remaining 63.
We report 75.2% accuracy, but 22of the 63 had 5 or fewer pairs of ordered events.
Fig-ure 5 therefore shows results from chains with morethan 5 pairs, and also 10 or more.
As we wouldhope, the accuracy improves the larger the orderednarrative chain.
We achieve 89.0% accuracy on the24 documents whose chains most progress throughtime, rather than chains that are difficult to orderwith just the before relation.Training without none relations resulted in highrecall for before decisions.
Perhaps due to data spar-sity, this produces our best results as reported above.6 Discrete Narrative Event ChainsUp till this point, we have learned narrative relationsacross all possible events, including their temporalorder.
However, the discrete lists of events for whichSchank scripts are most famous have not yet beenconstructed.We intentionally did not set out to reproduce ex-plicit self-contained scripts in the sense that the?restaurant script?
is complete and cannot includeother events.
The name narrative was chosen to im-ply a likely order of events that is common in spokenand written retelling of world events.
Discrete setshave the drawback of shutting out unseen and un-795Figure 6: An automatically learned Prosecution Chain.Arrows indicate the before relation.likely events from consideration.
It is advantageousto consider a space of possible narrative events andthe ordering within, not a closed list.However, it is worthwhile to construct discretenarrative chains, if only to see whether the combina-tion of event learning and ordering produce script-like structures.
This is easily achievable by usingthe PMI scores from section 4 in an agglomerativeclustering algorithm, and then applying the orderingrelations from section 5 to produce a directed graph.Figures 6 and 7 show two learned chains afterclustering and ordering.
Each arrow indicates a be-fore relation.
Duplicate arrows implied by rules oftransitivity are removed.
Figure 6 is remarkably ac-curate, and figure 7 addresses one of the chains fromour introduction, the employment narrative.
Thecore employment events are accurate, but cluster-ing included life events (born, died, graduated) fromobituaries of which some temporal information is in-correct.
The Timebank corpus does not include obit-uaries, thus we suffer from sparsity in training data.7 DiscussionWe have shown that it is possible to learn narrativeevent chains unsupervised from raw text.
Not onlydo our narrative relations show improvements overa baseline, but narrative chains offer hope for manyother areas of NLP.
Inference, coherence in summa-rization and generation, slot filling for question an-swering, and frame induction are all potential areas.We learned a new measure of similarity, the nar-Figure 7: An Employment Chain.
Dotted lines indicateincorrect before relations.rative relation, using the protagonist as a hook to ex-tract a list of related events from each document.The 37% improvement over a verb-only baselineshows that we may not need presorted topics of doc-uments to learn inferences.
In addition, we appliedstate of the art temporal classification to show thatsets of events can be partially ordered.
Judgementsof coherence can then be made over chains withindocuments.
Further work in temporal classificationmay increase accuracy even further.Finally, we showed how the event space of narra-tive relations can be clustered to create discrete sets.While it is unclear if these are better than an uncon-strained distribution of events, they do offer insightinto the quality of narratives.An important area not discussed in this paper isthe possibility of using narrative chains for semanticrole learning.
A narrative chain can be viewed asdefining the semantic roles of an event, constrainingit against roles of the other events in the chain.
Anargument?s class can then be defined as the set ofnarrative arguments in which it appears.We believe our model provides an important firststep toward learning the rich causal, temporal andinferential structure of scripts and frames.Acknowledgment: This work is funded in partby DARPA through IBM and by the DTO Phase IIIProgram for AQUAINT through Broad Agency An-nouncement (BAA) N61339-06-R-0034.
Thanks to thereviewers for helpful comments and the suggestion for anon-full-coreference baseline.796ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In ChristianBoitet and Pete Whitelock, editors, ACL-98, pages 86?90, San Francisco, California.
Morgan Kaufmann Pub-lishers.Regina Barzilay and Mirella Lapata.
2005.
Modeling lo-cal coherence: an entity-based approach.
Proceedingsof the 43rd Annual Meeting on Association for Com-putational Linguistics, pages 141?148.David Bean and Ellen Riloff.
2004.
Unsupervised learn-ing of contextual role knowledge for coreference reso-lution.
Proc.
of HLT/NAACL, pages 297?304.Samuel Brody.
2007.
Clustering Clauses for High-Level Relation Detection: An Information-theoreticApproach.
Proceedings of the 43rd Annual Meetingof the Association of Computational Linguistics, pages448?455.Nathanael Chambers, Shan Wang, and Dan Jurafsky.2007.
Classifying temporal relations between events.In Proceedings of ACL-07, Prague, Czech Republic.Timothy Chklovski and Patrick Pantel.
2004.
Verbocean:Mining the web for fine-grained semantic verb rela-tions.
In Proceedings of EMNLP-04.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC-06, pages 449?454.Tony Deyes.
1984.
Towards an authentic ?discoursecloze?.
Applied Linguistics, 5(2).Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-mura.
2003.
Automatic acquisition of script knowl-edge from a text collection.
In EACL, pages 91?94.David Graff.
2002.
English Gigaword.
Linguistic DataConsortium.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modelling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2).Mirella Lapata and Alex Lascarides.
2006.
Learningsentence-internal temporal relations.
In Journal of AIResearch, volume 27, pages 85?117.C.Y.
Lin and E. Hovy.
2000.
The automated acquisi-tion of topic signatures for text summarization.
Pro-ceedings of the 17th conference on Computationallinguistics-Volume 1, pages 495?501.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine learningof temporal relations.
In Proceedings of ACL-06, July.Raymond Mooney and Gerald DeJong.
1985.
Learningschemata for natural language processing.
In Ninth In-ternational Joint Conference on Artificial Intelligence(IJCAI), pages 681?687.Jane Morris and Graeme Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indicator of thestructure of text.
Computational Linguistics, 17:21?43.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
Proceedings ofHLT/NAACL, 4:321?328.James Pustejovsky, Patrick Hanks, Roser Sauri, AndrewSee, David Day, Lisa Ferro, Robert Gaizauskas, Mar-cia Lazo, Andrea Setzer, and Beth Sundheim.
2003.The timebank corpus.
Corpus Linguistics, pages 647?656.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,plans, goals and understanding.
Lawrence Erlbaum.Wilson L. Taylor.
1953.
Cloze procedure: a new tool formeasuring readability.
Journalism Quarterly, 30:415?433.797
