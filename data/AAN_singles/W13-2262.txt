Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 494?502,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsMulti-rate HMMs for Word AlignmentElif Eyigo?zComputer ScienceUniversity of RochesterRochester, NY 14627Daniel GildeaComputer ScienceUniversity of RochesterRochester, NY 14627Kemal OflazerComputer ScienceCarnegie Mellon UniversityPO Box 24866, Doha, QatarAbstractWe apply multi-rate HMMs, a tree struc-tured HMM model, to the word-alignmentproblem.
Multi-rate HMMs allow us tomodel reordering at both the morphemelevel and the word level in a hierarchicalfashion.
This approach leads to better ma-chine translation results than a morpheme-aware model that does not explicitly modelmorpheme reordering.1 IntroductionWe present an HMM-based word-alignmentmodel that addresses transitions between mor-pheme positions and word positions simultane-ously.
Our model is an instance of a multi-scaleHMM, a widely used method for modeling dif-ferent levels of a hierarchical stochastic process.In multi-scale modeling of language, the deepestlevel of the hierarchy may consist of the phonemesequence, and going up in the hierarchy, the nextlevel may consist of the syllable sequence, andthen the word sequence, the phrase sequence, andso on.
By the same token, in the hierarchical word-alignment model we present here, the lower levelconsists of the morpheme sequence and the higherlevel the word sequence.Multi-scale HMMs have a natural application inlanguage processing due to the hierarchical natureof linguistic structures.
They have been used formodeling text and handwriting (Fine et al 1998),in signal processing (Willsky, 2002), knowledgeextraction (Skounakis et al 2003), as well as inother fields of AI such as vision (Li et al 2006;Luettgen et al 1993) and robotics (Theocharouset al 2001).
The model we propose here is mostsimilar to multi-rate HMMs (C?etin et al 2007),which were applied to a classification problem inindustrial machine tool wear.The vast majority of languages exhibit morphol-ogy to some extent, leading to various efforts inmachine translation research to include morphol-ogy in translation models (Al-Onaizan et al 1999;Niessen and Ney, 2000; C?mejrek et al 2003;Lee, 2004; Chung and Gildea, 2009; Yeniterzi andOflazer, 2010).
For the word-alignment problem,Goldwater and McClosky (2005) and Eyigo?z et al(2013) suggested word alignment models that ad-dress morphology directly.Eyigo?z et al(2013) introduced two-level align-ment models (TAM), which adopt a hierarchi-cal representation of alignment: the first level in-volves word alignment, the second level involvesmorpheme alignment.
TAMs jointly induce wordand morpheme alignments using an EM algorithm.TAMs can align rarely occurring words throughtheir frequently occurring morphemes.
In otherwords, they use morpheme probabilities to smoothrare word probabilities.Eyigo?z et al(2013) introduced TAM 1, which isanalogous to IBM Model 1, in that the first level isa bag of words in a pair of sentences, and the sec-ond level is a bag of morphemes.
By introducingdistortion probabilities at the word level, Eyigo?zet al(2013) defined the HMM extension of TAM1, the TAM-HMM.
TAM-HMM was shown tobe superior to its single-level counterpart, i.e., theHMM-based word alignment model of Vogel et al(1996).The alignment example in Figure 1 shows aTurkish word aligned to an English phrase.
Themorphemes of the Turkish word are aligned tothe English words.
As the example shows, mor-phologically rich languages exhibit complex re-ordering phenomena at the morpheme level, whichis left unutilized in TAM-HMMs.
In this paper,we add morpheme sequence modeling to TAMsto capture morpheme level distortions.
The ex-ample also shows that the Turkish morpheme or-494Figure 1: Turkish word aligned to an Englishphrase.der is the reverse of the English word order.
Be-cause this pattern spans several English words, itcan only be captured by modeling morpheme re-ordering across word boundaries.
We chose multi-rate HMMs over other hierarchical HMM mod-els because multi-rate HMMs allow morpheme se-quence modeling across words over the entire sen-tence.It is possible to model the morpheme sequenceby treating morphemes as words: segmenting thewords into morphemes, and using word-basedword alignment models on the segmented data.Eyigo?z et al(2013) showed that TAM-HMM per-forms better than treating morphemes as words.Since the multi-rate HMM allows both wordand morpheme sequence modeling, it is a gener-alization of TAM-HMM, which allows only wordsequence modeling.
TAM-HMM in turn is a gen-eralization of the model suggested by Goldwaterand McClosky (2005) and TAM 1.
Our resultsshow that multi-rate HMMs are superior to TAM-HMMs.
Therefore, multi-rate HMMs are the besttwo-level alignment models proposed so far.2 Two-level Alignment Model (TAM)The two-level alignment model (TAM) takes theapproach of assigning probabilities to both word-to-word translations and morpheme-to-morphemetranslations simultaneously, allowing morpheme-level probabilities to guide alignment for rare wordpairs.
TAM is based on a concept of alignmentdefined at both the word and morpheme levels.2.1 Morpheme AlignmentA word alignment aw is a function mapping a setof word positions in a target language sentence eto a set of word positions in a source language sen-tence f , as exemplified in Figure 2.
A morphemealignment am is a function mapping a set of mor-pheme positions in a target language sentence toa set of morpheme positions in a source languagesentence.
A morpheme position is a pair of inte-gers (j, k), which defines a word position j and arelative morpheme position k in the word at posi-tion j, as shown in Figure 3.
The word and mor-pheme alignments below are depicted in Figures 2and 3.aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1A morpheme alignment am and a word alignmentaw are compatible if and only if they satisfy thefollowing conditions: If the morpheme alignmentam maps a morpheme of e to a morpheme of f ,then the word alignment aw maps e to f .
If theword alignment aw maps e to f , then the mor-pheme alignment am maps at least one morphemeof e to a morpheme of f .
If the word align-ment aw maps e to null, then all of its morphemesare mapped to null.
Figure 3 shows a morphemealignment that is compatible with, i.e., restrictedby, the word alignment in Figure 2.
The smallerboxes embedded inside the main box in Figure 3depict the embedding of the morpheme level in-side the word level in two-level alignment models(TAM).2.2 TAM 1We call TAM without sequence modeling TAM 1,because it defines an embedding of IBM Model 1(Brown et al 1993) for morphemes inside IBMModel 1 for words.
In TAM 1, p(e|f), the prob-ability of translating the sentence f into e is com-puted by summing over all possible word align-ments and all possible morpheme alignments thatare compatible with a given word alignment aw:Word MorphemeRw|e|?j=1|f |?i=0?
?t(ej |fi) Rm|ej |?k=1|fi|?n=0t(ekj |fni )??
(1)where fni is the nth morpheme of the word at po-sition i.
The probability of translating the word fiinto the word ej is computed by summing over allpossible morpheme alignments between the mor-phemes of ej and fi.
Rw substitutes P (le|lf )(lf+1)le foreasy readability.1 Rm is equivalent to Rw except1le = |e| is the number of words in sentence e and lf =|f |.495Figure 2: Word alignment Figure 3: Morpheme alignmentfor the fact that its domain is not the set of sen-tences but the set of words.
The length of a word isthe number of morphemes in the word.
The lengthof words ej and fi in R(ej , fi) are the number ofmorphemes of ej and fi.
We assume that all un-aligned morphemes in a sentence map to a specialnull morpheme.TAM 1 with the contribution of both word andmorpheme translation probabilities, as in Eqn.
1, iscalled ?word-and-morpheme?
version of TAM 1.The model is technically deficient probabilisti-cally, as it models word and morpheme transla-tion independently, and assigns mass to invalidword/morpheme combinations.
We can also de-fine the ?morpheme-only?
version of TAM 1 bycanceling out the contribution of word translationprobabilities and assigning 1 to t(ej |fi) in Eqn.
1.Please note that, although this version of the two-level alignment model does not use word transla-tion probabilities, it is also a word-aware model, asmorpheme alignments are restricted to correspondto a valid word alignment.
As such, it also allowsfor word level sequence modeling by HMMs.
Fi-nally, canceling out the contribution of morphemetranslation probabilities reduces TAM 1 to IBMModel 1.
Just as IBM Model 1 is used for initial-ization before HMM-based word-alignment mod-els (Vogel et al 1996; Och and Ney, 2003), TAMModel 1 is used to initialize its HMM extensions,which are described in the next section.3 Multi-rate HMMLike other multi-scale HMM models such as hi-erarchical HMM?s (Fine et al 1998) and hiddenMarkov trees (Crouse et al 1998), the multi-rateHMM characterizes the inter-scale dependenciesby a tree structure.
As shown in Figure 5, scalesare organized in a hierarchical manner from coarseto fine, which allows for efficient representation ofboth short- and long-distance context simultane-ously.We found that 51% of the dependency relationsin the Turkish Treebank (Oflazer et al 2003) arebetween the last morpheme of a dependent wordand the first morpheme (the root) of the head wordthat is immediately to its right, which is exempli-fied below.
The following examples show Englishsentences in Turkish word/morpheme order.
Thepseudo Turkish words are formed by concatena-tion of English morphemes, which are indicatedby the ?+?
between the morphemes.?
?
I will come from X.?
X+ABL come+will+I?
?
I will look at X.?
X+DAT look+will+IIn English, the verb ?come?
subcategorizes fora PP headed by ?from?
in the example above.In the pseudo Turkish version of this sentence,?come?
subcategorizes for a NP marked with abla-tive case (ABL), which corresponds to the prepo-sition ?from?.
Similarly, ?look?
subcategorizes fora PP headed by ?at?
in English, and a NP markedwith dative case (DAT) in Turkish.
Just as the verband the preposition that it subcategorizes for arefrequently found adjacent to each other in English,the verb and the case that it subcategorizes for arefrequently found adjacent to each other in Turk-ish.
Thus, we have a pattern of three correspond-ing morphemes appearing in reverse order in En-glish and Turkish, spanning two words in Turkishand three words in English.
In order to capturesuch regularities, we chose multi-rate HMMs overother hierarchically structured HMM models be-cause, unlike other models, multi-rate HMMs al-low morpheme sequence modeling across wordsover the entire sentence.
This allows us to capturemorpheme-mediated syntactic relations betweenwords (Eryig?it et al 2008), as exemplified above.Morpheme sequence modeling across words isshown in Figure 4 by the arrows after the nodes496Figure 4: Multi-rate HMM graph.representing fam(0,2) and fam(1,2).
The circlesrepresent the words and morphemes of the sourcelanguage, the squares represent the words andmorphemes of the target language.
e0,2 is the lastmorpheme of word e0, and e1,0 is the first mor-pheme of the next word e1.
fam(1,0) is conditionedon fam(0,2), which is in the previous word.In order to model the morpheme sequenceacross words, we define the function prev(j, k),which maps the morpheme position (j, k) to theprevious morpheme position:prev(j, k) ={(j, k ?
1) if k > 1(j ?
1, |ej?1|) if k = 1If a morpheme is the first morpheme of a word,then the previous morpheme is the last morphemeof the previous word.3.1 Transitions3.1.1 Morpheme transitionsBefore introducing the morpheme level transitionprobabilities, we first restrict morpheme level tran-sitions according to the assumptions of our model.We consider only the morpheme alignment func-tions that are compatible with a word alignmentfunction.
If we allow unrestricted transitions be-tween morphemes, then this would result in somemorpheme alignments that do not allow a validword alignment function.To avoid this problem, we restrict the transi-tion function as follows: at each time step, weallow transitions between morphemes in sentencef if the morphemes belong to the same word.This restriction reduces the transition matrix to ablock diagonal matrix.
The block diagonal matrixAb below is a square matrix which has blocks ofsquare matrices A1 ?
?
?An on the main diagonal,and the off-diagonal values are zero.Ab =????
?A0 0 ?
?
?
00 A1 ?
?
?
0... ... .
.
.
...0 0 ?
?
?
An????
?The square blocks A0, .
.
.
,An have the dimen-sions |f0|, .
.
.
, |fn|, the length of the words in sen-tence f .
In each step of the forward-backward al-gorithm, multiplying the forward (or backward)probability vectors with the block diagonal ma-trix restricts morpheme transitions to occur onlywithin the words of sentence f .In order to model the morpheme sequenceacross words, we also allow transitions betweenmorphemes across the words in sentence f .
How-ever, we allow cross-word transitions only at cer-tain time steps: between the last morpheme of aword in sentence e and the first morpheme of thenext word in sentence e. This does not result inmorpheme alignments that do not allow a validword alignment function.
Instead of the block di-agonal matrix Ab, we use a transition matrix Awhich is not necessarily block diagonal, to modelmorpheme transitions across words.In sum, we multiply the forward (or backward)probability vectors with either the transition ma-trix Ab or the transition matrix A, depending onwhether the transition is occurring at the last mor-pheme of a word in e. We introduce the function?
(p, q, r, s) to indicate whether a transition is al-lowed from source position (p, q) to source posi-497tion (r, s) when advancing one target position:?
(p, q, r, s) ={1 if p = r or s = 10 otherwiseMorpheme transition probabilities have fourcomponents.
First, the ?
function as describedabove.
Second, the jump width:J (p, q, r, s) = abs(r, s)?
abs(p, q)where abs(j, k) maps a word-relative morphemeposition to an absolute morpheme position, i.e., tothe simple left-to-right ordering of a morpheme ina sentence.
Third, the morpheme class of the pre-vious morpheme:2M(p, q) = Class(f qp )Fourth, as the arrow from faw(0) to fam(0,0) in Fig-ure 4 shows, there is a conditional dependence onthe word class that the morpheme is in:W(r) = Class(fr)Putting together these components, the morphemetransitions are formulated as follows:p(am(j, k) = (r, s) | am(prev(j, k)) = (p, q)) ?p(J (p, q, r, s)|M(p, q),W(r))?
(p, q, r, s)(2)The block diagonal matrix Ab consists of mor-pheme transition probabilities.3.1.2 Word transitionsIn the multi-rate HMM, word transition probabili-ties have two components.
First, the jump width:J (p, r) = r ?
pSecond, the word class of the previous word:W(p) = Class(fp)The jump width is conditioned on the word classof the previous word:p(aw(j) = r | aw(j ?
1) = p) ?p(J (p, r) | W(p)) (3)The transition matrix A, which is not necessarilyblock diagonal, consists of values which are theproduct of a morpheme transition probability, asdefined in Eqn.
2, and a word transition probabil-ity, as defined in Eqn.
3.2We used the mkcls tool in GIZA (Och and Ney, 2003)to learn the word and the morpheme classes.3.2 Probability of translating a sentenceFinally, putting together Eqn.
1, Eqn.
2 and Eqn.
3,we formulate the probability of translating a sen-tence p(e|f) as follows:Rw?aw|e|?j=1(t(ej |faw(j))p(aw(j)|aw(j?1))Rm?am|ej |?k=1t(ej,k|fam(j,k))p(am(j,k)|am(prev(j,k))))Rw is the same as it is in Eqn.
1, whereasRm = P (le|lf ).
If we cancel out morpheme tran-sitions by setting p(am(j, k)|am(prev(j, k))) =1/|fam(j,k)|, i.e., with a uniform distribution, thenwe get TAM with only word-level sequence mod-eling, which we call TAM-HMM.The complexity of the multi-rate HMM isO(m3n3), where n is the number of words, andm is the number of morphemes per word.
TAM-HMM differs from multi-rate HMM only by thelack of morpheme-level sequence modeling, andhas complexity O(m2n3).For the HMM to work correctly, we must han-dle jumping to and jumping from null positions.We learn the probabilities of jumping to a null po-sition from the data.
To compute the transitionprobability from a null position, we keep track ofthe nearest previous source word (or morpheme)that does not align to null, and use the position ofthe previous non-null word to calculate the jumpwidth.
In order to keep track of the previous non-null word, we insert a null word between words(Och and Ney, 2003).
Similarly, we insert a nullmorpheme after every non-null morpheme.3.3 CountsWe use Expectation Maximization (EM) to learnthe word and morpheme translation probabili-ties, as well as the transition probabilities of thereordering model.
This is done with forward-backward training at the morpheme level, collect-ing translation and transition counts for both theword and the morphemes from the morpheme-level trellis.In Figure 5, the grid on the right depicts themorpheme-level trellis.
The grid on the left isthe abstraction of the word-level trellis over the498Figure 5: Multi-rate HMM trellismorpheme-level trellis.
For each target word e andfor each source word f , there is a small HMM trel-lis with dimensions |e|?|f | inside the morpheme-level trellis, as shown by the shaded area inside thegrid on the right.
We collect counts for words bysumming over the values in the small HMM trellisassociated with the words.3.3.1 Translation countsMorpheme translation counts We compute ex-pected counts over the morpheme-level trellis.The morpheme translation count function belowcollects expected counts for a morpheme pair(h, g) in a sentence pair (e, f):cm(h|g; e, f) =?(j,k)s.t.h=ekj?
(p,q)s.t.g=fqp?j,k(p, q)where ?j,k(p, q) stands for the posterior mor-pheme translation probabilities for source position(p, q) and target position (i, j) that are computedwith the forward-backward algorithm.Word translation counts For each target worde and source word f , we collect word transla-tion counts by summing over posterior morphemetranslation probabilities that are in the small trellisassociated with e and f .Since ?
allows only within-word transitions tooccur inside the small trellis, the posterior proba-bility of observing the word e given the word fis preserved across time points within the smalltrellis associated with e and f .
In other words,the sum of the posterior probabilities in each col-umn of the small trellis is the same.
Therefore, wecollect word translation counts only from the lastmorphemes of the words in e.The word translation count function below col-lects expected counts from a sentence pair (e, f)for a particular source word f and target word e:cw(e|f ; e, f) =?js.t.e=ej?ps.t.f=fp?1?q?|f |?j,|e|(p, q)3.3.2 Transition countsMorpheme transition counts For all target po-sitions (j, k) and all pairs of source positions (p, q)and (r, s), we compute morpheme transition pos-teriors:?j,k((p, q), (r, s))using the forward-backward algorithm.
Theseexpected counts are accumulated to esti-mate the morpheme jump width probabilitiesp(J (p, q, r, s)|M(p, q),W(r)) used in Eqn.
2.Word transition counts We compute posteriorprobabilities for word transitions by summing overmorpheme transition posteriors between the mor-phemes of the words fl and fn:?j(p, r) =?1?q?|fp|?1?s?|fr|?j,|ej |((p, q), (r, s))Like the translation counts, the transition countsare collected from the last morphemes of wordsin e. These expected counts are accumulatedto estimate the word jump width probabilitiesp(J (p, r) | W(p)) used in Eqn.
3.Finally, Rm = P (le|lf ) does not cancel out inthe counts of the multi-rate HMM.
To compute theconditional probability P (le|lf ), we assume thatthe length of word e varies according to a Poissondistribution with a mean that is linear with lengthof the word f (Brown et al 1993).3.4 Variational BayesIn order to prevent overfitting, we use the Varia-tional Bayes extension of the EM algorithm (Beal,2003).
This amounts to a small change to theM step of the original EM algorithm.
We in-troduce Dirichlet priors ?
to perform an inexactnormalization by applying the function f(v) =exp(?
(v)) to the expected counts collected in theE step, where ?
is the digamma function (John-son, 2007).
The M-step update for a multinomialparameter ?x|y becomes:?x|y =f(E[c(x|y)] + ?
)f(?j E[c(xj |y)] + ?
)499Multi-rateHMMTAM-HMM WORDWord-MorphMorphonly IBM 4 BaselineBLEU TR to EN 30.82 29.48 29.98 29.13 27.91EN to TR 23.09 22.55  22.54 21.95 21.82AER 0.254 0.255 0.256 0.375 0.370Table 1: AER and BLEU ScoresWe set ?
to 10?20, a very low value, to have theeffect of anti-smoothing, as low values of ?
causethe algorithm to favor words which co-occur fre-quently and to penalize words that co-occur rarely.We used Dirichlet priors on morpheme translationprobabilities.4 Experiments and Results4.1 DataWe trained our model on a Turkish-English paral-lel corpus of approximately 50K sentences whichhave a maximum of 80 morphemes.
Our paralleldata consists mainly of documents in internationalrelations and legal documents from sources suchas the Turkish Ministry of Foreign Affairs, EU,etc.
The Turkish data was first morphologicallyparsed (Oflazer, 1994), then disambiguated (Saket al 2007) to select the contextually salient inter-pretation of words.
In addition, we removed mor-phological features that are not explicitly markedby an overt morpheme.
For English, we use part-of-speech tagged data.
The number of Englishwords is 1,033,726 and the size of the English vo-cabulary is 28,647.
The number of Turkish wordsis 812,374, the size of the Turkish vocabulary is57,249.
The number of Turkish morphemes is1,484,673 and the size of the morpheme vocab-ulary is 16,713.4.2 ExperimentsWe initialized our implementation of the singlelevel ?word-only?
model, which we call ?baseline?in Table 1, with 5 iterations of IBM Model 1, andfurther trained the HMM extension (Vogel et al1996) for 5 iterations.
Similarly, we initializedTAM-HMM and multi-rate HMM with 5 iterationsof TAM 1 as explained in Section 2.2.
Then wetrained TAM-HMM and the multi-rate HMM for 5iterations.
We also ran GIZA++ (IBM Model 1?4)on the data.
We translated 1000 sentence test sets.We used Dirichlet priors in both IBM Model 1and TAM 1 training.
We experimented with usingDirichlet priors on the HMM extensions of bothIBM-HMM and TAM-HMM.
We report the bestresults obtained for each model and translation di-rection.We evaluated the performance of our model intwo different ways.
First, we evaluated againstgold word alignments for 75 Turkish-English sen-tences.
Table 1 shows the AER (Och and Ney,2003) of the word alignments; we report the grow-diag-final (Koehn et al 2003) of the Viterbi align-ments.
Second, we used the Moses toolkit (Koehnet al 2007) to train machine translation systemsfrom the Viterbi alignments of our various models,and evaluated the results with BLEU (Papineni etal., 2002).In order to reduce the effect of nondetermin-ism, we run Moses three times per experiment set-ting, and report the highest BLEU scores obtained.Since the BLEU scores we obtained are close,we did a significance test on the scores (Koehn,2004).
In Table 1, the colors partition the tableinto equivalence classes: If two scores within thesame row have different background colors, thenthe difference between their scores is statisticallysignificant.
The best scores in the leftmost columnwere obtained from multi-rate HMMs with Dirich-let priors only during the TAM 1 training.
On thecontrary, the best scores for TAM-HMM and thebaseline-HMM were obtained with Dirichlet pri-ors both during the TAM 1 and the TAM-HMM500training.
In Table 1, as the scores improve grad-ually towards the left, the background color getsgradually lighter, depicting the statistical signifi-cance of the improvements.
The multi-rate HMMperforms better than the TAM-HMM, which inturn performs better than the word-only models.5 ConclusionWe presented a multi-rate HMM word alignmentmodel, which models the word and the morphemesequence simultaneously.
We have tested ourmodel on the Turkish-English pair and showedthat our model is superior to the two-level wordalignment model which has sequence modelingonly at the word level.Acknowledgments Partially funded by NSFaward IIS-0910611.
Kemal Oflazer acknowledgesthe generous support of the Qatar Foundationthrough Carnegie Mellon University?s Seed Re-search program.
The statements made herein aresolely the responsibility of this author(s), and notnecessarily that of Qatar Foundation.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and DavidYarowsky.
1999.
Statistical machine translation.Technical report, Final Report, JHU Summer Work-shop.Matthew J. Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, Univer-sity College London.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.O?zgu?r C?etin, Mari Ostendorf, and Gary D. Bernard.2007.
Multirate coupled Hidden Markov Modelsand their application to machining tool-wear clas-sification.
IEEE Transactions on Signal Processing,55(6):2885?2896, June.Tagyoung Chung and Daniel Gildea.
2009.
Unsu-pervised tokenization for machine translation.
InEMNLP, pages 718?726.Martin C?mejrek, Jan Cur??
?n, and Jir???
Havelka.
2003.Czech-English dependency-based machine transla-tion.
In EACL, pages 83?90.Matthew Crouse, Robert Nowak, and Richard Bara-niuk.
1998.
Wavelet-based statistical signal pro-cessing using Hidden Markov Models.
IEEE Trans-actions on Signal Processing, 46(4):886?902.Gu?ls?en Eryig?it, Joakim Nivre, and Kemal Oflazer.2008.
Dependency parsing of Turkish.
Computa-tional Linguistics, 34(3):357?389.Elif Eyigo?z, Daniel Gildea, and Kemal Oflazer.
2013.Simultaneous word-morpheme alignment for statis-tical machine translation.
In NAACL.Shai Fine, Yoram Singer, and Naftali Tishby.
1998.The hierarchical Hidden Markov model: Analysisand applications.
Machine Learning, 32(1):41?62,July.Sharon Goldwater and David McClosky.
2005.
Im-proving statistical MT through morphological anal-ysis.
In HLT-EMNLP.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In EMNLP-CoNLL, pages296?305, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL, pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In EMNLP, pages388?395.Young-suk Lee.
2004.
Morphological analysis for sta-tistical machine translation.
In HLT-NAACL, pages57?60.Jia Li, Robert Gray, and Richard Olshen.
2006.Multiresolution image classification by hierarchi-cal modeling with two-dimensional Hidden MarkovModels.
IEEE Transactions on Information Theory,46(5):1826?1841, September.Mark R. Luettgen, William C. Karl, Alan S. Willsky,and Robert R. Tenney.
1993.
Multiscale representa-tions of Markov Random Fields.
IEEE Transactionson Signal Processing, 41(12):3377?3396.Sonja Niessen and Hermann Ney.
2000.
ImprovingSMT quality with morpho-syntactic analysis.
InCOLING, pages 1081?1085.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentModels.
Computational Linguistics, 29(1):19?51.501Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, andGo?khan Tu?r.
2003.
Building a Turkish treebank.
InA.
Abeille?, editor, Treebanks: Building and UsingParsed Corpora, pages 261?277.
Kluwer, London.Kemal Oflazer.
1994.
Two-level description of Turk-ish morphology.
Literary and Linguistic Comput-ing, 9(2).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Conference of the Association forComputational Linguistics (ACL-02), pages 311?318.Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar.
2007.Morphological disambiguation of Turkish text withperceptron algorithm.
In CICLing, pages 107?118.Marios Skounakis, Mark Craven, and Soumya Ray.2003.
Hierarchical Hidden Markov Models for in-formation extraction.
In International Joint Con-ference on Artificial Intelligence, volume 18, pages427?433.Georgios Theocharous, Khashayar Rohanimanesh, andSridhar Maharlevan.
2001.
Learning hierarchi-cal observable Markov decision process Models forrobot navigation.
In ICRA 2001, volume 1, pages511?516.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In COLING, pages 836?841.Alan S. Willsky.
2002.
Multiresolution Markov Mod-els for signal and image processing.
In Proceedingsof the IEEE, pages 1396?1458.Reyyan Yeniterzi and Kemal Oflazer.
2010.
Syntax-to-morphology mapping in factored phrase-based sta-tistical machine translation from English to Turkish.In ACL 2010, pages 454?464.502
