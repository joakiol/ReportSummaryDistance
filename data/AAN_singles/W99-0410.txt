A Web.Based System for Automatic Language Skill Assessment:EVALINGCtdrick FaironLaboratoire d'automatique documentaire etlinguistique (LADL)University of Paris 72, Place Jussieu (CASE 7031)75251 Paris CEDEX 05Francefairon @ ladl.jussieu.frAbst rac tEVALING is a Leonardo da Vinci projectfunded by the European Union, involvingfour European laboratories 1.
The aim of theproject is to build an automatic system toevaluate language skills in people's nativelanguage.
This paper focuses on nativeFrench.
Other partners are working on theirown language and are building specific tests(Italian and German).
EVALING is an 'ItemBanking '2 system: exercise databaseallowing dynamic design of questionnaires.We present a technique based on the use ofNPL tools that assure easy and costlessupdating of these databases.
In addition, weunderline the interest of Local Grammars(Finite State Transducers) for scoringexercises on language.I n t roduct ionEVALING is a Leonardo da Vinci projectfunded by the European Union, involving fourEuropean laboratories.
The aim of the project isto build an automatic system to evaluatelanguage skills in people's mother language.Each partner is working on his own languageand building specific tests (at the present:i Association pour le traitement automatique deslangues (ASSTRIL) for French, Consorzio LexiconRicerche from the University of Salerno, for Italian &Piidagogische Hochschule Karlsruhe and Universit~tMtinehen for German.2 "Item banking covers any procedures that are usedto create, pilot, analyse, store, manage and select estitems so that multiple test forms can be created froma subset of the total 'bank' of items.
"Brown (1997).French, Italian and German).
In this paper wewill present French.The first step consists, for each language,in determining the fields to be tested and thetypes of exercises which can be computerized tocarry out this task.
We have observed this taskdiffers from one language to another.
Forexample, spelling exercises are relevant inFrench, but they are not very interesting forGerman, since people make few mistakes.
ForFrench, we decided to focus on syntax, lexicon,spelling and reading comprehension.
Hence, weoriented the reflection on the tests that could beautomatized and on those that could not.
At thispoint, automatization i the EVALING sytsembears on three phases of the evaluation process:- dynamic setting up of tests (exercises are stored in tablesof an exercise database).
We assume that if a personhas to take the test more than once, this person shouldnot get the same set of questions twice,- automatic grading of tests (including storage of marks in aclient database),- semi-automatic filling of exercise databases (with theassistance of linguistic tools).First, we will discuss some technicalaspects: EVALING is a Web-based programinteracting with large exercise databases and aclient database.
We will explain how this ' itemBanking' system has been implemented on aWeb Server as an ISAPI (Internet ServerApplication Programming for MicrosoftInformation Server 3) and how exercise databaseswere built.
This last point is a key issue, becausethe aim is not so much to gather a fixed amount3Chapman (1997) discuss advantages anddisadvantages of ISAPI programming..62of exercises, but rather to be able to renew themeasily.
We designed a set of linguistic tools tosatisfy this demand.
The set of tools is based onthe software INTEX, developed at the LADL byMax Silberztein 4.
Of course, it is not alwayspossible to automatize the creation of exercises.In certain cases, the work will have to be donemanually.Second, we will present the'Administrator side'.
We call 'administrator' theperson or team who needs to evaluate a largegroup (students, employees, applicants, etc.).
Aninterface nables the administrator to define theparameters of the test (length, level, etc.)
and toperform some statistical analysis on the clientdatabase.Finally, we will deal with the 'client side'.The client registers himself and then has accessto the test session through a Web-Clientsoftware.
All test forms are in HTML form.1.
The Developer side1.1 The Main Program and the DatabasesFrom a technical point of view,EVALING uses a Multithreaded AutomationObject.
This technology allows multiple threadsof execution (Apartment-model threading).Thus, the EVALING kernel is an 'Active XDLL'.
In the DLL, all exercises are designed ona single module which makes it easier todevelop and manage the whole system.
In fact,in order to add or remove xercises, we just haveto add or remove modules and to change aglobal variable in the main module whichindicates to the system the number of exercisesand their order in the session.
The system isdeveloped to run with Microsoft InformationServer and works as a server application.Sets of exercises passed down to a clientare dynamically composed by the system at thetime of the user's request.
All exercises arestored in tables of the Exercise database.
Foreach questionnaire, the system extracts a set of4 INTEX is a parsing system based on wide coveragedictionaries and local grammars (represented byFinite State Automata) which applies to lage corpora,that is 100Mo.
Cf.
Silberztein (1993).exercises from a table.
Each record in the tablecorresponds to one sentence, or one short text.Therefore, several records are retrieved by thesystem to constitute the questionnaire that willbe sent to the client.Corrections of the user's answers areprovided by JavaScfipt programs that use data ofhidden fields in HTML form or by morecomplex programs running on the server whennecessary.
When we make use of JavaScfiptprograms to correct a form, we never displayanswers clearly in the JavaScript source(functions that take ASCII values or binaryvalues evaluate the validity of answers).Answers are therefore not readable in HTMLsource code.
This practice is a second level ofprotection, because the whole test session occursin a Browser Window that does not contain themenu bar that allows the view source action.It is imperative that sets of exercises havethe same level of difficulty from one client toanother.
To signal this stability, a field in eachrecord indicates the level of difficulty.
When theEVALING system composes a questionnaire,records are chosen in such a way that the sum ofthe difficulty marks is always the same (this sumis given by the administrator who can choose theglobal level of difficulty).
Grading occurs at thedevelopment time.
A script, which useslinguistic tools (INTEX programs, localgrammars, dictionaries and linguisticdescriptions) evaluates the sentences.
ForFrench, the difficulty of a sentence depends on:-length in words,-presence/absence of negation, conjunction, relativepronoun,-lexical difficulty: to measure this parameter, we use anelectronic dictionary: the DELAF dictionary for Frenchthat contains more than 900.000 inflected forms.
It isdivided into three levels of difficulty: common andeasy words constitute the first level, technical andunusual words the third level and in between, we have asecond level consisting of words whose understandingis hazy.The dynamic design of a questionnaire requireslarge databases, with a large number ofsentences for each exercise.
It is costly toproduce all of them manually.
To avoid such adifficulty, we designed a set of search tools thatapply to large corpora and retrieve sentences or68short texts that match a given linguistic pattern.These tools constitute a full-fledged parser basedon the use of Finite State Transducers (FST) 5.They use morpho-syntactic informationprovided by wide coverage electronicdictionaries 6, allowing us to retrieve accuratelinguistic information 7.The software INTEX includes a graphic editor(FSGraph) which allows ergonomic designing ofFSTs (which are graphically represented asgraphs).
Linguists or other non computerscientists can easily design and maintain FSTlibraries.
Graphs can be used in three modesa:'simple mode' for retrieval purpose, 'fusionmode' for inserting patterns in the recognizedsequence and 'replace mode' for substituting apattern to the one matched by the graph.
Forexample, the following graph, when used in'simple mode', locates in a text the patternsdisplayed in the boxes (inputs) and, when usedin 'replace mode', it substitutes to the input thetext displayed below the boxes (outputs).they areFig.
1: Sample of graph1.2 Types of exercisesJWe built two types of exercises:First, we have a set of exercises whosecorrection consists of comparing the user5 NLP functionalities of these automata are discussedin Roche, 199716 For a description of dictionaries format and samplesof application, see Courtois (1990).7 An example of graph library is avaible on theLADL Web server: recognition of English compoundverbs: auxiliaries, modals, aspectuals etc.
Cf.
:http:llwww.ladl.jussieu.frltoolsltools.html#1emmas For a simple description, see Silberztein (1998).answers to the solutions registered in thedatabase.
This elementary way of sconng is usedfor exercises where the number of differentcorrect answers is limited (generally to one ortwo possibilities).
User answers are evaluated aseither right or wrong, with no intermediatescores.
On the screen, the exercise can take theappearance of a multiple choice test (if we adddistractors) or of a form to complete (if weremove from the original sentence patterns to betested).Our methodology produces this kind of exerciseeasily and economically.
Since we extract heitems for these exercises from corpora, all itemswe put in the database are well-formed items 9.Thus, the validity check of the users answers canbe a simple stnng comparison.
The examplebelow underline this simplicity (cf.
1.3 Sampleof exercise development).Second, we are working on the conception ofexercises where expected answers are more free,but not completely.
In this case, scoring isperformed by means of specialized grammars.Technically, these grammars are also FiniteState Transducers.
They describe all possiblestructures of a given context.
Because thesegrammars are specialized to the descriptions oflimited contexts, they are called 'localgrammars'.Local grammars introduce a lot of flexibility forrating exercises.
We are testing two ways ofsconng: binary scoring (fight/wrong) and multi-value scales.
In the first case, we use a grammarin 'simple mode' to verify an answer: answermatched by the grammar is valid, otherwise it isconsidered wrong.
In the second case, thegrammar is used in 'replace mode': if a path ofthe grammar matches the answer, the outputcombined to the path is produced.
This outputcan be a score or a formal information processedlater by a sconng program.
It is thus possible toproduce different outputs (scores), and evenoutputs for each path of the graph.
For example,this system allows us to rate an answeraccording to the linguistic register used by the9 If we assume that the corpora is error free, but it isnot.
A human reader has to verify the items of thedatabase.64testee or according to the amount of detailsgiven.3mener en bateau V1Fig.
2: Linguistic registerIn figure 2, words between brackets are lemmas:<Stre> refers to all the forms of the verb ~tre and<se> to the contracted form s' and se (thisinformation is provided by dictionaries).
In thisway, the followings utterances are matched bythe graph: s'est fair avoir, se faisait avoir, etc.The output number refers to the linguisticregister: 1 indicates the best forms, 2 a spokenform and 3 slang terms.1.3 Sample of  exercise developmentWe present a simple case of exercisebuilding.
The purpose of this exercise is to testthe ability to choose the right form of the Frenchword tout ('all') that can be a noun, pronoun oradverb and that can be spelled tout, tous, routeor toutes.1.3.1 Retrieving sentences with a graphWe design a graph and apply it to acorpus to retrieve sentences that match thesequences described by the graph.
Then thegraph is stored in the 'tool folder'.
It will bereused later, when we will need to update theExercises database.OFig.
3: Sample of locating graphThe graph is read from left to right (from theInitial node to the terminal one).
If a sequence ofwords in the text is matched by one of the pathsof the graph, the sequence is saved.
An output isassociated to this graph (represented below twoboxes: "\[" and "\]").
We apply the graph inmerge mode to insert this output in the text.Inserted signs are helpful to import sentences inthe database.Our corpus is preprocessed by INTEX.
Sincepreprocessing segments the text into sentences,it is possible to refer to the 'beginning of asentence' (<^> in our graph) or to the 'end of asentence' (<$> in our graph).
The boxcontaining 'MOT' is a link to another graph thatrepresents a string of words (max.
10 words) andoptional delimiters (apostrophe, comma,hyphen).
This string is optional, that is whythere is an alternative path under the box'MOT'.The following sentences are retrievedfrom a novel of Agatha Christie:Son air tr~s anglais avait \[tout\] pour stduire quelqu'un qui,comme moi, n'avait pas revu sa patrie depuis trois ans.Je voudrais que, de retour chez vous, vous observiez lemonde nouveau de rapr~s-guerre et que vous dtcidiez, en\[route\] libert6 et indtpendance, ceque vous en attendez.J'ai fait \[tout\] mon possible pour ne pas vous dire que jevous aime...Et ils vtcurent \[tous\] ensemble dans une petite maisonbiscornue.1.3.2 Estimation of the sentence difficulty levelThe tool is still under development and astudy is going on to refine the criteria we areusing.
At this time, we consider three levels ofdifficulty: easy, medium and difficult.
The levelassessment depends on the length in number ofwords, presence of a modality (interrogative ornot), lexical complexity and presence or absenceof a negation, conjunction, relative pronoun.
Forexample, the graph 'value.grf' below detectsnegation, conjunction and relative pronouns.
Inthis graph, GN is a link to another graph thatdescribes ummarily noun phrases.
It guaranteesthat the pronoun which comes after GN iseffectively a relative pronoun.
NCR is a link to agraph that repeats the possibility to have anegation, conjunction or relative pronoun.
Infact, a sentence can contain more than onepattern.
<CONJC> and <CONJS> are tags thatcome from dictionaries and local grammars65I I"" iSta'ds~alanaly, dsSettingsI..evd &Ti_~JFig.
5: EVALING system architecturebuilt and the equality of difliculty betweenquestionnaires is tested by a function that refersto a key value.
This key value is accessible tothe Administrator who can decide to raise it or toreduce the global key value that gives the systemthe level of difficulty to compute.
Since allmarks are stored in a database, it is possible toperform some statistical analysis.
Through theAdministrator Interface, one can handle theusual numerical data: who got highest/lowestmarks, where are the weaknesses/strengths ofthe group, etc.
Statistical information is thendisplayed graphically and in tables.
Example ofapplication: In October 99, at the beginning ofthe academic year, EVALING will be used tohelp a language teacher with information aboutthe specificity of this group of students.
Astatistical analysis of the marks obtained byabout two hundred students is already a richinformation.
The administrator can detectcommon weaknesses and adapt his course tothese difficulties.returns the next exercise (e.g.
exercises on pastparticiples) to the client.
At the present time, thetest session is not 'adaptative'l?
: all clientsanswer the same questions (layouts used tocompose sets of questions are the same for allusers).
At the end of the session, the clientreceives a report, in the case the Administratorhas configured the system to do so.
The reportmentions only the marks registered by eachexercise, without any other comments.
Marksare also represented graphically by a JavaApplet.We were careful to avoid interferencesbetween language skills and computer skills.Computerization should not interfere withlanguage skill assessment.
Our prerequirementwas to create a system that could easily be usedby someone not that familiar with computerhandling.
In fact, recent scientific studies10 Principles of Computer-Adaptative language Tests(CAT) are described inTung (1986).66showed that there is little or no evidence ofadverse ffects on the computer-based tests dueto lack of prior computer experience 11.
Toensure that, we decided to rule out technicalmanipulations like ' drag and drop ' and to makeevery effort to build a simple and intuitiveinterface.
A tutorial is available from the startpage on.
Before getting into the test, the user candiscover the different types of questionnairesand try each element of the answering system (infact, any item in HTML Forms: pop-up menu,text field, button, radio button, etc).
During thetest a help button always displayed at the samelocation provides contextual information bymeans of a simple click.4.
ConclusionThe originality of our work lies in the useof powerful inguistic tools that can be adaptedto a large variety of situations and that alloweasy, fast and cheap renewal of the stock ofexercises (with the aim of changing levels ofdifficulty or testing skills in technicallanguages).Our experiments, are intended to showthat local Grammars (FSTs) constitute apowerful tool for sconng exercises which have awide range of valid responses.via Paper-and-Pencil.
In "Education PolicyAnalysis Archives", Vol.
5.
N?3,\[http://olam.ed.asu.edu/epaa/arch.html\].SILBERZTEIN M. (1993) Dictionnaires ~lectroniqueset analyse automatique de textes.
Le systdmeINTEX.
Masson, Paris, France.SILBERZTEIN M. (1998).
Transducteurs pour letraitement automatique des textes.
In "Travaux delinguistique".
N?37.
Duculot, Bruxelles, Belgium,pp.
127-138.TAYLOR C., JAMIESON J., EIGNOR D. and KIRSCH I.
(1998) The Relationship Between ComputerFamiliarity and Performance on Computer-basedTOEFL Test Tasks.
Education Testing Service,Princeton.TUNG, P. (1986) Computerized adaptive testing:Implications for language test developers.
In"Technology and language testing", TESOL,Washington.ReferencesBROWN, J.D.
(1997) Computers in language testing:present research and some and some futuredirections.
In "Language learning & technology",N?I, \[http://polyglot.cal.msu.edu/llt/\].CHAPMAN D. (1997) Wed Development with VisualBasic5.
Que Corporation, MacMillan, Indianapolis.COURTOIS BI.
and SILBERZTEIN M., editors (1990)Dictionnaires ~lectroniques du franfais.
In"Langue fran~aise", N?87, Larousse, Paris, France.ROCHE, E. and SCHABES Y., editors (1997) Finite-State Language Processing, MIT Press,Cambridge, Mass./LondonRUSSELL M., HANEY W. (1997) Testing Writing onComputers: An Experiment Comparing StudentPerformance on Tests Conduced via Computer and11 Taylor (1998) for the TOEFLE study and Russell(1997) for a comparison on multiple-choice tests.67
