Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 7: The Joint Student Response Analysis and 8thRecognizing Textual Entailment ChallengeMyroslava O. DzikovskaSchool of Informatics, University of EdinburghEdinburgh, United Kingdomm.dzikovska@ed.ac.ukRodney D. NielsenUniversity of North TexasDenton, TX, USARodney.Nielsen@UNT.eduChris BrewNuance CommunicationsUSAcbrew@acm.orgClaudia LeacockCTB McGraw-HillUSAclaudia leacock@mheducation.comDanilo GiampiccoloCELCTItalygiampiccolo@celct.itLuisa BentivogliCELCT and FBKItalybentivo@fbk.euPeter ClarkVulcan Inc.USApeterc@vulcan.comIdo DaganBar-Ilan UniversityIsraeldagan@cs.biu.ac.ilHoa Trang DangNISThoa.dang@nist.govAbstractWe present the results of the Joint StudentResponse Analysis and 8th Recognizing Tex-tual Entailment Challenge, aiming to bring to-gether researchers in educational NLP tech-nology and textual entailment.
The task ofgiving feedback on student answers requiressemantic inference and therefore is related torecognizing textual entailment.
Thus, we of-fered to the community a 5-way student re-sponse labeling task, as well as 3-way and 2-way RTE-style tasks on educational data.
Inaddition, a partial entailment task was piloted.We present and compare results from 9 partic-ipating teams, and discuss future directions.1 IntroductionOne of the tasks in educational NLP systems is pro-viding feedback to students in the context of examquestions, homework or intelligent tutoring.
Muchprevious work has been devoted to the automatedscoring of essays (Attali and Burstein, 2006; Sher-mis and Burstein, 2013), error detection and correc-tion (Leacock et al 2010), and classification of textsby grade level (Petersen and Ostendorf, 2009; Shee-han et al 2010; Nelson et al 2012).
In these appli-cations, NLP methods based on shallow features andsupervised learning are often highly effective.
How-ever, for the assessment of responses to short-answerquestions (Leacock and Chodorow, 2003; Pulmanand Sukkarieh, 2005; Nielsen et al 2008a; Mohleret al 2011) and in tutorial dialog systems (Graesseret al 1999; Glass, 2000; Pon-Barry et al 2004; Jor-dan et al 2006; VanLehn et al 2007; Dzikovska etal., 2010) deeper semantic processing is likely to beappropriate.Since the task of making and testing a full edu-cational dialog system is daunting, Dzikovska et al(2012) identified a key subtask and proposed it as anew shared task for the NLP community.
Studentresponse analysis (henceforth SRA) is the task oflabeling student answers with categories that could263Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks.
How did youseparate the salt from the water?REF.
ANS.
The water was evaporated, leaving the salt.STUD.
ANS.
The water dried up and left the salt.Example 2 QUESTION Georgia found one brown mineral and one black mineral.
How will she know which one is harder?REF.
ANS.
The harder mineral will leave a scratch on the less hard mineral.
If the black mineral is harder, thebrown mineral will have a scratch.STUD.
ANS.
The harder will leave a scratch on the other.Figure 1: Example questions and answershelp a full dialog system to generate appropriate andeffective feedback on errors.
System designers typi-cally create a repertoire of questions that the systemcan ask a student, together with reference answers(see Figure 1 for an example).
For each student an-swer, the system needs to decide on the appropriatetutorial feedback, either confirming that the answerwas correct, or providing additional help to indicatehow the answer is flawed and help the student im-prove.
This task requires semantic inference, for ex-ample, to detect when the student answers are ex-plaining the same content but in different words, orwhen they are contradicting the reference answers.Recognizing Textual Entailment (RTE) is a se-ries of highly successful challenges used to evalu-ate tasks related to semantic inference, held annuallysince 2005.
Initial challenges used examples frominformation retrieval, question answering, machinetranslation and information extraction tasks (Daganet al 2006; Giampiccolo et al 2008).
Later chal-lenges started to explore the applicability and im-pact of RTE technology on specific application set-tings such as Summarization and Knowledge BasePopulation (Bentivogli et al 2009; Bentivogli et al2010; Bentivogli et al 2011).
The SRA Task offersa similar opportunity.We therefore organized a joint challenge atSemEval-2013, aiming to bring together the educa-tional NLP and the semantic inference communities.The goal of the challenge is to compare approachesfor student answer assessment and to evaluate themethods typically used in RTE on data from educa-tional applications.We present the corpus used in the task (Section2) and describe the Main task, including educationalNLP and textual entailment perspectives and data setcreation (Section 3).
We discuss evaluation metricsand results in Section 4.
Section 5 describes the Pi-lot task, including data set creation and evaluationresults.
Section 6 presents conclusions and futuredirections.2 Student Response Analysis CorpusWe used the Student Response Analysis corpus(henceforth SRA corpus) (Dzikovska et al 2012)as the basis for our data set creation.
The corpuscontains manually labeled student responses to ex-planation and definition questions typically seen inpractice exercises, tests, or tutorial dialogue.Specifically, given a question, a known correct?reference answer?
and a 1- or 2-sentence ?studentanswer?, each student answer in the corpus is label-led with one of the following judgments:?
?Correct?, if the student answer is a completeand correct paraphrase of the reference answer;?
?Partially correct incomplete?, if it is a par-tially correct answer containing some but notall information from the reference answer;?
?Contradictory?, if the student answer explicitlycontradicts the reference answer;?
?Irrelevant?
if the student answer is talkingabout domain content but not providing thenecessary information;?
?Non domain?
if the student utterance does notinclude domain content, e.g., ?I don?t know?,?what the book says?, ?you are stupid?.The SRA corpus consists of two distinct subsets:BEETLE data, based on transcripts of students in-teracting with BEETLE II tutorial dialogue system(Dzikovska et al 2010), and SCIENTSBANK data,264based on the corpus of student answers to assess-ment questions collected by Nielsen et al(2008b).The BEETLE corpus consists of 56 questions inthe basic electricity and electronics domain requir-ing 1- or 2- sentence answers, and approximately3000 student answers to those questions.
The SCI-ENTSBANK corpus contains approximately 10,000answers to 197 assessment questions in 15 differentscience domains (after filtering, see Section 3.3)Student answers in the BEETLE corpus were man-ually labeled by trained human annotators using ascheme that straightforwardly mapped into SRA an-notations.
The annotations in the SCIENTSBANKcorpus were converted into SRA labels from a sub-stantially more fine-grained scheme by first auto-matically labeling them using a set of question-specific heuristics and then manually revising themaccording to the class definitions (Dzikovska et al2012).
We further filtered and transformed the cor-pus to produce training and test data sets as dis-cussed in the next section.3 Main Task3.1 Educational NLP perspectiveThe 5-way SRA task focuses on associating studentanswers with categorical labels that can be used inproviding tutoring feedback.
Most NLP research onshort answer scoring reports agreement with a nu-meric score (Leacock and Chodorow, 2003; Pulmanand Sukkarieh, 2005; Mohler et al 2011), whichis a potential contrast with our task.
However, themajority of the NLP work makes use of underlyingrepresentations in terms of concepts, so the 5-waytask is still likely to mesh well with the availabletechnology.
Research on tutorial dialog has empha-sized generic methods that use latent semantic anal-ysis or other machine learning methods to determinewhen text strings express similar concepts (Hu et al2003; Jordan et al 2004; VanLehn et al 2007; Mc-Carthy et al 2008).
Most of these methods, likethe NLP methods, (with the notable exception of(Nielsen et al 2008a)), are however strongly depen-dent on domain expertise for the definitions of theconcepts.
In educational applications, there wouldbe great value in a system that could operate moreor less unchanged across a range of domains andquestion-types, requiring only a question text and areference answer supplied by the instructional de-signers.
Thus, the 5-way classification task at Se-mEval was set up to evaluate the feasibility of suchanswer assessment, either by adapting the existingeducational NLP methods to the categorical labelingtask or by employing the RTE approaches.3.2 RTE perspective and 2- and 3-way TasksAccording to the standard definition of Textual En-tailment, given two text fragments called Text (T)and Hypothesis (H), it is said that T entails H if, typ-ically, a human reading T would infer that H is mostlikely true (Dagan et al 2006).In a typical answer assessment scenario, we ex-pect that a correct student answer would entail thereference answer, while an incorrect answer wouldnot.
However, students often skip details that arementioned in the question or may be inferred fromit, while reference answers often repeat or make ex-plicit information that appears in or is implied fromthe question, as in Example 2 in Figure 1.
Hence, amore precise formulation of the task in this contextconsiders the entailing text T as consisting of boththe original question and the student answer, whileH is the reference answer.We carried out a feasibility study to check howwell the entailment judgments in this formulationalign with the annotated response assessment, by an-notating a sample of the data used in the SRA taskwith entailment judgments.
We found that some an-swers labeled as ?correct?
implied inferred or as-sumed pieces of information not present in the text.These reflected the teachers?
assessment of studentunderstanding but would not be considered entailedfrom the traditional RTE perspective.
However, weobserved that in most such cases, a substantial partof the hypothesis was still implied by the text.
More-over, answers assigned labels other than ?correct?were always judged as ?not entailed?.Overall, we concluded that the correlation be-tween assessment judgments of the two types wassufficiently high to consider an RTE approach.
Thechallenge for the textual entailment community wasto address the answer assessment task at varyinglevels of granularity, using textual entailment tech-niques, and explore how well these techniques canhelp in this real-world educational setting.In order to make the setup more similar to pre-265vious RTE tasks, we introduced 3-way and 2-wayversions of the task.
The data for those tasks wereobtained by automatically collapsing the 5-way la-bels.
In the 3-way task, the systems were required toclassify the student answer as either (i) correct; (ii)contradictory; or (iii) incorrect (combining the cat-egories partially correct but incomplete, irrelevantand not in the domain from the 5-way classification).In the two-way task, the systems were required toclassify the student answer as either correct or in-correct (combining the categories contradictory andincorrect from the 3-way classification)3.3 Data Preparation and Training DataIn preparation of the task four of the organizers ex-amined all questions in the SRA corpus, and decidedthat to remove some of the questions to make thedataset more uniform.We observed two main issues.
First, a num-ber of questions relied on external material, e.g.,charts and graphs.
In some cases, the informationin the reference answer was sufficient to make a rea-sonable assessment of student answer correctness,but in other cases the information contained in thequestions was deemed insufficient and the questionswere removed.Second, some questions in the SCIENTSBANKdataset could have multiple possible correct an-swers, e.g., a question asking for any example outof two or more unrelated possibilities.
Such ques-tions were also removed as they do not align wellwith the RTE perspective.Finally, parts of the data were re-checked for re-liability.
In BEETLE data, a second manual annota-tion pass was carried out on a subset of questionsto check for consistency.
In SCIENTSBANK, wemanually re-checked the test data.
The automaticconversion from the original SCIENTSBANK anno-tations into SRA labels was not perfectly accurate(Dzikovska et al 2012).
We did not have the re-sources to check the entire data set.
However, four ofthe organizers jointly hand-checked approximately100 examples to establish consensus, and then oneorganizer hand-checked all of the test data set.3.4 Test DataWe followed the evaluation methodology of Nielsenet al(2008a) for creating the test data.
Since ourgoal is to support systems that generalize acrossproblems and domains (see Section 3.1), we createdthree distinct test sets:1.
Unseen answers (UA): a held-out set to assesssystem performance on the answers to ques-tions contained in the training set (for whichthe system has seen example student answers).It was created by setting aside a subset if ran-domly selected learner answers to each ques-tion included in the training data set.2.
Unseen questions (UQ): a test set to assesssystem performance on responses to previouslyunseen questions but which still fall within theapplication domains represented in the trainingdata.
It was created by holding back all studentanswers to a subset of randomly selected ques-tions in each dataset.3.
Unseen domains (UD): a domain-independenttest set of responses to topics not seen in thetraining data, available only in the SCIENTS-BANK dataset.
It was created by setting asidethe complete set of questions and answers fromthree science modules from the fifteen modulesin the SCIENTSBANK data.The final label distribution for train and test datais shown in Table 1.4 Main Task Results4.1 ParticipantsThe participants were invited to submit up to threeruns in any combination of the tasks.
Nine teamsparticipated in the main task, most choosing to at-tempt all subtasks (5-way, 3-way and 2-way), with1 team entering only the 5-way and 1 team enteringonly the 2-way task.At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,LIMSI) of the 9 systems used some form of syn-tactic processing, in most cases going beyond partsof speech to dependencies or constituency structure.CNGL emphasized this as an important aspect of thesystem.
At least 5 (CoMeT, CU, EHUALM, ETSUKP) of the 9 systems used a system combinationapproach, with several components feeding into afinal decision made by some form of stacked clas-sifier.
The majority of the systems used some kind266label BEETLE SCIENTSBANKtrain (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)Table 1: Label distribution.
Percentages in parentheses.
UA, UQ, UD correspond to individual test sets.of measure of text-to-text similarity, whether the in-spiration was LSA, MT measures such as BLEUor in-house methods.
These methods were em-phasized as especially important by Celi, ETS andSOFTCARDINALITY.
These impressions are basedon short summaries sent to us by the participantsprior to the availability of the full system descrip-tions.
Check the individual system papers for detail.4.2 Evaluation MetricsFor each evaluation data set (test set), we computedthe per-class precision, recall and F1 score.
We alsocomputed three main summary metrics: accuracy,macro-average F1 and weighted average F1.Accuracy is the overall percentage of correctlyclassified examples.Macroaverage is the average value of each met-ric (precision, recall, F1) across classes, withouttaking class size into account.
It is defined as1/Nc?c metric(c), where Nc is the number ofclasses (2, 3, or 5 depending on the task).
Notethat in the 5-way SCIENTSBANK dataset the ?non-domain?
class is severely underrepresented, withonly 23 examples out of 4335 total (see Table 1).Therefore, we calculated macro-averaged P/R/F1over only 4 classes (i.e.
excluding the ?non-domain?class) for SCIENTSBANK 5-way data.Weighted Average (or simply weighted) is theaverage value for each metric weighted by class size,defined as 1/N?c |c| ?
metric(c) where N is thetotal number of test items and |c| is the number ofitems labeled as c in gold-standard data.11This metric is called microaverage in (Dzikovska et al2012).
However, microaverage is used to define a differentmetric in tasks where more than one label can be associatedwith each data item (Tsoumakas et al 2010).
therefore, we useweighted average to match the terminology used by the Wekatoolkit.
The micro-average precision, recall and F1 computedIn general, macro-averaging favors systems thatperform well across all classes regardless of classsize.
Accuracy and weighted average prefer systemsthat perform best on the largest number of examples,favoring higher performance on the most frequentclasses.
In practice, only a small number of the sys-tems were ranked differently by the different met-rics.
We discuss this further in Section 4.7.
Resultsfor all metrics are available online, and this paperfocuses on two metrics for brevity: weighted andmacro-average F1 scores.4.3 ResultsThe evaluation results for all metrics and all partic-ipant runs are provided online.2 The tables in thispaper present the F1 scores for the best system runs.Results are shown separately for each test set (TS),with the simple mean over the five TSs reported inthe final column.We used two baselines: the majority (most fre-quent) class baseline and a lexical overlap baselinedescribed in detail in (Dzikovska et al 2012).
Theperformance of the baselines is presented jointlywith system scores in the results tables.For each participant, we report the single run withthe best average TS performance, identified by thesubscript in the run title, with the exception of ETS.With all other participants, there was almost alwaysone run that performed best for a given metric on allthe TSs.
In the small number of cases where anotherrun performed best on a given TS, we instead reportthat value and indicate its run with a subscript (thesechanges never resulted in meaningful changes in theperformance rankings).
ETS, on the other hand, sub-using the multi-label metric are all equal and mathematicallyequivalent to accuracy.2http://bit.ly/11a7QpP267Dataset: BEETLE SCIENTSBANKRun UA UQ UA UQ UD MeanCELI1 0.423 0.386 0.372 0.389 0.367 0.387CNGL2 0.547 0.469 0.266 0.297 0.294 0.375CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471ETS1 0.552 0.547 0.535 0.487 0.447 0.514ETS2 0.705 0.614 0.625 0.356 0.434 0.547LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418Median 0.552 0.445 0.535 0.397 0.422 0.454Baselines:Lexical 0.483 0.463 0.435 0.402 0.396 0.436Majority 0.229 0.248 0.260 0.239 0.249 0.245Table 2: Five-way task weighted-average F1mitted results for systems that were substantially dif-ferent from one another, with performance varyingfrom being the top rank to nearly the lowest.
Hence,it seemed more appropriate to report two separateruns.3 In the rest of the discussion system is used torefer to a row in the tables as just described.Systems with performance that was not statisti-cally different from the best results for a given TSare all shown in bold (significance was not cal-culated for the TS mean).
Systems with perfor-mance statistically better than the lexical baselineare displayed in italics.
Statistical significance testswere conducted using approximate randomizationtest (Yeh, 2000) with 10,000 iterations; p ?
0.05was considered statistically significant.4.4 Five-way TaskThe results for the five-way task are shown in Tables2 and 3.Comparison to baselines All of the systems per-formed substantially better than the majority classbaseline (?correct?
for both BEETLE and SCIENTS-BANK), on average exceeding it on the TS mean by0.21 on the weighted F1 and 0.24 on the macro-average F1.
Six systems outperformed the lexicalbaseline on the mean TS results for the weightedF1 and five for the macro-average F1.
Nearly allof the top results on a given TS (shown in bold inthe tables) were statistically better than correspond-ing lexical baselines according to significance tests3In a small number of cases, ETS?s third run performedmarginally better, see full results online.Dataset: BEETLE 5way SCIENTSBANK 4wayRun UA UQ UA UQ UD MeanCELI1 0.315 0.300 0.278 0.286 0.269 0.270CNGL2 0.431 0.382 0.252 0.262 0.239 0.274CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382ETS1 0.444 0.461 0.467 0.372 0.334 0.377ETS2 0.619 0.552 0.581 0.274 0.339 0.428LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364Median 0.444 0.370 0.467 0.325 0.337 0.367Baselines:Lexical 0.424 0.414 0.375 0.329 0.311 0.333Majority 0.114 0.118 0.151 0.146 0.148 0.129Table 3: Five-way task macro-average F1(indicated by italics in the tables).Comparing UA and UQ/UD performance TheBEETLE UA (BUA) and SCIENTSBANK UA (SUA)test sets represent questions with example answersin training data, while the UQ and UD test sets repre-sent transfer performance to new questions and newdomains respectively.The top performers on UA test sets were CoMeT1and ETS2, with the addition of UKP-BIU1 on SUA.However, there was not a single best performer onUQ and UD sets.
ETS2 performed statistically bet-ter than all other systems on BEETLE UQ (BUQ),but it performed statistically worse than the lexicalbaseline on SCIENTSBANK UQ (SUQ), resulting inno overlap in the top performing systems on the twoUQ test sets.
SoftCardinality1 performed statisti-cally better than all other systems on SUD and wasamong the three or four top performers on SUQ, butwas not a top performer on the other three TSs, gen-erally not performing statistically better than the lex-ical baseline on the BEETLE TSs.Group performance The two UA TSs had moresystems that performed statistically better than thelexical baseline (generally six systems) than did theUQ TSs where on average only two systems per-formed statistically better than the lexical baseline.Over twice as many systems outperformed the lexi-cal baseline on UD as on the UQ TSs.
The top per-forming systems according to the macro-average F1were nearly identical to the top performing systemsaccording to the weighted F1.2684.5 Three-way TaskThe results for the three-way task are shown in Ta-bles 4 and 5.Comparison to baselines All of the systems per-formed substantially better than the majority base-line (?correct?
for BEETLE and ?incorrect?
for SCI-ENTSBANK), on average exceeding it on the TSmean by 0.28 on the weighted F1 and 0.31 on themacro-average F1.
Five of the eight systems out-performed the lexical baseline on the mean TS re-sults for the weighted F1 and five on the macro-average F1, and all top systems outperformed thelexical baseline with statistical significance.Comparing UA and UQ/UD performance The topperformers on both BUA and SUA were CoMeT1and ETS2.
As for the 5-way task there was no singlebest performer for UQ and UD sets, and no overlapin top performing systems on BUQ and SUQ testsets, with ETS2 being the top performer on BUQ,but statistically worse than the baseline on SUQand SUD.
On the weighted F1, SoftCardinality1performed statistically better than all other systemson SUD and was among the two statistically bestsystems on SUQ, but was not a top performer onBUQ or BUA/SUA TSs.
On the macro-average F1,UKP-BIU1 became one of the statistically best per-formers on all SCIENTSBANK TSs but, along withSoftCardinality1, never performed statistically bet-ter than the lexical baseline on the BEETLE TSs.Group performance With the exception of SUA,only around two systems performed statistically bet-ter than the lexical baseline on each TS.
The top per-forming systems were nearly the same according tothe weighted F1 and the macro-average F1.4.6 Two-way TaskThe results for the two-way task are shown in Ta-ble 6.
Because the labels are roughly balanced inthe two-way task, the results on the weighted andmacro-average F1 are very similar and the top per-forming systems are identical.
Hence this sectionwill focus only on the macro-average F1.As in the previous tasks, all of the systems per-formed substantially better than the majority base-line (?incorrect?
for all sets), on average exceedingit on the TS mean by 0.25 on the weighted F1 and0.30 on the macro-average F1.
However, just four ofDataset: BEETLE SCIENTSBANKRun UA UQ UA UQ UD MeanCELI1 0.519 0.463 0.500 0.555 0.534 0.514CNGL2 0.592 0.471 0.383 0.367 0.360 0.435CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599ETS1 0.619 0.542 0.603 0.631 0.600 0.599ETS2 0.723 0.597 0.709 0.537 0.505 0.614LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521Median 0.604 0.467 0.625 0.554 0.557 0.566Baselines:Lexical 0.578 0.500 0.523 0.520 0.554 0.535Majority 0.229 0.248 0.260 0.239 0.249 0.245Table 4: Three-way task weighted-average F1Dataset: BEETLE SCIENTSBANKRun UA UQ UA UQ UD MeanCELI1 0.494 0.441 0.373 0.412 0.415 0.427CNGL2 0.567 0.450 0.330 0.308 0.311 0.393CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521ETS1 0.592 0.521 0.477 0.459 0.439 0.498ETS2 0.710 0.585 0.643 0.389 0.367 0.539LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473Median 0.580 0.446 0.516 0.411 0.422 0.485Baselines:Lexical 0.552 0.477 0.405 0.390 0.416 0.448Majority 0.191 0.197 0.201 0.194 0.197 0.196Table 5: Three-way task macro-average F1the nine systems in the two-way task outperformedthe lexical baseline on the mean TS results.
In fact,the average performance fell below the lexical base-line.
The differences in the macro-average F1 be-tween the top results on a SCIENTSBANK TS andthe corresponding lexical baselines were all statis-tically significant.
Two of the top results on BUAwere not statistically better than the lexical base-line, and all systems performed below the baselineon BUQ.4.7 DiscussionAll of the systems consistently outperformed themost frequent class baseline.
Beating the lexicaloverlap baseline proved to be more challenging, be-ing achieved by just over half of the results withabout half of those being statistically significant im-provements.
This underscores the fact that there isstill a considerable opportunity to improve student269Dataset: BEETLE SCIENTSBANKRun UA UQ UA UQ UD MeanCELI1 0.640 0.656 0.588 0.619 0.615 0.624CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709CU1 0.778 0.689 0.603 0.638 0.673 0.676ETS1 0.802 0.720 0.705 0.688 0.683 0.720ETS2 0.833 0.702 0.762 0.602 0.543 0.688LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630Median 0.778 0.666 0.705 0.629 0.666 0.676Baselines:Lexical 0.788 0.725 0.617 0.630 0.650 0.682Majority 0.375 0.367 0.362 0.371 0.367 0.368Table 6: Two-way task macro-average F1response assessment systems.The set of top performing systems on theweighted F1 for a given TS were also always in thetop on the macro-average F1, but a small number ofadditional systems joined the top performing set onthe macro-average F1.
Specifically, one, three, andtwo results joined the top set in the five-way, three-way, and two-way tasks, respectively.
In principle,the metrics could differ substantially, because of thetreatment of minority classes, but in practice theyrarely did.
Only one pair of participants swap adja-cent TS mean rankings on the macro-average F1 rel-ative to the weighted F1 on the two-way task.
On thefive-way task, two pairs swap rankings and anotherparticipant moved up two positions in the ranking,ending at the median value.Most (28/34) rank changes were only one positionand most (21/34) were in positions at or below themedian ranking.
In the five-way task, a pair of sys-tems, UKP-BIU1 and ETS1, had a meaningful per-formance rank swap on the macro-average F1 rela-tive to the weighted F1 on the UD test set.
Specifi-cally, UKP-BIU1 moved up four positions from rank6, where it was not statistically better than the lexicalbaseline, to the second best performance.Not surprisingly, performance on UA was sub-stantially higher than on UQ and UD, since the UAis the only set which contains questions with exam-ple answers in training data.
Performance on BUAwas usually better than performance on SUA, mostlikely because BUA contains more similar questionsand answers, focusing on a single science area, Elec-tricity and Magnetism, compared to 12 distinct sci-ence topics in SUA).
In addition, the BEETLE studyparticipants may have used simpler language, sincethey were aware that they were talking to a computersystem instead of writing down answers for humanteachers to assess as in SCIENTSBANK.Performance on BUQ versus SUQ was muchmore varied, presumably since there was no directtraining data for either TS.
For the five-way task, thebest performance on the weighted F1 measure forBUQ is 0.09 below the best result for BUA and theanalogous decrease from SUA to SUQ is 0.13, withan additional 0.02 drop on SUD.
On the two-waytask, the best weighted F1 for BUQ drops 0.11 fromthe best BUA value, but the decrease from SUA toSUQ is just 0.03, with another 0.03 drop to SUD.While the drop in performance is fairly similar fromBUA to BUQ on all tasks and either metric, the de-crease from SUA to SUQ seems to potentially bedependent on the task, ranging from 0.13 on the five-way task to 0.08 on the three-way task and 0.03 onthe two-way task.5 Pilot Task on Partial EntailmentThe SCIENTSBANK corpus was originally devel-oped to assess student answers at a very fine-grainedlevel and contains additional annotations that breakdown the answers into ?facets?, or low-level con-cepts and relationships connecting them (hence-forth, SCIENTSBANK Extra).
This annotation aimsto support educational systems in recognizing whenspecific parts of a reference answer are expressedin the student answer, even if the reference answeris not entailed as a whole (Nielsen et al 2008b).The task of recognizing such partial entailment rela-tionships may also have various uses in applicationssuch as summarization or question answering, but ithas not been explored in previous RTE challenges.Therefore, we proposed a pilot task on partial en-tailment, in which systems are required to recognizewhether the semantic relation between specific partsof the Hypothesis is expressed by the Text, directlyor by implication, even though entailment might notbe recognized for the Hypothesis as a whole, basedon the SCIENTSBANK facet annotation.Each reference answer in SCIENTSBANK data isbroken down into facets, where a facet is a triplet270consisting of two key terms (both single words andmulti-words, e.g.
carbon dioxide, each other, burnsout) and a relation linking them, as shown in Figure2.
The student answers were then annotated withregards to each reference answer facet in order toindicate whether the facet was (i) expressed, eitherexplicitly or by assumption or easy inference; (ii)contradicted; or (iii) left unaddressed.
Consideringthe SCIENTSBANK reference answers as Hypothe-ses, the facets capture their atomic components, andfacet annotations may correspond to the judgmentson the sub-parts of the H which are entailed by T.We carried out a feasibility study to explore thisidea and to verify how well the facet annotationsalign with traditional entailment judgments.
Wefocused on the reference answer facets labeled inthe gold standard annotation as Expressed or Unad-dressed.
The working hypothesis was that Expressedlabels assigned in SCIENTSBANK annotations cor-responded to Entailed judgments in traditional tex-tual entailment annotations, while Unaddressed la-bels corresponded to No-entailment judgments.Similarly to the feasibility study reported in Sec-tion 3.2, we concluded that the correspondence be-tween educational labels and entailment judgmentswas not perfect due to the difference in educationaland textual entailment perspectives.
Nevertheless,the two classes of assessment appeared to be suffi-ciently well correlated so as to offer a good testbedfor partial entailment in a natural setting.5.1 Task DefinitionGiven (i) a text T, made up of a Question and a Stu-dent Answer; (ii) a hypothesis H, i.e.
the ReferenceAnswer for that question and (iii) a facet, i.e.
a pairof key terms in H, the task consists of determiningwhether T expresses, either directly or by implica-tion, the same relationship between the facet wordsas in H. In other words, for each of H?s facets thesystem assign one of the following judgments: Ex-pressed, if the Student Answer expresses the samerelationship between the meaning of the facet termsas in H; Unaddressed, if it does not.Consider the example shown in Figure 2.
Forfacet 3, the system must decide whether the same re-lation between the two terms ?contains?
and ?seeds?in H (the reference answer) is expressed, explicitlyor implicitly, in T (the combination of question andstudent response).
If the student answer is ?The partof a plant you are observing is a fruit if it has seeds.
?,the answer to the question is ?yes?
and the correctjudgment is ?Expressed?.
But if the student says?My rule is has to be sweet.
?, T does not expressthe same semantic relationship between ?contains?and ?seeds?
exhibited in H, thus the correct judgmentis ?Unaddressed?.
Note that even though this is anexercise in textual entailment, student response as-sessment labels were used instead of traditional en-tailment judgments, due to the partial mismatch be-tween the two assessment classes found in the feasi-bility study.5.2 DatasetWe used a subset of the SCIENTSBANK Extra cor-pus (Nielsen et al 2008b) with the same problem-atic questions filtered out as the main task (see Sec-tion 3.3).
We further filtered out all the studentanswer facets which were labeled other than ?Ex-pressed?
or ?Unaddressed?
in the gold standard an-notation; the facets in which the relationship be-tween the two key terms, as classified in the manualannotation, proved to be problematic to define andjudge, namely Topic, Agent, Root, Cause, Quanti-fier, Neg; and inter-propositional facets, i.e.
facetsthat expressed relations between higher-level propo-sitions.
Finally, the facet relations were removedfrom the dataset, leaving the relationship betweenthe two facet terms unspecified so as to allow a morefuzzy approach to the inference problem posed bythe exercise.We used the same training/test split as reported inSection 3.4.
The training set created from the Train-ing SCIENTSBANK Extra corpus contains 13,145reference answer facets, 5,939 of which were la-beled as ?Expressed?
in the student answers and7,206 as ?Unaddressed?.
The Test set was createdfrom the SCIENTSBANK Extra unseen data and isdivided into the same subsets as the main task (Un-seen Answers, Unseen Questions and Unseen Do-mains).
It contains 16,263 facets total, with 5,945instances labeled as ?Expressed?, and 10,318 labeledas ?Unaddressed?.5.3 Evaluation Metrics and BaselinesThe metrics used in the Pilot task were the same as inthe Main task, i.e.
Overall Accuracy, Macroaverage271QUESTION: What is your ?rule?
for deciding if the part of a plant you are observing is a fruit?REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.FACET 1: Relation NMod of Term1 part Term2 plantFACET 2: Relation Theme Term1 contains Term2 partFACET 3: Relation Material Term1 contains Term2 seedsFACET 4: Relation Be Term1 fruit Term2 partFigure 2: Example of facet annotations supporting the partial entailment taskRun UA UQ UD UA UQ UDWeighted Averaged Macro AverageRun1 0.756 0.71 0.76 0.7370 0.686 0.755Run 2 0.782 0.765 0.816 0.753 0.73 0.804Run 3 0.744 0.733 0.77 0.719 0.7050 0.761Baseline 0.54 0.547 0.478 0.402 0.404 0.384Table 7: Weighted-average and macro-average F1 scores(UA: Unseen Answers; UQ: Unseen Questions; UD Un-seen Domains).and Weighted Average Precision, Recall and F1, andcomputed as described in Section 4.2.
We used onlya majority class baseline, which labeled all facetsas ?Unaddressed?.
Its performance is presented inSection 5.4 jointly with the system results.5.4 Participants and resultsOnly one participant, UKP-BIU, participated in thePartial Entailment Pilot task.
The UKP-BIU systemis a hybrid of two semantic relationship approaches,namely (i) computing semantic textual similarityby combining multiple content similarity measures(Ba?r et al 2012), and (ii) recognizing textual en-tailment with BIUTEE (Stern and Dagan, 2011).The two approaches are combined by generating in-dicative features from each one and then applyingstandard supervised machine learning techniques totrain a classifier.
The system used several lexical-semantic resources as part of the BIUTEE entail-ment system, together with SCIENTSBANK depen-dency parses and ESA semantic relatedness indexesfrom Wikipedia.The team submitted the maximum allowed of 3runs.
Table 7 shows Weighted Average and MacroAverage F1 scores respectively, also for the major-ity baseline.
The system outperformed the majoritybaseline on both metrics.
The best performance wasobserved on Run 2, with the highest results on theUnseen Domains test set.6 Conclusions and Future WorkThe Joint Student Response Analysis and 8th Rec-ognizing Textual Entailment challenge has provento be a useful, interdisciplinary task using a realis-tic dataset from the educational domain.
In almostall cases the best systems significantly outperformedthe lexical overlap baseline, sometimes by a largemargin, showing that computational linguistics ap-proaches can contribute to educational tasks.
How-ever, the lexical baseline was not trivial to beat, par-ticularly in the 2-way task.
These results are consis-tent with similar findings in previous RTE exercises.Moreover, there is still significant room for improve-ment in the absolute scores, reflecting the interestingchallenges that both educational data and RTE taskspresent to computational linguistics.The educational setting places new stresses onsemantic inference technology because the educa-tional notion of ?Expressed?
and the RTE notion of?Entailed?
are slightly different.
This raises the ed-ucational question of whether RTE can work in thissetting, and the RTE question of whether this set-ting is meaningful for evaluating RTE system per-formance.
The experimental results suggests that theanswer to both questions is ?yes?, a significant find-ing for both educators and RTE technologists goingforward.The Pilot task, aimed at exploring notions of par-tial entailment, so far not explored in the series ofRTE challenges, has proven to be an interesting,though challenging exercise.
The novelty of thetask, namely performing textual entailment not on apair of full texts, but between a text and a hypothesisconsisting of a pair of words, may have representeda more complex task than expected for some textualentailment engines.
Despite this, the encouragingresults obtained by the team which carried out theexercise has shown that this partial entailment taskis worthy of further investigation.272AcknowledgmentsThe research reported here was supported by the USONR award N000141010085 and by the Institute ofEducation Sciences, U.S. Department of Education,through Grant R305A120808 to the University ofNorth Texas.
The opinions expressed are those ofthe authors and do not represent views of the Insti-tute or the U.S. Department of Education.
The RTE-related activities were partially supported by thePascal-2 Network of Excellence, ICT-216886-NOE.We would also like to acknowledge the contributionof Alessandro Marchetti and Giovanni Moretti fromCELCT to the organization of the challenge.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater v.2.
The Journal of Technology,Learning, and Assessment, 4(3), February.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the 6th International Work-shop on Semantic Evaluation, held in conjunction withthe 1st Joint Conference on Lexical and Computa-tional Semantics, pages 435?440, Montreal, Canada,June.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernando Magnini.
2009.
The fifthPASCAL recognizing textual entailment challenge.
InProceedings of Text Analysis Conference (TAC) 2009.Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa TrangDang, and Danilo Giampiccolo.
2010.
Thesixth PAS-CAL recognizing textual entailment challenge.
InNotebook papers and results, Text Analysis Confer-ence (TAC).Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa TrangDang, and Danilo Giampiccolo.
2011.
The seventhPASCAL recognizing textual entailment challenge.
InNotebook papers and results, Text Analysis Confer-ence (TAC).Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailment chal-lenge.
In J. Quin?onero-Candela, I. Dagan, B. Magnini,and F. d?Alche?
Buc, editors, Machine Learning Chal-lenges, volume 3944 of Lecture Notes in ComputerScience.
Springer.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, Gwendolyn Campbell, Elaine Farrow,and Charles B. Callaway.
2010.
Beetle II: a systemfor tutoring and computational linguistics experimen-tation.
In Proc.
of ACL 2010 System Demonstrations,pages 13?18.Myroslava O. Dzikovska, Rodney D. Nielsen, and ChrisBrew.
2012.
Towards effective tutorial feedback forexplanation questions: A dataset and baselines.
InProc.
of 2012 Conference of NAACL: Human Lan-guage Technologies, pages 200?210.Danilo Giampiccolo, Hoa Trang Dang, BernardoMagnini, Ido Dagan, Elena Cabrio, and Bill Dolan.2008.
The fourth PASCAL recognizing textual entail-ment challenge.
In Proceedings of Text Analysis Con-ference (TAC) 2008, Gaithersburg, MD, November.Michael Glass.
2000.
Processing language input in theCIRCSIM-Tutor intelligent tutoring system.
In Pa-pers from the 2000 AAAI Fall Symposium, Availableas AAAI technical report FS-00-01, pages 74?79.A.
C. Graesser, K. Wiemer-Hastings, P. Wiemer-Hastings, and R. Kreuz.
1999.
Autotutor: A simu-lation of a human tutor.
Cognitive Systems Research,1:35?51.Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-ney, Phanni Penumatsa, and Art Graesser.
2003.
Arevised algorithm for latent semantic analysis.
In Pro-ceedings of the 18th International Joint Conference onArtificial intelligence (IJCAI?03), pages 1489?1491,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Pamela W. Jordan, Maxim Makatchev, and Kurt Van-Lehn.
2004.
Combining competing language under-standing approaches in an intelligent tutoring system.In Proc.
of Intelligent Tutoring Systems Conference,pages 346?357.Pamela Jordan, Maxim Makatchev, Umarani Pap-puswamy, Kurt VanLehn, and Patricia Albacete.2006.
A natural language tutorial dialogue system forphysics.
In Proc.
of 19th Intl.
FLAIRS conference,pages 521?527.Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.
Com-puters and the Humanities, 37(4):389?405.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel R. Tetreault.
2010.
Automated Grammati-cal Error Detection for Language Learners.
SynthesisLectures on Human Language Technologies.
Morgan& Claypool Publishers.Philip M. McCarthy, Vasile Rus, Scott A. Crossley,Arthur C. Graesser, and Danielle S. McNamara.
2008.Assessing forward-, reverse-, and average-entailmentindices on natural language input from the intelligenttutoring system, iSTART.
In Proc.
of 21st Intl.
FLAIRSconference, pages 165?170.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.2011.
Learning to grade short answer questions using273semantic similarity measures and dependency graphalignments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 752?762, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Jessica Nelson, Charles Perfetti, David Liben, andMeredith Liben.
2012.
Measures of text difficulty:Testing their predictive value for grade levels and stu-dent performance.
Technical report, Student Achieve-ment Partners.
http://www.ccsso.org/Documents/2012/Measures%20ofText%20Difficulty_fina%l.2012.pdf.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2008a.
Learning to assess low-level conceptual under-standing.
In Proc.
of 21st Intl.
FLAIRS Conference,pages 427?432.Rodney D. Nielsen, Wayne Ward, James H. Martin, andMartha Palmer.
2008b.
Annotating students?
under-standing of science concepts.
In Proceedings of theSixth International Language Resources and Evalua-tion Conference, (LREC08), Marrakech, Morocco.Sarah Petersen and Mari Ostendorf.
2009.
A machinelearning approach to reading level assessment.
Com-puter, Speech and Language, 23(1):89?106.Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-beth Owen Bratt, and Stanley Peters.
2004.
Advan-tages of spoken language interaction in dialogue-basedintelligent tutoring systems.
In Proc.
of ITS-2004 Con-ference, pages 390?400.Stephen G Pulman and Jana Z Sukkarieh.
2005.
Au-tomatic short answer marking.
In Proceedings of theSecond Workshop on Building Educational Applica-tions Using NLP, pages 9?16, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, andMichael Flor.
2010.
Generating automated text com-plexity classifications that are aligned with targetedtext complexity standards.
Technical Report RR-10-28, Educational Testing Service.Mark D. Shermis and Jill Burstein, editors.
2013.
Hand-book on Automated Essay Evaluation: Current Appli-cations and New Directions.
Routledge.Asher Stern and Ido Dagan.
2011.
A confidencemodel for syntactically-motivated entailment proofs.In Recent Advances in Natural Language Process-ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,September.Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-havas.
2010.
Mining multi-label data.
In OdedMaimon and Lior Rokach, editors, Data Miningand Knowledge Discovery Handbook, pages 667?685.Springer US.Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007.Developing pedagogically effective tutorial dialoguetactics: Experiments and a testbed.
In Proc.
of SLaTEWorkshop on Speech and Language Technology in Ed-ucation, Farmington, PA, October.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the 18th International Conference on Compu-tational linguistics (COLING 2000), pages 947?953,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.274
