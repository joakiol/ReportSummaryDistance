Automatic Detection of Opinion Bearing Words and SentencesSoo-Min Kim and Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292-6695{skim, hovy}@isi.eduAbstractWe describe a sentence-level opiniondetection system.
We first define whatan opinion means in our research andintroduce an effective method for ob-taining opinion-bearing and non-opinion-bearing words.
Then we de-scribe recognizing opinion-bearing sen-tences using these words We test thesystem on 3 different test sets: MPQAdata, an internal corpus, and the TREC-2003 Novelty track data.
We show thatour automatic method for obtainingopinion-bearing words can be used ef-fectively to identify opinion-bearingsentences.1 IntroductionSophisticated language processing in recentyears has made possible increasingly complexchallenges for text analysis.
One such challengeis recognizing, classifying, and understandingopinionated text.
This ability is desirable forvarious tasks, including filtering advertisements,separating the arguments in online debate ordiscussions, and ranking web documents cited asauthorities on contentious topics.The challenge is made very difficult by ageneral inability to define opinion.
Our prelimi-nary reading of a small selection of the availableliterature (Aristotle, 1954; Toulmin et al, 1979;Perelman, 1970; Wallace, 1975), as well as ourown text analysis, indicates that a profitable ap-proach to opinion requires a system to knowand/or identify at least the following elements:the topic (T), the opinion holder (H), the belief(B), and the opinion valence (V).
For the pur-poses of the various interested communities,neutral-valence opinions (such as we believe thesun will rise tomorrow; Susan believes that Johnhas three children) is of less interest; more rele-vant are opinions in which the valence is posi-tive or negative.
Such valence often fallstogether with the actual belief, as in ?going toMars is a waste of money?
; in which the wordwaste signifies both the belief a lot [of money]and the valence bad/undesirable, but need notalways do so: ?Smith[the holder] believes thatabortion should be permissible[the topic] al-though he thinks that is a bad thing[the va-lence]?.As the core first step of our research, wewould like an automated system to identify,given an opinionated text, all instances of the[Holder/Topic/Valence] opinion triads it con-tains1.
Exploratory manual work has shown thisto be a difficult task.
We therefore simplify thetask as follows.
We build a classifier that sim-ply identifies in a text all the sentences express-ing a valence.
Such a two-way classification issimple to set up and evaluate, since enough test-ing data has been created.As primary indicators, we note from newspa-per editorials and online exhortatory text thatcertain modal verbs (should, must) and adjec-tives and adverbs (better, best, unfair, ugly, nice,desirable, nicely, luckily) are strong markers ofopinion.
Section 3 describes our construction ofa series of increasingly large collections of suchmarker words.
Section 4 describes our methodsfor organizing and combining them and usingthem to identify valence-bearing sentences.
Theevaluation is reported in Section 5.2 Past Computational StudiesThere has been a spate of research on identify-ing sentence-level subjectivity in general andopinion in particular.
The Novelty track1 In the remainder of the paper, we will mostly use ?opin-ion?
in place of ?valence?.
We will no longer discuss Be-lief, Holder, or Topic.61(Soboroff and Harman, 2003) of the TREC-2003competition included a task of recognizing opin-ion-bearing sentences (see Section 5.2).Wilson and Wiebe (2003) developed an anno-tation scheme for so-called subjective sentences(opinions and other private states) as part of aU.S.
government-sponsored project (ARDAAQUAINT NRRC) in 2002.
They created a cor-pus, MPQA, containing news articles manuallyannotated.
Several other approaches have beenapplied for learning words and phrases that sig-nal subjectivity.
Turney (2002) and Wiebe(2000) focused on learning adjectives and adjec-tival phrases and Wiebe et al (2001) focused onnouns.
Riloff et al (2003) extracted nouns andRiloff and Wiebe (2003) extracted patterns forsubjective expressions using a bootstrappingprocess.3 Data SourcesWe developed several collections of opinion-bearing and non-opinion-bearing words.
One isaccurate but small; another is large but relativelyinaccurate.
We combined them to obtain a morereliable list.
We obtained an additional list fromColumbia University.3.1 Collection 1: Using WordNetIn pursuit of accuracy, we first manually col-lected a set of opinion-bearing words (34 adjec-tives and 44 verbs).
Early classification trialsshowed that precision was very high (the systemfound only opinion-bearing sentences), but sincethe list was so small, recall was very low (itmissed many).
We therefore used this list asseed words for expansion using WordNet.
Ourassumption was that synonyms and antonyms ofan opinion-bearing word could be opinion-bearing as well, as for example ?nice, virtuous,pleasing, well-behaved, gracious, honorable,righteous?
as synonyms for ?good?, or ?bad, evil,disreputable, unrighteous?
as antonyms.
How-ever, not all synonyms and antonyms could beused: some such words seemed to exhibit bothopinion-bearing and non-opinion-bearing senses,such as ?solid, hot, full, ample?
for ?good?.This indicated the need for a scale of valencestrength.
If we can measure the ?opinion-basedcloseness?
of a synonym or antonym to a knownopinion bearer, then we can determine whetherto include it in the expanded set.To develop such a scale, we first created anon-opinion-bearing word list manually andproduced related words for it using WordNet.To avoid collecting uncommon words, westarted with a basic/common English word listcompiled for foreign students preparing for theTOEFL test.
From this we randomly selected462 adjectives and 502 verbs for human annota-tion.
Human1 and human2 annotated 462 adjec-tives and human3 and human2 annotated 502verbs, labeling each word as either opinion-bearing or non-opinion-bearing.O P N o n O Pw o r dS y n o n y m  se t  o f  O P S y n o n y m  s e t  o f  N o n O PS y n o n y m  s e t  o fa  g iv e n  w o r dO P         :  O p in io n -b e a r in g  w o r d sN o n O P : N o n -O p in io n -b e a r in gw o r d sFigure 1.
Automatic word expansion using WordNetNow, to obtain a measure of opinion/non-opinion strength, we measured the WordNetdistance of a target (synonym or antonym) wordto the two sets of manually selected seed wordsplus their current expansion words (see Figure1).
We assigned the new word to the closercategory.
The following equation represents thisapproach:(1)          ).....,|(maxarg)|(maxarg21 nccsynsynsyncPwcP?where c is a category (opinion-bearing or non-opinion-bearing), w is the target word, and synnis the synonyms or antonyms of the given wordby WordNet.
To compute equation (1), we builta classification model, equation (2):(2)   )|()(maxarg)|()(maxarg)|()(maxarg)|(maxarg1))(,(...3 2 1?====mkwsynsetfcountkcnccckcfPcPcsynsynsynsynPcPcwPcPwcPwhere kf  is the kth feature of category c which isalso a member of the synonym set of the targetword w, and count(fk, synset(w)) means the totalnumber of occurrences of fk in the synonym setof w. The motivation for this model is documentclassification.
(Although we used the synonymset of seed words achieved by WordNet, wecould instead have obtained word features froma corpus.)
After expansion, we obtained 268262opinion-bearing and 2548 non-opinion-bearingadjectives, and 1329 opinion-bearing and 1760non-opinion-bearing verbs, with strength values.By using these words as features we built a Na-ive bayesian classifier and we finally classified32373 words.3.2 Collection 2: WSJ DataExperiments with the above set did not providevery satisfactory results on arbitrary text.
Forone reason, WordNet?s synonym connectionsare simply not extensive enough.
However, ifwe know the relative frequency of a word inopinion-bearing texts compared to non-opinion-bearing text, we can use the statistical informa-tion instead of lexical information.
For this, wecollected a huge amount of data in order to makeup for the limitations of collection 1.Following the insight of Yu and Hatzivassi-loglou (2003), we made the basic and rough as-sumption that words that appear more often innewspaper editorials and letters to the editorthan in non-editorial news articles could be po-tential opinion-bearing words (even though edi-torials contain sentences about factual events aswell).
We used the TREC collection to collectdata, extracting and classifying all Wall StreetJournal documents from it either as Editorial ornonEditorial based on the occurrence of thekeywords ?Letters to the Editor?, ?Letter to theEditor?
or ?Editorial?
present in its headline.This produced in total 7053 editorial documentsand 166025 non-editorial documents.We separated out opinion from non-opinionwords by considering their relative frequency inthe two collections, expressed as a probability,using SRILM, SRI?s language modeling toolkit(http://www.speech.sri.com/projects/srilm/).
Forevery word W occurring in either of the docu-ment sets, we computed the followings:documents Editorialin   wordstotaldocuments Editorialin W #)(Pr =WobEditorialdocs alnonEditoriin   wordstotaldocs alnonEditoriin W #)(Pr =WobalnonEditoriWe used Kneser-Ney smoothing (Kneser andNey, 1995) to handle unknown/rare words.Having obtained the above probabilities we cal-culated the score of W as the following ratio:alProb(W)nonEditorirob(W)EditorialP )( =WScoreScore(W) gives an indication of the bias ofeach word towards editorial or non-editorialtexts.
We computed scores for 86,674,738 wordtokens.
Naturally, words with scores close to 1were untrustworthy markers of valence.
Toeliminate these words we applied a simple filteras follows.
We divided the Editorial and thenon-Editorial collections each into 3 subsets.
Foreach word in each {Editorial, non-Editorial}subset pair we calculated Score(W).
We retainedonly those words for which the scores in allthree subset pairs were all greater than 1 or allless than 1.
In other words, we only kept wordswith a repeated bias towards Editorial or non-Editorial.
This procedure helped eliminate someof the noisy words, resulting in 15568 words.3.3 Collection 3: With Columbia WordlistSimply partitioning WSJ articles into Edito-rial/non-Editorial is a very crude differentiation.In order to compare the effectiveness of our im-plementation of this idea with the implementa-tion by Yu and Hatzivassiloglou of ColumbiaUniversity, we requested their word list, whichthey kindly provided.
Their list contained167020 adjectives, 72352 verbs, 168614 nouns,and 9884 adverbs.
However, this figure is sig-nificantly inflated due to redundant counting ofwords with variations in capitalization and apunctuation.We merged this list and ours to ob-tain collection 4.
Among these words, we onlytook top 2000 opinion bearing words and top2000 non-opinion-bearing words for the finalword list.3.4 Collection 4: Final MergerSo far, we have classified words as either opin-ion-bearing or non-opinion-bearing by two dif-ferent methods.
The first method calculates thedegrees of closeness to manually chosen sets ofopinion-bearing and non-opinion-bearing wordsin WordNet and decides its class and strength.When the word is equally close to both classes,it is hard to decide its subjectivity, and whenWordNet doesn?t contain a word or its syno-nyms, such as the word ?antihomosexsual?, wefail to classify it.The second method, classification of wordsusing WSJ texts, is less reliable than the lexicalmethod.
However, it does for example success-fully handle ?antihomosexual?.
Therefore, wecombined the results of the two methods (collec-tions 1 and 2), since their different characteris-63tics compensate for each other.
Later we alsocombine 4000 words from the Columbia wordlist to our final 43700 word list.
Since all threelists include a strength between 0 and 1, wesimply averaged them, and normalized the va-lence strengths to the range from -1 to +1, withgreater opinion valence closer to 1 (see Table 1).Obviously, words that had a high valencestrength in all three collections had a high over-all positive strength.
When there was a conflictvote among three for a word, it aotomaticallygot weak strength.
Table 2 shows the distribu-tion of words according to their sources: Collec-tion1(C1), Collection2(C2) and Collection3(C3).4 Measuring Sentence Valence4.1   Two ModelsWe are now ready to automatically identifyopinion-bearing sentences.
We defined severalmodels, combining valence scores in differentways, and eventually kept two:Model 1: Total valence score of all words in asentenceModel 2: Presence of a single strong valencewordThe intuition underlying Model 1 is that sen-tences in which opinion-bearing words dominatetend to be opinion-bearing, while Model 2 re-flects the idea that even one strong valence wordis enough.
After experimenting with these mod-els, we decided to use Model 2.How strong is ?strong enough??
To deter-mine the cutoff threshold (?)
on the opinion-bearing valence strength of words, we experi-mented on human annotated data.4.2   Gold Standard AnnotationWe built two sets of human annotated sentencesubjectivity data.
Test set A contains 50 sen-tences about welfare reform, of which 24 sen-tences are opinion-bearing.
Test set B contains124 sentences on two topics (illegal aliens andterm limits), of which 53 sentences are opinion-bearing.
Three humans classified the sentencesas either opinion or non-opinion bearing.
Wecalculated agreement for each pair of humansand for all three together.
Simple pairwiseagreement averaged at 0.73, but the kappa scorewas only 0.49.Table 3 shows the results of experimentingwith different combinations of Model 1, Model2, and several cutoff values.
Recall, precision, F-score, and accuracy are defined in the normalway.
Generally, as the cutoff threshold in-creases, fewer opinion markers are included inthe lists, and precision increases while recalldrops.
The best F-core is obtained on Test set A,Model 2, with ?=0.1 or 0.2 (i.e., being ratherliberal).Table 1.
Examples of opinion-bearing/non-opinion-bearing wordsAdjectives Final score Verbs Final scoreCareless 0.63749 Harm 0.61715wasteful 0.49999 Hate 0.53847Unpleasant 0.15263 Yearn 0.50000Southern -0.2746 Enter -0.4870Vertical -0.4999 Crack -0.4999Scored -0.5874 combine -0.5852Table 2.
Distribution of wordsC1 C2 C3 # words %?
25605 58.60?
8202 18.77?
2291 5.24?
?
5893 13.49?
?
834 1.90?
?
236 0.54?
?
?
639 1.46Total # 32373 15568 4000 43700 100Table 3.
Determining ?
and performance for various models on gold standard data[?
: cutoff parameter, R: recall, P: precision, F: F-score, A: accuracy]Development Test set A Development Test set BModel1 Model2 Model1 Model2?
R P F A R P F A R P F A R P F A0.1 0.54 0.61 0.57 0.62 0.91 0.55 0.69 0.6 0.43 0.36 0.39 0.43 0.94 0.45 0.61 0.480.2 0.54 0.61 0.57 0.62 0.91 0.56 0.69 0.62 0.39 0.35 0.37 0.42 0.86 0.45 0.59 0.490.3 0.58 0.6 0.59 0.62 0.83 0.55 0.66 0.6 0.43 0.39 0.41 0.47 0.77 0.45 0.57 0.050.4 0.33 0.8 0.47 0.64 0.33 0.8 0.47 0.64 0.45 0.36 0.4 0.42 0.45 0.36 0.4 0.420.5 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.32 0.3 0.31 0.4 0.32 0.3 0.31 0.40.6 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.2 0.22 0.21 0.35 0.2 0.22 0.21 0.3564Table 4.
Test on MPQA dataAccuracy Precision RecallC Ours All C Ours All C Ours Allt=1 0.55 0.63 0.59 0.55 0.61 0.58 0.97 0.85 0.91t=2 0.57 0.65 0.63 0.56 0.70 0.63 0.92 0.62 0.75t=3 0.58 0.61 0.62 0.58 0.77 0.69 0.84 0.40 0.56t=4 0.59 0.55 0.60 0.60 0.83 0.74 0.74 0.22 0.39t=5 0.59 0.51 0.55 0.62 0.87 0.78 0.63 0.12 0.25t=6 0.58 0.48 0.52 0.64 0.91 0.82 0.53 0.06 0.15random 0.50 0.54 0.50C: Columbia word list(top 10682 words),  Ours : C1+C2 (top10682 words), All: C+Ours (top 19947 words)5 ResultsWe tested our system on three different data sets.First, we ran the system on MPQA data pro-vided by ARDA.
Second, we participated in thenovelty track of TREC 2003.
Third, we ran it onour own test data described in Section 4.2.5.1  MPQA TestThe MPQA corpus contains news articles manu-ally annotated using an annotation scheme forsubjectivity (opinions and other private statesthat cannot be directly observed or verified.
(Quirk et al, 1985), such as beliefs, emotions,sentiment, speculation, etc.).
This corpus wascollected and annotated as part of the summer2002 NRRC Workshop on Multi-PerspectiveQuestion Answering (MPQA) (Wiebe et al,2003) sponsored by ARDA.
It contains 535documents and 10,657 sentences.The annotation scheme contains two maincomponents: a type of explicit private state andspeech event, and a type of expressive subjec-tive element.
Several detailed attributes andstrengths are annotated as well.
More details areprovided in (Riloff et al, 2003).Subjective sentences are defined according totheir attributes and strength.
In order to applyour system at the sentence level, we followedtheir definition of subjective sentences.
The an-notation GATE_on is used to mark speechevents and direct expressions of private states.The onlyfactive attribute is used to indicatewhether the source of the private state or speechevent is indeed expressing an emotion, opinionor other private state.
GATE_expressive-subjectivity annotation marks words and phrasesthat indirectly express a private state.In our experiments, our system performedrelatively well in both precision and recall.
Weinterpret our opinion markers as coinciding with(enough of) the ?subjective?
words of MPQA.In order to see the relationship between thenumber of opinion-bearing words in a sentenceand its classification by MPQA as subjective,we varied the threshold number of opinion-bearing words required for subjectivity.
Table 4shows accuracy, precision, and recall accordingto the list used and the threshold value t.The random row shows the average of tenruns of randomly assigning sentences as eithersubjective or objective.
As we can see from Ta-ble 4, our word list which is the combination ofthe Collection1 and Collection2, achievedhigher accuracy and precision than the Colum-bia list.
However, the Columbia list achievedhigher recall than ours.
For a fair comparison,we took top 10682 opinion-bearing words fromeach side and ran the same sentence classifiersystem.25.2 TREC dataOpinion sentence recognition was a part of thenovelty track of TREC 2003 (Soboroff and Har-man, 2003).
The task was as follows.
Given aTREC topic and an ordered list of 25 documentsrelevant to the topic, find all the opinion-bearingsentences.
No definition of opinion was pro-vided by TREC; their assessor?s intuitions wereconsidered final.
In 2003, there were 22 opiniontopics containing 21115 sentences in total.
Theopinion topics generally related to the pros andcons of some controversial subject, such as,?partial birth abortion ban?, ?Microsoft antitrustcharges?, ?Cuban child refugee Elian Gonzalez?,?marijuana legalization?, ?Clinton relationshipwith Lewinsky?, ?death penalty?, ?adoptionsame-sex partners, and etc.
For the opinion top-ics, a sentence is relevant if it contains an opin-ion about that subject, as decided by the assessor.There was no categorizing of polarity of opinionor ranking of sentences by likelihood that theycontain an opinion.
F-score was used to measuresystem performance.We submitted 5 separate runs, using differentmodels.
Our best model among the five wasModel 2.
It performed the second best of the 55runs in the task, submitted by 14 participating2 In comparison, the HP-Subj (height precision subjectivityclassifier) (Riloff, 2003) produced recall 40.1 and precision90.2 on test data using text patterns, and recall 32.9 andprecision 91.3 without patterns.
These figures are compa-rable with ours.65institutions.
(Interestingly, and perhaps disturb-ingly, RUN3, which simply returned every sen-tence as opinion-bearing, fared extremely well,coming in 11th.
This model now provides abaseline for future research.)
After the TRECevaluation data was made available, we testedModel 1 and Model 2 further.
Table 5 shows theperformance of each model with the two best-performing cutoff values.Table 5.
System performance with different models andcutoff values on TREC 2003 dataModel System Parameter ?
F-score0.2 0.398 Model10.3 0.4250.2 0.514Model20.3 0.4645.3 Test with Our DataSection 4.2 described our manual data annota-tion by 3 humans.
Here we used the work of onehuman as development test data for parametertuning.
The other set with 62 sentences on thetopic of gun control we used as blind test data.Although the TREC and MPQA data sets are lar-ger and provide comparisons with others?
work,and despite the low kappa agreement values, wedecided to obtain cutoff values on this data too.The graphs in Figure 3 show the performance ofModels 1 and 2 with different values.6 Conclusions and Future WorkIn this paper, we described an efficient auto-matic algorithm to produce opinion-bearingwords by combining two methods.
The firstmethod used only a small set of human-annotated data.
We showed that one can findproductive synonyms and antonyms of an opin-ion-bearing word through automatic expansionin WordNet and use them as feature sets of aclassifier.
To determine a word?s closeness toopinion-bearing or non-opinion-bearing synoymset, we also used all synonyms of a given wordas well as the word itself.
An additional method,harvesting words from WSJ, can compensate thefirst method.Using the resulting list, we experimented withdifferent cutoff thresholds in the opinion/non-opinion sentence classification on 3 differenttest data sets.
Especially on the TREC 2003Novelty Track, the system performed well.
Weplan in future work to pursue the automatedanalysis of exhortatory text in order to producedetailed argument graphs reflecting their au-thors?
argumentation.ReferencesAristotle.
The Rhetorics and Poetics (trans.
W. Rhys Rob-erts, Modern Library, 1954).Fellbaum, C., D. Gross, and K. Miller.
1993.
Adjectives  inWordNet.
http://www.cosgi.princeton.edu/~wn.Kneser, R. and H. Ney.
1995.
Improved Backing-off for n-gram Language Modeling.
Proceedings of ICASSP, vol.1, 181?184.Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K.Miller.
1993.
Introduction to WordNet: An On-LineLexical Database.
http://www.cosgi.princeton.
edu/~wn.Pang, B. L. Lee, and S. Vaithyanathan, 2002.
Thumbs up?Sentiment classification using Machine Learning Tech-niques.
Proceedings of the EMNLP conference.Perelman, C. 1970.
The New Rhetoric: A Theory of Practi-cal Reasoning.
In The Great Ideas Today.
Chicago: En-cyclopedia Britannica.Riloff , E. and J. Wiebe.
2003.
Learning Extraction Pat-terns for Opinion-bearing Expressions.
Proceedings ofthe EMNLP-03.Riloff, E., J. Wiebe, and T. Wilson 2003.
Learning Subjec-tive Nouns Using Extraction Pattern Bootstrapping.Proceedings of CoNLL-03Soboroff, I. and D. Harman.
2003.
Overview of the TREC2003 Novelty Track.
Proceedings of TREC-2003.Toulmin, S.E., R. Rieke, and A. Janik.
1979.
An Introduc-tion to Reasoning.
Macmillan, New YorkTurney, P. 2002.
Thumbs Up or Thumbs Down?
SemanticOrientation Applied to Unsupervised Classification ofReviews.
Proceedings of the 40th Annual Meeting of theACL, Philadelphia, 417?424.Wallace, K. 1975.
Topoi and the Problem of Invention.
InW.
Ross Winterowd (ed), Contemporary Rhetoric.
Har-court Brace Jovanovich.Wilson, T. and J. Wiebe.
2003.
Annotating Opinions in theWorld Press.
Proceedings of the ACL SIGDIAL-03.Yu, H. and V. Hatzivassiloglou.
2003.
Towards AnsweringOpinion Questions: Separating Facts from Opinions andIdentifying the Polarity of Opinion Sentences.
Proceed-ings of EMNLP-2003.Model100.20.40.60.810.1 0.2 0.3 0.4 0.5 0.6cutoff parameterRecallPrecisionfscoreAccuracyModel200.20.40.60.810.1 0.2 0.3 0.4 0.5 0.6cutoff parameterRecallPrecisionfscoreAccuracyFigure 3.
Test on human-annotated sentences66
