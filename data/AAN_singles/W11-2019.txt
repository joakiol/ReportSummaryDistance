Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 162?172,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsExploring User Satisfaction in a Tutorial Dialogue SystemMyroslava O. Dzikovska, Johanna D. MooreSchool of Informatics, University of EdinburghEdinburgh, United Kingdomm.dzikovska,j.moore@ed.ac.ukNatalie Steinhauser, Gwendolyn CampbellNaval Air Warfare Center Training Systems DivisionOrlando, Florida, USAgwendolyn.campbell,natalie.steinhauser@navy.milAbstractUser satisfaction is a common evaluation met-ric in task-oriented dialogue systems, whereastutorial dialogue systems are often evaluatedin terms of student learning gain.
However,user satisfaction is also important for suchsystems, since it may predict technology ac-ceptance.
We present a detailed satisfactionquestionnaire used in evaluating the BEETLEII system (REVU-NL), and explore the un-derlying components of user satisfaction us-ing factor analysis.
We demonstrate interest-ing patterns of interaction between interpreta-tion quality, satisfaction and the dialogue pol-icy, highlighting the importance of more fine-grained evaluation of user satisfaction.1 IntroductionUser satisfaction is one of the primary evaluationmeasures for task-oriented spoken dialogue systems(SDS): the goal of an SDS is to accomplish the task,and to keep the user satisfied, so that they will wantto continue using the system.
Typically, the PAR-ADISE methodology (Walker et al, 2000) is used toestablish a performance function which relates usersatisfaction measured through questionnaires to in-teraction parameters that can be derived from sys-tem logs.
This function can then be used to betterunderstand which properties of the interaction havethe most impact on the users, and to compare differ-ent system versions.In contrast, tutorial dialogue systems are typicallyevaluated in terms of student learning gain, by com-paring student scores on standardized tests beforeand after interacting with the system.
This is clearlyan important evaluation metric, since it directly as-sesses the benefit students obtain from using the sys-tem.
However, it is also important to evaluate usersatisfaction, since it can influence students?
willing-ness to use computer tutors in a long run.
Thus,recent studies have looked at factors that could in-fluence user satisfaction in tutorial dialogue, such asdifferent tutoring policies (Forbes-Riley and Litman,2011), quality of speech output (Forbes-Riley et al,2006), and students?
prior attitudes towards technol-ogy (Jackson et al, 2009).Assessing user satisfaction, however, is not astraightforward task.
As we discuss in more detail inSection 2, user satisfaction is known to be a complexmulti-dimensional construct, composed of largelyindependent factors such as perceived ease of useand perceived usefulness.
Therefore, questionnairesused for assessing satisfaction need to be validatedthrough user studies, and different satisfaction di-mensions should be assessed independently.
There-fore, SDS researchers are now starting to use tech-niques from psychometrics for this purpose (Honeand Graham, 2000; Mo?ller et al, 2007).
However,user satisfaction studies tutorial dialogue currentlyrely on simple questionnaires adapted from eithertask-oriented SDS or non-dialogue intelligent tutor-ing systems (Michael et al, 2003; Forbes-Riley etal., 2006; Forbes-Riley and Litman, 2011; Jacksonet al, 2009), and these questionnaires have not beenvalidated for tutorial dialogue systems.In this paper, we make the first step towards de-veloping a better user satisfaction questionnaire fortutorial dialogue systems.
We present a user satis-162faction evaluation of the BEETLE II tutorial dialoguesystem.
Starting with a detailed user satisfactionquestionnaire, we employ exploratory factor analy-sis to discover a set of dimensions for the students?satisfaction with a dialogue-based tutor.
We thenuse the factors we derived to compare user satisfac-tion between two versions of our computer tutor thatuse different policies for generating the tutor?s feed-back.
We investigate the relationships between thesubjective satisfaction dimensions and the objectivelearning gain metric for the two systems.
Finally, wecarry out a more detailed investigation of our priorresults on the relationship between user satisfactionand interpretation quality in tutorial dialogue.
Ouranalysis also provides insights for further improvingthe questionnaire we developed and gives an exam-ple of how user satisfaction metrics developed fortask-oriented dialogue can be adapted to differentdialogue applications.
It also opens new questionsabout how different properties of the interaction af-fect user satisfaction in tutorial dialogue, which canbe investigated in future work.The rest of the paper is organized as follows.
Wediscuss the approaches for assessing user satisfac-tion with SDS in Section 2.
In Section 3 we describethe BEETLE II tutorial dialogue system used in thisevaluation.
We describe our questionnaire design inSection 4, and describe its use in BEETLE II evalu-ation in Section 5.
We conclude by discussing theimplication of our analysis for tutorial dialogue sys-tem evaluation in Section 6.2 BackgroundA typical approach to assessing user satisfaction indialogue systems is collecting user survey data byasking users to rate their agreement with statementssuch as ?the system was easy to use?.
In the simplestcase of early PARADISE studies, the questionnairescontained 5 items assessing different dimensions ofsatisfaction, which were then summed to produce atotal satisfaction score.However, using simple questionnaires has draw-backs now recognized by the SDS community.
First,if individual questions are expected to assess differ-ent dimensions of user satisfaction, they need to bevalidated first, or else they may be ambiguous andmean different things to different users.
Second,summing or averaging over questions measuring dif-ferent satisfaction components may not be the bestapproach, since it may conflate unrelated judgments(Hone and Graham, 2000).To address this problem, SDS researchers havestarted using more complex questionnaires, whereeach underlying dimension of user satisfaction is as-sessed through multiple questions.
Factor analysis isthen used to determine which questions are relatedto one another (and therefore are likely to be assess-ing the same underlying satisfaction dimension), andto discard possibly ambiguous questions.
Then, thePARADISE methodology can be used to relate dif-ferent interaction parameters to individual compo-nents of user satisfaction.Several such studies have been conducted recently(Hone and Graham, 2000; Larsen, 2003; Mo?ller etal., 2007; Wolters et al, 2009), covering command-and-control and information-seeking dialogue.
Thequestionnaires in those studies contained 25 to 50items, and factor analyses typically resulted in 6- or7-factor solutions, with dimensions such as accept-ability, affect, system response accuracy and cogni-tive demand.
The underlying factors found by thoseanalyses tend to match up well, but not to over-lap perfectly.
In comparison, all user satisfactionquestionnaires for tutorial dialogue systems that weare aware of contain 10-15 items which are eithersummed up for PARADISE studies, or comparedindividually to track system improvement (Michaelet al, 2003; Forbes-Riley et al, 2006; Forbes-Rileyand Litman, 2011; Jackson et al, 2009).In this paper, we apply the more sophisticatedSDS evaluation methodology to the BEETLE II tu-torial dialogue system.
We devise a more sophis-ticated user satisfaction questionnaire using SDSquestionnaires for guidance and then apply factoranalysis to investigate the underlying dimensions.We compare our results to analyses from two pre-vious studies: SASSI (Hone and Graham, 2000),which is a validated questionnaire intended for usewith a variety of task-oriented dialogue systems,and a more recent ?modified SASSI?
questionnairewhich is a version of SASSI adapted for use with theINSPIRE home control system (Mo?ller et al, 2007).Henceforth we will refer to this as INSPIRE.1633 BEETLE II Tutorial Dialogue SystemThe goal of BEETLE II (Dzikovska et al, 2010c)is to teach students conceptual knowledge in the do-main of basic electricity and electronics.
The systemis built on the premise that encouraging students toexplain their answers and to talk about the domainwill lead to improved learning, a finding consistentwith analyses of human-human tutoring in severaldomains (Purandare and Litman, 2008; Litman etal., 2009).
BEETLE II has been engineered to testthis hypothesis by eliciting contentful talk throughexplanation questions.The BEETLE II learning material consists of twoself-contained lessons suitable for college-level stu-dents with no prior knowledge of basic electricityand electronics.
The lessons take 4 to 5 hours tocomplete, and consist of reading materials and inter-active exercises.
During the exercises, the studentsinteract with a circuit simulator, building electricalcircuits containing bulbs, batteries and switches, andusing a multimeter to measure voltage.
Then thetutor asks students to explain circuit behavior, forexample, ?Why was bulb A on when switch Y wasopen and switch Z was closed??
In addition, at dif-ferent points in the lesson the tutor asks ?summary?questions, asking students to define concepts suchas voltage, and verbalize general patterns such as?What are the conditions that are required for a bulbto light??.
At present, students use a typed chat in-terface to communicate with the system.1We built and evaluated two versions of the sys-tem (Dzikovska et al, 2010a).
The baseline non-adaptive tutor (BASE) requires students to produceanswers, but does not provide any remediation andimmediately states the correct answer.
The fullyadaptive version (FULL) engages in dialogue withthe student, and tailors its feedback to the student?sanswer by confirming its correct parts and givinghints in order to help students fix missing or incor-rect parts.
The FULL system generates feedback au-tomatically based on a detailed analysis of the stu-dent?s input, and is capable of giving hints at differ-ent levels of specificity depending on the student?sprevious performance.1A speech interface is being developed, but typed communi-cation is common in online and distance learning, and thereforeis an acceptable choice for tutorial dialogue as well.These two system versions were designed to eval-uate the impact of adaptive feedback (within the lim-itations of current language interpretation technol-ogy) on student learning and satisfaction.
Our initialdata analysis focused on the differences in studentlanguage depending on the condition (Dzikovska etal., 2010a), and on the impact of different types ofinterpretation errors on learning gain and user sat-isfaction (Dzikovska et al, 2010b).
However, theseinitial results were based on an aggregate satisfac-tion score obtained by averaging over scores for allquestions in our user satisfaction questionnaire.
Inthis analysis, we take a more detailed look at the dif-ferent factors that contribute to students satisfactionwith the system, and their relationship with learninggain and interpretation quality.4 Data Collection4.1 Questionnaire DesignTo support user satisfaction evaluation we developeda satisfaction questionnaire, REVU-IT (Report onthe Enjoyment, Value, and Usability of an Intelli-gent Tutor).
It consists of 63 items which cover allaspects of interaction with the tutoring system: theclarity and usefulness of the reading material; thegraphical user interface to the circuit simulator; in-teraction with the dialogue tutor; and the overall im-pression of the BEETLE II system as a whole.
Thereading material, graphical user interface and inter-action with the tutor sections are complementary,because they cover separate parts of the BEETLE IIinterface.
We expect that all of these three compo-nents contribute to the overall impression score.
Forpurposes of this paper, we will focus on the part ofthe questionnaire that relates to the natural languageinteraction with the tutor (REVU-NL), and its re-lationship to the overall impression score (REVU-OVERALL).The REVU-IT questionnaire was developed byexperienced cognitive psychologists (two of the au-thors of this paper).
The REVU-NL section con-sists of 35 items shown in Appendix A.
Its designwas guided by questionnaires used in previous re-search, including INSPIRE and a questionnaire usedto evaluate the ITSPOKE tutorial dialogue system(Forbes-Riley et al, 2006).
REVU-NL contains anumber of items from these, but omits items that are164not relevant to the BEETLE II domain (e.g, ?Domes-tic devices can be operated efficiently with the sys-tem?
or ?The tutor responded effectively after I wasuncertain?
), and adds extra questions related to tu-toring (e.g., ?Our dialogues quickly led to me hav-ing a deeper understanding of the material?
), basedon the authors?
previous experience in human factorsresearch.
We also slightly rephrased all questions torefer to ?the tutor?
rather than ?the system?.The REVU-OVERALL section of REVU-ITconsists of 5 items assessing the student?s satis-faction with their learning as a whole.
The ques-tions are: ?Overall, I am satisfied with my experi-ence learning about electricity from this system.?
;?Working in this learning environment was just likeworking one-on-one with a human tutor?
; ?I wouldhave preferred to learn about electricity in a differentway.?
; ?I would use this system again in the future tocontinue to learn about electricity.?
; ?I would like tobe able to use a system like this to learn about othertopics in the future.?.
We use the averaged score overthese 5 items to represent the student?s overall satis-faction with the learning environment, referring to itas ?overall satisfaction?.Adding new questions to the REVU-NL ques-tionnaire on top of already existing questions is theinitial step in addressing the issues discussed in Sec-tion 2: validating the individual questions and dis-covering the underlying dimensions of user satis-faction.
Having a large number of questions ask-ing about the same aspects of the interaction willallow us to group related questions together into di-mensions (?factors?
), and also to discover ambigu-ous questions that will need to be improved in futurestudies.
The detailed discussion of the technique andissues involved is presented in Hone and Graham(2000).4.2 ParticipantsWe used REVU-IT as part of a controlled experi-ment comparing the BASE and FULL versions of thesystem.
We recruited 87 participants from a uni-versity in the Southern US, paid for participation.Participants had little knowledge of the domain.Each participant signed consent forms and com-pleted a pre-test, then worked through both lessons(with breaks), and then completed a post-test and aREVU-IT questionnaire.
Each session lasted 3.5hours on average.Out of 87 participants that completed the study, 13had an inordinate amount of trouble with interface:they typed utterances that could not be interpretedby the tutor (defined as having more than 3 standarddeviations in interpretation errors compared to therest), did not follow tutor?s instructions or experi-enced system crashes.
In addition, two participantswere learning gain outliers (again, more than 3 stan-dard deviations from average).
These participantswere removed from the analysis.
The questionnairesfrom the remaining 72 participants are used in ourdata analysis.5 Analysis5.1 Underlying satisfaction dimensionsEach item in the REVU-NL questionnaire used a5-point Likert scale, from ?completely disagree?
(1)to ?fully agree?
(5).
Most of the items were phrasedso that the agreement with the statement meant apositive evaluation of the system.
For a few items,however, the polarity was reversed (e.g., ?The tutorwas not helpful?).
Those items were reverse-coded,with 1 meaning ?fully agree?
and 5 ?completely dis-agree?, to ensure that a lower score on all questionscorresponds to a negative assessment.Following Hone and Graham (2000), we usedexploratory factor analysis to group questionnaireitems into clusters representing different dimen-sions.
One of the standard approaches in determin-ing how many factors (?question clusters?)
to useis the scree test which checks the number of eigen-values in the question covariance matrix which aregreater than 1.
These typically correspond to prin-cipal components which reflect the underlying ques-tionnaire structure.
The scree test showed 7 eigen-values greater than 1, resulting in the 7-factor solu-tion presented in Table 1.The loadings in the table are the correlation coef-ficients between the individual question scores andthe variables representing the factors.
Most of thecorrelations are quite high, indicating that the ques-tions are strongly correlated both among themselvesand the underlying factor.
However, the last two fac-tors contain only non-loading questions according tothe criteria in (Hone and Graham, 2000), i.e., ques-tions for which the correlations are too weak to be165# Question Load-ing1 t29: Knew what to say at each point 0.821 t22: Easy to interact with the tutor.
0.791 t9: Not sure what was expected.
0.731 t18: Knew what to say to the tutor.
0.701 t14: The tutor was too inflexible.
0.691 t19: Able to recover easily from errors 0.691 t24: Easy to learn to speak to tutor.
0.691 t16: Tutor didn?t do what I wanted.
0.651 t3: Tutor understood me well.
0.651 t15: Working as easy as with a human.
0.641 t13: Had to concentrate when talking.
0.622 t31 Tutor was an efficient way to learn.
0.792 t32: Easy to learn from the tutor.
0.782 t34: Tutor was worthwhile 0.723 t28: Tutor was irritating.
0.763 t10: Tutor was fun.
0.743 t7: Enjoyed talking with tutor.
0.723 t30: Dialogues were boring.
0.664 t2: Tutor took too long to respond 0.844 t33: Tutor responded quickly 0.845 t26: Didn?t always understand tutor 0.896 (t3: The tutor understood me well) 0.47 (t25: Comfortable talking with tutor) 0.59Table 1: Factors derived from the REVU-NL question-naire, with question loadings for the factor to which eachquestion was assigned.
Question text shortened due tospace limitations, full text presented in the appendix.Non-loading questions in parentheses.reliable.
In addition, factors 4 and 5 had fewer than3 questions.
Since the number of subjects in our dataset is small, such factors may not be reliable.
There-fore, we focus our remaining analysis on the top 3factors from the questionnaire, each of which con-tains 3 or more questions.Twelve questions in REVU-NL were ?cross-loading?
according to criteria in Hone and Graham(2000), that is, their two top loadings differed byless than 0.2.
This indicates questions that are likelyto be ambiguous, since they are strongly correlatedwith two (theoretically independent) variables.
Suchquestions should be refined and re-designed in futuresurveys.
These were questions t1, t4, t6, t11, t12,t17, t20, t21, t23, t25, t27, t35 from the appendix.We removed them from our solution, and discuss theimplications for survey design in Section 6.The first component in our analysis lines up wellwith the Transparency and Cognitive load factorsfrom INSPIRE, and Response accuracy, Cognitivedemand and Habitability from SASSI, though it wasnot split into individual factors as in those analyses.We will refer to this factor as Transparency.
Thesecond component contains questions specific to tu-toring.
However, it is similar to the Acceptabilitydimension from INSPIRE (the original SASSI ques-tionnaire did not include similar questions), whichasked users to rate statements such as ?domestic de-vices can be operated efficiently with the system?.Thus, we will refer to it as Acceptability.
Finally,our third dimension lines up best with the Affect andAnnoyance items from SASSI.2 We will refer to it asAffect.Although the correspondences between our fac-tors and those derived from SASSI and INSPIREare not perfect, the fact that similar underlying fac-tors are derived from different user groups and sys-tems indicates that they are likely to be measuringthe same underlying constructs.5.2 Comparing satisfaction in different systemsRecall that in this study we combined the data fromtwo systems: FULL, where the system provided stu-dents with adaptive feedback and hints, and BASE,where the system simply acknowledged the stu-dent?s answers and then provided a correct answerwithout engaging in dialogue.
Table 2 separates outthe average factor scores for these two conditions,where a factor score is computed by averaging overscores of all questions assigned to that factor.When comparing learning gain and overall satis-faction between the two systems (which is the over-all impression of the system behavior as a whole,including circuit simulation and lesson design), thedifference is not statistically significant (learninggain t(69) = ?0.95, p = 0.35, overall satisfac-tion t(69) = ?1.52, p = 0.13).
In contrast, onindividual dimensions related to tutoring the scoresfor BASE is significantly higher than the score forFULL (Transparency, t(69) = ?7.19, p < 0.0001;Acceptability: t(69) = ?3.24, p < 0.01; Affect:2The acceptability dimension from INSPIRE is split be-tween our factors 2 and 3, but most of the questions correspondto our factor 2 questions.166FULL BASETransparency 2.15 (0.56) 3.36 (0.81)Acceptability 3.11 (1.02) 3.80 (0.77)Affect 2.43 (0.80) 2.86 (0.996)Overall 3.39 (0.88) 3.70 (0.83)Learning gain 0.61 (0.15) 0.65 (0.22)Table 2: Average scores for different satisfaction dimen-sions in FULL and BASE (standard deviation in parenthe-ses)t(69) = ?1.97, p = 0.05).
Comparing the means,the biggest difference in student ratings shows on theTransparency scale, while the affective reaction forthe two systems is more similar (though still ratedhigher for BASE).It is somewhat unexpected to see that the studentswere equally satisfied overall with both systems butrated the tutor in BASE more highly than in FULL,since the tutor behavior was the only thing differentbetween conditions.
We are at present investigatingthe reasons for this result.
One possibility is thatwhen students did not get much feedback from thetutor (as in BASE), other factors became more im-portant to overall satisfaction, such as course designand quality of user simulation.5.3 Relationships between subjective andobjective outcome measuresWe investigated the correlations between learninggain and different user satisfaction factors for thetwo system versions.
Results are presented in Table3.
As can be seen from the table, learning gain anduser satisfaction are only significantly correlated inFULL, and only for the acceptability and overall sat-isfaction factors.
None of the factors in the BASEsystem correlate with learning gain.
This indicatesthat the student?s affective reaction to the system isnot necessarily linked directly to its objective bene-fits.
We discuss these results further in Section 65.4 Impact of interpretation quality on usersatisfactionIt is generally known in SDS research that measuresof interpretation quality such as word error rate andconcept accuracy are strongly correlated with userFULL BASETransparency 0.32 (0.07) 0.06 (0.69)Acceptability 0.38 (0.03) 0.23 (0.16)Affect 0.29 (0.08) -0.10 (0.53)Overall 0.38 (0.02) 0.18 (0.28)Table 3: Correlations between satisfaction factors andlearning gain for two dialogue policies.
Significance levelin parentheses.
Bold indicates significance at p < 0.05level.satisfaction (e.g., (Walker et al, 2000; Mo?ller et al,2007)).
Our system uses typed input and producescomplex logical representations (rather than sim-ple slot-value pairs), thus, these measures cannot becomputed directly.
However, in an earlier study weshowed that another measure of interpretation qual-ity, namely, percentage of utterances that could notbe interpreted by the system (?uninterpretable utter-ances?)
is negatively correlated with learning gainand user satisfaction (Dzikovska et al, 2010b).3That study revealed an unexpected pattern.
Al-though the system recorded the number of utter-ances it could not interpret in both FULL and BASE,students in BASE were never informed of any in-terpretation problems.
Nevertheless, the proportionof such uninterpretable utterances was still signifi-cantly negatively correlated with user satisfaction inBASE.
After analyzing correlations between differ-ent types of errors and user satisfaction, we hypoth-esized that this can be explained by the lack of align-ment between the system and the student, in partic-ular when students used terminology different fromthat used by the system (Dzikovska et al, 2010b).We can now analyze this relationship in more de-tail, looking at correlations between interpretationproblems and different components of user satisfac-tion.
The results are presented in Table 4.As can be seen from the table, the proportionof uninterpretable answers is significantly correlatedwith Acceptability in FULL, but not in BASE.
Thisis not surprising, indicating that students who weretold that they were not understood perceived thesystem as less useful for them.
More surprisingly,Transparency, which is related to perceived ease of3In that study, we computed user satisfaction with the tutorby averaging over the entire 35 questions in our questionnaireas an initial approximation.167FULL BASETransparency -0.28 (0.1) -0.25 (0.10)Acceptability -0.58 (< 0.001) -0.29 (0.07)Affect -0.35 (0.04) -0.34 (0.04)Overall -0.38 (0.03) -0.27 (0.11)Learning gain -0.38 (0.03) -0.09(0.60)Table 4: Correlations between satisfaction factors and un-interpretable utterances for two different policies.
Signif-icance level in parentheses.use for the system, was not correlated with uninter-pretable utterances.
Finally, the proportion of unin-terpretable utterances is significantly correlated withAffect for both systems.
Moreover, the unexpectednegative correlation we observed in the earlier studybetween satisfaction with the tutor and interpretationproblems in BASE can be primarily attributed to thenegative correlation with the Affect score.6 DiscussionIn this study, we attempted to apply insights fromstudies of user satisfaction in spoken dialogue sys-tems to a different type of dialogue application: tu-torial dialogue.
We were looking to develop a betteruser satisfaction questionnaire for evaluating tutorialdialogue systems, and to implement an evaluationmethodology which takes into account different un-derlying dimensions of user satisfaction.The three dimensions we obtained based on ex-ploratory factor analysis of REVU-NL align wellwith the dimensions reported in the SDS litera-ture, which provides some evidence of their valid-ity.
However, the results are preliminary becauseof the small number of participants involved, andneed to be replicated with additional participants anddifferent tutoring systems.
Regardless, our analysishighlighted important issues in designing satisfac-tion surveys for different dialogue genres.When choosing which questions to include in asatisfaction questionnaire for a new system type,SASSI is a very attractive starting point, becauseit was validated across multiple SDS in two gen-res (command and control and information seeking).This also means that SASSI items are phrased verygenerally and therefore easier to adapt.
In contrast,INSPIRE contains a number of questions specific tothe command and control domain, asking whetherthe user thinks the system is useful in achieving theirgoals (i.e., operating the domestic devices).
SASSIincludes only one similar item, ?The system wasuseful?.
It was classed as Affect, most likely be-cause there were no other similar items.
However,we think that such questions represent an importantseparate dimension, namely the ?perceived useful-ness?
factor known to predict technology acceptance(Adams et al, 1989).
Therefore we included sev-eral items in REVU-NL with similar intent, askingwhether users thought the system was beneficial totheir goal (i.e., learning the material).
These itemswere clustered into a separate dimension by factoranalysis, indicating that they should be included inother satisfaction surveys.Moreover, some of the questions that appearedgenre-independent to us proved to be cross-loadingin our analysis, which is an indicator of ambiguity.Apparently, some of the items from task-oriented di-alogue questionnaires did not transfer well.
For ex-ample, statements like ?The system didn?t always dowhat I expected?
are unambiguous for task-orienteddialogue, where the user is supposed to be in controlof the interaction, and therefore has clear expecta-tions of what the system should do.
In contrast, intutorial dialogue the tutor has control over the learn-ing material.
Thus, it may be more ambiguous asto what, if anything, students are expecting from theinteraction.Overall, our experience shows that it may notbe possible, or indeed useful, to create completelygeneric surveys.
However, we believe that question-naires can be phrased generally enough to apply to arange of systems with similar goals, and REVU-NLin particular is useful starting point for comparingdialogue-based tutoring systems.
We believe that the18 questions that we retained as unambiguous in ouranalysis provide adequate assessment of user satis-faction, and are grouped into factors consistent withresults of previous research.
However, the question-naire could be further improved by revisiting thecross-loading items we rejected as ambiguous, andseeing if their wording could be improved.
We arealso intending to use REVU-IT in evaluating a spo-ken version of BEETLE II, thus providing additionalvalidation data on a different version of the interface.With respect to evaluation methodology, our re-sults highlight the need to look at different satis-168faction dimensions separately.
We used our fac-tors to further investigate a pattern that we discov-ered in previous research, namely, that students whospeak in a way that is difficult for the system to in-terpret tend to be less satisfied with the tutor, evenwhen they are not told of the interpretation prob-lems.
Looking at correlations with individual di-mensions shows that this relationship is primarilyexplained by the Affect dimension.
Our working hy-pothesis is that the lack of alignment between in-correct student answers and the answers supplied bythe system caused students to perceive the system asa less likeable or cooperative conversational partner.We also observed that Acceptability, but no otherdimensions, were correlated with learning gain inFULL.
One possible explanation is that students whoare learning more believe that the system is help-ing them reach their goals (our definition of Accept-ability).
The FULL condition provides students withmore explicit feedback as to their learning; whereasin BASE students may have a less accurate estimateof how well they are doing, and hence no satisfactiondimensions are correlated with learning gain.It is worth noting that an earlier study investigat-ing the relationship between user satisfaction andlearning in two different tutorial dialogue systems(Forbes-Riley and Litman, 2009) found little corre-lation between the answers to individual questionson their satisfaction questionnaire and learning gain.Only one correlation, with the question ?The tutorhelped me to concentrate?, reached significance inonly one of the 4 conditions they investigated.
Thisadds further evidence that the relationship betweenlearning gain and satisfaction is not straightforward.However, our results are difficult to compare sincethe questionnaires used are different, and Forbes-Riley and Litman (2009) are studying correlationswith individual questions rather than grouping re-lated questions together.
Developing better validatedquestionnaires will make such results easier to com-pare and interpret, and we believe that REVU-NLmakes a significant step in that direction.7 Conclusion and Future WorkIn this paper, we proposed an improved question-naire (REVU-NL) for evaluating user satisfactionin tutorial dialogue systems, which is an importantevaluation metric alongside learning gain.
We usedthe methodology from SDS evaluations to investi-gate different dimensions of user satisfaction, andtheir relationship to learning gain and different in-teraction properties.
Next, we are planning to usethe PARADISE methodology to establish predictivemodels that relate satisfaction dimensions to mea-surable interaction properties, so that we can de-termine development priorities, and make it eas-ier to compare different system versions.
We arealso planning to collect additional questionnaire datawith a speech-enabled version of the system, andverify our analyses on this extended data set.AcknowledgmentsThis work has been supported in part by US Of-fice of Naval Research grants N000141010085 andN0001410WX20278.
We would like to thank oursponsors from the Office of Naval Research, Dr. Su-san Chipman and Dr. Ray Perez, and the ResearchAssociates who worked on this project, Kather-ine Harrison, Leanne Taylor, Charles Scott, SimonCaine, Elaine Farrow and Charles Callaway for theircontribution to this effort.ReferencesDennis A. Adams, R. Ryan Nelson, and Peter A. Todd.1989.
Perceived usefulness, ease of use, and usage ofinformation technology.
MIS Quarterly., 13:319?339.Myroslava Dzikovska, Natalie B. Steinhauser, Jo-hanna D. Moore, Gwendolyn E. Campbell, Kather-ine M. Harrison, and Leanne S. Taylor.
2010a.
Con-tent, social, and metacognitive statements: An em-pirical study comparing human-human and human-computer tutorial dialogue.
In Sustaining TEL: FromInnovation to Learning and Practice - 5th EuropeanConference on Technology Enhanced Learning (EC-TEL 2010), pages 93?108, Barcelona, Spain, October.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, and Gwendolyn Campbell.
2010b.
Theimpact of interpretation problems on tutorial dialogue.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics(ACL-2010),Uppsala, Sweden, July.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, Gwendolyn Campbell, Elaine Farrow,and Charles B. Callaway.
2010c.
Beetle II: a systemfor tutoring and computational linguistics experimen-tation.
In Proceedings of the 48th Annual Meeting of169the Association for Computational Linguistics (ACL-2010) demo session, Uppsala, Sweden, July.Katherine Forbes-Riley and Diane J. Litman.
2009.Adapting to student uncertainty improves tutoring dia-logues.
In Artificial Intelligence in Education: Build-ing Learning Systems that Care: From KnowledgeRepresentation to Affective Modelling, Proceedingsof the 14th International Conference on Artificial In-telligence in Education (AIED 2009), pages 33?40,Brighton, UK, July.Katherine Forbes-Riley and Diane J. Litman.
2011.Designing and evaluating a wizarded uncertainty-adaptive spoken dialogue tutoring system.
ComputerSpeech & Language, 25(1):105?126.Katherine Forbes-Riley, Diane J. Litman, Scott Silliman,and Joel R. Tetreault.
2006.
Comparing synthesizedversus pre-recorded tutor speech in an intelligent tu-toring spoken dialogue system.
In Proceedings ofthe Nineteenth International Florida Artificial Intelli-gence Research Society Conference, pages 509?514,Melbourne Beach, Florida, USA, May.Kate S. Hone and Robert Graham.
2000.
Towards atool for the subjective assessment of speech systeminterfaces (SASSI).
Natural Language Engineering,6(3&4):287?303.G.
Tanner Jackson, Arthur C. Graesser, and Danielle S.McNamara.
2009.
What students expect may havemore impact than what they know or feel.
In Proceed-ings 14th International Conference on Artificial Intel-ligence in Education (AIED), Brighton, UK.Lars Bo Larsen.
2003.
Issues in the evaluation of spo-ken dialogue systems using objective and subjectivemeasures.
In Proceedings of 2003 IEEE Workshopon Automatic Speech Recognition and Understanding(ASRU?03), pages 209 ?
214, December.Diane Litman, Johanna Moore, Myroslava Dzikovska,and Elaine Farrow.
2009.
Using natural language pro-cessing to analyze tutorial dialogue corpora across do-mains and modalities.
In Proceedings of 14th Interna-tional Conference on Artificial Intelligence in Educa-tion (AIED), Brighton, UK, July.Joel Michael, Allen Rovick, Michael Glass, Yujian Zhou,and Martha Evens.
2003.
Learning from a computertutor with natural language capabilities.
InteractiveLearning Environments, 11:233?262(30).Sebastian Mo?ller, Paula Smeele, Heleen Boland, and JanKrebber.
2007.
Evaluating spoken dialogue systemsaccording to de-facto standards: A case study.
Com-puter Speech & Language, 21(1):26 ?
53.Amruta Purandare and Diane Litman.
2008.
Content-learning correlations in spoken tutoring dialogs atword, turn and discourse levels.
In Proceedings ofthe 21st International FLAIRS Conference, CoconutGrove, Florida, May.Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-man.
2000.
Towards Developing General Models ofUsability with PARADISE.
Natural Language Engi-neering, 6(3).Maria Wolters, Kallirroi Georgila, Robert Logie, SarahMacPherson, Johanna Moore, and Matt Watson.
2009.Reducing working memory load in spoken dialoguesystems.
Interacting with Computers, 21(4):276?287.170A REVU-NL Questionst1 I felt in control of my conversations with the tutor.t2 It took the tutor too long to respond to my statements.t3 I felt that the tutor understood me well.t4 The tutor didn?t always do what I expected.t5 The information that the tutor provided to me was incomplete.t6 It was easy for me to become confused during our dialogue.t7 I enjoyed talking with the tutor.t8 The tutor interfered with my understanding of the topics in electricity and circuits.t9 I was not always sure what the tutor expected of me.t10 Conversing with the tutor was fun.t11 It was easy to understand the things that the tutor said.t12 The dialogue between me and the tutor was very repetitive.t13 I had to really concentrate when I was talking with the tutor.t14 The tutor was too inflexible.t15 Working through the lessons with the computer tutor was as easy as working through the lessonswith a human tutor.t16 The tutor didn?t always do what I wanted.t17 I felt confident when talking with the tutor.t18 I always knew what to say to the tutor.t19 I was able to recover easily from errors during our dialogues.t20 Talking with the tutor was frustrating.t21 The information provided by the tutor was clear.t22 It was easy to interact with the tutor.t23 The tutor?s dialogue was clumsy and unnatural.t24 It was easy to learn how to speak to the tutor in a way that the tutor understood.t25 I felt comfortable talking with the tutor.t26 I didn?t always understand what the tutor meant.t27 The tutor was not helpful.t28 I found conversing with the tutor to be irritating.t29 I knew what I could say or do at each point in the conversation with the tutor.t30 I found our dialogues to be boring.t31 Having the tutor help me with the material was an efficient way to learn.t32 It was easy to learn from the tutor.t33 The tutor responded quickly.t34 Having the tutor was worthwhilet35 Our dialogues quickly led to me having a deeper understanding of the material.B REVU-OVERALL questionso1 Overall, I am satisfied with my experience learning about electricity from this system.o2 Working in this learning environment was just like working one-on-one with a human tutor.o3 I would have preferred to learn about electricity in a different way.o4 I would use this system again in the future to continue to learn about electricity.o5 I would like to be able to use a system like this to learn about other topics in the future.171C REVU-IT questions related to GUI and reading material (mentioned but not analyzedin the paper)sl1 It was easy to navigate through the slides.sl2 It took a long time for each new slide to be displayed.sl3 The material on the slides was easy to understand.sl4 The material on the slides was poorly written.sl5 I would have benefited from more instrucion on how to move through the slides.sl6 The material on the slides was interesting.sl7 The slide navigation buttons didn?t always work the way I expected them to.sl8 The slides were annoying.sl9 The material on the slides was written at a level far beneath my abilities.sl10 I would prefer reading a text book over reading these slides.e1 I found it difficult to learn how to build circuits and take measurements in the workspace.e2 Completing exercises in the workspace was fun.e3 Before beginning the lesson, I received the right amount of instruction on how to build circuits inthe workspace and take measurements.e4 The exercises were well designed to illustrate the important lesson concepts.e5 Sometimes I didn?t understand what I was supposed to do for an exercise.e6 The method for connecting components with wires was counter-intuitive.e7 Having to build all those circuits was annoying.e8 I always knew exactly what to build and/or measure in the workspace, and how to do it.e9 Circuits loaded quickly.e10 Even if I didn?t predict the outcome correctly ahead of time, once I completed an exercise, Ialways understood the point.e11 It was easy to use the meter.e12 There were more exercises than necessary to cover the lesson topics.e13 I would have learned more if I had been able to build circuits with actual light bulbs and batteries.172
