Story Link Detection based on Dynamic Information ExtendingXiaoyan Zhang Ting Wang Huowang ChenDepartment of Computer Science and Technology, School of Computer,National University of Defense TechnologyNo.137, Yanwachi Street, Changsha, Hunan 410073, P.R.China{zhangxiaoyan, tingwang, hwchen}@nudt.edu.cnAbstractTopic Detection and Tracking refers to au-tomatic techniques for locating topically re-lated materials in streams of data.
As thecore technology of it, story link detection isto determine whether two stories are aboutthe same topic.
To overcome the limitationof the story length and the topic dynamicevolution problem in data streams, this pa-per presents a method of applying dynamicinformation extending to improve the per-formance of link detection.
The proposedmethod uses previous latest related story toextend current processing story, generatesnew dynamic models for computing the sim-ilarity between the current two stories.
Thework is evaluated on the TDT4 Chinese cor-pus, and the experimental results indicatethat story link detection using this methodcan make much better performance on allevaluation metrics.1 IntroductionTopic Detection and Tracking (TDT) (Allan, 2002)refers to a variety of automatic techniques for dis-covering and threading together topically relatedmaterial in streams of data such as newswire orbroadcast news.
Such automatic discovering andthreading could be quite valuable in many appli-cations where people need timely and efficient ac-cess to large quantities of information.
Supportedby such technology, users could be alerted with newevents and new information about known events.
Byexamining one or two stories, users define the topicdescribed in them.
Then with TDT technologiesthey could go to a large archive, find all the storiesabout this topic, and learn how it evolved.Story link detection, as the core technology de-fined in TDT, is a task of determining whether twostories are about the same topic, or topically linked.In TDT, a topic is defined as ?something that hap-pens at some specific time and place?
(Allan, 2002).Link detection is considered as the basis of otherevent-based TDT tasks, such as topic tracking, topicdetection, and first story detection.
Since story linkdetection focuses on the streams of news stories,it has its specific characteristic compared with thetraditional Information Retrieval (IR) or Text Clas-sification task: new topics usually come forth fre-quently during the procedure of the task, but nothingabout them is known in advance.The paper is organized as follows: Section 2 de-scribes the procedure of story link detection; Section3 introduces the related work in story link detection;Section 4 explains a baseline method which willbe compared with the proposed dynamic method inSection 5; the experiment results and analysis aregiven in Section 6; finally, Section 7 concludes thepaper.2 Problem DefinitionIn the task definition of story link detection (NIST,2003), a link detection system is given a se-quence of time-ordered news source files S =?S1, S2, S3, .
.
.
, Sn?
where each Si includes a setof stories, and a sequence of time-ordered storypairs P = ?P1, P2, P3, .
.
.
, Pm?
where Pi =40(si1, si2), si1 ?
Sj , si2 ?
Sk, 1 ?
i ?
m, 1 ?
j ?k ?
n. The system is required to make decisions onall story pairs to judge if they describe a same topic.We formalize the procedure for processing a pairof stories as follows:For a story pair Pi = (si1, si2):1.
Get background corpus Bi of Pi.
According tothe supposed application situation and the cus-tom that people usually look ahead when theybrowse something, in TDT research the systemis usually allowed to look ahead N (usually 10)source files when deciding whether the currentpair is linked.
So Bi = {S1, S2, S3, .
.
.
, Sl},wherel ={k + 10 , si2 ?
Sk and (k + 10) ?
nn , si2 ?
Sk and (k + 10) > n .2.
Produce the representation models (Mi1,Mi2)for two stories in Pi.
M = {(fs, ws) | s ?
1},where fs is a feature extracted from a story andws is the weight of the feature in the story.
Theyare computed with some parameters countedfrom current story and the background.3.
Choose a similarity function F and computingthe similarity between two models.
If t is a pre-defined threshold and F (Mi1,Mi2) ?
t, thenstories in Pi are topically linked.3 Related WorkA number of works has been developed on story linkdetection.
It can be classified into two categories:vector-based methods and probabilistic-based meth-ods.The vector space model is widely used in IR andText Classification research.
Cosine similarity be-tween document vectors with tf?idf term weighting(Connell et al, 2004) (Chen et al, 2004) (Allan etal., 2003) is also one of the best technologies for linkdetection.
We have examined a number of similaritymeasures in story link detection, including cosine,Hellinger and Tanimoto, and found that cosine sim-ilarity produced outstanding results.
Furthermore,(Allan et al, 2000) also confirms this conclusionamong cosine, weighted sum, language modelingand Kullback-Leibler divergence in its story link de-tection research.Probabilistic-based method has been proven to bevery effective in several IR applications.
One of itsattractive features is that it is firmly rooted in the the-ory of probability, thereby allowing the researcherto explore more sophisticated models guided by thetheoretical framework.
(Nallapati and Allan, 2002)(Lavrenko et al, 2002) (Nallapati, 2003) all ap-ply probability models (language model or relevancemodel) for story link detection.
And the experimentresults indicate that the performances are compara-ble with those using traditional vector space models,if not better.On the basis of vector-based methods, this paperrepresents a method of dynamic information extend-ing to improve the performance of story link detec-tion.
It makes use of the previous latest topically re-lated story to extend the vector model of current be-ing processed story.
New dynamic models are gen-erated for computing the similarity between two sto-ries in current pair.
This method resolves the prob-lems of information shortage in stories and topic dy-namic evolution in streams of data.Before introducing the proposed method, we firstdescribe a method which is implemented with vectormodel and cosine similarity function.
This straightand classic method is used as a baseline to be com-pared with the proposed method.4 Baseline Story Link DetectionThe related work in story link detection shows thatvector representation model with cosine functioncan be used to build the state-of-the-art story link de-tection systems.
Many research organizations takethis as their baseline system (Connell et al, 2004)(Yang et al, 2002).
In this paper, we make a similarchoice.The baseline method represents each story as avector in term space, where the coordinates repre-sent the weights of the term features in the story.Each vector terms (or feature) is a single word plusits tag which is produced by a segmenter and part ofspeech tagger for Chinese.
So if two tokens withsame spelling are tagged with different tags, theywill be taken as different terms (or features).
It isnotable that in it is independent between processingany two comparisons the baseline method.414.1 PreprocessingA preprocessing has been performed for TDT Chi-nese corpus.
For each story we tokenize the text, tagthe generated tokens, remove stop words, and thenget a candidate set of terms for its vector model.
Af-ter that, the term-frequency for each token in thestory and the length of the story will also be ac-quired.
In the baseline and dynamic methods, bothtraining and test data are preprocessed in this way.The segmenter and tagger used here is ICTCLAS1 .
The stop word list is composed of 507 terms.
Al-though the term feature in the vector representationis the word plus its corresponding tag, we will ig-nore the tag information when filtering stop words,because almost all the words in the list should befiltered out whichever part of speech is used to tagthem.4.2 Feature WeightingOne important issue in the vector model is weight-ing the individual terms (features) that occur in thevector.
Most IR systems employed the traditionaltf ?
idf weighting, which also provide the base forthe baseline and dynamic methods in this paper.
Fur-thermore, this paper adopts a dynamic way to com-pute the tf ?
idf weighting:wi(fi, d) = tf(fi, d) ?
idf(fi)tf = t/(t+ 0.5 + 1.5dl/dlavg)idf = log((N + 0.5)/df)/log(N + 1)where t is the term frequency in a story, dl is thelength of a story, dlavg is the average length of sto-ries in the background corpus, N is the number ofstories in the corpus, df is the number of the storiescontaining the term in the corpus.The tf shows how much a term represents thestory, while the idf reflects the distinctive abilityof distinguishing current story from others.
Thedynamic attribute of the tf ?
idf weighting lies inthe dynamic computation of dlavg, N and df .
Thebackground corpus used for statistics is incremen-tal.
As more story pairs are processed, more sourcefiles could be seen, and the background is expand-ing as well.
Whenever the size of the background1http://sewm.pku.edu.cn/QA/reference/ICTCLAS/FreeICT-CLAS/has changed, the values of dlavg, N and df will up-date accordingly.
We call this as incremental tf ?idfweighting.
A story might have different term vectorsin different story pairs.4.3 Similarity FunctionAnother important issue in the vector model is de-termining the right function to measure the similar-ity between two vectors.
We have firstly tried threefunctions: cosine, Hellinger and Tanimoto, amongwhich cosine function performs best for its substan-tial advantages and the most stable performance.
Sowe consider the cosine function in baseline method.Cosine similarity, as a classic measure and con-sistent with the vector representation, is simply aninner product of two vectors where each vector isnormalized to the unit length.
It represents cosineof the angle between two vector models M1 ={(f1i, w1i), i ?
1} and M2 = {(f2i, w2i), i ?
1}.cos(M1,M2) = (?
(w1i ?
w2i))/?
(?w21i)(?w22i)Cosine similarity tends to perform best at full di-mensionality, as in the case of comparing two sto-ries.
Performance degrades as one of the vectors be-comes shorter.
Because of the built-in length nor-malization, cosine similarity is less dependent onspecific term weighting.5 Dynamic Story Link Detection5.1 MotivationInvestigation on the TDT corpus shows that newsstories are usually short, which makes that their rep-resentation models are too sparse to reflect topicsdescribed in them.
A possible method of solvingthis problem is to extend stories with other relatedinformation.
The information can be synonym ina dictionary, related documents in external corpora,etc.
However, extending with synonym is mainlyadding repetitious information, which can not definethe topics more clearly.
On the other hand, topic-based research should be real-sensitive.
The corporain the same period as the test corpora are not easyto gather, and the number of related documents inprevious period is very few.
So it is also not feasi-ble to extend the stories with related documents inother corpora.
We believe that it is more reason-able that the best extending information may be the42story corpus itself.
Following the TDT evaluationrequirement, we will not use entire corpus at a time.Instead, when we process current pair of stories, weutilize all the stories before the current pair in thestory corpus.In addition, topics described by stories usuallyevolve along with time.
A topic usually begins witha seminal event.
After that, it will focus mainly onthe consequence of the event or other directly re-lated events as the time goes.
When the focus inlater stories has changed, the words used in themmay change remarkably.
Keeping topic descrip-tions unchanged from the beginning to the end isobviously improper.
So topic representation mod-els should also be updated as the topic emphases instories has changed.
Formerly we have planed to userelated information to extend a story to make up theinformation shortage in stories.
Considering moreabout topic evolution, we extend a story with its lat-est related story.
In addition, up to now almost allresearch in story link detection takes the hypothe-sis that whether two stories in one pair are topicallylinked is independent of that in another pair.
But werealize that if two stories in a pair describe a sametopic, one story can be taken as related informationto extend another story in later pairs.
Compared withextending with more than one story, extending onlywith its latest related story can keep representationof the topic as fresh as possible, and avoid extend-ing too much similar information at the same time,which makes the length of the extended vector toolong.
Since the vector will be renormalized, a toobig length means evidently decreasing the weightof an individual feature which will instead cause alower cosine similarity.
This idea has also been con-firmed by the experiment showing that the perfor-mance extending with one latest related story is su-perior to that extending with more than one relatedstory, as described in section 6.3.
The experiment re-sults also show that this method of dynamic informa-tion extending apparently improves the performanceof story link detection.5.2 Method DescriptionThe proposed dynamic method is actually the base-line method plus dynamic information extending.The preprocessing, feature weighting and similaritycomputation in dynamic method are similar as thosein baseline method.
However, the vector representa-tion for a story here is dynamic.
This method needs atraining corpus to get the extending threshold decid-ing whether a story should be used to extend anotherstory in a pair.
We split the sequence of time-orderedstory pairs into two parts: the former is for trainingand the later is for testing.
The following is the pro-cessing steps:1.
Preprocess to create a set of terms for repre-senting each story as a term vector, which issame as baseline method.2.
Run baseline system on the training corporaand find an optimum topically link threshold.We take this threshold as extending threshold.The topically link threshold used for makinglink decision in dynamic method is another pre-defined one.3.
Along with the ordered story pairs in the testcorpora, repeat a) and b):(a) When processing a pair of stories Pi =(si1, si2), if si1 or si2 has an extendingstory, then update the corresponding vec-tor model with its related story to a newdynamic one.
The generation procedureof dynamic vector will be described innext subsection.
(b) Computing the cosine similarity betweenthe two dynamic term vectors.
If it ex-ceeds the extending threshold, then si1 andsi2 are the latest related stories for eachother.
If one story already has an extend-ing story, replace the old one with the newone.
So a story always has no more thanone extending story at any time.
If thesimilarity exceeds topically link threshold,si1 and si2 are topically linked.From the above description, it is obvious that dy-namic method needs two thresholds, one for makingextending decision and the other for making link de-cision.
Since in this paper we will focus on the op-timum performance of systems, the first threshold ismore important.
But topically link threshold is alsonecessary to be properly defined to approach a bet-ter performance.
In the baseline method, term vec-tors are dynamic because of the incremental tf ?
idf43weighting.
However, dynamic information extend-ing is another more important reason in the dynamicmethod.
Whenever a story has an extending story, itsvector representation will update to include the ex-tending information.
Having the extending method,the representation model can have more informationto describe the topic in a story and make the topicevolve along with time.
The dynamic method candefine topic description clearer and get a more accu-rate similarity between stories.5.3 Dynamic Vector ModelIn the dynamic method, we have tried two ways forthe generation of dynamic vector models: incrementmodel and average model.
Supposing we use vectormodel M1 = {(f1i, w1i), i ?
1} of story s1 to ex-tend vector model M2 = {(f2i, w2i), i ?
1} of storys2, M2 will change to representing the latest evolv-ing topic described in current story after extending.1.
Increment Model: For each term f1i in M1, ifit also occurs as f2j in M2, then w2j will notchange, otherwise (f1i, w1i) will be added intoM2.
This dynamic vector model only takes in-terest in the new information that occurs only inM1.
For features both occurred in M1 and M2,the dynamic model will respect to their originalweights.2.
Average Model: For each term f1i in M1, ifit also occurs as f2j in M2, then w2j = 0.5 ?
(w1i+w2j), otherwise (f1i, w1i) will be addedinto M2.
This dynamic model will take accountof all information in M1.
So the difference be-tween those two dynamic models is the weightrecalculation method of the feature occurred inboth M1 and M2.Both the above two dynamic models can take ac-count of information extending and topic evolution.Increment Model is closer to topic description sinceit is more dependent on latest term weights, whileAverage Model makes more reference to the cen-troid concept.
The experiment results show that dy-namic method with Average Model is a little supe-rior to that with Increment Model.6 Experiment and Discussion6.1 Experiment DataTo evaluate the proposed method, we use the Chi-nese subset of TDT4 corpus (LDC, 2003) devel-oped by the Linguistic Data Consortium (LDC) forTDT research.
This subset contains 27145 storiesall in Chinese from October 2000 through January2001, which are gathered from news, broadcast orTV shows.LDC totally labeled 40 topics on TDT4 for 2003evaluation.
There are totally 12334 stories pairsfrom 1151 source files in the experiment data.
Theanswers for these pairs are based on 28 topics ofthese topics, generated from the LDC 2003 anno-tation documents.
The first 2334 pairs are used fortraining and finding extending threshold of dynamicmethod.
The rest 10000 pairs are testing data usedfor comparing performances of baseline and the dy-namic methods.6.2 Evaluation MeasuresThe work is measured by the TDT evaluation soft-ware, which could be referred to (Hoogma, 2005)for detail.
Here is a brief description.
The goal oflink detection is to minimize the cost due to errorscaused by the system.
The TDT tasks are evaluatedby computing a ?detection cost?
:Cdet = Cmiss?Pmiss?Ptarget+Cfa?Pfa?Pnon?targetwhere Cmiss is the cost of a miss, Pmiss is the es-timated probability of a miss, Ptarget is the priorprobability under which a pair of stories are linked,Cfa is the cost of a false alarm, Pfa is the estimatedprobability of a false alarm, and Pnon?target is theprior probability under which a pair of stories arenot linked.
A miss occurs when a linked story pair isnot identified as being linked by the system.
A falsealarm occurs when the pair of stories that are notlinked are identified as being linked by the system.A target is a pair of linked stories; conversely a non-target is a pair of stories that are not linked.
For thelink detection task these parameters are set as fol-lows: Cmiss is 1, Ptarget is 0.02, and Cfa is 0.1.
Thecost for each topic is equally weighted (usually thecost of topic-weighted is the mainly evaluation pa-rameter) and normalized so that for a given system,the normalized value (Cdet)norm can be no less than44one without extracting information from the sourcedata:(Cdet)norm = Cdetmin(CmissPtarget, CfaPnon?target)(Cdet)overall = ?i(Cidet)norm/#topicswhere the sum is over topics i.
A detection curve(DET curve) is computed by sweeping a thresholdover the range of scores, and the minimum cost overthe DET curve is identified as the minimum detec-tion cost or min DET.
The topic-weighted DET costis dependent on both a good minimum cost and agood method for selecting an operating point, whichis usually implemented by selecting a threshold.
Asystem with a very low min DET cost can have amuch larger topic-weighted DET score.
Therefore,we focus on the minimum DET cost for the experi-ments.6.3 Experiment ResultsIn this paper, we have tried three methods for storylink detection: the baseline method described inSection 4 and two dynamic methods with differentdynamic vectors introduced in Section 5.
The fol-lowing table gives their evaluation results.metrics baseline dynamic 1 dynamic 2Pmiss 0.0514 0.0348 0.0345Pfa 0.0067 0.0050 0.0050Clinkmin 0.0017 0.0012 0.0012Clinknorm 0.0840 0.0591 0.0588Table 1: Experiment Results of Baseline System andDynamic SystemsIn the table, Clinkmin is the minimum(Cdet)overall, DET Graph Minimum DetectionCost (topic-weighted), Clinknorm is the normal-ized minimum (Cdet)overall, the dynamic 1 is thedynamic method which uses Increment Model andthe dynamic 2 is the dynamic method which usesAverage Model.
We can see that the proposed twodynamic methods are both much better than base-line method on all four metrics.
The ClinkNormof dynamic 1 and 2 are improved individually by27.2% and 27.8% as compared to that of baselinemethod.
The difference between two dynamicmethods is due to different in the Pmiss.
However,it is too little to compare the two dynamic systems.We also make additional experiments in which astory is extended with all of its previous relatedstories.
The minimum (Cdet)overall is 0.0614 forthe system using Increment Model, and 0.0608 forthe system using Average Model.
Although theperformances are also much superior to baseline, itis still a little poorer than that with only one latestrelated story, which confirm the ideal described insection 5.1.Figure 1, 2 and 3 show the detail evaluation in-formation for individual topic on Minimum NormDetection Cost, Pmiss and Pfa.
From Figure 1 weknow these two dynamic methods have improved theperformance on almost all the topic, except topic 12,26 and 32.
Note that detection cost is a function ofPmiss and Pfa.
Figure 2 shows that both two dy-namic methods reduce the false alarm rates on allevaluation topics.
In Figure 3 there are 20 topicson which the miss rates remain zero or unchange.The dynamic methods reduce the miss rates on 5topics.
However, dynamic methods get relativelypoorer results on topic 12, 26 and 32 .
Altogetherdynamic methods can notably improve system per-formance on evaluation metrics of both individualand weighted topic, especially the false alarm rate,but on some topics, it gets poorer results.Further investigation shows that topic 12, 26 and32 are about Presidential election in Ivory Coaston October 25, 2000, Airplane Crash in ChiangKai Shek International Airport in Taiwan on Octo-ber 31, 2000, and APEC Conference on Novem-ber 12-15, 2000 at Brunei.
After analyzing thosestory pairs with error link decision, we can splitthem into two sets.
One is that two stories in a pairare general linked but not TDT specific topicallylinked.
Here general linked means that there aremany common words in two stories, but the eventsdescribed in them happened in different times or dif-ferent places.
For example, Airplane Crash is a gen-eral topic, while Airplane Crash in certain locationat specification time is a TDT topic.
The other isthat two stories in a pair are TDT topically linkedwhile they describe the topic from different perspec-tives.
In this condition they will have few commonwords.
These may be due to that the informationextracted from stories is still not accurate enoughto represent them.
It also may be because of the4500.050.10.150.20.250.30.351 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39Topic IDNormClinkNorm Clink(Baseline)Norm Clink(Dynamic 1)Norm Clink(Dynamic 2)Figure 1: Normalized Minimum Detection Cost for individual topic00.0050.010.0150.020.0250.030.0351 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39Topic IDFalseAlarmprobabilityPfa(Baseline)Pfa(Dynamic 1)Pfa(Dynamic 2)Figure 2: Pfa for individual topic00.050.10.150.20.250.31 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39Topic IDMissprobabilityPmiss(Baseline)Pmiss(Dynamic 1)Pmiss(Dynamic 2)Figure 3: Pmiss for individual topic46deficiency of vector model itself.
Furthermore, weknow that the extending story is chosen by cosinesimilarity, which results that the extending story andthe extended story are usually topically linked fromthe same perspectives, seldom from different per-spectives.
Therefore the method of information ex-tending may sometimes turn the above first problemworse and have no impact on the second problem.So mining more useful information or making moreuse of other useful resources to solve these problemswill be the next work.
In addition, how to repre-sent this information with a proper model and seek-ing better or more proper representation models forTDT stories are also important issues.
In a word,the method of information extending has been veri-fied efficient in story link detection and can provide areference to improve the performance of some othersimilar systems whose data must be processed seri-ally, and it is also hopeful to combined with otherimprovement technologies.7 ConclusionStory link detection is a key technique in TDT re-search.
Though many approaches have been tried,there are still some characters ignored.
After analyz-ing the characters and deficiency in TDT stories andstory link detection, this paper presents a method ofdynamic information extending to improve the sys-tem performance by focus on two problems: infor-mation deficiency and topic evolution.
The exper-iment results indicate that this method can effec-tively improve the performance on both miss andfalse alarm rates, especially the later one.
How-ever, we should realize that there are still some prob-lems to solve in story link detection.
How to com-pare general topically linked stories and how to com-pare stories describing a TDT topic from differentangles will be very vital to improve system perfor-mance.
The next work will focus on mining moreand deeper useful information in TDT stories andexploiting more proper models to represent them.AcknowledgementThis research is supported by the National Natu-ral Science Foundation of China (60403050), Pro-gram for New Century Excellent Talents in Uni-versity (NCET-06-0926) and the National GrandFundamental Research Program of China underGrant(2005CB321802).ReferencesJames Allan, Victor Lavrenko, Daniella Malin, and Rus-sell Swan.
2000.
Detections, bounds, and timelines:Umass and tdt?3.
In Proceedings of Topic Detectionand Tracking (TDT?3), pages 167?174.J.
Allan, A. Bolivar, M. Connell, S. Cronen-Townsend,A Feng, F. Feng, G. Kumaran, L. Larkey, V. Lavrenko,and H. Raghavan.
2003.
Umass tdt 2003 researchsummary.
In proceedings of TDT workshop.James Allan, editor.
2002.
Topic Detection and Track-ing: Event-based Information Organization.
KluwerAcademic Publishers, Norvell, Massachusetts.Francine Chen, Ayman Farahat, and Thorsten Brants.2004.
Multiple similarity measures and source-pairinformation in story link detection.
In HLT-NAACL,pages 313?320.Margaret Connell, Ao Feng, Giridhar Kumaran, HemaRaghavan, Chirag Shah, and James Allan.
2004.Umass at tdt 2004.
In TDT2004 Workshop.Niek Hoogma.
2005.
The modules and methods of topicdetection and tracking.
In 2nd Twente Student Confer-ence on IT.Victor Lavrenko, James Allan, Edward DeGuzman,Daniel LaFlamme, Veera Pollard, and StephenThomas.
2002.
Relevance models for topic detec-tion and tracking.
In Proceedings of Human LanguageTechnology Conference (HLT), pages 104?110.LDC.
2003.
Topic detection and tracking - phase 4.Technical report, Linguistic Data Consortium.Ramesh Nallapati and James Allan.
2002.
Capturingterm dependencies using a language model based onsentence trees.
In Proceedings of the eleventh interna-tional conference on Information and knowledge man-agement, pages 383?390.
ACM Press.Ramesh Nallapati.
2003.
Semantic language models fortopic detection and tracking.
In HLT-NAACL.NIST.
2003.
The 2003 topic detection and tracking taskdefinition and evaluation plan.
Technical report, Na-tional Institute of Standards and Technology(NIST).Yiming Yang, Jian Zhang, Jaime Carbonell, and ChunJin.
2002.
Topic-conditioned novelty detection.
InProceedings of the eighth ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 688?693.
ACM Press.47
