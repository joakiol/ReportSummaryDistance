Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111?121,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsBilingually-constrained Phrase Embeddings for Machine TranslationJiajun Zhang1, Shujie Liu2, Mu Li2, Ming Zhou2and Chengqing Zong11National Laboratory of Pattern Recognition, CASIA, Beijing, P.R.
China{jjzhang,cqzong}@nlpr.ia.ac.cn2Microsoft Research Asia, Beijing, P.R.
China{shujliu,muli,mingzhou}@microsoft.comAbstractWe propose Bilingually-constrained Re-cursive Auto-encoders (BRAE) to learnsemantic phrase embeddings (compactvector representations for phrases), whichcan distinguish the phrases with differ-ent semantic meanings.
The BRAE istrained in a way that minimizes the seman-tic distance of translation equivalents andmaximizes the semantic distance of non-translation pairs simultaneously.
Aftertraining, the model learns how to embedeach phrase semantically in two languagesand also learns how to transform semanticembedding space in one language to theother.
We evaluate our proposed methodon two end-to-end SMT tasks (phrase ta-ble pruning and decoding with phrasal se-mantic similarities) which need to mea-sure semantic similarity between a sourcephrase and its translation candidates.
Ex-tensive experiments show that the BRAEis remarkably effective in these two tasks.1 IntroductionDue to the powerful capacity of feature learn-ing and representation, Deep (multi-layer) NeuralNetworks (DNN) have achieved a great success inspeech and image processing (Kavukcuoglu et al,2010; Krizhevsky et al, 2012; Dahl et al, 2012).Recently, statistical machine translation (SMT)community has seen a strong interest in adaptingand applying DNN to many tasks, such as wordalignment (Yang et al, 2013), translation confi-dence estimation (Mikolov et al, 2010; Liu et al,2013; Zou et al, 2013), phrase reordering predic-tion (Li et al, 2013), translation modelling (Auli etal., 2013; Kalchbrenner and Blunsom, 2013) andlanguage modelling (Duh et al, 2013; Vaswani etal., 2013).
Most of these works attempt to im-prove some components in SMT based on wordembedding, which converts a word into a dense,low dimensional, real-valued vector representation(Bengio et al, 2003; Bengio et al, 2006; Collobertand Weston, 2008; Mikolov et al, 2013).However, in the conventional (phrase-based)SMT, phrases are the basic translation units.
Themodels using word embeddings as the direct in-puts to DNN cannot make full use of the wholesyntactic and semantic information of the phrasaltranslation rules.
Therefore, in order to success-fully apply DNN to model the whole translationprocess, such as modelling the decoding process,learning compact vector representations for the ba-sic phrasal translation units is the essential andfundamental work.In this paper, we explore the phrase embedding,which represents a phrase (sequence of words)with a real-valued vector.
In some previous works,phrase embedding has been discussed from differ-ent views.
Socher et al (2011) make the phraseembeddings capture the sentiment information.Socher et al (2013a) enable the phrase embed-dings to mainly capture the syntactic knowledge.Li et al (2013) attempt to encode the reorderingpattern in the phrase embeddings.
Kalchbrennerand Blunsom (2013) utilize a simple convolutionmodel to generate phrase embeddings from wordembeddings.
Mikolov et al (2013) consider aphrase as an indivisible n-gram.
Obviously, thesemethods of learning phrase embeddings either fo-cus on some aspects of the phrase (e.g.
reorderingpattern), or impose strong assumptions (e.g.
bag-of-words or indivisible n-gram).
Therefore, thesephrase embeddings are not suitable to fully repre-sent the phrasal translation units in SMT due to thelack of semantic meanings of the phrase.Instead, we focus on learning phrase embed-dings from the view of semantic meaning, sothat our phrase embedding can fully represent thephrase and best fit the phrase-based SMT.
As-suming the phrase is a meaningful composition111of its internal words, we propose Bilingually-constrained Recursive Auto-encoders (BRAE) tolearn semantic phrase embeddings.
The core ideabehind is that a phrase and its correct translationshould share the same semantic meaning.
Thus,they can supervise each other to learn their seman-tic phrase embeddings.
Similarly, non-translationpairs should have different semantic meanings,and this information can also be used to guidelearning semantic phrase embeddings.In our method, the standard recursive auto-encoder (RAE) pre-trains the phrase embeddingwith an unsupervised algorithm by minimizing thereconstruction error (Socher et al, 2010), whilethe bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the se-mantic distance between translation equivalentsand maximizing the semantic distance betweennon-translation pairs.We use an example to explain our model.
Asillustrated in Fig.
1, the Chinese phrase on theleft and the English phrase on the right are trans-lations with each other.
If we learn the embeddingof the Chinese phrase correctly, we can regard itas the gold representation for the English phraseand use it to guide the process of learning Englishphrase embedding.
In the other direction, the Chi-nese phrase embedding can be learned in the sameway.
This procedure can be performed with anco-training style algorithm so as to minimize thesemantic distance between the translation equiva-lents1.
In this way, the result Chinese and Englishphrase embeddings will capture the semantics asmuch as possible.
Furthermore, a transformationfunction between the Chinese and English seman-tic spaces can be learned as well.With the learned model, we can accurately mea-sure the semantic similarity between a sourcephrase and a translation candidate.
Accordingly,we evaluate the BRAE model on two end-to-end SMT tasks (phrase table pruning and decod-ing with phrasal semantic similarities) which needto check whether a translation candidate and thesource phrase are in the same meaning.
In phrasetable pruning, we discard the phrasal translationrules with low semantic similarity.
In decodingwith phrasal semantic similarities, we apply thesemantic similarities of the phrase pairs as newfeatures during decoding to guide translation can-1For simplicity, we do not show non-translation pairshere.source phrase embedding ps??
?
???
France and Russiatarget phrase embedding ptFigure 1: A motivation example for the BRAEmodel.didate selection.
The experiments show that up to72% of the phrase table can be discarded withoutsignificant decrease on the translation quality, andin decoding with phrasal semantic similarities upto 1.7 BLEU score improvement over the state-of-the-art baseline can be achieved.In addition, our semantic phrase embeddingshave many other potential applications.
For in-stance, the semantic phrase embeddings can bedirectly fed to DNN to model the decoding pro-cess.
Besides SMT, the semantic phrase embed-dings can be used in other cross-lingual tasks (e.g.cross-lingual question answering) and monolin-gual applications such as textual entailment, ques-tion answering and paraphrase detection.2 Related WorkRecently, phrase embedding has drawn more andmore attention.
There are three main perspectiveshandling this task in monolingual languages.One method considers the phrases as bag-of-words and employs a convolution model to trans-form the word embeddings to phrase embeddings(Collobert et al, 2011; Kalchbrenner and Blun-som, 2013).
Gao et al (2013) also use bag-of-words but learn BLEU sensitive phrase embed-dings.
This kind of approaches does not take theword order into account and loses much informa-tion.
Instead, our bilingually-constrained recur-sive auto-encoders not only learn the compositionmechanism of generating phrases from words, butalso fine tune the word embeddings during themodel training stage, so that we can induce the fullinformation of the phrases and internal words.Another method (Mikolov et al, 2013) dealswith the phrases having a meaning that is not asimple composition of the meanings of its indi-vidual words, such as New York Times.
They firstfind the phrases of this kind.
Then, they regardthese phrases as indivisible units, and learn theirembeddings with the context information.
How-112ever, this kind of phrase embedding is hard to cap-ture full semantics since the context of a phraseis limited.
Furthermore, this method can only ac-count for a very small part of phrases, since mostof the phrases are compositional.
In contrast, ourmethod attempts to learn the semantic vector rep-resentation for any phrase.The third method views any phrase as the mean-ingful composition of its internal words.
The re-cursive auto-encoder is typically adopted to learnthe way of composition (Socher et al, 2010;Socher et al, 2011; Socher et al, 2013a; Socheret al, 2013b; Li et al, 2013).
They pre-train theRAE with an unsupervised algorithm.
And then,they fine-tune the RAE according to the label ofthe phrase, such as the syntactic category in pars-ing (Socher et al, 2013a), the polarity in sentimentanalysis (Socher et al, 2011; Socher et al, 2013b),and the reordering pattern in SMT (Li et al, 2013).This kind of semi-supervised phrase embedding isin fact performing phrase clustering with respectto the phrase label.
For example, in the RAE-based phrase reordering model for SMT (Li etal., 2013), the phrases with the similar reorder-ing tendency (e.g.
monotone or swap) are closeto each other in the embedding space, such as theprepositional phrases.
Obviously, this kind meth-ods of semi-supervised phrase embedding do notfully address the semantic meaning of the phrases.Although we also follow the composition-basedphrase embedding, we are the first to focus onthe semantic meanings of the phrases and proposea bilingually-constrained model to induce the se-mantic information and learn transformation of thesemantic space in one language to the other.3 Bilingually-constrained RecursiveAuto-encodersThis section introduces the Bilingually-constrained Recursive Auto-encoders (BRAE),that is inspired by two observations.
First, therecursive auto-encoder provides a reasonablecomposition mechanism to embed each phrase.And the semi-supervised phrase embedding(Socher et al, 2011; Socher et al, 2013a; Li etal., 2013) further indicates that phrase embeddingcan be tuned with respect to the label.
Second,even though we have no correct semantic phraserepresentation as the gold label, the phrasessharing the same meaning provide an indirect butfeasible way.x1 x2 x3 x4y1=f(W(1)[x1; x2]+b)y2=f(W(1)[y1; x3]+b)y3=f(W(1)[y2; x4]+b)Figure 2: A recursive auto-encoder for a four-word phrase.
The empty nodes are the reconstruc-tions of the input.We will first briefly present the unsupervisedphrase embedding, and then describe the semi-supervised framework.
After that, we introducethe BRAE on the network structure, objectivefunction and parameter inference.3.1 Unsupervised Phrase Embedding3.1.1 Word Vector RepresentationsIn phrase embedding using composition, the wordvector representation is the basis and serves as theinput to the neural network.
After learning wordembeddings with DNN (Bengio et al, 2003; Col-lobert and Weston, 2008; Mikolov et al, 2013),each word in the vocabulary V corresponds to avector x ?
Rn, and all the vectors are stacked intoan embedding matrix L ?
Rn?|V |.Given a phrase which is an ordered list of mwords, each word has an index i into the columnsof the embedding matrix L. The index i is used toretrieve the word?s vector representation using asimple multiplication with a binary vector e whichis zero in all positions except for the ith index:xi= Lei?
Rn(1)Note that n is usually set empirically, such as n =50, 100, 200.
Throughout this paper, n = 3 is usedfor better illustration as shown in Fig.
1.3.1.2 RAE-based Phrase EmbeddingAssuming we are given a phrase w1w2?
?
?wm,it is first projected into a list of vectors(x1, x2, ?
?
?
, xm) using Eq.
1.
The RAE learnsthe vector representation of the phrase by recur-sively combining two children vectors in a bottom-up manner (Socher et al, 2011).
Fig.
2 illustratesan instance of a RAE applied to a binary tree, in113which a standard auto-encoder (in box) is re-usedat each node.
The standard auto-encoder aims atlearning an abstract representation of its input.
Fortwo children c1= x1and c2= x2, the auto-encoder computes the parent vector y1as follows:p = f(W(1)[c1; c2] + b(1)) (2)Where we multiply the parameter matrix W(1)?Rn?2nby the concatenation of two children[c1; c2] ?
R2n?1.
After adding a bias term b(1),we apply an element-wise activation function suchas f = tanh(?
), which is used in our experiments.In order to apply this auto-encoder to each pair ofchildren, the representation of the parent p shouldhave the same dimensionality as the ci?s.To assess how well the parent?s vector repre-sents its children, the standard auto-encoder recon-structs the children in a reconstruction layer:[c?1; c?2] = f(2)(W(2)p+ b(2)) (3)Where c?1and c?2are reconstructed children, W(2)and b(2)are parameter matrix and bias term for re-construction respectively, and f(2)= tanh(?
).To obtain the optimal abstract representation ofthe inputs, the standard auto-encoder tries to min-imize the reconstruction errors between the inputsand the reconstructed ones during training:Erec([c1; c2]) =12||[c1; c2]?
[c?1; c?2]||2(4)Given y1= p, we can use Eq.
2 again to com-pute y2by setting the children to be [c1; c2] =[y1;x3].
The same auto-encoder is re-used untilthe vector of the whole phrase is generated.For unsupervised phrase embedding, the onlyobjective is to minimize the sum of reconstructionerrors at each node in the optimal binary tree:RAE?
(x) = argminy?A(x)?s?yErec([c1; c2]s) (5)Where x is the list of vectors of a phrase, andA(x)denotes all the possible binary trees that can bebuilt from inputs x.
A greedy algorithm (Socheret al, 2011) is used to generate the optimal binarytree y.
The parameters ?
= (W, b) are optimizedover all the phrases in the training data.3.2 Semi-supervised Phrase EmbeddingThe above RAE is completely unsupervised andcan only induce general representations of theReco nstr uctio n Erro r  Pred ictio n Erro rW (1)W (2)  W (label)Figure 3: An illustration of a semi-supervisedRAE unit.
Red nodes show the label distribution.multi-word phrases.
Several researchers extendthe original RAEs to a semi-supervised setting sothat the induced phrase embedding can predict atarget label, such as polarity in sentiment analysis(Socher et al, 2011), syntactic category in parsing(Socher et al, 2013a) and phrase reordering pat-tern in SMT (Li et al, 2013).In the semi-supervised RAE for phrase embed-ding, the objective function over a (phrase, label)pair (x, t) includes the reconstruction error and theprediction error, as illustrated in Fig.
3.E(x, t; ?)
= ?Erec(x, t; ?)+(1??
)Epred(x, t; ?
)(6)Where the hyper-parameter ?
is used to balancethe reconstruction and prediction error.
For labelprediction, the cross-entropy error is usually usedto calculate Epred.
By optimizing the above ob-jective, the phrases in the vector embedding spacewill be grouped according to the labels.3.3 The BRAE ModelWe know from the semi-supervised phrase embed-ding that the learned vector representation can bewell adapted to the given label.
Therefore, we canimagine that learning semantic phrase embeddingis reasonable if we are given gold vector represen-tations of the phrases.However, no gold semantic phrase embeddingexists.
Fortunately, we know the fact that thetwo phrases should share the same semantic rep-resentation if they express the same meaning.
Wecan make inference from this fact that if a modelcan learn the same embedding for any phrase pairsharing the same meaning, the learned embeddingmust encode the semantics of the phrases and thecorresponding model is our desire.As translation equivalents share the same se-mantic meaning, we employ high-quality phrasetranslation pairs as training corpus in thiswork.
Accordingly, we propose the Bilingually-constrained Recursive Auto-encoders (BRAE),114Sour ce Recons truction Err o rSour ce Prediction Err o rW s (1)W s (2)  W s (label)Ta rg et Recons truction Err o rW t (1)W t (2)W t (label)  Ta rg et Prediction Err o rSource Language Phrase Target Language PhraseFigure 4: An illustration of the bilingual-constrained recursive auto-encoders.
The twophrases are translations with each other.whose basic goal is to minimize the semantic dis-tance between the phrases and their translations.3.3.1 The Objective FunctionUnlike previous methods, the BRAE model jointlylearns two RAEs (Fig.
4 shows the network struc-ture): one for source language and the other fortarget language.
For a phrase pair (s, t), two kindsof errors are involved:1. reconstruction errorErec(s, t; ?
): how wellthe learned vector representations psand ptrepre-sent the phrase s and t respectively?Erec(s, t; ?)
= Erec(s; ?)
+ Erec(t; ?)
(7)2. semantic error Esem(s, t; ?
): what is thesemantic distance between the learned vector rep-resentations psand pt?Since word embeddings for two languages arelearned separately and locate in different vectorspace, we do not enforce the phrase embeddingsin two languages to be in the same semantic vectorspace.
We suppose there is a transformation be-tween the two semantic embedding spaces.
Thus,the semantic distance is bidirectional: the distancebetween ptand the transformation of ps, and thatbetween psand the transformation of pt.
As a re-sult, the overall semantic error becomes:Esem(s, t; ?)
= Esem(s|t, ?)
+ Esem(t|s, ?)
(8)Where Esem(s|t, ?)
= Esem(pt, f(Wlsps+ bls))means the transformation of psis performed asfollows: we first multiply a parameter matrix Wlsby ps, and after adding a bias term blswe applyan element-wise activation function f = tanh(?
).Finally, we calculate their Euclidean distance:Esem(s|t, ?)
=12||pt?
f(Wlsps+ bls)||2(9)Esem(t|s, ?)
can be calculated in exactly the sameway.
For the phrase pair (s, t), the joint error is:E(s, t; ?)
= ?Erec(s, t; ?)
+ (1??
)Esem(s, t; ?
)(10)The hyper-parameter ?
weights the reconstructionand semantic error.
The final BRAE objective overthe phrase pairs training set (S, T ) becomes:JBRAE=1N?(s,t)?
(S,T )E(s, t; ?
)+?2||?||2(11)3.3.2 Max-Semantic-Margin ErrorIdeally, we want the learned BRAE model canmake sure that the semantic error for the positiveexample (a source phrase s and its correct transla-tion t) is much smaller than that for the negativeexample (the source phrase s and a bad translationt?).
However, the current model cannot guaranteethis since the above semantic error Esem(s|t, ?
)only accounts for positive ones.We thus enhance the semantic error with bothpositive and negative examples, and the corre-sponding max-semantic-margin error becomes:E?sem(s|t, ?)
= max{0, Esem(s|t, ?)?
Esem(s|t?, ?)
+ 1}(12)It tries to minimize the semantic distance betweentranslation equivalents and maximize the semanticdistance between non-translation pairs simultane-ously.
Using the above error function, we needto construct a negative example for each positiveexample.
Suppose we are given a positive exam-ple (s, t), the correct translation t can be convertedinto a bad translation t?by replacing the wordsin t with randomly chosen target language words.Then, a negative example (s, t?)
is available.3.3.3 Parameter InferenceLike semi-supervised RAE (Li et al, 2013), theparameters ?
in our BRAE model can also be di-vided into three sets:?L: word embedding matrix L for two lan-guages (Section 3.1.1);?rec: recursive auto-encoder parameter matricesW(1), W(2), and bias terms b(1), b(2)for two lan-guages (Section 3.1.2);?sem: transformation matrix Wland bias termblfor two directions in semantic distance compu-tation (Section 3.3.1).115To have a deep understanding of the parameters,we rewrite Eq.
10:E(s, t; ?)
= ?
(Erec(s; ?)
+ Erec(t; ?
))+ (1?
?
)(E?sem(s|t, ?)
+ E?sem(t|s, ?
))= (?Erec(s; ?s) + (1?
?
)E?sem(s|t, ?s))+ (?Erec(t; ?t) + (1?
?
)E?sem(t|s, ?t))(13)We can see that the parameters ?
can be dividedinto two classes: ?sfor the source language and ?tfor the target language.
The above equation alsoindicates that the source-side parameters ?scan beoptimized independently as long as the semanticrepresentation ptof the target phrase t is given tocompute Esem(s|t, ?)
with Eq.
9.
It is similar forthe target-side parameters ?t.Assuming the target phrase representation ptis available, the optimization of the source-sideparameters is similar to that of semi-supervisedRAE.
We apply the Stochastic Gradient Descent(SGD) algorithm to optimize each parameter:?s= ?s?
??Js?
?s(14)In order to run SGD algorithm, we need to solvetwo problems: one for parameter initialization andthe other for partial gradient calculation.In parameter initialization, ?recand ?semfor thesource language is randomly set according to anormal distribution.
For the word embedding Ls,there are two choices.
First, Lsis initialized ran-domly like other parameters.
Second, the wordembedding matrix Lsis pre-trained with DNN(Bengio et al, 2003; Collobert and Weston, 2008;Mikolov et al, 2013) using large-scale unlabeledmonolingual data.
We prefer to the second onesince this kind of word embedding has alreadyencoded some semantics of the words.
In thiswork, we employ the toolkit Word2Vec (Mikolovet al, 2013) to pre-train the word embedding forthe source and target languages.
The word em-beddings will be fine-tuned in our BRAE model tocapture much more semantics.The partial gradient for one instance is com-puted as follows:?Js?
?s=?E(s|t, ?s)?
?s+ ?
?s(15)Where the source-side error given the target phraserepresentation includes reconstruction error andupdated semantic error:E(s|t, ?s) = ?Erec(s; ?s) + (1??
)E?sem(s|t, ?s)(16)Given the current ?s, we first construct the binarytree (as illustrated in Fig.
2) for any source-sidephrase using the greedy algorithm (Socher et al,2011).
Then, the derivatives for the parameters inthe fixed binary tree will be calculated via back-propagation through structures (Goller and Kuch-ler, 1996).
Finally, the parameters will be updatedusing Eq.
14 and a new ?sis obtained.The target-side parameters ?tcan be optimizedin the same way as long as the source-side phraserepresentation psis available.
It seems a para-dox that updating ?sneeds ptwhile updating ?tneeds ps.
To solve this problem, we propose anco-training style algorithm which includes threesteps:1.
Pre-training: applying unsupervised phraseembedding with standard RAE to pre-train thesource- and target-side phrase representations psand ptrespectively (Section 2.1.2);2.
Fine-tuning: with the BRAE model, us-ing target-side phrase representation ptto updatethe source-side parameters ?sand obtain the fine-tuned source-side phrase representation p?s, andmeanwhile using psto update ?tand get the fine-tuned p?t, and then calculate the joint error over thetraining corpus;3.
Termination Check: if the joint errorreaches a local minima or the iterations reachthe pre-defined number (25 is used in our exper-iments), we terminate the training procedure, oth-erwise we set ps= p?s, pt= p?t, and go to step2.4 ExperimentsWith the semantic phrase embeddings and the vec-tor space transformation function, we apply theBRAE to measure the semantic similarity betweena source phrase and its translation candidates inthe phrase-based SMT.
Two tasks are involved inthe experiments: phrase table pruning that dis-cards entries whose semantic similarity is very lowand decoding with the phrasal semantic similari-ties as additional new features.4.1 Hyper-Parameter SettingsThe hyper-parameters in the BRAE model includethe dimensionality of the word embedding n in Eq.1, the balance weight ?
in Eq.
10, ?s in Eq.
11,and the learning rate ?
in Eq.
14.For the dimensionality n, we have tried threesettings n = 50, 100, 200 in our experiments.
We116empirically set the learning rate ?
= 0.01.
Wedraw ?
from 0.05 to 0.5 with step 0.05, and ?sfrom {10?6, 10?5, 10?4, 10?3, 10?2}.
The over-all error of the BRAE model is employed to guidethe search procedure.
Finally, we choose ?
=0.15, ?L= 10?2, ?rec= 10?3and ?sem= 10?3.4.2 SMT SetupWe have implemented a phrase-based translationsystem with a maximum entropy based reorderingmodel using the bracketing transduction grammar(Wu, 1997; Xiong et al, 2006).The SMT evaluation is conducted on Chinese-to-English translation.
Accordingly, our BRAEmodel is trained on Chinese and English.
Thebilingual training data from LDC2contains 0.96Msentence pairs and 1.1M entity pairs with 27.7MChinese words and 31.9M English words.
A 5-gram language model is trained on the Xinhua por-tion of the English Gigaword corpus and the En-glish part of bilingual training data.
The NISTMT03 is used as the development data.
NISTMT04-06 and MT08 (news data) are used as thetest data.
Case-insensitive BLEU is employedas the evaluation metric.
The statistical signif-icance test is performed by the re-sampling ap-proach (Koehn, 2004).In addition, we pre-train the word embeddingwith toolkit Word2Vec on large-scale monolingualdata including the aforementioned data for SMT.The monolingual data contains 1.06B words forChinese and 1.12B words for English.
To ob-tain high-quality bilingual phrase pairs to trainour BRAE model, we perform forced decodingfor the bilingual training sentences and collect thephrase pairs used.
After removing the duplicates,the remaining 1.12M bilingual phrase pairs (lengthranging from 1 to 7) are obtained.4.3 Phrase Table PruningPruning most of the phrase table without muchimpact on translation quality is very importantfor translation especially in environments wherememory and time constraints are imposed.
Manyalgorithms have been proposed to deal with thisproblem, such as significance pruning (Johnson etal., 2007; Tomeh et al, 2009), relevance prun-ing (Eck et al, 2007) and entropy-based pruning2LDC category numbers: LDC2000T50, LDC2002L27,LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06,LDC2005T10 and LDC2005T34.
(Ling et al, 2012; Zens et al, 2012).
These algo-rithms are based on corpus statistics including co-occurrence statistics, phrase pair usage and com-position information.
For example, the signifi-cance pruning, which is proven to be a very ef-fective algorithm, computes the probability namedp-value, that tests whether a source phrase s and atarget phrase t co-occur more frequently in a bilin-gual corpus than they happen just by chance.
Thehigher the p-value, the more likely of the phrasepair to be spurious.Our work has the same objective, but instead ofusing corpus statistics, we attempt to measure thequality of the phrase pair from the view of seman-tic meaning.
Given a phrase pair (s, t), the BRAEmodel first obtains their semantic phrase represen-tations (ps, pt), and then transforms psinto targetsemantic space ps?, ptinto source semantic spacept?.
We finally get two similarities Sim(ps?, pt)and Sim(pt?, ps).
Phrase pairs that have a lowsimilarity are more likely to be noise and moreprone to be pruned.
In experiments, we discardthe phrase pair whose similarity in two directionsare smaller than a threshold3.Table 1 shows the comparison results betweenour BRAE-based pruning method and the signif-icance pruning algorithm.
We can see a commonphenomenon in both of the algorithms: for the firstfew thresholds, the phrase table becomes smallerand smaller while the translation quality is notmuch decreased, but the performance jumps a lotat a certain threshold (16 for Significance pruning,0.8 for BRAE-based one).Specifically, the Significance algorithm cansafely discard 64% of the phrase table at its thresh-old 12 with only 0.1 BLEU loss in the overalltest.
In contrast, our BRAE-based algorithm canremove 72% of the phrase table at its threshold0.7 with only 0.06 BLEU loss in the overall eval-uation.
When the two algorithms using a similarportion of the phrase table4(35% in BRAE and36% in Significance), the BRAE-based algorithmoutperforms the Significance algorithm on all thetest sets except for MT04.
It indicates that ourBRAE model is a good alternative for phrase tablepruning.
Furthermore, our model is much more in-3To avoid the situation that all the translation candidatesfor a source phrase are pruned, we always keep the first 10best according to the semantic similarity.4In the future, we will compare the performance by en-forcing the two algorithms to use the same portion of phrasetable117Method Threshold PhraseTable MT03 MT04 MT05 MT06 MT08 ALLBaseline 100% 35.81 36.91 34.69 33.83 27.17 34.82BRAE0.4 52% 35.94 36.96 35.00 34.71 27.77 35.160.5 44% 35.67 36.59 34.86 33.91 27.25 34.890.6 35% 35.86 36.71 34.93 34.63 27.34 35.050.7 28% 35.55 36.62 34.57 33.97 27.10 34.760.8 20% 35.06 36.01 34.13 33.04 26.66 34.04Significance8 48% 35.86 36.99 34.74 34.53 27.59 35.1312 36% 35.59 36.73 34.65 34.17 27.16 34.7216 25% 35.19 36.24 34.26 33.32 26.55 34.0920 18% 35.05 36.09 34.02 32.98 26.37 33.97Table 1: Comparison between BRAE-based pruning and Significance pruning of phrase table.
Thresholdmeans similarity in BRAE and negative-log-p-value in Significance.
?ALL?
combines the developmentand test sets.
Bold numbers denote that the result is better than or comparable to that of baseline.
n = 50is used for embedding dimensionality.tuitive because it is directly based on the semanticsimilarity.4.4 Decoding with Phrasal SemanticSimilaritiesBesides using the semantic similarities to prunethe phrase table, we also employ them as two in-formative features like the phrase translation prob-ability to guide translation hypotheses selectionduring decoding.
Typically, four translation prob-abilities are adopted in the phrase-based SMT, in-cluding phrase translation probability and lexicalweights in both directions.
The phrase transla-tion probability is based on co-occurrence statis-tics and the lexical weights consider the phrase asbag-of-words.
In contrast, our BRAE model fo-cuses on compositional semantics from words tophrases.
Therefore, the semantic similarities com-puted using our BRAE model are complementaryto the existing four translation probabilities.The semantic similarities in two directionsSim(ps?, pt) and Sim(pt?, ps) are integrated intoour baseline phrase-based model.
In order to in-vestigate the influence of the dimensionality of theembedding space, we have tried three different set-tings n = 50, 100, 200.As shown in Table 2, no matter what n is, theBRAE model can significantly improve the trans-lation quality in the overall test data.
The largestimprovement can be up to 1.7 BLEU score (MT06for n = 50).
It is interesting that with dimen-sionality growing, the translation performance isnot consistently improved.
We speculate that us-ing n = 50 or n = 100 can already distinguishgood translation candidates from bad ones.4.5 Analysis on Semantic Phrase EmbeddingTo have a better intuition about the power of theBRAE model at learning semantic phrase embed-dings, we show some examples in Table 3.
Giventhe BRAE model and the phrase training set, wesearch from the set the most semantically similarEnglish phrases for any new input English phrase.The input phrases contain different number ofwords.
The table shows that the unsupervisedRAE can at most capture the syntactic propertywhen the phrases are short.
For example, theunsupervised RAE finds do not want for the in-put phrase do not agree.
When the phrase be-comes longer, the unsupervised RAE cannot evencapture the syntactic property.
In contrast, ourBRAE model learns the semantic meaning foreach phrase no matter whether it is short or rel-atively long.
This indicates that the proposedBRAE model is effective at learning semanticphrase embeddings.5 Discussions5.1 Applications of The BRAE modelAs the semantic phrase embedding can fully rep-resent the phrase, we can go a step further in thephrase-based SMT and feed the semantic phraseembeddings to DNN in order to model the wholetranslation process (e.g.
derivation structure pre-diction).
We will explore this direction in our fu-ture work.
Besides SMT, the semantic phrase em-beddings can be used in other cross-lingual tasks,such as cross-lingual question answering, sincethe semantic similarity between phrases in differ-ent languages can be calculated accurately.In addition to the cross-lingual applications, webelieve the BRAE model can be applied in many118Method n MT03 MT04 MT05 MT06 MT08 ALLBaseline 35.81 36.91 34.69 33.83 27.17 34.82BRAE50 36.43 37.64 35.35 35.53 28.59 35.84+100 36.45 37.44 35.58 35.42 28.57 36.03+200 36.34 37.35 35.78 34.87 27.84 35.62+Table 2: Experimental results of decoding with phrasal semantic similarities.
n is the embedding dimen-sionality.
?+?
means that the model significantly outperforms the baseline with p < 0.01.New Phrase Unsupervised RAE BRAEmilitary forcecore force military powermain force military strengthlabor force armed forcesat a meetingto a meeting at the meetingat a rate during the meetinga meeting , at the conferencedo not agreeone can accept do not favori can understand will not compromisedo not want not to approveeach people in this nationeach country regards every citizen in this countryeach country has its all the people in the countryeach other , and people all over the countryTable 3: Semantically similar phrases in the training set for the new phrases.monolingual NLP tasks which depend on goodphrase representations or semantic similarity be-tween phrases, such as named entity recognition,parsing, textual entailment, question answeringand paraphrase detection.5.2 Model ExtensionsIn fact, the phrases having the same meaning aretranslation equivalents in different languages, butare paraphrases in one language.
Therefore, ourmodel can be easily adapted to learn semanticphrase embeddings using paraphrases.Our BRAE model still has some limitations.For example, as each node in the recursive auto-encoder shares the same weight matrix, the BRAEmodel would become weak at learning the seman-tic representations for long sentences with tens ofwords.
Improving the model to semantically em-bed sentences is left for our future work.6 Conclusions and Future WorkThis paper has explored the bilingually-constrained recursive auto-encoders in learningphrase embeddings, which can distinguish phraseswith different semantic meanings.
With the ob-jective to minimize the semantic distance betweentranslation equivalents and maximize the semanticdistance between non-translation pairs simultane-ously, the learned model can semantically embedany phrase in two languages and can transformthe semantic space in one language to the other.Two end-to-end SMT tasks are involved to testthe power of the proposed model at learning thesemantic phrase embeddings.
The experimentalresults show that the BRAE model is remarkablyeffective in phrase table pruning and decodingwith phrasal semantic similarities.We have also discussed many other potential ap-plications and extensions of our BRAE model.
Inthe future work, we will explore four directions.1) we will try to model the decoding process withDNN based on our semantic embeddings of thebasic translation units.
2) we are going to learnsemantic phrase embeddings with the paraphrasecorpus.
3) we will apply the BRAE model in othermonolingual and cross-lingual tasks.
4) we plan tolearn semantic sentence embeddings by automati-cally learning different weight matrices for differ-ent nodes in the BRAE model.AcknowledgmentsWe thank Nan Yang for sharing the baselinecode and anonymous reviewers for their valu-able comments.
The research work has beenpartially funded by the Natural Science Founda-tion of China under Grant No.
61333018 and61303181, and Hi-Tech Research and Develop-ment Program (863 Program) of China underGrant No.
2012AA011102.119ReferencesMichael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint language and translationmodeling with recurrent neural networks.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1044?1054.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Yoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
In In-novations in Machine Learning, pages 137?186.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.George E Dahl, Dong Yu, Li Deng, and Alex Acero.2012.
Context-dependent pre-trained deep neuralnetworks for large-vocabulary speech recognition.IEEE Transactions on Audio, Speech, and LanguageProcessing, 20(1):30?42.Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-jime Tsukada.
2013.
Adaptation data selection us-ing neural language models: Experiments in ma-chine translation.
In 51st Annual Meeting of the As-sociation for Computational Linguistics, pages 678?683.Matthias Eck, Stephen Vogal, and Alex Waibel.
2007.Estimating phrase pair relevance for translationmodel pruning.
In MTSummit XI.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2013.
Learning semantic representationsfor the phrase translation model.
arXiv preprintarXiv:1312.0482.Christoph Goller and Andreas Kuchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In IEEE Inter-national Conference on Neural Networks, volume 1,pages 347?352.John Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qualityby discarding most of the phrasetable.
In Proceed-ings of EMNLP.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1700?1709.Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,Karol Gregor, Micha?el Mathieu, and Yann L Cun.2010.
Learning convolutional feature hierarchies forvisual recognition.
In Advances in neural informa-tion processing systems, pages 1090?1098.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388?395.Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.2012.
Imagenet classification with deep convolu-tional neural networks.
In Advances in Neural Infor-mation Processing Systems 25, pages 1106?1114.Peng Li, Yang Liu, and Maosong Sun.
2013.
Recur-sive autoencoders for itg-based translation.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.Wang Ling, Joao Grac?a, Isabel Trancoso, and AlanBlack.
2012.
Entropy-based pruning for phrase-based machine translation.
In Proceedings of the2012 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning, pages 962?971.Lemao Liu, Taro Watanabe, Eiichiro Sumita, andTiejun Zhao.
2013.
Additive neural networks forstatistical machine translation.
In 51st Annual Meet-ing of the Association for Computational Linguis-tics, pages 791?801.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
In Proceedings of NIPS.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013a.
Parsing with composi-tional vector grammars.
In Proceedings of ACL.120Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013b.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.2009.
Complexity-based phrase-table filtering forstatistical machine translation.
In Proceedings ofSummit XII, pages 144?151.Ashish Vaswani, Yinggong Zhao, Victoria Fossum,and David Chiang.
2013.
Decoding with large-scale neural language models improves translation.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages1387?1392.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational linguistics, 23(3):377?403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of ACL-COLING, pages 505?512.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-hai Yu.
2013.
Word alignment modeling with con-text dependent deep neural network.
In 51st AnnualMeeting of the Association for Computational Lin-guistics.Richard Zens, Daisy Stanton, and Peng Xu.
2012.
Asystematic comparison of phrase table pruning tech-niques.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 972?983.Will Y Zou, Richard Socher, Daniel Cer, and Christo-pher D Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing, pages 1393?1398.121
