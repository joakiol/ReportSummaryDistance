Last WordsOn Becoming a DisciplineMark Steedman?University of EdinburghThe title of this column, Last Words, reminds me of an occasion in 2005, when I had theprivilege of attending the award ceremony for the prestigious Benjamin FranklinMedal,given annually to a few scientists who have made outstanding lifetime contributions toscience.
This time, a computational linguist, Aravind Joshi, was among them, so severalpast, present, and future presidents and officers of the ACL joined the Great and theGood at the ceremony at the Franklin Institute in Philadelphia.The eight medal recipients were each represented by a short video presentation,which mostly consisted of voice-over by a narrator, interspersed with sound-bites fromthe recipients about their life and work, in the last of which they had clearly been askedto deliver as their last words a brief take-home message.I couldn?t help noticing that the warmest applause was reserved for the physicist, adistinguished pioneer of string theory.
I was initially puzzled by the enthusiasm on thepart of a mostly lay audience for such theoretical work, which for all its elegance andbeauty, could not (as far as I could see) be expected to have nearly as much impact ontheir everyday lives as that of some of the other recipients, who that year included notonly Aravind, but another computer scientist whose impact on information processingwill be obvious to the members of ACL, Andrew Viterbi.But then I recalled that the physicist?s take-home message had had nothing to dowith string theory.
This admirable man?s last words to us had been the following:Everything is made of particles.
So physics is very important.1.
The Public Image of a ScienceI realized then that what we were applauding was not a physicist, or the beauty ofstring theory, but physics itself.
I was reminded that physicists are looked on as publicintellectuals who can be relied on to deliver Truth, and who are generally assumedto be doing A Good Job, even when they say that the universe must really be ten-dimensional, except that exactly six of the dimensions are curled up so tightly that thereis no conceivable way of detecting them, nor any prospect of bringing to bear the hugeenergies that would be required to straighten them out a bit so we could take a look atthem.
(As many of you will know, I Am Not Making This Up.)?
School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh UK EH8 9LW.E-mail: steedman@inf.ed.ac.uk.
These remarks are excerpted from the author?s Presidential Addressto the 45th Annual Meeting of the ACL, Prague, June 2007.
I?m grateful to that audience; to Mona Diab,Julia Hockenmaier, Miles Osborne, and Bonnie Webber for comments and advice; and to Franz Ochfor Google Language Tools.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 1How different this public image is from that of computational linguistics, or ofartificial intelligence, and even core computer science.
To the extent that the publicthinks at all about what we do, they think of us as producing gadgets, such as amusingnew sorts of telephone.
Only the other day, a colleague was called up by someone in aneighboring department asking if we could mend his PC for him.
(?Don?t you have aLittle Man?
You used to have a Little Man.?
)As a result, computer science is continually subjected to governmental reviewsseeking assurance that we know what we are doing, and are doing enough for theeconomy.
Many of these reviews draw very negative conclusions?the 1966 report ofthe Automatic Language Processing Advisory Committee (ALPAC) of the US NationalAcademy of Sciences effectively shut down research in machine translation for overa decade, and was the main reason for ACL changing its name in 1968 from the As-sociation for Machine Translation and Computational Linguistics (AMTCL).
The 1973report of Sir James Lighthill (a fluid dynamicist known for his foundational work in thefield of aeroacoustics) to the UK Science Research Council (SRC) closed down artificialintelligence and NLP research for a decade, until the Alvey report decided that Britishindustry had fallen behind in this area, and opened it up again.
There have been manysimilar examples since then, though few as catastrophic.Nobody goes around telling physicists what not to work on, or setting up commis-sions chaired by complete outsiders (roboticists or computational linguists, perhaps?)
todecide whether physicists are earning their keep.
The physicists tell the goverment whatthey think it is right to do, and the government either funds it or it doesn?t.
Even whenit doesn?t, as in the case of the superconducting supercollider, it?s because they can?tafford it, or lack the political power, not because of low esteem.
How do the physicistsdo it?Of course, as Duke Ellington said when asked how he kept his band togetherthrough the Beatles?
era, you have to have a gimmick.
The physicists gave us atomicenergy and the bomb, so no one can ever suggest again that they do not deliver Bangfor the Buck, even when they actually don?t, as may well turn out to be the case for the pasttwenty years or so of research in string theory and supersymmetry.1However, it is farmore important that physics consists of a body of great empiricallyproven laws that all scientists recognize, from the laws of thermodynamics to thespecial and general theories of relativity and quantum theory.
This body of knowledgelends both authority, and a breadth of vision that transcends any individual physicist?swork and any individual theory, even if parts of it can be temporarily ignored whenconvenient.But we too have awesome devices.
Search engines have arguably changed people?slives at least as profoundly as atomic energy.
The statistical machine translation toolsthat Google launched around May 2006 with Arabic for all the world to freely beta-test, and which have since been extended to Chinese, Russian, Japanese, and Korean,imperfect as they are, may well have an even bigger impact.
Our colleagues in AI rejoicein beating international chess Grand Masters with Deep Blue, and boast of robots onMars and autonomous vehicles charging around the Mojave desert.
Computer sciencehas the Internet itself to show off.We too have discovered great truths?Zipf?s Law, Information Theory, the power ofstatistically approximate language models, the only-just-trans-context-free automata-theoretic level of natural languages, the surface-compositionality of natural language1 For a recent review of this question, see Smolin 2006.138Steedman Last Wordssemantics, and have dreams of a grand unified head dependency-driven Theory ofEverything that that will one day make probabilistic and deterministic componentswork together to yield Meaning.
How come we don?t get respect?The answer, I?m afraid, is that we haven?t yet managed to form ourselves into a dis-cipline, within which those important truths are respected and held beyond reasonabledoubt, in whose name we stand united in the eyes of the world.
Instead, our history hasbeen sectarian, with dominant factions seeking to suppress proposals that diverge toomuch from the current orthodoxy, until they too are overthrown.This tendency is very evident in computer science peer review of proposals tofunding agencies, which are notably harsher than in other sciences, and which havethe effect of making the agencies assume a shortage of good science in the field, so thatthey reduce funding accordingly.
Worse still, when we have theoretical disagreements,we play them out in public.
(The split between connectionists and the rest of AI/NLP isa case in point.
It is sad to note that similar internal dissension seems to have played arole in the adverse conclusions reached in both the ALPAC and Lighthill reports.
)The physicists don?t do this.
When they review proposals, they rate them excel-lent at around twice the rate in CS, even if it means not getting funded themselvesin the current round.
The funding agencies conclude that there is a surplus of goodscience there, and seek or allocate further funding for the next round.
When there aredisagreements?as there seem to be currently about string theory?they are arguedout behind closed doors, until a consensus can be reached and be presented to thegeneral public.
The biologists behaved similarly over a recent disagreement concerningmethods for sequencing the genome.One measure of our own fisiparous tendencies is that, while from the 1950s to the1980s, the information theoreticians and statistical modelers among us used to makecommon cause with the linguists, we have subsequently drifted apart.
We have come tobelieve that the linguists have forgotten Zipf?s law, which says that most of the variancein language behavior can be captured by a small part of the system.The linguists, on the other hand, think that it is we who have forgotten Zipf?s law,which also says that most of the information about the language system as a whole is in theLong Tail.It is we who are at fault here, because the machine learning techniques that we relyon are actually very bad at inducing systems for which the crucial information is in rareevents?like physics, for example.There is a grave danger here for the future of our putative discipline.
One day, eitherbecause of the demise of Moore?s law, or simply because we have done all the easy stuff,the Long Tail will come back to haunt us.
For example, consider the current state of ourformer nemesis MT.2.
Machine Translation Then and NowMachine translation (MT) was one of the earliest applications envisaged for computingmachinery.
Weaver (1949) identified the extreme ambiguity of natural language as thecentral problem of MT, and outlined as possible solutions not only the ?noisy channel?model of SMT and its basis in n-gram language models, investigated with Claude Shan-non (Shannon andWeaver 1949), but also the notion of parsing as logical deduction, andthe interlingua-based syntax-driven approach to MT, which he based on the linguists?notion of Universal Grammar.
(No sectarianism there, at least.
)It is interesting that, in that bright dawn, MT seemed likely to prove one of theeasier cognitive tasks to mimic by machine.
There seemed to be a real possibility that139Computational Linguistics Volume 34, Number 1there might be ways to bypass understanding altogether using simple purely syntacticand probabilistic devices.
Over the next few years, there was a gradual disillusion withthis expectation, for at least three reasons.First, Chomsky (1957) showed beyond a reasonable doubt that natural languagesnot only fell outside the class that could be completely recognized using finite-statemachines of the class implicit in Shannon?s models, but also fell outside the class thatcould be adequately represented by context-free grammars.
It wasn?t immediately clearhow to translate such grammars into computational terms, and they appeared to breakthe unity of grammar and probabilistic model that had been one of the attractions ofthe information-theoretic approach (although Chomsky was careful to acknowledge thepossibility of statistically approximating such grammars with Markov processes for thepurpose of reducing ambiguity).Second, it became clear that the grammars that were needed for real texts, evenwithin narrow genres like newspaper text or scientific papers, were very large indeed.The lack of computational transparency of grammar formalisms tended to give thehand-built grammars of this period the character of unstructured programs, with noapparent prospect of inducing them automatically.Third, the problem of ambiguity for grammars of this size was much worse thananyone had expected.As a result, opinion changed, and MT became seen as one of the hardest cognitiveproblems, requiring deep understanding of semantics and knowledge of the world, aswell as full syntactic processing.There is an often-repeated story about these days (one which has all the hallmarksof a myth) concerning a demonstration of an early Russian dictionary-based MT systemusing back-translation from English to Russian and back again.
According to legend,the demonstrators were disconcerted to find that the sentence Time flies like an arrowreturned as Time flies enjoy arrows, revealing a syntactic analysis of the sentence thatwould have been appropriate for the sentence Fruit flies like a banana.2Of course we now know that the problem of the 1950s and 1960s was the lackof language models, of a kind that is now commonplace, thanks to Moore?s law andHMMs, together with a method for learning such models and integrating them withstructural rules.
Some real progress has been made, and MT is once again thought of asan ?easy?
problem, that can be at least partly solved with relatively low-level methods.3.
An ExperimentSo, how much progress have we made?
We can repeat the back-translation experi-ment with Google Language Tools Beta n-gram-and-finite-state-transducer-based Ara-bic SMT.
(The first line is the English input, the Arabic is its SMT translation, the thirdline is a gloss of the Arabic words, and the last line is the result of translating the Arabicback again by SMT.
)2 There are many variants of this apocryphal story, involving other examples, all of which are likely tohave their origin in much older jests about human dictionary-based translation (Hutchins 1995).140Steedman Last WordsOn the basis of the result of the back-translation, this looks OK, give or takea bit of morphology, but of course end-to-end back-translation is a very weak test,where you can just get lucky.
Readers of Arabic will notice that the translation of likeis indeed a comparative, not a verb meaning enjoy, as in the legend.
However, theywill also notice that flies translates as the noun, rather than the verb, just as the storyforetold.If we try the same test on Fruit flies like a banana, the flies are still insects, but like isstill comparative, rather than a verb.
So the two sentences are analyzed the same way,as in the story.Of course, all this is very unfair, and not at all surprising.
The examples are out ofdomain, so the language model doesn?t help us at all.
So let?s try an in-domain exampleof newswire text.The following is almost the first text I found by searching Arabic Web pages forthe Arabic for ?Google Machine Translation,?
simply because I had already read theEnglish reference document,3 and I was pretty sure it would be out there somewhere.
Itis a human-authored Arabic translation of a recent Reuters story about the launching ofGoogle Language Tools, taken from Al Jazeera:4Here is the SMT translation, delivered in about the time it would take a native speakerto read the original:The German Franz Ouch which leads efforts Google translation computerfeeds hundreds of millions of words of parallel texts such as Arabic,3 http://www.reuters.com/article/technologyNews/idUSN1921881520070328?feedType=RSS%20target=.4 http://www.aljazeera.net/news/archive/archive?ArchiveId=1037015.141Computational Linguistics Volume 34, Number 1English, using documents of the United Nations and the European Unionkey sources.And how a new translation Ouch said that although the quality would notbe complete That was a good in the previous translation mechanism, andthat the correct translation mostly might be good enough for some tasks.He stated that more data be fed by the results were better.. .
.
He commended Miles Osborne Professor at the University ofEdinburgh, who died last year at work in the company?s efforts to Google,but he pointed out that the software will not prevail over people skilled intranslations as they do in the game of chess and should use software tounderstand and not to complete documents.The first thing to notice is that this is really very good.
It is quite clear what the story is,and you can even guess that what Franz Och actually said in the English reference textwas: ?The more data we feed into the system, the better it gets.?
It even seems to knowthat ?Google?
can be a verb.However, it exhibits all the problems to which we have always known MT is heir.Both pronouns ?he?
in the last paragraph will be understood as referring to Franz,whereas in the reference text it is Miles Osborne who does the commending and thepointing out.
Moreover, the alarming rumor of the latter?s death has been greatly exag-gerated by the English language-model: The reference text says he ?spent a sabbaticallast year working on the Google project.?
The human Arabic translation says much thesame, but the Arabic words for spent and died are homographs, and the newswire-basedmodel favors the latter.And of course, our friends the Construction Grammarians will gleefully point outthat the systemmakes a hash of the unbounded dependency in Franz?s use of what theycall the ?MORE-MORE?
construction.We can specifically probe the disability with respect to other kinds of unbound-ed dependencies, using back-translation on artificially generated (but in-domain)examples:142Steedman Last WordsComparing the examples that are translated correctly and those (labeled *) that are not,the generalization is already clear: even a 5-gram model can only handle root subjectrelative clauses.
Object relatives are beyond the horizon.
(These effects are robust undervariation of the content words.)
Here are some more challenging embedded examplesthat confirm the diagnosis:4. Who Cares?What does this tell us?
Nothing that we shouldn?t have already known.
We knew thatn-gram models and FSTs weren?t going to handle long-range dependencies, becauseChomsky told us so.
That?s one of the Big Truths of computational linguistics.Our situation is in fact rather like that of the physicists.
We have one theory fortalking about phenomena on a large scale, just as they have the General Theory ofRelativity, and another theory for talking about the very small scale, just as they haveQuantum Theory.
Like the physicists, we have difficulty in reconciling those theoreticallevels.
Like them, some of us think it?s fine to have two theories, whereas others of usthink it?s intolerable.The former kind of computational linguist will point out that long-range depen-dencies are sparse.
(There are around 20K *T* empty categories in around 16K ofthe roughly 40K sentences in the Penn Treebank, of which around 6K seem to benon-subject, non-sentential wh-traces of some kind.)
Worrying about them isn?t goingto significantly impact overall parser dependency recovery rates, much less n-gram-precision-based BLEU scores.
By the time we have fed enough data into the systemto make it know that spending a sabbatical at Google is more likely than dying there,and Moore?s law has made the machines exponentially bigger and faster, and fancieralgorithms allow us to deal with bigger n-grams, maybe this problem will go away.Maybe.
These are certainly good reasons for the IR and SMT researchers to keepworking at the large scale, giving the world these amazing search engines and transla-tion aids that give human beings vastly increased access to other cultures.
This is ourdiscipline?s equivalent in street-credibility terms of delivering atomic energy and robotson Mars.Nevertheless, to the other kind of computational linguist, it sounds depressinglylike getting better and better at recalling what is already well-known, and understand-ing what has often been said before.They point out that, in the long run, finite statemethods alonemay simply not work.Accuracy in most areas (WER in ASR, BLEU score in SMT, Eval-b for parsers) is at best143Computational Linguistics Volume 34, Number 1linear in the logarithm of the amount of training data.
Even optimistic extrapolationof current learning curves suggests truly incredible amounts of data will be needed(Lamel, Gauvain, and Adda 2002; Moore 2003; Knight and Koehn 2004; Brants et al2007).Moreover, the more of the local stuff we get right, the more users will come to trustthe software, and hence the more noticeable long range dependencies will become, andthe more upset people will get if they are deceived by a wrong analysis.What this should tell us is that the Long Tail is not mocked.
Long-range depen-dencies of the kind investigated above are semantically crucial.
Ignoring them disruptsall the other dependencies in those examples.
(They are also more frequent in genreslike questions.)
So we need to remember?and above all, teach our students?what ourdiscipline tells us the problem is, even when it?s not doing much for our BLEU score.In this connection, it is encouraging to see that many of the MT papers in the 2007 ACLexplicitly invoked syntax-level representations.But we need to do more than this.
To get respect, and avoid the risk of yet anotherAI/NLP Winter, we will need to pull ourselves together as a discipline, lift our eyesabove our rivalries, and take a longer view of where we are going (and where we havecome from) than seems to be the current style.
This will probably require a gradualmove to a more considered and authoritative style of publication, with journal articlestaking the place of hastily written and reviewed conference papers, as another authorof this column recently suggested.It will mean speaking with one voice, as the physicists and biologists do, andsupporting a diversity of views that transcends fashion and funding, wherever thescience is good.
It also means telling the public in honest terms how to think aboutwhat we do, what can be relied on and what the really hard problems are, in good timesand bad.
This should not be too difficult if we keep reminding them and ourselves ofthe following:Human knowledge is expressed in language.
So computational linguistics isvery important.ReferencesBrants, Thorsten, Ashok C. Popat, Peng Xu,Franz J. Och, and Jeffrey Dean.
2007.
Largelanguage models in machine translation.In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning (EMNLP-CoNLL),pages 858?867, Prague.Chomsky, Noam.
1957.
Syntactic Structures.Mouton, The Hague.Hutchins, John.
1995.
?The whisky wasinvisible?
or persistent myths of MT.MT News International, 11, 17?18.Knight, Kevin and Philipp Koehn.
2004.What?s new in statistical machinetranslation.
Tutorial, HLT/NAACL 2004.Lamel, Lori, J. L. Gauvain, and G. Adda.2002.
Unsupervised acoustic modeltraining.
In Proceedings of the IEEEConference on Acoustics, Speech, and SignalProcessing, pages 877?880, Orlando, FL.Moore, Roger.
2003.
A comparison of thedata requirements of automatic speechrecognition systems and human listeners.In Proceedings of Eurospeech Conference,pages 2582?2585, Geneva.Shannon, Claude and Warren Weaver.
1949.The Mathematical Theory of Communication.University of Illinois Press, Urbana.Smolin, Lee, ed.
2006.
The Trouble withPhysics.
Houghton Mifflin, New York.Weaver, Warren.
1949.
Translation.Manuscript, Rockefeller Institute.Reproduced as Weaver (1955).Weaver, Warren.
1955.
Translation.
InWilliam Locke and Andrew Booth, editors,Machine Translation of Languages: FourteenEssays.
MIT Press, Cambridge, MA,pages 15?23.144
