An Empirical Study of the Domain Dependence of SupervisedWord Sense Disambiguation Systems*Gerard  Escudero ,  L lu is  M~trquez~ and German R igauTALP Research Center.
LSI Department.
Universitat Polit~cnica de Catalunya (UPC)Jordi Girona Salgado 1-3.
E-08034 Barcelona.
Catalonia{escudero, lluism, g. rigau}@isi, upc.
esAbst ractThis paper describes a set of experiments car-ried out to explore the domain dependenceof alternative supervised Word Sense Disam-biguation algorithms.
The aim of the work isthreefold: studying the performance of thesealgorithms when tested on a different cor-pus from that they were trained on; explor-ing their ability to tune to new domains,and demonstrating empirically that the Lazy-Boosting algorithm outperforms tate-of-the-art supervised WSD algorithms in both previ-ous situations.Keywords:  Cross-corpus evaluation of Ni_Psystems, Word Sense Disambiguation, Super-vised Machine Learning1 In t roduct ionWord Sense Disambiguation (WSD) is theproblem of assigning the appropriate meaning(sense) to a given word in a text or discourse.Resolving the ambiguity of words is a centralproblem for large scale language understand-ing applications and their associate tasks (Ideand V4ronis, 1998), e.g., machine transla-tion, information retrieval, reference resolu-tion, parsing, etc.WSD is one of the most important openproblems in NLP.
Despite the wide range ofapproaches investigated and the large effortdevoted to tackle this problem, to date, nolarge-scale broad-coverage and highly accu-rate WSD system has been built --see themain conclusions of the first edition of Sen-sEval (Kilgarriff and Rosenzweig, 2000).One of the most successful current linesof research is the corpus-based approach in" This research has been partially funded by the Span-ish Research Department (CICYT's project TIC98-0423-C06).
by the EU Commission (NAMIC IST-1999-12392), and by the Catalan Research Depart-ment (CIRIT's consolidated research group 1999SGR-150 and CIRIT's grant 1999FI 00773).which statistical or Machine Learning (ML) al-gorithms are applied to learn statistical mod-els or classifiers from corpora in order to per-form WSD.
Generally, supervised approaches 1have obtained better results than unsuper-vised methods on small sets of selected am-biguous words, or artificial pseudo-words.Many standard M L algorithms for supervisedlearning have been applied, such as: DecisionLists (?arowsky, 1994; Agirre and Martinez,2000), Neural Networks (Towell and Voorhees,1998), Bayesian learning (Bruce and Wiebe,1999), Exemplar-Based learning (Ng, 1997a;Fujii et al, 1998), Boosting (Escudero et al,2000a), etc.
Unfortunately, there have beenvery few direct comparisons between alterna-tive methods for WSD.In general, supervised learning presumesthat the training examples are somehow re-flective of the task that will be performed bythe trainee on other data.
Consequently, theperformance of such systems is commonly es-timated by testing the algorithm on a separatepart of the set of training examples (say 10-20% of them), or by N-fold cross-validation,in which the set of examples i partitioned intoN disjoint sets (or folds), and the training-test procedure is repeated N times using allcombinations of N-1  folds for training and 1fold for testing.
In both cases, test examplesare different from those used for training, butthey belong to the same corpus, and, there-fore, they are expected to be quite similar.Although this methodology could be validfor certain NLP problems, such as EnglishPart-of-Speech tagging, we think that thereexists reasonable vidence to say that, inWSD, accuracy results cannot be simply ex-trapolated to other domains (contrary to theopinion of other authors (Ng, 1997b)): On theaSupervised approaches, also known as data-drivenor corpus-dmven, are those that learn from a previ-ously semantically annotated corpus.172one hand, WSD is very dependant to the do-main of application (Gale et al, 1992b) --seealso (Ng and Lee, 1996; Ng, 1997a), in whichquite different accuracy figures are obtainedwhen testing an exemplar-based WSD classi-fier on two different corpora.
Oi1 the otherhand, it does not seem reasonable to thinkthat the training material is large and repre-sentative nough to cover "all" potential typesof examples.To date, a thorough study of the domaindependence of WSD - - in  the style of otherstudies devoted to parsing (Sekine, 1997)--has not been carried out.
We think that suchan study is needed to assess the validity ofthe supervised approach, and to determine towhich extent a tuning process is necessary tomake real WSD systems portable.
In orderto corroborate the previous hypotheses, thispaper explores the portability and tuning offour different ML algorithms (previously ap-plied to WSD) by training and testing themon different corpora.Additionally, supervised methods sufferfrom the "knowledge acquisition bottle-neck" (Gale et al, 1992a).
(Ng, 1997b) esti-mates that the manual annotation effort nec-essary to build a broad coverage semanticallyannotated English corpus is about 16 person-years.
This overhead for supervision could bemuch greater if a costly tuning procedure isrequired before applying any existing systemto each new domain.Due to this fact, recent works have focusedon reducing the acquisition cost as well as theneed for supervision i  corpus-based methods.It is our belief that the research by (Leacock etal., 1998; Mihalcea and Moldovan, 1999) 2 pro-vide enough evidence towards the "opening"of the bottleneck in the near future.
For thatreason, it is worth further investigating therobustness and portability of existing super-vised ML methods to better resolve the WSDproblem.It is important o note that the focus ofthis work will be on the empirical cross-corpus evaluation of several M L supervised al-gorithms.
Other important issues, such as:selecting the best attribute set, discussing anappropriate definition of senses for the task,etc., are not addressed in this paper.eIn the line of using lexical resources and search en-gunes to automatically collect training examples fromlarge text collections or Internet.This paper is organized as follows: Section 2presents the four ML algorithms compared.In section 3 the setting is presented in de-tail, including the corpora and the experimen-tal methodology used.
Section 4 reports theexperiments carried out and the results ob-tained.
Finally, section 5 concludes and out-lines some lines for further esearch.2 Learn ing  A lgor i thms Tested2.1 Naive-Bayes (NB)Naive Bayes is intended as a simple represen-tative of statistical learning methods.
It hasbeen used in its most classical setting (Dudaand Hart, 1973).
That is, assuming indepen-dence of features, it classifies a new exampleby assigning the class that maximizes the con-ditional probability of the class given the ob-served sequence of features of that example.Model probabilities are estimated uringtraining process using relative frequencies.
Toavoid the effect of zero counts when esti-mating probabilities, a very simple smooth-ing technique has been used, which was pro-posed in (Ng, 1997a).
Despite its simplicity,Naive Bayes is claimed to obtain state-of-the-art accuracy on supervised WSD in many pa-pers (Mooney, 1996; Ng, 1997a; Leacock etal., 1998).2.2 Exemplar -based  Classif ier (EB)In Exemplar-based learning (Aha et al, 1991)no generalization of training examples is per-formed.
Instead, the examples are storedin memory and the classification of new ex-amples is based on the classes of the mostsimilar stored examples.
In our implemen-tation, all examples are kept in memory andthe classification of a new example is basedon a k-NN (Nearest-Neighbours) algorithmusing Hamming distance 3 to measure close-ness (in doing so, all examples are examined).For k's greater than 1, the resulting sense isthe weighted majority sense of the k near-est neighbours --where each example votes itssense with a strength proportional to its close-ness to the test example.In the experiments explained in section 4,the EB algorithm is run several times usingdifferent number of nearest neighbours (1, 3,SAlthough the use of MVDM metric (Cost andSalzberg, 1993) could lead to better results, currentimplementations have prohivitive computational over-heads(Escudero et al, 2000b)1735, 7, 10, 15, 20 and 25) and the results corre-sponding to the best choice are reported 4.Exemplar-based learning is said to be thebest option for VSD (Ng, 1997a).
Other au-thors (Daelemans et al, 1999) point out thatexemplar-based methods tend to be superiorin language learning problems because theydo not forget exceptions.2.3 Snow: A Winnow-based  Classif ierSnow stands for Sparse Network Of Winnows,and it is intended as a representative of on-line learning algorithms.The basic component is the Winnow al-gorithm (Littlestone, 1988).
It consists of alinear threshold algorithm with multiplicativeweight updating for 2-class problems, whichlearns very fast in the presence of many bi-nary input features.In the Snow architecture there is a winnownode for each class, which learns to separatethat class from all the rest.
During training,each example is considered a positive xamplefor winnow node associated to its class anda negative example for all the rest.
A keypoint that allows a fast learning is that thewinnow nodes are not connected to all featuresbut only to those that are "relevant" for theirclass.
When classifying a new example, Snowis similar to a neural network which takes theinput features and outputs the class with thehighest activation.Snow is proven to perform very well inhigh dimensional domains, where both, thetraining examples and the target function re-side very sparsely in the feature space (Roth,1998), e.g: text categorization, context-sensitive spelling correction, WSD, etc.In this paper, our approach to WSD usingSnow follows that of (Escudero et al, 2000c).2.4 LazyBoost ing  (LB)The main idea of boosting algorithms is tocombine many simple and moderately accu-rate hypotheses (called weak classifiers) intoa single, highly accurate classifier.
The weakclassifiers are trained sequentially and, con-ceptually, each of them is trained on the ex-amples which were most difficult to classifyby the preceding weak classifiers.
These weak4In order to construct a real EB-based system forWSD, the k parameter should be estimated by cross-validation using only the training set (Ng, 1997a),however, in our case, this cross-validation i side thecross-validation i volved in the testing process wouldgenerate a prohibitive overhead.hypotheses are then linearly combined into asingle rule called the combined hypothesis.More particularly, the Schapire and Singer'sreal AdaBoost.MH algorithm for multi-class multi-label classification (Schapire andSinger, to appear) has been used.
As in thatpaper, very simple weak hypotheses are used.They test the value of a boolean predicate andmake a real-valued prediction based on thatvalue.
The predicates used, which are the bi-narization of the attributes described in sec-tion 3.2, are of the form "f = v", where f is afeature and v is a value (e.g: "-r v" p e mus_word= hosp i ta l " ) .
Each weak rule uses a singlefeature, and, therefore, they can be seen assimple decision trees with one internal node(testing the value of a binary feature) and twoleaves corresponding to the yes/no answers tothat test.LazyBoosting (Escudero et al, 2000a), is asimple modification of the AdaBoost.MH al-gorithm, which consists of reducing the fea-ture space that is explored when learning eachweak classifier.
More specifically, a small pro-portion p of attributes are randomly selectedand the best weak rule is selected only amongthem.
The idea behind this method is thatif the proportion p is not too small, probablya sufficiently good rule can be found at eachiteration.
Besides, the chance for a good ruleto appear in the whole learning process is veryhigh.
Another important characteristic is thatno attribute needs to be discarded and, thus,the risk of eliminating relevant attributes isavoided.
The method seems to work quite wellsince no important degradation is observed inperformance for values of p greater or equalto 5% (this may indicate that there are manyirrelevant or highly dependant attributes inthe WSD domain).
Therefore, this modifica-tion significantly increases the efficiency of thelearning process (empirically, up to 7 timesfaster) with no loss in accuracy.3 Set t ing3.1 The  DSO CorpusThe DSO corpus is a semantically annotatedcorpus containing 192,800 occurrences of 121nouns and 70 verbs, corresponding to the mostfrequent and ambiguous English words.
Thiscorpus was collected by Ng and colleagues (Ngand Lee, 1996) and it is available from theLinguistic Data Consortium (LDC) 5.5LDC address: http://www.
Idc.upeaa.
ed~/174The D50 corpus contains sentences fromtwo different corpora, namely Wall StreetJournal (WSJ) and Brown Corpus (BC).Therefore, it is easy to perform experimentsabout the portability of alternative systemsby training them on the WSJ part and testingthem on the BE part, or vice-versa.
Here-inafter, the WSJ part of DSO will be referredto as corpus A, and the BC part to as corpus B.At a word level, we force the number of exam-ples of corpus A and B be the same 6 in orderto have symmetry and allow the comparisonin both directions.From these corpora, a group of 21 wordswhich frequently appear in the WSD litera-ture has been selected to perform the com-parative experiments (each word is treatedas a different classification problem).
Thesewords are 13 nouns (age, art, body, car, child,cost, head, interest, line, point, state, thing,work) and 8 verbs (become, fall, grow, lose,set, speak, strike, tell).
Table 1 contains in-formation about the number of examples, thenumber of senses, and the percentage of themost frequent sense (MF5) of these referencewords, grouped by nouns, verbs, and all 21words.3.2 At t r ibutesTwo kinds of information are used to performdisambiguation: local and topical context.Let "... w-3 w-2 w-1 w W+l w+2 w+3..."be the context of consecutive words aroundthe word w to be disambiguated, and p?,( -3  < i _< 3)be  the part-of-speech tagof word w?~.
Attributes referring to localcontext are the following 15: P-3, P-2,P- l ,  P+i, P+2, P+3, w- l ,  W+l, (W-2,W-1),(w-i.w+i), (w+l,w+2),(w-2, W-l, w+l), (w-i ,  w+l, w+2), and(w+l,w+2, w+3), where the last seven cor-respond to collocations of two and threeconsecutive words.The topical context is formed by Cl,..., Cm,which stand for the unordered set of open classwords appearing in the sentence 7.The four methods tested translate thisinformation into features in different ways.Snow and LB algorithms require binary fea-6This is achieved by ramdomly reducing the size ofthe largest corpus to the size of the smallest.7The already described set of attributes containsthose attributes used in (Ng and Lee, 1996), with theexception of the morphology of the target word andthe verb-object syntactic relation.tures.
Therefore, local context attributes haveto be binarized in a preprocess, while the top-ical context attributes remain as binary testsabout the presence/absence of a concrete wordin the sentence.
As a result the number ofattributes is expanded to several thousands(from 1,764 to 9,900 depending on the partic-ular word).The binary representation of attributes isnot appropriate for NB and EB algorithms.Therefore, the 15 local-context attributes aretaken straightforwardly.
Regarding the binarytopical-context attributes, we have used thevariants described in (Escudero et al, 2000b).For EB, the topical information is codified asa single set-valued attribute (containing allwords appearing in the sentence) and the cal-culation of closeness is modified so as to han-dle this type of attribute.
For NB, the top-ical context is conserved as binary features,but when classifying new examples only theinformation of words appearing in the exam-ple (positive information) is taken into ac-count.
In that paper, these variants are calledpositive Exemplar-based (PEB) and positiveNaive Bayes (PNB), respectively.
PNB andPEB algorithms are empirically proven to per-form much better in terms of accuracy andefficiency in the WSD task.3.3 Exper imenta l  Methodo logyThe comparison of algorithms has been per-formed in series of controlled experiments us-ing exactly the same training and test sets.There are 7 combinations of training-test setscalled: A+B-A+B, A+B-A, A+B-B, A-A, B-B, A-B, and B-A, respectively.
In this nota-tion, the training set is placed at the left handside of symbol "-", while the test set is at theright hand side.
For instance, A-B means thatthe training set is corpus A and the test setis corpus B.
The symbol "+" stands for setunion, therefore A+B-B means that the train-ing set is A union B and the test set is B.When comparing the performance oftwo al-gorithms, two different statistical tests of sig-nificance have been apphed depending on thecase.
A-B and B-A combinations represent asingle training-test experiment.
In this cases,the McNemar's test of significance is used(with a confidence value of: X1,0.952 = 3.842),which is proven to be more robust than a sim-ple test for the difference of tw0_proportions.In the other combinations, a 10-fold cross-validation was performed in order to prevent175nounsverbsAorBexamplesrainsensesmin max avgAi MFS (%)min minsensesBMFS (%!min max max avg max avg max avg avg122 714 420 2 24 7.7 37.9 90.7 59.8 3 24 8.8 21.0 87.7 45.3101 741 369 4 13 8.9120.8 81.6 49.3 4 14 11.4 28.0 71.7 46.3101 741 401 2 24 8.1 J20.8 90.7 56.1 3 24 9.8 21.0 87.7 45.6Table 1: Information about the set of 21 words of reference.testing on the same material used for training.In these cases, accuracy/error rate figures re-ported in section 4 are averaged over the re-sults of the 10 folds.
The associated statisticaltests of significance is a paired Student's t-testwith a confidence value of: t9,0.975 = 2.262.Information about both statistical tests canbe found at (Dietterich, 1998).4 Exper iments4.1 F i rs t  Exper imentTable 2 shows the accuracy figures of the fourmethods in all combinations of training andtest sets .
Standard deviation numbers aresupplied in all cases involving cross valida-tion.
M FC stands for a Most-Frequent-senseClassifier, that is, a naive classifier that learnsthe most frequent sense of the training setand uses it to classify all examples of the testset.
Averaged results are presented for nouns.verbs, and overall, and the best results foreach case are printed in boldface.The following conclusions can be drawn:?
LB outperforms all other methods inall cases.
Additionally, this superiorityis statistically significant, except whencomparing LB to the PEB approach in thecases marked with an asterisk.?
Surprisingly, LB in A+B-A (or A+B-B)does not achieve substantial improvementto the results of A-A (or B-S) w in  fact,the first variation is not statistically sig-nificant and the second is only slightlysignificant.
That is, the addition of extraexamples from another domain does notnecessarily contribute to improve the re-sults on the original corpus.
This effect isalso observed in the other methods, spe-cially in some cases (e.g.
Snow in A+B-Avs.
A-A) in which the joining of bothtraining corpora is even counterproduc-tive.SThe second and third column correspond to thetrain and test sets used by (Ng and Lee, 1996; Ng,1997a)?
Regarding the portability of the systems,very disappointing results are obtained.Restricting to \[B results, we observe thatthe accuracy obtained in A-B is 47.1%while the accuracy in B-B (which canbe considered an upper bound for LB inB corpus) is 59.0%, that is, a drop of12 points.
Furthermore, 47.1% is onlyslightly better than the most frequentsense in corpus B, 45.5%.
The compari-son in the reverse direction is even worse:a drop from 71.3% (A-A) to 52.0% (B-A), which is lower than the most frequentsense of corpus A, 55.9%.4.2 Second Exper imentThe previous experiment shows that classi-tiers trained on the A corpus do not work wellon the B corpus, and vice-versa.
Therefore,it seems that some kind of tuning process isnecessary to adapt supervised systems to eachnew domain.This experiment explores the effect of a sim-ple tuning process consisting of adding to theoriginal training set a relatively small sarn-ple of manually sense tagged examples of thenew domain.
The size of this supervised por-tion varies from 10% to 50% of the availablecorpus in steps of 10% (the remaining 50% iskept for testing).
This set of experiments willbe referred to as A+%B-B, or conversely, toB+%A-A.In order to determine to which extent theoriginal training set contributes to accuratelydisambiguate in the new domain, we also cal-culate the results for %A-A (and %B-B), thatis, using only the tuning corpus for training.Figure 1 graphically presents the results ob-tained by all methods.
Each plot contains theX+%Y-Y and %Y-Y curves, and the straightlines corresponding to the lower bound MFC,and to the upper bounds Y-Y and X+Y-Y.As expected, the accuracy of all methodsgrows (towards the upper bound) as more tun-ing corpus is added to the training set.
How-ever, the relation between X+%Y-Y and %Y-Y reveals some interesting facts.
In plots 2a,176nounsMFC verbstotalnounsPNB verbstotalnounsPEB verbstotalnounsSnow verbstotalnounsLB verbstotalA+B-A+B46.59?1.0846.49?1.3746.55?0.7162.29?1.2560.18?1.6461.55?1.0462.66?0.8763.67?1.9463.01?0.9361.24?1.1460.35?1.5760.92?1.0966.00?1.4766.91?2.2566.32?1.34A+B-A56.68?2.7948.74?1.9853.90?2.0168.89?0.9364.21?2.2667.25?1.0769.45?1 5168.39?3.2569.08?1.6666.36?1 5764.11?2.7665.57?1.332.09?1.6171.23?2.9971.79?1.51Accuracy (%)A+B-B36.49?2.4144.23?2.6739.21?1.9055.69?1.9456.14?2.7955.85?1.8156.09?1.1258.58?2.4056.97?1.2256.11?1.4556.58?2.4556.28?1.1059.92?1.9362.58?2.9360.85?1.81A-A59.77?1.4448.85?2.0955.94?1.1066.93?1.4463.87?1.8065.86?1.1169.38?1.2468.25?2.8468.98?1.0668.85?1.3663.91?1.5167.12?1.1671.69?1.5470.45?2.14"71.26?1.15B-B \[ A-B B-A45.28?1.81 33.97 39.4645.96?2.6O 40.91 37.3145.52?1.27 36.40 38.7156.17?1.60 36.62 45.9957.97?2.86 50.20 50.7556.80?1.12 41.38 47.6656.17?1.80 42.15 50.5359.57?2.86 51.19 52.2457.36?1.68 45.32 51.1356.55?1.31 42.13 49.9655.36?3.27 47.66 49.3956.13?1.23 44.07 49.7658.33?2.26 43.92 51.28"60.14?3.43" 52.99 53.29*58.96?1.86 47.10 51.99"Table 2: Accuracy results (:i: standard eviation) of the methods on all training-test combina-tions3a, and lb the contribution of the originaltraining corpus is null.
Furthermore, in plotsla, 2b, and 3b a degradation on the accuracyperformance is observed.
Summarizing, thesesix plots show that for Naive Bayes, ExemplarBased, and Snow methods it is not worth keep-ing the original training examples.
Instead, abetter (but disappointing) strategy would besimply using the tuning corpus.However, this is not the situation of Lazy-Boosting (plots 4a and 4b), for which a mod-erate (but consistent) improvement of accu-racy is observed when retaining the originaltraining set.
Therefore, Lazy\[3oosting showsagain a better behaviour than their competi-tors when moving from one domain to an-other.4.3 Th i rd  Exper imentThe bad results about portability could be ex-plained by, at least, two reasons: 1) CorpusA and \[3 have a very different distribution ofsenses, and, therefore, different a-priori bi-ases; 2) Examples of corpus A and \[3 con-tain different information, and, therefore, thelearning algorithms acquire different (and noninterchangeable) classification cues from bothcorpora,.The first hypothesis confirmed by observ-ing the bar plots of figure 2, which contain thedistribution of the four most frequent sensesof some sample words in the corpora A andB.
respectively.
In order to check the secondhypothesis, two new sense-balanced corporahave been generated from the DSO corpus, byequilibrating the number of examples of eachsense between A and B parts.
In this way, thefirst difficulty is artificially overrided and thealgorithms hould be portable if examples ofboth parts are quite similar.Table 3 shows the results obtained by Lazy-Boosting on these new corpora.Regarding portability, we observe a signifi-cant accuracy decrease of 7 and 5 points fromA-A to B-A, and from B-B to A-B, respec-tively 9.
That is, even when the sazne distri-bution of senses is conserved between trainingand test examples, the portability of the su-pervised WSD systems is not guaranteed.These results imply that examples have tobe largely different from one corpus to an-other.
By studying the weak rules generatedby kazyBoosting in both cases, we could cor-roborate this fact.
On the one hand, the typeof features used in the rules were significantlydifferent between corpora, and, additionally,there were very few rules that apply to bothsets; On the other hand, the sign of the pre-diction of many of these common rules wassomewhat contradictory between corpora.9This loss in accuracy is not as important as m thefirst experiment, due to the simplification provided bythe balancing ofsense distributions.177Naive BayesExemplar BasedSnowLazyBoosting5856 154~?52o50444O5856Af~52~o~ 48465856 '54o 5250466260 '58~o48464.4Test on B corpus(la).
.
.
.
.
-MF'~ .
.
.
.o B,-BA+B-B oA+%B-B%B-B .
.
.
.
./ .
f "5 10 15 20 25 30 35 40 45 50(2a).
.
.
.
.
.
.
?
; .
.
.
.
; .
.
.
.
.
.
~ --:-" ~S-- :~.
: : , - -  ~B-B ---A+B-B oA+%B-B * - -%B-B - "u ' -/+.-J5 10 15 20 25 30 355 40 45 50(3a)MFSA+B-B oA+%B-B ~- -%B-B ==-5 10 15 20 25 30 35 40 45 50(4a)B-B  ~ - -A+%B-B .
.
.
.%B-B - - - -/ /+  / / " "  --/ , /.+$+*, , = , = , i , ,5 10 15 20 25 30 35 40 45 50727068~66~,646260585654727068666462.60585654727O68 ~6660585654727O68o  .64~: eo585654Test on A corpus(lb)MFSA-A - - - -B+A-AB+%A-A ~ - -o o .
%,~oA -~-.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
./, , , , , .
.
.
.5 10 15 20 25 30 35 40 45 50(2b)MFSA-A~- -B+A-A .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
= .
-  ~.
.
, .Z~:~.
.
.
~ ~.~%A-A .
.
.
._~.
- -++ ?
, , , , , , , , ,5 10 15 20 25 30 35 40 45 50(3b)MF$A-A .
.
.
.B+A-A aB+%A-A ~-%A-A .
.
.
.
.j.
"/5 10 15 20 25 30 ,35 40 45 50(4b).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
f~tE~ .
.
.
.
.A-A .
.
.
.B+A=AB+%A-A .
.
.
./P  +/"+" 1"////,', , , , , , , , i5 10  15  20  25  30  35  40  45  50Figure 1: Results of the tuning experiment5 Conc lus ions  and  Fur ther  WorkThis work has pointed out some difficultiesregarding the portability of supervised WSDsystems, a very important issue that has beenpaid little attention up to the present.According to our experiments, it seems thatthe performance of supervised sense taggers isnot guaranteed when moving from one domainto another (e.g.
from a balanced corpus, suchas BC, to an economic domain, such as WSJ).These results implies that some_kind of adap-tation is required for cross-corpus application.178.
.
.
.
.
il ii\["~ head ~a I r l temt  ~o fa l l  oo growmL.Figure 2: Distribution of the four most frequent senses for two nouns (head, interest) and twoverbs (line, state).
Black bars = A corpus; Grey bars = B corpusnounsMFC verbstotalnounsLB verbstotalAccuracy (%)A+B-A+B A+B-B A-A48.75?0.9148.22?1 6848.55?1 662.82?1.4366.82?1.5364.35?1.16A+B-A48.90?1.6948.22?1.9048.64?1.0464.26?2.0769.33?2.9266.20?2.1248.61?0.9648.22?3 0648.46?1.2161.38?2.0864.32?3.2762.50?1.4748.87?1 6848.22?1.9048.62?1.0963.19?1.6568.51?2.4565.22?1.50B-B A-B B-A48.61?0.96 48.99 48.9948.22?3.06 48.22 48.2248.46?1.21 48.70 48.7060.65?1.01 53.45 55.2763.49?2.27 60.44 62.5561.74?1.18 56.12 58.05Table 3: Accuracy results (5= standard eviation) of LazyBoosting on the sense-balanced corporaFurthermore, these results are in contradic-tion with the idea of "robust broad-coverageWSD" introduced by (Ng, 1997b), in which asupervised system trained on a large enoughcorpora (say a thousand examples per word)~hould provide accurate disambiguation onany corpora (or, at least significantly betterthan MFS).Consequently, it is our belief that a numberof issues regarding portability, tuning, knowl-edge acquisition, etc., should be thoroughlystudied before stating that the supervised MLparadigm is able to resolve a realistic WSDproblem.Regarding the M L algorithms tested, thecontribution of this work consist of empiri-cally demonstrating that the LazyBoosting al-gorithm outperforms other three state-of-the-art supervised ML methods for WSD.
Further-more.
this algorithm is proven to have betterproperties when is applied to new domains.Further work is planned to be done in thefollowing directions:?
Extensively evaluate LazyBoosting on theWSD task.
This would include tak-ing into account additional/alternativeattributes and testing the algorithm inother corpora --specially on sense-taggedcorpora automatically obtained from In-ternet or large text collections using non-supervised methods (Leazock et al, 1998;Mihalcea and Moldovan, 1999).?
Since most of the knowledge l arned froma domain is not useful when changingto a new domain, further investigation isneeded on tuning strategies, pecially onthose using non-supervised algorithms.?
It is known that mislabelled examples re-sulting from annotation errors tend to behard examples to classify correctly, and,therefore, tend to have large weights inthe final distribution.
This observationallows both to identify the noisy exam-ples and use LazyBoosting as a way toimprove data quality.
Preliminary exper-iments have been already carried out inthis direction on the DSO corpus.?
Moreover, the inspection of the ruleslearned by kazyBoosting could provideevidence about similar behaviours of a-priori different senses.
This type ofknowledge could be useful to performclustering of too fine-grained or artificialsenses.Re ferencesE.
Agirre and D. Martinez.
2000.
Decision Listsand Automatic Word Sense Disambiguation.
InProceedings o\] the COLING Workshop on Se-mantic Annotation and Intelligent ContentD.
Aha, D. Kibler, and M. Albert.
1991.
Instance-based Learning Algorithms.
Machine Learning,7:37-66.R.
F. Bruce and J. M. Wiebe.
1999.
Decompos-179able Modeling in Natural Language Processing.Computatwnal Linguistics.
25(2):195-207.S.
Cost and S. Salzberg.
1993.
A weighted nearestneighbor algorithm for learning with symbolicfeatures.
Machine Learning, 10(1), 57-78.W.
Daelemans, A. van den Bosch, and J. Zavrel.1999.
Forgetting Exceptions is Harmful in Lan-guage Learning.
Machine Learning, 34:11-41.T.
G. Dietterich.
1998.
Approximate Statisti-cal Tests for Comparing Supervised Classifi-cation Learning Algorithms.
Neural Computa-tion, 10(7).R.
O. Duda and P. E. Hart.
1973.
Pattern Clas-sificatwn and Scene Analysis.
Wiley.G.
Escudero, L. M~rquez, and G. Rigau.
2000a.Boosting Applied to Word Sense Disam-biguation.
In Proceedings of the 12th Euro-pean Conference on Machine Learning, ECML,Barcelona, Spain.G.
Escudero.
L. M~rquez, and G. Rigau.
2000b.Naive Bayes and Exemplar-Based Approachesto Word Sense Disambiguation Revisited.
InTo appear in Proceedings of the 14th EuropeanConference on Artificial Intelligence, ECAI.G.
Escudero, L. M~quez, and G. Rigau.
2000c.On the Portability and Tuning of Super-vised Word Sense Disambiguation Systems.
Re-search Report LSI-00-30-R, Software Depart-ment (LSI).
Technical University of Catalonia(UPC).A.
Fujii, K. Inui.
T. Tokunaga, and H. Tanaka.1998.
Selective Sampling for Example-basedW'ord Sense Disambiguation.
ComputatwnalLinguistics, 24(4):573-598.W.
Gale, K. W. Church, and D. Yarowsky.
1992a.A Method for Disambiguating Word Senses in aLarge Corpus.
Computers and the Humanities,26:415-439.W.
Gale, K. W. Church, and D. Yarowsky.
1992b.Estimating Upper and Lower Bounds on thePerformance of Word Sense Disambiguation.In Proceedings of the 30th Annual Meeting ofthe Association for Computational Linguistics.ACL.N.
Ide and J. V@ronis.
1998.
Introduction to theSpecial Issue on Word Sense Disambiguation:The State of the Art.
Computational Linguis-tics, 24(1):1-40.A.
Kilgarriff and J. Rosenzweig.
2000.
EnglishSENSEVAL: Report and Results.
In Proceed-ings of the 2nd International Conference onLanguage Resources and Evaluatwn, LREC,Athens, Greece.C.
Leacock, M. Chodorow, and G. A. Miller.
1998.Using Corpus Statistics and WordNet Relationsfor Sense Identification.
Computatwnal Lin-guistwcs, 24(1):147-166.N.
Littlestone.
1988.
Learning Quickly when Irrel-evant Attributes Abound.
Machine Learning,2:285-318.R.
Mihalcea and I. Moldovan.
1999.
An Au-tomatic Method for Generating Sense TaggedCorpora.
In Proceedings of the 16th NationalConference on Artificial Intelligence.
AAAIPress.R.
J. Mooney.
1996.
Comparative Experimentson Disambiguating Word Senses: An Illustra-tion of the Role of Bias in Machine Learning.In Proceedings of the 1st Conference on Empir-ical Methods m Natural Language Processing,EMNLP.H.
T. Ng and H. B. Lee.
1996.
Integrating Multi-ple Knowledge Sources to Disambiguate WordSense: An Exemplar-based Approach.
In Pro-ceedmgs of the 3~th Annual Meeting of the As-sociation for Computational Linguistics.
ACL.H.
T. Ng.
1997a.
Exemplar-Base Wbrd Sense Dis-ambiguation: Some Recent Improvements.
InProceedings of the 2nd Conference on Empir-zcal Methods in Natural Language Processing,EMNLP.H.
T. Ng.
1997b.
Getting Serious about WordSense Disambiguation.
In Proceedings of theACL SIGLEX Workshop: Tagging Text withLexical Semantics: Why, what and how?, Wash-ington, USA.D.
Roth.
1998.
Learning to Resolve Natural Lan-guage Ambiguities: A Unified Approach.
InProceedings of the National Conference on Ar-tzficial Intelhgence, AAAI 'Y8, July.R.
E. Schapire and Y.
Singer.
to appear.
ImprovedBoosting Algorithms Using Confidence-ratedPredictions.
Machine Learning.
Also appearingin Proceedings of the 11th Annual Conference onComputatzonal Learning Theory, 1998.S.
Sekine.
1997.
The Domain Dependence ofPars-ing.
In Proceedings o\] the 5th Conference onApplied Natural Language Processing, ANLP,Washington DC.
ACL.G.
Towell and E. M. Voorhees.
1998.
Disam-biguating Highly Ambiguous Words.
Computa-tional Lingu~stzcs.
24(1):125-146.D.
Yarowsky.
1994.
Decision Lists for LexicalAmbiguity Resolution: Application to AccentRestoration i  Spanish and French.
In Proceed-ings of the 32nd Annual Meeting of the Associ-ation for Computational Linguistics, pages 88-95, Las Cruces, NM.
ACL.180
