Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 785?792,Sydney, July 2006. c?2006 Association for Computational LinguisticsYou Can?t Beat Frequency (Unless You Use Linguistic Knowledge) ?A Qualitative Evaluation of Association Measures forCollocation and Term ExtractionJoachim Wermter Udo HahnJena University Language & Information Engineering (JULIE) LabD-07743 Jena, Germany{wermter|hahn}@coling-uni-jena.deAbstractIn the past years, a number of lexicalassociation measures have been studiedto help extract new scientific terminol-ogy or general-language collocations.
Theimplicit assumption of this research wasthat newly designed term measures involv-ing more sophisticated statistical criteriawould outperform simple counts of co-occurrence frequencies.
We here explic-itly test this assumption.
By way of fourqualitative criteria, we show that purelystatistics-based measures reveal virtuallyno difference compared with frequencyof occurrence counts, while linguisticallymore informed metrics do reveal such amarked difference.1 IntroductionResearch on domain-specific automatic termrecognition (ATR) and on general-language collo-cation extraction (CE) has gone mostly separateways in the last decade although their underlyingprocedures and goals turn out to be rather simi-lar.
In both cases, linguistic filters (POS taggers,phrase chunkers, (shallow) parsers) initially col-lect candidates from large text corpora and thenfrequency- or statistics-based evidence or associa-tion measures yield scores indicating to what de-gree a candidate qualifies as a term or a colloca-tion.
While term mining and collocation mining,as a whole, involve almost the same analytical pro-cessing steps, such as orthographic and morpho-logical normalization, normalization of term orcollocation variation etc., it is exactly the measurewhich grades termhood or collocativity of a can-didate on which alternative approaches diverge.Still, the output of such mining algorithms looksimilar.
It is typically constituted by a ranked liston which, ideally, the true terms or collocationsare placed in the top portion of the list, while thenon-terms / non-collocations occur in its bottomportion.While there have been lots of approaches tocome up with a fully adequate ATR/CE metric(cf.
Section 2), we have made observations in ourexperiments that seem to indicate that simplicityrules, i.e., frequency of occurrence is the dominat-ing factor for the ranking in the result lists evenwhen much smarter statistical machinery is em-ployed.
In this paper, we will discuss data whichreveals that purely statistics-based measures ex-hibit virtually no difference compared with fre-quency of occurrence counts, while linguisticallymore informed measures do reveal such a markeddifference ?
for the problem of term and colloca-tion mining at least.2 Related WorkAlthough there has been a fair amount of workemploying linguistically sophisticated analysis ofcandidate items (e.g., on CE by Lin (1998) andLin (1999) as well as on ATR by Daille (1996),Jacquemin (1999), and Jacquemin (2001)), theseapproaches are limited by the difficulty to portgrammatical specifications to other domains (inthe case of ATR) or by the error-proneness offull general-language parsers (in the case of CE).Therefore, most recent approaches in both areashave backed off to more shallow linguistic filter-ing techniques, such as POS tagging and phrasechunking (e.g., Frantzi et al (2000), Krenn andEvert (2001), Nenadic?
et al (2004), Wermter andHahn (2005)).785After linguistic filtering, various measuresare employed in the literature for grading thetermhood / collocativity of collected candidates.Among the most widespread ones, both for ATRand CE, are statistical and information-theoreticmeasures, such as t-test, log-likelihood, entropy,and mutual information.
Their prominence isalso reflected by the fact that a whole chapter ofa widely used textbook on statistical NLP (viz.Chapter 5 (Collocations) in Manning and Schu?tze(1999)) is devoted to them.
In addition, the C-value (Frantzi et al, 2000) ?
basically a frequency-based approach ?
has been another widely usedmeasure for multi-word ATR.
Recently, more lin-guistically informed algorithms have been intro-duced both for CE (Wermter and Hahn, 2004) andfor ATR (Wermter and Hahn, 2005), which havebeen shown to outperform several of the statistics-only metrics.3 Methods and Experiments3.1 Qualitative CriteriaBecause various metrics assign a score to the can-didates indicating as to what degree they qualifyas a collocation or term (or not), these candidatesshould ideally be ranked in such a way that thefollowing two conditions are met:?
true collocations or terms (i.e., the true pos-itives) are ranked in the upper portion of theoutput list.?
non-collocations or non-terms (i.e., the truenegatives) are ranked in the lower part of theoutput list.1While a trivial solution to the problem mightbe to simply count the number of occurrences ofcandidates in the data, employing more sophis-ticated statistics-based / information-theoretic oreven linguistically-motivated algorithms for grad-ing term and collocation candidates is guided bythe assumption that this additional level of sophis-tication yields more adequate rankings relative tothese two conditions.Several studies (e.g., Evert and Krenn (2001),Krenn and Evert (2001), Frantzi et al (2000),Wermter and Hahn (2004)), however, have al-ready observed that ranking the candidates merelyby their frequency of occurrence fares quite well1Obviously, this goal is similar to ranking documents ac-cording to their relevance for information retrieval.compared with various more sophisticated as-sociation measures (AMs such as t-test, log-likelihood, etc.).
In particular, the precision/recallvalue comparison between the various AMs ex-hibits a rather inconclusive picture in Evert andKrenn (2001) and Krenn and Evert (2001) as towhether sophisticated statistical AMs are actuallymore viable than frequency counting.Commonly used statistical significance testing(e.g., the McNemar or the Wilcoxon sign ranktests; see (Sachs, 1984)) does not seem to providean appropriate evaluation ground either.
AlthoughEvert and Krenn (2001) and Wermter and Hahn(2004) provide significance testing of some AMswith respect to mere frequency counting for collo-cation extraction, they do not differentiate whetherthis is due to differences in the ranking of true pos-itives or true negatives or a combination thereof.2As for studies on ATR (e.g., Wermter and Hahn(2005) or Nenadic?
et al (2004)), no statistical test-ing of the term extraction algorithms to mere fre-quency counting was performed.But after all, these kinds of commonly used sta-tistical significance tests may not provide the rightmachinery in the first place.
By design, they arerather limited (or focused) in their scope in thatthey just check whether a null hypothesis can berejected or not.
In such a sense, they do not pro-vide a way to determine, e.g., to which degree ofmagnitude some differences pertain and thus donot offer the facilities to devise qualitative criteriato test whether an AM is superior to co-occurrencefrequency counting.The purpose of this study is therefore to postu-late a set of criteria for the qualitative testing ofdifferences among the various CE and ATR met-rics.
We do this by taking up the two conditionsabove which state that a good CE or ATR algo-rithm would rank most of the true positives in acandidate set in the upper portion and most ofthe true negatives in the lower portion of the out-put.
Thus, compared to co-occurrence frequencycounting, a superior CE/ATR algorithm shouldachieve the following four objectives:2In particular Evert and Krenn (2001) use the chi-squaretest which assumes independent samples and is thus not re-ally suitable for testing the significance of differences of twoor more measures which are typically run on the same setof candidates (i.e., a dependent sample).
Wermter and Hahn(2004) use the McNemar test for dependent samples, whichonly examines the differences in which two metrics do notcoincide.7861.
keep the true positives in the upper portion2.
keep the true negatives in the lower portion3.
demote true negatives from the upper portion4.
promote true positives from the lower por-tion.We take these to be four qualitative criteria bywhich the merit of a certain AM against mere oc-currence frequency counting can be determined.3.2 Data SetsFor collocation extraction (CE), we used the dataset provided by Wermter and Hahn (2004) whichconsists of a 114-million-word German newspa-per corpus.
After shallow syntactic analysis, theauthors extracted Preposition-Noun-Verb (PNV)combinations occurring at least ten times and hadthem classified by human judges as to whetherthey constituted a valid collocation or not, re-sulting in 8644 PNV-combinations with 13.7%true positives.
As for domain-specific automaticterm recognition (ATR), we used a biomedicalterm candidate set put forth by Wermter and Hahn(2005), who, after shallow syntactic analysis, ex-tracted 31,017 trigram term candidates occurringat least eight times out of a 104-million-wordMEDLINE corpus.
Checking these term candi-dates against the 2004 edition UMLS Metathe-saurus (UMLS, 2004)3 resulted in 11.6% true pos-itives.
This information is summarized in Table 1.Collocations Termsdomain newspaper biomedicinelanguage German Englishlinguistic type PP-Verb noun phrasescombinations (trigrams)corpus size 114 million 104 millioncutoff 10 8# candidates 8,644 31,017# true positives 1,180 (13.7%) 3,590 (11.6%)# true negatives 7,464 (86.3%) 27,427 (88.4%)Table 1: Data sets for Collocation Extraction (CE) and Au-tomatic Term Dioscovery (ATR)3The UMLS Metathesaurus is an extensive and carefullycurated terminological resource for the biomedical domain.3.3 The Association MeasuresWe examined both standard statistics-based andmore recent linguistically rooted association mea-sures against mere frequency of occurrence count-ing (henceforth referred to as Frequency).
As thestandard statistical AM, we selected the t-test (seealso Manning and Schu?tze (1999) for a descrip-tion on its use in CE and ATR) because it hasbeen shown to be the best-performing statistics-only measure for CE (cf.
Evert and Krenn (2001)and Krenn and Evert (2001)) and also for ATR (seeWermter and Hahn (2005)).Concerning more recent linguistically groundedAMs, we looked at limited syntagmatic modifia-bility (LSM) for CE (Wermter and Hahn, 2004)and limited paradigmatic modifiability (LPM) forATR (Wermter and Hahn, 2005).
LSM exploitsthe well-known linguistic property that colloca-tions are much less modifiable with additional lex-ical material (supplements) than non-collocations.For each collocation candidate, LSM determinesthe lexical supplement with the highest probabil-ity, which results in a higher collocativity score forthose candidates with a particularly characteristiclexical supplement.
LPM assumes that domain-specific terms are linguistically more fixed andshow less distributional variation than commonnoun phrases.
Taking n-gram term candidates, itdetermines the likelihood of precluding the ap-pearance of alternative tokens in various token slotcombinations, which results in higher scores formore constrained candidates.
All measures assigna score to the candidates and thus produce a rankedoutput list.3.4 Experimental SetupIn order to determine any potential merit of theabove measures, we use the four criteria describedin Section 3.1 and qualitatively compare the differ-ent rankings given to true positives and true neg-atives by an AM and by Frequency.
For this pur-pose, we chose the middle rank as a mark to di-vide a ranked output list into an upper portion anda lower portion.
Then we looked at the true pos-itives (TPs) and true negatives (TNs) assigned tothese portions by Frequency and quantified, ac-cording to the criteria postulated in Section 3.1,to what degree the other AMs changed these rank-ings (or not).
In order to better quantify the de-grees of movement, we partitioned both the upperand the lower portions into three further subpor-tions.787Association upper portion (ranks 1 - 4322) lower portion (ranks 4323 - 8644)Measure 0% - 16.7% 16.7% - 33.3% 33.3% - 50% 50% - 66.7% 66.7% - 83.3% 83.3% - 100%Criterion 1 Freq 545 (60.2%) 216 (23.9%) 144 (15.9%) 0 0 0(905 TPs) t-test 540 (59.7%) 198 (21.9%) 115 (12.7%) 9 (1.0%) 12 (1.3%) 12 (1.3%)LSM 606 (67.0%) 237 (26.2%) 35 (3.9%) 10 (1.1%) 12 (1.3%) 5 (0.6%)Criterion 2 Freq 0 0 0 1361 (33.6%) 1357 (33.5%) 1329 (32.8%)(4047 TNs) t-test 0 34 (0.8%) 613 (15.2%) 1121 (27.7%) 1100 (27.2%) 1179 (29.1%)LSM 118 (2.9%) 506 (12.5%) 726 (17.9%) 808 (20.0%) 800 (19.8%) 1089 (26.9%)Criterion 3 Freq 896 (26.2%) 1225 (35.9%) 1296 (37.9%) 0 0 0(3417 TNs) t-test 901 (26.4%) 1243 (36.4%) 932 (27.3%) 95 (2.8%) 47 (1.4%) 199 (5.8%)LSM 835 (24.4%) 1150 (33.7%) 342 (10.0%) 218 (6.4%) 378 (11.1%) 494 (14.5%)Criterion 4 Freq 0 0 0 113 (41.1%) 85 (30.9%) 77 (28.0%)(275 TPs) t-test 0 0 31 (11.3%) 88 (32.6%) 59 (21.5%) 95 (34.5%)LSM 0 10 (3.6%) 144 (52.4%) 85 (30.9%) 27 (9.8%) 9 (3.3%)Table 2: Results on the four qualitative criteria for Collocation Extraction (CE)Association upper portion (ranks 1 - 15508) lower portion (ranks 15509 - 31017)Measure 0% - 16.7% 16.7% - 33.3% 33.3% - 50% 50% - 66.7% 66.7% - 83.3% 83.3% - 100%Criterion 1 Freq 1252 (50.7%) 702 (28.4%) 515 (20.9%) 0 0 0(2469 TPs) t-test 1283 (52.0%) 709 (28.7%) 446 (18.1%) 13 (0.5%) 2 (0.1%) 16 (0.6%)LPM 1346 (54.5%) 513 (20.8%) 301 (12.2%) 163 (6.6%) 95 (3.8%) 51 (2.1%)Criterion 2 Freq 0 0 0 4732 (32.9%) 4822 (33.5%) 4833 (33.6%))(14387 TNs) t-test 0 0 580 (4.0%) 4407 (30.6%) 4743 (33.0%) 4657 (32.4%)LPM 1009 (7.0%) 1698 (11.8%) 2190 (15.2%) 2628 (18.3%) 3029 (21.1%) 3834 (26.6%)Criterion 3 Freq 3917 (30.0%) 4467 (34.3%) 4656 (35.7%) 0 0 0(13040 TNs) t-test 3885 (29.8%) 4460 (34.2%) 4048 (31.0%) 315 (2.4%) 76 (0.6%) 256 (2.0%)LPM 2545 (19.5%) 2712 (20.8%) 2492 (19.1%) 2200 (16.9%) 1908 (14.6%) 1182 (9.1%)Criterion 4 Freq 0 0 0 438 (39.1%) 347 (31.0%) 336 (30.0%)(1121 TPs) t-test 0 0 97 (8.7%) 436 (38.9%) 348 (31.0%) 240 (21.4%)LPM 268 (23.9%) 246 (21.9%) 188 (16.8%) 180 (16.1%) 137 (12.2%) 102 (9.1%)Table 3: Results on the four qualitative criteria for Automatic Term Discovery (ATR)4 Results and DiscussionThe first two criteria examine how conservative anassociation measure is with respect to Frequency,i.e., a superior AM at least should keep the status-quo (or even improve it) by keeping the true pos-itives in the upper portion and the true negativesin the lower one.
In meeting criteria 1 for CE,Table 2 shows that t-test behaves very similar toFrequency in keeping roughly the same amount ofTPs in each of the upper three subportions.
LSMeven promotes its TPs from the third into the firsttwo upper subportion (i.e., by a 7- and 2-point in-crease in the first and in the second subportion aswell as a 12-point decrease in the third subportion,compared to Frequency).With respect to the same criterion for ATR (seeTable 3), Frequency and t-test again show quitesimilar distributions of TPs in the top three sub-portions.
LPM, on the other hand, demonstrates amodest increase (by 4 points) in the top upper sub-portion, but decreases in the second and third oneso that a small fraction of TPs gets demoted to thelower three subportions (6.6%, 3.8% and 2.1%).Regarding criterion 2 for CE (see Table 2), t-test?s share of TNs in the lower three subportionsis slightly less than that of Frequency, leadingto a 15-point increase in the adjacent third up-per subportion.
This local ?spilling over?
to theupper portion is comparatively small consideringthe change that occurs with respect to LSM.
Here,TNs appear in the second (12.5%) and the third(17.9%) upper subportions.
For ATR, t-test oncemore shows a very similar distribution comparedto Frequency, whereas LPM again promotes someof its lower TNs into the upper subportions (7%,11.8% and 15.2%).Criteria 3 and 4 examine the kinds of re-rankings (i.e., demoting upper portion TNs andpromoting lower portion TPs) which an AM needsto perform in order to qualify as being superior toFrequency.
These criteria look at how well an AMis able to undo the unfavorable ranking of TPs andTNs by Frequency.
As for criterion 3 (the demo-tion of TNs from the upper portion) in CE, Table 2shows that t-test is only marginally able to undothe unfavorable rankings in its third upper sub-portion (11 percentage points less of TNs).
Thiscauses a small fraction of TNs getting demoted to788Rank in FrequencyRankinLSM100%83.3%66.7%50%33.3%16.70%0% 16.7% 33.3% 50%Figure 1: Collocations: True negatives moved from upperto lower portion (LSM rank compared to Frequency rank)Rank in FrequencyRankint?test100%83.3%66.7%50%33.3%16.70%0% 16.7% 33.3% 50%Figure 2: Collocations: True negatives moved from upperto lower portion (t-test rank compared to Frequency rank)the lower three subportions (viz.
2.8%, 1.4%, and5.8%).A view from another angle on this rather slightre-ranking is offered by the scatterplot in Figure2, in which the rankings of the upper portion TNsRank in FrequencyRankinLPM0% 16.7% 33.3% 50%100%83.3%66.7%50%33.3%16.70%Figure 3: Terms: True negatives moved from upper tolower portion (LPM rank compared to Frequency rank)Rank in FrequencyRankint?test100%83.3%66.7%50%33.3%16.70%0% 16.7% 33.3% 50%Figure 4: Terms: True negatives moved from upper tolower portion (t-test rank compared to Frequency rank)of Frequency are plotted against their ranking int-test.
Here it can be seen that, in terms of the ranksubportions considered, the t-test TNs are concen-trated along the same line as the Frequency TNs,with only a few being able to break this line and789Rank in FrequencyRankinLSM100%83.3%66.7%50%33.3%16.70%50% 66.7% 83.3% 100%Figure 5: Collocations: True positives moved from lowerto upper portion (LSM rank compared to Frequency rank)Rank in FrequencyRankint?test100%83.3%66.7%50%33.3%16.70%50% 66.7% 83.3% 100%Figure 6: Collocations: True positives moved from lowerto upper portion (t-test rank compared to Frequency rank)get demoted to a lower subportion.A strikingly similar picture holds for this cri-terion in ATR: as can be witnessed from Figure4, the vast majority of upper portion t-test TNs isstuck on the same line as in Frequency.
The sim-Rank in FrequencyRankinLPM50% 66.7% 83.3% 100%100%83.3%66.7%50%33.3%16.70%Figure 7: Terms: True positives moved from lower to upperportion (LPM rank compared to Frequency rank)Rank in FrequencyRankint?test100%83.3%66.7%50%33.3%16.70%50% 66.7% 83.3% 100%Figure 8: Terms: True positives moved from lower to upperportion (t-test rank compared to Frequency rank)ilarity of t-test in both CE and ATR is even moreremarkable given the fact in the actual number ofupper portion TNs is more than four times higherin ATR (13040) than in CE (3076).
A look at theactual figures in Table 3 indicates that t-test is even790less able to deviate from Frequency?s TN distribu-tion (i.e., the third upper subportion is only occu-pied by 4.7 points less TNs, with the other twosubportions essentially remaining the same as inFrequency).The two linguistically rooted measures, LSMfor CE and LPM for ATR, offer quite a differentpicture regarding this criterion.
With LSM, almostone third (32%) of the upper portion TNs get de-moted to the three lower portions (see Table 2);with LPM, this proportion even amounts to 40.6%(see Table 3).
The scatterplots in Figure 1 andFigure 3 visualize this from another perspective:in particular, LPM completely breaks the originalFrequency ranking pattern and scatters the upperportion TNs in almost all possible directions, withthe vast majority of them thus getting demoted toa lower rank than in Frequency.
Although LSMstays more in line, still substantially more upperportion TNs get demoted than with t-test.With regard to Criterion 4 (the promotion ofTPs from the lower portion) in CE, t-test managesto promote 11.3% of its lower portion TPs to theadjacent third upper subportion, but at the sametime demotes more TPs to the third lower subpor-tion (34.5% compared to 28% in Frequency; seeTable 2).
Figure 6 thus shows the t-test TPs tobe a bit more dispersed in the lower portion.
ForATR, the t-test distribution of TPs differs even lessfrom Frequency.
Table 3 reveals that only 8.7% ofthe lower portion TPs get promoted to the adjacentthird upper portion.
The staggered groupinlpr g oflower portion t-test TPs (visualized in the respec-tive scatterplot in Figure 8) actually indicates thatthere are certain plateaus beyond which the TPscannot get promoted.The two non-standard measures, LSM andLPM, once more present a very different picture.Regarding LSM, 56% of all lower portion TPs getpromoted to the upper three subportions.
The ma-jority of these (52.4%) gets placed the third uppersubportion.
This can also be seen in the respectivescatterplot in Figure 5 which shows a marked con-centration of lower portion TPs in the third uppersubportion.
With respect to LPM, even 62.6% ofall lower portion TPs make it to the upper portions?
with the majority (23.9%) even getting promotedto the first upper subportion.
The respective scat-terplot in Figure 7 additionally shows that this up-ward movement of TPs, like the downward move-ment of TNs in Figure 3, is quite dispersed.5 ConclusionsFor lexical processing, the automatic identifica-tion of terms and collocations constitutes a re-search theme that has been dealt with by employ-ing increasingly complex probabilistic criteria (t-test, mutual information, log-likelihood etc.).
Thistrend is also reflected by their prominent status instandard textbooks on statistical NLP.
The implicitjustification in using these statistics-only metricswas that they would markedly outperform fre-quency of co-occurrence counting.
We devisedfour qualitative criteria for explicitly testing thisassumption.
Using the best performing standardassociation measure (t-test) as a pars pro toto, ourstudy indicates that the statistical sophisticationdoes not pay off when compared with simple fre-quency of co-occurrence counting.This pattern changes, however, when proba-bilistic measures incorporate additional linguisticknowledge about the distributional properties ofterms and the modifiability properties of colloca-tions.
Our results show that these augmented met-rics reveal a marked difference compared to fre-quency of occurrence counts ?
to a larger degreewith respect to automatic term recognition, to aslightly lesser degree for collocation extraction.ReferencesBe?atrice Daille.
1996.
Study and implementation ofcombined techniques for automatic extraction of ter-minology.
In Judith L. Klavans and Philip Resnik,editors, The Balancing Act: Combining Statisticaland Symbolic Approaches to Language, pages 49?66.
Cambridge, MA: MIT Press.Stefan Evert and Brigitte Krenn.
2001.
Methods forthe qualitative evaluation of lexical association mea-sures.
In ACL?01/EACL?01 ?
Proceedings of the39th Annual Meeting of the Association for Com-putational Linguistics and the 10th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 188?195.
Toulouse,France, July 9-11, 2001.
San Francisco, CA: Mor-gan Kaufmann.Katerina T. Frantzi, Sophia Ananiadou, and HidekiMima.
2000.
Automatic recognition of multi-wordterms: The C-value/NC-value method.
Interna-tional Journal on Digital Libraries, 3(2):115?130.Christian Jacquemin.
1999.
Syntagmatic and paradig-matic representations of term variation.
In Proceed-ings of the 37rd Annual Meeting of the Associationfor Computational Linguistics, pages 341?348.
Col-lege Park, MD, USA, 20-26 June 1999.
San Fran-cisco, CA: Morgan Kaufmann.791Christian Jacquemin.
2001.
Spotting and DiscoveringTerms through NLP.
Mass.
: MIT Press.Brigitte Krenn and Stefan Evert.
2001.
Can we do bet-ter than frequency?
A case study on extracting pp-verb collocations.
In Proceedings of the ACL Work-shop on Collocations.
Toulouse, France.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In COLING/ACL?98 ?
Pro-ceedings of the 36th Annual Meeting of the Asso-ciation for Computational Linguistics & 17th In-ternational Conference on Computational Linguis-tics, volume 2, pages 768?774.
Montre?al, Quebec,Canada, August 10-14, 1998.
San Francisco, CA:Morgan Kaufmann.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of the 37thAnnual Meeting of the Association for Computa-tional Linguistics, pages 317?324.
College Park,MD, USA, 20-26 June 1999.
San Francisco, CA:Morgan Kaufmann.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
Cambridge, MA; London, U.K.: BradfordBook & MIT Press.Goran Nenadic?, Sophia Ananiadou, and John Mc-Naught.
2004.
Enhancing automatic term recog-nition through recognition of variation.
In COL-ING Geneva 2004 ?
Proceedings of the 20th Inter-national Conference on Computational Linguistics,pages 604?610.
Geneva, Switzerland, August 23-27,2004.
Association for Computational Linguistics.Lothar Sachs.
1984.
Applied Statistics: A Handbookof Techniques.
New York: Springer, 2nd edition.UMLS.
2004.
Unified Medical Language System.Bethesda, MD: National Library of Medicine.Joachim Wermter and Udo Hahn.
2004.
Collocationextraction based on modifiability statistics.
In COL-ING Geneva 2004 ?
Proceedings of the 20th Inter-national Conference on Computational Linguistics,volume 2, pages 980?986.
Geneva, Switzerland, Au-gust 23-27, 2004.
Association for ComputationalLinguistics.Joachim Wermter and Udo Hahn.
2005.
Paradig-matic modifiability statistics for the extraction of ofcomplex multi-word terms.
In HLT-EMNLP?05 ?Proceedings of the 5th Human Language Technol-ogy Conference and 2005 Conference on Empiri-cal Methods in Natural Language Processing, pages843?850.
Vancouver, Canada, October 6-8, 2005.Association for Computational Linguistics.792
