Statistical Machine Translation based Passage Retrievalfor Cross-Lingual Question AnsweringTomoyosi Akiba Kei ShimizuDept.
of Information and Computer Sciences,Toyohashi University of Technology1-1 Hibarigaoka, Tenpaku-cho, Toyohashi-shi,441-8580, JAPANakiba@cl.ics.tut.ac.jpAtsushi FujiiGraduate School of Library,Information and Media Studies,University of Tsukuba1-2 Kasuga, Tsukuba, 305-8550, JAPANfujii@slis.tsukuba.ac.jpAbstractIn this paper, we propose a novel ap-proach for Cross-Lingual Question Answer-ing (CLQA).
In the proposed method, thestatistical machine translation (SMT) isdeeply incorporated into the question an-swering process, instead of using it as thepre-processing of the mono-lingual QA pro-cess as in the previous work.
The proposedmethod can be considered as exploiting theSMT-based passage retrieval for CLQA task.We applied our method to the English-to-Japanese CLQA system and evaluated theperformance by using NTCIR CLQA 1 and2 test collections.
The result showed that theproposed method outperformed the previouspre-translation approach.1 IntroductionOpen-domain Question Answering (QA) was firstevaluated extensively at TREC-8 (Voorhees andTice, 1999).
The goal in the factoid QA task is toextract words or phrases as the answer to a questionfrom an unorganized document collection, ratherthan the document lists obtained by traditional infor-mation retrieval (IR) systems.
The cross-lingual QAtask, which has been evaluated at CLEF (Magnini etal., 2003) and NTCIR (Sasaki et al, 2005), gener-alizes the factoid QA task by allowing the differentlanguages pair between the question and the answer.Basically, the CLQA system can be constructedsimply by translating either the question sentenceor the target documents into the language of theother side, and applying a mono-lingual QA system.For example, after the English question sentence istranslated into Japanese, a Japanese mono-lingualQA system can be applied to extract the answer fromthe Japanese target documents.
Depending on thetranslation techniques used for the pre-processing,the previous CLQA approach can be classified intothe machine translation based approach (Shimizu etal., 2005; Mori and Kawagishi, 2005) and the dic-tionary based approach (Isozaki et al, 2005).In this paper, we propose a novel approach forCLQA task.
In the proposed method, the statisti-cal machine translation (SMT) (Brown et al, 1993)is deeply incorporated into the question answer-ing process, instead of using the SMT as the pre-processing before the mono-lingual QA process asin the previous work.
Though the proposed methodcan be applied to any language pairs in principle, wefocus on the English-to-Japanese (EJ) CLQA task,where a question sentence is given in English and itsanswer is extracted from a document collection inJapanese.Recently, language modeling approach for infor-mation retrieval has been widely studied (Croft andLafferty, 2003).
Among them, statistical transla-tion model has been applied for mono-lingual IR(Berger and Lafferty, 1999), cross-lingual IR (Xuet al, 2001), and mono-lingual QA (Murdock andCroft, 2004).
Our method can be considered as thatapplying the translation model to cross-lingual QA.In the rest of this paper, Section 2 summarizes theprevious approach for CLQA.
Section 3 describesour proposed method in detail.
Section 4 describesthe experimental evaluation conducted to see theperformance of the proposed method by comparingit to some reference methods.
Section 5 describesour conclusion and future works.2 Previous CLQA SystemsFigure 1 shows the configuration of our previ-ous English-to-Japanese cross-lingual QA system,which has almost the same configuration to the con-ventional CLQA systems.
Firstly, the input En-glish question is translated into the correspondingJapanese question by using a machine translation.Alternatively, the machine translation can be re-751EnglishQuestionJapaneseQuestionDocument RetrievalJapaneseDocumentsAnswer ExtractionAnswerCandidatesPassage SimilarityCalculationType Matching ScoreCalculationAnswer RescoringJapaneseAnswerJapaneseDocumentCollectionExpected AnswerType DetectionExpectedAnswer TypeMachine Translation(or Dictionary-basedTranslation)Figure 1: The configuration of the conventionalCLQA system.placed by the dictionary-based term-by-term transla-tion.
Then, either the English question or the trans-lated Japanese question is analyzed to get the ex-pected answer type.After that, the mono-lingual QA process is in-voked.
The translated Japanese question is used asthe query of the document retrieval to get the doc-uments that include the query terms.
From the re-trieved documents, the answer candidates that matchwith the expected answer type are extracted withtheir location in the documents.
Next, the extractedcandidates are rescored by the two points of views;the passage similarity and the type matching.
Thepassage similarity is calculated between the trans-lated Japanese question and the Japanese passagethat surrounds the answer candidate, while the typematching score is calculated as the likelihood thatthe candidate is matched with the expected answertype.
Finally the reordered candidates are outputtedas the answers of the given question.3 Proposed CLQA SystemOn the other hand, Figure 2 shows the configurationof our proposed cross-lingual QA system.
It doesnot use the machine translation (nor the dictionary-based translation) as the pre-processing of the inputEnglish question.
The original English question isEnglishQuestionDocument RetrievalJapaneseDocumentsAnswer ExtractionAnswerCandidatesPassage SimilarityCalculationType Matching ScoreCalculationAnswer RescoringJapaneseAnswerJapaneseDocumentCollectionExpected AnswerType DetectionExpectedAnswer TypeFigure 2: The configuration of the proposed CLQAsystem.used directly in the QA process.
In order to makethis approach possible, the two subsystems, the doc-ument retrieval subsystem and the passage similar-ity calculation subsystem, which are pointed by thedirect arrow from the English question and are em-phasized by the thick frames in Figure 2, are cross-lingualized to accept the English question directlyinstead of the Japanese question, by means of incor-porating the statistical machine translation (SMT)process deeply into them.In the following two subsections, we will explainhow these two subsystems can deal with the En-glish question directly.
The document retrieval sub-system is modified so that the Japanese documentsare indexed by English terms.
The word transla-tion probability used in the SMT is used to index theJapanese document with the corresponding Englishterms without losing the consistency.
The passagesimilarity calculation subsystem calculates the sim-ilarity between an English question and a Japanesepassage in terms of the probability that the Japanesepassage is translated into the English question.3.1 Document RetrievalGiven an English question sentence, the documentretrieval subsystem of our proposed CLQA systemretrieves Japanese documents directly.
In order to doso, each Japanese document in the target collection752How much did the Japan Bank for International Cooperationdecide to loan to the Taiwan High-Speed Corporation?Q??????????
?????????????????????????...??????????????????????
???????????
??????????????????????????????????????
????????????????????????????????????????????????????????
?...???
?articleheadlineprevious sentencetarget sentence (including an answer candidate)next sentencean answer candidateH(S) = S{ } SHS{ } S?1S{ } SS+1{ } SHS?1S{ } SHSS+1{ } S?1SS+1{ } SHS?1SS+1{ }{ }SHS?1SS+1Figure 3: An examples of a question and the corre-sponding passage candidates.has been indexed by English terms by using the wordtranslation probability used in the SMT framework.The expected term frequency tf(e,D) of an En-glish term e that would be used as an index to aJapanese document D can be estimated by the fol-lowing equation.tf(e,D) =?j?Dt(e|j)tf(j,D) (1)where tf(j,D) is the term frequency of a Japaneseterm j in D and t(e|j) is the word translation prob-ability that j is translated into e. The probabilityt(e|j) is trained by using a large parallel corpus asthe SMT framework.
Because the expected term fre-quency tf(e,D) is consistent with tf(j,D) that iscalculated from the statistics of D, the conventionalvector space IR model based on the TF-IDF termweighting can be used for implementing our IR sub-system.
We used GETA 1 as the IR engine in ourCLQA system.3.2 SMT based Passage RetrievalIn order to enable the direct passage retrieval, wherethe query and the passage are in different languages,the statistical machine translation is utilized to cal-culate the similarity between them.
In order words,we calculate the similarity between them as theprobability that the Japanese passage is translatedinto the English question.The similarity sim(Q,S|A) between a questionQ and a sentence S including an answer candidateA is calculated by the following equation.sim(Q,S|A) = maxD?H(S)P (Q|D ?
A) (2)1http://geta.ex.nii.ac.jpwhere P (Q|D ?
A) is the probability that a wordsequence D except A is translated into a questionsentence Q, and H(S) is the set of the candidatepassage (term sequences) that are related to a sen-tence S. The set consists of S and the power set ofSH, S?1, and S+1, where SHis the headline of thearticle that S belongs, S?1is the previous sentenceof S, and S+1is the next sentence of S (Figure 3).In this paper, we use IBM model 1 (Brown et al,1993) in order to get the probability P (Q|D?A) asfollows.P (Q|D ?
A) =1(n+ 1)mm?j=1?i=1,???,k?1,k+l+1,??
?,nt(qj|di)(3)where q1?
?
?
qmis a English term sequence ofa question Q, d1?
?
?
dnis a Japanese term se-quence of a candidate passage D, dk?
?
?
dk+lisa Japanese term sequence of an answer candi-date A.
Therefore, the Japanese term sequenced1, ?
?
?
, dk?1, dk+l+1, ?
?
?
, dn(= D - A) is just D ex-cept A.
We exclude the answer term sequence Afrom the calculation of the translation probability,because the English terms that corresponds to theanswer should not be appeared in the question sen-tence as the nature of question answering.4 Experimental EvaluationThe experimental evaluation was conducted to seethe total performance of cross language question an-swering by using our proposed method.4.1 Test collectionsThe NTCIR-5 CLQA1 test collection (Sasaki etal., 2005) and the NTCIR-6 CLQA2 test collection(Sasaki et al, 2007) for English-to-Japanese taskwere used for the evaluation.
Each collection con-tains 200 factoid questions in English.
The targetdocuments for CLQA1 are two years newspaper ar-ticles from ?YOMIURI SHINBUN?
(2000-2001),while those for CLQA2 are two years articles from?MAINICHI SHINBUN?
(1998-1999).In the test collections, the answer candidates arejudged with three categories; Right, Unsupported,and Wrong.
The answer labeled Right is correctand supported by the document that it is from.
Theanswer labeled Unsupported is correct but not sup-ported by the document that it is from.
The answerlabeled Wrong is incorrect.
We used two kind ofgolden set for our evaluation: the set including only753Right answers (referred as to R) and the set includ-ing Right and Unsupported answers (referred as toR+U).Note that the evaluation results obtained fromCLQA2 are more reliable than that from CLQA1,because we participated in CLQA2 formal run withour proposed method (and our reference method la-beled DICT) and most of the answers by the systemwere manually checked for the pooling.4.2 Translation ModelThe translation model used for our method wastrained from the following English-Japanese paral-lel corpus.?
170,379 example sentence pairs from theJapanese-English and English-Japanese dictio-naries.?
171,186 sentence pairs from newspaper articlesobtained by the automatic sentence alignment(Utiyama and hitoshi Isahara, 2003).A part of the latter sentence pairs were ob-tained from the paired newspapers that are ?YOMI-URI SHINBUN?
and its English translation ?DailyYomiuri?.
Because the target documents of CLQA1are the articles from ?YOMIURI SHINBUN?
asdescribed above, the corresponding sentence pairs,which are extracted from the articles from 2000 to2001, were removed from the training corpus forCLQA1.Before training the translation model, both En-glish and Japanese sides of the sentence pairs in par-allel corpus were normalized.
For the sentences ofJapanese side, the inflectional words were normal-ized to their basic forms by using a Japanese mor-phological analyzer.
For the sentences of Englishside, the inflectional words were also normalizedto their basic forms by using a Part-of-Speech tag-ger and all the words were lowercased.
GIZA++(Och and Ney, 2003) was used for training the IBMmodel 4 from the normalized parallel corpus.
Thevocabulary sizes were about 58K words for Japaneseside and 74K words for English side.
The trainedJapanese-to-English word translation model t(e|j)was used for our proposed document retrieval (Sec-tion 3.1) and passage similarity calculation (Section3.2).4.3 Compared methodsThe proposed method was compared with the sev-eral reference methods.
As the methods from pre-vious works, three pre-translation methods were in-vestigated.The first two methods translate the question by us-ing machine translation.
One of them used a com-mercial off-the-shell machine translation software 2(referred to as RMT).
The other used the statisti-cal machine translation that had been created by us-ing the IBM model 4 obtained from the same par-allel corpus and tools described in Section 4.2, thetri-gram language model constructed by using thetarget documents of CLQA1, and the existing SMTdecoder (Germann, 2003) (referred to as SMT).The two methods, RMT and SMT, differ only inthe translation methods, while their backend mono-lingual QA systems are common.The third method translates the question by us-ing translation dictionary (referred to as DICT).The cross-lingual IR system described in (Fujii andIshikawa, 2001) was used for our ?document re-trieval?
subsystem in Figure 2.
The CLIR systemenhances the basic translation dictionary, which hasabout 1,000,000 entries, with the compound wordsobtained by using the statistics of the target doc-uments and with the borrowed words by using thetransliteration method.
Note that, as the other partsof the system than the document retrieval, includ-ing proposed SMT based passage retrieval, are allidentical to the proposed method, this comparison isfocused only on the difference in the document re-trieval methods.In order to investigate the performance if the idealtranslation is made, the reference Japanese transla-tions of the English questions included in the testcollections were used as the input of the mono-lingual QA system (referred to as JJ).As the variations of the proposed method, the fol-lowing four methods were compared.Proposed The same method as described in Section3.Proposed +r The document retrieval score is alsoused to rescore the answer candidates in?Rescoring?
subsystem in Figure 2, in addi-tion to the passage similarity score and the typematching score.Proposed -p For the passage similarity calculation,the passage is always fixed only the central sen-tence S, i.e.
the equation (2) is replaced by thefollowing.sim(Q,S|A) = P (Q|S ?
A) (4)Proposed -p+r Combination of above two modifi-cations.2?IBM Japan, honyaku-no-oosama ver.
5?754Table 1: Comparison of the JJ results between the test collections.test collection R R+UTop1 Acc.
Top5 Acc.
MRR Top1 Acc.
Top5 Acc.
MRRCLQA1 0.140 0.300 0.196 0.260 0.535 0.354CLQA2 0.245 0.410 0.307 0.270 0.530 0.366Table 2: The performances of the proposed and ref-erence CLQA systems with respect to CLQA1 testcollection.method Top1 Acc.
Top5 Acc.
MRRRMT 0.065 0.175 0.099SMT 0.060 0.175 0.098Dict 0.095 0.195 0.134Proposed 0.090 0.225 0.146Table 3: The performances among the proposedmethods with respect to CLQA1 test collection.method Top1 Acc.
Top5 Acc.
MRRProposed 0.090 0.225 0.146Proposed +r 0.105 0.285 0.173Proposed -p 0.105 0.245 0.155Proposed -p+r 0.120 0.280 0.178JJ 0.260 0.535 0.3544.4 Evaluation MetricsEach system outputted five ranked answers a1?
?
?
a5for each question q.
We investigated the perfor-mance of the systems in terms of three evaluationmetrics that are obtained by averaging over all thequestions: the accuracy of the top ranked answers(referred to as Top 1 Acc.
), the accuracy of up-tofifth ranked answers (referred to as Top 5 Acc.
), andthe reciprocal rank (referred to asMRR)RR(q) cal-culated by the following equation.rr(ai) ={1/i if aiis a correct answer0 otherwise (5)RR(q) = maxairr(ai) (6)4.5 ResultsFirstly, we compared the results obtained by usingCLQA1 test collection with that obtained by usingCLQA2.
Table 1 shows the results for JJ system.By using the R judgment, the JJ results of CLQA1was much worse than that of CLQA2, while the re-sults were almost same by using the R+U judgment.Because the difference with respect to the difficul-ties between the two test collections seems small andthe results from CLQA2 are more reliable, we con-cluded that the R judgment of CLQA1 was unreli-able.
Therefore, for CLQA1 test collection, we onlyinvestigated the result by using R+U judgment.Secondly, we compared the proposed method(Proposed) with the previous methods (RMT,SMT, and Dict).
Table 2 shows the results with re-spect to CLQA1 test collection.
The two methodsbased on the machine translation (RMT and SMT)indicated almost same performance, while the per-formance of the proposed method was about 1.3 to1.5 times better for CLQA1.
Especially, because thesame training data was used to build the translationmodels both in SMT and Proposed, it was shownthat the method to build the SMT model in the QAprocess was better than that to use the same SMTmodel for pre-processing (pre-translating) the inputsentence.The DICT performed almost same as the Pro-posed for CLQA1, while Proposed was 1.7 to 1.9times better than DICT for CLQA2 as shown in Ta-ble 4.
Note again that this comparison was focusedon the document retrieval subsystem, because thepassage retrieval subsystems of these two methodswere same.Thirdly, the variations between the proposedmethods were compared.
Table 3 shows the resultswith respect to CLQA1 test collection.
For CLQA1,both the additional use of the document retrievalscore (+r) and the use of the fixed central sentencefor passage similarity calculation (-p) improved theperformance.
However, for CLQA2, the documentretrieval score (+r) did not contribute to improve theperformance, as shown in Table 4.Finally, seeing from the comparison between JJand Proposed, it was shown that the performance ofthe proposed CLQA system was about half of that ofthe ideal CLQA system.755Table 4: The performances of the proposed and reference CLQA systems with respect to CLQA2 testcollection.methods R R+UTop1 Acc.
Top5 Acc.
MRR Top1 Acc.
Top5 Acc.
MRRDict 0.070 0.155 0.102 0.100 0.275 0.163Proposed 0.130 0.200 0.155 0.165 0.295 0.210Proposed +r 0.120 0.220 0.153 0.155 0.325 0.211JJ 0.245 0.410 0.307 0.270 0.530 0.3665 ConclusionIn this paper, a novel approach for CLQA was pro-posed.
The proposed method did not translate theinput question in source language into the targetlanguage as the preprocessing of QA process.
In-stead, the statistical machine translation was deeplyincorporated into the two QA subsystems in orderto deal with the question in source language directlyin the QA process.
Especially, SMT-based passageretrieval was explored.For the passage similarity calculation in this pa-per, the simple IBM model 1 was used.
In the futurework, we will investigate if the more sophisticatedtranslation model or that specialized for CLQA taskcan improve the performance further.ReferencesAdam Berger and John Lafferty.
1999.
Information retrieval asstatistical translation.
In Proceedings of the 22nd AnnualConference on Research and Development in InformationRetrieval (ACM SIGIR), pages 222?229.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLinguistics, 18(4):263?311.W.
Bruce Croft and John Lafferty, editors.
2003.
LanguageModeling for Information Retrieval.
Kluwer Academic Pub-lishers.Atsushi Fujii and Tetsuya Ishikawa.
2001.
Japanese/englishcross-language information retrieval: Exploration of querytranslation and transliteration.
Computers and the Humani-ties, 35(4):389?420.Ulrich Germann.
2003.
Greedy decoding for statistical ma-chine translatioin in slmost linear time.
In Proceedings ofHLT-NAACL.Hideki Isozaki, Katsuhito Sudoh, and Hajime Tsukada.
2005.NTT?s japanese-english cross-language question answeringsystem.
In Proceedings of The Fifth NTCIRWorkshop, pages186?193.Bernardo Magnini, Alessandro Vallin, Christelle Ayache, Gre-gor Erbach, Anselmo Pe nas, Maarten de Rijke, PauloRocha, Kiril Simov, and Richard Sutcliffe.
2003.
Overviewof the CLEF 2004 multilingual question answering track.
InMultilingual Information Access for Text, Speech and Im-ages, pages 371?391.Tatsunori Mori and Masami Kawagishi.
2005.
A method ofcross language question-answering based on machine trans-lation and transliteration.
In Proceedings of The Fifth NTCIRWorkshop, pages 215?222.Vanessa Murdock and W. Bruce Croft.
2004.
Simple trans-lation models for sentence retrieval in factoid question an-swering.
In Proceedings of the Workshop on InformationRetrieval for Question Answering.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Yutaka Sasaki, Hsin-Hsi Chen, Kuang hua Chen, and Chuan-JieLin.
2005.
Overview of the NTCIR-5 cross-lingual questionanswering task (clqa1).
In Proceedings of The Fifth NTCIRWorkshop, pages 175?185.Yutaka Sasaki, Chuan-Jie Lin, Kuang hua Chen, and Hsin-HsiChen.
2007.
Overview of the NTCIR-6 cross-lingual ques-tion answering (clqa) task.
In Proceedings of The NTCIR-6Workshop Meeting.Kei Shimizu, Tomoyosi Akiba, Atsushi Fujii, and KatunobuItou.
2005.
Bi-directional cross language question answer-ing using a single monolingual QA system.
In Proceedingsof The Fifth NTCIR Workshop, pages 236?237.Masao Utiyama and hitoshi Isahara.
2003.
Reliable measuresfor aligning japanese-english news articles and sentences.
InProceedings of Annual Meeting of the Association for Com-putational Linguistics, pages 72?79.E.
Voorhees and D. Tice.
1999.
The TREC-8 question answer-ing track evaluation.
In Proceedings of the 8th Text RetrievalConference, pages 83?106, Gaithersburg, Maryland.Jinxi Xu, Ralph Weischedel, and Chanh Nguyen.
2001.
Evalu-ating a probabilistic model for cross-lingual information re-trieval.
In Proceedings of the 24th Annual Conference onResearch and Development in Information Retrieval (ACMSIGIR), pages 105?110.756
