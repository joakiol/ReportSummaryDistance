Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 628?635, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsWBI-DDI: Drug-Drug Interaction Extraction using Majority VotingPhilippe Thomas Mariana Neves Tim Rockta?schel Ulf LeserHumboldt-Universita?t zu BerlinKnowledge Management in BioinformaticsUnter den Linden 6Berlin, 10099, Germany{thomas,neves,trocktae,leser}@informatik.hu-berlin.deAbstractThis work describes the participation of theWBI-DDI team on the SemEval 2013 ?
Task9.2 DDI extraction challenge.
The task con-sisted of extracting interactions between pairsof drugs from two collections of documents(DrugBank and MEDLINE) and their clas-sification into four subtypes: advise, effect,mechanism, and int.
We developed a two-stepapproach in which pairs are initially extractedusing ensembles of up to five different clas-sifiers and then relabeled to one of the fourcategories.
Our approach achieved the sec-ond rank in the DDI competition.
For interac-tion detection we achieved F1 measures rang-ing from 73 % to almost 76 % depending onthe run.
These results are on par or even higherthan the performance estimation on the train-ing dataset.
When considering the four inter-action subtypes we achieved an F1 measure of60.9 %.1 IntroductionA drug-drug interaction (DDI) can be described asinterplay between drugs taken during joint adminis-tration.
DDIs usually lead to an increase or decreasein drug effects when compared to isolated treatment.For instance, sildenafil (Viagra) in combination withnitrates can cause a potentially live-threatening de-crease in blood pressure (Cheitlin et al 1999).
It istherefore crucial to consider potential DDI effectswhen co-administering drugs to patients.
As thelevel of medication generally is raising all over theworld, the potential risk of unwanted side effects,such as DDIs, is constantly increasing (Haider et al2007).Only a fraction of knowledge about DDIs iscontained in specialized databases such as Drug-Bank (Knox et al 2011).
These structured knowl-edge bases are often the primary resource of infor-mation for researchers.
However, the majority ofnew DDI findings are still initially reported in scien-tific publications, which results in the situation thatstructured knowledge bases lag behind recently pub-lished research results.
Thus, there is an urgent needfor researchers and database curators to cope withthe fast growth of biomedical literature (Hunter andCohen, 2006).The SemEval 2013 ?
Task 9.2 (Extraction ofDrug-Drug Interactions from BioMedical Texts)is a competitive evaluation of methods for ex-tracting mentions of drug-drug interactions fromtexts (Segura-Bedmar et al 2013).
For training,the organizers provide a corpus annotated with drug-names and interactions between them.
This corpusis composed of 572 articles collected from Drug-Bank and 142 PubMed abstracts.
Interactions arebinary (always between two drugs) and undirected,as target and agent roles are not annotated.
Fur-thermore, the two interacting drugs are always men-tioned within the same sentence.
In contrast tothe previous DDI-challenge 2011 (Segura-Bedmaret al 2011), four different DDI-subtypes (advise,effect, mechanism, and int) have been introduced.Details about the four subclasses can be found in thetask?s annotation guideline.628Figure 1: Workflow developed for the SemEval 2013Task 9.2 challenge.2 MethodsBinary relationship extraction is often tackled as apair-wise classification problem, where all(n2)co-occurring entities in a sentence are classified as in-teracting or not.
To account for the four differentsubtypes of DDIs, the problem definition could betranslated into a multiclass classification problembetween all co-occurring entities.Contrary to that, we propose a two step strat-egy: First, we detect general drug-drug interac-tions regardless of subtype using a multitude of dif-ferent machine-learning methods.
The output ofthese methods is aggregated using a majority vot-ing approach.
Second, detected interactions are re-classified into one of the four possible DDI cate-gories.
The latter is referred to as DDI relabelingthroughout this paper.
A detailed view on the pro-posed workflow is depicted in Figure 1.2.1 PreprocessingSentences have been parsed using Charniak-JohnsonPCFG reranking-parser (Charniak and Johnson,2005) with a self-trained re-ranking model aug-mented for biomedical texts (McClosky, 2010).
Re-sulting constituent parse trees have been convertedinto dependency graphs using the Stanford con-verter (De Marneffe et al 2006).
In the last step, wecreated an augmented XML using the open sourceCorpus SentencesPairsPositive Negative TotalDrugBank 5,675 3,788 22,217 26,005MEDLINE 1,301 232 1,555 1,787Table 1: Basic statistics of the DDI training corpus shownfor DrugBank and MEDLINE separately.framework from Tikk et al(2010).
This XML fileencompasses tokens with respective part-of-speechtags, constituent parse tree, and dependency parsetree information.
This format has been subsequentlytransformed into a related XML format1 used by twoof the utilized classifiers.
Properties of the trainingcorpus are shown for DrugBank and MEDLINE inTable 1.2.2 Machine Learning MethodsTikk et al(2010) systematically analyzed nine dif-ferent machine learning approaches for the extrac-tion of undirected binary protein-protein interac-tions.
This framework has been successfully appliedto other domains, such as the I2B2 relation extrac-tion challenge (Solt et al 2010), the previous DDIextraction challenge (Thomas et al 2011), and tothe extraction of neuroanatomical connectivity state-ments (French et al 2012).Drug entities are blinded by replacing the entityname with a generic string to ensure the generalityof the approach.
Without entity blinding drug namesare incorporated as features, which clearly affectsgeneralization capabilities of a classifier on unseenentity mentions (Pyysalo et al 2008).We decided to use the following methodsprovided by the framework: All-paths graph(APG) (Airola et al 2008), shallow lin-guistic (SL) (Giuliano et al 2006), subtree(ST) (Vishwanathan and Smola, 2002), subset tree(SST) (Collins and Duffy, 2001), and spectrum tree(SpT) (Kuboyama et al 2007) method.
The SLmethod uses only shallow linguistic features, i.e.,token, stem, part-of-speech tag and morphologicproperties of the surrounding words.
APG buildsa classifier using surface features and a weighting1https://github.com/jbjorne/TEES/wiki/Interaction-XML629scheme for dependency parse tree features.
Theremaining three classifier (ST, SST, and SpT) buildkernel functions based on different subtree repre-sentations on the constituent parse tree.
To calculatethe constituent?tree kernels ST and SST we usedthe SVM-LIGHT-TK toolkit (Moschitti, 2006).Before applying these methods, constituent parsetrees have been reduced to the shortest-enclosedparse following the recommendations from Zhanget al(2006).
For a more detailed descriptionof the different methods we refer to the originalpublications.In addition to the PPI framework, we alsoemployed the general purpose relationship ex-traction tool ?Turku Event Extraction System?
(TEES) (Bjo?rne et al 2011), a customized versionof the case-based reasoning system Moara (Neveset al 2009), and a self-developed feature basedclassifier which is referred to as SLW.
RegardingTEES, we have used the edge extraction function-ality for performing relationship extraction.
TEESconsiders features related to the tokens (e.g., part-of-speech tags), dependency chains, dependency pathN-grams, entities (e.g., entity types) and external re-sources, such as hypernyms in WordNet.Moara is a case-based reasoning system for theextraction of relationships and events.
During train-ing, interaction pairs are converted into cases andsaved into a HyperSQL database which are re-trieved through case similarity during the classifica-tion.
Cases are composed by the following features:the type of the entities (e.g.
Brand and Group),the part-of-speech tag of the tokens between the twodrugs (inclusive), the tags of the shortest depen-dency path between the two drugs, and the lemmaof the non-entity tokens of the shortest dependencypath using BioLemmatizer (Liu et al 2012).
Wealso consider the PHARE ontology (Coulet et al2011) in the lemma feature: When a lemma matchesany of the synonyms contained in this ontology, thecategory of the respective term is considered instead.Case similarity is calculated by exact feature match-ing, except for the part-of-speech tags whose com-parison is based on global alignment using insertion,deletion, and substitution costs as proposed by Spa-sic et al(2005).SLW is inspired by SL (Giuliano et al 2006;Bunescu and Mooney, 2006) and uses the Breeze2library.
We generate n-grams over sequences ofarbitrary features (e.g.
POS-tags, morphologicaland syntactical features) to describe the global con-text of an entity pair.
Furthermore, we calculatefeatures from the local context of entities, but inaddition to SL, we include domain-specific fea-tures used for identifying and classifying pharma-cological substances (see our paper for DDI Task9.1 (Rockta?schel et al 2013)).
In addition, we takethe name of the classes of a pair?s two entities asfeature to capture that entities of some class (e.g.Brand and Group) are more likely to interact thanothers (e.g.
Brand and Brand).2.3 Ensemble learningSeveral community competitions previously notedthat combinations of predictions from different toolshelp to achieve better results than one methodalone (Kim et al 2009; Leitner et al 2010).
Moreimportantly, it is well known that ensembles increaserobustness by decreasing the risk of selecting a badclassifier (Polikar, 2006).
In this work we combinedthe output of several classifiers by using majorityvoting.
The ensemble is used to predict DDIs re-gardless of the four different subtypes.
This com-plies with the partial match evaluation criterion de-fined by the competition organizers.2.4 RelabelingTo account for DDI subtypes, we compared two ap-proaches: (a) using the subtype prediction of TEES;(b) training a multi-class classifier (SLW) on theavailable training data for DDI subtypes.
We de-cided on using TEES, as it generated superior resultsover SLW (data not shown).
Thus, previously identi-fied DDIs are relabeled into one of the four possiblesubtypes using the most likely interaction subtypefrom TEES.3 Results3.1 Cross validationIn order to compare the different approaches, weperformed document-wise 10-fold cross validation(CV) on the training set.
It has been shown that such2http://www.scalanlp.org/630Type Pairs Precision Recall F1total 3,119 78.6 78.6 78.6effect 1,633 79.8 79.1 79.4mechanism 1,319 79.8 79.2 79.4advise 826 77.3 76.4 76.9int 188 68.5 80.9 74.1Table 4: Performance estimation for relabeling DDIs.Pairs denotes the number of instances of this type in thetraining corpus.a setting provides more realistic performance esti-mates than instance-wise CV (S?tre et al 2008).All approaches have been tested using the samesplits to ensure comparability.
For APG, ST, SST,and SpT we followed the parameter optimizationstrategy defined by Tikk et al(2010).
For TEESand Moara, we used the cost parameter C (50000)and best performing features, respectively, based onthe CV results.
For SL and SLW, we used the defaultparameters.We performed several different CV experiments:First, we performed CV on the two corpora (Drug-Bank and MEDLINE) separately.
Second, datafrom the other corpus has been additionally usedduring the training phase.
This allows us to esti-mate the impact of additional, but potentially differ-ent text.
CV results for DrugBank and MEDLINEare shown in Table 2 and 3 respectively.3.2 RelabelingPerformance of relabeling is evaluated by perform-ing 10-fold CV on the training set using the samesplits as in previous analysis.
Note that this experi-ment is solely performed on positive DDI instancesto estimate separability of the four different DDI-subtypes.
Results for relabeling are shown in Ta-ble 4.3.3 Test datasetFor the test set we submitted results using the fol-lowing three majority voting ensembles.
For Run 1we used Moara+SL+TEES, for Run 2 we usedAPG+Moara+SL+SLW+TEES and for Run 3 weused SL+SLW+TEES.
Due to time constraints wedid not use different ensembles for the two corpora.We rather decided to use ensembles which achievedgenerally good results for both training corpora.
Allclassifiers, except APG, have been retrained on thecombination of MEDLINE and DrugBank usingthe parameter setting yielding the highest F1 in thetraining phase.
For APG, we trained two differentmodels: One model is trained on MEDLINE andDrugBank and one model is trained on DrugBanksolely.
The first model is applied on the MEDLINEtest set and the latter on the DrugBank test set.
Esti-mated results on the training corpus and official re-sults on the test corpus are shown in Table 5.4 Discussion4.1 Training datasetDocument-wise CV results for the DrugBank corpusshow no clear effect when using MEDLINE as ad-ditional training data.
By using MEDLINE duringthe training phase we observe an average decrease of0.3 percentage points (pp) in F1 and an average in-crease of 0.7 pp in area under the receiver operatingcharacteristic curve (AUC).
The strongest impactcan be observed for APG with a decrease of 2.3 ppin F1.
We therefore decided to train APG mod-els for DrugBank without additional MEDLINEdata.
For almost all ensembles (with the excep-tion of APG+SpT+SL) we observe superior resultswhen using only DrugBank as training data.
Inter-estingly, this effect can mostly be attributed to anaverage increase of 3.3 pp in recall, whereas preci-sion remains fairly stable between ensembles usingDrugBank solely and those with additional trainingdata.In contrast for MEDLINE, all methods largelybenefit from additional training data with an aver-age increase of 9.8 pp and 3.6 pp for F1 and AUC re-spectively.
For the ensemble based approaches, weobserve an average increase of 13.8 pp for F1whenusing DrugBank data in addition.When ranking the different methods by F1 andcalculating correlation between the two differ-ent corpora, we observe only a weak correlation(Kendall?s ?
= 0.286, p< 1).
In other words, ma-chine learning methods show varying performance-ranks between the two corpora.
This difference ismost pronounced for SL and SpT, with four ranksdifference between DrugBank and MEDLINE.
Itis noteworthy that the two corpora are not directly631Regular CV Combined CVMethod P R F1 AUC P R F1 AUCSL 61.5 79.0 69.1 92.8 62.1 78.4 69.2 93.0APG 77.2 62.6 69.0 91.5 75.9 59.8 66.7 91.6TEES 77.2 62.0 68.6 87.3 75.5 60.9 67.3 86.9SLW 73.7 60.0 65.9 91.3 73.4 61.2 66.6 91.3Moara 72.1 55.2 62.5 ?
72.0 54.7 62.1 ?SpT 51.4 73.4 60.3 87.3 52.7 71.4 60.6 87.7SST 51.9 61.2 56.0 85.4 55.1 57.1 56.0 86.1ST 47.3 64.2 54.2 82.3 48.3 64.3 54.9 82.7SL+SLW+TEES 76.1 69.9 72.7 ?
75.9 65.3 70.1 ?APG+SL+TEES 79.3 69.9 74.2 ?
79.2 65.4 71.5 ?Moara+SL+TEES 79.9 69.6 74.2 ?
79.6 65.1 71.6 ?Moara+SL+APG 81.4 70.6 75.5 ?
81.3 70.3 75.3 ?APG+Moara+SL+SLW+TEES 84.0 68.1 75.1 ?
83.7 64.2 72.6 ?APG+SpT+TEES 76.8 68.0 72.1 ?
77.1 63.4 69.6 ?APG+SpT+SL 68.7 74.8 71.5 ?
69.7 73.8 71.6 ?Table 2: Cross validation results on DrugBank corpus.
Regular CV is training and evaluation on DrugBank only.Combined CV is training on DrugBank and MEDLINE and testing on DrugBank.
Higher F1 between these twosettings are indicated in boldface for each method.
Single methods are ranked by F1.Regular CV Combined CVMethod P R F1 AUC P R F1 AUCTEES 70.7 36.0 44.5 82.2 59.6 46.5 51.4 84.9SpT 37.8 38.6 34.6 78.6 42.3 55.3 47.1 80.4APG 46.5 44.3 42.4 82.3 38.1 62.2 46.4 82.8SST 31.3 37.7 31.8 74.1 36.7 61.7 44.9 79.5SL 43.7 40.1 38.7 78.9 34.7 67.1 44.7 81.1SLW 58.0 14.3 20.4 73.4 50.1 38.0 42.0 82.4Moara 49.8 31.9 37.6 ?
45.6 43.2 41.9 ?ST 25.2 43.8 30.1 70.5 36.1 48.3 39.8 74.2SL+SLW+TEES 73.6 29.0 37.6 ?
55.2 52.7 53.1 ?APG+SL+TEES 60.7 37.9 43.4 ?
49.9 62.4 54.3 ?Moara+SL+TEES 68.0 33.0 42.2 ?
62.1 55.5 57.4 ?Moara+SL+APG 57.7 36.7 42.4 ?
48.3 60.9 52.8 ?APG+Moara+SL+SLW+TEES 73.3 28.3 36.8 ?
60.6 54.4 56.5 ?APG+SpT+TEES 58.5 37.4 41.7 ?
57.5 59.2 57.1 ?APG+SpT+SL 48.3 39.9 40.0 ?
43.6 64.3 51.0 ?Table 3: Cross validation results on MEDLINE corpus.
Regular CV is training and evaluation on MEDLINE only.Combined CV is training on DrugBank and MEDLINE and testing on MEDLINE.
Higher F1 between these twosettings are indicated in boldface for each method.
Single methods are ranked by F1.632EvaluationTraining TestRun 1 Run 2 Run 3 Run 1 Run 2 Run 3P R F1 P R F1 P R F1 P R F1 P R F1 P R F1Partial 78.7 67.3 72.6 82.9 66.4 73.7 75.2 67.6 71.2 84.1 65.4 73.6 86.1 65.7 74.5 80.1 72.2 75.9Strict 65.7 56.1 60.5 70.0 56.0 62.2 63.0 56.7 59.7 68.5 53.2 59.9 69.5 53.0 60.1 64.2 57.9 60.9-mechanism 61.8 49.7 55.1 68.1 50.0 57.7 59.2 50.3 54.4 72.2 51.7 60.2 74.9 52.3 61.6 65.3 58.6 61.8-effect 68.8 57.9 62.9 71.8 57.6 63.9 66.1 57.4 61.5 63.7 57.5 60.4 63.6 55.8 59.5 60.7 61.4 61.0-advise 64.6 60.5 62.5 68.2 59.7 63.6 61.1 61.5 61.3 73.3 53.4 61.8 74.5 55.7 63.7 69.0 58.4 63.2-int 68.6 50.0 57.8 75.4 52.1 61.6 70.9 56.9 63.1 67.8 41.7 51.6 67.3 38.5 49.0 67.8 41.7 51.6Table 5: Relation extraction results on the training and test set.
Run 1 builds a majority voting on Moara+SL+TEES,Run 2 on APG+Moara+SL+SLW+TEES, and Run 3 on SL+SLW+TEES.
Partial characterizes only DDI detectionwithout classification of subtypes, whereas strict requires correct identification of subtypes as well.comparable, as DrugBank is one order of magnitudelarger in terms of instances than the MEDLINE cor-pus.
Additionally, documents come from differentsources and it is tempting to speculate that theremight be a certain amount of domain specificity be-tween DrugBank and MEDLINE sentences.We tested for domain specificity by performingcross-corpus experiments, i.e., we trained a classi-fier on DrugBank, applied it on MEDLINE and viceversa.
When training on MEDLINE and testingon DrugBank, we observe an average decrease ofabout 15 pp in F1 in comparison to DrugBank in-domain CV results.
For the other setting, we observea lower decrease of approximately 5 pp in compari-son to MEDLINE in-domain CV results.From the current results, it seems that the doc-uments from DrugBank and MEDLINE have dif-ferent syntactic properties.
However, this requires amore detailed analysis of different aspects like dis-tribution of sentence length, negations, or passivesbetween the two corpora (Cohen et al 2010; Tikket al 2013).
We assume that transfer learning tech-niques could improve results on both corpora (Panand Yang, 2010).The DDI-relabeling capability of TEES is verybalanced with F1 measures ranging from 74.1 % to79.4 % for all four DDI subclasses.
This is unex-pected since classes like ?effect?
occur almost tentimes more often than classes like ?int?
and classi-fiers often have problems with predicting minorityclasses.4.2 Test datasetOn the test set, our best run achieves an F1 of 76 %using the partial evaluation schema.
This is slightlybetter than the performance for DrugBank trainingdata shown in Table 2 and substantially better thanestimations for MEDLINE (see Table 3).
WithF1 measures ranging between 74 % to 76 % onlyminor performance differences can be observed be-tween the three different ensembles.When switching from partial to strict evaluationscheme an average decrease of 15 pp in F1 can be ob-served.
As estimated on the training data, relabelingperformance is indeed very similar for the differentDDI-subtypes.
Only for the class with the least in-stances (int), a larger decrease in comparison to theother three classes can be observed for the test set.In general, results for test set are on par or higherthan results for the training set.5 ConclusionIn this paper we presented our approach for theSemEval 2013 ?
Task 9.2 DDI extraction challenge.Our strategy builds on a cascaded (coarse to finegrained) classification strategy, where a majorityvoting ensemble of different methods is initiallyused to find generic DDIs.
Predicted interactionsare subsequently relabeled into four different sub-types.
DDI extraction seems to be a more difficulttask for MEDLINE abstracts than for DrugBank ar-ticles.
In our opinion, this cannot be fully attributedto the slightly higher ratio of positive instances inDrugBank and points towards structural differencesbetween the two corpora.AcknowledgmentsThis work was supported by the German ResearchFoundation (DFG) [LE 1428/3-1] and the Federal633Ministry of Economics and Technology (BMWi)[KF 2205209MS2].ReferencesA.
Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala,F.
Ginter, and T. Salakoski.
2008.
All-paths graphkernel for protein-protein interaction extractionwith evaluation of cross-corpus learning.
BMCBioinformatics, 9 Suppl 11:S2.J.
Bjo?rne, J. Heimonen, F. Ginter, A. Airola,T.
Pahikkala, and T. Salakoski.
2011.
ExtractingContextualized Complex Biological Events withRich Graph-Based Features Sets.
ComputationalIntelligence, 27(4):541?557.R.
C. Bunescu and R. J. Mooney.
2006.
Sub-sequence Kernels for Relation Extraction.
Ad-vances in Neural Information Processing Sys-tems, 18:171.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proc.
of ACL?05, pages 173?180.M.
D. Cheitlin, A. M. Hutter, R. G. Brindis, P. Ganz,S.
Kaul, R. O. Russell, and R. M. Zusman.
1999.Use of sildenafil (viagra) in patients with cardio-vascular disease.
J Am Coll Cardiol, 33(1):273?282.K.
Cohen, Helen L Johnson, Karin Verspoor,Christophe Roeder, and Lawrence E Hunter.2010.
The structural and content aspects of ab-stracts versus bodies of full text journal articlesare different.
BMC Bioinformatics, 11:492.M.
Collins and N. Duffy.
2001.
Convolution Kernelsfor Natural Language.
In Proc.
of NIPS?01, pages625?632.A.
Coulet, Y. Garten, M. Dumontier, R. Altman,M.
Musen, and N. Shah.
2011.
Integration andpublication of heterogeneous text-mined relation-ships on the semantic web.
Journal of BiomedicalSemantics, 2(Suppl 2):S10.M.C.
De Marneffe, B. MacCartney, and C.D.
Man-ning.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In Proc.
of LREC2006, pages 449?454.L.
French, S. Lane, L. Xu, C. Siu, C. Kwok, Y. Chen,C.
Krebs, and P. Pavlidis.
2012.
Application andevaluation of automated methods to extract neu-roanatomical connectivity statements from freetext.
Bioinformatics, 28(22):2963?2970.C.
Giuliano, A. Lavelli, and L. Romano.
2006.
Ex-ploiting Shallow Linguistic Information for Re-lation Extraction from Biomedical Literature.
InProc.
of EACL?06, pages 401?408.S.
I. Haider, K. Johnell, M. Thorslund, and J. Fast-bom.
2007.
Trends in polypharmacy and potentialdrug-drug interactions across educational groupsin elderly patients in Sweden for the period 1992- 2002.
Int J Clin Pharmacol Ther, 45(12):643?653.L.
Hunter and K. Cohen.
2006.
Biomedical languageprocessing: what?s beyond PubMed?
Mol Cell,21(5):589?594.J.D.
Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-jii.
2009.
Overview of BioNLP?09 shared task onevent extraction.
In Proc.
of BioNLP?09, pages1?9.C.
Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frol-kis, A. Pon, K. Banco, C. Mak, V. Neveu,Y.
Djoumbou, R. Eisner, A. Chi Guo, and D. SWishart.
2011.
Drugbank 3.0: a comprehensiveresource for ?omics?
research on drugs.
NucleicAcids Res, 39(Database issue):D1035?D1041.T.
Kuboyama, K. Hirata, H. Kashima, K. F. Aoki-Kinoshita, and H. Yasuda.
2007.
A SpectrumTree Kernel.
Information and Media Technolo-gies, 2(1):292?299.F.
Leitner, S.A. Mardis, M. Krallinger, G. Ce-sareni, L.A. Hirschman, and A. Valencia.
2010.An overview of BioCreative II.
5.
IEEEIEEE/ACM Transactions on Computational Biol-ogy and Bioinformatics, pages 385?399.H.
Liu, T. Christiansen, W. Baumgartner, andK.
Verspoor.
2012.
Biolemmatizer: a lemmatiza-tion tool for morphological processing of biomed-ical text.
Journal of Biomedical Semantics, 3(1):3.D.
McClosky.
2010.
Any Domain Parsing: Auto-matic Domain Adaptation for Natural LanguageParsing.
Ph.D. thesis, Brown University.A.
Moschitti.
2006.
Efficient Convolution Kernelsfor Dependency and Constituent Syntactic Trees.In Proc.
of ECML?06, pages 318?329.M.
Neves, J.-M. Carazo, and A. Pascual-Montano.2009.
Extraction of biomedical events using case-based reasoning.
In Proc.
of BioNLP?09, pages68?76.S.
J. Pan and Q. Yang.
2010.
A Survey on Transfer634Learning.
IEEE Transactions on Knowledge andData Engineering, 22(10):1345?1359.R.
Polikar.
2006.
Ensemble Based Systems in Deci-sion Making.
IEEE Circuits and Systems Maga-zine, 6(3):21?45.S.
Pyysalo, R. S?tre, J. Tsujii, and T. Salakoski.2008.
Why Biomedical Relation Extraction Re-sults are Incomparable and What to do about it.In Proc.
of SMBM?08, pages 149?152.T.
Rockta?schel, T. Huber, M. Weidlich, and U. Leser.2013.
WBI-NER: The impact of domain-specificfeatures on the performance of identifying andclassifying mentions of drugs.
In Proceedings ofthe 7th International Workshop on Semantic Eval-uation (SemEval 2013).R.
S?tre, K. Sagae, and J. Tsujii.
2008.
Syntacticfeatures for protein-protein interaction extraction.In Proc.
of LBM?07.I.
Segura-Bedmar, P.
Mart?
?nez, and M. Herrero-Zazo.
2013.
Semeval-2013 task 9: Extraction ofdrug-drug interactions from biomedical texts.
InProc.
of the 7th International Workshop on Se-mantic Evaluation (SemEval 2013).I.
Segura-Bedmar, P.
Mart?
?nez, and D. Sanchez-Cisneros.
2011.
The 1st ddiextraction-2011 chal-lenge task: Extraction of drug-drug interactionsfrom biomedical text.
In Proc.
of the 1st Chal-lenge Task on Drug-Drug Interaction Extraction2011, pages 1?9.I.
Solt, F. P. Szidarovszky, and D. Tikk.
2010.
Con-cept, Assertion and Relation Extraction at the2010 i2b2 Relation Extraction Challenge usingparsing information and dictionaries.
In Proc.
ofi2b2/VA Shared-Task.I.
Spasic, S. Ananiadou, and J. Tsujii.
2005.
MaS-TerClass: a case-based reasoning system for theclassification of biomedical terms.
Bioinformat-ics, 21(11):2748?2758.P.
Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.2011.
Relation extraction for drug-drug interac-tions using ensemble learning.
In Proc.
of the1st Challenge Task on Drug-Drug Interaction Ex-traction 2011, pages 11?18.D.
Tikk, I. Solt, P. Thomas, and U. Leser.
2013.A detailed error analysis of 13 kernel methodsfor protein-protein interaction extraction.
BMCBioinformatics, 14(1):12.D.
Tikk, P. Thomas, P. Palaga, J. Hakenberg, andU.
Leser.
2010.
A comprehensive benchmark ofkernel methods to extract protein-protein interac-tions from literature.
PLoS Comput Biol, 6.S.
V. N. Vishwanathan and A. J. Smola.
2002.
FastKernels for String and Tree Matching.
In Proc.
ofNIPS?02, pages 569?576.M.
Zhang, J. Zhang, J. Su, and G. Zhou.
2006.
AComposite Kernel to Extract Relations betweenEntities with Both Flat and Structured Features.In Proc.
of ICML?06, pages 825?832.635
