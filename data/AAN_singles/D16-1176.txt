Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1703?1712,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsModelling Interaction of Sentence Pair with Coupled-LSTMsPengfei Liu Xipeng Qiu?
Yaqian Zhou Jifan Chen Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{pfliu14,xpqiu,zhouyaqian,jfchen14, xjhuang}@fudan.edu.cnAbstractRecently, there is rising interest in modellingthe interactions of two sentences with deepneural networks.
However, most of the exist-ing methods encode two sequences with sepa-rate encoders, in which a sentence is encodedwith little or no information from the othersentence.
In this paper, we propose a deeparchitecture to model the strong interactionof sentence pair with two coupled-LSTMs.Specifically, we introduce two coupled waysto model the interdependences of two LSTMs,coupling the local contextualized interactionsof two sentences.
We then aggregate these in-teractions and use a dynamic pooling to selectthe most informative features.
Experiments ontwo very large datasets demonstrate the effi-cacy of our proposed architectures.1 IntroductionDistributed representations of words or sentenceshave been widely used in many natural languageprocessing (NLP) tasks, such as text classification(Kalchbrenner et al, 2014; Liu et al, 2015), ques-tion answering and machine translation (Sutskeveret al, 2014) and so on.
Among these tasks, a com-mon problem is modelling the relevance/similarityof the sentence pair, which is also called text seman-tic matching.Recently, deep learning based models is rising asubstantial interest in text semantic matching andhave achieved some great progresses (Hu et al,2014; Qiu and Huang, 2015; Wan et al, 2016).
?Corresponding author.According to the phases of interaction betweentwo sentences, previous models can be classifiedinto three categories.Weak interaction Models Some early works fo-cus on sentence level interactions, such as ARC-I(Hu et al, 2014), CNTN(Qiu and Huang, 2015)and so on.
These models first encode two sequenceswith some basic (Neural Bag-of-words, BOW) oradvanced (RNN, CNN) components of neural net-works separately, and then compute the matchingscore based on the distributed vectors of two sen-tences.
In this paradigm, two sentences have no in-teraction until arriving final phase.Semi-interaction Models Some improved meth-ods focus on utilizing multi-granularity represen-tation (word, phrase and sentence level), such asMultiGranCNN (Yin and Schu?tze, 2015) and Multi-Perspective CNN (He et al, 2015).
Another kindof models use soft attention mechanism to obtainthe representation of one sentence by depending onrepresentation of another sentence, such as ABCNN(Yin et al, 2015), Attention LSTM(Rockta?schel etal., 2015; Hermann et al, 2015).
These models canalleviate the weak interaction problem, but are stillinsufficient to model the contextualized interactionon the word as well as phrase level.Strong Interaction Models These models di-rectly build an interaction space between two sen-tences and model the interaction at different posi-tions, such as ARC-II (Hu et al, 2014), MV-LSTM(Wan et al, 2016) and DF-LSTMs(Liu et al, 2016).These models can easily capture the difference be-tween semantic capacity of two sentences.In this paper, we propose a new deep neural net-work architecture to model the strong interactions1703of two sentences.
Different with modelling two sen-tences with separated LSTMs, we utilize two inter-dependent LSTMs, called coupled-LSTMs, to fullyaffect each other at different time steps.
The out-put of coupled-LSTMs at each step depends on bothsentences.
Specifically, we propose two interdepen-dent ways for the coupled-LSTMs: loosely coupledmodel (LC-LSTMs) and tightly coupled model (TC-LSTMs).
Similar to bidirectional LSTM for singlesentence (Schuster and Paliwal, 1997; Graves andSchmidhuber, 2005), there are four directions can beused in coupled-LSTMs.
To utilize all the informa-tion of four directions of coupled-LSTMs, we aggre-gate them and adopt a dynamic pooling strategy toautomatically select the most informative interactionsignals.
Finally, we feed them into a fully connectedlayer, followed by an output layer to compute thematching score.The contributions of this paper can be summa-rized as follows.1.
Different with the architectures of using sim-ilarity matrix, our proposed architecture di-rectly model the strong interactions of two sen-tences with coupled-LSTMs, which can cap-ture the useful local semantic relevances of twosentences.
Our architecture can also capturethe multiple granular interactions by severalstacked coupled-LSTMs layers.2.
Compared to previous works on text matching,we perform extensive empirical studies on twovery large datasets.
The massive scale of thedatasets allows us to train a very deep neu-ral network and present an elaborate qualitativeanalysis of our models, which gives an intuitiveunderstanding how our model worked.2 Sentence Modelling with LSTMLong short-term memory network (LSTM) (Hochre-iter and Schmidhuber, 1997) is a type of recurrentneural network (RNN) (Elman, 1990), and specifi-cally addresses the issue of learning long-term de-pendencies.We define the LSTM units at each time step t tobe a collection of vectors in Rd: an input gate it, aforget gate ft, an output gate ot, a memory cell ctand a hidden state ht.
d is the number of the LSTMunits.
The elements of the gating vectors it, ft andot are in [0, 1].The LSTM is precisely specified as follows.????c?totitft????
=????tanh??????
?TA,b[xtht?1], (1)ct = c?t  it + ct?1  ft, (2)ht = ot  tanh (ct) , (3)where xt is the input at the current time step; TA,bis an affine transformation which depends on param-eters of the network A and b. ?
denotes the logisticsigmoid function and  denotes elementwise multi-plication.The update of each LSTM unit can be written pre-cisely as follows(ht, ct) = LSTM(ht?1, ct?1,xt).
(4)Here, the function LSTM(?, ?, ?)
is a shorthand forEq.
(1-3).3 Coupled-LSTMs for Strong SentenceInteractionTo deal with two sentences, one straightforwardmethod is to model them with two separate LSTMs.However, this method is difficult to model local in-teractions of two sentences.
An improved way is tointroduce attention mechanism, which has been usedin many tasks, such as machine translation (Bah-danau et al, 2014) and question answering (Her-mann et al, 2015).Inspired by the multi-dimensional recurrent neu-ral network (Graves et al, 2007; Graves andSchmidhuber, 2009; Byeon et al, 2015) and gridLSTM (Kalchbrenner et al, 2015) in computer vi-sion community, we propose two models to capturethe interdependences between two parallel LSTMs,called coupled-LSTMs (C-LSTMs).To facilitate our models, we firstly give some def-initions.
Given two sequences X = x1, x2, ?
?
?
, xnand Y = y1, y2, ?
?
?
, ym, we let xi ?
Rd denote theembedded representation of the word xi.
The stan-dard LSTM have one temporal dimension.
Whendealing with a sentence, LSTM regards the posi-tion as time step.
At position i of sentence x1:n,1704h(1)1 h(1)2 h(1)3h(2)1 h(2)2 h(2)3(a) Parallel LSTMsh(1)1 h(1)2 h(1)3h(2)1 h(2)2 h(2)3(b) Attention LSTMsh(1)41 h(2)41 h(1)42 h(2)42 h(1)43 h(2)43 h(1)44 h(2)44h(1)31 h(2)31 h(1)32 h(2)32 h(1)33 h(2)33 h(1)34 h(2)34h(1)21 h(2)21 h(1)22 h(2)22 h(1)23 h(2)23 h(1)24 h(2)24h(1)11 h(2)11 h(1)12 h(2)12 h(1)13 h(2)13 h(1)14 h(2)14(c) Loosely coupled-LSTMsh41 h42 h43 h44h31 h32 h33 h34h21 h22 h23 h24h11 h12 h13 h14(d) Tightly coupled-LSTMsFigure 1: Four different coupled-LSTMs.the output hi reflects the meaning of subsequencex0:i = x0, ?
?
?
, xi.To model the interaction of two sentences as earlyas possible, we define hi,j to represent the interac-tion of the subsequences x0:i and y0:j .Figure 1(c) and 1(d) illustrate our two proposemodels.
For intuitive comparison of weak interac-tion parallel LSTMs, we also give parallel LSTMsand attention LSTMs in Figure 1(a) and 1(b)1.We describe our two proposed models as follows.3.1 Loosely Coupled-LSTMs (LC-LSTMs)To model the local contextual interactions of twosentences, we enable two LSTMs to be interde-pendent at different positions.
Inspired by GridLSTM (Kalchbrenner et al, 2015) and word-by-word attention LSTMs (Rockta?schel et al, 2015),we propose a loosely coupling model for two inter-dependent LSTMs.More concretely, we refer to h(1)i,j as the encodingof subsequence x0:i in the first LSTM influenced bythe output of the second LSTM on subsequence y0:j .Meanwhile, h(2)i,j is the encoding of subsequence y0:jin the second LSTM influenced by the output of thefirst LSTM on subsequence x0:i1In Rockta?schel et al (2015) model, conditioned LSTM wasused, meaning that h(1)1 is produced conditioned on h(2)3h(1)i,j and h(2)i,j are computed ash(1)i,j = LSTM1(H(1)i?1, c(1)i?1,j ,xi), (5)h(2)i,j = LSTM2(H(2)j?1, c(2)i,j?1,yj), (6)whereH(1)i?1 = [h(1)i?1,j ,h(2)i?1,j ], (7)H(2)j?1 = [h(1)i,j?1,h(2)i,j?1].
(8)3.2 Tightly Coupled-LSTMs (TC-LSTMs)The hidden states of LC-LSTMs are the combi-nation of the hidden states of two interdependentLSTMs, whose memory cells are separated.
In-spired by the configuration of the multi-dimensionalLSTM (Byeon et al, 2015), we further conflateboth the hidden states and the memory cells oftwo LSTMs.
We assume that hi,j directly modelthe interaction of the subsequences x0:i and y0:j ,which depends on two previous interaction hi?1,jand hi,j?1, where i, j are the positions in sentenceX and Y .We define a tightly coupled-LSTMs units as fol-lows.??????c?i,joi,jii,jf1i,jf2i,j??????=??????tanh??????????TA,b????xiyjhi,j?1hi?1,j????
, (9)ci,j = c?i,j  ii,j + [ci,j?1, ci?1,j ]T[f1i,jf2i,j](10)hi,j = ot  tanh (ci,j) (11)where the gating units ii,j and oi,j determine whichmemory units are affected by the inputs through c?i,j ,and which memory cells are written to the hiddenunits hi,j .
TA,b is an affine transformation whichdepends on parameters of the network A and b. Incontrast to the standard LSTM defined over time,each memory unit ci,j of a tightly coupled-LSTMshas two preceding states ci,j?1 and ci?1,j and twocorresponding forget gates f1i,j and f2i,j .3.3 Analysis of Two Proposed ModelsOur two proposed coupled-LSTMs can be formu-lated as(hi,j , ci,j) = C-LSTMs(hi?1,j ,hi,j?1, ci?1,j , ci,j?1,xi,yj),(12)1705x1, ?
?
?
,xny 1,??
?,y m?
??
?
?
Pooling FullyConnectedLayerOutputLayerInput Layer Stacked C-LSTMs Pooling LayerFigure 2: Architecture of coupled-LSTMs for sentence-pair encoding.
Inputs are fed to four C-LSTMs fol-lowed by an aggregation layer.
Blue cuboids represent different contextual information from four directions.where C-LSTMs can be either TC-LSTMs orLC-LSTMs.The input consists of two type of informationat step (i, j) in coupled-LSTMs: temporal dimen-sion hi?1,j ,hi,j?1, ci?1,j , ci,j?1 and depth dimen-sion xi,yj .
The difference between TC-LSTMs andLC-LSTMs is the dependence of information fromtemporal and depth dimension.Interaction Between Temporal Dimensions TheTC-LSTMs model the interactions at position (i, j)by merging the internal memory ci?1,j ci,j?1 andhidden state hi?1,j hi,j?1 along row and column di-mensions.
In contrast with TC-LSTMs, LC-LSTMsfirstly use two standard LSTMs in parallel, produc-ing hidden states h1i,j and h2i,j along row and columndimensions respectively, which are then merged to-gether flowing next step.Interaction Between Depth Dimension In TC-LSTMs, each hidden state hi,j at higher layer re-ceives a fusion of information xi and yj , flowedfrom lower layer.
However, in LC-LSTMs, the in-formation xi and yj are accepted by two corre-sponding LSTMs at the higher layer separately.The two architectures have their own charac-teristics, TC-LSTMs give more strong interactionsamong different dimensions while LC-LSTMs en-sures the two sequences interact closely without be-ing conflated using two separated LSTMs.Comparison of LC-LSTMs and word-by-wordAttention LSTMs The characteristic of attentionLSTMs is that they obtain the attention weightedrepresentation of one sentence considering he align-ment between the two sentences, which is asymmet-ric unidirectional encoding.
Nevertheless, in LC-LSTM, each hidden state of each step is obtainedwith the consideration of interaction between twosequences with symmetrical encoding fashion.4 End-to-End Architecture for SentenceMatchingIn this section, we present an end-to-end deep ar-chitecture for matching two sentences, as shown inFigure 2.4.1 Embedding LayerTo model the sentences with neural model, we firstlyneed transform the one-hot representation of wordinto the distributed representation.
All words oftwo sequences X = x1, x2, ?
?
?
, xn and Y =y1, y2, ?
?
?
, ym will be mapped into low dimensionalvector representations, which are taken as input ofthe network.4.2 Stacked Coupled-LSTMs LayersA basic block consists of five layers.
We firstly usefour directional coupled-LSTMs to model the localinteractions with different information flows.
Andthen we sum the outputs of these LSTMs by aggre-gation layer.
To increase the learning capabilities ofthe coupled-LSTMs, we stack the basic block on topof each other.4.2.1 Four Directional Coupled-LSTMs LayersThe C-LSTMs is defined along a certain pre-defined direction, we can extend them to access tothe surrounding context in all directions.
Similarto bi-directional LSTM, there are four directions incoupled-LSTMs.
(h1i,j , c1i,j) = C-LSTMs(hi?1,j ,hi,j?1, ci?1,j , ci,j?1,xi,yj),(h2i,j , c2i,j) = C-LSTMs(hi?1,j ,hi,j+1, ci?1,j , ci,j+1,xi,yj),(h3i,j , c3i,j) = C-LSTMs(hi+1,j ,hi,j+1, ci+1,j , ci,j+1,xi,yj),1706(h4i,j , c4i,j) = C-LSTMs(hi+1,j ,hi,j?1, ci+1,j , ci,j?1,xi,yj).4.2.2 Aggregation LayerThe aggregation layer sums the outputs of four di-rectional coupled-LSTMs into a vector.h?i,j =4?d=1hdi,j , (13)where the superscript t of hi,j denotes the differentdirections.4.2.3 Stacking C-LSTMs BlocksTo increase the capabilities of network of learningmultiple granularities of interactions, we stack sev-eral blocks (four C-LSTMs layers and one aggrega-tion layer) to form deep architectures.4.3 Pooling LayerThe output of stacked coupled-LSTMs layers is atensor H ?
Rn?m?d, where n andm are the lengthsof sentences, and d is the number of hidden neurons.We apply dynamic pooling to automatically extractRp?q subsampling matrix in each slice Hi ?
Rn?m,similar to (Socher et al, 2011).More formally, for each slice matrix Hi, we par-tition the rows and columns of Hi into p?q roughlyequal grids.
These grid are non-overlapping.
Thenwe select the maximum value within each gridthereby obtaining a p?
q ?
d tensor.4.4 Fully-Connected LayerThe vector obtained by pooling layer is fed into a fullconnection layer to obtain a final more abstractiverepresentation.4.5 Output LayerThe output layer depends on the types of the tasks,we choose the corresponding form of output layer.There are two popular types of text matching tasks inNLP.
One is ranking task, such as community ques-tion answering.
Another is classification task, suchas textual entailment.1.
For ranking task, the output is a scalar matchingscore, which is obtained by a linear transforma-tion after the last fully-connected layer.MQA RTEEmbedding size 100 100Hidden layer size 50 50Initial learning rate 0.05 0.005Regularization 5E?5 1E?5Pooling (p, q) (2,1) (1,1)Table 1: Hyper-parameters for our model on twotasks.2.
For classification task, the outputs are the prob-abilities of the different classes, which is com-puted by a softmax function after the last fully-connected layer.5 TrainingOur proposed architecture can deal with differentsentence matching tasks.
The loss functions varieswith different tasks.
More concretely, we use max-margin loss (Bordes et al, 2013; Socher et al, 2013)for ranking task and cross-entropy loss for classifi-cation task.To minimize the objective, we use stochastic gra-dient descent with the diagonal variant of AdaGrad(Duchi et al, 2011).
To prevent exploding gradients,we perform gradient clipping by scaling the gradientwhen the norm exceeds a threshold (Graves, 2013).6 ExperimentIn this section, we investigate the empirical perfor-mances of our proposed model on two different textmatching tasks: classification task (recognizing tex-tual entailment) and ranking task (matching of ques-tion and answer).6.1 Hyperparameters and TrainingThe word embeddings for all of the models are ini-tialized with the 100d GloVe vectors (840B tokenversion, (Pennington et al, 2014)) and fine-tunedduring training to improve the performance.
Theother parameters are initialized by randomly sam-pling from uniform distribution in [?0.1, 0.1].For each task, we take the hyperparameters whichachieve the best performance on the development setvia an small grid search over combinations of the ini-tial learning rate [0.05, 0.0005, 0.0001], l2 regular-ization [0.0, 5E?5, 1E?5, 1E?6] and the threshold1707value of gradient norm [5, 10, 100].
The final hyper-parameters are set as Table 1.6.2 Competitor Methods?
Neural bag-of-words (NBOW): Each sequenceas the sum of the embeddings of the words itcontains, then they are concatenated and fed toa MLP.?
Single LSTM: A single LSTM to encode thetwo sequences, which is used in (Rockta?schelet al, 2015).?
Parallel LSTMs: Two sequences are encodedby two LSTMs separately, then they are con-catenated and fed to a MLP.?
Attention LSTMs: An attentive LSTM to en-code two sentences into a semantic space,which used in (Hermann et al, 2015;Rockta?schel et al, 2015).?
Word-by-word Attention LSTMs: An improve-ment of attention LSTM by introducing word-by-word attention mechanism, which used in(Hermann et al, 2015; Rockta?schel et al,2015).6.3 Experiment-I: Recognizing TextualEntailmentRecognizing textual entailment (RTE) is a task to de-termine the semantic relationship between two sen-tences.
We use the Stanford Natural Language In-ference Corpus (SNLI) (Bowman et al, 2015).
Thiscorpus contains 570K sentence pairs, and all of thesentences and labels stem from human annotators.SNLI is two orders of magnitude larger than all otherexisting RTE corpora.
Therefore, the massive scaleof SNLI allows us to train powerful neural networkssuch as our proposed architecture in this paper.6.3.1 ResultsTable 2 shows the evaluation results on SNLI.
The3rd column of the table gives the number of param-eters of different models without the word embed-dings.Our proposed two C-LSTMs models with fourstacked blocks outperform all the competitor mod-els, which indicates that our thinner and deeper net-work does work effectively.Model k |?|M TestNBOW 100 80K 75.1single LSTM(Rockta?schel et al, 2015) 100 111K 80.9parallel LSTMs(Bowman et al, 2015) 100 221K 77.6Attention LSTMs(Rockta?schel et al, 2015) 100 252K 82.3Attention(w-by-w) LSTMs(Rockta?schel et al, 2015) 100 252K 83.5LC-LSTMs (Single Direction) 50 45K 80.5LC-LSTMs 50 45K 80.9four stacked LC-LSTMs 50 135K 84.3TC-LSTMs (Single Direction) 50 77.5K 80.1TC-LSTMs 50 77.5K 81.6four stacked TC-LSTMs 50 190K 85.1Table 2: Results on SNLI corpus.Besides, we can see both LC-LSTMs and TC-LSTMs benefit from multi-directional layer, whilethe latter obtains more gains than the former.
We at-tribute this discrepancy between two models to theirdifferent mechanisms of controlling the informationflow from depth dimension.Compared with attention LSTMs, our two mod-els achieve comparable results to them using muchfewer parameters (nearly 1/5).
By stacking C-LSTMs2 , the performance of them are improvedsignificantly, and the four stacked TC-LSTMsachieve 85.1% accuracy on this dataset.Moreover, we can see TC-LSTMs achieve betterperformance than LC-LSTMs on this task, whichneed fine-grained reasoning over pairs of words aswell as phrases.6.3.2 Understanding Behaviors of Neurons inC-LSTMsTo get an intuitive understanding of how the C-LSTMs work on this problem, we examined the neu-ron activations in the last aggregation layer whileevaluating the test set using TC-LSTMs.
We findthat some cells are bound to certain roles.Let hi,j,k denotes the activation of the k-th neu-ron at the position of (i, j), where i ?
{1, .
.
.
, n}and j ?
{1, .
.
.
,m}.
By visualizing the hidden statehi,j,k and analyzing the maximum activation, we2To make a fair comparison, we also train a stackedattention-based LSTM with the same setting as our models,while it does not make significant improvement with 83.7% ac-curacy.1708Index of Cell Word or Phrase Pairs3-th (in a pool, swimming), (near a fountain, next to the ocean), (street, outside)9-th (doing a skateboard, skateboarding), (sidewalk with, inside), (standing, seated)17-th (blue jacket, blue jacket), (wearing black, wearing white), (green uniform, red uniform)25-th (a man, two other men), (a man, two girls), (an old woman, two people)Table 3: Multiple interpretable neurons and the word-pairs/phrase-pairs captured by these neurons.A person iswearing a green shirt ..over hunchedpants blackand shirtred ain personA ?0.5 0 0.5(a) 3rd neuronAperson isoutside .
.street thedown walkingis jeanswearing womanA 0 0.2 0.4 0.6(b) 17th neuronFigure 3: Illustration of two interpretable neuronsand some word-pairs capture by these neurons.
Thedarker patches denote the corresponding activationsare higher.can find that there exist multiple interpretable neu-rons.
For example, when some contextualized localperspectives are semantically related at point (i, j)of the sentence pair, the activation value of hiddenneuron hi,j,k tend to be maximum, meaning that themodel could capture some reasoning patterns.Figure 3 illustrates this phenomenon.
In Fig-ure 3(a), a neuron shows its ability to monitorthe local contextual interactions about color.
Theactivation in the patch, including the word pair?
(red, green)?, is much higher than others.This is informative pattern for the relation predic-tion of these two sentences, whose ground truthis contradiction.
An interesting thing is thereare two words describing color in the sentence?
A person in a red shirt and blackpants hunched over.?.
Our model ignoresthe useless word ?black?, which indicates that thisneuron selectively captures pattern by contextual un-derstanding, not just word level interaction.In Figure 3(b), another neuron shows that itcan capture the local contextual interactions,such as ?
(walking down the street,outside)?.
These patterns can be easily capturedby pooling layer and provide a strong support forthe final prediction.Table 3 illustrates multiple interpretable neuronsand some representative word or phrase pairs whichcan activate these neurons.
These cases show thatour models can capture contextual interactions be-yond word level.6.3.3 Error AnalysisAlthough our models C-LSTMs are more sen-sitive to the discrepancy of the semantic capacitybetween two sentences, some semantic mistakes atthe phrasal level still exist.
For example, our modelsfailed to capture the key informative pattern whenpredicting the entailment sentence pair ?A girltakes off her shoes and eats bluecotton candy/The girl is eatingwhile barefoot.
?Besides, despite the large size of the trainingcorpus, it?s still very different to solve somecases, which depend on the combination of theworld knowledge and context-sensitive infer-ences.
For example, given an entailment pair?a man grabs his crotch during apolitical demonstration/The manis making a crude gesture?, all modelspredict ?neutral?.
This analysis suggests thatsome architectural improvements or external worldknowledge are necessary to eliminate all errorsinstead of simply scaling up the basic model.6.4 Experiment-II: Matching Question andAnswerMatching question answering (MQA) is a typicaltask for semantic matching.
Given a question, weneed select a correct answer from some candidateanswers.In this paper, we use the dataset collected fromYahoo!
Answers with the getByCategory function1709Model k P@1(5) P@1(10)Random Guess - 20.0 10.0NBOW 50 63.9 47.6single LSTM 50 68.2 53.9parallel LSTMs 50 66.9 52.1Attention LSTMs 50 73.5 62.0Attention LSTMs (w-by-w) 50 75.1 64.0LC-LSTMs (Single Direction) 50 75.4 63.0LC-LSTMs 50 76.1 64.1three stacked LC-LSTMs 50 78.5 66.2TC-LSTMs (Single Direction) 50 74.3 62.4TC-LSTMs 50 74.9 62.9three stacked TC-LSTMs 50 77.0 65.3Table 4: Results on Yahoo question-answer pairsdataset.provided in Yahoo!
Answers API, which produces963, 072 questions and corresponding best answers.We then select the pairs in which the length of ques-tions and answers are both in the interval [4, 30], thusobtaining 220, 000 question answer pairs to form thepositive pairs.For negative pairs, we first use each question?sbest answer as a query to retrieval top 1, 000 resultsfrom the whole answer set with Lucene, where 4 or9 answers will be selected randomly to construct thenegative pairs.The whole dataset is divided into training, vali-dation and testing data with proportion 20 : 1 : 1.Moreover, we give two test settings: selecting thebest answer from 5 and 10 candidates respectively.6.4.1 ResultsResults of MQA are shown in the Table 4.
For ourmodels, due to stacking block more than three layerscan not make significant improvements on this task,we just use three stacked C-LSTMs.By analyzing the evaluation results of question-answer matching in table 4, we can see strong in-teraction models (attention LSTMs, our C-LSTMs)consistently outperform the weak interaction mod-els (NBOW, parallel LSTMs) with a large margin,which suggests the importance of modelling stronginteraction of two sentences.Our proposed two C-LSTMs surpass the competi-tor methods and C-LSTMs augmented with multi-directions layers and multiple stacked blocks fullyutilize multiple levels of abstraction to directly boostthe performance.Additionally, LC-LSTMs is superior to TC-LSTMs.
The reason may be that MQA is a relativesimple task, which requires less reasoning abilities,compared with RTE task.
Moreover, the parametersof LC-LSTMs are less than TC-LSTMs, which en-sures the former can avoid suffering from overfittingon a relatively smaller corpus.7 Related WorkOur architecture for sentence pair encoding can beregarded as strong interaction models, which havebeen explored in previous models.An intuitive paradigm is to compute similari-ties between all the words or phrases of the twosentences.
Socher et al (2011) firstly used thisparadigm for paraphrase detection.
The represen-tations of words or phrases are learned based on re-cursive autoencoders.A major limitation of this paradigm is the inter-action of two sentence is captured by a pre-definedsimilarity measure.
Thus, it is not easy to in-crease the depth of the network.
Compared withthis paradigm, we can stack our C-LSTMs to modelmultiple-granularity interactions of two sentences.Rockta?schel et al (2015) used two LSTMsequipped with attention mechanism to capture the it-eration between two sentences.
This architecture isasymmetrical for two sentences, where the obtainedfinal representation is sensitive to the two sentences?order.Compared with the attentive LSTM, our proposedC-LSTMs are symmetrical and model the local con-textual interaction of two sequences directly.8 Conclusion and Future WorkIn this paper, we propose an end-to-end deep archi-tecture to capture the strong interaction informationof sentence pair.
Experiments on two large scale textmatching tasks demonstrate the efficacy of our pro-posed model and its superiority to competitor mod-els.
Besides, we present an elaborate qualitativeanalysis of our models, which gives an intuitive un-derstanding how our model worked.In future work, we would like to incorporate somegating strategies into the depth dimension of our pro-posed models, like highway or residual network, toenhance the interactions between depth and other di-1710mensions thus training more deep and powerful neu-ral networks.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir valuable comments.
This work was partiallyfunded by National Natural Science Foundation ofChina (No.
61532011 and 61672162), the NationalHigh Technology Research and Development Pro-gram of China (No.
2015AA015408).ReferencesD.
Bahdanau, K. Cho, and Y. Bengio.
2014.
Neural ma-chine translation by jointly learning to align and trans-late.
ArXiv e-prints, September.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,Jason Weston, and Oksana Yakhnenko.
2013.
Trans-lating embeddings for modeling multi-relational data.In NIPS.Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning.
2015.
A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing.Wonmin Byeon, Thomas M Breuel, Federico Raue, andMarcus Liwicki.
2015.
Scene labeling with lstm re-current neural networks.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recogni-tion, pages 3547?3555.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Jeffrey L Elman.
1990.
Finding structure in time.
Cog-nitive science, 14(2):179?211.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.Alex Graves and Ju?rgen Schmidhuber.
2009.
Offlinehandwriting recognition with multidimensional recur-rent neural networks.
In Advances in Neural Informa-tion Processing Systems, pages 545?552.Alex Graves, Santiago Ferna?ndez, and Ju?rgen Schmid-huber.
2007.
Multi-dimensional recurrent neural net-works.
In Artificial Neural Networks?ICANN 2007,pages 549?558.
Springer.Alex Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.Hua He, Kevin Gimpel, and Jimmy Lin.
2015.
Multi-perspective sentence similarity modeling with convo-lutional neural networks.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 1576?1586.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural InformationProcessing Systems, pages 1684?1692.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.2014.
Convolutional neural network architectures formatching natural language sentences.
In Advances inNeural Information Processing Systems.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of ACL.Nal Kalchbrenner, Ivo Danihelka, and Alex Graves.2015.
Grid long short-term memory.
arXiv preprintarXiv:1507.01526.PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, andXuanjing Huang.
2015.
Multi-timescale long short-term memory neural network for modelling sentencesand documents.
In Proceedings of the Conference onEMNLP.Pengfei Liu, Xipeng Qiu, Jifan Chen, and XuanjingHuang.
2016.
Deep fusion LSTMs for text seman-tic matching.
In Proceedings of Annual Meeting of theAssociation for Computational Linguistics.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for wordrepresentation.
Proceedings of the Empiricial Meth-ods in Natural Language Processing (EMNLP 2014),12:1532?1543.Xipeng Qiu and Xuanjing Huang.
2015.
Convolutionalneural tensor network architecture for community-based question answering.
In Proceedings of Interna-tional Joint Conference on Artificial Intelligence.Tim Rockta?schel, Edward Grefenstette, Karl Moritz Her-mann, Toma?s?
Koc?isky`, and Phil Blunsom.
2015.
Rea-soning about entailment with neural attention.
arXivpreprint arXiv:1509.06664.Mike Schuster and Kuldip K Paliwal.
1997.
Bidirec-tional recurrent neural networks.
Signal Processing,IEEE Transactions on, 45(11):2673?2681.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dynamicpooling and unfolding recursive autoencoders for para-phrase detection.
In Advances in Neural InformationProcessing Systems.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In1711Advances in Neural Information Processing Systems,pages 926?934.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Sys-tems, pages 3104?3112.Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, LiangPang, and Xueqi Cheng.
2016.
A deep architecture forsemantic matching with multiple positional sentencerepresentations.
In AAAI.Wenpeng Yin and Hinrich Schu?tze.
2015.
Convolutionalneural network for paraphrase identification.
In Pro-ceedings of the 2015 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 901?911.Wenpeng Yin, Hinrich Schu?tze, Bing Xiang, and BowenZhou.
2015.
Abcnn: Attention-based convolutionalneural network for modeling sentence pairs.
arXivpreprint arXiv:1512.05193.1712
