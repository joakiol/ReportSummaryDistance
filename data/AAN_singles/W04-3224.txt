A Distributional Analysis of a Lexicalized Statistical Parsing ModelDaniel M. BikelDepartment of Computer and Information ScienceUniversity of Pennsylvania3330 Walnut StreetPhiladelphia, PA 19104dbikel@cis.upenn.eduAbstractThis paper presents some of the first data visualiza-tions and analysis of distributions for a lexicalizedstatistical parsing model, in order to better under-stand their nature.
In the course of this analysis,we have paid particular attention to parameters thatinclude bilexical dependencies.
The prevailing viewhas been that such statistics are very informative butsuffer greatly from sparse data problems.
By using aparser to constrain-parse its own output, and by hy-pothesizing and testing for distributional similaritywith back-off distributions, we have evidence thatfinally explains that (a) bilexical statistics are actu-ally getting used quite often but that (b) the distri-butions are so similar to those that do not includehead words as to be nearly indistinguishable inso-far as making parse decisions.
Finally, our analysishas provided for the first time an effective way todo parameter selection for a generative lexicalizedstatistical parsing model.1 IntroductionLexicalized statistical parsing models, such as thosebuilt by Black et al (1992a), Magerman (1994),Collins (1999) and Charniak (2000), have beenenormously successful, but they also have an enor-mous complexity.
Their success has often beenattributed to their sensitivity to individual lexicalitems, and it is precisely this incorporation of lexicalitems into features or parameter schemata that givesrise to their complexity.
In order to help determinewhich features are helpful, the somewhat crude-but-effective method has been to compare a model?soverall parsing performance with and without a fea-ture.
Often, it has seemed that features that arederived from linguistic principles result in higher-performing models (cf.
(Collins, 1999)).
Whilethis may be true, it is clearly inappropriate to high-light ex post facto the linguistically-motivated fea-tures and rationalize their inclusion and state howeffective they are.
A rigorous analysis of features orparameters in relation to the entire model is calledfor.
Accordingly, this work aims to provide a thor-ough analysis of the nature of the parameters in aCollins-style parsing model, with particular focuson the two parameter classes that generate lexical-ized modifying nonterminals, for these are whereall a sentence?s words are generated except for thehead word of the entire sentence; also, these twoparameter classes have by far the most parametersand suffer the most from sparse data problems.
Inspite of using a Collins-style model as the basis foranalysis, throughout this paper, we will attempt topresent information that is widely applicable be-cause it pertains to properties of the widely-usedTreebank (Marcus et al, 1993) and lexicalized pars-ing models in general.This work also sheds light on the much-discussed?bilexical dependencies?
of statistical parsing mod-els.
Beginning with the seminal work at IBM (Blacket al, 1991; Black et al, 1992b; Black et al, 1992a),and continuing with such lexicalist approaches as(Eisner, 1996), these features have been lauded fortheir ability to approximate a word?s semantics asa means to override syntactic preferences with se-mantic ones (Collins, 1999; Eisner, 2000).
How-ever, the work of Gildea (2001) showed that, withan approximate reimplementation of Collins?
Model1, removing all parameters that involved dependen-cies between a modifier word and its head resultedin a surprisingly small decrease in overall parse ac-curacy.
The prevailing assumption was then thatsuch bilexical statistics were not useful for mak-ing syntactic decisions, although it was not entirelyclear why.
Subsequently, we replicated Gildea?sexperiment with a complete emulation of Model2 and presented additional evidence that bilexicalstatistics were barely getting used during decod-ing (Bikel, 2004), appearing to confirm the origi-nal result.
However, the present work will showthat such statistics do get frequently used for thehighest-probability parses, but that when a Collins-style model generates modifier words, the bilexicalparameters are so similar to their back-off distribu-tions as to provide almost no extra predictive infor-mation.2 MotivationA parsing model coupled with a decoder (an al-gorithm to search the space of possible trees for agiven terminal sequence) is largely an engineeringeffort.
In the end, the performance of the parserwith respect to its evaluation criteria?typically ac-curacy, and perhaps also speed?are all that matter.Consequently, the engineer must understand whatthe model is doing only to the point that it helpsmake the model perform better.
Given the some-what crude method of determining a feature?s ben-efit by testing a model with and without the fea-ture, a researcher can argue for the efficacy of thatfeature without truly understanding its effect on themodel.
For example, while adding a particular fea-ture may improve parse accuracy, the reason mayhave little to do with the nature of the feature andeverything to do with its canceling other featuresthat were theretofore hurting performance.
In anycase, since this is engineering, the rationalizationfor a feature is far less important than the model?soverall performance increase.On the other hand, science would demand that,at some point, we analyze the multitude of featuresin a state-of-the-art lexicalized statistical parsingmodel.
Such analysis is warranted for two reasons:replicability and progress.
The first is a basic tenetof most sciences: without proper understanding ofwhat has been done, the relevant experiment(s) can-not be replicated and therefore verified.
The sec-ond has to do with the idea that, when a disciplinematures, it can be difficult to determine what newfeatures can provide the most gain (or any gain, forthat matter).
A thorough analysis of the various dis-tributions being estimated in a parsing model allowsresearchers to discover what is being learned mostand least well.
Understanding what is learned mostwell can shed light on the types of features or depen-dencies that are most efficacious, pointing the wayto new features of that type.
Understanding what islearned least well defines the space in which to lookfor those new features.3 Frequencies3.1 Definitions and notationIn this paper we will refer to any estimated dis-tribution as a parameter that has been instantiatedfrom a parameter class.
For example, in an n-gram language model, p(wi |wi?1) is a parameterclass, whereas the estimated distribution p?
( ?
| the)is a particular parameter from this class, consistingof estimates of every word that can follow the word?the?.For this work, we used the model described in(Bikel, 2002; Bikel, 2004).
Our emulation ofCollins?
Model 2 (hereafter referred to simply as?the model?)
has eleven parameter classes, each ofwhich employs up to three back-off levels, whereback-off level 0 is just the ?un-backed-off?
maximalcontext history.1 In other words, a smoothed prob-ability estimate is the interpolation of up to threedifferent unsmoothed estimates.
The notation anddescription for each of these parameter classes isshown in Table 1.3.2 Basic frequenciesBefore looking at the number of parameters in themodel, it is important to bear in mind the amountof data on which the model is trained and on whichactual parameters will be induced from parameterclasses.
The standard training set for English con-sists of Sections 02?21 of the Penn Treebank, whichin turn consist of 39,832 sentences with a total of950,028 word tokens (not including null elements).There are 44,113 unique words (again, not includ-ing null elements), 10,437 of which occur 6 timesor more.2 The trees consist of 904,748 bracketswith 28 basic nonterminal labels, to which func-tion tags such as -TMP and indices are added inthe data to form 1184 observed nonterminals, notincluding preterminals.
After tree transformations,the model maps these 1184 nonterminals down tojust 43.
There are 42 unique part of speech tags thatserve as preterminals in the trees; the model prunesaway three of these (?, ?
and .
).Induced from these training data, the model con-tains 727,930 parameters; thus, there are nearly asmany parameters as there are brackets or word to-kens.
From a history-based grammar perspective,there are 727,930 types of history contexts fromwhich futures are generated.
However, 401,447 ofthese are singletons.
The average count for a historycontext is approximately 35.56, while the averagediversity is approximately 1.72.
The model contains1,252,280 unsmoothed maximum-likelihood proba-bility estimates (727, 930 ?1.72 ?
1, 252, 280).
Evenwhen a given future was not seen with a particu-lar history, it is possible that one of its associated1Collins?
model splits out the PM and PMw classes into left-and right-specific versions, and has two additional classes fordealing with coordinating conjunctions and inter-phrasal punc-tuation.
Our emulation of Collins?
model incorporates the in-formation of these specialized parameter classes into the exist-ing PM and PMw parameters.2We mention this statistic because Collins?
thesis experi-ments were performed with an unknown word threshold of 6.Notation Description No.
of back-off levelsPH Generates unlexicalized head child given lexicalized parent 3PsubcatL Generates subcat bag on left side of head child 3PsubcatR Generates subcat bag on right side of head child 3PM (PM,NPB) Generates partially-lexicalized modifying nonterminal (with NPB parent) 3PMw (PMw,NPB) Generates head word of modifying nonterminal (with NPB parent) 3PpriorNT Priors for nonterminal conditioning on its head word and part of speech 2Ppriorlex Priors for head word/part of speech pairs (unconditional probabilities) 0PTOPNT Generates partially-lexicalized child of +TOP+?
1PTOPw Generates the head word for children of +TOP+?
2Table 1: All eleven parameter classes in our emulation of Collins?
Model 2.
A partially-lexicalized nonter-minal is a nonterminal label and its head word?s part of speech (such as NP(NN)).
?The hidden nonterminal+TOP+ is added during training to be the parent of every observed tree.PP(IN/with)IN(IN/with) {NP?A} NP?A(NN/ .
.
.
)Figure 1: A frequent PMw history context, illustratedas a tree fragment.
The .
.
.
represents the future thatis to be generated given this history.back-off contexts was seen with that future, leadingto a non-zero smoothed estimate.
The total num-ber of possible non-zero smoothed estimates in themodel is 562,596,053.
Table 2 contains count anddiversity statistics for the two parameter classes onwhich we will focus much of our attention, PM andPMw .
Note how the maximal-context back-off lev-els (level 0) for both parameter classes have rela-tively little training: on average, raw estimates areobtained with history counts of only 10.3 and 4.4 inthe PM and PMw classes, respectively.
Conversely,observe how drastically the average number of tran-sitions n increases as we remove dependence on thehead word going from back-off level 0 to 1.3.3 Exploratory data analysis: a commondistributionTo begin to get a handle on these distributions, par-ticularly the relatively poorly-trained and/or high-entropy distributions of the PMw class, it is useful toperform some exploratory data analysis.
Figure 1illustrates the 25th-most-frequent PMw history con-text as a tree fragment.
In the top-down model, thefollowing elements have been generated:?
a parent nonterminal PP(IN/with) (a PPheaded by the word with with the part-of-speech tag IN)?
the parent?s head child IN?
a right subcat bag containing NP-A (a single NPargument must be generated somewhere on the00.10.20.30.40.50.60.70.80.910  500  1000  1500  2000  2500  3000  3500cummulativedensityrankFigure 2: Cumulative density function for the PMwhistory context illustrated in Figure 1.right side of the head child)?
a partially-lexicalized right-modifying nonter-minalAt this point in the process, a PMw parameter condi-tioning on all of this context will be used to estimatethe probability of the head word of the NP-A(NN),completing the lexicalization of that nonterminal.
Ifa candidate head word was seen in training in thisconfiguration, then it will be generated conditioningon the full context that crucially includes the headword with; otherwise, the model will back off to ahistory context that does not include the head word.In Figure 2, we plot the cumulative density func-tion of this history context.
We note that of the3258 words with non-zero probability in this con-text, 95% of the probability mass is covered by the1596 most likely words.In order to get a better visualization of the proba-bility distribution, we plotted smoothed probabilityestimates versus the training-data frequencies of thewords being generated.
Figure 3(a) shows smoothedestimates that make use of the full context (i.e., in-clude the head word with) wherever possible, andFigure 3(b) shows smoothed estimates that do notuse the head word.
Note how the plot in Figure 3(b)appears remarkably similar to the ?true?
distribu-Back-off PM PMwlevel c?
?d n c?
?d n0 10.268 1.437 7.145 4.413 1.949 2.2641 558.047 3.643 153.2 60.19 8.454 7.1202 1169.6 5.067 230.8 21132.1 370.6 57.02Table 2: Average counts and diversities of histories of the PM and PMw parameter classes.
c and d areaverage history count and diversity, respectively.
n = cdis the average number of transitions from a historycontext to some future.1e-061e-050.00010.0010.010.11  10  100  1000  10000smoothedprobabilityestimateword frequency(a) prob.
vs. word freq., back-off level 11e-061e-050.00010.0010.010.11  10  100  1000  10000  100000smoothedprobabilityestimateword frequency(b) prob.
vs. word freq., back-off level 2Figure 3: Probability versus word frequency for head words of NP-A(NN) in the PP construction.tion of 3(a).
3(b) looks like a slightly ?compressed?version of 3(b) (in the vertical dimension), but theshape of the two distributions appears to be roughlythe same.
This observation will be confirmed andquantified by the experiments of ?5.34 EntropiesA good measure of the discriminative efficacy of aparameter is its entropy.
Table 3 shows the aver-age entropy of all distributions for each parameterclass.4 By far the highest average entropy is for thePMw parameter class.Having computed the entropy for every distri-bution in every parameter class, we can actuallyplot a ?meta-distribution?
of entropies for a pa-rameter class, as shown in Figure 4.
As an ex-ample of one of the data points of Figure 4, con-sider the history context explored in the previoussection.
While it may be one of the most fre-quent, it also has the highest entropy at 9.1413The astute reader will further note that the plots in Figure3 both look bizarrely truncated with respect to low-frequencywords.
This is simply due to the fact that all words below afixed frequency are generated as the +UNKNOWN+ word.4The decoder makes use of two additional parameter classesthat jointly estimate the prior probability of a lexicalized non-terminal; however, these two parameter classes are not part ofthe generative model.PH 0.2516 PTOPNT 2.517PsubcatL 0.02342 PTOPw 2.853PsubcatR 0.2147PM 1.121PMw 3.923Table 3: Average entropies for each parameter class.0123456789100  50000  100000  150000  200000  250000entropyrankFigure 4: Entropy distribution for the PMw parame-ters.bits, as shown by Table 4.
This value not onlyconfirms but quantifies the long-held intuition thatPP-attachment requires more than just the localphrasal context; it is, e.g., precisely why the PP-specific features of (Collins, 2000) were likely tobe very helpful, as cases such as these are amongthe most difficult that the model must discrimi-nate.
In fact, of the top 50 of the highest-entropyBack-off PM PMwlevel min max avg median min max avg median0 3.080E-10 4.351 1.128 0.931 4.655E-8 9.141 3.904 3.8061 4.905E-7 4.254 0.910 0.667 2.531E-6 9.120 4.179 4.2242 8.410E-4 3.501 0.754 0.520 0.002 8.517 3.182 2.451Overall 3.080E-10 4.351 1.121 0.917 4.655E-8 9.141 3.922 3.849Table 4: Entropy distribution statistics for PM and PMw .Figure 5: Total modifier word?generation entropybroken down by parent-head-modifier triple.distributions from PMw , 25 involve the config-uration PP --> IN(IN/<prep>) NP-A(NN/.
.
.
),where <prep> is some preposition whose tag is IN.Somewhat disturbingly, these are also some of themost frequent constructions.To gauge roughly the importance of thesehigh-frequency, high-entropy distributions, we per-formed the following analysis.
Assume for the mo-ment that every word-generation decision is roughlyindependent from all others (this is clearly not true,given head-propagation).
We can then compute thetotal entropy of word-generation decisions for theentire training corpus viaHPMw =?c?PMwf (c) ?
H(c) (1)where f (c) is the frequency of some history con-text c and H(c) is that context?s entropy.
The to-tal modifier word-generation entropy for the cor-pus with the independence assumption is 3,903,224bits.
Of these, the total entropy for contexts of theform PP ?
IN NP-A is 618,640 bits, representinga sizable 15.9% of the total entropy, and the sin-gle largest percentage of total entropy of any parent-head-modifier triple (see Figure 5).On the opposite end of the entropy spectrum,there are tens of thousands of PMw parameterswith extremely low entropies, mostly having to dowith extremely low-diversity, low-entropy part-of-speech tags, such as DT, CC, IN or WRB.
Perhaps evenmore interesting is the number of distributions withidentical entropies: of the 206,234 distributions,there are only 92,065 unique entropy values.
Dis-tributions with the same entropy are all candidatesfor removal from the model, because most of theirprobability mass resides in the back-off distribution.Many of these distributions are low- or one-counthistory contexts, justifying the common practice ofremoving transitions whose history count is below acertain threshold.
This practice could be made morerigorous by relying on distributional similarity.
Fi-nally, we note that the most numerous low-entropydistributions (that are not trivial) involve generatingright-modifier words of the head child of an SBARparent.
The model is able to learn these construc-tions extremely well, as one might expect.5 Distributional similarity and bilexicalstatisticsWe now return to the issue of bilexical statis-tics.
As alluded to earlier, Gildea (2001) per-formed an experiment with his partial reimplemen-tation of Collins?
Model 1 in which he removed themaximal-context back-off level from PMw , whicheffectively removed all bilexical statistics from hismodel.
Gildea observed that this change resultedin only a 0.5% drop in parsing performance.
Therewere two logical possibilities for this behavior: ei-ther such statistics were not getting used due tosparse data problems, or they were not informa-tive for some reason.
The prevailing view of theNLP community had been that bilexical statisticswere sparse, and Gildea (2001) adopted this viewto explain his results.
Subsequently, we duplicatedGildea?s experiment with a complete emulation ofCollins?
Model 2, and found that when the decoderrequested a smoothed estimate involving a bigramwhen testing on held-out data, it only received anestimate that made use of bilexical statistics a mere1.49% of the time (Bikel, 2004).
The conclusionwas that the minuscule drop in performance from re-moving bigrams must have been due to the fact thatthey were barely able to be used.
In other words, itappeared that bigram coverage was not nearly goodenough for bigrams to have an impact on parsingperformance, seemingly confirming the prevailingview.But the 1.49% figure does not tell the whole story.The parser pursues many incorrect and ultimatelylow-scoring theories in its search (in this case, us-ing probabilistic CKY).
So rather than asking howmany times the decoder makes use of bigram statis-tics on average, a better question is to ask howmany times the decoder can use bigram statisticswhile pursuing the top-ranked theory.
To answerthis question, we used our parser to constrain-parseits own output.
That is, having trained it on Sec-tions 02?21, we used it to parse Section 00 of thePenn Treebank (the canonical development test set)and then re-parse that section using its own highest-scoring trees (without lexicalization) as constraints,so that it only pursued theories consistent with thosetrees.
As it happens, the number of times the de-coder was able to use bigram statistics shot up to28.8% overall, with a rate of 22.4% for NPB con-stituents.So, bigram statistics are getting used; in fact, theyare getting used more than 19 times as often whenpursuing the highest-scoring theory as when pursu-ing any theory on average.
And yet there is no dis-puting the fact that their use has a surprisingly smalleffect on parsing performance.
The exploratory dataanalysis of ?3.3 suggests an explanation for this per-plexing behavior: the distributions that include thehead word versus those that do not are so similaras to make almost no difference in terms of parseaccuracy.5.1 Distributional similarityA useful metric for measuring distributional simi-larity, as explored by (Lee, 1999), is the Jensen-Shannon divergence (Lin, 1991):JS (p ?
q ) = 12[D(p???
avgp,q)+ D(q???
avgp,q)](2)where D is the Kullback-Leibler divergence(Cover and Thomas, 1991) and where avgp,q =12 (p(A) + q(A)) for an event A in the event spaceof at least one of the two distributions.
One inter-pretation for the Jensen-Shannon divergence due toSlonim et al (2002) is that it is related to the log-likelihood that ?the two sample distributions orig-inate by the most likely common source,?
relatingthe quantity to the ?two-sample problem?.In our case, we have p = p(y | x1, x2) and q =p(y | x1), where y is a possible future and x1, x2 areelements of a history context, with q representinga back-off distribution using less context.
There-fore, whereas the standard JS formulation is agnos-min max avg.
medianJS 0?1 2.729E-7 2.168 0.1148 0.09672JS 1?2 0.001318 1.962 0.6929 0.6986JS 0?2 0.001182 1.180 0.3774 0.3863Table 5: Jensen-Shannon statistics for back-off pa-rameters in PMw .tic with respect to its two distributions, and averagesthem in part to ensure that the quantity is definedover the entire space, we have the prior knowledgethat one history context is a superset of the other,that ?x1?
is defined wherever ?x1, x2?
is.
In this case,then, we have a simpler, ?one-sided?
definition forthe Jensen-Shannon divergence, but generalized tothe multiple distributions that include an extra his-tory component:JS (p ?
q ) =?x2p(x2) ?
D (p(y | x1, x2) ?
p(y | x1) )= Ex2 D (p(y | x1, x2) ?
p(y | x1) ) (3)An interpretation in our case is that this is the ex-pected number of bits x2 gives you when trying topredict y.5 If we allow x2 to represent an arbitraryamount of context, then the Jensen-Shannon diver-gence JS b?a = JS (pb || pa) can be computed forany two back-off levels, where a, b are back-off lev-els s.t.
b < a (meaning pb is a distribution usingmore context than pa).
The actual value in bits ofthe Jensen-Shannon divergence between two distri-butions should be considered in relation to the num-ber of bits of entropy of the more detailed distribu-tion; that is, JS b?a should be considered relative toH(pb).
Having explored entropy in ?4, we will nowlook at some summary statistics for JS divergence.5.2 ResultsWe computed the quantity in Equation 3 for everyparameter in PMw that used maximal context (con-tained a head word) and its associated parameterthat did not contain the head word.
The results arelisted in Table 5.
Note that, for this parameter classwith a median entropy of 3.8 bits, we have a medianJS divergence of only 0.097 bits.
The distributionsare so similar that the 28.8% of the time that the de-coder uses an estimate based on a bigram, it mightas well be using one that does not include the headword.5Or, following from Slonim et al?s interpretation, this quan-tity is the (negative of the) log-likelihood that all distributionsthat include an x2 component come from a ?common source?that does not include this component.?
40 words?00 ?23Model LR LP LR LPm3 n/a n/a 88.6 88.7m2-emu 89.9 90.0 88.8 88.9reduced 90.0 90.2 88.7 88.9all sentencesModel ?00 ?23m3 n/a n/a 88.0 88.3m2-emu 88.8 89.0 88.2 88.3reduced 89.0 89.0 88.0 88.2Table 6: Parsing results on Sections 00 and 23 withCollins?
Model 3, our emulation of Collins?
Model2 and the reduced version at a threshold of 0.06.
LR= labeled recall, LP = labeled precision.66 Distributional Similarity and ParameterSelectionThe analysis of the previous two sections providesa window onto what types of parameters the pars-ing model is learning most and least well, and ontowhat parameters carry more and less useful infor-mation.
Having such a window holds the promiseof discovering new parameter types or features thatwould lead to greater parsing accuracy; such is thescientific, or at least, the forward-minded researchperspective.From a much more purely engineering perspec-tive, one can also use the analysis of the previoustwo sections to identify individual parameters thatcarry little to no useful information and simply re-move them from the model.
Specifically, if pb isa particular distribution and pb+1 is its correspond-ing back-off distribution, then one can remove allparameters pb such thatJS (pb||pb+1)H(pb) < t,where 0 < t < 1 is some threshold.
Table 6 showsthe results of this experiment using a threshold of0.06.
To our knowledge, this is the first exampleof detailed parameter selection in the context of agenerative lexicalized statistical parsing model.
Theconsequence is a significantly smaller model thatperforms with no loss of accuracy compared to thefull model.6Further insight is gained by looking at the per-centage of parameters removed from each parame-ter class.
The results of (Bikel, 2004) suggested thatthe power of Collins-style parsing models did not6None of the differences between the Model 2?emulationresults and the reduced model results is statistically significant.PH 13.5% PTOPw 0.023%PsubcatL 0.67% PM 10.1%PsubcatR 1.8% PMw 29.4%Table 7: Percentage of parameters removed fromeach parameter class for the 0.06-reduced model.lie primarily with the use of bilexical dependenciesas was once thought, but in lexico-structural depen-dencies, that is, predicting syntactic structures con-ditioning on head words.
The percentages of Table7 provide even more concrete evidence of this as-sertion, for whereas nearly a third of the PMw pa-rameters were removed, a much smaller fraction ofparameters were removed from the PsubcatL , PsubcatRand PM classes that generate structure conditioningon head words.7 DiscussionExamining the lower-entropy PMw distributions re-vealed that, in many cases, the model was not somuch learning how to disambiguate a given syn-tactic/lexical choice, but simply not having muchto learn.
For example, once a partially-lexicalizednonterminal has been generated whose tag is fairlyspecialized, such as IN, then the model has ?painteditself into a lexical corner?, as it were (the extremeexample is TO, a tag that can only be assigned to theword to).
This is an example of the ?label bias?problem, which has been the subject of recent dis-cussion (Lafferty et al, 2001; Klein and Manning,2002).
Of course, just because there is ?label bias?does not necessarily mean there is a problem.
Ifthe decoder pursues a theory to a nonterminal/part-of-speech tag preterminal that has an extremely lowentropy distribution for possible head words, thenthere is certainly a chance that it will get ?stuck?
in apotentially bad theory.
This is of particular concernwhen a head word?which the top-down model gen-erates at its highest point in the tree?influences anattachment decision.
However, inspecting the low-entropy word-generation histories of PMw revealedthat almost all such cases are when the model isgenerating a preterminal, and are thus of little to noconsequence vis-a-vis syntactic disambiguation.8 Conclusion and Future WorkWith so many parameters, a lexicalized statisticalparsing model seems like an intractable behemoth.However, as statisticians have long known, an ex-cellent angle of attack for a mass of unruly datais exploratory data analysis.
This paper presentssome of the first data visualizations of parametersin a parsing model, and follows up with a numericalanalysis of properties of those distributions.
In thecourse of this analysis, we have focused in on thequestion of bilexical dependencies.
By constrain-parsing the parser?s own output, and by hypothe-sizing and testing for distributional similarity, wehave presented evidence that finally explains that(a) bilexical statistics are actually getting used withgreat frequency in the parse theories that will ulti-mately have the highest score, but (b) the distribu-tions involving bilexical statistics are so similar totheir back-off counterparts as to make them nearlyindistinguishable insofar as making different parsedecisions.
Finally, our analysis has provided for thefirst time an effective way to do parameter selec-tion with a generative lexicalized statistical parsingmodel.Of course, there is still much more analysis, hy-pothesizing, testing and extrapolation to be done.
Athorough study of the highest-entropy distributionsshould reveal new ways in which to use grammartransforms or develop features to reduce the entropyand increase parse accuracy.
A closer look at thelow-entropy distributions may reveal additional re-ductions in the size of the model, and, perhaps, away to incorporate hard constraints without disturb-ing the more ambiguous parts of the model moresuited to machine learning than human engineering.9 AcknowledgementsThanks to Mitch Marcus, David Chiang and Ju-lia Hockenmaier for their helpful comments on thiswork.
I would also like to thank Bob Moore forasking some insightful questions that helped promptthis line of research.
Thanks also to FernandoPereira, with whom I had invaluable discussionsabout distributional similarity.
This work was sup-ported in part by DARPA grant N66001-00-1-9815.ReferencesDaniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
In Pro-ceedings of HLT2002, San Diego, CA.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics.
To appear.E.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavens, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverage ofEnglish grammars.
In Speech and Natural LanguageWorkshop, pages 306?311, Pacific Grove, California.Morgan Kaufmann Publishers.Ezra Black, Frederick Jelinek, John Lafferty, DavidMagerman, Robert Mercer, and Salim Roukos.1992a.
Towards history-based grammars: Usingricher models for probabilistic parsing.
In Proceed-ings of the 5th DARPA Speech and Natural LanguageWorkshop, Harriman, New York.Ezra Black, John Lafferty, and Salim Roukos.
1992b.Development and evaluation of a broad-coverageprobabilistic grammar of english-language computermanuals.
In Proceedings of the 30th ACL, pages 185?192.Eugene Charniak.
2000.
A maximum entropy?inspiredparser.
In Proceedings of the 1st NAACL, pages 132?139, Seattle, Washington, April 29 to May 4.Michael John Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In International Conferenceon Machine Learning.Thomas Cover and Joy A. Thomas.
1991.
Elements ofInformation Theory.
John Wiley & Sons, Inc., NewYork.Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proceed-ings of the 16th International Conference on Com-putational Linguistics (COLING-96), pages 340?345,Copenhagen, August.Jason Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In Harry Bunt and An-ton Nijholt, editors, Advances in Probabilistic andOther Parsing Technologies, pages 29?62.
KluwerAcademic Publishers, October.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing,Pittsburgh, Pennsylvania.Dan Klein and Christopher D. Manning.
2002.
Condi-tional structure versus conditional estimation in NLPmodels.
In Proceedings of the 2002 Conference onEmpirical Methods for Natural Language Processing.John Lafferty, Fernando Pereira, and Andrew McCal-lum.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
InICML.Lillian Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th ACL, pages 25?32.Jianhua Lin.
1991.
Divergence measures based on theShannon entropy.
IEEE Transactions on InformationTheory, 37(1):145?151.David Magerman.
1994.
Natural Language Parsing asStatistical Pattern Recognition.
Ph.D. thesis, Univer-sity of Pennsylvania, Philadelphia, Pennsylvania.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19:313?330.Noam Slonim, Nir Friedman, and Naftali Tishby.2002.
Unsupervised document classification usingsequential information maximization.
Technical Re-port 2002?19, Leibniz Center, The School of Com-puter Science and Engineering, Hebrew University,Jerusalem, Israel.
