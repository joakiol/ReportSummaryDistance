Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1473?1481,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLong-Short Range Context Neural Networks for Language ModelingYoussef Oualil1,2 and Mittul Singh1,3 and Clayton Greenberg1,2,3 and Dietrich Klakow1,2,31Spoken Language Systems (LSV)2Collaborative Research Center on Information Density and Linguistic Encoding3Graduate School of Computer ScienceSaarland University, Saarbru?cken, Germany{firstname.lastname}@lsv.uni-saarland.deAbstractThe goal of language modeling techniques isto capture the statistical and structural prop-erties of natural languages from training cor-pora.
This task typically involves the learningof short range dependencies, which generallymodel the syntactic properties of a languageand/or long range dependencies, which are se-mantic in nature.
We propose in this paper anew multi-span architecture, which separatelymodels the short and long context informa-tion while it dynamically merges them to per-form the language modeling task.
This is donethrough a novel recurrent Long-Short RangeContext (LSRC) network, which explicitlymodels the local (short) and global (long)context using two separate hidden states thatevolve in time.
This new architecture is anadaptation of the Long-Short Term Memorynetwork (LSTM) to take into account the lin-guistic properties.
Extensive experiments con-ducted on the Penn Treebank (PTB) and theLarge Text Compression Benchmark (LTCB)corpus showed a significant reduction of theperplexity when compared to state-of-the-artlanguage modeling techniques.1 IntroductionA high quality Language Model (LM) is consideredto be an integral component of many systems forspeech and language technology applications, suchas machine translation (Brown et al, 1990), speechrecognition (Katz, 1987), etc.
The goal of an LMis to identify and predict probable sequences of pre-defined linguistic units, which are typically words.These predictions are typically guided by the seman-tic and syntactic properties encoded by the LM.In order to capture these properties, classical LMswere typically developed as fixed (short) contexttechniques such as, the word count-based meth-ods (Rosenfeld, 2000; Kneser and Ney, 1995), com-monly known as N -gram language models, as wellas the Feedforward Neural Networks (FFNN) (Ben-gio et al, 2003), which were introduced as an al-ternative to overcome the exponential growth of pa-rameters required for larger context sizes in N -grammodels.In order to overcome the short context constraintand capture long range dependencies known to bepresent in language, Bellegarda (1998a) proposed touse Latent Semantic Analysis (LSA) to capture theglobal context, and then combine it with the standardN -gram models, which capture the local context.
Ina similar but more recent approach, Mikolov andZweig (2012) showed that Recurrent Neural Net-work (RNN)-based LM performance can be signif-icantly improved using an additional global topicinformation obtained using Latent Dirichlet Allo-cation (LDA).
In fact, although recurrent architec-tures theoretically allow the context to indefinitelycycle in the network, Hai Son et al (2012) haveshown that, in practice, this information changesquickly in the classical RNN (Mikolov et al, 2010)structure, and that it is experimentally equivalentto an 8-gram FFNN.
Another alternative to modellinguistic dependencies, Long-Short Term Memory(LSTM) (Sundermeyer et al, 2012), addresses somelearning issues from the original RNN by control-ling the longevity of context information in the net-1473work.
This architecture, however, does not particu-larly model long/short context but rather uses a sin-gle state to model the global linguistic context.Motivated by the works in (Bellegarda, 1998a;Mikolov and Zweig, 2012), this paper proposes anovel neural architecture which explicitly models 1)the local (short) context information, generally syn-tactic, as well as 2) the global (long) context, whichis semantic in nature, using two separate recurrenthidden states.
These states evolve in parallel withina long-short range context network.
In doing so,the proposed architecture is particularly adapted tomodel natural languages that manifest local-globalcontext information in their linguistic properties.We proceed as follows.
Section 2 presents abrief overview of short vs long range context lan-guage modeling techniques.
Section 3 introducesthe novel architecture, Long-Short Range Context(LSRC), which explicitly models these two depen-dencies.
Then, Section 4 evaluates the proposed net-work in comparison to different state-of-the-art lan-guage models on the PTB and the LTCB corpus.
Fi-nally, we conclude in Section 5.2 Short vs Long Context Language ModelsThe goal of a language model is to estimate theprobability distribution p(wT1 ) of word sequenceswT1 = w1, ?
?
?
, wT .
Using the chain rule, this dis-tribution can be expressed asp(wT1 ) =T?t=1p(wt|wt?11 ) (1)This probability is generally approximated underdifferent simplifying assumptions, which are typi-cally derived based on different linguistic observa-tions.
All these assumptions, however, aim at mod-eling the optimal context information, be it syntac-tic and/or semantic, to perform the word prediction.The resulting models can be broadly classified intotwo main categories: long and short range contextmodels.
The rest of this section presents a briefoverview of these categories with a particular focuson Neural Network (NN)-based models.2.1 Short Range ContextThis category includes models that approximate (1)based on the Markov dependence assumption of or-derN?1.
That is, the prediction of the current worddepends only on the last N ?
1 words in the history.In this case, (1) becomesp(wT1 ) ?T?t=1p(wt|wt?1t?N+1) (2)The most popular methods that subscribe in thiscategory are the N -gram models (Rosenfeld, 2000;Kneser and Ney, 1995) as well as the FFNNmodel (Bengio et al, 2003), which estimateseach of the terms involved in this product, i.e,p(wt|wt?1t?N+1) in a single bottom-up evaluation ofthe network.Although these methods perform well and areeasy to learn, the natural languages that they try toencode, however, are not generated under a Markovmodel due to their dynamic nature and the longrange dependencies they manifest.
Alleviating thisassumption led to an extensive research to developmore suitable modeling techniques.2.2 Long Range ContextConventionally, N-gram related LMs have not beenbuilt to capture long linguistic dependencies, al-though significant word triggering information isstill available for large contexts.
To illustrate suchtriggering correlations spread over a large context,we use correlation defined over a distance d, givenby cd(w1, w2) = Pd(w1,w2)P (w1)P (w2) .
A value greater than1 shows that it is more likely that the word w1 fol-lows w2 at a distance d than expected without theoccurrence ofw2.
In Figure 1, we show the variationof this correlation for pronouns with the distance d.It can be observed that seeing another ?he?
abouttwenty words after having seen a first ?he?
is muchmore likely.
A similar observation can be made forthe word ?she?.
It is, however, surprising that seeing?he?
after ?he?
is three times more likely than see-ing ?she?
after ?she?, so ?he?
is much more predic-tive.
In the cases of cross-word triggering of ?he???she?
and ?she??
?he?, we find that the correlationis suppressed in comparison to the same word trig-gering for distances larger than three.
In summary,Figure 1 demonstrates that word triggering informa-tion exists at large distances, even up to one thou-sand words.
These conclusions were confirmed bysimilar correlation experiments that we conducted1474for different types of words and triggering relations.024681012141  10  100  1000CorrelationDistancehe -> shehe -> heshe -> heshe -> sheStat.
IndFigure 1: Variation of word triggering correlations for pro-nouns over large distances.In order to model this long-term correlation andovercome the restrictive Markov assumption, recur-rent language models have been proposed to approx-imate (1) according top(wT1 ) ?T?t=1p(wt|wt?1, ht?1) =T?t=1p(wt|ht) (3)In NN-based recurrent models, ht is a contextvector which represents the complete history, andmodeled as a hidden state that evolves within thenetwork.2.2.1 Elman-Type RNN-based LMThe classical RNN (Mikolov et al, 2010) esti-mates each of the product terms in (3) according toHt = f (Xt?1 + V ?Ht?1) (4)Pt = g (W ?Ht) (5)where Xt?1 is a continuous representation (i.e,embedding) of the word wt?1, V encodes the re-current connection weights and W is the hidden-to-output connection weights.
These parameters definethe network and are learned during training.
More-over, f(?)
is an activation function, whereas g(?)
isthe softmax function.
Figure (2) shows an exampleof the standard RNN architecture.Theoretically, the recurrent connections of anRNN allow the context to indefinitely cycle in theFigure 2: Elman RNN architecture.network and thus, modeling long context.
In prac-tice, however, Hai Son et al (2012) have shown thatthis information changes quickly over time, and thatit is experimentally equivalent to an 8-gram FFNN.This observation was confirmed by the experimentsthat we report in this paper.2.2.2 Long-Short Term Memory NetworkIn order to alleviate the rapidly changing contextissue in standard RNNs and control the longevityof the dependencies modeling in the network, theLSTM architecture (Sundermeyer et al, 2012) in-troduces an internal memory state Ct, which explic-itly controls the amount of information, to forget orto add to the network, before estimating the currenthidden state.
Formally, this is done according to{i, f, o}t = ?
(U i,f,o ?Xt?1 + V i,f,o ?Ht?1)(6)C?t = f (U c ?Xt?1 + V c ?Ht?1) (7)Ct = ft  Ct?1 + it  C?t (8)Ht = ot  f (Ct) (9)Pt = g (W ?Ht) (10)where  is the element-wise multiplication opera-tor, C?t is the memory candidate, whereas it, ft andot are the input, forget and output gates of the net-work, respectively.
Figure 3 illustrates the recurrentmodule of an LSTM network.
Learning of an LSTMmodel requires the training of the network parame-ters U i,f,o,c, V i,f,o,c and W .Although LSTM models have been shown to out-perform classical RNN in modeling long range de-pendencies, they do not explicitly model long/shortcontext but rather use a single state to encode theglobal linguistic context.1475Figure 3: Block diagram of the recurrent module of an LSTMnetwork.3 Multi-Span Language ModelsThe attempts to learn and combine short and longrange dependencies in language modeling led towhat is known as multi-span LMs (Bellegarda,1998a).
The goal of these models is to learn thevarious constraints, both local and global, that arepresent in a language.
This is typically done usingtwo different models, which separately learn the lo-cal and global context, and then combine their re-sulting linguistic information to perform the wordprediction.
For instance, Bellegarda (1998b) pro-posed to use Latent Semantics Analysis (LSA) tocapture the global context, and then combine it withthe standard N -gram models, which capture the lo-cal context, whereas Mikolov and Zweig (2012)proposed to model the global topic information us-ing Latent Dirichlet Allocation (LDA), which is thencombined with an RNN-based LM.
This idea is notparticular to language modeling but has been alsoused in other Natural Language Processing (NLP)tasks, e.g., Anastasakos et al (2014) proposed to usea local/global model to perform a spoken languageunderstanding task.3.1 Long-Short Range Context NetworkFollowing the line of thoughts in (Bellegarda,1998b; Mikolov and Zweig, 2012), we propose anew multi-span model, which takes advantage of theLSTM ability to model long range context while,simultaneously, learning and integrating the shortcontext through an additional recurrent, local state.In doing so, the resulting Long-Short Range Con-text (LSRC) network is able to separately model theshort/long context while it dynamically combinesthem to perform the next word prediction task.
For-mally, this new model is defined asH lt = f(Xt?1 + U cl ?H lt?1)(11){i, f, o}t = ?
(V i,f,ol ?H lt + V i,f,og ?Hgt?1)(12)C?t = f(V cl ?H lt + V cg ?Hgt?1)(13)Ct = ft  Ct?1 + it  C?t (14)Hgt = ot  f (Ct) (15)Pt = g (W ?Hgt ) (16)Learning of an LSRC model requires the trainingof the local parameters V i,f,o,cl and U cl , the globalparameters V i,f,o,cg and the hidden-to-output connec-tion weightsW .
This can be done using the standardBack-Propagation Through Time (BPTT) algorithm,which is typically used to train recurrent networks.The proposed approach uses two hidden states,namely, H lt and Hgt to model short and long rangecontext, respectively.
More particularly, the localstate H lt evolves according to (11) which is noth-ing but a simple recurrent model as it is defined in(4).
In doing so, H lt is expected to have a similar be-havior to RNN, which has been shown to capturelocal/short context (up to 10 words), whereas theglobal state Hgt follows the LSTM model, which isknown to capture longer dependencies (see examplein Figure 5).
The main difference here, however, isthe dependence of the network modules (gates andmemory candidate) on the previous local state H ltinstead of the last seen word Xt?1.
This model isbased on the assumption that the local context car-ries more linguistic information, and is therefore,more suitable to combine with the global context andupdate LSTM, compared to the last seen word.
Fig-ure 4 illustrates the recurrent module of an LSRCnetwork.
It is worth mentioning that this model wasnot particularly developed to separately learn syn-tactic and semantic information.
This may come,however, as a result of the inherent local and globalnature of these two types of linguistic properties.3.2 Context Range EstimationFor many NLP applications, capturing the globalcontext information can be a crucial component todevelop successful systems.
This is mainly due to1476Figure 4: Block diagram of the recurrent module of an LSRCnetwork.the inherent nature of languages, where a single ideaor topic can span over few sentences, paragraphs ora complete document.
LSA-like approaches take ad-vantage of this property, and aim at extracting somehidden ?concepts?
that best explain the data in a low-dimension ?semantic space?.
To some extent, thehidden layer of LSRC/LSTM can be seen as a vec-tor in a similar space.
The information stored in thisvector, however, changes continuously based on theprocessed words.
Moreover, interpreting its contentis generally difficult.
As an alternative, measuringthe temporal correlation of this hidden vector canbe used as an indicator of the ability of the networkto model short and long context dependencies.
For-mally, the temporal correlation of a hidden state Hover a distance d is given bycd =1Dt=D?t=1SM(Ht, Ht+d) (17)where D is the test data size in words and SM isa similarity measure such as the cosine similarity.This measure allows us to evaluate how fast does theinformation stored in the hidden state change overtime.In Figure 5, we show the variation of this tempo-ral correlation for the local and global states of theproposed LSRC network in comparison to RNN andLSTM for various values of the distance d (up to3000).
This figure was obtained on the test set ofthe Penn Treebank (PTB) corpus, described in Sec-tion (4).
The main conclusion we can draw from thisfigure is the ability of the LSRC local and global100 101 102 10300.20.40.60.811.2LSRC Local StateLSRC Global StateLSTM StateRNN StateFigure 5: Temporal correlation of the proposed network incomparison to LSTM and RNN.states (trained jointly) to behave in a similar fash-ion to RNN and LSTM states (trained separately),respectively.
We can also conclude that the LSRCglobal state and LSTM are able to capture long rangecorrelations, whereas the context changes rapidlyover time in RNN and LSRC local state.4 Experiments and Results4.1 Experimental SetupWe evaluated the proposed architecture on two dif-ferent benchmark tasks.
The first set of experi-ments was conducted on the commonly used PennTreebank (PTB) corpus using the same experimentalsetup adopted in (Mikolov et al, 2011) and (Zhanget al, 2015).
Namely, sections 0-20 are used fortraining while sections 21-22 and 23-24 are used forvalidation an testing, respectively.
The vocabularywas limited to the most 10k frequent words while theremaining words were mapped to the token <unk>.In order to evaluate how the proposed approachperforms on large corpora in comparison to othermethods, we run a second set of experiments on theLarge Text Compression Benchmark (LTCB) (Ma-honey, 2011).
This corpus is based on the enwik9dataset which contains the first 109 bytes of enwiki-20060303-pages-articles.xml.
We adopted the sametraining-test-validation data split as well as the thesame data processing1 which were used in (Zhang etal., 2015).
The vocabulary is limited to the most 80k1All the data processing steps described here forPTB and LTCB were performed using the FOFEtoolkit in (Zhang et al, 2015), which is available athttps://wiki.eecs.yorku.ca/lab/MLL/_media/projects:fofe:fofe-code.zip1477frequent words with all remaining words replaced by<unk>.
Details about the sizes of these two corporacan be found in Table 1.Corpus Train Dev TestPTB 930K 74K 82KLTCB 133M 7.8M 7.9MTable 1: Corpus size in number of words.Similarly to the RNN LM toolkit2 (Mikolov et al,2011), we have used a single end sentence tag be-tween each two consecutive sentences, whereas thebegin sentence tag was not included3.4.2 Baseline ModelsThe proposed LSRC architecture is compared todifferent LM approaches that model short or longrange context.
These include the commonly usedN -gram Kneser-Ney (KN) (Kneser and Ney, 1995)model with and without cache (Kuhn and De Mori,1990), as well as different feedforward and recurrentneural architectures.
For short (fixed) size contextmodels, we compare our method to 1) the FFNN-based LM (Bengio et al, 2003), as well as 2) theFixed-size Ordinally Forgetting Encoding (FOFE)approach, which is implemented in (Zhang et al,2015) as a sentence-based model.
For these shortsize context models, we report the results of dif-ferent history window sizes (1, 2 and 4).
The 1st,2nd and 4th-order FOFE results were either reportedin (Zhang et al, 2015) or obtained using the freelyavailable FOFE toolkit 1.For recurrent models that were designed to cap-ture long term context, we compared the pro-posed approach to 3) the full RNN (withoutclasses) (Mikolov et al, 2011), 4) to a deep RNN(D-RNN)4 (Pascanu et al, 2013), which investigatesdifferent approaches to construct mutli-layer RNNs,and finally 5) to the LSTM model (Sundermeyer etal., 2012), which explicitly regulates the amount of2The RNN LM toolkit is available at http://www.rnnlm.org/3This explains the difference in the corpus size compared tothe one reported in (Zhang et al, 2015).4The deep RNN results were obtained using Lp and maxoutunits, dropout regularization and gradient control techniques,which are known to significantly improve the performance.None of these techniques, however, were used in our experi-ments.information that propagates in the network.
Therecurrent models results are reported for differentnumbers of hidden layers (1 or 2).
In order to inves-tigate the impact of deep models on the LSRC ar-chitecture, we added a single hidden, non-recurrentlayer (of size 400 for PTB and 600 for the LTCB ex-periments) to the LSRC model (D-LSRC).
This wassufficient to improve the performance with a negli-gible increase in the number of model parameters.4.3 PTB ExperimentsFor the PTB experiments, the FFNN and FOFEmodels use a word embedding size of 200, whereasthe hidden layer(s) size is fixed at 400, with all hid-den units using the Rectified Linear Unit (ReLu)i.e., f(x) = max(0, x) as activation function.
Wealso use the same learning setup adopted in (Zhanget al, 2015).
Namely, we use the stochastic gra-dient descent algorithm with a mini-batch size of200, the learning rate is initialized to 0.4, the mo-mentum is set to 0.9, the weight decay is fixed at4?10?5, whereas the training is done in epochs.
Theweights initialization follows the normalized initial-ization proposed in (Glorot and Bengio, 2010).
Sim-ilarly to (Mikolov et al, 2010), the learning rate ishalved when no significant improvement of the val-idation data log-likelihood is observed.
Then, wecontinue with seven more epochs while halving thelearning rate after each epoch.Regarding the recurrent models, we use f =tanh(?)
as activation function for all recurrent lay-ers, whereas ?f = sigmoid(?)?
is used for the input,forget and output gates of LSTM and LSRC.
Theadditional non-recurrent layer in D-LSRC, however,uses the ReLu activation function.
The word em-bedding size was set to 200 for LSTM and LSRCwhereas it is the same as the hidden layer size forRNN (result of the RNN equation 4).
In order toillustrate the effectiveness of the LSRC model, wealso report the results when the embedding size isfixed at 100, LSRC(100).
The training uses theBPTT algorithm for 5 time steps.
Similarly to shortcontext models, the mini-batch was set to 200.
Thelearning rate, however, was set to 1.0 and the weightdecay to 5 ?
10?5.
The use of momentum did notlead to any additional improvement.
Moreover, thedata is processed sequentially without any sentenceindependence assumption.
Thus, the recurrent mod-1478els will be able to capture long range dependenciesthat exist beyond the sentence boundary.In order to compare the model sizes, we also re-port the Number of Parameters (NoP) to train foreach of the models above.model model+KN5 NoPN-1= 1 2 4 1 2 4 4KN 186 148 141 ?
?
?
?KN+cache 168 134 129 ?
?
?
?1 Hidden LayerFFNN 176 131 119 132 116 107 6.32MFOFE 123 111 112 108 100 101 6.32MRecurrent Models (1 Layer)RNN 117 104 8.16MLSTM (1L) 113 99 6.96MLSRC(100) 109 96 5.81MLSRC(200) 104 94 7.0M2 Hidden LayersFFNN 176 129 114 132 114 102 6.96MFOFE 116 108 109 104 98 97 6.96MDeep Recurrent ModelsD-LSTM (2L) 110 97 8.42MD-RNN4 (3L) 107.5 NR 6.16MD-LSRC(100) 103 93 5.97MD-LSRC(200) 102 92 7.16MTable 2: LMs performance on the PTB test set.Table 2 shows the perplexity evaluation on thePTB test set.
As a first observation, we can clearlysee that the proposed approach outperforms all othermodels for all configurations, in particular, RNN andLSTM.
This observation includes other models thatwere reported in the literature, such as random for-est LM (Xu and Jelinek, 2007), structured LM (Fil-imonov and Harper, 2009) and syntactic neural net-work LM (Emami and Jelinek, 2004).
More partic-ularly, we can conclude that LSRC, with an embed-ding size of 100, achieves a better performance thanall other models while reducing the number of pa-rameters by ?
29% and ?
17% compared to RNNand LSTM, respectively.
Increasing the embeddingsize to 200, which is used by the other models, im-proves significantly the performance with a resultingNoP comparable to LSTM.
The significance of theimprovements obtained here over LSTM were con-firmed through a statistical significance t-test, whichled to p-values ?
10?10 for a significance level of5% and 0.01%, respectively.The results of the deep models in Table 2 alsoshow that adding a single non-recurrent hidden layerto LSRC can significantly improve the performance.In fact, the additional layer bridges the gap betweenthe LSRC models with an embedding size of 100and 200, respectively.
The resulting architecturesoutperform the other deep recurrent models with asignificant reduction of the number of parameters(for the embedding size 100), and without usageof dropout regularization, Lp and maxout units orgradient control techniques compared to the deepRNN4(D-RNN).We can conclude from these experiments that theexplicit modeling of short and long range dependen-cies using two separate hidden states improves theperformance while significantly reducing the num-ber of parameters.100 200 300 400 500 600 700 80095100105110115120125130135140145Hidden Layer SizePPLRNNLSTMLSRC (100)LSRC (200)D?LSRC (200)Figure 6: Perplexity of the different NN-based LMs with dif-ferent hidden layer sizes on the PTB test set.In order to show the consistency of the LSRC im-provement over the other recurrent models, we re-port the variation of the models performance withrespect to the hidden layer size in Figure 6.
This fig-ure shows that increasing the LSTM or RNN hiddenlayer size could not achieve a similar performance tothe one obtained using LSRC with a small layer size(e.g., 300).
It is also worth mentioning that this ob-servation holds when comparing a 2-recurrent lay-ers LSTM to LSRC with an additional non-recurrentlayer.14794.4 LTCB ExperimentsThe LTCB experiments use the same PTB setupwith minor modifications.
The results shown in Ta-ble 3 follow the same experimental setup proposedin (Zhang et al, 2015).
More precisely, these resultswere obtained without use of momentum or weightdecay (due to the long training time required forthis corpus), the mini-batch size was set to 400, thelearning rate was set to 0.4 and the BPTT step wasfixed at 5.
The FFNN and FOFE architectures use 2hidden layers of size 600, whereas RNN, LSTM andLSRC have a single hidden layer of size 600.
More-over, the word embedding size was set to 200 for allmodels except RNN, which was set to 600.
We alsoreport results for an LSTM with 2 recurrent layers aswell as for LSRC with an additional non-recurrentlayer.
The recurrent layers are marked with an ?R?in Table 3.model NoPContext Size M=N-1 1 2 4 4KN 239 156 132 ?KN+cache 188 127 109 ?FFNN [M*200]-600-600-80k 235 150 114 64.84MFOFE [M*200]-600-600-80k 112 107 100 64.84MRNN [600]-R600-80k 85 96.36MLSTM [200]-R600-80k 66 65.92MLSTM [200]-R600-R600-80k 61 68.80MLSRC [200]-R600-80k 63 65.96MLSRC [200]-R600-600-80k 59 66.32MTable 3: LMs performance on the LTCB test set.The results shown in Table 3 generally confirmthe conclusions we drew from the PTB experimentsabove.
In particular, we can see that the proposedLSRC model largely outperforms all other models.In particular, LSRC clearly outperforms LSTM witha negligible increase in the number of parameters(resulting from the additional 200 ?
200 = 0.04Mlocal connection weights U cl ) for the single layerresults.
We can also see that this improvement ismaintained for deep models (2 hidden layers), wherethe LSRC model achieves a slightly better perfor-mance while reducing the number of parametersby ?
2.5M and speeding up the training time by?
20% compared to deep LSTM.The PTB and LTCB results clearly highlight theimportance of recurrent models to capture longrange dependencies for LM tasks.
The training ofthese models, however, requires large amounts ofdata to significantly outperform short context mod-els.
This can be seen in the performance of RNNand LSTM in the PTB and LTCB tables above.
Wecan also conclude from these results that the explicitmodeling of long and short context in a multi-spanmodel can lead to a significant improvement overstate-of-the are models.5 Conclusion and Future WorkWe investigated in this paper the importance, fol-lowed by the ability, of standard neural networks toencode long and short range dependencies for lan-guage modeling tasks.
We also showed that thesemodels were not particularly designed to, explicitlyand separately, capture these two linguistic informa-tion.
As an alternative solution, we proposed a novellong-short range context network, which takes ad-vantage of the LSTM ability to capture long rangedependencies, and combines it with a classical RNNnetwork, which typically encodes a much shorterrange of context.
In doing so, this network is ableto encode the short and long range linguistic de-pendencies using two separate network states thatevolve in time.
Experiments conducted on the PTBand the large LTCB corpus have shown that the pro-posed approach significantly outperforms differentstate-of-the are neural network architectures, includ-ing LSTM and RNN, even when smaller architec-tures are used.
This work, however, did not investi-gate the nature of the long and short context encodedby this network or its possible applications for otherNLP tasks.
This is part of our future work.AcknowledgmentsThis work was in part supported by the Cluster ofExcellence for Multimodal Computing and Interac-tion, the German Research Foundation (DFG) aspart of SFB 1102, the EU FP7 Metalogue project(grant agreement number: 611073) and the EU Mal-orca project (grant agreement number: 698824).References[Anastasakos et al2014] Tasos Anastasakos, Young-BumKim, and Anoop Deoras.
2014.
Task specific continu-1480ous word representations for mono and multi-lingualspoken language understanding.
In IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing, ICASSP 2014, Florence, Italy, May 4-9,2014, pages 3246?3250.
[Bellegarda1998a] J. R. Bellegarda.
1998a.
A multi-span language modeling framework for large vocabu-lary speech recognition.
IEEE Transactions on Speechand Audio Processing, 6(5):456?467, Sep.[Bellegarda1998b] Jerome R. Bellegarda.
1998b.
Ex-ploiting both local and global constraints for multi-span statistical language modeling.
In Proceedings ofthe 1998 IEEE International Conference on Acoustics,Speech and Signal Processing, ICASSP ?98, Seattle,Washington, USA, May 12-15, 1998, pages 677?680.
[Bengio et al2003] Yoshua Bengio, Re?jean Ducharme,Pascal Vincent, and Christian Jauvin.
2003.
A neuralprobabilistic language model.
J. Mach.
Learn.
Res.,3:1137?1155, Mar.
[Brown et al1990] Peter F. Brown, John Cocke, StephenA.
Della Pietra, Vincent J. Della Pietra, Fredrick Je-linek, John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A statistical approach to machinetranslation.
Comput.
Linguist., 16(2):79?85, Jun.
[Emami and Jelinek2004] Ahmad Emami and FrederickJelinek.
2004.
Exact training of a neural syntactic lan-guage model.
In IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP),pages 245?248, Montreal, Quebec, Canada, May.
[Filimonov and Harper2009] Denis Filimonov andMary P. Harper.
2009.
A joint language model withfine-grain syntactic tags.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), A meeting of SIGDAT, aSpecial Interest Group of the ACL, pages 1114?1123,Singapore, Aug.[Glorot and Bengio2010] Xavier Glorot and Yoshua Ben-gio.
2010.
Understanding the difficulty of trainingdeep feedforward neural networks.
In Proceedings ofthe Thirteenth International Conference on ArtificialIntelligence and Statistics (AISTATS), pages 249?256,Chia Laguna Resort, Sardinia, Italy, May.
[Hai Son et al2012] Le Hai Son, Alexandre Allauzen,and Franc?ois Yvon.
2012.
Measuring the influenceof long range dependencies with neural network lan-guage models.
In Proceedings of the NAACL-HLT2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modelingfor HLT, pages 1?10, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.
[Katz1987] S. Katz.
1987.
Estimation of probabilitiesfrom sparse data for the language model component ofa speech recognizer.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 35(3):400?401, Mar.
[Kneser and Ney1995] Reinhard Kneser and HermannNey.
1995.
Improved backing-off for m-gram lan-guage modeling.
In International Conference onAcoustics, Speech, and Signal Processing, (ICASSP),pages 181?184, Detroit, Michigan, USA, May.
[Kuhn and De Mori1990] Roland Kuhn and RenatoDe Mori.
1990.
A cache-based natural languagemodel for speech recognition.
IEEE Trans.
PatternAnal.
Mach.
Intell., 12(6):570?583.
[Mahoney2011] Matt Mahoney.
2011.
Large text com-pression benchmark.
[Mikolov and Zweig2012] Tomas Mikolov and GeoffreyZweig.
2012.
Context dependent recurrent neuralnetwork language model.
In 2012 IEEE Spoken Lan-guage Technology Workshop (SLT), Miami, FL, USA,December 2-5, 2012, pages 234?239.
[Mikolov et al2010] Tomas Mikolov, Martin Karafia?t,Luka?s Burget, Jan Cernocky?, and Sanjeev Khudan-pur.
2010.
Recurrent neural network based lan-guage model.
In 11th Annual Conference of the Inter-national Speech Communication Association (INTER-SPEECH), pages 1045?1048, Makuhari, Chiba, Japan,Sep.
[Mikolov et al2011] T. Mikolov, S. Kombrink, L. Burget,J.
ernock, and S. Khudanpur.
2011.
Extensions of re-current neural network language model.
In Acoustics,Speech and Signal Processing (ICASSP), 2011 IEEEInternational Conference on, pages 5528?5531, May.
[Pascanu et al2013] Razvan Pascanu, C?aglar Gu?lc?ehre,Kyunghyun Cho, and Yoshua Bengio.
2013.
Howto construct deep recurrent neural networks.
CoRR,abs/1312.6026.
[Rosenfeld2000] Ronald Rosenfeld.
2000.
Two decadesof statistical language modeling: Where do we go fromhere?
In Proceedings of the IEEE, volume 88, pages1270?1278.
[Sundermeyer et al2012] Martin Sundermeyer, RalfSchlu?ter, and Hermann Ney.
2012.
LSTM neuralnetworks for language modeling.
In Interspeech,pages 194?197, Portland, OR, USA, sep.[Xu and Jelinek2007] Peng Xu and Frederick Jelinek.2007.
Random forests and the data sparseness prob-lem in language modeling.
Computer Speech & Lan-guage, 21(1):105?152.
[Zhang et al2015] Shiliang Zhang, Hui Jiang, MingbinXu, Junfeng Hou, and Li-Rong Dai.
2015.
The fixed-size ordinally-forgetting encoding method for neuralnetwork language models.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing of the Asian Federa-tion of Natural Language Processing ACL, volume 2,pages 495?500, july.1481
