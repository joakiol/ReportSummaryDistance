Regularisation Techniques for ConditionalRandom Fields: Parameterised VersusParameter-FreeAndrew Smith and Miles OsborneSchool of Informatics, University of Edinburgh, United Kingdoma.p.smith-2@sms.ed.ac.uk, miles@inf.ed.ac.ukAbstract.
Recent work on Conditional Random Fields (CRFs) hasdemonstrated the need for regularisation when applying these modelsto real-world NLP data sets.
Conventional approaches to regularisingCRFs has focused on using a Gaussian prior over the model parameters.In this paper we explore other possibilities for CRF regularisation.
Weexamine alternative choices of prior distribution and we relax the usualsimplifying assumptions made with the use of a prior, such as constanthyperparameter values across features.
In addition, we contrast the effec-tiveness of priors with an alternative, parameter-free approach.
Specifi-cally, we employ logarithmic opinion pools (LOPs).
Our results showthat a LOP of CRFs can outperform a standard unregularised CRF andattain a performance level close to that of a regularised CRF, withoutthe need for intensive hyperparameter search.1 IntroductionRecent work on Conditional Random Fields (CRFs) has demonstrated the needfor regularisation when applying these models to real-world NLP data sets ([8],[9]).
Standard approaches to regularising CRFs, and log-linear models in general,has focused on the use of a Gaussian prior.
Typically, for simplicity, this prior isassumed to have zero mean and constant variance across model parameters.
Todate, there has been little work exploring other possibilities.
One exception isPeng & McCallum [8].
They investigated feature-dependent variance for a Gaus-sian prior, and explored different families of feature sets.
They also compareddifferent priors for CRFs on an information extraction task.In the first part of this paper, we compare priors for CRFs on standardsequence labelling tasks in NLP: NER and POS tagging.
Peng & McCallumused variable hyperparameter values only for a Gaussian prior, based on featurecounts in the training data.
We use an alternative Bayesian approach to mea-sure confidence in empirical expected feature counts, and apply this to all thepriors we test.
We also look at varying the Gaussian prior mean.
Our resultsshow that: (1) considerable search is required to identify good hyperparametervalues for all priors (2) for optimal hyperparameter values, the priors we testedperform roughly equally well (3) in some cases performance can be improvedusing feature-dependent hyperparameter values.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
896?907, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Regularisation Techniques for Conditional Random Fields 897As can be seen, a significant short-coming of using priors for CRF regular-isation is the requirement for intensive search of hyperparameter space.
In thesecond part of the paper we contrast this parameterised prior approach withan alternative, parameter-free method.
We factor the CRF distribution into aweighted product of individual expert CRF distributions, each focusing on aparticular subset of the distribution.
We call this model a logarithmic opinionpool (LOP) of CRFs (LOP-CRFs).Our results show that LOP-CRFs, which are unregularised, can outperformthe unregularised standard CRF and attain a performance level that rivals thatof the standard CRF regularised with a prior.
This performance may be achievedwith a considerably lower time for training by avoiding the need for intensivehyperparameter search.2 Conditional Random FieldsA linear chain CRF defines the conditional probability of a label sequence sgiven an observed sequence o via:p(s | o) = 1Z(o)exp(T+1?t=1?k?kfk(st?1, st,o, t))(1)where T is the length of both sequences, ?k are parameters of the model and Z(o)is the partition function that ensures (1) represents a probability distribution.The functions fk are feature functions representing the occurrence of differentevents in the sequences s and o.The parameters ?k can be estimated by maximising the conditional log-likelihood of a set of labelled training sequences.
The log-likelihood is given by:LL(?)
=?op?(o)?sp?(s|o)[T+1?t=1?
?
f(s,o, t)]??op?
(o) log Z(o; ?
)where p?
(s|O) and p?
(o) are empirical distributions defined by the training set.
Atthe maximum likelihood solution the model satisfies a set of feature constraints,whereby the expected count of each feature under the model is equal to itsempirical count on the training data:Ep?
(o,s)[fk] ?
Ep(s|o)[fk] = 0, ?kIn general this cannot be solved for the ?k in closed form so numerical routinesmust be used.
Malouf [6] and Sha & Pereira [9] show that gradient-based algo-rithms, particularly limited memory variable metric (LMVM), require much lesstime to reach convergence, for some NLP tasks, than the iterative scaling meth-ods previously used for log-linear optimisation problems.
In all our experimentswe use the LMVM method to train the CRFs.For CRFs with general graphical structure, calculation of Ep(s|o)[fk] is in-tractable, but for the linear chain case Lafferty et al [5] describe an efficient898 A. Smith and M. Osbornedynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models.Given a trained CRF model defined as in (1), the most probable labellingunder the model for a new observed sequence o is given by argmaxsp(s|o).
Thiscan be recovered efficiently using the Viterbi algorithm.3 Parameterised Regularisation: Priors for CRFsMost approaches to CRF regularisation have focused on the use of a prior distri-bution over the model parameters.
A prior distribution encodes prior knowledgeabout the nature of different models.
However, prior knowledge can be difficultto encode reliably and the optimal choice of prior family may vary from task totask.
In this paper we investigate the use of three prior families for the CRF.3.1 Gaussian PriorThe most common prior used for CRF regularisation has been the Gaussian.
Useof the Gaussian prior assumes that each model parameter is drawn independentlyfrom a Gaussian distribution.
Ignoring terms that do not affect the parameters,the regularised log-likelihood with a Gaussian prior becomes:LL(?)
?
12?k(?k ?
?k?k)2where ?k is the mean and ?k the variance for parameter ?k.
At the optimalpoint, for each ?k, the model satisfies:Ep?
(o,s)[fk] ?
Ep(s|o)[fk] =?k ?
?k?2k(2)Usually, for simplicity, each ?k is assumed zero and ?k is held constant acrossthe parameters.
In this paper we investigate other possibilities.
In particular,we allow the means to take on non-zero values, and the variances to be feature-dependent.
This is described in more detail later.
In each case values for meansand variances may be optimised on a development set.We can see from (2) that use of a Gaussian prior enforces the constraint thatthe expected count of a feature under the model is discounted with respect to thecount of that feature on the training data.
As discussed in [1], this correspondsto a form of logarithmic discounting in feature count space and is similar innature to discounting schemes employed in language modelling.3.2 Laplacian PriorUse of the Laplacian prior assumes that each model parameter is drawn inde-pendently from the Laplacian distribution.
Ignoring terms that do not affect theparameters, the regularised log-likelihood with a Laplacian prior becomes:LL(?)
?
?k|?k|?kRegularisation Techniques for Conditional Random Fields 899where ?k is a hyperparameter, and at the optimal point the model satisfies:Ep?
(o,s)[fk] ?
Ep(s|o)[fk] =sign(?k)?k, ?k = 0 (3)Peng & McCallum [8] note that the exponential prior (a one-sided version of theLaplacian prior here) represents applying an absolute discount to the empiricalfeature count.
They fix the ?k across features and set it using an expression forthe discount used in absolute discounting for language modelling.
By contrast weallow the ?k to vary with feature and optimise values using a development set.The derivative of the penalty term above with respect to a parameter ?k isdiscontinuous at ?k = 0.
To tackle this problem we use an approach describedby Williams, who shows how the discontinuity may be handled algorithmically[13].
His method leads to sparse solutions, where, at convergence, a substantialproportion of the model parameters are zero.
The result of this pruning effect isdifferent, however, to feature induction, where features are included in the modelbased on their effect on log-likelihood.3.3 Hyperbolic PriorUse of the hyperbolic prior assumes that each model parameter is drawn inde-pendently from the hyperbolic distribution.
Ignoring constant terms that do notinvolve the parameters, the regularised log-likelihood becomes:LL(?)
?
?klog(e?k?k + e?
?k?k2)where ?k is a hyperparameter, and at the optimal point the model satisfies:Ep?
(o,s)[fk] ?
Ep(s|o)[fk] = ?k(e?k?k ?
e?
?k?ke?k?k + e?
?k?k)(4)3.4 Feature-Dependent RegularisationFor simplicity it is usual when using a prior to assume constant hyperparametervalues across all features.
However, as a hyperparameter value determines theamount of regularisation applied to a feature, we may not want to assume equalvalues.
We may have seen some features more frequently than others and sobe more confident that their empirical expected counts are closer to the trueexpected counts in the underlying distribution.Peng & McCallum [8] explore feature-dependent variance for the Gaussianprior.
They use different schemes to determine the variance for a feature basedon its observed count in the training data.
In this paper we take an alternative,Bayesian approach motivated more directly by our confidence in the reliabilityof a feature?s empirical expected count.In equations (2), (3) and (4) the level of regularisation applied to a featuretakes the form of a discount to the expected count of the feature on the training900 A. Smith and M. Osbornedata.
It is natural, therefore, that the size of this discount, controlled througha hyperparameter, is related to our confidence in the reliability of the empiri-cal expected count.
We formulate a measure of this confidence.
We follow theapproach of Kazama & Tsujii [4], extending it to CRFs.The empirical expected count, Ep?
(o,s)[fk], of a feature fk is given by:?o,sp?
(o, s)?tfk(st?1, st,o, t)=?op?(o)?sp?
(s|o)?tfk(st?1, st,o, t)=?op?(o)?t,s?,s??p?
(st?1 = s?, st = s?
?|o)fk(s?, s?
?,o, t)Now, our CRF features have the following form:fk(st?1, st,o, t) ={1 if st?1 = s1, st = s2 and hk(o, t) = 10 otherwisewhere s1 and s2 are the labels associated with feature fk and hk(o, t) is a binary-valued predicate defined on observation sequence o at position t. With thisfeature definition, and contracting notation for the empirical probability to savespace, Ep?
(o,s)[fk] becomes:?op?(o)?t,s?,s??p?
(s?, s??|o)?
(s?, s1)?(s?
?, s2)hk(o, t) =?op?(o)?tp?
(s1, s2|o)hk(o, t)=?op?(o)?t:hk(o,t)=1p?
(s1, s2|o)Contributions to the inner sum are only made at positions t in sequence o wherethe hk(o, t) = 1.
Suppose that we make the assumption that at these positionsp?
(s?, s?
?|o) ?
p?
(s?, s?
?|hk(o, t) = 1).
Then:Ep?
(o,s)[fk] =?op?(o)?t:hk(o,t)=1p?
(s1, s2|hk(o, t) = 1)Now, if we assume that we can get a reasonable estimate of p?
(o) from the trainingdata then the only source of uncertainty in the expression for Ep?
(o,s)[fk] is the termp?
(st?1 = s1, st = s2|hk(o, t) = 1).
Assuming this term is independent of sequenceo and position t, we can model it as the parameter ?
of a Bernoulli random variablethat takes the value 1 when feature fk is active and 0 when the feature is not activebut hk(o, t) = 1.
Suppose there are a and b instances of these two events, respec-tively.
We endow the Bernoulli parameter with a uniform prior Beta distributionBe(1,1) and, having observed the training data, we calculate the variance of theposterior distribution, Be(1 + a, 1 + b).
The variance is given by:var[?]
= V =(1 + a)(1 + b)(a + b + 2)2(a + b + 3)The variance of Ep?
(o,s)[fk] therefore given by:var[Ep?
(o,s)[fk]]= V???o?t:hk(o,t)=1p?(o)2?
?Regularisation Techniques for Conditional Random Fields 901We use this variance as a measure of the confidence we have in Ep?
(o,s)[fk] as anestimate of the true expected count of feature fk.
We therefore adjust hyper-parameters in the different priors according to this confidence for each feature.Note that this value for each feature can be calculated off-line.4 Parameter-Free Regularisation: Logarithmic OpinionPoolsSo far we have considered CRF regularisation through the use of a prior.
Aswe have seen, most prior distributions are parameterised by a hyperparameter,which may be used to tune the level of regularisation.
In this paper we alsoconsider a parameter-free method.
Specifically, we explore the use of logarithmicopinion pools [3].Given a set of CRF model experts with conditional distributions p?
(s|o)and a set of non-negative weights w?
with??
w?
= 1, a logarithmic opinionpool is defined as the distribution:p?
(s|o) = 1Z?(o)??[p?(s|o)]w?
, with Z?
(o) =?s??[p?
(s|o)]w?Suppose that there is a ?true?
conditional distribution q(s|o) which eachp?
(s|o) is attempting to model.
In [3] Heskes shows that the KL divergencebetween q(s|o) and the LOP can be decomposed as follows:K (q, p?)
=?
?w?K (q, p?)
??
?w?K (p?, p?)
= E ?
A (5)This explicitly tells us that the closeness of the LOP model to q(s|o) is governedby a trade-off between two terms: an E term, which represents the closeness ofthe individual experts to q(s|o), and an A term, which represents the closeness ofthe individual experts to the LOP, and therefore indirectly to each other.
Hencefor the LOP to model q well, we desire models p?
which are individually goodmodels of q (having low E) and are also diverse (having large A).Training LOPs for CRFs.
The weights w?
may be defined a priori or may befound by optimising an objective criterion.
In this paper we combine pre-trainedexpert CRF models under a LOP and train the weights w?
to maximise thelikelihood of the training data under the LOP.
See [10] for details.Decoding LOPs for CRFs.
Because of the log-linear form of a CRF, aweighted product of expert CRF distributions corresponds to a single CRF distri-bution with log potentials given by a linear combination (with the same weights)of the corresponding log potentials of the experts.
Consequently, it is easy to formthe LOP given a set of weights and expert models, and decoding with the LOPis no more complex than decoding with a standard CRF.
Hence LOP decodingcan be achieved efficiently using the Viterbi algorithm.902 A. Smith and M. Osborne5 The TasksIn this paper we compare parametric and LOP-based regularisation techniquesfor CRFs on two sequence labelling tasks in NLP: named entity recognition(NER) and part-of-speech tagging (POS tagging).5.1 Named Entity RecognitionAll our results for NER are reported on the CoNLL-2003 shared task dataset[12].
For this dataset the entity types are: persons (PER), locations (LOC),organisations (ORG) and miscellaneous (MISC).
The training set consists of14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and46, 666 tokens.5.2 Part-of-Speech TaggingFor our experiments we use the CoNLL-2000 shared task dataset [11].
This has48 different POS tags.
In order to make training time manageable, we collapsethe number of POS tags from 48 to 5 following the procedure used in [7].
Insummary: (1) All types of noun collapse to category N. (2) All types of verbcollapse to category V. (3) All types of adjective collapse to category J.
(4)All types of adverb collapse to category R. (5) All other POS tags collapse tocategory O.
The training set consists of 7, 300 sentences and 173, 542 tokens, thedevelopment set consists of 1, 636 sentences and 38, 185 tokens and the test setconsists of 2, 012 sentences and 47, 377 tokens.5.3 Experts and Expert SetsAs we have seen, our parameter-free LOP models require us to define and traina number of expert models.
For each task we define a single, complex CRF,which we call a monolithic CRF, and a range of expert sets.
The monolithicCRF for NER comprises a number of word and POS features in a window of fivewords around the current word, along with a set of orthographic features definedon the current word.
The monolithic CRF for NER has 450, 345 features.
Themonolithic CRF for POS tagging comprises word and POS features similar tothose in the NER monolithic model, but over a smaller number of orthographicfeatures.
The monolithic model for POS tagging has 188, 488 features.Each of our expert sets consists of a number of CRF experts.
Usually theseexperts are designed to focus on modelling a particular aspect or subset of thedistribution.
The experts from a particular expert set are combined under aLOP-CRF with the unregularised monolithic CRF.We define our expert sets as follows: (1) Simple consists of the monolithicCRF and a single expert comprising a reduced subset of the features in themonolithic CRF.
This reduced CRF models the entire distribution rather thanfocusing on a particular aspect or subset, but is much less expressive than theRegularisation Techniques for Conditional Random Fields 903monolithic model.
The reduced model comprises 24, 818 features for NER and47, 420 features for POS tagging.
(2) Positional consists of the monolithic CRFand a partition of the features in the monolithic CRF into three experts, eachconsisting only of features that involve events either behind, at or ahead ofthe current sequence position.
(3) Label consists of the monolithic CRF and apartition of the features in the monolithic CRF into five experts, one for eachlabel.
For NER an expert corresponding to label X consists only of features thatinvolve labels B-X or I-X at the current or previous positions, while for POStagging an expert corresponding to label X consists only of features that involvelabel X at the current or previous positions.
These experts therefore focus ontrying to model the distribution of a particular label.
(4) Random consists ofthe monolithic CRF and a random partition of the features in the monolithicCRF into four experts.
This acts as a baseline to ascertain the performancethat can be expected from an expert set that is not defined via any linguisticintuition.6 Experimental ResultsFor each task our baseline model is the monolithic model, as defined earlier.All the smoothing approaches that we investigate are applied to this model.
ForNER we report F-scores on the development and test sets, while for POS taggingwe report accuracies on the development and test sets.6.1 PriorsFeature-Independent Hyperparameters.
Tables 1 and 2 give results on thetwo tasks for different priors with feature-independent hyperparameters.
In thecase of the Gaussian prior, the mean was fixed at zero with the variance being theadjustable hyperparameter.
In each case hyperparameter values were optimisedon the development set.
In order to obtain the results shown, extensive searchof the hyperparameter space was required.
The results show that: (1) For eachprior there is a performance improvement over the unregularised model.
(2) Eachof the priors gives roughly the same optimal performance.These results are contrary to the conclusions of Peng & McCallum in [8].
Onan information extraction task they found that the Gaussian prior performedTable 1.
F-scores for priors on NERPrior Development TestUnreg.
monolithic 88.33 81.87Gaussian 89.84 83.98Laplacian 89.56 83.43Hyperbolic 89.84 83.90Table 2.
Accuracies for priors on POStaggingPrior Development TestUnreg.
monolithic 97.92 97.65Gaussian 98.02 97.84Laplacian 98.05 97.78Hyperbolic 98.00 97.85904 A. Smith and M. Osbornesignificantly better than alternative priors.
Indeed they appeared to report per-formance figures for the hyperbolic and Laplacian priors that were lower thanthose of the unregularised model.
There are several possible reasons for thesedifferences.
Firstly, for the hyperbolic prior, Peng & McCallum appeared notto use an adjustable hyperparameter.
In that case the discount applied to eachempirical expected feature count was dependent only on the current value of therespective model parameter and corresponds in our case to using a fixed valueof 1 for the ?
hyperparameter.
Our results for this value of the hyperparameterare similarly poor.
The second reason is that for the Laplacian prior, they againused a fixed value for the hyperparameter, calculated via an absolute discount-ing method used language modelling [1].
Having achieved poor results with thisvalue they experimented with other values but obtained even worse performance.By contrast, we find that, with some search of the hyperparameter space, we canachieve performance close to that of the other two priors.Feature-Dependent Hyperparameters.
Tables 3 and 4 give results for dif-ferent priors with feature-dependent hyperparameters.
Again, for the Gaussianprior the mean was held at 0.
We see here that trends differ between the twotasks.
For POS tagging we see performance improvements with all the priors overthe corresponding feature-independent hyperparameter case.
Using McNemar?smatched-pairs test [2] on point-wise labelling errors, and testing at a significancelevel of 5% level, all values in Table 4 represent a significant improvement overthe corresponding model with feature-independent hyperparameter values, ex-cept the one marked with ?.
However, for NER the opposite is true.
There is aperformance degradation over the corresponding feature-independent hyperpa-rameter case.
Values marked with ?
are significantly worse at the 5% level.
Thehyperbolic prior performs particularly badly, giving no improvement over theunregularised monolithic.
The reasons for these results are not clear.
One pos-sibility is that defining the degree of regularisation on a feature specific basis istoo dependent on the sporadic properties of the training data.
A better idea maybe to use an approach part-way between feature-independent hyperparametersand feature-specific hyperparameters.
For example, features could be clusteredbased on confidence in their empirical expected counts, with a single confidencebeing associated with each cluster.Varying the Gaussian Mean.
When using a Gaussian prior it is usual to fixthe mean at zero because there is usually no prior information to suggest penal-ising large positive values of model parameters any more or less than large mag-Table 3.
F-scores for priors on NERPrior Development TestGaussian 89.43 83.27?Laplacian 89.28 83.37Hyperbolic 88.34?
81.63?Table 4.
Accuracies for priors on POStaggingPrior Development TestGaussian 98.12 97.88?Laplacian 98.12 97.92Hyperbolic 98.15 97.92Regularisation Techniques for Conditional Random Fields 905nitude negative values.
It also simplifies the hyperparameter search, requiringthe need to optimise only the variance hyperparameter.
However, it is unlikelythat optimal performance is always achieved for a mean value of zero.To investigate this we fix the Gaussian variance at the optimal value foundearlier on the development set, with a mean of zero, and allow the mean tovary away from zero.
For both tasks we found that we could achieve significantperformance improvements for non-zero mean values.
On NER a model withmean 0.7 (and variance 40) achieved an F-score of 90.56% on the development setand 84.71% on the test set, a significant improvement over the best model withmean 0.
We observe a similar pattern for POS tagging.
These results suggestthat considerable benefit may be gained from a well structured search of thejoint mean and variance hyperparameter space when using a Gaussian priorfor regularisation.
There is of course a trade-off here, however, between findingbetter hyperparameters values and suffering increased search complexity.6.2 LOP-CRFsTables 5 and 6 show the performance of LOP-CRFs for the NER and POStagging experts respectively.
The results demonstrate that: (1) In every casethe LOPs significantly outperform the unregularised monolithic.
(2) In mostcases the performance of LOPs is comparable to that obtained using the differentpriors on each task.
In fact, values marked with ?
show a significant improvementover the performance obtained with the Gaussian prior with feature-independenthyperparameter values.
Only the value marked with ?
in Table 6 significantlyunder performs that model.Table 5.
LOP F-scores on NERExpert set Development set Test setUnreg.
monolithic 88.33 81.87Simple 90.26 84.22?Positional 90.35 84.71?Label 89.30 83.27Random 88.84 83.06Table 6.
LOP accuracies on POS taggingExpert set Development set Test setUnreg.
monolithic 97.92 97.65Simple 98.31?
98.12?Positional 98.03 97.81Label 97.99 97.77Random 97.99 97.76?We can see that the performance of the LOP-CRFs varies with the choice ofexpert set.
For example, on NER the LOP-CRFs for the simple and positionalexpert sets perform better than those for the label and random sets.
Lookingback to equation 5, we conjecture that the simple and positional expert setsachieve good performance in the LOP-CRF because they consist of experts thatare diverse while simultaneously being reasonable models of the data.
The labelexpert set exhibits greater diversity between the experts, because each expertfocuses on modelling a particular label only, but each expert is a relatively poormodel of the entire distribution.
Similarly, the random experts are in generalbetter models of the entire distribution but tend to be less diverse because they906 A. Smith and M. Osbornedo not focus on any one aspect or subset of it.
Intuitively, then, we want todevise experts that are simultaneously diverse and accurate.The advantage of the LOP-CRF approach over the use of a prior is that itis ?parameter-free?
in the sense that each expert in the LOP-CRF is unregu-larised.
Consequently, we are not required to search a hyperparameter space.For example, to carefully tune the hyperbolic hyperparameter in order to obtainthe optimal value we report here, we ran models for 20 different hyperparametervalues.
In addition, in most cases the expert CRFs comprising the expert setsare small, compact models that train more quickly than the monolithic with aprior, and can be trained in parallel.7 ConclusionIn this paper we compare parameterised and parameter-free approaches tosmoothing CRFs on two standard sequence labelling tasks in NLP.
For theparameterised methods, we compare different priors.
We use both feature-independent and feature-dependent hyperparameters in the prior distributions.In the latter case we derive hyperparameter values using a Bayesian approachto measuring our confidence in empirical expected feature counts.
We find that:(1) considerable search is required to identify good hyperparameter values forall priors (2) for optimal hyperparameter values, the priors we tested performroughly equally well (3) in some cases performance can be improved usingfeature-dependent hyperparameter values.We contrast the use of priors to an alternative, parameter-free method usinglogarithmic opinion pools.
Our results show that a LOP of CRFs, which containsunregularised models, can outperform the unregularised standard CRF and at-tain a performance level that rivals that of the standard CRF regularised with aprior.
The important point, however, is that this performance may be achievedwith a considerably lower time for training by avoiding the need for intensivehyperparameter search.References1.
Chen, S. and Rosenfeld, R.: A Survey of Smoothing Techniques for ME Models.IEEE Transactions on Speech and Audio Processing (2000) 8(1) 37?502.
Gillick, L., Cox, S.: Some statistical issues in the comparison of speech recognitionalgorithms.
ICASSP (1989) 1 532?5353.
Heskes, T.: Selecting weighting factors in logarithmic opinion pools.
NIPS (1998)4.
Kazama, J. and Tsujii, J.: Evaluation and Extension of Maximum Entropy Modelswith Inequality Constraints.
EMNLP (2003)5.
Lafferty, J. and McCallum, A. and Pereira, F.: Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Sequence Data.
ICML (2001)6.
Malouf, R.: A comparison of algorithms for maximum entropy parameter estima-tion.
CoNLL (2002)7.
McCallum, A., Rohanimanesh, K. Sutton, C.: Dynamic Conditional Random Fieldsfor Jointly Labeling Multiple Sequences.
NIPS Workshop on Syntax, Semantics,Statistics (2003)Regularisation Techniques for Conditional Random Fields 9078.
Peng, F. and McCallum, A.: Accurate Information Extraction from Research Pa-pers using Conditional Random Fields.
HLT-NAACL (2004)9.
Sha, F. and Pereira, F.: Shallow Parsing with Conditional Random Fields.
HLT-NAACL (2003)10.
Smith, A., Cohn, T., Osborne, M.: Logarithmic Opinion Pools for ConditionalRandom Fields.
ACL (2005)11.
Tjong Kim Sang, E. F. and Buchholz, S.: Introduction to the CoNLL-2000 sharedtask: Chunking.
CoNLL (2000)12.
Tjong Kim Sang, E. F. and De Meulder, F.: Introduction to the CoNLL-2003Shared Task: Language-Independent Named Entity Recognition.
CoNLL (2003)13.
Williams, P.: Bayesian Regularisation and Pruning using a Laplace Prior.
NeuralComputation (1995) 7(1) 117?143
