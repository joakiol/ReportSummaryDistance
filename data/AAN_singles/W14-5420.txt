Proceedings of the 25th International Conference on Computational Linguistics, pages 118?120,Dublin, Ireland, August 23-29 2014.The Last 10 Metres: Using Visual Analysis and Verbal Communication inGuiding Visually Impaired Smartphone Users to EntrancesAnja BelzComputing, Engineering and MathsUniversity of BrightonLewes Road, Brighton BN2 4GJ, UKa.s.belz@brighton.ac.ukAnil BharathDepartment of BioengineeringImperial College LondonPrince Consort Road, London SW7 2BP, UKa.bharath@imperial.ac.uk1 IntroductionBlindness and partial sight are increasing, due to changing demographics and greater incidence of dis-eases such as diabetes, at vast financial and human cost (WHO, 2013).
Organisations for the visuallyimpaired stress the importance of independent living, of which safe and independent travel is an integralpart.
While existing smartphone facilities such as Apple?s Siri are encouraging, the supporting localisa-tion services are not sufficiently accurate or precise to enable navigation between e.g.
a bus stop or taxirank and the entrance to a public space such as a hospital, supermarket or train station.In this paper, we report plans and progress to date of research addressing ?the problem of the Last 10Metres.?
We are developing methods for safely guiding users not just to the general vicinity of a targetdestination (as done by GPS-based services), but right up to the main entrance of the target destination,by a combination of semantically and visually enriched maps, visual analysis, and language generation.2 OverviewThe core task is to help users navigate approach paths to building entrances.
Navigation guidance isdelivered via a smartphone app with voice and haptic output.
The app uses detailed, semantically taggedmaps in which public buildings (museums, schools, hospitals, etc.)
and the pavements, landmarks andother visual cues found in the approaches to their entrances (See Figure 2) are annotated.
The maps differfrom existing resources in that they have (i) more detailed information on pedestrian-relevant features,including obstructions and hazards, and (ii) computational descriptions of ?visual paths,?
i.e.
informationabout approach paths to entrances including image sequencess taken along the path (visual cues).The navigation app provides guidance from the point where a GPS-based system drops the user: the-oretically within 10m of a destination building, but in reality, anything up to a few hundred metres awayfrom the actual building entrance.
Our research is focused on developing a novel pedestrian guidance sys-tem that uses semantically and visually enriched maps, visual cues from user-generated live-feed video,and verbal and haptic communication to guide visually impaired pedestrians during the last few metresto the entrance of their destination, dropping them not just somewhere near, say, the British Museum, butmore precisely and much more challengingly, right in front of the museum?s main entrance.3 Usage ScenarioThe user employs their usual GPS-based app to get near a target destination, then our Last 10m app takesover: (1) User requests guidance to an entrance to their target building; (2) System retrieves relevantlocal map from server; (3) System converts guidance request to a specific target entrance T annotatedon map; (4) Given location of T on map, system determines location U of user on map; (5) Systemcomputes approach path P from U to T ; (6) System starts guiding user along P ; at the same timesystem carries out continuous monitoring of user behaviour and surroundings, interacting with user asnecessary: (a) System monitors that user stays on track; (b) System monitors path ahead to identify anyobstacles; (c) System issues warnings and update information as necessary, and deals with user requests,e.g.
information about an object detected by the user, location updates or output modality changes.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/118Figure 1: Illustration of the navigational context that we are addressing.4 Key Challenges4.1 Mapping ChallengesSemantically enriched local maps: Using OpenStreetMap,1which already includes many different kindsof relevant ?urban?
tags such as ?tree?, ?bus stop?, ?post box?, ?traffic signals?, etc., as a starting point,we are investigating ways of involving some of the 1.5 million volunteer mappers to create a new OSMlayer of highly fine-grained local information and snapshots of visual cues.Computing path from U to T: Adapting methods developed for similar purposes (Zeng et al., 2008),compute geometric paths from U to T ; if necessary recompute these paths on the fly on the basis ofobstacles that have been detected (see below).4.2 Vision ChallengesLocating user on map based on visual cues: The task is to locate the user precisely on the map (withina given radius determined on the basis of GPS output) by identifying landmarks and visual cues in user-generated live feed and matching these to the tags and images in the semantically enriched local maps.In a pilot study (Rivera-Rubio et al., 2013), conducted within indoor, but highly ambiguous corridors, wehave found that with relatively modest processes, paths can be distinguished with reasonable certaintyusing visual cues alone.
In more extensive tests, verified with surveying equipment (Rivera-Rubio et al.,2014), we found that user location on a path can be inferred from hand-held and wearable cameras.Continuous route monitoring: (a) monitoring of path ahead to identify obstacles and other dangerusing computer vision techniques and map information, (b) monitoring actual path against target path,updating target path and adapting instructions to user as necessary.
Monitoring is based on local maps, vi-sual information obtained on the fly (Davison et al., 2007; Alcantarilla et al., 2010; Pradeep and Medioni,2010) from smartphone camera live feeds, as well as information from inertial sensors, etc.4.3 Communication ChallengesWhile ?smart canes?
are promising technological improvements for visually impaired (VI) navigation,our research has shown that the VI community sharply divides into white cane users and guide dogowners, with the latter category in particular objecting to the use of a white cane.
For this reason weare focusing on smartphone apps delivering verbal and haptic output (which is suitable for both types ofusers).
We view the main communication challenges to be the following.Interaction Management: Managing (a) the interaction between user and system, including allowinguser interrupts and system alerts, and (b) any resulting changes to system behaviour.
This includesallowing the user to input navigation and configuration options for the route before or during the journey.Communicating navigation guidance: In the absence of interrupts from the continuous route moni-toring processes described above, the system communicates route guidance along the target path to theuser.
We will carry out detailed requirements analyses to determine what kind of instructions and what1http://www.openstreetmap.org119level of detail are most useful.
While the assumption is that most instructions are best communicated viabrief spoken outputs, a core question is what part of the guidance can be delivered by haptic output, e.g.different types/locations of vibration indicating different direction/speed of movement.Communicating warnings: The properties required of warnings differ from navigation guidance, inthat the nature of the danger and the required user reaction need to be conveyed as quickly and as effi-ciently as possible, with information ordered in terms of urgency.
It is likely that a larger proportion ofwarnings (than of navigation instructions) are best conveyed by haptic and simple audio output.Communicating uncertainty: If the system detects a hazard in the path ahead, identification of thetype of hazard and appropriate user action will come with a confidence measure < 1.
The degree ofuncertainty in what the system has identified must be conveyed to the user.
E.g.
if a postbox is tagged inthe map, and the continuous monitoring component has detected an object ahead that it has recognisedwith high confidence as a postbox, then it may be enough to simply steer the user around it.
However, ifthe system detects an obstruction at head height which is not annotated in the map and which it classifieswith similar confidence levels as several things, then this uncertainty has to be expressed in the verbaloutput, and the user may have to further investigate.Communicating varying levels of detail: Similarly, when describing a hazard or verbalising routeguidance, not all the detail about objects and routes available to the system needs to be conveyed to theuser in every situation.
For this purpose the system design incorporates a content selection component(CSC) which decides the appropriate level of detail given the context.A suitable way to generate verbal output in line with the above communication requirements is proba-bilistic natural language generation (NLG) technology (Belz, 2008) which offers the possibility of auto-matically training the verbal output generator to adapt to different user requirements and usage contexts.5 Current WorkWe are currently in the early stages of developing the various components of the Last 10m system.
Wehave carried out preliminary experiments in indoors path recognition identification (Rivera-Rubio et al.,2013; 2014), and conducted initial consultation sessions with VI people.
The next step is to designWizard-of-Oz experiments in order to obtain sizeable corpora of example instructions (produced byhumans playing the role of the system) appropriate in a variety of contexts which is then used both fortraining NLG components and for other aspects of system design.
At the same time we are improvingthe path computation algorithms (which provide important input to the CSC), using, for the time being,a small number of semantically and visually enriched local maps of entrances at our universities.ReferencesP.
F. Alcantarilla, L. M. Bergasa, and F. Dellaert.
2010.
Visual odometry priors for robust EKF-SLAM.
InProceedings of the 2010 IEEE International Conference on Robotics and Automation, pages 3501?3506.A.
Belz.
2008.
Automatic generation of weather forecast texts using comprehensive probabilistic generation-spacemodels.
Natural Language Engineering, 14(4):431?455.A.
Davison, I. D. Reid, N. D. Molton, and O. Stasse.
2007.
MonoSLAM: Real-time single camera SLAM.
IEEETransactions on Pattern Analysis and Machine Intelligence, 29(6):1052?1067.V.
Pradeep and G. Medioni.
2010.
Robot vision for the visually impaired.
In Proceedings of the 2010 ComputerVision and Pattern Recognition Workshop (CVPR), pages 15?22.J.
Rivera-Rubio, S. Idrees, I. Alexiou, L. Hadjilucas, and A.A. Bharath.
2013.
Mobile visual assistive apps:Benchmarks of vision algorithm performance.
In New Trends in Image Analysis and Processing (ICIAP 2013),volume 8158 of Lecture Notes in Computer Science, pages 30?40.J.
Rivera-Rubio, I. Alexiou, A.A. Bharath, R. Secoli, Dickens, and E. Lupu.
2014.
Associating locations fromwearable cameras.
In Proceedings of the 25thBritish Machine Vision Conference.
To Appear.WHO.
2013.
Visual impairment and blindness.
Fact Sheet No.
282, World Health Organization.Q.
Zeng, C. L. Teo, B. Rebsamen, and E. Burdet.
2008.
Collaborative path planning for a robotic wheelchair.Disability and Rehabilitation Assistive Technology, 3(6):315?324.120
