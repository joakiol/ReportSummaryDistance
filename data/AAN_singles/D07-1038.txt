Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
360?368, Prague, June 2007. c?2007 Association for Computational LinguisticsSyntactic Re-Alignment Models for Machine TranslationJonathan MayInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292jonmay@isi.eduKevin KnightInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292knight@isi.eduAbstractWe present a method for improving wordalignment for statistical syntax-based ma-chine translation that employs a syntacti-cally informed alignment model closer tothe translation model than commonly-usedword alignment models.
This leads to ex-traction of more useful linguistic patternsand improved BLEU scores on translationexperiments in Chinese and Arabic.1 Methods of statistical MTRoughly speaking, there are two paths commonlytaken in statistical machine translation (Figure 1).The idealistic path uses an unsupervised learningalgorithm such as EM (Demptser et al, 1977)to learn parameters for some proposed translationmodel from a bitext training corpus, and then di-rectly translates using the weighted model.
Someexamples of the idealistic approach are the directIBM word model (Berger et al, 1994; Germannet al, 2001), the phrase-based approach of Marcuand Wong (2002), and the syntax approaches of Wu(1996) and Yamada and Knight (2001).
Idealisticapproaches are conceptually simple and thus easy torelate to observed phenomena.
However, as moreparameters are added to the model the idealistic ap-proach has not scaled well, for it is increasingly dif-ficult to incorporate large amounts of training dataefficiently over an increasingly large search space.Additionally, the EM procedure has a tendency tooverfit its training data when the input units havevarying explanatory powers, such as variable-sizephrases or variable-height trees.The realistic path also learns a model of transla-tion, but uses that model only to obtain Viterbi word-for-word alignments for the training corpus.
Thebitext and corresponding alignments are then usedas input to a pattern extraction algorithm, whichyields a set of patterns or rules for a second trans-lation model (which often has a wider parameterspace than that used to obtain the word-for-wordalignments).
Weights for the second model are thenset, typically by counting and smoothing, and thisweighted model is used for translation.
Realistic ap-proaches scale to large data sets and have yieldedbetter BLEU performance than their idealistic coun-terparts, but there is a disconnect between the firstmodel (hereafter, the alignment model) and the sec-ond (the translation model).
Examples of realisticsystems are the phrase-based ATS system of Ochand Ney (2004), the phrasal-syntax hybrid systemHiero (Chiang, 2005), and the GHKM syntax sys-tem (Galley et al, 2004; Galley et al, 2006).
Foran alignment model, most of these use the AachenHMM approach (Vogel et al, 1996), the implemen-tation of IBM Model 4 in GIZA++ (Och and Ney,2000) or, more recently, the semi-supervised EMDalgorithm (Fraser and Marcu, 2006).The two-model approach of the realistic path hasundeniable empirical advantages and scales to largedata sets, but new research tends to focus on devel-opment of higher order translation models that areinformed only by low-order alignments.
We wouldlike to add the analytic power gained from mod-ern translation models to the underlying alignmentmodel without sacrificing the efficiency and empiri-cal gains of the two-model approach.
By adding the360u n s u p e r v i s e dle a r n i n gta r g ets e nte n c e ss o u r c es e nte n c e su n w e i g hte dm o d elw e i g hte dm o d elp at te r n s(u n w e i g hte dm o d el )c o u nti n ga n ds m o oth i n gw e i g hte dm o d eld e c o d e rs o u r c es e nte n c e sta r g ets e nte n c e sp at te r ne xtr a cti o nta r g ets e nte n c e ss o u r c es e nte n c e sVite r b iali g n m e ntsI d e a l i s t i cS ys t e mR e a l i s t i cS ys t e md e c o d e rs o u r c es e nte n c e sta r g ets e nte n c e sFigure 1: General approach to idealistic and realistic statistical MT systemssyntactic information used in the translation modelto our alignment model we may improve alignmentquality such that rule quality and, in turn, systemquality are improved.
In the remainder of this workwe show how a touch of idealism can improve anexisting realistic syntax-based translation system.2 Multi-level syntactic rules for syntax MTGalley et al (2004) and Galley et al (2006) de-scribe a syntactic translation model that relates En-glish trees to foreign strings.
The model describesjoint production of a (tree, string) pair via a non-deterministic selection of weighted rules.
Each rulehas an English tree fragment with variables and acorresponding foreign string fragment with the samevariables.
A series of rules forms an explanation (orderivation) of the complete pair.As an example, consider the parsed English andcorresponding Chinese at the top of Figure 2.
Thethree columns underneath the example are differentrule sequences that can explain this pair; there aremany other possibilities.
Note how rules specify ro-tation (e.g.
R10, R5), direct translation (R12, R8),insertion and deletion (R11, R1), and tree traversal(R7, R15).
Note too that the rules explain variable-size fragments (e.g.
R6 vs. R14) and thus the possi-ble derivation trees of rules that explain a sentencepair have varying sizes.
The smallest such deriva-tion tree has a single large rule (which does not ap-pear in Figure 2; we leave the description of sucha rule as an exercise for the reader).
A string-to-tree decoder constructs a derivation forest of deriva-tion trees where the right sides of the rules in a tree,taken together, explain a candidate source sentence.It then outputs the English tree corresponding to thehighest-scoring derivation in the forest.3 Introducing syntax into the alignmentmodelWe now lay the ground for a syntactically motivatedalignment model.
We begin by reviewing an align-ment model commonly seen in realistic MT systemsand compare it to a syntactically-aware alignmentmodel.3.1 The traditional IBM alignment modelIBM Model 4 (Brown et al, 1993) learns a set of 4probability tables to compute p(f |e) given a foreignsentence f and its target translation e via the follow-ing (greatly simplified) generative story:361NP-CNPBNPBNNPtaiwanPOS?sNNsurplusPPINinNP-CNPBNNtradePPINbetweenNP-CNPBDTtheCDtwoNNSshores?
l ?
?
?
?
4 ?
~ ?TAIWAN IN TWO-SHORES TRADE MIDDLE SURPLUSR1: NP-CNPBx0:NPB x1:NNx2:PP?
x0 x2 ?
x1 R10: NP-CNPBx0:NPB x1:NNx2:PP?
x0 x2 x1 R10: NP-CNPBx0:NPB x1:NNx2:PP?
x0 x2 x1R2: NPBNNPtaiwanPOS?s?
?
l R11: NPBx0:NNP POS?s?
x0 R17: NPBNNPtaiwanx0:POS?
x0R12: NNPtaiwan?
?
l R18: POS?s?
?
lR3: PPx0:IN x1:NP-C?
x0 x1 R13: PPINinx0:NP-C?
?
x0 ?
R19: PPINinx0:NP-C?
x0R4: INin?
?R5: NP-Cx0:NPB x1:PP?
x1 x0 R5: NP-Cx0:NPB x1:PP?
x1 x0 R20: NP-Cx0:NPB PPx1:IN x2:NP-C?
x2 x0 x1R6: PPINbetweenNP-CNPBDTtheCDtwoNNSshores?
?
?
R14: PPINbetweenx0:NP-C?
x0 R21: INbetween?
?R15: NP-Cx0:NPB?
x0 R15: NP-Cx0:NPB?
x0R16: NPBDTtheCDtwoNNSshores?
?
?
R22: NPBx0:DT CDtwox1:NNS?
x0 x1R23: NNSshores?
?
?
R24: DTthe?
?R7: NPBx0:NN?
x0 R7: NPBx0:NN?
x0 R7: NPBx0:NN?
x0R8: NNtrade?
?
4 R9: NNsurplus?
~ ?
R8: NNtrade?
?
4 R9: NNsurplus?
~ ?
R8: NNtrade?
?
4 R9: NNsurplus?
~ ?Figure 2: A (English tree, Chinese string) pair and three different sets of multilevel tree-to-string rules thatcan explain it; the first set is obtained from bootstrap alignments, the second from this paper?s re-alignmentprocedure, and the third is a viable, if poor quality, alternative that is not learned.362S-CNP-CNPBNNPguangxiPOS?sVPVBGopeningPRTRPupPPTOtoNP-CNPBDTtheJJoutsideNNworld ?
?
i8GUANGXI OUTSIDE-WORLD OPENING-UPR24: S-CNP-CNPBx0:NNP POS?sVPVBGopeningPRTRPupPPTOtoNP-CNPBDTtheJJoutsideNNworld?
x0 ?
i8 R25: NNPguangxi?
 ?R26: S-Cx0:NP-C x1:VP?
x0 x1 R15: NP-Cx0:NPB?
x0 R11: NPBx0:NNP POS?s?
x0 R27: VPVBGopeningPRTRPupx0:PP?
x08R28: PPTOtox0:NP-C?
x0 R15: NP-Cx0:NPB?
x0 R29: NPBDTtheJJoutsideNNworld?
?
i R25: NNPguangxi?
 ?Figure 3: The impact of a bad alignment on rule extraction.
Including the alignment link indicated by thedotted line in the example leads to the rule set in the second row.
The re-alignment procedure described inSection 3.2 learns to prefer the rule set at bottom, which omits the bad link.1.
A fertility y for each word ei in e is chosenwith probability pfert(y|ei).2.
A null word is inserted next to eachfertility-expanded word with probabilitypnull.3.
Each token ei in the fertility-expandedword and null string is translated intosome foreign word fi in f with probabilityptrans(fi|ei).4.
The position of each foreign wordfi that was translated from ei ischanged by ?
(which may be posi-tive, negative, or zero) with probabilitypdistortion(?|A(ei),B(fi)), where A andB are functions over the source and targetvocabularies, respectively.Brown et al (1993) describes an EM algorithmfor estimating values for the four tables in the gener-ative story.
However, searching the space of all pos-sible alignments is intractable for EM, so in practicethe procedure is bootstrapped by models with nar-rower search space such as IBM Model 1 (Brown etal., 1993) or Aachen HMM (Vogel et al, 1996).3633.2 A syntax re-alignment modelNow let us contrast this commonly used model forobtaining alignments with a syntactically motivatedalternative.
We recall the rules described in Section2.
Our model learns a single probability table tocompute p(etree, f) given a foreign sentence f anda parsed target translation etree.
In the followinggenerative story we assume a starting variable withsyntactic type v.1.
Choose a rule r to replace v, with proba-bility prule(r|v).2.
For each variable with syntactic type vi inthe partially completed (tree, string) pair,continue to choose rules ri with probabil-ity prule(ri|vi) to replace these variablesuntil there are no variables remaining.In Section 5.1 we discuss an EM learning proce-dure for estimating these rule probabilities.As in the IBM approach, we must miti-gate intractability by limiting the parameter spacesearched, which is potentially much wider than inthe word-to-word case.
We would like to supply toEM all possible rules that explain the training data,but this implies a rule relating each possible treefragment to each possible string fragment, which isinfeasible.
We follow the approach of bootstrappingfrom a model with a narrower parameter space as isdone in, e.g.
Och and Ney (2000) and Fraser andMarcu (2006).To reduce the model space we employ the rule ac-quisition technique of Galley et al (2004), whichobtains rules given a (tree, string) pair as well asan initial alignment between them.
We are agnos-tic about the source of this bootstrap alignment andin Section 5 present results based on several differ-ent bootstrap alignment qualities.
We require an ini-tial set of alignments, which we obtain from a word-for-word alignment procedure such as GIZA++ orEMD.
Thus, we are not aligning input data, butrather re-aligning it with a syntax model.4 The appeal of a syntax alignment modelConsider the example of Figure 2 again.
The left-most derivation is obtained from the bootstrap align-ment set.
This derivation is reasonable but there aresome poorly motivated rules, from a linguistic stand-point.
The Chinese word ?
?
roughly means ?theSENTENCE PAIRSDESCRIPTION CHINESE ARABICTUNE NIST 2002 short 925 696TEST NIST 2003 919 663Table 1: Tuning and testing data sets for the MTsystem described in Section 5.2.two shores?
in this context, but the rule R6 learnedfrom the alignment incorrectly includes ?between?.However, other sentences in the training corpus havethe correct alignment, which yields rule R16.
Mean-while, rules R13 and R14, learned from yet othersentences in the training corpus, handle the ?
... ?structure (which roughly translates to ?in between?
),thus allowing the middle derivation.EM distributes rule probabilities in such a way asto maximize the probability of the training corpus.It thus prefers to use one rule many times insteadof several different rules for the same situation overseveral sentences, if possible.
R6 is a possible rulein 46 of the 329,031 sentence pairs in the trainingcorpus, while R16 is a possible rule in 100 sentencepairs.
Well-formed rules are more usable than ill-formed rules and the partial alignments behind theserules, generally also well-formed, become favoredas well.
The top row of Figure 3 contains an exam-ple of an alignment learned by the bootstrap align-ment model that includes an incorrect link.
RuleR24, which is extracted from this alignment, is apoor rule.
A set of commonly seen rules learnedfrom other training sentences provide a more likelyexplanation of the data, and the consequent align-ment omits the spurious link.5 ExperimentsIn this section, we describe the implementation ofour semi-idealistic model and our means of evaluat-ing the resulting re-alignments in an MT task.5.1 The re-alignment setupWe begin with a training corpus of Chinese-Englishand Arabic-English bitexts, the English side parsedby a reimplementation of the standard Collins model(Bikel, 2004).
In order to acquire a syntactic rule set,we also need a bootstrap alignment of each trainingsentence.
We use an implementation of the GHKM364BOOTSTRAP GIZA CORPUS RE-ALIGNMENT EXPERIMENTENGLISH WORDS CHINESE WORDS TYPE RULES TUNE TEST9,864,294 7,520,779baseline 19,138,252 39.08 37.77initial 18,698,549 39.49 38.39adjusted 26,053,341 39.76 38.69Table 2: A comparison of Chinese BLEU performance between the GIZA baseline (no re-alignment), re-alignment as proposed in Section 3.2, and re-alignment as modified in Section 5.4algorithm (Galley et al, 2004) to obtain a rule set foreach bootstrap alignment.Now we need an EM algorithm for learn-ing the parameters of the rule set that maximize?corpusp(tree, string).
Such an algorithm is pre-sented by Graehl and Knight (2004).
The algorithmconsists of two components: DERIV, which is a pro-cedure for constructing a packed forest of derivationtrees of rules that explain a (tree, string) bitext cor-pus given that corpus and a rule set, and TRAIN,which is an iterative parameter-setting procedure.We initially attempted to use the top-down DE-RIV algorithm of Graehl and Knight (2004), but asthe constraints of the derivation forests are largelylexical, too much time was spent on exploring dead-ends.
Instead we build derivation forests using thefollowing sequence of operations:1.
Binarize rules using the synchronous bina-rization algorithm for tree-to-string trans-ducers described in Zhang et al (2006).2.
Construct a parse chart with a CKY parsersimultaneously constrained on the foreignstring and English tree, similar to thebilingual parsing of Wu (1997) 1.3.
Recover all reachable edges by traversingthe chart, starting from the topmost entry.Since the chart is constructed bottom-up, leaf lex-ical constraints are encountered immediately, result-ing in a narrower search space and faster runningtime than the top-down DERIV algorithm for thisapplication.
Derivation forest construction takesaround 400 hours of cumulative machine time (4-processor machines) for Chinese.
The actual run-ning of EM iterations (which directly implementsthe TRAIN algorithm of Graehl and Knight (2004))1In the cases where a rule is not synchronous-binarizablestandard left-right binarization is performed and proper permu-tation of the disjoint English tree spans must be verified whenbuilding the part of the chart that uses this rule.takes about 10 minutes, after which the Viterbiderivation trees are directly recoverable.
The Viterbiderivation tree tells us which English words producewhich Chinese words, so we can extract a word-to-word alignment from it.
We summarize the ap-proach described in this paper as:1.
Obtain bootstrap alignments for a trainingcorpus using GIZA++.2.
Extract rules from the corpus and align-ments using GHKM, noting the partialalignment that is used to extract each rule.3.
Construct derivation forests for each (tree,string) pair, ignoring the alignments, andrun EM to obtain Viterbi derivation trees,then use the annotated partial alignmentsto obtain Viterbi alignments.4.
Use the new alignments as input to the MTsystem described below.5.2 The MT system setupA truly idealistic MT system would directly applythe rule weight parameters learned via EM to a ma-chine translation task.
As mentioned in Section 1,we maintain the two-model, or realistic approach.Below we briefly describe the translation model, fo-cusing on comparison with the previously describedalignment model.
Galley et al (2006) provides amore complete description of the translation modeland DeNeefe et al (2007) provides a more completedescription of the end-to-end translation pipeline.Although in principle the re-alignment model andtranslation model learn parameter weights over thesame rule space, in practice we limit the rules usedfor re-alignment to the set of smallest rules that ex-plain the training corpus and are consistent with thebootstrap alignments.
This is a compromise madeto reduce the search space for EM.
The translationmodel learns multiple derivations of rules consistentwith the re-alignments for each sentence, and learns365(a) Chinese re-alignment corpus has 9,864,294 English and 7,520,779 Chinese wordsBOOTSTRAP GIZA CORPUS RE-ALIGNMENT EXPERIMENTENGLISH WORDS CHINESE WORDS TYPE RULES TUNE TEST9,864,294 7,520,779 baseline 19,138,252 39.08 37.77re-alignment 26,053,341 39.76 38.69221,835,870 203,181,379 baseline 23,386,535 39.51 38.93re-alignment 33,374,646 40.17 39.96(b) Arabic re-alignment corpus has 4,067,454 English and 3,147,420 Arabic wordsBOOTSTRAP GIZA CORPUS RE-ALIGNMENT EXPERIMENTENGLISH WORDS ARABIC WORDS TYPE RULES TUNE TEST4,067,454 3,147,420 baseline 2,333,839 47.92 47.33re-alignment 2,474,737 47.87 47.89168,255,347 147,165,003 baseline 3,245,499 49.72 49.60re-alignment 3,600,915 49.73 49.99Table 3: Machine Translation experimental results evaluated with case-insensitive BLEU4.weights for these by counting and smoothing.
Adozen other features are also added to the rules.
Weobtain weights for the combinations of the featuresby performing minimum error rate training (Och,2003) on held-out data.
We then use a CKY decoderto translate unseen test data using the rules and tunedweights.
Table 1 summarizes the data used in tuningand testing.5.3 Initial resultsAn initial re-alignment experiment shows a reason-able rise in BLEU scores from the baseline (Table2), but closer inspection of the rules favored by EMimplies we can do even better.
EM has a tendencyto favor few large rules over many small rules, evenwhen the small rules are more useful.
Referring tothe rules in Figure 2, note that possible derivationsfor (taiwan ?s, ?
l)2 are R2, R11-R12, and R17-R18.
Clearly the third derivation is not desirable,and we do not discuss it further.
Between the firsttwo derivations, R11-R12 is preferred over R2, asthe conditioning for possessive insertion is not re-lated to the specific Chinese word being inserted.Of the 1,902 sentences in the training corpus wherethis pair is seen, the bootstrap alignments yield theR2 derivation 1,649 times and the R11-R12 deriva-tion 0 times.
Re-alignment does not change the re-sult much; the new alignments yield the R2 deriva-tion 1,613 times and again never choose R11-R12.The rules in the second derivation themselves are2The Chinese gloss is simply ?taiwan?.not rarely seen ?
R11 is in 13,311 forests other thanthose where R2 is seen, and R12 is in 2,500 addi-tional forests.
EM gives R11 a probability of e?7.72?
better than 98.7% of rules, and R12 a probabilityof e?2.96.
But R2 receives a probability of e?6.32and is preferred over the R11-R12 derivation, whichhas a combined probability of e?10.68.5.4 Making EM fairThe preference for shorter derivations containinglarge rules over longer derivations containing smallrules is due to a general tendency for EM to pre-fer derivations with few atoms.
Marcu and Wong(2002) note this preference but consider the phe-nomenon a feature, rather than a bug.
Zollmannand Sima?an (2005) combat the overfitting aspectfor parsing by using a held-out corpus and a straightmaximum likelihood estimate, rather than EM.
Wetake a modeling approach to the phenomenon.As the probability of a derivation is determined bythe product of its atom probabilities, longer deriva-tions with more probabilities to multiply have an in-herent disadvantage against shorter derivations, allelse being equal.
EM is an iterative procedure andthus such a bias can lead the procedure to convergewith artificially raised probabilities for short deriva-tions and the large rules that comprise them.
Therelatively rare applicability of large rules (and thuslower observed partial counts) does not overcomethe inherent advantage of large coverage.
To com-bat this, we introduce size terms into our generativestory, ensuring that all competing derivations for the366LANGUAGE PAIR TYPE RULES TUNE TESTCHINESE-ENGLISH baseline 55,781,061 41.51 40.55EMD re-align 69,318,930 41.23 40.55ARABIC-ENGLISH baseline 8,487,656 51.90 51.69EMD re-align 11,498,150 51.88 52.11Table 4: Re-alignment performance with semi-supervised EMD bootstrap alignmentssame sentence contain the same number of atoms:1.
Choose a rule size s with cost csize(s)s?1.2.
Choose a rule r (of size s) to replace thestart symbol with probability prule(r|s, v).3.
For each variable in the partially com-pleted (tree, string) pair, continue tochoose sizes followed by rules, recur-sively to replace these variables until thereare no variables remaining.This generative story changes the derivation com-parison from R2 vs R11-R12 to S2-R2 vs R11-R12,where S2 is the atom that represents the choice ofsize 2 (the size of a rule in this context is the numberof non-leaf and non-root nodes in its tree fragment).Note that the variable number of inclusions impliedby the exponent in the generative story above en-sures that all derivations have the same size.
For ex-ample, a derivation with one size-3 rule, a derivationwith one size-2 and one size-1 rule, and a deriva-tion with three size-1 rules would each have threeatoms.
With this revised model that allows for faircomparison of derivations, the R11-R12 derivationis chosen 1636 times, and S2-R2 is not chosen.
R2does, however, appear in the translation model, asthe expanded rule extraction described in Section 5.2creates R2 by joining R11 and R12.The probability of size atoms, like that of ruleatoms, is decided by EM.
The revised generativestory tends to encourage smaller sizes by virtue ofthe exponent.
This does not, however, simply ensurethe largest number of rules per derivation is used inall cases.
Ill-fitting and poorly-motivated rules suchas R22, R23, and R24 in Figure 2 are not preferredover R16, even though they are smaller.
However,R14 and R16 are preferred over R6, as the formerare useful rules.
Although the modified model doesnot sum to 1, it leads to an improvement in BLEUscore, as can be seen in the last row of Table 2.5.5 ResultsWe performed primary experiments on two differentbootstrap setups in two languages: the initial exper-iment uses the same data set for the GIZA++ initialalignment as is used in the re-alignment, while anexperiment on better quality bootstrap alignmentsuses a much larger data set.
For each bootstrap-ping in each language we compared the baselineof using these alignments directly in an MT sys-tem with the experiment of using the alignments ob-tained from the re-alignment procedure described inSection 5.4.
For each experiment we report: thenumber of rules extracted by the expanded GHKMalgorithm of Galley et al (2006) for the translationmodel, converged BLEU scores on the tuning set,and finally BLEU performance on the held-out testset.
Data set specifics for the GIZA++ bootstrappingand BLEU results are summarized in Table 3.5.6 DiscussionThe results presented demonstrate we are able toimprove on unsupervised GIZA++ alignments byabout 1 BLEU point for Chinese and around 0.4BLEU point for Arabic using an additional unsu-pervised algorithm that requires no human aligneddata.
If human-aligned data is available, the EMDalgorithm provides higher baseline alignments thanGIZA++ that have led to better MT performance(Fraser and Marcu, 2006).
As a further experi-ment we repeated the experimental conditions fromTable 3, this time bootstrapped with the semi-supervised EMD method, which uses the largerbootstrap GIZA corpora described in Table 3 andan additional 64,469/48,650 words of hand-alignedEnglish-Chinese and 43,782/31,457 words of hand-aligned English-Arabic.
The results of this advancedexperiment are in Table 4.
We show a 0.42 gain inBLEU for Arabic, but no movement for Chinese.
Webelieve increasing the size of the re-alignment cor-pora will increase BLEU gains in this experimental367condition, but leave those results for future work.We can see from the results presented that the im-pact of the syntax-aware re-alignment procedure ofSection 3.2, coupled with the addition of size param-eters to the generative story from Section 5.4 servesto remove links from the bootstrap alignments thatcause less useful rules to be extracted, and thus in-crease the overall quality of the rules, and hence thesystem performance.
We thus see the benefit to in-cluding syntax in an alignment model, bringing thetwo models of the realistic machine translation pathsomewhat closer together.AcknowledgmentsWe thank David Chiang, Steve DeNeefe, AlexFraser, Victoria Fossum, Jonathan Graehl, LiangHuang, Daniel Marcu, Michael Pust, Oana Pos-tolache, Michael Pust, Jason Riesa, Jens Vo?ckler,and Wei Wang for help and discussion.
This re-search was supported by NSF (grant IIS-0428020)and DARPA (contract HR0011-06-C-0022).ReferencesAdam Berger, Peter Brown, Stephen Della Pietra, Vin-cent Della Pietra, John Gillett, John Lafferty, RobertMercer, Harry Printz, and Lubos?
Ures?.
1994.
Thecandide system for machine translation.
In Proc.
HLT,pages 157?162, Plainsboro, New Jersey, March.Daniel Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263?311.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
ACL, pages263?270, Ann Arbor, Michigan, June.Arthur P. Demptser, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(1):1?38.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In Proc.
EMNLP/CONLL, Prague,June.Alexander Fraser and Daniel Marcu.
2006.
Semi-supervised training for statistical word alignment.
InProc.
COLING-ACL, pages 769?776, Sydney, July.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.HLT-NAACL, pages 273?280, Boston, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steven DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic models.
In Proc.
COLING-ACL, pages 961?968, Sydney, July.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Proc.ACL, pages 228?235, Toulouse, France, July.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proc.
HLT-NAACL, pages 105?112,Boston, May.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proc.
EMNLP, pages 133?139, Philadelphia,July.Franz Och and Hermann Ney.
2000.
Improved statisti-cal alignment models.
In Proc.
ACL, pages 440?447,Hong Kong, October.Franz Och and Hermann Ney.
2004.
The alignment tem-plate approach to statistical machine translation.
Com-putational Linguistics, 30(4):417?449.Franz Och.
2003.
Minimum error rate training for sta-tistical machine translation.
In Proc.
ACL, pages 160?167, Sapporo, Japan, July.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proc.
COLING, pages 836?841, Copen-hagen, August.Dekai Wu.
1996.
A polynomial-time algorithm for sta-tistical machine translation.
In Proc.
ACL, pages 152?158, Santa Cruz, California, June.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proc.
ACL, pages 523?530, Toulouse, France, July.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proc.
HLT-NAACL, pages 256?263,New York City, June.Andreas Zollmann and Khalil Sima?an.
2005.
A consis-tent and efficient estimator for data-oriented parsing.Journal of Automata, Languages and Combinatorics,10(2/3):367?388.368
