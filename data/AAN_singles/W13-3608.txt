Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62?67,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsLFG-based Features for Noun Number and Article Grammatical ErrorsGa?bor Berend1, Veronika Vincze2, Sina Zarriess3, Richa?rd Farkas11University of SzegedDepartment of Informatics{berendg,rfarkas}@inf.u-szeged.hu2Research Group on Artificial IntelligenceHungarian Academy of Sciencesvinczev@inf.u-szeged.hu3University of StuttgartInstitute for Natural Language Processingzarriesa@ims.uni-stuttgart.deAbstractWe introduce here a participating systemof the CoNLL-2013 Shared Task ?Gram-matical Error Correction?.
We focused onthe noun number and article error cate-gories and constructed a supervised learn-ing system for solving these tasks.
We car-ried out feature engineering and we foundthat (among others) the f-structure of anLFG parser can provide very informativefeatures for the machine learning system.1 IntroductionThe CoNLL-2013 Shared Task aimed at identify-ing and correcting grammatical errors in the NU-CLE learner corpus of English (Dahlmeier et al2013).
This task has become popular in the naturallanguage processing (NLP) community in the lastfew years (Dale and Kilgariff, 2010), which mani-fested in the organization of shared tasks.
In 2011,the task Helping Our Own (HOO 2011) was held(Dale and Kilgariff, 2011), which targeted the pro-motion of NLP tools and techniques in improvingthe textual quality of papers written by non-nativespeakers of English within the field of NLP.
Thenext year, HOO 2012 (Dale et al 2012) specifi-cally focused on the correction of determiner andpreposition errors in a collection of essays writ-ten by candidates sitting for the Cambridge ESOLFirst Certificate in English (FCE) examination.
In2013, the CoNLL-2013 Shared Task has continuedthis direction of research.The CoNLL-2013 Shared Task is based on theNUCLE corpus, which consists of about 1,400student essays from undergraduate university stu-dents at The National University of Singapore(Dahlmeier et al 2013).
The corpus contains overone million words and it is completely annotatedwith grammatical errors and corrections.
Amongthe 28 error categories, this year?s shared task fo-cused on the automatic detection and correction offive specific error categories.In this paper, we introduce our contribution ofthe CoNLL-2013 Shared Task.
We propose a su-pervised learning-based approach.
The main con-tribution of this work is the exploration of severalfeature templates for grammatical error categories.We focused on the two ?nominal?
error categories:1.1 Article and Determiner ErrorsThis error type involved all kinds of errorswhich were related to determiners and articles(ArtOrDet).
It required multiple correctionstrategies.
On the one hand, superfluous articlesor determiners should be deleted from the text.On the other hand, missing articles or determin-ers should be inserted and at the same time it wassometimes also necessary to replace a certain typeof article or determiner to an other type.
Here isan example:For nations like Iran and North Ko-rea, the development of nuclear poweris mainly determined by the politicalforces.
?
For nations like Iran andNorth Korea, the development of nu-clear power is mainly determined by po-litical forces.621.2 Wrong Number of the NounThe wrong number of nouns (Nn) meant that eithera singular noun should occur in the plural form ora plural noun should occur in the singular form.A special case of such errors was that sometimesuncountable nouns were used in the plural, whichis ungrammatical.
The correction involved herethe change of the number.
Below we provide anexample:All these measures are implemented tomeet the safety expectation of the op-eration of nuclear power plant.
?
Allthese measures are implemented to meetthe safety expectation of the operationof nuclear power plants.2 System DescriptionOur approach for grammatical error detection wasto construct supervised classifiers for each candi-date of grammatical error locations.
In general,our candidate extraction and features are basedon the output of the preprocessing step providedby the organizers which contained both the POS-tag sequences and the constituency phrase struc-ture outputs for every sentence in the training andtest sets determined by Stanford libraries.
We em-ployed the Maximum Entropy based supervisedclassification model using the MALLET API (Mc-Callum, 2002), which was responsible for suggest-ing the various corrections.The most closely related approach to ours isprobably the work of De Felice and Pulman(2008).
We also employ a Maximum Entropy clas-sifier and a syntax-motivated feature set.
However,we investigate deeper linguistic features (based onthe f-structure of an LFG parser).In the following subsections we introduce ourcorrection candidate recognition procedure andthe features used for training and prediction ofthe machine learning classifier.
We employed thesame feature set for each classification task.2.1 Candidate LocationsWe used the following heuristics for the recogni-tion of the possible locations of grammatical er-rors.
We also describe the task of various classi-fiers at these candidate locations.Article and Determiner Error category Wehandled the beginning of each noun phrase(NP) as a possible location for errors relatedto articles or determiners.
The NP waschecked if it started with any definite orindefinite article.
If it did, we asked ourthree-class classifier whether to leave itunmodified, change its type (i.e.
an indefiniteto a definite one or vice versa) or simplydelete it.
However, when there was no articleat all at the beginning of a noun phrase,the decision made by a different three-classclassifier was whether to leave that positionempty or to put a definite or indefinite articlein that place.Wrong Number of the Noun Error categoryEvery token tagged as a noun (either in pluralor singular) was taken into consideration atthis subtask.
We constructed two ?
i.e.
onefor the word forms originally written in plu-ral and singular ?
binary classifiers whetherthe number (i.e.
plural or singular) of thenoun should be changed or left unchanged.2.2 LFG parse-based featuresWe looked for the minimal governing NP for eachcandidate location.
We reparsed this NP with-out context by a Lexical Functional Grammar(LFG) parser and we acquired features from itsf-structure.
In the following paragraph, LFG isintroduced briefly while Table 1 summarizes thefeatures extracted from the LFG parse.Lexical Functional Grammar (LFG) (Bresnan,2000) is a constraint-based theory of grammar.
Itposits two levels of representation, c(onstituent)-structure and f(unctional)-structure.C-structure is represented by context freephrase-structure trees, and captures surface gram-matical configurations.
F-structures approximatebasic predicate-argument and adjunct structures.The experiments reported in this paper use theEnglish LFG grammar constructed as part of theParGram project (Butt et al 2002).
The gram-mar is implemented in XLE, a grammar develop-ment environment, which includes a very efficientLFG parser.
Within the spectrum of approaches tonatural language parsing, XLE can be considereda hybrid system combining a hand-crafted gram-mar with a number of automatic ambiguity man-agement techniques:(i) c-structure pruning where, based on informa-tion from statistically obtained parses, some treesare ruled out before f-structure unification (Cahillet al 2007)63COORD NP/PP is coordinated +/-COORD-LEVEL syntactic category of coordi-nated phraseDEG-DIM dimension for comparitive NPs,(?equative?/?pos?/?neg?
)DEGREE semantic type of adjec-tival modifier (?posi-tive?/?comparative?/?superlative?
)DET-TYPE type of determiner(?def?/?indef?/?demon?
)LOCATION-TYPE marks locative NPsNAME-TYPE ?first name?/?last name?NSYN syntactic noun type (?com-mon?/?proper?/?pronoun?
)PRON-TYPE syntactic pronoun type (e.g.
?pers?, ?refl?, ?poss?
)PROPER-TYPE type of proper noun (e.g.
?com-pany?, ?location?, ?name?
)Table 1: Short characterization of the LFG fea-tures incorporated in our models designed to cor-rect noun phrase-related grammatical errors(ii) an Optimality Theory-style constraint mecha-nism for filtering and ranking competing analyses(Frank et al 2001),and (iii) a stochastic disambiguation componentwhich is based on a log-linear probability model(Riezler et al 2002) and works on the packed rep-resentations.Although we use a deep, hand-crafted LFGgrammar for processing the data, our approach issubstantially different from other grammar-basedapproaches to CALL.
For instance, Fortmann andForst (2004) supplement a German LFG devel-oped for newspaper text with so-called malrulesthat accept marked or ungrammatical input ofsome predefined types.
In our work, we apply anLFG parser developed for standard texts to get arich feature representation that can be exploitedby a classifier.
While malrules would certainly beuseful for finding other error types, such as agree-ment errors, the NP- and PP-errors are often ana-lyzed as grammatical by the parser (e.g.
?the po-litical forces?
vs. ?political forces?).
Thus, thegrammaticality of a phrase predicted by the gram-mar is not necessarily a good indicator for correc-tion in our case.2.3 Phrase-based contextual featuresBesides the LFG features describing the internalstructure of the minimal NP that dominates a can-didate location, we defined features describing itscontext as well.
Phrase-based contextual featuressearched for those minimal prepositional and nounphrases that governed a token at a certain can-Final results Corrected outputP 0.0552 0.1260R 0.0316 0.0292F 0.0402 0.0474Table 2: Overall results aggregated over the fiveerror typesdidate location of the sentence where a decisionwas about to be taken.
Then features encoding thetypes of the phrases that preceded and succeededa given minimal governing noun or prepositionalphrase were extracted.The length of those minimal governing nounand prepositional phrases as well as those of thepreceding and succeeding ones were taken intoaccount as numeric features.
The motivation be-hind using the span size of the minimal governingand neighboring noun and prepositional phrasesis that it was assumed that grammatical errors inthe sentence result in unusual constituency subtreepatterns that could manifest in minimal governingphrases having too long spans for instance.
Therelative position of the candidate position insidethe smallest dominating noun and prepositionalphrases was also incorporated as a feature sincethis information might carry some information fornoun errors.2.4 Token-based contextual featuresA third group of features described the context ofthe candidate location at the token level.
Here, twosets of binary features were introduced to mark thefact if the token was present in the four token-sizedwindow to its left or right.
Dedicated nominal fea-tures were introduced to store the word form ofthe token immediately preceding a decision pointwithin a sentence and the POS-tags at the preced-ing and actual token positions.Two lists were manually created which con-sisted of entirely uncountable nouns (e.g.
blood)and nouns that are uncountable most of the times(e.g.
aid or dessert).
When generating fea-tures for those classifiers that can modify the plu-rality of a noun, we marked the fact in a binaryfeature if they were present in any of these lists.Another binary feature indicated if the actual nounto be classified could be found at an earlier pointof the document.64Only erroneous All sentencesP 0.1260 0.1061R 0.0292 0.0085F 0.0474 0.0158Table 3: Overall results aggregated over the fiveerror typesOnly erroneous All sentencesP 0.2500 0.0167R 0.0006 0.0006F 0.0012 0.0012Table 4: Overall results aggregated over the fiveerror types, not using the LFG parser based fea-tures3 ResultsIt is important to note that our officially submit-ted architecture included an unintended step whichmeant that whenever our system predicted that ata certain point an indefinite article should be in-serted or (re-)written, the indefinite article an wasput at that place erroneously when the succeedingtoken started with a consonant (e.g.
outputting anserious instead of a serious).Since the output that contained this kind of errorserved as the basis of the official ranking we in-clude in Table 2 the results achieved with the out-put affected by this unintended behavior, however,in the following we present our results in such amanner where this kind of error is eliminated fromthe output of our system.Upon training our systems we followed twostrategies.
For the first approach we used all thesentences regardless if they had any error in themat all.
However, in an alternative approach we uti-lized only those sentences from the training corpusthat had at least one error in them from the five er-ror categories to be dealt with in the shared task.The different results achieved on the test set ac-cording to the two approaches are detailed in Ta-ble 3.
Turning off the LFG features ended up inthe results detailed in Table 4.Since our framework in its present state onlyaims at the correction of errors explicitly re-lated to noun phrases, no error categories besidesArtOrDet and Nn (for more details see Sections1.1 and 1.2, respectively) could be possibly cor-rected by our system.
Note that these two errorcategories covered 66.1% of the corrections on thetest set, so with our approach this was the highestpossibly achievable score in recall.In order to get a clearer picture on the effective-ness of our proposed methodology on the two errortypes that we aimed at, we present results focusingon those two error classes.Nn ArtOrDetP 0.4783 (44/92) 0.0151 (4/263)R 0.1111 (44/396) 0.0058 (4/690)F 0.1803 0.0084Table 5: The scores achieved and the number oftrue positive, suggestions, real errors for the NounNumber (Nn) and Article and Determiner Errors(ArtOrDet) categories.4 Error AnalysisIn order to analyze the performance of our systemin more detail, we carried out an error analysis.As our system was optimized for errors related tonouns (i.e.
Nn and ArtOrDet errors), we focuson these error categories in our discussion and ne-glect verbal and prepositional errors.Some errors in our system?s output were dueto pronouns, which are conventionally tagged asnouns (e.g.
something), but were incorrectly putin the plural, resulting in the erroneous correc-tion somethings.
These errors would have beenavoided by including a list of pronouns whichcould not be used in the plural (even if they aretagged as nouns).Another common source of errors was thatcountable and uncountable uses of nouns whichcan have both features in different senses ormetonymic usage (e.g.
coffee as a substance is un-countable but coffee meaning ?a cup of coffee?
iscountable) were hard to separate.
Performance onthis class of nouns could be ameliorated by apply-ing word sense disambiguation/discrimination ora metonymy detector would also prove useful fore.g.
mass nouns.A great number of nominal errors involvedcases where a singular noun occurred in the textwithout any article or determiner.
In English, thisis only grammatical in the case of uncountablenouns which occur in generic sentences, for in-stance:Radio-frequency identification is atechnology which uses a wireless non-contact system to scan and transfer thedata [...]65The above sentence offers a definition of radio-frequency identification, hence it is a generic state-ment and should be left as it is.
In other cases,two possible strategies are available for correc-tion.
First, the noun gets an article or a determiner.The actual choice among the articles or determin-ers depends on the context: if the noun has beenmentioned previously and thus is already known(definite) in the context, it usually gets a definitearticle (or a possessive determiner).
If it is men-tioned for the first time, it gets an indefinite arti-cle (unless it is a unique thing such as the sun).The difficulty of the problem lies in the fact thatin order to adequately assign an article or deter-miner to the noun, it is not sufficient to rely onlyon the sentence.
Thus, is also necessary to go be-yond the sentence and move on the level of textor discourse, which requires natural language pro-cessing techniques that we currently lack but arehighly needed.
With the application of such tech-niques, we would have probably achieved betterresults but this remains now for future work.Second, the noun could be put in the plural.This strategy is usually applied when either thereare more than one of the thing mentioned or it is ageneric sentence (i.e.
things are discussed in gen-eral and no specific instances of things are spo-ken of).
In this case, the detection of generic sen-tences/events would be helpful, which again re-quires deep semantic processing of the discourseand is also a possible direction for future work.To conclude, the successful identification ofnoun number and article errors would require amuch deeper semantic (and even pragmatic) anal-ysis and representation of the texts in question.5 Discussion and further workComparing the columns of Table 3 we can con-clude that restricting the training sentences to onlythose which had some kind of grammatical errorin them had a useful effect on the overall effec-tiveness of our system.In a similar way, it can be stated based on theresults in Table 4 that composing features from theoutput of an LFG parser is essentially beneficialfor the determination of Nn-type errors.
Table 5reveals, however, that those features which workrelatively well on the correction of Nn type errorsare less useful on ArtOrDet-type errors withoutany modification.As our only target at this point was to suggesterror corrections related to noun phrases, our ob-vious future plans include the extension of our sys-tem to deal with error categories of different types.Simultaneously, we are planning to utilize largescale corpus statistics, such as the Google N-gramCorpus to build a more effective system.AcknowledgementsThis work was supported in part by the EuropeanUnion and the European Social Fund through theproject FuturICT.hu (grant no.
: TA?MOP-4.2.2.C-11/1/KONV-2012-0013).ReferencesJoan Bresnan.
2000.
Lexical-Functional Syntax.Blackwell, Oxford.Miriam Butt, Helge Dyvik, Tracy Holloway King,Hiroshi Masuichi, and Christian Rohrer.
2002.The Parallel Grammar Project.
In Proceedings ofCOLING-2002 Workshop on Grammar Engineeringand Evaluation, Taipei, Taiwan.Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-tian Rohrer, and Victoria Rose?n.
2007.
Speedingup LFG Parsing using C-Structure Pruning.
In Col-ing 2008: Proceedings of the workshop on GrammarEngineering Across Frameworks, pages 33 ?
40.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of Learner En-glish.
In Proceedings of the 8th Workshop on Inno-vative Use of NLP for Building Educational Appli-cations (BEA 2013), Atlanta, Georgia, USA.
Asso-ciation for Computational Linguistics.Robert Dale and Adam Kilgariff.
2010.
Helping OurOwn: Text massaging for computational linguisticsas a new shared task.
In Proceedings of the 6th Inter-national Natural Language Generation Conference,pages 261?265, Dublin, Ireland.Robert Dale and Adam Kilgariff.
2011.
Helping OurOwn: The HOO 2011 Pilot Shared Task.
In Pro-ceedings of the 13th European Workshop on NaturalLanguage Generation, Nancy, France.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A Report on the Prepositionand Determiner Error Correction Shared Task.
InProceedings of the Seventh Workshop on BuildingEducational Applications Using NLP, pages 54?62,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Rachele De Felice and Stephen G. Pulman.
2008.
AClassifier-Based Approach to Preposition and Deter-miner Error Correction in L2 English.
In Proceed-ings of the 22nd International Conference on Com-66putational Linguistics (Coling 2008), pages 169?176.Christian Fortmann and Martin Forst.
2004.
An LFGGrammar Checker for CALL.
In Proceedings ofICALL 2004.Anette Frank, Tracy Holloway King, Jonas Kuhn, andJohn T. Maxwell.
2001.
Optimality Theory StyleConstraint Ranking in Large-Scale LFG Grammars.In Peter Sells, editor, Formal and Empirical Issues inOptimality Theoretic Syntax, pages 367?397.
CSLIPublications.Andrew Kachites McCallum.
2002.
MAL-LET: A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu.Stefan Riezler, Tracy Holloway King, Ronald M. Ka-plan, Richard Crouch, John T. Maxwell, and MarkJohnson.
2002.
Parsing the Wall Street Journal us-ing a Lexical-Functional Grammar and Discrimina-tive Estimation Techniques.
In Proceedings of ACL2002.67
