Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 5?13,Uppsala, July 2010.Creating and Evaluating a Consensus for Negated and Speculative Wordsin a Swedish Clinical CorpusHercules Dalianis, Maria SkeppstedtDepartment of Computer and Systems Sciences (DSV)Stockholm UniversityForum 100SE-164 40 Kista, Sweden{hercules, mariask}@dsv.su.seAbstractIn this paper we describe the creationof a consensus corpus that was obtainedthrough combining three individual an-notations of the same clinical corpus inSwedish.
We used a few basic rules thatwere executed automatically to create theconsensus.
The corpus contains nega-tion words, speculative words, uncertainexpressions and certain expressions.
Weevaluated the consensus using it for nega-tion and speculation cue detection.
Weused Stanford NER, which is based on themachine learning algorithm ConditionalRandom Fields for the training and detec-tion.
For comparison we also used theclinical part of the BioScope Corpus andtrained it with Stanford NER.
For our clin-ical consensus corpus in Swedish we ob-tained a precision of 87.9 percent and a re-call of 91.7 percent for negation cues, andfor English with the Bioscope Corpus weobtained a precision of 97.6 percent and arecall of 96.7 percent for negation cues.1 IntroductionHow we use language to express our thoughts, andhow we interpret the language of others, varies be-tween different speakers of a language.
This istrue for various aspects of a language, and alsofor the topic of this article; negations and spec-ulations.
The differences in interpretation are ofcourse most relevant when a text is used for com-munication, but it also applies to the task of anno-tation.
When the same text is annotated by morethan one annotator, given that the annotating taskis non-trivial, the resulting annotated texts will notbe identical.
This will be the result of differencesin how the text is interpreted, but also of differ-ences in how the instructions for annotation areinterpreted.
In order to use the annotated texts,it must first be decided if the interpretations by thedifferent annotators are similar enough for the pur-pose of the text, and if so, it must be decided howto handle the non-identical annotations.In the study described in this article, we haveused a Swedish clinical corpus that was anno-tated for certainty and uncertainty, as well as fornegation and speculation cues by three Swedish-speaking annotators.
The article describes an eval-uation of a consensus annotation obtained througha few basic rules for combining the three differentannotations into one annotated text.12 Related research2.1 Previous studies on detection of negationand speculation in clinical textClinical text often contains reasoning, and therebymany uncertain or negated expressions.
When,for example, searching for patients with a specificsymptom in a clinical text, it is thus important tobe able to detect if a statement about this symptomis negated, certain or uncertain.The first approach to identifying negations inSwedish clinical text was carried out by Skeppst-edt (2010), by whom the well-known NegEx algo-rithm (Chapman et al, 2001), created for Englishclinical text, was adapted to Swedish clinical text.Skeppstedt obtained a precision of 70 percent anda recall of 81 percent in identifying negated dis-eases and symptoms in Swedish clinical text.
TheNegEx algorithm is purely rule-based, using listsof cue words indicating that a preceding or follow-ing disease or symptom is negated.
The Englishversion of NegEx (Chapman et al, 2001) obtaineda precision of 84.5 percent and a recall of 82.0 per-cent.1This research has been carried out after approval fromthe Regional Ethical Review Board in Stockholm (Etikprvn-ingsnmnden i Stockholm), permission number 2009/1742-31/5.5Another example of negation detection in En-glish is the approach used by Huang and Lowe(2007).
They used both parse trees and regu-lar expressions for detecting negated expressionsin radiology reports.
Their approach could de-tect negated expressions both close to, and alsoat some distance from, the actual negation cue (orwhat they call negation signal).
They obtained aprecision of 98.6 percent and a recall of 92.6 per-cent.Elkin et al (2005) used the terms in SNOMED-CT (Systematized Nomenclature of Medicine-Clinical Terms), (SNOMED-CT, 2010) andmatched them to 14 792 concepts in 41 healthrecords.
Of these concepts, 1 823 were identifiedas negated by humans.
The authors used MayoVocabulary Server Parsing Engine and lists of cuewords triggering negation as well as words in-dicating the scope of these negation cues.
Thisapproach gave a precision of 91.2 percent anda recall of 97.2 percent in detecting negatedSNOMED-CT concepts.In Rokach et al (2008), they used clinical nar-rative reports containing 1 766 instances annotatedfor negation.
The authors tried several machinelearning algorithms for detecting negated findingsand diseases, including hidden markov models,conditional random fields and decision trees.
Thebest results were obtained with cascaded decisiontrees, with nodes consisting of regular expressionsfor negation patterns.
The regular expressionswere automatically learnt, using the LCS (longestcommon subsequence) algorithm on the trainingdata.
The cascaded decision trees, built with LCS,gave a precision of 94.4 percent, a recall of 97.4percent and an F-score of 95.9 percent.Szarvas (2008) describes a trial to automaticallyidentify speculative sentences in radiology reports,using Maximum Entropy Models.
Advanced fea-ture selection mechanisms were used to automat-ically extract cue words for speculation from aninitial seed set of cues.
This, combined with man-ual selection of the best extracted candidates forcue words, as well as with outer dictionaries ofcue words, yielded an F-score of 82.1 percent fordetecting speculations in radiology reports.
Anevaluation was also made on scientific texts, andit could be concluded that cue words for detectingspeculation were domain-specific.Morante and Daelemans (2009) describe a ma-chine learning system detecting the scope of nega-tions, which is based on meta-learning and istrained and tested on the annotated BioScope Cor-pus.
In the clinical part of the corpus, the au-thors obtained a precision of 100 percent, a re-call of 97.5 percent and finally an F-score of 98.8percent on detection of cue words for negation.The authors used TiMBL (Tilburg Memory BasedLearner), which based its decision on featuressuch as the words annotated as negation cues andthe two words surrounding them, as well as thepart of speech and word forms of these words.For detection of the negation scope, the task wasto decide whether a word in a sentence contain-ing a negation cue was either the word startingor ending a negation scope, or neither of thesetwo.
Three different classifiers were used: sup-port vector machines, conditional random fieldsand TiMBL.
Features that were used included theword and the two words preceding and followingit, the part of speech of these words and the dis-tance to the negation cue.
A fourth classifier, alsobased on conditional random fields, used the out-put of the other three classifiers, among other fea-tures, for the final decision.
The result was a pre-cision of 86.3 percent and a recall of 82.1 percentfor clinical text.
It could also be concluded that thesystem was portable to other domains, but with alower result.2.2 The BioScope CorpusAnnotated clinical corpora in English for nega-tion and speculation are described in Vincze et al(2008), where clinical radiology reports (a sub-set of the so called BioScope Corpus) encompass-ing 6 383 sentences were annotated for negation,speculation and scope.
Henceforth, when refer-ring to the BioScope Corpus, we only refer to theclinical subset of the BioScope Corpus.
The au-thors found 877 negation cues and 1 189 specu-lation cues, (or what we call speculative cues) inthe corpora in 1 561 sentences.
This means thatfully 24 percent of the sentences contained someannotation for negation or uncertainty.
However,of the original 6 383 sentences, 14 percent con-tained negations and 13 percent contained spec-ulations.
Hence some sentences contained bothnegations and speculations.
The corpus was anno-tated by two students and their work was led by achief annotator.
The students were not allowed todiscuss their annotations with each other, except atregular meetings, but they were allowed to discuss6with the chief annotator.
In the cases where thetwo student annotators agreed on the annotation,that annotation was chosen for the final corpus.
Inthe cases where they did not agree, an annotationmade by the chief annotator was chosen.2.3 The Stanford NER based on CRFThe Stanford Named Entity Recognizer (NER) isbased on the machine learning algorithm Condi-tional Random Fields (Finkel et al, 2005) and hasbeen used extensively for identifying named enti-ties in news text.
For example in the CoNLL-2003,where the topic was language-independent namedentity recognition, Stanford NER CRF was usedboth on English and German news text for train-ing and evaluation.
Where the best results for En-glish with Stanford NER CRF gave a precision of86.1 percent, a recall of 86.5 percent and F-scoreof 86.3 percent, for German the best results hada precision of 80.4 percent, a recall of 65.0 per-cent and an F-score of 71.9 percent, (Klein et al,2003).
We have used the Stanford NER CRF fortraining and evaluation of our consensus.2.4 The annotated Swedish clinical corpusfor negation and speculationA process to create an annotated clinical corpusfor negation and speculation is described in Dalia-nis and Velupillai (2010).
A total of 6 740 ran-domly extracted sentences from a very large clin-ical corpus in Swedish were annotated by threenon-clinical annotators.
The sentences were ex-tracted from the text field Assessment (Bed?omningin Swedish).
Each sentence and its context fromthe text field Assessment were presented to the an-notators who could use five different annotationclasses to annotate the corpora.
The annotatorshad discussions every two days on the previousdays?
work led by the experiment leader.As described in Velupillai (2010), the anno-tation guidelines were inspired by the BioScopeCorpus guidelines.
There were, however, somedifferences, such as the scope of a negation or ofan uncertainty not being annotated.
It was insteadannotated if a sentence or clause was certain, un-certain or undefined.
The annotators could thuschoose to annotate the entire sentence as belong-ing to one of these three classes, or to break up thesentence into subclauses.Pairwise inter-annotator agreement was alsomeasured in the article by Dalianis and Velupillai(2010) .
The average inter-annotator agreement in-creased after the first annotation rounds, but it waslower than the agreement between the annotatorsof the BioScope Corpus.The annotation classes used were thus negationand speculative words, but also certain expressionand uncertain expression as well as undefined.
Theannotated subset contains a total of 6 740 sen-tences or 71 454 tokens, including its context.3 Method for constructing the consensusWe constructed a consensus annotation out of thethree different annotations of the same clinical cor-pus that is described in Dalianis and Velupillai(2010).
The consensus was constructed with thegeneral idea of choosing, as far as possible, an an-notation for which there existed an identical anno-tation performed by at least two of the annotators,and thus to find a majority annotation.
In the caseswhere no majority was found, other methods wereused.Other options would be to let the annotators dis-cuss the sentences that were not identically an-notated, or to use the method of the BioScopeCorpus, where the sentences that were not iden-tically annotated were resolved by a chief annota-tor (Vincze et al, 2008).
A third solution, whichmight, however, lead to a very biased corpus,would be to not include the sentences for whichthere was not a unanimous annotation in the re-sulting consensus corpus.3.1 The creation of a consensusThe annotation classes that were used for annota-tion can be divided into two levels.
The first levelconsisted of the annotation classes for classifyingthe type of sentence or clause.
This level thus in-cluded the annotation classes uncertain, certainand undefined.
The second level consisted ofthe annotation classes for annotating cue wordsfor negation and speculation, thus the annotationclasses negation and speculative words.
The an-notation classes on the first level were consideredas more important for the consensus, since if therewas no agreement on the kind of expression, itcould perhaps be said to be less important whichcue phrases these expressions contained.
In thefollowing constructed example, the annotation tagUncertain is thus an annotation on the first level,while the annotation tags Negation and Specula-tive words are on the second level.7<Sentence><Uncertain><Speculative_words><Negation>Not</Negation>really</Speculative_words>much worse than before</Uncertain><Sentence>When constructing the consensus corpus, theannotated sentences from the first rounds of an-notation were considered as sentences annotatedbefore the annotators had fully learnt to applythe guidelines.
The first 1 099 of the annotatedsentences, which also had a lower inter-annotatoragreement, were therefore not included when con-structing the consensus.
Thereby, 5 641 sentenceswere left to compare.The annotations were compared on a sentencelevel, where the three versions of each sentencewere compared.
First, sentences for which thereexisted an identical annotation performed by atleast two of the annotators were chosen.
This wasthe case for 5 097 sentences, thus 90 percent of thesentences.For the remaining 544 sentences, only annota-tion classes on the first level were compared for amajority.
For the 345 sentences where a majoritywas found on the first level, a majority on the sec-ond level was found for 298 sentences when thescope of these tags was disregarded.
The annota-tion with the longest scope was then chosen.
Forthe remaining 47 sentences, the annotation withthe largest number of annotated instances on thesecond level was chosen.The 199 sentences that were still not resolvedwere then once again compared on the first level,this time disregarding the scope.
Thereby, 77 sen-tences were resolved.
The annotation with thelongest scopes on the first-level annotations waschosen.The remaining 122 sentences were removedfrom the consensus.
Thus, of the 5 641 sentences,2 percent could not be resolved with these basicrules.
In the resulting corpus, 92 percent of thesentences were identically annotated by at leasttwo persons.3.2 Differences between the consensus andthe individual annotationsAspects of how the consensus annotation differedfrom the individual annotations were measured.The number of occurrences of each annotationclass was counted, and thereafter normalised onthe number of sentences, since the consensus an-notation contained fewer sentences than the origi-nal, individual annotations.The results in Table 1 show that there are feweruncertain expressions in the consensus annotationthan in the average of the individual annotations.The reason for this could be that if the annotationis not completeley free of randomness, the classwith a higher probability will be more frequent ina majority consensus, than in the individual anno-tations.
In the cases where the annotators are un-sure of how to classify a sentence, it is not unlikelythat the sentence has a higher probability of beingclassified as belonging to the majority class, thatis, the class certain.The class undefined is also less common inthe consensus annotation, and the same reasoningholds true for undefined as for uncertain, perhapsto an even greater extent, since undefined is evenless common.Also the speculative words are fewer in the con-sensus.
Most likely, this follows from the uncer-tain sentences being less common.The words annotated as negations, on the otherhand, are more common in the consensus anno-tation than in the individual annotations.
Thiscould be partly explained by the choice of the 47sentences with an annotation that contained thelargest number of annotated instances on the sec-ond level, and it is an indication that the consensuscontains some annotations for negation cues whichhave only been annotated by one person.Type of Annot.
class Individ.
Consens.Negation 853 910Speculative words 1 174 1 077Uncertain expression 697 582Certain expression 4 787 4 938Undefined expression 257 146Table 1: Comparison of the number of occurrencesof each annotation class for the individual annota-tions and the consensus annotation.
The figuresfor the individual annotations are the mean of thethree annotators, normalised on the number of sen-tences in the consensus.Table 2 shows how often the annotators havedivided the sentences into clauses and annotatedeach clause with a separate annotation class.
Fromthe table we can see that annotator A and also an-8notator H broke up sentences into more than onetype of the expressions Certain, Uncertain or Un-defined expressions more often than annotator F.Thereby, the resulting consensus annotation has alower frequency of sentences that contained theseannotations than the average of the individual an-notations.
Many of the more granular annotationsthat break up sentences into certain and uncertainclauses are thus not included in the consensus an-notation.
There are instead more annotations thatclassify the entire sentence as either Certain, Un-certain or Undefined.Annotators A F H Cons.No.
sentences 349 70 224 147Table 2: Number of sentences that contained morethan one instance of either one of the annotationclasses Certain, Uncertain or Undefined expres-sions or a combination of these three annotationclasses.3.3 Discussion of the methodThe constructed consensus annotation is thus dif-ferent from the individual annotations, and it couldat least in some sense be said to be better, since 92percent of the sentences have been identically an-notated by at least two persons.
However, since forexample some expressions of uncertainty, whichdo not have to be incorrect, have been removed, itcan also be said that some information containingpossible interpretations of the text, has also beenlost.The applied heuristics are in most cases specificto this annotated corpus.
The method is, however,described in order to exemplify the more generalidea to use a majority decision for selecting thecorrect annotations.
What is tested when using themajority method described in this article for de-ciding which annotation is correct, is the idea thata possible alternative to a high annotator agree-ment would be to ask many annotators to judgewhat they consider to be certain or uncertain.
Thiscould perhaps be based on a very simplified ideaof language, that the use and interpretation of lan-guage is nothing more than a majority decision bythe speakers of that language.A similar approach is used in Steidl et al(2005), where they study emotion in speech.
Sincethere are no objective criteria for deciding withwhat emotion something is said, they use manualclassification by five labelers, and a majority vot-ing for deciding which emotion label to use.
If lessthan three labelers agreed on the classification, itwas omitted from the corpus.It could be argued that this is also true for un-certainty, that if there is no possibility to ask theauthor of the text, there are no objective criteriafor deciding the level of certainty in the text.
It isalways dependent on how it is perceived by thereader, and therefore a majority method is suit-able.
Even if the majority approach can be used forsubjective classifications, it has some problems.For example, to increase validity more annotatorsare needed, which complicates the process of an-notation.
Also, the same phenomenon that wasobserved when constructing the consensus wouldprobably also arise, that a very infrequent classsuch as uncertain, would be less frequent in themajority consensus than in the individual annota-tions.
Finally, there would probably be many caseswhere there is no clear majority for either com-pletely certain or uncertain: in these cases, havingmany annotators will not help to reach a decisionand it can only be concluded that it is difficult toclassify this part of a text.
Different levels of un-certainty could then be introduced, where the ab-sence of a clear majority could be an indication ofweak certainty or uncertainty, and a very weak ma-jority could result in an undefined classification.However, even though different levels of cer-tainty or uncertainty are interesting when study-ing how uncertainties are expressed and perceived,they would complicate the process of informationextraction.
Thus, if the final aim of the annota-tion is to create a system that automatically detectswhat is certain or uncertain, it would of course bemore desirable to have an annotation with a higherinter-annotator agreement.
One way of achievinga this would be to provide more detailed annota-tion guidelines for what to define as certainty anduncertainty.
However, when it comes to such avague concept as uncertainty, there is always a thinline between having guidelines capturing the gen-eral perception of uncertainty in the language andcapturing a definition of uncertainty that is specificto the writers of the guidelines.
Also, there mightperhaps be a risk that the complex concept of cer-tainty and uncertainty becomes overly simplifiedwhen it has to be formulated as a limited set ofguidelines.
Therefore, a more feasible method ofachieving higher agreement is probably to instead9Class Neg-Spec Relevant Retrieved Corpus Precision Recall F-scoreNegation 782 890 853 0.879 0.917 0.897Speculative words 376 558 1061 0.674 0.354 0.464Total 1 158 1 448 1 914 0.800 0.605 0.687Table 3: The results for negation and speculation on consensus when executing Stanford NER CRF usingten-fold cross validation.Class Cert-Uncertain Relevant Retrieved Corpus Precision Recall F-scoreCertain expression 4 022 4 903 4 745 0.820 0.848 0.835Uncertain expression 214 433 577 0.494 0.371 0.424Undefined expression 2 5 144 0.400 0.014 0.027Total 4 238 5 341 5 466 0.793 0.775 0.784Table 4: The results for certain and uncertain on consensus when executing Stanford NER CRF usingten-fold cross validation.simplify what is being annotated, and not annotatefor such a broad concept as uncertainty in general.Among other suggestions for improving the an-notation guidelines for the corpus that the consen-sus is based on, Velupillai (2010) suggests that theguidelines should also include instructions on thefocus of the uncertainties, that is, what conceptsare to be annotated for uncertainty.The task could thus, for example, be tailored to-wards the information that is to be extracted, andthereby be simplified by only annotating for un-certainty relating to a specific concept.
If diseasesor symptoms that are present in a patient are to beextracted, the most relevant concept to annotate iswhether a finding is present or not present in thepatient, or whether it is uncertain if it is present ornot.
This approach has, for example, achieved avery high inter-annotator agreement in the anno-tation of the evaluation data used by Chapman etal.
(2001).
Even though this approach is perhapslinguistically less interesting, not giving any infor-mation on uncertainties in general, if the aim is tosearch for diseases and symptoms in patients, itshould be sufficient.In light of the discussion above, the question towhat extent the annotations in the constructed con-sensus capture a general perception of certainty oruncertainty must be posed.
Since it is constructedusing a majority method with three annotators,who had a relatively low pairwise agreement, thecorpus could probably not be said to be a precisecapture of what is a certainty or uncertainty.
How-ever, as Artstein and Poesio (2008) point out, itcannot be said that there is a fixed level of agree-ment that is valid for all purposes of a corpus, butthe agreement must be high enough for a certainpurpose.
Therefore, if the information on whetherthere was a unanimous annotation of a sentence ornot is retained, serving as an indicator of how typ-ical an expression of certainty or uncertainty is,the constructed corpus can be a useful resource.Both for studying how uncertainty in clinical textis constructed and perceived, and as one of the re-sources that is used for learning to automaticallydetect certainty and uncertainty in clinical text.4 Results of training with Stanford NERCRFAs a first indication of whether it is possible to usethe annotated consensus corpus for finding nega-tion and speculation in clinical text, we trained theStanford NER CRF, (Finkel et al, 2005) on the an-notated data.
Artstein and Poesio (2008) write thatthe fact that annotated data can be generalized andlearnt by a machine learning system is not an in-dication that the annotations capture some kind ofreality.
If it would be shown that the constructedconsensus is easily generalizable, this can thus notbe used as an evidence of its quality.
However, if itwould be shown that the data obtained by the an-notations cannot be learnt by a machine learningsystem, this can be used as an indication that thedata is not easily generalizable and that the taskto learn perhaps should, if possible, be simplified.Of course, it could also be an indication that an-other learning algorithm should be used or otherfeatures selected.We created two training sets of annotated con-sensus material.The first training set contained annotations onthe second level, thus annotations that containedthe classes Speculative words and Negation.
In 76cases, the tag for Negation was inside an annota-tion for Speculative words, and these occurrences10Class Neg-Spec Bio Relevant Retrieved Corpus Precision Recall F-scoreNegation 843 864 872 0.976 0.967 0.971Speculative words 1 021 1 079 1 124 0.946 0.908 0.927Scope11 295 1 546 1 59520.838 0.812 0.825Table 5: The results for negations, speculation cues and scopes on the BioScope Corpus when executingStanford NER CRF using ten-fold cross validation.Class Neg-Spec Relevant Retrieved Corpus Precision Recall F-scoreNegation A 791 1 005 896 0.787 0.883 0.832Speculative words 684 953 1 699 0.718 0.403 0.516Negation F 938 1097 1023 0.855 0.916 0.884Speculative words 464 782 1 496 0.593 0.310 0.407Negation H 722 955 856 0.756 0.843 0.797Speculative words 552 853 1 639 0.647 0.336 0.443Table 6: The results for negations and speculation cues and scopes for annotator A, F and H respectivelywhen executing Stanford NER CRF using ten-fold cross validation.of the tag Negation were removed.
It is detectingthis difference between a real negation cue and anegation word inside a cue for speculation that isone of the difficulties that distinguishes the learn-ing task from a simple string matching.The second training set only contained the con-sensus annotations on the first level, thus the anno-tation classes Certain, Uncertain and Undefined.We used the default settings on Stanford NERCRF.
The results of the evaluation using ten-foldcross validation (Kohavi, 1995) are shown in Table3 and Table 4.As a comparison, and to verify the suitabil-ity of the chosen machine learning method, wealso trained and evaluated the BioScope Corpususing Stanford NER CRF for negation, specula-tion and scope.
The results can be seen in Ta-ble 5.
When training the detection of scope, onlyBioScope sentences that contained an annotationfor negation and speculation were selected for thetraining and evaluation material for the StanfordNER CRF.
This division into two training sets fol-lows the method used by Morante and Daelemans(2009), where sentences containing a cue are firstdetected, and then, among these sentences, thescope of the cue is determined.We also trained and evaluated the annotationsthat were carried out by each annotator A, F andH separately, i.e.
the source of consensus.
The re-sults can be seen in Table 6.We also compared the distribution of Negationand Speculative words in the consensus versus theBioScope Corpus and we found that the consen-sus, in Swedish, used about the same number of(types) for negation as the BioScope Corpus inEnglish (see Table 7), but for speculative wordsthe consensus contained many more types than theBioScope Corpus.
In the constructed consensus,72 percent of the Speculative words occurred onlyonce, whereas in the BioScope Corpus this was thecase for only 24 percent of the Speculative words.Type of word Cons.
BioUnique words (Types)annotated as Negation 13 19Negations thatoccurred only once 5 10Unique words (Types)annotated as Speculative 408 79Speculative words thatoccurred only once 294 19Table 7: Number of unique words both in the Con-sensus and in the BioScope Corpus that were an-notated as Negation and as Speculative words, andhow many of these that occurred only once.5 DiscussionThe training results using our clinical consensuscorpus in Swedish gave a precision of 87.9 percentand a recall of 91.7 percent for negation cues and aprecision of 67.4 percent and a recall of 35.4 per-cent for speculation cues.
The results for detectingnegation cues are thus much higher than for de-tecting cues for speculation using Stanford NERCRF.
This difference is not very surprising, given1The scopes were trained and evaluated separetely fromthe negations and speculations.2The original number of annotated scopes in the BioScopeCorpus is 1 981.
Of these, 386 annotations for nested scopeswere removed.11the data in Table 7, which shows that there are onlya very limited number of negation cues, whereasthere exist over 400 different cue words for spec-ulation.
One reason why the F-score for negationcues is not even higher, despite the fact that thenumber of cues for negations is very limited, couldbe that a negation word inside a tag for speculativewords is not counted as a negation cue.
There-fore, the word not in, for example, not really couldhave been classified as a negation cue by StanfordNER CRF, even though it is a cue for speculationand not for negation.
Another reason could be thatthe word meaning without in Swedish (utan) alsomeans but, which only sometimes makes it a nega-tion cue.We can also observe in Table 4, that the resultsfor detection of uncertain expressions are very low(F-score 42 percent).
For undefined expressions,due to scarce training material, it is not possibleto interpret the results.
For certain expressions theresults are acceptable, but since the instances arein majority, the results are not very useful.Regarding the BioScope Corpus we can ob-serve (see Table 5) that the training results bothfor detecting cues for negation and for specula-tions are very high, with an F-score of 97 and 93percent, respectively.
For scope detection, the re-sult is lower but acceptable, with an F-score of83 percent.
These results indicate that the chosenmethod is suitable for the learning task.The main reason for the differences in F-scorebetween the Swedish consensus corpus and theBioScope Corpus, when it comes to the detectionof speculation cues, is probably that the variationof words that were annotated as Speculative wordis much larger in the constructed consensus thanin the BioScope Corpus.As can be seen in Table 7, there are many moretypes of speculative words in the Swedish consen-sus than in the BioScope Corpus.
We believe thatone reason for this difference is that the sentencesin the constructed consensus are extracted froma very large number of clinics (several hundred),whereas the BioScope Corpus comes from one ra-diology clinic.
This is supported by the findings ofSzarvas (2008), who writes that cues for specula-tion are domain-specific.
In this case, however, thetexts are still within the domain of clinical texts.Another reason for the larger variety of cues forspeculation in the Swedish corpus could be thatthe guidelines for annotating the BioScope Cor-pus and the method for creating a consensus weredifferent.When comparing the results for the individualannotators with the constructed consensus, the fig-ures in Tables 3 and 6 indicate that there are nobig differences in generalizability.
When detectingcues for negation, the precision for the consensusis better than the precision for the individual an-notations.
However, the results for the recall areonly slightly better or equivalent for the consensusthan for the individual annotations.
If we analysethe speculative cues we can observe that the con-sensus and the individual annotations have similarresults.The low results for learning to detect cues forspeculation also serve as an indicator that the taskshould be simplified to be more easily generaliz-able.
For example, as previously suggested forincreasing the inter-annotator agreement, the taskcould be tailored towards the specific informationthat is to be extracted, such as the presence of adisease in a patient.6 Future workTo further investigate if a machine learning algo-rithm such as Conditional Random Fields can beused for detecting speculative words, more infor-mation needs to be provided for the ConditionalRandom Fields, such as part of speech or if anyof the words in the sentence can be classified as asymptom or a disease.
One Conditional RandomFields system that can treat nested annotations isCRF++ (CRF++, 2010).
CRF++ is used by severalresearch groups and we are interested in trying itout for the negation and speculation detection aswell as scope detection.7 ConclusionA consensus clinical corpus was constructed byapplying a few basic rules for combining three in-dividual annotations into one.
Compared to theindividual annotations, the consensus containedfewer annotations of uncertainties and fewer an-notations that divided the sentences into clauses.It also contained fewer annotations for speculativewords, and more annotations for negations.
Ofthe sentences in the constructed corpus, 92 percentwere identically annotated by at least two persons.In comparison with the BioScope Corpus, theconstructed consensus contained both a largernumber and a larger variety of speculative cues.12This might be one of the reasons why the resultsfor detecting cues for speculative words using theStanford NER CRF are much better for the Bio-Scope Corpus than for the constructed consensuscorpus; the F-scores are 93 percent versus 46 per-cent.Both the BioScope Corpus and the constructedconsensus corpus had high values for detection ofnegation cues, F-scores 97 and 90 percent, respec-tively.As is suggested by Velupillai (2010), the guide-lines for annotation should include instructionson the focus of the uncertainties.
To focus thedecision of uncertainty on, for instance, the dis-ease of a patient, might improve both the inter-annotator agreement and the possibility of auto-matically learning to detect the concept of uncer-tainty.AcknowledgmentsWe are very grateful for the valuable comments bythe three anonymous reviewers.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Wendy W. Chapman, Will Bridewell, Paul Hanbury,Gregory F. Cooper, and Bruce G. Buchanan.
2001.A simple algorithm for identifying negated findingsand diseases in discharge summaries.
Journal ofbiomedical informatics, 34(5):301?310.CRF++.
2010.
CRF++: Yet another CRF toolkit, May8.
http://crfpp.sourceforge.net//.Hercules Dalianis and Sumithra Velupillai.
2010.How certain are clinical assessments?
AnnotatingSwedish clinical text for (un)certainties, specula-tions and negations.
In Proceedings of the Seventhconference on International Language Resourcesand Evaluation (LREC?10), Valletta, Malta, May.Peter L. Elkin, Steven H. Brown, Brent A. Bauer,Casey S. Husser, William Carruth, Larry R.Bergstrom, and Dietlind L. Wahner-Roedler.
2005.A controlled trial of automated classification ofnegation from clinical notes.
BMC Medical Infor-matics and Decision Making, 5(1):13.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In ACL ?05: Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 363?370.Yang Huang and Henry J. Lowe.
2007.
A novel hybridapproach to automated negation detection in clinicalradiology reports.
Journal of the American MedicalInformatics Association, 14(3):304.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Proceedings of theseventh conference on Natural language learning atHLT-NAACL 2003, pages 180?183.
Association forComputational Linguistics.Ron Kohavi.
1995.
A study of cross-validation andbootstrap for accuracy estimation and model selec-tion.
In International Joint Conference on ArtificialIntelligence, volume 14, pages 1137?1145.Roser Morante and Walter Daelemans.
2009.
A met-alearning approach to processing the scope of nega-tion.
In CoNLL ?09: Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning, pages 21?29.
Association for Com-putational Linguistics.Lior Rokach, Roni Romano, and Oded Maimo.
2008.Negation recognition in medical narrative reports.Information Retrieval, 11(6):499?538.Maria Skeppstedt.
2010.
Negation detection inSwedish clinical text.
In Louhi?10 - Second LouhiWorkshop on Text and Data Mining of Health Doc-uments, held in conjunction with NAACL HLT 2010,Los Angeles, June.SNOMED-CT. 2010.
Systematized nomen-clature of medicine-clinical terms, May 8.http://www.ihtsdo.org/snomed-ct/.Stefan Steidl, Michael Levit, Anton Batliner, ElmarN?oth, and Heinrich Niemann.
2005.
?Off allthings the measure is man?
Automatic classificationof emotions and inter-labeler consistency.
In Pro-ceeding of the IEEE ICASSP,2005, pages 317?320.Gy?orgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervised selec-tion of keywords.
In Proceedings of ACL-08: HLT,pages 281?289, Columbus, Ohio, June.
Associationfor Computational Linguistics.Sumithra Velupillai.
2010.
Towards a better un-derstanding of uncertainties and speculations inswedish clinical text ?
analysis of an initial anno-tation trial.
To be published in the proceedings ofthe Negation and Speculation in Natural LanguageProcessing Workshop, July 10, 2010, Uppsala, Swe-den.Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas,Gy?orgy M?ora, and J?anos Csirik.
2008.
The bio-scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(S-11).13
