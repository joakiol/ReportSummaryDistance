Splitting Input Sentence for Machine Translation UsingLanguage Model with Sentence SimilarityTakao Doi Eiichiro SumitaATR Spoken Language Translation Research Laboratories2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan{takao.doi, eiichiro.sumita}@atr.jpAbstractIn order to boost the translation quality ofcorpus-based MT systems for speech transla-tion, the technique of splitting an input sen-tence appears promising.
In previous re-search, many methods used N-gram clues tosplit sentences.
In this paper, to supplementN-gram based splitting methods, we introduceanother clue using sentence similarity basedon edit-distance.
In our splitting method,we generate candidates for sentence splittingbased on N-grams, and select the best one bymeasuring sentence similarity.
We conductedexperiments using two EBMT systems, one ofwhich uses a phrase and the other of whichuses a sentence as a translation unit.
Thetranslation results on various conditions wereevaluated by objective measures and a subjec-tive measure.
The experimental results showthat the proposed method is valuable for bothsystems.1 IntroductionWe are exploring methods to boost the transla-tion quality of corpus-based Machine Translation(MT) systems for speech translation.
Amongthem, the technique of splitting an input sentenceand translating the split sentences appears promis-ing (Doi and Sumita, 2003).An MT system sometimes fails to translate aninput correctly.
Such a failure occurs particu-larly when an input is long.
In such a case, bysplitting the input, translation may be successfullyperformed for each portion.
Particularly in a di-alogue, sentences tend not to have complicatednested structures, and many long sentences can besplit into mutually independent portions.
There-fore, if the splitting positions and the translationsof the split portions are adequate, the possibilitythat the arrangement of the translations can pro-vide an adequate translation of the complete in-put is relatively high.
For example, the input sen-tence, ?This is a medium size jacket I think it?sa good size for you try it on please?
1 can be splitinto three portions, ?This is a medium size jacket?,?I think it?s a good size for you?
and ?try it onplease?.
In this case, translating the three portionsand arranging the results in the same order give usthe translation of the input sentence.In previous research on splitting sentences,many methods have been based on word-sequencecharacteristics like N-gram (Lavie et al, 1996;Berger et al, 1996; Nakajima and Yamamoto,2001; Gupta et al, 2002).
Some research effortshave achieved high performance in recall and pre-cision against correct splitting positions.
Despitesuch a high performance, from the view point oftranslation, MT systems are not always able totranslate the split sentences well.In order to supplement sentence splitting basedon word-sequence characteristics, this paper intro-duces another measure of sentence similarity.
Inour splitting method, we generate candidates forsplitting positions based on N-grams, and selectthe best combination of positions by measuringsentence similarity.
This selection is based on theassumption that a corpus-based MT system cancorrectly translate a sentence that is similar to asentence in its training corpus.The following sections describe the proposedsplitting method, present experiments using twoExample-Based Machine Translation (EBMT)systems, and evaluate the effect of introducing thesimilarity measure on translation quality.2 Splitting MethodWe define the term sentence-splitting as the re-sult of splitting a sentence.
A sentence-splittingis expressed as a list of sub-sentences that are1Punctuation marks are not used in translation input inthis paper.InputSplitterGenerationSelectionCorpus-based MTTranslation  ParallelCorpus SourceSentencesLanguageModel TranslationKnowledge ProbabilitySimilarityffffffffffFigure 1: Configurationportions of the original sentence.
A sentence-splitting includes a portion or several portions.
Weuse an N-gram Language Model (NLM) to gener-ate sentence-splitting candidates, and we use theNLM and sentence similarity to select one of thecandidates.
The configuration of the method isshown in Figure 1.2.1 Probability Based on N-gram LanguageModelThe probability of a sentence can be calculatedby an NLM of a corpus.
The probability of asentence-splitting, Prob, is defined as the productof the probabilities of the sub-sentences in equa-tion (1), where P is the probability of a sentencebased on an NLM, S is a sentence-splitting, thatis, a list of sub-sentences that are portions of a sen-tence, and P is applied to the sub-sentences.Prob(S) =?s?SP (s) (1)To judge whether a sentence is split at aposition, we compare the probabilities of thesentence-splittings before and after splitting.When calculating the probability of a sentenceincluding a sub-sentence, we put pseudo words atthe head and tail of the sentence to evaluate theprobabilities of the head word and the tail word.For example, the probability of the sentence,?This is a medium size jacket?
based on a trigramlanguage model is calculated as follows.
Here,p(z | x y) indicates the probability that z occursafter the sequence x y, and SOS and EOS indicatethe pseudo words.P (this is a medium size jacket) =p(this | SOS SOS) ?p(is | SOS this) ?p(a | this is) ?...p(jacket | medium size) ?p(EOS | size jacket) ?p(EOS | jacket EOS)This causes a tendency for the probability ofthe sentence-splitting after adding a splitting posi-tion to be lower than that of the sentence-splittingbefore adding the splitting position.
Therefore,when we find a position that makes the probabilityhigher, it is plausible that the position divides thesentence into sub-sentences.2.2 Sentence SimilarityAn NLM suggests where we should split a sen-tence, by using the local clue of several wordsamong the splitting position.
To supplement itwith a wider view, we introduce another cluebased on similarity to sentences, for which trans-lation knowledge is automatically acquired from aparallel corpus.
It is reasonably expected that MTsystems can correctly translate a sentence that issimilar to a sentence in the training corpus.Here, the similarity between two sentences isdefined using the edit-distance between word se-quences.
The edit-distance used here is extendedto consider a semantic factor.
The edit-distance isnormalized between 0 and 1, and the similarity is 1minus the edit-distance.
The definition of the sim-ilarity is given in equation (2).
In this equation, Lis the word count of the corresponding sentence.I and D are the counts of insertions and deletionsrespectively.
Substitutions are permitted only be-tween content words of the same part of speech.Substitution is considered as the semantic distancebetween two substituted words, described as Sem,which is defined using a thesaurus and ranges from0 to 1.
Sem is the division of K (the level of theleast common abstraction in the thesaurus of twowords) by N (the height of the thesaurus) accord-ing to equation (3) (Sumita and Iida, 1991).Sim0(s1, s2) = 1 ?I + D + 2?SemLs1+ Ls2(2)Sem =KN(3)Using Sim0, the similarity of a sentence-splitting to a corpus is defined as Sim in equa-tion (4).
In this equation, S is a sentence-splittingand C is a given corpus that is a set of sentences.Sim is a mean similarity of sub-sentences againstthe corpus weighted with the length of each sub-sentence.
The similarity of a sentence including asub-sentence to a corpus is the greatest similaritybetween the sentence and a sentence in the corpus.Sim(S) =?s?SLs?
max{Sim0(s, c)|c ?
C}?s?SLs(4)2.3 Generating Sentence-SplittingCandidatesTo calculate Sim is similar to retrieving the mostsimilar sentence from a corpus.
The retrieval pro-cedure can be efficiently implemented by the tech-niques of clustering (Cranias et al, 1997) or usingA* search algorithm on word graphs (Doi et al,2004).
However, it still takes more cost to cal-culate Sim than Prob when the corpus is large.Therefore, in the splitting method, we first gen-erate sentence-splitting candidates by Prob alone.In the generating process, for a given sentence, thesentence itself is a candidate.
For each sentence-splitting of two portions whose Prob does not de-crease, the generating process is recursively exe-cuted with one of the two portions and then withthe other.
The results of recursive execution arecombined into candidates for the given sentence.Through this process, sentence-splittings whoseProbs are lower than that of the original sentence,are filtered out.2.4 Selecting the Best Sentence-SplittingNext, among the candidates, we select the onewith the highest score using not only Prob butalso Sim.
We use the product of Prob and Sim asthe measure to select a sentence-splitting by.
Themeasure is defined as Score in equation (5), where?, ranging from 0 to 1, gives the weight of Sim.In particular, the method uses only Prob when ?is 0, and the method generates candidates by Proband selects a candidate by only Sim when ?
is 1.Score = Prob1??
?
Sim?
(5)2.5 ExampleHere, we show an example of generating sentence-splitting candidates with Prob and selecting oneby Score.
For the input sentence, ?This is amedium size jacket I think it?s a good size foryou try it on please?, there may be many candi-dates.
Below, five candidates, whose Prob are notless than that of the original sentence, are gener-ated.
A ?|?
indicates a splitting position.
The leftnumbers indicate the ranking based on Prob.
The5th candidate is the input sentence itself.
For eachcandidate, Sim, and further, Score are calculated.Among the candidates, the 2nd is selected becauseits Score is the highest.1.
This is a medium size jacket | I think it?s a goodsize for you try it on please2.
This is a medium size jacket | I think it?s a goodsize for you | try it on please3.
This is a medium size jacket | I think it?s a goodsize | for you try it on please4.
This is a medium size jacket | I think it?s a goodsize | for you | try it on please5.
This is a medium size jacket I think it?s a goodsize for you try it on please3 Experimental ConditionsWe evaluated the splitting method through experi-ments, whose conditions are as follows.3.1 MT SystemsWe investigated the splitting method using MTsystems in English-to-Japanese translation, to de-termine what effect the method had on transla-tion.
We used two different EBMT systems astest beds.
One of the systems was HierarchicalPhrase Alignment-based Translator (HPAT) (Ima-mura, 2002), whose unit of translation expres-sion is a phrase.
HPAT translates an input sen-tence by combining phrases.
The HPAT system isequipped with another sentence splitting methodbased on parsing trees (Furuse et al, 1998).
Theother system was DP-match Driven transDucer(D3) (Sumita, 2001), whose unit of expression is asentence.
For both systems, translation knowledgeis automatically acquired from a parallel corpus.3.2 Linguistic ResourcesWe used Japanese-and-English parallel corpora,i.e., a Basic Travel Expression Corpus (BTEC)and a bilingual travel conversation corpus of Spo-ken Language (SLDB) for training, and Englishsentences in Machine-Translation-Aided bilingualDialogues (MAD) for a test set (Takezawa andKikui, 2003).
BTEC is a collection of Japanesesentences and their English translations usuallyfound in phrase-books for foreign tourists.
Thecontents of SLDB are transcriptions of spokendialogues between Japanese and English speak-ers through human interpreters.
The Japaneseand English parts of the corpora correspond toeach other sentence-to-sentence.
The dialogues ofMAD took place between Japanese and Englishspeakers through human typists and an experimen-tal MT system.
(Kikui et al, 2003) shows that BTEC and SLDBare both required for handling MAD-type tasks.Therefore, in order to translate test sentences inMAD, we merged the parallel corpora, 152,170sentence pairs of BTEC and 72,365 of SLDB,into a training corpus for HPAT and D3.
The En-glish part of the training corpus was also used tomake an NLM and to calculate similarities for thesentence-splitting method.
The statistics of thetraining corpus are shown in Table 1.
The per-plexity in the table is word trigram perplexity.The test set of this experiment was 505 Englishsentences uttered by human speakers in MAD, in-cluding no sentences generated by the MT system.The average length of the sentences in the test setwas 9.52 words per sentence.
The word trigramperplexity of the test set against the training cor-pus was 63.66.We also used a thesaurus whose hierarchies arebased on the Kadokawa Ruigo-shin-jiten (Ohnoand Hamanishi, 1984) with 80,250 entries.English Japanese# of sentences 224,535# of words 1,589,983 1,865,298avg.
sentence length 7.08 8.31vocabulary size 14,548 21,686perplexity 27.58 27.37Table 1: Statistics of the training corpus3.3 Instantiation of the MethodFor the splitting method, the NLM was the wordtrigram model using Good-Turing discounting.The number of split portions was limited to 4 persentence.
The weight of Sim, ?
in equation (5)was assigned one of 5 values: 0, 1/2, 2/3, 3/4 or 1.3.4 EvaluationWe compared translation quality under the con-ditions of with or without splitting.
To evalu-ate translation quality, we used objective measuresand a subjective measure as follows.The objective measures used were the BLEUscore (Papineni et al, 2001), the NIST score (Dod-dington, 2002) and Multi-reference Word ErrorRate (mWER) (Ueffing et al, 2002).
They werecalculated with the test set.
Both BLEU and NISTcompare the system output translation with a setof reference translations of the same source textby finding sequences of words in the referencetranslations that match those in the system outputtranslation.
Therefore, achieving higher scores bythese measures means that the translation resultscan be regarded as being more adequate transla-tions.
mWER indicates the error rate based onthe edit-distance between the system output andthe reference translations.
Therefore, achieving alower score by mWER means that the translationresults can be regarded as more adequate transla-tions.
The number of references was 15 for thethree measures.In the subjective measure (SM), the transla-tion results of the test set under different twoconditions were evaluated by paired comparison.Sentence-by-sentence, a Japanese native speakerwho had acquired a sufficient level of English,judged which result was better or that they wereof the same quality.
SM was calculated comparedto a baseline.
As in equation (6), the measure wasthe gain per sentence, where the gain was the num-ber of won translations subtracted by the numberof defeated translations as judged by the humanevaluator.SM =# of wins?
# of defeats# of test sentences(6)4 Effect of Splitting for MT4.1 Translation QualityTable 2 shows evaluations of the translation resultsof two MT systems, HPAT and D3, under six con-ditions.
In ?original?, the input sentences of thesystems were the test set itself without any split-ting.
In the other conditions, the test set sentenceswere split using Prob into sentence-splitting can-didates, and a sentence-splitting per input sentencewas selected with Score.
The weights of Proband Sim in the definition of Score in equation (5)were varied from only Prob to only Sim.
Thebaseline of SM was the original.The number of input sentences, which havemulti-candidates generated with Prob, was 237,where the average and the maximum number ofcandidates were respectively 5.07 and 64.
The av-erage length of the 237 sentences was 12.79 wordsoriginal P1S0 P 1/2S1/2 P 1/3S2/3 P 1/4S3/4 P 0S1# of split sentences 0 237 236 236 235 233BLEU 0.2979 0.3179 0.3201 0.3192 0.3193 0.3172NIST 7.1030 7.2616 7.2618 7.2709 7.2748 7.2736mWER 0.5828 0.5683 0.5665 0.5666 0.5658 0.5703HPAT SM +6.9% +8.7% +10.1% +10.1% +9.5%# of wins 89 95 99 99 104# of defeats 54 51 48 48 56# of draws 94 90 89 88 73BLEU 0.2992 0.3702 0.3704 0.3685 0.3695 0.3705NIST 2.1302 5.7809 5.8524 5.9115 5.9786 6.2545mWER 0.5844 0.5432 0.5433 0.5434 0.5424 0.5440D3 SM +20.6% +21.8% +21.8% +22.4% +23.0%# of wins 141 145 145 146 151# of defeats 37 35 35 33 35# of draws 59 56 56 56 47Table 2: MT Quality: Using splitting vs. not using splitting, on the test set of 505 sentences (P indicatesProb and S indicates Sim)per sentence.
The word trigram perplexity of theset of the 237 sentences against the training corpuswas 73.87.The table shows certain tendencies.
The differ-ences in the evaluation scores between the origi-nal and the cases with splitting are significant forboth systems and especially for D3.
Although thedifferences among the cases with splitting are notso significant, SM steadily increases when usingSim compared to using only Prob, by 3.2% forHPAT and by 2.4% for D3.
Among objective mea-sures, the NIST score corresponds well to SM.4.2 Effect of Selection Using SimilarityTable 3 allows us to focus on the effect of Sim inthe sentence-splitting selection.
The table showsthe evaluations on 237 sentences of the test set,where selection was required.
In this table, thenumber of changes is the number of cases where acandidate other than the best candidate using Probwas selected.
The table also shows the average andmaximum Prob ranking of candidates which werenot the best using Prob but were selected as thebest using Score.
The condition of ?IDEAL?
is toselect such a candidate that makes the mWER ofits translation the best value in any candidate.
InIDEAL, the selections are different between MTsystems.
The two values of the number of changesare for HPAT and for D3.
The baseline of SM wasthe condition of using only Prob.From the table, we can extract certain tenden-cies.
The number of changes is very small whenusing both Prob and Sim in the experiment.
Inthese cases, the procedure selects the best candi-dates or the second candidates in the measure ofProb.
Although the change is small when theweights of Prob and Sim are equal, SM showsthat most of the changed translations become bet-ter, some remain even and none become worse.The heavier the weight of Sim is, the higher theSM score is.
The NIST score also increases espe-cially for D3 when the weight of Sim increases.The IDEAL condition overcomes most of the con-ditions as was expected, except that the SM scoreand the NIST score of D3 are worse than thosein the condition using only Sim.
For D3, thesentence-splitting selection with Sim is a matchfor the ideal selection.So far, we have observed that SM and NISTtend to correspond to each other, although SM andBLEU or SM and mWER do not.
The NIST scoreuses information weights when comparing the re-sult of an MT system and reference translations.We can infer that the translation of a sentence-splitting, which was judged as being superior toanother by the human evaluator, is more informa-tive than the other.4.3 Effect of Using ThesaurusFurthermore, we conducted an experiment with-out using a thesaurus in calculating Sim.
In thedefinition of Sim, all semantic distances of SemP1S0P1/2S1/2P1/3S2/3P1/4S3/4P0S1 IDEAL# of changes 10 19 25 91 111; 111changed rank avg.
2.00 2.00 2.00 4.01 3.77; 3.78(max) (2) (2) (2) (20) (29); (23)BLEU 0.3004 0.3036 0.3022 0.3025 0.2994 0.3351NIST 7.1883 7.1911 7.2034 7.2068 7.1993 7.3057mWER 0.6363 0.6324 0.6328 0.6310 0.6405 0.5820HPAT SM +3.4% +3.8% +3.8% +5.9% +14.8%# of wins 8 12 15 40 59# of defeats 0 3 6 26 24# of draws 2 4 4 25 28BLEU 0.3310 0.3316 0.3291 0.3308 0.3340 0.3917NIST 6.0700 6.1687 6.2450 6.3372 6.6778 5.3250mWER 0.6181 0.6183 0.6185 0.6164 0.6197 0.5567D3 SM +3.4% +3.4% +5.5% +6.3% +5.5%# of wins 8 10 15 37 41# of defeats 0 2 2 22 28# of draws 2 7 8 32 42Table 3: MT Quality: Using similarity vs. not using similarity, on the test set of 237 sentences (Pindicates Prob and S indicates Sim)P1S0P1/2S1/2P1/3S2/3P1/4S3/4P0S1 IDEAL# of changes 10 19 26 93 111; 111changed rank avg.
2.00 2.00 2.00 4.05 3.77; 3.78(max) (2) (2) (2) (20) (29); (23)BLEU 0.3004 0.3027 0.3034 0.3039 0.2973 0.3351NIST 7.1883 7.1830 7.1921 7.2003 7.1741 7.3057mWER 0.6363 0.6342 0.6320 0.6321 0.6346 0.5820HPAT SM +1.7% +3.8% +3.4% +6.3% +14.8%# of wins 6 13 15 40 59# of defeats 2 4 7 25 24# of draws 2 2 4 28 28BLEU 0.3310 0.3301 0.3310 0.3290 0.3370 0.3917NIST 6.0700 6.1387 6.2414 6.3341 6.6739 5.3250mWER 0.6181 0.6196 0.6188 0.6198 0.6175 0.5567D3 SM +3.0% +4.6% +5.9% +7.6% +5.5%# of wins 7 12 16 41 41# of defeats 0 1 2 23 28# of draws 3 6 8 29 42Table 4: MT Quality: Using similarity vs. not using similarity, on the test set of 237 sentences, withouta thesaurus (P indicates Prob and S indicates Sim)were assumed to be equal to 0.5.
Table 4 showsevaluations on the 237 sentences.Compared to Table 3, the SM score is worsewhen the weight of Sim in Score is small, andbetter when the weight of Sim is great.
However,the difference between the conditions of using ornot using a thesaurus is not so significant.5 Concluding RemarksIn order to boost the translation quality of corpus-based MT systems for speech translation, thetechnique of splitting an input sentence appearspromising.
In previous research, many methodsused N-gram clues to split sentences.
To supple-ment N-gram based splitting methods, we intro-duce another clue using sentence similarity basedon edit-distance.
In our splitting method, we gen-erate sentence-splitting candidates based on N-grams, and select the best one by the measureof sentence similarity.
The experimental resultsshow that the method is valuable for two kinds ofEBMT systems, one of which uses a phrase andthe other of which uses a sentence as a translationunit.Although we used English-to-Japanese transla-tion in the experiments, the method depends onno particular language.
It can be applied to multi-lingual translation.
Because the semantic distanceused in the similarity definition did not show anysignificant effect, we need to find another fac-tor to enhance the similarity measure.
Further-more, as future work, we?d like to make the split-ting method cooperate with sentence simplifica-tion methods like (Siddharthan, 2002) in order toboost the translation quality much more.AcknowledgementsThe authors?
heartfelt thanks go to Kadokawa-Shotenfor providing the Ruigo-Shin-Jiten.
The research re-ported here was supported in part by a contract withthe Telecommunications Advancement Organization ofJapan entitled, ?A study of speech dialogue translationtechnology based on a large corpus?.ReferencesA.L.
Berger, S. A. Della Pietra, and V. J.Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Compu-tational Linguistics, 22(1):1?36.L.
Cranias, H. Papageorgiou, and S. Piperidis.1997.
Example retrieval from a transla-tion memory.
Natural Language Engineering,3(4):255?277.G.
Doddington.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
Proc.
of the HLT 2002Conference.T.
Doi and E. Sumita.
2003.
Input sentence split-ting and translating.
Proc.
of Workshop onBuilding and Using Parallel Texts, HLT-NAACL2003, pages 104?110.T.
Doi, E. Sumita, and H. Yamamoto.
2004.
Ef-ficient retrieval method and performance evalu-ation of example-based machine translation us-ing edit-distance (in Japanese).
Transactions ofIPSJ, 45(6).O.
Furuse, S. Yamada, and K. Yamamoto.1998.
Splitting long or ill-formed input forrobust spoken-language translation.
Proc.
ofCOLING-ACL?98, pages 421?427.N.K.
Gupta, S. Bangalore, and M. Rahim.
2002.Extracting clauses for spoken language under-standing in conversational systems.
Proc.
of IC-SLP 2002, pages 361?364.K.
Imamura.
2002.
Application of transla-tion knowledge acquired by hierarchical phrasealignment for pattern-based mt.
Proc.
of TMI-2002, pages 74?84.G.
Kikui, E. Sumita, T. Takezawa, and S. Ya-mamoto.
2003.
Creating corpora for speech-to-speech translation.
Proc.
of EUROSPEECH,pages 381?384.A.
Lavie, D. Gates, N. Coccaro, and L. Levin.1996.
Input segmentation of spontaneousspeech in janus: a speech-to-speech translationsystem.
Proc.
of ECAI-96 Workshop on Dia-logue Processing in Spoken Language Systems,pages 86?99.H.
Nakajima and H. Yamamoto.
2001.
The statis-tical language model for utterance splitting inspeech recognition (in Japanese).
Transactionsof IPSJ, 42(11):2681?2688.S.
Ohno and M. Hamanishi.
1984.
Ruigo-Shin-Jiten (in Japanese).
Kadokawa, Tokyo, Japan.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.2001.
Bleu: a method for automatic evaluationof machine translation.
RC22176, September17, 2001, Computer Science.A.
Siddharthan.
2002.
An architecture for a textsimplification system.
Proc.
of LEC 2002.E.
Sumita and H. Iida.
1991.
Experiments andprospects of example-based machine transla-tion.
Proc.
of 29th Annual Meeting of ACL,pages 185?192.E.
Sumita.
2001.
Example-based machine trans-lation using dp-matching between word se-quences.
Proc.
of 39th ACL Workshop onDDMT, pages 1?8.T.
Takezawa and G. Kikui.
2003.
Collectingmachine-translation-aided bilingual dialoguesfor corpus-based speech translation.
Proc.
ofEUROSPEECH, pages 2757?2760.N.
Ueffing, F.J. Och, and H. Ney.
2002.
Genera-tion of word graphs in statistical machine trans-lation.
Proc.
of Conf.
on Empirical Methods forNatural Language Processing, pages 156?163.
