Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 144?151,New York, June 2006. c?2006 Association for Computational LinguisticsPartial Training for a Lexicalized-Grammar ParserStephen ClarkOxford University Computing LaboratoryWolfson Building, Parks RoadOxford, OX1 3QD, UKstephen.clark@comlab.ox.ac.ukJames R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiajames@it.usyd.edu.auAbstractWe propose a solution to the annotationbottleneck for statistical parsing, by ex-ploiting the lexicalized nature of Combi-natory Categorial Grammar (CCG).
Theparsing model uses predicate-argumentdependencies for training, which are de-rived from sequences of CCG lexical cate-gories rather than full derivations.
A sim-ple method is used for extracting depen-dencies from lexical category sequences,resulting in high precision, yet incompleteand noisy data.
The dependency parsingmodel of Clark and Curran (2004b) is ex-tended to exploit this partial training data.Remarkably, the accuracy of the parsertrained on data derived from category se-quences alone is only 1.3% worse in termsof F-score than the parser trained on com-plete dependency structures.1 IntroductionState-of-the-art statistical parsers require largeamounts of hand-annotated training data, and aretypically based on the Penn Treebank, the largesttreebank available for English.
Even robust parsersusing linguistically sophisticated formalisms, suchas TAG (Chiang, 2000), CCG (Clark and Curran,2004b; Hockenmaier, 2003), HPSG (Miyao et al,2004) and LFG (Riezler et al, 2002; Cahill et al,2004), often use training data derived from the PennTreebank.
The labour-intensive nature of the tree-bank development process, which can take manyyears, creates a significant barrier for the develop-ment of parsers for new domains and languages.Previous work has attempted parser adaptationwithout relying on treebank data from the new do-main (Steedman et al, 2003; Lease and Charniak,2005).
In this paper we propose the use of anno-tated data in the new domain, but only partially an-notated data, which reduces the annotation effort re-quired (Hwa, 1999).
We develop a parsing modelwhich can be trained using partial data, by exploitingthe properties of lexicalized grammar formalisms.The formalism we use is Combinatory CategorialGrammar (Steedman, 2000), together with a parsingmodel described in Clark and Curran (2004b) whichwe adapt for use with partial data.Parsing with Combinatory Categorial Grammar(CCG) takes place in two stages: first, CCG lexicalcategories are assigned to the words in the sentence,and then the categories are combined by the parser(Clark and Curran, 2004a).
The lexical categoriescan be thought of as detailed part of speech tags andtypically express subcategorization information.
Weexploit the fact that CCG lexical categories containa lot of syntactic information, and can therefore beused for training a full parser, even though attach-ment information is not explicitly represented in acategory sequence.
Our partial training regime onlyrequires sentences to be annotated with lexical cate-gories, rather than full parse trees; therefore the datacan be produced much more quickly for a new do-main or language (Clark et al, 2004).The partial training method uses the log-lineardependency model described in Clark and Curran(2004b), which uses sets of predicate-argument de-144pendencies, rather than derivations, for training.
Ournovel idea is that, since there is so much informa-tion in the lexical category sequence, most of thecorrect dependencies can be easily inferred from thecategories alone.
More specifically, for a given sen-tence and lexical category sequence, we train onthose predicate-argument dependencies which occurin k% of the derivations licenced by the lexical cat-egories.
By setting the k parameter high, we canproduce a set of high precision dependencies fortraining.
A similar idea is proposed by Carroll andBriscoe (2002) for producing high precision data forlexical acquisition.Using this procedure we are able to produce de-pendency data with over 99% precision and, re-markably, up to 86% recall, when compared againstthe complete gold-standard dependency data.
Thehigh recall figure results from the significant amountof syntactic information in the lexical categories,which reduces the ambiguity in the possible depen-dency structures.
Since the recall is not 100%, werequire a log-linear training method which workswith partial data.
Riezler et al (2002) describe apartial training method for a log-linear LFG parsingmodel in which the ?correct?
LFG derivations for asentence are those consistent with the less detailedgold standard derivation from the Penn Treebank.We use a similar method here by treating a CCGderivation as correct if it is consistent with the high-precision partial dependency structure.
Section 3 ex-plains what we mean by consistency in this context.Surprisingly, the accuracy of the parser trained onpartial data approaches that of the parser trained onfull data: our best partial-data model is only 1.3%worse in terms of dependency F-score than the full-data model, despite the fact that the partial data doesnot contain any explicit attachment information.2 The CCG Parsing ModelClark and Curran (2004b) describes two log-linearparsing models for CCG: a normal-form derivationmodel and a dependency model.
In this paper weuse the dependency model, which requires sets ofpredicate-argument dependencies for training.11Hockenmaier and Steedman (2002) describe a generativemodel of normal-form derivations; one possibility for trainingthis model on partial data, which has not been explored, is touse the EM algorithm (Pereira and Schabes, 1992).The predicate-argument dependencies are repre-sented as 5-tuples: ?hf , f, s, ha, l?, where hf is thelexical item of the lexical category expressing thedependency relation; f is the lexical category; s isthe argument slot; ha is the head word of the ar-gument; and l encodes whether the dependency isnon-local.
For example, the dependency encodingcompany as the object of bought (as in IBM boughtthe company) is represented as follows:?bought2, (S\NP1 )/NP2 , 2, company4,??
(1)CCG dependency structures are sets of predicate-argument dependencies.
We define the probabilityof a dependency structure as the sum of the probabil-ities of all those derivations leading to that structure(Clark and Curran, 2004b).
?Spurious ambiguity?
inCCG means that there can be more than one deriva-tion leading to any one dependency structure.
Thus,the probability of a dependency structure, pi, given asentence, S, is defined as follows:P (pi|S) =?d??
(pi)P (d, pi|S) (2)where ?
(pi) is the set of derivations which lead to pi.The probability of a ?d, pi?
pair, ?, conditional ona sentence S, is defined using a log-linear form:P (?|S) = 1ZSe?.f (?)
(3)where ?.f(?)
=?i ?ifi(?).
The function fi is theinteger-valued frequency function of the ith feature;?i is the weight of the ith feature; and ZS is a nor-malising constant.Clark and Curran (2004b) describes the trainingprocedure for the dependency model, which uses adiscriminative estimation method by maximising theconditional likelihood of the model given the data(Riezler et al, 2002).
The optimisation of the objec-tive function is performed using the limited-memoryBFGS numerical optimisation algorithm (Nocedaland Wright, 1999; Malouf, 2002), which requirescalculation of the objective function and the gradi-ent of the objective function at each iteration.The objective function is defined below, whereL(?)
is the likelihood and G(?)
is a Gaussian priorterm for smoothing.145He anticipates growth for the auto makerNP (S [dcl ]\NP)/NP NP (NP\NP)/NP NP [nb]/N N /N NFigure 1: Example sentence with CCG lexical categoriesL?(?)
= L(?)
?G(?)
(4)=m?j=1log?d??
(pij)e?.f (d,pij)?m?j=1log????
(Sj)e?.f (?)
?n?i=1?2i2?2S1, .
.
.
, Sm are the sentences in the training data;pi1, .
.
.
, pim are the corresponding gold-standard de-pendency structures; ?
(S) is the set of possible?derivation, dependency-structure?
pairs for S; ?
isa smoothing parameter; and n is the number of fea-tures.
The components of the gradient vector are:?L?(?)??i=m?j=1?d??
(pij)e?.f (d,pij)fi(d, pij)?d??
(pij) e?.f (d,pij)(5)?m?j=1????
(Sj)e?.f (?)fi(?)????
(Sj) e?.f (?)?
?i?2The first two terms of the gradient are expecta-tions of feature fi: the first expectation is overall derivations leading to each gold-standard depen-dency structure, and the second is over all deriva-tions for each sentence in the training data.
The es-timation process attempts to make the expectationsin (5) equal (ignoring the Gaussian prior term).
An-other way to think of the estimation process is thatit attempts to put as much mass as possible on thederivations leading to the gold-standard structures(Riezler et al, 2002).Calculation of the feature expectations requiressumming over all derivations for a sentence, andsumming over all derivations leading to a gold-standard dependency structure.
Clark and Cur-ran (2003) shows how the sum over the completederivation space can be performed efficiently usinga packed chart and the inside-outside algorithm, andClark and Curran (2004b) extends this method tosum over all derivations leading to a gold-standarddependency structure.3 Partial TrainingThe partial data we use for training the dependencymodel is derived from CCG lexical category se-quences only.
Figure 1 gives an example sentenceadapted from CCGbank (Hockenmaier, 2003) to-gether with its lexical category sequence.
Note that,although the attachment of the prepositional phraseto the noun phrase is not explicitly represented, itcan be inferred in this example because the lexicalcategory assigned to the preposition has to combinewith a noun phrase to the left, and in this examplethere is only one possibility.
One of the key insightsin this paper is that the significant amount of syntac-tic information in CCG lexical categories allows usto infer attachment information in many cases.The procedure we use for extracting dependenciesfrom a sequence of lexical categories is to return allthose dependencies which occur in k% of the deriva-tions licenced by the categories.
By giving the k pa-rameter a high value, we can extract sets of depen-dencies with very high precision; in fact, assumingthat the correct lexical category sequence licencesthe correct derivation, setting k to 100 must result in100% precision, since any dependency which occursin every derivation must occur in the correct deriva-tion.
Of course the recall is not guaranteed to behigh; decreasing k has the effect of increasing recall,but at the cost of decreasing precision.The training method described in Section 2 canbe adapted to use the (potentially incomplete) setsof dependencies returned by our extraction proce-dure.
In Section 2 a derivation was considered cor-rect if it produced the complete set of gold-standarddependencies.
In our partial-data version a deriva-tion is considered correct if it produces dependen-cies which are consistent with the dependencies re-turned by our extraction procedure.
We define con-sistency as follows: a set of dependencies D is con-sistent with a set G if G is a subset of D. We alsosay that a derivation d is consistent with dependencyset G if G is a subset of the dependencies producedby d.146This definition of ?correct derivation?
will intro-duce some noise into the training data.
Noise arisesfrom sentences where the recall of the extracted de-pendencies is less than 100%, since some of thederivations which are consistent with the extracteddependencies for such sentences will be incorrect.Noise also arises from sentences where the preci-sion of the extracted dependencies is less than 100%,since for these sentences every derivation which isconsistent with the extracted dependencies will beincorrect.
The hope is that, if an incorrect derivationproduces mostly correct dependencies, then it canstill be useful for training.
Section 4 shows how theprecision and recall of the extracted dependenciesvaries with k and how this affects parsing accuracy.The definitions of the objective function (4) andthe gradient (5) for training remain the same in thepartial-data case; the only differences are that ?
(pi)is now defined to be those derivations which are con-sistent with the partial dependency structure pi, andthe gold-standard dependency structures pij are thepartial structures extracted from the gold-standardlexical category sequences.2Clark and Curran (2004b) gives an algorithm forfinding all derivations in a packed chart which pro-duce a particular set of dependencies.
This algo-rithm is required for calculating the value of the ob-jective function (4) and the first feature expectationin (5).
We adapt this algorithm for finding all deriva-tions which are consistent with a partial dependencystructure.
The new algorithm is shown in Figure 2.The algorithm relies on the definition of a packedchart, which is an instance of a feature forest (Miyaoand Tsujii, 2002).
The idea behind a packed chart isthat equivalent chart entries of the same type and inthe same cell are grouped together, and back point-ers to the daughters indicate how an individual entrywas created.
Equivalent entries form the same struc-tures in any subsequent parsing.A feature forest is defined in terms of disjunctiveand conjunctive nodes.
For a packed chart, the indi-vidual entries in a cell are conjunctive nodes, and theequivalence classes of entries are disjunctive nodes.The definition of a feature forest is as follows:A feature forest ?
is a tuple ?C,D,R, ?, ??
where:2Note that the procedure does return all the gold-standarddependencies for some sentences.
?C,D,R, ?, ??
is a packed chart / feature forestG is a set of dependencies returned by the extraction procedureLet c be a conjunctive nodeLet d be a disjunctive nodedeps(c) is the set of dependencies on node ccdeps(c) = |deps(c) ?
G|dmax(c) =?d??
(c) dmax(d) + cdeps(c)dmax(d) = max{dmax(c) | c ?
?
(d)}mark(d):mark d as a correct nodeforeach c ?
?
(d)if dmax(c) == dmax(d)mark c as a correct nodeforeach d?
?
?(c)mark(d?
)foreach dr ?
R such that dmax.
(dr) = |G|mark(dr)Figure 2: Finding nodes in derivations consistentwith a partial dependency structure?
C is a set of conjunctive nodes;?
D is a set of disjunctive nodes;?
R ?
D is a set of root disjunctive nodes;?
?
: D ?
2C is a conjunctive daughter function;?
?
: C ?
2D is a disjunctive daughter function.Dependencies are associated with conjunctivenodes in the feature forest.
For example, if thedisjunctive nodes (equivalence classes of individualentries) representing the categories NP and S\NPcombine to produce a conjunctive node S , the re-sulting S node will have a verb-subject dependencyassociated with it.In Figure 2, cdeps(c) is the number of dependen-cies on conjunctive node c which appear in partialstructure G; dmax(c) is the maximum number ofdependencies in G produced by any sub-derivationheaded by c; dmax(d) is the same value for disjunc-tive node d. Recursive definitions for calculatingthese values are given; the base case occurs whenconjunctive nodes have no disjunctive daughters.The algorithm identifies all those root nodes head-ing derivations which are consistent with the partialdependency structure G, and traverses the chart top-down marking the nodes in those derivations.
Theinsight behind the algorithm is that, for two con-junctive nodes in the same equivalence class, if onenode heads a sub-derivation producing more depen-dencies in G than the other node, then the node with147less dependencies inG cannot be part of a derivationconsistent with G.The conjunctive and disjunctive nodes appearingin derivations consistent with G form a new ?gold-standard?
feature forest.
The gold-standard forest,and the complete forest containing all derivationsspanning the sentence, can be used to estimate thelikelihood value and feature expectations requiredby the estimation algorithm.
Let E?
?fi be the ex-pected value of fi over the forest ?
for model ?
;then the values in (5) can be obtained by calculatingE?j?
fi for the complete forest ?j for each sentenceSj in the training data (the second sum in (5)), andalso E?j?
fi for each forest ?j of derivations consis-tent with the partial gold-standard dependency struc-ture for sentence Sj (the first sum in (5)):?L(?)??i=m?j=1(E?j?
fi ?E?j?
fi) (6)The likelihood in (4) can be calculated as follows:L(?)
=m?j=1(logZ?j ?
logZ?j ) (7)where logZ?
is the normalisation constant for ?.4 ExperimentsThe resource used for the experiments is CCGbank(Hockenmaier, 2003), which consists of normal-form CCG derivations derived from the phrase-structure trees in the Penn Treebank.
It also containspredicate-argument dependencies which we use fordevelopment and final evaluation.4.1 Accuracy of Dependency ExtractionSections 2-21 of CCGbank were used to investigatethe accuracy of the partial dependency structures re-turned by the extraction procedure.
Full, correct de-pendency structures for the sentences in 2-21 werecreated by running our CCG parser (Clark and Cur-ran, 2004b) over the gold-standard derivation foreach sentence, outputting the dependencies.
This re-sulted in full dependency structures for 37,283 of thesentences in sections 2-21.Table 1 gives precision and recall values for thedependencies obtained from the extraction proce-dure, for the 37,283 sentences for which we havek Precision Recall SentAcc0.99999 99.76 74.96 13.840.9 99.69 79.37 16.520.85 99.65 81.30 18.400.8 99.57 82.96 19.510.7 99.09 85.87 22.460.6 98.00 88.67 26.28Table 1: Accuracy of the Partial Dependency Datacomplete dependency structures.
The SentAcc col-umn gives the percentage of training sentences forwhich the partial dependency structures are com-pletely correct.
For a given sentence, the extrac-tion procedure returns all dependencies occurring inat least k% of the derivations licenced by the gold-standard lexical category sequence.
The lexical cat-egory sequences for the sentences in 2-21 can easilybe read off the CCGbank derivations.The derivations licenced by a lexical category se-quence were created using the CCG parser describedin Clark and Curran (2004b).
The parser uses a smallnumber of combinatory rules to combine the cate-gories, along with the CKY chart-parsing algorithmdescribed in Steedman (2000).
It also uses someunary type-changing rules and punctuation rules ob-tained from the derivations in CCGbank.3 The parserbuilds a packed representation, and counting thenumber of derivations in which a dependency occurscan be performed using a dynamic programming al-gorithm similar to the inside-outside algorithm.Table 1 shows that, by varying the value of k, itis possible to get the recall of the extracted depen-dencies as high as 85.9%, while still maintaining aprecision value of over 99%.4.2 Accuracy of the ParserThe training data for the dependency model was cre-ated by first supertagging the sentences in sections2-21, using the supertagger described in Clark andCurran (2004b).4 The average number of categories3Since our training method is intended to be applicable inthe absence of derivation data, the use of such rules may appearsuspect.
However, we argue that the type-changing and punc-tuation rules could be manually created for a new domain byexamining the lexical category data.4An improved version of the supertagger was used for thispaper in which the forward-backward algorithm is used to cal-culate the lexical category probability distributions.148assigned to each word is determined by a parameter,?, in the supertagger.
A category is assigned to aword if the category?s probability is within ?
of thehighest probability category for that word.For these experiments, we used a ?
value of 0.01,which assigns roughly 1.6 categories to each word,on average; we also ensured that the correct lexi-cal category was in the set assigned to each word.
(We did not do this when parsing the test data.)
Forsome sentences, the packed charts can become verylarge.
The supertagging approach we adopt for train-ing differs to that used for testing: if the size of thechart exceeds some threshold, the value of ?
is in-creased, reducing ambiguity, and the sentence is su-pertagged and parsed again.
The threshold whichlimits the size of the charts was set at 300 000 indi-vidual entries.
Two further values of ?
were used:0.05 and 0.1.Packed charts were created for each sentence andstored in memory.
It is essential that the packedcharts for each sentence contain at least one deriva-tion leading to the gold-standard dependency struc-ture.
Not all rule instantiations in CCGbank can beproduced by our parser; hence it is not possible toproduce the gold standard for every sentence in Sec-tions 2-21.
For the full-data model we used 34 336sentences (86.7% of the total).
For the partial-datamodels we were able to use slightly more, since thepartial structures are easier to produce.
Here weused 35,709 sentences (k = 0.85).Since some of the packed charts are very large,we used an 18-node Beowulf cluster, together witha parallel version of the BFGS training algorithm.The training time and number of iterations to con-vergence were 172 minutes and 997 iterations for thefull-data model, and 151 minutes and 861 iterationsfor the partial-data model (k = 0.85).
Approximatememory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of fea-tures described in Clark and Curran (2004b): de-pendency features representing predicate-argumentdependencies (with and without distance measures);rule instantiation features encoding the combiningcategories together with the result category (withand without a lexical head); lexical category fea-tures, consisting of word?category pairs at the leafnodes; and root category features, consisting ofheadword?category pairs at the root nodes.
Furtherk LP LR F CatAcc0.99999 85.80 84.51 85.15 93.770.9 85.86 84.51 85.18 93.780.85 85.89 84.50 85.19 93.710.8 85.89 84.45 85.17 93.700.7 85.52 84.07 84.79 93.720.6 84.99 83.70 84.34 93.65FullData 87.16 85.84 86.50 93.79Random 74.63 72.53 73.57 89.31Table 2: Accuracy of the Parser on Section 00generalised features for each feature type are formedby replacing words with their POS tags.Only features which occur more than once in thetraining data are included, except that the cutofffor the rule features is 10 or more and the count-ing is performed across all derivations licenced bythe gold-standard lexical category sequences.
Thelarger cutoff was used since the productivity of thegrammar can lead to large numbers of these features.The dependency model has 548 590 features.
In or-der to provide a fair comparison, the same feature setwas used for the partial-data and full-data models.The CCG parsing consists of two phases: first thesupertagger assigns the most probable categories toeach word, and then the small number of combina-tory rules, plus the type-changing and punctuationrules, are used with the CKY algorithm to build apacked chart.5 We use the method described in Clarkand Curran (2004b) for integrating the supertaggerwith the parser: initially a small number of cat-egories is assigned to each word, and more cate-gories are requested if the parser cannot find a span-ning analysis.
The ?maximum-recall?
algorithm de-scribed in Clark and Curran (2004b) is used to findthe highest scoring dependency structure.Table 2 gives the accuracy of the parser on Section00 of CCGbank, evaluated against the predicate-argument dependencies in CCGbank.6 The tablegives labelled precision, labelled recall and F-score,and lexical category accuracy.
Numbers are givenfor the partial-data model with various values of k,and for the full-data model, which provides an up-5Gold-standard POS tags from CCGbank were used for allthe experiments in this paper.6There are some dependency types produced by our parserwhich are not in CCGbank; these were ignored for evaluation.149LP LR F CatAcck = 0.85 86.21 85.01 85.60 93.90FullData 87.50 86.37 86.93 94.01Table 3: Accuracy of the Parser on Section 23k Precision Recall SentAcc0.99999 99.71 80.16 17.480.9999 99.68 82.09 19.130.999 99.49 85.18 22.180.99 99.00 88.95 27.690.95 98.34 91.69 34.950.9 97.82 92.84 39.18Table 4: Accuracy of the Partial Dependency Datausing Inside-Outside Scoresper bound for the partial-data model.
We also give alower bound which we obtain by randomly travers-ing a packed chart top-down, giving equal proba-bility to each conjunctive node in an equivalenceclass.
The precision and recall figures are over thosesentences for which the parser returned an analysis(99.27% of Section 00).The best result is obtained for a k value of 0.85,which produces partial dependency data with a pre-cision of 99.7 and a recall of 81.3.
Interestingly, theresults show that decreasing k further, which resultsin partial data with a higher recall and only a slightloss in precison, harms the accuracy of the parser.The Random result also dispels any suspicion thatthe partial-model is performing well simply becauseof the supertagger; clearly there is still much workto be done after the supertagging phase.Table 3 gives the accuracy of the parser on Sec-tion 23, using the best performing partial-data modelon Section 00.
The precision and recall figures areover those sentences for which the parser returnedan analysis (99.63% of Section 23).
The resultsshow that the partial-data model is only 1.3% F-score short of the upper bound.4.3 Further Experiments with Inside-OutsideIn a final experiment, we attempted to exploit thehigh accuracy of the partial-data model by using itto provide new training data.
For each sentence inSection 2-21, we parsed the gold-standard lexicalcategory sequences and used the best performingpartial-data model to assign scores to each depen-dency in the packed chart.
The score for a depen-dency was the sum of the probabilities of all deriva-tions producing that dependency, which can be cal-culated using the inside-outside algorithm.
(This isthe score used by the maximum-recall parsing algo-rithm.)
Partial dependency structures were then cre-ated by returning all dependencies whose score wasabove some threshold k, as before.
Table 4 gives theaccuracy of the data created by this procedure.
Notehow these values differ to those reported in Table 1.We then trained the dependency model on thispartial data using the same method as before.
How-ever, the peformance of the parser on Section 00 us-ing these new models was below that of the previousbest performing partial-data model for all values ofk.
We report this negative result because we had hy-pothesised that using a probability model to scorethe dependencies, rather than simply the number ofderivations in which they occur, would lead to im-proved performance.5 ConclusionsOur main result is that it is possible to train a CCGdependency model from lexical category sequencesalone and still obtain parsing results which are only1.3% worse in terms of labelled F-score than amodel trained on complete data.
This is a notewor-thy result and demonstrates the significant amountof information encoded in CCG lexical categories.The engineering implication is that, since the de-pendency model can be trained without annotatingrecursive structures, and only needs sequence in-formation at the word level, then it can be portedrapidly to a new domain (or language) by annotatingnew sequence data in that domain.One possible response to this argument is that,since the lexical category sequence contains somuch syntactic information, then the task of anno-tating category sequences must be almost as labourintensive as annotating full derivations.
To test thishypothesis fully would require suitable annotationtools and subjects skilled in CCG annotation, whichwe do not currently have access to.However, there is some evidence that annotat-ing category sequences can be done very efficiently.Clark et al (2004) describes a porting experiment150in which a CCG parser is adapted for the ques-tion domain.
The supertagger component of theparser is trained on questions annotated at the lex-ical category level only.
The training data consistsof over 1,000 annotated questions which took lessthan a week to create.
This suggests, as a veryrough approximation, that 4 annotators could an-notate 40,000 sentences with lexical categories (thesize of the Penn Treebank) in a few months.Another advantage of annotating with lexical cat-egories is that a CCG supertagger can be used to per-form most of the annotation, with the human an-notator only required to correct the mistakes madeby the supertagger.
An accurate supertagger can bebootstrapped quicky, leaving only a small number ofcorrections for the annotator.
A similar procedure issuggested by Doran et al (1997) for porting an LTAGgrammar to a new domain.We have a proposed a novel solution to the an-notation bottleneck for statistical parsing which ex-ploits the lexicalized nature of CCG, and may there-fore be applicable to other lexicalized grammar for-malisms such as LTAG.ReferencesA.
Cahill, M. Burke, R. O?Donovan, J. van Genabith, andA.
Way.
2004.
Long-distance dependency resolution in au-tomatically acquired wide-coverage PCFG-based LFG ap-proximations.
In Proceedings of the 42nd Meeting of theACL, pages 320?327, Barcelona, Spain.John Carroll and Ted Briscoe.
2002.
High precision extrac-tion of grammatical relations.
In Proceedings of the 19th In-ternational Conference on Computational Linguistics, pages134?140, Taipei, Taiwan.David Chiang.
2000.
Statistical parsing with an automatically-extracted Tree Adjoining Grammar.
In Proceedings of the38th Meeting of the ACL, pages 456?463, Hong Kong.Stephen Clark and James R. Curran.
2003.
Log-linear mod-els for wide-coverage CCG parsing.
In Proceedings of theEMNLP Conference, pages 97?104, Sapporo, Japan.Stephen Clark and James R. Curran.
2004a.
The importance ofsupertagging for wide-coverage CCG parsing.
In Proceed-ings of COLING-04, pages 282?288, Geneva, Switzerland.Stephen Clark and James R. Curran.
2004b.
Parsing the WSJusing CCG and log-linear models.
In Proceedings of the42nd Meeting of the ACL, pages 104?111, Barcelona, Spain.Stephen Clark, Mark Steedman, and James R. Curran.
2004.Object-extraction and question-parsing using CCG.
InProceedings of the EMNLP Conference, pages 111?118,Barcelona, Spain.C.
Doran, B. Hockey, P. Hopely, J. Rosenzweig, A. Sarkar,B.
Srinivas, F. Xia, A. Nasr, and O. Rambow.
1997.
Main-taining the forest and burning out the underbrush in XTAG.In Proceedings of the ENVGRAM Workshop, Madrid, Spain.Julia Hockenmaier and Mark Steedman.
2002.
Generativemodels for statistical parsing with Combinatory CategorialGrammar.
In Proceedings of the 40th Meeting of the ACL,pages 335?342, Philadelphia, PA.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with Combinatory Categorial Grammar.
Ph.D. the-sis, University of Edinburgh.Rebbeca Hwa.
1999.
Supervised grammar induction usingtraining data with limited constituent information.
In Pro-ceedings of the 37th Meeting of the ACL, pages 73?79, Uni-versity of Maryland, MD.Matthew Lease and Eugene Charniak.
2005.
Parsing biomed-ical literature.
In Proceedings of the Second Interna-tional Joint Conference on Natural Language Processing(IJCNLP-05), Jeju Island, Korea.Robert Malouf.
2002.
A comparison of algorithms for max-imum entropy parameter estimation.
In Proceedings of theSixth Workshop on Natural Language Learning, pages 49?55, Taipei, Taiwan.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum entropyestimation for feature forests.
In Proceedings of the HumanLanguage Technology Conference, San Diego, CA.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004.Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the Penn Treebank.
InProceedings of the First International Joint Conference onNatural Language Processing (IJCNLP-04), pages 684?693,Hainan Island, China.Jorge Nocedal and Stephen J. Wright.
1999.
Numerical Opti-mization.
Springer, New York, USA.Fernando Pereira and Yves Schabes.
1992.
Inside-outside rees-timation from partially bracketed corpora.
In Proceedings ofthe 30th Meeting of the ACL, pages 128?135, Newark, DE.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell III, and Mark Johnson.
2002.Parsing the Wall Street Journal using a Lexical-FunctionalGrammar and discriminative estimation techniques.
In Pro-ceedings of the 40th Meeting of the ACL, pages 271?278,Philadelphia, PA.Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steve Baker,and Jeremiah Crim.
2003.
Bootstrapping statistical parsersfrom small datasets.
In Proceedings of the 11th Conferenceof the European Association for Computational Linguistics,Budapest, Hungary.Mark Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge, MA.151
