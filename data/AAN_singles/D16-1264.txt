Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383?2392,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSQuAD: 100,000+ Questions for Machine Comprehension of TextPranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang{pranavsr,zjian,klopyrev,pliang}@cs.stanford.eduComputer Science DepartmentStanford UniversityAbstractWe present the Stanford Question Answer-ing Dataset (SQuAD), a new reading compre-hension dataset consisting of 100,000+ ques-tions posed by crowdworkers on a set ofWikipedia articles, where the answer to eachquestion is a segment of text from the cor-responding reading passage.
We analyze thedataset to understand the types of reason-ing required to answer the questions, lean-ing heavily on dependency and constituencytrees.
We build a strong logistic regressionmodel, which achieves an F1 score of 51.0%,a significant improvement over a simple base-line (20%).
However, human performance(86.8%) is much higher, indicating that thedataset presents a good challenge problem forfuture research.
The dataset is freely availableat https://stanford-qa.com.1 IntroductionReading Comprehension (RC), or the ability to readtext and then answer questions about it, is a chal-lenging task for machines, requiring both under-standing of natural language and knowledge aboutthe world.
Consider the question ?what causes pre-cipitation to fall??
posed on the passage in Figure 1.In order to answer the question, one might first lo-cate the relevant part of the passage ?precipitation ...falls under gravity?, then reason that ?under?
refersto a cause (not location), and thus determine the cor-rect answer: ?gravity?.How can we get a machine to make progresson the challenging task of reading comprehension?Historically, large, realistic datasets have playedIn meteorology, precipitation is any productof the condensation of atmospheric water vaporthat falls under gravity.
The main forms of pre-cipitation include drizzle, rain, sleet, snow, grau-pel and hail... Precipitation forms as smallerdroplets coalesce via collision with other raindrops or ice crystals within a cloud.
Short, in-tense periods of rain in scattered locations arecalled ?showers?.What causes precipitation to fall?gravityWhat is another main form of precipitation be-sides drizzle, rain, snow, sleet and hail?graupelWhere do water droplets collide with ice crystalsto form precipitation?within a cloudFigure 1: Question-answer pairs for a sample passage in theSQuAD dataset.
Each of the answers is a segment of text fromthe passage.a critical role for driving fields forward?famousexamples include ImageNet for object recognition(Deng et al, 2009) and the Penn Treebank forsyntactic parsing (Marcus et al, 1993).
Existingdatasets for RC have one of two shortcomings: (i)those that are high in quality (Richardson et al,2013; Berant et al, 2014) are too small for trainingmodern data-intensive models, while (ii) those thatare large (Hermann et al, 2015; Hill et al, 2015) aresemi-synthetic and do not share the same character-istics as explicit reading comprehension questions.To address the need for a large and high-qualityreading comprehension dataset, we present the Stan-2383ford Question Answering Dataset v1.0 (SQuAD),freely available at https://stanford-qa.com, con-sisting of questions posed by crowdworkers on aset of Wikipedia articles, where the answer to ev-ery question is a segment of text, or span, from thecorresponding reading passage.
SQuAD contains107,785 question-answer pairs on 536 articles, andis almost two orders of magnitude larger than previ-ous manually labeled RC datasets such as MCTest(Richardson et al, 2013).In contrast to prior datasets, SQuAD does notprovide a list of answer choices for each question.Rather, systems must select the answer from all pos-sible spans in the passage, thus needing to cope witha fairly large number of candidates.
While ques-tions with span-based answers are more constrainedthan the more interpretative questions found in moreadvanced standardized tests, we still find a rich di-versity of questions and answer types in SQuAD.We develop automatic techniques based on distancesin dependency trees to quantify this diversity andstratify the questions by difficulty.
The span con-straint also comes with the important benefit thatspan-based answers are easier to evaluate than free-form answers.To assess the difficulty of SQuAD, we imple-mented a logistic regression model with a range offeatures.
We find that lexicalized and dependencytree path features are important to the performanceof the model.
We also find that the model perfor-mance worsens with increasing complexity of (i) an-swer types and (ii) syntactic divergence between thequestion and the sentence containing the answer; in-terestingly, there is no such degradation for humans.Our best model achieves an F1 score of 51.0%,1which is much better than the sliding window base-line (20%).
Over the last four months (since June2016), we have witnessed significant improvementsfrom more sophisticated neural network-based mod-els.
For example, Wang and Jiang (2016) obtained70.3% F1 on SQuAD v1.1 (results on v1.0 are sim-ilar).
These results are still well behind humanperformance, which is 86.8% F1 based on inter-annotator agreement.
This suggests that there isplenty of room for advancement in modeling andlearning on the SQuAD dataset.1All experimental results in this paper are on SQuAD v1.0.Dataset Questionsource Formulation SizeSQuAD crowdsourced RC, spansin passage 100KMCTest(Richardson et al, 2013) crowdsourced RC, multiplechoice 2640Algebra(Kushman et al, 2014) standardizedtests computation 514Science(Clark and Etzioni, 2016) standardizedtests reasoning,multiplechoice855WikiQA(Yang et al, 2015) query logs IR, sentenceselection 3047TREC-QA(Voorhees and Tice, 2000) query logs +human editor IR, free form 1479CNN/Daily Mail(Hermann et al, 2015) summary +cloze RC, fill insingle entity 1.4MCBT(Hill et al, 2015) cloze RC, fill insingle word 688KTable 1: A survey of several reading comprehension and ques-tion answering datasets.
SQuAD is much larger than all datasetsexcept the semi-synthetic cloze-style datasets, and it is similarto TREC-QA in the open-endedness of the answers.2 Existing DatasetsWe begin with a survey of existing reading com-prehension and question answering (QA) datasets,highlighting a variety of task formulation and cre-ation strategies (see Table 1 for an overview).Reading comprehension.
A data-driven approachto reading comprehension goes back to Hirschmanet al (1999), who curated a dataset of 600 real 3rd?6th grade reading comprehension questions.
Theirpattern matching baseline was subsequently im-proved by a rule-based system (Riloff and Thelen,2000) and a logistic regression model (Ng et al,2000).
More recently, Richardson et al (2013) cu-rated MCTest, which contains 660 stories createdby crowdworkers, with 4 questions per story and4 answer choices per question.
Because many ofthe questions require commonsense reasoning andreasoning across multiple sentences, the dataset re-mains quite challenging, though there has been no-ticeable progress (Narasimhan and Barzilay, 2015;Sachan et al, 2015; Wang et al, 2015).
Both curateddatasets, although real and difficult, are too small tosupport very expressive statistical models.Some datasets focus on deeper reasoning abili-ties.
Algebra word problems require understandinga story well enough to turn it into a system of equa-2384tions, which can be easily solved to produce the an-swer (Kushman et al, 2014; Hosseini et al, 2014).BAbI (Weston et al, 2015), a fully synthetic RCdataset, is stratified by different types of reasoningrequired to solve each task.
Clark and Etzioni (2016)describe the task of solving 4th grade science exams,and stress the need to reason with world knowledge.Open-domain question answering.
The goal ofopen-domain QA is to answer a question from alarge collection of documents.
The annual eval-uations at the Text REtreival Conference (TREC)(Voorhees and Tice, 2000) led to many advancesin open-domain QA, many of which were used inIBM Watson for Jeopardy!
(Ferrucci et al, 2013).Recently, Yang et al (2015) created the WikiQAdataset, which, like SQuAD, use Wikipedia pas-sages as a source of answers, but their task is sen-tence selection, while ours requires selecting a spe-cific span in the sentence.Selecting the span of text that answers a questionis similar to answer extraction, the final step in theopen-domain QA pipeline, methods for which in-clude bootstrapping surface patterns (Ravichandranand Hovy, 2002), using dependency trees (Shen andKlakow, 2006), and using a factor graph over mul-tiple sentences (Sun et al, 2013).
One key differ-ence between our RC setting and answer extractionis that answer extraction typically exploits the factthat the answer occurs in multiple documents (Brillet al, 2002), which is more lenient than in our set-ting, where a system only has access to a single read-ing passage.Cloze datasets.
Recently, researchers have con-structed cloze datasets, in which the goal is to pre-dict the missing word (often a named entity) in apassage.
Since these datasets can be automaticallygenerated from naturally occurring data, they can beextremely large.
The Children?s Book Test (CBT)(Hill et al, 2015), for example, involves predictinga blanked-out word of a sentence given the 20 previ-ous sentences.
Hermann et al (2015) constructed acorpus of cloze style questions by blanking out enti-ties in abstractive summaries of CNN / Daily Newsarticles; the goal is to fill in the entity based on theoriginal article.
While the size of this dataset is im-pressive, Chen et al (2016) showed that the datasetrequires less reasoning than previously thought, andFigure 2: The crowd-facing web interface used to collect thedataset encourages crowdworkers to use their own words whileasking questions.concluded that performance is almost saturated.One difference between SQuAD questions andcloze-style queries is that answers to cloze queriesare single words or entities, while answers inSQuAD often include non-entities and can be muchlonger phrases.
Another difference is that SQuADfocuses on questions whose answers are entailedby the passage, whereas the answers to cloze-stylequeries are merely suggested by the passage.3 Dataset CollectionWe collect our dataset in three stages: curatingpassages, crowdsourcing question-answers on thosepassages, and obtaining additional answers.Passage curation.
To retrieve high-quality arti-cles, we used Project Nayuki?s Wikipedia?s internalPageRanks to obtain the top 10000 articles of En-glish Wikipedia, from which we sampled 536 arti-cles uniformly at random.
From each of these ar-ticles, we extracted individual paragraphs, strippingaway images, figures, tables, and discarding para-graphs shorter than 500 characters.
The result was23,215 paragraphs for the 536 articles covering awide range of topics, from musical celebrities to ab-stract concepts.
We partitioned the articles randomlyinto a training set (80%), a development set (10%),2385and a test set (10%).Question-answer collection.
Next, we employedcrowdworkers to create questions.
We used theDaemo platform (Gaikwad et al, 2015), with Ama-zon Mechanical Turk as its backend.
Crowdworkerswere required to have a 97% HIT acceptance rate, aminimum of 1000 HITs, and be located in the UnitedStates or Canada.
Workers were asked to spend 4minutes on every paragraph, and paid $9 per hour forthe number of hours required to complete the article.The task was reviewed favorably by crowdworkers,receiving positive comments on Turkopticon.On each paragraph, crowdworkers were taskedwith asking and answering up to 5 questions on thecontent of that paragraph.
The questions had to beentered in a text field, and the answers had to behighlighted in the paragraph.
To guide the work-ers, tasks contained a sample paragraph, and exam-ples of good and bad questions and answers on thatparagraph along with the reasons they were cate-gorized as such.
Additionally, crowdworkers wereencouraged to ask questions in their own words,without copying word phrases from the paragraph.On the interface, this was reinforced by a reminderprompt at the beginning of every paragraph, and bydisabling copy-paste functionality on the paragraphtext.Additional answers collection.
To get an indica-tion of human performance on SQuAD and to makeour evaluation more robust, we obtained at least 2additional answers for each question in the develop-ment and test sets.
In the secondary answer gener-ation task, each crowdworker was shown only thequestions along with the paragraphs of an article,and asked to select the shortest span in the para-graph that answered the question.
If a question wasnot answerable by a span in the paragraph, workerswere asked to submit the question without markingan answer.
Workers were recommended a speed of 5questions for 2 minutes, and paid at the same rate of$9 per hour for the number of hours required for theentire article.
Over the development and test sets,2.6% of questions were marked unanswerable by atleast one of the additional crowdworkers.Answer type Percentage ExampleDate 8.9% 19 October 1512Other Numeric 10.9% 12Person 12.9% Thomas CokeLocation 4.4% GermanyOther Entity 15.3% ABC SportsCommon Noun Phrase 31.8% property damageAdjective Phrase 3.9% second-largestVerb Phrase 5.5% returned to EarthClause 3.7% to avoid trivializationOther 2.7% quietlyTable 2: We automatically partition our answers into the fol-lowing categories.
Our dataset consists of large number of an-swers beyond proper noun entities.4 Dataset AnalysisTo understand the properties of SQuAD, we analyzethe questions and answers in the development set.Specifically, we explore the (i) diversity of answertypes, (ii) the difficulty of questions in terms of typeof reasoning required to answer them, and (iii) thedegree of syntactic divergence between the questionand answer sentences.Diversity in answers.
We automatically catego-rize the answers as follows: We first separatethe numerical and non-numerical answers.
Thenon-numerical answers are categorized using con-stituency parses and POS tags generated by Stan-ford CoreNLP.
The proper noun phrases are furthersplit into person, location and other entities usingNER tags.
In Table 2, we can see dates and othernumbers make up 19.8% of the data; 32.6% of theanswers are proper nouns of three different types;31.8% are common noun phrases answers; and theremaining 15.8% are made up of adjective phrases,verb phrases, clauses and other types.Reasoning required to answer questions.
To geta better understanding of the reasoning required toanswer the questions, we sampled 4 questions fromeach of the 48 articles in the development set, andthen manually labeled the examples with the cate-gories shown in Table 3.
The results show thatall examples have some sort of lexical or syntacticdivergence between the question and the answer inthe passage.
Note that some examples fall into morethan one category.2386Reasoning Description Example PercentageLexical variation(synonymy)Major correspondences betweenthe question and the answer sen-tence are synonyms.Q: What is the Rankine cycle sometimes called?Sentence: The Rankine cycle is sometimes re-ferred to as a practical Carnot cycle.33.3%Lexical variation(world knowledge)Major correspondences betweenthe question and the answer sen-tence require world knowledge toresolve.Q: Which governing bodies have veto power?Sen.
: The European Parliament and the Council ofthe European Union have powers of amendmentand veto during the legislative process.9.1%Syntactic variation After the question is paraphrasedinto declarative form, its syntac-tic dependency structure does notmatch that of the answer sentenceeven after local modifications.Q: What Shakespeare scholar is currently on thefaculty?Sen.
: Current faculty include the anthropol-ogist Marshall Sahlins, ..., Shakespeare scholarDavid Bevington.64.1%Multiple sentencereasoningThere is anaphora, or higher-levelfusion of multiple sentences is re-quired.Q: What collection does the V&A Theatre & Per-formance galleries hold?Sen.
: The V&A Theatre & Performance gal-leries opened in March 2009.
... Theyhold the UK?s biggest national collection ofmaterial about live performance.13.6%Ambiguous We don?t agree with the crowd-workers?
answer, or the questiondoes not have a unique answer.Q: What is the main goal of criminal punishment?Sen.
: Achieving crime control via incapacitationand deterrence is a major goal of criminal punish-ment.6.1%Table 3: We manually labeled 192 examples into one or more of the above categories.
Words relevant to the correspondingreasoning type are bolded, and the crowdsourced answer is underlined.Q: What department store is thought to be the first in the world?S: Bainbridge?s is often cited as the world?s first department store.Path:first xcomp????
?thought nsubjpass??????
store det??
?what?delete ?substitute ?insertfirst amod???
?store nmod????
cited nsubjpass?????
?Bainbridge?sEdit cost:1 +2 +1=4Figure 3: An example walking through the computation of thesyntactic divergence between the question Q and answer sen-tence S.Stratification by syntactic divergence.
We alsodevelop an automatic method to quantify the syntac-tic divergence between a question and the sentencecontaining the answer.
This provides another way tomeasure the difficulty of a question and to stratifythe dataset, which we return to in Section 6.3.We illustrate how we measure the divergence withthe example in Figure 3.
We first detect anchors(word-lemma pairs common to both the questionand answer sentences); in the example, the anchoris ?first?.
The two unlexicalized paths, one fromthe anchor ?first?
in the question to the wh-word?what?, and the other from the anchor in the answersentence and to the answer span ?Bainbridge?s?, arethen extracted from the dependency parse trees.
Wemeasure the edit distance between these two paths,which we define as the minimum number of dele-tions or insertions to transform one path into theother.
The syntactic divergence is then defined asthe minimum edit distance over all possible anchors.The histogram in Figure 4a shows that there is awide range of syntactic divergence in our dataset.We also show a concrete example where the edit dis-tance is 0 and another where it is 6.
Note that oursyntactic divergence ignores lexical variation.
Also,small divergence does not mean that a question iseasy since there could be other candidates with sim-ilarly small divergence.5 MethodsWe developed a logistic regression model and com-pare its accuracy with that of three baseline methods.23870 1 2 3 4 5 6 7 8Syntactic divergence0.05.010.015.020.025.030.0Percentage(a) Histogram of syntactic divergence.Q: Who went to Wittenberg to hear Luther speak?S: Students thronged to Wittenberg to hear Lutherspeak.Path:Wittenberg nmod????
went nsubj????
WhoWittenberg nmod????
thronged nsubj????
Students(b) An example of a question-answer pair with edit distance 0 be-tween the dependency paths (note that lexical variation is ignoredin the computation of edit distance).Q: What impact did the high school education movement have on the presence of skilled workers?S: During the mass high school education movement from 1910 ?
1940 , there was an increase in skilled workers.Path:school compound??????
movement nsubj????
have dobj???
impact det??
Whatschool compound??????
movement nmod????
1910 acl??
was nsubj????
increase(c) An example of a question-answer pair with edit distance 6.Figure 4: We use the edit distance between the unlexicalized dependency paths in the question and the sentence containing theanswer to measure syntactic divergence.Candidate answer generation.
For all four meth-ods, rather than considering all O(L2) spans as can-didate answers, where L is the number of wordsin the sentence, we only use spans which are con-stituents in the constituency parse generated byStanford CoreNLP.
Ignoring punctuation and arti-cles, we find that 77.3% of the correct answers in thedevelopment set are constituents.
This places an ef-fective ceiling on the accuracy of our methods.
Dur-ing training, when the correct answer of an exampleis not a constituent, we use the shortest constituentcontaining the correct answer as the target.5.1 Sliding Window BaselineFor each candidate answer, we compute the uni-gram/bigram overlap between the sentence contain-ing it (excluding the candidate itself) and the ques-tion.
We keep all the candidates that have the max-imal overlap.
Among these, we select the bestone using the sliding-window approach proposedin Richardson et al (2013).In addition to the basic sliding window ap-proach, we also implemented the distance-based ex-tension (Richardson et al, 2013).
Whereas Richard-son et al (2013) used the entire passage as the con-text of an answer, we used only the sentence con-taining the candidate answer for efficiency.5.2 Logistic RegressionIn our logistic regression model, we extract severaltypes of features for each candidate answer.
Wediscretize each continuous feature into 10 equally-sized buckets, building a total of 180 million fea-tures, most of which are lexicalized features or de-pendency tree path features.
The descriptions andexamples of the features are summarized in Table 4.The matching word and bigram frequencies aswell as the root match features help the model pickthe correct sentences.
Length features bias themodel towards picking common lengths and posi-tions for answer spans, while span word frequenciesbias the model against uninformative words.
Con-stituent label and span POS tag features guide themodel towards the correct answer types.
In addi-tion to these basic features, we resolve lexical vari-ation using lexicalized features, and syntactic varia-tion using dependency tree path features.The multiclass log-likelihood loss is optimizedusing AdaGrad with an initial learning rate of 0.1.Each update is performed on the batch of all ques-tions in a paragraph for efficiency, since they sharethe same candidates.
L2 regularization is used, witha coefficient of 0.1 divided by the number of batches.The model is trained with three passes over the train-2388Feature Groups Description ExamplesMatching WordFrequenciesSum of the TF-IDF of the words that occur in both the question and thesentence containing the candidate answer.
Separate features are usedfor the words to the left, to the right, inside the span, and in the wholesentence.Span: [0 ?
sum < 0.01]Left: [7.9 ?
sum < 10.7]Matching BigramFrequenciesSame as above, but using bigrams.
We use the generalization of theTF-IDF described in Shirakawa et al (2015).Span: [0 ?
sum < 2.4]Left: [0 ?
sum < 2.7]Root Match Whether the dependency parse tree roots of the question and sentencematch, whether the sentence contains the root of the dependency parsetree of the question, and whether the question contains the root of thedependency parse tree of the sentence.Root Match = FalseLengths Number of words to the left, to the right, inside the span, and in thewhole sentence.Span: [1 <= num < 2]Left: [15 ?
num < 19]Span WordFrequenciesSum of the TF-IDF of the words in the span, regardless of whether theyappear in the question.Span: [5.2 ?
sum < 6.9]Constituent Label Constituency parse tree label of the span, optionally combined with thewh-word in the question.Span: NPSpan: NP, wh-word: ?what?Span POS Tags Sequence of the part-of-speech tags in the span, optionally combinedwith the wh-word in the question.Span: [NN]Span: [NN], wh-word: ?what?Lexicalized Lemmas of question words combined with the lemmas of words withindistance 2 to the span in the sentence based on the dependency parsetrees.
Separately, question word lemmas combined with answer wordlemmas.Q: ?cause?, S: ?under?
case??
?Q: ?fall?, A: ?gravity?Dependency TreePathsFor each word that occurs in both the question and sentence, the pathin the dependency parse tree from that word in the sentence to the span,optionally combined with the path from the wh-word to the word in thequestion.
POS tags are included in the paths.VBZ nmod????
NNwhat nsubj???
VBZ advcl??
?+ VBZ nmod???
?NNTable 4: Features used in the logistic regression model with examples for the question ?What causes precipitation to fall?
?, sentence?In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity.?
and answer?gravity?.
Q denotes question, A denotes candidate answer, and S denotes sentence containing the candidate answer.ing data.6 Experiments6.1 Model EvaluationWe use two different metrics to evaluate model accu-racy.
Both metrics ignore punctuations and articles(a, an, the).Exact match.
This metric measures the percent-age of predictions that match any one of the groundtruth answers exactly.
(Macro-averaged) F1 score.
This metric mea-sures the average overlap between the prediction andground truth answer.
We treat the prediction andground truth as bags of tokens, and compute theirF1.
We take the maximum F1 over all of the groundtruth answers for a given question, and then averageover all of the questions.6.2 Human PerformanceWe assess human performance on SQuAD?s devel-opment and test sets.
Recall that each of the ques-tions in these sets has at least three answers.
To eval-uate human performance, we treat the second an-swer to each question as the human prediction, andkeep the other answers as ground truth answers.
Theresulting human performance score on the test set is77.0% for the exact match metric, and 86.8% for F1.Mismatch occurs mostly due to inclusion/exclusionof non-essential phrases (e.g., monsoon trough ver-sus movement of the monsoon trough) rather thanfundamental disagreements about the answer.6.3 Model PerformanceTable 5 shows the performance of our models along-side human performance on the v1.0 of developmentand test sets.
The logistic regression model signifi-cantly outperforms the baselines, but underperforms2389Exact Match F1Dev Test Dev TestRandom Guess 1.1% 1.3% 4.1% 4.3%Sliding Window 13.2% 12.5% 20.2% 19.7%Sliding Win.
+ Dist.
13.3% 13.0% 20.2% 20.0%Logistic Regression 40.0% 40.4% 51.0% 51.0%Human 80.3% 77.0% 90.5% 86.8%Table 5: Performance of various methods and humans.
Logis-tic regression outperforms the baselines, while there is still asignificant gap between humans.F1Train DevLogistic Regression 91.7% 51.0%?
Lex., ?
Dep.
Paths 33.9% 35.8%?
Lexicalized 53.5% 45.4%?
Dep.
Paths 91.4% 46.4%?
Match.
Word Freq.
91.7% 48.1%?
Span POS Tags 91.7% 49.7%?
Match.
Bigram Freq.
91.7% 50.3%?
Constituent Label 91.7% 50.4%?
Lengths 91.8% 50.5%?
Span Word Freq.
91.7% 50.5%?
Root Match 91.7% 50.6%Table 6: Performance with feature ablations.
We find that lexi-calized and dependency tree path features are most important.humans.
We note that the model is able to selectthe sentence containing the answer correctly with79.3% accuracy; hence, the bulk of the difficulty liesin finding the exact span within the sentence.Feature ablations.
In order to understand the fea-tures that are responsible for the performance of thelogistic regression model, we perform a feature ab-lation where we remove one group of features fromour model at a time.
The results, shown in Table 6,indicate that lexicalized and dependency tree pathfeatures are most important.
Comparing our analy-sis to the one in Chen et al (2016), we note that thedependency tree path features play a much biggerrole in our dataset.
Additionally, we note that withlexicalized features, the model significantly overfitsthe training set; however, we found that increasingL2 regularization hurts performance on the develop-ment set.Performance stratified by answer type.
To gainmore insight into the performance of our logistic re-gression model, we report its performance acrossLogistic Regression HumanDev F1 Dev F1Date 72.1% 93.9%Other Numeric 62.5% 92.9%Person 56.2% 95.4%Location 55.4% 94.1%Other Entity 52.2% 92.6%Common Noun Phrase 46.5% 88.3%Adjective Phrase 37.9% 86.8%Verb Phrase 31.2% 82.4%Clause 34.3% 84.5%Other 34.8% 86.1%Table 7: Performance stratified by answer types.
Logistic re-gression performs better on certain types of answers, namelynumbers and entities.
On the other hand, human performance ismore uniform.0 1 2 3 4 5 6 7 8Syntactic divergence2030405060708090100Preformance(%)Logistic Regression Dev F1Human Dev F1Figure 5: Performance stratified by syntactic divergence ofquestions and sentences.
The performance of logistic regres-sion degrades with increasing divergence.
In contrast, humanperformance is stable across the full range of divergence.the answer types explored in Table 2.
The re-sults (shown in Table 7) show that the model per-forms best on dates and other numbers, categoriesfor which there are usually only a few plausible can-didates, and most answers are single tokens.
Themodel is challenged more on other named entities(i.e., location, person and other entities) becausethere are many more plausible candidates.
How-ever, named entities are still relatively easy to iden-tify by their POS tag features.
The model performsworst on other answer types, which together form47.6% of the dataset.
Humans have exceptional per-formance on dates, numbers and all named entities.Their performance on other answer types degradesonly slightly.2390Performance stratified by syntactic divergence.As discussed in Section 4, another challenging as-pect of the dataset is the syntactic divergence be-tween the question and answer sentence.
Figure 5shows that the more divergence there is, the lowerthe performance of the logistic regression model.Interestingly, humans do not seem to be sensitiveto syntactic divergence, suggesting that deep under-standing is not distracted by superficial differences.Measuring the degree of degradation could thereforebe useful in determining the extent to which a modelis generalizing in the right way.7 ConclusionTowards the end goal of natural language under-standing, we introduce the Stanford Question An-swering Dataset, a large reading comprehensiondataset on Wikipedia articles with crowdsourcedquestion-answer pairs.
SQuAD features a diverserange of question and answer types.
The perfor-mance of our logistic regression model, with 51.0%F1, against the human F1 of 86.8% suggests ampleopportunity for improvement.
We have made ourdataset freely available to encourage exploration ofmore expressive models.
Since the release of ourdataset, we have already seen considerable interestin building models on this dataset, and the gap be-tween our logistic regression model and human per-formance has more than halved (Wang and Jiang,2016).
We expect that the remaining gap will beharder to close, but that such efforts will result insignificant advances in reading comprehension.ReproducibilityAll code, data, and experiments for this paper areavailable on the CodaLab platform:https://worksheets.codalab.org/worksheets/0xd53d03a48ef64b329c16b9baf0f99b0c/ .AcknowledgmentsWe would like to thank Durim Morina and ProfessorMichael Bernstein for their help in crowdsourcingthe collection of our dataset, both in terms of fund-ing and technical support of the Daemo platform.ReferencesJ.
Berant, V. Srikumar, P. Chen, A. V. Linden, B. Harding,B.
Huang, P. Clark, and C. D. Manning.
2014.
Mod-eling biological processes for reading comprehension.In Empirical Methods in Natural Language Process-ing (EMNLP).E.
Brill, S. Dumais, and M. Banko.
2002.
An analysis ofthe AskMSR question-answering system.
In Associa-tion for Computational Linguistics (ACL), pages 257?264.D.
Chen, J. Bolton, and C. D. Manning.
2016.
Athorough examination of the CNN / Daily Mail read-ing comprehension task.
In Association for Computa-tional Linguistics (ACL).P.
Clark and O. Etzioni.
2016.
My computer is an honorstudent but how intelligent is it?
standardized tests asa measure of AI.
AI Magazine, 37(1):5?12.J.
Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei.
2009.
ImageNet: A large-scale hierarchical im-age database.
In Computer Vision and Pattern Recog-nition (CVPR), pages 248?255.D.
Ferrucci, E. Brown, J. Chu-Carroll, J.
Fan, D. Gondek,A.
A. Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg,J.
Prager, N. Schlaefer, and C. Welty.
2013.
Build-ing Watson: An overview of the DeepQA project.
AIMagazine, 31(3):59?79.S.
N. Gaikwad, D. Morina, R. Nistala, M. Agarwal,A.
Cossette, R. Bhanu, S. Savage, V. Narwal, K. Raj-pal, J. Regino, et al 2015.
Daemo: A self-governedcrowdsourcing marketplace.
In Proceedings of the28th Annual ACM Symposium on User Interface Soft-ware & Technology, pages 101?102.K.
M. Hermann, T.
Koc?isky?, E. Grefenstette, L. Espeholt,W.
Kay, M. Suleyman, and P. Blunsom.
2015.
Teach-ing machines to read and comprehend.
In Advances inNeural Information Processing Systems (NIPS).F.
Hill, A. Bordes, S. Chopra, and J. Weston.
2015.The goldilocks principle: Reading children?s bookswith explicit memory representations.
In InternationalConference on Learning Representations (ICLR).L.
Hirschman, M. Light, E. Breck, and J. D. Burger.1999.
Deep read: A reading comprehension system.In Association for Computational Linguistics (ACL),pages 325?332.M.
J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kush-man.
2014.
Learning to solve arithmetic word prob-lems with verb categorization.
In Empirical Meth-ods in Natural Language Processing (EMNLP), pages523?533.N.
Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzilay.2014.
Learning to automatically solve algebra wordproblems.
In Association for Computational Linguis-tics (ACL).2391M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Linguistics,19:313?330.K.
Narasimhan and R. Barzilay.
2015.
Machine compre-hension with discourse relations.
In Association forComputational Linguistics (ACL).H.
T. Ng, L. H. Teo, and J. L. P. Kwan.
2000.
A machinelearning approach to answering questions for readingcomprehension tests.
In Joint SIGDAT conference onempirical methods in natural language processing andvery large corpora - Volume 13, pages 124?132.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
In As-sociation for Computational Linguistics (ACL), pages41?47.M.
Richardson, C. J. Burges, and E. Renshaw.
2013.Mctest: A challenge dataset for the open-domain ma-chine comprehension of text.
In Empirical Methods inNatural Language Processing (EMNLP), pages 193?203.E.
Riloff and M. Thelen.
2000.
A rule-based questionanswering system for reading comprehension tests.
InANLP/NAACL Workshop on reading comprehensiontests as evaluation for computer-based language un-derstanding sytems - Volume 6, pages 13?19.M.
Sachan, A. Dubey, E. P. Xing, and M. Richardson.2015.
Learning answer-entailing structures for ma-chine comprehension.
In Association for Computa-tional Linguistics (ACL).D.
Shen and D. Klakow.
2006.
Exploring correlation ofdependency relation paths for answer extraction.
In In-ternational Conference on Computational Linguisticsand Association for Computational Linguistics (COL-ING/ACL), pages 889?896.M.
Shirakawa, T. Hara, and S. Nishio.
2015.
N-gram idf:A global term weighting scheme based on informationdistance.
In World Wide Web (WWW), pages 960?970.H.
Sun, N. Duan, Y. Duan, and M. Zhou.
2013.
Answerextraction from passage graph for question answering.In International Joint Conference on Artificial Intelli-gence (IJCAI).E.
M. Voorhees and D. M. Tice.
2000.
Building a ques-tion answering test collection.
In ACM Special InterestGroup on Information Retreival (SIGIR), pages 200?207.Shuohang Wang and Jing Jiang.
2016.
Machine compre-hension using match-lstm and answer pointer.
CoRR,abs/1608.07905.H.
Wang, M. Bansal, K. Gimpel, and D. McAllester.2015.
Machine comprehension with syntax, frames,and semantics.
In Association for Computational Lin-guistics (ACL).J.
Weston, A. Bordes, S. Chopra, and T. Mikolov.
2015.Towards AI-complete question answering: A set ofprerequisite toy tasks.
arXiv.Y.
Yang, W. Yih, and C. Meek.
2015.
WikiQA: A chal-lenge dataset for open-domain question answering.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 2013?2018.2392
