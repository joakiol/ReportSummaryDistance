Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 302?306,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsEfforts on Machine Learning overHuman-mediated Translation Edit RateEleftherios AvramidisGerman Research Center for Artificial Intelligence (DFKI)Language Technology LabAlt Moabit 91c, 10559 Berlin, Germanyeleftherios.avramidis@dfki.deAbstractIn this paper we describe experiments onpredicting HTER, as part of our submis-sion in the Shared Task on Quality Esti-mation, in the frame of the 9th Workshopon Statistical Machine Translation.
In ourexperiment we check whether it is possi-ble to achieve better HTER prediction bytraining four individual regression modelsfor each one of the edit types (deletions,insertions, substitutions, shifts), howeverno improvements were yielded.
We alsohad no improvements when investigat-ing the possibility of adding more datafrom other non-minimally post-edited andfreely translated datasets.
Best HTER pre-diction was achieved by adding dedupli-cated WMT13 data and additional featuressuch as (a) rule-based language correc-tions (language tool) (b) PCFG parsingstatistics and count of tree labels (c) posi-tion statistics of parsing labels (d) positionstatistics of tri-grams with low probability.1 IntroductionAs Machine Translation (MT) gets integrated intoregular translation workflows, its use as base forpost-editing is radically increased.
As a result,there is a great demand for methods that can auto-matically assess the MT outcome and ensure thatit is useful for the translator and can lead to moreproductive translation work.Although many agree that the quality of theMT output itself is not adequate for the profes-sional standards, there has not yet been a widely-accepted way to measure its quality on par withhuman translations.
One such metric, the Hu-man Translation Edit Rate (HTER) (Snover etal., 2006), is the focus of the current submission.HTER is highly relevant to the need of adaptingMT to the needs of translators, as it aims to mea-sure how far it is from an acceptable equivalenttranslation done by humans.HTER is used here in the frame of Quality Es-timation, i.e.
having the goal of being able to pre-dict the post-editing effort in a real case environ-ment, right before the translation is given to theuser, without real access to the correct translation.For this purpose the text of the source and the pro-duced translation is analyzed by automatic toolsin order to infer indications (numerical features)that may be relevant to the quality of the transla-tion.
These features are used in a statistical modelwhose parameters are estimated with common su-pervised Machine Learning techniques.This work presents an extensive search over var-ious set-ups and parameters for such techniques,aiming to build a model that better predicts HTERover the data of the Shared Task of the 9th Work-shop on Statistical Machine Translation.2 New approaches being tested2.1 Break HTER apartHTER is a complex metric, in the sense that it iscalculated as a linear function over specific typesof edit distance.
The official algorithm performsa comparison between the MT output and the cor-rected version of this output by a human translator,who performed the minimum number of changes.The comparison results in counting the number ofinsertions, deletions, substitutions and shifts (e.g.reordering).
The final HTER score is the totalnumber of edits divided by the number of refer-ence words.HTER =#insertions + #dels + #subs + #shifts#reference wordsWe notice that the metric is clearly based on fouredit types that are seemingly independent of eachother.
This poses the question whether the existing302approach of learning the entire metric altogetherintroduces way too much complexity in the ma-chine learning process.
Instead, we test the hy-pothesis that it is more effective to build a separatemodel for each error type and then put the outputof each model on the overall HTER fraction shownabove.Following this idea, we score the given transla-tions again in order to produce all four HTER fac-tors (insertions, deletions, substitutions and shifts)and we train four regression models accordingly.This way, each model can be optimized separately,in order to better fit the particular error type, unaf-fected by the noise that other error types may infer.2.2 Rounding of individual edit typepredictionsDue to the separate model per error type, it is pos-sible to perform corrections on the predicted errorcount for each error type, before the calculation ofthe entire HTER score.
This may be helpful, giventhe observation that continuous statistical modelsmay produce a real number as prediction for thecount of edits, whereas the actual requirement isan integer.Here, we take this opportunity and test the hy-pothesis that prediction of the overall HTER is bet-ter, if the output of the four individual models isrounded to the closest integer, before entered inthe HTER ratio.2.3 More data by approximating minimalpost-editsWe investigate whether prediction performancecan be improved by adding further data.
This risesfrom the fact that the original number of sentencesis relatively small, given the amount of usable fea-tures.
Unfortunately, the amount of openly avail-able resources of minimally post-edited transla-tions are few, given the fact that this relies on acostly manual process usually done by profession-als.Consequently, we add more training samples,using reference translations of the source whichare not post-edited.
In order to ensure that the ad-ditional data still resemble minimally post-editedtranslations as required for HTER, we includethose additional sentences only if they match spe-cific similarity criteria.
In particular, the trans-lations are filtered, based on the amount of editsbetween the MT output and the reference transla-tion; sentences with an amount of edits above thethreshold are omitted.3 Methods3.1 Machine Learning on a regressionproblemFitting a statistical model in order to predict con-tinuous values is clearly a regression problem.
Thetask takes place on a sentence level, given a set offeatures describing the source and translation text,and the respective edit score for the particular sen-tence.For this purpose we use Support Vector Regres-sion - SVR (Basak et al., 2007), which uses lin-ear learning machines in order to map a non-linearfunction into a feature space induce by a high-dimensional kernel.
Similar to the baseline, theRBF kernel was used, whose parameters whereadjusted via a grid search, cross-validated (10folds) on all data that was available for each vari-ation of the training.3.2 FeaturesAs explained, the statistical model predicts theedit counts based on a set of features.
Our anal-ysis focuses on ?black-box?
features, which onlylook superficially on the given text and the pro-duced translation, without further knowledge onhow this translation was produced.
These featuresdepend on several automatic extraction mecha-nisms, mostly based on existing language process-ing tools.3.2.1 Baseline featuresA big set of features is adopted from the baselineof the Shared Task description:Language models: provide the smoothed n-gram probability and the n-gram perplexity of thesentence.Source frequency: A set of eight features in-cludes the percentage of uni-grams, bi-grams andtri-grams of the processed sentence in frequencyquartiles 1 (lower frequency words) and 4 (higherfrequency words) in the source side of a parallelcorpus (Callison-Burch et al., 2012).Count-based features include count and per-centage of tokens, unknown words, punctuationmarks, numbers, tokens which do or do not con-tain characters ?a-z?
; the absolute difference be-tween number of tokens in source and target nor-malized by source length, number of occurrences303of the target word within the target hypothesis av-eraged for all words in the hypothesis (type/tokenratio).3.2.2 Additional featuresAdditionally to the baseline features, the followingfeature groups are considered:Rule-based language correction is a result ofhand-written controlled language rules, that indi-cate mistakes on several pre-defined error cate-gories (Naber, 2003).
We include the number oferrors of each category as a feature.Parsing Features: We parse the text with aPCFG grammar (Petrov et al., 2006) and we de-rive the counts of all node labels (e.g.
count ofverb phrases, noun phrases etc.
), the parse log-likelihood and the number of the n-best parse treesgenerated (Avramidis et al., 2011).
In order to re-duce unnecessary noise, in some experiments weseparate a group of ?basic?
parsing labels, whichinclude only verb phrases, noun phrases, adjec-tives and subordinate clauses.Position statistics: This are derivatives of theprevious feature categories and focus on the po-sition of unknown words, or node tree tags.
Foreach of them, we calculate the average position in-dex over the sentence and the standard deviation ofthese indices.3.3 EvaluationAll specific model parameters were tested withcross validation with 10 equal folds on the train-ing data.
Cross validation is useful as it reducesthe possibility of overfitting, yet using the entireamount of data.The regression task is evaluated in terms ofMean Average Error (MAE).4 Experiment setup4.1 ImplementationThe open source language tool1is used to an-notate source and target sentences with automati-cally detected monolingual error tags.
Languagemodel features are computed with the SRILMtoolkit (Stolcke, 2002) with an order of 5, based onmonolingual training material from Europarl v7.0(Koehn, 2005) and News Commentary (Callison-Burch et al., 2011).
For the parsing parsing fea-tures we used the Berkeley Parser (Petrov and1Open source at http://languagetool.orgdatasets feature set MAEwmt14 baseline 0.142wmt14 all features 0.143wmt14,wmt13 baseline 0.140wmt14,wmt13 all features 0.138Table 1: Better scores are achieved when trainingwith both WMT14 and deduplicated WMT13 dataKlein, 2007) trained over an English and a Span-ish treebank (Taul?e et al., 2008).2Baseline fea-tures are extracted using Quest and HTER editsand scores are recalculated by modifying the orig-inal TERp code.
The annotation process is or-ganised with the Ruffus library (Goodstadt, 2010)and the learning algorithms are executed using theScikit Learn toolkit (Pedregosa et al., 2011).4.2 DataIn our effort to reproduce HTER in a higher gran-ularity, we noticed that HTER scoring on the of-ficial data was reversed: the calculation was per-formed by using the MT output as reference andthe human post-edition as hypothesis.
Therefore,the denominator on the ?official?
scores is thenumber of tokens on the MT output.
This makesthe prediction even easier, as this number of tokensis always known.Apart from the data provided by the WMT14,we include additional minimally post-edited datafrom WMT13.
It was observed that about 30% ofthe WMT13 data already occurred in the WMT14set.
Since this would negatively affect the credibil-ity of the cross-fold evaluation (section 3.3) andalso create duplicates, we filtered out incomingsentences with a string match higher than 85% tothe existing ones.The rest of the additional data (section 2.3)was extracted from the test-sets of shared tasksWMT2008-2011.5 Results5.1 Adding data from previous yearAdding deduplicated data from the HTER predic-tion task of WMT13 (Section 4.2) leads to an im-provement of about 0.004 of MAE for the bestfeature-set, as it can be seen by comparing the re-spective entries of the two horizontal blocks of Ta-ble 1.2although the Spanish grammar performed purely in thiscase and was eliminated as a feature304feature set MAEbaseline (b) 0.140b + language tool 0.141b + source parse 0.141b + parse pos 0.142b + basic parse pos 0.139b + parse count 0.139b + low prob trigram pos 0.139all without char count 0.139all without lang.
tool 0.139all features 0.138Table 2: Comparing models built with several dif-ferent feature sets, including various combinationsof the features described in section 3.2.
All modelstrained on combination of WMT14 and WMT13data5.2 Feature setsWe tested separately several feature sets, addition-ally to the baseline feature set and the feature setcontaining all features.
The feature sets testedare based on the feature categories explained inSection 3.2.2 and the results are seen in Table 2.One can see that there is little improvement on theMAE score, which is achieved best by using allfeatures.Adding individual categories of features on thebaseline has little effect.
Namely, the languagetool annotation, the source parse features and thesource and target parse positional features deteri-orate the MAE score, when added to the baselinefeatures.On the contrary, there is a small positive con-tribution by using the position statistics of onlythe ?basic?
parsing nodes (i.e.
noun phrases, verbphrases, adjectives and subordinate clauses).
Sim-ilarly positive is the effect of the count of parsednode labels for source and target and the featuresindicating the position of tri-grams with low prob-ability (lower than the deviation of the mean).
Al-though language tool features deteriorate the scoreof the baseline model when added, their absensehas a negative effect when compared to the fullfeature set.5.3 Separate vs. single HTER predictorTable 3 includes comparisons of models that testthe hypothesis mentioned in Section 2.1.
For bothmodels trained over the baseline or with additionalfeatures, the MAE score is higher (worse), whenfeatures mode MAE std +/-baseline single 0.140 0.012baseline combined 0.148 0.018baseline combined round 0.152 0.018all single 0.138 0.009all combined 0.160 0.019all combined round 0.162 0.020Table 3: The combination of 4 different estima-tors (combined) does not bring any improvement,when compared to the single HTER estimator.Models trained on both WMT14 and WMT13 dataseparate models are trained.
This indicates thatour hypothesis does not hold, at least for the cur-rent setting of learning method and feature sets.Rounding up individual edit type predictions to thecloses integer, before the calculation of the HTERratio, deteriorates the scores even more.5.4 Effect of adding non-postedited sentencesIn Table 4 we can see that adding more data, whichare not minimally post-edited (but normal refer-ences), does not contribute to a better model, evenif we limit the number of edits.
The lowest MAEis 0.176, when compared to the one of our bestmodel which is 0.138.The best score when additional sentences areimported, is achieved by allowing sentences thathave between up to edits, and particularly up to 3substitutions and up to 1 deletion.
Increasing thenumber of edits on more than 4, leads to a furtherdeterioration of the model.
One can also see thatadding training instances where MT outputs didnot require any edit, also yields scores worse thanthe baseline.6 Conclusion and further workIn our submission, we process the test set with themodel using all features (Table 2).
We addition-ally submit the model trained with additional fil-tered sentences, as indicated in the second row ofTable 4.One of the basic hypothesis of this experiment,that each edit type can better be learned individu-ally, was not confirmed given these data and set-tings.
Further work could include more focus onthe individual models and more elaborating onfeatures that may be specific for each error type.305del ins sub shifts total add.
sentences MAE std+/-0 0 0 0 0 275 0.177 0.0491 0 3 0 4 480 0.176 0.0401 0 2 0 3 433 0.177 0.0400 0 4 0 4 432 0.177 0.0402 1 0 0 3 296 0.177 0.0482 0 3 0 5 530 0.178 0.0384 0 2 0 6 485 0.178 0.0414 4 0 0 8 310 0.178 0.0462 1 0 1 4 309 0.178 0.0471 0 5 0 6 558 0.179 0.0391 4 5 0 10 1019 0.200 0.031Table 4: Indicative MAE scores achieved by adding filtered not minimally post-edited WMT translationReferencesEleftherios Avramidis, Maja Popovi?c, David Vilar, andAljoscha Burchardt.
2011.
Evaluate with Confi-dence Estimation : Machine ranking of translationoutputs using grammatical features.
In Proceedingsof the Sixth Workshop on Statistical Machine Trans-lation, pages 65?70, Edinburgh, Scotland, July.
As-sociation for Computational Linguistics.Debasish Basak, Srimanta Pal, and Dipak Chandra Pa-tranabis.
2007.
Support vector regression.
Neu-ral Information Processing-Letters and Reviews,11(10):203?224.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011Workshop on Statistical Machine Translation.
InProceedings of the Sixth Workshop on Statisti-cal Machine Translation, pages 22?64, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Leo Goodstadt.
2010.
Ruffus: a lightweight Pythonlibrary for computational pipelines.
Bioinformatics,26(21):2778?2779.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
Proceedings of thetenth Machine Translation Summit, 5:79?86.Daniel Naber.
2003.
A rule-based style and gram-mar checker.
Technical report, Bielefeld University,Bielefeld, Germany.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in python.
Journalof Machine Learning Research, 12:2825?2830.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofthe 2007 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, Rochester, New York.
Association forComputational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, andInterpretable Tree Annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440,Sydney, Australia, July.
Association for Computa-tional Linguistics.Matthew Snover, B Dorr, Richard Schwartz, L Micci-ulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProceedings of Association for Machine Translationin the Americas, pages 223?231.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Sev-enth International Conference on Spoken LanguageProcessing, pages 901?904.
ISCA, September.Mariona Taul?e, Ant`onia Mart?
?, and Marta Recasens.2008.
AnCora: Multilevel Annotated Corpora forCatalan and Spanish.
In Proceedings of the SixthInternational Conference on Language Resourcesand Evaluation (LREC?08), Marrakech, Morocco,May.
European Language Resources Association(ELRA).306
