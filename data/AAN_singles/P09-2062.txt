Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245?248,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPSyntax is from Mars while Semantics from Venus!Insights from Spectral Analysis of Distributional Similarity NetworksChris BiemannMicrosoft/Powerset, San FranciscoChris.Biemann@microsoft.comMonojit ChoudhuryMicrosoft Research Lab Indiamonojitc@microsoft.comAnimesh MukherjeeIndian Institute of Technology Kharagpur, Indiaanimeshm@cse.iitkgp.ac.inAbstractWe study the global topology of the syn-tactic and semantic distributional similar-ity networks for English through the tech-nique of spectral analysis.
We observe thatwhile the syntactic network has a hierar-chical structure with strong communitiesand their mixtures, the semantic networkhas several tightly knit communities alongwith a large core without any such well-defined community structure.1 IntroductionSyntax and semantics are two tightly coupled, yetvery different properties of any natural language?
as if one is from ?Mars?
and the other from?Venus?.
Indeed, this exploratory work shows thatthe distributional properties of syntax are quite dif-ferent from those of semantics.
Distributional hy-pothesis states that the words that occur in thesame contexts tend to have similar meanings (Har-ris, 1968).
Using this hypothesis, one can define avector space model for words where every wordis a point in some n-dimensional space and thedistance between them can be interpreted as theinverse of the semantic or syntactic similarity be-tween their corresponding distributional patterns.Usually, the co-occurrence patterns with respect tothe function words are used to define the syntacticcontext, whereas that with respect to the contentwords define the semantic context.
An alternative,but equally popular, visualization of distributionalsimilarity is through graphs or networks, whereeach word is represented as nodes and weightededges indicate the extent of distributional similar-ity between them.What are the commonalities and differences be-tween the syntactic and semantic distributionalpatterns of the words of a language?
This study isan initial attempt to answer this fundamental andintriguing question, whereby we construct the syn-tactic and semantic distributional similarity net-work (DSN) and analyze their spectrum to un-derstand their global topology.
We observe thatthere are significant differences between the twonetworks: the syntactic network has well-definedhierarchical community structure implying a sys-tematic organization of natural classes and theirmixtures (e.g., words which are both nouns andverbs); on the other hand, the semantic networkhas several isolated clusters or the so called tightlyknit communities and a core component that lacksa clear community structure.
Spectral analysisalso reveals the basis of formation of the natu-ral classes or communities within these networks.These observations collectively point towards awell accepted fact that the semantic space of nat-ural languages has extremely high dimension withno clearly observable subspaces, which makes the-orizing and engineering harder compared to itssyntactic counterpart.Spectral analysis is the backbone of severaltechniques, such as multi-dimensional scaling,principle component analysis and latent semanticanalysis, that are commonly used in NLP.
In re-cent times, there have been some work on spec-tral analysis of linguistic networks as well.
Belkinand Goldsmith (2002) applied spectral analysis tounderstand the struture of morpho-syntactic net-works of English words.
The current work, onthe other hand, is along the lines of Mukherjee etal.
(2009), where the aim is to understand not onlythe principles of organization, but also the globaltopology of the network through the study of thespectrum.
The most important contribution here,however, lies in the comparison of the topologyof the syntactic and semantic DSNs, which, to thebest of our knowledge, has not been explored pre-viously.2452 Network ConstructionThe syntactic and semantic DSNs are constructedfrom a raw text corpus.
This work is restricted tothe study of English DSNs only1.Syntactic DSN: We define our syntactic net-work in a similar way as previous works in unsu-pervised parts-of-speech induction (cf.
(Sch?utze,1995; Biemann, 2006)): The most frequent 200words in the corpus (July 2008 dump of EnglishWikipedia) are used as features in a word windowof ?2 around the target words.
Thus, each targetword is described by an 800-dimensional featurevector, containing the number of times we observeone of the most frequent 200 words in the respec-tive positions relative to the target word.
In ourexperiments, we collect data for the most frequent1000 and 5000 target words, arguing that all syn-tactic classes should be represented in those.
Asimilarity measure between target words is definedby the cosine between the feature vectors.
Thesyntactic graph is formed by inserting the targetwords as nodes and connecting nodes with edgeweights equal to their cosine similarity if this sim-ilarity exceeds a threshold t = 0.66.Semantic DSN: The construction of this net-work is inspired by (Lin, 1998).
Specifically,we parsed a dump of English Wikipedia (July2008) with the XLE parser (Riezler et al, 2002)and extracted the following dependency relationsfor nouns: Verb-Subject, Verb-Object, Noun-coordination, NN-compound, Adj-Mod.
Theselexicalized relations act as features for the nouns.Verbs are recorded together with their subcatego-rization frame, i.e.
the same verb lemmas in dif-ferent subcat frames would be treated as if theywere different verbs.
We compute log-likelihoodsignificance between features and target nouns (asin (Dunning, 1993)) and keep only the most signif-icant 200 features per target word.
Each feature fgets a feature weight that is inversely proportionalto the logarithm of the number of target words itapplies on.
The similarity of two target nouns isthen computed as the sum of the feature weightsthey share.
For our analysis, we restrict the graphto the most frequent 5000 target common nounsand keep only the 200 highest weighted edges pertarget noun.
Note that the degree of a node can1As shown in (Nath et al, 2008), the basic structureof these networks are insensitive to minor variations in theparameters (e.g., thresholds and number of words) and thechoice of distance metric.Figure 1: The spectrum of the syntactic and se-mantic DSNs of 1000 nodes.still be larger than 200 if this node is contained inmany 200 highest weighted edges of other targetnouns.3 Spectrum of DSNsSpectral analysis refers to the systematic study ofthe eigenvalues and eigenvectors of a network.
Al-though here we study the spectrum of the adja-cency matrix of the weighted networks, it is alsoquite common to study the spectrum of the Lapla-cian of the adjacency matrix (see for example,Belkin and Goldsmith (2002)).
Fig.
1 comparesthe spectrum of the syntactic and semantic DSNswith 1000 nodes, which has been computed as fol-lows.
First, the 1000 eigenvalues of the adjacencymatrix are sorted in descending order.
Then wecompute the spectral coverage till the ith eigen-value by adding the squares of the first i eigenval-ues and normalizing it by the sum of the squaresof all the eigenvalues - a quantity also known asthe Frobenius norm of the matrix.We observe that for the semantic DSN the first10 eigenvalues cover only 40% of the spectrumand the first 500 together make up 75% of thespectrum.
On the other hand, for the syntacticDSN, the first 10 eigenvalues cover 75% of thespectrum while the first 20 covers 80%.
In otherwords, the structure of the syntactic DSN is gov-erned by a few (order of 10) significant principles,whereas that of the semantic DSN is controlled bya large number of equally insignificant factors.The aforementioned observation has the fol-lowing alternative, but equivalent interpretations:(a) the syntactic DSN can be clustered in lowerdimensions (e.g., 10 or 20) because, most ofthe rows in the matrix can be approximately ex-pressed as a linear combination of the top 10 to 20246Figure 2: Plot of corpus frequency based rank vs.eigenvector centrality of the words in the DSNs of5000 nodes.eigenvectors.
Furthermore, the graceful decay ofthe eigenvalues of the syntactic DSN implies theexistence of a hierarchical community structure,which has been independently verified by Nath etal.
(2008) through analysis of the degree distribu-tion of such networks; and (b) a random walk con-ducted on the semantic DSN will have a high ten-dency to drift away very soon from the semanticclass of the starting node, whereas in the syntacticDSN, the random walk is expected to stay withinthe same syntactic class for a long time.
There-fore, it is reasonable to advocate that characteriza-tion and processing of syntatic classes is far lessconfusing than that of the semantic classes ?
a factthat requires no emphasis.4 Eigenvector AnalysisThe first eigenvalue tells us to what extent therows of the adjacency matrix are correlated andtherefore, the corresponding eigenvector is not adimension pointing to any classificatory basis ofthe words.
However, as we shall see shortly, theother eigenvectors corresponding to the signifi-cantly high eigenvalues are important classifica-tory dimensions.Fig 2 shows the plot of the first eigenvectorcomponent (aka eigenvector centrality) of a wordversus its rank based on the corpus frequency.
Weobserve that the very high frequency (i.e., lowrank) nodes in both the networks have low eigen-vector centrality, whereas the medium frequencynodes display a wide range of centrality values.However, the most striking difference between thenetworks is that while in the syntactic DSN thecentrality values are approximately normally dis-tributed for the medium frequency words, the leastfrequent words enjoy the highest centrality for thesemantic DSN.
Furthermore, we observe that themost central nodes in the semantic DSN corre-spond to semantically unambiguous words of sim-ilar nature (e.g., deterioration, abandonment, frag-mentation, turmoil).
This indicates the existenceof several ?tightly knit communities consisting ofnot so high frequency words?
which pull in a sig-nificant fraction of the overall centrality.
Sincethe high frequency words are usually polysemous,they on the other hand form a large, but non-cliqueish structure at the core of the network witha few connections to the tightly knit communities.This is known as the tightly knit community ef-fect (TKC effect) that renders very low central-ity values to the ?truly?
central nodes of the net-work (Lempel and Moran, 2000).
The structureof the syntactic DSN, however, is not governed bythe TKC effect to such an extreme extent.
Hence,one can expect to easily identify the natural classesof the syntactic DSN, but not its semantic counter-part.In fact, this observation is further corroboratedby the higher eigenvectors.
Fig.
3 shows the plotof the second eigenvector component versus thefourth one for the two DSNs consisting of 5000words.
It is observed that for the syntactic net-work, the words get neatly clustered into two setscomprised of words with the positive and negativesecond eigenvector components.
The same plotfor the semantic DSN shows that a large number ofwords have both the components close to zero andonly a few words stand out on one side of the axes?
those with positive second eigenvector compo-nent and those with negative fourth eigenvectorcomponent.
In essence, none of these eigenvec-tors can neatly classify the words into two sets ?a trend which is observed for all the higher eigen-vectors (we conducted experiments for up to thetwentieth eigenvector).Study of the individual eignevectors further re-veals that the nodes with either the extreme pos-itive or the extreme negative components havestrong linguistic correlates.
For instance, in thesyntactic DSN, the two ends of the second eigen-247Figure 3: Plot of the second vs. fourth eigenvectorcomponents of the words in the DSNs.vector correspond to nouns and adjectives; one ofthe ends of the fourth, fifth, sixth and the twelftheigenvectors respectively correspond to locationnouns, prepositions, first names and initials, andverbs.
In the semantic DSN, one of the ends ofthe second, third, fourth and tenth eigenvectorsrespectively correspond to professions, abstractterms, food items and body parts.
One would ex-pect that the higher eigenvectors (say the 50thone)would show no clear classificatory basis for thesyntactic DSN, while for the semantic DSN thosecould be still associated with prominent linguisticcorrelates.5 Conclusion and Future WorkHere, we presented some initial investigations intothe nature of the syntactic and semantic DSNsthrough the method of spectral analysis, wherebywe could observe that the global topology of thetwo networks are significantly different in termsof the organization of their natural classes.
Whilethe syntactic DSN seems to exhibit a hierarchi-cal structure with a few strong natural classes andtheir mixtures, the semantic DSN is composed ofseveral tightly knit small communities along witha large core consisting of very many smaller ill-defined and ambiguous sets of words.
To visual-ize, one could draw an analogy of the syntacticand semantic DSNs respectively to ?crystalline?and ?amorphous?
solids.This work can be furthered in several directions,such as, (a) testing the robustness of the findingsacross languages, different network constructionpolicies, and corpora of different sizes and fromvarious domains; (b) clustering of the words on thebasis of eigenvector components and using them inNLP applications such as unsupervised POS tag-ging and WSD; and (c) spectral analysis of Word-Net and other manually constructed ontologies.AcknowledgementCB and AM are grateful to Microsoft ResearchIndia, respectively for hosting him while this re-search was conducted, and financial support.ReferencesM.
Belkin and J. Goldsmith 2002.
Using eigenvec-tors of the bigram graph to infer morpheme identity.In Proceedings of the ACL-02Workshop onMorpho-logical and Phonological Learning, pages 4147, As-sociation for Computational Linguistics.Chris Biemann 2006.
Unsupervised part-of-speechtagging employing efficient graph clustering.
InProceedings of the COLING/ACL-06 Student Re-search Workshop.Ted Dunning 1993.
Accurate methods for the statis-tics of surprise and coincidence.
In ComputationalLinguistics 19, 1, pages 61?74Z.S.
Harris 1968.
Mathematical Structures of Lan-guage.
Wiley, New York.R.
Lempel and S. Moran 2000.
The stochastic ap-proach for link-structure analysis (SALSA) and theTKC effect.
In Computer Networks, 33, pages 387-401Dekang Lin 1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING?98.Animesh Mukherjee, Monojit Choudhury and RaviKannan 2009.
Discovering Global Patterns in Lin-guistic Networks through Spectral Analysis: A CaseStudy of the Consonant Inventories.
In The Pro-ceedings of EACL 2009, pages 585-593.Joydeep Nath, Monojit Choudhury, Animesh Mukher-jee, Christian Biemann and Niloy Ganguly 2008.Unsupervised parts-of-speech induction for Bengali.In The Proceedings of LREC?08, ELRA.S.
Riezler, T.H.
King, R.M.
Kaplan, R. Crouch, J.T.Maxwell, M. Johnson 2002.
Parsing the Wall StreetJournal using a lexical-functional grammar and dis-criminative estimation techniques.
In Proceedingsof the 40th Annual Meeting of the ACL, pages 271-278.Hinrich Sch?utze 1995.
Distributional part-of-speechtagging.
In Proceedings of EACL, pages 141-148.248
