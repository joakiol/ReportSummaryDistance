Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468?476,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPParaphrase Identification as Probabilistic Quasi-Synchronous RecognitionDipanjan Das and Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{dipanjan,nasmith}@cs.cmu.eduAbstractWe present a novel approach to decid-ing whether two sentences hold a para-phrase relationship.
We employ a gen-erative model that generates a paraphraseof a given sentence, and we use proba-bilistic inference to reason about whethertwo sentences share the paraphrase rela-tionship.
The model cleanly incorporatesboth syntax and lexical semantics usingquasi-synchronous dependency grammars(Smith and Eisner, 2006).
Furthermore,using a product of experts (Hinton, 2002),we combine the model with a comple-mentary logistic regression model basedon state-of-the-art lexical overlap features.We evaluate our models on the task ofdistinguishing true paraphrase pairs fromfalse ones on a standard corpus, givingcompetitive state-of-the-art performance.1 IntroductionThe problem of modeling paraphrase relation-ships between natural language utterances (McK-eown, 1979) has recently attracted interest.
Forcomputational linguists, solving this problem mayshed light on how best to model the semanticsof sentences.
For natural language engineers, theproblem bears on information management sys-tems like abstractive summarizers that must mea-sure semantic overlap between sentences (Barzi-lay and Lee, 2003), question answering modules(Marsi and Krahmer, 2005) and machine transla-tion (Callison-Burch et al, 2006).The paraphrase identification problem askswhether two sentences have essentially the samemeaning.
Although paraphrase identification isdefined in semantic terms, it is usually solved us-ing statistical classifiers based on shallow lexical,n-gram, and syntactic ?overlap?
features.
Suchoverlap features give the best-published classifi-cation accuracy for the paraphrase identificationtask (Zhang and Patrick, 2005; Finch et al, 2005;Wan et al, 2006; Corley and Mihalcea, 2005, in-ter alia), but do not explicitly model correspon-dence structure (or ?alignment?)
between the partsof two sentences.
In this paper, we adopt a modelthat posits correspondence between the words inthe two sentences, defining it in loose syntacticterms: if two sentences are paraphrases, we expecttheir dependency trees to align closely, thoughsome divergences are also expected, with somemore likely than others.
Following Smith and Eis-ner (2006), we adopt the view that the syntacticstructure of sentences paraphrasing some sentences should be ?inspired?
by the structure of s.Because dependency syntax is still only a crudeapproximation to semantic structure, we augmentthe model with a lexical semantics component,based on WordNet (Miller, 1995), that models howwords are probabilistically altered in generatinga paraphrase.
This combination of loose syntaxand lexical semantics is similar to the ?Jeopardy?model of Wang et al (2007).This syntactic framework represents a major de-parture from useful and popular surface similarityfeatures, and the latter are difficult to incorporateinto our probabilistic model.
We use a product ofexperts (Hinton, 2002) to bring together a logis-tic regression classifier built from n-gram overlapfeatures and our syntactic model.
This combinedmodel leverages complementary strengths of thetwo approaches, outperforming a strong state-of-the-art baseline (Wan et al, 2006).This paper is organized as follows.
We intro-duce our probabilistic model in ?2.
The modelmakes use of three quasi-synchronous grammarmodels (Smith and Eisner, 2006, QG, hereafter) ascomponents (one modeling paraphrase, one mod-eling not-paraphrase, and one a base grammar);these are detailed, along with latent-variable in-ference and discriminative training algorithms, in?3.
We discuss the Microsoft Research ParaphraseCorpus, upon which we conduct experiments, in?4.
In ?5, we present experiments on paraphrase468identification with our model and make compar-isons with the existing state-of-the-art.
We de-scribe the product of experts and our lexical over-lap model, and discuss the results achieved in ?6.We relate our approach to prior work (?7) and con-clude (?8).2 Probabilistic ModelSince our task is a classification problem, we re-quire our model to provide an estimate of the pos-terior probability of the relationship (i.e., ?para-phrase,?
denoted p, or ?not paraphrase,?
denotedn), given the pair of sentences.1 Here, pQ denotesmodel probabilities, c is a relationship class (p orn), and s1 and s2 are the two sentences.
We choosethe class according to:c?
= argmaxc?
{p,n}pQ(c | s1, s2)= argmaxc?{p,n}pQ(c)?
pQ(s1, s2 | c) (1)We define the class-conditional probabilities ofthe two sentences using the following generativestory.
First, grammar G0 generates a sentence s.Then a class c is chosen, corresponding to a class-specific probabilistic quasi-synchronous grammarGc.
(We will discuss QG in detail in ?3.
For thepresent, consider it a specially-defined probabilis-tic model that generates sentences with a specificproperty, like ?paraphrases s,?
when c = p.) Givens, Gc generates the other sentence in the pair, s?.When we observe a pair of sentences s1 and s2we do not presume to know which came first (i.e.,which was s and which was s?).
Both orderingsare assumed to be equally probable.
For class c,pQ(s1, s2 | c) =0.5?
pQ(s1 | G0)?
pQ(s2 | Gc(s1))+ 0.5?
pQ(s2 | G0)?
pQ(s1 | Gc(s2))(2)where c can be p or n; Gp(s) is the QG that gen-erates paraphrases for sentence s, while Gn(s) isthe QG that generates sentences that are not para-phrases of sentence s. This latter model may seemcounter-intuitive: since the vast majority of pos-sible sentences are not paraphrases of s, why is aspecial grammar required?
Our use of a Gn fol-lows from the properties of the corpus currentlyused for learning, in which the negative examples1Although we do not explore the idea here, the modelcould be adapted for other sentence-pair relationships like en-tailment or contradiction.were selected to have high lexical overlap.
We re-turn to this point in ?4.3 QG for Paraphrase ModelingHere, we turn to the models Gp and Gn in detail.3.1 BackgroundSmith and Eisner (2006) introduced the quasi-synchronous grammar formalism.
Here, we de-scribe some of its salient aspects.
The modelarose out of the empirical observation that trans-lated sentences have some isomorphic syntacticstructure, but divergences are possible.
Therefore,rather than an isomorphic structure over a pair ofsource and target sentences, the syntactic tree overa target sentence is modeled by a source sentence-specific grammar ?inspired?
by the source sen-tence?s tree.
This is implemented by associatingwith each node in the target tree a subset of thenodes in the source tree.
Since it loosely linksthe two sentences?
syntactic structures, QG is wellsuited for problems like word alignment for MT(Smith and Eisner, 2006) and question answering(Wang et al, 2007).Consider a very simple quasi-synchronouscontext-free dependency grammar that generatesone dependent per production rule.2 Let s =?s1, ..., sm?
be the source sentence.
The grammarrules will take one of the two forms:?t, l?
?
?t, l?
?t?, k?
or ?t, l?
?
?t?, k?
?t, l?where t and t?
range over the vocabulary of thetarget language, and l and k ?
{0, ...,m} are in-dices in the source sentence, with 0 denoting null.3Hard or soft constraints can be applied between land k in a rule.
These constraints imply permissi-ble ?configurations.?
For example, requiring l 6= 0and, if k 6= 0 then sk must be a child of sl in thesource tree, we can implement a synchronous de-pendency grammar similar to (Melamed, 2004).Smith and Eisner (2006) used a quasi-synchronous grammar to discover the correspon-dence between words implied by the correspon-dence between the trees.
We follow Wang et al(2007) in treating the correspondences as latentvariables, and in using a WordNet-based lexicalsemantics model to generate the target words.2Our actual model is more complicated; see ?3.2.3A more general QG could allow one-to-many align-ments, replacing l and k with sets of indices.4693.2 Detailed ModelWe describe how we model pQ(t | Gp(s)) andpQ(t | Gn(s)) for source and target sentences sand t (appearing in Eq.
2 alternately as s1 and s2).A dependency tree on a sequence w =?w1, ..., wk?
is a mapping of indices of words toindices of syntactic parents, ?p : {1, ..., k} ?
{0, ..., k}, and a mapping of indices of words todependency relation types in L, ?` : {1, ..., k} ?L.
The set of indices children of wi to its left,{j : ?w(j) = i, j < i}, is denoted ?w(i), and?w(i) is used for right children.
wi has a singleparent, denoted by w?p(i).
Cycles are not allowed,and w0 is taken to be the dummy ?wall?
symbol,$, whose only child is the root word of the sen-tence (normally the main verb).
The label for wiis denoted by ?`(i).
We denote the whole tree ofa sentence w by ?w, the subtree rooted at the ithword by ?w,i.Consider two sentences: let the source sen-tence s contain m words and the target sentencet contain n words.
Let the correspondence x :{1, ..., n} ?
{0, ...,m} be a mapping from in-dices of words in t to indices of words in s. (Werequire each target word to map to at most onesource word, though multiple target words canmap to the same source word, i.e., x(i) = x(j)while i 6= j.)
When x(i) = 0, the ith target wordmaps to the wall symbol, equivalently a ?null?word.
Each of our QGs Gp and Gn generates thealignments x, the target tree ?
t, and the sentencet.
Both Gp and Gn are structured in the same way,differing only in their parameters; henceforth wediscuss Gp; Gn is similar.We assume that the parse trees of s and t areknown.4 Therefore our model defines:pQ(t | Gp(s)) = p(?t | Gp(?s))=?x p(?t, x | Gp(?
s)) (3)Because the QG is essentially a context-free de-pendency grammar, we can factor it into recur-sive steps as follows (let i be an arbitrary indexin {1, ..., n}):P (?
t,i | ti, x(i), ?s) = pval (|?t(i)|, |?t(i)| | ti)4In our experiments, we use the parser described by Mc-Donald et al (2005), trained on sections 2?21 of the WSJPenn Treebank, transformed to dependency trees followingYamada and Matsumoto (2003).
(The same treebank datawere also to estimate many of the parameters of our model, asdiscussed in the text.)
Though it leads to a partial ?pipeline?approximation of the posterior probability p(c | s, t), we be-lieve that the relatively high quality of English dependencyparsing makes this approximation reasonable.??j??t(i)?
?t(i)m?x(j)=0P (?
t,j | tj , x(j), ?s)?pkid (tj , ?t` (j), x(j) | ti, x(i), ?s) (4)where pval and pkid are valence and child-production probabilities parameterized as dis-cussed in ?3.4.
Note the recursion in the second-to-last line.We next describe a dynamic programming so-lution for calculating p(?
t | Gp(?
s)).
In ?3.4 wediscuss the parameterization of the model.3.3 Dynamic ProgrammingLet C(i, l) refer to the probability of ?
t,i, assum-ing that the parent of ti, t?tp(i), is aligned to sl.
Forleaves of ?
t, the base case is:C(i, l) = pval (0, 0 | ti)?
(5)?mk=0 pkid (ti, ?t` (i), k | t?tp(i), l, ?s)where k ranges over possible values of x(i), thesource-tree node to which ti is aligned.
The recur-sive case is:C(i, l) = pval (|?t(i)|, |?t(i)| | ti) (6)?
?mk=0 pkid (ti, ?t` (i), k | t?tp(i), l, ?s)??j??t(i)?
?t(i)C(j, k)We assume that the wall symbols t0 and s0 arealigned, so p(?
t | Gp(?
s)) = C(r, 0), where r isthe index of the root word of the target tree ?
t. Itis straightforward to show that this algorithm re-quires O(m2n) runtime and O(mn) space.3.4 ParameterizationThe valency distribution pval in Eq.
4 is estimatedin our model using the transformed treebank (seefootnote 4).
For unobserved cases, the conditionalprobability is estimated by backing off to the par-ent POS tag and child direction.We discuss next how to parameterize the prob-ability pkid that appears in Equations 4, 5, and 6.This conditional distribution forms the core of ourQGs, and we deviate from earlier research usingQGs in defining pkid in a fully generative way.In addition to assuming that dependency parsetrees for s and t are observable, we also assumeeach word wi comes with POS and named entitytags.
In our experiments these were obtained au-tomatically using MXPOST (Ratnaparkhi, 1996)and BBN?s Identifinder (Bikel et al, 1999).470For clarity, let j = ?
tp(i) and let l = x(j).pkid(ti, ?t` (i), x(i) | tj , l, ?s) =pconfig(config(ti, tj , sx(i), sl) | tj , l, ?s) (7)?punif (x(i) | config(ti, tj , sx(i), sl)) (8)?plab(?t` (i) | config(ti, tj , sx(i), sl)) (9)?ppos(pos(ti) | pos(sx(i))) (10)?pne(ne(ti) | ne(sx(i))) (11)?plsrel (lsrel(ti) | sx(i)) (12)?pword (ti | lsrel(ti), sx(i)) (13)We consider each of the factors above in turn.Configuration In QG, ?configurations?
refer tothe tree relationship among source-tree nodes(above, sl and sx(i)) aligned to a pair of parent-child target-tree nodes (above, tj and ti).
In deriv-ing ?
t,j , the model first chooses the configurationthat will hold among ti, tj , sx(i) (which has yetto be chosen), and sl (line 7).
This is defined forconfiguration c log-linearly by:5pconfig(c | tj , l, ?s) =?c?c?
:?sk,config(ti,tj ,sk,sl)=c??c?
(14)Permissible configurations in our model are shownin Table 1.
These are identical to prior work(Smith and Eisner, 2006; Wang et al, 2007),except that we add a ?root?
configuration thataligns the target parent-child pair to null and thehead word of the source sentence, respectively.Using many permissible configurations helps re-move negative effects from noisy parses, whichour learner treats as evidence.
Fig.
1 shows someexamples of major configurations that Gp discov-ers in the data.Source tree alignment After choosing the config-uration, the specific node in ?
s that ti will alignto, sx(i) is drawn uniformly (line 8) from amongthose in the configuration selected.Dependency label, POS, and named entity classThe newly generated target word?s dependencylabel, POS, and named entity class drawn frommultinomial distributions plab , ppos , and pne thatcondition, respectively, on the configuration andthe POS and named entity class of the alignedsource-tree word sx(i) (lines 9?11).5We use log-linear models three times: for the configura-tion, the lexical semantics class, and the word.
Each time,we are essentially assigning one weight per outcome andrenormalizing among the subset of outcomes that are possiblegiven what has been derived so far.Configuration Descriptionparent-child ?
sp(x(i)) = x(j), appended with ?s` (x(i))child-parent x(i) = ?
sp(x(j)), appended with ?s` (x(j))grandparent-grandchild?
sp(?sp(x(i))) = x(j), appended with?
s` (x(i))siblings ?
sp(x(i)) = ?sp(x(j)), x(i) 6= x(j)same-node x(i) = x(j)c-command the parent of one source-side word is anancestor of the other source-side wordroot x(j) = 0, x(i) is the root of schild-null x(i) = 0parent-null x(j) = 0, x(i) is something other thanroot of sother catch-all for all other types of configura-tions, which are permittedTable 1: Permissible configurations.
i is an index in t whoseconfiguration is to be chosen; j = ?
tp(i) is i?s parent.WordNet relation(s) The model next chooses alexical semantics relation between sx(i) and theyet-to-be-chosen word ti (line 12).
FollowingWang et al (2007),6 we employ a 14-feature log-linear model over all logically possible combina-tions of the 14 WordNet relations (Miller, 1995).7Similarly to Eq.
14, we normalize this log-linearmodel based on the set of relations that are non-empty in WordNet for the word sx(i).Word Finally, the target word is randomly chosenfrom among the set of words that bear the lexicalsemantic relationship just chosen (line 13).
Thisdistribution is, again, defined log-linearly:pword (ti | lsrel(ti) = R, sx(i)) =?ti?w?:sx(i)Rw?
?w?
(15)Here ?w is the Good-Turing unigram probabilityestimate of a word w from the Gigaword corpus(Graff, 2003).3.5 Base Grammar G0In addition to the QG that generates a second sen-tence bearing the desired relationship (paraphraseor not) to the first sentence s, our model in ?2 alsorequires a base grammar G0 over s.We view this grammar as a trivial special caseof the same QG model already described.
G0 as-sumes the empty source sentence consists only of6Note that Wang et al (2007) designed pkid as an inter-polation between a log-linear lexical semantics model and aword model.
Our approach is more fully generative.7These are: identical-word, synonym, antonym (includ-ing extended and indirect antonym), hypernym, hyponym,derived form, morphological variation (e.g., plural form),verb group, entailment, entailed-by, see-also, causal relation,whether the two words are same and is a number, and no re-lation.471(a) parent-childfillquestionnairecompletequestionnairedozenswoundedinjureddozens(b) child-parent (c) grandparent-grandchildwillchiefwillSecretaryLiscouskiquarterfirstfirst-quarter(e) same-nodeU.Srefundingmassive(f) siblingsU.Streasurytreasury(g) rootnullfellnulldropped(d) c-commandsignaturesnecessarysignaturesneeded897,158thetwiceapproachingcollectedFigure 1: Some example configurations from Table 1 that Gp discovers in the dev.
data.
Directed arrows show head-modifierrelationships, while dotted arrows show alignments.a single wall node.
Thus every word generated un-der G0 aligns to null, and we can simplify the dy-namic programming algorithm that scores a tree?
s under G0:C ?
(i) = pval (|?t(i)|, |?t(i)| | si)?plab(?t` (i))?
ppos(pos(ti))?
pne(ne(ti))?pword(ti)??j:?t(j)=iC?
(j) (16)where the final product is 1 when ti has no chil-dren.
It should be clear that p(s | G0) = C ?
(0).We estimate the distributions over dependencylabels, POS tags, and named entity classes usingthe transformed treebank (footnote 4).
The dis-tribution over words is taken from the Gigawordcorpus (as in ?3.4).It is important to note thatG0 is designed to givea smoothed estimate of the probability of a partic-ular parsed, named entity-tagged sentence.
It isnever used for parsing or for generation; it is onlyused as a component in the generative probabilitymodel presented in ?2 (Eq.
2).3.6 Discriminative TrainingGiven training data?
?s(i)1 , s(i)2 , c(i)?
?Ni=1, we trainthe model discriminatively by maximizing regu-larized conditional likelihood:max?N?i=1log pQ(c(i) | s(i)1 , s(i)2 ,?)?
??
?Eq.
2 relates this to G{0,p,n}?C??
?22(17)The parameters ?
to be learned include the classpriors, the conditional distributions of the depen-dency labels given the various configurations, thePOS tags given POS tags, the NE tags given NEtags appearing in expressions 9?11, the configura-tion weights appearing in Eq.
14, and the weightsof the various features in the log-linear model forthe lexical-semantics model.
As noted, the distri-butions pval , the word unigram weights in Eq.
15,and the parameters of the base grammar are fixedusing the treebank (see footnote 4) and the Giga-word corpus.Since there is a hidden variable (x), the objec-tive function is non-convex.
We locally optimizeusing the L-BFGS quasi-Newton method (Liu andNocedal, 1989).
Because many of our parametersare multinomial probabilities that are constrainedto sum to one and L-BFGS is not designed to han-dle constraints, we treat these parameters as un-normalized weights that get renormalized (using asoftmax function) before calculating the objective.4 Data and TaskIn all our experiments, we have used the Mi-crosoft Research Paraphrase Corpus (Dolan et al,2004; Quirk et al, 2004).
The corpus contains5,801 pairs of sentences that have been markedas ?equivalent?
or ?not equivalent.?
It was con-structed from thousands of news sources on theweb.
Dolan and Brockett (2005) remark thatthis corpus was created semi-automatically by firsttraining an SVM classifier on a disjoint annotated10,000 sentence pair dataset and then applyingthe SVM on an unseen 49,375 sentence pair cor-pus, with its output probabilities skewed towardsover-identification, i.e., towards generating somefalse paraphrases.
5,801 out of these 49,375 pairswere randomly selected and presented to humanjudges for refinement into true and false para-phrases.
3,900 of the pairs were marked as having472About 120 potential jurors were being asked to complete a lengthy questionnaire .The jurors were taken into the courtroom in groups of 40 and asked to fill out a questionnaire .Figure 2: Discovered alignment of Ex.
19 produced by Gp.
Observe that the model aligns identical words and also ?complete?and ?fill?
in this specific case.
This kind of alignment provides an edge over a simple lexical overlap model.
?mostly bidirectional entailment,?
a standard def-inition of the paraphrase relation.
Each sentencewas labeled first by two judges, who averaged 83%agreement, and a third judge resolved conflicts.We use the standard data split into 4,076 (2,753paraphrase, 1,323 not) training and 1,725 (1147paraphrase, 578 not) test pairs.
We reserved a ran-domly selected 1,075 training pairs for tuning.Wecite some examples from the training set here:(18) Revenue in the first quarter of the year dropped 15percent from the same period a year earlier.With the scandal hanging over Stewart?s company,revenue in the first quarter of the year dropped 15percent from the same period a year earlier.
(19) About 120 potential jurors were being asked tocomplete a lengthy questionnaire.The jurors were taken into the courtroom in groups of40 and asked to fill out a questionnaire.Ex.
18 is a true paraphrase pair.
Notice the highlexical overlap between the two sentences (uni-gram overlap of 100% in one direction and 72%in the other).
Ex.
19 is another true paraphrasepair with much lower lexical overlap (unigramoverlap of 50% in one direction and 30% in theother).
Notice the use of similar-meaning phrasesand irrelevant modifiers that retain the same mean-ing in both sentences, which a lexical overlapmodel cannot capture easily, but a model like a QGmight.
Also, in both pairs, the relationship cannotbe called total bidirectional equivalence becausethere is some extra information in one sentencewhich cannot be inferred from the other.Ex.
20 was labeled ?not paraphrase?
:(20) ?There were a number of bureaucratic andadministrative missed signals - there?s not one personwho?s responsible here,?
Gehman said.In turning down the NIMA offer, Gehman said, ?therewere a number of bureaucratic and administrativemissed signals here.There is significant content overlap, making a de-cision difficult for a na?
?ve lexical overlap classifier.
(In fact, pQ labels this example n while the lexicaloverlap models label it p.)The fact that negative examples in this corpuswere selected because of their high lexical over-lap is important.
It means that any discrimina-tive model is expected to learn to distinguish mereoverlap from paraphrase.
This seems appropriate,but it does mean that the ?not paraphrase?
relationought to be denoted ?not paraphrase but decep-tively similar on the surface.?
It is for this reasonthat we use a special QG for the n relation.5 Experimental EvaluationHere we present our experimental evaluation usingpQ.
We trained on the training set (3,001 pairs)and tuned model metaparameters (C in Eq.
17)and the effect of different feature sets on the de-velopment set (1,075 pairs).
We report accuracyon the official MSRPC test dataset.
If the poste-rior probability pQ(p | s1, s2) is greater than 0.5,the pair is labeled ?paraphrase?
(as in Eq.
1).5.1 BaselineWe replicated a state-of-the-art baseline model forcomparison.
Wan et al (2006) report the best pub-lished accuracy, to our knowledge, on this task,using a support vector machine.
Our baseline isa reimplementation of Wan et al (2006), usingfeatures calculated directly from s1 and s2 with-out recourse to any hidden structure: proportionof word unigram matches, proportion of lemma-tized unigram matches, BLEU score (Papineni etal., 2001), BLEU score on lemmatized tokens, Fmeasure (Turian et al, 2003), difference of sen-tence length, and proportion of dependency rela-tion overlap.
The SVM was trained to classifypositive and negative examples of paraphrase us-ing SVMlight (Joachims, 1999).8 Metaparameters,tuned on the development data, were the regu-larization constant and the degree of the polyno-mial kernel (chosen in [10?5, 102] and 1?5 respec-tively.
).9It is unsurprising that the SVM performs verywell on the MSRPC because of the corpus creationprocess (see Sec.
4) where an SVM was appliedas well, with very similar features and a skeweddecision process (Dolan and Brockett, 2005).8http://svmlight.joachims.org9Our replication of the Wan et al model is approxi-mate, because we used different preprocessing tools: MX-POST for POS tagging (Ratnaparkhi, 1996), MSTParserfor parsing (McDonald et al, 2005), and Dan Bikel?sinterface (http://www.cis.upenn.edu/?dbikel/software.html#wn) to WordNet (Miller, 1995) forlemmatization information.
Tuning led to C = 17 and poly-nomial degree 4.473Model Accuracy Precision Recallbaselinesall p 66.49 66.49 100.00Wan et al SVM (reported) 75.63 77.00 90.00Wan et al SVM (replication) 75.42 76.88 90.14pQlexical semantics features removed 68.64 68.84 96.51all features 73.33 74.48 91.10c-command disallowed (best; see text) 73.86 74.89 91.28?6 pL 75.36 78.12 87.44product of experts 76.06 79.57 86.05oraclesWan et al SVM and pL 80.17 100.00 92.07Wan et al SVM and pQ 83.42 100.00 96.60pQ and pL 83.19 100.00 95.29Table 2: Accuracy,p-class precision, andp-class recall on the testset (N = 1,725).
Seetext for differences inimplementationbetween Wan et al andour replication; theirreported score does notinclude the full test set.5.2 ResultsTab.
2 shows performance achieved by the base-line SVM and variations on pQ on the test set.
Weperformed a few feature ablation studies, evaluat-ing on the development data.
We removed the lex-ical semantics component of the QG,10 and disal-lowed the syntactic configurations one by one, toinvestigate which components of pQ contributes tosystem performance.
The lexical semantics com-ponent is critical, as seen by the drop in accu-racy from the table (without this component, pQbehaves almost like the ?all p?
baseline).
Wefound that the most important configurations are?parent-child,?
and ?child-parent?
while damagefrom ablating other configurations is relativelysmall.
Most interestingly, disallowing the ?c-command?
configuration resulted in the best ab-solute accuracy, giving us the best version of pQ.The c-command configuration allows more distantnodes in a source sentence to align to parent-childpairs in a target (see Fig.
1d).
Allowing this con-figuration guides the model in the wrong direction,thus reducing test accuracy.
We tried disallowingmore than one configuration at a time, without get-ting improvements on development data.
We alsotried ablating the WordNet relations, and observedthat the ?identical-word?
feature hurt the modelthe most.
Ablating the rest of the features did notproduce considerable changes in accuracy.The development data-selected pQ achieveshigher recall by 1 point than Wan et al?s SVM,but has precision 2 points worse.5.3 DiscussionIt is quite promising that a linguistically-motivatedprobabilistic model comes so close to a string-similarity baseline, without incorporating string-local phrases.
We see several reasons to prefer10This is accomplished by eliminating lines 12 and 13 fromthe definition of pkid and redefining pword to be the unigramword distribution estimated from the Gigaword corpus, as inG0, without the help of WordNet.the more intricate QG to the straightforward SVM.First, the QG discovers hidden alignments be-tween words.
Alignments have been leveraged inrelated tasks such as textual entailment (Giampic-colo et al, 2007); they make the model more inter-pretable in analyzing system output (e.g., Fig.
2).Second, the paraphrases of a sentence can be con-sidered to be monolingual translations.
We modelthe paraphrase problem using a direct machinetranslation model, thus providing a translation in-terpretation of the problem.
This framework couldbe extended to permit paraphrase generation, or toexploit other linguistic annotations, such as repre-sentations of semantics (see, e.g., Qiu et al, 2006).Nonetheless, the usefulness of surface overlapfeatures is difficult to ignore.
We next provide anefficient way to combine a surface model with pQ.6 Product of ExpertsIncorporating structural alignment and surfaceoverlap features inside a single model can makeexact inference infeasible.
As an example, con-sider features like n-gram overlap percentages thatprovide cues of content overlap between two sen-tences.
One intuitive way of including these fea-tures in a QG could be including these only atthe root of the target tree, i.e.
while calculatingC(r, 0).
These features have to be included inestimating pkid, which has log-linear componentmodels (Eq.
7- 13).
For these bigram or trigramoverlap features, a similar log-linear model hasto be normalized with a partition function, whichconsiders the (unnormalized) scores of all possibletarget sentences, given the source sentence.We therefore combine pQ with a lexical overlapmodel that gives another posterior probability es-timate pL(c | s1, s2) through a product of experts(PoE; Hinton, 2002), pJ(c | s1, s2)=pQ(c | s1, s2)?
pL(c | s1, s2)?c??{p,n}pQ(c?
| s1, s2)?
pL(c?
| s1, s2)(21)474Eq.
21 takes the product of the two models?
poste-rior probabilities, then normalizes it to sum to one.PoE models are used to efficiently combine severalexpert models that individually constrain differentdimensions in high-dimensional data, the producttherefore constraining all of the dimensions.
Com-bining models in this way grants to each expertcomponent model the ability to ?veto?
a class bygiving it low probability; the most probable classis the one that is least objectionable to all experts.Probabilistic Lexical Overlap Model We de-vised a logistic regression (LR) model incorpo-rating 18 simple features, computed directly froms1 and s2, without modeling any hidden corre-spondence.
LR (like the QG) provides a proba-bility distribution, but uses surface features (likethe SVM).
The features are of the form precisionn(number of n-gram matches divided by the num-ber of n-grams in s1), recalln (number of n-grammatches divided by the number of n-grams in s2)and Fn (harmonic mean of the previous two fea-tures), where 1 ?
n ?
3.
We also used lemma-tized versions of these features.
This model givesthe posterior probability pL(c | s1, s2), wherec ?
{p, n}.
We estimated the model parametersanalogously to Eq.
17.
Performance is reported inTab.
2; this model is on par with the SVM, thoughtrading recall in favor of precision.
We view it as aprobabilistic simulation of the SVM more suitablefor combination with the QG.Training the PoE Various ways of training a PoEexist.
We first trained pQ and pL separately asdescribed, then initialized the PoE with those pa-rameters.
We then continued training, maximizing(unregularized) conditional likelihood.Experiment We used pQ with the ?c-command?configuration excluded, and the LR model in theproduct of experts.
Tab.
2 includes the final re-sults achieved by the PoE.
The PoE model outper-forms all the other models, achieving an accuracyof 76.06%.11 The PoE is conservative, labeling apair as p only if the LR and the QG give it strongp probabilities.
This leads to high precision, at theexpense of recall.Oracle Ensembles Tab.
2 shows the results ofthree different oracle ensemble systems that cor-rectly classify a pair if either of the two individualsystems in the combination is correct.
Note thatthe combinations involving pQ achieve 83%, the11This accuracy is significant over pQ under a paired t-test(p < 0.04), but is not significant over the SVM.human agreement level for the MSRPC.
The LRand SVM are highly similar, and their oracle com-bination does not perform as well.7 Related WorkThere is a growing body of research that uses theMSRPC (Dolan et al, 2004; Quirk et al, 2004)to build models of paraphrase.
As noted, the mostsuccessful work has used edit distance (Zhang andPatrick, 2005) or bag-of-words features to mea-sure sentence similarity, along with shallow syn-tactic features (Finch et al, 2005; Wan et al, 2006;Corley and Mihalcea, 2005).
Qiu et al (2006)used predicate-argument annotations.Most related to our approach, Wu (2005) usedinversion transduction grammars?a synchronouscontext-free formalism (Wu, 1997)?for this task.Wu reported only positive-class (p) precision (notaccuracy) on the test set.
He obtained 76.1%,while our PoE model achieves 79.6% on that mea-sure.
Wu?s model can be understood as a stricthierarchical maximum-alignment method.
In con-trast, our alignments are soft (we sum over them),and we do not require strictly isomorphic syntac-tic structures.
Most importantly, our approach isfounded on a stochastic generating process and es-timated discriminatively for this task, while Wudid not estimate any parameters from data at all.8 ConclusionIn this paper, we have presented a probabilisticmodel of paraphrase incorporating syntax, lexi-cal semantics, and hidden loose alignments be-tween two sentences?
trees.
Though it fully de-fines a generative process for both sentences andtheir relationship, the model is discriminativelytrained to maximize conditional likelihood.
Wehave shown that this model is competitive for de-termining whether there exists a semantic rela-tionship between them, and can be improved byprincipled combination with more standard lexicaloverlap approaches.AcknowledgmentsThe authors thank the three anonymous review-ers for helpful comments and Alan Black, Freder-ick Crabbe, Jason Eisner, Kevin Gimpel, RebeccaHwa, David Smith, and Mengqiu Wang for helpfuldiscussions.
This work was supported by DARPAgrant NBCH-1080004.475ReferencesRegina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: an unsupervised approach usingmultiple-sequence alignment.
In Proc.
of NAACL.Daniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
Machine Learning, 34(1-3):211?231.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine transla-tion using paraphrases.
In Proc.
of HLT-NAACL.Courtney Corley and Rada Mihalcea.
2005.
Mea-suring the semantic similarity of texts.
In Proc.
ofACL Workshop on Empirical Modeling of SemanticEquivalence and Entailment.William B. Dolan and Chris Brockett.
2005.
Auto-matically constructing a corpus of sentential para-phrases.
In Proc.
of IWP.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: exploiting massively parallel news sources.
InProc.
of COLING.Andrew Finch, Young Sook Hwang, and EiichiroSumita.
2005.
Using machine translation evalua-tion techniques to determine sentence-level seman-tic equivalence.
In Proc.
of IWP.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third PASCAL recog-nizing textual entailment challenge.
In Proc.
of theACL-PASCAL Workshop on Textual Entailment andParaphrasing.David Graff.
2003.
English Gigaword.
LinguisticData Consortium.Geoffrey E. Hinton.
2002.
Training products of ex-perts by minimizing contrastive divergence.
NeuralComputation, 14:1771?1800.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Advances in Kernel Methods -Support Vector Learning.
MIT Press.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Math.
Programming (Ser.
B), 45(3):503?528.Erwin Marsi and Emiel Krahmer.
2005.
Explorationsin sentence fusion.
In Proc.
of EWNLG.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proc.
of ACL.Kathleen R. McKeown.
1979.
Paraphrasing usinggiven and new information in a question-answer sys-tem.
In Proc.
of ACL.I.
Dan Melamed.
2004.
Statistical machine translationby parsing.
In Proc.
of ACL.George A. Miller.
1995.
Wordnet: a lexical databasefor English.
Commun.
ACM, 38(11):39?41.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of ACL.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2006.Paraphrase recognition via dissimilarity significanceclassification.
In Proc.
of EMNLP.Chris Quirk, Chris Brockett, and William B. Dolan.2004.
Monolingual machine translation for para-phrase generation.
In Proc.
of EMNLP.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proc.
ofEMNLP.David A. Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projec-tion of syntactic dependencies.
In Proc.
of the HLT-NAACL Workshop on Statistical Machine Transla-tion.Joseph P. Turian, Luke Shen, and I. Dan Melamed.2003.
Evaluation of machine translation and itsevaluation.
In Proc.
of Machine Translation SummitIX.Stephen Wan, Mark Dras, Robert Dale, and Ce?cileParis.
2006.
Using dependency-based features totake the ?para-farce?
out of paraphrase.
In Proc.
ofALTW.Mengqiu Wang, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
a quasi-synchronous grammar for QA.
In Proc.
of EMNLP-CoNLL.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Comput.
Linguist., 23(3).Dekai Wu.
2005.
Recognizing paraphrases and textualentailment using inversion transduction grammars.In Proc.
of the ACL Workshop on Empirical Model-ing of Semantic Equivalence and Entailment.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proc.
of IWPT.Yitao Zhang and Jon Patrick.
2005.
Paraphrase identi-fication by text canonicalization.
In Proc.
of ALTW.476
