Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2042?2051, Dublin, Ireland, August 23-29 2014.RED: A Reference Dependency Based MT Evaluation MetricHui Yu?
?Xiaofeng Wu?Jun Xie?Wenbin Jiang?Qun Liu?
?Shouxun Lin?
?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences?University of Chinese Academy of Sciences{yuhui,xiejun,jiangwenbin,sxlin}@ict.ac.cn?CNGL, School of Computing, Dublin City University{xiaofengwu,qliu}@computing.dcu.ieAbstractMost of the widely-used automatic evaluation metrics consider only the local fragments of thereferences and translations, and they ignore the evaluation on the syntax level.
Current syntax-based evaluation metrics try to introduce syntax information but suffer from the poor pars-ing results of the noisy machine translations.
To alleviate this problem, we propose a noveldependency-based evaluation metric which only employs the dependency information of the ref-erences.
We use two kinds of reference dependency structures: headword chain to capture thelong distance dependency information, and fixed and floating structures to capture the local con-tinuous ngram.
Experiment results show that our metric achieves higher correlations with humanjudgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013.
By introducing extralinguistic resources and tuning parameters, the new metric gets the state-of-the-art performancewhich is better than METEOR and SEMPOS on system level, and is comparable with METEORon sentence level on WMT 2012 and WMT 2013.1 IntroductionAutomatic machine translation (MT) evaluation plays an important role in the evolution of MT.
It notonly evaluates the performance of MT systems, but also makes the development of MT systems rapider(Och, 2003).
According to the type of the employed information, the automatic MT evaluation metricscan be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-basedmetrics.The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR(Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturingthe lexicon or phrase level information, e.g.
fixed phrases or idioms.
But they cannot adequately reflectthe syntax similarity.
Current efforts in syntax-based metrics, such as the headword chain based metric(HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) andsyntactic/semantic-role overlap (Gim?enez and M`arquez, 2007) , suffer from the parsing of the potentiallynoisy machine translations, so the improvement of their performance is restricted due to the seriousparsing errors.
Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have thesimilar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors intranslations.
To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay andBrew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did notachieve the state-of-the-art performance.In this paper, we propose a novel dependency tree based MT evaluation metric.
The new metric onlyemploys the reference dependency tree, leaving the translation unparsed to avoid the error propagation.We use two kinds of reference dependency structures in our metric.
One is the headword chain (Liu andGildea, 2005) which can capture long distance dependency information.
The other is fixed and floatingstructure (Shen et al., 2010) which can capture local continuous ngram.
When calculating the matchingscore between the headword chain and the translation, we use a distance-based similarity.
ExperimentThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2042results show that our metric achieves higher correlations with human judgments than BLEU, TER andHWCM on WMT 2012 and WMT 2013.
After introducing extra resources and tuning parameters onWMT 2010, the new metric is better than METEOR and SEMPOS on system level and comparable withMETEOR on sentence level on WMT 2012 and WMT2013.The remainder of this paper is organized as follows.
Section 2 describes our new reference dependencybased MT evaluation metric.
In Section 3, we introduce some extra resources to this new metric.
Section4 presents the parameter tuning for the new metric.
Section 5 gives the experiment results.
Conclusionsand future work are discussed in Section 6.2 RED: A Reference Dependency Based MT Evaluation MetricThe new metric is a REference Dependency based automatic evaluation metric, so we name it RED.We present the new metric detailedly in this section.
The description of dependency ngrams is given inSection 2.1.
The method to score the dependency ngram is presented in Section 2.2.
At last, the methodof calculating the final score is introduced in Section 2.3.2.1 Two Kinds of Dependency NgramsTo capture both the long distance dependency information and the local continuous ngrams, we use boththe headword chain and the fixed-floating structures in our new metric, which correspond to the twokinds of dependency ngram (dep-ngram), headword chain ngram and fixed-floating ngram.Figure 1: An example of dependency tree.Figure 2: Different kinds of structures extractedfrom the dependency tree in Figure 1.
(a): Head-word chain.
(b): Fixed structure.
(c): Floating struc-ture.2.1.1 Headword chainHeadword chain is a sequence of words which corresponds to a path in the dependency tree (Liu andGildea, 2005).
For example, Figure 2(a) is a 3-word headword chain extracted from the dependency treein Figure 1.
Headword chain can represent the long distance dependency information, but cannot capturemost of the continuous ngrams.
In our metric, headword chain corresponds to the headword chain ngramin which the positions of the words are considered.
So the form of headword chain ngram is expressedas (w1pos1, w2pos2, ..., wnposn), where n is the length of the headword chain ngram.
For example, theheadword chain in Figure 2(a) is expressed as (saw2, with5,magnifier7).2.1.2 Fixed and floating structuresFixed and floating structures are defined in Shen et al.
(2010).
Fixed structures consist of a sub-root withchildren, each of which must be a complete constituent.
They are called fixed dependency structuresbecause the head is known or fixed.
For example, Figure 2(b) shows a fixed structure.
Floating structuresconsist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified.Each of the siblings must be a complete constituent.
Figure 2(c) shows a floating structure.
Fixed-floating structures correspond to fixed-floating ngrams in our metric.
Fixed-floating ngrams don?t needthe position information, and can be simply expressed as (w1, w2, ..., wn), where n is the length of the2043Figure 3: An example of calculating matching score for a headword chain ngram(saw2, with5,magnifier7).
dis r1and dis r2are the distances between the corresponding twowords in the reference.
dis h1and dis h2are the distances between the corresponding two words in thehypothesis.fixed-floating ngram.
For example, the fixed structure in Figure 2(b) and the floating structre in Figure2(c) can be expressed as (I, saw, an, ant) and (an, ant, with, a,magnifier) respectively.2.2 Scoring Dep-ngramsHeadword chain ngrams may not be continuous, while fixed-floating ngrams must be continuous.
So thescoring methods of the two kinds of dep-ngrams are different, and we introduce the two scoring methodsin Section 2.2.1 and Section 2.2.2 respectively.2.2.1 Scoring headword chain ngramFor a headword chain ngram (w1pos1, w2pos2, ..., wnposn), if we can find all these n words in the stringof the translation with the same order as they appear in the reference sentence, we consider it a match andthe matching score is a distance-based similarity which is calculated by the relative distance, otherwise itis not a match and the score is 0.
The matching score is a decimal value between 0 and 1, which is moresuitable than just use integer 0 and 1.
For example, if the distance between two words in reference is 1,but the distance in two different hypotheses are 2 and 5 respectively.
It?s more reasonable to score them0.5 and 0.2 rather than 1 and 0.The relative distance dis ribetween every two adjacent words in this kind of dep-ngram is calculatedby Formula (1), where poswiis the position of word wi in the sentence.
In Formula (1), we have1 ?
i ?
n ?
1 and n is the length of the dep-ngram.
Then a vector (dis r1, dis r2, ..., dis rn?1) isobtained.
In the same way, we obtain vector (dis h1, dis h2, ..., dis hn?1) for the translation side.dis ri= |posw(i+1)?
poswi| (1)The matching score p(d,hyp)for a headword chain ngram (d) and the translation (hyp) is calculatedaccording to Formula (2), where n > 1.
When the length of the dep-ngram equals 1, the matching scoreequals 1 if the translation has the same word, otherwise, the matching score equals 0.p(d,hyp)=???exp(?
?n?1i=1|dis ri?
dis hi|n?
1) if match0 if unmatch(2)An example illustrating the calculation of the matching score p(d,hyp)is shown in Figure 3.
There isa 3-word headword chain ngram (saw2, with5,magnifier7) in the dependency tree of the reference.2044For this dep-3gram, the words are represented with underline in the reference dependency tree and thereference sentence in Figure 3.
We can also find all the same three underlined words in the translationwith the same order as they appear in the reference.
Therefore, there is a match for this dep-3gram.
Tocompute the matching score between this dep-3gram and the translation, we have:?
Calculate the distancedis r1= |poswith?
possaw| = |5?
2| = 3 dis r2= |posmagnifier?
poswith| = |7?
5| = 2dis h1= |poswith?
possaw| = |5?
2| = 3 dis h2= |posmagnifier?
poswith| = |6?
5| = 1?
Get the matching score as Formula (3) according to Formula (2).
d denotes(saw2, with5,magnifier7) and hyp denotes the translation in the example.p(d,hyp)= exp(?|dis r1?
dis h1|+ |dis r2?
dis h2|3?
1) = exp(?|3?
3|+ |2?
1|3?
1) = exp(?0.5)(3)We also tried other methods to calculate the matching score, such as the cosine distance and theabsolute distance, but the relative distance performed best.
For a headword chain ngram with more thanone matches in the translation, we choose the one with the highest matching score.2.2.2 Scoring fixed-floating ngramThe words in the fixed-floating ngram are continuous, so we restrict the matched string in the translationalso to being continuous.
That means, for a fixed-floating ngram (w1, w2, ..., wn), if we can find all thesen words continuous in the translation with the same order as they appear in the reference, we think thedep-ngram can match with the translation.
The matching score can be obtained by Formula (4), where dstands for a fixed-floating ngram and hyp stands for the translation.p(d,hyp)={1 if match0 if unmatch(4)2.3 Scoring REDIn the new metric, we use Fscore to obtain the final score.
Fscore is calculated by Formula (5), where ?is a value between 0 and 1.Fscore =precision ?
recall?
?
precision+ (1?
?)
?
recall(5)The dep-ngrams of the reference and the string of the translation are used to calculate the precision andrecall.
In order to calculate precision, the number of the dep-ngrams in the translation should be given,but there is no dependency tree for the translation in our method.
We know that the number of dep-ngrams has an approximate linear relationship with the length of the sentence, so we use the length ofthe translation to replace the number of the dep-ngrams in the translation dependency tree.
Recall canbe calculated directly since we know the number of the dep-ngrams in the reference.
The precision andrecall are computed as follows.precision =?d?Dnp(d,hyp)lenh, recall =?d?Dnp(d,hyp)countn(ref)Dnis the set of dep-ngrams with the length of n. lenhis the length of the translation.
countn(ref)is thenumber of the dep-ngrams with the length of n in the reference.2045The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams?Fscore is calculated.
wngram(0 ?
wngram?
1) is the weight of dep-ngram with the length of n. Fscorenis the Fscore for the dep-ngrams with the length of n.RED =N?n=1(wngram?
Fscoren) (6)3 Introducing Extra ResourcesMany automatic evaluation metrics can only find the exact match between the reference and the transla-tion, and the information provided by the limited number of references is not sufficient.
Some evaluationmetrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand thereference information.
We also introduce some extra resources to RED, such as stem, synonym andparaphrase.
The words within a sentence can be classified into content words and function words.
Theeffects of the two kinds of words are different and they shouldn?t have the same matching score, so weintroduce a parameter to distinguish them.
The methods of applying these resources are introduced asfollows.?
Stem and SynonymStem(Porter, 2001) and synonym (WordNet1) are introduced to RED in the following three steps.First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not onlyexact match but also stem and synonym are considered.
We use stem and synonym together withexact match as three match modules.
Second, the alignment is used to match for a dep-ngram.
Wethink the dep-ngram can match with the translation if the following conditions are satisfied.
1) Eachof the words in the dep-ngram has a matched word in the translation according to the alignment;2) The words in dep-ngram and the matched words in translation appear in the same order; 3) Thematched words in translation must be continuous if the dep-ngram is a fixed-floating ngram.
At last,the match module score of a dep-ngram is calculated according to Formula (7).
Different matchmodules have different effects, so we give them different weights.smod=?ni=1wmin, 0 ?
wmi?
1 (7)miis the match module (exact, stem or synonym) of the ith word in a dep-ngram.
wmiis the matchmodule weight of the ith word in a dep-ngram.
n is the number of words in a dep-ngram.?
ParaphraseWhen introducing paraphrase, we don?t consider the dependency tree of the reference, becauseparaphrases may not be contained in the headword chain and fixed-floating structures.
First, thealignment is obtained with METEOR Aligner, only considering paraphrase.
Second, the matchedparaphrases are extracted from the alignment and defined as paraphrase-ngram.
The score of aparaphrase is 1?
wpar, where wparis the weight of paraphrase-ngram.?
Function wordWe introduce a parameter wfun(0 ?
wfun?
1) to distinguish function words and content words.wfunis the weight of function words.
The function word score of a dep-ngram or paraphrase-ngramis computed according to Formula (8).sfun=Cfun?
wfun+ Ccon?
(1?
wfun)Cfun+ Ccon(8)Cfunis the number of function words in the dep-ngram or paraphrase-ngram.
Cconis the numberof content words in the dep-ngram or paraphrase-ngram.1http://wordnet.princeton.edu/2046We use RED-plus (REDp) to represent RED with extra resources, and the final score are calculated asFormula (9), in which Fscorepis obtained using precisonpand recallpas Formula (10).REDp =N?n=1(wngram?
Fscorepn) (9)Fscorep=precisionp?
recallp?
?
precisionp+ (1?
?)
?
recallp(10)precisionpand recallPin Formula (10) are calculated as follows.precisionp=scoreparn+ scoredepnlenh, recallp=scoreparn+ scoredepncountn(ref) + countn(par)lenhis the length of the translation.
countn(ref)is the number of the dep-ngrams with the length of nin the reference.
countn(par) is the number of paraphrases with length of n in reference.
scoreparnisthe match score of paraphrase-ngrams with the length of n. scoredepnis the match score of dep-ngramswith the length of n. scoreparnand scoredepnare calculated as follows.scoreparn=?par?Pn(1?
wpar?
sfum) , scoredepn=?d?Dn(p(d,hyp)?
smod?
sfun)Pnis the set of paraphrase-ngrams with the length of n. Dnis the set of dep-ngrams with the length of n.4 Parameter TuningThere are several parameters in REDp, and different parameter values can make the performance ofREDp different.
For example,wngramrepresents the weight of dep-ngram with the length of n. Theeffect of ngrams with different lengths are different, and they shouldn?t have the same weight.
So we cantune the parameters to find their best values.We try a preliminary optimization method to tune parameters in REDp.
A heuristic search is employedand the parameters are classified into two subsets.
The parameter optimization is a grid search over thetwo subsets of parameters.
When searching Subset 1, the parameters in Subset 2 are fixed, and thenSubset 1 and Subset 2 are exchanged to finish this iteration.
Several iterations are executed to finish theparameter tuning process.
This heuristic search may not find the global optimum but it can save a lot oftime compared with exhaustive search.
The optimization goal is to maximize the sum of Spearman?s ?rank correlation coefficient on system level and Kendall?s ?
correlation coefficient on sentence level.
?is calculated using the following equation.?
= 1?6?d2in(n2?
1)where diis the difference between the human rank and metric?s rank for system i. n is the number ofsystems.
?
is calculated as follows.?
=number of concordant pairs?
number of discordant pairsnumber of concordant pairs + number of discordant pairsThe data of into-English tasks in WMT 2010 are used to tune parameters.
The tuned parameters arelisted in Table 1.5 Experiments5.1 DataThe test sets in experiments are WMT 2012 and WMT 2013.
The language pairs are German-to-English(de-en), Czech-to-English (cz-en), French-to-English (fr-en), Spanish-to-English (es-en) and Russian-to-English (ru-en).
The number of translation systems for each language pair are showed in Table 2.
Foreach language pair, there are 3003 sentences in WMT 2012 and 3000 sentences in WMT 2013.2047Parameter ?
wfunwexactwstemwsynwparw1gramw2gramw3gramtuned values 0.9 0.2 0.9 0.6 0.6 0.6 0.6 0.5 0.1Table 1: Parameter values after tuning on WMT 2010. ?
is from Formula (10).
wfunis the weight offunction word.
wexact, wstemandwsynare the weights of the three match modules ?exact stem synonym?respectively.
wparis the weight of paraphrase-ngram.
w1gram, w2gramand w3gramare the weights ofdep-ngram with the length of 1, 2 and 3 respectively.Language pairs cz-en de-en es-en fr-en ru-enWMT2012 6 16 12 15 -WMT2013 12 23 17 19 23Table 2: The number of translation systems for each language pair on WMT 2012 and WMT 2013.We parsed the reference into constituent tree by Berkeley parser2and then converted the constituenttree into dependency tree by Penn2Malt3.
Presumably, the performance of the new metric will be betterif the dependency trees are labeled by human.
Reference dependency trees are labeled only once and canbe used forever so it will not increase costs.5.2 BaselinesIn the experiments, we compare the performance of our metric with the widely-used lexicon-based met-rics such as BLEU4, TER5and METEOR6, dependency-based metric HWCM and semantic-based metricSEMPOS (Mach?a?cek and Bojar, 2011) which has the best performance on system level according to thepublished results of WMT 2012.The results of BLEU are obtained using 4-gram with smoothing option.
The version of TER is 0.7.25.The results of METEOR are obtained by Version 1.4 with task option ?rank?.
We re-implement HWCMwhich employs an epsilon value of 10?3to replace zero for smoothing purpose.
The correlations ofSEMPOS are obtained from the published results of WMT 2012 and WMT 2013.5.3 Experiment ResultsThe experiments on both system level and sentence level are carried out.
On system level, the correlationsare calculated using Spearman?s rank correlation coefficient ?
(Pirie, 1988).
Kendall?s rank correlationcoefficient ?
(Kendall, 1938) is employed to evaluate the sentence level correlation.
Our method performsbest when the maximum length of dep-ngram is set to 3, so we only present the results with the maximumlength of 3.
RED represents the new metric with exact match and the parameter values are set as follows.?
= 0.5. w1gram= w2gram= w3gram= 1/3.
REDp represents the new metric with extra resourcesand tuned parameter values which are listed in Table (1).5.3.1 System level correlationsThe system level correlations are shown in Table 3.
RED is better than BLEU, TER and HWCM onaverage on both WMT 2012 and WMT 2013, which reflects that using syntactic information and onlyparsing the reference side are helpful.
REDp gets the best result on all of the language pairs exceptcz-en on WMT 2012.
The significant improvement from RED to REDp illustrates the effect of extraresources and the parameter tuning.
Stem, synonym and paraphrase can enrich the reference and provideextra knowledge for automatic evaluation metric.
There are several parameters in REDp, and differentparameter values can make the performance of REDp different.
So the performance can be optimizedthrough parameter tuning.
SEMPOS got the best correlation according to the published results of WMT2http://code.google.com/p/berkeleyparser/downloads/list3http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl5http://www.cs.umd.edu/?snover/tercom6http://www.cs.cmu.edu/?alavie/METEOR/download/meteor-1.4.tgz20482012, and METEOR got the best correlation according to the published results of WMT 2013 on into-English task on system level.
REDp gets better result than SEMPOS and METEOR on both WMT 2012and WMT 2013, so REDp achieves the state-of-the-art performance on system level.data WMT 2012 WMT 2013Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en aveBLEU .886 .671 .874 .811 .811 .936 .895 .888 .989 .670 .876TER .886 .624 .916 .821 .812 .800 .833 .825 .951 .581 .798HWCM .943 .762 .937 .818 .865 .902 .904 .886 .951 .756 .880METEOR .657 .885 .951 .843 .834 .964 .961 .979 .984 .789 .935SEMPOS .943 .924 .937 .804 .902 .955 .919 .930 .938 .823 .913RED 1.0 .759 .951 .818 .882 .964 .951 .930 .989 .725 .912REDp .943 .947 .965 .843 .925 .982 .973 .986 .995 .800 .947Table 3: System level correlations on WMT 2012 and WMT 2013.
The value in bold is the best result ineach column.
ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.5.3.2 Sentence level correlationsThe sentence level correlations on WMT 2012 and WMT 2013 are shown in Table 4.
RED is better thanBLEU and HWCM on all the language pairs, which reflects the effectiveness of syntactic informationand only parsing the reference.
By introducing extra resources and parameter tuning, REDp achievessignificant improvement over RED.
Stem, synonym and paraphrase can enrich the reference and provideextra knowledge for automatic evaluation metric.
There are several parameters in REDp, and differentparameter values can make the performance of REDp different.
A better performance can be exploitedthrough parameter tuning.
From the results of REDp and METEOR, we can see that REDp gets thecomparable results with METEOR on sentence level on both WMT 2012 and WMT 2013.data WMT 2012 WMT 2013Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en aveBLEU .157 .191 .189 .210 .187 .199 .220 .259 .224 .162 .213HWCM .158 .207 .203 .204 .193 .187 .208 .247 .227 .175 .209METEOR .212 .275 .249 .251 .247 .265 .293 .324 .264 .239 .277RED .165 .218 .203 .221 .202 .210 .239 .292 .246 .196 .237REDp .212 .271 .234 .250 .242 .259 .290 .323 .260 .223 .271Table 4: Sentence level correlations on WMT 2012 and WMT 2013.
The value in bold is the best resultin each column.
ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.6 Conclusion and Future WorkIn this paper, we propose a reference dependency based automatic MT evaluation metric RED.
Thenew metric only uses the dependency trees of the reference, which avoids the parsing of the potentiallynoisy translations.
Both long distance dependency information and the local continuous ngrams arecaptured by the new metric.
The experiment results indicate that RED achieves better correlations thanBLEU, TER and HWCM on both system level and sentence level.
REDp, the improved version of REDthrough adding extra resources and preliminary parameter tuning, gets state-of-the-art results which arebetter than METEOR and SEMPOS on system level.
On sentence level, REDp gets the comparableperformance with METEOR.In the future, we will use the dependency forest instead of the dependency tree to reduce the effectof parsing errors.
We will also apply RED and REDp to the tuning process of SMT to improve thetranslation quality.2049AcknowledgementsThe authors were supported by National Natural Science Foundation of China (Contract 61202216)and National Natural Science Foundation of China (Contract 61379086).
Qun Liu?s work was partiallysupported by the Science Foundation Ireland (Grant No.
07/CE/I1142) as part of the CNGL at DublinCity University.
Sincere thanks to the three anonymous reviewers for their thorough reviewing andvaluable suggestions.ReferencesBoxing Chen and Roland Kuhn.
2011.
Amber: A modified bleu, enhanced ranking metric.
In Proceedings ofthe Sixth Workshop on Statistical Machine Translation, pages 71?77, Edinburgh, Scotland, July.
Association forComputational Linguistics.Boxing Chen, Roland Kuhn, and George Foster.
2012.
Improving amber, an mt evaluation metric.
In Proceedingsof the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 59?63, Stroudsburg, PA, USA.Association for Computational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3: Automatic metric for reliable optimization and evaluationof machine translation systems.
In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages85?91.
Association for Computational Linguistics.Jes?us Gim?enez and Llu?
?s M`arquez.
2007.
Linguistic features for automatic evaluation of heterogenous mt systems.In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264.
Association forComputational Linguistics.Maurice G Kendall.
1938.
A new measure of rank correlation.
Biometrika, 30(1/2):81?93.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: an automatic metric for mt evaluation with high levels ofcorrelation with human judgments.
In Proceedings of the Second Workshop on Statistical Machine Translation,StatMT ?07, pages 228?231, Stroudsburg, PA, USA.
Association for Computational Linguistics.Ding Liu and Daniel Gildea.
2005.
Syntactic features for evaluation of machine translation.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,pages 25?32.Chi-kiu Lo and Dekai Wu.
2013.
MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic framebased MT evaluation metric.
In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages422?428, Sofia, Bulgaria, August.
Association for Computational Linguistics.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012.
Fully automatic semantic mt evaluation.
In Pro-ceedings of the Seventh Workshop on Statistical Machine Translation, pages 243?252, Montr?eal, Canada, June.Association for Computational Linguistics.Matou?s Mach?a?cek and Ond?rej Bojar.
2011.
Approximating a deep-syntactic metric for mt evaluation and tun-ing.
In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92?98.
Association forComputational Linguistics.Dennis Mehay and Chris Brew.
2007.
BLEUTRE: Flattening Syntactic Dependencies for MT Evaluation.
InProceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).F.J.
Och.
2003.
Minimum error rate training in statistical machine translation.
In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics-Volume 1, pages 160?167.
Association for Computa-tional Linguistics.Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007.
Dependency-based automatic evaluation formachine translation.
In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Sta-tistical Translation, SSST ?07, pages 80?87, Stroudsburg, PA, USA.
Association for Computational Linguistics.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.
BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th annual meeting on association for computational linguistics, pages311?318.
Association for Computational Linguistics.W Pirie.
1988.
Spearman rank correlation coefficient.
Encyclopedia of statistical sciences.2050Martin F Porter.
2001.
Snowball: A language for stemming algorithms.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.
String-to-dependency statistical machine translation.
Compu-tational Linguistics, 36(4):649?671.Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation in theAmericas, pages 223?231.Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz.
2009.
Fluency, adequacy, or hter?
:exploring different human judgments with a tunable mt metric.
In Proceedings of the Fourth Workshop onStatistical Machine Translation, pages 259?268.
Association for Computational Linguistics.2051
