An Unsupervised Method for Multifingual Word Sense Tagging UsingParallel Corpora: A Preliminary InvestigationMona DiabLinguistics Department & UMIACS,University of Maryland, College Park, MD 20742mdiab @umiacs.umd.
eduAbstractWith an increasing number of languagesmaking their way to our desktops everydayvia the Internet, researchers have come torealize the lack of linguistic knowledgeresources for scarcely represented/studiedlanguages.
In an attempt to bootstrap someof the required linguistic resources for someof those languages, this paper presents anunsupervised method for automaticmultilingual word sense tagging usingparallel corpora.
The method is evaluated onthe English Brown corpus and its translationinto three different languages: French,German and Spanish.
A preliminaryevaluation of the proposed method yieldedresults of up to 79% accuracy rate for theEnglish data on 81.8% of the SemCormanually tagged ata.KeywordsUnsupervised; multilingual; alignments;parallel corpora; word sense tagging1.
IntroductionWith the term "globalization" becoming thetheme of cuxrent political and economicdiscourse, communications technology -exemplified by the World Wide Web OVWW) -has become a source of an abundance oflanguages.
Language researchers are faced withan ever so present challenge and excitement ofbeing able to study and process these languagesand create the appropriate NLP applications forthem.
Yet, a major bottleneck for many NLPapplications uch as machine translation, crosslanguage information retrieval, natural languageunderstanding, etc, is word sense ambiguity.
Theproblem escalates as we deal with languages thatare scarce in processing resources andknowledge bases.
The availability of large scale,accurately, sense tagged data should helpalleviate the problem.It has been acknowledged that best way toacquire sense tags for words in a corpus ismanually, which has proven to be a veryexpensive and labor intensive ndeavor.
In anattempt to approximate the human effort, bothsupervised \[Bruce & Weibe, 1994; Lin,1999;etc.\] and unsupervised methods \[Resnik1997; Yarowsky, 1992&1995; etc.\] have beenproposed to solve the problem automatically.
Onaverage supervised methods report higheraccuracy rates, but they are faced with theproblem of requiring large amounts of sensetagged data as training material.
Most of themethods, to date, aim at solving the problem forone language, namely the language with themost available linguistic resources.
Moreover,most of the proposed approaches report resultson a handful of the data, rendering themsolutions for a small scale of the data.Many researchers in the field have looked atlanguage translations as a source for sensedistinctions \[Dagan & Itai, 1994; Dyvik, 1998;Ide, in press; Resnik & Yarowsky, 1999; etc.\].The idea is that polysemons words in onelanguage can be translated as distinct words in adifferent language.
The problem has alwaysbeen the availability of large corpora intranslation, i.e.
parallel corpora.
Resnik \[1999\]proposed a method for facilitating theacquisition of parallel corpora from the WWW.Potentially, we can have parallel corpora in amyriad of languages, yet the downside is thescarcity of  linguistic knowledge resources andprocessing tools for less widelyrepresented/studied languages.
Consequently,we decided to bootstrap the process of wordsense tagging for both languages in a parallelcorpus using the translations a  a source of wordsense distinction.
Thereby, attaining sensetagged ata for languages with scarce resourcesas well as creating a supply of large-scale,automatically sense tagged data for a thelanguage with more knowledge resources -albeitnoisy - to be utilized by supervised algorithms.In this paper, we propose an unsupervisedmethod for word sense tagging of' both corporaautomatically.
The algorithm assumes theavailability of a word sense inventory in one ofthe languages.
The preliminary evaluation ofthe method on the nouns in an English corpus,yielded accuracy rates in the range of 69-77%against he polysemous nouns in a hand taggedtest set, which contrasts with a random baselineof 25.6%, and a baseline of the most frequentsense of 67.6%.In the following section we describe theproposed method, followed by a preliminaryevaluation of the method.
Section 4 discussesrelated work and we conclude with somethoughts on future directions in section 5.2.
Proposed methodWe propose amethod that utilizes translations afilters for sense distinctions.
The method isunsupervised since it does not rely on theavailability of sense tagged data.
As anillustration, i f  we look up the canonicalambiguous word bank in the Oxford HachetteEnglish-French dictionary, we find that ittranslates toseveral words indicating its possiblesenses.
Bank, as a noun, translates to the Frenchwords banque, rive, bord, etc.
If we reverse theFrench translations into English, we get theoriginal word bank as well as other Englishequivalents.
Accordingly, rive translates backinto English as bank and shore; bord translatesinto bank, edge, and rim.
Therefore, given aparallel corpus with a source and targetlanguage, if there exists a method of findingword afignments from the source languagecorpus to words in the target language corpus,one can create a set of all the words in the targetcorpus that are aligned with a word in the sourcecorpus.
For example, given a French/Englishparallel corpus, we would expect he word rive,on the French side, to align with the words bankand shore, on the English side, in the correctcontexts with a high probability.
This approachessentially hinges upon the diversity of contextsin which words are translated.We will refer to the English side of the parallelcorpus as the target language corpus since weassume the knowledge resources exist forEnglish.
The foreign language side is referred toas the source corpus.The required linguistic knowledge resource is alexical ontology that has the words in the targetlanguage and a listing of their associated senses.There are several databases of that sort availablefor language researchers, among which isWordNet \[Fellbanm, 1998; Miller et al, 1990\].WordNet is a lexical ontology - a variant onsemantic networks with more of a hierarchicalstructure, even though some of the nodes canhave multiple parents - that was manuallyconstructed for the English language.
Itcomprises four taxonomies for four parts ofspeech: nouns, verbs, adverbs and adjectives.Accordingly, given a taxonomy like WordNetfor the target language, and an appropriatedistance measure between words with theirassociated senses, the distance between all thesenses for both shore and bank is calculated.
InWordNet 1.6, bank has 10 senses, the 3 topmostfrequent senses are:I. a financial institution that acceptsdeposits and channels the money intolending activities2.
sloping land (especially the slope besidea body of  water)3. a supply or stock held in reserveespecially for future use (especially inemergencies)shore has two senses listed:1. the land along the edge of  a body ofwater (a lake or ocean or r/vet)2. a beam that is propped against astructure to provide supportOne would expect that the distance betweensense #2 of bank and sense #1 of shore to besmaller than the latter's distance from the othertwo senses of bank.
Accordingly, with anappropriate optimization function over thedistance measures between all the senses of thetwo words, sense #2 for bank and sense # 1 forshore are assigned as the correct ags for thewords, respectively.
In effect, we have assignedsense tags to rfi, e in its respective alignments, inthe appropriate contexts.
Therefore the instanceswhere rive is aligned with bank gets assignedsense #2 for the noun bank; instances where riveis aLigned with shore is assigned sense #1 forshore.
Furthermore, we created linEqautomatically in WordNet for the French wordrive.
Our approach is described as follows:?
Preprocessing ofcorporaTokenizc both corporaAlign the sentences of the corpora suchthat each sentence in the source corpusis aligned with one correspondingsentence in the target corpus.?
For each source and corresponding targetsentence, find the best token levelalignments.
Methods for automating thisprocess have been proposed in the Literature.\[A10naizan et al, 1999; Melamed, 2000;etc.\]?
For each source language token, create aListof its alignments o target language tokens,target set?
Using the taxonomy, calculate the distancebetween the senses of the tokens in thetarget set; assign the appropriate sense(s) toeach of the tokens in the target set based onan optimiTation function over the entire setof target token senses* Propagate the assigned senses back to bothtarget and source corpora tokens,effectively, creating two tag sets, one foreach the target and source corpus?
Evaluate the resnlting tag sets against ahandtagged test set.3.
Preliminary Evaluation3.1.
MaterialsWe chose the Brown Corpus of AmericanEnglish \[Francis & Kutera, 1982\] - of onemilLion words - as our target language corpus.
Itis a balanced corpus and it has more than 200Kwords that are manually sense tagged as aproduct of the semantic oncordance (SemCor)effort using WordNet \[Miller et al 1994\].
TheSemCor data is tagged in lamning text - wordsof varying parts of speech are tagged in context- using WordNet 1.6.
Hence, we used WordNet1.6 taxonomy as the Linguistic knowledgeresource.
\[Fellbaum, 1998\] For purposes of thispreliminary investigation, we only explorednouns in the corpus, yet there are no inherentrestrictions inthe method for applying it to otherparts of speech.
Accordingly, we used part ofspeech tags that were available in the Penn TreeBank for the Brown Corpus.The test set was created from the polysemousnouns in SemCor.
The nouns were extractedfrom the Brown corpus with their relative corpusand sentence position information.
The test setcomprised 58372 noun instances of 6824polysemous nouns.
The nouns were notlemmatized.Two baselines were cons~ucted.
A randombaseline (RBL), where each noun instance inthetest set was assigned a random sense from theList of senses pertaining to that noun in thetaxonomy.
And a default baseline (DBL), whereeach noun instance in the test set is assigned itsmost frequent sense according to WordNet 1.6.The Brown Corpus only exists in English;therefore, we decided to automatically translateit into three different languages using twocommercially available machine translation(MT) packages, Systran Professional 2.0 (SYS)and Globalink Power Translator Pro v.6.4 (GL).We used two different ranslation packages tomaximize the variability of the word translationselection, in an attempt to approximate a humantranslation.
The idea is that different MTpackages use different bilingual exicons in thetranslation process.
Moreover, we decided to usemore than one language since polysemous wordscan be translated in different ways in differentlanguages, i.e.
an ambiguous word that has twosenses could be translated into two distinctwords into one language but into one word inanother language.
We translated the BrownCorpus into French, German and Spanish, sincethese are considered the most reliable languagesfor the translation quality of the MT packages.Furthermore, the fact that EuroWordNet existsfor these languages faciLitates the process ofevaluating the source language tag set.3.2.
ExperimentsOnce we had the translations available, theseven corpora - namely, English Brown corpus,French GL, German GL, Spanish GL, FrenchSYS, German SYS, and Spanish SYS - weretokenized and the sentences were alignedk For1 This was a relatively easy task since the corpora reartificially created, therefore there was a one to onetoken level alignments, we used the GIZAprogram \[Al Onaizan et al 1999\]\[.
GIZA is anintermediate program in a statistical machinetranslation system, EGYPT.
It is animplementation f Models 1-4 of Brown et al\[1993\], where each of these models produces aViterbi alignment.
The models are trained insuccession where the final paraaneter valuesfrom one model are used as the startingparameters for the next model.
We trained eachmodel for I0 iterations.
Given a source andtarget pair of  afigned sentences, GIZA producesthe most probable token-level alignments.Multiple token alignments are allowed on thetarget language side, i.e.
a token in Englishcould align with multiple tokens :in the foreignlanguage.
Tokens on either side could align withnothing, designated as a null token.
GIZArequires a large corpus in order to producereliable alignments, hence, the use of  the entireBrown corpus: both the SemCor tagged datawithout the tags and the untagged data.Therefore, we produced the alignments for the 6parallel corpora - a parallel cortms comprisesthe English eorpns and its translation i to one ofthe three languages using one of  the MTpackages - with English as the target language.The Brown Corpus has 52282 sentences.
Due toprocessing limitations, GIZA ignores sentencesthat exceed 50 words in length, therefore itignored -3000 sentences on average per parallelcorpus alignment.
GIZA output was converted toan internal format: sentence number followed byall the tokens 2 in the sentence represented astoken positions in the target language alignedwith corresponding source language tokenpositions in the aligned foreign sentence.All the token positions were replaced by theactual tokens from the corresponding corpora.Tokens that were aligned with null tokens oneither side of  the parallel corpus were ignored.All the tokens were tagged with the sentencenumber and sentence position.
In order to reducethe search space, we reduced the list to thenouns in the corpus.
We created a list of  thesource language words that were aligned tonouns in the target language, thereby creating asource-target noun list for each source word.
Wecorrespondence between thesentences.2 Tokens include punctuationsource and targetremoved punctuation marks and theircorresponding afignments; also, we filtered outstop words from the source language.
Finally,we compressed the source-target list to have thefollowing format:Src wdi trgt_nnl, trgt_nnz,...,trgt_nn,where Src wdi is a word J in the sourcecorpus and trgt_nnj is the noun 4 italigned to in the target corpus.Source words that were aligned with one targetword only throughout the corpus were excludedfrom the final fist of  words to be tagged in ourtag set.
Each resulting set - a set had to includeat least 2 nouns - of  Engfish target nouns,corresponding to a source word, was passed onto the distance measure routine.We used an optimization function over thesenses of  the nouns in a set.
The function aims atmaximizing a similarity of  meaning over all themembers of  a set based on a pair wise similaritycalculation over all the listed senses in WordNet1.6.
The algorithm~ disambiguate_class, whichis implemented by Resnik and described indetail in \[Resnik, 1999\], calculates the similaritybetween all the words' senses of  words in a set.R assigns a confidence score based on sharedinformation content of  the sense combinations,which is measured via the most informativesubsumer in the taxonomy.
The senses with thehighest confidence scores are the senses thatcontribute the most to the maximization functionfor the set.
The algorithm expects the words tobe input as a set for calculating the confidencescores.
In many instances, we observedconsiderable noise in the target noun set.
Forexample, the French source word accord wasaligned with the English nouns accord,agreement, signing, consonance, andencyclopaedia n the target corpus.
All but thelast word in the target set seem to be related tothe word accord in French exceptencyclopaedia.
The source of  noise can beattributed to the specific translation system, or tothe alignment program~ or in other cases to the3 Parts of speech are not necessarily symmetric inalignments, i.e.
nouns could very well map to verbsor other parts of speech.4 Note that the nouns at this point are types nottokens, i.e.
not instances in the corpus rather aconflafion of instances4fact that the source language word itself isambiguous.Consequently, we conducted three types ofexperiments in an attempt to reduce the noise inthe target sets: Class sire, Pair_sinai  andPair_simall .
They essentially varied in inputformat o disambiguate_class.For Class sire, the target noun data wasproduced irectly from the source-target list andinput to the distance measure routine with nospecial formatting.
Each of the target nouns wasassigned the sense(s) that had the maximumconfidence level from among the senses listedfor it in the taxonomy.
Thereby creating the tagset for the target language, English.
If a noundoes not have an entry in the taxonomy, it isassigned a null sense.On the other hand, for both Pair_sire 1 andPair_siin all the nouns in the target fist for eachsource word were formatted into all paircombinations in the set and then sent todisambiguate_class.
The idea was to localize thenoise to the pair level comparison, sincedisambiguate_class optimizes over the entire setof nouns.
The senses that were selected were theones with the maximum confidence score fromthe noun pair sense comparison.
All the senseswith a maximum confidence score for a nounwere aggregated into a final list of senses forthat noun and duplicates were removed.In Pair_sinu1, only the senses that had aconfidence score of 100% were considered, i.e.if disambiguate_class is agnostic as to whetherthe senses of the target noun pair are similar,each noun in this pak comparison is assigned anull sense, for the noun pair in the localcomparison, respectively.
That does notnecessarily mean that either noun will have afinal null sense in the aggregate list, it ratherdepends on the sum total of comparisons foreach of them with all the nouns in the set.In Pair sire all, the same conditions apply as inPair_sinai, yet there is no threshold of a 100%.A pair of nouns in a local comparison isassigned a null sense if one of the nouns in thepair is not in WordNet or all the senses get aconfidence score of 0%.Once we had the tag set for each of our parallelcorpora, we evaluated it against he manuallytagged test set.
So far, we only evaluated the tagset for the target language, English.
Evaluationof the source tag set is in progress; a serioushurdle is that EuroWordNet is interfaced withWordNet 1.5 only.
The preliminary evaluationmetric is:~ co .~ta~o~joU~, lO0 \[I\]ace  = total hum testsensesWe only considered the first sense assigned inthe test set for any noun instance in the processof our evaluation.
The system was not penalizedif it assigned more than one sense to the noun inthe tag set i f  the correct sense was among thesenses assigned.We conducted the three types of experiments onthe 6 parallel corpora.
In the following section,we present the results for GL translations for thethree languages and the SYS translation forSpanish, since we found no significantdifference in the results across the twotranslation systems for the three experimenttypes.
Furthermore, we wanted to test the effectof merging the token alignments of the two MTsystems on the accuracy rates.
For all theexperiment conditions, the noun instances thatwere excluded from the tag set and were in thetest set were sense tagged using the defaultbaseline of 67.6%, in order to report he resultsat 100% coverage for the test set, the results ofwhich are presented in table 2 below.3.3.
Results and DiscussionThe investigation yielded the following resultsClass sire Pak sire 1 Pair sire allCoy Acc.
Cov Ac~ Cov Ace.FG 62.4 45.0 55.4 60.1 60.1  73.9GG 49.0 48.2 41.6 57.1 48.0 70.7SG 57.2 47.2 50.7 57.1 56.1 72.8SS 56.8 46.0 50.6 55.7 55.5 72.9MSp 83.6 45.5 75.8 63.0 81.8 79.0Table 1: Results for the different experimenttypes at various coverage levels of the test setTable 1. presents the results at differentcoverage percentages of the test set data for theEnglish target corpus.
The first cohunn has the 5experiment conditions used as source languagefilters of the English target corpus, and the firstrow has the three experiment types.
FG is theFrench translation of the Brown corpus renderedby the MT system GL; GG is the Germantranslation by GL; SG is the Spanish translationby GL; SS is the Spanish translation by the MTsystem SYS; and MSp is the merged Spanishtranslations from both NIT systems.
All theresults are presented as percentages, where theCoy.
indicates the percentage covered by the tagset of the test set.
Ace.
is the percent correct atthe coverage level based on the evaluationmeasure in \[ 1\].Across the board, the results from Pair sire allfor all the experiment conditions are hider thanthe results from Pai~sin~l,  which in turn arehigher than Class sire results.
The results do notseem to suggest any significant difference in theresults from the two Spanish translations SG andSS across the three expernnent types.
On theother hand, results from MSp outperform theindividual Spanish translation systems for thePair_sire 1 and Pair sire all experiments by amargin ~-25% more in coverage and -6% inaccuracy rates.
In the Class_sire xperiment, theindividual Spanish translations outperform theMSp condition.
We also note that coverage ishigher for all the experiment conditions.FGGGSGSSMSpClass sim53.5Pair sire 163.4Pair sire all71.458.1 63.2 69.155.9 62.3 70.555.3 61.6 70.649.1 64.1 76.925.6 % RBLDB?
67.6 %Table 2: Results at 100% coverage of the testsetTable 2 reports the results at 100?,6 coverage ofthe test set data for the target ag set.
FG, GG,SG, SS, MSp, are the same as in table 1.
RBL isthe random baseline, while DBL is the defaultbaseline.
All the experimental conditionssignificantly outperformed the random baseline.None of the conditions outperformed the defaultbaseline, DBL, in both Class_sire andPair sire 1 experiments.
Pair_sinai had ahigher accuracy rate than Class_sire for all theexperiment conditions.
Similar to theobservations in table I, Pair sire alloutperformed the other two experiment types forall the experiment conditions.
Pair sire all alsooutperformed the default baseline with animprovement of 1.4 (marginal in this case) to9%.
It is worth noting that there was nosignificant difference between the experimentalconditions SG and SS across the experimenttypes.
As in Table 1, the results from MSp aresignificantly higher than those obtained from theindividual Spanish translation conditions forboth Pair sire 1 and Pair sire all, while theresults for Class sire were much lower than theindividual Spanish conditions.
This can beattributed to the fact that while combiningevidence from both translations, we aggregatedthe noise in the target set from both translations.The noise causes disambiguate class to gettrapped into assizning higher confidences toirrelevant senses.In terms of the overall performance of thedifferent conditions, the results suggest thatmerging the two translation systems yields thebest results, with an improvement of 6% overthe individual translations independently forSpanish in PaL_sire_all.
Examining the resultsacross the three languages, it seems there wereslight variations in the accuracy rates in thePair sire 1 and Pair sire all experiments atfullcoverage, exemplified in table 2.
Yet we notethe low relative coverage of the test data in theGerman, GG condition, as shown in table 1.
Thiscan be explained as a result of the nature of theGerman language, which is highly agglutinative,thereby affecting the quality of the alignments.Also it could be a reflection of the quality of theGL MT system for the German language.The most interesting result is the result of  theMSp condition in table 1, which indicates that81.8% of the target data can be sense taggedwith an accuracy of  79%, significantly higherthan chance (25.6%) as well as it is higher thanthe default tagging of 67.6%.
We have yet toinvestigate the source tag set in order to see howmany of these source words can transparentlyacquire the target noun senses.
The finegraininess of WordNet leads us to suspect hatthe appropriate l vel of  evaluation will be at themost informative subsumer level in thetaxonomy (a coarser grain) as opposed to theactual sense tagged for the correspondingaligned target noun.The low accuracy rates over the full test set(table 2) may be attributed to the cascading ofdifferent sources of noise in the evaluationmethod, starting off with a less than perfecttranslation s and an automated alignmentprogram with a reported accuracy rate of ~92%for word alignments, English to German.
\[Och& Ney, 2000\] The latter result has to beconsidered with caution in the presentexperimental design context since the evaluationof the alignments was done with a humantranslation on a closed domain corpus, for onlyone of the languages under consideration i thecurrent investigation.
A large-scale multilingualevaluation of the alignment program is muchneeded.
By qualitatively looking at some of theautomatic ali~ments, some of the cases hadvery fight all,merits in the target language.
Forinstance, the French word abr/was aligned withcover and shed; agitation, in French, wasaligned with the nouns agitation, bustle,commotion, flurry, fuss, restlessness, andturmoil.Word ambiguity in the source language couldhave contributed to the low accuracy ratesattained.
In many cases, we noticed that thesource language seemed to preserve theambiguity found in the target language.
Forexample, (a) the French word canon was alignedwith the target nouns: cannon, cannonball,canon, theologian; Co) the French word bandeswas aligned with the target nouns: band, gang,mob, streaks, strips, tapes, tracks.
In bothexamples we see at least two clusters in thetarget noun sets, in (a), cannon and cannonballare one cluster and canon and theologian formthe other cluster; in Co), the word band isambiguous, we can see that band, gang and mobcan form a cluster, while band, streaks, strips,tapes and tracks could form another.
We arecurrently investigating the effect ofincorporating co-occurrence information as ameans of clustering the words in the target set,aiming at delineating the senses for the sourcelanguage word.
Another source of noise is themetaphoric as well as slang usage of some of thewords in the target language, for instance, be'b~s,in French, was aligned with babes and babies inthe target language.We expect the results to improve the moredistant he language pair.
Moreover, combiningdifferent language sources imultaneously couldyield improved results due to the fact that5 We do not know of any formal evaluation on thequality of the two wanslation packages usedlanguages will differ in the manner in whichthey conflate senses.We would like to explore different evaluationmetrics for the target language, which are fine-tuned to the fine granularity of WordNet.
Aswell as, devise methods for obtaining aquantitative measure of evaluation for the sourcetag set.4.
Related WorkThere are many proposed unsupervised methodsin the literature addressing the problem of senseambiguity in language.
All the reportedunsupervised methods use monolingualmaterials, therefore comparable to the resultsobtained on the target ag set of our preliminaryinvestigation.
Moreover, due to differences inthe knowledge resources and evaluation materialit is hard to establish a direct comparison.
Forinstance, Yarowsky \[1992&1995\] reports thehighest accuracy rates, to date, for anunsupervised method of a mean of 92%, yet hisevaluation was measured using a knowledgeresource, Roget's thesaurus, which has a coarsergranularity in its sense representation thanWordNet.The most comparable results to our preliminaryresults are those reported by Resnik \[1997\] sincehe used the same corpus and evaluated againstthe same test set.
He did not restrict hisevaluation to nouns only.
Resnik proposed anunsupervised method for sense disambiguationusing selectional preference information, therebyusing grammatical relations between words in acorpus in order to arrive at the correct sense fora word.
He reports accuracy rates in the range of40.1% on average for five grammatical relations.Yet, Resnik explores a different dimension ofmeaning that uses a linguistically motivatedcontext window which we expect will be veryuseful if combined with our approach forexamining the verb data, for example.The most related work reported in the literatureis that of Ide \[in press\].
Ide explores the questionof whether using cross-linguistic nformation forsense distinction is worth pursuing.
She reporteda preliminary analysis of translation equivalentsin four different languages of George OrweU'sNineteen-Eighty-four.
The translations werehuman translations, i.e.
natural parallel corpora.In her study, only 4 words were considered.Native speakers of the four respective languages7aligned the chosen English words to theirforeign translations manually.
The goal of herresearch was to explore the degree to whichwords are lexiealized ifferently in translatedtext.
Ide classifies translation types based onhow much they vary in what they align with intranslation, for example, if a word aligns with asingle word or a phrase or nothing, etc.
Shereports that in Nineteen-Eighty-Four, only86.6% of the English words have a single lexicalitem used in the translation.
This suggests thatwith using alignment methods that target singleword to single word alignments he upper boundthat the approach can yield is 86.6% for thisspecific corpus.
It will be interesting to conducta similar study here of the Brown corpus.5.
Conclusion and Future DirectionsWe presented an unsupervised method for wordsense tagging for both the source and the targetlanguages ina parallel corpus.
The :method relieson translations as a source of sense distinction.The goal of the proposed algorithm is tobootstrap the process of word sense tagging on alarge scale for a language vdth linguisticknowledge resources as well as for languageswith scarce resources.
As a proof of concept, weevaluated the approach on 6 artificially createdtranslation corpora.
The preliminary evaluationyielded accuracy rates of up to 79% for 81.8%of the test set in the target language.
The sourcelanguage tag set is yet to be evaluated.
Futuredirections include devising methods for reducingthe noise in the target sets.
Moreover, testing theapproach on other parts of speech.
Furthermore,it would be interesting to test the method onnaturally created parallel corpora.AcknowledgementsThis work has been has been supported in partby DARPA contract N6600197C8540 andDARPA co-operative agreementN660010028910.
We would like to thank PhilipResnik for his support and feedback throughoutthe different phases of this investigation.
Wewould like also to acknowledge an anonymousreviewer for his/her useful comments.ReferencesA1-Onaizan, J. Y.Curin, M. Jahr, K. Knight, J.Laferty, D. Melamed, F. Och, D. Purdy, N.Smith, & D. Yarowsky (1999).
StatisticalMachine Translation, Final Report, JHUworkshop.http://www.clsp.jhu.edu/ws99/proj eets/mt/final.report/mt-final-report.psBrown, P. F., S. S. Della Pietra, V. J. DellaPietra, and R. L. Mercer (1993).
Themathematics of statistical machinetranslation: Parameter estimation.Computational Linguistics, 19(2): 263-311.Bruce, Rebecca & Janyee Wiebe (1994).
Word-sense Disambiguation UsingDecomposable Models.
Proe.
of 32 ~dAssociation of Computational Linguistics,Las Cruces, NM.Dagan, Ido & Alon Itai (1994).
Word SenseDisambiguation Using a Second LanguageMonolingual Corpus.
ComputationalLinguistics 20, pp.
563-596Dyvik, Helge (1998).
A Translational Basis forSemantics.
In Stig Johansson and SigneOksefjell (eds.
): Corpora and Cross-linguistic Research: Theory, Method andCase Studies, 51-86.Fellbaum~ C.
(ed.)
(1998).
WordNet: AnElectronic Lexical Database.
MIT Press.Francis, W. & H. Ku~era (1982).
FrequencyAnalysis of English Usage.
HoughtonMifflin Co: New York.Ide, Nancy (in press).
Cross-lingual sensedetermination: Can it work?
Computersand the Humanities, 34.Lin, Dekang (1999).
A Case-base Algorithm forWord Sense Disambiguation.
PacificAssociation for Computational Linguistics,Waterloo, Canada.Melamed, I. Dan (2000).
Models ofTranslational Equivalence among Words,Computational Linguistics 26(2), pp.
221-249, June.Miller, G., M. Chodorow, S. Landes, C.Leacock, and R. Thomas (1994).
Using aSemantic Concordance for SenseIdentification.
ARPA Human LanguageTechnology Workshop, San Francisco, CA.Miller, George A., Richard Beckwith, ChristianeFellbaum~ Derek Gross and KatherineMiller (1990).
WordNet: An on-line lexicaldatabase.
International Journal ofLexicography, 3(4), 235-244.Oeh, Franz J.
& Herwann Ney (2000).
AComparison of Alignment Models forStatistical Machine Translation.
8 t~ Int.Conference on Computational Linguistics,Saarbrfieken, Germany, July.Resnik, Philip (1999).
Mining the Web forBilingual Text, 37 th meeting of Associationfor Computational Linguistics, CollegePark, Maryland, USA, Julle.Resnik, Philip (1997).
Selectional Preferenceand Sense Disambiguation, SIGLEXWorkshop on Tagging Text with LexiealSemantics: Why, What, and How?,Washington, D.C., USA, April.Reszfik, Philip (1999).
Semantic Similarity in aTaxonomy: An information-based Measureand its Application to Problems ofAmbiguity in Natural Language.
Journal ofArtificial Intelligence Research, 11, 95-130.Resnik, Philip & David Yarowsky (1998).Distinguishing Systems and DistinguishingSenses: New Evaluation Methods for WordSense Disambiguation.
Natural LanguageEngineering, 1,1-25.Yarowsky, David (1992).
Word-senseDisambiguation Using Statistical Models ofRoget~ Categories Trained on LargeCorpora.
Proe.
of 14 tu InternationalConference on Computational Linguistics,Nantes, France, July.Yarowsky, David (1995).
Unsupervised WordSense Disambiguation Rivalling SupervisedMethods.
33 ~t meeting of Association forComputational Linguistics, Cambridge, MA.
