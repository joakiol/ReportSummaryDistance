Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 109?112,New York, June 2006. c?2006 Association for Computational LinguisticsExtracting Salient Keywords from Instructional Videos Using Joint Text,Audio and Visual CuesYoungja Park and Ying LiIBM T.J. Watson Research CenterHawthorne, NY 10532{young park, yingli}@us.ibm.comAbstractThis paper presents a multi-modal feature-based system for extracting salient keywordsfrom transcripts of instructional videos.
Specif-ically, we propose to extract domain-specifickeywords for videos by integrating variouscues from linguistic and statistical knowledge,as well as derived sound classes and charac-teristic visual content types.
The acquisitionof such salient keywords will facilitate videoindexing and browsing, and significantly im-prove the quality of current video search en-gines.
Experiments on four government in-structional videos show that 82% of the salientkeywords appear in the top 50% of the highlyranked keywords.
In addition, the audiovisualcues improve precision and recall by 1.1% and1.5% respectively.1 IntroductionWith recent advances in multimedia technology, the num-ber of videos that are available to both general public andparticular individuals or organizations is growing rapidly.This consequently creates a high demand for efficientvideo searching and categorization as evidenced by theemergence of various offerings for web video searching.
1While videos contain a rich source of audiovisual in-formation, text-based video search is still among the mosteffective and widely used approaches.
However, the qual-ity of such text-based video search engines still lags be-hind the quality of those that search textual informationlike web pages.
This is due to the extreme difficulty oftagging domain-specific keywords to videos.
How toeffectively extract domain-specific or salient keywords1For example, see http://video.google.com andhttp://video.yahoo.comfrom video transcripts has thus become a critical andchallenging issue for both the video indexing and search-ing communities.Recently, with the advances in speech recognitionand natural language processing technologies, systemsare being developed to automatically extract keywordsfrom video transcripts which are either transcribed fromspeech or obtained from closed captions.
Most of thesesystems, however, simply treat all words equally or di-rectly ?transplant?
keyword extraction techniques devel-oped for pure text documents to the video domain withouttaking specific characteristics of videos into account (M.Smith and T. Kanade, 1997).In the traditional information retrieval (IR) field, mostexisting methods for selecting salient keywords rely pri-marily on word frequency or other statistical informa-tion obtained from a collection of documents (Salton andMcGill, 1983; Salton and Buckley, 1988).
These tech-niques, however, do not work well for videos for two rea-sons: 1) most video transcripts are very short, as com-pared to a typical text collection; and 2) it is impracticalto assume that there is a large video collection on a spe-cific topic, due to the video production costs.
As a result,many keywords extracted from videos using traditionalIR techniques are not really content-specific, and conse-quently, the video search results that are returned basedon these keywords are generally unsatisfactory.In this paper, we propose a system for extracting salientor domain-specific keywords from instructional videosby exploiting joint audio, visual, and text cues.
Specif-ically, we first apply a text-based keyword extraction sys-tem to find a set of keywords from video transcripts.
Thenwe apply various audiovisual content analysis techniquesto identify cue contexts in which domain-specific key-words are more likely to appear.
Finally, we adjust thekeyword salience by fusing the audio, visual and text cuestogether, and ?discover?
a set of salient keywords.Professionally produced educational or instructional109videos are the main focus of this work since they are play-ing increasingly important roles in people?s daily lives.For the system evaluation, we used training and educationvideos that are freely downloadable from various DHS(Department of Homeland Security) web sites.
Thesewere selected because 1) DHS has an increasing need forquickly browsing, searching and re-purposing its learningresources across its over twenty diverse agencies; 2) mostDHS videos contain closed captions in compliance withfederal accessibility requirements such as Section 508.2 A Text-based Keyword ExtractionSystemThis section describes the text-based keyword extrac-tion system, GlossEx, which we developed in our earlierwork (Park et al 2002).
GlossEx applies a hybrid method,which exploits both linguistic and statistical knowledge,to extract domain-specific keywords in a document col-lection.
GlossEx has been successfully used in large-scale text analysis applications such as document author-ing and indexing, back-of-book indexing, and contactcenter data analysis.An overall outline of the algorithm is given below.First, the algorithm identifies candidate glossary items byusing syntactic grammars as well as a set of entity recog-nizers.
To extract more cohesive and domain-specificglossary items, it then conducts pre-nominal modifierfiltering and various glossary item normalization tech-niques such as associating abbreviations with their fullforms, and misspellings or alternative spellings with theircanonical spellings.
Finally, the glossary items are rankedbased on their confidence values.The confidence value of a term T,C(T ), is defined asC(T ) = ?
?
TD(T ) + ?
?
TC(T ) (1)where TD and TC denote the term domain-specificityand term cohesion, respectively.
?
and ?
are two weightswhich sum up to 1.
The domain specificity is further de-fined asTD =?wi?TPd(wi)Pg(wi)| T | (2)where, | T | is the number of words in term T , pd(wi) isthe probability of word wi in a domain document collec-tion, and pg(wi) is the probability of word wi in a generaldocument collection.
And the term cohesion is defined asTC = | T | ?f(T )?
log10f(T )?wi?T f(wi)(3)where, f(T ) is the frequency of term T , and f(wi) is thefrequency of a component word wi.Finally, GlossEx normalizes the term confidence val-ues to the range of [0, 3.5].
Figure 1 shows the normal-ized distributions of keyword confidence values that weobtained from two instructional videos by analyzing theirtext transcripts with GlossEx.
Superimposed on each plotis the probability density function (PDF) of a gamma dis-tribution (Gamma(?, ?))
whose two parameters are di-rectly computed from the confidence values.
As we cansee, the gamma PDF fits very well with the data distrib-ution.
This observation has also been confirmed by othertest videos.0 0.5 1 1.5 2 2.5 3 3.5 40510152025 Video on Bioterrorism HistoryConfidence value 0 0.5 1 1.5 2 2.5 3 3.5 40510152025303540 Video on Massive Weapon DestructionConfidence value(a) (b)Figure 1: Normalized distribution of keyword salienciesfor two DHS video, superimposed by Gamma PDFs.3 Salient Keyword Extraction forInstructional VideosIn this section, we elaborate on our approach for extract-ing salient keywords from instructional videos based onthe exploitation of audiovisual and text cues.3.1 Characteristics of Instructional VideosCompared to general videos, professionally producedinstructional videos are usually better structured, that is,they generally contain well organized topics and sub-topics due to education nature.
In fact, there are certaintypes of production patterns that could be observed fromthese videos.
For instance, at the very beginning sectionof the video, a host will usually give an overview of themain topics (as well as a list of sub-topics) that are tobe discussed throughout the video.
Then each individualtopic or sub-topic is sequentially presented following apre-designed order.
When one topic is completed, someinformational credit pages will be (optionally) displayed,followed by either some informational title pages show-ing the next topic, or a host introduction.
A relativelylong interval of music or silence that accompanies thistransitional period could usually be observed in this case.To effectively deliver the topics or materials to an au-dience, the video producers usually apply the followingtypes of content presentation forms: host narration, inter-views and site reports, presentation slides and informa-tion bulletins, as well as assisted content that are relatedwith the topic under discussion.
For convenience, we callthe last two types as informative text and linkage scene110in this work.
Figure 2 shows the individual examples ofvideo frames that contain narrator, informative text, andthe linkage scene.
(a) (b) (c)Figure 2: Three visual content types: (a) narrator, (b) in-formative text, and (c) linkage scene.3.2 AudioVisual Content AnalysisThis section describes our approach on mining the afore-mentioned content structure and patterns for instructionalvideos based on the analysis of both audio and visual in-formation.
Specifically, given an instructional video, wefirst apply an audio classification module to partition itsaudio track into homogeneous audio segments.
Each seg-ment is then tagged with one of the following five soundlabels: speech, silence, music, environmental sound, andspeech with music (Li and Dorai, 2004).
The supportvector machine technique is applied for this purpose.Meanwhile, a homogeneous video segmentationprocess is performed which partitions the video into aseries of video segments in which each segment con-tains content in the same physical setting.
Two groupsof visual features are then extracted from each segmentso as to further derive its content type.
Specifically, fea-tures regarding the presence of human faces are first ex-tracted using a face detector, and these are subsequentlyapplied to determine if the segment contains a narrator.The other feature group contains features regarding de-tected text blobs and sentences from the video?s text over-lays.
This information is mainly applied to determine ifthe segment contains informative text.
Finally, we labelsegments that do not contain narrators or informative textas linkage scenes.
These could be an outdoor landscape, afield demonstration or indoor classroom overview.
Moredetails on this part are presented in (Li and Dorai, 2005).The audio and visual analysis results are then inte-grated together to essentially assign a semantic audiovi-sual label to each video segment.
Specifically, given asegment, we first identify its major audio type by findingthe one that lasts the longest.
Then, the audio and visuallabels are integrated in a straightforward way to reveal itssemantics.
For instance, if the segment contains a narra-tor while its major audio type is music, it will be taggedas narrator with music playing.
A total of fifteen possi-ble constructs is thus generated, coming from the com-bination of three visual labels (narrator, informative textand linkage scene) and five sound labels (speech, silence,music, environmental sound, and speech with music).3.3 AudioVisual and Text Cues for Salient KeywordExtractionHaving acquired video content structure and segmentcontent types, we now extract important audiovisual cuesthat imply the existence of salient keywords.
Specifically,we observe that topic-specific keywords are more likelyappearing in the following scenarios (a.k.a cue context):1) the first N1 sentences of segments that contain narra-tor presentation (i.e.
narrator with speech), or informa-tive text with voice-over; 2) the first N2 sentences of anew speaker (i.e.
after a speaker change); 3) the questionsentence; 4) the first N2 sentences right after the ques-tion (i.e.
the corresponding answer); and 5) the first N2sentences following the segments that contain silence, orinformative text with music.
Specifically, the first 4 cuesconform with our intuition that important content sub-jects are more likely to be mentioned at the beginning partof narration, presentation, answers, as well as in ques-tions; while the last cue corresponds to the transitionalperiod between topics.
Here, N1 is a threshold whichwill be automatically adjusted for each segment duringthe process.
Specifically, we set N1 to min(SS, 3) whereSS is the number of sentences that are overlapped witheach segment.
In contrast, N2 is fixed to 2 for this workas it is only associated with sentences.Note that currently we identify the speaker changesand question sentences by locating the signature charac-ters (such as ?>>?
and ???)
in the transcript.
However,when this information is unavailable, numerous exist-ing techniques on speaker change detection and prosodyanalysis could be applied to accomplish the task (Chenet al, 1998).3.4 Keyword Salience AdjustmentNow, given each keyword (K) obtained from GlossEx,we recalculate its salience by considering the followingthree factors: 1) its original confidence value assigned byGlossEx (CGlossEx(K)); 2) the frequency of the keywordoccurring in the aforementioned cue context (Fcue(K));and 3) the number of component words in the keyword(|K|).
Specifically, we give more weight or incentive(I(K)) to keywords that are originally of high confi-dence, appear more frequently in cue contexts, and havemultiple component words.
Note that if keyword K doesnot appear in any cue contexts, its incentive value will bezero.Figure 3 shows the detailed incentive calculation steps.Here, mode and ?
denote the mode and standard devia-tion derived from the GlossEx ?s confidence value distri-bution.
MAX CONFIDENCE is the maximum con-fidence value used for normalization by GlossEx, whichis set to 3.5 in this work.
As we can see, the three afore-mentioned factors have been re-transformed into C(K),F (K) and L(K), respectively.
Please also note that we111have re-adjusted the frequency of keyword K in the cuecontext if it is larger than 10.
This intends to reduce thebiased influence of a high frequency.
Finally, we add asmall value ?
to |K| and Fcue respectively in order toavoid zero values for F (K) and L(K).
Now, we havesimilar value scales for F (K) and L(K) ([1.09, 2.xx])and C(K) ([0, 2.yy]), which is desirable.As the last step, we boost keyword K?s originalsalience CGlossEx(K) by I(K).if (CGlossEx(K) >= modeC(K) = CGlossEx(K)modeelse C(K) = CGlossEx(K)MAX CONFIDENCEif ( Fcue(K) > 10)Fcue(K) = 10 + log10(Fcue(K)?
10)F (K) = ln(Fcue(K) + ?
)L(K) = ln(|K|+ ?
)I(K) = ?
?
C(K)?
F (K)?
L(K)Figure 3: Steps for computing incentive value for key-word K appearing in cue context4 Experimental ResultsFour DHS videos were used in the experiment, whichcontain diverse topics ranging from bio-terrorism history,weapons of mass destruction, to school preparation forterrorism.
The video length also varies a lot from 30minutes to 2 hours.
Each video also contains a variety ofsub-topics.
Video transcripts were acquired by extractingthe closed captions with our own application.To evaluate system performance, we compare the key-words generated from our system against the human-generated gold standard.
Note that for this experiment,we only consider nouns and noun phrases as keywords.To collect the ground truth, we invited a few human eval-uators, showed them the four test videos, and presentedthem with all candidate keywords extracted by GlossEx.We then asked them to label all keywords that they con-sidered to be domain-specific, which is guidelined by thefollowing question: ?would you be satisfied if you get thisvideo when you use this keyword as a search term?
?.Table 1 shows the number of candidate keywords andmanually labeled salient keywords for all four test videos.As we can see, approximately 50% of candidate key-words were judged to be domain-specific by humans.Based on this observation, we selected the top 50% ofhighly ranked keywords based on the adjusted salience,and examined their presence in the pool of salient key-words for each video.
As a result, an average of 82%of salient keywords were identified within these top 50%of re-ranked keywords.
In addition, the audiovisual cuesimprove precision and recall by 1.1% and 1.5% respec-tively.videos v1 v2 v3 v4no.
of candidate keywords 477 934 1303 870no.
of salient keywords 253 370 665 363ratio of salient keywords 53% 40% 51% 42%Table 1: The number of candidate and manually labeledsalient keywords in the four test videos5 Conclusion and Future WorkWe described a mutimodal feature-based system for ex-tracting salient keywords from instructional videos.
Thesystem utilizes a richer set of information cues which notonly include linguistic and statistical knowledge but alsosound classes and characteristic visual content types thatare available to videos.
Experiments conducted on theDHS videos have shown that incorporating multimodalfeatures for extracting salient keywords from videos isuseful.Currently, we are performing more sophisticated ex-periments on different ways to exploit additional audio-visual cues.
There is also room for improving the calcu-lation of the incentive values of keywords.
Our next planis to conduct an extensive comparison between GlossExand the proposed scheme.ReferencesY.
Park, R. Byrd and B. Boguraev.
2002.
Automatic Glos-sary Extraction: Beyond Terminology Identification.
Proc.of the 19th International Conf.
on Computational Linguistics(COLING02), pp 772?778.Y.
Li and C. Dorai.
2004 SVM-based Audio Classification forInstructional Video Analysis.
IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP?04).Y.
Li and C. Dorai.
2005 Video frame identification for learn-ing media content understanding.
IEEE International Con-ference on Multimedia & Expo (ICME?05).M.
Smith and T. Kanade.
1997 Video Skimming and Charac-terization through the Combination of Image and LanguageUnderstanding Techniques.
IEEE Computer Vision and Pat-tern Recognition, pp.
775-781.G.
Salton and J. McGill 1983.
Introduction to modern infor-mation Retrieval.
.
New York: McGraw-Hill.G.
Salton and C. Buckley 1988.
Term-Weighting Approachesin Automatic Text Retrieval.
Information Processing & Man-agement, 24 (5), 513-523.S.
Chen and P. Gopalakrishnan 1998.
Speaker, Environ-ment and Channel Change Detection and Clustering via theBayesian Information Criterion.
Proc.
of DARPA BroadcastNews Transcription and Understanding Workshop.112
