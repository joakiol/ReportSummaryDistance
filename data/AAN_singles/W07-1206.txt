Proceedings of the 5th Workshop on Important Unresolved Matters, pages 41?48,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsQuestion Answering based on Semantic RolesMichael Kaisser Bonnie WebberUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWScotlandm.kaisser@sms.ed.ac.uk, bonnie@inf.ed.ac.ukAbstractThis paper discusses how lexical resourcesbased on semantic roles (i.e.
FrameNet,PropBank, VerbNet) can be used for Ques-tion Answering, especially Web QuestionAnswering.
Two algorithms have been im-plemented to this end, with quite differentcharacteristics.
We discuss both approacheswhen applied to each of the resources and acombination of these and give an evaluation.We argue that employing semantic roles canindeed be highly beneficial for a QA system.1 IntroductionA large part of the work done in NLP deals withexploring how different tools and resources can beused to improve performance on a task.
The qualityand usefulness of the resource certainly is a majorfactor for the success of the research, but equally sois the creativity with which these tools or resourcesare used.
There usually is more than one way toemploy these, and the approach chosen largely de-termines the outcome of the work.This paper illustrates the above claims with re-spect to three lexical resources ?
FrameNet (Bakeret al, 1998), PropBank (Palmer et al, 2005) andVerbNet (Schuler, 2005) ?
that convey informationabout lexical predicates and their arguments.
We de-scribe two new and complementary techniques forusing these resources and show the improvements tobe gained when they are used individually and thentogether.
We also point out problems that must beovercome to achieve these results.Compared with WordNet (Miller et al, 1993)?which has been used widely in QA?FrameNet, Prop-Bank and VerbNet are still relatively new, and there-fore their usefulness for QA has still to be proven.They offer the following features which can be usedto gain a better understanding of questions, sen-tences containing answer candidates, and the rela-tions between them:?
They all provide verb-argument structures for alarge number of lexical entries.?
FrameNet and PropBank contain semanticallyannotated sentences that exemplify the under-lying frame.?
FrameNet contains not only verbs but also lex-ical entries for other part-of-speeches.?
FrameNet provides inter-frame relations thatcan be used for more complex paraphrasing tolink the question and answer sentences.In this paper we describe two methods that usethese resources to annotate both questions and sen-tences containing answer candidates with seman-tic roles.
If these annotations can successfully bematched, an answer candidate can be extracted.
Weare able, for example, to give a complete frame-semantic analysis of the following sentences and torecognize that they all contain an answer to the ques-tion ?When was Alaska purchased??
:The United States purchased Alaska in 1867.Alaska was bought from Russia in 1867.In 1867, Russia sold Alaska to the United States.The acquisition of Alaska by the United Statesin 1867 is known as ?Seward?s Folly.41The first algorithm we present uses the threelexical resources to generate potential answer-containing templates.
While the templates containholes ?
in particular, for the answer ?
the parts thatare known can be used to create exact quoted searchqueries.
Sentences can then be extracted from theoutput of the search engine and annotated with re-spect to the resource being used.
From this, an an-swer candidate (if present) can be extracted.
Thesecond algorithm analyzes the dependency structureof the annotated example sentences in FrameNet andPropBank.
It then poses rather abstract queries to theweb, but can in its candidate sentence analysis stagedeal with a wider range of syntactic possibilities.
Aswe will see, the two algorithms are nicely comple-mentary.2 Method 1: Question Answering byNatural Language GenerationThe first method implemented uses the data avail-able in the resources to generate potential answersentences to the question.
While at least one com-ponent of such a sentence (the answer) is yet un-known, the remainder of the sentence can be used toquery a web search engine.
The results can then beanalyzed, and if they match the originally-proposedanswer sentence structure, an answer candidate canbe extracted.The first step is to annotate the question with itssemantic roles.
For this task we use a classical se-mantic role labeler combined with a rule-based ap-proach.
Keep in mind that our task is to annotatequestions, not declarative sentences.
This is impor-tant for several reasons:1.
The role labeler we use is trained on FrameNetand PropBank data, i.e.
mostly on declarativesentences, whose syntax often differs consider-ably from the sytax of questions.
Aa a result,the training and test set differ substantially innature.2.
Questions tend to be shorter and simpler syn-tactically than declarative sentences?especiallythose occurring in news corpora.3.
Questions contain one semantic role that has tobe annotated but which is not or is only implic-itly (through the question word) mentioned ?the answer.Because of these reasons and especially becausemany questions tend to be gramatically simple, wefound that a few simple rules can help the questionannotation process dramatically.
We rely on Mini-Par (Lin, 1998) to find the question?s head verb, e.g.?purchase?
for ?Who purchased YouTube??
(In thefollowing we will often refer to this question to il-lustrate our approach.)
We then look up all entriesin one of the resources, and for FrameNet and Prop-Bank we simplify the annotated sentences until weachieve a set of abstract frame structures, similar tothose in VerbNet.
By doing this we intentionally re-move certain levels of information that were presentin the original data, i.e.
tense, voice, mood and nega-tion.
(In a later step we will reintroduce some of it.
)Here is what we find in FrameNet for ?purchase?
:Buyer[Subj,NP] VERB Goods[Obj,NP]Buyer[Subj,NP] VERB Goods[Obj,NP]Seller[Dep,PP-from]Buyer[Subj,NP] VERB Goods[Obj,NP]Money[Dep,PP-for]Buyer[Subj,NP] VERB Goods[Obj,NP]Recipient[Dep,PP-for]...A syntactic analysis of the question (also obtainedfrom MiniPar) shows that ?Who?
is the (deep) sub-ject and ?YouTube?, the (deep) object.
The first ofthe above frames fits this analysis best, because itlists only the two roles with the desired grammaticalfunctions.
By mapping the question analysis to thisframe, we can assign the roles Goods to ?YouTube?and Buyer to ?Who?.
From this we can concludethat the question asks for the Buyer role.An additional point suitable to illustrate why afew simple rules can achieve in many cases morethat a statistical classifier, are When- and Where-questions.
Here, the hint that leads to the correct de-tection of the answer role lies in the question word,which is of course not present in the answer sen-tence.
Furthermore, the answer role in an answersentence will usually be realized as a PP with a to-tally different dependency path than the one of ques-tion?s question word.
In contrast, a rule that statesthat whenever a temporal or location question is de-tected the answer role becomes, in FrameNet terms,Place or Time, respectively, is very helpful here.Once the role assignment is complete, we useall abstract frames which contain the roles found inthe question to generate potential answer templates.42This is also the point where we reintroduce tense andvoice information:1 If the question was asked in thea past tense, we will now create from each abstractframe, all surface realizations in all past tenses, bothin active and passive voice.
If we had used the an-notated data directly without the detour over the ab-stract frames, we would have difficulty sorting outnegated sentences, those in undesired moods andthose in unsuitable tenses.
In contrast our approachguarantees that all possible tenses in both voices aregenerated, and no meaning-altering information likemood and negation is present.
For the example givenabove we would create inter alia the following an-swer templates:ANSWER[NP] purchased YouTubeYouTube was purchased by ANSWER[NP]ANSWER[NP] had purchased YouTube...The part (or parts) of the templates that areknown are quoted and sent to a search en-gine.
For the second example, this would be"YouTube was purchased by".
From the snippetsreturned by the search engine, we extract candi-date sentences and match them against the abstractframe structure from which the queries were origi-nally created.
In this way, we annotate the candidatesentences and are now able to identify the filler ofthe answer role.
For example, the above query re-turns ?On October 9, 2006, YouTube was purchasedby Google for an incredible US$1.65 billion?, fromwhich we can extract ?Google?, because it is the NPfilling the buyer role.So far, we have mostly discussed questions whoseanswer role is an argument of the head verb.
How-ever, for questions like ?When was YouTube pur-chased??
this assumption does not hold.
Here, thequestion asks for an adjunct.
This is an importantdifference for at least three reasons:1.
FrameNet and VerbNet do not or only sparselyannotate peripheral adjuncts.
(PropBank how-ever does.)2.
In English, the position of adjuncts varies muchmore than those of arguments.3.
In English, different kinds of adjuncts can oc-cupy the same position in a sentence, althoughnaturally not at the same time.1While we strip off mood and negation during the creationof the abstract frames, we have not yet reintroduced them.The following examples illustrate point 2:YouTube was purchased by Google on October 9.On October 9, YouTube was purchased by Google.YouTube was purchased on October 9 by Google.All variations are possible, although they may dif-fer in frequency.
PPs conveying other peripheral ad-juncts ( e.g.
?for $1.65 billion?)
could replace all theabove temporals PPs, or they could be added at otherpositions.The special behavior of these types of questionshas not only to be accounted for when annotatingthe question with semantic roles, but also and whencreating and processing potential answer sentences.We use an abstract frame structure like the followingto create the queries:Buyer[Subj,NP,unknown]VERB Goods[Obj,NP,"YouTube"]While this lacks a role for the answer, wecan still use it to create, for example, the query"has purchased YouTube".
When sentences re-turned from the search engine are then matchedagainst the abstract structure, we can extract all PPsdirectly before the Buyer role, between the Buyerrole and the verb and directly behind the Goods role.Then we can check all these PPs on their semantictypes and keep only those that match the answer typeof the question (if any).3 Making use of FrameNet Frames andInter-Frame RelationsThe method presented so far can be used with allthree resources.
But FrameNet goes a step furtherthan just listing verb-argument structures: It orga-nizes all of its lexical entries in frames2, with rela-tions between frames that can be used for a widerparaphrasing and inference.
This section will ex-plain how we make use of these relations.The purchase.v entry is organized in a framecalled Commerce buy which also contains theentries for buy.v and purchase ((act)).n.
Boththese entries are annotated with the same frameelements as purchase.v.
This makes it possible toformulate alternative answer templates, for exam-ple: YouTube was bought by ANSWER[NP] and2Note the different meaning of frame in FrameNet and Prop-Bank/VerbNet respectively.43ANSWER[NP-Genitive] purchase of YouTube.The latter example illustrates that we can alsogenerate target paraphrases with heads which arenot verbs.
Handling these is usually easier thansentences based on verbs, because no tense/voiceinformation has to be introduced.Furthermore, frames themselves can stand indifferent relations.
The frame Commerce goods-transfer, for example, relates both to the alreadymentioned Commerce buy frame and to Com-merce sell in an is perspectivized in relation.
Thelatter contains the lexical entries retail.v, retailer.n,sale.n, sell.v, vend.v and vendor.n.
Again, theframe elements used are the same as for pur-chase.v.
Thus we can now create answer templateslike YouTube was sold to ANSWER[NP].
Othertemplates created from this frame seem odd, e.g.YouTube has been retailed to ANSWER[NP].because the verb ?to retail?
usually takes mass-products as its object argument and not a company.But FrameNet does not make such fine-graineddistinctions.
Interestingly, we did not come acrossa single example in our experiments where sucha phenomenon caused an overall wrong answer.Sentences like the one above will most likely not befound on the web (just because they are in a narrowsemantic sense not well-formed).
Yet even if wewould get a hit, it probably would be a legitimate tocount the odd sentence ?YouTube had been retailedto Google?
as evidence for the fact that Googlebought YouTube.4 Method 2: Combining Semantic Rolesand Dependency PathsThe second method we have implemented com-pares the dependency structure of example sentencesfound in PropBank and FrameNet with the depen-dency structure of candidate sentences.
(VerbNetdoes not list example sentences for lexical entries,so could not be used here.
)In a pre-processing step, all example sentences inPropBank and FrameNet are analyzed and the de-pendency paths from the head to each of the frameelements are stored.
For example, in the sentence?The Soviet Union has purchased roughly eight mil-lion tons of grain this month?
(found in PropBank),?purchased?
is recognized as the head, ?The So-viet Union?
as ARG0, ?roughly eight million tons ofgrain?
as ARG1, and ?this month?
as an adjunct oftype TMP.
The stored paths to each are as follows:headPath = ?
irole = ARG0, paths = {?s, ?subj}role = ARG1, paths = {?obj}role = TMP, paths = {?mod}This says that the head is at the root, ARG0 is at bothsurface subject (s) and deep subject (subj) position3,ARG1 is the deep object (obj), and TMP is a directadjunct (mod) of the head.Questions are annotated as described in Section 2.Sentences that potentially contain answer candidatesare then retrieved by posing a rather abstract queryconsisting of key words from the question.
Oncewe have obtained a set of candidate-containing sen-tences, we ask the following questions of their de-pendency structures compared with those of the ex-ample sentences from PropBank4:1a Does the candidate-containing sentence sharethe same head verb as the example sentence?1b Do the candidate sentence and the example sen-tence share the same path to the head?2a In the candidate sentence, do we find one ormore of the example?s paths to the answer role?2b In the candidate sentence, do we find all of theexample?s paths to the answer role?3a Can some of the paths for the other roles befound in the candidate sentence?3b Can all of the paths for the other roles be foundin the candidate sentence?4a Do the surface strings of the other roles par-tially match those of the question?4b Do the surface strings of the other roles com-pletely match those of the question?Tests 1a and 2a of the above are required criteria:If the candidate sentence does not share the samehead verb or if we can find no path to the answerrole, we exclude it from further processing.3MiniPar allows more than one path between nodes due, forexample, to traces.
The given example is MiniPar?s way of in-dicating that this is a sentence in active voice.4Note that our proceeding is not too different from what aclassical role labeler would do: Both approaches are primarilybased on comparing dependency paths.
However, a standardrole labeler would not take tests 3a, 3b, 4a and 4b into account.44Each sentence that passes steps 1a and 2a isassigned a weight of 1.
For each of the remainingtests that succeeds, we multiply that weight by2.
Hence a candidate sentence that passes all thetests is assigned a weight 64 times higher than acandidate that only passes tests 1a and 2a.
We takethis as reasonable, as the evidence for having founda correct answer is indeed very weak if only tests 1aand 2a succeeded and very high if all tests succeed.Whenever condition 2a holds, we can extract ananswer candidate from the sentence: It is the phrasethat the answer role-path points to.
All extractedanswers are stored together with their weights, ifwe retrieve the same answer more than once, wesimple add the new weight to the old ones.
Afterall candidate sentences have been compared withall pre-extracted structures, the ones that do notshow the correct semantic type are removed.
Thisis especially important for answers that are realizedas adjuncts, see Section 2.
We choose the answercandidate with the highest score as the final answer.We now illustrate this method with respect to ourquestion ?Who purchased YouTube??
The roles as-signment process produces this result: ?YouTube?is ARG1 and the answer is ARG0.
From the webwe retrieve inter alia the following sentence: ?Theiraim is to compete with YouTube, which Google re-cently purchased for more than $1 billion.?
The de-pendency analysis of the relevant phrases is:headPath = ?i?i?pred?i?mod?pcom-n?rel?iphrase = ?Google?, paths = {?s, ?subj}phrase = ?which?, paths = {?obj}phrase = ?YouTube?, paths = {?i?rel}phrase = ?for more than $1 billion?, paths = {?mod}If we annotate this sentence by using the analy-sis from the above example sentence (?The SovietUnion has purchased ...?)
we get the following (par-tially correct) role assignment: ?Google?
is ARG0,?which?
is ARG1, ?for more than $1 billion?
is TMP.The following table shows the results of the 8 testsdescribed above:1a OK1b ?2a OK2b OK3a OK3b OK4a ?4b ?Test 1a and 2a succeeded, so this sentence is as-signed an initial weight of 1.
However, only threeother tests succeed as well, so its final weight is8.
This rather low weight for a positive candi-date sentence is due to the fact that we comparedit against a dependency structure which it only par-tially matched.
However, it might very well be thecase that another of the annotated sentences shows aperfect fit.
In such a case this comparison wouldresult in a weight of 64.
If these were the onlytwo sentences that produce a weight of 1 or greater,the final weight for this answer candidate would be8 + 64 = 72.5 EvaluationWe choose to evaluate our experiments with theTREC 2002 QA test set because test sets from 2004and beyond contain question series that pose prob-lems that are separate from the research describedin this paper.
While we participated in TREC 2004,2005 and 2006, with an anaphora-resolution com-ponent that performed quite well, we feel that ifone wants to evaluate a particular method, adding anadditional module, unrelated to the actual problem,can distort the results.
Additionally, because we aresearching for answers on the web rather than in theAQUAINT corpus, we do not distinguish betweensupported and unsupported judgments.Of the 500 questions in the TREC 2002 test set,236 have be as their head verb.
As the work de-scribed here essentially concerns verb semantics,such questions fall outside its scope.
Evaluationhas thus been carried out on only the remaining 264questions.For the first method (cf.
Section 2), we evaluatedsystem accuracy separately for each of the three re-sources, and then together, obtaining the followingvalues:FrameNet PropBank VerbNet combined0.181 0.227 0.223 0.261For the combined run we looked up the verbin all three resources simultaneously and all en-tries from every resource were used.
As canbe seen, PropBank and VerbNet perform equallywell, while FrameNet?s performance is significantlylower.
These differences are due to coverage issues:FrameNet is still in development, and further ver-sions with a higher coverage will be released.
How-ever, a closer look shows that coverage is a problemfor all of the resources.
The following table showsthe percentage of the head verbs that were looked45up during the above experiments based on the 2002question set, that could not be found (not found).
Italso lists the percentage of lexical entries that con-tain no annotated sentences (s = 0), five or fewer(s <= 5), ten or fewer (s <= 10), or more than50 (s > 50).
Furthermore, the table lists the aver-age number of lexical entries found per head verb(avg senses) and the average number of annotatedsentences found per lexical entry (avg sent).
5FrameNet PropBanknot found 11% 8%s = 0 41% 7%s <= 5 48% 35%s <= 10 57% 45%s > 50 8% 23%avg senses 2.8 4.4avg sent.
16.4 115.0The problem with lexical entires only containinga small number of annotated sentences is that thesesentences often do not exemplify common argumentstructures, but rather seldom ones.
As a solution tothis coverage problem, we experimented with a cau-tious technique for expanding coverage.
Any headverb, we assumed displays the following three pat-terns:intransitive: [ARG0] VERBtransitive: [ARG0] VERB [ARG1]ditransitive: [ARG0] VERB [ARG1] [ARG2]During processing, we then determined whetherthe question used the head verb in a standard in-transitive, transitive or ditransitive way.
If it did,and that pattern for the head verb was not containedin the resources, we temporarily added this abstractframe to the list of abstract frames the system used.This method rarely adds erroneous data, because thequestion shows that such a verb argument structureexists for the verb in question.
By applying this tech-nique, the combined performance increased from0.261 to 0.284.In Section 2 we reported on experiments thatmake use of FrameNet?s inter-frame relations.
Thenext table lists the results we get when (a) using onlythe question head verb for the reformulations, (b) us-ing the other entries in the same frame as well, (c)using all entries in all frames to which the starting5As VerbNet contains no annotated sentences, it is not listed.Note also, that these figures are not based on the resources intotal, but on the head verbs we looked up for our evaluation.frame is related via the Inheritance, Perspective onand Using relations (by using only those frameswhich show the same frame elements).
(a) only question head verb 0.181(b) all entries in frame 0.204all entries in related frames(c) (with same frame elements) 0.215Our second method described in Section 4, canonly be used with FrameNet and PropBank, becauseVerbNet does not give annotated example sentences.Here are the results:FrameNet PropBank0.030 0.159Analysis shows that PropBank dramatically out-performs FrameNet for three reasons:1.
PropBank?s lexicon contains more entries.2.
PropBank provides many more example sen-tences for each entry.3.
FrameNet does not annotate peripheral ad-juncts, and so does not apply to When- orWhere-questions.The methods we have described above are com-plementary.
When they are combined so that whenmethod 1 returns an answer it is always chosenas the final one, and only if method 1 did notreturn an answer were the results from method2 used, we obtain a combined accuracy of 0.306when only using PropBank.
When using method 1with all three resources and our cautious coverage-extension strategy, with all additional reformulationsthat FrameNet can produce and method 2, usingPropBank and FrameNet, we achieve an accuracy of0.367.We also evaluated how much increase the de-scribed approaches based on semantic roles bring toour existing QA system.
This system is completlyweb-based and employs two answer finding strate-gies.
The first is based on syntactic reformulationrules, which are similar to what we described in sec-tion 2.
However, in contrast to the work describedin this paper, these rules are manually created.
Thesecond strategy uses key words from the question asqueries, and looks for frequently occuring n-gramsin the snippets returned by the search engine.
Thesystem received the fourth best result for factoids inTREC 2004 (Kaisser and Becker, 2004) (where both46just mentioned approaches are described in more de-tail) and TREC 2006 (Kaisser et al, 2006), so it initself is a state-of-the-art, high performing QA sys-tem.
We observe an increase in performance by 21%over the mentioned baseline system.
(Without thecomponents based on semantic roles 130 out of 264questions are answered correct, with these compo-nents 157.
)6 Related WorkSo far, there has been little work at the intersectionof QA and semantic roles.
Fliedner (2004) describesthe functionality of a planned system based on theGerman version of FrameNet, SALSA, but no so farno paper describing the completed system has beenpublished.Novischi and Moldovan (2006) use a techniquethat builds on a combination of lexical chains andverb argument structures extracted from VerbNet tore-rank answer candidates.
The authors?
aim is torecognize changing syntactic roles in cases wherean answer sentence shows a head verb different fromthe question (similar to work described here in Sec-tion 2).
However, since VerbNet is based on the-matic rather than semantic roles, there are problemsin using it for this purpose, illustrated by the follow-ing VerbNet pattern for buy and sell:[Agent] buy [Theme] from [Source][Agent] sell [Recipient] [Theme]Starting with the sentence ?Peter bought a guitarfrom Johnny?, and mapping the above roles for buyto those for sell, the resulting paraphrase in termsof sell would be ?Peter sold UNKNOWN a guitar?.That is, there is nothing blocking the Agent role ofbuy being mapped to the Agent role of sell, nor any-thing linking the Source role of buy to any role insell.
There is also a coverage problem: The authorsreport that their approach only applies to 15 of 230TREC 2004 questions.
They report a performancegain of 2.4% (MMR for the top 50 answers), but itdoes not become clear whether that is for these 15questions or for the complete question set.The way in which we use the web in our firstmethod is somewhat similar to (Dumais et al, 2002).However, our system allows control of verb argu-ment structures, tense and voice and thus we cancreate a much larger set of reformulations.Regarding our second method, two papers de-scribe related ideas: Firstly, in (Bouma et al, 2005)the authors describe a Dutch QA system whichmakes extensive use of dependency relations.
In apre-processing step they parsed and stored the fulltext collection for the Dutch CLEF QA-task.
Whentheir system is asked a question, they match the de-pendency structure of the question against the de-pendency structures of potential answer candidates.Additionally, a set of 13 equivalence rules allowstransformations of the kind ?the coach of Norway,Egil Olsen?
?
?Egil Olsen, the coach of Norway?.Secondly, Shen and Klakow (2006) use depen-dency relation paths to rank answer candidates.
Intheir work, a candidate sentence supports an answerif relations between certain phrases in the candidatesentence are similar to the corresponding ones in thequestion.Our work complements that described in boththese papers, based as it is on a large collection ofsemantically annotated example sentences: We onlyrequire a candidate sentence to match one of the an-notated example sentences.
This allows us to dealwith a much wider range of syntactic possibilities, asthe resources we use do not only document verb ar-gument structures, but also the many ways they canbe syntactically realized.7 DiscussionBoth methods presented in this paper employ se-mantic roles but with different aims in mind: Thefirst method focuses on creating obvious answer-containing sentences.
Because in these sentences,the head and the semantic roles are usually adjacent,it is possible to create exact search queries that willlead to answer candidates of a high quality.
Oursecond method can deal with a wider range of syn-tactic variations but here the link to the answer sen-tences?
surface structure is not obvious, thus no ex-act queries can be posed.The overall accuracy we achieved suggests thatemploying semantic roles for question answering isindeed useful.
Our results compare nicely to re-cent TREC evaluation results.
This is an especiallystrong point, because virtually all high performingTREC systems combine miscellaneous strategies,which are already know to perform well.
Because47the research question driving this work was to deter-mine how semantic roles can benefit QA, we deliber-ately designed our system to only build on semanticroles.
We did not chose to extend an already exist-ing system, using other methods with a few featuresbased on semantic roles.Our results are convincing qualitatively as well asquantitavely: Detecting paraphrases and drawing in-ferences is a key challenge in question answering,which our methods achieve in various ways:?
They both recognize different verb-argumentstructures of the same verb.?
Method 1 controls for tense and voice: Our sys-tem will not take a future perfect sentence foran answer to a present perfect question.?
For method 1, no answer candidates altered bymood or negation are accepted.?
Method 1 can create and recognize answer sen-tences, whose head is synonymous or related inmeaning to the answers head.
In such transfor-mations, we are also aware of potential changesin the argument structure.?
The annotated sentences in the resources en-ables method 2 to deal with a wide range ofsyntactic phenomena.8 ConclusionThis paper explores whether lexical resources likeFrameNet, PropBank and VerbNet are beneficial forQA and describes two different methods in whichthey can be used.
One method uses the data in theseresources to generate potential answer-containingsentences that are searched for on the web by usingexact, quoted search queries.
The second methoduses only a keyword-based search, but it can anno-tate a larger set of candidate sentences.
Both meth-ods perform well solemnly and they nicely comple-ment each other.
Our methods based on semanticroles alone achieves an accuracy of 0.39.
Further-more adding the described features to our alreadyexisting system boosted accuracy by 21%.AcknowledgmentsThis work was supported by Microsoft Researchthrough the European PhD Scholarship Programme.ReferencesColin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of COLING-ACL.Gosse Bouma, Jori Mur, Gertjan van Noord, Lonnekevan der Plas, and Jo?rg Tiedemann.
2005.
QuestionAnswering for Dutch using Dependency Relations.
InProceedings of the CLEF 2005 Workshop.Susan Dumais, Michele Bankom, Eric Brill, Jimmy Lin,and Andrew Ng.
2002.
Web Question Answering: IsMore Always Better?
Proceedings of UAI 2003.Gerhard Fliedner.
2004.
Towards Using FrameNet forQuestion Answering.
In Proceedings of the LREC2004 Workshop on Building Lexical Resources fromSemantically Annotated Corpora.Michael Kaisser and Tilman Becker.
2004.
Question An-swering by Searching Large Corpora with LinguisticMethods.
In The Proceedings of the 2004 Edition ofthe Text REtrieval Conference, TREC 2004.Michael Kaisser, Silke Scheible, and Bonnie Webber.2006.
Experiments at the University of Edinburgh forthe TREC 2006 QA track.
In The Proceedings of the2006 Edition of the Text REtrieval Conference, TREC2006.Dekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
In Workshop on the Evaluation of ParsingSystems.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1993.
In-troduction to WordNet: An On-Line Lexical Database.Adrian Novischi and Dan Moldovan.
2006.
QuestionAnswering with Lexical Chains Propagating Verb Ar-guments.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the ACL.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Karin Kipper Schuler.
2005.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. thesis,University of Pennsylvania.Dan Shen and Dietrich Klakow.
2006.
Exploring Corre-lation of Dependency Relation Paths for Answer Ex-traction.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the ACL.48
