Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 30?38,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Language-Independent Approach to Automatic Text DifficultyAssessment for Second-Language LearnersWade Shen1, Jennifer Williams1, Tamas Marius2, and Elizabeth Salesky ?11MIT Lincoln Laboratory Human Language Technology Group,244 Wood Street Lexingon, MA 02420, USA{swade,jennifer.williams,elizabeth.salesky}@ll.mit.edu2DLI Foreign Language Center, Bldg.
420, Room 119 Monterey, CA 93944, USAtamas.g.marius.civ@mail.milAbstractIn this paper, we introduce a new base-line for language-independent text diffi-culty assessment applied to the Intera-gency Language Roundtable (ILR) profi-ciency scale.
We demonstrate that readinglevel assessment is a discriminative prob-lem that is best-suited for regression.
Ourbaseline uses z-normalized shallow lengthfeatures and TF-LOG weighted vectors onbag-of-words for Arabic, Dari, English,and Pashto.
We compare Support VectorMachines and the Margin-Infused RelaxedAlgorithm measured by mean squared er-ror.
We provide an analysis of which fea-tures are most predictive of a given level.1 IntroductionThe ability to obtain new materials of an appro-priate language proficiency level is an obstaclefor second-language learners and educators alike.With the growth of publicly available Internet andnews sources, learners and instructors of foreignlanguages should have ever-increasing access tolarge volumes of foreign language text.
How-ever, sifting through this pool of foreign languagedata poses a significant challenge.
In this paperwe demonstrate two machine learning regressionmethods which can be used to help both learn-ers and course developers by automatically rat-ing documents based on the text difficulty.
Thesemethods can be used to automatically identifydocuments at specific levels in order to speedcourse or test development, providing learners?
This work was sponsored by the Department of De-fense under Air Force Contract FA8721-05-C-0002.
Opin-ions, interpretations, conclusions, and recommendations arethose of the authors and are not necessarily endorsed by theUnited States Government.with custom-tailored materials that match theirlearning needs.ILR (Interagency Language Roundtable) levelsreflect differences in text difficulty for second-language learners at different stages of their edu-cation.
A description of each level is shown in Ta-ble 1 (Interagency Language Roundtable, 2013).Some levels differ in terms of sentence structure,length of document, type of communication, etc.,while others, especially the higher levels, differ interms of the domain and style of writing.
Giventhese differences, we expect that both semanticcontent and grammar-related features will be nec-essary to distinguish between documents at differ-ent levels.Level Description0 No proficiency0+ Memorized proficiency1 Elementary proficiency1+ Elementary proficiency, plus2 Limited working proficiency2+ Limited working proficiency, plus3 General professional proficiency3+ General professional proficiency, plus4 Advanced professional proficienty4+ Advanced professional proficiency, plus5 Functionally native proficiencyTable 1: Description of ILR levels.Automatically determining ILR levels fromdocuments is a research problem without knownsolutions.
We have developed and adapted a se-ries of rating algorithms and a set of experimentsgauging the feasibility of automatic ILR level as-signment for text documents.
Using data providedby the Defense Language Institute Foreign Lan-guage Center (DLIFLC), we show that while theproblem is tractable, the performance of automatic30methods is not perfect.Our general approach treats the ILR rating prob-lem as one of text classification; given the contentsand structure of a document, which of the ILR lev-els should this document be assigned to?
Thisdiffers from traditional topic classification taskswhere word-usage often uniquely defines topics,since we are also interested in features of text com-plexity that describe structure.
Leveling text is aproblem better fit to regression because readinglevel is a continuous scale.
We want to know howclose a document is to a given level (or betweenlevels), so we measured performance using meansquared error (MSE).
We show that language-independent features can be used for regressionwith Support Vector Machines (SVMs) and theMargin-Infused Relaxed Algorithm (MIRA), andwe present our results for this new baseline forArabic, Dari, English, and Pashto.
To the best ofour knowledge, this is the first study to systemati-cally examine a language-independent approach toreadability using the ILR rating scale for second-language learners.This paper is structured as follows: Section 2describes previous work on reading level assess-ment as a text classification problem, Section 3describes the two algorithms that we used in ourpresent work, Section 4 describes our data and ex-periments, Section 5 reports our results, Section 6provides an analysis of our results, and Section 7proposes different kinds of future work that can bedone to improve this baseline.2 Related WorkIn this section we describe some work on the read-ability problem that is most closely related to ourown.One of the earliest formulas for reading levelassessment, called the Flesch Reading Ease For-mula, measured readability based on shallowlength features (Flesch, 1948).
This metric in-cluded two measurements: the average number ofwords per sentence and the average number of syl-lables per word.
Although these features appear tobe shallow at the offset, the number of syllablesper word could be taken as an abstraction of wordcomplexity.
Those formulas, as well as their var-ious revisions, have become popular because theyare easy to compute for a variety of applications,including structuring highly technical text that iscomprehensible at lower reading levels (Kincaidet al 1975).
Some of the revisions to the FleschReading Ease Formula have included weightingthese shallow features in order to linearly regressacross different difficulty levels.Much effort has been placed into automatingthe scoring process, and recent work on this is-sue has examined machine learning methods totreat reading level as a text classification prob-lem.
Schwarm and Ostendorf (2005) worked onautomatically classifying text by grade level forfirst-language learners.
Their machine learningapproach was a one vs. all method using a setof SVM binary classifiers that were constructedfor each grade level category: 2, 3, 4, and 5.The following features were used for classfication:average sentence length, average number of syl-lables per word, Flesch-Kincaid score, 6 out-of-vocabulary (OOV) rate scores, syntactic parse fea-tures, and 12 language model perplexity scores.Their data was taken from the Weekly Readernewspaper, already separated by grade level.
Theyfound that the error rate for misclassification bymore than one grade level was significantly lowerfor the SVM classifier than for both Lexile andFlesch-Kincaid.
Petersen and Ostendorf (2009)later replicated and expanded Schwarm and Os-tendorf (2005), reaffirming that both classifica-tion and regression with SVMs provided a betterapproximation of readabilty by grade level whencompared with more traditional methods such asthe Flesch-Kincaid score.
In the current work, wealso use SVM for regression, but have decided toreport mean squared error as a more meaningfulmetric.In an effort to uncover which features are themost salient for discriminating among reading lev-els, Feng et al (2010) studied classification per-formance using combinations of different kinds ofreadability features using data from the WeeklyReader newspaper.
Their work examined thefollowing types of features: discourse, languagemodeling, parsed syntactic features, POS fea-tures, shallow length features, as well as somefeatures replicated from Schwarm and Ostendorf(2005).
They reported classifier accuracy andmean squared error from two classifiers, SVM andLogistic Regression, which were used to predictgrade level for grades 2 through 5.
While theyfound that POS features were the most predictiveoverall, they also found that the average number ofwords per sentence was the most predictive length31feature.
This length feature alone achieved 52%accuracy with the Logistic Regression classifier.In the present work, we use the average number ofwords per sentence as a length feature and showthat this metric has some correspondence with thedifferent ILR levels.Another way to examine readability is to treatit as a sorting problem; that is, given some collec-tion of texts, to sort them from easiest to most dif-ficult.
Tanaka-Ishii et al (2010) presented a novelmethod for determining readibility based on sort-ing texts using text from two groups: low difficultyand high difficulty.
They reported their resultsin terms of the Spearman correlation coefficientto compare performance of Flesch-Kincaid, Dale-Chall, SVM regression, and their sorting method.They showed that their sorting method was supe-rior to the other methods, followed by SVM re-gression.
However, they call for a more mod-ern and efficient approach to the problem, such asonline learning, that would estimate weights forregression.
We answer their call with an onlinelearning approach in this work.3 AlgorithmsIn this section, we describe two maximum marginapproaches that we used in our experiments.
Bothare based on the principle of structural risk mini-mization.
We selected the SVM algorithm becauseof its proven usefulness for automatic readabilityassessment.
In addition, the Margin-Infused Re-laxed Algorithm is advantageous because it is anonline algorithm and therefore allows for incre-mental training while still taking advantange ofstructural risk minimization.3.1 Structural Risk MinimizationFor many classification and regression problems,maximum margin approaches are shown to per-form well with minimal amounts of training data.In general, these approaches involve linear dis-criminative classifiers that attempt to learn hy-perplane decision boundaries which separate oneclass from another.
Since multiple hyperplanesthat separate classes can exist, these methods addan additional constraint: they attempt to learn hy-perplanes while maximizing a region around theboundary called the margin.
We show an exam-ple of this kind of margin in Figure 1, where themargin represents the maximum distance betweenthe decision boundary and support vectors.
Themaximum margin approach helps prevent overfit-ting issues that can occur during training, a princi-ple called structural risk minimization.
Thereforewe experiment with two such margin-maximizingalgorithms, described below.Figure 1: Graphical depiction of the maximummargin principle.3.2 Support Vector MachinesFor text classification problems, the most popularmaximum margin approach is the SVM algorithm,introduced by Vapnik (1995).
This approach usesa quadratic programming method to find the sup-port vectors that define the margin.
This is a batchtraining algorithm requiring all training data to bepresent in order to perform the optimization pro-cedure (Joachims, 1998a).
We used LIBSVM toimplement our own SVM for regression (Changand Lin, 2001).Discriminative methods seek to best dividetraining examples in each class from out-of-classexamples.
SVM-based methods are examplesof this approach and have been successfully ap-plied to other text classification problems, includ-ing previous work on reading level assessment(Schwarm and Ostendorf, 2005; Petersen and Os-tendorf, 2009; Feng et al 2010).
This approachattempts to explicitly model the decision boundarybetween classes.
Discriminative methods build amodel for each class c that is defined by the bound-ary between examples of class c and examplesfrom all other classes in the training data.323.3 Margin-Infused Relaxed AlgorithmOnline approaches have the advantage of allowingincremental adaptation when new labeled exam-ples are added during training.
We implementeda version of MIRA from Crammer and Singer(2003), which we used for regression.
Cram-mer and Singer (2003) proved MIRA as an on-line multiclass classifier that employs the prin-ciple of structural risk minimization, and is de-scribed as ultraconservative because it only up-dates weights for misclassified examples.
Forclassification, MIRA is formulated as shown inequation (1):c?
= argmaxc2Cfc(d) (1)wherefc(d) = w ?
d (2)and w is the weight vector which defines themodel for class c. During training, examples arepresented to the algorithm in an online fashion (i.e.one at a time) and the weight vector is updatedaccourding to the update shown in equation (2):wt = wt 1 + l(wt 1,dt 1)vt 1 (3)l(wt 1,dt 1) = ||dt 1  wt 1||  ?
(4)vt 1 = (sign(||dt 1  wt 1||)  ?
)dt 1 (5)where l(?)
is the loss function, ?
corresponds tothe margin slack, and vt 1 is the negative gradientof the loss vector for the previously seen example||dt 1   wt 1||.
This update forces the weightvector towards erroneous examples during train-ing.
The magnitude of the change is proportionalto the l(?).
For correct training examples, no up-date is performed as l(?)
= 0.
In a binary classi-fication task, MIRA attempts to minimize the lossfunction in (4), such that the magnitude of the dis-tance between a document vector and the weightvector is also minimized.However, unlike topic classification or classi-fication of words based on their semantic classwhere the classes are generally discrete, the ILRlevels lie on a continuum (i.e.
level 2 >> level1 >> level 0).
Therefore we are more interestedin using MIRA for regression because we wantto compare the predicted value with the true real-valued label, rather than a class label.
For regres-sion, we can redefine the MIRA loss function asfollows:l(wt,dt) = |lt   dt ?
wt|  ?
(6)In this case, lt is the correct value (in our case,ILR level) for training document dt and dt ?
wt isthe predicted value given the current weight vectorwt.
We expect that minimizing this loss functioncumulatively over the entire training set will yielda regression model that can predict ILR levels forunseen documents.This revised loss function results in a modi-fied update equation for each online update ofthe MIRA weight vector (generating a new set ofweights wt from the previously seen example):wt = wt 1 + l(wt 1,dt 1)vt 1 (7)vt 1 = (sign(|lt 1 dt 1 ?wt 1|) ?
)dt 1 (8)vt 1 defines the direction of loss and the mag-nitude of the update relative to the current train-ing example dt 1.
Since this approach is online,MIRA does not guarantee minimal loss or maxi-mummargin constraints for all of the training data.However, in practice, these methods perform aswell as their SVM counterparts without the needfor batch training (Crammer et al 2006).4 Experiments4.1 DataAll of our experiments used data from four lan-guages: Arabic (AR), Dari (DAR), English (EN),and Pashto (PS).
In Table 2, we show the distri-bution of number of documents per ILR level foreach language.
All of our data was obtained fromthe Directorate of Language Science and Technol-ogy (LST) and the Language Technology Evalua-tion and Application Division (LTEA) at the De-fense Language Institute Foreign Language Cen-ter (DLIFLC).
The data was compiled using anonline resource (Domino).
Language experts (na-tive speakers) used various texts from the Inter-net which they considered to be authentic mate-rial and they created the Global Language OnlineSupport System (GLOSS) system.
The texts wereused to debug the GLOSS system and to see howwell GLOSS worked for the respective languages.Each of the texts were labeled by two independentlinguists expertly trained in ILR level scoring.
Theratings from these two linguists were then adjudi-cated by a third linguist.
We used the resultingadjudicated labels for our training and evaluation.We preprocessed the data by doing the follow-ing tokenization: removed extra whitespace, nor-malized URIs, normalized currency, normalized33Level AR DAR EN PS1 204 197 198 1971+ 200 197 197 1992 199 201 204 2002+ 199 194 196 1983 198 195 202 1983+ 194 194 198 2004 198 195 190 195Overall 1394 1375 1390 1394Table 2: Total collection documents per languageper ILR level.numbers, normalized abbreviations, normalizedpunctuation, and folded to lowercase.
We identi-fied words by splitting text on whitespace and weidentified sentences by splitting text on punctua-tion.4.2 FeaturesIt is necessary to define a set of features to helpthe regressors distinguish between the ILR levels.We conducted our experiments using two differenttypes of features: word-usage features and shallowlength features.
Shallow length features are shownto be useful in reading level prediction tasks (Fenget al 2010).
Word-usage features, such as theones used here, are meant to capture some low-level topical differences between ILR levels.Word-usage features: Word frequencies (orweighted word frequencies) are commonly usedas features for topic classification problems, asthese features are highly correlated with topics(e.g.
words like player and touchdown are verycommon in documents about topics like football,whereas they are much less common in documentsabout opera).
We used TF-LOG weighted wordfrequencies on bag-of-words for each document.Length features: In addition to word-usage, weadded three z-normalized length features: (1) av-erage sentence length (in words) per document,(2) number of words per document, and (3) aver-age word length (in characters) per document.
Weused these as a basic measure of language levelcomplexity.
These features are easily computedby automatic means, and they capture some of thestructural differences between the ILR levels.Figures 2, 3, 4, and 5 show the z-normalizedaverage word count per sentence for Arabic, Dari,English, and Pashto respectively.
The overall dataset for each language has a normalized mean ofFigure 2: Arabic, z-normalized average wordcount per sentence for ILR levels 1, 2 and 3.Figure 3: Dari, z-normalized average word countper sentence for ILR levels 1, 2 and 3.Figure 4: English, z-normalized average wordcount per sentence for ILR levels 1, 2 and 3.34MIRA SVM (linear)LEN WORDS COMBINED LEN WORDS COMBINEDAR 4.527 0.283 0.222 0.411 0.263 0.198DAR 5.538 0.430 0.330 0.473 0.409 0.301EN 5.155 0.181 0.148 0.430 0.181 0.147PS 5.371 0.410 0.360 1.871 0.393 0.391Table 3: Performance results (MSE) for SVM and MIRA on Arabic, Dari, English and Pashto for threedifferent kinds of features/combinations.Figure 5: Pashto, z-normalized average wordcount per sentence for ILR levels 1, 2 and 3.zero and unit variance, which were calculated sep-arately for a given length feature.
The x-axisshows the deviation of documents relative to thedata set mean, in units of overall standard devia-tion.
It is clear from the separability of the levelsin these figures that sentence length could be animportant indicator of ILR level, though no fea-ture is a perfect discriminator.
This is indicated bythe significant overlap between the distributions ofdocument lengths at different ILR levels.4.3 TrainingWe split the data between training and testing us-ing an 80/20 split of the total data for each lan-guage.
To formulate the ILR scale as continuous-valued, we assumed that ?+?
levels are 0.5 higherthan their basis (e.g.
2+ = 2.5).
Though this maynot be optimal if distances between levels are non-constant, the best systems in our experiments showgood prediction performance using this assump-tion.Both of the classifiers were trained to predict theILR value as a continuous value using regression.We measured the performance of each method interms of the mean squared error on the unseen testdocuments.
We tested the following three con-ditions: length-based features only (LEN), word-usage features only (WORDS), and word andlength features combined (COMBINED).
Sinceeach algorithm (SVM and MIRA) has a numberof parameters that can be tuned to optimize per-formance, we report results for the best settings foreach of the algorithms.
These settings were deter-mined by sweeping parameters to optimize perfor-mance on the training data for a range of values,for both MIRA and SVM.
For both algorithms,we varied the number of training iterations from500 to 3100 for each language, with stepsize of100.
We also varied the minimum word frequencycount from 2 to 26, with stepsize 1.
For MIRAonly, we varied the slack parameter from 0.0005to 0.0500, with stepsize 0.00025.
For SVM (linearkernel only), we varied the C parameter and   at acoarse setting of 2n with values of n ranging from-15 to 6 with stepsize 1.5 ResultsWe compared the performance of the onlineMIRA approach with the SVM-based approach.Table 3 shows the overall performance of MIRAregression and SVM regression, respectively, forthe combinations of features for each language.Mean squared error was averaged over all of thelevels in a given language.
MIRA is an approx-imation to SVM, however one of the advantagesof MIRA is that it is an online algorithm so it isadaptable after training and training can be en-hanced later with more data with a small numberof additional data points.Figures 6 and 7 show the per-level performancefor each classifier with the overall best features(COMBINED) for each language.
The highestlevel (Level 4) and lowest levels (Level 1) tend to35exhibit the worst performance across all languagesfor each regression method.
Poorer performanceon the outlying levels could be due to overfittingfor both SVM and MIRA on those levels.
TheILR scale includes 4 major levels at half-step in-tervals between each one.
We are not sure if us-ing a different scale, such as grade levels rangingfrom 1 to 12, would also exhibit poorer perfor-mance on the outlying levels because the highestILR level corresponds to native-like fluency.
ThisU-shaped performance is seen across both classi-fiers for each of the languages.6 AnalysisOur results show that SVM slightly outperformedMIRA for all of the languages.
We believe thatthe reason whyMIRA performed worse than SVMis because it was overfit during training whereasSVM was not.
This could be due to the parame-ters that we set during our sweep in training.
Weselected C and   as parameters to SVM linear-kernel for the best performance.
The   values forEnglish and Arabic were set at more than 1000times smaller than the values for Pashto and Dari(AR: =6.1035156 ?
10 5, DAR: =0.0078125,EN: =3.0517578 ?
10 5, PS: =0.03125).
Thismeans that the margins for Pashto and Dari wereset to be larger respective to English and Arabic.One reason why these margins were larger is be-cause the features that we used had more discrimi-native power for English and Arabic.
In fact, bothMIRA and SVM performed worse on Pashto andDari.Since the method described here makes use ofFigure 6: MIRA performance (MSE) per ILR levelfor each language.Figure 7: SVM performance (MSE) per ILR levelfor each language.linear classifiers that weigh word-usage and lengthfeatures, it is possible to examine the weights thata classifier learns during training to see which fea-tures the algorithm deems most useful in discrim-inating between ILR levels.
One way to do thisis to use a multiclass classifier on our data for thecategorical levels (e.g.
1, 1+, 2, etc.)
and exam-ine the weights that were generated for each class.MIRA is formulated to be a multiclass classifierso we examined its weights for the features.
Wechose MIRA instead of SVM, even though LIB-SVM supports multiclass classification, becausewe wanted to capture differences between levelswhich we could not do with one vs. all.
We exam-ined classifier weights of greatest magnitude to seewhich features were the most indicative and mostcontra-indicative for that level.
We report thesetwo types of features for Level 3 and Level 4 inTables 4 and 5, respectively.
Level 3 documentscan have some complex topics, such as politicsand art, however it can be noted that some of themore abstract topics like love and hate are contra-indicative of Level 3.
On the other hand, we seethat abstract topics are highly indicative Level 4documents where topics such as philosophy, reli-gion, virtue, hypothesis, and theory are discussed.We also note that moral is highly contra-indicativeof Level 3 but is highly indicative of Level 4.7 Discussion and Future WorkWe have presented an approach to score docu-ments based on their ILR level automatically us-ing language-independent features.
Measures ofstructural complexity like the length-based fea-36MostIndicative + Most Contra-Indicative -obama 1.739 said -2.259to 1.681 your -1.480republicans 1.478 is -1.334?
1.398 moral -0.893than 1.381 this -0.835more 1.365 were -0.751cells 1.355 area -0.751american 1.338 love -0.730americans 1.335 says -0.716art 1.315 hate -0.702it?s 1.257 against -0.682could 1.180 people -0.669democrats 1.143 body -0.669as 1.139 you -0.666a 1.072 man -0.652but 1.041 all -0.644america 0.982 over -0.591Table 4: Dominant features for English at ILRLevel 3.tures used in this work are important to achiev-ing good ILR prediction performance.
We intendto investigate further measures that could improvethis baseline, including features from automaticparsers or unsupervised morphology to measuresyntactic complexity.
Here we have shown thathigher reading levels in English correspond morewith abstract topics.
In future work, we also wantto capture some of the stylistic features of text,such as the complexity of dialogue exchanges.For both SVM and MIRA, the combination oflength and word-usage features had the best im-pact on performance across languages.
We foundbetter performance on this task overall for SVMand we believe that MIRA was overfitting duringtraining.
For MIRA, this is likely due to an inter-action between a small number of features and thestopping criterion (mean squared error = 0) thatwe used in training, which tends to overfit.
We in-tend to investigate the stopping criterion in futurework.
Still, we have shown that MIRA can be use-ful in this task because it is an online algorithm,and it allows for incremental training and activelearning.Our current approach can be quickly adaptedfor a new subset of languages because the featuresthat we used here were language-independent.
Weplan to build a flexible architecture that enableslanguage-specific feature extraction to be com-MostIndicative + Most Contra-Indicative -of 3.298 +number+ -2.524this 2.215 .
-2.514moral 1.880 government -1.120philosophy 1.541 have -1.109is 1.242 people -1.007theory 1.138 would -0.909in 1.131 could -0.878absolute 1.034 after -0.875religion 1.011 you -0.874hyperbole 0.938 ,?
-0.870mind 0.934 were -0.827as 0.919 was -0.811hypothesis 0.904 years -0.795schelling 0.883 your -0.747thought 0.854 americans -0.746virtue 0.835 at -0.745alchemy 0.828 they -0.720Table 5: Dominant features for English at ILRLevel 4.bined with our method so that these techniquescan be easily used for new languages.
We willcontinuously improve this baseline using the ap-proaches described in this paper.
We found thatthese two algorithms along with these types offeatures performed pretty well on 4 different lan-guages.
It is surprising that these features wouldcorrelate across languages even though there areindividual differences between each language.
Infuture work, we are interested to look deeper intothe nature of language-independence for this task.With respect to content, we are interested to findout if more word features are needed for somelanguages but not others.
There could be diver-sity of vocabulary at higher ILR levels, which wecould measure with entropy.
Additionally, sincethe MIRA classifier that we are using is an on-line classifier with weight vector representationfor each feature, we could examine the weightsand measure the mutual information by ILR levelabove a certain threshold to find which features arethe most predictive of an ILR level, for each lan-guage.
Lastly, we have assumed that the ILR rat-ing metric is approximately linear, and althoughwe have used linear classifiers in this task, we areinterested to learn if other transformations wouldgive us a better sense of ILR level discrimination.37ReferencesChih-Chung Chang and Chih-Jen Lin.
2001.LIBSVM: a library for support vec-tor machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative Online Algorithms for Multiclass Prob-lems.
Journal of Machine Learning Research,3(2003):951-991.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
OnlinePassive-Agressive Algorithms.
Journal of MachineLearning Research, 7(2006):551-585.George R. Doddington, Mark A. Przybocki, Alvin F.Martin, and Douglas A. Reynolds.
2000.
The NISTspeaker recognition evaluation - overview, method-ology, systems, results, perspective.
Speech Com-munication, 31(2-3):225-254.Lijun Feng, Martin Jansche, Matt Huenerfauth,Noe?mie Elhadad.
2010.
A Comparison of Fea-tures for Automatic Readability Assessment.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters.
Association forComputational Linguistics, 2010.Rudolph Flesch.
1948.
A new readability yardstick.Journal of Applied Psychology, 32(3):221-233.Interagency Language Roundtable.
ILR Skill Scale.http://www.govtilr.org/Skills/ILRscale4.htm, 2013.
Accessed February 27,2013.Thorsten Joachims.
1998a.
Text categorization withsupport vector machines: learning with many rel-evant features.
In Proceedings of the EuropeanConference on Machine Learning, pages 137-142,1998a.Peter J. Kincaid, Lieutenant Robert P. Fishburne, Jr.,Richard L. Rogers, and Brad S. Chissom.
1975.Derivation of new readability formulas for Navy en-listed personnel.
Research Branch Report 8-75, U.S.Naval Air Station, Memphis, 1975.Sarah E. Petersen and Mari Ostendorf.
2009.
A ma-chine learning approach to reading level assessment.Computer Speech and Language, 23(2009):89-106.Sarah.
E. Schwarm and Mari Ostendorf.
2005.
Read-ing Level Assessment Using Support Vector Ma-chines and Statistical Language Models.
In Pro-ceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics.Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-ada.
2010.
Sorting texts by readability.
Computa-tional Linguistics, 36(2):203-227.Vladimir Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, New York, 1995.38
