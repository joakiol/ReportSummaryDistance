Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 197?207,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSteps to Excellence: Simple Inference with Refined Scoring ofDependency TreesYuan Zhang, Tao Lei, Regina Barzilay, Tommi JaakkolaMassachusetts Institute of Technology{yuanzh, taolei, regina, tommi}@csail.mit.eduAmir GlobersonThe Hebrew Universitygamir@cs.huji.ac.ilAbstractMuch of the recent work on depen-dency parsing has been focused on solv-ing inherent combinatorial problems as-sociated with rich scoring functions.
Incontrast, we demonstrate that highly ex-pressive scoring functions can be usedwith substantially simpler inference pro-cedures.
Specifically, we introduce asampling-based parser that can easily han-dle arbitrary global features.
Inspiredby SampleRank, we learn to take guidedstochastic steps towards a high scoringparse.
We introduce two samplers fortraversing the space of trees, Gibbs andMetropolis-Hastings with Random Walk.The model outperforms state-of-the-art re-sults when evaluated on 14 languagesof non-projective CoNLL datasets.
Oursampling-based approach naturally ex-tends to joint prediction scenarios, suchas joint parsing and POS correction.
Theresulting method outperforms the best re-ported results on the CATiB dataset, ap-proaching performance of parsing withgold tags.11 IntroductionDependency parsing is commonly cast as a max-imization problem over a parameterized scoringfunction.
In this view, the use of more expres-sive scoring functions leads to more challengingcombinatorial problems of finding the maximiz-ing parse.
Much of the recent work on parsing hasbeen focused on improving methods for solvingthe combinatorial maximization inference prob-lems.
Indeed, state-of-the-art results have been ob-1The source code for the work is available athttp://groups.csail.mit.edu/rbg/code/global/acl2014.tained by adapting powerful tools from optimiza-tion (Martins et al, 2013; Martins et al, 2011;Rush and Petrov, 2012).
We depart from this viewand instead focus on using highly expressive scor-ing functions with substantially simpler inferenceprocedures.
The key ingredient in our approach ishow learning is coupled with inference.
Our com-bination outperforms the state-of-the-art parsersand remains comparable even if we adopt theirscoring functions.Rich scoring functions have been used for sometime.
They first appeared in the context of rerank-ing (Collins, 2000), where a simple parser is usedto generate a candidate list which is then rerankedaccording to the scoring function.
Because thenumber of alternatives is small, the scoring func-tion could in principle involve arbitrary (global)features of parse trees.
The power of this method-ology is nevertheless limited by the initial set ofalternatives from the simpler parser.
Indeed, theset may already omit the gold parse.
We dispensewith the notion of a candidate set and seek to ex-ploit the scoring function more directly.In this paper, we introduce a sampling-basedparser that places few or no constraints on thescoring function.
Starting with an initial candi-date tree, our inference procedure climbs the scor-ing function in small (cheap) stochastic steps to-wards a high scoring parse.
The proposal distri-bution over the moves is derived from the scoringfunction itself.
Because the steps are small, thecomplexity of the scoring function has limited im-pact on the computational cost of the procedure.We explore two alternative proposal distributions.Our first strategy is akin to Gibbs sampling andsamples a new head for each word in the sentence,modifying one arc at a time.
The second strat-egy relies on a provably correct sampler for first-order scores (Wilson, 1996), and uses it within aMetropolis-Hastings algorithm for general scoringfunctions.
It turns out that the latter optimizes the197score more efficiently than the former.Because the inference procedure is so simple,it is important that the parameters of the scoringfunction are chosen in a manner that facilitateshow we climb the scoring function in small steps.One way to achieve this is to make sure that im-provements in the scoring functions are correlatedwith improvements in the quality of the parse.This approach was suggested in the SampleRankframework (Wick et al, 2011) for training struc-tured prediction models.
This method was origi-nally developed for a sequence labeling task withlocal features, and was shown to be more effec-tive than state-of-the-art alternatives.
Here we ap-ply SampleRank to parsing, applying several mod-ifications such as the proposal distributions men-tioned earlier.The benefits of sampling-based learning go be-yond stand-alone parsing.
For instance, we canuse the framework to correct preprocessing mis-takes in features such as part-of-speech (POS)tags.
In this case, we combine the scoring func-tion for trees with a stand-alone tagging model.When proposing a small move, i.e., sampling ahead of the word, we can also jointly sample itsPOS tag from a set of alternatives provided bythe tagger.
As a result, the selected tag is influ-enced by a broad syntactic context above and be-yond the initial tagging model and is directly opti-mized to improve parsing performance.
Our jointparsing-tagging model provides an alternative tothe widely-adopted pipeline setup.We evaluate our method on benchmark multi-lingual dependency corpora.
Our method outper-forms the Turbo parser across 14 languages on av-erage by 0.5%.
On four languages, we top the bestpublished results.
Our method provides a moreeffective mechanism for handling global featuresthan reranking, outperforming it by 1.3%.
In termsof joint parsing and tagging on the CATiB dataset,we nearly bridge (88.38%) the gap between in-dependently predicted (86.95%) and gold tags(88.45%).
This is better than the best publishedresults in the 2013 SPMRL shared task (Seddah etal., 2013), including parser ensembles.2 Related WorkEarlier works on dependency parsing focused oninference with tractable scoring functions.
For in-stance, a scoring function that operates over eachsingle dependency can be optimized using themaximum spanning tree algorithm (McDonald etal., 2005).
It was soon realized that using higherorder features could be beneficial, even at the costof using approximate inference and sacrificing op-timality.
The first successful approach in this arenawas reranking (Collins, 2000; Charniak and John-son, 2005) on constituency parsing.
Rerankingcan be combined with an arbitrary scoring func-tion, and thus can easily incorporate global fea-tures over the entire parse tree.
Its main disadvan-tage is that the output parse can only be one of thefew parses passed to the reranker.Recent work has focused on more powerful in-ference mechanisms that consider the full searchspace (Zhang and McDonald, 2012; Rush andPetrov, 2012; Koo et al, 2010; Huang, 2008).
Forinstance, Nakagawa (2007) deals with tractabil-ity issues by using sampling to approximatemarginals.
Another example is the dual decompo-sition (DD) framework (Koo et al, 2010; Martinset al, 2011).
The idea in DD is to decompose thehard maximization problem into smaller parts thatcan be efficiently maximized and enforce agree-ment among these via Lagrange multipliers.
Themethod is essentially equivalent to linear program-ming relaxation approaches (Martins et al, 2009;Sontag et al, 2011), and also similar in spirit toILP approaches (Punyakanok et al, 2004).A natural approach to approximate global in-ference is via search.
For instance, a transition-based parsing system (Zhang and Nivre, 2011)incrementally constructs a parsing structure us-ing greedy beam-search.
Other approaches op-erate over full trees and generate a sequenceof candidates that successively increase thescore (Daum?e III et al, 2009; Li et al, 2013;Wick et al, 2011).
Our work builds on one suchapproach ?
SampleRank (Wick et al, 2011), asampling-based learning algorithm.
In SampleR-ank, the parameters are adjusted so as to guide thesequence of candidates closer to the target struc-ture along the search path.
The method has beensuccessfully used in sequence labeling and ma-chine translation (Haddow et al, 2011).
In thispaper, we demonstrate how to adapt the methodfor parsing with rich scoring functions.3 Sampling-Based Dependency Parsingwith Global FeaturesIn this section, we introduce our novel sampling-based dependency parser which can incorporate198arbitrary global features.
We begin with the no-tation before addressing the decoding and learningalgorithms.
Finally, we extend our model to a jointparsing and POS correction task.3.1 NotationsWe denote sentences by x and the correspondingdependency trees by y ?
Y(x).
Here Y(x) is theset of valid (projective or non-projective) depen-dency trees for sentence x.
We use xjto referto the jth word of sentence x, and hjto the headword of xj.
A training set of size N is given as aset of pairs D = {(x(i), y(i))}Ni=1where y(i)is theground truth parse for sentence x(i).We parameterize the scoring function s(x, y) ass(x, y) = ?
?
f(x, y) (1)where f(x, y) is the feature vector associated withtree y for sentence x.
We do not make any assump-tions about how the feature function decomposes.In contrast, most state-of-the-art parsers operateunder the assumption that the feature function de-composes into a sum of simpler terms.
For exam-ple, in the second-order MST parser (McDonaldand Pereira, 2006), all the feature terms involvearcs or consecutive siblings.
Similarly, parsersbased on dual decomposition (Martins et al, 2011;Koo et al, 2010) assume that s(x, y) decomposesinto a sum of terms where each term can be maxi-mized over y efficiently.3.2 DecodingThe decoding problem consists of finding a validdependency tree y ?
Y(x) that maximizes thescore s(x, y) = ?
?
f(x, y) with parameters ?.For scoring functions that extend beyond first-order arc preferences, finding the maximizing non-projective tree is known to be NP-hard (McDonaldand Pereira, 2006).
We find a high scoring treethrough sampling, and (later) learn the parameters?
so as to further guide this process.Our sampler generates a sequence of depen-dency structures so as to approximate independentsamples fromp(y|x, T, ?)
?
exp (s(x, y)/T ) (2)The temperature parameter T controls how con-centrated the samples are around the maximumof s(x, y) (e.g., see Geman and Geman (1984)).Sampling from target distribution p is typically ashard as (or harder than) that maximizing s(x, y).Inputs: ?, x, T0(initial temperature), c (temperatureupdate rate), proposal distribution q.Outputs: y?T ?
T0Set y0to some random treey??
y0repeaty??
q(?|x, yt, T, ?
)if s(x, y?)
> s(x, y?)
theny??
y??
= min[1,p(y?)q(yt|y?
)p(yt)q(y?|yt)]Sample Bernouli variable Z with P [Z = 1] = ?.if Z = 0 thenyt+1?
ytelseyt+1?
y?t?
t+ 1T ?
c ?
Tuntil convergencereturn y?Figure 1: Sampling-based algorithm for decoding(i.e., approximately maximizing s(x, y)).We follow here a Metropolis-Hastings samplingalgorithm (e.g., see Andrieu et al (2003)) andexplore different alternative proposal distributionsq(y?|x, y, ?, T ).
The distribution q governs thesmall steps that are taken in generating a sequenceof structures.
The target distribution p folds intothe procedure by defining the probability that wewill accept the proposed move.
The general struc-ture of our sampling algorithm is given in Figure 1.3.2.1 Gibbs SamplingPerhaps the most natural choice of the proposaldistribution q is a conditional distribution from p.This is feasible if we restrict the proposed movesto only small changes in the current tree.
In ourcase, we choose a word j randomly, and then sam-ple its head hjaccording to p with the constraintthat we obtain a valid tree (when projective treesare sought, this constraint is also incorporated).For this choice of q, the probability of acceptingthe new tree (?
in Figure 1) is identically one.Thus new moves are always accepted.3.2.2 Exact First-Order SamplingOne shortcoming of the Gibbs sampler is that itonly changes one variable (arc) at a time.
Thisusually leads to slow mixing, requiring more sam-ples to get close to the parse with maximumscore.
Ideally, we would change multiple headsin the parse tree simultaneously, and sample thosechoices from the corresponding conditional distri-bution of p. While in general this is increasinglydifficult with more heads, it is indeed tractable if199Inputs: x, yt, ?, K (number of heads to change).Outputs: y?for i = 1 to |x| doinTree[i]?
falseChangeNode[i]?
falseSet ChangeNode to true for K random nodes.head[0]?
?1for i = 1 to |x| dou?
iwhile not inTree[u] doif ChangeNode[u] thenhead[u]?
randomHead(u, ?)elsehead[u]?
yt(u)u?
head[u]if LoopExist(head) thenEraseLoop(head)u?
iwhile not inTree[u] doinTree[u]?
trueu?
head[u]return Construct tree y?from the head array.Figure 2: A proposal distribution q(y?|yt) basedon the random walk sampler of Wilson (1996).The function randomHead samples a new head fornode u according to the first-order weights givenby ?.the model corresponds to a first-order parser.
Onesuch sampling algorithm is the random walk sam-pler of Wilson (1996).
It can be used to obtaini.i.d.
samples from distributions of the form:p(y) ?
?i?j?ywij, (3)where y corresponds to a tree with a spcified rootand wijis the exponential of the first-order score.y is always a valid parse tree if we allow multiplechildren of the root and do not impose projectiveconstraint.
The algorithm in Wilson (1996) iter-ates over all the nodes, and for each node performsa random walk according to the weights wijuntilthe walk creates a loop or hits a tree.
In the firstcase the algorithm erases the loop and continuesthe walk.
If the walk hits the current tree, the walkpath is added to form a new tree with more nodes.This is repeated until all the nodes are included inthe tree.
It can be shown that this procedure gen-erates i.i.d.
trees from p(y).Since our features do not by design correspondto a first-order parser, we cannot use the Wilsonalgorithm as it is.
Instead we use it as the proposalfunction and sample a subset of the dependen-cies from the first-order distribution of our model,while fixing the others.
In each step we uniformlysample K nodes to update and sample their new1!2!not?Monday?
not ssssssssssss" ?"""
wasloop erased!Black?Monday?wasROOT!
It!
was!
not!
Black!
Monday!2!1!3!ROOT!
It!
was!
not!
Black!
Monday!
(b) walk path:!
(c) walk path:!
(a) original tree!ROOT!
It!
was!
not!
Black!
Monday!Figure 3: An illustration of random walk sam-pler.
The index on each edge indicates its order oneach walk path.
The heads of the red words aresampled while others are fixed.
The blue edgesrepresent the current walk path and the black onesare already in the tree.
Note that the walk direc-tion is opposite to the dependency direction.
(a)shows the original tree before sampling; (b) and(c) show the walk path and how the tree is gener-ated in two steps.
The loop not?
Monday?
notin (b) is erased.heads using the Wilson algorithm (in the experi-ments we use K = 4).
Note that blocked Gibbssampling would be exponential in K, and is thusvery slow already at K = 4.
The procedure is de-scribed in Figure 2 with a graphic illustration inFigure 3.3.3 TrainingIn this section, we describe how to learn theadjustable parameters ?
in the scoring function.The parameters are learned in an on-line fash-ion by successively imposing soft constraints be-tween pairs of dependency structures.
We intro-duce both margin constraints and constraints per-taining to successive samples generated along thesearch path.
We demonstrate later that both typesof constraints are essential.We begin with the standard margin constraints.An ideal scoring function would always rank thegold parse higher than any alternative.
Moreover,alternatives that are far from the gold parse shouldscore even lower.
As a result, we require thats(x(i), y(i))?
s(x(i), y) ?
?
(y(i), y) ?y (4)where ?
(y(i), y) is the number of head mistakesin y relative to the gold parse y(i).
We adopt herea shorthand Err(y) = ?
(y(i), y), where the de-200pendence on y(i)is implied from context.
Notethat Equation 4 contains exponentially many con-straints and cannot be enforced jointly for generalscoring functions.
However, our sampling proce-dure generates a small number of structures alongthe search path.
We enforce only constraints cor-responding to those samples.The second type of constraints are enforced be-tween successive samples along the search path.To illustrate the idea, consider a parse y that dif-fers from y(i)in only one arc, and a parse y?thatdiffers from y(i)in ten arcs.
We cannot necessarilyassume that s(x, y) is greater than s(x, y?)
withoutadditional encouragement.
Thus, we can comple-ment the constraints in Equation 4 with additionalpairwise constraints (Wick et al, 2011):s(x(i), y)?
s(x(i), y?)
?
Err(y?)?
Err(y) (5)where similarly to Equation 4, the difference inscores scales with the differences in errors with re-spect to the target y(i).
We only enforce the aboveconstraints for y, y?that are consecutive samplesin the course of the sampling process.
These con-straints serve to guide the sampling process de-rived from the scoring function towards the goldparse.We learn the parameters ?
in an on-line fashionto satisfy the above constraints.
This is done viathe MIRA algorithm (Crammer and Singer, 2003).Specifically, if the current parameters are ?t, andwe enforce constraint Equation 5 for a particularpair y, y?, then we will find ?t+1that minimizesmin ||?
?
?t||2+ C?s.t.
?
?
(f(x, y)?
f(x, y?))
?
Err(y?)?
Err(y)?
?
(6)The updates can be calculated in closed form.
Fig-ure 4 summarizes the learning algorithm.
We re-peatedly generate parses based on the current pa-rameters ?tfor each sentence x(i), and use succes-sive samples to enforce constraints in Equation 4and Equation 5 one at a time.3.4 Joint Parsing and POS CorrectionIt is easy to extend our sampling-based parsingframework to joint prediction of parsing and otherlabels.
Specifically, when sampling the new heads,we can also sample the values of other variables atthe same time.
For instance, we can sample thePOS tag, the dependency relation or morphologyinformation.
In this work, we investigate a jointInputs: D = {(x(i), y(i))}Ni=1.Outputs: Learned parameters ?.?0?
0for e = 1 to #epochs dofor i = 1 toN doy??
q(?|x(i), ytii, ?t)y+= arg miny?{ytii,y?
}Err(y)y?= arg maxy?{ytii,y?}Err(y)yti+1i?
acceptOrReject(y?, ytii, ?t)ti?
ti+ 1?f = f(x(i), y+)?
f(x(i), y?
)?Err = Err(y+)?
Err(y?
)if ?Err 6= 0 and ?t?
?f < ?Err then?t+1?
updateMIRA(?f,?Err, ?t)t?
t+ 1?fg= f(x(i), y(i))?
f(x(i), ytii)if ?t?
?fg< Err(ytii) then?t+1?
updateMIRA(?fg, Err(ytii), ?t)t?
t+ 1return Average of ?0, .
.
.
, ?tparameters.Figure 4: SampleRank algorithm for learning.
Therejection strategy is as in Figure 1. ytiiis the tithtree sample of x(i).
The first MIRA update (seeEquation 6) enforces a ranking constraint betweentwo sampled parses.
The second MIRA update en-forces constraints between a sampled parse and thegold parse.
In practice several samples are drawnfor each sentence in each epoch.POS correction scenario in which only the pre-dicted POS tags are provided in the testing phase,while both gold and predicted tags are availablefor the training set.We extend our model such that it jointly learnshow to predict a parse tree and also correct the pre-dicted POS tags for a better parsing performance.We generate the POS candidate list for each wordbased on the confusion matrix on the training set.Let c(tg, tp) be the count when the gold tag is tgand the predicted one is tp.
For each word w, wefirst prune out its POS candidates by using the vo-cabulary from the training set.
We don?t pruneanything if w is unseen.
Assuming that the pre-dicted tag forw is tp, we further remove those tagst if their counts are smaller than some thresholdc(t, tp) < ?
?
c(tp, tp)2.After generating the candidate lists for eachword, the rest of the extension is rather straight-forward.
For each sampling, let H be the set ofcandidate heads and T be the set of candidate POStags.
The Gibbs sampler will generate a new sam-ple from the space H ?
T .
The other parts of thealgorithm remain the same.2In our work we choose ?
= 0.003, which gives a 98.9%oracle POS tagging accuracy on the CATiB development set.201arc!head bigram!
!h h m m+1arbitrary sibling!?
!h m sh m consecutive sibling!h m s grandparent!g h mgrand-sibling!g h m s tri-siblings!h m s t grand-grandparent!g h mggouter-sibling-grandchild!h m sgc h s gcminner-sibling-grandchild!Figure 5: First- to third-order features.4 FeaturesFirst- to Third-Order Features The featuretemplates of first- to third-order features aremainly drawn from previous work on graph-based parsing (McDonald and Pereira, 2006),transition-based parsing (Nivre et al, 2006) anddual decomposition-based parsing (Martins et al,2011).
As shown in Figure 5, the arc is the basicstructure for first-order features.
We also definefeatures based on consecutive sibling, grandpar-ent, arbitrary sibling, head bigram, grand-siblingand tri-siblings, which are also used in the Turboparser (Martins et al, 2013).
In addition to thesefirst- to third-order structures, we also considergrand-grandparent and sibling-grandchild struc-tures.
There are two types of sibling-grandchildstructures: (1) inner-sibling when the sibling isbetween the head and the modifier and (2) outer-sibling for the other cases.Global Features We used feature shown promis-ing in prior reranking work Charniak and Johnson(2005), Collins (2000) and Huang (2008).?
Right Branch This feature enables the modelto prefer right or left-branching trees.
It countsthe number of words on the path from the rootnode to the right-most non-punctuation word,normalized by the length of the sentence.?
Coordination In a coordinate structure, the twoadjacent conjuncts usually agree with each otheron POS tags and their span lengths.
For in-stance, in cats and dogs, the conjuncts are bothshort noun phrases.
Therefore, we add differ-ent features to capture POS tag and span lengthconsistency in a coordinate structure.?
PP Attachment We add features of lexical tu-eat!
with!
knife!
and!
fork!Figure 6: An example of PP attachment with coor-dination.
The arguments should be knife and fork,not and.ples involving the head, the argument and thepreposition of prepositional phrases.
Generally,this feature can be defined based on an instanceof grandparent structure.
However, we also han-dle the case of coordination.
In this case, the ar-guments should be the conjuncts rather than thecoordinator.
Figure 6 shows an example.?
Span Length This feature captures the distribu-tion of the binned span length of each POS tag.It also includes flags of whether the span reachesthe end of the sentence and whether the span isfollowed by the punctuation.?
Neighbors The POS tags of the neighboringwords to the left and right of each span, togetherwith the binned span length and the POS tag atthe span root.?
Valency We consider valency features for eachPOS tag.
Specifically, we add two types of va-lency information: (1) the binned number ofnon-punctuation modifiers and (2) the concate-nated POS string of all those modifiers.?
Non-projective Arcs A flag indicating if a de-pendency is projective or not (i.e.
if it spans aword that does not descend from its head) (Mar-tins et al, 2011).
This flag is also combined withthe POS tags or the lexical words of the head andthe modifier.POS Tag Features In the joint POS correctionscenario, we also add additional features specifi-cally for POS prediction.
The feature templatesare inspired by previous feature-rich POS taggingwork (Toutanova et al, 2003).
However, we arefree to add higher order features because we donot rely on dynamic programming decoding.
Inour work we use feature templates up to 5-gram.Table 1 summarizes all POS tag feature templates.5 Experimental SetupDatasets We evaluate our model on standardbenchmark corpora ?
CoNLL 2006 and CoNLL2008 (Buchholz and Marsi, 2006; Surdeanu et al,2008) ?
which include dependency treebanks for14 different languages.
Most of these data sets2021-gram?ti?, ?ti, wi?2?, ?ti, wi?1?, ?ti, wi?, ?ti, wi+1?,?ti, wi+2?2-gram?ti?1, ti?, ?ti?2, ti?, ?ti?1, ti, wi?1?,?ti?1, ti, wi?3-gram?ti?1, ti, ti+1?, ?ti?2, ti, ti+1, ?, ?ti?1, ti, ti+2?,?ti?2, ti, ti+2?4-gram?ti?2, ti?1, ti, ti+1?, ?ti?2, ti?1, ti, ti+2?,?ti?2, ti, ti+1, ti+2?5-gram ?ti?2, ti?1, ti, ti+1, ti+2?Table 1: POS tag feature templates.
tiand wide-notes the POS tag and the word at the current posi-tion.
ti?xand ti+xdenote the left and right contexttags, and similarly for words.contain non-projective dependency trees.
We useall sentences in CoNLL datasets during trainingand testing.
We also use the Columbia ArabicTreebank (CATiB) (Marton et al, 2013).
CATiBmostly includes projective trees.
The trees are an-notated with both gold and predicted versions ofPOS tags and morphology information.
Follow-ing Marton et al (2013), for this dataset we use12 core POS tags, word lemmas, determiner fea-tures, rationality features and functional gendersand numbers.Some CATiB sentences exceed 200 tokens.
Forefficiency, we limit the sentence length to 70 to-kens in training and development sets.
However,we do not impose this constraint during testing.We handle long sentences during testing by apply-ing a simple split-merge strategy.
We split the sen-tence based on the ending punctuation, predict theparse tree for each segment and group the roots ofresulting trees into a single node.Evaluation Measures Following standard prac-tice, we use Unlabeled Attachment Score (UAS)as the evaluation metric in all our experiments.We report UAS excluding punctuation on CoNLLdatasets, following Martins et al (2013).
For theCATiB dataset, we report UAS including punctu-ation in order to be consistent with the publishedresults in the 2013 SPMRL shared task (Seddah etal., 2013).Baselines We compare our model with the Turboparser and the MST parser.
For the Turbo parser,we directly compare with the recent published re-sults in (Martins et al, 2013).
For the MST parser,we train a second-order non-projective model us-ing the most recent version of the code3.We also compare our model against a discrim-inative reranker.
The reranker operates over the3http://sourceforge.net/projects/mstparser/top-50 list obtained from the MST parser4.
Weuse a 10-fold cross-validation to generate candi-date lists for training.
We then train the rerankerby running 10 epochs of cost-augmented MIRA.The reranker uses the same features as our model,along with the tree scores obtained from the MSTparser (which is a standard practice in reranking).Experimental Details Following Koo and Collins(2010), we always first train a first-order pruner.For each word xi, we prune away the incomingdependencies ?hi, xi?
with probability less than0.005 times the probability of the most likely head,and limit the number of candidate heads up to 30.This gives a 99% pruning recall on the CATiBdevelopment set.
The first-order model is alsotrained using the algorithm in Figure 4.
Af-ter pruning, we tune the regularization parameterC = {0.1, 0.01, 0.001} on development sets fordifferent languages.
Because the CoNLL datasetsdo not have a standard development set, we ran-domly select a held out of 200 sentences from thetraining set.
We also pick the training epochs from{50, 100, 150} which gives the best performanceon the development set for each language.
Aftertuning, the model is trained on the full training setwith the selected parameters.We apply the Random Walk-based samplingmethod (see Section 3.2.2) for the standard de-pendency parsing task.
However, for the jointparsing and POS correction on the CATiB datasetwe do not use the Random Walk method becausethe first-order features in normal parsing are nolonger first-order when POS tags are also vari-ables.
Therefore, the first-order distribution is notwell-defined and we only employ Gibbs samplingfor simplicity.
On the CATiB dataset, we restrictthe sample trees to always be projective as de-scribed in Section 3.2.1.
However, we do not im-pose this constraint for the CoNLL datasets.6 ResultsComparison with State-of-the-art Parsers Ta-ble 2 summarizes the performance of our modeland of the baselines.
We first compare our modelto the Turbo parser using the Turbo parser fea-ture set.
This is meant to test how our learningand inference methods compare to a dual decom-position approach.
The first column in Table 24The MST parser is trained in projective mode for rerank-ing because generating top-k list from second-order non-projective model is intractable.203Our Model (UAS)Turbo (UAS)MST 2nd-Ord.
(UAS)Best Published UASTop-50RerankerTop-500RerankerTurbo Feat.
Full Feat.Arabic 79.86 80.21 79.64 78.75 81.12 (Ma11) 79.03 78.91Bulgarian 92.97 93.30 93.10 91.56 94.02 (Zh13) 92.81 -Chinese 92.06 92.63 89.98 91.77 91.89 (Ma10) 92.25 -Czech 90.62 91.04 90.32 87.30 90.32 (Ma13) 88.14 -Danish 91.45 91.80 91.48 90.50 92.00 (Zh13) 90.88 90.91Dutch 85.83 86.47 86.19 84.11 86.19 (Ma13) 81.01 -English 92.79 92.94 93.22 91.54 93.22 (Ma13) 92.41 -German 91.79 92.07 92.41 90.14 92.41 (Ma13) 91.19 -Japanese 93.23 93.42 93.52 92.92 93.72 (Ma11) 93.40 -Portuguese 91.82 92.41 92.69 91.08 93.03 (Ko10) 91.47 -Slovene 86.19 86.82 86.01 83.25 86.95 (Ma11) 84.81 85.37Spanish 88.24 88.21 85.59 84.33 87.96 (Zh13) 86.85 87.21Swedish 90.48 90.71 91.14 89.05 91.62 (Zh13) 90.53 -Turkish 76.82 77.21 76.90 74.39 77.55 (Ko10) 76.35 76.23Average 88.87 89.23 88.72 86.86 89.33 87.92 -Table 2: Results of our model, the Turbo parser, and the MST parser.
?Best Published UAS?
includes themost accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martinset al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald(2012) and Zhang et al (2013).
Martins et al (2013) is the current Turbo parser.
The last two columnsshows UAS of the discriminative reranker.shows the result for our model with an average of88.87%, and the third column shows the resultsfor the Turbo parser with an average of 88.72%.This suggests that our learning and inference pro-cedures are as effective as the dual decompositionmethod in the Turbo parser.Next, we add global features that are not used bythe Turbo parser.
The performance of our modelis shown in the second column with an average of89.23%.
It outperforms the Turbo parser by 0.5%and achieves the best reported performance onfour languages.
Moreover, our model also outper-forms the 88.80% average UAS reported in Mar-tins et al (2011), which is the top performing sin-gle parsing system (to the best of our knowledge).Comparison with Reranking As column 6 of Ta-ble 2 shows, our model outperforms the rerankerby 1.3%5.
One possible explanation of this perfor-mance gap between the reranker and our model isthe small number of candidates considered by thereranker.
To test this hypothesis, we performedexperiments with top-500 list for a subset of lan-guages.6As column 7 shows, this increase in thelist size does not change the relative performanceof the reranker and our model.Joint Parsing and POS Correction Table 3shows the results of joint parsing and POS cor-rection on the CATiB dataset, for our model and5Note that the comparison is conservative because wecan also add MST scores as features in our model as inreranker.
With these features our model achieves an averageUAS 89.28%.6We ran this experiment on 5 languages with smalldatasets due to the scalability issues associated with rerank-ing top-500 list.state-of-the-art systems.
As the upper part of thetable shows, the parser with corrected tags reaches88.38% compared to the accuracy of 88.46% onthe gold tags.
This is a substantial increase fromthe parser that uses predicted tags (86.95%).To put these numbers into perspective, the bot-tom part of Table 3 shows the accuracy of the bestsystems from the 2013 SPMRL shared task onArabic parsing using predicted information (Sed-dah et al, 2013).
Our system not only out-performs the best single system (Bj?orkelund etal., 2013) by 1.4%, but it also tops the ensem-ble system that combines three powerful parsers:the Mate parser (Bohnet, 2010), the Easy-Firstparser (Goldberg and Elhadad, 2010) and theTurbo parser (Martins et al, 2013)Impact of Sampling Methods We compare twosampling methods introduced in Section 3.2 withrespect to their decoding efficiency.
Specifically,we measure the score of the retrieved trees in test-ing as a function of the decoding speed, measuredby the number of tokens per second.
We changethe temperature update rate c in order to decodewith different speed.
In Figure 7 we show the cor-responding curves for two languages: Arabic andChinese.
We select these two languages as theycorrespond to two extremes in sentence length:Arabic has the longest sentences on average, whileChinese has the shortest ones.
For both languages,the tree score improves over time.
Given sufficienttime, both sampling methods achieve the samescore.
However, the Random Walk-based sam-pler performs better when the quality is traded forspeed.
This result is to be expected given that each204Dev.
Set (?
70) Testing SetPOS Acc.
UAS POS Acc.
UASGold - 90.27 - 88.46Predicted 96.87 88.81 96.82 86.95POS Correction 97.72 90.08 97.49 88.38CADIM 96.87 87.4- 96.82 85.78IMS-Single - - - 86.96IMS-Ensemble - - - 88.32Table 3: Results for parsing and corrective taggingon the CATiB dataset.
The upper part shows UASof our model with gold/predicted information orPOS correction.
Bottom part shows UAS of thebest systems in the SPMRL shared task.
IMS-Single (Bj?orkelund et al, 2013) is the best singleparsing system, while IMS-Ensemble (Bj?orkelundet al, 2013) is the best ensemble parsing system.We also show results for CADIM (Marton et al,2013), the second best system, because we usetheir predicted features.0 20 40 60 80 1002.6482.652.6522.6542.6562.658 x 104Toks/secScoreGibbsRandom Walk(a) Arabic0 100 200 300 400 500 600 700 8001.8971.8981.8991.9 x 104Toks/secScoreGibbsRandom Walk(b) ChineseFigure 7: Total score of the predicted test trees asa function of the decoding speed, measured in thenumber of tokens per second.iteration of this sampler makes multiple changesto the tree, in contrast to a single-edge change ofGibbs sampler.The Effect of Constraints in Learning Our train-ing method updates parameters to satisfy the pair-wise constraints between (1) subsequent sampleson the sampling path and (2) selected samples andthe ground truth.
Figure 8 shows that applyingboth types of constraints is consistently better thanusing either of them alone.
Moreover, these re-sults demonstrate that comparison between subse-quent samples is more important than comparisonagainst the gold tree.Decoding Speed Our sampling-based parser is anDanish Japanese Portuguese Swedish899091929394UAS(%)BothNeighborGoldFigure 8: UAS on four languages when train-ing with different constraints.
?Neighbor?
corre-sponds to pairwise constraints between subsequentsamples, ?Gold?
represents constraints between asingle sample and the ground truth, ?Both?
meansapplying both types of constraints.anytime algorithm, and therefore its running timecan be traded for performance.
Figure 7 illustratesthis trade-off.
In the experiments reported above,we chose a conservative cooling rate and contin-ued to sample until the score no longer changed.The parser still managed to process all the datasetsin a reasonable time.
For example, the time that ittook to decode all the test sentences in Chinese andArabic were 3min and 15min, respectively.
Ourcurrent implementation is in Java and can be fur-ther optimized for speed.7 ConclusionsThis paper demonstrates the power of combining asimple inference procedure with a highly expres-sive scoring function.
Our model achieves the bestresults on the standard dependency parsing bench-mark, outperforming parsing methods with elabo-rate inference procedures.
In addition, this frame-work provides simple and effective means for jointparsing and corrective tagging.AcknowledgmentsThis research is developed in collaboration withthe Arabic Language Technologies (ALT) groupat Qatar Computing Research Institute (QCRI)within the IYAS project.
The authors acknowledgethe support of the MURI program (W911NF-10-1-0533, the DARPA BOLT program and the US-Israel Binational Science Foundation (BSF, GrantNo 2012330).
We thank the MIT NLP group andthe ACL reviewers for their comments.205ReferencesChristophe Andrieu, Nando De Freitas, ArnaudDoucet, and Michael I Jordan.
2003.
An introduc-tion to mcmc for machine learning.
Machine learn-ing, 50(1-2):5?43.Anders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,Thomas Mueller, and Wolfgang Seeker.
2013.
(re)ranking meets morphosyntax: State-of-the-artresults from the SPMRL 2013 shared task.
In Pro-ceedings of the Fourth Workshop on Statistical Pars-ing of Morphologically-Rich Languages, pages 135?145, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In COLING,pages 89?97.Sabine Buchholz and Erwin Marsi.
2006.
Conll-xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, pages 149?164.Association for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theSeventeenth International Conference on MachineLearning, ICML ?00, pages 175?182.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.The Journal of Machine Learning Research, 3:951?991.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
Machinelearning, 75(3):297?325.Stuart Geman and Donald Geman.
1984.
Stochas-tic relaxation, gibbs distributions, and the bayesianrestoration of images.
Pattern Analysis andMachineIntelligence, IEEE Transactions on, (6):721?741.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 742?750.
Association for Computa-tional Linguistics.Barry Haddow, Abhishek Arun, and Philipp Koehn.2011.
Samplerank training for phrase-based ma-chine translation.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 261?271.
Association for Computational Linguistics.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL, pages 586?594.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1?11.
Association forComputational Linguistics.Terry Koo, Alexander M Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1288?1298.
Association for Compu-tational Linguistics.Quannan Li, Jingdong Wang, Zhuowen Tu, andDavid P Wipf.
2013.
Fixed-point model for struc-tured labeling.
In Proceedings of the 30th Interna-tional Conference on Machine Learning (ICML-13),pages 214?221.Andr?e FT Martins, Noah A Smith, and Eric P Xing.2009.
Concise integer linear programming formula-tions for dependency parsing.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 1-Volume 1, pages 342?350.
Association forComputational Linguistics.Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-dro MQ Aguiar, and M?ario AT Figueiredo.
2010.Turbo parsers: Dependency parsing by approxi-mate variational inference.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 34?44.
Association forComputational Linguistics.Andr?e FT Martins, Noah A Smith, Pedro MQ Aguiar,and M?ario AT Figueiredo.
2011.
Dual decompo-sition with many overlapping components.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 238?249.
As-sociation for Computational Linguistics.Andr?e FT Martins, Miguel B Almeida, and Noah ASmith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proceedings of the51th Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Yuval Marton, Nizar Habash, Owen Rambow, andSarah Alkhulani.
2013.
Spmrl13 shared task sys-tem: The cadim arabic dependency parser.
In Pro-ceedings of the Fourth Workshop on Statistical Pars-ing of Morphologically-Rich Languages, pages 76?80.Ryan T McDonald and Fernando CN Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In EACL.206R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 523?530.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning, pages 216?220.
Association forComputational Linguistics.Tetsuji Nakagawa.
2007.
Multilingual dependencyparsing using global features.
In EMNLP-CoNLL,pages 952?956.Joakim Nivre, Johan Hall, Jens Nilsson, G?uls?en Eryiit,and Svetoslav Marinov.
2006.
Labeled pseudo-projective dependency parsing with support vectormachines.
In Proceedings of the Tenth Confer-ence on Computational Natural Language Learning,pages 221?225.
Association for Computational Lin-guistics.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic role labeling via integerlinear programming inference.
In Proceedings ofthe 20th international conference on ComputationalLinguistics, page 1346.
Association for Computa-tional Linguistics.Alexander M Rush and Slav Petrov.
2012.
Vine prun-ing for efficient multi-pass dependency parsing.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 498?507.
Association for Computational Lin-guistics.Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, MarieCandito, Jinho D Choi, Rich?ard Farkas, JenniferFoster, Iakes Goenaga, Koldo Gojenola Gallete-beitia, Yoav Goldberg, et al 2013.
Overview of thespmrl 2013 shared task: A cross-framework evalua-tion of parsing morphologically rich languages.
InProceedings of the Fourth Workshop on StatisticalParsing of Morphologically-Rich Languages, pages146?182.D.
Sontag, A. Globerson, and T. Jaakkola.
2011.
In-troduction to dual decomposition for inference.
InOptimization for Machine Learning, pages 219?254.MIT Press.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
Theconll-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe Twelfth Conference on Computational NaturalLanguage Learning, pages 159?177.
Association forComputational Linguistics.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Michael L. Wick, Khashayar Rohanimanesh, KedarBellare, Aron Culotta, and Andrew McCallum.2011.
Samplerank: Training factor graphs withatomic gradients.
In Lise Getoor and Tobias Schef-fer, editors, Proceedings of the 28th InternationalConference on Machine Learning, ICML 2011,pages 777?784.David Bruce Wilson.
1996.
Generating random span-ning trees more quickly than the cover time.
InProceedings of the twenty-eighth annual ACM sym-posium on Theory of computing, pages 296?303.ACM.Hao Zhang and Ryan McDonald.
2012.
Generalizedhigher-order dependency parsing with cube prun-ing.
In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 320?331.
Association for Computational Lin-guistics.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2, pages188?193.
Association for Computational Linguis-tics.Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-ald.
2013.
Online learning for inexact hypergraphsearch.
In Proceedings of EMNLP.207
