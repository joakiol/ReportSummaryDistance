Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858?1869,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsModeling Joint Entity and Relation Extraction with Table RepresentationMakoto Miwa and Yutaka SasakiToyota Technological Institute2-12-1 Hisakata, Tempaku-ku, Nagoya, 468-8511, Japan{makoto-miwa, yutaka.sasaki}@toyota-ti.ac.jpAbstractThis paper proposes a history-based struc-tured learning approach that jointly ex-tracts entities and relations in a sentence.We introduce a novel simple and flexibletable representation of entities and rela-tions.
We investigate several feature set-tings, search orders, and learning meth-ods with inexact search on the table.
Theexperimental results demonstrate that ajoint learning approach significantly out-performs a pipeline approach by incorpo-rating global features and by selecting ap-propriate learning methods and search or-ders.1 IntroductionExtraction of entities and relations from texts hasbeen traditionally treated as a pipeline of two sep-arate subtasks: entity recognition and relation ex-traction.
This separation makes the task easy todeal with, but it ignores underlying dependenciesbetween and within subtasks.
First, since entityrecognition is not affected by relation extraction,errors in entity recognition are propagated to re-lation extraction.
Second, relation extraction isoften treated as a multi-class classification prob-lem on pairs of entities, so dependencies betweenpairs are ignored.
Examples of these dependen-cies are illustrated in Figure 1.
For dependenciesbetween subtasks, a Live in relation requires PERand LOC entities, and vice versa.
For in-subtaskdependencies, the Live in relation between ?Mrs.Tsutayama?
and ?Japan?
can be inferred from thetwo other relations.Figure 1 also shows that the task has a flexiblegraph structure.
This structure usually does notcover all the words in a sentence differently fromother natural language processing (NLP) taskssuch as part-of-speech (POS) tagging and depen-Mrs. Tsuruyama is from Kumamoto Prefecture in Japan .PER LOC LOCLive_in Located_inLive_inFigure 1: An entity and relation example (Rothand Yih, 2004).
Person (PER) and location (LOC)entities are connected by Live in and Located inrelations.dency parsing, so local constraints are consideredto be more important in the task.Joint learning approaches (Yang and Cardie,2013; Singh et al., 2013) incorporate these de-pendencies and local constraints in their models;however most approaches are time-consuming andemploy complex structures consisting of multi-ple models.
Li and Ji (2014) recently proposeda history-based structured learning approach thatis simpler and more computationally efficient thanother approaches.
While this approach is promis-ing, it still has a complexity in search and restrictsthe search order partly due to its semi-Markov rep-resentation, and thus the potential of the history-based learning is not fully investigated.In this paper, we introduce an entity and relationtable to address the difficulty in representing thetask.
We propose a joint extraction of entities andrelations using a history-based structured learningon the table.
This table representation simplifiesthe task into a table-filling problem, and makesthe task flexible enough to incorporate several en-hancements that have not been addressed in theprevious history-based approach, such as searchorders in decoding, global features from relationsto entities, and several learning methods with in-exact search.2 MethodIn this section, we first introduce an entity and re-lation table that is utilized to represent the whole1858entity and relation structures in a sentence.
Wethen overview our model on the table.
We finallyexplain the decoding, learning, search order, andfeatures in our model.2.1 Entity and relation tableThe task we address in this work is the extractionof entities and their relations from a sentence.
En-tities are typed and may span multiple words.
Re-lations are typed and directed.We use words to represent entities and relations.We assume entities do not overlap.
We employa BILOU (Begin, Inside, Last, Outside, Unit) en-coding scheme that has been shown to outperformthe traditional BIO scheme (Ratinov and Roth,2009), and we will show that this scheme inducesseveral label dependencies between words and be-tween words and relations in ?2.3.2.
A label isassigned to a word according to the relative posi-tion to its corresponding entity and the type of theentity.
Relations are represented with their typesand directions.
?
denotes a non-relation pair, and?
and?
denote left-to-right and right-to-left re-lations, respectively.
Relations are defined on notentities but words, since entities are not alwaysgiven when relations are extracted.
Relations onentities are mapped to relations on the last wordsof the entities.Based on this representation, we propose an en-tity and relation table that jointly represents en-tities and relations in a sentence.
Figure 2 illus-trates an entity and relation table corresponding toan example in Figure 1.
We use only the lower tri-angular part because the table is symmetric, so thenumber of cells is n(n + 1)/2 when there are nwords in a sentence.
With this entity and relationtable representation, the joint extraction problemcan be mapped to a table-filling problem in thatlabels are assigned to cells in the table.2.2 ModelWe tackle the table-filling problem by a history-based structured learning approach that assigns la-bels to cells one by one.
This is mostly the same asthe traditional history-based model (Collins, 2002)except for the table representation.Let x be an input table, Y(x) be all possibleassignments to the table, and s(x,y) be a scoringfunction that assesses the assignment of y ?
Y(x)to x.
With these definitions, we define our modelto predict the most probable assignment as fol-lows:y?= argmaxy?Y(x)s(x,y) (1)This scoring function is a decomposable function,and each decomposed function assesses the as-signment of a label to a cell in the table.s(x,y) =|x|?i=1s(x,y, 1, i) (2)Here, i represents an index of a cell in the table,which will be explained in ?2.3.1.
The decom-posed function s(x,y, 1, i) corresponds to the i-thcell.
The decomposed function is represented as alinear model, i.e., an inner product of features andtheir corresponding weights.s(x,y, 1, i) = w?f(x,y, 1, i) (3)The scoring function are further divided into twofunctions as follows:s(x,y, 1, i) = slocal(x,y, i) + sglobal(x,y, 1, i)(4)Here, slocal(x,y, i) is a local scoring func-tion that assesses the assignment to the i-thcell without considering other assignments, andsglobal(x,y, 1, i) is a global scoring function thatassesses the assignment in the context of 1st to(i ?
1)-th assignments.
This global scoring func-tion represents the dependencies between entities,between relations, and between entities and rela-tions.
Similarly, features f are divided into localfeatures flocaland global features fglobal, and theyare defined on its target cell and surrounding con-texts.
The features will be explained in ?2.5.
Theweights w can also be divided, but they are tunedjointly in learning as shown in ?2.4.2.3 DecodingThe scoring function s(x,y, 1, i) in Equation (2)uses all the preceding assignments and does notrely on the Markov assumption, so we cannot em-ploy dynamic programming.We instead employ a beam search to find thebest assignment with the highest score (Collinsand Roark, 2004).
The beam search assigns la-bels to cells one by one with keeping the top Kbest assignments when moving from a cell to thenext cell, and it returns the best assignment whenlabels are assigned to all the cells.
The pseudocode for decoding with the beam search is shownin Figure 3.1859Mrs.
Tsutayama is from Kumamoto Prefecture in Japan .Mrs.
B-PERTsutayama ?
L-PERis ?
?
Ofrom ?
?
?
OKumamoto ?
?
?
?
B-LOCPrefecture ?
Live in?
?
?
?
L-LOCin ?
?
?
?
?
?
OJapan ?
Live in?
?
?
?
Located in?
?
U-LOC.
?
?
?
?
?
?
?
?
?Figure 2: The entity and relation table for the example in Figure 1.INPUT: x: input table with no assignment,K: beam sizeOUTPUT: best assignment y?for x1: b?
[x]2: for i = 1 to |x| do3: T ?
?4: for k = 1 to |b| do5: for a ?A(i, b[k]) do6: T ?
T ?
append(a, b[k])7: end for8: end for9: b?
top K tables from T using the scoringfunction in Equation (2)10: end for11: return b[0]Figure 3: Decoding with the beam search.
A(i, t)returns possible assignments for i-th cell of a tablet, and append(a, t) returns a table t updated withan assignment a.We explain how to map the table to a sequence(line 2 in Figure 3), and how to calculate possibleassignments (line 6 in Figure 3) in the followingsubsections.2.3.1 Table-to-sequence mappingCells in an input table are originally indexed intwo dimensions.
To apply our model in ?2.2 to thecells, we need to map the two-dimensional tableto a one-dimensional sequence.
This is equivalentto defining a search order in the table, so we willuse the terms ?mapping?
and ?search order?
inter-changeably.Since it is infeasible to try all possible map-pings, we define six promising static mappings(search orders) as shown in Figure 4.
Note that the?left?
and ?right?
directions in the captions cor-respond to not word orders, but tables.
We de-1 3 6A B CA 1B 2 3C 4 5 6A B C52 4(a) Up todown, left toright1 2 4A B CA 1B 3 2C 6 5 4A B C53 6(b) Up todown, rightto left4 2 1A B CA 4B 5 2C 6 3 1A B C35 6(c) Right toleft, up todown6 3 1A B CA 6B 5 3C 4 2 1A B C25 4(d) Right toleft, down toup1 2 3A B CA 1B 4 2C 6 5 3A B C54 6(e) Close-first, left toright3 2 1A B CA 3B 5 2C 6 4 1A B C45 6(f) Close-first, right toleftFigure 4: Static search orders.fine two mappings (Figures 4(a) and 4(b)) with thehighest priority on the ?up to down?
order, whichchecks a sentence forwardly (from the beginningof a sentence).
Similarly, we also define two map-pings (Figures 4(c) and 4(d)) with the highest pri-ority on the ?right to left?
order, which check asentence backwardly (from the end of a sentence).From another point of view, entities are detectedbefore relations in Figures 4(b) and 4(c) whereasthe order in a sentence is prioritized in Figures 4(a)1860Condition Possible labels on wiRelation(s) on wi?1B-*, O, U-*Relation(s) on wiL-*, U-*Table 1: Label dependencies from relations to en-tities.
* indicates any type.Label on wiRelations from/to wiB-*, I-*, O ?L-*, U-* *Label on wi+1Relations from/to wiI-*, L-* ?B-*, U-*, O *Table 2: Label dependencies from entities to rela-tions.and 4(d).
We further define two close-first map-pings (Figures 4(e) and 4(f)) since entities areeasier to find than relations and close relations areeasier to find than distant relations.We also investigate dynamic mappings (searchorders) with an easy-first policy (Goldberg and El-hadad, 2010).
Dynamic mappings are differentfrom the static mappings above, since we reorderthe cells before each decoding1.
We evaluate thecells using the local scoring function, and assignindices to the cells so that the cells with higherscores have higher priorities.
In addition to thisna?
?ve easy-first policy, we define two other dy-namic mappings that restricts the reordering bycombining the easy-first policy with one of the fol-lowing two policies: entity-first (all entities are de-tected before relations) and close-first (closer cellsare detected before distant cells) policies.2.3.2 Label dependenciesTo avoid illegal assignments to a table, we haveto restrict the possible assignments to the cells ac-cording to the preceding assignments.
This restric-tion can also reduce the computational costs.We consider all the dependencies between cellsto allow the assignments of labels to the cells inan arbitrary order.
Our representation of entitiesand relations in ?2.1 induces the dependencies be-tween entities and between entities and relations.Tables 1-3 summarize these dependencies on the i-th wordwiin a sentence.
We can further utilize de-pendencies between entity types and relation typesif some entity types are involved in a limited num-1It is also possible to reorder the cells during decoding,but it greatly increases the computational costs.Label on wi?2Possible labels on wiB-TYPE B-*, I-TYPE, L-TYPE, O, U-*I-TYPE B-*, I-TYPE, L-TYPE, O, U-*L-TYPE B-*, I-*, L-*, O, U-*O B-*, I-*, L-*, O, U-*U-TYPE B-*, I-*, L-*, O, U-*O/S B-*, I-*, L-*, O, U-*Label on wi?1Possible labels on wiB-TYPE I-TYPE, L-TYPEI-TYPE I-TYPE, L-TYPEL-TYPE B-*, O, U-*O B-*, O, U-*U-TYPE B-*, O, U-*O/S B-*, O, U-*Label on wi+1Possible labels on wiB-TYPE L-*, O, U-*I-TYPE B-TYPE, I-TYPEL-TYPE B-TYPE, I-TYPEO L-*, O, U-*U-TYPE L-*, O, U-*O/S L-*, O, U-*Label on wi+2Possible labels on wiB-TYPE B-*, I-*, L-*, O, U-*I-TYPE B-TYPE, I-TYPE, L-*, O, U-*L-TYPE B-TYPE, I-TYPE, L-*, O, U-*O B-*, I-*, L-*, O, U-*U-TYPE B-*, I-*, L-*, O, U-*O/S B-*, I-*, L-*, O, U-*Table 3: Label dependencies between entities.TYPE represents an entity type, and O/S meansthe word is outside of a sentence.ber of relation types or vice versa.
We note thatthe dependencies between entity types and rela-tion types include not only words participating inrelations but also their surrounding words.
For ex-ample, the label on wi?1can restrict the types ofrelations involving wi.
We employ these type de-pendencies in the evaluation, but we omit these de-pendencies here since these dependencies are de-pendent on the tasks.2.4 LearningThe goal of learning is to minimize errors betweenpredicted assignments y?and gold assignmentsygoldby tuning the weights w in the scoring func-tion in Equation 3.
We employ a margin-basedstructured learning approach to tune the weightsw.
The pseudo code is shown in Figure 5.
This ap-proach enhances the traditional structured percep-1861INPUT: training sets D = {(xi,yi)}Ni=1,T: iterationsOUTPUT: weights w1: w?
02: for t = 1 to T do3: for x,y ?
D do4: y??
best assignment for x using decod-ing in Figure 3 with s?in Equation (5)5: if y?
?= ygoldthen6: m?
argmaxi{s?
(x,ygold, 1, i)?s?
(x,y?, 1, i)}7: w?
update(w, f(x,ygold, 1,m),f(x,y?, 1,m))8: end if9: end for10: end for11: return wFigure 5: Margin-based structured learn-ing approach with a max-violation update.update(w, f(x,ygold, 1,m), f(x,y?, 1,m))depends on employed learning methods.tron (Collins, 2002) in the following ways.
Firstly,we incorporate a margin ?
into the scoring func-tion as follows so that wrong assignments withsmall differences from gold assignments are pe-nalized (lines 4 and 6 in Figure 5) (Freund andSchapire, 1999).s?
(x,y) = s(x,y) + ?
(y,ygold) (5)Similarly to the scoring function s, the margin ?is defined as a decomposable function using 0-1loss as follows:?
(y,ygold) =|x|?i=1?
(yi, ygoldi),?
(yi, ygoldi) ={0 if yi= ygoldi1 otherwise(6)Secondly, we update the weights w based on amax-violation update rule following Huang et al.
(2012) (lines 6-7 in Figure 5).
Finally, we em-ploy not only perceptron (Collins, 2002) but alsoAROW (Mejer and Crammer, 2010; Crammer etal., 2013), AdaGrad (Duchi et al., 2011), andDCD-SSVM (Chang and Yih, 2013) for learningmethods (line 7 in Figure 5.)
We employ parame-ter averaging except for DCD-SSVM.
AROW andAdaGrad store additional information for covari-ance and feature counts respectively, and DCD-SSVM keeps a working set and performs addi-tional updates in each iteration.
Due to space limi-tations, we refer to the papers for the details of thelearning methods.2.5 FeaturesHere, we explain the local features flocaland theglobal features fglobalintroduced in ?2.2.2.5.1 Local featuresOur focus is not to exploit useful local featuresfor entities and relations, so we incorporate severalfeatures from existing work to realize a reasonablebaseline.
Table 4 summarizes the local features.Local features for entities (or words) are similarto the features used by Florian et al.
(2003), butsome features are generalized and extended, andgazetteer features are excluded.
For relations (orpairs of words), we employ and extend features inMiwa et al.
(2009).2.5.2 Global featuresWe design global features to represent dependen-cies among entities and relations.
Table 5 summa-rizes the global features2.
These global featuresare activated when all the information is availableduring decoding.We incorporate label dependency features liketraditional sequential labeling for entities.
Al-though our model can include other non-local fea-tures between entities (Ratinov and Roth, 2009),we do not include them expecting that global fea-tures on entities and relations can cover them.
Wedesign three types of global features for relations.These features are activated when all the partic-ipating relations are not ?
(non-relations).
Fea-tures except for the ?Crossing?
category are simi-lar to global relation features in Li and Ji (2014).We further incorporate global features for both en-tities and relations.
These features are activatedwhen the relation label is not ?.
These featurescan act as a bridge between entities and relations.3 EvaluationIn this section, we first introduce the corpus andevaluation metrics that we employed for evalua-tion.
We then show the performance on the train-ing data set with explaining the parameters used2We tried other ?Entity+Relation?
features to represent arelation and both its participating entities, but they slightlydegraded the performance in our preliminary experiments.1862Target Category FeaturesWord Lexical Character n-grams (n=2,3,4)(Entity) Attributes by parsers (base form, POS)Word types (all-capitalized, initial-capitalized, all-digits, all-puncts, all-digits-or-puncts)Contextual Word n-grams (n=1,2,3) within a context window size of 2Word pair Entity Entity lexical features of each word(Relation) Contextual Word n-grams (n=1,2,3) within a context window size of 2ShortestpathWalk features (word-dependency-word or dependency-word-dependency) on the shortest paths in parsers?
outputsn-grams (n=2,3) of words and dependencies on the pathsn-grams (n=1,2) of token modifier-modifiee pairs on the pathsThe length of the pathsTable 4: Local features.Target Category DetailsEntity Bigram Bigrams of labelsCombinations of two labels and their corresponding POS tagsCombinations of two labels and their corresponding wordsTrigram Trigrams of labelsCombinations of three labels and each of their corresponding POS tagsCombinations of three labels and each of their corresponding wordsEntity Combinations of a label and its corresponding entityRelation Entity-sharingCombinations of two relation labels that share a word (i.e., relations insame columns or same rows in a table)Combinations of two relation labels and the shared wordRelation shortest path features between non-shared words, augmented bya combination of relation labels and the shared wordCyclic Combinations of three relation labels that make a cycleCrossing Combinations of two relation labels that cross each otherEntity + Entity- Relation label and the label of its participating entityRelation relation Relation label and the label and word of its participating entityTable 5: Global features.for the test set evaluation, and show the perfor-mance on the test data set.3.1 Evaluation settingsWe used an entity and relation recognition corpusby Roth and Yih (2004)3.
The corpus defines fournamed entity types Location, Organization, Per-son, andOther and five relation typesKill, Live In,Located In, OrgBased In and Work For.All the entities were words in the original cor-pus because all the spaces in entities were replacedwith slashes.
Previous systems (Roth and Yih,2007; Kate and Mooney, 2010) used these word3conll04.corp at http://cogcomp.cs.illinois.edu/page/resource_view/43boundaries as they were, treated the boundaries asgiven, and focused the entity classification prob-lem alone.
Differently from such systems, we re-covered these spaces by replacing these slasheswith spaces to evaluate the entity boundary detec-tion performance on this corpus.
Due to this re-placement and the inclusion of the boundary de-tection problem, our task is more challenging thanthe original task, and our results are not compara-ble with those by the previous systems.The corpus contains 1,441 sentences that con-tain at least one relation.
Instead of 5-fold crossvalidation on the entire corpus by the previous sys-tems, we split the data set into training (1,153 sen-tences) and blind test (288 sentences) data sets and1863developed the system on the training data set.
Wetuned the hyper-parameters using a 5-fold crossvalidation on the training data set, and evaluatedthe performance on the test set.We prepared a pipeline approach as a baseline.We first trained an entity recognition model usingthe local and global features, and then trained arelation extraction model using the local featuresand global features without global ?Relation?
fea-tures in Table 5.
We did not employ the global?Relation?
features in this baseline since it is com-mon to treat relation extraction as a multi-classclassification problem.We extracted features using the results from twosyntactic parsers Enju (Miyao and Tsujii, 2008)and LRDEP (Sagae and Tsujii, 2007).
We em-ployed feature hashing (Weinberger et al., 2009)and limited the feature space to 224.
The num-bers of features greatly varied for categories andtargets.
They also caused biased predictions thatprefer entities to relations in our preliminary ex-periments.
We thus chose to re-scale the featuresas follows.
We normalized local features for eachfeature category and then for each target.
We alsonormalized global features for each feature cate-gory, but we did not normalize them for each targetsince normalization was impossible during decod-ing.
We instead scaled the global features, and thescaling factor was tuned by using the same 5-foldcross validation above.We used the F1 score on relations with entitiesas our primary evaluation measure and used it fortuning parameters.
In this measure, a relation withtwo entities is considered correct when the offsetsand types of the entities and the type of the relationare all correct.
We also evaluated the F1 scores forentities and relations individually on the test dataset by checking their corresponding cells.
An en-tity is correct when the offset and type are correct,and a relation is correct when the type is correctand the last words of two entities are correct.3.2 Performance on Training Data SetIt is infeasible to investigate all the combinationsof the parameters, so we greedily searched for adefault parameter setting by using the evaluatedresults on the training data set.
The default pa-rameter setting was the best setting except for thebeam size.
We show learning curves on the train-ing data set in Figure 6 when we varied each pa-rameter from the default parameter setting.
Weemployed 5-fold cross validation.
The default pa-rameter setting used DCD-SSVM as the learningmethod, entity-first, easy-first as the search order,local and global features, and 8 as the beam size.This section discusses how these parameters affectthe performance on the training data set and ex-plains how the parameter setting was selected forthe test set.Figure 6(a) compares the learning methods in-troduced in ?2.4.
DCD-SSVM and AdaGrad per-formed slightly better than perceptron, which hasoften been employed in history-based structuredlearning.
AROW did not show comparable per-formance to the others.
We ran 100 iterations tofind the number of iterations that saturates learn-ing curves.
The large number of iterations tooktime and the performance of DCD-SSVM almostconverged after 30 iterations, so we employed 50iterations for other evaluation on the training dataset.
AdaGrad got its highest performance morequickly than other learning methods and AROWconverged slower than other methods, so we em-ployed 10 for AdaGrad, 90 for AROW, and 50 it-erations for other settings on the test data set.The performance was improved by wideningthe beam as in Figure 6(b), but the improvementwas gradually diminished as the beam size in-creased.
Since the wider beam requires more train-ing and test time, we chose 8 for the beam size.Figure 6(c) shows the effects of joint learningas well as features explained in ?2.5.
We show theperformance of the pipeline approach (Pipeline)introduced in ?3.1, and the performance with lo-cal features alone (Local), local and global fea-tures without global ?Relation?
features in Table 5(Local+global (?relation)) and all local and globalfeatures (Local+global).
We note that Pipelineshows the learning curve of relation extraction inthe pipeline approach.
Features in ?Local+global(?relation)?
are the same as the features in thepipeline approach, and the result shows that thejoint learning approach performed slightly betterthan the pipeline approach.
The incorporationof global ?Entity?
and ?Entity+Relation?
featuresimproved the performance as is common with theexisting pipeline approaches, and relation-relatedfeatures further improved the performance.Static search orders in ?2.3.1 also affected theperformance as shown in Figure 6(d), althoughsearch orders are not investigated in the joint en-tity and relation extraction.
Surprisingly, the gap1864(a) Learning methods (b) Beam sizes(c) Features and pipeline / joint approaches (d) Static search orders(e) Dynamic search ordersFigure 6: Learning curves of entity and relation extraction on the training data set using 5-fold crossvalidation.between the performances with the best order andworst order was about 0.04 in an F1 score, whichis statistically significant, and the performance canbe worse than the pipeline approach in Figure 6(c).This means improvement by joint learning can beeasily cancelled out if we do not carefully con-sider search order.
It is also surprising that the sec-ond worst order (Figure 4(b)) is the most intuitive?left-to-right?
order, which is closest to the orderin Li and Ji (2014) among the six search orders.Figure 6(e) shows the performance with dy-namic search orders.
Unfortunately, the easy-firstpolicy did not work well on this entity and relationtask, but, with the two enhancements, dynamic or-ders performed as well as the best static order inFigure 6(d).
This shows that entities should be de-1865tected earlier than relations on this data set.3.3 Performance on Test Data SetTable 6 summarizes the performance on the testdata set.
We employed the default parameter set-ting explained in ?3.2, and compared parametersby changing the parameters shown in the first col-umn.
We performed a statistical test using the ap-proximate randomization method (Noreen, 1989)on our primary measure (?Entity+Relation?).
Theresults are almost consistent with the results on thetraining data set with a few exceptions.Differently from the results on the training dataset, AdaGrad and AROW performed significantlyworse than perceptron and DCD-SSVM and theyperformed slightly worse than the pipeline ap-proach.
This result shows that DCD-SSVM per-forms well with inexact search and the selection oflearning methods can significantly affect the entityand relation extraction performance.The joint learning approach showed a signifi-cant improvement over the pipeline approach withrelation-related global features, although the jointlearning approach alone did not show a signif-icant improvement over the pipeline approach.Unfortunately, no joint learning approach outper-formed the pipeline approach in entity recognition.This may be partly because hyper-parameters weretuned to the primary measure.
The results on thepipeline approach also indicate that the better per-formance on entity recognition does not necessar-ily improve the relation extraction performance.Search orders also affected the performance,and the worst order (right to left, down to up) andbest order (close-first, left to right) were signifi-cantly different.
The performance of the worst or-der was worse than that of the pipeline approach,although the difference was not significant.
Theseresults show that it is necessary to carefully selectthe search order for the joint entity and relationextraction task.3.4 Comparison with Other SystemsTo compare our model with the other sys-tems (Roth and Yih, 2007; Kate and Mooney,2010), we evaluated the performance of our modelwhen the entity boundaries were given.
Differ-ently from our setting in ?3.1, we used the goldentity boundaries encoded in the BILOU schemeand assigned entity labels to the boundaries.
Weperformed 5-fold cross validation on the data setfollowing Roth and Yih (2007) although the splitwas different from theirs since their splits were notavailable.
We employed the default parameter set-ting in ?3.2 for this comparison.Table 7 shows the evaluation results.
Althoughwe cannot directly compare the results, our modelperforms better than the other models.
Comparedto Table 6, Table 7 also shows that the inclusionof entity boundary detection degrades the perfor-mance about 0.09 in F-score.4 Related WorkSearch order in structured learning has been stud-ied in several NLP tasks.
Left-to-right and right-to-left orderings have been often investigated insequential labeling tasks (Kudo and Matsumoto,2001).
Easy-first policy was firstly introducedby Goldberg and Elhadad (2010) for dependencyparsing, and it was successfully employed in sev-eral tasks, such as joint POS tagging and depen-dency parsing (Ma et al., 2012) and co-referenceresolution (Stoyanov and Eisner, 2012).
Searchorder, however, has not been focused in relationextraction tasks.Named entity recognition (Florian et al., 2003;Nadeau and Sekine, 2007) and relation extrac-tion (Zelenko et al., 2003; Miwa et al., 2009)have often been treated as separate tasks, butthere are some previous studies that treat enti-ties and relations jointly in learning.
Most stud-ies built joint learning models upon individualmodels for subtasks, such as Integer Linear Pro-gramming (ILP) (Roth and Yih, 2007; Yang andCardie, 2013) and Card-Pyramid Parsing (Kateand Mooney, 2010).
Our approach does not re-quire such individual models, and it also can de-tect entity boundaries that these approaches exceptfor Yang and Cardie (2013) did not treat.
Otherstudies (Yu and Lam, 2010; Singh et al., 2013)built global probabilistic graphical models.
Theyneed to compute distributions over variables, butour approach does not.
Li and Ji (2014) proposedan approach to jointly find entities and relations.They incorporated a semi-Markov chain in repre-senting entities and they defined two actions dur-ing search, but our approach does not employ suchrepresentation and actions, and thus it is more sim-ple and flexible to investigate search orders.5 ConclusionsIn this paper, we proposed a history-based struc-tured learning approach that jointly detects enti-1866Parameter Entity Relation Entity+RelationPerceptron 0.809 / 0.809 / 0.809 0.760 / 0.547 / 0.636 0.731 / 0.527 / 0.612?AdaGrad 0.801 / 0.790 / 0.795 0.732 / 0.486 / 0.584 0.716 / 0.476 / 0.572AROW 0.810 / 0.802 / 0.806 0.797 / 0.468 / 0.590 0.758 / 0.445 / 0.561DCD-SSVM?0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610?Pipeline 0.823 / 0.814 / 0.818 0.672 / 0.542 / 0.600 0.647 / 0.522 / 0.577Local 0.819 / 0.812 / 0.815 0.844 / 0.399 / 0.542 0.812 / 0.384 / 0.522Local + global (?relation) 0.809 / 0.799 / 0.804 0.784 / 0.481 / 0.596 0.747 / 0.458 / 0.568Local + global?0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610?
(a) Up to down, left to right 0.824 / 0.801 / 0.813 0.821 / 0.433 / 0.567 0.787 / 0.415 / 0.543(b) Up to down, right to left 0.828 / 0.808 / 0.818 0.850 / 0.461 / 0.597 0.822 / 0.445 / 0.578(c) Right to left, up to down 0.823 / 0.799 / 0.811 0.826 / 0.448 / 0.581 0.789 / 0.427 / 0.554(d) Right to left, down to up 0.811 / 0.784 / 0.797 0.774 / 0.445 / 0.565 0.739 / 0.425 / 0.540(e) Close-first, left to right 0.821 / 0.806 / 0.813 0.807 / 0.522 / 0.634 0.780 / 0.504 / 0.612?
(f) Close-first, right to left 0.817 / 0.801 / 0.809 0.832 / 0.491 / 0.618 0.797 / 0.471 / 0.592Easy-first 0.811 / 0.790 / 0.801 0.862 / 0.415 / 0.560 0.831 / 0.399 / 0.540Entity-first, easy-first?0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610?Close-first, easy-first 0.816 / 0.803 / 0.810 0.796 / 0.486 / 0.603 0.767 / 0.468 / 0.581Table 6: Performance of entity and relation extraction on the test data set (precision / recall / F1 score).The?denotes the default parameter setting in ?3.2 and?represents a significant improvement over theunderlined ?Pipeline?
baseline (p<0.05).
Labels (a)-(f) correspond to those in Figure 4.Kate and Mooney (2010) Roth and Yih (2007) Entity-first, easy-firstPerson 0.921 / 0.942 / 0.932 0.891 / 0.895 / 0.890 0.931 / 0.948 / 0.939Location 0.908 / 0.942 / 0.924 0.897 / 0.887 / 0.891 0.922 / 0.939 / 0.930Organization 0.905 / 0.887 / 0.895 0.895 / 0.720 / 0.792 0.903 / 0.896 / 0.899All entities - - 0.924 / 0.924 / 0.924Located In 0.675 / 0.567 / 0.583 0.539 / 0.557 / 0.513 0.821 / 0.549 / 0.654Work For 0.735 / 0.683 / 0.707 0.720 / 0.423 / 0.531 0.886 / 0.642 / 0.743OrgBased In 0.662 / 0.641 / 0.647 0.798 / 0.416 / 0.543 0.768 / 0.572 / 0.654Live In 0.664 / 0.601 / 0.629 0.591 / 0.490 / 0.530 0.819 / 0.532 / 0.644Kill 0.916 / 0.641 / 0.752 0.775 / 0.815 / 0.790 0.933 / 0.797 / 0.858All relations - - 0.837 / 0.599 / 0.698Table 7: Results of entity classification and relation extraction on the data set using the 5-fold crossvalidation (precision / recall / F1 score).ties and relations.
We introduced a novel entityand relation table that jointly represents entitiesand relations, and showed how the entity and re-lation extraction task can be mapped to a simpletable-filling problem.
We also investigated searchorders and learning methods that have been fixedin previous research.
Experimental results showedthat the joint learning approach outperforms thepipeline approach and the appropriate selection oflearning methods and search orders is crucial toproduce a high performance on this task.As future work, we plan to apply this approachto other relation extraction tasks and explore moresuitable search orders for relation extraction tasks.We also plan to investigate the potential of this ta-ble representation in other tasks such as semanticparsing and co-reference resolution.AcknowledgmentsWe thank Yoshimasa Tsuruoka and Yusuke Miyaofor valuable discussions, and the anonymous re-viewers for their insightful comments.
This workwas supported by the TTI Start-Up ResearchSupport Program and the JSPS Grant-in-Aid forYoung Scientists (B) [grant number 25730129].1867ReferencesMing-Wei Chang and Wen-Tau Yih.
2013.
Dual coor-dinate descent algorithms for efficient large marginstructured prediction.
Transactions of the Associa-tion for Computational Linguistics, 1:207?218.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, pages 1?8.
Associ-ation for Computational Linguistics, July.Koby Crammer, Alex Kulesza, and Mark Dredze.2013.
Adaptive regularization of weight vectors.Machine learning, 91(2):155?187.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Radu Florian, Abe Ittycheriah, Hongyan Jing, andTong Zhang.
2003.
Named entity recognitionthrough classifier combination.
In Walter Daele-mans andMiles Osborne, editors, Proceedings of theSeventh Conference on Natural Language Learningat HLT-NAACL 2003, pages 168?171.Yoav Freund and Robert E Schapire.
1999.
Largemargin classification using the perceptron algorithm.Machine learning, 37(3):277?296.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 742?750, Los Angeles, California,June.
Association for Computational Linguistics.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages142?151, Montr?eal, Canada, June.
Association forComputational Linguistics.Rohit J. Kate and Raymond Mooney.
2010.
Joint en-tity and relation extraction using card-pyramid pars-ing.
In Proceedings of the Fourteenth Conference onComputational Natural Language Learning, pages203?212, Uppsala, Sweden, July.
Association forComputational Linguistics.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proceedings of the Sec-ond Meeting of the North American Chapter of theAssociation for Computational Linguistics on Lan-guage Technologies, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Qi Li and Heng Ji.
2014.
Incremental joint extrac-tion of entity mentions and relations.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 402?412, Baltimore, Maryland, June.Association for Computational Linguistics.Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren.2012.
Easy-first Chinese POS tagging and depen-dency parsing.
In Proceedings of COLING 2012,pages 1731?1746, Mumbai, India, December.
TheCOLING 2012 Organizing Committee.Avihai Mejer and Koby Crammer.
2010.
Confidencein structured-prediction using confidence-weightedmodels.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, pages 971?981, Cambridge, MA, October.
As-sociation for Computational Linguistics.Makoto Miwa, Rune S?tre, Yusuke Miyao, andJun?ichi Tsujii.
2009.
A rich feature vector forprotein-protein interaction extraction from multiplecorpora.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, pages 121?130, Singapore, August.
Associationfor Computational Linguistics.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80, March.David Nadeau and Satoshi Sekine.
2007.
A sur-vey of named entity recognition and classification.Lingvisticae Investigationes, 30(1):3?26.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses : An Introduction.
Wiley-Interscience, April.Lev Ratinov and Dan Roth.
2009.
Design chal-lenges and misconceptions in named entity recog-nition.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL-2009), pages 147?155, Boulder, Colorado,June.
Association for Computational Linguistics.Dan Roth and Wen-Tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Hwee Tou Ng and Ellen Riloff, ed-itors, HLT-NAACL 2004 Workshop: Eighth Confer-ence on Computational Natural Language Learning(CoNLL-2004), pages 1?8, Boston, Massachusetts,USA, May.
Association for Computational Linguis-tics.Dan Roth and Wen-Tau Yih, 2007.
Global Inferencefor Entity and Relation Identification via a LinearProgramming Formulation.
MIT Press.1868Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR models andparser ensembles.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages1044?1050, Prague, Czech Republic, June.
Associ-ation for Computational Linguistics.Sameer Singh, Sebastian Riedel, Brian Martin, JiapingZheng, and Andrew McCallum.
2013.
Joint infer-ence of entities, relations, and coreference.
In Pro-ceedings of the 2013 workshop on Automated knowl-edge base construction, pages 1?6.
ACM.Veselin Stoyanov and Jason Eisner.
2012.
Easy-firstcoreference resolution.
In Proceedings of COLING2012, pages 2519?2534, Mumbai, India, December.The COLING 2012 Organizing Committee.Kilian Weinberger, Anirban Dasgupta, John Langford,Alex Smola, and Josh Attenberg.
2009.
Featurehashing for large scale multitask learning.
In Pro-ceedings of the 26th Annual International Confer-ence on Machine Learning, ICML ?09, pages 1113?1120, New York, NY, USA.
ACM.Bishan Yang and Claire Cardie.
2013.
Joint infer-ence for fine-grained opinion extraction.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1640?1649, Sofia, Bulgaria, August.Association for Computational Linguistics.Xiaofeng Yu and Wai Lam.
2010.
Jointly identifyingentities and extracting relations in encyclopedia textvia a graphical model approach.
In Coling 2010:Posters, pages 1399?1407, Beijing, China, August.Coling 2010 Organizing Committee.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
The Journal of Machine Learning Re-search, 3:1083?1106.1869
