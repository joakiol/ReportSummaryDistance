2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29?38,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsFast Inference in Phrase Extraction Models with Belief PropagationDavid Burkett and Dan KleinComputer Science DivisionUniversity of California, Berkeley{dburkett,klein}@cs.berkeley.eduAbstractModeling overlapping phrases in an align-ment model can improve alignment qualitybut comes with a high inference cost.
Forexample, the model of DeNero and Klein(2010) uses an ITG constraint and beam-basedViterbi decoding for tractability, but is stillslow.
We first show that their model can beapproximated using structured belief propaga-tion, with a gain in alignment quality stem-ming from the use of marginals in decoding.We then consider a more flexible, non-ITGmatching constraint which is less efficient forexact inference but more efficient for BP.
Withthis new constraint, we achieve a relative errorreduction of 40% in F5 and a 5.5x speed-up.1 IntroductionModern statistical machine translation (MT) sys-tems most commonly infer their transfer rules fromword-level alignments (Koehn et al, 2007; Li andKhudanpur, 2008; Galley et al, 2004), typicallyusing a deterministic heuristic to convert these tophrase alignments (Koehn et al, 2003).
There havebeen many attempts over the last decade to developmodel-based approaches to the phrase alignmentproblem (Marcu and Wong, 2002; Birch et al, 2006;DeNero et al, 2008; Blunsom et al, 2009).
How-ever, most of these have met with limited successcompared to the simpler heuristic method.
One keyproblem with typical models of phrase alignmentis that they choose a single (latent) segmentation,giving rise to undesirable modeling biases (DeNeroet al, 2006) and reducing coverage, which in turnreduces translation quality (DeNeefe et al, 2007;DeNero et al, 2008).
On the other hand, the extrac-tion heuristic identifies many overlapping options,and achieves high coverage.In response to these effects, the recent phrasealignment work of DeNero and Klein (2010) mod-els extraction sets: collections of overlapping phrasepairs that are consistent with an underlying wordalignment.
Their extraction set model is empiricallyvery accurate.
However, the ability to model over-lapping ?
and therefore non-local ?
features comesat a high computational cost.
DeNero and Klein(2010) handle this in part by imposing a structuralITG constraint (Wu, 1997) on the underlying wordalignments.
This permits a polynomial-time algo-rithm, but it is still O(n6), with a large constantfactor once the state space is appropriately enrichedto capture overlap.
Therefore, they use a heavilybeamed Viterbi search procedure to find a reason-able alignment within an acceptable time frame.
Inthis paper, we show how to use belief propagation(BP) to improve on the model?s ITG-based struc-tural formulation, resulting in a new model that issimultaneously faster and more accurate.First, given the model of DeNero and Klein(2010), we decompose it into factors that admitan efficient BP approximation.
BP is an inferencetechnique that can be used to efficiently approxi-mate posterior marginals on variables in a graphicalmodel; here the marginals of interest are the phrasepair posteriors.
BP has only recently come into usein the NLP community, but it has been shown to beeffective in other complex structured classificationtasks, such as dependency parsing (Smith and Eis-ner, 2008).
There has also been some prior successin using BP for both discriminative (Niehues andVogel, 2008) and generative (Cromie`res and Kuro-hashi, 2009) word alignment models.By aligning all phrase pairs whose posterior underBP exceeds some fixed threshold, our BP approxi-mation of the model of DeNero and Klein (2010) can29achieve a comparable phrase pair F1.
Furthermore,because we have posterior marginals rather than asingle Viterbi derivation, we can explicitly force thealigner to choose denser extraction sets simply bylowering the marginal threshold.
Therefore, we alsoshow substantial improvements over DeNero andKlein (2010) in recall-heavy objectives, such as F5.More importantly, we also show how the BP fac-torization allows us to relax the ITG constraint, re-placing it with a new set of constraints that per-mit a wider family of alignments.
Compared toITG, the resulting model is less efficient for exactinference (where it is exponential), but more effi-cient for our BP approximation (where it is onlyquadratic).
Our new model performs even betterthan the ITG-constrained model on phrase align-ment metrics while being faster by a factor of 5.5x.2 Extraction Set ModelsFigure 1 shows part of an aligned sentence pair, in-cluding the word-to-word alignments, and the ex-tracted phrase pairs licensed by those alignments.Formally, given a sentence pair (e, f), a word-levelalignment a is a collection of links between targetwords ei and source words fj .
Following past work,we further divide word links into two categories:sure and possible, shown in Figure 1 as solid andhatched grey squares, respectively.
We represent aas a grid of ternary word link variables aij , each ofwhich can take the value sure to represent a sure linkbetween ei and fj , poss to represent a possible link,or off to represent no link.An extraction set pi is a set of aligned phrase pairsto be extracted from (e, f), shown in Figure 1 asgreen rounded rectangles.
We represent pi as a set ofboolean variables pighk`, which each have the valuetrue when the target span [g, h] is phrase-aligned tothe source span [k, `].
Following previous work onphrase extraction, we limit the size of pi by imposinga phrase length limit d: pi only contains a variablepighk` if h?
g < d and `?
k < d.There is a deterministic mapping pi(a) from aword alignment to the extraction set licensed by thatword alignment.
We will briefly describe it here, andthen present our factorized model.e3 e4 e5 e6 e7f5f6f7f8f9?f5 = [7, 7]?f6 = [5, 6]?f7 = [5, 6]?f8 = [4, 4]?f9 = [?1,?
]Figure 1: A schematic representation of part of a sen-tence pair.
Solid grey squares indicate sure links (e.g.a48 = sure), and hatched squares possible links (e.g.a67 = poss).
Rounded green rectangles are extractedphrase pairs (e.g.
pi5667 = true).
Target spans are shownas blue vertical lines and source spans as red horizontallines.
Because there is a sure link at a48, ?f8 = [4, 4] doesnot include the possible link at a38.
However, f7 onlyhas possible links, so ?f7 = [5, 6] is the span containingthose.
f9 is null-aligned, so ?f9 = [?1,?
], which blocksall phrase pairs containing f9 from being extracted.2.1 Extraction Sets from Word AlignmentsThe mapping from a word alignment to the set oflicensed phrase pairs pi(a) is based on the standardrule extraction procedures used in most modern sta-tistical systems (Koehn et al, 2003; Galley et al,2006; Chiang, 2007), but extended to handle pos-sible links (DeNero and Klein, 2010).
We start byusing a to find a projection from each target word eionto a source span, represented as blue vertical linesin Figure 1.
Similarly, source words project ontotarget spans (red horizontal lines in Figure 1).
pi(a)contains a phrase pair iff every word in the targetspan projects within the source span and vice versa.Figure 1 contains an example for d = 2.Formally, the mapping introduces a set of spans?.
We represent the spans as variables whose valuesare intervals, where ?ei = [k, `] means that the tar-get word ei projects to the source span [k, `].
Theset of legal values for ?ei includes any interval with0 ?
k ?
` < |f| and ` ?
k < d, plus the special in-terval [?1,?]
that indicates ei is null-aligned.
Thespan variables for source words ?fj have target spans[g, h] as values and are defined analogously.For a set I of positions, we define the range func-30tion:range(I) ={[?1,?]
I = ?
[mini?I i,maxi?I i] else(1)For a fixed word alignment a we set the targetspan variable ?ei :?ei,s = range({j : aij = sure}) (2)?ei,p = range({j : aij 6= off}) (3)?ei = ?ei,s ?
?ei,p (4)As illustrated in Figure 1, this sets ?ei to the min-imal span containing all the source words with asure link to ei if there are any.
Otherwise, becauseof the special case for range(I) when I is empty,?ei,s = [?1,?
], so ?ei is the minimal span containingall poss-aligned words.
If all word links to ei are off,indicating that ei is null-aligned, then ?ei is [?1,?
],preventing the alignment of any phrase pairs con-taining ei.Finally, we specify which phrase pairs should beincluded in the extraction set pi.
Given the spans ?based on a, pi(a) sets pighk` = true iff every word ineach phrasal span projects within the other:?ei ?
[k, `] ?i ?
[g, h] (5)?fj ?
[g, h] ?j ?
[k, `]2.2 Formulation as a Graphical ModelWe score triples (a, pi, ?)
as the dot product of aweight vector w that parameterizes our model and afeature vector ?
(a, pi, ?).
The feature vector decom-poses into word alignment features ?a, phrase pairfeatures ?pi and target and source null word features?e?
and ?f?
:1?
(a, pi, ?)
=?i,j?a(aij) +?g,h,k,`?pi(pighk`)+?i?e?
(?ei ) +?j?f?
(?fj ) (6)This feature function is exactly the same as thatused by DeNero and Klein (2010).2 However, while1In addition to the arguments we write out explicitly, all fea-ture functions have access to the observed sentence pair (e, f).2Although the null word features are not described in DeN-ero and Klein (2010), all of their reported results include thesefeatures (DeNero, 2010).they formulated their inference problem as a searchfor the highest scoring triple (a, pi, ?)
for an ob-served sentence pair (e, f), we wish to derive a con-ditional probability distribution p(a, pi, ?|e, f).
Wedo this with the standard transformation for linearmodels: p(a, pi, ?|e, f) ?
exp(w??
(a, pi, ?)).
Due tothe factorization in Eq.
(6), this exponentiated formbecomes a product of local multiplicative factors,and hence our model forms an undirected graphicalmodel, or Markov random field.In addition to the scoring function, our modelalso includes constraints on which triples (a, pi, ?
)have nonzero probability.
DeNero and Klein (2010)implicitly included these constraints in their repre-sentation: instead of sets of variables, they used astructured representation that only encodes triples(a, pi, ?)
satisfying both the mapping pi = pi(a) andthe structural constraint that a can be generated bya block ITG grammar.
However, our inference pro-cedure, BP, requires that we represent (a, pi, ?)
as anassignment of values to a set of variables.
Therefore,we must explicitly encode all constraints into themultiplicative factors that define the model.
To ac-complish this, in addition to the soft scoring factorswe have already mentioned, our model also includesa set of hard constraint factors.
Hard constraint fac-tors enforce the relationships between the variablesof the model by taking a value of 0 when the con-straints they encode are violated and a value of 1when they are satisfied.
The full factor graph rep-resentation of our model, including both soft scor-ing factors and hard constraint factors, is drawnschematically in Figure 2.2.2.1 Soft Scoring FactorsThe scoring factors all take the form exp(w ?
?
),and so can be described in terms of their respectivelocal feature vectors, ?.
Depending on the values ofthe variables each factor depends on, the factor canbe active or inactive.
Features are only extracted foractive factors; otherwise ?
is empty and the factorproduces a value of 1.SURELINK.
Each word alignment variable aijhas a corresponding SURELINK factor Lij to incor-porate scores from the features ?a(aij).
Lij is ac-tive whenever aij = sure.
?a(aij) includes poste-riors from unsupervised jointly trained HMM wordalignment models (Liang et al, 2006), dictionary31a11 a21LijL11 L21a12ai1Li1L12 L22a22a1j aijL1jA(a) ITG factoragk ahkag?
ah?ag|f| ah|f|a|e|ka|e|?Pghk?Rghk?
?ghk?Seg SehNehNeg?eg ?ehSfkSf?Nf?Nfk?fk?f?
(b) SPAN and EXTRACT factorsFigure 2: A factor graph representation of the ITG-based extraction set model.
For visual clarity, we draw the graphseparated into two components: one containing the factors that only neighbor word link variables, and one containingthe remaining factors.and identical word features, a position distortion fea-ture, and features for numbers and punctuation.PHRASEPAIR.
For each phrase pair variablepighk`, scores from ?pi(pighk`) come from the factorRghk`, which is active if pighk` = true.
Most of themodel?s features are on these factors, and includerelative frequency statistics, lexical template indica-tor features, and indicators for numbers of words andChinese characters.
See DeNero and Klein (2010)for a more comprehensive list.NULLWORD.
We can determine if a word isnull-aligned by looking at its corresponding spanvariable.
Thus, we include features from ?e?
(?ei ) ina factor N ei that is active if ?ei = [?1,?].
Thefeatures are mostly indicators for common words.There are also factors Nfj for source words, whichare defined analogously.2.2.2 Hard Constraint FactorsWe encode the hard constraints on relationshipsbetween variables in our model using three fami-lies of factors, shown graphically in Figure 2.
TheSPAN and EXTRACT factors together ensure thatpi = pi(a).
The ITG factor encodes the structuralconstraint on a.SPAN.
First, for each target word ei we includea factor Sei to ensure that the span variable ?ei hasa value that agrees with the projection of the wordalignment a.
As shown in Figure 2b, Sei dependson ?ei and all the word alignment variables aij incolumn i of the word alignment grid.
Sei has value1 iff the equality in Eq.
(4) holds.
Our model alsoincludes a factor Sfj to enforce the analogous rela-tionship between each ?fj and corresponding row jof a.EXTRACT.
For each phrase pair variable pighk`we have a factor Pghk` to ensure that pighk` = trueiff it is licensed by the span projections ?.
As shownin Figure 2b, in addition to pighk`, Pghk` depends onthe range of span variables ?ei for i ?
[g, h] and ?fjfor j ?
[k, `].
Pghk` is satisfied when pighk` = trueand the relations in Eq.
(5) all hold, or when pighk` =false and at least one of those relations does not hold.ITG.
Finally, to enforce the structural constrainton a, we include a single global factor A that de-pends on all the word link variables in a (see Fig-ure 2a).
A is satisfied iff a is in the family ofblock inverse transduction grammar (ITG) align-ments.
The block ITG family permits multiple linksto be on (aij 6= off) for a particular word ei via termi-nal block productions, but ensures that every word is32in at most one such terminal production, and that thefull set of terminal block productions is consistentwith ITG reordering patterns (Zhang et al, 2008).3 Relaxing the ITG ConstraintThe ITG factor can be viewed as imposing two dif-ferent types of constraints on allowable word align-ments a.
First, it requires that each word is alignedto at most one relatively short subspan of the othersentence.
This is a linguistically plausible con-straint, as it is rarely the case that a single word willtranslate to an extremely long phrase, or to multiplewidely separated phrases.3The other constraint imposed by the ITG factoris the ITG reordering constraint.
This constraintis imposed primarily for reasons of computationaltractability: the standard dynamic program for bi-text parsing depends on ITG reordering (Wu, 1997).While this constraint is not dramatically restric-tive (Haghighi et al, 2009), it is plausible that re-moving it would permit the model to produce betteralignments.
We tested this hypothesis by develop-ing a new model that enforces only the constraintthat each word align to one limited-length subspan,which can be viewed as a generalization of the at-most-one-to-one constraint frequently considered inthe word-alignment literature (Taskar et al, 2005;Cromie`res and Kurohashi, 2009).Our new model has almost exactly the same formas the previous one.
The only difference is that A isreplaced with a new family of simpler factors:ONESPAN.
For each target word ei (and eachsource word fj) we include a hard constraint factorU ei (respectively Ufj ).
U ei is satisfied iff |?ei,p| < d(length limit) and either ?ei,p = [?1,?]
or ?j ?
?ei,p, aij 6= off (no gaps), with ?ei,p as in Eq.
(3).
Fig-ure 3 shows the portion of the factor graph from Fig-ure 2a redrawn with the ONESPAN factors replacingthe ITG factor.
As Figure 3 shows, there is no longera global factor; each U ei depends only on the wordlink variables from column i.3Short gaps can be accomodated within block ITG (and inour model are represented as possible links) as long as the totalaligned span does not exceed the block size.a11 a21LijL11 L21a12ai1Li1L12 L22a22a1j aijL1jUf1Uf2UfjUe1 Ue2 UeiFigure 3: ONESPAN factors4 Belief PropagationBelief propagation is a generalization of the wellknown sum-product algorithm for undirected graph-ical models.
We will provide only a proceduralsketch here, but a good introduction to BP for in-ference in structured NLP models can be foundin Smith and Eisner (2008), and Chapters 16 and 23of MacKay (2003) contain a general introduction toBP in the more general context of message-passingalgorithms.At a high level, each variable maintains a localdistribution over its possible values.
These local dis-tribution are updated via messages passed betweenvariables and factors.
For a variable V , N (V ) de-notes the set of factors neighboring V in the fac-tor graph.
Similarly, N (F ) is the set of variablesneighboring the factor F .
During each round of BP,messages are sent from each variable to each of itsneighboring factors:q(k+1)V?F (v) ?
?G?N (V ),G 6=Fr(k)G?V (v) (7)and from each factor to each of its neighboring vari-ables:r(k+1)F?V (v) ?
?XF ,XF [V ]=vF (XF )?U?N (F ),U 6=Vq(k)U?F (v) (8)where XF is a partial assignment of values to justthe variables in N (F ).33Marginal beliefs at time k can be computed bysimply multiplying together all received messagesand normalizing:b(k)V (v) ?
?G?N (V )r(k)G?V (v) (9)Although messages can be updated according toany schedule, generally one iteration of BP updateseach message once.
The process iterates until somestopping criterion has been met: either a fixed num-ber of iterations or some convergence metric.For our models, we say that BP has convergedwhenever?V,v(b(k)V (v)?
b(k?1)V (v))2< ?
forsome small ?
> 0.4 While we have no theoreticalconvergence guarantees, it usually converges within10 iterations in practice.5 Efficient BP for Extraction Set ModelsIn general, the efficiency of BP depends directly onthe arity of the factors in the model.
Performedna?
?vely, the sum in Eq.
(8) will take time that growsexponentially with the size of N (F ).
For the soft-scoring factors, which each depend only on a singlevariable, this isn?t a problem.
However, our modelalso includes factors whose arity grows with the in-put size: for example, explicitly enumerating all as-signments to the word link variables that the ITGfactor depends on would take O(3n2) time.5To run BP in a reasonable time frame, we needefficient factor-specific propagators that can exploitthe structure of the factor functions to compute out-going messages in polynomial time (Duchi et al,2007; Smith and Eisner, 2008).
Fortunately, all ofour hard constraints permit dynamic programs thataccomplish this propagation.
Space does not permita full description of these dynamic programs, but wewill briefly sketch the intuitions behind them.SPAN and ONESPAN.
Marginal beliefs for Sei orU ei can be computed inO(nd2) time.
The key obser-vation is that for any legal value ?ei = [k, `], Sei andU ei require that aij = off for all j /?
[k, `].6 Thus, westart by computing the product of all the off beliefs:4We set ?
= 0.001.5For all asymptotic analysis, we define n = max(|e|, |f|).6For ease of exposition, we assume that all alignments areeither sure or off ; the modifications to account for the generalcase are straightforward.Factor Runtime Count TotalSURELINK O(1) O(n2) O(n2)PHRASEPAIR O(1) O(n2d2) O(n2d2)NULLWORD O(nd) O(n) O(n2d)SPAN O(nd2) O(n) O(n2d2)EXTRACT O(d3) O(n2d2) O(n2d5)ITG O(n6) 1 O(n6)ONESPAN O(nd2) O(n) O(n2d2)Table 1: Asymptotic complexity for all factors.b?
=?j qaij (off).
Then, for each of the O(nd) legalsource spans [k, `] we can efficiently find a joint be-lief by summing over consistent assignments to theO(d) link variables in that span.EXTRACT.
Marginal beliefs for Pghk` can becomputed inO(d3) time.
For each of theO(d) targetwords, we can find the total incoming belief that ?eiis within [k, `] by summing over the O(d2) values[k?, `?]
where [k?, `?]
?
[k, `].
Likewise for sourcewords.
Multiplying together these per-word beliefsand the belief that pighk` = true yields the joint be-lief of a consistent assignment with pighk` = true,which can be used to efficiently compute outgoingmessages.ITG.
To build outgoing messages, the ITG fac-torA needs to compute marginal beliefs for all of theword link variables aij .
These can all be computedin O(n6) time by using a standard bitext parser torun the inside-outside algorithm.
By using a normalform grammar for block ITG with nulls (Haghighiet al, 2009), we ensure that there is a 1-1 correspon-dence between the ITG derivations the parser sumsover and word alignments a that satisfy A.The asymptotic complexity for all the factors isshown in Table 1.
The total complexity for inferencein each model is simply the sum of the complexitiesof its factors, so the complexity of the ITG model isO(n2d5 + n6), while the complexity of the relaxedmodel is just O(n2d5).
The complexity of exact in-ference, on the other hand, is exponential in d for theITG model and exponential in both d and n for therelaxed model.346 Training and DecodingWe use BP to compute marginal posteriors, whichwe use at training time to get expected feature countsand at test time for posterior decoding.
For each sen-tence pair, we continue to pass messages until eitherthe posteriors converge, or some maximum numberof iterations has been reached.7 After running BP,the marginals we are interested in can all be com-puted with Eq.
(9).6.1 TrainingWe train the model to maximize the log likelihood ofmanually word-aligned gold training sentence pairs(with L2 regularization).
Because pi and ?
are deter-mined when a is observed, the model has no latentvariables.
Therefore, the gradient takes the standardform for loglinear models:OLL = ?
(a, pi, ?)
?
(10)?a?,pi?,?
?p(a?, pi?, ?
?|e, f)?
(a?, pi?, ??)?
?wThe feature vector ?
contains features on sureword links, extracted phrase pairs, and null-alignedwords.
Approximate expectations of these featurescan be efficiently computed using the marginal be-liefs baij (sure), bpighk`(true), and b?ei ([?1,?])
andb?fj ([?1,?
]), respectively.
We learned our finalweight vectorw using AdaGrad (Duchi et al, 2010),an adaptive subgradient version of standard stochas-tic gradient ascent.6.2 TestingWe evaluate our model by measuring precision andrecall on extracted phrase pairs.
Thus, the decod-ing problem takes a sentence pair (e, f) as input, andmust produce an extraction set pi as output.
Our ap-proach, posterior thresholding, is extremely simple:we set pighk` = true iff bpighk`(true) ?
?
for somefixed threshold ?
.
Note that this decoding methoddoes not require that there be any underlying wordalignment a licensing the resulting extraction set pi,87See Section 7.2 for an empirical investigation of this maxi-mum.8This would be true even if we computed posteriors ex-actly, but is especially true with approximate marginals fromBP, which are not necessarily consistent.but the structure of the model is such that two con-flicting phrase pairs are unlikely to simultaneouslyhave high posterior probability.Most publicly available translation systems ex-pect word-level alignments as input.
These canalso be generated by applying posterior threshold-ing, aligning target word i to source word j when-ever baij (sure) ?
t.97 ExperimentsOur experiments are performed on Chinese-to-English alignment.
We trained and evaluated allmodels on the NIST MT02 test set, which consistsof 150 training and 191 test sentences and has beenused previously in alignment experiments (Ayan andDorr, 2006; Haghighi et al, 2009; DeNero andKlein, 2010).
The unsupervised HMM word alignerused to generate features for the model was trainedon 11.3 million words of FBIS newswire data.
Wetest three models: the Viterbi ITG model of DeNeroand Klein (2010), our BP ITG model that uses theITG factor, and our BP Relaxed model that replacesthe ITG factor with the ONESPAN factors.
In all ofour experiments, the phrase length d was set to 3.107.1 Phrase AlignmentWe tested the models by computing precision andrecall on extracted phrase pairs, relative to the goldphrase pairs of up to length 3 induced by the goldword alignments.
For the BP models, we tradeoff precision and recall by adjusting the decodingthreshold ?
.
The Viterbi ITG model was trained tooptimize F5, a recall-biased measure, so in additionto F1, we also report the recall-biased F2 and F5measures.
The maximum number of BP iterationswas set to 5 for the BP ITG model and to 10 for theBP Relaxed model.The phrase alignment results are shown in Fig-ure 4.
The BP ITG model performs comparably tothe Viterbi ITG model.
However, because posteriordecoding permits explicit tradeoffs between preci-sion and recall, it can do much better in the recall-biased measures, even though the Viterbi ITG modelwas explicitly trained to maximize F5 (DeNero and9For our experiments, we set t = 0.2.10Because the runtime of the Viterbi ITG model grows expo-nentially with d, it was not feasible to perform comparisons forhigher phrase lengths.35beta p r f2 0.69 0.742 0.7309823606570758060 65 70 75 80 85RecallPrecisionViterbi ITG BP ITG BP RelaxedModelBest Scores SentencesF1 F2 F5 per SecondViterbi ITG 71.6 73.1 74.0 0.21BP ITG 71.8 74.8 83.5 0.11BP Relaxed 72.6 75.2 84.5 1.15Figure 4: Phrase alignment results.
A portion of the Pre-cision/Recall curve is plotted for the BP models, with theresult from the Viterbi ITG model provided for reference.Klein, 2010).
The BP Relaxed model performs thebest of all, consistently achieving higher recall forfixed precision than either of the other models.
Be-cause of its lower asymptotic runtime, it is also muchfaster: over 5 times as fast as the Viterbi ITG modeland over 10 times as fast as the BP ITG model.117.2 TimingBP approximates marginal posteriors by iterativelyupdating beliefs for each variable based on cur-rent beliefs about other variables.
The iterative na-ture of the algorithm permits us to make an explicitspeed/accuracy tradeoff by limiting the number ofiterations.
We tested this tradeoff by limiting bothof the BP models to run for 2, 3, 5, 10, and 20 iter-ations.
The results are shown in Figure 5.
Neithermodel benefits from running more iterations thanused to obtain the results in Figure 4, but each canbe sped up by a factor of almost 1.5x in exchangefor a modest (< 1 F1) drop in accuracy.11The speed advantage of Viterbi ITG over BP ITG comesfrom Viterbi ITG?s aggressive beaming.Speed F12.08333333 61.32 67.61.58730159 71.91.14942529 72.60.96153846 72.6676869707172730.5 1 2 4 8 16BestF1Time (seconds per sentence)Viterbi ITG BP ITG BP Relaxed676869707172730.0625 0.125 0.25 0.5 1 2BestF1Speed (sentences per second)Viterbi ITG BP ITG BP RelaxedFigure 5: Speed/accuracy tradeoff.
The speed axis is ona logarithmic scale.
From fastest to slowest, data pointscorrespond to maximums of 2, 5, 10, and 20 BP itera-tions.
F1 for the BP Relaxed model was very low whenlimited to 2 iterations, so that data point is outside thevisible area of the graph.Model BLEURelative Hours toImprove.
Train/AlignBaseline 32.8 +0.0 5Viterbi ITG 33.5 +0.7 831BP Relaxed 33.6 +0.8 39Table 2: Machine translation results.7.3 TranslationWe ran translation experiments using Moses (Koehnet al, 2007), which we trained on a 22.1 mil-lion word parallel corpus from the GALE program.We compared alignments generated by the baselineHMM model, the Viterbi ITG model and the Re-laxed BP model.12 The systems were tuned andevaluated on sentences up to length 40 from theNIST MT04 and MT05 test sets.
The results, shownin Table 2, show that the BP Relaxed model achivesa 0.8 BLEU improvement over the HMM baseline,comparable to that of the Viterbi ITG model, but tak-ing a fraction of the time,13 making the BP Relaxedmodel a practical alternative for real translation ap-plications.12Following a simplified version of the procedure describedby DeNero and Klein (2010), we added rule counts from theHMM alignments to the extraction set algners?
counts.13Some of the speed difference between the BP Relaxed andViterbi ITG models comes from better parallelizability due todrastically reduced memory overhead of the BP Relaxed model.368 ConclusionFor performing inference in a state-of-the-art, but in-efficient, alignment model, belief propagation is aviable alternative to greedy search methods, such asbeaming.
BP also results in models that are muchmore scalable, by reducing the asymptotic complex-ity of inference.
Perhaps most importantly, BP per-mits the relaxation of artificial constraints that aregenerally taken for granted as being necessary forefficient inference.
In particular, a relatively mod-est relaxation of the ITG constraint can directly beapplied to any model that uses ITG-based inference(e.g.
Zhang and Gildea, 2005; Cherry and Lin, 2007;Haghighi et al, 2009).AcknowledgementsThis project is funded by an NSF graduate researchfellowship to the first author and by BBN underDARPA contract HR0011-06-C-0022.ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
Going be-yond AER: An extensive analysis of word alignmentsand their impact on MT.
In ACL.Alexandra Birch, Chris Callison-Burch, and Miles Os-borne.
2006.
Constraining the phrase-based, jointprobability statistical translation model.
In AMTA.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In ACL-IJCNLP.Colin Cherry and Dekang Lin.
2007.
Inversion transduc-tion grammar for joint phrasal translation modeling.
InNAACL Workshop on Syntax and Structure in Statisti-cal Translation.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Fabien Cromie`res and Sadao Kurohashi.
2009.
Analignment algorithm using belief propagation and astructure-based distortion model.
In EACL.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In EMNLP-CoNLL.John DeNero and Dan Klein.
2010.
Discriminative mod-eling of extraction sets for machine translation.
InACL.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In NAACL Workshop on StatisticalMachine Translation.John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In EMNLP.John DeNero.
2010.
Personal Communication.John Duchi, Danny Tarlow, Gal Elidan, and DaphneKoller.
2007.
Using combinatorial optimizationwithin max-product belief propagation.
In NIPS 2006.John Duchi, Elad Hazan, and Yoram Singer.
2010.Adaptive subgradient methods for online learning andstochastic optimization.
In COLT.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and trainingof context-rich syntactic translation models.
InCOLING-ACL.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In ACL-IJCNLP.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In ACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In ACLSSST.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In HLT-NAACL.David J.C. MacKay.
2003.
Information theory, infer-ence, and learning algorithms.
Cambridge Univ Press.Daniel Marcu and Daniel Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In EMNLP.Jan Niehues and Stephan Vogel.
2008.
Discriminativeword alignment via alignment matrix modeling.
InACL Workshop on Statistical Machine Translation.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In EMNLP.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005.A discriminative matching approach to word align-ment.
In EMNLP.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Hao Zhang and Daniel Gildea.
2005.
Stochastic lexical-ized inversion transduction grammar for alignment.
InACL.37Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InACL:HLT.38
