Coling 2010: Poster Volume, pages 171?179,Beijing, August 2010Exploiting Paraphrases and Deferred Sense Commitmentto Interpret Questions more ReliablyPeter Clark and Phil HarrisonBoeing Research & Technology{peter.e.clark,philip.harrison}@boeing.comAbstractCreating correct, semantic representa-tions of questions is essential for appli-cations that can use formal reasoning toanswer them.
However, even within arestricted domain, it is hard to anticipateall the possible ways that a questionmight be phrased, and engineer reliableprocessing modules to produce a correctsemantic interpretation for the reasoner.In our work on posing questions to a bi-ology knowledge base, we address thisbrittleness in two ways: First, we exploitthe DIRT paraphrase database to intro-duce alternative phrasings of a question;Second, we defer word sense and se-mantic role commitment until questionanswering.
Resulting ambiguities arethen resolved by interleaving additionalinterpretation with question-answering,allowing the combinatorics of alterna-tives to be controlled and domainknowledge to guide paraphrase andsense selection.
Our evaluation suggeststhat the resulting system is able to un-derstand exam-style questions more re-liably.1 IntroductionOur goal is to allow users to pose exam-stylequestions to a biology knowledge base (KB),containing formal representations of biologicalstructures and processes expressed in first-orderlogic.
As the questions typically require auto-mated reasoning to answer them, a semanticinterpretation of each question is needed.
In ourearlier work (Clark et al 2007), questions wereinterpreted using a conventional pipeline (parse,coreference, sense and role disambiguation).However, despite moderate performance, theoriginal ("base") system suffered from well-known problems of brittleness, arising fromboth premature commitments in the pipeline andthe system's limited knowledge of the multipleways that questions can be expressed.
In thispaper, we describe how deferred commitmentand a large paraphrase database can be used toreduce these problems, drawing on prior workand applying it in the context of a large KB be-ing available.
In particular, by interleaving in-terpretation and answering, we are able to con-trol the combinatorics of alternatives that wouldotherwise arise.
An evaluation suggests that thisimproves the ability of the system to correctlyinterpret, and hence answer, questions.2 Context and Related WorkOur system aims to interpret and answer high-school level, exam-style biology questions, ex-pressed in sentence form.
Our source of answersis a formal knowledge-base and reasoning en-gine (rather than a text corpus), placing specificrequirements on the interpretation process - inparticular, a full semantic interpretation of thequestion is required.
Questions are typically oneor two sentences long, for example:(1) Does a prokaryotic cell contain ribosomes?
(2) A eukaryotic cell has a nucleus.
Does thatnucleus contain RRNA?
(3) Is adenine found in RNA molecules?
(4) Does a prokaryotic cell have a region con-sisting of cytosol?
(5) Do ribosomes synthesize proteins in the cy-toplasm?
(6) What is the material, containing DNA andprotein, that forms into chromosomes dur-ing mitosis?171Interpreting and answering this style of questionhas a long history in NLP, both for answersfound via database retrieval and formal reason-ing, and for answers extracted from a large textcorpus.For answers found using reasoning, the focus ofthis paper, early NL systems typically used apipelined architecture for question interpretation(e.g., Bobrow, 1964; Woods 1977), with latersystems also using semantic constraints to guidedisambiguation decisions (e.g., Novak, 1977).More recently, as well as there being significantimprovements in the performance of typicalpipeline modules, e.g., word sense disambigua-tion (Navigli, 2009), there has been substantialwork on various forms of deferred commitment,underspecification, and paraphrasing to expandthe space of interpretations considered, and thusimprove interpretation.
Underspecified repre-sentations (e.g., van Deemter and Peters, 1996;Pinkal, 1999) allow ambiguity (in particularscope ambiguity) to be preserved in a singlestructure and commitments deferred until later,allowing multiple interpretations to be carriedthrough the system.
Similarly, a system can de-fer commitment by simply carrying multiple,alternative interpretations forward as individualstructures, or packed together into a singlestructure (e.g., Alshawi and van Eijck, 1989,Bobrow et al, 2005; Kim et al, 2010a,b).
Fi-nally, canonicalized representations are oftenused to represent (and hence carry through thesystem) multiple, equivalent surface forms as asingle structure, e.g., normalizing active andpassive forms, or alternative forms of nounmodification  (Rinaldi et al, 2003).
All thesetechniques help avoid premature commitment ininterpretation.As well as avoiding early rejection of interpreta-tions in these ways, there has been substantial,recent work on expanding the space of possibleinterpretations considered through the use ofparaphases (e.g., Sekine and Inui, 2007).
Para-phrasing is based on the observation that thereare many ways of saying (roughly) the samething, and that syntatic manipulation alone isnot sufficient to enumerate them all.
Para-phrases aim to enumerate these additional alter-natives, and may be generated synthetically(e.g., Rinaldi et al, 2003), drawn from similartexts (e.g., from similar questions for QA,Harabagiu et al, 2000), or mined from a corpususing machine learning techniques (e.g., Linand Pantel, 2001).
They have proved to be par-ticularly useful in the context of textual entail-ment (e.g., Bentivogli et al, 2009), and in cor-pus-based question answering (e.g., Harabagiuet al, 2003).Our work builds on this prior work, applyingand extending these ideas to the context where aformal knowledge base and reasoning engine isavailable.
In particular, we interleave the proc-ess of expanding the space of interpretationsconsidered (using paraphrases and deferredcommitment) with the process of question an-swering (which narrows down that space byselecting interpretations supported by the KB),thus controlling the otherwise combinatorialexplosion of alternatives.
This makes it feasibleto use the DIRT paraphrase database (12 millionparaphrases) for generating a full semantic in-terpretation of the original question, extendingits previous use in the semi-formal context oftextual entailment (Bentivogli et al, 2009).
Ouruse of reasoning to guide disambiguation fol-lows Hobbs et al (1993) method of "interpreta-tion as abduction", where the system searches aspace of possible interpretations for one(s) thatare provable from the KB, preferring those in-terpretations.3 The ProblemAlthough the biology KB we are using con-tains the knowledge to answer the six earlierquestions (1)-(6), only the first two are correctlyanswered with the original pipelined (?base?)system.
For question (3):(3) Is adenine found in RNA molecules?the system (mis-)interprets this as referring tosome actual ?finding?
event, not recognizingthat this is an alternative way of phrasing aquestion about physical structure.
Similarly, thenotion of "consisting of" in question (4) is anunexpected phrasing that the system does notunderstand.
Questions (5) and (6) are also an-swered incorrectly by the base system due toerrors in semantic role labeling during interpre-tation.
In (5):(5) Do ribosomes synthesize proteins in the cy-toplasm?172"in" is (mis-)interpreted by the language inter-preter as an is-inside(x,y) relation, while the KBitself represents this relationship as site(x,y),hence the system fails to produce the correctanswer (yes).
Similarly, for (6) "into" is(mis)interpreted as destination(x,y) but repre-sented in the KB as result(x,y).Clearly, one can tweak the original interpreterto overcome these particular problems.
How-ever, it is a slow, expensive process, and in gen-eral it is impossible to anticipate all such prob-lems up front.
Statistical methods (e.g., Man-ning and Schutze, 1999) offer an alternativeapproach but one that is similarly noisy, prob-lematic for question-answering applications.4 Solution ApproachThe brittleness of the base system can be par-tially attributed to its eager commitments,ahead of specifics that might be discovered dur-ing question-answering itself.
To address this,we have modified the system in two ways.
First,we have added use of paraphrases to exploreadditional interpretations of the question duringquestion-answering.
Second, we defer sense andsemantic role disambiguation until question an-swering.
As a result, part of interpretation oc-curs during answering itself: multiple interpreta-tions are tried and a commitment is made to theone(s) that produce a non-null answer.
The jus-tification for this commitment is a benevolentuser assumption, namely that the interpretationthat ?makes sense?
with respect to the KB (i.e.,produces a non-null answer) is the one that theuser intended.This use of question-answering to drive dis-ambiguation follows Hobbs et al.
(1993) workon Interpretation as Abduction.
In that frame-work, a system searches for an interpretationthat is provable from the KB plus a minimalcost set of assumptions, the interpretation corre-sponding to a particular way to disambiguatethe text.
In our work we do a similar thing, al-though restrict the assumptions to disambigua-tion decisions and exclude assuming newknowledge, as we are dealing with questionsrather than assertions (if no interpretations areprovable, then we treat the answer as "no"rather than treating the unproven query as some-thing that should be asserted as true).4.1 ParaphrasesSeveral paraphrase databases are now availableto the NLP community1, typically built by auto-matically finding phrases that occur in distribu-tionally similar contexts (e.g., Dras et al 2005).To date, paraphrase databases have primarilybeen exploited for recognizing textual entail-ment (e.g., Bentivogli et al, 2009, Clark et al2009), and for corpus-based question answering(e.g., Harabagiu et al, 2003).
Here we use themfor generating a full semantic interpretation inthe context of querying a formal knowledge re-source.We use the DIRT paraphrase database (Linand Pantel, 2001), containing approximately 12million automatically learned rules of the form:IF X relation Y THEN X relation' Ywhere relation is a path in the dependency treebetween constitutents X and Y, or equivalently(as we use later) a chain of literals:{p0(x0,x1), w1(x1), ?pn-1(x n-1,xn)}where pi is the syntactic relation between (non-prepositional) constituents xi and xi+1, and wi isthe word used for xi.
An example from DIRT is:IF X is found in Y THEN X is inside YThe condition ?X is found in Y?
can be ex-pressed as the chain of literals:{ object-of(x,f), "find"(f), "in"(f,y) }The database itself is noisy, containing bothgood and nonsensical paraphrases.
Interestingly,their use in question-answering tends to filterout most bad paraphrases, as it is rare that anonsensical paraphrases will by chance producean answer (i.e., the question + KB together help"triangulate" on good paraphrases).
Neverthe-less, bad paraphrases can sometimes produceincorrect answers.
To handle this in a practicalsetting, we are adding an interactive interface(outside the scope of this paper) that shows theuser any paraphrases used, and allows him/herto verify/block them as desired.4.2 Deferred Sense CommitmentA second, common cause of failure of the basesystem was incorrect assignment of senses and1 e.g., http://www.aclweb.org/aclwiki/index.php?title=RTE_Knowledge_Resources173semantic relations during word sense disam-biguation (WSD) and semantic role labeling(SRL).
While domain-specific terms are gener-ally reliably disambiguated, disambiguation ofgeneral terms (e.g., whether "split" denotes theconcept of Separate or Divide) and semanticroles (e.g., whether "into" denotes destina-tion(x,y) or result(x,y)) is less reliable, withonly limited improvement attainable throughmanual engineering or machine learning.
Theproblem is compounded by a degree of subjec-tivity in the way knowledge is encoded in theKB, for example whether the KB engineerchose to conceptualize a  biological object asthe "agent" or "instrument" or "site" of an activ-ity is to a degree a matter of viewpoint.To overcome this, we defer WSD and SRLcommitments until question-answering itself.One can view this as a trivial form of preservingunderspecification (eg.
Pinkal, 1999) in the ini-tial language processing, where the words them-selves denote their possible meanings.4.3 Algorithm and ImplementationQuestions are first parsed using a broad cov-erage, phrase structure parser, followed bycoreference resolution, producing an initial"syntactic" logical form, for example:Question: Do mitotic spindles consist of hollowmicrotubules?Logical Form (LF): "mitotic-spindle"(s), "con-sist"(c), "hollow"(h), "microtubule"(m), sub-ject(c,s), "of"(c,m), modifier(m,h).Next, rather than attempting word sense dis-ambiguation (WSD) and semantic role labeling(SRL) as would be done in the base system, thesystem immediately starts work on answeringthe question, even though a complete semanticinterpretation has not yet been produced.
In theprocess of answering, the system explores alter-native word senses, semantic roles, and para-phrases for the particular literals it is workingon (described shortly), and if any are provablefrom the knowledge in the knowledge base thenthose branch(es) of the search are explored fur-ther.
There are two basic steps in this process:(a) setup: create an instance X0 of the objectbeing universally quantified over2  (identi-fied during initial language interpretation)(b) query: for each literal in the LF with atleast one bound variable, iteratively querythe KB to see if some interpretation of thoseliterals are provable i.e., already known.In this example, illustrated in Figure 1, for step(a) the system first creates an instance X0 of amitotic spindle, i.e., asserts the instantiated firstliteral isa(X0,Mitotic-Spindle), and then queriesthe inference engine with the remaining LF lit-erals.
(If there are multiple senses for ?mitoticspindle?, then an instance for each sense is cre-ated, to be explored in parallel).
For step (b), thesystem uses the algorithm as follows:repeatselect a chain Cu of ?syntactic?
literals inthe LF with at least 1 bound variableCu = {p(x,y)} or {w(x)} or{p1(x,z), w(z), p2(z,y)}select some interpretation C of Cu where:C is a possible interpretation of Cuor C'u is a possible paraphrase for Cu andC is a possible interpretation of C'utry prove C[bindings] ?
new-bindingsIf success:replace Cu with Cadd new-bindings to bindingsuntilall clauses are provedwhere:?
A syntactic literal is a literal whose predi-cate is a word or syntactic role (subject, ob-ject, modifier, etc.)
All literals in the initialLF are syntactic literals.?
A chain of literals is a set of syntactic liter-als in the LF of the form {p(x,y)} or {w(x)}or {p1(x,z), w(z), p2(z,y)}, where pi, w arewords or syntactic roles (subject, mod, etc).?
A possible paraphrase is a possible substi-tution of one chain of literals with another,listed in the DIRT paraphrase database.2 If the system can prove the answer for a (new) in-stance X0 of the universally quantified class, then itholds for all instances, i.e., if KB ?
f(X0) ?
g(X0)then KB?
f(X0)?g(X0), hence KB?
?x f(x)?g(x)via the principle of universal generalization (UG).174X0:Mitotic-SpindleX1:CentrosomeX3:Microtubule has-functionhas-parthas-regionis-athas-partX4:Hollow shapeisa(X0,Mitotic-Spindle), isa(X4,Hollow), isa(X3,Microtubule), has-part(X0,X3), shape(X3,X4).
"mitotic-spindle"(s), "consist"(c), "hollow"(h), "microtubule"(m), subject(c,s), "of"(c,m),modifier(m,h).isa(X0,Mitotic-Spindle), "consist"(c), "hollow"(h), "microtubule"(m), subject(c,X0), "of"(c,m),modifier(m,h).isa(X0,Mitotic-Spindle), "hollow"(h), isa(X3,Microtubule), has-part(X0,X3), modifier(X3,h).isa(X0,Mitotic-Spindle), ?part"(p), "hollow"(h), "microtubule"(m), subject(p,X0), "of"(p,X0),modifier(m,h).
(a)(b)(c)(d)(Graphical depiction of) (part of) the representation of Mitotic-Spindle:LF interpretation:?Figure 1: The path found through the search space for an interpretation of the example question.
(a)setup (b) paraphrase substitution (IF X consists of Y THEN Y is part of X) (c) interpretation of{subject-of(X0,p),?part?
(p), ?of?
(p,X0)} as has-part(X0,m), preferred as it is provable from the KB,resulting in m=X3 (d) interpretation of the syntactic modifier(X3,h) relation (from ?hollow micro-tubule?)
as shape(X3,h)  as it is provable from the KB.X2:Spindle-PoleRecognized KnowledgeRecognized Knowledge?
A possible interpretation of the singletonchain of literals {w(x)} is isa(x,class),where class is a possible sense of word w.As there are several points of non-determinismin the algorithm, e.g., which literals to select,which interpretation to explore, it is a searchprocess.
Our current implementation uses most-instantiated-first query ordering plus breadth-first search, although other implementationscould traverse the space in other ways.?
A possible interpretation of a chain of liter-als {p(x,y)} or {p1(x,z),w(z),p2(z,y)} isr(x,y), where r is a semantic relation corre-sponding to syntactic relation p (e.g.,"in"(x,y) ?
is-inside(x,y)) or word w (e.g.,{subject-of(e,h), "have"(h), "of"(h,n)} ?has-part(e,n)).5 EvaluationTo evaluate the system, we measured itsquestion-answering performance on a set of 141true/false biology questions, ablating para-phrases and deferred commitment to measuretheir impact.
The 141 questions were senten-cized versions of the multiple choice options in22 original AP-level exam questions that, in anearlier evaluation (Clark, 2009), users had diffi-culty rephrasing into a form that the system un-derstood.
Each original multiple choice optionwas minimally rewritten as a complete sentence(most multiple choice questions were partial se-Possible word-to-class and word-to-predicatemappings are specified in the KB.Figure 1 illustrates this procedure for the exam-ple sentence.
The procedure iteratively replacessyntactic literals with semantic literals that cor-respond to an interpretation that is provablefrom the KB.
If all the literals are proved, thenthe answer is ?yes?, as there exists an interpreta-tion under which it can be proved from the KB,under the benevolent user assumption that thisis the interpretation that the user intended.175system/actual answers Configuration Accuracy(score = y/y+n/n) y/y n/y y/n n/nNaive(all false) 67% (94) 0 47 0 94Base system 72% (102) 8 41 0 94+ Paraphrases 75% (106) 13 34 1 93+ Deferred commitment 76% (107) 13 34 0 94+ Both (full system) 84% (118) 25 22 1 93Table 1: Performance of different configurations of the system.
The y/y column shows the numberof questions for which the system answered ?yes?
and the correct answer is ?yes?, etc.ntences), while preserving the original Eng-lish phrasing.
For example the original ques-tion:73.
Which of the following best describes theDNA molecule?a.
Two parallel strands of nitrogen basesheld together by hydrogen bondingb.
Two complementary strands of deoxyri-bose and phosphates held together byhydrogen bondingc.
Two antiparallel strands of nucleotidesheld together by hydrogen bondingd.
A single strand of nitrogen bases coiledupon itself by hydrogen bondinge.
A single strand of nucleotides coiled intoa helix.was rewritten as five questions:?
Does a DNA molecule have two parallelstrands of nitrogen bases held together byhydrogen bonding??
Does a DNA molecule have two com-plementary strands of deoxyribose andphosphates held together by hydrogenbonding??
Does a DNA molecule have two antipar-allel strands of nucleotides held togetherby hydrogen bonding??
Does a DNA molecule have a singlestrand of nitrogen bases coiled upon itselfby hydrogen bonding??
Does a DNA molecule have a singlestrand of nucleotides coiled into a helix?Similarly:79.
All of the following organelles are associ-ated with protein synthesis EXCEPT:a. ribosomes; b. Golgi bodies;...; e...was rewritten as five questions:?
Are ribosomes associated with protein syn-thesis??
Are Golgi bodies associated with...etc.For 18 of the original questions, each of the 5options expanded to 1 true/false question.
For3 comparison questions (?Which X is in Ybut not Z??
), each option expanded into 2questions (?Is X in Y??
?Is X in Z??).
Finally1 question involved parallelism (?Which ofthe following A,B,C do X,Y,Z respec-tively??)
which expanded into 21 questions(?Does A do X??
?Does A do Y??
etc.)
afterremoving duplicates.
Of the resulting 141questions, 47 had the "gold" answer of true,94 false.
Of the 47 positives, 4 were out ofscope of the reasoning engine, involvingquestions about possibility rather than truth,for example:?
Can a DNA adenine bond to an RNAuracil?Another 3 were out of scope of the knowl-edge in the KB (2 requiring unrepresentedtemporal knowledge and 1 requiring com-monsense knowledge).
Thus the upper boundon performance, given the particular KB andreasoning engine that we are using, is134/141 (95%).We ran the base system alone, with para-phrasing (only), with deferred commitment(only), and with both.
The results are shownin Table 1.
As can be seen, true negatives(n/y) are a substantially larger challenge thanfalse positives (y/n), as the system answers"no" by defalt if it is unable to prove the factsin the interpreted question from the KB.
Dur-ing interpretation, the base "pipeline" systemcommits to disambiguation decisions at eachstep, and if any commitment is wrong then itwill also get the answer wrong, as reflected176by the only small (8) increase in number cor-rectly answered.Paraphrases allow the system to search foralternative interpretations, adding five morequestions to be answered correctly but alsointroducing one false positive (y/n).
The falsepositive was for the question:Do peroxisomes make proteins?This was (incorrectly) answered "yes" by thesystem as it used a bad DIRT paraphrase (IFX makes Y THEN X is made from Y), se-lected because it led to a provable interpreta-tion (peroxisomes are made (synthesized)from proteins), but not the one the author in-tended.
It is an interesting and perhapssomewhat surprising result that this was theonly false positive, given that the DIRT data-base is noisy (approximately half its para-phrases are questionable or invalid).
The lownumber of false positives appears to be due tothe fact that the vast number of invalid para-phrases produce nonsensical, hence unprov-able and rejected, interpretations.Similarly, deferred commitment (alone) al-lowed five additional questions (different tothose for paraphrasing) to be answered, againas premature word sense and semantic rolelabeling was avoided.
For example, for "...thepolymerase builds a strand...", the pipelineprematurely commits to the strand being theobject of the build, while in the KB it is rep-resented as the result of the build.
Deferredcommitment allows the system to search andfind such alternatives.Finally there were several (7) questions re-quiring both paraphrases and deferred com-mitment to answer.
For example, "Do mito-chondria provide cellular energy?"
was an-swered using both a paraphrase (IF X pro-vides Y THEN X creates Y) and deferredcommitment (mitochondria was correctly in-terpreted as the site of the creation, as repre-sented in the KB, while the pipeline prema-turely committed to agent).Although deferring SRL and WSD com-mitment, the final system still eagerly com-mits to a single syntactic analysis, and insome cases that analysis was wrong (e.g.,wrong PP attachment), causing failure forsome of the 16 in-scope, positive examplesthat the final system failed to answer.
Clearlydeferred commitment can be further extendedto explore alternative syntactic analyses.
Theremaining failures were due to incorrect se-mantic interpretation of the syntactic analysis,primarily due to poor handling of coordina-tion.The median, average, and maximum cputimes per question were 0.7, 4.9, and 20.3seconds respectively.6 Discussion and ConclusionAlthough question interpretation is challeng-ing, we are in the unusual position of havingsubstantial, formal domain (biology) knowl-edge available.
We have illustrated how thisknowledge can be exploited to improve ques-tion understanding by interleaving interpreta-tion and answering together, allowing theDIRT paraphrase database to be feasibly usedand avoiding premature sense commitment.The result is an improved understanding ofthe original biology questions.Our work extends previous work (Section 2)on exploring multiple interpretations and ex-ploiting paraphrases, doing so in the contextof a task involving formal reasoning.
In par-ticular, by interleaving the expansion of pos-sible interpretations with reasoning (that con-tracts those alternatives), a viable system canbe constructed in which the combinatorics arecontrolled.
However, although the systemdefers WSD and SRL commitment, there areother sources of brittleness ?
in particular itscommitment to a single semantic analysis ?that could also benefit from exploration ofalternatives, e.g., by using packed representa-tions (Bobrow et al, 2005).A second limitation of the current approach isthat it assumes the (semantics of the) questionis a generalized subset of information in (orinferrable from) the KB, i.e., questions are"pure queries" about the KB that do not positany new information.
However some ques-tions, in particular hypotheticals ("X is true.Does Y follow?
"), violate this "pure query"assumption by asserting a novel premise (X)that is not in the KB, and hence cannot bedisambiguated by searching for the premiseX.
Although such questions are relatively rare177in biology, they are common in other sciences(e.g., physics).
Handling such questionswould require extension of this approach, egby matching a generalized form of the asser-tion X against the KB to identify how to dis-ambiguate it.
Similarly, if we wished to usethe system to read new knowledge, as op-posed to identify old knowledge, further ex-tensions would be needed, as new knowledgeby definition cannot be proved from the KB.Finally, this work suggests that paraphrasedatabases such as DIRT offer potential forlanguage understanding in the context of pos-ing formal questions to a reasoning system ordatabase, by bridging gaps that would other-wise have to be hand-engineered, extendingtheir previous use in semi-formal settingssuch as textual entailment (Bentivogli et al,2009).
Despite noise, the question plus KBhelp "triangulate" on good paraphrases, andwith a suitable user interface to expose theiruse, this work suggests that there is substan-tial potential for deploying them in a practi-cal, end-user environment.AcknowledgementsWe are grateful to Vulcan Inc., who fundedthis work as part of Project Halo.ReferencesAlshawi H., van Eijck, J.
1989.
Logical Forms inthe Core Language Engine.
Proc ACL, pp25-32.Bentivogli, L., Dagan, I., Dang, Hoa, Giampic-colo, D., Magnini, B.
2009.
The Fifth PASCALRecognizing Textual Entailment Challenge.
InProc Text Analysis Conference (TAC?09).Bobrow, D. 1964.
A Question-Answering Systemfor High School Algebra Word Problems.AFIPS conference proceedings, 16: 591-614.Bobrow, D. G., Condoravdi, Crouch, R. Kaplan,R.
Karttunen, L., King, L.T.H., de Paiva, V.,Zaenen, A.
2005.
A Basic Logic for Textual In-ference.
In Proceedings of the AAAI Workshopon Inference for Textual Question Answering,Pittsburgh, PA.Chierchia, G. 1993.
Questions with Quantifiers.
InNatural Language Semantics 1, 181-234.Clark, P. 2009.
A Study of Some ?Hard to Formu-late?
Biology Questions.
Working Note 33,Boeing Technical Report.Clark, P., Chaw, J., Chaudhri, V., Harrison, P.2007.
Capturing and Answering QuestionsPosed to a Knowledge-Based System.
In Proc.KCap 2007.Clark, P. Harrison, P. 2009.
An inference-basedapproach to textual entailment.
In Proc TAC2009 (Text Analysis conference).Curtis, J., Matthews, G., Baxter, D. 2005.
On theEffective Use of Cyc in a Question-AnsweringSystem.
Proc Workshop on Knowledge andReasoning for Answering Questions, IJCAI?05,pp 61-70.Dras, M., Yamamoto, K. (Eds).
2005.
Proc 3rdInternationl Workshop of Paraphrasing.
SouthKorea.Harabagiu, S., Moldovan, D., Pasca, M., Mihal-cea, R.,et al, 2000.
FALCON: BoostingKnowledge for Answer Engines.
ProcTREC'2000 (9th Text Retrieval Conf), pp 479-488.Hobbs, J. Stickel, M., Appelt, D., Martin, P. 1993.Interpretation as Abduction.
Artificial Intelli-gence 63 (1-2), pp 69-142.Kim, D., Barker, K., Porter, B.
2010a.
Building anEnd-to-End Text Reading System based on aPacked Representation.
Proc NAACL-HLTWorkshop on Machine Reading.Kim, D., Barker, K., Porter, B.
2010b.
Improvingthe Quality of Text Understanding by DelayingAmbiguity Resolution.
Proc COLING 2010.Lin, D. and Pantel, P. 2001.
Discovery of Infer-ence Rules for Question Answering.
NaturalLanguage Engineering 7 (4) pp 343-360.Manning, C., Schutze, H. 1999.
Foundations ofStatistical Natural Language Processing.
MA:MIT Press.Navigli.
R. 2009.
Word Sense Disambiguation: aSurvey.
ACM Computing Surveys, 41(2), ACMPress, pp.
1-69Novak, G. 1977.
Representations of Knowledge ina Program for Solving Physics Problems,IJCAI?77, pp.
286-291Pinkal, M. 1999.
On Semantic Underspecification.In Bunt, H./Muskens, R.
(Eds.).
Proceedings ofthe 2nd International Workshop on Compua-tional Linguistics (IWCS 2).178Rinaldi, F., Dowall, J. et al, 2003.
ExploitingParaphrases in a Question Answering System.In Proc 2003 ACL Workshop on Paraphrasing(IWP 2003).Sekine, S., Inui, K. 2007.
Proc ACL-PASCALWorkshop on Textual Entailment and Para-phrasing.van Deemter, K., Peters, S. 1996.
Semantic Ambi-guity and Underspecification.
CA: CSLI.Woods, W. 1977.
Lunar rocks in natural English:Explorations in natural language question an-swering.
Fundamental Studies in ComputerScience.
A. Zampolli, Ed.
North Holland, 521-569.179
