Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 30?39,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsEnhancing Referential Success by Tracking Hearer GazeAlexander KollerUniversity of Potsdamkoller@ling.uni-potsdam.deKonstantina GaroufiUniversity of Potsdamgaroufi@uni-potsdam.deMaria StaudteSaarland Universitymasta@coli.uni-saarland.deMatthew CrockerSaarland Universitycrocker@coli.uni-saarland.deAbstractThe ability to monitor the communicative suc-cess of its utterances and, if necessary, providefeedback and repair is useful for a dialog sys-tem.
We show that in situated communication,eyetracking can be used to reliably and effi-ciently monitor the hearer?s reference resolu-tion process.
An interactive system that drawson hearer gaze to provide positive or nega-tive feedback after referring to objects outper-forms baseline systems on metrics of referen-tial success and user confusion.1 IntroductionBecause dialog is interactive, interlocutors are con-stantly engaged in a process of predicting and mon-itoring the effects of their utterances.
Typically, aspeaker produces an utterance with a specific com-municative goal in mind?e.g., that the hearer willperform an action or adopt a certain belief?, andchooses one particular utterance because they pre-dict that it will achieve this communicative goal.They will then monitor the hearer?s reactions andinfer from their observations whether the predictionactually came true.
If they recognize that the hearermisunderstood the utterance, they may repair theproblem by diagnosing what caused the misunder-standing and giving the hearer feedback.
In a task-oriented dialog in which the hearer must perform apart of the task, feedback is especially important toinform the hearer when they made a mistake in thetask.
Ideally, the speaker should even detect whenthe hearer is about to make a mistake, and use feed-back to keep them from making the mistake at all.Many implemented dialog systems include a com-ponent for monitoring and repair.
For instance,Traum (1994) presents a model for monitoring thegrounding status of utterances in the TRAINS sys-tem; Young et al (1994) show how the student?sutterances in a dialog system can be used to un-cover mistaken assumptions about their mental state;and Paek and Horvitz (1999) discuss an automatedhelpdesk system that can track grounding under un-certainty.
However, most of these systems rely onthe user?s verbal utterances as their primary sourceof information; monitoring thus presupposes an(error-prone) language understanding module.In the context of situated communication, wherethe speaker and hearer share a physical (or virtual)environment, one type of observation that can poten-tially give us a very direct handle on the hearer?s un-derstanding of an utterance is eye gaze.
Eyetrackingstudies in psycholinguistics have shown that whenlisteners hear a referring expression, they tend torapidly attend to the object in a scene to which theyresolve this expression (Tanenhaus et al, 1995; Al-lopenna et al, 1998).
For utterances that involve ref-erences to objects in the current environment, onecan therefore ask whether eyetracking can be usedto reliably judge the communicative success of theutterance.
This would be of practical interest forimplemented dialog systems once eyetracking be-comes a mainstream technology; and even today, asystem that reliably monitors communicative suc-cess using eyetracking could serve as a testbed forexploring monitoring and repair strategies.In this paper, we present an interactive natural-language generation (NLG) system that uses eye-30tracking to monitor communicative success.
Oursystem gives real-time instructions that are designedto help the user perform a treasure-hunt task in thevirtual 3D environments of the recent Challengeson Generating Instructions in Virtual Environments(GIVE; Koller et al (2010)).
It monitors how theuser resolves referring expressions (REs) by map-ping the user?s gaze to objects in the virtual environ-ment.
The system takes gaze to the intended referentas evidence of successful understanding, and givesthe user positive feedback; by contrast, gaze to otherobjects triggers negative feedback.
Crucially, thisfeedback comes before the user interacts with theobject in the virtual environment, keeping the userfrom making mistakes before they happen.We evaluate our system against one baseline thatgives no feedback, and another that bases its feed-back on monitoring the user?s movements and theirfield of view.
We find that the eyetracking-basedsystem outperforms both on referential success, andthat users interacting with it show significantly fewersigns of confusion about how to complete their task.This demonstrates that eyetracking can serve as areliable source of evidence in monitoring commu-nicative success.
The system is, to our knowledge,the first dialog or NLG system that uses the hearer?sgaze to monitor understanding of REs.Plan of the paper.
The paper is structured as fol-lows.
We first discuss related work in Section 2.
Wethen describe our approach as well as the baselinesin Section 3, set up the evaluation in Section 4 andpresent the results in Section 5.
In Sections 6 and 7we discuss our findings and conclude.2 Related workDialog systems model a process of grounding, inwhich they decide to what extent the user has under-stood the utterance and the communicative goal hasbeen reached.
Observing the user behavior to moni-tor the state of understanding is a key component inthis process.
A full solution may require plan recog-nition or abductive or epistemic reasoning (see e.g.Young et al (1994), Hirst et al (1994)); in practice,many systems use more streamlined (Traum, 1994)or statistical methods (Paek and Horvitz, 1999).Most dialog systems focus on the verbal interactionof the system and user, and the user?s utterances aretherefore the primary source of evidence in the mon-itoring process.
Some incremental dialog systemscan monitor the user?s verbal reactions to the sys-tem?s utterances in real time, and continuously up-date the grounding state while the system utteranceis still in progress (Skantze and Schlangen, 2009;Buss and Schlangen, 2010).In this paper, we focus on the generation side of adialog system?the user is the hearer?and on mon-itoring the user?s extralinguistic reactions, in par-ticular their gaze.
Tanenhaus et al (1995) and Al-lopenna et al (1998) showed that subjects in psy-cholinguistic experiments who hear an RE visuallyattend to the object to which they resolve the RE.The ?visual world?
experimental paradigm exploitsthis by presenting objects on a computer screen andusing an eyetracker to monitor the subject?s gaze.This research uses gaze only as an experimental tooland not as part of an interactive dialog system, andthe visual worlds are usually limited to static 2Dscenes.
Also, such setups cannot account for the re-ciprocal nature of dialog and the consequences thathearer gaze has for the speaker?s monitoring process.In the context of situated dialog systems, previ-ous studies have employed robots and virtual agentsas speakers to explore how and when speaker gazehelps human hearers to ground referring expressions(Foster, 2007).
For instance, Staudte and Crocker(2011) show that an agent can make it easier for the(human) hearer to resolve a system-generated RE bylooking at the intended referent, using head and eyemovements.
Conversely, the performance of a sys-tem for resolving human-produced REs can be im-proved by taking the (human) speaker?s gaze into ac-count (Iida et al, 2011).
Gaze has also been used totrack the general dynamics of a dialog, such as turntaking (Jokinen et al, in press).Here we are interested in monitoring the hearer?sgaze in order to determine whether they have under-stood an RE.
To our knowledge, there has been noresearch on this; in particular, not in dynamic 3Denvironments.
The closest earlier work of which weare aware comes from the context of the GIVE Chal-lenge, a shared task for interactive, situated naturallanguage generation systems.
These systems typi-cally approximate hearer gaze as visibility of objectson the screen and monitor grounding based on this(Denis, 2010; Racca et al, 2011).31Figure 1: A first-person view of a virtual 3D environment.3 Interactive natural-language generationin virtual environmentsIn this paper, we consider the communicative situ-ation of the GIVE Challenge (Koller et al, 2010;Striegnitz et al, 2011).
In this task, a human user canmove about freely in a virtual indoor environmentfeaturing several interconnected rooms and corri-dors.
A 3D view of the environment is displayed ona computer screen as in Fig.
1, and the user can walkforward/backward and turn left/right, using the cur-sor keys.
They can also press buttons attached to thewalls, by clicking on them with the mouse once theyare close enough.
The small and big white circles inFig.
1, which represent eyetracking information, arenot actually visible to the user.The user interacts with a real-time NLG system inthe context of a treasure-hunt game, where their taskis to find a trophy hidden in a wall safe.
They mustpress certain buttons in the correct sequence in or-der to open the safe; however, they do not have priorknowledge of which buttons to press, so they relyon instructions and REs generated by the system.
Aroom may contain several buttons other than the tar-get, which is the button that the user must press next.These other buttons are called distractors.
Next tobuttons, rooms also contain a number of landmarkobjects, such as chairs and plants, which cannot di-rectly be interacted with, but may be used in REsto nearby targets.
Fig.
2 shows a top-down map ofthe virtual environment in which the scene of Fig.
1arose.
We call an entire game up to the successfuldiscovery of the trophy, an interaction of the systemand the user.Figure 2: A map of the environment in Fig.
1; note theuser in the upper right room.3.1 Monitoring communicative successNLG systems in the GIVE setting are in an interac-tive communicative situation.
This situation repre-sents one complete half of a dialog situation: Onlythe system gets to use language, but the user movesand acts in response to the system?s utterances.
As aresult, the system should continuously monitor andreact to what the user does, in real time.
This ismost tangible in the system?s use of REs.
When auser misinterprets (or simply does not understand)a system-generated RE, there is a high chance thatthey will end up pressing the wrong button.
Thiswill hinder the completion of the task.
A systemthat predicts how the user resolves the RE by mon-itoring their movements and actions, and that canproactively give the user feedback to keep them frommaking a mistake, will therefore perform better thanone which cannot do this.
Furthermore, if the sys-tem can give positive feedback when it detects thatthe user is about to do the right thing, this may in-crease the user?s confidence.Monitoring communicative success in GIVE in-teractions and providing the right feedback can bechallenging.
For example, in the original interactionfrom which we took the screenshot of Fig.
1, the sys-tem instructed the user to ?push the right button tothe right of the green button?, referring to the right-most blue button in the scene.
In response, the userfirst walked hesitantly towards the far pair of buttons(green and blue), and then turned to face the otherpair, as seen in Fig.
3.
A typical NLG system used32Figure 3: The scene of Fig.
1, after the user moved andturned in response to a referring expression.in the GIVE Challenge (e.g., Dionne et al (2009),Denis (2010), Racca et al (2011)) may try to predicthow the user might resolve the RE based on the vis-ibility of objects, timing data, or distances.
Relyingonly on such data, however, even a human observercould have difficulties in interpreting the user?s reac-tion; the user in Fig.
3 ended up closer to the greenand blue buttons, but the other buttons (the two blueones) are, to similar degrees, visually in focus.The contribution of this paper is to present amethod for monitoring the communicative successof an RE based on eyetracking.
We start from thehypothesis that when the user resolves an RE to acertain object, they will tend to gaze at this object.In the scene of Fig.
3, the user was indeed lookingat the system?s intended referent, which they laterpressed; the small white circles indicate a trace of re-cent fixations on the screen, and the big white circlemarks the object in the virtual environment to whichthe system resolved these screen positions.
Our sys-tem takes this gaze information, which is available inreal time, as evidence for how the user has resolvedits RE, and generates positive or negative feedbackbased on this.3.2 NLG systemsTo demonstrate the usefulness of the eyetracking-based approach, we implemented and comparedthree different NLG systems.
All of these usean identical module for generating navigation in-structions, which guides the user to a specific lo-cation, as well as object manipulation instructionssuch as ?push the blue button?
; ?the blue button?is an RE that describes an object to the user.
Thesystems generate REs that are optimized for beingeasy for the hearer to understand, according to acorpus-based model of understandability (Garoufiand Koller, 2011).
The model was trained on humaninstructions produced in a subset of the virtual envi-ronments we use in this work.
The resulting systemcomputes referring expressions that are correct anduniquely describe the referent as seen by the hearerat the moment in which generation starts.Unlike in the original GIVE Challenge, the gen-erated instructions are converted to speech by theMary text-to-speech system (Schro?der and Trouvain,2003) and presented via loudspeaker.
At any point,the user may press the ?H?
key on their keyboard toindicate that they are confused and request a clari-fication.
This will cause the system to generate aninstruction newly; if it contains an RE, this RE mayor may not be the same as the one used in the origi-nal utterance.The difference between the three systems is in theway they monitor communicative success and deter-mine when to give feedback to the user.The no-feedback system.
As a baseline system,we used a system which does not monitor successat all, and therefore never gives feedback on its owninitiative.
Notice that the system still re-generates anRE when the user presses the ?H?
key.Movement-based monitoring.
As a second base-line, we implemented a system that attempts to mon-itor whether a user understood an RE based on theirmovements.
This system is intended to representthe user monitoring that can be implemented, witha reasonable amount of effort, on the basis of imme-diately available information in the GIVE setting.The movement-based system gives no feedbackuntil only a single button in the current room is vis-ible to the user, since it can be hard to make a re-liable prediction if the user sees several buttons ontheir screen.
Then it tracks the user?s distance fromthis button, where ?distance?
is a weighted sum ofwalking distance to the button and the angle the usermust turn to face the button.
If, after hearing the RE,the user has decreased the distance by more than agiven threshold, the system concludes that the hearerhas resolved the RE as this button.
If that is the but-ton the system intended to refer to, the system utters33the positive feedback ?yes, that one?.
For incorrectbuttons, it utters the negative feedback ?no, not thatone?.
Although the negative feedback is relativelyvague, it has the advantage of limiting the variabilityof the system?s outputs, which facilitates evaluation.Eyetracking-based monitoring.
Finally, theeyetracking-based system attempts to predictwhether the user will press the correct buttonor not by monitoring their gaze.
At intervals ofapproximately 15 ms, the system determines the(x,y) position on the screen that the user is lookingat.
It then identifies the object in the environmentthat corresponds to this position by casting a rayfrom the (virtual) camera through the screen plane,and picking the closest object lying within a smallrange of this ray (Fig.
1; see Staudte et al (2012) fordetails).
If the user continously looks at the sameobject for more than a certain amount of time, thesystem counts this as an inspection of the object; forour experiments, we chose a threshold of 300 ms.Once the system detects an inspection to a button inthe room, it generates positive or negative feedbackutterances in exactly the same way as the movementsystem does.Both the movement-based and the eyetracking-based model withhold their feedback until a firstfull description of the referent (a first-mention RE)has been spoken.
Additionally, they only providefeedback once for every newly approached or in-spected button and will not repeat this feedback un-less the user has approached or inspected anotherbutton in the meantime.
Example interactions of auser with each of the three systems are presented inAppendix A.4 EvaluationWe set up a human evaluation study in order to as-sess the performance of the eyetracking system ascompared against the two baselines on the situatedinstruction giving task.
For this, we record partic-ipant interactions with the three systems employedin three different virtual environments.
These en-vironments were taken from Gargett et al (2010);they vary as to the visual and spatial properties ofthe objects they contain.
One of these environmentsis shown in Fig.
2.
Overall, 31 participants (12 fe-males) were tested.
All reported their English skillsas fluent, and all were capable of completing thetasks.
Their mean age was 27.6 years.4.1 Task and procedureA faceLAB eyetracking system (http://www.seeingmachines.com/product/facelab)remotely monitored participants?
eye movements ona 24-inch monitor, as in Fig.
4 and 5 of Appendix B.Before the experiment, participants received writteninstructions that described the task and explainedthat they would be given instructions by an NLGsystem.
They were encouraged to request additionalhelp any time they felt that the instructions were notsufficient (by pressing the ?H?
key).The eyetracker was calibrated using a nine-pointfixation stimulus.
We disguised the importance ofgaze from the participants by telling them that wevideotaped them and that the camera needed calibra-tion.
Each participant started with a short practicesession to familiarize themselves with the interfaceand to clarify remaining questions.
We then col-lected three complete interactions, each with a dif-ferent virtual environment and NLG system (alter-nated according to a Latin square design).
Finally,each participant received a questionnaire which wasaimed to reveal whether they noticed that they wereeyetracked and that one of the generation systemsmade use of that, and how satisfied they were withthis interaction.
The entire experiment lasted ap-proximately 30 minutes.4.2 AnalysisFor the assessment of communicative success inthese interactions, we considered as referentialscenes the parts of the interaction between the onsetof a first-mention RE to a given referent and the par-ticipant?s reaction (pressing a button or navigatingaway to another room).
To control for external fac-tors that could have an impact on this, we discardedindividual scenes in which the systems rephrasedtheir first-mention REs (e.g.
by adding further at-tributes), as well as a few scenes which the partic-ipants had to go through a second time due to tech-nical glitches.
To remove errors in eyetracker cali-bration, we included interactions with the eyetrack-ing NLG system in the analysis only when we wereable to record inspections (to the referent or any dis-tractor) in at least 80% of all referential scenes.
This34success success w/out confusion #scenessystem all easy hard all easy hard all easy hardeyetracking 93.4 100.0 90.4 91.9 100.0 88.2 198 62 136with feedback 94.3 100.0 91.7 92.8 100.0 89.4 194 62 132without feedback 50.0 - 50.0 50.0 - 50.0 4 0 4no-feedback 86.6* 100.0?
80.6* 83.5** 98.9?
76.5** 284 88 196movement 89.8?
100.0?
85.2?
87.5?
97.8?
82.8?
295 92 203with feedback 93.9 100.0 90.6 91.9 97.7 88.7 247 88 159without feedback 68.8 100.0 65.9 64.6 100.0 61.4 48 4 44Table 1: Mean referential success rate (%) and number of scenes for the systems, broken down by scene complexityand presence of feedback.
Differences of overall system performances to the eyetracking system are: significant at** p < 0.01, * p < 0.05; ?
not significant.filtered out 9 interactions out of the 93 we collected.Inferential statistics on this data were carried outusing mixed-effect models from the lme4 packagein R (Baayen et al, 2008).
Specifically, we usedlogistic regression for modeling binary data, Poissonregression for count variables and linear regressionfor continuous data.5 ResultsOn evaluating the post-task questionnaires, we didnot find any significant preferences for a particularNLG system.
Roughly the same number of themchose each of the systems on questions such as?which system did you prefer??.
When asked fordifferences between the systems in free-form ques-tions, no participant mentioned the system?s reactionto their eye gaze?though some noticed the (lack of)feedback.
We take this to mean that the participantsdid not realize they were being eyetracked.Below, we report results on objective metrics thatdo not depend on participants?
judgments.5.1 ConfusionA key goal of any RE generation system is thatthe user understands the REs easily.
One measureof the ease of understanding is the frequency withwhich participants pressed the ?H?
key to indicatetheir confusion and ask for help.
The overall averageof ?H?
keystrokes per interaction was 1.14 for theeyetracking-based system, 1.77 for the movement-based system, and 2.26 for the no-feedback system.A model fitted to the keystroke distribution per sys-tem shows significant differences both between theeyetracking and the no-feedback system (Coeff.
=0.703, SE = 0.233, Wald?s Z = 3.012, p < .01) andbetween the eyetracking and the movement-basedsystem (Coeff.
= 0.475, SE = 0.241, Wald?s Z =1.967, p < .05).
In other words, the feedbackgiven by the eyetracking-based system significantlyreduces user confusion.5.2 Referential successAn even more direct way to measure the interac-tion quality is the ratio of generated REs that theparticipants were able to resolve correctly.
In ourevaluation, we looked at two different definitionsof success.
First, an RE can count as success-ful if the first button that the user pressed afterhearing the RE was the system?s intended referent.The results of this evaluation are shown in the left-most part of Table 1, under ?success?.
A logis-tic mixed-effects model fitted to the referential suc-cess data revealed a marginal main effect of sys-tem (?2(2) = 5.55, p = .062).
Pairwise com-parisons further show that the eyetracking systemperforms significantly better than the no-feedbacksystem (Coeff.
= ?0.765, SE = 0.342, Wald?s Z =?2.24, p < .05); no significant difference was foundbetween the eyetracking-based and the movement-based system.Second, we can additionally require that an REonly counts as successful if the user did not pressthe ?H?
key between hearing the first-mention REand pressing the correct button.
This is a stricterversion of referential success, which requires thatthe system recognized cases of potential confusion35and did not force the user to take the initiative incase of difficulties.
It is in line with Dethlefs et al?s(2010) findings that metrics that penalize difficul-ties the user encountered before successfully com-pleting the task are better predictors of user satisfac-tion than ones that only consider the eventual taskcompletion.
Our results on this metric are shownin the middle part of Table 1, under ?success with-out confusion?.
We observe again a main effect ofsystem (?2(2) = 7.78, p < .05); furthermore, theeyetracking system elicited again more correct but-tons than the no-feedback system (Coeff.
= ?0.813,SE = 0.306, Wald?s Z = ?2.66, p < 0.01).To obtain a more detailed view of when and towhat extent the systems?
behavior differed, we dis-tinguished scenes according to their complexity.
Ascene was classified as easy if a) there were no dis-tractors in it, or b) all distractors had different colorsfrom the target, while the system included the colorattribute in its RE.
All other scenes were consideredhard.
Note that ?easy?
and ?hard?
are properties ofthe scene and not of the system, because every sys-tem generated the same REs in each scene.In the experiments, we found essentially no differ-ence between the success rates of different systemson easy scenes (see the ?easy?
columns of Table 1):All systems were almost always successful.
Thedifferences came almost exclusively from the hardscenes, where the eyetracking system performed sig-nificantly better than the no-feedback system (suc-cess: Coeff.
= ?0.793, SE = 0.348, Wald?s Z =?2.28, p < 0.05; success without confusion: Coeff.= ?0.833, SE = 0.315, Wald?s Z = ?2.64, p < 0.01)and, at least numerically, also much better than themovement system.There was a particularly interesting difference inthe feedback behavior of the eyetracking and move-ment systems on hard scenes (see the rightmost partof Table 1, labeled ?#scenes?).
In easy scenes,both systems almost always gave feedback (62/62= 100.0%; 88/92 = 95.6%); but for hard scenes,the ratio of scenes in which the movement systemgave feedback at all dropped to 159/203 = 78.3%,whereas the ratio for the eyetracking system re-mained high.
This may have contributed to the over-all performance difference between the two systems.#actions distance duration idlesystem (norm.)
(norm.)
(norm.)
(sec)eyetracking 1.06 1.22 1.49 256.6no-feedback 1.22* 1.27 1.59 272.5movement 1.16 1.26 1.56 274.4Table 2: Mean values of additional metrics.
Differencesto the eyetracking system are significant at * p < 0.05.5.3 Further performance metricsFinally, we measured a number of other objectivemetrics, including the number of actions (i.e., but-ton presses), the distance the user traveled, the to-tal duration of the interaction, and the mean timea participant spent idle.
Even though these mea-sures only partly provide statistically significant re-sults, they help to draw a clearer picture of how theeyetracking-based feedback affects performance.Because the three virtual environments were ofdifferent complexity, we normalized the number ofactions, distance, and duration by dividing the valuefor a given interaction by the minimum value for allinteractions of the same virtual environment.
The re-sulting measures are shown in Table 2.
Participantsperformed significantly fewer actions in the eye-tracking system than in the no-feedback system (Co-eff.
= 0.174, SE = 0.067, t = 2.57, p(mcmc) < .05);there were also trends that users of the eyetracking-based system traveled the shortest distance, neededthe least overall time, and spent the least time idle.The only measure deviating from this trend ismovement speed, i.e., the speed at which users re-acted to the systems?
instructions to press certainbuttons.
For all successful scenes (without confu-sion), we computed the speed by dividing the GIVEdistance (including turning distance) between thetarget referent and the user?s location at the time ofthe instruction containing the first-mention RE bythe time (in seconds) between hearing the instruc-tion and pressing the target.
The mean movementspeed is 0.518 for the no-feedback system, 0.493 forthe movement system, and 0.472 for the eyetrackingsystem.
A marginal main effect of movement speedconfirms this trend (?2(2) = 5.58, p = .061) andshows that participants moved more slowly whengetting eyetracking-based feedback than when get-ting no feedback at all (Coeff.
= 0.0352, SE =360.0166, t = ?4.97, p(mcmc) < .05).6 DiscussionThe results in Section 5 demonstrate the usefulnessof eyetracking as a foundation for monitoring andfeedback.
Compared to the no-feedback system, theeyetracking-based system achieved a significantlylower confusion rate and a significantly higher REsuccess rate, especially on hard instances.
The dif-ference increases further if we discount scenes inwhich the user had to ask for help, thus forcing thesystem to give feedback anyway.
In other words,eyetracking provides reliable and direct access to thehearer?s reference resolution process.
Real-time di-alog systems can use gaze information to monitorthe success of REs and generate feedback before theuser actually makes a mistake.Monitoring and feedback could also be achievedwithout using eyetracking.
To explore this alterna-tive, we compared eyetracking against a movement-based system.
We found that the former outper-formed the latter on hearer confusion and (at leastnumerically) on referential success, while not per-forming worse on other measures.
This means thatthe improvement comes not merely from the factthat feedback was given; it is also important whenand where feedback is given.
The crucial weaknessof the movement-based system is that it gave feed-back for hard instances much more rarely than theeyetracking system.
Increasing recall by loweringthe system?s confidence threshold would introducefresh errors.
Further improvements must thereforecome at the cost of a more complex monitoring sys-tem, both conceptually and in terms of implementa-tion effort.
From this perspective, eyetracking offersgood performance at low implementation cost.One result that seems to go against the trend is thatusers of the eyetracking system moved significantlymore slowly on their way to a target.
We see twopossible explanations for this.
First, it may be thatusers needed some time to listen to the feedback, orwere encouraged by it to look at more objects.
Asecond explanation is that this is not really a differ-ence in the quality of the systems?
behavior, but adifference in the populations over which the meanspeed was computed: The speed was only averagedover scenes in which the users resolved the RE cor-rectly, and the eyetracking system achieved commu-nicative success in many cases in which the othersdid not?presumably complex scenes in which theuser had to work harder to find the correct button.This issue bears more careful analysis.Finally, the eyetracking-based system could beimproved further in many ways.
On the one hand,it suffers from the fact that all objects in the 3D en-vironment shift on the screen when the user turnsor moves.
The user?s eyes will typically follow theobject they are currently inspecting, but lag behinduntil the screen comes to a stop again.
One topicfor future work would be to remove noise of thiskind from the eyetracker signal.
On the other hand,the negative feedback our system gave (?no, not thatone?)
was quite unspecific.
More specific feedback(?no, the BLUE button?)
might further improve thesystem?s performance.7 ConclusionWe described an interactive NLG system that useseyetracking to monitor the communicative successof the REs it generates.
The communication is sit-uated in a virtual 3D environment in which the usercan move freely, and our system automatically mapseyetracking screen coordinates to objects in the en-vironment.
A task-based evaluation found that theeyetracking-based system outperforms both a no-feedback system and a system whose feedback isbased on the user?s movements in the virtual envi-ronment, along with their field of view.Eyetracking is currently widely available in re-search institutions, which should make our systemeasy to reimplement in other situated domains.
Weanticipate that eyetracking may become mainstreamtechnology in the not-too-distant future.
But evenin a purely research context, we believe that the di-rectness with which eyetracking allows us to observethe hearer?s interpretation process may be useful asa testbed for efficient theories of grounding.Acknowledgments.
This research was partly sup-ported by the Cluster of Excellence ?MultimodalComputing and Interaction?
at Saarland University.We are grateful to Irena Dotcheva for help withdata collection as well as to Alexandre Denis andChristoph Clodo for software support, and to Kristi-ina Jokinen for helpful comments.37Figure 4: A screenshot from the faceLAB software, including visualization of eye-gaze position in 3D space.ReferencesPaul Allopenna, James Magnuson, and Michael Tanen-haus.
1998.
Tracking the Time Course of SpokenWord Recognition Using Eye Movements: Evidencefor Continuous Mapping Models.
Journal of Memoryand Language, 38:419?439.R.H.
Baayen, D.J.
Davidson, and D.M.
Bates.
2008.Mixed-effects modeling with crossed random effectsfor subjects and items.
Journal of Memory and Lan-guage, 59:390?412.Okko Buss and David Schlangen.
2010.
Modellingsub-utterance phenomena in spoken dialogue systems.In Aspects of Semantics and Pragmatics of Dialogue.SemDial 2010, 14th Workshop on the Semantics andPragmatics of Dialogue, pages 33?41.Alexandre Denis.
2010.
Generating referring expres-sions with reference domain theory.
In Proceedingsof the 6th International Natural Language GenerationConference.Nina Dethlefs, Heriberto Cuayahuitl, Kai-FlorianRichter, Elena Andonova, and John Bateman.
2010.Evaluating task success in a dialogue system forindoor navigation.
In Aspects of Semantics and Prag-matics of Dialogue.
SemDial 2010, 14th Workshopon the Semantics and Pragmatics of Dialogue, pages143?146.Daniel Dionne, Salvador de la Puente, Carlos Leo?n,Pablo Gerva?s, and Raquel Herva?s.
2009.
A modelfor human readable instruction generation using level-based discourse planning and dynamic inference of at-tributes.
In Proceedings of the 12th European Work-shop on Natural Language Generation.Mary Ellen Foster.
2007.
Enhancing human-computerinteraction with embodied conversational agents.
InProceedings of HCI International 2007.Andrew Gargett, Konstantina Garoufi, Alexander Koller,and Kristina Striegnitz.
2010.
The GIVE-2 Corpus ofGiving Instructions in Virtual Environments.
In Pro-ceedings of the 7th Conference on International Lan-guage Resources and Evaluation.Konstantina Garoufi and Alexander Koller.
2011.
ThePotsdam NLG systems at the GIVE-2.5 Challenge.
InProceedings of the Generation Challenges Session atthe 13th European Workshop on Natural LanguageGeneration.Graeme Hirst, Susan McRoy, Peter Heeman, Philip Ed-monds, and Diane Horton.
1994.
Repairing conver-sational misunderstandings and non-understandings.Speech Communications, 15:213?229.Ryu Iida, Masaaki Yasuhara, and Takenobu Tokunaga.2011.
Multi-modal reference resolution in situateddialogue by integrating linguistic and extra-linguisticclues.
In Proceedings of 5th International Joint Con-ference on Natural Language Processing.K.
Jokinen, H. Furukawa, M. Nishida, and S. Yamamoto.in press.
Gaze and turn-taking behaviour in casualconversational interactions.
ACM Trans.
InteractiveIntelligent Systems.
Special Issue on Eye Gaze in In-telligent Human-Machine Interaction.Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-tine Cassell, Robert Dale, Johanna Moore, and Jon38Oberlander.
2010.
The First Challenge on GeneratingInstructions in Virtual Environments.
In Emiel Krah-mer and Mariet Theune, editors, Empirical Methods inNatural Language Generation, number 5790 in LNCS,pages 337?361.
Springer.Tim Paek and Eric Horvitz.
1999.
Uncertainty, utility,and misunderstanding: A decision-theoretic perspec-tive on grounding in conversational systems.
In AAAIFall Symposium on Psychological Models of Commu-nication in Collaborative Systems.David Nicola?s Racca, Luciana Benotti, and PabloDuboue.
2011.
The GIVE-2.5 C Generation System.In Proceedings of the Generation Challenges Sessionat the 13th European Workshop on Natural LanguageGeneration.Marc Schro?der and J. Trouvain.
2003.
The GermanText-to-Speech Synthesis System MARY: A Tool forResearch, Development and Teaching.
InternationalJournal of Speech Technology, 6:365?377.Gabriel Skantze and David Schlangen.
2009.
Incre-mental dialogue processing in a micro-domain.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics.Maria Staudte and Matthew W. Crocker.
2011.
Inves-tigating joint attention mechanisms through human-robot interaction.
Cognition, 120(2):268?291.Maria Staudte, Alexander Koller, Konstantina Garoufi,and Matthew W. Crocker.
2012.
Using listener gazeto augment speech generation in a virtual 3D environ-ment.
In Proceedings of the 34th Annual Conferenceof the Cognitive Science Society.
To appear.Kristina Striegnitz, Alexandre Denis, Andrew Gargett,Konstantina Garoufi, Alexander Koller, and MarietTheune.
2011.
Report on the Second Second Chal-lenge on Generating Instructions in Virtual Environ-ments (GIVE-2.5).
In Proceedings of the GenerationChallenges Session at the 13th European Workshop onNatural Language Generation.Michael K. Tanenhaus, Michael J. Spivey-Knowlton,Kathleen M. Eberhard, and Julie C. Sedivy.
1995.
In-tegration of visual and linguistic information in spokenlanguage comprehension.
Science, 268:1632?1634.David Traum.
1994.
A computational theory of ground-ing in natural language conversation.
Ph.D. thesis,University of Rochester.Michael Young, Johanna Moore, and Martha Pollack.1994.
Towards a principled representation for dis-course plans.
In Proceedings of the Sixteenth AnnualMeeting of the Cognitive Science Society.A Example interactionsThe following interactions between a user (U) andeach of the three systems (S) were recorded duringthe systems?
attempts to instruct the user to press therightmost blue button shown in Fig.
1.A.1 Eyetracking system(1) S: Push the right button to the right of the greenbutton.U: (approaches the pair of blue and green but-ton and inspects one of them)S: No, not that one!.
.
.
(U inspects other buttons in the scene, whileS provides appropriate feedback)U: (inspects the correct target)S: Yes, that one!U: (presses the correct button)A.2 Movement system(2) S: Push the right button to the right of the greenbutton.U: (approaches the pair of blue and green but-tons; once the user is very close to the blue but-ton, it happens to become the only button visi-ble on screen)U: (continues moving closer to the blue button)S: No, not that one!U: (has no time to react to the system?s feed-back and presses the wrong blue button)A.3 No-feedback system(3) S: Push the right button to the right of the greenbutton.U: (presses the wrong blue button)B The experimental setupFigure 5: A faceLAB eyetracking system monitored par-ticipants?
eye movements during the interactions.39
