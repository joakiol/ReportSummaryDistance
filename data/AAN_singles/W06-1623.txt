Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 189?198,Sydney, July 2006. c?2006 Association for Computational LinguisticsInducing Temporal GraphsPhilip Bramsen Pawan Deshpande Yoong Keok Lee Regina BarzilayMIT CSAIL MIT CSAIL DSO National Laboratories MIT CSAILbramsen@mit.edu pawan@mit.edu lyoongke@dso.org.sg regina@csail.mit.eduAbstractWe consider the problem of constructinga directed acyclic graph that encodes tem-poral relations found in a text.
The unit ofour analysis is a temporal segment, a frag-ment of text that maintains temporal co-herence.
The strength of our approach liesin its ability to simultaneously optimizepairwise ordering preferences and globalconstraints on the graph topology.
Ourlearning method achieves 83% F-measurein temporal segmentation and 84% accu-racy in inferring temporal relations be-tween two segments.1 IntroductionUnderstanding the temporal flow of discourse isa significant aspect of text comprehension.
Con-sequently, temporal analysis has been a focus oflinguistic research for quite some time.
Tem-poral interpretation encompasses levels rangingfrom the syntactic to the lexico-semantic (Re-ichenbach, 1947; Moens and Steedman, 1987)and includes the characterization of temporal dis-course in terms of rhetorical structure and prag-matic relations (Dowty, 1986; Webber, 1987; Pas-sonneau, 1988; Lascarides and Asher, 1993).Besides its linguistic significance, temporalanalysis has important practical implications.
Inmultidocument summarization, knowledge aboutthe temporal order of events can enhance both thecontent selection and the summary generation pro-cesses (Barzilay et al, 2002).
In question an-swering, temporal analysis is needed to determinewhen a particular event occurs and how events re-late to each other.
Some of these needs can beaddressed by emerging technologies for temporalanalysis (Wilson et al, 2001; Mani et al, 2003;Lapata and Lascarides, 2004; Boguraev and Ando,2005).This paper characterizes the temporal flow ofdiscourse in terms of temporal segments and theirordering.
We define a temporal segment to bea fragment of text that does not exhibit abruptchanges in temporal focus (Webber, 1988).
A seg-ment may contain more than one event or state, butthe key requirement is that its elements maintaintemporal coherence.
For instance, a medical casesummary may contain segments describing a pa-tient?s admission, his previous hospital visit, andthe onset of his original symptoms.
Each of thesesegments corresponds to a different time frame,and is clearly delineated as such in a text.Our ultimate goal is to automatically constructa graph that encodes ordering between temporalsegments.
The key premise is that in a coherentdocument, temporal progression is reflected in awide range of linguistic features and contextualdependencies.
In some cases, clues to segment or-dering are embedded in the segments themselves.For instance, given a pair of adjacent segments,the temporal adverb next day in the second seg-ment is a strong predictor of a precedence relation.In other cases, we can predict the right order be-tween a pair of segments by analyzing their rela-tion to other segments in the text.
The interactionbetween pairwise ordering decisions can easily beformalized in terms of constraints on the graphtopology.
An obvious example of such a con-straint is prohibiting cycles in the ordering graph.We show how these complementary sources of in-formation can be incorporated in a model usingglobal inference.We evaluate our temporal ordering algorithm ona corpus of medical case summaries.
Temporal189analysis in this domain is challenging in several re-spects: a typical summary exhibits no significanttense or aspect variations and contains few abso-lute time markers.
We demonstrate that humanscan reliably mark temporal segments and deter-mine segment ordering in this domain.
Our learn-ing method achieves 83% F-measure in temporalsegmentation and 84% accuracy in inferring tem-poral relations between two segments.Our contributions are twofold:Temporal Segmentation We propose a fullyautomatic, linguistically rich model for temporalsegmentation.
Most work on temporal analysisis done on a finer granularity than proposed here.Our results show that the coarse granularity of ourrepresentation facilitates temporal analysis and isespecially suitable for domains with sparse tempo-ral anchors.Segment Ordering We introduce a new methodfor learning temporal ordering.
In contrast to ex-isting methods that focus on pairwise ordering, weexplore strategies for global temporal inference.The strength of the proposed model lies in its abil-ity to simultaneously optimize pairwise orderingpreferences and global constraints on graph topol-ogy.
While the algorithm has been applied at thesegment level, it can be used with other temporalannotation schemes.2 Related WorkTemporal ordering has been extensively studiedin computational linguistics (Passonneau, 1988;Webber, 1988; Hwang and Schubert, 1992; Las-carides and Asher, 1993; Lascarides and Ober-lander, 1993).
Prior research has investigateda variety of language mechanisms and knowl-edge sources that guide interpretation of tempo-ral ordering, including tense, aspect, temporal ad-verbials, rhetorical relations and pragmatic con-straints.
In recent years, the availability of an-notated corpora, such as TimeBank (Pustejovskyet al, 2003), has triggered the use of machine-learning methods for temporal analysis (Mani etal., 2003; Lapata and Lascarides, 2004; Boguraevand Ando, 2005).
Typical tasks include identifica-tion of temporal anchors, linking events to times,and temporal ordering of events.Since this paper addresses temporal ordering,we focus our discussion on this task.
Existing or-dering approaches vary both in terms of the or-dering unit ?
it can be a clause, a sentence oran event ?
and in terms of the set of orderingrelations considered by the algorithm.
Despitethese differences, most existing methods have thesame basic design: each pair of ordering units (i.e.,clauses) is abstracted into a feature vector and asupervised classifier is employed to learn the map-ping between feature vectors and their labels.
Fea-tures used in classification include aspect, modal-ity, event class, and lexical representation.
It is im-portant to note that the classification for each pairis performed independently and is not guaranteedto yield a globally consistent order.In contrast, our focus is on globally optimaltemporal inference.
While the importance ofglobal constraints has been previously validatedin symbolic systems for temporal analysis (Fikeset al, 2003; Zhou et al, 2005), existing corpus-based approaches operate at the local level.
Theseimprovements achieved by a global model moti-vate its use as an alternative to existing pairwisemethods.3 TDAG: A representation of temporalflowWe view text as a linear sequence of temporalsegments.
Temporal focus is retained within asegment, but radically changes between segments.The length of a segment can range from a singleclause to a sequence of adjacent sentences.
Fig-ure 1 shows a sample of temporal segments froma medical case summary.
Consider as an examplethe segment S13 of this text.
This segment de-scribes an examination of a patient, encompassingseveral events and states (i.e., an abdominal andneurological examination).
All of them belong tothe same time frame, and temporal order betweenthese events is not explicitly outlined in the text.We represent ordering of events as a temporaldirected acyclic graph (TDAG).
An example of thetransitive reduction1 of a TDAG is shown in Fig-ure 1.
Edges in a TDAG capture temporal prece-dence relations between segments.
Because thegraph encodes an order, cycles are prohibited.
Wedo not require the graph to be fully connected ?
ifthe precedence relation between two nodes is notspecified in the text, the corresponding nodes willnot be connected.
For instance, consider the seg-ments S5 and S7 from Figure 1, which describeher previous tests and the history of eczema.
Any1The transitive reduction of a graph is the smallest graphwith the same transitive closure.190S1 S12 S13 S14S2 S10 S6S8S4S3 S5S7S9S11S1 A 32-year-old woman was admitted to the hospital because of left subcostal pain...S2 The patient had been well until four years earlier,S5 Three months before admission an evaluation elsewhere included an ultrasonographic ex-amination, a computed tomographic (CT) scan of the abdomen...S7 She had a history of eczema and of asthma...S8 She had lost 18 kg in weight during the preceding 18 months.S13 On examination the patient was slim and appeared well.
An abdominal examination re-vealed a soft systolic bruit... and a neurologic examination was normal...Figure 1: An example of the transitive reduction of a TDAG for a case summary.
A sample of segmentscorresponding to the nodes marked in bold is shown in the table.order between the two events is consistent with ourinterpretation of the text, therefore we cannot de-termine the precedence relation between the seg-ments S5 and S7.In contrast to many existing temporal represen-tations (Allen, 1984; Pustejovsky et al, 2003),TDAG is a coarse annotation scheme: it does notcapture interval overlap and distinguishes only asubset of commonly used ordering relations.
Ourchoice of this representation, however, is not ar-bitrary.
The selected relations are shown to beuseful in text processing applications (Zhou et al,2005) and can be reliably recognized by humans.Moreover, the distribution of event ordering linksunder a more refined annotation scheme, such asTimeML, shows that our subset of relations cov-ers a majority of annotated links (Pustejovsky etal., 2003).4 Method for Temporal SegmentationOur first goal is to automatically predict shiftsin temporal focus that are indicative of segmentboundaries.
Linguistic studies show that speakersand writers employ a wide range of language de-vices to signal change in temporal discourse (Best-gen and Vonk, 1995).
For instance, the presence ofthe temporal anchor last year indicates the lack oftemporal continuity between the current and theprevious sentence.
However, many of these pre-dictors are heavily context-dependent and, thus,cannot be considered independently.
Instead ofmanually crafting complex rules controlling fea-ture interaction, we opt to learn them from data.We model temporal segmentation as a binaryclassification task.
Given a set of candidate bound-aries (e.g., sentence boundaries), our task is to se-lect a subset of the boundaries that delineate tem-poral segment transitions.
To implement this ap-proach, we first identify a set of potential bound-aries.
Our analysis of the manually-annotated cor-pus reveals that boundaries can occur not only be-tween sentences, but also within a sentence, at theboundary of syntactic clauses.
We automaticallysegment sentences into clauses using a robust sta-tistical parser (Charniak, 2000).
Next, we encodeeach boundary as a vector of features.
Given aset of annotated examples, we train a classifier2 topredict boundaries based on the following featureset:Lexical Features Temporal expressions, suchas tomorrow and earlier, are among the strongestmarkers of temporal discontinuity (Passonneau,1988; Bestgen and Vonk, 1995).
In addition toa well-studied set of domain-independent tempo-ral markers, there are a variety of domain-specifictemporal markers.
For instance, the phrase ini-tial hospital visit functions as a time anchor in themedical domain.To automatically extract these expressions, weprovide a classifier with n-grams from each of thecandidate sentences preceding and following thecandidate segment boundary.Topical Continuity Temporal segmentation isclosely related to topical segmentation (Chafe,1979).
Transitions from one topic to another mayindicate changes in temporal flow and, therefore,2BoosTexter package (Schapire and Singer, 2000).191identifying such transitions is relevant for tempo-ral segmentation.We quantify the strength of a topic changeby computing a cosine similarity between sen-tences bordering the proposed segmentation.
Thismeasure is commonly used in topic segmenta-tion (Hearst, 1994) under the assumption thatchange in lexical distribution corresponds to topi-cal change.Positional Features Some parts of the docu-ment are more likely to exhibit temporal changethan others.
This property is related to patterns indiscourse organization of a document as a whole.For instance, a medical case summary first dis-cusses various developments in the medical his-tory of a patient and then focuses on his currentconditions.
As a result, the first part of the sum-mary contains many short temporal segments.
Weencode positional features by recording the rela-tive position of a sentence in a document.Syntactic Features Because our segmentboundaries are considered at the clausal level,rather than at the sentence level, the syntax sur-rounding a hypothesized boundary may be indica-tive of temporal shifts.
This feature takes into ac-count the position of a word with respect to theboundary.
For each word within three words ofthe hypothesized boundary, we record its part-of-speech tag along with its distance from the bound-ary.
For example, NNP+1 encodes the presenceof a proper noun immediately following the pro-posed boundary.5 Learning to Order SegmentsOur next goal is to automatically construct a graphthat encodes ordering relations between tempo-ral segments.
One possible approach is to castgraph construction as a standard binary classifica-tion task: predict an ordering for each pair of dis-tinct segments based on their attributes alone.
Ifa pair contains a temporal marker, like later, thenaccurate prediction is feasible.
In fact, this methodis commonly used in event ordering (Mani et al,2003; Lapata and Lascarides, 2004; Boguraev andAndo, 2005).
However, many segment pairs lacktemporal markers and other explicit cues for order-ing.
Determining their relation out of context canbe difficult, even for humans.
Moreover, by treat-ing each segment pair in isolation, we cannot guar-antee that all the pairwise assignments are consis-tent with each other and yield a valid TDAG.Rather than ordering each pair separately, ourordering model relies on global inference.
Giventhe pairwise ordering predictions of a local clas-sifier3, our model finds a globally optimal assign-ment.
In essence, the algorithm constructs a graphthat is maximally consistent with individual order-ing preferences of each segment pair and at thesame time satisfies graph-level constraints on theTDAG topology.In Section 5.2, we present three global inferencestrategies that vary in their computational and lin-guistic complexity.
But first we present our under-lying local ordering model.5.1 Learning Pairwise OrderingGiven a pair of segments (i, j), our goal is to as-sign it to one of three classes: forward, backward,and null (not connected).
We generate the train-ing data by using all pairs of segments (i, j) thatbelong to the same document, such that i appearsbefore j in the text.The features we consider for the pairwise order-ing task are similar to ones used in previous re-search on event ordering (Mani et al, 2003; Lapataand Lascarides, 2004; Boguraev and Ando, 2005).Below we briefly summarize these features.Lexical Features This class of features cap-tures temporal markers and other phrases indica-tive of order between two segments.
Represen-tative examples in this category include domain-independent cues like years earlier and domain-specific markers like during next visit.
To automat-ically identify these phrases, we provide a classi-fier with two sets of n-grams extracted from thefirst and the second segments.
The classifier thenlearns phrases with high predictive power.Temporal Anchor Comparison Temporal an-chors are one of the strongest cues for the order-ing of events in text.
For instance, medical casesummaries use phrases like two days before ad-mission and one day before admission to expressrelative order between events.
If the two segmentscontain temporal anchors, we can determine theirordering by comparing the relation between thetwo anchors.
We identified a set of temporal an-chors commonly used in the medical domain anddevised a small set of regular expressions for theircomparison.4 The corresponding feature has three3The perceptron classifier.4We could not use standard tools for extraction and analy-sis of temporal anchors as they were developed on the news-paper corpora, and are not suitable for analysis of medical192values that encode preceding, following and in-compatible relations.Segment Adjacency Feature Multiple studieshave shown that two subsequent sentences arelikely to follow a chronological progression (Best-gen and Vonk, 1995).
To encode this information,we include a binary feature that captures the adja-cency relation between two segments.5.2 Global Inference Strategies for SegmentOrderingGiven the scores (or probabilities) of all pairwiseedges produced by a local classifier, our task isto construct a TDAG.
In this section, we describethree inference strategies that aim to find a con-sistent ordering between all segment pairs.
Thesestrategies vary significantly in terms of linguisticmotivation and computational complexity.
Exam-ples of automatically constructed TDAGs derivedfrom different inference strategies are shown inFigure 2.5.2.1 Greedy Inference in Natural ReadingOrder (NRO)The simplest way to construct a consistentTDAG is by adding segments in the order of theirappearance in a text.
Intuitively speaking, thistechnique processes segments in the same orderas a reader of the text.
The motivation underly-ing this approach is that the reader incrementallybuilds temporal interpretation of a text; when anew piece of information is introduced, the readerknows how to relate it to already processed text.This technique starts with an empty graph andincrementally adds nodes in order of their appear-ance in the text.
When a new node is added, wegreedily select the edge with the highest score thatconnects the new node to the existing graph, with-out violating the consistency of the TDAG.
Next,we expand the graph with its transitive closure.We continue greedily adding edges and applyingtransitive closure until the new node is connectedto all other nodes already in the TDAG.
The pro-cess continues until all the nodes have been addedto the graph.5.2.2 Greedy Best-first Inference (BF)Our second inference strategy is also greedy.
Itaims to optimize the score of the graph.
The scoreof the graph is computed by summing the scores oftext (Wilson et al, 2001).its edges.
While this greedy strategy is not guar-anteed to find the optimal solution, it finds a rea-sonable approximation (Cohen et al, 1999).This method begins by sorting the edges by theirscore.
Starting with an empty graph, we add oneedge at a time, without violating the consistencyconstraints.
As in the previous strategy, at eachstep we expand the graph with its transitive clo-sure.
We continue this process until all the edgeshave been considered.5.2.3 Exact Inference with Integer LinearProgramming (ILP)We can cast the task of constructing a globallyoptimal TDAG as an optimization problem.
Incontrast to the previous approaches, the methodis not greedy.
It computes the optimal solu-tion within the Integer Linear Programming (ILP)framework.For a document with N segments, each pair ofsegments (i, j) can be related in the graph in oneof three ways: forward, backward, and null (notconnected).
Let si?j , si?j , and si=j be the scoresassigned by a local classifier to each of the threerelations respectively.
Let Ii?j , Ii?j , and Ii=jbe indicator variables that are set to 1 if the corre-sponding relation is active, or 0 otherwise.The objective is then to optimize the score of aTDAG by maximizing the sum of the scores of alledges in the graph:maxNXi=1NXj=i+isi?jIi?j + si?jIi?j + si=jIi=j (1)subject to:Ii?j , Ii?j , Ii=j ?
{0, 1} ?
i, j = 1, .
.
.
N, i < j (2)Ii?j + Ii?j + Ii=j = 1 ?
i, j = 1, .
.
.
N, i < j (3)We augment this basic formulation with two moresets of constraints to enforce validity of the con-structed TDAG.Transitivity Constraints The key requirementon the edge assignment is the transitivity of theresulting graph.
Transitivity also guarantees thatthe graph does not have cycles.
We enforce tran-sitivity by introducing the following constraint forevery triple (i, j, k):Ii?j + Ij?k ?
1 ?
Ii?k (4)If both indicator variables on the left side of theinequality are set to 1, then the indicator variable193on the right side must be equal to 1.
Otherwise, theindicator variable on the right can take any value.Connectivity Constraints The connectivityconstraint states that each node i is connected toat least one other node and thereby enforces con-nectivity of the generated TDAG.
We introducethese constraints because manually-constructedTDAGs do not have any disconnected nodes.
Thisobservation is consistent with the intuition that thereader is capable to order a segment with respectto other segments in the TDAG.
(i?1?j=1Ii=j +N?j=i+1Ij=i) < N ?
1 (5)The above constraint rules out edge assignmentsin which node i has null edges to the rest of thenodes.Solving ILP Solving an integer linear programis NP-hard (Cormen et al, 1992).
Fortunately,there exist several strategies for solving ILPs.
Weemploy an efficient Mixed Integer Programmingsolver lp solve5 which implements the Branch-and-Bound algorithm.
It takes less than five sec-onds to decode each document on a 2.8 GHz IntelXeon machine.6 Evaluation Set-UpWe first describe the corpora used in our experi-ments and the results of human agreement on thesegmentation and the ordering tasks.
Then, we in-troduce the evaluation measures that we use to as-sess the performance of our model.6.1 Corpus CharacteristicsWe applied our method for temporal ordering toa corpus of medical case summaries.
The medicaldomain has been a popular testbed for methods forautomatic temporal analyzers (Combi and Shahar,1997; Zhou et al, 2005).
The appeal is partly dueto rich temporal structure of these documents andthe practical need to parse this structure for mean-ingful processing of medical data.We compiled a corpus of medical case sum-maries from the online edition of The New Eng-land Journal of Medicine.6 The summaries arewritten by physicians of Massachusetts General5http://groups.yahoo.com/group/lp_solve6http://content.nejm.orgHospital.
A typical summary describes an admis-sion status, previous diseases related to the cur-rent conditions and their treatments, family his-tory, and the current course of treatment.
Forprivacy protection, names and dates are removedfrom the summaries before publication.The average length of a summary is 47 sen-tences.
The summaries are written in the pasttense, and a typical summary does not include in-stances of the past perfect.
The summaries donot follow a chronological order.
The ordering ofinformation in this domain is guided by stylisticconventions (i.e., symptoms are presented beforetreatment) and the relevance of information to thecurrent conditions (i.e., previous onset of the samedisease is summarized before the description ofother diseases).6.2 Annotating Temporal SegmentationOur approach for temporal segmentation requiresannotated data for supervised training.
We firstconducted a pilot study to assess the human agree-ment on the task.
We employed two annotators tomanually segment a portion of our corpus.
The an-notators were provided with two-page instructionsthat defined the notion of a temporal segment andincluded examples of segmented texts.
Each an-notator segmented eight summaries which on av-erage contained 49 sentences.
Because annotatorswere instructed to consider segmentation bound-aries at the level of a clause, there were 877 po-tential boundaries.
The first annotator created 168boundaries, while the second ?
224 boundaries.We computed a Kappa coefficient of 0.71 indicat-ing a high inter-annotator agreement and therebyconfirming our hypothesis about the reliability oftemporal segmentation.Once we established high inter-annotator agree-ment on the pilot study, one annotator seg-mented the remaining 52 documents in the cor-pus.7 Among 3,297 potential boundaries, 1,178(35.7%) were identified by the annotator as seg-ment boundaries.
The average segment length isthree sentences, and a typical document containsaround 20 segments.6.3 Annotating Temporal OrderingTo assess the inter-annotator agreement, we askedtwo human annotators to construct TDAGs from7It took approximately 20 minutes to segment a case sum-mary.194five manually segmented summaries.
These sum-maries consist of 97 segments, and their transi-tive closure contain a total of 1,331 edges.
Wecomputed the agreement between human judgesby comparing the transitive closure of the TDAGs.The annotators achieved a surprisingly high agree-ment with a Kappa value of 0.98.After verifying human agreement on this task,one of the annotators constructed TDAGs for an-other 25 summaries.8 The transitive reduction ofa graph contains on average 20.9 nodes and 20.5edges.
The corpus consists of 72% forward, 12%backward and 16% null segment edges inclusiveof edges induced by transitive closure.
At theclause level, the distribution is even more skewed?
forward edges account for 74% edges, equal for18%, backward for 3% and null for 5%.6.4 Evaluation MeasuresWe evaluate temporal segmentation by consider-ing the ratio of correctly predicted boundaries.We quantify the performance using F-measure, acommonly used binary classification metric.
Weopt not to use the Pk measure, a standard topicalsegmentation measure, because the temporal seg-ments are short and we are only interested in theidentification of the exact boundaries.Our second evaluation task is concerned withordering manually annotated segments.
In theseexperiments, we compare an automatically gener-ated TDAG against the annotated reference graph.In essence, we compare edge assignment in thetransitive closure of two TDAGs, where each edgecan be classified into one of the three types: for-ward, backward, or null.Our final evaluation is performed at the clausallevel.
In this case, each edge can be classified intoone of the four classes: forward, backward, equal,or null.
Note that the clause-level analysis allowsus to compare TDAGs based on the automaticallyderived segmentation.7 ResultsWe evaluate temporal segmentation using leave-one-out cross-validation on our corpus of 60 sum-maries.
The segmentation algorithm achieves aperformance of 83% F-measure, with a recall of78% and a precision of 89%.8It took approximately one hour to build a TDAG for eachsegmented document.To evaluate segment ordering, we employ leave-one-out cross-validation on 30 annotated TDAGsthat overall contain 13,088 edges in their transi-tive closure.
In addition to the three global in-ference algorithms, we include a majority base-line that classifies all edges as forward, yieldinga chronological order.Our results for ordering the manually annotatedtemporal segments are shown in Table 1.
All infer-ence methods outperform the baseline, and theirperformance is consistent with the complexity ofthe inference mechanism.
As expected, the ILPstrategy, which supports exact global inference,achieves the best performance ?
84.3%.An additional point of comparison is the accu-racy of the pairwise classification, prior to the ap-plication of global inference.
The accuracy of thelocal ordering is 81.6%, which is lower than thatof ILP.
The superior performance of ILP demon-strates that accurate global inference can furtherrefine local predictions.
Surprisingly, the localclassifier yields a higher accuracy than the twoother inference strategies.
Note, however, the localordering procedure is not guaranteed to produce aconsistent TDAG, and thus the local classifier can-not be used on its own to produce a valid TDAG.Table 2 shows the ordering results at the clausallevel.
The four-way classification is computedusing both manually and automatically generatedsegments.
Pairs of clauses that belong to the samesegment stand in the equal relation, otherwise theyhave the same ordering relation as the segments towhich they belong.On the clausal level, the difference between theperformance of ILP and BF is blurred.
When eval-uated on manually-constructed segments, ILP out-performs BF by less than 1%.
This unexpected re-sult can be explained by the skewed distribution ofedge types ?
the two hardest edge types to clas-sify (see Table 3), backward and null, account onlyfor 7.4% of all edges at the clause level.When evaluated on automatically segmentedtext, ILP performs slightly worse than BF.
We hy-pothesize that this result can be explained by thedifference between training and testing conditionsfor the pairwise classifier: the classifier is trainedon manually-computed segments and is tested onautomatically-computed ones, which negativelyaffects the accuracy on the test set.
While allthe strategies are negatively influenced by this dis-crepancy, ILP is particularly vulnerable as it relies195Algorithm AccuracyInteger Linear Programming (ILP) 84.3Best First (BF) 78.3Natural Reading Order (NRO) 74.3Baseline 72.2Table 1: Accuracy for 3-way ordering classifica-tion over manually-constructed segments.Algorithm Manual Seg.
Automatic Seg.ILP 91.9 84.8BF 91.0 85.0NRO 87.8 81.0Baseline 73.6 73.6Table 2: Results for 4-way ordering classificationover clauses, computed over manually and auto-matically generated segments.on the score values for inference.
In contrast, BFonly considers the rank between the scores, whichmay be less affected by noise.We advocate a two-stage approach for temporalanalysis: we first identify segments and then orderthem.
A simpler alternative is to directly performa four-way classification at the clausal level usingthe union of features employed in our two-stageprocess.
The accuracy of this approach, however,is low ?
it achieves only 74%, most likely dueto the sparsity of clause-level representation forfour-way classification.
This result demonstratesthe benefits of a coarse representation and a two-stage approach for temporal analysis.8 ConclusionsThis paper introduces a new method for temporalordering.
The unit of our analysis is a temporalsegment, a fragment of text that maintains tem-poral coherence.
After investigating several infer-ence strategies, we concluded that integer linearprogramming and best first greedy approach arevaluable alternatives for TDAG construction.In the future, we will explore a richer set of con-straints on the topology on the ordering graph.
Wewill build on the existing formal framework (Fikeset al, 2003) for the verification of ordering con-sistency.
We are also interested in expanding ourframework for global inference to other temporalannotation schemes.
Given a richer set of temporalrelations, the benefits from global inference can beeven more significant.Algorithm Forward Backward NullILP 92.5 45.6 76.0BF 91.4 42.2 74.7NRO 87.7 43.6 66.4Table 3: Per class accuracy for clause classifica-tion over manually computed segments.AcknowledgmentsThe authors acknowledge the support of the Na-tional Science Foundation and National Instituteof Health (CAREER grant IIS-0448168, grant IIS-0415865).
Thanks to Terry Koo, Igor Malioutov,Zvika Marx, Benjamin Snyder, Peter Szolovits,Luke Zettlemoyer and the anonymous reviewersfor their helpful comments and suggestions.
Anyopinions, findings, conclusions or recommenda-tions expressed above are those of the authors anddo not necessarily reflect the views of the NSF orNIH.ReferencesJames F. Allen.
1984.
Towards a general theory ofaction and time.
Artificial Intelligence, 23(2):123?154.Regina Barzilay, Noemie Elhadad, and Kathleen McK-eown.
2002.
Inferring strategies for sentence order-ing in multidocument news summarization.
Journalof Artificial Intelligence Research, 17:35?55.Yves Bestgen and Wietske Vonk.
1995.
The roleof temporal segmentation markers in discourse pro-cessing.
Discourse Processes, 19:385?406.Branimir Boguraev and Rie Kubota Ando.
2005.Timeml-compliant text analysis for temporal reason-ing.
In Proceedings of IJCAI, pages 997?1003.Wallace Chafe.
1979.
The flow of thought and theflow of language.
In Talmy Givon, editor, Syntaxand Semantics: Discourse and Syntax, volume 12,pages 159?182.
Academic Press.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the NAACL, pages132?139.William Cohen, Robert Schapire, and Yoram Singer.1999.
Learning to order things.
Journal of ArtificialIntelligence, 10:243?270.Carlo Combi and Yuval Shahar.
1997.
Temporal rea-soning and temporal data maintenance in medicine:Issues and challenges.
Computers in Biology andMedicine, 27(5):353?368.196Thomas H. Cormen, Charles E. Leiserson, andRonald L. Rivest.
1992.
Intoduction to Algorithms.The MIT Press.David R. Dowty.
1986.
The effects of aspectual classon the temporal structure of discourse: Semantics orPragmatics?
Linguistics and Philosophy, 9:37?61.R.
Fikes, J. Jenkins, and G. Frank.
2003.
A systemarchitecture and component library for hybrid rea-soning.
Technical report, Stanford University.Marti Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of the ACL, pages9?16.Chung Hee Hwang and Lenhart K. Schubert.
1992.Tense trees as the ?fine structure?
of discourse.
InProceedings of the ACL, pages 232?240.Mirella Lapata and Alex Lascarides.
2004.
Inferringsentence-internal temporal relations.
In Proceedingsof HLT-NAACL, pages 153?160.Alex Lascarides and Nicholas Asher.
1993.
Tem-poral interpretation, discourse relations, and com-monsense entailment.
Linguistics and Philosophy,16:437?493.Alex Lascarides and John Oberlander.
1993.
Temporalconnectives in a discourse context.
In Proceeding ofthe EACL, pages 260?268.Inderjeet Mani, Barry Schiffman, and Jianping Zhang.2003.
Inferring temporal ordering of events in news.In Proceeding of HLT-NAACL, pages 55?57.Mark Moens and Mark J. Steedman.
1987.
Temporalontology in natural language.
In Proceedings of theACL, pages 1?7.Rebecca J. Passonneau.
1988.
A computational modelof the semantics of tense and aspect.
ComputationalLinguistics, 14(2):44?60.James Pustejovsky, Patrick Hanks, Roser Sauri,Andrew See, David Day, Lissa Ferro, RobertGaizauskas, Marcia Lazo, Andrea Setzer, and BethSundheim.
2003.
The timebank corpus.
CorpusLinguistics, pages 647?656.Hans Reichenbach.
1947.
Elements of Symbolic Logic.Macmillan, New York, NY.Robert E. Schapire and Yoram Singer.
2000.
Boostex-ter: A boosting-based system for text categorization.Machine Learning, 39(2/3):135?168.Bonnie L. Webber.
1987.
The interpretation of tensein discourse.
In Proceedings of the ACL, pages 147?154.Bonnie L. Webber.
1988.
Tense as discourse anaphor.Computational Linguistics, 14(2):61?73.George Wilson, Inderjeet Mani, Beth Sundheim, andLisa Ferro.
2001.
A multilingual approach to anno-tating and extracting temporal information.
In Pro-ceedings of the ACL 2001 Workshop on Temporaland Spatial Information Processing, pages 81?87.Li Zhou, Carol Friedman, Simon Parsons, and GeorgeHripcsak.
2005.
System architecture for temporalinformation extraction, representation and reason-ing in clinical narrative reports.
In Proceedings ofAMIA, pages 869?873.197                 (a) Reference TDAG  fiff ffifl fiffi!"(b) ILP generated TDAG with an accuracy of 84.6%#$ #%#&#'#( #)fi* #ffi+# ,#)#))#)$ #)fi%#ffi-#)"&(b) BF generated TDAG with an accuracy of 71.4%; NRO produces the same graph for this example.S1 A 32-year-old woman was admitted to the hospital because of left subcostal pain.
.
.S2 The patient had been well until four years earlier,S3 when she began to have progressive, constant left subcostal pain, with an intermittent in-crease in the temperature to 39.4?C, anorexia, and nausea.
The episodes occurred approxi-mately every six months and lasted for a week or two;S4 they had recently begun to occur every four months.S5 Three months before admission an evaluation elsewhere included an ultrasonographic ex-amination, a computed tomographic (CT) scan of the abdomen.
.
.S6 Because of worsening pain she came to this hospital.S7 The patient was an unemployed child-care worker.
She had a history of eczema and ofasthma.
.
.S8 She had lost 18 kg in weight during the preceding 18 months.S9 Her only medications were an albuterol inhaler, which was used as needed,S10 and an oral contraceptive, which she had taken during the month before admission.S11 There was no history of jaundice, dark urine, light stools, intravenous drug abuse, hyper-tension, diabetes mellitus, tuberculosis, risk factors for infection with the human immunod-eficiency virus, or a change in bowel habits.
She did not smoke and drank little alcohol.S12 The temperature was 36.5?C, the pulse was 68, and the respirations were 16. .
.S13 On examination the patient was slim and appeared well.
.
.
An abdominal examination re-vealed a soft systolic bruit.
.
.
and a neurologic examination was normal.
.
.S14 A diagnostic procedure was performed.
(d) An example of a case summaryFigure 2: Examples of automatically constructed TDAGs with the reference TDAG and text.198
