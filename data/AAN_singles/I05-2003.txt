A Hybrid Chinese Language Model based on a Combination ofOntology with Statistical MethodDequan Zheng, Tiejun Zhao, Sheng Li and Hao YuMOE-MS Key Laboratory of Natural Language Proceessing and SpeechHarbin Institute of TechnologyHarbin, China, 150001{dqzheng, tjzhao, lisheng, yu}@mtlab.hit.edu.cnAbstractIn this paper, we present a hybrid Chi-nese language model based on a com-bination of ontology with statisticalmethod.
In this study, we determinedthe structure of such a Chinese lan-guage model.
This structure is firstlycomprised of an ontology descriptionframework for Chinese words and arepresentation of Chinese lingual on-tology knowledge.
Subsequently, aChinese lingual ontology knowledgebank is automatically acquired by de-termining, for each word, its co-occurrence with semantic, pragmatics,and syntactic information from thetraining corpus and the usage of Chi-nese words will be gotten from lingualontology knowledge bank for a actualdocument.
To evaluate the performanceof this language model, we completedtwo groups of experiments on texts re-ordering for Chinese information re-trieval and texts similarity computing.Compared with previous works, theproposed method improved the preci-sion of nature language processing.1 IntroductionLanguage modeling is a description of naturallanguage and a good language model can help toimprove the performance of the natural languageprocessing.Traditional statistical language model(SLM) is fundamental to many natural languageapplications like automatic speech recognitionP[1]P,statistical machine translationP[2]P, and informationretrievalP[3]P. Different statistical models havebeen proposed in the past, but n-gram models (inparticular, bi-gram and tri-gram models) stilldominate SLM research.
After that, other ap-proaches were put forward, such as thecombination of statistical-based approach andrule-based approachP[4,5]P, self-adaptive languagemodelsP[6]P, topic-based model P[7]P and cache-basedmodel P[8]P. But when the models are applied, thecrucial disadvantages are that they can?t repre-sent and process the semantic information of anatural language, so they can?t adapt well to theenvironment with changeful topics.Ontology was recognized as a conceptualmodeling tool, which can descript an informa-tion system in the semantic level and knowledgelevel.
After it was first introduced in the field ofArtificial IntelligenceP[9]P, it was closed combinedwith natural language processing and are widelyapplied in many field such as knowledge engi-neering, digital library, information retrieval,semantic Web, and etc.In this paper, combining with the character-istic of ontology and statistical method, we pre-sent a hybrid Chinese language model.
In thisstudy, we determined the structure of Chineselanguage model and evaluate its performancewith two groups of experiments on texts reorder-ing for Chinese information retrieval and textssimilarity computing.The rest of this paper is organized as fol-lows.
In section 2, we describe the Chinese lan-guage model.
In section 3, we evaluate thelanguage model by several experiments aboutnatural language processing.
In section 4, wepresent the conclusion and some future work.2 The language model descriptionTraditional SLM is make use to estimate thelikelihood (or probability) of a word string, in13this study, we determined the structure of Chi-nese language model, first, we gave the ontologydescription framework of Chinese word and therepresentation of Chinese lingual ontologyknowledge, and then, automatically acquired theusage of a word with its co-occurrence of con-text in using semantic, pragmatics, syntactic, etcfrom the corpus to act as Chinese lingual ontol-ogy knowledge bank.
In actual document, theusage of lingual knowledge will be gotten fromlingual ontology knowledge bank.2.1   Ontology description frameworkTraditional ontology mainly emphasizes theinterrelations between essential concept, domainontology is a public concept set of this do-main P[10]P. We make use of this to present Chineselingual ontology knowledge bank.In practical application, ontology can befigured in many waysP[11]P, natural languages,frameworks, semantic webs, logical languages,etc.
Presently, popular models, such as Ontolin-gua, CycL and Loom, are all based on logicallanguage.
Though logical language has a strongexpression, its deduction is very difficult to lin-gual knowledge.
Semantic web and natural lan-guage are non-formal, which have disadvantagesin grammar and expression.For a Chinese word, we provided a frame-work structure that can be understood by com-puter combined with WordNet, HowNet andChinese Thesaurus.
This framework includes aChinese word in concept, part of speech (POS),semantic, synonyms, English translation.
Fig-ure1 shows the ontology description frameworkof a Chinese word.Fig.
1.
Ontology description framework2.2   Lingual ontology knowledge representationA word is the basic factor that composes thenatural language, to acquire lingual ontologyknowledge, we need to know POS, means andsemantic of a word in a sentence.
For example,for a Chinese sentence, the POS, means andSemantic label of ???
in HowNet are shown intable 1.
For the Chinese sentence ???????????
?, after words segmented, POS tag-ging and semantic tagging, we get a characteris-tic string.
They are shown in table 2.Table 1. the usage of ???
in Chinese sentenceChinese Sentence POS Means Semantic Num???
Verb Weave 525(weave|??
)??
?
Verb Buy 348(buy|?
)Table 2.
Segmentation, POS and Semantic taggingItems Results (????
acts as keyword)Chinese sentence ?????????
?Words segmenta-tion??
??
?
??
??
?POS tagging ??
nd/ ??
Keyword/?
vg/ ??
nd/ ?
?vg/ ?wj/Semantic labeltagging??
nd/021243 ??
Keyword/070366?vg/017545 ??
nd/021243 ??
vg/092317 ?wj/-1Characteristic string nd/021243 ??
Keyword/070366  vg/017545nd/021243 vg/092317Explanation ofSemantic label021243 represents ???
?, 070366 represents??
?, 092317 represents ?
??
?, ?-1?
repre-sents not to be defined or exist this semantic inHowNet.In order to use and express easily, we gavea description for ontology knowledge of everyChinese word, which learned from corpus, to beshown as expression 1.
All of them composedthe Chinese lingual ontology knowledge bank.
( ) ( ) ( )????????
== UUnrrrrmllll CLPOSSemCLPOSSemontologyKeyWord11,,,,,,,,Where, KeyWord(ontology) is the ontologydescription of a Chinese word, ( )iii CLPOSSem ,,,  isthe left co-occurrence knowledge of a Chineseword got from its context and ( )iii CLPOSSem ,,,  isthe right co-occurrence knowledge.
Symbol??
?
represents the aggregate of all the co-occurrence with the KeyWord.
( )iii CLPOSSem ,,,  denotes the multi-gramsfrom context of a Chinese word, which is com-posed of semantic information SemBi B, part ofspeech POS Bi B, the position L from the wordKeyWord to its co-occurrence, the average dis-tance lC  from the word to its left (or right) i-thword.
( )( )LPOSSemKeyword ii ,,,  denotes a seman-tic relation pair between the keyword and its co-occurrence in current context.The multi-grams of a Chinese word in con-text, including the co-occurrence and their posi-tion will act as the composition of lingualontology knowledge too.
In figure 2, the charac-teristic string WB1 B, WB2 B, ?, WBi B represents POS andsemantic label, Keyword is keyword itself, l or rKeywords  <?>Concept             <?>Part of Speech  <?>Ontology   Semantic            <?>Synonym           <?>E-translation     <?>14is the position of word that is left or right co-occurrence with keyword.Fig.
2.
Co-occurrence and the position information2.3   Lingual ontology knowledge acquisitionAccording to the course that human being ac-quires and accumulates knowledge, we proposea measurable description for Chinese lingualontology knowledge through automaticallylearning typical corpus.
In this approach, we willacquire the usage of a Chinese word in semantic,pragmatic and syntactic in all documents.
Wecombine with the multi-grams in context includ-ing its co-occurrence, POS, semantic, synonym,position.
In practical application, we will proc-ess every Chinese keyword that has the samegrammar expression, semantic representationand syntactic structure with Chinese lingual on-tology knowledge bank.2.3.1   Algorithm of automatic acquisitionStep 1: corpus pre-processing.For any Chinese document DBi B in the docu-ment set {D}, we treat the sentence that includeskeyword as a processing unit.
First, we have aChinese word segmentation, POS tagging, Se-mantic label tagging based on HowNet, and then,confirm a word to act as the keyword for acquir-ing its co-occurrence knowledge.
We wipe offthe word that can do little contribution to thelingual ontology knowledge, such as preposition,conjunction, auxiliary word and etc.Step 2: Unify the keyword.Making use of the ontology description ofChinese word, we make the synonym into uni-form one.Step 3: Calculate the co-occurrence distance.In our proposal, first, we treat the sentencethat includes keyword as a processing unit andmake POS tagging, semantic label tagging, then,we get Characteristic string.
We take the key-word as the center, define the left and right dis-tance factor B Bl B and B Br B to be shown at formula 1.mlB????????=211211nrB???????
?=211211     (1)Where, m and n represent the left and rightnumber of word that centered with the keyword.In this way, we try to get the language intuition,in a word, if the co-occurrence is nearer to thekeyword, we will get more the co-occurrencedistant.
Final, we respectively get the left-sideand right-side co-occurrence distant from key-word to its co-occurrence to be shown as for-mula 2.lili BC121 ??????
?=  (i=1,?,m)rjrj BC121 ??????
?=  (j=1,?,n)       (2)Step4: Calculate the average co-occurrencedistance.For a keyword, in the current sentence ofdocument DBi,B we regard the keyword and its co-occurrence (SemBi B, POS Bi B, L) as semantic relationpair, and CBjB is their co-occurrence distance.
Wecalculate the average of CBjB that appear in corpusand act as the average co-occurrence distancelCbetween the keyword and its co-occurrence(SemBi B, POS Bi B, L).When all of documents are learned, all ofkeyword and their co-occurrence information ( )iii CLPOSSem ,,,  compose the Chinese lingualontology knowledge bank.Step 5: Rebuild the index.In order to improve the processing speed,for acquired lingual ontology knowledge bank,we first build an index according to Chineseword, and then, we respectively make a sortingaccording to the semantic label SemBi B for everyChinese word.2.3.2 Lingual ontology knowledge applicationIn practical application, we will respectively getdifferent evaluation of a document from the lin-gual ontology knowledge bank.
For the naturallanguage processing, e.g.
documents similaritycomputing, text re-ranking for information re-trieval, information filtering, the general proc-essing is as follow.Step 1: Pre-processing and unify the key-word.The processing is the same as Step 1 andStep 2 in section 2.3.1.Step 2: Fetch the average co-occurrencedistance from lingual ontology knowledge bank.We regard a sentence including keyword indocument D as a processing unit.
First, we makePOS tagging, semantic label tagging and getCharacteristic string, and then, for every key-word, if it has the same semantic relation pair aslingual ontology knowledge bank, i.e.
the key-word and its co-occurrence (SemBi B, POS Bi B, L) inpractical document is the same one as lingual15ontology knowledge bank, we add up all theaverage co-occurrence distancelC  from Chineselingual ontology knowledge bank acquired insection 2.3.1.Step 3: Get the evaluation value of a docu-ment.Repeat Step 2 until all keywords be proc-essed and the accumulation of the average co-occurrence distancelC will act as the evaluationvalue of current document.3 Evaluation of language modelWe completed two groups of experiments ontext re-ranking for information retrieval, textsimilarity computing to verify the performanceof lingual ontology knowledge.3.1   Texts reorderingInformation retrieval is used to retrieve relevantdocuments from a large document set for a userquery, where the user query can be a simple de-scription by natural.
As a general rule, usershope more to acquire relevant information fromthe top ranking documents, so they concernmore on the precision of top ranking documentsthan the recall.We use the Chinese document set CIRB011(132,173 documents) and CIRB020 (249,508documents) from NTCIR3 CLIR dataset andselect 36 topics from 50 search topics (seehttp://research.nii.ac.jp/ntcir-ws3/work-en.htmlfor more information) to evaluate our method.We use the same method to retrieve documentsmentioned by Yang LingpengP[12]P, i.e.
we usevector space model to retrieve documents, usecosine to calculate the similarity between docu-ment and user query.
We respectively use bi-grams and words as indexing unitsP[13,14]P, the av-erage precision of top N ranking documents actsas the normal results.
In this paper, we used aChinese dictionary that contains about 85,000items to segment Chinese document and query.To measure the effectiveness of informa-tion retrieval, we use the same two kinds ofrelevant measures: relax-relevant and rigid-relevantP[14,15]P. A document is rigid-relevant if it?shighly relevant or relevant with user query, anda document is relax-relevant if it is high relevantor relevant or partially relevant with user query.We also use PreAt10 and PreAt100 to representthe precision of top 10 ranking documents andtop 100 ranking documents.3.1.1   Strategy of texts reorderingFirst, we get some keywords to every topic byquery description.
For example,Title: ?????
(The birth of a clonedcalf)Description: ??????????????????????????
(Find Arti-cles relating to the birth of cloned calves usingthe technique called somatic cell nuclear transfer)We extract ??
?, ??
?, ?
?, ?????
as feature word in this topic.Second, acquire lingual ontology knowl-edge every topic by their feature words.
In thisproposal, we arrange 300 Chinese texts of thistopic as learning corpus to get lingual ontologyknowledge bank.Third, get the evaluation value of every textabout this topic, i.e.
respectively add up all theaverage co-occurrence distance lC  to the samesemantic relation pairs in every text from lingualontology knowledge bank.If a text has several keywords, repeat step3to acquire every evaluation value to these key-words, and then, add up each evaluation value toact as the text evaluation value.Final, we reorder the initial retrieval textsaccording to the every text evaluation value ofevery topic.3.1.2   Experimental results and analysisWe calculate the evaluation value of every textin each topic to reorder the initial relevantdocuments.Table 3 lists the normal results and our re-sults based on bi-gram indexing, our results areacquired based on Chinese lingual ontologyknowledge to enhance the effectiveness.PreAt10 is the average precision of 36 topics inprecision of top 10 ranking documents, whilePreAt100 is top 100 ranking documents.Table 4 lists the normal results and our re-sults based on word indexing.
Ratio displays anincrease ratio of our result compared with nor-mal result.Table 3.
Precision (bi-gram as indexing unit)Items Normal Our method RatioPreAt10 (Relax) 0.3704 0.4389 18.49%PreAt100 (Relax) 0.1941 0.2239 15.35%PreA10 (Rigid) 0.2625 0.3083 17.45%PreAt100 (Rigid) 0.1312 0.1478 12.65%16Table 4.
Precision (word as indexing unit)Items Normal Our method RatioPreAt10 (Relax) 0.3829 0.4481 17.03%PreAt100 (Relax) 0.2022 0.2306 14.05%PreAt10 (Rigid) 0.2745 0.3169 15.45%PreAt100 (Rigid) 0.1405 0.1573 11.96%In table 3, it is shown that compared withbi-grams as indexing units, our method respec-tively increases 18.49% in relax relevant meas-ure and 17.45% in rigid in PreAt10.
In PreAt100level, our method respectively increases 15.35%in relax relevant and 12.65% in rigid relevantmeasure.
Figure 3 displays the PreAt10 valuesof each topic in relax relevant measure based onbi-gram indexing where one denotes the preci-sion enhanced with our method, another denotesthe normal precision.
It is shown the precision ofeach topic is all improved by using our method.Fig.
3.
PreAt10 of all topics in relax judgmentIn table 4, using words as indexing units,our method respectively increases 17.03% inrelax relevant measure and 15.45% in rigid inPreAt10.
In PreAt100 level, our method respec-tively increases 14.05% in relax relevant meas-ure and 11.96% in rigid.In our experiments, compared with the twoChinese indexing units: bi-gram and words, ourmethod increases the average precision of allqueries in top 10 and top 100 measure levels forabout 17.1% and 13.5%.
What lies behind ourmethod is that for each topic, we manually selectsome Chinese corpus to acquire the lingual on-tology knowledge, and can help us to focus onrelevant documents.
Our experiment also showsimproper extract and corpus may decrease theprecision of top documents.
So our method de-pends on right keywords in texts, queries and thecorpus.3.2   Text similarity computingText similarity is a measure for the matchingdegree between two or more texts, the more highthe similarity degree is, the more the meaning oftext expressing is closer, vice versa.
Some pro-posal methods include Vector Space Model P[16]P,Ontology-based P[17]P, Distributional Semanticsmodel P[18]P.3.2.1   Strategy of similarity computationFirst, for two Chinese texts DBiB and DBjB, we re-spectively extract k same feature words, if thesame feature words in the two texts is less than k,we don?t compare their similarity.Second, acquire lingual ontology knowl-edge every text by their feature words.Third, get the evaluation value of every text,i.e.
respectively add up all the average co-occurrence distancelC  to the same semanticrelation pairs in two texts.Final, compute the similarity ratio of everytwo text DBi B and DBj B.
The similarity ratio equals tothe ratio of the similarity evaluation value oftext DBi B and DBj B, if the ratio is in the threshold ?,then we think that text DBi B is similar to text DBj B.3.2.2   Experimental results and analysisWe download four classes of text for testingfrom Sina, Yahoo, Sohu and Tom, which in-clude 71 current affairs news, 68 sports news, 69IT news, 74 education news.For the test of current affairs texts, accord-ing to the strategy of similarity computation, wechoose five words as feature word.
They are ??
?, ?
?, ?
?, ?
?, ???.
In the texts, theword ??
?, ???
are all replaced by word ????
and other classes are similar.
The testingresult is shown in table 5.Table 5.
Testing results for text similarity0.95<?<1.05 0.85<?<1.15 ItemsPrecision Recall FB1 B-measure Precision Recall FB1 B-measureCurrent affairs news 97.14% 97.14% 97.14% 94.60% 100% 97.23%Sports News 88.57% 91.18% 89.86% 84.62% 97.06% 90.41%IT news 93.75% 96.77% 95.24% 91.18% 100% 95.39%Education news 94.74% 97.30% 96.00% 90.24 100% 94.87%General results 93.57% 95.62% 94.58% 90.07% 99.27% 94.42%17We analyzed all the experimental results tofind that the results for current affairs texts arethe best, while the sports texts are lower thanothers.
We think it is mainly because somesports terms are unprofessional for the lowersports texts recognition, such as ???
?, ??,???.
Other feature words are more fixed andmore concentrated.4 ConclusionIn this paper, we presented a hybrid Chineselanguage model based on a combination of on-tology with statistical method.
We discuss themodeling and evaluate its performance.
In thetest about texts reordering, our experiences showthat our method can increase the performance ofChinese information retrieval about 17.1% and13.5% at top 10 and top 100 documents measurelevel.
In another test about texts similarity com-puting, F1-measure is above 95%.On the other hand, in the current disposalof our information processing, we only makeuse of some characteristics ontology and usesome co-occurrence information, such as seman-tics, POS, context, position, distance, and etc.For the further research and experiment, we willbe on the following: (1) Research on the charac-teristics of relations between semantics andcombine with some mature natural languageprocessing techniques.
(2) Research traditionalontology representation to keep up with interna-tional stand.
(3) Apply our key techniques toEnglish information retrieval and cross-lingualinformation retrieval systems and study ageneral approach.References1.
Jelinek, F. 1990.
Self-organized language model-ing for speech recognition.
In Readings in SpeechRecognition,A.
Waibel and K. F. Lee, eds.
Mor-gan-Kaufmann, San Mateo, CA,1990, 450-506.2.
Brown, P., Pietra, S. D., Pietra, V. D., and Mercer,R.
1993.
The mathematics of statistical machine-translation: Parameter estimation.
ComputationalLinguistics 19, 2 (1993), 269-311.3.
Croft, W. B. and Lafferty, J.
(EDS.)
2003.
Lan-guage Modeling for Information Retrieval.
KluwerAcademic,Amsterdam.4.
Wang Xiaolong, Wang Kaizhu.
1994.
Speech in-put by sentence, Chinese Journal of Computers,17(2): 96-1035.
Zhou Ming, Huang Changning, Zhang Min, BaiShuanhu, and Wu Sheng.
1994.
A Chinese parsingmodel based on corpus, rules and statistics, Com-puter research and development, 31(2):40-496.
R DeMori, M Federico.
1999.
Language modeladaptation.
In: Keith Pointing ed.
ComputationalModels of Speech Pattern Processing.
NATO ASISeries.
Berlin: Springer Verlag, 102-1117.
R Kuhn , R D Mori.
1990.
A cache-based naturallanguage model for speech reproduction.
IEEETrans on Pattern Analysis and Machine Intelli-gence, PAM2-12(6), 570-5838.
Daniel Gildea, Thomas Hofmannl.
1999.
Topic-based language models using EM1.
In : Proceed-ing of the 6th European Conf on Speech Commu-nication and Technology, Budapest, Hungary:ESCA, 2167-21709.
Neches R., Fikes R., Finin T., Gruber T., Patil R.,Senator T., and Swartout W. R.. 1991.
EnablingTechnology for Knowledge Sharing.
AI Magazine,12(3) :16~3610.
Gruber, T. R. 1993.
Toward principles for thedesign of ontologies used for knowledge sharing.International Workshop on Formal Ontology, Pa-dova, Italy11.
Uschold M. 1996.
Building Ontologies-TowardsA Unified Methodology.
In expert systems 9612.
Yang Lingpeng, Ji Donghong, TangLi.
2004.Document Re-ranking Based on AutomaticallyAcquired Key Terms in Chinese Information Re-trieval.
In Proceedings of the COLING'2004, pp.480-48613.
Kwok, K.L.
1997.
Comparing Representation inChinese Information Retrieval.
In Proceeding ofthe ACM SIGIR-97, pp.
34-414.
Nie, J.Y., Gao, J., Zhang, J., Zhou, M. 2000.
Onthe Use of Words and N-grams for Chinese Infor-mation Retrieval.
In Proceedings of the IRAL-2000, pp.
141-14815.
Robertson, S.E.
and Walker, S. 2001.
MicrosoftCambridge at TREC-9: Filtering track: In Pro-ceeding of the TREC 2000, pages 361-36916.
Salton, G., Buckley, C. Term weighting ap-proaches in automatic text retrieval.
InformationProcessing and Management, 1988, 24(5),pp.513?52317.
Vladimir Oleshchuk, Asle Pedersen.
OntologyBased Semantic Similarity Comparison of Docu-ments, 14th International Workshop on Databaseand Expert Systems Applications, September,2003, pp.735-73818.
Besancon, R., Rajman, M., Chappelier, J. C. Tex-tual similarities based on a distributional approach,Tenth International Workshop on Database andExpert Systems Applications, 1-3 Sept. 1999,pp.180-18418
