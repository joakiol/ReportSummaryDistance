Mining Spoken Dialogue Corpora for System Evaluation andModelingFrederic BechetLIA-CNRSUniversity of Avignon, Francefrederic.bechet@lia.univ-avignon.frGiuseppe RiccardiAT&T LabsFlorham Park, NJ, USAdsp3@research.att.comDilek Hakkani-TurAT&T LabsFlorham Park, NJ, USAdtur@research.att.comAbstractWe are interested in the problem of modelingand evaluating spoken language systems in thecontext of human-machine dialogs.
Spoken di-alog corpora allow for a multidimensional anal-ysis of speech recognition and language under-standing models of dialog systems.
Thereforelanguage models can be directly trained basedeither on the dialog history or its equivalenceclass (or cluster).
In this paper we propose analgorithm to mine dialog traces which exhibitsimilar patterns and are identified by the sameclass.
For this purpose we apply data clusteringmethods to large human-machine spoken dia-logue corpora.
The resulting clusters can beused for system evaluation and language mod-eling.
By clustering dialog traces we expect tolearn about the behavior of the system with re-gards to not only the automation rate but thenature of the interaction (e.g.
easy vs difficultdialogs).
The equivalence classes can also beused in order to automatically adapt the lan-guage model, the understanding module and thedialogue strategy to better fit the kind of in-teraction detected.
This paper investigates dif-ferent ways for encoding dialogues into multi-dimensional structures and different clusteringmethods.
Preliminary results are given for clus-ter interpretation and dynamic model adapta-tion using the clusters obtained.1 IntroductionThe deployment of large scale automatic spokendialog systems, like How May I Help You?SM(HMIHY) (Gorin et al, 1997), makes avail-able large corpora of real human-machine di-alog interactions.
Traditionally, this data isused for supervised system evaluation.
For in-stance, in (Kamm et al, 1999) they propose astatic analysis aimed at measuring the perfor-mance of a dialog system, especially in an at-tempt to automatically estimate user satisfac-tion.
In (Hastie et al, 2002), a dynamic strat-egy in the error handling process is proposed.
Inall these studies, supervised learning techniquesare used in order to classify dialogs to predictuser satisfaction or dialog failures.A novel approach to the exploitation of dialogcorpora is for speech recognition and languageunderstanding modeling.
In fact, such corporaallow for a multidimensional analysis of speechand language models of dialog systems.
Ourwork differs from previous studies in the algo-rithmic approach and learning scenario.First we use unsupervised speech miningtechniques.
We apply data clustering methodsto large spoken dialog corpora.
Two kinds ofclustering methods are used: a hierarchical onebased on decision trees and the optimization ofa statistical criterion; a partitional one basedon a k-means algorithm applied to vectors rep-resenting the dialogs.
We interpret the clustersobtained and define a label for each of them.Second we perform our analyses on large cor-pora of real dialogs collected from deployed sys-tems.
These log files contain a trace of the inter-action between the users and a particular sys-tem at a certain point in time.
Our goal is tohighlight the structures behind these traces.Lastly, we investigate several ways of encod-ing a dialog into a multidimensional structure inorder to apply data clustering methods.
Infor-mation about the system channel and the userchannel are discussed and two ways of encodingare proposed, one for hierarchical clustering andthe other for partitional clustering.The clusters obtained can be used to learnabout the behavior of the system with regardsto the automation rate and the type of inter-action (e.g.
easy vs difficult dialog).
More-over, the clusters can be used on-the-fly to auto-matically adapt the language model, the under-standing module and even the dialog strategyto better fit the kind of interaction detected.In this study, we present two levels of clus-tering: clustering at the utterance level and the?
System: How may I help you??
User: Hello??
Call-type: Hello?
System: Hello, how may I help you??
User: I have a question.?
Call-type: Ask(Info)?
System: OK, What is your question??
User: I would like to know my accountbalance.?
Call-type: Request(Account Balance)?
System: I can help you with that.
Whatis your account number??
User: ...Figure 1: An example dialog from our corpus.dialog level.
Clustering at the utterance is formodeling the language people use in a specificdialog context; clustering at the dialog level al-lows us to characterize the whole interaction be-tween the users and a system.
In the next sec-tion we describe the corpora data structure.
Insection 3 we describe the clustering algorithms.In sections 4 and 5 we report on experimentsand results for utterance-based and dialog clus-tering, respectively.2 Dialog corporaThe corpora is collected from an automatic callrouting system where the aim is to transfer theuser to the right route in a large call center.An example dialog from a customer care ap-plication is given in Figure 1.
After the greet-ing prompt, the speaker?s response is recognizedusing an automatic speech recognizer (ASR).Then, the intent of the speaker is identified fromthe recognized sequence, using a spoken lan-guage understanding (SLU) component.
Thisstep can be framed as a classification problem,where the aim is to classify the intent of the userinto one of the predefined call-types (Gorin etal., 1997).
Then, the user would be engaged in adialog via clarification or confirmation promptsuntil a final route is determined.3 Hierarchical and PartitionalclusteringThe goal of clustering is to reduce the amountof data by categorizing or grouping similar dataitems together.
Clustering methods can be di-vided into two basic types: hierarchical and par-titional clustering.
A lot of different algorithmshave been proposed for both types of clusteringmethods in order to split a data set into clusters.Hierarchical clustering proceeds successivelyby either merging smaller clusters into largerones, or by splitting larger clusters.
The resultof the algorithm is a tree of clusters.
By cuttingthe tree at a desired level a clustering of thedata items into disjoint groups is obtained.
Weuse in this study a decision-tree based clusteringmethod.Partitional clustering, on the other hand, at-tempts to directly decompose the data set intoa set of disjoint clusters.
A criterion function isused in order to estimate the distance betweenthe samples of the different clusters.
By mini-mizing this function between the samples of thesame clusters and maximizing it among the dif-ferent clusters, the clustering algorithm itera-tively finds the best cluster distribution accord-ing to the criteria used.
We use in this study ak-means algorithm applied to vectors encodingthe dialogs.
The number of clusters is fixed inadvance.4 Clustering at the utterance levelPerforming clustering at the utterance level in adialog corpus aims to capture different kinds oflanguage that people would use in a specific di-alog context.
This is a way of grouping togetherturns of dialogs belonging to completely differ-ent requests but sharing some common proper-ties according to their dialog contexts.The immediate application of such a studycan be the training of specialized LanguageModels (LMs) that can be used in replacementof a generic one once a specific situation is de-tected.4.1 Decision-tree based clusteringIn order to obtain utterance clusters from whichwe can build LMs we use a decision tree methodbased on an optimization criterion that has adirect influence on the recognition process: theperplexity measure of the Language Model onthe manually transcribed training corpus.
Wedecide to use this criterion because even if thereis no evidence that a gain in perplexity resultsin a Word Error Rate (WER) reduction, thesetwo quantities are generally related.The clustering algorithm chosen is based on adecision-tree approach inspired by the SemanticClassification Tree method proposed in (Kuhnand Mori, 1995) and already used for corpusclustering in (Esteve et al, 2001) and (Bechetet al, 2003).
One originality of this kind of deci-sion tree is the dynamic generation of questionsduring the growing process of a tree.4.2 Decision tree featuresEach turn of the spoken dialog corpus used forthe clustering process is represented by a mul-tidimensional structure.
Two kinds of channelcan be considered in order to define the features:the system channel (which contains all the infor-mation managed by the system like the promptsor the states of the dialog) and the user chan-nel (which contain the utterances uttered by theuser with all their characteristics: length, vo-cabulary, perplexity, semantic calltypes, etc.
).Because the clusters obtained are going to beused dynamically by training specific LMs oneach of them, we used mostly system channelfeatures in these experiments.
On the HMIHYcorpus we used the following features:?
prompt text: this is the word string ut-tered by the system before each user?s ut-terance;?
prompt category: prompt category ac-cording to the kind of information re-quested (conf if the prompt asks for a con-firmation, numerical if the information re-quested is a numerical value like a phonenumber, other in all the other cases);?
dialog state: a label given by the DialogManager characterizing the current state ofthe dialog;?
dialog history: the string of dialog statelabels given by the Dialog Manager duringthe previous turns of the same dialog?
utterance lengths: the utterance lengthsare estimated on the transcriptions (man-ual or automatic) and represented by a setof discrete symbols l0 for less than 5 words,l1 between 5 and 10 words, l2 between 10and 15 and l3 for more than 15 words);The only feature that does not belong to thesystem channel is the utterance lengths.
Thisfeature is part of the user channel but it canbe estimated rather accurately from the wordgraph produced during the speech recognitionprocess.4.3 Results on the hierarchicalclusteringThis experiment was made on the HMIHY cor-pus.
The training corpus used to grow theclustering-tree comprises about 102k utterancesfrom live customer traffic.
The test corpus wasmade of 7k utterances.After the clustering process we obtained the6 clusters represented in table 1.The size of each cluster is calculated accord-ing to the number of words of all the utterancesbelonging to it.
This number is expressed asa percentage of the total number of words ofthe training corpus (column % words of table1).
One can notice that the cluster sizes arenot homogeneous.
Indeed more than 70% ofthe words of the training corpus are in the samecluster.
This result is not surprising: indeed theopen ended prompts like How may I help you ?represent a very large chunk of all the possibleprompts and moreover most of the answers tothese prompts are quite long with more than 15words.
It is therefore very difficult to split achunk where all the utterances share the samecharacteristics.It is interesting to see the features consideredrelevant in the tree splitting of the training cor-pus.
These 6 clusters contain the following typeof utterances:?
C1: answers to a prompt asking for aphone number and containing between 10and 15 words;?
C2: answers to the confirmation promptAre you phoning from your home phone ?containing between 5 and 10 words;?
C3: answers to the same confirmationprompt containing less than 5 words;?
C4: answers to other prompts and contain-ing between 5 and 10 words;?
C5: answers to other prompts and contain-ing between 10 and 15 words;?
C6: answers to other prompts and contain-ing more than 15 words;As we can see, 3 kinds of interaction are dis-tinguished: request for a phone number, re-quest for confirmation and other.
These interac-tions correspond to the different types of systemprompts defined in section 4.2.It is interesting to notice that first it is al-ways a specific prompt and not the prompt cat-egory numeric, conf or other which is chosen bythe tree, and second that an utterance length issystematically attached to each prompt.
Thismeans that these prompts (which are very fre-quent) have their own behaviors independentlyfrom the other prompts part of the same promptcategory.Utterance lengths are very strong and reli-able indicators for characterizing an answer to agiven prompt.
Cluster 1 contains mostly phonenumbers, cluster 2 contains confirmation an-swers with explanation (mostly negative), andcluster 3 contains confirmation answers withoutexplanation (mostly positive).
We observe thatno dialog state label or dialog history label waschosen as feature by the tree in the clusteringprocess.
One possible explanation is the lim-ited length of the dialogs in the HMIHY corpus(4 turns on average).
Therefore the dialog con-text and history are negligible compared to thesystem prompt alone.Perplexity WER %C % words 1-pass 2-pass 1-pass 2-pass1 1.8 18.6 13.9 11.3 11.12 1.3 5.0 3.2 14.5 12.53 1.2 3.2 1.5 4.4 2.54 4.7 11 7.4 19.2 185 13.8 11.3 9.5 19.7 18.86 73.9 38.4 27.4 30.8 29.8Table 1: Results for each cluster obtained withthe hierarchical clustering method, at the utter-ance level, on the HMIHY corpusBy using these clusters for training specificLMs and by dynamically choosing a specificLM according to the dialog context for perform-ing LM rescoring, we obtain the perplexity andWord-Error-Rate (WER) results of table 1 onthe HMIHY test corpus.
Significant perplex-ity improvement can be seen for all the clus-ters between the first and the second pass.
Onthe whole test corpus, the perplexity drops from25.3 to 18.5, so a relative improvement of 26.8%.On the speech recognition side, even if the de-crease in WER is not as significant, we obtaina gain for all of them.4.4 K-means clusteringIn order to split the utterances into clustersmore equally, we decided to use another cluster-ing method based on a partitional k-means al-gorithm.
In these experiments we do not try toexplicitly optimize a specific criterion, like theperplexity, but we just want to group togetherutterances sharing common properties and pututterances that are very dissimilar into differ-ent clusters.
Perplexity measures obtained withLMs trained on such clusters will tell us if thismethod splits the utterances according to thelanguage used.
The first step in this process isto encode the dialogs into vectors, one vectorfor each dialog.4.5 Representing utterances as featurevectorsThe decision-tree method used symbolic fea-tures in order to generate the questions thatsplit the corpus.
The k-means clusteringmethod is purely numerical therefore we needhere to encode dialog turns into numerical fea-ture vectors.
According to what we learnedfrom the previous experiments we decide to putin the vectors only the semantic calltype labels.Indeed as 70% of the utterances share the sameprompt and the same utterance length category,it did not seem relevant to us to put these fea-tures in the vectors as we are looking for clustersthat are quite balanced in size.
The calltype la-bels are relevant as we are interested here inclusters that can be semantically different.Therefore, a feature vector representing anutterance is a first order statistic on the call-types.
A component value is the number of oc-currences of the corresponding calltype withinthe utterance.
We kept the 34 most frequentcalltype labels in the training corpus in orderto build the vectors.
They cover over 96% ofthe calltype occurrences on the HMIHY train-ing corpus.4.6 Results on the partitionalclusteringThis experiment is made on the same applica-tion than the last one (HMIHY) but on anotherdata set.
The training corpus contains 10k di-alogs and 35.5k utterances and the test corpuscontains 1.4k dialogs and 5k utterances.
Thenumber of clusters is set to 5.
This value hasthe advantages of both providing a good con-vergence to the k-means process and splittingrather equally the training corpus.Table 2 illustrates the partitional clusteringof utterances on this corpus.
As we can see,the distribution of the total amount of wordsof the corpus among the clusters is much moreeven than the one obtained with the hierarchi-cal clustering.
The largest cluster contains only38.5% of the words compared to 73.9% previ-ously.
To check if these clusters can be usefulfor training specific LMs, we first split the testcorpus according to the clustering model esti-mated on the training data.
We use here, tobuild the vectors encoding the test corpus, au-tomatic calltypes calculated by the Spoken Lan-guage Understanding Module.
Then we com-pare the perplexity measures obtained with ageneral LM trained on the whole training cor-pus and the one obtained with a specific LMadapted on the corresponding cluster.
Table 2shows these results: the gain in perplexity ob-tained is smaller than the one obtained with theother method.
Indeed the total perplexity onthe test corpus is 21.3 and the one obtained withthe specific LMs is 17.8, so a gain of 16% com-pared to the 26.8% obtained previously.
How-ever this result is not surprising as the previousmethod was designed for specifically decreasingthe perplexity measure.PerplexityC % words utt.
length 1-pass 2-pass1 8.6 5.5 16.9 12.62 20.3 17.8 25.5 21.03 14.5 11.6 21.6 18.24 38.5 7.5 19.6 17.25 18.1 8.6 22.8 18.6Table 2: Results for each cluster obtained withthe partitional clustering method, at the utter-ance level, on the HMIHY corpus5 Clustering at the dialog levelAs we have seen in the previous section, specificdialog situations (like those obtained with thehierarchical clustering) proved to be more effi-cient than the semantic channel (represented bythe calltype labels) for clustering utterances inrelation with the language used, at least fromthe perplexity point of view.However, for clustering dialogs rather thanutterances, the semantic channel is the mainchannel that we are going to use because in thiscase we want to characterize the whole interac-tion between a system and a user rather thanjust the language used.For example, an utterance like I want to paymy bill can be used in very different dialog con-texts: in a standard interaction if this requestis expressed just after the opening prompt or ata different stage of the dialog.
To capture thiskind of phenomena, we have to cluster at thedialog level rather than the utterance level.After describing how we encode dialogs intofeature vectors in the next section, we presentin section 5.2 some preliminary work on the in-terpretation of the clusters obtained.5.1 Representing dialogs as featurevectorsWe use here the partitional clustering method.Each dialog is represented by a vector contain-ing three kinds of information representing theinteraction:1. the number of turns in the dialog: associ-ated to other features this parameter canbe a strong indicator that the dialog is go-ing fine (associated with a lot of confirma-tions or item values) or that the interactionis difficult (lot of repetitions for example).2. first order statistics data on the calltypelabels: these are the 34 calltypes pre-sented in section 2 and representing bothapplication-specific requests (Pay Bill) anddialog-based concepts like Yes, No, I wantto talk to somebody, Help, etc.
.
.
.3. second order statistics data on the calltypelabels: we chose the bigrams of the previ-ous calltypes that had the highest weightedMutual Information and we store in thevectors their frequencies.
These features al-low us to observe certain patterns like rep-etition of a request, request followed by aconfirmation, people asking twice for a rep-resentative, etc.
.
.
.5.2 Analyzing the clusters obtainedThe experiment reported here is made on theHMIHY corpus.
The vectors used contain 55components: 1 for the number of turns, 34 forthe calltypes and 20 for the bigrams on the call-types.
The number of clusters to be found bythe k-means clustering algorithm was set to 5.Firstly because as we want to give an interpreta-tion to each cluster we need to keep a relativelysmall number of them.
Secondly because thisnumber leads to a fast convergence of the k-mean clustering process.
The clustering modelis obtained on the training corpus and appliedto the test corpus.
Table 3 shows the distribu-tion of the dialogs among the clusters, on thetraining corpus, ranging from 35.4% for C1 to7% for C2.There are two ways of analyzing the clustersobtained: from the language point of view andfrom the semantic point of view.
Table 3 illus-trates the language channel features with theaverage amount of turns (#turn), the averageutterance lengths (utt.
length) and the per-plexity measure (pplex).
This perplexity mea-sure is obtained with a 3-gram LM trained onthe whole training corpus and applied to themanual transcriptions of each cluster, of the testand the training corpus.The differences in utterance lengths are notas big as those observed in section 4.6.
This isan indicator that, unlike the clustering at theutterance level, the clusters obtained representsimilar dialog situation.
It is the way the dia-log progresses, the dialog pattern, rather thanthe theme of the dialog which distinguishes theclusters.
The lengths of the dialogs and the per-plexity measures are indicators of these differentdialog patterns.The results on the test corpus presented inTable 3 are obtained with automatic calltypes,completely unsupervised.
The F-measure scoreon the detection of these calltypes on the testcorpus is 75%.
As one can see, having errors inthe calltypes detected does not affect too muchthe characteristics of the clusters obtained.Training corpusC % dialogs #turn utt.
length pplex1 35.4% 3.9 7.3 7.322 7% 3.2 11.7 9.93 22.3% 3.5 9 7.34 10.4% 3.9 12.4 9.55 25% 2.9 9.3 7.9Test corpusC % dialogs #turn utt.
length pplex1 32.4% 4.0 8.5 17.22 7.4% 3.3 11 30.23 20.8% 3.5 7.5 13.24 12.0% 3.8 11.6 28.25 27.5% 2.8 8.3 17Table 3: Language features attached to thedialog clusters obtained with the partitionalmethod, at the dialog level, on the HMIHY cor-pusIn order to analyze the clusters on the seman-tic channel we plot the weighed Mutual Infor-mation, wMI(ci; tj), between each cluster, ci,and each vector component, tj .
This measure isestimated in the following way:wMI(ci; tj) = P (ci; tj)logP (ci; tj)P (ci)P (tj)This plot is shown on Figure 2 for the HMIHYtraining corpus.
By grouping together com-ponents corresponding to a phenomenon wewant to observe we are able to characterizemore closely each cluster.
In the color-codedgraph, x-axis corresponds to call-type unigramsor selected call-type pairs, y-axis correspondsto each cluster.
The color of each rectangleshows the degree of correlation, determined bythe weighted mutual information between call-type and the cluster.
As also can be seen fromthe color spectrum on the right hand side, darkred means high correlation (top), and dark bluemeans reverse correlation (bottom).
?0.01?0.008?0.006?0.004?0.00200.0020.0040.0060.0080.01Call?typesClustersWeighted MI(Call?type;Cluster)wMI(Call?type;Cluster)5 10 15 20 25 30 35 40 45 5012345Figure 2: Weighted Mutual Information mea-sures between and clusters on the HMIHY train-ing corpus.
The color of each rectangle showsthe degree of correlation, determined by theweighted mutual information between the vec-tor components and the cluster.
As can be seenfrom the color spectrum on the right, dark redmeans high correlation, and dark blue meansreverse correlation.We chose to analyze the clusters according to3 dimensions:1.
Request = the kind of request expressedby the user.
We split all the request call-types into two categories: the easy requeststhat correspond to the calltypes well de-tected by the SLU module and the difficultrequests that contain all the calltypes thatare often badly recognized.2.
Understanding = to try to character-ize the understanding of a user by thesystem we use two features: the bigramsrequest + yes (conf) that can be indica-tors that the request is understood becausethe following concept expressed by the useris yes; the bigrams request + request(repeat) which indicate that the same re-quest is repeated twice in a row, which canindicate that the system misunderstood it.3.
Problems = we grouped in this categorythe features that can be related to someproblems the user have during the interac-tion.
These features are: request for help(help), two requests in a row for a rep-resentative (rep) and a calltype indicat-ing that no meaningful information is ex-tracted from the user?s utterance (null).Request Underst.
ProblemsC easy dif conf repeat help rep null1 + ?
+ = ?
?
?2 ?
+ ?
+ + ?
+3 + ?
= = = + =4 ?
+ + + + + +5 ?
+ + + = = ?Table 4: Interaction features attached to thedialog clusters obtained with the partitionalmethod, at the dialog level, on the HMIHY cor-pusTable 4 represents the 5 clusters from figure 2,ternary encoded according to these semantic di-mensions.
A ?+?
means a higher value in thewMI scale, ???
means a lower value in wMI scaleand ?
=?
stands for 0 in the wMI scale.
Accord-ing to the results presented in Tables 3 and 4,the following description can be given to the 5clusters:?
Cluster C1 : easy requests, expressed clearly(low perplexity, short utterances), under-stood correctly (conf=?+?
), and with noevidence of a problem.?
Cluster C2 : difficult requests, withconfusing formulation (high perplexity,long utterances), often misunderstood(repeat=?+?
), leading users to have prob-lems (request for help, no info).?
Cluster C3 : rather similar to C1 exceptthat people ask more often for a representa-tive, maybe because the requests are not al-ways recognized immediately (conf=?=?).?
Cluster C4 : difficult requests, with confus-ing formulation (high perplexity, long ut-terances), leading users to have problems(request for help, no info), but unlike C2eventually leading to a success (high num-ber of turns and conf=?+?).?
Cluster C5 : difficult requests, expressedclearly (low perplexity, rather short ut-terances), understood correctly (conf=?+?
)even after some repetition because theseare difficult requests, and with no evidenceof a problem.Figure 3 gives portions of two example dialogsfrom clusters C1 and C2.
As can be seen, in theExample dialog from C1:?
System: How may I help you??
User: I want to pay a bill.?
System Call-type: Make(Payment)?
System: Do you want to arrange a billpayment??
User: Yes.?
System Call-type: YesExample dialog from C2:?
System: How may I help you??
User: Yes somebody just called me fromI don?t know it?s collect or something likethat and it had to do with A T and T andwhen...?
System Call-type: Explain(Bill)?
System: Do you have a question about aspecific charge on your bill??
User: No?
System Call-type: No?
System: I?m sorry .
How may I help you??
User: It just said call you guys for helpabout trying to get through to a long dis-tant number and it?s not getting throughwhen I called the long distant ...?
System Call-type: OtherFigure 3: Two example dialogs from our corpus.first dialog, the user?s responses are very clear,whereas in the second one they are very vagueand complicated.6 Using clusters for Speech MiningIn the previous section we have interpreted theclusters obtained with the respech to the dialogcontexts.
Each cluster is identified by variousfeatures computed, characterizing the languageand the interaction.
These features can be seenas a set of tools allowing the designer of a Spo-ken Dialog System to monitor the behavior ofthe system.
Indeed, once a clustering modelhas been build on a manually labeled trainingcorpus, this model can be applied in a fully un-supervised way to non transcribed data and allthe features presented in Figure 2 and Tables 3and 4 can be extracted.
Even if mistakes inthe calltype detection occur, the general struc-ture of the clusters is stable as shown on theHMIHY test corpus in Table 3 and in Figure 4that plot the same parameters for the test cor-pus with automatic calltypes that Figure 2 hasfor the training corpus with manual labels.
?0.01?0.008?0.006?0.004?0.00200.0020.0040.0060.0080.01Call?typesClustersWeighted MI(Call?type;Cluster)wMI(Call?type;Cluster)5 10 15 20 25 30 35 40 45 5012345Figure 4: Weighted Mutual Information mea-sures between calltype labels and clusters on theHMIHY test corpus with automatic calltype la-bels7 ConclusionsWe presented in this paper an applicationof data clustering methods to large Human-Computer spoken dialog corpora.
Differentways for encoding dialogs into multidimensionalstructures (symbolic and numerical) and dif-ferent clustering methods have been proposed.Preliminary results are given for cluster inter-pretation and dynamic model adaptation usingthe clusters obtained.AcknowledgementsThe authors would like to thank Driss Matrouffor providing the k-means clustering tools usedin this study.ReferencesFrederic Bechet, Giuseppe Riccardi, and DilekHakkani-Tur.
2003.
Multi-channel sentenceclassification for spoken dialogue languagemodeling.
In Proceedings of Eurospeech?03,Geneve, Switzerland.Yannick Esteve, Frederic Bechet, Alexis Nasr,and Renato De Mori.
2001.
Stochastic fi-nite state automata language model triggeredby dialogue states.
In Proceedings of Eu-rospeech?01, volume 1, pages 725?728, Aal-borg, Danemark.A.L.
Gorin, G. Riccardi, and J.H.
Wright.
1997.How May I Help You?
Speech Communica-tion, 23:113?127.Helen Wright Hastie, Rashmi Prasad, and Mar-ilyn A. Walker.
2002.
What?s the trou-ble: Automatically identifying problematicdialogs in darpa communicator dialog sys-tems.
In Proceedings of the Association ofComputational Linguistics Meeting, Philadel-phia, USA.Candace Kamm, Marilyn A. Walker, and DianeLitman.
1999.
Evaluating spoken languagesystems.
In Proceedings of American VoiceInput/Output Society AVIOS.R.
Kuhn and R. De Mori.
1995.
The applica-tion of semantic classification trees to natu-ral language understanding.
IEEE Trans.
onPattern Analysis and Machine Intelligence,17(449-460).
