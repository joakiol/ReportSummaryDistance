Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 101?104,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSelf-Training for Biomedical ParsingDavid McClosky and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{dmcc|ec}@cs.brown.eduAbstractParser self-training is the technique oftaking an existing parser, parsing extradata and then creating a second parserby treating the extra data as furthertraining data.
Here we apply this tech-nique to parser adaptation.
In partic-ular, we self-train the standard Char-niak/Johnson Penn-Treebank parser us-ing unlabeled biomedical abstracts.
Thisachieves an f -score of 84.3% on a stan-dard test set of biomedical abstracts fromthe Genia corpus.
This is a 20% error re-duction over the best previous result onbiomedical data (80.2% on the same testset).1 IntroductionParser self-training is the technique of taking anexisting parser, parsing extra data and then cre-ating a second parser by treating the extra dataas further training data.
While for many years itwas thought not to help state-of-the art parsers,more recent work has shown otherwise.
In thispaper we apply this technique to parser adap-tation.
In particular we self-train the standardCharniak/Johnson Penn-Treebank (C/J) parserusing unannotated biomedical data.
As is wellknown, biomedical data is hard on parsers be-cause it is so far from more ?standard?
English.To our knowledge this is the first application ofself-training where the gap between the trainingand self-training data is so large.In section two, we look at previous work.
Inparticular we note that there is, in fact, verylittle data on self-training when the corpora forself-training is so different from the original la-beled data.
Section three describes our mainexperiment on standard test data (Clegg andShepherd, 2005).
Section four looks at somepreliminary results we obtained on developmentdata that show in slightly more detail how self-training improved the parser.
We conclude insection five.2 Previous WorkWhile self-training has worked in several do-mains, the early results on self-training for pars-ing were negative (Steedman et al, 2003; Char-niak, 1997).
However more recent results haveshown that it can indeed improve parser perfor-mance (Bacchiani et al, 2006; McClosky et al,2006a; McClosky et al, 2006b).One possible use for this technique is forparser adaptation ?
initially training the parseron one type of data for which hand-labeled treesare available (e.g., Wall Street Journal (M. Mar-cus et al, 1993)) and then self-training on a sec-ond type of data in order to adapt the parserto the second domain.
Interestingly, there is lit-tle to no data showing that this actually works.Two previous papers would seem to address thisissue: the work by Bacchiani et al (2006) andMcClosky et al (2006b).
However, in both casesthe evidence is equivocal.Bacchiani and Roark train the Roark parser(Roark, 2001) on trees from the Brown treebankand then self-train and test on data from WallStreet Journal.
While they show some improve-ment (from 75.7% to 80.5% f -score) there areseveral aspects of this work which leave its re-101sults less than convincing as to the utility of self-training for adaptation.
The first is the pars-ing results are quite poor by modern standards.1Steedman et al (2003) generally found that self-training does not work, but found that it doeshelp if the baseline results were sufficiently bad.Secondly, the difference between the Browncorpus treebank and the Wall Street Journalcorpus is not that great.
One way to see thisis to look at out-of-vocabulary statistics.
TheBrown corpus has an out-of-vocabulary rate ofapproximately 6% when given WSJ training asthe lexicon.
In contrast, the out-of-vocabularyrate of biomedical abstracts given the same lex-icon is significantly higher at about 25% (Leaseand Charniak, 2005).
Thus the bridge the self-trained parser is asked to build is quite short.This second point is emphasized by the sec-ond paper on self-training for adaptation (Mc-Closky et al, 2006b).
This paper is based on theC/J parser and thus its results are much morein line with modern expectations.
In particu-lar, it was able to achieve an f -score of 87% onBrown treebank test data when trained and self-trained on WSJ-like data.
Note this last point.It was not the case that it used the self-trainingto bridge the corpora difference.
It self-trainedon NANC, not Brown.
NANC is a news corpus,quite like WSJ data.
Thus the point of thatpaper was that self-training a WSJ parser onsimilar data makes the parser more flexible, notbetter adapted to the target domain in particu-lar.
It said nothing about the task we addresshere.
Thus our claim is that previous results arequite ambiguous on the issue of bridging corporafor parser adaptation.Turning briefly to previous results on Medlinedata, the best comparative study of parsers isthat of Clegg and Shepherd (2005), which eval-uates several statistical parsers.
Their best re-sult was an f -score of 80.2%.
This was on theLease/Charniak (L/C) parser (Lease and Char-niak, 2005).2 A close second (1% behind) was1This is not a criticism of the work.
The results arecompletely in line with what one would expect given thebase parser and the relatively small size of the Browntreebank.2This is the standard Charniak parser (withoutthe parser of Bikel (2004).
The other parserswere not close.
However, several very good cur-rent parsers were not available when this paperwas written (e.g., the Berkeley Parser (Petrovet al, 2006)).
However, since the newer parsersdo not perform quite as well as the C/J parseron WSJ data, it is probably the case that theywould not significantly alter the landscape.3 Central Experimental ResultWe used as the base parser the standardly avail-able C/J parser.
We then self-trained the parseron approximately 270,000 sentences ?
a ran-dom selection of abstracts from Medline.3 Med-line is a large database of abstracts and citationsfrom a wide variety of biomedical literature.
Aswe note in the next section, the number 270,000was selected by observing performance on a de-velopment set.We weighted the original WSJ hand anno-tated sentences equally with self-trained Med-line data.
So, for example, McClosky et al(2006a) found that the data from the hand-annotated WSJ data should be considered atleast five times more important than NANCdata on an event by event level.
We did no tun-ing to find out if there is some better weightingfor our domain than one-to-one.The resulting parser was tested on a test cor-pus of hand-parsed sentences from the GeniaTreebank (Tateisi et al, 2005).
These are ex-actly the same sentences as used in the com-parisons of the last section.
Genia is a corpusof abstracts from the Medline database selectedfrom a search with the keywords Human, BloodCells, and Transcription Factors.
Thus the Ge-nia treebank data are all from a small domainwithin Biology.
As already noted, the Medlineabstracts used for self-training were chosen ran-domly and thus span a large number of biomed-ical sub-domains.The results, the central results of this paper,are shown in Figure 1.
Clegg and Shepherd(2005) do not provide separate precision andrecall numbers.
However we can see that thereranker) modified to use an in-domain tagger.3http://www.ncbi.nlm.nih.gov/PubMed/102System Precision Recall f -scoreL/C ?
?
80.2%Self-trained 86.3% 82.4% 84.3%Figure 1: Comparison of the Medline self-trainedparser against the previous bestMedline self-trained parser achieves an f -scoreof 84.3%, which is an absolute reduction in er-ror of 4.1%.
This corresponds to an error ratereduction of 20% over the L/C baseline.4 DiscussionPrior to the above experiment on the test data,we did several preliminary experiments on devel-opment data from the Genia Treebank.
Theseresults are summarized in Figure 2.
Here weshow the f -score for four versions of the parseras a function of number of self-training sen-tences.
The dashed line on the bottom is theraw C/J parser with no self-training.
At 80.4, itis clearly the worst of the lot.
On the other hand,it is already better than the 80.2% best previousresult for biomedical data.
This is solely due tothe introduction of the 50-best reranker whichdistinguishes the C/J parser from the precedingCharniak parser.The almost flat line above it is the C/J parserwith NANC self-training data.
As mentionedpreviously, NANC is a news corpus, quite likethe original WSJ data.
At 81.4% it gives us aone percent improvement over the original WSJparser.The topmost line, is the C/J parser trainedon Medline data.
As can be seen, even just athousand lines of Medline is already enough todrive our results to a new level and it contin-ues to improve until about 150,000 sentences atwhich point performance is nearly flat.
How-ever, as 270,000 sentences is fractionally betterthan 150,000 sentences that is the number ofself-training sentences we used for our resultson the test set.Lastly, the middle jagged line is for an inter-esting idea that failed to work.
We mention itin the hope that others might be able to succeedwhere we have failed.We reasoned that textbooks would be a par-ticularly good bridging corpus.
After all, theyare written to introduce someone ignorant ofa field to the ideas and terminology within it.Thus one might expect that the English of a Bi-ology textbook would be intermediate betweenthe more typical English of a news article andthe specialized English native to the domain.To test this we created a corpus of seven texts(?BioBooks?)
on various areas of biology thatwere available on the web.
We observe in Fig-ure 2 that for all quantities of self-training dataone does better with Medline than BioBooks.For example, at 37,000 sentences the BioBookcorpus is only able to achieve and an f-measureof 82.8% while the Medline corpus is at 83.4%.Furthermore, BioBooks levels off in performancewhile Medline has significant improvement leftin it.
Thus, while the hypothesis seems reason-able, we were unable to make it work.5 ConclusionWe self-trained the standard C/J parser on270,000 sentences of Medline abstracts.
By do-ing so we achieved a 20% error reduction overthe best previous result for biomedical parsing.In terms of the gap between the supervised dataand the self-trained data, this is the largest thathas been attempted.Furthermore, the resulting parser is of interestin its own right, being as it is the most accuratebiomedical parser yet developed.
This parser isavailable on the web.4Finally, there is no reason to believe that84.3% is an upper bound on what can beachieved with current techniques.
Lease andCharniak (2005) achieve their results using smallamounts of hand-annotated biomedical part-of-speech-tagged data and also explore other pos-sible sources or information.
It is reasonable toassume that its use would result in further im-provement.AcknowledgmentsThis work was supported by DARPA GALE con-tract HR0011-06-2-0001.
We would like to thank theBLLIP team for their comments.4http://bllip.cs.brown.edu/biomedical/1030 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000Number of sentences added80.080.280.480.680.881.081.281.481.681.882.082.282.482.682.883.083.283.483.683.884.084.284.4Rerankingparserf-scoreWSJ+MedlineWSJ+BioBooksWSJ+NANCWSJ (baseline)Figure 2: Labeled Precision-Recall results on development data for four versions of the parser as a functionof number of self-training sentencesReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation ofstochastic grammars.
Computer Speech and Lan-guage, 20(1):41?68.Daniel M. Bikel.
2004.
Intricacies of collins parsingmodel.
Computational Linguistics, 30(4).Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InProc.
AAAI, pages 598?603.Andrew B. Clegg and Adrian Shepherd.
2005.Evaluating and integrating treebank parsers ona biomedical corpus.
In Proceedings of the ACLWorkshop on Software.Matthew Lease and Eugene Charniak.
2005.
Pars-ing biomedical literature.
In Second InternationalJoint Conference on Natural Language Processing(IJCNLP?05).M.
Marcus et al 1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Comp.Linguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006a.
Effective self-training for parsing.In Proceedings of the Human Language Technol-ogy Conference of the NAACL, Main Conference,pages 152?159.David McClosky, Eugene Charniak, and Mark John-son.
2006b.
Reranking and self-training forparser adaptation.
In Proceedings of COLING-ACL 2006, pages 337?344, Sydney, Australia,July.
Association for Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
Learning accurate, compact,and interpretable tree annotation.
In Proceed-ings of COLING-ACL 2006, pages 433?440, Syd-ney, Australia, July.
Association for Computa-tional Linguistics.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27(2):249?276.Mark Steedman, Miles Osborne, Anoop Sarkar,Stephen Clark, Rebecca Hwa, Julia Hockenmaier,Paul Ruhlen, Steven Baker, and Jeremiah Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proc.
of European ACL (EACL),pages 331?338.Y.
Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii.2005.
Syntax Annotation for the GENIA corpus.Proc.
IJCNLP 2005, Companion volume, pages222?227.104
