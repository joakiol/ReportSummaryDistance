Proceedings of ACL-08: HLT, pages 780?788,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsPivot Approach for Extracting Paraphrase Patterns from Bilingual CorporaShiqi Zhao1, Haifeng Wang2, Ting Liu1, Sheng Li11Harbin Institute of Technology, Harbin, China{zhaosq,tliu,lisheng}@ir.hit.edu.cn2Toshiba (China) Research and Development Center, Beijing, Chinawanghaifeng@rdc.toshiba.com.cnAbstractParaphrase patterns are useful in paraphraserecognition and generation.
In this paper, wepresent a pivot approach for extracting para-phrase patterns from bilingual parallel cor-pora, whereby the English paraphrase patternsare extracted using the sentences in a for-eign language as pivots.
We propose a log-linear model to compute the paraphrase likeli-hood of two patterns and exploit feature func-tions based on maximum likelihood estima-tion (MLE) and lexical weighting (LW).
Us-ing the presented method, we extract over1,000,000 pairs of paraphrase patterns from2M bilingual sentence pairs, the precisionof which exceeds 67%.
The evaluation re-sults show that: (1) The pivot approach iseffective in extracting paraphrase patterns,which significantly outperforms the conven-tional method DIRT.
Especially, the log-linearmodel with the proposed feature functionsachieves high performance.
(2) The coverageof the extracted paraphrase patterns is high,which is above 84%.
(3) The extracted para-phrase patterns can be classified into 5 types,which are useful in various applications.1 IntroductionParaphrases are different expressions that conveythe same meaning.
Paraphrases are important inplenty of natural language processing (NLP) ap-plications, such as question answering (QA) (Linand Pantel, 2001; Ravichandran and Hovy, 2002),machine translation (MT) (Kauchak and Barzilay,2006; Callison-Burch et al, 2006), multi-documentsummarization (McKeown et al, 2002), and naturallanguage generation (Iordanskaja et al, 1991).Paraphrase patterns are sets of semanticallyequivalent patterns, in which a pattern generallycontains two parts, i.e., the pattern words and slots.For example, in the pattern ?X solves Y?, ?solves?
isthe pattern word, while ?X?
and ?Y?
are slots.
Onecan generate a text unit (phrase or sentence) by fill-ing the pattern slots with specific words.
Paraphrasepatterns are useful in both paraphrase recognitionand generation.
In paraphrase recognition, if twotext units match a pair of paraphrase patterns and thecorresponding slot-fillers are identical, they can beidentified as paraphrases.
In paraphrase generation,a text unit that matches a pattern P can be rewrittenusing the paraphrase patterns of P.A variety of methods have been proposed on para-phrase patterns extraction (Lin and Pantel, 2001;Ravichandran and Hovy, 2002; Shinyama et al,2002; Barzilay and Lee, 2003; Ibrahim et al, 2003;Pang et al, 2003; Szpektor et al, 2004).
However,these methods have some shortcomings.
Especially,the precisions of the paraphrase patterns extractedwith these methods are relatively low.In this paper, we extract paraphrase patterns frombilingual parallel corpora based on a pivot approach.We assume that if two English patterns are alignedwith the same pattern in another language, they arelikely to be paraphrase patterns.
This assumptionis an extension of the one presented in (Bannardand Callison-Burch, 2005), which was used for de-riving phrasal paraphrases from bilingual corpora.Our method involves three steps: (1) corpus prepro-cessing, including English monolingual dependency780parsing and English-foreign language word align-ment, (2) aligned patterns induction, which producesEnglish patterns along with the aligned pivot pat-terns in the foreign language, (3) paraphrase pat-terns extraction, in which paraphrase patterns are ex-tracted based on a log-linear model.Our contributions are as follows.
Firstly, we arethe first to use a pivot approach to extract paraphrasepatterns from bilingual corpora, though similarmethods have been used for learning phrasal para-phrases.
Our experiments show that the pivot ap-proach significantly outperforms conventional meth-ods.
Secondly, we propose a log-linear model forcomputing the paraphrase likelihood.
Besides, weuse feature functions based on maximum likeli-hood estimation (MLE) and lexical weighting (LW),which are effective in extracting paraphrase patterns.Using the proposed approach, we extract over1,000,000 pairs of paraphrase patterns from 2Mbilingual sentence pairs, the precision of which isabove 67%.
Experimental results show that the pivotapproach evidently outperforms DIRT, a well knownmethod that extracts paraphrase patterns frommono-lingual corpora (Lin and Pantel, 2001).
Besides, thelog-linear model is more effective than the conven-tional model presented in (Bannard and Callison-Burch, 2005).
In addition, the coverage of the ex-tracted paraphrase patterns is high, which is above84%.
Further analysis shows that 5 types of para-phrase patterns can be extracted with our method,which can by used in multiple NLP applications.The rest of this paper is structured as follows.Section 2 reviews related work on paraphrase pat-terns extraction.
Section 3 presents our method indetail.
We evaluate the proposed method in Section4, and finally conclude this paper in Section 5.2 Related WorkParaphrase patterns have been learned and used ininformation extraction (IE) and answer extraction ofQA.
For example, Lin and Pantel (2001) proposed amethod (DIRT), in which they obtained paraphrasepatterns from a parsed monolingual corpus based onan extended distributional hypothesis, where if twopaths in dependency trees tend to occur in similarcontexts it is hypothesized that the meanings of thepaths are similar.
The examples of obtained para-(1) X solves YY is solved by XX finds a solution to Y......(2) born in <ANSWER> , <NAME><NAME> was born on <ANSWER> ,<NAME> ( <ANSWER> -......(3) ORGANIZATION decides ?ORGANIZATION confirms ?......Table 1: Examples of paraphrase patterns extracted withthe methods of Lin and Pantel (2001), Ravichandran andHovy (2002), and Shinyama et al (2002).phrase patterns are shown in Table 1 (1).Based on the same hypothesis as above, somemethods extracted paraphrase patterns from the web.For instance, Ravichandran and Hovy (2002) de-fined a question taxonomy for their QA system.They then used hand-crafted examples of each ques-tion type as queries to retrieve paraphrase patternsfrom the web.
For instance, for the question type?BIRTHDAY?, The paraphrase patterns produced bytheir method can be seen in Table 1 (2).Similar methods have also been used by Ibrahimet al (2003) and Szpektor et al (2004).
The maindisadvantage of the above methods is that the pre-cisions of the learned paraphrase patterns are rela-tively low.
For instance, the precisions of the para-phrase patterns reported in (Lin and Pantel, 2001),(Ibrahim et al, 2003), and (Szpektor et al, 2004)are lower than 50%.
Ravichandran and Hovy (2002)did not directly evaluate the precision of the para-phrase patterns extracted using their method.
How-ever, the performance of their method is dependenton the hand-crafted queries for web mining.Shinyama et al (2002) presented a method thatextracted paraphrase patterns from multiple news ar-ticles about the same event.
Their method was basedon the assumption that NEs are preserved acrossparaphrases.
Thus the method acquired paraphrasepatterns from sentence pairs that share comparableNEs.
Some examples can be seen in Table 1 (3).The disadvantage of this method is that it greatlyrelies on the number of NEs in sentences.
The preci-781start Palestinian suicide bomber blew himself up in SLOT1 on SLOT2killing SLOT3 other people and injuringwounding SLOT4 enddetroit the*e*a?s*e* buildingbuilding in detroitflattenedgroundlevelledtoblastedleveled*e*wasreducedrazedleveledto downrubbleinto ashes*e*to *e*(1)(2)Figure 1: Examples of paraphrase patterns extracted byBarzilay and Lee (2003) and Pang et al (2003).sion of the extracted patterns may sharply decreaseif the sentences do not contain enough NEs.Barzilay and Lee (2003) applied multi-sequencealignment (MSA) to parallel news sentences and in-duced paraphrase patterns for generating new sen-tences (Figure 1 (1)).
Pang et al (2003) built finitestate automata (FSA) from semantically equivalenttranslation sets based on syntactic alignment.
Thelearned FSAs could be used in paraphrase represen-tation and generation (Figure 1 (2)).
Obviously, itis difficult for a sentence to match such complicatedpatterns, especially if the sentence is not from thesame domain in which the patterns are extracted.Bannard and Callison-Burch (2005) first ex-ploited bilingual corpora for phrasal paraphrase ex-traction.
They assumed that if two English phrasese1 and e2 are aligned with the same phrase c inanother language, these two phrases may be para-phrases.
Specifically, they computed the paraphraseprobability in terms of the translation probabilities:p(e2|e1) =?cpMLE(c|e1)pMLE(e2|c) (1)In Equation (1), pMLE(c|e1) and pMLE(e2|c) arethe probabilities of translating e1 to c and c to e2,which are computed based on MLE:pMLE(c|e1) =count(c, e1)?c?
count(c?, e1)(2)where count(c, e1) is the frequency count thatphrases c and e1 are aligned in the corpus.pMLE(e2|c) is computed in the same way.This method proved effective in extracting highquality phrasal paraphrases.
As a result, we extendit to paraphrase pattern extraction in this paper.S T E (take)shouldWe takemarketintoconsiderationtakemarketintoconsiderationtakeintoconsiderationP S T E (take)firstT EdemanddemandFigure 2: Examples of a subtree and a partial subtree.3 Proposed Method3.1 Corpus PreprocessingIn this paper, we use English paraphrase patterns ex-traction as a case study.
An English-Chinese (E-C) bilingual parallel corpus is employed for train-ing.
The Chinese part of the corpus is used as pivotsto extract English paraphrase patterns.
We conductword alignment with Giza++ (Och and Ney, 2000) inboth directions and then apply the grow-diag heuris-tic (Koehn et al, 2005) for symmetrization.Since the paraphrase patterns are extracted fromdependency trees, we parse the English sentencesin the corpus with MaltParser (Nivre et al, 2007).Let SE be an English sentence, TE the parse treeof SE , e a word of SE , we define the subtree andpartial subtree following the definitions in (Ouan-graoua et al, 2007).
In detail, a subtree STE(e)is a particular connected subgraph of the tree TE ,which is rooted at e and includes all the descendantsof e. A partial subtree PSTE(e) is a connected sub-graph of the subtree STE(e), which is rooted at e butdoes not necessarily include all the descendants of e.For instance, for the sentence ?We should first takemarket demand into consideration?, STE(take) andPSTE(take) are shown in Figure 21.3.2 Aligned Patterns InductionTo induce the aligned patterns, we first induce theEnglish patterns using the subtrees and partial sub-trees.
Then, we extract the pivot Chinese patternsaligning to the English patterns.1Note that, a subtree may contain several partial subtrees.
Inthis paper, all the possible partial subtrees are considered whenextracting paraphrase patterns.782Algorithm 1: Inducing an English pattern1: Input: words in STE(e) : wiwi+1...wj2: Input: PE(e) = ?3: For each wk (i ?
k ?
j)4: If wk is in PSTE(e)5: Append wk to the end of PE(e)6: Else7: Append POS(wk) to the end of PE(e)8: End ForAlgorithm 2: Inducing an aligned pivot pattern1: Input: SC = t1t2...tn2: Input: PC = ?3: For each tl (1 ?
l ?
n)4: If tl is aligned with wk in SE5: If wk is a word in PE(e)6: Append tl to the end of PC7: If POS(wk) is a slot in PE(e)8: Append POS(wk) to the end of PC9: End ForStep-1 Inducing English patterns.
In this paper, anEnglish pattern PE(e) is a string comprising wordsand part-of-speech (POS) tags.
Our intuition forinducing an English pattern is that a partial sub-tree PSTE(e) can be viewed as a unit that conveysa definite meaning, though the words in PSTE(e)may not be continuous.
For example, PSTE(take)in Figure 2 contains words ?take ... into consid-eration?.
Therefore, we may extract ?take X intoconsideration?
as a pattern.
In addition, the wordsthat are in STE(e) but not in PSTE(e) (denoted asSTE(e)/PSTE(e)) are also useful for inducing pat-terns, since they can constrain the pattern slots.
Inthe example in Figure 2, the word ?demand?
indi-cates that a noun can be filled in the slot X and thepattern may have the form ?take NN into considera-tion?.
Based on this intuition, we induce an Englishpattern PE(e) as in Algorithm 12.For the example in Figure 2, the generated pat-tern PE(take) is ?take NN NN into considera-tion?.
Note that the patterns induced in this wayare quite specific, since the POS of each word inSTE(e)/PSTE(e) forms a slot.
Such patterns aredifficult to be matched in applications.
We there-2POS(wk) in Algorithm 1 denotes the POS tag of wk.N N _1 ??
NN _2 NN_1 ??
N N _2NN_1NN_2 considered byis NN_1 consider NN_2Figure 3: Aligned patterns with numbered slots.fore take an additional step to simplify the patterns.Let ei and ej be two words in STE(e)/PSTE(e),whose POS posi and posj are slots in PE(e).
If eiis a descendant of ej in the parse tree, we removeposi from PE(e).
For the example above, the POSof ?market?
is removed, since it is the descendant of?demand?, whose POS also forms a slot.
The sim-plified pattern is ?take NN into consideration?.Step-2 Extracting pivot patterns.
For each En-glish pattern PE(e), we extract an aligned Chinesepivot pattern PC .
Let a Chinese sentence SC be thetranslation of the English sentence SE , PE(e) a pat-tern induced from SE , we extract the pivot patternPC aligning to PE(e) as in Algorithm 2.
Note thatthe Chinese patterns are not extracted from parsetrees.
They are only sequences of Chinese wordsand POSes that are aligned with English patterns.A pattern may contain two or more slots shar-ing the same POS.
To distinguish them, we assigna number to each slot in the aligned E-C patterns.
Indetail, the slots having identical POS in PC are num-bered incrementally (i.e., 1,2,3...), while each slot inPE(e) is assigned the same number as its alignedslot in PC .
The examples of the aligned patternswith numbered slots are illustrated in Figure 3.3.3 Paraphrase Patterns ExtractionAs mentioned above, if patterns e1 and e2 arealigned with the same pivot pattern c, e1 and e2 maybe paraphrase patterns.
The paraphrase likelihoodcan be computed using Equation (1).
However, wefind that using only the MLE based probabilities cansuffer from data sparseness.
In order to exploit moreand richer information to estimate the paraphraselikelihood, we propose a log-linear model:score(e2|e1) =?cexp[N?i=1?ihi(e1, e2, c)] (3)where hi(e1, e2, c) is a feature function and ?i is the783weight.
In this paper, 4 feature functions are used inour log-linear model, which include:h1(e1, e2, c) = scoreMLE(c|e1)h2(e1, e2, c) = scoreMLE(e2|c)h3(e1, e2, c) = scoreLW (c|e1)h4(e1, e2, c) = scoreLW (e2|c)Feature functions h1(e1, e2, c) and h2(e1, e2, c)are based on MLE.
scoreMLE(c|e) is computed as:scoreMLE(c|e) = log pMLE(c|e) (4)scoreMLE(e|c) is computed in the same way.h3(e1, e2, c) and h4(e1, e2, c) are based on LW.LW was originally used to validate the quality of aphrase translation pair in MT (Koehn et al, 2003).
Itchecks how well the words of the phrases translateto each other.
This paper uses LW to measure thequality of aligned patterns.
We define scoreLW (c|e)as the logarithm of the lexical weight3:scoreLW (c|e) =1nn?i=1log(1|{j|(i, j) ?
a}|??
(i,j)?aw(ci|ej)) (5)where a denotes the word alignment between c ande.
n is the number of words in c. ci and ej are wordsof c and e. w(ci|ej) is computed as follows:w(ci|ej) =count(ci, ej)?c?icount(c?i, ej)(6)where count(ci, ej) is the frequency count ofthe aligned word pair (ci, ej) in the corpus.scoreLW (e|c) is computed in the same manner.In our experiments, we set a threshold T .
If thescore between e1 and e2 based on Equation (3) ex-ceeds T , e2 is extracted as the paraphrase of e1.3.4 Parameter EstimationFive parameters need to be estimated, i.e., ?1, ?2,?3, ?4 in Equation (3), and the threshold T .
Toestimate the parameters, we first construct a devel-opment set.
In detail, we randomly sample 7,0863The logarithm of the lexical weight is divided by n so asnot to penalize long patterns.groups of aligned E-C patterns that are obtained asdescribed in Section 3.2.
The English patterns ineach group are all aligned with the same Chinesepivot pattern.
We then extract paraphrase patternsfrom the aligned patterns as described in Section 3.3.In this process, we set ?i = 1 (i = 1, ..., 4) and as-sign T a minimum value, so as to obtain all possibleparaphrase patterns.A total of 4,162 pairs of paraphrase patterns havebeen extracted and manually labeled as ?1?
(correctparaphrase patterns) or ?0?
(incorrect).
Here, twopatterns are regarded as paraphrase patterns if theycan generate paraphrase fragments by filling the cor-responding slots with identical words.
We use gra-dient descent algorithm (Press et al, 1992) to esti-mate the parameters.
For each set of parameters, wecompute the precision P , recall R, and f-measureF as: P = |set1?set2||set1| , R =|set1?set2||set2| , F =2PRP+R ,where set1 denotes the set of paraphrase patterns ex-tracted under the current parameters.
set2 denotesthe set of manually labeled correct paraphrase pat-terns.
We select the parameters that can maximizethe F-measure on the development set4.4 ExperimentsThe E-C parallel corpus in our experiments was con-structed using several LDC bilingual corpora5.
Afterfiltering sentences that are too long (> 40 words) ortoo short (< 5 words), 2,048,009 pairs of parallelsentences were retained.We used two constraints in the experiments to im-prove the efficiency of computation.
First, only sub-trees containing no more than 10 words were used toinduce English patterns.
Second, although any POStag can form a slot in the induced patterns, we onlyfocused on three kinds of POSes in the experiments,i.e., nouns (tags include NN, NNS, NNP, NNPS),verbs (VB, VBD, VBG, VBN, VBP, VBZ), and ad-jectives (JJ, JJS, JJR).
In addition, we constrainedthat a pattern must contain at least one content word4The parameters are: ?1 = 0.0594137, ?2 = 0.995936,?3 = ?0.0048954, ?4 = 1.47816, T = ?10.002.5The corpora include LDC2000T46, LDC2000T47,LDC2002E18, LDC2002T01, LDC2003E07, LDC2003E14,LDC2003T17, LDC2004E12, LDC2004T07, LDC2004T08,LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24,LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T04,LDC2007T02, LDC2007T09.784Method #PP (pairs) PrecisionLL-Model 1,058,624 67.03%MLE-Model 1,015,533 60.60%DIRT top-1 1,179 19.67%DIRT top-5 5,528 18.73%Table 2: Comparison of paraphrasing methods.so as to filter patterns like ?the [NN 1]?.4.1 Evaluation of the Log-linear ModelAs previously mentioned, in the log-linear model ofthis paper, we use both MLE based and LW basedfeature functions.
In this section, we evaluate thelog-linear model (LL-Model) and compare it withthe MLE based model (MLE-Model) presented byBannard and Callison-Burch (2005)6.We extracted paraphrase patterns using two mod-els, respectively.
From the results of each model,we randomly picked 3,000 pairs of paraphrase pat-terns to evaluate the precision.
The 6,000 pairs ofparaphrase patterns were mixed and presented to thehuman judges, so that the judges cannot know bywhich model each pair was produced.
The sampledpatterns were then manually labeled and the preci-sion was computed as described in Section 3.4.The number of the extracted paraphrase patterns(#PP) and the precision are depicted in the first twolines of Table 2.
We can see that the numbers ofparaphrase patterns extracted using the two mod-els are comparable.
However, the precision of LL-Model is significantly higher than MLE-Model.Actually, MLE-Model is a special case of LL-Model and the enhancement of the precision ismainly due to the use of LW based features.It is not surprising, since Bannard and Callison-Burch (2005) have pointed out that word alignmenterror is the major factor that influences the perfor-mance of the methods learning paraphrases frombilingual corpora.
The LW based features validatethe quality of word alignment and assign low scoresto those aligned E-C pattern pairs with incorrectalignment.
Hence the precision can be enhanced.6In this experiment, we also estimated a threshold T ?
forMLE-Model using the development set (T ?
= ?5.1).
The pat-tern pairs whose score based on Equation (1) exceed T ?
wereextracted as paraphrase patterns.4.2 Comparison with DIRTIt is necessary to compare our method with anotherparaphrase patterns extraction method.
However, itis difficult to find methods that are suitable for com-parison.
Some methods only extract paraphrase pat-terns using news articles on certain topics (Shinyamaet al, 2002; Barzilay and Lee, 2003), while someothers need seeds as initial input (Ravichandran andHovy, 2002).
In this paper, we compare our methodwith DIRT (Lin and Pantel, 2001), which does notneed to specify topics or input seeds.As mentioned in Section 2, DIRT learns para-phrase patterns from a parsed monolingual corpusbased on an extended distributional hypothesis.
Inour experiment, we implemented DIRT and ex-tracted paraphrase patterns from the English part ofour bilingual parallel corpus.
Our corpus is smallerthan that reported in (Lin and Pantel, 2001).
To alle-viate the data sparseness problem, we only kept pat-terns appearing more than 10 times in the corpus forextracting paraphrase patterns.
Different from ourmethod, no threshold was set in DIRT.
Instead, theextracted paraphrase patterns were ranked accord-ing to their scores.
In our experiment, we kept top-5paraphrase patterns for each target pattern.From the extracted paraphrase patterns, we sam-pled 600 groups for evaluation.
Each group com-prises a target pattern and its top-5 paraphrase pat-terns.
The sampled data were manually labeled andthe top-n precision was calculated asPNi=1 niN?n , whereN is the number of groups and ni is the number ofcorrect paraphrase patterns in the top-n paraphrasepatterns of the i-th group.
The top-1 and top-5 re-sults are shown in the last two lines of Table 2.
Al-though there are more correct patterns in the top-5results, the precision drops sequentially from top-1to top-5 since the denominator of top-5 is 4 timeslarger than that of top-1.Obviously, the number of the extracted para-phrase patterns is much smaller than that extractedusing our method.
Besides, the precision is alsomuch lower.
We believe that there are two reasons.First, the extended distributional hypothesis is notstrict enough.
Patterns sharing similar slot-fillers donot necessarily have the same meaning.
They mayeven have the opposite meanings.
For example, ?Xworsens Y?
and ?X solves Y?
were extracted as para-785Type Count Exampletrivial change 79 (e1) all the members of [NNPS 1] (e2) all members of [NNPS 1]phrase replacement 267 (e1) [JJ 1] economic losses (e2) [JJ 1] financial lossesphrase reordering 56 (e1) [NN 1] definition (e2) the definition of [NN 1]structural paraphrase 71 (e1) the admission of [NNP 1] to the wto (e2) the [NNP 1] ?s wto accessioninformation + or - 27 (e1) [NNS 1] are in fact women (e2) [NNS 1] are womenTable 3: The statistics and examples of each type of paraphrase patterns.phrase patterns by DIRT.
The other reason is thatDIRT can only be effective for patterns appearingplenty of times in the corpus.
In other words, it seri-ously suffers from data sparseness.
We believe thatDIRT can perform better on a larger corpus.4.3 Pivot Pattern ConstraintsAs described in Section 3.2, we constrain that thepattern words of an English pattern e must be ex-tracted from a partial subtree.
However, we do nothave such constraint on the Chinese pivot patterns.Hence, it is interesting to investigate whether theperformance can be improved if we constrain thatthe pattern words of a pivot pattern c must also beextracted from a partial subtree.To conduct the evaluation, we parsed the Chinesesentences of the corpus with a Chinese dependencyparser (Liu et al, 2006).
We then induced Englishpatterns and extracted aligned pivot patterns.
For thealigned patterns (e, c), if c?s pattern words were notextracted from a partial subtree, the pair was filtered.After that, we extracted paraphrase patterns, fromwhich we sampled 3,000 pairs for evaluation.The results show that 736,161 pairs of paraphrasepatterns were extracted and the precision is 65.77%.Compared with Table 2, the number of the extractedparaphrase patterns gets smaller and the precisionalso gets lower.
The results suggest that the perfor-mance of the method cannot be improved by con-straining the extraction of pivot patterns.4.4 Analysis of the Paraphrase PatternsWe sampled 500 pairs of correct paraphrase pat-terns extracted using our method and analyzed thetypes.
We found that there are 5 types of para-phrase patterns, which include: (1) trivial change,such as changes of prepositions and articles, etc; (2)phrase replacement; (3) phrase reordering; (4) struc-tural paraphrase, which contain both phrase replace-ments and phrase reordering; (5) adding or reducinginformation that does not change the meaning.
Somestatistics and examples are shown in Table 3.The paraphrase patterns are useful in NLP appli-cations.
Firstly, over 50% of the paraphrase patternsare in the type of phrase replacement, which canbe used in IE pattern reformulation and sentence-level paraphrase generation.
Compared with phrasalparaphrases, the phrase replacements in patterns aremore accurate due to the constraints of the slots.The paraphrase patterns in the type of phrase re-ordering can also be used in IE pattern reformula-tion and sentence paraphrase generation.
Especially,in sentence paraphrase generation, this type of para-phrase patterns can reorder the phrases in a sentence,which can hardly be achieved by the conventionalMT-based generation method (Quirk et al, 2004).The structural paraphrase patterns have the advan-tages of both phrase replacement and phrase reorder-ing.
More paraphrase sentences can be generatedusing these patterns.The paraphrase patterns in the type of ?informa-tion + and -?
are useful in sentence compression andexpansion.
A sentence matching a long pattern canbe compressed by paraphrasing it using shorter pat-terns.
Similarly, a short sentence can be expandedby paraphrasing it using longer patterns.For the 3,000 pairs of test paraphrase patterns, wealso investigate the number and type of the patternslots.
The results are summarized in Table 4 and 5.From Table 4, we can see that more than 92%of the paraphrase patterns contain only one slot,just like the examples shown in Table 3.
In addi-tion, about 7% of the paraphrase patterns containtwo slots, such as ?give [NN 1] [NN 2]?
vs. ?give[NN 2] to [NN 1]?.
This result suggests that ourmethod tends to extract short paraphrase patterns,786Slot No.
#PP Percentage Precision1-slot 2,780 92.67% 66.51%2-slots 218 7.27% 73.85%?3-slots 2 <1% 50.00%Table 4: The statistics of the numbers of pattern slots.Slot Type #PP Percentage PrecisionN-slots 2,376 79.20% 66.71%V-slots 273 9.10% 70.33%J-slots 438 14.60% 70.32%Table 5: The statistics of the type of pattern slots.which is mainly because the data sparseness prob-lem is more serious when extracting long patterns.From Table 5, we can find that near 80% of theparaphrase patterns contain noun slots, while about9% and 15% contain verb slots and adjective slots7.This result implies that nouns are the most typicalvariables in paraphrase patterns.4.5 Evaluation within Context SentencesIn Section 4.1, we have evaluated the precision ofthe paraphrase patterns without considering contextinformation.
In this section, we evaluate the para-phrase patterns within specific context sentences.The open test set includes 119 English sentences.We parsed the sentences with MaltParser and in-duced patterns as described in Section 3.2.
For eachpattern e in sentence SE , we searched e?s paraphrasepatterns from the database of the extracted para-phrase patterns.
The result shows that 101 of the119 sentences contain at least one pattern that canbe paraphrased using the extracted paraphrase pat-terns, the coverage of which is 84.87%.Furthermore, since a pattern may have severalparaphrase patterns, we exploited a method to au-tomatically select the best one in the given contextsentence.
In detail, a paraphrase pattern e?
of e wasreranked based on a language model (LM):score(e?|e, SE) =?scoreLL(e?|e) + (1 ?
?
)scoreLM (e?|SE) (7)7Notice that, a pattern may contain more than one type ofslots, thus the sum of the percentages is larger than 1.Here, scoreLL(e?|e) denotes the score based onEquation (3).
scoreLM (e?|SE) is the LM basedscore: scoreLM (e?|SE) = 1n logPLM (S?E), whereS?E is the sentence generated by replacing e in SEwith e?.
The language model in the experiment wasa tri-gram model trained using the English sentencesin the bilingual corpus.
We empirically set ?
= 0.7.The selected best paraphrase patterns in contextsentences were manually labeled.
The context infor-mation was also considered by our judges.
The re-sult shows that the precision of the best paraphrasepatterns is 59.39%.
To investigate the contributionof the LM based score, we ran the experiment againwith ?
= 1 (ignoring the LM based score) and foundthat the precision is 57.09%.
It indicates that the LMbased reranking can improve the precision.
How-ever, the improvement is small.
Further analysisshows that about 70% of the correct paraphrase sub-stitutes are in the type of phrase replacement.5 ConclusionThis paper proposes a pivot approach for extractingparaphrase patterns from bilingual corpora.
We usea log-linear model to compute the paraphrase like-lihood and exploit feature functions based on MLEand LW.
Experimental results show that the pivot ap-proach is effective, which extracts over 1,000,000pairs of paraphrase patterns from 2M bilingual sen-tence pairs.
The precision and coverage of the ex-tracted paraphrase patterns exceed 67% and 84%,respectively.
In addition, the log-linear model withthe proposed feature functions significantly outper-forms the conventional models.
Analysis shows that5 types of paraphrase patterns are extracted with ourmethod, which are useful in various applications.In the future we wish to exploit more feature func-tions in the log-linear model.
In addition, we will tryto make better use of the context information whenreplacing paraphrase patterns in context sentences.AcknowledgmentsThis research was supported by National Nat-ural Science Foundation of China (60503072,60575042).
We thank Lin Zhao, Xiaohang Qu, andZhenghua Li for their help in the experiments.787ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Proceed-ings of ACL, pages 597-604.Regina Barzilay and Lillian Lee.
2003.
Learning to Para-phrase: An Unsupervised Approach Using Multiple-Sequence Alignment.
In Proceedings of HLT-NAACL,pages 16-23.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved Statistical Machine Trans-lation Using Paraphrases.
In Proceedings of HLT-NAACL, pages 17-24.Ali Ibrahim, Boris Katz, and Jimmy Lin.
2003.
Extract-ing Structural Paraphrases from Aligned MonolingualCorpora.
In Proceedings of IWP, pages 57-64.Lidija Iordanskaja, Richard Kittredge, and AlainPolgue`re.
1991.
Lexical Selection and Paraphrase in aMeaning-Text Generation Model.
In Ce?cile L. Paris,William R. Swartout, and William C. Mann (Eds.
):Natural Language Generation in Artificial Intelligenceand Computational Linguistics, pages 293-312.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for Automatic Evaluation.
In Proceedings of HLT-NAACL, pages 455-462.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proceedings of IWSLT.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of HLT-NAACL, pages 127-133.De-Kang Lin and Patrick Pantel.
2001.
Discovery ofInference Rules for Question Answering.
In NaturalLanguage Engineering 7(4): 343-360.Ting Liu, Jin-Shan Ma, Hui-Jia Zhu, and Sheng Li.
2006.Dependency Parsing Based on Dynamic Local Opti-mization.
In Proceedings of CoNLL-X, pages 211-215.Kathleen R. Mckeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and Summarizing News ona Daily Basis with Columbia?s Newsblaster.
In Pro-ceedings of HLT, pages 280-285.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A Language-Independent System for Data-Driven DependencyParsing.
In Natural Language Engineering 13(2): 95-135.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proceedings of ACL,pages 440-447.A?
?da Ouangraoua, Pascal Ferraro, Laurent Tichit, andSerge Dulucq.
2007.
Local Similarity between Quo-tiented Ordered Trees.
In Journal of Discrete Algo-rithms 5(1): 23-35.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based Alignment of Multiple Translations: Ex-tracting Paraphrases and Generating New Sentences.In Proceedings of HLT-NAACL, pages 102-109.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
1992.
Numerical Recipesin C: The Art of Scientific Computing.
CambridgeUniversity Press, Cambridge, U.K., 1992, 412-420.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual Machine Translation for ParaphraseGeneration.
In Proceedings of EMNLP, pages 142-149.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing Surface Text Patterns for a Question AnsweringSystem.
In Proceedings of ACL, pages 41-47.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic Paraphrase Acquisition from NewsArticles.
In Proceedings of HLT, pages 40-46.Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-tura Coppola.
2004.
Scaling Web-based Acquisitionof Entailment Relations.
In Proceedings of EMNLP,pages 41-48.788
