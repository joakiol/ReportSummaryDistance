Never Look Back: An Alternative to CenteringMichael StrubeIRCS - Institute for Research in Cognitive ScienceUniversity of Pennsylvania3401 Walnut Street, Suite 400APhiladelphia PA 19104S trube@l inc,  cis.
upenn, eduAbstractI propose a model for determining the hearer's at-tentional state which depends olely on a list ofsalient discourse entities (S-list).
The orderingamong the elements of the S-list covers also thefunction of the backward-looking center in the cen-tering model.
The ranking criteria for the S-listare based on the distinction between hearer-old andhearer-new discourse ntities and incorporate pref-erences for inter- and intra-sentential anaphora.
Themodel is the basis for an algorithm which operatesincrementally, word by word.1 IntroductionI propose a model for determining the heater's at-tentional state in understanding discourse.
My pro-posal is inspired by the centering model (Groszet al, 1983; 1995) and draws on the conclusions ofStrube & Hahn's (1996) approach for the ranking ofthe forward-looking center list for German.
Theirapproach as been proven as the point of departurefor a new model which is valid for English as well.The use of the centering transitions in Brennanet al's (1987) algorithm prevents it from being ap-plied incrementally (cf.
Kehler (1997)).
In my ap-proach, I propose to replace the functions of thebackward-looking center and the centering transi-tions by the order among the elements of the list ofsalient discourse ntities (S-list).
The S-list rank-ing criteria define a preference for hearer-old overhearer-new discourse ntities (Prince, 1981) gener-alizing Strube & Hahn's (1996) approach.
Becauseof these ranking criteria, I can account for the dif-ference in salience between definite NPs (mostlyhearer-old) and indefinite NPs (mostly hearer-new).The S-list is not a local data structure associ-ated with individual utterances.
The S-list ratherdescribes the attentional state of the hearer at anygiven point in processing a discourse.
The S-list isgenerated incrementally, word by word, and usedimmediately.
Therefore, the S-list integrates in thesimplest manner preferences for inter- and intra-sentential anaphora, making further specificationsfor processing complex sentences unnecessary.Section 2 describes the centering model as therelevant background for my proposal.
In Section 3,I introduce my model, its only data structure, theS-list, and the accompanying algorithm.
In Section4, I compare the results of my algorithm with theresults of the centering algorithm (Brennan et al,1987) with and without specifications for complexsentences (Kameyama, 1998).2 A Look Back: CenteringThe centering model describes the relation betweenthe focus of attention, the choices of referring ex-pressions, and the perceived coherence ofdiscourse.The model has been motivated with evidence frompreferences for the antecedents of pronouns (Groszet al, 1983; 1995) and has been applied to pronounresolution (Brennan et al (1987), inter alia, whoseinterpretation differs from the original model).The centering model itself consists of two con-structs, the backward-looking center and the listof forward-looking centers, and a few rules andconstraints.
Each utterance Ui is assigned a listof forward-looking centers, C f (Ui), and a uniquebackward-looking center, Cb(Ui).
A ranking im-posed on the elements of the Cf  reflects the as-sumption that the most highly ranked element ofC f (Ui) (the preferred center Cp(Ui)) is most likelyto be the Cb(Ui+l).
The most highly ranked el-ement of Cf(Ui) that is realized in Ui+x (i.e., isassociated with an expression that has a valid inter-pretation in the underlying semantic representation)is the Cb(Ui+l).
Therefore, the ranking on the Cfplays a crucial role in the model.
Grosz et al (1995)and Brennan et al (1987) use grammatical relationsto rank the Cf (i.e., subj -.< obj -< ...) but state thatother factors might also play a role.1251Cb(Ui) =Cp(Vi)Cb(Ui) y?Cp(t:i)For their centering algorithm, Brennan et al(1987, henceforth BFP-algorithm) extend the notionof centering transition relations, which hold acrossadjacent utterances, to differentiate types of shift(cf.
Table 1 taken from Walker et al (1994)).Cb(Ui) = Cb(Ui-1) Cb(Ui)OR no Cb(Ui-1) Cb(Vi-1)CONTINUE SMOOTH-SHIFTRETAIN  ROUGH-SHIFTTable 1: Transition TypesBrennan et al (1987) modify the second of tworules on center movement and realization whichwere defined by Grosz et al (1983; 1995):Rule 1: If some element of Cf(Ui-1) is realized asa pronoun in Ui, then so is Cb(Ui).Rule 2" Transition states are ordered.
CONTINUE ispreferred to RETAIN is preferred to SMOOTH-SHIFT is preferred to ROUGH-SHIFT.The BFP-algorithm (cf.
Walker et al (1994)) con-sists of three basic steps:1.
GENERATE possible Cb-Cfcombinations.2.
FILTER by constraints, e.g., contra-indexing,sortal predicates, centering rules and con-straints.3.
RANK by transition orderings.To illustrate this algorithm, we consider example (1)(Brennan et al, 1987) which has two different finalutterances (ld) and (ld~).
Utterance (ld) containsone pronoun, utterance (ld t) two pronouns.
We lookat the interpretation of (ld) and (ldt).
After step 2,the algorithm has produced two readings for eachvariant which are rated by the corresponding tran-sitions in step 3.
In (ld), the pronoun "she" isresolved to "her" (= Brennan) because the CON-TINUE transition is ranked higher than SMOOTH-SHIFT in the second reading.
In (ld~), the pronoun"she" is resolved to "Friedman" because SMOOTH-SHIFT is preferred over ROUGH-SHIFT.
(1) a. Brennan drives an Alfa Romeo.b.
She drives too fast.c.
Friedman races her on weekends.d.
She goes to Laguna Seca.d.'
She often beats her.3 An Alternative to Centering3.1 The ModelThe realization and the structure of my model de-parts significantly from the centering model:?
The model consists of one construct with oneoperation: the list of salient discourse ntities(S-list) with an insertion operation.?
The S-list describes the attentional state of thehearer at any given point in processing a dis-course.?
The S-list contains some (not necessarily all)discourse ntities which are realized in the cur-rent and the previous utterance.?
The elements of the S-list are ranked accordingto their information status.
The order amongthe elements provides directly the preferencefor the interpretation of anaphoric expressions.In contrast o the centering model, my model doesnot need a construct which looks back; it does notneed transitions and transition ranking criteria.
In-stead of using the Cb to account for local coherence,in my model this is achieved by comparing the firstelement of the S-list with the preceding state.3.2 S-List RankingStrube & Hahn (1996) rank the Cfaccording to theinformation status of discourse ntities.
I here gen-eralize these ranking criteria by redefining them inPrince's (1981; 1992) terms.
I distinguish betweenthree different sets of expressions, hearer-old dis-course entities (OLD), mediated iscourse entities(MED), and hearer-new discourse ntities (NEW).These sets consist of the elements of Prince's fa-miliarity scale (Prince, 1981, p.245).
OLD con-sists of evoked (E) and unused (U) discourse ntitieswhile NEW consists of brand-new (BN) discourseentities.
MED consists of inferrables (I), con-taining inferrables (I c) and anchored brand-new(BN A) discourse ntities.
These discourse ntitiesare discourse-new but mediated by some hearer-oMdiscourse ntity (cf.
Figure 1).
I do not assume anydifference between the elements of each set with re-spect to their information status.
E.g., evoked andunused iscourse ntities have the same informationstatus because both belong to OLD.For an operationalization f Prince's terms, I stip-ulate that evoked iscourse ntitites are co-referringexpressions (pronominal and nominal anaphora,previously mentioned proper names, relative pro-nouns, appositives).
Unused discourse ntities are1252-<Figure 1: S-list Ranking and Familiarityproper names and titles.
In texts, brand-new propernames are usually accompanied by a relative clauseor an appositive which relates them to the hearer'sknowledge.
The corresponding discourse ntity isevoked only after this elaboration.
Whenever theselinguistic devices are missing, proper names aretreated as unused I .
I restrict inferrables to the par-ticular subset defined by Hahn et al (1996).
An-chored brand-new discourse ntities require that theanchor is either evoked or unused.I assume the following conventions for the rank-ing constraints on the elements of the S-list.
The3-tuple (x, uttx, posz) denotes a discourse ntity xwhich is evoked in utterance uttz at the text posi-tion posz.
With respect o any two discourse n-tities (x, uttz,posz) and (y, utty,pOSy), uttz andutty specifying the current utterance Ui or the pre-ceding utterance U/_ 1, I set up the following order-ing constraints on elements in the S-list (Table 2) 2 .For any state of the processor/hearer, the orderingof discourse ntities in the S-list that can be derivedfrom the ordering constraints (1) to (3) is denotedby the precedence r lation --<.
(I) If x E OLD and y E MED, then x -~ y.I fx  E OLD and y E NEW, then x -< y.l f x  E MED and y E NEW, then x -< V.(2) If x, y E OLD, or x, v E MED, or x, y E NEW,then if uttx >- utt~, then x -< y,if uttz = utt~ and pos~ < pos~, then x -< y.Table 2: Ranking Constraints on the S-listSummarizing Table 2, I state the following pref-erence ranking for discourse ntities in Ui and Ui-l:hearer-oM discourse ntities in Ui, hearer-old is-course entities in Ui-1, mediated iscourse ntitiesin Ui, mediated iscourse ntities in Ui-1, hearer-new discourse ntities in Ui, hearer-new discourseentities in Ui-1.
By making the distinction in (2)~For examples of brand-new proper names and their intro-duction cf., e.g., the "obituaries" section of the New York Times.2The relations >- and = indicate that the utterance containingx follows (>-) the utterance containing y or that x and y areelements of the same utterance (=).between discourse ntities in Ui and discourse nti-ties in Ui-1, I am able to deal with intra-sententialanaphora.
There is no need for further specificationsfor complex sentences.
A finer grained ordering isachieved by ranking discourse ntities within eachof the sets according to their text position.3.3 The AlgorithmAnaphora resolution is performed with a simplelook-up in the S-list 3.
The elements of the S-list aretested in the given order until one test succeeds.
Justafter an anaphoric expression is resolved, the S-listis updated.
The algorithm processes a text from leftto fight (the unit of processing is the word):1.
If a referring expression is encountered,(a) if it is a pronoun, test the elements of theS-list in the given order until the test suc-ceeds4;(b) update S-list; the position of the referringexpression under consideration is deter-mined by the S-list-ranking criteria whichare used as an insertion algorithm.2.
If the analysis of utterance U 5 is finished, re-move all discourse entities from the S-list,which are not realized in U.The analysis for example (1) is given in Table 3 6.I show only these steps which are of interest for thecomputation of the S-list and the pronoun resolu-tion.
The preferences for pronouns (in bold font)are given by the S-list immediately above them.
Thepronoun "she" in (lb) is resolved to the first el-ement of the S-list.
When the pronoun "her" in(lc) is encountered, FRIEDMAN is the first elementof the S-list since FRIEDMAN is unused and in thecurrent utterance.
Because of binding restrictions,"her" cannot be resolved to FRIEDMAN but tO thesecond element, BRENNAN.
In both (ld) and (ld ~)the pronoun "she" is resolved to FRIEDMAN.3The S-list consists of referring expressions which are spec-ified for text position, agreement, sortal information, and infor-mation status.
Coordinated NPs are collected in a set.
The S-list does not contain predicative NPs, pleonastic "'it", and anyelements of direct speech enclosed in double quotes.4The test for pronominal anaphora involves checking agree-ment criteria, binding and sortal constraints.5I here define that an utterance is a sentence.61n the following Tables, discourse ntities are representedby SMALLCAPS, while the corresponding surface expressionappears on the right side of the colon.
Discourse ntitites areannotated with their information status.
An "e" indicates anelliptical NP.1253(la) Brerman drives an Alfa RomeoS: \[BRENNANu: Brennan,ALFA ROMEOBN: Alfa Romeo\](lb) She drives too fast.S: \[BRENNANE: she\](1 c) FriedmanS: \[FRIEDMANu: Friedman, BRENNANE: she\]races her on weekends.S: \[FRIEDMANu: Friedman, BRENNANE: her\](ld) She drives to Laguna Seca.S: \[FRIEDMANE: she,LAGUNA SECAu: Laguna Seca\](ld') SheS: \[FRIEDMANE: she, BRENNANE: her\]often beats her.S: \[FRIEDMANE: she, BRENNANE: her\]Table 3: Analysis for (1)(2a) Brennan drives an Alfa RomeoS: \[BRENNANu: Brennan,ALFA ROMEOBN: Alfa Romeo\](2b) She drives too fast.S: \[BRENNANE: she\](2c) A professional driverS: \[BRENNANE: she, DRIVERBN: Driver\]races her on weekends.S: \[BRENNANE: her, DRIVERBN: Driver\](2d) She drives to Laguna Seca.S: \[BRENNANE: she,LAGUNA SECAu: Laguna Seca\](2d') SheS: \[BRENNANE: she, DRIVERBN: Driver\]often beats her.S: \[BRENNANE: she, DRIVERE: her\]Table 4: Analysis for (2)The difference between my algorithm and theBFP-algorithm becomes clearer when the unuseddiscourse ntity "Friedman" is replaced by a brand-new discourse ntity, e.g., "a professional driver ''7(cf.
example (2)).
In the BFP-algorithm, the rank-ing of the Cf-list depends on grammatical roles.Hence, DRIVER is ranked higher than BRENNAN inthe Cf(2c).
In (2d), the pronoun "she" is resolvedto BRENNAN because of the preference for CON-TINUE over RETAIN.
In (2d~), "she" is resolved toDRIVER because SMOOTH-SHIFT is preferred overROUGH-SHIFT.
In my algorithm, at the end of (2c)the evoked phrase "her" is ranked higher than thebrand-new phrase "a professional driver" (cf.
Ta-ble 4).
In both (2d) and (2d ~) the pronoun "she" isresolved to BRENNAN.
(2) a. Brennan drives an Alfa Romeo.b.
She drives too fast.c.
A professional driver aces her on weekends.d.
She goes to Laguna Seca.d/ She often beats her.Example (3) 8 illustrates how the preferences forintra- and inter-sentential anaphora interact with theinformation status of discourse entitites (Table 5).Sentence (3a) starts a new discourse segment.
Thephrase "a judge" is brand-new.
"Mr. Curtis" ismentioned several times before in the text, Hence,7I owe this variant Andrew Kehler.
-This example can mis-direct readers because the phrase "'a professional driver" is as-signed the "default" gender masculine.
Anyway, this example- like the original example - seems not to be felicitous Englishand has only illustrative character.Sin: The New York Times.
Dec. 7, 1997, p.A48 ("Shot inhead, suspect goes free, then to college").the discourse ntity CURTIS is evoked and rankedhigher than the discourse entity JUDGE.
In thenext step, the ellipsis refers to JUDGE which isevoked then.
The nouns "request" and "prosecu-tors" are brand-new 9.
The pronoun "he" and thepossessive pronoun "his" are resolved to CURTIS.
"Condition" is brand-new but anchored by the pos-sessive pronoun.
For (3b) and (3c) I show onlythe steps immediately before the pronouns are re-solved.
In (3b) both "Mr. Curtis" and "the judge"are evoked.
However, "Mr. Curtis" is the left-mostevoked phrase in this sentence and therefore themost preferred antecedent for the pronoun "him".For my experiments I restricted the length of theS-list to five elements.
Therefore "prosecutors" in(3b) is not contained in the S-list.
The discourseentity SMIRGA is introduced in (3c).
It becomesevoked after the appositive.
Hence SM1RGA is themost preferred antecedent for the pronoun "he".
(3) a.
A judge ordered that Mr. Curtis be released, bute agreed with a request from prosecutors that hebe re-examined ach year to see if his conditionhas improved.b.
But authorities lost contact with Mr. Curtis afterthe Connecticut Supreme Court ruled in 1990that the judge had erred, and that prosecutorshad no right to re-examine him.c.
John Smirga, the assistant state's attorney incharge of the original case, said last week thathe always had doubts about he psychiatric re-ports that said Mr. Curtis would never improve.9I restrict inferrables tothe cases pecified by Hahn et al(1996).
Therefore "prosecutors" is brand-new (cf.
Prince(1992) for a discussion of the form of inferrables).1254(3a) A judgeS: \[JUDGEBN: judge\]ordered that Mr. CurtisS: \[CURTISE: Mr. Curtis, JUDGEBN: judge\]be released, but eS: \[CURTISE: Mr. Curtis, JUDGEE: e\]agreed with a requestS: \[CURTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request\]from prosecutorsS: \[CURTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request, PROSECUTORSBN: prosecutors\]that heS: \[CURTISE: he, JUDGEE: e, REQUESTBN: request, PROSECUTORSBN: prosecutors\]be re-examined ach yearS: \[CURTISE: he, JUDGEE: ~, REQUESTBN: request, PROSECUTORSBN: prosecutors, YEARBN: year\]to see if hisS: \[CURTISE: his, JUDGEE: ~, REQUESTBN: request, PROSECUTORSBN: prosecutors, YEARBN: year\]conditionS: \[CURTISE: his, JUDGEE: e, CONDITIONBNA : condition, REQUESTBN: request, PROSECUTORSBN: prosec.\]has improved.S: \[CURTISE: his, JUDGEE: e, CONDITIONBNA: condition, REQUESTBN: request, PROSECUTORSBN: prosec.\](3b) But authorities lost contact with Mr. Curtis after the Connecticut Supreme Court ruled in 1990 that the judge haderred, and that prosecutors had no rightS: \[CURTISE: his, CS COURTu: CS Court, JUDGEE: judge, CONDITIONBNA: condition, AUTH.BN: auth.\]to re-examine him.S: \[CURTISE: him, CS COURTu: CS Court, JUDGEE: judge, CONDITIONBNA: condition, AUTH.BN: auth.\](3c) John Smirga, the assistant state's attorney in charge of the original case, said last weekS: \[SMIRGAE: attorney, CASEE: case, CURTISE: him, CS COURTu: CS Court, JUDGEE: judge \]that he had doubts about he psychiatric reports that said Mr. Curtis would never improve.S: \[SMIRGAE: he, CASEE: case, REPORTSE: reports, CURTISE: Mr. Curtis, DOUBTSBN: doubts\]Table 5: Analysis for (3)4 Some Empirical Dat:iIn the first experiment, I compare my algorithm withthe BFP-algorithm which was in a second experi-ment extended by the constraints for complex sen-tences as described by Kameyama (1998).Method.
I use the following guidelines for thehand-simulated analysis (Walker, 1989).
I do not as-sume any world knowledge as part of the anaphoraresolution process.
Only agreement criteria, bind-ing and sortal constraints are applied.
I do not ac-count for false positives and error chains.
FollowingWalker (1989), a segment is defined as a paragraphunless its first sentence has a pronoun in subject po-sition or a pronoun where none of the precedingsentence-internal oun phrases matches its syntacticfeatures.
At the beginning of a segment, anaphoraresolution is preferentially performed within thesame utterance.
My algorithm starts with an emptyS-list at the beginning of a segment.The basic unit for which the centering data struc-tures are generated is the utterance U.
For the BFP-algorithm, I define U as a simple sentence, a com-plex sentence, or each full clause of a compoundsentence.
Kameyama's (1998) intra-sentential cen-tering operates at the clause level.
While tensedclauses are defined as utterances on their own, un-tensed clauses are processed with the main clause,so that the Cf-list of the main clause containsthe elements of the untensed embedded clause.Kameyama distinguishes for tensed clauses furtherbetween sequential and hierarchical centering.
Ex-cept for reported speech (embedded and inaccessi-ble to the superordinate l vel), non-report comple-ments, and relative clauses (both embedded but ac-cessible to the superordinate l vel; less salient hanthe higher levels), all other types of tensed clausesbuild a chain of utterances on the same level.According to the preference for inter-sententialcandidates in the centering model, I define the fol-lowing anaphora resolution strategy for the BFP-algorithm: (1) Test elements of Ui-1.
(2) Test el-ements of Ui left-to-right.
(3) Test elements ofCf(Ui-2), Cf(Ui-3) ....
In my algorithm steps (1)and (2) fall together.
(3) is performed using previ-ous states of the system.Results.
The test set consisted of the beginningsof three short stories by Hemingway (2785 words,153 sentences) and three articles from the NewYork Times (4546 words, 233 sentences).
The re-suits of my experiments are given in Table 6.
The1255first row gives the number of personal and posses-sive pronouns.
The remainder of the Table showsthe results for the BFP-algorithm, for the BFP-algorithm extended by Kameyama's intra-sententialspecifications, and for my algorithm.
The overallerror rate of each approach is given in the rowsmarked with wrong.
The rows marked with wrong(strat.)
give the numbers of errors directly producedby the algorithms' strategy, the rows marked withwrong (ambig.)
the number of analyses with am-biguities generated by the BFP-algorithm (my ap-proach does not generate ambiguities).
The rowsmarked with wrong (intra) give the number of er-rors caused by (missing) specifications for intra-sentential anaphora.
Since my algorithm integratesthe specifications for intra-sentential naphora, Icount these errors as strategic errors.
The rowsmarked with wrong (chain) give the numbers of er-rors contained in error chains.
The rows markedwith wrong (other) give the numbers of the remain-ing errors (consisting of pronouns with split an-tecedents, errors because of segment boundaries,and missing specifications for event anaphora).Hem.
NYTPron.
and Poss.
Pron.
274 302BFP-Algo.BFP/Kam.My Algo.CorrectWrongWrong (strat.
)Wrong (ambig.
)Wrong (intra)Wrong (chain)Wrong (other)CorrectWrongWrong (strat.
)Wrong (ambig.
)Wrong (intra)Wrong (chain)Wrong (other)CorrectWrongWrong (strat.
)Wrong (chain)Wrong (other)189 23185 7114 29 1517 1329 3216 919381245573 017 817 2729 1515 7217572752721 1222 914 6576420156162430612543813832544442249284333120Table 6: Evaluation ResultsInterpretation.
The results of my experimentsshowed not only that my algorithm performed bet-ter than the centering approaches but also revealedinsight in the interaction between inter- and intra-sentential preferences for anaphoric antecedents.Kameyama's pecifications reduce the complexityin that the Cf-lists in general are shorter after split-ting up a sentence into clauses.
Therefore, theBFP-algorithm combined with her specificationshas almost no strategic errors while the number ofambiguities remains constant.
But this benefit isachieved at the expense of more errors caused by theintra-sentential specifications.
These errors occur incases like example (3), in which Kameyama's intra-sentential strategy makes the correct antecedent lesssalient, indicating that a clause-based approach istoo fine-grained and that the hierarchical syntacticalstructure as assumed by Kameyama does not have agreat impact on anaphora resolution.I noted, too, that the BFP-algorithm can gener-ate ambiguous readings for Ui when the pronounin Ui does not co-specify the Cb(Ui-1).
In cases,where the Cf(Ui-1) contains more than one possi-ble antecedent for the pronoun, several ambiguousreadings with the same transitions are generated.An examplel?
: There is no Cb(4a) because no ele-ment of the preceding utterance is realized in (4a).The pronoun "them" in (4b) co-specifies "deer" butthe BFP-algorithm generates two readings both ofwhich are marked by a RETAIN transition.
(4) a. Jim pulled the burlap sacks off the deerb.
and Liz looked at them.In general, the strength of the centering model isthat it is possible to use the Cb(Ui-t) as the mostpreferred antecedent for a pronoun in Ui.
In mymodel this effect is achieved by the preference forhearer-old iscourse ntities.
Whenever this prefer-ence is misleading both approaches give wrong re-sults.
Since the Cb is defined strictly local whilehearer-old iscourse ntities are defined global, mymodel produces less errors.
In my model the pref-erence is available immediately while the BFP-algorithm can use its preference not before the sec-ond utterance has been processed.
The more globaldefinition of hearer-old iscourse ntities leads alsoto shorter error chains.
- However, the test set istoo small to draw final conclusions, but at least forthe texts analyzed the preference for hearer-old is-course entities is more appropriate than the prefer-ence given by the BFP- algorithm.5 Comparison to Related ApproachesKameyama's (1998) version of centering also omitsthe centering transitions.
But she uses the Cb anda ranking over simplified transitions preventing theincremental pplication of her model.l?In: Emest Hemingway.
Up in Michigan.
ln.
The Com-plete Short Stories of Ernest Hemingway.
New York: CharlesScribner's Sons, 1987, p.60.1256The focus model (Sidner, 1983; Suri & McCoy,1994) accounts for evoked iscourse entities explic-itly because it uses the discourse focus, which is de-termined by a successful anaphora resolution.
In-cremental processing is not a topic of these papers.Even models which use salience measures for de-termining the antecedents of pronoun use the con-cept of evoked discourse entities.
Haji~ov~i et al(1992) assign the highest value to an evoked dis-course entity.
Also Lappin & Leass (1994), whogive the subject of the current sentence the high-est weight, have an implicit notion of evokedness.The salience weight degrades from one sentence toanother by a factor of two which implies that a re-peatedly mentioned discourse entity gets a higherweight than a brand-new subject.6 Conc lus ionsIn this paper, I proposed a model for determiningthe hearer's attentional state which is based on thedistinction between hearer-old and hearer-new dis-course entities.
I showed that my model, thoughit omits the backward-looking center and the cen-tering transitions, does not lose any of the predic-tive power of the centering model with respect toanaphora resolution.
In contrast to the centeringmodel, my model includes a treatment for intra-sentential anaphora and is sufficiently well specifiedto be applied to real texts.
Its incremental characterseems to be an answer to the question Kehler (1997)recently raised.
Furthermore, it neither has the prob-lem of inconsistency Kehler mentioned with respectto the BFP-algorithm nor does it generate unneces-sary ambiguities.Future work will address whether the text posi-tion, which is the weakest grammatical concept, issufficient for the order of the elements of the S-listat the second layer of my ranking constraints.
I willalso try to extend my model for the analysis of def-inite noun phrases for which it is necessary to inte-grate it into a more global model of discourse pro-cessing.Acknowledgments: This work has been fundedby a post-doctoral grant from DFG (Str 545/1-1)and is supported by a post-doctoral fellowshipaward from IRCS.
I would like to thank Nobo Ko-magata, Rashmi Prasad, and Matthew Stone whocommented on earlier drafts of this paper.
I amgrateful for valuable comments by Barbara Grosz,Udo Hahn, Aravind Joshi, Lauri Karttunen, AndrewKehler, Ellen Prince, and Bonnie Webber.ReferencesBrennan, S. E., M. W. Friedman & C. J. Pollard (1987).
A cen-tering approach to pronouns.
In Proc.
of the 25 th AnnualMeeting of the Association for Computational Linguis-tics; Stanford, Cal., 6-9 July 1987, pp.
155-162.Grosz, B. J., A. K. Joshi & S. Weinstein (1983).
Providinga unified account of definite noun phrases in discourse.In Proc.
of the 21 st Annual Meeting of the Associationfor Computational Linguistics; Cambridge, Mass., 15-17June 1983, pp.
44-50.Grosz, B. J., A. K. Joshi & S. Weinstein (1995).
Centering:A framework for modeling the local coherence of dis-course.
Computational Linguistics, 21 (2):203-225.Hahn, U., K. Markert & M. Strube (1996).
A conceptual rea-soning approach to textual ellipsis.
In Proc.
of the 12 thEuropean Conference on Artificial h~telligence (ECAI'96); Budapest, Hungary, 12-16 August 1996, pp.
572-576.
Chichester: John Wiley.Haji~ov~i, E., V. Kubofi & P. Kubofi (1992).
Stock of sharedknowledge: A tool for solving pronominal anaphora.
InProc.
of the 14 th h~t.
Conference on Computational Lin-guistics; Nantes, France, 23-28 August 1992, Vol.
1, pp.127-133.Kameyama, M. (1998).
Intrasentential centering: A case study.In M. Walker, A. Joshi & E. Prince (Eds.
), CenteringTheory in Discourse, pp.
89-112.
Oxford, U.K.: OxfordUniv.
Pr.Kehler, A.
(1997).
Current theories of centering for pronouninterpretation: A critical evaluation.
Computational Lin-guistics, 23(3):467-475.Lappin, S. & H. J. Leass (1994).
An algorithm for pronom-inal anaphora resolution.
Computational Linguistics,20(4):535-56 I.Prince, E. E (1981).
Toward a taxonomy of given-new informa-tion.
In E Cole (Ed.
), Radical Pragmatics, pp.
223-255.New York, N.Y.: Academic Press.Prince, E. E (1992).
The ZPG letter: Subjects, definiteness, andinformation-status.
In W. Mann & S. Thompson (Eds.
),Discourse Description.
Diverse Linguistic Analyses of aFund-Raisbzg Text, pp.
295-325.
Amsterdam: John Ben-jamins.Sidner, C. L. (1983).
Focusing in the comprehension of definiteanaphora.
In M. Brady & R. Berwick (Eds.
), Con,pu-tational Models of Discourse, pp.
267-330.
Cambridge,Mass.
: MIT Press.Strube, M. & U. Hahn (1996).
Functional centering.
In Proc.
ofthe 34 th Annual Meeting of the Association for Compu-tational Linguistics; Santa Cruz, Cal., 23-28 June 1996,pp.
270-277.Suri, L. Z.
& K. E McCoy (1994).
RAFT/RAPR and centering:A comparison and discussion of problems related to pro-cessing complex sentences.
Computational Linguistics,20(2):301-317.Walker, M. A.
(1989).
Evaluating discourse processing algo-rithms.
In Proc.
of the 27 th Annual Meeting of the Asso-ciation for Computational Linguistics; Vancouver, B.C.,Canada, 26-29 June 1989, pp.
251-261.Walker, M. A., M. lida & S. Cote (1994).
Japanese discourseand the process of centering.
Computational Linguistics,20(2): 193-233.1257
