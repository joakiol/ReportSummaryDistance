Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481?1491,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOvercoming the Lack of Parallel Data in Sentence CompressionKatja Filippova and Yasemin AltunGoogleBrandschenkestr.
110Zu?rich, 8004 Switzerlandkatjaf|altun@google.comAbstractA major challenge in supervised sentencecompression is making use of rich feature rep-resentations because of very scarce paralleldata.
We address this problem and presenta method to automatically build a compres-sion corpus with hundreds of thousands ofinstances on which deletion-based algorithmscan be trained.
In our corpus, the syntactictrees of the compressions are subtrees of theiruncompressed counterparts, and hence super-vised systems which require a structural align-ment between the input and output can be suc-cessfully trained.
We also extend an exist-ing unsupervised compression method with alearning module.
The new system uses struc-tured prediction to learn from lexical, syntac-tic and other features.
An evaluation with hu-man raters shows that the presented data har-vesting method indeed produces a parallel cor-pus of high quality.
Also, the supervised sys-tem trained on this corpus gets high scoresboth from human raters and in an automaticevaluation setting, significantly outperforminga strong baseline.1 Introduction and related workSentence compression is a paraphrasing task wherethe goal is to generate sentences shorter than givenwhile preserving the essential content.
A robustcompression system would be useful for mobile de-vices as well as a module in an extractive sum-marization system (Mani, 2001).
Although a com-pression may differ lexically and structurally fromthe source sentence, to date most systems are ex-tractive and proceed by deleting words from theinput (Knight & Marcu, 2000; Dorr et al 2003;Turner & Charniak, 2005; Clarke & Lapata, 2008;Berg-Kirkpatrick et al 2011, inter alia).
To de-cide which words, dependencies or phrases can bedropped, (i) rule-based approaches (Grefenstette,1998; Jing & McKeown, 2000; Dorr et al 2003;Zajic et al 2007), (ii) supervised models trainedon parallel data (Knight & Marcu, 2000; Turner &Charniak, 2005; McDonald, 2006; Gillick & Favre,2009; Galanis & Androutsopoulos, 2010, inter alia)and (iii) unsupervised methods which make use ofstatistics collected from non-parallel data (Hori &Furui, 2004; Zajic et al 2007; Clarke & Lapata,2008; Filippova & Strube, 2008) have been investi-gated.
Since it is infeasible to manually devise a setof accurate deletion rules with high coverage, recentresearch has been devoted to developing statisticalmethods and possibly augmenting them with a fewlinguistic rules to improve output readability (Clarke& Lapata, 2008; Nomoto, 2009).Supervised models.
A major problem for super-vised deletion-based systems is very limited amountof parallel data.
Many approaches make use of asmall portion of the Ziff-Davis corpus which hasabout 1K sentence-compression pairs1.
Other mainsources of training data are the two manually craftedcompression corpora from the University of Edin-burgh (?written?
and ?spoken?, each approx.
1.4Kpairs).
Galanis & Androutsopoulos (2011) attemptat getting more parallel data by applying a deletion-based compressor together with an automatic para-1The method of Galley & McKeown (2007) could benefitfrom a larger number of sentences.1481phraser and generating multiple alternative com-pressions.
To our knowledge, this extended data sethas not yet been used for successful training of com-pression systems.Scarce parallel data makes it hard to go beyond asmall set of features and explore lexicalization.
Forexample, Knight & Marcu (2000) only induce non-lexicalized CFG rules, many of which occurred onlyonce in the training data.
The features of McDon-ald (2006) are formulated exclusively in terms ofsyntactic categories.
Berg-Kirkpatrick et al(2011)have as few as 13 features to decide whether a con-stituent can be dropped.
Galanis & Androutsopou-los (2010) use many features when deciding whichbranches of the input dependency tree can be prunedbut require a reranker to select most fluent com-pressions from a pool of candidates generated in thepruning phase, many of which are ungrammatical.Even further data limitations exist for the algo-rithms which operate on syntactic trees and refor-mulate the compression task as a tree pruning one(Nomoto, 2008; Filippova & Strube, 2008; Cohn &Lapata, 2009; Galanis & Androutsopoulos, 2010, in-ter alia).
These methods are sensitive to alignmenterrors, their performance degrades if the syntacticstructure of the compression is very different fromthat of the input.
For example, see Nomoto?s 2009analysis of the poor performance of the T3 system ofCohn & Lapata (2009) when retrained on a corpus ofloosely similar RSS feeds and news.Unsupervised models.
Few approaches requireno training data at all.
The model of Hori & Fu-rui (2004) combines scores estimated from mono-lingual corpora to generate compressions of tran-scribed speech.
Adopting an integer linear program-ming (ILP) framework, Clarke & Lapata (2008) usehand-crafted syntactic constraints and an ngram lan-guage model, trained on uncompressed sentences, tofind best compressions.
The model of Filippova &Strube (2008) also uses ILP but the problem is for-mulated over dependencies and not ngrams.
Condi-tional probabilities and word counts collected froma large treebank are combined in an ad hoc man-ner to assess grammatical importance and informa-tiveness of dependencies.
Similarly, Woodsend &Lapata (2010) formulate an ILP problem to gener-ate news story highlights using precomputed scores.Again, an ad hoc combination of the scores learnedindependently of the task is used in the objectivefunction.Contributions of this paper.
Our work is moti-vated by the obvious need for a large parallel corpusof sentences and compressions on which extractivesystems can be trained.
Furthermore, we want thecompressions in the corpus to be structurally veryclose to the input.
Ideally, in every pair, the com-pression should correspond to a subtree of the input.To this end, our contributions are three-fold:?
We describe an automatic procedure of con-structing a parallel corpus of 250,000 sentence-compression pairs such that the dependencytree of the compression is a subtree of thesource tree.
An evaluation with human ratersdemonstrates high quality of the parallel datain terms of readability and informativeness.?
We successfully apply the acquired data to traina novel supervised compression system whichproduces readable and informative compres-sions without employing a separate reranker.In particular, we start with the unsupervisedmethod of Filippova & Strube (2008) and re-place the ad hoc edge weighting with a lin-ear function over a rich feature representation.The parameter vector is learned from our cor-pus specifically for the compression task us-ing structured prediction (Collins, 2002).
Thenew system significantly outperforms the base-line and hence provides further evidence for theutility of the parallel data.?
We demonstrate that sparse lexical features arevery useful for sentence compression, and thata large parallel corpus is a requirement for ap-plying them successfully.The compression framework we adopt and the un-supervised baseline are introduced in Section 2, thetraining algorithm for learning edge weights fromparallel data is described in Section 3.
In Section4 we explain how to obtain the data and present anevaluation of its quality.
In Section 5 we comparethe baseline with our system and report the resultsof an experiment with humans as well as the resultsof an automatic evaluation.14822 Framework and baselineWe adopt the unsupervised compression frameworkof Filippova & Strube (2008) as our baseline and ex-tend it to a supervised structured prediction problem.In the experiments reported by Filippova & Strube(2008), the system was evaluated on the Edinburghcorpora.
It achieved an F-score (Riezler et al 2003)higher than reported by other systems on the samedata under an aggressive compression rate and thuspresents a competitive baseline.Tree pruning as optimization.
In this framework,compressions are obtained by deleting edges of thesource dependency structure so that (1) the retainededges form a valid syntactic tree, and (2) their to-tal edge weight is maximized.
The objective func-tion is defined over set X = {xe, e ?
E} of bi-nary variables, corresponding to the set E of thesource edges, subject to the structural and lengthconstraints,f(X) =?e?Exe ?
w(e) (1)Here, w(e) denotes the weight of edge e. This con-strained optimization problem is solved under thetree structure and length constraints using ILP.
If xeis resolved to 1, the respective edge is retained, oth-erwise it is deleted.
The tree structure constraints en-force at most one parent for every node and structureconnectivity (i.e., no disconnected subtrees).
Giventhat length(node(e)) denotes the length of the nodeto which edge e points and ?
is the maximum per-mitted length for the compression, the length con-straint is simply?e?Exe ?
length(node(e)) ?
?
(2)Word limit is used in the original paper, whereas weuse character length which is more appropriate forsystem comparisons (Napoles et al 2011).
If uni-form weights are used in Eq.
(1), the optimal so-lution would correspond to a subtree covering asmany edges as possible while keeping the compres-sion length under given limit.The solution to the surface realization problem(Belz et al 2011) is standard: the words in the com-pression subtree are put in the same order they arefound in the source.Due to space limitations, we refer the reader to(Filippova & Strube, 2008) for a detailed descrip-tion on the method.
Essential for the present discus-sion is that source dependency trees are transformedto dependency graphs in that (1) auxiliary, deter-miner, preposition, negation and possessive nodesare collapsed with their heads; (2) prepositions re-place labels on the edges to their arguments; (3) thedummy root node is connected with every inflectedverb.
Figures 1(a)-1(b) illustrate most of the trans-formations.
The transformations are deterministicand reversible, they can be implemented in a singletop-down tree traversal2.The set E of edges in Eq.
(1) is thus the set ofedges of the transformed dependency graph, like inFig.
1(b).
A benefit of the transformations is thatfunction words and negation appear in the compres-sion if and only if their head words are present.Hence no separate constraints are required to en-sure that negation or a determiner is preserved.
Thedummy root node makes constraint formulation eas-ier and also allows for the generation of compres-sions from any finite clause of the source.The described pruning optimization frameworkis used both for the unsupervised baseline and forour supervised system.
The difference between thebaseline and our system is in how edge weights,w(e)?s in Eq.
(1), are instantiated.Baseline edge weights.
The precomputed edgeweights reflect syntactic importance as well as infor-mativeness of the nodes they point to.
Given edgee from head node h to node n, the edge weight isthe product of the syntactic and the informativenessweights,w(e) = wsynt(e)?
winfo(e) (3)The syntactic weight is defined aswsynt(e) = P (label(e)|lemma(h)) (4)For example, verb kill may have multiple argu-ments realized with dependency labels subj, dobj, in,etc.
However, these argument labels are not equallylikely, e.g., P (subj|kill) > P (in|kill).
When forcedto prune an edge, the system would prefer to keep2Some of the transformations are comparable to what is im-plemented in the Stanford parser (de Marneffe et al 2006).1483Britain ?s Ministry of Defense says a British soldier was killed in a roadside blast in southern Afghanistanpspossprep pobjnsubjamoddetauxpassnsubjccomppreppobjamoddetpreppobjamodroot(a) Source dependency treeroot Britain?s Ministry of Defense says British a soldier was killed roadside in a blast southern in Afghanistanofsubjrootrootccompsubjamodinamodinamod(b) Transformed graphroot British a soldier was killed in a blast in Afghanistansubjrootamod in in(c) Tree of extracted headline A British soldier was killed in a blast inAfghanistanA British soldier was killed in a blast in Afghanistandetamodsubjauxpass preppobjdet prep pobjroot(d) Tree of extracted headline with transformations undoneFigure 1: Source, transformed and extracted trees given headline British soldier killed in Afghanistanthe subject edge over the preposition-in edge since itcontributes more weight to the objective function.The informativeness score is inspired by Wood-send & Lapata (2012) and is defined aswinfo(e) =Pheadline(lemma(n))Particle(lemma(n))(5)This weight tells us how likely it is that a wordfrom an article appears in the headline.
For exam-ple, given two edges one of which points to verb sayand another one to verb kill, the latter would be pre-ferred over the former because kill is more ?head-liny?
than say.
When collecting counts for the syn-tactic and informativeness scores, we used 9M newsarticles crawled from the Internet, much more thanFilippova & Strube (2008).
As a result our estimatesare probably more accurate than theirs.Although both wsynt and winfo have a meaning-ful interpretation, there is no guarantee that productis the best way to combine the two when assign-ing edge weights.
Also, it is unclear how to inte-grate other signals, such as distance to the root, nodelength or information about the siblings, which pre-sumably all play a role in determining the overalledge importance.3 Learning edge weightsOur supervised system differs from the unsupervisedbaseline in that instead of relying on precomputedscores, we define edge weight w(e) in Eq.
(1) with alinear function over a feature representation,w(e) = w ?
f(e) (6)Here f(e) is a vector of binary variables for everyfeature from the set of all possible but very infre-quent features in the training set.
f(e) has 1 for everyfeature extracted for edge e and zero otherwise.Table 1 gives an overview of the feature typeswe use (edge e points from head h to node n).Note that syntactic, structural and semantic featuresare closed-class.
For all the structural features butchar length, seven is used as maximum possiblevalue; all possible character lengths are bucketedinto six classes.
All the features are local ?
for agiven edge, contextual information is included about1484syntactic label(e); for e* to h, label(e*); pos(h); pos(n)structural depth(n); #children(n); #children(h); char length(n); #words in(n)semantic NE tag(h); NE tag(n); is negated(n)lexical lemma(n); lemma(h)-label(e); for e* to n?s siblings, lemma(h)-label(e*)Table 1: Types of features extracted for edge e from h to nthe head and the target nodes, and the siblings aswell as the children of the latter.
The negation fea-ture is only applicable to verb nodes which containa negative particle, like not, after the tree transfor-mations.
Lexical features which combine lemmasand syntactic labels are inspired by the unsupervisedbaseline and are very sparse.In what follows, our assumption is that we have acompression corpus at our disposal where for everyinput sentence there is a correct ?oracle?
compres-sion such that its transformed parse tree matches asubtree of the transformed input graph.
Given sucha corpus, we can apply structured prediction meth-ods to learn the parameter vector w. In our studywe employ an averaged variant of online structuredperceptron (Collins, 2002).
In the context of sen-tence fusion, a similar dependency structure prun-ing framework and a similar learning approach wasadopted by Elsner & Santhanam (2011).At every iteration, for every input graph, we findthe optimal solution with ILP under the current pa-rameter vector w. The maximum permitted com-pression length is set to be the same as the lengthof the oracle compression.
Since the oracle com-pression is a subtree of the input graph, it representsa feasible solution for ILP.
The parameter vector isupdated if there is a mismatch between the predictedand the oracle sets of edges for all the features witha non-zero net count.
More formally, given an inputgraph with the set of edges E, oracle compressionC ?
E and compression Ct ?
E predicted at itera-tion t , the parameter update vector at t+ 1 is givenbywt+1 = wt +?e?C\Ctf(e)?
?e?Ct\Cf(e) (7)w is averaged over all the wt?s so that featureswhose weight fluctuated a lot during training are pe-nalized (Freund & Shapire, 1999).Of course, training a model with a large numberof features, such as a lexicalized model, is only pos-sible if there is a large compression corpus wherethe dependency tree of the compression is a subtreeof the source sentence.
In the next section we in-troduce our method of getting a sufficient amount ofsuch data.4 Acquiring parallel data automaticallyIn this section we explain how we obtained a parallelcorpus of sentences and compressions.
The underly-ing idea is to harvest news articles from the Internetwhere the headline appears to be similar to the firstsentence and use it to find an extractive compressionof the sentence.Collecting headline-sentence pairs.
Using anews crawler, we collected a corpus of news arti-cles in English from the Internet.
Similarly to previ-ous work (Dolan et al 2004; Wubben et al 2009;Bejan & Harabagiu, 2010, inter alia), the GoogleNews service3 was used to identify news.
From ev-ery article, the headline and the first sentence, whichare known to be semantically similar (Dorr et al2003), were extracted.
Predictably, very few head-lines are extractive compressions of the first sen-tence, therefore simply looking for pairs where theheadline is a subsequence of the words from the firstsentence would not solve the problem of getting alarge amount of parallel data.
Importantly, headlinesare syntactically quite different from ?normal?
sen-tences.
For example, they may have no main verb,omit determiners and appear incomplete, making ithard for a supervised deletion-based system to learnuseful rules.
Moreover, we observed poor parsingaccuracy for headlines which would make syntacticannotations for headlines hardly useful.Thus, instead of taking the headline as it is, we useit to find a proper extractive compression of the sen-3http://news.google.com, Jan-Dec 2012.1485tence by matching lemmas of content words (nouns,verbs, adjectives, adverbs) and coreference IDs ofentities from the headline with those of the sentence.The exact procedure is as follows (H, S and T standfor headline, sentence and transformed graph of thesentence):PREPROCESSING H and S are preprocessed in astandard way: tokenized, lemmatized, PoS and NEtagged.
Additionally, S is parsed with a dependencyparser (Nivre, 2006) and transformed as described inSection 2 to obtain T. Finally, pronominal anaphorais resolved in S. Recall that S is the first sentence,so the antecedent must be located in a preceding,higher-level clause.FILTERING To restrict the corpus to grammaticaland informative headlines, we implemented a cas-cade of filters.
Pair (H, S) is discarded if any of thequestions in Table 2 is answered positively.Is H a question?Is H or S too short?
(less than four word tokens)Is H about as long as S?
(min ratio: 1.5)Does H lack a verb?Does H begin with a verb?Is there a noun, verb, adj, adv lemma from Hnot found in S?Are the noun, verb, adj, adv lemmas from Hfound in S in a different order?Table 2: Filters applied to candidate pair (H, S)MATCHING Given the content words of H, a sub-set of nodes in T is selected based on lemma orcoreference identity of the main (head) word in thenodes.
For example, the main word of a collapsednode in T, which covers two words was killed, iskilled; was is its child attached with label aux in theuntransformed parse tree.
This node is marked if Hcontains word killed or killing because of the lemmaidentity.
In some cases there are multiple possiblematches.
For example, given S Barack Obama saidhe will attend G20 and H mentioning Obama, bothBarack Obama and he nodes are marked in T. Onceall the nodes in T which match content words andentities from H are identified, a minimum subtreecovering these nodes is found such that every wordor entity from H occurs as many times in T as inH.
So if H mentions Obama only once, then eitherBarack Obama or he must be covered by the subtreebut not both.
This minimum subtree corresponds toan extractive headline, H*, which we generate byordering the surface forms of all the words in thesubtree nodes by their offsets in S. Finally, the char-acter length of H* is compared with the length ofH.
If H* is much longer than H, the pair (H, S) isdiscarded (max ratio 1.5).As an illustration to the procedure, consider theexample from Figure 1 with the extracted headlineand its tree presented in Figure 1(c).
Given theheadline British soldier killed in Afghanistan, theextracted headline would be A British soldier waskilled in a blast in Afghanistan.
The lemmas british,soldier, kill, afghanistan from the headline match thenodes British, a soldier, was killed, in Afghanistanin the transformed graph.
The node in a blast isadded because it is on the path from was killed to inAfghanistan.
Of course, it is possible to determinis-tically undo the transformations in order to obtain astandard dependency tree.
In this case the extractedheadline would still correspond to a subtree of theinput (compare Fig.
1(d) with Fig.
1(a)).
Also notethat a similar procedure can be implemented for con-stituency parses.The resulting corpus consists of 250K tuples (S,T, H, H*), Appendix provides more examples ofsource sentences, original headlines and extractedheadlines.
We did not attempt to tune the values forminimum/maximum length and ratio ?
lower thresh-olds may have produced comparable results.Evaluating data quality.
The described proce-dure produces a comparatively large compressioncorpus but how good are automatically constructedcompressions?
To answer this question, we ran-domly selected 50 tuples from the corpus and set upan experiment with human raters to validate and as-sess data quality in terms of readability4 and infor-mativeness5 which are standard measures of com-pression quality (Clarke & Lapata, 2006).
Raterswere asked to read a sentence and a compression(original H or extracted H* headline) and then ratethe compression on two five-point scales.
Three rat-ings were collected for every item.
Table 3 gives4Also called grammaticality and fluency.5Also called importance and representativeness.1486average ratings with standard deviation.AVG read AVG infoORIG.
HEADLINE 4.36 (0.75) 3.86 (0.79)EXTR.
HEADLINE 4.26 (1.01) 3.70 (1.04)Table 3: Results for two kinds of headlinesIn terms of readability and informativeness theextracted headlines are comparable with human-written ones: at 95% confidence there is no statis-tically significant difference between the two.Encouraged by the results of the validation exper-iment we proceeded to our next question: Can a su-pervised compression system be successfully trainedon this corpus?5 System evaluation and discussionFrom the corpus of 250K tuples we used 100K toget pairs of extracted headlines and sentences fortraining (on the development set we did not observemuch improvement from using more training data),250 for development and the rest for testing.
Weran the learning algorithm for 20 iterations, checkingthe performance on the development set.
Featureswhich applied to less than 20 edges were pruned,the size of the feature set is about 28K.5.1 Evaluation with humans50 pairs of original headlines and sentences (differ-ent from the data validation set in Sec.
4) were ran-domly selected for an evaluation with humans fromthe test data.
As in the data quality validation ex-periment, we asked raters to assess the readabilityand informativeness of proposed compressions forthe unsupervised system, our system and human-written headlines.
The latter provide us with upperbounds on the evaluation criteria.
Three ratings peritem per parameter were collected.
To get compara-ble results, the unsupervised and our systems usedthe same compression rate: for both, the requestedmaximum length was set to the length of the head-line.
Table 4 summarizes the results.The results indicate that the trained model signifi-cantly outperforms the unsupervised system, gettingparticularly good marks for readability.
The differ-ence in readability between our system and originalheadlines is not statistically significant.
Note thatAVG read AVG infoORIG.
HEADLINE 4.66?
4.10?
?OUR SYSTEM 4.30?
3.52?UNSUP.
SYSTEM 3.70 2.70Table 4: Results for the systems and original headline: ?and ?
stand for significantly better than Unsupervised andOur system at 95% confidence, respectivelythe unsupervised baseline is also capable of generat-ing readable compressions but does a much poorerjob in selecting most important information.
Ourtrained model successfully learned to optimize bothscores.
We refer the reader to Appendix for inputand compression examples.
Note that the ratings forthe human-written headlines in this experiment areslightly different from the ratings in the data valida-tion experiment because a different data sample wasused.5.2 Automatic evaluationOur automatic evaluation had the goal of explic-itly addressing two relevant questions related to ourclaims about (1) the benefits of having a large paral-lel corpus and (2) employing a supervised approachwith a rich feature representation.1.
Our primary motivation for collecting paralleldata has been that having access to sparse lex-ical features, which considerably increase thefeature space, would benefit compression sys-tems.
But is it really the case for sentence com-pression?
Can a comparable performance beachieved with a closed, moderately sized set ofdense, non-lexical features?
If yes, then a largecompression corpus is probably not needed.Furthermore, to demonstrate that a large corpusis not only sufficient but also necessary to learnweights for thousands of features, we need tocompare the performance of the system whentrained on the full data set and a small portionof it.2.
The syntactic and informativeness scores in Eq.
(3) were calculated over millions of news arti-cles and do provide us with meaninful statis-tics (see Sec.
2).
Is there any benefit in re-placing those scores with weights learned for1487their feature counterparts?
Recall that one ofour feature types in Table 1 is the concate-nation of lemma(h) (parent lemma) and la-bel(e) which relies on the same informationas wsynt = P (label(e)|lemma(h)).
The fea-ture counterpart of winfo defined in Eq.
(5) islemma(n)?the lemma of the node to which edgepoints.
How would the supervised system per-form against the unsupervised one, if it only ex-tracted features of these two types?To answer these questions, we sampled 1,000 tu-ples from the unused test data and measured F1score (Riezler et al 2003) by comparing the treesof the generated compression and the ?correct?, ex-tracted headline.
The systems we compared are theunsupervised baseline (UNSUP.
SYSTEM) and thesupervised model trained on three kinds of featuresets: (1) SYNT-INFO FEATURES, corresponding tothe supervised training of the unsupervised base-line model (i.e., lemma(h)-label(e) and lemma(n));(2) NON-LEX FEATURES, corresponding to a dense,non-lexical feature representation (i.e., all the fea-ture types from Table 1 excluding the three involv-ing lemmas); (3) ALL FEATURES (same as OURSYSTEM).
Additionally, we trained the system on10% of the data?10K as opposed to 100K tuples,ALL FEATURES (10K)?for 20 iterations ignoringfeatures which applied to less than three edges6.
Asbefore, the same compression rate was used for allthe systems.
The results are summarized in Table 5.F1 score #featuresUNSUP.
SYSTEM 52.3 N.A.SYNT-INFO FEATURES 75.0 12,490NON-LEX FEATURES 79.6 330ALL FEATURES 84.3 27,813ALL FEATURES (10K) 81.4 22,529Table 5: Results for the unsupervised baseline and thesupervised system trained on three kinds of feature setsClearly, having more features, lexicalized and un-lexicalized, is important: there is a significant im-6Recall from the beginning of the section that for the full(100K) training set the threshold was set to 20 with no tuning.For the 10K training set, we tried values of two, three, five andvaried the number of iterations.
The result we report is the high-est we could get for 10K.provement in going beyond the closed set of 330non-lexical features to all, from 79.6 to 84.3 points.Moreover, successful training requires a large cor-pus since the performance of the system degrades ifonly 10K training instances are used.
Note that thisnumber already exceeds all the existing compressioncorpora taken together.
Hence, sparse lexical fea-tures are useful for compression and a large paral-lel corpus is a requirement for successful supervisedtraining.Concerning our second question, learning featureweights from the data produces significantly betterresults than the hand-crafted way of making use ofthe same information, even if a much larger dataset is used to collect statistics.
We observed a dra-matic increase from 52.3 to 75.0 points.
Thus, wemay conclude that training with dense and sparsefeatures directly from data definitely improves theperformance of the dependency pruning system.5.3 DiscussionIt is important to note that the data we used is chal-lenging: first sentences in news articles tend to belong, in fact longer than other news sentences, whichimplies less reliable syntactic analysis and noisierinput to the syntax-based systems.
In the test setwe used for the evaluation with humans, the meansentence length is 165 characters.
The average com-pression rate in characters is 0.46 ?
0.16 which isquite aggressive7.
Recall that we used the very sameframework for the unsupervised baseline and oursystem as well as the same compression rate.
All thepreprocessing errors affect both systems equally andthe comparison of the two is fair.
Predictably, wrongsyntactic parses significantly increase chances of anungrammatical compression, and parser errors seemto be a major source of readability deficiencies.A property of the described compression frame-work is that a desired compression length is ex-pected to be provided by the user.
This can be seenboth as a strength and as a weakness, depending onthe application.
In a scenario where mobile deviceswith a limited screen size are used, or in a summa-rization scenario where a total summary length isprovided (see the DUC/TAC guidelines8), being able7We follow the standard terminology where smaller valuesimply shorter compressions.8http://www.nist.gov/tac/1488to specify a length is definitely an advantage.
How-ever, one can also think of other applications wherethe user does not have a strict length constraint butwants the text to be somewhat shorter.
In this case,a reranker which compares compressions generatedfor a range of possible lengths can be employed tofind a single compression (e.g., mean edge weight inthe solution or a language model-based score).6 ConclusionsWe have addressed a major problem for supervisedextractive compression models ?
the lack of a largeparallel corpus.
To this end, we presented a methodto automatically build such a corpus from web doc-uments available on the Internet.
An evaluationwith humans demonstrates that the quality of thecorpus is high ?
the compressions are grammati-cal and informative.
We also significantly improveda competitive unsupervised method achieving highreadability and informativeness scores by incorpo-rating thousands of features and learning the featureweights from our corpus.
This result further con-firms the practical utility of the automatically ob-tained data.
We have shown that employing lexi-cal features is important for sentence compression,and that our supervised module can successfullylearn their weights from the corpus.
To our knowl-edge, we are the first to empirically demonstrate thatsparse features are useful for compression and that alarge parallel corpus is a requirement for a success-ful learning of their weights.
We believe that othersupervised deletion-based systems can benefit fromour work.Acknowledgements: The authors are thankful tothe EMNLP reviewers for their feedback and sug-gestions.AppendixThe appendix presents examples of source sentences(S), original headlines (H), extracted headlines (H*),unsupervised baseline (U) and our system (O) com-pressions.ReferencesBejan, C. & S. Harabagiu (2010).
Unsupervisedevent coreference resolution with rich linguisticfeatures.
In Proc.
of ACL-10, pp.
1412?1422.Belz, A., M. White, D. Espinosa, E. Kow, D. Hogan& A. Stent (2011).
The first surface realizationshared task: Overview and evaluation results.
InProc.
of ENLG-11, pp.
217?226.Berg-Kirkpatrick, T., D. Gillick & D. Klein (2011).Jointly learning to extract and compress.
In Proc.of ACL-11.Clarke, J.
& M. Lapata (2006).
Models for sen-tence compression: A comparison across do-mains, training requirements and evaluation mea-sures.
In Proc.
of COLING-ACL-06, pp.
377?385.Clarke, J.
& M. Lapata (2008).
Global inferencefor sentence compression: An integer linear pro-gramming approach.
Journal of Artificial Intelli-gence Research, 31:399?429.Cohn, T. & M. Lapata (2009).
Sentence compres-sion as tree transduction.
Journal of Artificial In-telligence Research, 34:637?674.Collins, M. (2002).
Discriminative training methodsfor Hidden Markov Models: Theory and exper-iments with perceptron algorithms.
In Proc.
ofEMNLP-02, pp.
1?8.de Marneffe, M.-C., B. MacCartney & C. D. Man-ning (2006).
Generating typed dependency parsesfrom phrase structure parses.
In Proc.
of LREC-06, pp.
449?454.Dolan, B., C. Quirk & C. Brokett (2004).
Unsu-pervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
InProceedings of the 20th International Conferenceon Computational Linguistics, Geneva, Switzer-land, 23?27 August 2004, pp.
350?356.Dorr, B., D. Zajic & R. Schwartz (2003).
Hedgetrimmer: A parse-and-trim approach to headlinegeneration.
In Proceedings of the Text Summa-rization Workshop at HLT-NAACL-03, Edmonton,Alberta, Canada, 2003, pp.
1?8.1489S Country star Sara Evans has married former University of Alabama quarterback Jay Barker.H Country star Sara Evans marriesH* Country star Sara Evans has marriedU Sara Evans has married Jay BarkerO Sara Evans has married Jay BarkerS Intel would be building car batteries, expanding its business beyond its core strength, the company said in a statementH Intel to build car batteriesH* Intel would be building car batteriesU would be building the company saidO Intel would be building car batteriesS A New Orleans Saints team spokesman says tight end Jeremy Shockey was taken to a hospital but is doing fine.H Spokesman: Shockey taken to hospital, doing fineH* spokesman says Jeremy Shockey was taken to a hospital but is doing fineU A New Orleans Saints team spokesman says Jeremy Shockey was takenO tight end Jeremy Shockey was taken to a hospital but is doing fineS President Obama declared a major disaster exists in the State of Florida and ordered Federal aid to supplementState and local recovery efforts in the area struck by severe storms, flooding, tornadoes, and straight-line windsbeginning on May 17, 2009, and continuing.H President Obama declares major disaster exists in the State of FloridaH* President Obama declared a major disaster exists in the State of FloridaU President Obama declared a major disaster exists and ordered Federal aidO President Obama declared a major disaster exists in the State of FloridaS Regulators Friday shut down a small Florida bank, bringing to 119 the number of US bank failures this year amidmounting loan defaults.H Regulators shut down small Florida bankH* Regulators shut down a small Florida bankU shut down bringing the number of failuresO Regulators shut down a small Florida bankS Three men were arrested Wednesday night and Dayton police said their arrests are in connection to a west Daytonbank robbery.H 3 men arrested in connection with Bank robberyH* Three men were arrested are in connection to a bank robberyU were arrested and Dayton police said their arrests areO Three men were arrested and police said their arrests areS The government and the social partners will resume the talks on the introduction of the so-called crisis tax,which will be levied on all salaries, pensions and incomes over HRK 3,000.H Government, social partners to resume talks on introduction of ?crisis?
tax.H* The government and the social partners will resume the talks on the introduction of the crisis taxU The government will resume the talks on the introduction of the crisis tax which will be leviedO The government and the social partners will resume the talks on the introduction of the crisis taxS England star David Beckham may have the chance to return to AC Milan after the Italian club?s coach saidhe was open to his move on Sunday.H Beckham has chance of returning to MilanH* David Beckham may have the chance to return to AC MilanU David Beckham may have the chance to return said star wasO David Beckham may have the chance to return to AC MilanS Eastern Health and its insurance company have accepted liability for some patients involved in the breast cancertesting scandal, according to a statement released Friday afternoon.H Eastern Health accepts liability for some patientsH* Eastern Health have accepted liability for some patientsU Health have accepted liability according to a statementO Eastern Health have accepted liability for some patientsS Frontier Communications Corp., a provider of phone, TV and Internet services, said Thursdayit has started a cash tender offer to purchase up to $700 million of its notes.H Frontier Communications starts tender offer for up to $700 million of notesH* Frontier Communications has started a tender offer to purchase $700 million of its notesU Frontier Communications said Thursday a provider has started a tender offerO Frontier Communications has started a tender offer to purchase $700 million of its notes1490Elsner, M. & D. Santhanam (2011).
Learning to fusedisparate sentences.
In Proceedings of the Work-shop on Monolingual Text-to-text Generation, Prt-land, OR, June 24 2011, pp.
54?63.Filippova, K. & M. Strube (2008).
Dependency treebased sentence compression.
In Proc.
of INLG-08, pp.
25?32.Freund, Y.
& R. E. Shapire (1999).
Large marginclassification using the perceptron algorithm.
Ma-chine Learning, 37:277?296.Galanis, D. & I. Androutsopoulos (2010).
An ex-tractive supervised two-stage method for sentencecompression.
In Proc.
of NAACL-HLT-10, pp.885?893.Galanis, D. & I. Androutsopoulos (2011).
A newsentence compression dataset and its use in an ab-stractive generate-and-rank sentence compressor.In Proc.
of UCNLG+Eval-11, pp.
1?11.Galley, M. & K. R. McKeown (2007).
LexicalizedMarkov grammars for sentence compression.
InProc.
of NAACL-HLT-07, pp.
180?187.Gillick, D. & B. Favre (2009).
A scalable globalmodel for summarization.
In ILP for NLP-09, pp.10?18.Grefenstette, G. (1998).
Producing intelligent tele-graphic text reduction to provide an audio scan-ning service for the blind.
In Working Notes ofthe Workshop on Intelligent Text Summarization,Palo Alto, Cal., 23 March 1998, pp.
111?117.Hori, C. & S. Furui (2004).
Speech summariza-tion: An approach through word extraction anda method for evaluation.
IEEE Transactions onInformation and Systems, E87-D(1):15?25.Jing, H. & K. McKeown (2000).
Cut and paste basedtext summarization.
In Proc.
of NAACL-00, pp.178?185.Knight, K. & D. Marcu (2000).
Statistics-basedsummarization ?
step one: Sentence compression.In Proc.
of AAAI-00, pp.
703?711.Mani, I.
(2001).
Automatic Summarization.
Amster-dam, Philadelphia: John Benjamins.McDonald, R. (2006).
Discriminative sentence com-pression with soft syntactic evidence.
In Proc.
ofEACL-06, pp.
297?304.Napoles, C., C. Callison-Burch, J. Ganitkevitch &B.
Van Durme (2011).
Paraphrastic sentence com-pression with a character-based metric: Tighten-ing without deletion.
In Proceedings of the Work-shop on Monolingual Text-to-text Generation, Prt-land, OR, June 24 2011, pp.
84?90.Nivre, J.
(2006).
Inductive Dependency Parsing.Springer.Nomoto, T. (2008).
A generic sentence trimmer withCRFs.
In Proc.
of ACL-HLT-08, pp.
299?307.Nomoto, T. (2009).
A comparison of model free ver-sus model intensive approaches to sentence com-pression.
In Proc.
of EMNLP-09, pp.
391?399.Riezler, S., T. H. King, R. Crouch & A. Zaenen(2003).
Statistical sentence condensation usingambiguity packing and stochastic disambiguationmethods for Lexical-Functional Grammar.
InProc.
of HLT-NAACL-03, pp.
118?125.Turner, J.
& E. Charniak (2005).
Supervised andunsupervised learning for sentence compression.In Proc.
of ACL-05, pp.
290?297.Woodsend, K. & M. Lapata (2010).
Automatic gen-eration of story highlights.
In Proc.
of ACL-10,pp.
565?574.Woodsend, K. & M. Lapata (2012).
Multiple as-pect summarization using Integer Linear Pro-gramming.
In Proc.
of EMNLP-12, pp.
233?243.Wubben, S., A. van den Bosch, E. Krahmer &E. Marsi (2009).
Clustering and matching head-lines for automatic paraphrase acquisition.
InProc.
of ENLG-09, pp.
122?125.Zajic, D., B. J. Dorr, J. Lin & R. Schwartz (2007).Multi-candidate reduction: Sentence compressionas a tool for document summarization tasks.
In-formation Processing & Management, Special Is-sue on Text Summarization, 43(6):1549?1570.1491
