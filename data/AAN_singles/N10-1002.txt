Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 10?18,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsChart Mining-based Lexical Acquisition with Precision GrammarsYi Zhang,?
Timothy Baldwin,??
Valia Kordoni,?
David Martinez?
and Jeremy Nicholson???
DFKI GmbH and Dept of Computational Linguistics, Saarland University, Germany?
Dept of Computer Science and Software Engineering, University of Melbourne, Australia?
NICTA Victoria Research Laboratoryyzhang@coli.uni-sb.de, tb@ldwin.net, kordoni@dfki.de,{davidm,jeremymn}@csse.unimelb.edu.auAbstractIn this paper, we present an innovative chartmining technique for improving parse cover-age based on partial parse outputs from preci-sion grammars.
The general approach of min-ing features from partial analyses is applica-ble to a range of lexical acquisition tasks, andis particularly suited to domain-specific lexi-cal tuning and lexical acquisition using low-coverage grammars.
As an illustration of thefunctionality of our proposed technique, wedevelop a lexical acquisition model for En-glish verb particle constructions which oper-ates over unlexicalised features mined froma partial parsing chart.
The proposed tech-nique is shown to outperform a state-of-the-artparser over the target task, despite being basedon relatively simplistic features.1 IntroductionParsing with precision grammars is increasinglyachieving broad coverage over open-domain textsfor a range of constraint-based frameworks (e.g.,TAG, LFG, HPSG and CCG), and is being used inreal-world applications including information ex-traction, question answering, grammar checking andmachine translation (Uszkoreit, 2002; Oepen et al,2004; Frank et al, 2006; Zhang and Kordoni, 2008;MacKinlay et al, 2009).
In this context, a ?preci-sion grammar?
is a grammar which has been engi-neered to model grammaticality, and contrasts witha treebank-induced grammar, for example.Inevitably, however, such applications demandcomplete parsing outputs, based on the assumptionthat the text under investigation will be completelyanalysable by the grammar.
As precision grammarsgenerally make strong assumptions about completelexical coverage and grammaticality of the input,their utility is limited over noisy or domain-specificdata.
This lack of complete coverage can makeparsing with precision grammars less attractive thanparsing with shallower methods.One technique that has been successfully appliedto improve parser and grammar coverage over agiven corpus is error mining (van Noord, 2004;de Kok et al, 2009), whereby n-grams with low?parsability?
are gathered from the large-scale out-put of a parser as an indication of parser or (pre-cision) grammar errors.
However, error mining isvery much oriented towards grammar engineering:its results are a mixture of different (mistreated) lin-guistic phenomena together with engineering errorsfor the grammar engineer to work through and actupon.
Additionally, it generally does not provideany insight into the cause of the parser failure, and itis difficult to identify specific language phenomenafrom the output.In this paper, we instead propose a chart min-ing technique that works on intermediate parsing re-sults from a parsing chart.
In essence, the methodanalyses the validity of different analyses for wordsor constructions based on the ?lifetime?
and prob-ability of each within the chart, combining the con-straints of the grammar with probabilities to evaluatethe plausibility of each.For purposes of exemplification of the proposedtechnique, we apply chart mining to a deep lexicalacquisition (DLA) task, using a maximum entropy-based prediction model trained over a seed lexiconand treebank.
The experimental set up is the fol-lowing: given a set of sentences containing puta-tive instances of English verb particle constructions,10extract a list of non-compositional VPCs optionallywith valence information.
For comparison, we parsethe same sentence set using a state-of-the-art statisti-cal parser, and extract the VPCs from the parser out-put.
Our results show that our chart mining methodproduces a model which is superior to the treebankparser.To our knowledge, the only other work that haslooked at partial parsing results of precision gram-mars as a means of linguistic error analysis is that ofKiefer et al (1999) and Zhang et al (2007a), wherepartial parsing models were proposed to select a setof passive edges that together cover the input se-quence.
Compared to these approaches, our pro-posed chart mining technique is more general andcan be adapted to specific tasks and domains.
Whilewe experiment exclusively with an HPSG grammarin this paper, it is important to note that the proposedmethod can be applied to any grammar formalismwhich is compatible with chart parsing, and where itis possible to describe an unlexicalised lexical entryfor the different categories of lexical item that are tobe extracted (see Section 3.2 for details).The remainder of the paper is organised as fol-lows.
Section 2 defines the task of VPC extraction.Section 3 presents the chart mining technique andthe feature extraction process for the VPC extractiontask.
Section 4 evaluates the model performancewith comparison to two competitor models over sev-eral different measures.
Section 5 further discussesthe general applicability of chart mining.
Finally,Section 6 concludes the paper.2 Verb Particle ConstructionsThe particular construction type we target for DLAin this paper is English Verb Particle Constructions(henceforth VPCs).
VPCs consist of a head verband one or more obligatory particles, in the formof intransitive prepositions (e.g., hand in), adjec-tives (e.g., cut short) or verbs (e.g., let go) (Villav-icencio and Copestake, 2002; Huddleston and Pul-lum, 2002; Baldwin and Kim, 2009); for the pur-poses of our dataset, we assume that all particles areprepositional?by far the most common and produc-tive of the three types?and further restrict our atten-tion to single-particle VPCs (i.e., we ignore VPCssuch as get alng together).One aspect of VPCs that makes them a partic-ularly challenging target for lexical acquisition isthat the verb and particle can be non-contiguous (forinstance, hand the paper in and battle right on).This sets them apart from conventional collocationsand terminology (cf., Manning and Schu?tze (1999),Smadja (1993) and McKeown and Radev (2000))in that they cannot be captured effectively using n-grams, due to their variability in the number and typeof words potentially interceding between the verband the particle.
Also, while conventional colloca-tions generally take the form of compound nounsor adjective?noun combinations with relatively sim-ple syntactic structure, VPCs occur with a range ofvalences.
Furthermore, VPCs are highly productivein English and vary in use across domains, makingthem a prime target for lexical acquisition (Dehe?,2002; Baldwin, 2005; Baldwin and Kim, 2009).In the VPC dataset we use, there is an addi-tional distinction between compositional and non-compositional VPCs.
With compositional VPCs,the semantics of the verb and particle both corre-spond to the semantics of the respective simplexwords, including the possibility of the semantics be-ing specific to the VPC construction in the case ofparticles.
For example, battle on would be clas-sified as compositional, as the semantics of bat-tle is identical to that for the simplex verb, andthe semantics of on corresponds to the continua-tive sense of the word as occurs productively inVPCs (cf., walk/dance/drive/govern/... on).
Withnon-compositional VPCs, on the other hand, thesemantics of the VPC is somehow removed fromthat of the parts.
In the dataset we used for eval-uation, we are interested in extracting exclusivelynon-compositional VPCs, as they require lexicalisa-tion; compositional VPCs can be captured via lexi-cal rules and are hence not the target of extraction.English VPCs can occur with a number of va-lences, with the two most prevalent and productivevalences being the simple transitive (e.g., hand inthe paper) and intransitive (e.g., back off ).
For thepurposes of our target task, we focus exclusively onthese two valence types.Given the above, we define the English VPC ex-traction task to be the production of triples of theform ?v, p, s?, where v is a verb lemma, p is a prepo-sitional particle, and s ?
{intrans , trans} is the va-11lence; additionally, each triple has to be semanticallynon-compositional.
The triples are extracted relativeto a set of putative token instances for each of theintransitive and transitive valences for a given VPC.That is, a given triple should be classified as positiveif and only if it is associated with at least one non-compositional token instance in the provided token-level data.The dataset used in this research is the one usedin the LREC 2008 Multiword Expression WorkshopShared Task (Baldwin, 2008).1 In the dataset, thereis a single file for each of 4,090 candidate VPCtriples, containing up to 50 sentences that have thegiven VPC taken from the British National Cor-pus.
When the valence of the VPC is ignored,the dataset contains 440 unique VPCs among 2,898VPC candidates.
In order to be able to fairly com-pare our method with a state-of-the-art lexicalisedparser trained over the WSJ training sections of thePenn Treebank, we remove any VPC types from thetest set which are attested in the WSJ training sec-tions.
This removes 696 VPC types from the testset, and makes the task even more difficult, as theremaining testing VPC types are generally less fre-quent ones.
At the same time, it unfortunately meansthat our results are not directly comparable to thosefor the original shared task.23 Chart Mining for Parsing with a LargePrecision Grammar3.1 The TechniqueThe chart mining technique we use in this paperis couched in a constituent-based bottom-up chartparsing paradigm.
A parsing chart is a data struc-ture that records all the (complete or incomplete) in-termediate parsing results.
Every passive edge onthe parsing chart represents a complete local analy-sis covering a sub-string of the input, while each ac-tive edge predicts a potential local analysis.
In thisview, a full analysis is merely a passive edge thatspans the whole input and satisfies certain root con-1Downloadable from http://www.csse.unimelb.edu.au/research/lt/resources/vpc/vpc.tgz.2In practice, there was only one team who participated inthe original VPC task (Ramisch et al, 2008), who used a vari-ety of web- and dictionary-based features suited more to high-frequency instances in high-density languages, so a simplisticcomparison would not have been meaningful.ditions.
The bottom-up chart parser starts with edgesinstantiated from lexical entries corresponding to theinput words.
The grammar rules are used to incre-mentally create longer edges from smaller ones untilno more edges can be added to the chart.Standardly, the parser returns only outputs thatcorrespond to passive edges in the parsing chart thatspan the full input string.
For those inputs without afull-spanning edge, no output is generated, and thechart becomes the only source of parsing informa-tion.A parsing chart takes the form of a hierarchy ofedges.
Where only passive edges are concerned,each non-lexical edge corresponds to exactly onegrammar rule, and is connected with one or moredaughter edge(s), and zero or more parent edge(s).Therefore, traversing the chart is relatively straight-forward.There are two potential challenges for the chart-mining technique.
First, there is potentially a hugenumber of parsing edges in the chart.
For in-stance, when parsing with a large precision gram-mar like the HPSG English Resource Grammar(ERG, Flickinger (2002)), it is not unusual for a20-word sentence to receive over 10,000 passiveedges.
In order to achieve high efficiency in pars-ing (as well as generation), ambiguity packing isusually used to reduce the number of productivepassive edges on the parsing chart (Tomita, 1985).For constraint-based grammar frameworks like LFGand HPSG, subsumption-based packing is used toachieve a higher packing ratio (Oepen and Carroll,2000), but this might also potentially lead to an in-consistent packed parse forest that does not unpacksuccessfully.
For chart mining, this means that notall passive edges are directly accessible from thechart.
Some of them are packed into others, and thederivatives of the packed edges are not generated.Because of the ambiguity packing, zero or morelocal analyses may exist for each passive edge onthe chart, and the cross-combination of the packeddaughter edges is not guaranteed to be compatible.As a result, expensive unification operations must bereapplied during the unpacking phase.
Carroll andOepen (2005) and Zhang et al (2007b) have pro-posed efficient k-best unpacking algorithms that canselectively extract the most probable readings fromthe packed parse forest according to a discrimina-12tive parse disambiguation model, by minimising thenumber of potential unifications.
The algorithm canbe applied to unpack any passive edges.
Becauseof the dynamic programming used in the algorithmand the hierarchical structure of the edges, the costof the unpacking routine is empirically linear in thenumber of desired readings, and O(1) when invokedmore than once on the same edge.The other challenge concerns the selection of in-formative and representative pieces of knowledgefrom the massive sea of partial analyses in the pars-ing chart.
How to effectively extract the indicativefeatures for a specific language phenomenon is avery task-specific question, as we will show in thecontext of the VPC extraction task in Section 3.2.However, general strategies can be applied to gener-ate parse ranking scores on each passive edge.
Themost widely used parse ranking model is the log-linear model (Abney, 1997; Johnson et al, 1999;Toutanova et al, 2002).
When the model does notuse non-local features, the accumulated score on asub-tree under a certain (unpacked) passive edge canbe used to approximate the probability of the partialanalysis conditioned on the sub-string within thatspan.33.2 The Application: Acquiring Features forVPC ExtractionAs stated above, the target task we use to illustratethe capabilities of our chart mining method is VPCextraction.The grammar we apply our chart mining methodto in this paper is the English Resource Grammar(ERG, Flickinger (2002)), a large-scale precisionHPSG for English.
Note, however, that the methodis equally compatible with any grammar or grammarformalism which is compatible with chart parsing.The lexicon of the ERG has been semi-automatically extended with VPCs extractedby Baldwin (2005).
In order to show the effective-ness of chart mining in discovering ?unknowns?and remove any lexical probabilities associatedwith pre-existing lexical entries, we block the3To have a consistent ranking model on any sub-analysis,one would have to retrain the disambiguation model on everypassive edge.
In practice, we find this to be intractable.
Also,the approximation based on full-parse ranking model works rea-sonably well.lexical entries for the verb in the candidate VPCby substituting the input token with a DUMMY-Vtoken, which is coupled with four candidate lexicalentries of type: (1) intransitive simplex verb (v - e),(2) transitive simplex verb (v np le), (3) intransitiveVPC (v p le), and (4) transitive VPC (v p-np le),respectively.
These four lexical entries represent thetwo VPC valences we wish to distinguish betweenin the VPC extraction task, and the competingsimplex verb candidates.
Based on these lexicaltypes, the features we extract with chart mining aresummarised in Table 1.
The maximal constituent(MAXCONS) of a lexical entry is defined to be thepassive edge that is an ancestor of the lexical entryedge that: (i) must span over the particle, and (ii)has maximal span length.
In the case of a tie,the edge with the highest disambiguation score isselected as the MAXCONS.
If there is no edge foundon the chart that spans over both the verb and theparticle, the MAXCONS is set to be NULL, with aMAXSPAN of 0, MAXLEVEL of 0 and MAXCRANKof 4 (see Table 1).
The stem of the particle is alsocollected as a feature.One important characteristic of these features isthat they are completely unlexicalised on the verb.This not only leads to a fair evaluation with the ERGby excluding the influence from the lexical coverageof VPCs in the grammar, but it also demonstratesthat complete grammatical coverage over simplexverbs is not a prerequisite for chart mining.To illustrate how our method works, we presentthe unpacked parsing chart for the candidate VPCshow off and input sentence The boy shows off hisnew toys in Figure 1.
The non-terminal edges aremarked with their syntactic categories, i.e., HPSGrules (e.g., subjh for the subject-head-rule, hadj forthe head-adjunct-rule, etc.
), and optionally their dis-ambiguation scores.
By traversing upward throughparent edges from the DUMMY-V edge, all featurescan be efficiently extracted (see the third column inTable 1).It should be noted that none of these features areused to deterministically dictate the predicted VPCcategory.
Instead, the acquired features are used asinputs to a statistical classifier for predicting the typeof the VPC candidate at the token level (in the con-text of the given sentence).
In our experiment, weused a maximum entropy-based model to do a 3-13Feature Description ExamplesLE:MAXCONSA lexical entry together with the maximal constituentconstructed from itv - le:subjh, v np le:hadj,v p le:subjh, v p-np le:subjLE:MAXSPANA lexical entry together with the length of the span ofthe maximal constituent constructed from the LEv - le:7, v np le:5, v p le:4,v p-np le:7LE:MAXLEVELA lexical entry together with the levels of projectionsbefore it reaches its maximal constituentv - le:2, v np le:1, v p le:2,v p-np le:3LE:MAXCRANKA lexical entry together with the relative disambigua-tion score ranking of its maximal constituent amongall MaxCons from different LEsv - le:4, v np le:3, v p le:1,v p-np le:2PARTICLE The stem of the particle in the candidate VPC offTable 1: Chart mining features used for VPC extractionhis new toysoffshowsPREPPRTLv_?_leNP1VP4?hcompNP2VP5?hcompPP?hcomp0 2 3 4 7DUMMY?VS1?subjh(.125)S3?subjh(.875)VP1?hadj VP3?hcompS2?subjh(.925)VP2?hadj(.325)v_p?np_lev_np_le v_p_lethe boyFigure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERGcategory classification: non-VPC, transitive VPC,or intransitive VPC.
For the parameter estimationof the ME model, we use the TADM open sourcetoolkit (Malouf, 2002).
The token-level predictionsare then combined with a simple majority voting toderive the type-level prediction for the VPC candi-date.
In the case of a tie, the method backs off tothe na?
?ve baseline model described in Section 4.2,which relies on the combined probability of the verband particle forming a VPC.We have also experimented with other ways of de-riving type-level predictions from token-level classi-fication results.
For instance, we trained a separateclassifier that takes the token-level prediction as in-put in order to determine the type-level VPC predic-tion.
Our results indicate no significant differencebetween these methods and the basic majority vot-ing approach, so we present results exclusively forthis simplistic approach in this paper.4 Evaluation4.1 Experiment SetupTo evaluate the proposed chart mining-based VPCextraction model, we use the dataset from the LREC2008 Multiword Expression Workshop shared task(see Section 2).
We use this dataset to perform threedistinct DLA tasks, as detailed in Table 2.The chart mining feature extraction is imple-mented as an extension to the PET parser (Callmeier,14Task DescriptionGOLD VPC Determine the valence for a verb?preposition combination which is known to occuras a non-compositional VPC (i.e.
known VPC, with unknown valence(s))FULL Determine whether each verb?preposition combination is a VPC or not, and furtherpredict its valence(s) (i.e.
unknown if VPC, and unknown valence(s))VPC Determine whether each verb?preposition combination is a VPC or not ignoring va-lence (i.e.
unknown if VPC, and don?t care about valence)Table 2: Definitions of the three DLA tasks2001).
We use a slightly modified version of theERG in our experiments, based on the nov-06 re-lease.
The modifications include 4 newly-addeddummy lexical entries for the verb DUMMY-V andthe corresponding inflectional rules, and a lexicaltype prediction model (Zhang and Kordoni, 2006)trained on the LOGON Treebank (Oepen et al, 2004)for unknown word handling.
The parse disambigua-tion model we use is also trained on the LOGONTreebank.
Since the parser has no access to any ofthe verbs under investigation (due to the DUMMY-V substitution), those VPC types attested in theLOGON Treebank do not directly impact on themodel?s performance.
The chart mining feature ex-traction process took over 10 CPU days, and col-lected a total of 44K events for 4,090 candidate VPCtriples.4 5-fold cross validation is used to train/testthe model.
As stated above (Section 2), the VPCtriples attested in the WSJ training sections of thePenn Treebank are excluded in each testing fold forcomparison with the Charniak parser-based model(see Section 4.2).4.2 Baseline and BenchmarkFor comparison, we first built a na?
?ve baseline modelusing the combined probabilities of the verb and par-ticle being part of a VPC.
More specifically, P (c|v)and P (c|p) are the probabilities of a given verbv and particle p being part of a VPC candidateof type s ?
{intrans , trans , null}, for transitive4Not all sentences in the dataset are successfully chart-mined.
Due to the complexity of the precision grammar weuse, the parser is unlikely to complete the parsing chart for ex-tremely long sentences (over 50 words).
Moreover, sentenceswhich do not receive any spanning edge over the verb and theparticle are not considered as an indicative event.
Nevertheless,the coverage of the chart mining is much higher than the full-parse coverage of the grammar.VPC, intransitive VPC, and non-VPC, respectively.P?
(s|v, p) = P (s|v) ?
P (s|p) is used to approxi-mate the joint probability of verb-particle (v, p) be-ing of type s, and the prediction type is chosen ran-domly based on this probabilistic distribution.
BothP (s|v) and P (s|p) can be estimated from a list ofVPC candidate types.
If v is unseen, P (s|v) is set tobe 1|V |?vi?V P (s|vi) estimated over all verbs |V |seen in the list of VPC candidates.
The na?
?ve base-line performed poorly, mainly because there is notenough knowledge about the context of use of VPCs.This also indicates that the task of VPC extractionis non-trivial, and that context (evidence from sen-tences in which the VPC putatively occurs) must beincorporated in order to make more accurate predic-tions.As a benchmark VPC extraction system, we usethe Charniak parser (Charniak, 2000).
This sta-tistical parser induces a context-free grammar anda generative parsing model from a training set ofgold standard parse trees.
Traditionally, it has beentrained over the WSJ component of the Penn Tree-bank, and for this work we decided to take the sameapproach and train over sections 1 to 22, and use sec-tion 23 for parameter-tuning.
After parsing, we sim-ply search for the VPC triples in each token instancewith tgrep2,5 and decide on the classification ofthe candidate by majority voting over all instances,breaking ties randomly.5Noting that the Penn POS tagset captures essentially thecompositional vs. non-compositional VPC distinction requiredin the extraction task, through the use of the RP (prepositionalparticle, for non-compositional VPCs) and RB (adverb, for com-positional VPCs) tags.154.3 ResultsThe results of our experiments are summarised inTable 3.
For the na?
?ve baseline and the chart mining-based models, the results are averaged over 5-foldcross validation.We evaluate the methods in the form of the threetasks described in Table 2.
Formally, GOLD VPCequates to extracting ?v, p, s?
tuples from the sub-set of gold-standard ?v, p?
tuples; FULL equates toextracting ?v, p, s?
tuples for all VPC candidates;and VPC equates to extracting ?v, p?
tuples (ignor-ing valence) over all VPC candidates.
In each case,we present the precision (P), recall (R) and F-score(?
= 1: F).
For multi-category classifications (i.e.the two tasks where we predict the valence s, indi-cated as ?All?
in Table 3), we micro-average the pre-cision and recall over the two VPC categories, andcalculate the F-score as their harmonic mean.From the results, it is obvious that the chartmining-based model performs best overall, and in-deed for most of the measures presented.
The Char-niak parser-based extraction method performs rea-sonably well, especially in the VPC+valence extrac-tion task over the FULL task, where the recall washigher than the chart mining method.
Althoughnot reported here, we observe a marked improve-ment in the results for the Charniak parser whenthe VPC types attested in the WSJ are not filteredfrom the test set.
This indicates that the statisti-cal parser relies heavily on lexicalised VPC infor-mation, while the chart mining model is much moresyntax-oriented.
In error analysis of the data, we ob-served that the Charniak parser was noticeably moreaccurate at extracting VPCs where the verb was fre-quent (our method, of course, did not have accessto the base frequency of the simplex verb), under-lining again the power of lexicalisation.
This pointsto two possibilities: (1) the potential for our methodto similarly benefit from lexicalisation if we were toremove the constraint on ignoring any pre-existinglexical entries for the verb; and (2) the possibilityfor hybridising between lexicalised models for fre-quent verbs and unlexicalised models for infrequentverbs.
Having said this, it is important to reinforcethat lexical acquisition is usually performed in theabsence of lexicalised probabilities, as if we haveprior knowledge of the lexical item, there is no needto extract it.
In this sense, the first set of results inTable 3 over Gold VPCs are the most informative,and illustrate the potential of the proposed approach.From the results of all the models, it would ap-pear that intransitive VPCs are more difficult to ex-tract than transitive VPCs.
This is partly because thedataset we use is unbalanced: the number of transi-tive VPC types is about twice the number of intran-sitive VPCs.
Also, the much lower numbers overthe FULL set compared to the GOLD VPC set are dueto the fact that only 1/8 of the candidates are trueVPCs.5 Discussion and Future WorkThe inventory of features we propose for VPC ex-traction is just one illustration of how partial parseresults can be used in lexical acquisition tasks.The general chart mining technique can easily beadapted to learn other challenging linguistic phe-nomena, such as the countability of nouns (Bald-win and Bond, 2003), subcategorization propertiesof verbs or nouns (Korhonen, 2002), and generalmultiword expression (MWE) extraction (Baldwinand Kim, 2009).
With MWE extraction, e.g., eventhough some MWEs are fixed and have no internalsyntactic variability, such as ad hoc, there is a verylarge proportion of idioms that allow various de-grees of internal variability, and with a variable num-ber of elements.
For example, the idiom spill thebeans allows internal modification (spill mountainsof beans), passivisation (The beans were spilled inthe latest edition of the report), topicalisation (Thebeans, the opposition spilled), and so forth (Sag etal., 2002).
In general, however, the exact degree ofvariability of an idiom is difficult to predict (Riehe-mann, 2001).
The chart mining technique we pro-pose here, which makes use of partial parse results,may facilitate the automatic recognition task of evenmore flexible idioms, based on the encouraging re-sults for VPCs.The main advantage, though, of chart mining isthat parsing with precision grammars does not anylonger have to assume complete coverage, as hastraditionally been the case.
As an immediate con-sequence, the possibility of applying our chart min-ing technique to evolving medium-sized grammarsmakes it especially interesting for lexical acquisi-16Task VPC Type Na?
?ve Baseline Charniak Parser Chart-MiningP R F P R F P R FGOLD VPCIntrans-VPC 0.300 0.018 0.034 0.549 0.753 0.635 0.845 0.621 0.716Trans-VPC 0.676 0.348 0.459 0.829 0.648 0.728 0.877 0.956 0.915All 0.576 0.236 0.335 0.691 0.686 0.688 0.875 0.859 0.867FULLIntrans-VPC 0.060 0.018 0.028 0.102 0.593 0.174 0.153 0.155 0.154Trans-VPC 0.083 0.348 0.134 0.179 0.448 0.256 0.179 0.362 0.240All 0.080 0.236 0.119 0.136 0.500 0.213 0.171 0.298 0.218VPC 0.123 0.348 0.182 0.173 0.782 0.284 0.259 0.332 0.291Table 3: Results for the different methods over the three VPC extraction tasks detailed in Table 2tion over low-density languages, for instance, wherethere is a real need for rapid-prototyping of languageresources.The chart mining approach we propose in thispaper is couched in the bottom-up chart parsingparadigm, based exclusively on passive edges.
Asfuture work, we would also like to look into thetop-level active edges (those active edges that arenever completed), as an indication of failed assump-tions.
Moreover, it would be interesting to investi-gate the applicability of the technique in other pars-ing strategies, e.g., head-corner or left-corner pars-ing.
Finally, it would also be interesting to in-vestigate whether by using the features we acquirefrom chart mining enhanced with information on theprevalence of certain patterns, we could achieve per-formance improvements over broader-coverage tree-bank parsers such as the Charniak parser.6 ConclusionWe have proposed a chart mining technique for lex-ical acquisition based on partial parsing with preci-sion grammars.
We applied the proposed methodto the task of extracting English verb particle con-structions from a prescribed set of corpus instances.Our results showed that simple unlexicalised fea-tures mined from the chart can be used to effec-tively extract VPCs, and that the model outperformsa probabilistic baseline and the Charniak parser atVPC extraction.AcknowledgementsNICTA is funded by the Australian Government as rep-resented by the Department of Broadband, Communica-tions and the Digital Economy and the Australian Re-search Council through the ICT Centre of Excellence pro-gram.
The first was supported by the German ExcellenceCluster of Multimodal Computing and Interaction.ReferencesSteven Abney.
1997.
Stochastic attribute-value gram-mars.
Computational Linguistics, 23:597?618.Timothy Baldwin and Francis Bond.
2003.
Learningthe countability of English nouns from corpus data.In Proceedings of the 41st Annual Meeting of the As-sociation for Computational Linguistics (ACL 2003),pages 463?470, Sapporo, Japan.Timothy Baldwin and Su Nam Kim.
2009.
Multiwordexpressions.
In Nitin Indurkhya and Fred J. Damerau,editors, Handbook of Natural Language Processing.CRC Press, Boca Raton, USA, 2nd edition.Timothy Baldwin.
2005.
The deep lexical acquisition ofEnglish verb-particle constructions.
Computer Speechand Language, Special Issue on Multiword Expres-sions, 19(4):398?414.Timothy Baldwin.
2008.
A resource for evaluating thedeep lexical acquisition of English verb-particle con-structions.
In Proceedings of the LREC 2008 Work-shop: Towards a Shared Task for Multiword Expres-sions (MWE 2008), pages 1?2, Marrakech, Morocco.Ulrich Callmeier.
2001.
Efficient parsing with large-scale unification grammars.
Master?s thesis, Univer-sita?t des Saarlandes, Saarbru?cken, Germany.John Carroll and Stephan Oepen.
2005.
High efficiencyrealization for a wide-coverage unification grammar.In Proceedings of the 2nd International Joint Confer-ence on Natural LanguageProcessing (IJCNLP 2005),pages 165?176, Jeju Island, Korea.Eugene Charniak.
2000.
A maximum entropy-basedparser.
In Proceedings of the 1st Annual Meeting ofthe North American Chapter of Association for Com-putational Linguistics (NAACL2000), Seattle, USA.Daniel de Kok, Jianqiang Ma, and Gertjan van Noord.2009.
A generalized method for iterative error min-ing in parsing results.
In Proceedings of the ACL2009Workshop on Grammar Engineering Across Frame-works (GEAF), Singapore.17Nicole Dehe?.
2002.
Particle Verbs in English: Syn-tax, Information, Structure and Intonation.
John Ben-jamins, Amsterdam, Netherlands/Philadelphia, USA.Dan Flickinger.
2002.
On building a more efficientgrammar by exploiting types.
In Stephan Oepen, DanFlickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-tors, Collaborative Language Engineering, pages 1?17.
CSLI Publications.Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, HansUszkoreit, Berthold Crysmann, Brigitte Jo?rg, and Ul-rich Scha?fer.
2006.
Question answering from struc-tured knowledge sources.
Journal of Applied Logic,Special Issue on Questions and Answers: Theoreticaland Applied Perspectives., 5(1):20?48.Rodney Huddleston and Geoffrey K. Pullum.
2002.
TheCambridge Grammar of the English Language.
Cam-bridge University Press, Cambridge, UK.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochas-tic unifcation-based grammars.
In Proceedings of the37th Annual Meeting of the Association for Computa-tional Linguistics (ACL 1999), pages 535?541, Mary-land, USA.Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, andRob Malouf.
1999.
A Bag of Useful Techniques forEfficient and Robust Parsing.
In Proceedings of the37th Annual Meeting of the Association for Computa-tional Linguistics, pages 473?480, Maryland, USA.Anna Korhonen.
2002.
Subcategorization Acquisition.Ph.D.
thesis, University of Cambridge.Andrew MacKinlay, David Martinez, and Timothy Bald-win.
2009.
Biomedical event annotation with CRFsand precision grammars.
In Proceedings of BioNLP2009: Shared Task, pages 77?85, Boulder, USA.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the 6th Conferencde on Natural LanguageLearning (CoNLL 2002), pages 49?55, Taipei, Taiwan.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press.Kathleen R. McKeown and Dragomir R. Radev.
2000.Collocations.
In Robert Dale, Hermann Moisl, andHarold Somers, editors, Handbook of Natural Lan-guage Processing.Stephan Oepen and John Carroll.
2000.
Ambiguity pack-ing in constraint-based parsing ?
practical results.
InProceedings of the 1st Annual Meeting of the NorthAmerican Chapter of Association for ComputationalLinguistics (NAACL 2000), pages 162?169, Seattle,USA.Stephan Oepen, Helge Dyvik, Jan Tore L?nning, ErikVelldal, Dorothee Beermann, John Carroll, DanFlickinger, Lars Hellan, Janne Bondi Johannessen,Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.2004.
Som a?
kapp-ete med trollet?
Towards MRS-Based Norwegian?English Machine Translation.
InProceedings of the 10th International Conference onTheoretical and Methodological Issues in MachineTranslation, Baltimore, USA.Carlos Ramisch, Paulo Schreiner, Marco Idiart, and AlineVillavicencio.
2008.
An evaluation of methods for theextraction of multiword expressions.
In Proceedingsof the LREC 2008 Workshop: Towards a Shared Taskfor Multiword Expressions (MWE 2008), pages 50?53,Marrakech, Morocco.Susanne Riehemann.
2001.
A Constructional Approachto Idioms and Word Formation.
Ph.D. thesis, StanfordUniversity, CA, USA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiword ex-pressions: A pain in the neck for NLP.
In Proceedingsof the 3rd International Conference on Intelligent TextProcessing and Computational Linguistics (CICLing-2002), pages 1?15, Mexico City, Mexico.Frank Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1):143?178.Masaru Tomita.
1985.
An efficient context-free parsingalgorithm for natural languages.
In Proceedings of the9th International Joint Conference on Artificial Intel-ligence, pages 756?764, Los Angeles, USA.Kristina Toutanova, Christoper D. Manning, Stuart M.Shieber, Dan Flickinger, and Stephan Oepen.
2002.Parse ranking for a rich HPSG grammar.
In Proceed-ings of the 1st Workshop on Treebanks and LinguisticTheories (TLT 2002), pages 253?263, Sozopol, Bul-garia.Hans Uszkoreit.
2002.
New chances for deep linguis-tic processing.
In Proceedings of the 19th interna-tional conference on computational linguistics (COL-ING 2002), Taipei, Taiwan.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In Proceedings of the42nd Annual Meeting of the Association for Computa-tional Linguistics), pages 446?453, Barcelona, Spain.Aline Villavicencio and Ann Copestake.
2002.
Verb-particle constructions in a computational grammar ofEnglish.
In Proceedings of the 9th International Con-ference on Head-Driven Phrase Structure Grammar(HPSG-2002), Seoul, Korea.Yi Zhang and Valia Kordoni.
2006.
Automated deeplexical acquisition for robust open texts processing.In Proceedings of the 5th International Conferenceon Language Resources and Evaluation (LREC 2006),pages 275?280, Genoa, Italy.Yi Zhang and Valia Kordoni.
2008.
Robust parsingwith a large HPSG grammar.
In Proceedings of theSixth International Language Resources and Evalua-tion (LREC?08), Marrakech, Morocco.Yi Zhang, Valia Kordoni, and Erin Fitzgerald.
2007a.Partial parse selection for robust deep processing.
InProceedings of ACL 2007 Workshop on Deep Linguis-tic Processing, pages 128?135, Prague, Czech Repub-lic.Yi Zhang, Stephan Oepen, and John Carroll.
2007b.
Ef-ficiency in unification-based N-best parsing.
In Pro-ceedings of the 10th International Conference on Pars-ing Technologies (IWPT 2007), pages 48?59, Prague,Czech.18
