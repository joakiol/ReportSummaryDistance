Proceedings of the ACL-HLT 2011 Student Session, pages 1?5,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsWord Alignment Combination over Multiple Word SegmentationNing Xi, Guangchao Tang, Boyuan Li, Yinggong ZhaoState Key Laboratory for Novel Software Technology,Department of Computer Science and Technology,Nanjing University, Nanjing, 210093, China{xin,tanggc,liby,zhaoyg}@nlp.nju.edu.cnAbstractIn this paper, we present a new word alignmentcombination approach on language pairs whereone language has no explicit word boundaries.Instead of combining word alignments of dif-ferent models (Xiang et al, 2010), we try tocombine word alignments over multiple mono-lingually motivated word segmentation.
Ourapproach is based on link confidence score de-fined over multiple segmentations, thus thecombined alignment is more robust to inappro-priate word segmentation.
Our combination al-gorithm is simple, efficient, and easy toimplement.
In the Chinese-English experiment,our approach effectively improved word align-ment quality as well as translation performanceon all segmentations simultaneously, whichshowed that word alignment can benefit fromcomplementary knowledge due to the diversityof multiple and monolingually motivated seg-mentations.1 IntroductionWord segmentation is the first step prior to wordalignment for building statistical machine transla-tions (SMT) on language pairs without explicitword boundaries such as Chinese-English.
Manyworks have focused on the improvement of wordalignment models.
(Brown et al, 1993; Haghighi etal., 2009; Liu et al, 2010).
Most of the wordalignment models take single word segmentationas input.
However, for languages such as Chinese,it is necessary to segment sentences into appropri-ate words for word alignment.A large amount of works have stressed the im-pact of word segmentation on word alignment.
Xuet al (2004), Ma et al (2007), Chang et al (2008),and Chung et al (2009) try to learn word segmen-tation from bilingually motivated point of view;they use an initial alignment to learn word segmen-tation appropriate for SMT.
However, their per-formance is limited by the quality of the initialalignments, and the processes are time-consuming.Some other methods try to combine multiple wordsegmentation at SMT decoding step (Xu et al,2005; Dyer et al, 2008; Zhang et al, 2008; Dyer etal., 2009; Xiao et al, 2010).
Different segmenta-tions are yet independently used for word align-ment.Instead of time-consuming segmentation optimi-zation based on alignment or postponing segmenta-tion combination late till SMT decoding phase, wetry to combine word alignments over multiplemonolingually motivated word segmentation onChinese-English pair, in order to improve wordalignment quality and translation performance forall segmentations.
We introduce a tabular structurecalled word segmentation network (WSN for short)to encode multiple segmentations of a Chinese sen-tence, and define skeleton links (SL for short) be-tween spans of WSN and words of Englishsentence.
The confidence score of a SL is definedover multiple segmentations.
Our combination al-gorithm picks up potential SLs based on their con-fidence scores similar to Xiang et al (2010), andthen projects each selected SL to link in all seg-mentation respectively.
Our algorithm is simple,efficient, easy to implement, and can effectivelyimprove word alignment quality on all segmenta-tions simultaneously, and alignment errors caused1by inappropriate segmentations from single seg-menter can be substantially reduced.Two questions will be answered in the paper: 1)how to define the link confidence over multiplesegmentations in combination algorithm?
2) Ac-cording to Xiang et al (2010), the success of theirword alignment combination of different modelslies in the complementary information that thecandidate alignments contain.
In our work, aremultiple monolingually motivated segmentationscomplementary enough to improve the alignments?The rest of this paper is structured as follows:WSN will be introduced in section 2.
Combinationalgorithm will be presented in section 3.
Experi-ments of word alignment and SMT will be reportedin section 4.2  Word Segmentation NetworkWe propose a new structure called word segmenta-tion network (WSN) to encode multiple segmenta-tions.
Due to space limitation, all definitions arepresented by illustration of a running example of asentence pair:????
(xia-yu-lu-hua)Road is slippery when rainingWe first introduce skeleton segmentation.
Giventwo segmentation S1 and S2 in Table 1, the wordboundaries of their skeleton segmentation is theunion of word boundaries (marked by ?/?)
in S1and S2.SegmentationS1 ?
/ ?
/ ?
?S2 ??
/ ?
/ ?skeleton ?
/ ?
/ ?
/ ?Table 1: The skeleton segmentation of two seg-mentations S1 and S2.The WSN of S1 and S2 is shown in Table 2.
Asis depicted, line 1 and 2 represent words in S1 andS2 respectively, line 3 represents skeleton words.Each column, or span, comprises a skeleton wordand words of S1 and S2 with the skeleton word astheir morphemes at that position.
The number ofcolumns of a WSN is equal to the number of skele-ton words.
It should be noted that there may bewords covering two or more spans, such as ???
?in S1, because the word ????
in S1 is split intotwo words ???
and ???
in S2.S1 ?
1 ?
2 ??
3S2 ??
1 ?
2 ?
3skeleton ?
1 ?
2 ?
3 ?
4Table 2:  The WSN of Table 1.
Subscriptsindicate indexes of words.The skeleton word can be projected onto wordsin the same span in S1 and S2.
For clarity, words ineach segmentation are indexed (1-based), for ex-ample, ????
in S1 is indexed by 3.
We use a pro-jection function       to denote the index of theword onto which the j-th skeleton word is project-ed in the k-th segmentation, for example,and        .In the next, we define the links between spans ofthe WSN and English words as skeleton links (SL),the subset of all SLs comprise the skeleton align-ment (SA).
Figure 1 shows an SA of the example.Figure 1: An example alignment between WSN inTable 2 and English sentence ?Road is slipperywhen raining?.
(a) skeleton link; (b) skeletonalignment.Each span of the WSN comprises words fromdifferent segmentations (Figure 1a), which indi-cates that the confidence score of a SL can be de-fined over words in the same span.
By projectionfunction, a SL can be projected onto the link foreach segmentation.
Therefore, the problem ofcombining word alignment over different segmen-tations can be transformed into the problem of se-lecting SLs for SA first, and then project theselected SLs onto links for each segmentation re-spectively.3  Combination AlgorithmGiven k alignments    over segmentationsrespectively         ), and       is the pairRoad?
1 ?
2 ??
3??
1 ?
2 ?
3?
1 ?
2 ?
3 ?
4(a)(b)??
3?
2?
3Road is slippery when raining2of the Chinese WSN and its parallel English sen-tence.
Suppose     is the SL between the j-th spanand i-th English word   ,is the link betweenthe j-th Chinese wordin    and   .
Inspired byHuang (2009), we define the confidence score ofeach SL as follows(   |   )  ?
(1)whereis the confidence score of thelink, defined as(|   )?
(|   )(2)where c-to-e link posterior probability is defined as(|   )?
(3)and I is the length of  .
E-to-c link posterior prob-ability     (|   )  can be defined similarly,Our alignment combination algorithm is as fol-lows.1.
Build WSN for Chinese sentence.2.
Compute the confidence score for each SLbased on Eq.
(1).
A SL     gets a vote fromifappears in             .
Denotethe set of all SLs getting at least one vote by.3.
All SLs in    are sorted in descending orderand evaluated sequentially.
A SL     is includ-ed if its confidence score is higher than a tuna-ble threshold  , and one of the following istrue1:?
Neither    nor    is aligned so far;?
is not aligned and its left or right neigh-boring word is aligned to    so far;?
is not aligned and its left or rightneighboring word is aligned to    so far.4.
Repeat 3 until no more SLs can be included.All included SLs comprise   .5.
Map SLs in    on each    to get k new align-mentsrespectively, i.e.2         .
For each  , we sort all1 SLs getting   votes are forced to be included without furtherexamination.2 Two or more SLs in    may be projected onto one links in, in this case, we keep only one in.links inin ascending order and evaluatedthem sequentially  Compareand   , A linkis removed fromif it is not appeared in, and one of the following is true:?
bothand    are aligned in;?
There is a word which is neither left norright neighboring word of    but alignedtoin;?
There is a word which is neither left norright neighboring word ofbut alignedto    in.The heuristic in step 3 is similar to Xiang et al(2010), which avoids adding error-prone links.
Weapply the similar heuristic again in step 5 in eachto delete error-prone links.
Theweights in Eq.
(1) and   can be tuned in a hand-aligned dataset to maximize word alignment F-score on anywith hill climbing algorithm.Probabilities in Eq.
(2) and Eq.
(3) can be estimat-ed using GIZA.4 Experiment4.1   DataOur training set contains about 190K Chinese-English sentence pairs from LDC2003E14 corpus.The NIST?06 test set is used as our developmentset and the NIST?08 test set is used as our test set.The Chinese portions of all the data are prepro-cessed by three monolingually motived segmentersrespectively.
These segmenters differ in eithertraining method or specification, includingICTCLAS (I)3, Stanford segmenters with CTB (C)and PKU (P) specifications4 respectively.
We useda phrase-based MT system similar to (Koehn et al,2003), and generated two baseline alignments us-ing GIZA++ enhanced by gdf heuristics (Koehn etal., 2003) and a linear discriminative word align-ment model (DIWA) (Liu et al, 2010) on trainingset with the three segmentations respectively.
A 5-gram language model trained from the Xinhua por-tion of Gigaword corpus was used.
The decodingweights were optimized with Minimum Error RateTraining (MERT) (Och, 2003).
We used the hand-aligned set of 491 sentence pairs in Haghighi et al(2009), the first 250 sentence pairs were used totune the weights in Eq.
(1), and the other 241 were3 http://www.ictclas.org/4 http://nlp.stanford.edu/software/segmenter.shtml3[???]
[?]
[380] [?]
[??]
[???
]relief funds worth 3.8 million us dollars from the national foodstuff department[??]
[??]
[???]
[??]
[??
]chief executive in the hksar[???]
[?]
[380] [?]
[??]
[???]
[??]
[??]
[???]
[??]
[??
]Figure 2: Two examples (left and right respectively) of word alignment on segmentation C. Baselines(DIWA) are in the top half, combined alignments are in the bottom half.
The solid line represents the cor-rect link while the dashed line represents the bad link.
Each word is enclosed in square brackets.used to measure the word alignment quality.
Notethat we adapted the Chinese portion of this hand-aligned set to segmentation C.4.2 Improvement of Word AlignmentWe first evaluate our combination approach on thehand-aligned set (on segmentation C).
Table 3shows the precision, recall and F-score of baselinealignments and combined alignments.As shown in Table 3, the combination align-ments outperformed the baselines (setting C) in allsettings in both GIZA and DIWA.
We notice thatthe higher F-score is mainly due to the higher pre-cision in GIZA but higher recall in DIWA.
InGIZA, the result of C+I and C+P achieve 8.4% and9.5% higher F-score respectively, and both of themoutperformed C+P+I, we speculate it is becauseGIZA favors recall rather than DIWA, i.e.
GIZAmay contain more bad links than DIWA, whichwould lead to more unstable F-score if morealignments produced by GIZA are combined, justas the poor precision (69.68%) indicated.
However,DIWA favors precision than recall (this observa-tion is consistent with Liu et al (2010)), whichmay explain that the more diversified segmenta-tions lead to better results in DIWA.GIZA DIWAsetting P R F P R FC 61.84 84.99 71.59 83.12 78.88 80.94C+P 80.16 79.80 79.98 84.15 79.41 81.57C+I 82.96 79.28 81.08 84.41 81.69 83.03C+I+P 69.68 85.17 77.81 83.38 82.98 83.18Table 3: Alignment precision, recall and F-score.C: baseline, C+I: Combination of C and I.Figure 2 gives baseline alignments and com-bined alignments on two sentence pairs in thetraining data.
As can be seen, alignment errorscaused by inappropriate segmentations by singlesegmenter were substantially reduced.
For exam-ple, in the second example, the word ????????
hksar?
appears in segmentation I of the Chi-nese sentence, which benefits the generation of thethree correct links connecting for words ????
,???
?, ?????
respectively in the com-bined alignment.4.3   Improvement in MT performanceWe then evaluate our combination approach on theSMT training data on all segmentations.
For effi-ciency, we just used the first 50k sentence pairs ofthe aligned training corpus with the three segmen-tations to build three SMT systems respectively.Table 4 shows the BLEU scores of baselines andcombined alignment (C+P+I, and then projectedonto C, P, I respectively).
Our approach achievesimprovement over baseline alignments on all seg-mentations consistently, without using any latticedecoding techniques as Dyer et al (2009).
Thegain of translation performance purely comes fromimprovements of word alignment on all segmenta-tions by our proposed word alignment combination.GIZA DIWASegmentation B Comb B CombC 19.77 20.9 20.18 20.71P 20.5 21.16 20.41 21.14I 20.11 21.14 20.46 21.30Table 4: Improvement in BLEU scores.
B:Baselinealignment, Comb: Combined alignment.45 ConclusionWe evaluated our word alignment combinationover three monolingually motivated segmentationson Chinese-English pair.
We showed that the com-bined alignment significantly outperforms thebaseline alignment with both higher F-score andhigher BLEU score on all segmentations.
Our workalso proved the effectiveness of link confidencescore in combining different word alignment mod-els (Xiang et al, 2010), and extend it to combineword alignments over different segmentations.Xu et al (2005) and Dyer et al (2009) combinedifferent segmentations for SMT.
They aim toachieve better translation but not higher alignmentquality of all segmentations.
They combine multi-ple segmentations at SMT decoding step, while wecombine segmentation alternatives at word align-ment step.
We believe that we can further improvethe performance by combining these two kinds ofworks.
We also believe that combining wordalignments over both monolingually motivated andbilingually motivated segmentations (Ma et al,2009) can achieve higher performance.In the future, we will investigate combiningword alignments on language pairs where bothlanguages have no explicit word boundaries suchas Chinese-Japanese.AcknowledgmentsThis work was supported by the National NaturalScience Foundation of China under Grant No.61003112, and the National Fundamental ResearchProgram of China (2010CB327903).
We wouldlike to thank Xiuyi Jia and Shujie Liu for usefuldiscussions and the anonymous reviewers for theirconstructive comments.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. Del-la Peitra, Robert L. Mercer.
1993.
The Mathematicsof statistical machine translation: parameter estima-tion.
Computational Linguistics, 19(2):263-311.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmenta-tion for machine translation performance.
In Pro-ceedings of third workshop on SMT, Pages:224-232.Tagyoung Chung and Daniel Gildea.
2009.
Unsuper-vised tokenization for machine translation.
In Pro-ceedings of EMNLP, Pages:718-726.Christopher Dyer, Smaranda Muresan, and Philip Res-nik.
2008.
Generalizing word lattice translation.
InProceedings of ACL, Pages:1012-1020.Christopher Dyer.
2009.
Using a maximum entropymodel to build segmentation lattices for mt.
In Pro-ceedings of NAACL, Pages:406-414.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL, Pages:440-447.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In Proceedings of ACL, Pages: 923-931.Fei Huang.
2009.
Confidence measure for word align-ment.
In Proceedings of ACL, Pages:932-940.Philipp Koehn, Franz Josef Och and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL, Pages:48-54.Yang Liu, Qun Liu, Shouxun Lin.
2010.
Discriminativeword alignment by linear modeling.
ComputationalLinguistics, 36(3):303-339.Yanjun Ma, Nicolas Stroppa, and Andy Way.
2007.Bootstrapping word alignment via word packing.
InProceedings of ACL, Pages:304-311.Yanjun Ma and Andy Way.
2009.
Bilingually motivateddomain-adapted word segmentation for statisticalmachine translation.
In Proceedings of EACL, Pag-es:549-557.Bing Xiang, Yonggang Deng, and Bowen Zhou.
2010.Diversify and combine: improving word alignmentfor machine translation on low-resource languages.In Proceedings of ACL, Pages:932-940.Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu,Shouxun Lin.
2010.
Joint tokenization and transla-tion.
In Proceedings of COLING, Pages:1200-1208.Jia Xu, Richard Zens, and Hermann Ney.
2004.
Do weneed Chinese word segmentation for statistical ma-chine translation?
In Proceedings of the ACLSIGHAN Workshop, Pages: 122-128.Jia Xu, Evgeny Matusov, Richard Zens, and HermannNey.
2005.
Integrated Chinese word segmentation instatistical machine translation.
In Proceedings ofIWSLT.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008.
Improved statistical machine translation bymultiple Chinese word segmentation.
In Proceedingsof the Third Workshop on SMT, Pages:216-223.5
