Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 167?176,Honolulu, October 2008. c?2008 Association for Computational LinguisticsBetter Binarization for the CKY ParsingXinying Song?
?
Shilin Ding?
?
Chin-Yew Lin?
?MOE-MS Key Laboratory of NLP and Speech, Harbin Institute of Technology, Harbin, China?Department of Statistics, University of Wisconsin-Madison, Madison, USA?Microsoft Research Asia, Beijing, Chinaxysong@mtlab.hit.edu.cn dingsl@gmail.com cyl@microsoft.comAbstractWe present a study on how grammar binariza-tion empirically affects the efficiency of theCKY parsing.
We argue that binarizations af-fect parsing efficiency primarily by affectingthe number of incomplete constituents gener-ated, and the effectiveness of binarization alsodepends on the nature of the input.
We pro-pose a novel binarization method utilizing richinformation learnt from training corpus.
Ex-perimental results not only show that differ-ent binarizations have great impacts on pars-ing efficiency, but also confirm that our learntbinarization outperforms other existing meth-ods.
Furthermore we show that it is feasible tocombine existing parsing speed-up techniqueswith our binarization to achieve even betterperformance.1 IntroductionBinarization, which transforms an n-ary grammarinto an equivalent binary grammar, is essential forachieving an O(n3) time complexity in the context-free grammar parsing.
O(n3) tabular parsing al-gorithms, such as the CKY algorithm (Kasami,1965; Younger, 1967), the GHR parser (Grahamet al, 1980), the Earley algorithm (Earley, 1970) andthe chart parsing algorithm (Kay, 1980; Klein andManning, 2001) all convert their grammars into bi-nary branching forms, either explicitly or implicitly(Charniak et al, 1998).In fact, the number of all possible binarizationsof a production with n + 1 symbols on its right?This work was done when Xinying Song and Shilin Dingwere visiting students at Microsoft Research Asia.hand side is known to be the nth Catalan NumberCn = 1n+1(2nn).
All binarizations lead to the sameparsing accuracy, but maybe different parsing effi-ciency, i.e.
parsing speed.
We are interested in in-vestigating whether and how binarizations will af-fect the efficiency of the CKY parsing.Do different binarizations lead to different pars-ing efficiency?
Figure 1 gives an example to helpanswer this question.
Figure 1(a) illustrates the cor-rect parse of the phrase ?get the bag and go?.
Weassume that NP ?
NP CC NP is in the originalgrammar.
The symbols enclosed in square bracketsin the figure are intermediate symbols.VPVP VPVB NP CC VBget DT NN and gothe bag(a) final parseVP[NP CC]VP VPVB NP CC VBget DT NN and gobag[VP CC] NP?NP?the(b) with leftVPVP VPVB NP CC VBget DT NN and gobag[CC VP]the(c) with rightFigure 1: Parsing with left and right binarization.If a left binarized grammar is used, see Fig-ure 1(b), an extra constituent [NP CC] spanning?the bag and?
will be produced.
Because rule[NP CC] ?
NP CC is in the left binarized gram-mar and there is an NP over ?the bag?
and a CCover the right adjacent ?and?.
Having this con-stituent is unnecessary, because it lacks an NP tothe right to complete the production.
However, if aright binarization is used, as shown in Figure 1(c),such unnecessary constituent can be avoided.One observation from this example is that differ-ent binarizations affect constituent generation, thusaffect parsing efficiency.
Another observation is that167for rules like X ?
Y CC Y , it is more suitable tobinarize them in a right branching way.
This canbe seen as a linguistic nature: for ?and?, usuallythe right neighbouring word can indicate the correctparse.
A good binarization should reflect such ligu-istic nature.In this paper, we aim to study the effect of bina-rization on the efficiency of the CKY parsing.
To ourknowledge, this is the first work on this problem.We propose the problem to find the optimal bina-rization in terms of parsing efficiency (Section 3).We argue that binarizations affect parsing efficiencyprimarily by affecting the number of incompleteconstituents generated, and the effectiveness of bi-narization also depends on the nature of the input(Section 4).
Therefore we propose a novel binariza-tion method utilizing rich information learnt fromtraining corpus (Section 5).
Experimental resultsshow that our binarization outperforms other exist-ing methods (Section 7.2).Since binarization is usually a preprocessing stepbefore parsing, we argue that better performance canbe achieved by combining other parsing speed-uptechniques with our binarization (Section 6).
Weconduct experiments to confirm this (Section 7.3).2 BinarizationIn this paper we assume that the original gram-mar, perhaps after preprocessing, contains no ?-productions or useless symbols.
However, we allowthe existence of unary productions, since we adoptan extended version of the CKY algorithm whichcan handle the unary productions.
Moreover we donot distinguish nonterminals and terminals explic-itly.
We treat them as symbols.
What we focus on isthe procedure of binarization.Definition 1.
A binarization is a function pi, map-ping an n-ary grammar G to an equivalent binarygrammar G?.
We say that G?
is a binarized grammarof G, denoted as pi(G).Two grammars are equivalent if they define thesame probability distribution over strings (Charniaket al, 1998).We use the most widely used left binarization(Aho and Ullman, 1972) to show the procedure ofbinarization, as illustrated in Table 1, where p and qare the probabilities of the productions.Original grammar Left binarized grammarY ?
ABC : p [AB] ?
AB : 1.0Z ?
ABD : q Y ?
[AB]C : pZ ?
[AB]D : qTable 1: Left binarizationIn the binarized grammar, symbols of form [AB]are new (also called intermediate) nonterminals.Left binarization always selects the left most pair ofsymbols and combines them to form an intermedi-ate nonterminal.
This procedure is repeated until allproductions are binary.In this paper, we assume that all binarizations fol-low the fashion above, except that the choice of pairof symbols for combination can be arbitrary.
Nextwe show three other known binarizations.Right binarization is almost the same with leftbinarization, except that it always selects the rightmost pair, instead of left, to combine.Head binarization always binarizes from the headoutward (Klein and Manning, 2003b).
Please referto Charniak et al (2006) for more details.Compact binarization (Schmid, 2004) tries tominimize the size of the binarized grammar.
It leadsto a compact grammar.
We therefore call it compactbinarization.
It is done via a greedy approach: it al-ways selects the pair that occurs most on the righthand sides of rules to combine.3 The optimal binarizationThe optimal binarization should help CKY parsingto achieve its best efficiency.
We formalize the ideaas follows:Definition 2.
The optimal binarization is pi?, for agiven n-ary grammar G and a test corpus C:pi?
= argminpi T (pi(G), C) (1)where T (pi(G), C) is the running time for CKY toparse corpus C, using the binarized grammar pi(G).It is hard to find the optimal binarization directlyfrom Definition 2.
We next give an empirical anal-ysis of the running time of the CKY algorithm andsimplify the problem by introducing assumptions.3.1 Analysis of CKY parsing efficiencyIt is known that the complexity of the CKY algo-rithm is O(n3L).
The constant L depends on the bi-168narized grammar in use.
Therefore binarization willaffect L. Our goal is to find a good binarization thatmakes parsing more efficient.It is also known that in the inner most loop ofCKY as shown in Algorithm 1, the for-statement inLine 1 can be implemented in several different meth-ods.
The choice will affect the efficiency of CKY.We present here four possible methods:M1 Enumerate all rules X ?
Y Z, and check if Y is inleft span and Z in right span.M2 For each Y in left span, enumerate all rules X ?Y Z, and check if Z is in right span.M3 For each Z in right span, enumerate all rules X ?Y Z, and check if Y is in left span.M4 Enumerate each Y in left span and Z in right span1,check if there are any rules X ?
Y Z.Algorithm 1 The inner most loop of CKY1: for X ?
Y Z, Y in left span and Z in right span2: Add X to parent span3.2 Model assumptionWe have shown that both binarization and the for-statement implementation in the inner most loop ofCKY will affect the parsing speed.About the for-statement implementations, no pre-vious study has addressed which one is superior.The actual choice may affect our study on binariza-tion.
If using M1, since it enumerates all rules inthe grammar, the optimal binarization will be theone with minimal number of rules, i.e.
minimal bi-narized grammar size.
However, M1 is usually notpreferred in practice (Goodman, 1997).
For othermethods, it is hard to tell which binarization is op-timal theoretically.
In this paper, for simplicity rea-sons we do not consider the effect of for-statementimplementations on the optimal binarization.On the other hand, it is well known that reduc-ing the number of constituents produced in parsingcan greatly improve CKY parsing efficiency.
Thatis how most thresholding systems (Goodman, 1997;Tsuruoka and Tsujii, 2004; Charniak et al, 2006)speed up CKY parsing.
Apparently, the number of1Note that we should skip Y (Z) if it never appears as thefirst (second) symbol on the right hand side of any rule.constituents produced in parsing is not affected byfor-statement implementations.Therefore we assume that the running time ofCKY is primarily determined by the number of con-stituents generated in parsing.
We simplify the opti-mal binarization to be:pi?
?
argminpi E(pi(G), C) (2)where E(pi(G), C) is the number of constituentsgenerated when CKY parsing C with pi(G).We next discuss how binarizations affect the num-ber of constituents generated in parsing, and presentour algorithm for finding a good binarization.4 How binarizations affect constituentsThroughout this section and the next, we will use anexample to help illustrate the idea.
The grammar is:X ?
A B C DY ?
A B CC ?
C DZ ?
A B C EW ?
F C D EThe input sentence is 0A1B2C3D4E5, where thesubscripts are used to indicate the positions of spans.For example, [1, 3] stands for BC.
The final parse2is shown in Figure 2.
Symbols surrounded by dashedcircles are fictitious, which do not actually exist inthe parse.FB:[1,2]A:[0,1] C:[2,3] D:[3,4] E:[4,5]WY:[0,3] X:[0,4] C:[2,4]Y:[0,4] Z:[0,5]Figure 2: Parse of the sentence ABC DE4.1 Complete and incomplete constituentsIn the procedure of CKY parsing, there are two kindsof constituents generated: complete and incomplete.Complete constituents (henceforth CCs) are thosecomposed by the original grammar symbols and2More precisely, it is more than a parse tree for it containsall symbols recognized in parsing.169spans.
For example in Figure 2, X : [0, 4], Y : [0, 3]and Y :[0, 4] are all CCs.Incomplete constituents (henceforth ICs) arethose labeled by intermediate symbols.
Figure 2does not show them directly, but we can still read thepossible ones.
For example, if the binarized gram-mar in use contains an intermediate symbol [ABC],then there will be two related ICs [ABC]:[0, 3] and[ABC]:[0, 4] (the latter is due to C:[2, 4]) producedin parsing.
ICs represent the intermediate steps torecognize and complete CCs.4.2 Impact on complete constituentsBinarizations do not affect whether a CC will be pro-duced.
If there is a CC in the parse, whatever bi-narization we use, it will be produced.
The differ-ence merely lies on what intermediate ICs are used.Therefore given a grammar and an input sentence,no matter what binarization is used, the CKY pars-ing will generate the same set of CCs.For example in Figure 2 there is a CC X : [0, 4],which is associated with rule X ?
ABC D. Nomatter what binarization we use, this CC will be rec-ognized eventually.
For example if using left bina-rization, we will get [AB]:[0, 2], [ABC]:[0, 3] andfinally X :[0, 4]; if using right binarization, we willget [C D]:[2, 4], [BC D]:[1, 4] and again X:[0, 4].4.3 Impact on incomplete constituentsBinarizations do affect the generation of ICs, be-cause they generate different intermediate symbols.We discuss the impact on two aspects:Shared IC.
Some ICs can be used to generatemultiple CCs in parsing.
We call them shared.
If abinarization can lead to more shared ICs, then over-all there will be fewer ICs needed in parsing.For example, in Figure 2, if we use left binariza-tion, then [AB]:[0, 2] can be shared to generate bothX :[0, 4] and Y :[0, 3], in which we can save one ICoverall.
However, if right binarization is used, therewill be no common ICs to share in the generationsteps of X :[0, 4] and Y :[0, 3], and overall there areone more IC generated.Failed IC.
For a CC, if it can be recognized even-tually by applying an original rule of length k, what-ever binarization to use, we will have to generate thesame number of k ?
2 ICs before we can completethe CC.
However, if the CC cannot be fully recog-nized but only partially recognized, then the numberof ICs needed will be quite different.For example, in Figure 2, the rule W ?
F C DEcan be only partially recognized over [2, 5], so it can-not generate the corresponding CC.
Right binariza-tion needs two ICs ([DE]:[3, 5] and [C DE]:[2, 5])to find that the CC cannot be recognized, while leftbinarization needs none.As mentioned earlier, ICs are auxiliary means togenerate CCs.
If an IC cannot help generate anyCCs, it is totally useless and even harmful.
We callsuch an IC failed, otherwise it is successful.
There-fore, if a binarization can help generate fewer failedICs then parsing would be more efficient.4.4 Binarization and the nature of the inputNow we show that the impact of binarization alsodepends on the actual input.
When the inputchanges, the impact may also change.For example, in the previous example about therule W ?
F C DE in Figure 2, we believe thatleft binarization is better based on the observationthat there are more snippets of [C DE] in the in-put which lack for F to the left.
If there are moresnippets of [F C D] in the input lacking for E to theright, then right binarization would be better.The discussion above confirms such a view: theeffect of binarization depends on the nature of theinput language, and a good binarization should re-flect this nature.
This accords with our intuition.
Sowe use training corpus to learn a good binarization.And we verify the effectiveness of the learnt bina-rization using a test corpus with the same nature.In summary, binarizations affect the efficiency ofparsing primarily by affecting the number of ICsgenerated, where more shared and fewer failed ICswill help lead to higher efficiency.
Meanwhile, theeffectiveness of binarization also depends on the na-ture of its input language.5 Towards a good binarizationBased on the analysis in the previous section, weemploy a greedy approach to find a good binariza-tion.
We use training corpus to compute metricsfor every possible intermediate symbol.
We use thisinformation to greedily select the best pair to com-bine.1705.1 AlgorithmGiven the original grammar G and training corpusC, for every sentence in C, we firstly obtain the finalparse (like Figure 2).
For every possible intermedi-ate symbol, i.e.
every ngram of the original symbols,denoted by w, we compute the following two met-rics:1.
How many ICs labeled by w can be generatedin the final parse, denoted by num(w) (numberof related ICs).2.
How many CCs can be generated via ICs la-beled by w, denoted by ctr(w) (contribution ofrelated ICs).For example in Figure 2, for a possible inter-mediate symbol [ABC], there are two related ICs([ABC] : [0, 3] and [ABC] : [0, 4]) in the parse,so we have num([ABC]) = 2.
Meanwhile, fourCCs (Y : [0, 3], X : [0, 4], Y : [0, 4] and Z : [0, 5]) canbe generated from the two related ICs.
Thereforectr([ABC]) = 4.
We list the two metrics for everyngram in Figure 2 in Table 2.
We will discuss howto compute these two metrics in Section 5.2.w num ctr w num ctr[AB] 1 4 [BC E] 1 1[ABC] 2 4 [C D] 1 2[ABC D] 1 1 [C DE] 1 0[ABC E] 1 1 [C E] 1 1[BC] 2 4 [DE] 1 0[BC D] 1 1Table 2: Metrics of every ngramThe two metrics indicate the goodness of a possi-ble intermediate symbol w: num(w) indicates howmany ICs labeled by w are likely to be generated inparsing; while ctr(w) represents how much w cancontribute to the generation of CCs.
If ctr(w) islarger, the corresponding ICs are more likely to beshared.
If ctr is zero, those ICs are surely failed.Therefore the smaller num(w) is and the largerctr(w) is, the better w would be.Combining num and ctr, we define a utility func-tion for each ngram w in the original grammar:utility(w) = f(num(w), ctr(w)) (3)where f is a ranking function, satisfying that f(x, y)is larger when x is smaller and y is larger.
We willdiscuss more details about it in Section 5.3.Using utility as the ranking function, we sort allpairs of symbols and choose the best to combine.The formal algorithm is as follows:S1 For every symbol pair of ?v1, v2?
(where v1 andv2 can be original symbols or intermediate symbolsgenerated in previous rounds), let w1 and w2 be thengrams of original symbols represented by v1 andv2, respectively.
Let w = w1w2 be the ngram rep-resented by the symbol pair.
Compute utility(w).S2 Select the ngram w with the highest utility(w), letit be w?
(in case of a tie, select the one with asmaller num).
Let the corresponding symbol pairbe ?v?1 , v?2?.S3 Add a new intermediate symbol v?, and replace allthe occurrences of ?v?1 , v?2?
on the right hand sidesof rules with v?.S4 Add a new rule v?
?
v?1v?2 : 1.0.S5 Repeat S1 ?
S4, until there are no rules with morethan two symbols on the right hand side.5.2 Metrics computingIn this section, we discuss how to compute num andctr in details.Computing ctr is straightforward.
First we getfinal parses like in Figure 2 for training sentences.From a final parse, we traverse along every parentnode and enumerate every subsequence of its childnodes.
For example in Figure 2, from the parentnode of X : [0, 4], we can enumerate the follow-ing: [AB] : [0, 2], [ABC] : [0, 3], [ABC D] : [0, 4],[BC]:[1, 3], [BC D]:[1, 4], [C D]:[2, 4].
We add 1 toall the ctr of these ngrams, respectively.To compute num, we resort to the same ideaof dynamic programming as in CKY.
We performa normal left binarization except that we add allngrams in the original grammar G as intermediatesymbols into the binarized grammar G?.
For exam-ple, for the rule of S ?
ABC : p, the constructedgrammar is as follows:[AB] ?
A B : 1.0S ?
[AB] C : p[BC] ?
B C : 1.0Using the constructed G?, we employ a normalCKY parsing on the training corpus and compute171how many constituents are produced for each ngram.The result is num.
Suppose the length of the train-ing sentence is n, the original grammar G has Nsymbols, and the maximum length of rules is k,then the complexity of this method can be writtenas O(Nkn3).5.3 Ranking functionWe discuss the details of the ranking function f usedto compute the utility of each ngram w. We comeup with two forms for f : linear and log-linear1.
linear: f(x, y) = ?
?1x+ ?2y2.
log-linear3: f(x, y) = ?
?1 log(x) + ?2 log(y)where ?1 and ?2 are non-negative weights subject to?1 + ?2 = 14.We will use development set to determine whichform is better and to learn the best weight settings.6 Combination with other techniquesBinarization usually plays a role of preprocessing inthe procedure of parsing.
Grammars are binarizedbefore they are fed into the stage of parsing.
Thereare many known works on speeding up the CKYparsing.
So we can expect that if we replace thepart of binarization by a better one while keepingthe subsequent parsing unchanged, the parsing willbe more efficient.
We will conduct experiment toconfirm this idea in the next section.We would like to make more discussions be-fore we advance to the experiments.
The first isabout parsing accuracy in combining binarizationwith other parsing speed-up techniques.
Binariza-tion itself does not affect parsing accuracy.
Whencombined with exact inference algorithms, like theiterative CKY (Tsuruoka and Tsujii, 2004), the ac-curacy will be the same.
However, if combined withother inexact pruning techniques like beam-pruning(Goodman, 1997) or coarse-to-fine parsing (Char-niak et al, 2006), binarization may interact withthose pruning methods in a complicated way to af-fect parsing accuracy.
This is due to different bina-rizations generate different sets of intermediate sym-3For log-linear form, if num(w) = 0 (and consequentlyctr(w) = 0), we set f(num(w), ctr(w)) = 0; if num(w) >0 but ctr(w) = 0, we set f(num(w), ctr(w)) = ?
?.4Since f is used for ranking, the magnitude is not important.bols.
With the same complete constituents, one bi-narization might derive incomplete constitutes thatcould be pruned while another binarization may not.This would affect the accuracy.
We do not addressthis interaction on in this paper, but leave it to thefuture work.
In Section 7.3 we will use the iterativeCKY for testing.In addition, we believe there exist some speed-uptechniques which are incompatible with our bina-rization.
One such example may be the top-downleft-corner filtering (Graham et al, 1980; Moore,2000), which seems to be only applicable to the pro-cess of left binarization.
A detailed investigation onthis problem will be left to the future work.The last issue is how our binarization performson a lexicalized parser, like Collins (1997).
Our in-tuition is that we cannot apply our binarization toCollins (1997).
The key fact in lexicalized parsersis that we cannot explicitly write down all rulesand compute their probabilities precisely, due to thegreat number of rules and the severe data sparsityproblem.
Therefore in Collins (1997) grammar rulesare already factorized into a set of probabilities.In order to capture the dependency relationship be-tween lexcial heads Collins (1997) breaks down therules from head outwards, which prevents us fromfactorizing them in other ways.
Therefore our bina-rization cannot apply to the lexicalized parser.
How-ever, there are state-of-the-art unlexicalized parsers(Klein and Manning, 2003b; Petrov et al, 2006), towhich we believe our binarization can be applied.7 ExperimentsWe conducted two experiments on Penn Treebank IIcorpus (Marcus et al, 1994).
The first is to com-pare the effects of different binarizations on parsingand the second is to test the feasibility to combineour work with iterative CKY parsing (Tsuruoka andTsujii, 2004) to achieve even better efficiency.7.1 Experimental setupFollowing conventions, we learnt the grammar fromWall Street Journal (WSJ) section 2 to 21 and mod-ified it by discarding all functional tags and emptynodes.
The parser obtained this way is a pure un-lexicalized context-free parser with the raw treebankgrammar.
Its accuracy turns out to be 72.46% in172terms of F1 measure, quite the same as 72.62% asstated in Klein and Manning (2003b).
We adopt thisparser in our experiment not only because of sim-plicity but also because we focus on parsing effi-ciency.For all sentences with no more than 40 words insection 22, we use the first 10% as the developmentset, and the last 90% as the test set.
There are 158and 1,420 sentences in development set and test set,respectively.
We use the whole 2,416 sentences insection 23 as the training set.We use the development set to determine the bet-ter form of the ranking function f as well as totune its weights.
Both metrics of num and ctrare normalized before use.
Since there is only onefree variable in ?1 and ?2, we can just enumerate0 ?
?1 ?
1, and set ?2 = 1 ?
?1.
The increasingstep is firstly set to 0.05 for the approximate loca-tion of the optimal weight, then set to 0.001 to learnmore precisely around the optimal.We find that the optimal is 5,773,088 (constituentsproduced in parsing development set) with ?1 =0.014 for linear form, while for log-linear form theoptimal is 5,905,292 with ?1 = 0.691.
Therefore wedetermine that the better form for the ranking func-tion is linear with ?1 = 0.014 and ?2 = 0.986.The size of each binarized grammar used in theexperiment is shown in Table 3.
?Original?
refersto the raw treebank grammar.
?Ours?
refers to thelearnt binarized grammar by our approach.
For therest please refer to Section 2.# of Symbols # of RulesOriginal 72 14,971Right 10,654 25,553Left 12,944 27,843Head 11,798 26,697Compact 3,644 18,543Ours 8,407 23,306Table 3: Grammar size of different binarizationsWe also tested whether the size of the training setwould have significant effect.
We use the first 10%,20%, ?
?
?
, up to 100% of section 23 as the trainingset, respectively, and parse the development set.
Wefind that all sizes examined have a similar impact,since the numbers of constituents produced are allaround 5,780,000.
It means the training corpus doesnot have to be very large.The entire experiments are conducted on a serverwith an Intel Xeon 2.33 GHz processor and 8 GBmemory.7.2 Experiment 1: compare amongbinarizationsIn this part, we use CKY to parse the entire test setand evaluate the efficiency of different binarizations.The for-statement implementation of the innermost loop of CKY will affect the parsing timethough it won?t affect the number of constituentsproduced as discussed in Section 3.2.
The best im-plementations may be different for different bina-rized grammars.
We examine M1?M4, testing theirparsing time on the development set.
Results showthat for right binarization the best method is M3,while for the rest the best is M2.
We use the bestmethod for each binarized grammar when compar-ing the parsing time in Experiment 1.Table 4 reports the total number of constituentsand total time required for parsing the entire test set.It shows that different binarizations have great im-pacts on the efficiency of CKY.
With our binariza-tion, the number of constituents produced is nearly20% of that required by right binarization and nearly25% of that by the widely-used left binarization.
Asfor the parsing time, CKY with our binarization isabout 2.5 times as fast as with right binarization andabout 1.75 times as fast as with left binarization.This illustrates that our binarization can significantlyimprove the efficiency of the CKY parsing.Binarization Constituents Time (s)Right 241,924,229 5,747Left 193,238,759 3,474Head 166,425,179 3,837Compact 94,257,478 2,302Ours 52,206,466 2,182Table 4: Performance on test setFigure 3 reports the detailed number of completeconstituents, successful incomplete constituents andfailed incomplete constituents produced in parsing.The result proves that our binarization can signifi-cantly reduce the number of failed incomplete con-stituents, by a factor of 10 in contrast with left bi-narization.
Meanwhile, the number of successful in-173complete constituents is also reduced by a factor of2 compared to left binarization.0.0e+002.0e+074.0e+076.0e+078.0e+071.0e+081.2e+081.4e+081.6e+081.8e+08Right Left Head Compact Ours#ofConstituentsBinarizationscompletesuccessful incompletefailed incompleteFigure 3: Comparison on various constituentsAnother interesting observation is that parsingwith a smaller grammar does not always yield ahigher efficiency.
Our binarized grammar is morethan twice the size of compact binarization, but oursis more efficient.
It proves that parsing efficiency isrelated to both the size of grammar in use as well asthe number of constituents produced.In Section 1, we used an example of ?get thebag and go?
to illustrate that for rules like X ?Y CC Y , right binarization is more suitable.
Wealso investigated the corresponding linguistic naturethat the word to the right of ?and?
is more likely toindicate the true relationship represented by ?and?.We argued that a better binarization can reflect suchlinguistic nature of the input language.
To our sur-prise, our learnt binarization indeed captures this lin-guistic insight, by binarizing NP ?
NP CC NPfrom right to left.Finally, we would like to acknowledge the limi-tation of our assumption made in Section 3.2.
Ta-ble 4 shows that the parsing time of CKY is notalways monotonic increasing with the number ofconstituents produced.
Head binarization producesfewer constituents than left binarization but con-sumes more parsing time.7.3 Experiment 2: combine with iterative CKYIn this part, we test the performance of combiningour binarization with the iterative CKY (Tsuruokaand Tsujii, 2004) (henceforth T&T) algorithm.Iterative CKY is a procedure of multiple passesof normal CKY: in each pass, it uses a threshold toprune bad constituents; if it cannot find a successfulparse in one pass, it will relax the threshold and startanother; this procedure is repeated until a successfulparse is returned.
T&T used left binarization.
Were-implement their experiments and combine itera-tive CKY with our binarization.
Note that iterativeCKY is an exact inference algorithm that guaranteesto return the optimal parse.
As discussed in Sec-tion 6, the parsing accuracy is not changed in thisexperiment.T&T used a held-out set to learn the best step ofthreshold decrease.
They reported that the best stepwas 11 (in log-probability).
We found that the beststep was indeed 11 for left binarization; for our bina-rizaiton, the best step was 17.
T&T used M4 as thefor-statement implementation of CKY.
In this part,we follow the same method.The result is shown in Table 5.
We can see thatiterative CKY can achieve better performance by us-ing a better binarization.
We also see that the reduc-tion by binarization with pruning is less significantthan without pruning.
It seems that the pruning itselfin iterative CKY can counteract the reduction effectof binarization to some extent.
Still the best per-formance is archieved by combining iterative CKYwith a better binarization.CKY + Binarization Constituents Time (s)Tsuruoka and Tsujii (2004)CKY + Left 45,406,084 1,164Iterative CKY + Left 17,520,427 613ReimplementCKY + Left 52,128,941 932CKY + Ours 14,892,203 571Iterative CKY + Left 23,267,594 377Iterative CKY + Ours 10,966,272 314Table 5: Combining with iterative CKY parsing8 Related workAlmost all work on parsing starts from a binarizedgrammar.
Usually binarization plays a role of pre-processing.
Left binarization is widely used (Ahoand Ullman, 1972; Charniak et al, 1998; Tsuruokaand Tsujii, 2004) while right binarization is rarelyused in the literature.
Compact binarization was in-troduced in Schmid (2004), based on the intuitionthat a more compact grammar will help acheive ahighly efficient CKY parser, though from our exper-iment it is not always true.174We define the fashion of binarizations in Sec-tion 2, where we encode an intermediate symbol us-ing the ngrams of original symbols (content) it de-rives.
This encoding is known as the Inside-Trie (I-Trie) in Klein and Manning (2003a), in which theyalso mentioned another encoding called Outside-Trie (O-Trie).
O-Trie encodes an intermediate sym-bol using the its parent and the symbols surroundingit in the original rule (context).
Klein and Manning(2003a) claimed that O-Trie is superior for calculat-ing estimates for A* parsing.
We plan to investigatebinarization defined by O-Trie in the future.Both I-Trie and O-Trie are equivalent encodings,resulting in equivalent grammars, because they bothencode using the complete content or context infor-mation of an intermediate symbol.
If we use part ofthe information to encode, for example just parent inO-Trie case, the encoding will be non-equivalent.Proper non-equivalent encodings are used to gen-eralize the grammar and prevent the binarized gram-mar becoming too specific (Charniak et al, 2006).
Itis equipped with head binarization to help improveparsing accuracy, following the traditional linguisticinsight that phrases are organized around the head(Collins, 1997; Klein and Manning, 2003b).
In con-trast, we focus our attention on parsing efficiencynot accuracy in this paper.Binarization also attracts attention in the syntax-based models for machine translation, where trans-lation can be modeled as a parsing problem and bi-narization is essential for efficient parsing (Zhanget al, 2006; Huang, 2007).Wang et al (2007) employs binarization to de-compose syntax trees to acquire more re-usabletranslation rules in order to improve translation ac-curacy.
Their binarization is restricted to be a mix-ture of left and right binarization.
This constraintmay decrease the power of binarization when ap-plied to speeding up parsing in our problem.9 Conclusions and future workWe have studied the impact of grammar binarizationon parsing efficiency and presented a novel bina-rization which utilizes rich information learnt fromtraining corpus.
Experiments not only showed thatour learnt binarization outperforms other existingones in terms of parsing efficiency, but also demon-strated the feasibility to combine our binarizationwith known parsing speed-up techniques to achieveeven better performance.An advantage of our approach to finding a goodbinarization would be that the training corpus doesnot need to be parsed sentences.
Only POS taggedsentences will suffice for training.
This will save theeffort to adapt the model to a new domain.Our approach is based on the assumption that theefficiency of CKY parsing is primarily determinedby the number of constituents produced.
This is afairly sound one, but not always true, as shown inSection 7.2.
One future work will be relaxing theassumption and finding a better appraoch.Another future work will be to apply our work tochart parsing.
It is known that binarization is alsoessential for an O(n3) complexity of chart parsing,where dotted rules are used to binarize the grammarimplicitly from left.
As shown in Charniak et al(1998), we can binarize explicitly and use intermedi-ate symbols to replace dotted rules in chart parsing.Therefore chart parsing can use multiple binariza-tions.
We expect that a better binarization will alsohelp improve the efficiency of chart parsing.AcknowledgementsWe thank the anonymous reviwers for their pertinentcomments, Yoshimasa Tsuruoka for the detailed ex-planations on his referred paper, Yunbo Cao, Shu-jian Huang, Zhenxing Wang , John Blitzer and LiangHuang for their valuable suggestions in preparingthe paper.ReferencesAho, A. V. and Ullman, J. D. (1972).
The theoryof parsing, translation, and compiling.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.Charniak, E., Goldwater, S., and Johnson, M.(1998).
Edge-based best-first chart parsing.
InProceedings of the Six Workshop on Very LargeCorpora, pages 127?133.Charniak, E., Johnson, M., Elsner, M., Austerweil,J., Ellis, D., Haxton, I., Hill, C., Shrivaths, R.,Moore, J., Pozar, M., and Vu, T. (2006).
Multi-level coarse-to-fine pcfg parsing.
In HLT-NAACL.Collins, M. (1997).
Three generative, lexicalisedmodels for statistical parsing.
In ACL.175Earley, J.
(1970).
An efficient context-free parsingalgorithm.
Commun.
ACM, 13(2):94?102.Goodman, J.
(1997).
Global thresholding andmultiple-pass parsing.
In EMNLP.Graham, S. L., Harrison, M. A., and Ruzzo, W. L.(1980).
An improved context-free recognizer.ACM Trans.
Program.
Lang.
Syst., 2(3):415?462.Huang, L. (2007).
Binarization, synchronous bina-rization, and target-side binarization.
In Proceed-ings of SSST, NAACL-HLT 2007 / AMTA Work-shop on Syntax and Structure in Statistical Trans-lation, pages 33?40, Rochester, New York.
Asso-ciation for Computational Linguistics.Kasami, T. (1965).
An efficient recognition andsyntax analysis algorithm for context-free lan-guages.
Technical Report AFCRL-65-758, AirForce Cambridge Research Laboratory, Bedford,Massachusetts.Kay, M. (1980).
Algorithm schemata and data struc-tures in syntactic processing.
Technical ReportCSL80-12, Xerox PARC, Palo Alto, CA.Klein, D. and Manning, C. D. (2001).
Parsing andhypergraphs.
In IWPT.Klein, D. and Manning, C. D. (2003a).
A* parsing:Fast exact viterbi parse selection.
In HLT-NAACL.Klein, D. and Manning, C. D. (2003b).
Accurateunlexicalized parsing.
In ACL.Marcus, M. P., Kim, G., Marcinkiewicz, M. A.,MacIntyre, R., Bies, A., Ferguson, M., Katz, K.,and Schasberger, B.
(1994).
The penn treebank:Annotating predicate argument structure.
In HLT-NAACL.Moore, R. C. (2000).
Improved left-corner chartparsing for large context-free grammars.
In IWPT.Petrov, S., Barrett, L., Thibaux, R., and Klein, D.(2006).
Learning accurate, compact, and inter-pretable tree annotation.
In ACL.Schmid, H. (2004).
Efficient parsing of highly am-biguous context-free grammars with bit vectors.In COLING.Tsuruoka, Y. and Tsujii, J.
(2004).
Iterative cky pars-ing for probabilistic context-free grammars.
InIJCNLP.Wang, W., Knight, K., and Marcu, D. (2007).
Bina-rizing syntax trees to improve syntax-based ma-chine translation accuracy.
In EMNLP-CoNLL.Younger, D. H. (1967).
Recognition and parsing ofcontext-free languages in time n3.
Informationand Control, 10(2):189?208.Zhang, H., Huang, L., Gildea, D., and Knight, K.(2006).
Synchronous binarization for machinetranslation.
In HLT-NAACL.176
