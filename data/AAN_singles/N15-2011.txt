Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 79?87,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsLearning Kernels for Semantic Clustering: A Deep ApproachIgnacio Arroyo-Fern?andezUniversidad Nacional Aut?onoma de M?exico (UNAM)iarroyof@iingen.unam.mxAbstractIn this thesis proposal we present a novelsemantic embedding method, which aims atconsistently performing semantic clustering atsentence level.
Taking into account specialaspects of Vector Space Models (VSMs), wepropose to learn reproducing kernels in clas-sification tasks.
By this way, capturing spec-tral features from data is possible.
These fea-tures make it theoretically plausible to modelsemantic similarity criteria in Hilbert spaces,i.e.
the embedding spaces.
We could im-prove the semantic assessment over embed-dings, which are criterion-derived representa-tions from traditional semantic vectors.
Thelearned kernel could be easily transferred toclustering methods, where the Multi-Class Im-balance Problem is considered (e.g.
semanticclustering of definitions of terms).1 IntroductionOverall in Machine Learning algorithms (Duda etal., 2012), knowledge is statistically embedded viathe Vector Space Model (VSM), which is alsonamed the semantic space (Landauer et al, 1998;Pad?o and Lapata, 2007; Baroni and Lenci, 2010).Contrarily to it is usually conceived in text dataanalysis (Manning et al, 2009; Aggarwal and Zhai,2012), not any data set is suitable to embed into `pmetric spaces, including euclidean spaces (p = 2)(Riesz and Nagy, 1955).
This implies that, in par-ticular, clustering algorithms are being adapted tosome `p-derived metric, but not to semantic vectorsets (clusters) (Qin et al, 2014).The above implication also means that seman-tic similarity measures are commonly not consis-tent, e.g.
the cosine similarity or transformation-based distances (Sidorov et al, 2014).
These aremainly based on the concept of triangle.
Thusif the triangle inequality does not hold (which in-duces norms for Hilbert spaces exclusively), thenthe case of the cosine similarity becomes mathe-matically inconsistent1.
Despite VSMs are some-times not mathematically analyzed, traditional algo-rithms work well enough for global semantic anal-ysis (hereinafter global analysis, i.e.
at documentlevel where Zipf?s law holds).
Nevertheless, for lo-cal analysis (hereinafter local analysis, i.e., at sen-tence, phrase or word level) the issue remains stillopen (Mikolov et al, 2013).In this thesis proposal, we will address the maindifficulties raised from traditional VSMs for localanalysis of text data.
We consider the latter as an ill-posed problem (which implies unstable algorithms)in the sense of some explicit semantic similarity cri-terion (hereinafter criterion), e.g.
topic, concept,etc.
(Vapnik, 1998; Fernandez et al, 2007).
The fol-lowing feasible reformulation is proposed.
By learn-ing a kernel in classification tasks, we want to inducean embedding space (Lanckriet et al, 2004; Corteset al, 2009).
In this space, we will consider rele-vance (weighting) of spectral features of data, whichare in turn related to the shape of semantic vectorsets (Xiong et al, 2014).
These vectors would bederived from different Statistical Language Models(SLMs); i.e.
countable things, e.g.
n-grams, bag-of-words (BoW), etc.
; which in turn encode language1Riesz (1955) gives details about Hilbert spaces.79aspects (e.g.
semantics, syntax, morphology, etc.
).Learned kernels are susceptible to be transferred toclustering methods (Yosinski et al, 2014; Bengio etal., 2014), where spectral features would be properlyfiltered from text (Gu et al, 2011).When both learning and clustering processes areperformed, the kernel approach is tolerant enoughfor data scarcity.
Thus, eventually, we could haveany criterion-derived amount of semantic clustersregardless of the Multi-Class Imbalance Problem(MCIP) (Sugiyama and Kawanabe, 2012).
It is ararely studied problem in Natural Language Pro-cessing (NLP), however, contributions can be help-ful in a number of tasks such as IE, topic modeling,QA systems, opinion mining, Natural Language Un-derstanding, etc.This paper is organized as follows: In Section2 we show our case study.
In Section 3 we showthe embedding framework.
In Section 4 we presentour learning problem.
Sections 5 and 6 respectivelyshow research directions and related work.
In Sec-tion 7, conclusions and future work are presented.2 A case study and backgroundA case study.
Semantic clustering of definitions ofterms is our case study.
See the next extracted2ex-amples for the terms window and mouse.
For eachof them, the main acception is showed first, and af-terwards three secondary acceptions:1.
A window is a frame including a sheet of glass or othermaterial capable of admitting light...(a) The window is the time elapsed since a passengercalls to schedule...(b) A window is a sequence region of 20-codon lengthon an alignment of homologous genes...(c) A window is any GUI element and is usually iden-tified by a Windows handle...2.
A mouse is a mammal classified in the order Rodentia,suborder Sciurognathi....(a) A mouse is a small object you can roll along ahard, flat surface...(b) A mouse is a handheld pointing device used toposition a cursor on a computer...(c) The Mouse is a fictional character in Alice?s Ad-ventures in Wonderland by Lewis Carroll...In the example 1, it is possible to assign the fouracceptions to four different semantic groups (thewindow (1), transport services (1a), genetics (1b)2www.describe.com.mxand computing (1c)) by using lexical features (boldterms).
This example also indicates how abstractconcepts are always latent in the definitions.
Theexample 2 is a bit more complex.
Unlike to example1, there would be three clusters because there aretwo semantically similar acceptions (2a and 2b arerelated to computing).
However, they are lexicallyvery distant.
See that in both examples the amountof semantic clusters can?t be defined a priory (un-like to Wikipedia).
Additionally, it is impossible toknow what topic the users of an IE system could beinterested in.
These issues, point out the need for an-alyzing the way we are currently treating semanticspaces in the sense of stability of algorithms (Vap-nik, 1998), i.e.
the existence of semantic similarityconsistence, although Zipf?s law scarcely holds (e.g.in local analysis).Semantic spaces and embeddings.
Erk (2012)and Brychc?
?n (2014) showed insightful empiricismabout well known semantic spaces for differentcases in global analysis.
In this work we have spe-cial interest in local analysis, where semantic vec-tors are representations (embeddings) derived fromlearned feature maps for specific semantic assess-ments (Mitchell and Lapata, 2010).
These featuremaps are commonly encoded in Artificial NeuralNetworks (ANNs) (Kalchbrenner et al, 2014).ANNs have recently attracted worldwide atten-tion.
Given their surprising adaptability to unknowndistributions, they are used in NLP for embeddingand feature learning in local analysis, i.e.
DeepLearning (DL) (Socher et al, 2011; Socher et al,2013).
However, we require knowledge transfer to-wards clustering tasks.
It is still not feasible by usingANNs (Yosinski et al, 2014).
Thus, theoretical ac-cess becomes ever more necessary, so it is worth ex-tending Kernel Learning (KL) studies as alternativefeature learning method in NLP (Lanckriet et al,2004).
Measuring subtle semantic displacements,according to a criterion, is theoretically attainable ina well defined (learned) reproducing kernel Hilbertspace (RKHS), e.g.
some subset of L2(Aronszajn,1950).
In these spaces, features are latent abstrac-tion levels3of data spectrum, which improves kernelscaling (Dai et al, 2014; Anandkumar et al, 2014).3Mainly in DL, it is known there are different hierarchies ofgenerality of features learned by a learning machine.80Figure 1: General schema of the transformation frame-work from some traditional VSM (left) to a well definedembedding space (right).3 RKHS and semantic embeddingsWe propose mapping sets of semantic vectors (e.g.BoW) into well defined function spaces (RKHSs),prior to directly endowing such sets (not elliptical orat least convex (Qin et al, 2014)) with the euclideannorm, ?.
?2(see Figure 1).
For the aforesaid purpose,we want to take advantage of the RKHSs.Any semantic vector xo?
X could be consis-tently embedded (transformed) into a well definedHilbert space by using the reproducing property of akernel k(?, ?)
(Shawe-Taylor and Cristianini, 2004):fxo(x) = ?f(?
), k(?, x)?H; ?x ?
X (1)where: H ?
L2is a RKHS, fxo(?)
?
H is the em-bedding derived from xo, which can be seen as fixedparameter of k(?, xo) = f(?)
?
H. This embeddingfunction is defined over the vector domain {x} ?
Xand ?
?, ?
?H: X ?
H is the inner product inH.Always that (1) holds, k(?, ?)
is a positive definite(PD) kernel function, so X does not need even tobe a vector space and even then, convergence of anysequence {fn(x) : fn?
H;n ?
N} can be ensured.The above is a highly valuable characteristic of theresulting function space (Smola et al, 2007):limn?
?fn= f ??
limn?
?kn(?, x) = k(?, x).
(2)The result (2) implies that convergence of summa-tion of initial guessing kernel functions kn(?, ?)
?
Halways occurs, hence talking about the existence ofa suitable kernel function k(?, ?)
?
H in (1) is ab-solutely possible.
It means that L2operations canbe consistently applied, e.g.
the usual norm ?
?
?2,trigonometric functions (e.g.
cos ?)
and distanced2= ?fn?
fm?2: m 6= n. Thus, from right sideof (2), in order that (1) holds convergence of theFourier series decomposition of k(?, ?)
towards thespectrum of desired features from data is necessary;i.e., by learning parameters and hyperparameters4ofthe series (Ong et al, 2005; B?az?avan et al, 2012).3.1 Learnable kernels for language featuresAssume (1) and (2) hold.
For some SLM a encodedin a traditional semantic space, it is possible to de-fine a learnable kernel matrix Kaas follows (Lanck-riet et al, 2004; Cortes et al, 2009):Ka:=p?i=1?iKi, (3)where {Ki}pi=1?
K is the set of p initial guess-ing kernel matrices (belonging to the family K, e.g.Gaussian) with fixed hyperparameters and ?i?s areparameters weighting Ki?s.
Please note that, forsimplicity, we are using matrices associated to ker-nel functions ki(?, ?
), ka(?, ?)
?
H, respectively.In the Fourier domain and bandwidth.
In fact(3) is a Fourier series, where ?i?s are decomposi-tion coefficients of Ka(B?az?avan et al, 2012).
Thiskernel would be fitting the spectrum of some SLMthat encodes some latent language aspect from text(Landauer et al, 1998).
On one hand, in Fourierdomain operations (e.g.
the error vector norm) areclosed inL2, i.e., according to (2) convergence is en-sured as a Hilbert space is well defined.
Moreover,the L2-regularizer is convex in terms of the Fourierseries coefficients (Cortes et al, 2009).
The afore-mentioned facts imply benefits in terms of compu-tational complexity (scaling) and precision (Dai etal., 2014).
On the other hand, hyperparameters ofinitial guessing kernels are learnable for detectingthe bandwitdh of data (Ong et al, 2005; B?az?avan etal., 2012; Xiong et al, 2014).
Eventually, the lat-ter fact would lead us to know (learning) bounds for4So called in order to make distinction between weights(kernel parameters or coefficients) and the basis function pa-rameters (hyperparameters), e.g.
mean and variance.81the necessary amount of data to properly train ourmodel (the Nyquist theorem).Cluster shape.
A common shape among clustersis considered even for unseen clusters with differ-ent, independent and imbalanced prior probabilitydensities (Vapnik, 1998; Sugiyama and Kawanabe,2012).
For example, if data is Guassian-distributedin the input space, then shape of different clusterstend to be elliptical (the utopian `2case), althoughtheir densities are not regular or even very imbal-anced.
Higher abstraction levels of the data spec-trum possess mentioned traits (Ranzato et al, 2007;Baktashmotlagh et al, 2013).
We will suggest belowa more general version of (3), thereby consideringhigher abstraction levels of text data.4 Learning our kernel in a RKHSA transducer is a setting for learning parameters andhyperparameters of a multikernel linear combinationlike the Fourier series (3) (B?az?avan et al, 2012).Overall, the above setting consists on defininga multi-class learning problem over a RKHS: letY?= {y`}y`?Nbe a sequence of targets induc-ing a semantic criterion ?, likewise a training setX = {x`}x`?Rnand a set of initial guessing kernels{K?i}pi=1?
K with the associated hyperparametervector ?a= {?i}pi=1.
Then for some SLM a ?
A,we would learn the associated kernel matrix Kabyoptimizing the SLM empirical risk functional:JA(?a, ?a) = LA(Ka,X ,Y?)
+ ?
(?a) + ?
(?a),(4)where in JA(?, ?)
we have:Ka=?1?i?p?iK?i.
(5)The learning is divided in two interrelated stages:at the first stage, the free parameter vector ?a={?i}pi=1in (5) (a particular version of (3)), is opti-mized for learning a partial kernel?Ka, given a fixed(sufficiently small) ?aand by using the regularizer?
(?a) over the SLM prediction loss LA(?, ?)
in (4).Conversely at the second stage ?ais free, thus byusing the regularizer ?
(?a) over the prediction lossLA(?, ?
), given that the optimal ?
?awas found at thefirst stage, we could have the optimal ?
?aand there-fore K?ais selected.At higher abstraction levels, given the association{X ,Y?
}, the transducer setting would learn a ker-nel function that fits a multi-class partition of X viasummation of Ka?s.
Thus, we can use learned ker-nels K?aas new initial guesses in order to learn acompound kernel matrix K?for a higher abstractionlevel:J (??)
= L(K?,X ,Y?)
+ ?(??
), (6)where in the general risk functional J (?)
we have:K?=?a?A?aK?a.
(7)In (6) the vector ?
?= {?a}a?Aweights seman-tic representations K?aassociated to each SLM and?(??)
is a proper regularizer over the general lossL(?, ?).
The described learning processes can evenbe jointly performed (B?az?avan et al, 2012).
Theaforementioned losses and regularizers can be con-veniently defined (Cortes et al, 2009).4.1 The learned kernel functionIn order to make relevant features to emerge fromtext, we would use our learned kernel K??.
Thus if{??
?, {?
?a, ?
?a}a?A} is the solution set of the learn-ing problems (4) and (6), then combining (5) and (7)gives the embedding kernel function, for |A| differ-ent SLMs as required (see Figure 2):Definition 1.
Given a semantic criterion ?, then thelearned parameters {??
?, {?
?a, ?
?a}a?A} are eigen-values of kernels {K?a}a?A?
K?
?, respectively5.Thus according to (1), we have for any semantic vec-tor xo?
X its representation fxo(x) ?
H:fxo(x) :=?a?Ap?i=1??a?
?iki(x, xo)= k?
(x, xo) ?
K??xo.
(8)In (8), ki(?, ?
), k?
(?, ?)
?
H ?
L2are reproducingkernel functions associated to matrices K?iand K?,respectively.
The associated {?
?a}a?Awould be op-timally fitting the bandwidth of data.
X ?
X is acompounding semantic space from different SLMs5(i) The symbol ???
denotes subordination (from right toleft) between operators, i.e.
hierarchy of abstraction levels.
(ii)See (Shawe-Taylor and Cristianini, 2004; Anandkumar et al,2014) for details about eigendecompositions.82Figure 2: Sketch (bold plot) of the abstraction levels ofsome learned kernel function k?(?.?)
?
H ?
L2.a ?
A (B?az?avan et al, 2012).
According to ?, se-mantic clustering could be consistently performed inH by computing any L2similarity measure betweenembeddings {fxn, fxm}, which are derived from anysemantic vectors xn, xm?
X , e.g.
(i) the kernelcorrelation coefficient ?
?= ?k?
(xn, xm) ?
[0, 1];with ?
=1?fxn?
?fxm?, and (ii) the distance by sim-ply computing d2= ?fxn?
fxm?2.Please note that we could extend Definition 1 todeeper levels (layers) associated to abstraction lev-els of SLMs.
These levels could explicitly encodemorphology, syntax, semantics or compositional se-mantics, i.e.
{Ka}a?A= KSLMs?
Kaspects.5 Research directionsOur main research direction is to address in detaillinguistic interpretations associated to second mem-ber of (8), which is still not clear.
There are poten-tial ways of interpreting pooling operations over theexpansion of either eigenvalues or eigenfunctions offxo(?).
This fact could lead us to an alternative wayof analyzing written language, i.e.
in terms of thespectral decomposition of X given ?.As another direction we consider data scarcity(low annotated resources).
It is a well handled issueby spectral approaches like the proposed one, so itis worth investigating hyperparameter learning tech-niques.
We consider hyperparameters as the lowestabstraction level of the learned kernel and they areaimed at data bandwidth estimation (i.e.
by tuningthe ?iassociated to each ki(?, ?)
in (8)).
This esti-mation could help us to try to answer the questionof how much training data is enough.
This ques-tion is also related to the quality bounds of a learnedkernel.
These bounds could be used to investigatethe possible relation among the number of annotatedclusters, the training set size and the generalizationability.
The latter would be provided (transferred)by the learned kernel to a common clustering algo-rithm for discovering imbalanced unseen semanticclusters.
We are planning to perform the above por-trayed experiments at least for a couple of semanticcriteria6, including term acception discovering (Sec-tion 2).
Nevertheless, much remains to be done.6 Related workClustering of definitional contexts.
Molina (2009)processed snippets containing definitions of terms(Sierra, 2009).
The obtained PD matrix is not morethan a homogeneous quadratic kernel that induces aHilbert space: The Textual Energy of data (Fernan-dez et al, 2007; Torres-Moreno et al, 2010).
Hi-erarchical clustering is performed over the resultingspace, but some semantic criterion was not consid-ered.
Thus, such as Cigarran (2008), they ranked re-trieved documents by simply relying on lexical fea-tures (global analysis).
ML analysis was not per-formed, so their approach suffers from high sensibil-ity to lexical changes (instability) in local analysis.Paraphrase extraction from definitional sen-tences.
Hashimoto, et.al.
(2011) and Yan, et.al.
(2013) engineered vectors from contextual, syntac-tical and lexical features of definitional sentenceparaphrases (similarly to Lapata (2007) and Ferrone(2014)).
As training data they used a POS anno-tated corpus of sentences that contain noun phrases.It was trained a binary SVM aimed at both para-phrase detection and multi-word term equivalenceassertion (Choi and Myaeng, 2012; Abend et al,2014).
More complex constructions were not con-sidered, but their feature mixure performs very well.Socher et al, (2011) used ANNs for paraphrasedetection.
According to labeling, the network unsu-pervisedly capture as many language features as la-tent in data (Kalchbrenner et al, 2014).
The networksupervisedly learns to represent desired contents in-side phrases (Mikolov et al, 2013); thus paraphrasedetection is highly generalized.
Nevertheless, it isnotable the necessity of a tree parser.
Unlike to(Socher et al, 2013), the network must to learn syn-tactic features separately.6For example: SemEval-2014; Semantic Evaluation Exer-cises.83Definitional answer ranking.
Fegueroa (2012)and (2014) proposed to represent definitional an-swers by a Context Language Model (CLM), i.e.
aMarkovian process as probabilistic language model.A knowledge base (WordNET) is used as an an-notated corpus of specific domains (limited toWikipedia).
Unlike to our approach, queries mustbe previously disambiguated; for instance: ?what isa computer virus?
?, where ?computer virus?
disam-biguates ?virus?.
Answers are classified accordingto relevant terms (Mikolov et al, 2013), similarly tothe way topic modeling approaches work (Fernan-dez et al, 2007; Lau et al, 2014).Learning kernels for clustering.
Overall forknowledge transfer from classification (source)tasks to clustering (target) tasks, the state of the art isnot bast.
This setting is generally explored by usingtoy Gaussian-distributed data and predefined kernels(Jenssen et al, 2006; Jain et al, 2010).
Particularlyfor text data, Gu et.al.
(2011) addressed the settingby using multi-task kernels for global analysis.
Intheir work, it was not necessary neither to discoverclusters nor to model some semantic criterion.
Boththem are assumed as a presetting of their analysis,which differs from our proposal.Feasibility of KL over DL.
We want to performclustering over an embedding space.
At the best ofour knowledge there exist two dominant approachesfor feature learning: KL and DL.
However, knowl-edge transfer is equally important for us, so bothprocedures should be more intuitive by adopting theKL approach instead of DL.
We show the main rea-sons: (i) Interpretability.
The form (8) has been de-ducted from punctual items (e.g.
SLMs encodinglanguage aspects), which leads us to think that a la-tent statistical interpretation of language is worthyof further investigation.
(ii) Modularity.
Any ker-nel can be transparently transferred into kernelizedand non-kernelized clustering methods (Sch?olkopfet al, 1997; Aguilar-Martin and De M?antaras, 1982;Ben-Hur et al, 2002).
(iii) Mathematical sup-port.
Theoretical access provided by kernel meth-ods would allow for future work on semantic assess-ments via increasingly abstract representations.
(iv)Data scarcity.
It is one of our principal challenges,so kernel methods are feasible because of their gen-eralization predictability (Cortes and Vapnik, 1995).Regardless of its advantages, our theoreticalframework exhibit latent drawbacks.
The main ofthem is that feature learning is not fully unsuper-vised, which suggests the underlying possibility ofpreventing learning from some decisive knowledgerelated to, mainly, the tractability of the MCIP.
Thus,many empirical studies are pending.7 Conclusions and future workAt the moment, our theoretical framework analyzessemantic embedding in the sense of a criterion forsemantic clustering.
However, correspondences be-tween linguistic intuitions and the showed theoret-ical framework (interpretability) are actually incip-ient, although we consider these challenging corre-spondences are described in a generalized way in theseminal work of Harris (1968).
It is encouraging(not determinant) that our approach can be associ-ated to his operator hypothesis on composition andseparability of both linguistic entities and languageaspects.
That is why we consider it is worth inves-tigating spectral decomposition methods for NLP aspossible rapprochement to elucidate improvementsin semantic assessments (e.g.
semantic clustering).Thus, by performing this research we also expect toadvance the state of the art in statistical features ofwritten language.As immediate future work we are planning tolearn compositional distributional operators (ker-nels), which can be seen as stable solutions of op-erator equations (Harris, 1968; Vapnik, 1998).
Wewould like to investigate this approach for morphol-ogy, syntax and semantics (Mitchell and Lapata,2010; Lazaridou et al, 2013).
Another future pro-posal could be derived from the abovementioned ap-proach (operator learning), i.e.
multi-sentence com-pression for automatic sumarization.A further extension could be ontology learning.
Itwould be proposed as a multi-structure KL frame-work (Ferrone and Zanzotto, 2014).
In this case, IEand knowledge organization would be our main aims(Anandkumar et al, 2014).Aknowledgements.
This work is fundedby CONACyT Mexico (grant: 350326/178248).Thanks to the UNAM graduate program in CS.Thanks to Carlos M?endez-Cruz, to Yang Liu and toanonymous reviewers for their valuable comments.84ReferencesOmri Abend, B. Shay Cohen, and Mark Steedman.
2014.Lexical inference over multi-word predicates: A distri-butional approach.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 644?654.
ACL.Charu C. Aggarwal and Cheng Xiang Zhai.
2012.
Anintroduction to text mining.
In Charu C Aggarwal andChengXiang Zhai, editors, Mining Text Data, pages 1?10.
Springer US.J Aguilar-Martin and R De M?antaras.
1982.
The processof classification and learning the meaning of linguis-tic descriptors of concepts.
Approximate Reasoning inDecision Analysis, 1982:165?175.Animashree Anandkumar, Rong Ge, Daniel Hsu,Sham M Kakade, and Matus Telgarsky.
2014.
Ten-sor decompositions for learning latent variable mod-els.
The Journal of Machine Learning Research,15(1):2773?2832.Nachman Aronszajn.
1950.
Theory of reproducing ker-nels.
Transactions of the American mathematical so-ciety, pages 337?404.Mahsa Baktashmotlagh, Mehrtash T Harandi, Brian CLovell, and Mathieu Salzmann.
2013.
Unsuper-vised domain adaptation by domain invariant projec-tion.
In Computer Vision (ICCV), 2013 IEEE Interna-tional Conference on, pages 769?776.
IEEE.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.Eduard Gabriel B?az?avan, Fuxin Li, and Cristian Smin-chisescu.
2012.
Fourier kernel learning.
In ComputerVision?ECCV 2012, pages 459?473.
Springer.Asa Ben-Hur, David Horn, Hava T Siegelmann, andVladimir Vapnik.
2002.
Support vector clustering.The Journal of Machine Learning Research, 2:125?137.Yoshua Bengio, Ian J. Goodfellow, and Aaron Courville.2014.
Deep learning.
Book in preparation for MITPress.Tom?a?s Brychc?
?n and Miroslav Konop??k.
2014.
Semanticspaces for improving language modelling.
ComputerSpeech and Language, 28:192?209.Sung-Pil Choi and Sung-Hyon Myaeng.
2012.
Termino-logical paraphrase extraction from scientific literaturebased on predicate argument tuples.
Journal of Infor-mation Science, pages 1?19.Juan Manuel Cigarr?an Recuero.
2008.
Organizaci?on deresultados de b?usqueda mediante an?alisis formal deconceptos.
Ph.D. thesis, Universidad Nacional de Ed-ucaci?on a Distancia; Escuela T?ecnica Superior de In-genier?
?a Inform?atica.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Corinna Cortes, Mehryar Mohri, and Afshin Ros-tamizadeh.
2009.
L 2 regularization for learning ker-nels.
In Proceedings of the Twenty-Fifth Conferenceon Uncertainty in Artificial Intelligence, pages 109?116.
AUAI Press.Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj,Maria-Florina F Balcan, and Le Song.
2014.
Scalablekernel methods via doubly stochastic gradients.
InAdvances in Neural Information Processing Systems,pages 3041?3049.Richard O Duda, Peter E Hart, and David G Stork.
2012.Pattern classification.
John Wiley & Sons.Katrin Erk.
2012.
Vector space models of word meaningand phrase meaning: A survey.
Language and Lin-guistics Compass, 6(10):635?653.Silvia Fernandez, Eric San Juan, and Juan-ManuelTorres-Moreno.
2007.
Textual energy of associa-tive memories: Performant applications of enertex al-gorithm in text summarization and topic segmenta-tion.
MICAI 2007: Advances in Artificial Intelligence,pages 861?871.Lorenzo Ferrone and Fabio Massimo Zanzotto.
2014.Towards syntax-aware compositional distributional se-mantic models.
In Proceedings of COLING 2014:Technical Papers, pages 721?730.
Dublin City Uni-versity and Association for Computational Linguistics(ACL).Alejandro Figueroa and John Atkinson.
2012.
Contex-tual language models for ranking answers to naturallanguage definition questions.
Computational Intelli-gence, pages 528?548.Alejandro Figueroa and G?unter Neumann.
2014.Category-specific models for ranking effective para-phrases in community question answering.
ExpertSystems with Applications, 41(10):4730?4742.Quanquan Gu, Zhenhui Li, and Jiawei Han.
2011.Learning a kernel for multi-task clustering.
In Pro-ceedings of the 25th AAAI conference on artificial in-telligence.
Association for the Advancement of Artifi-cial Intelligence (AAAI).Zellig S. Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York, NY, USA.Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,Jun?ichi Kazama, and Sadao Kurohashi.
2011.
Ex-tracting paraphrases from definition sentences on theweb.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies-Volume 1, pages 1087?1097.Prateek Jain, Brian Kulis, and Inderjit S Dhillon.
2010.Inductive regularized learning of kernel functions.
In85Advances in Neural Information Processing Systems,pages 946?954.Robert Jenssen, Torbj?rn Eltoft, Mark Girolami, andDeniz Erdogmus.
2006.
Kernel maximum entropydata transformation and an enhanced spectral cluster-ing algorithm.
In Advances in Neural InformationProcessing Systems, pages 633?640.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
arXiv preprint arXiv:1404.2188.Gert R. G. Lanckriet, Nello Cristianini, Peter Bartlett,Laurent El Ghaoui, and Michael I. Jordan.
2004.Learning the kernel matrix with semidefinite program-ming.
J. Mach.
Learn.
Res., 5:27?72, December.Thomas K Landauer, Peter W. Foltz, and Darrell Laham.1998.
An introduction to latent semantic analysis.Discourse Processes, 25(2-3):259?284.Jey Han Lau, Paul Cook, Diana McCarthy, SpandanaGella, and Timothy Baldwin.
2014.
Learning wordsense distributions, detecting unattested senses andidentifying novel senses using topic models.
In Pro-ceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics, ACL 2014, vol-ume 1, pages 259?270.Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli,and Marco Baroni.
2013.
Compositional-ly derivedrepresentations of morphologically complex words indistributional semantics.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1517?1526.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Sch?utze.
2009.
An Introduction to InformationRetrieval.
Cambridge UP.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(34):1388?1429.
Cognitive Science Society, ISSN:1551-6709.A Molina.
2009.
Agrupamiento sem?antico de contex-tos definitorios.
M?emoire de Master, Universidad Na-cional Aut?onoma de M?exico?Posgrado en Ciencia eIngenier?
?a de la Computaci?on, M?exico, 108.Cheng S Ong, Robert C Williamson, and Alex J Smola.2005.
Learning the kernel with hyperkernels.
In Jour-nal of Machine Learning Research, pages 1043?1071.Sebastian Pad?o and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Danfeng Qin, Xuanli Chen, Matthieu Guillaumin, andLuc V Gool.
2014.
Quantized kernel learning forfeature matching.
In Advances in Neural InformationProcessing Systems, pages 172?180.M Ranzato, Fu Jie Huang, Y-L Boureau, and Yann Le-Cun.
2007.
Unsupervised learning of invariant featurehierarchies with applications to object recognition.In Computer Vision and Pattern Recognition, 2007.CVPR?07.
IEEE Conference on, pages 1?8.
IEEE.F.
Riesz and Sz Nagy.
1955.
Functional analysis.Dover Publications, Inc., New York.
First published in,3(6):35.Bernhard Sch?olkopf, Alexander Smola, and Klaus-Robert M?uller.
1997.
Kernel principal componentanalysis.
In Artificial Neural Networks?ICANN?97,pages 583?588.
Springer.Jhon Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UP.
ISBN:978-0-521-81397-6.Grigori Sidorov, Alexander Gelbukh, Helena G?omez-Adorno, and David Pinto.
2014.
Soft similarity andsoft cosine measure: Similarity of features in vectorspace model.
Computaci?on y Sistemas, 18(3).Gerardo Sierra.
2009.
Extracci?on de contex-tos definitorios en textos de especialidad a partirdel reconocimiento de patrones ling?u??sticos.
Lin-guaM?ATICA, 2:13?38, Dezembro.Alex Smola, Arthur Gretton, Le Song, and BernhardSch?olkopf.
2007.
A hilbert space embeddingfor distributions.
In Algorithmic Learning Theory:18th International Conference, pages 13?31.
Springer-Verlag.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dynamicpooling and unfolding recursive autoencoders for para-phrase detection.
In Advances in Neural InformationProcessing Systems, pages 801?809.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages1631?1642.
Citeseer.M.
Sugiyama and M. Kawanabe.
2012.
Machine Learn-ing in Non-stationary Environments: Introduction toCovariate Shift Adaptation.
Adaptive computationand machine learning.
MIT Press.Juan-Manuel Torres-Moreno, Alejandro Molina, andGerardo Sierra.
2010.
La energ?
?a textual como me-dida de distancia en agrupamiento de definiciones.
InStatistical Analysis of Textual Data, pages 215?226.Vladimir Naumovich Vapnik.
1998.
Statistical learningtheory.
Wiley New York.86Yuanjun Xiong, Wei Liu, Deli Zhao, and Xiaoou Tang.2014.
Zeta hull pursuits: Learning nonconvex datahulls.
In Advances in Neural Information ProcessingSystems, pages 46?54.Yulan Yan, Chikara Hashimoto, Kentaro Torisawa, TakaoKawai, Jun?ichi Kazama, and Stijn De Saeger.
2013.Minimally supervised method for multilingual para-phrase extraction from definition sentences on the web.In HLT-NAACL, pages 63?73.Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lip-son.
2014.
How transferable are features in deep neu-ral networks?
In Advances in Neural Information Pro-cessing Systems, pages 3320?3328.87
