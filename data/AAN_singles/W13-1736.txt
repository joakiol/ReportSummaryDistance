Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 279?287,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsIdentifying the L1 of non-native writers: the CMU-Haifa systemYulia Tsvetkov?
Naama Twitto?
Nathan Schneider?
Noam Ordan?Manaal Faruqui?
Victor Chahuneau?
Shuly Wintner?
Chris Dyer?
?Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PAcdyer@cs.cmu.edu?Department of Computer ScienceUniversity of HaifaHaifa, Israelshuly@cs.haifa.ac.ilAbstractWe show that it is possible to learn to identify, withhigh accuracy, the native language of English testtakers from the content of the essays they write.Our method uses standard text classification tech-niques based on multiclass logistic regression, com-bining individually weak indicators to predict themost probable native language from a set of 11 pos-sibilities.
We describe the various features used forclassification, as well as the settings of the classifierthat yielded the highest accuracy.1 IntroductionThe task we address in this work is identifying thenative language (L1) of non-native English (L2) au-thors.
More specifically, given a dataset of shortEnglish essays (Blanchard et al 2013), composedas part of the Test of English as a Foreign Lan-guage (TOEFL) by authors whose native language isone out of 11 possible languages?Arabic, Chinese,French, German, Hindi, Italian, Japanese, Korean,Spanish, Telugu, or Turkish?our task is to identifythat language.This task has a clear empirical motivation.
Non-native speakers make different errors when theywrite English, depending on their native language(Lado, 1957; Swan and Smith, 2001); understand-ing the different types of errors is a prerequisite forcorrecting them (Leacock et al 2010), and systemssuch as the one we describe here can shed interest-ing light on such errors.
Tutoring applications canuse our system to identify the native language ofstudents and offer better-targeted advice.
Forensiclinguistic applications are sometimes required to de-termine the L1 of authors (Estival et al 2007b; Es-tival et al 2007a).
Additionally, we believe that thetask is interesting in and of itself, providing a bet-ter understanding of non-native language.
We arethus equally interested in defining meaningful fea-tures whose contribution to the task can be linguis-tically interpreted.
Briefly, our features draw heav-ily on prior work in general text classification andauthorship identification, those used in identifyingso-called translationese (Volansky et al forthcom-ing), and a class of features that involves determin-ing what minimal changes would be necessary totransform the essays into ?standard?
English (as de-termined by an n-gram language model).We address the task as a multiway text-classification task; we describe our data in ?3 andclassification model in ?4.
As in other author attri-bution tasks (Juola, 2006), the choice of features forthe classifier is crucial; we discuss the features wedefine in ?5.
We report our results in ?6 and con-clude with suggestions for future research.2 Related workThe task of L1 identification was introduced by Kop-pel et al(2005a; 2005b), who work on the Inter-national Corpus of Learner English (Granger et al2009), which includes texts written by students from5 countries, Russia, the Czech Republic, Bulgaria,France, and Spain.
The texts range from 500 to850 words in length.
Their classification methodis a linear SVM, and features include 400 standardfunction words, 200 letter n-grams, 185 error typesand 250 rare part-of-speech (POS) bigrams.
Ten-279fold cross-validation results on this dataset are 80%accuracy.The same experimental setup is assumed by Tsurand Rappoport (2007), who are mostly interestedin testing the hypothesis that an author?s choice ofwords in a second language is influenced by thephonology of his or her L1.
They confirm this hy-pothesis by carefully analyzing the features used byKoppel et al controlling for potential biases.Wong and Dras (2009; 2011) are also motivatedby a linguistic hypothesis, namely that syntactic er-rors in a text are influenced by the author?s L1.Wong and Dras (2009) analyze three error types sta-tistically, and then add them as features in the sameexperimental setup as above (using LIBSVM with aradial kernel for classification).
The error types aresubject-verb disagreement, noun-number disagree-ment and misuse of determiners.
Addition of thesefeatures does not improve on the results of Kop-pel et al Wong and Dras (2011) further extendthis work by adding as features horizontal slices ofparse trees, thereby capturing more syntactic struc-ture.
This improves the results significantly, yielding78% accuracy compared with less than 65% usingonly lexical features.Kochmar (2011) uses a different corpus, the Cam-bridge Learner Corpus, in which texts are 200-400word long, and are authored by native speakers offive Germanic languages (German, Swiss German,Dutch, Swedish and Danish) and five Romance lan-guages (French, Italian, Catalan, Spanish and Por-tuguese).
Again, SVMs are used as the classificationdevice.
Features include POS n-grams, character n-grams, phrase-structure rules (extracted from parsetrees), and two measures of error rate.
The classi-fier is evaluated on its ability to distinguish betweenpairs of closely-related L1s, and the results are usu-ally excellent.A completely different approach is offered byBrooke and Hirst (2011).
Since training corpora forthis task are rare, they use mainly L1 (blog) cor-pora.
Given English word bigrams ?e1, e2?, they tryto assess, for each L1, how likely it is that an L1 bi-gram was translated literally by the author, resultingin ?e1, e2?.
Working with four L1s (French, Span-ish, Chinese, and Japanese), and evaluating on theInternational Corpus of Learner English, accuracy isbelow 50%.3 DataOur dataset in this work consists of TOEFL essayswritten by speakers of eleven different L1s (Blan-chard et al 2013), distributed as part of the Na-tive Language Identification Shared Task (Tetreaultet al 2013).
The training data consists of 1000essays from each native language.
The essays areshort, consisting of 10 to 20 sentences each.
Weused the provided splits of 900 documents for train-ing and 100 for development.
Each document is an-notated with the author?s English proficiency level(low, medium, high) and an identification (1 to 8) ofthe essay prompt.
All essays are tokenized and splitinto sentences.
In table 1 we provide some statisticson the training corpora, listed by the authors?
profi-ciency level.
All essays were tagged with the Stan-ford part-of-speech tagger (Toutanova et al 2003).We did not parse the dataset.Low Medium High# Documents 1,069 5,366 3,456# Tokens 245,130 1,819,407 1,388,260# Types 13,110 37,393 28,329Table 1: Training set statistics.4 ModelFor our classification model we used the creg re-gression modeling framework to train a 11-class lo-gistic regression classifier.1 We parameterize theclassifier as a multiclass logistic regression:p?
(y | x) =exp?j ?
jh j(x, y)Z?
(x),where x are documents, h j(?)
are real-valued featurefunctions of the document being classified, ?
j are thecorresponding weights, and y is one of the eleven L1class labels.
To train the parameters of our model,we minimized the following objective,L = ?`2 reg.????
?j?2j ??
{(xi,yi)}|D|i=1(log likelihood?
??
?log p?
(yi | xi) +?Ep?(y?
|xi) log p?(y?
| xi)?
??
?
?conditional entropy),1https://github.com/redpony/creg280which combines the negative log likelihood of thetraining dataset D, an `2 (quadratic) penalty on themagnitude of ?
(weighted by ?
), and the negative en-tropy of the predictive model (weighted by ?).
Whilean `2 weight penalty is standard in regression prob-lems like this, we found that the the additional en-tropy term gave more reliable results.
Intuitively,the entropic regularizer encourages the model to re-main maximally uncertain about its predictions.
Inthe metaphor of ?maximum entropy?, the entropicprior finds a solution that has more entropy than the?maximum?
model that is compatible with the con-straints.The objective cannot be minimized in closedform, but it does have a unique minimum andis straightforwardly differentiable, so we used L-BFGS to find the optimal weight settings (Liu et al1989).5 Feature OverviewWe define a large arsenal of features, our motivationbeing both to improve the accuracy of classificationand to be able to interpret the characteristics of thelanguage produced by speakers of different L1s.While some of the features were used in priorwork (?2), we focus on two broad novel categoriesof features: those inspired by the features usedto identify translationese by Volansky et al(forth-coming) and those extracted by automatic statisti-cal ?correction?
of the essays.
Refer to figure 1 tosee the set of features and their values that were ex-tracted from an example sentence.POS n-grams Part-of-speech n-grams were used invarious text-classification tasks.Prompt Since the prompt contributes informationon the domain, it is likely that some words (and,hence, character sequences) will occur more fre-quently with some prompts than with others.
Wetherefore use the prompt ID in conjunction withother features.Document length The number of tokens in the textis highly correlated with the author?s level of flu-ency, which in turn is correlated with the author?sL1.Pronouns The use of pronouns varies greatlyamong different authors.
We use the same listof 25 English pronouns that Volansky et al(forth-coming) use for identifying translationese.Punctuation Similarly, different languages usepunctuation differently, and we expect this to taintthe use of punctuation in non-native texts.
Ofcourse, character n-grams subsume this feature.Passives English uses passive voice more fre-quently than other languages.
Again, the use ofpassives in L2 can be correlated with the author?sL1.Positional token frequency The choice of the firstand last few words in a sentence is highly con-strained, and may be significantly influenced bythe author?s L1.Cohesive markers These are 40 function words(and short phrases) that have a strong discoursefunction in texts (however, because, in fact,etc.).
Translators tend to spell out implicit utter-ances and render them explicitly in the target text(Blum-Kulka, 1986).
We use the list of Volanskyet al(forthcoming).Cohesive verbs This is a list of manually compiledverbs that are used, like cohesive markers, to spellout implicit utterances (indicate, imply, contain,etc.
).Function words Frequent tokens, which are mostlyfunction words, have been used successfully forvarious text classification tasks.
Koppel and Or-dan (2011) define a list of 400 such words, ofwhich we only use 100 (using the entire list wasnot significantly different).
Note that pronounsare included in this list.Contextual function words To further capitalizeon the ability of function words to discriminate,we define pairs consisting of a function word fromthe list mentioned above, along with the POS tagof its adjacent word.
This feature captures pat-terns such as verbs and the preposition or particleimmediately to their right, or nouns and the deter-miner that precedes them.
We also define 3-gramsconsisting of one or two function words and thePOS tag of the third word in the 3-gram.Lemmas The content of the text is not considered agood indication of the author?s L1, but many textcategorization tasks use lemmas (more precisely,the stems produced by the tagger) as features ap-proximating the content.Misspelling features Learning to perceive, pro-duce, and encode non-native phonemic contrasts281Firstly the employers live more savely because they are going to have more money to spend for luxury .Presence Considered alternatives/editsCharacters"CHAR_l_y_ ": log 2 + 1"CharPrompt_P5_g_o_i": log 1 + 1"MFChar_e_ ": log 1 + 1"Punc_period": log 1 + 1"DeleteP_p_.
": 1.0"InsertP_p_,": 1.0"MID:SUBST:v:f": log 1 + 1"SUBST:v:f": log 1 + 1Words"DocLen_": log 19 + 1"MeanWordRank": 422.6"CohMarker_because": log 1 + 1"MostFreq_have": log 1 + 1"PosToken_last_luxury": log 1 + 1"Pronouns_they": log 1 + 1"MSP:safely": log 1 + 1"Match_p_to": 0.5"Delete_p_to": 0.5"Delete_p_are": 1.0"Delete_p_because": 1.0"Delete_p_for": 1.0POS "POS_VBP_VBG_TO": log 1 + 1"POS_p_VBP_VBG_TO": 0.059Words + POS "VBP_VBG_to": log 1 + 1"FW__more RB": log 1 + 1Figure 1: Some of the features extracted for an L1 German sentence.is extremely difficult for L2 learners (Hayes-Harband Masuda, 2008).
Since English?s orthogra-phy is largely phonemic?even if it is irregularin many places, we expect leaners whose na-tive phoneme contrasts are different from thoseof English to make characteristic spelling errors.For example, since Japanese and Korean lack aphonemic /l/-/r/ contrast, we expect native speak-ers of those languages to be more likely to makespelling errors that confuse l and r relative tonative speakers of languages such as Spanish inwhich that pair is contrastive.
To make this in-formation available to our model, we use a noisychannel spelling corrector (Kernighan, 1990) toidentify and correct misspelled words in the train-ing and test data.
From these corrections, we ex-tract minimal edit features that show what inser-tions, deletions, substitutions and joinings (wheretwo separate words are written merged into a sin-gle orthographic token) were made by the authorof the essay.Restored tags We focus on three important tokenclasses defined above: punctuation marks, func-tion words and cohesive verbs.
We first removewords in these classes from the texts, and thenrecover the most likely hidden tokens in a se-quence of words, according to an n-gram lan-guage model trained on all essays in the trainingcorpus corrected with a spell checker and con-taining both words and hidden tokens.
This fea-ture should capture specific words or punctuationmarks that are consistently omitted (deletions),or misused (insertions, substitutions).
To restorehidden tokens we use the hidden-ngram util-ity provided in SRI?s language modeling toolkit(Stolcke, 2002).Brown clusters (Brown et al 1992) describe an al-gorithm that induces a hierarchical clustering ofa language?s vocabulary based on each vocabu-lary item?s tendency to appear in similar left andright contexts in a training corpus.
While origi-nally developed to reduce the number of parame-ters required in n-gram language models, Brownclusters have been found to be extremely effectiveas lexical representations in a variety of regres-sion problems that condition on text (Koo et al2008; Turian et al 2010; Owoputi et al 2013).Using an open-source implementation of the al-gorithm,2 we clustered 8 billion words of Englishinto 600 classes.3 We included log counts of all4-grams of Brown clusters that occurred at least100 times in the NLI training data.5.1 Main FeaturesWe use the following four feature types as the base-line features in our model.
For features that are sen-sitive to frequency, we use the log of the (frequency-plus-one) as the feature?s value.
Table 2 reports theaccuracy of using each feature type in isolation (with2https://github.com/percyliang/brown-cluster3http://www.ark.cs.cmu.edu/cdyer/en-600/cluster_viewer.html282Feature Accuracy (%)POS 55.18FreqChar 74.12CharPrompt 65.09Brown 72.26DocLen 11.81Punct 27.41Pron 22.81Position 53.03PsvRatio 12.26CxtFxn (bigram) 62.79CxtFxn (trigram) 62.32Misspell 37.29Restore 47.67CohMark 25.71CohVerb 22.85FxnWord 42.47Table 2: Independent performance of feature types de-tailed in ?5.1, ?5.2 and ?5.3.
Accuracy is averaged over10 folds of cross-validation on the training set.10-fold cross-validation on the training set).POS Part-of-speech n-grams.
Features were ex-tracted to count every POS 1-, 2-, 3- and 4-gramin each document.FreqChar Frequent character n-grams.
We exper-imented with character n-grams: To reduce thenumber of parameters, we removed features onlythose character n-grams that are observed morethan 5 times in the training corpus, and n rangesfrom 1 to 4.
High-weight features include:TUR:<Turk>; ITA:<Ital>; JPN:<Japa>.CharPrompt Conjunction of the character n-gramfeatures defined above with the prompt ID.Brown Substitutions, deletions and insertionscounts of Brown cluster unigrams and bigrams ineach document.The accuracy of the classifier on the development setusing these four feature types is reported in table 3.45.2 Additional FeaturesTo the basic set of features we now add more spe-cific, linguistically-motivated features, each addinga small number of parameters to the model.
Asabove, we indicate the accuracy of each feature typein isolation.4For experiments in this paper combining multiple types offeatures, we used Jonathan Clark?s workflow management tool,ducttape (https://github.com/jhclark/ducttape).Feature Group # Params Accuracy (%) `2POS 540,947 55.18 1.0+ FreqChar 1,036,871 79.55 1.0+ CharPrompt 2,111,175 79.82 1.0+ Brown 5,664,461 81.09 1.0Table 3: Dev set accuracy with main feature groups,added cumulatively.
The number of parameters is alwaysa multiple of 11 (the number of classes).
Only `2 regular-ization was used for these experiments; the penalty wastuned on the dev set as well.DocLen Document length in tokens.Punct Counts of each punctuation mark.Pron Counts of each pronoun.Position Positional token frequency.
We use thecounts for the first two and last three words be-fore the period in each sentence as features.
High-weight features for the second word include:ARA:2<,>; CHI:2<is>; HIN:2<can>.PsvRatio The proportion of passive verbs out of allverbs.CxtFxn Contextual function words.
High-weightfeatures include: CHI:<some JJ>;HIN:<as VBN>.Misspell Spelling correction edits.
Featuresincluded substitutions, deletions, insertions,doubling of letters and missing doublings ofletters, and splittings (alot?a lot), as well as theword position where the error occurred.High-weight features include: ARA:DEL<e>,ARA:INS<e>, ARA:SUBST<e>/<i>;GER:SUBST<z>/<y>; JPN:SUBST<l>/<r>,JPN:SUBST<r>/<l>; SPA:DOUBLE<s>,SPA:MID_INS<s>, SPA:INS<s>.Restore Counts of substitutions, deletions andinsertions of predefined tokens that we restoredin the texts.
High-weight features include:CHI:DELWORD<do>; GER:DELWORD<on>;ITA:DELWORD<be>Table 4 reports the empirical improvement that eachof these brings independently when added to themain features (?5.1).5.3 Discarded FeaturesWe also tried several other feature types that did notimprove the accuracy of the classifier on the devel-opment set.CohMark Counts of each cohesive marker.283Feature Group # Params Accuracy (%) `2main + Position 6,153,015 81.00 1.0main + PsvRatio 5,664,472 81.00 1.0main 5,664,461 81.09 1.0main + DocLen 5,664,472 81.09 1.0main + Pron 5,664,736 81.09 1.0main + Punct 5,664,604 81.09 1.0main + Misspell 5,799,860 81.27 5.0main + Restore 5,682,589 81.36 5.0main + CxtFxn 7,669,684 81.73 1.0Table 4: Dev set accuracy with main features plus addi-tional feature groups, added independently.
`2 regulariza-tion was tuned as in table 3 (two values, 1.0 and 5.0, weretried for each configuration; more careful tuning mightproduce slightly better accuracy).
Results are sorted byaccuracy; only three groups exhibited independent im-provements over the main feature set.CohVerb Counts of each cohesive verb.FxnWord Counts of function words.
These featuresare subsumed by the highly discriminative CxtFxnfeatures.6 ResultsThe full model that we used to classify the test setcombines all features listed in table 4.
Using allthese features, the accuracy on the development setis 84.55%, and on the test set it is 81.5%.
The valuesfor ?
and ?
were tuned to optimize development setperformance, and found to be ?
= 5, ?
= 2.Table 5 lists the confusion matrix on the test set,as well as precision, recall and F1-score for each L1.The largest error type involved predicting Teluguwhen the true label was Hindi, which happened 18times.
This error is unsurprising since many Hindiand Telugu speakers are arguably native speakers ofIndian English.Production of L2 texts, not unlike translating fromL1 to L2, involves a tension between the impos-ing models of L1 (and the source text), on the onehand, and a set of cognitive constraints resultingfrom the efforts to generate the target text, on theother.
The former is called interference in Trans-lation Studies (Toury, 1995) and transfer in secondlanguage acquisition (Selinker, 1972).
Volansky etal.
(forthcoming) designed 32 classifiers to test thevalidity of the forces acting on translated texts, andfound that features sensitive to interference consis-tently yielded the best performing classifiers.
Andindeed, in this work too, we find fingerprints of thesource language are dominant in the makeup of L2texts.
The main difference, however, between textstranslated by professionals and the texts we addresshere, is that more often than not professional trans-lators translate into their mother tongue, whereas L2writers write out of their mother tongue by defini-tion.
So interference is ever more exaggerated inthis case, for example, also phonologically (Tsur andRappoport, 2007).We explore the effects of interference by analyz-ing several patterns we observe in the features.
Ourclassifier finds that the character sequence alot isoverrepresented in Arabic L2 texts.
Arabic has noindefinite article and we speculate that Arabic speak-ers conceive a lot as a single word; the Arabic equiv-alent for a lot is used adverbially like an -ly suffixin English.
For the same reason, another promi-nent feature is a missing definite article before nounsand adjectives.
Additionally, Arabic, being an Ab-jad language, rarely indicates vowels, and indeed wefind many missing e?s and i?s in the texts of Arabicspeakers.
Phonologically, because Arabic conflates/I/ and /@/ into /i/ (at least in Modern Standard Ara-bic), we see that many e?s are indeed substituted fori?s in these texts.We find that essays that contain hyphens are morelikely to be from German authors.
We again findevidence of interference from the native languagehere.
First, relative clauses are widely used in Ger-man, and we see this pattern in L2 English of L1German speakers.
For example, any given rationalbeing ?
let us say Immanual Kant ?
we find that.Another source of extra hyphens stems from com-pounding convention.
So, for example, we find well-known, community-help, spare-time, football-club,etc.
Many of these reflect an effort to both connectand separate connected forms in the original (e.g.,Fussballklub, which in English would be more natu-rally rendered as football club).
Another unexpectedfeature of essays by native Germans is a frequentsubstitution of the letter y for z and vice versa.
Wesuspect this owes to their switched positions on Ger-man keyboards.Lexical item frequency also provides clues to theL1 of the essay writers.
The word that occurs morefrequently in the texts of German L1 speakers.
We284true?
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision (%) Recall (%) F1 (%)ARA 80 0 2 1 3 4 1 0 4 2 3 80.8 80.0 80.4CHI 3 80 0 1 1 0 6 7 1 0 1 88.9 80.0 84.2FRE 2 2 81 5 1 2 1 0 3 0 3 86.2 81.0 83.5GER 1 1 1 93 0 0 0 1 1 0 2 87.7 93.0 90.3HIN 2 0 0 1 77 1 0 1 5 9 4 74.8 77.0 75.9ITA 2 0 3 1 1 87 1 0 3 0 2 82.1 87.0 84.5JPN 2 1 1 2 0 1 87 5 0 0 1 78.4 87.0 82.5KOR 1 5 2 0 1 0 9 81 1 0 0 80.2 81.0 80.6SPA 2 0 2 0 1 8 2 1 78 1 5 77.2 78.0 77.6TEL 0 1 0 0 18 1 2 1 1 73 3 85.9 73.0 78.9TUR 4 0 2 2 0 2 2 4 4 0 80 76.9 80.0 78.4Table 5: Official test set confusion matrix with the full model.
Accuracy is 81.5%.hypothesize that in English it is optional in rela-tive clauses whereas in German it is not, so Ger-man speakers are less comfortable using the non-obligatory form.
Also, often is over represented.
Wehypothesize that since it is cognate of German oft, itis not cognitively expensive to retrieve it.
We findmany times?a literal translation of muchas veces?in Spanish essays.Other informative features that reflect L1 featuresinclude frequent misspellings involving confusionsof l and r in Japanese essays.
More mysteriously,the characters r and s are misused in Chinese andSpanish, respectively.
The word then is dominantin the texts of Hindi speakers.
Finally, it is clearthat authors refer to their native cultures (and, conse-quently, native languages and countries); the stringsTurkish, Korea, and Ita were dominant in the texts ofTurkish, Korean and Italian native speakers, respec-tively.7 DiscussionWe experimented with different classifiers and alarge set of features to solve an 11-way classifica-tion problem.
We hope that studying this problemwill improve to facilitate human assessment, grad-ing, and teaching of English as a second language.While the core features used are sparse and sensitiveto lexical and even orthographic features of the writ-ing, many of them are linguistically informed andprovide insight into how L1 and L2 interact.Our point of departure was the analogy betweentranslated texts as a genre in its own and L2 writ-ers as pseudo translators, relying heavily on theirmother tongue and transferring their native modelsto a second language.
In formulating our features,we assumed that like translators, L2 writers willwrite in a simplified manner and overuse explicitmarkers.
Although this should be studied vis-?-viscomparable outputs of mother tongue writers in En-glish, we observe that the best features of our clas-sifiers are of the ?interference?
type, i.e.
phonolog-ical, morphological and syntactic in nature, mostlyin the form of misspelling features, restoration tags,punctuation and lexical and syntactic modeling.We would like to stress that certain features indi-cating a particular L1 have no bearing on the qualityof the English produced.
This has been discussedextensively in Translation Studies (Toury, 1995),where interference is observed by the overuse or un-deruse of certain features reflecting the typologicaldifferences between a specific pair of languages, butwhich is still within grammatical limits.
For exam-ple, the fact that Italian native speakers favor thesyntactic sequence of determiner + adjective + noun(e.g., a big risk or this new business) has little pre-scriptive value for teachers.A further example of how L2 quality and theability to predict L1 are uncorrelated, we notedthat certain L2 writers often repeat words appear-ing in their essay prompts, and including informa-tion about whether the writer was reusing promptwords improved classification accuracy.
We suggestthis reflects different educational backgrounds.
Thisfeature says nothing about the quality of the text, justas the tendency of Korean and Italian writers to men-tion their home country more often does not.285AcknowledgmentsThis research was supported by a grant from the Is-raeli Ministry of Science and Technology.ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: Acorpus of non-native English.
Technical report, Edu-cational Testing Service.Shoshana Blum-Kulka.
1986.
Shifts of cohesion and co-herence in translation.
In Juliane House and ShoshanaBlum-Kulka, editors, Interlingual and interculturalcommunication Discourse and cognition in translationand second language acquisition studies, volume 35,pages 17?35.
Gunter Narr Verlag.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
In Conferenceof Learner Corpus Research (LCR2011), Louvain-la-Neuve, Belgium.
Presses universitaires de Louvain.Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4).Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007a.
Author profil-ing for English emails.
In Proc.
of PACLING, pages263?272, Melbourne, Australia.Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007b.
TAT: An authorprofiling tool with application to Arabic emails.
InProc.
of the Australasian Language Technology Work-shop, pages 21?30, Melbourne, Australia, December.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English.
Presses universitaires de Louvain,Louvain-la-Neuve.Rachel Hayes-Harb and Kyoko Masuda.
2008.
Devel-opment of the ability to lexically encode novel secondlanguage phonemic contrasts.
Second Language Re-search, 24(1):5?33.Patrick Juola.
2006.
Authorship attribution.
Founda-tions and Trends in Information Retrieval, 1(3):233?334.Mark D. Kernighan.
1990.
A spelling correction pro-gram based on a noisy channel model.
In Proc.
ofCOLING.Ekaterina Kochmar.
2011.
Identification of a writer?s na-tive language by error analysis.
Master?s thesis, Uni-versity of Cambridge.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Proc.of ACL.Moshe Koppel and Noam Ordan.
2011.
Translationeseand its dialects.
In Proc.
of ACL-HLT, pages 1318?1326, Portland, Oregon, USA, June.
Association forComputational Linguistics.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005a.Automatically determining an anonymous author?s na-tive language.
Intelligence and Security Informatics,pages 41?76.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005b.Determining an author?s native language by mininga text for errors.
In Proc.
of KDD, pages 624?628,Chicago, IL.
ACM.Robert Lado.
1957.
Linguistics across cultures: appliedlinguistics for language teachers.
University of Michi-gan Press, Ann Arbor, Michigan, June.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morgan andClaypool.Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-cedal.
1989.
On the limited memory BFGS methodfor large scale optimization.
Mathematical Program-ming B, 45(3):503?528.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conversa-tional text with word clusters.
In Proc.
of NAACL.Larry Selinker.
1972.
Interlanguage.
InternationalReview of Applied Linguistics in Language Teaching,10(1?4):209?232.Andreas Stolcke.
2002.
SRILM?an extensible lan-guage modeling toolkit.
In Procedings of Interna-tional Conference on Spoken Language Processing,pages 901?904.Michael Swan and Bernard Smith.
2001.
Learner En-glish: A Teacher?s Guide to Interference and OtherProblems.
Cambridge Handbooks for LanguageTeachers.
Cambridge University Press.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proc.
of the Eighth Workshop on Inno-vative Use of NLP for Building Educational Applica-tions, Atlanta, GA, USA, June.
Association for Com-putational Linguistics.Gideon Toury.
1995.
Descriptive Translation Studiesand beyond.
John Benjamins, Amsterdam / Philadel-phia.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proc.of HLT-NAACL, pages 173?180, Edmonton, Canada,June.
Association for Computational Linguistics.286Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proc.
ofthe Workshop on Cognitive Aspects of ComputationalLanguage Acquisition, pages 9?16, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proc.
of ACL.Vered Volansky, Noam Ordan, and Shuly Wintner.
forth-coming.
On the features of translationese.
Literaryand Linguistic Computing.Sze-Meng Jojo Wong and Mark Dras.
2009.
Contrastiveanalysis and native language identification.
In Proc.of the Australasian Language Technology AssociationWorkshop, pages 53?61, Sydney, Australia, December.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploitingparse structures for native language identification.
InProc.
of EMNLP, pages 1600?1610, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.287
