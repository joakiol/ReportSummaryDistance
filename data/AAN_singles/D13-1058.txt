Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 613?623,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsCentering Similarity Measures to Reduce HubsIkumi SuzukiNational Institute of GeneticsMishima, Shizuoka, Japansuzuki.ikumi@gmail.comKazuo HaraNational Institute of GeneticsMishima, Shizuoka, Japankazuo.hara@gmail.comMasashi ShimboNara Institute of Science and TechnologyIkoma, Nara, Japanshimbo@is.naist.jpMarco SaerensUniversite?
catholique de LouvainLouvain-la-Neuve, Belgiummarco.saerens@uclouvain.beKenji FukumizuThe Institute of Statistical MathematicsTachikawa, Tokyo, Japanfukumizu@ism.ac.jpAbstractThe performance of nearest neighbor methodsis degraded by the presence of hubs, i.e., ob-jects in the dataset that are similar to manyother objects.
In this paper, we show that theclassical method of centering, the transforma-tion that shifts the origin of the space to thedata centroid, provides an effective way to re-duce hubs.
We show analytically why hubsemerge and why they are suppressed by cen-tering, under a simple probabilistic model ofdata.
To further reduce hubs, we also movethe origin more aggressively towards hubs,through weighted centering.
Our experimentalresults show that (weighted) centering is effec-tive for natural language data; it improves theperformance of the k-nearest neighbor classi-fiers considerably in word sense disambigua-tion and document classification tasks.1 Introduction1.1 BackgroundThe k-nearest neighbor (kNN) algorithm is a sim-ple nonparametric method of classification.
It hasbeen applied to various natural language process-ing (NLP) tasks such as document classification(Masand et al 1992; Yang and Liu, 1999), part-of-speech tagging (S?gaard, 2011), and word sensedisambiguation (Navigli, 2009).To apply the kNN algorithm, data is typically rep-resented as a vector object in a feature space, and(dis)similarity between data is measured by the dis-tance between the vectors, their inner product, or co-sine of the angle between them (Jurafsky and Mar-tin, 2008).
With such a (dis)similarity measure, theunknown class label of a test object is predicted bya majority vote of the classes of its k most similarobjects in the labeled training set.Recent studies (Radovanovic?
et al 2010a;Radovanovic?
et al 2010b) have shown that if thefeature space is high-dimensional, some objects inthe dataset emerge as hubs; i.e., these objects fre-quently appear in the k nearest neighbors of otherobjects.The emergence of hubs may deteriorate the per-formance of kNN classification and nearest neighborsearch in general:?
If hub objects exist in the training set, they havea strong chance to be a kNN of many test ob-jects.
Because the class of a test object is pre-dicted by a majority vote from its k nearestneighbors, prediction is biased toward the la-bels of the hubs.?
In information retrieval, nearest neighborsearch finds objects in the database that aremost relevant, or similar, to user-providedqueries.
If particular objects, such as hubs, arenearly always returned for any query, the re-trieved results are probably not very useful.These drawbacks may hinder application of near-est neighbor methods in NLP, as typical natural lan-guage data are extremely high-dimensional (Juraf-sky and Martin, 2008) and thus prone to producehubs.1.2 ContributionsCentering (Mardia et al 1979; Fisher and Lenz,1996; Eriksson et al 2006) is a standard technique613for removing observation bias in the data.
It is atransformation of feature space in a way that the ori-gin of the space is moved to the data centroid (sam-ple mean).
The distance between data objects is notchanged by centering, but their inner product and co-sine are affected; see Section 3 for detail.In this paper, we advocate the use of centering as ameans of reducing hubs.
Specifically, we propose tomeasure the similarity of objects by the inner prod-uct (not distance or cosine) in the centered featurespace.Our approach is motivated by the observation thatthe objects similar to the data centroid tend to be-come hubs (Radovanovic?
et al 2010a).
This ob-servation suggests that the number of hubs may bereduced if we can define a similarity measure thatmakes all objects in a dataset equally similar to thecentroid (Suzuki et al 2012).
The inner product inthe centered space indeed enjoys this property.In Section 4, we analyze why hubs emerge undera simple probabilistic model of data, and also givean account of why they are suppressed by centering.Using both synthetic and real datasets, we showthat objects similar to the centroid also emerge ashubs in multi-cluster data (Section 5), so the applica-tion of centering is wider than expected.
To furtherreduce hubs, we also propose to move the origin ofthe space more aggressively towards hubs, throughweighted centering (Section 6).In Section 7, we show that centering and weightedcentering are effective for natural language data.these methods markedly improve the performanceof kNN classifiers in word sense disambiguation anddocument classification tasks.2 Related workCentering is a classical technique widely used inmany fields of science.
For instance, centeringforms a preprocessing step in principal componentanalysis and Fisher linear discriminant analysis.In NLP, however, centering is seldom used; theuse of cosine and inner product similarities is quitecommon, but they are nearly always used uncen-tered.
Non-centered cosine is used, for instance, inword sense disambiguation (Schu?tze, 1998; Navigli,2009), paraphrasing (Erk and Pado?, 2008; Thateret al 2010), and compositional semantics (Mitchelland Lapata, 2008), to name a few.There have been several approaches to improv-ing kNN classification: learning similarity/distancemeasures from training data (metric learning)(Weinberger and Saul, 2009; Qamar et al 2008),weighting nearest neighbors for similarity-basedclassification (Chen et al 2009), and neighbor-hood size selection (Wang et al 2006; Guo andChakraborty, 2010).
However, none of these haveaddressed the reduction of hubs.More recently, Schnitzer et al(2012) proposedthe Mutual Proximity transformation that rescalesdistance measures to decrease hubs in a dataset.Suzuki et al(2012) showed that kernels based ongraph Laplacian, such as the commute-time kernels(Saerens et al 2004) and the regularized Laplacian(Chebotarev and Shamis, 1997; Smola and Kondor,2003), make all objects equally similar to the datacentroid, which in turn reduce hubs.In Section 7, we evaluate centering, Mutual Prox-imity, and Laplacian kernels in NLP tasks, anddemonstrate that centering is equally or even moreeffective.
Section 4 presents a theoretical justifica-tion for using centering to reduce hubs, but this kindof analysis is missing for the Laplacian kernels.Centering is easier to compute as well.
For adataset of n objects, it takes O(n2) time to com-pute, whereas computing a Laplacian-based kernelrequires O(n3) time for matrix inversion.
MutualProximity also has a time complexity of O(n2).3 CenteringConsider a dataset of n objects in an m-dimensionalfeature space, x1, ?
?
?
, xn ?
Rm.
Throughout thispaper, we use the inner product ?xi, x j?
as a measureof similarity between xi and x j.
Let K be the Grammatrix of the n feature vectors, i.e., the n ?
n matrixwhose (i, j) element holds ?xi, x j?.
Using m?
n datamatrix X = [x1, ?
?
?
, xn], we can write K asK = XTX,where XT represents the matrix transpose of X.Centering is a transformation in which the originof the feature space is shifted to the data centroidx?
=1nn?i=1xi, (1)614and object x is mapped to the centered feature vectorxcent = x ?
x?.
(2)The similarity between two objects x and x?
is nowmeasured by ?xcent, x?cent?
= ?x ?
x?, x?
?
x?
?.After centering, the inner product between anyobject and the data centroid (which is a zero vectorbecause x?cent = x?
?
x?
= 0) is uniformly 0; in otherwords, all objects in the dataset have an equal simi-larity to the centroid.
According to the observationthat the objects similar to the centroid become hubs(Radovanovic?
et al 2010a), we can expect hubs tobe reduced after centering.Intuitively, centering reduces hubs because itmakes the length of the feature vector xcent shortfor (hub) objects x that lie close to the data centroidx?
; see Eq.
(2).
And since we measure object simi-larity by inner product, shorter vectors tend to pro-duce smaller similarity scores.
Hence objects closeto the data centroid become less similar to other ob-jects after centering, and no longer be hubs.
In Sec-tion 4, we analyze the effect of centering on hubnessin more detail.3.1 Centered Gram matrixLet I be an n ?
n identity matrix and 1 be an n-dimensional all-ones vector.
The symmetric matrixH = I?
(1/n)11T is called centering matrix, becausethe centered data matrix Xcent = [xcent1 , ?
?
?
, xcentn ]can be computed by Xcent = XH (Mardia et al1979).The Gram matrix Kcent of the centered featurevectors, whose (i, j) element holds the inner prod-uct ?xcenti , xcentj ?, can be calculated from the originalGram matrix K byKcent =(Xcent)T (Xcent)= HXTXH = HKH.
(3)Eq.
(3) implies that the original data matrix X isnot needed to compute the centered Gram matrixKcent, provided that K is given.
It is hence possi-ble to use the so-called kernel trick; i.e., centeringcan be applied even if data matrix X is not availablebut the similarity of objects can be measured by akernel function in an implicit feature space.4 Theoretical analysis of the effect ofcentering on hubnessWe now analyze why objects most similar to thecentroid tend to be hubs in the dataset, and give anexplanation as to why centering may suppress theemergence of hubs.4.1 Before centeringConsider a dataset of m-dimensional feature vectors,with each vector x ?
Rm generated independentlyfrom a distribution with a finite mean vector ?.
Inother words, objects x in this dataset are drawn froma distribution P(x), i.e.,x ?
P(x),and?
= E[x] =?x dP(x) (4)where E[?]
denotes the expectation of a random vari-able.We will use the following elementary lemma onthe distributions of inner product subsequently.Lemma 1.
Let a ?
Rm be a fixed vector, and x ?
Rmbe an object sampled according to distribution P(x).Then the inner product ?a, x?
follows a distributionwith mean ?a,??.Proof.
From the linearity of the inner product andEq.
(4), we obtainE[?a, x?]
=?
?a, x?
dP(x)= ?a,?x dP(x)?
= ?a,??.
Now, imagine that we have an object x sam-pled from P(x), and we want to compute its nearestneighbor in a dataset.
Let h and ` be two fixed ob-jects in the dataset, such that the inner product to thetrue mean ?
is higher for h than for `, i.e.,?h,??
?
?`,??
> 0.
(5)We are interested in which of h and ` is more similarto x (in terms of inner product), or in other words,the difference of two inner productsz = ?h, x?
?
?`, x?
= ?h ?
`, x?.
(6)615Because x is a random variable, so is z.
Let Q(z) bethe distribution of z; i.e., z ?
Q(z).Using Lemma 1 with a = h ?
`, together withEq.
(5), we haveE[z] = ?h ?
`,??
= ?h,??
?
?`,??
> 0.
(7)Note that the above statement is only concernedabout the mean, so it does not in general assure that?h, x?
> ?`, x?
(8)holds with high probability; there is a chance thata small number of outliers are inflating the mean.To assure that inequality (8) holds with probabilitygreater than 1/2 for instance, the median rather thanthe mean of the distribution Q(z) must be greaterthan 0.If the distribution Q(z) is symmetric, the medianoccurs at the same point as the mean, and the aboveclaim holds.
Indeed, if the components of x are gen-erated independently from (possibly non-identical)normal distributions, we can show that Q(z) alsoobeys a normal distribution.
Because it is a symmet-ric distribution, we can safely say that in this case,Eq.
(8) holds with probability greater than 1/2.For a general non-symmetric distribution with afinite variance, the median is known to be within thestandard deviation of the mean (Mallows, 1991), sowe could still say that Eq.
(8) is likely to hold if ?h?`,??
is sufficiently large compared to the standarddeviation.Now, if we let h be the object in a given datasetwith the highest similarity (inner product) to themean ?, and let ` be any other object in the set, thenwe see from the above discussion that h is likely tohave higher similarity to x, a test sample drawn fromdistribution P(x).
Because this holds for any ` inthe dataset, the conclusion is that the objects in thedataset most similar to ?
are likely to become hubs.4.2 After centeringNext let us investigate what happens if the datasetis centered.
Let x?
be the sample (empirical) meangiven by Eq.
(1).
After centering, the similarity of xwith each of the two fixed objects h and ` are evalu-ated by ?h?
x?, x?
x??
and ?`?
x?, x?
x?
?, respectively.Their difference zcent is given byzcent = ?h ?
x?, x ?
x??
?
?` ?
x?, x ?
x?
?= ?h ?
`, x ?
x?
?= ?h ?
`, x?
?
?h ?
`, x?
?= z ?
?h ?
`, x?
?.The last equality follows from Eq.
(6).
By definitionwe have z ?
Q(z), and since ?h ?
`, x??
is a constant,zcent = z ?
?h ?
`, x??
?
Q(z + ?h ?
`, x??
).In other words, the shape of the distribution does notchange, but the mean is shifted toE[zcent] = E[z] ?
?h ?
`, x?
?= ?h ?
`,??
?
?h ?
`, x?
?= ?h ?
`,?
?
x?
?,where E[z] is given by Eq.
(7).
If the sample meanx?
is close enough to the true mean ?, i.e., x?
?
?, wehave an approximationE[zcent] = ?h ?
`,?
?
x??
?
0.
(9)Thus, if the median and the mean of distributionQ(z) are again not far apart, Eq.
(9) suggests thath ?
x?
and ` ?
x?
are about equally likely to be moresimilar to x ?
x?
; i.e., neither has a greater chance tobecome a hub.5 Hubs in multi-cluster dataIn this section, we discuss emergence of hubs whenthe data consists of multiple clusters.
In fact, theanalysis of Section 4 is distribution-free, and thusalso applies to the case of multi-modal P(x).
How-ever, one might still argue that objects similar to thedata centroid should hardly occur in that case.
Us-ing both synthetic and real datasets, we demonstratebelow that even in multi-cluster data, objects thatare only slightly more similar to the data mean (cen-troid) may emerge as hubs.5.1 Synthetic data5.1.1 Data generationWe generated a high-dimensional multi-clusterdataset by modeling it as a mixture of ten von Mises-Fisher distributions (Mardia and Jupp, 2000) in6160 50 100 1500.450.50.550.6N10Similarity withcentroid(a) Before centering: N10 vs. innerproduct similarity to the data cen-troid200 400 600 800 10002004006008001000Object IDObject ID5101520253035404550(b) Before centering: kNN matrix200 400 600 800 100015010050050100150Object IDFrequency(c) Before centering: Breakdown ofN10 by cluster match/mismatchbetween objects and neighbors0 50 100 150?0.1?0.0500.050.1N10Similarity withcentroid(d) After centering: N10 vs. innerproduct similarity to the data cen-troid200 400 600 800 10002004006008001000Object IDObject ID5101520253035404550(e) After centering: kNN matrix200 400 600 800 100015010050050100150Object IDFrequency(f) After centering: Breakdown ofN10 by cluster match/mismatchbetween objects and neighborsFigure 1: 300-dimensional synthetic data.
(a), (d): scatter plot of the N10 value of objects and their similarity tocentroid.
(b), (e): kNN matrices.
The points are colored according to the N10 value of object x; warmer colors indicatehigher N10 values.
(c), (f): the number of times (y-axis) an object (whose ID is on the x-axis) appears in the 10 nearestneighbors of objects of the same cluster (black bars), and those of different clusters (magenta).R300.
The von Mises-Fisher distribution is a distri-bution of unit vectors (it can roughly be thought ofas a normal distribution on a unit hypersphere), sofor objects (feature vectors) sampled from this dis-tribution, inner product reduces to cosine similarity.We sampled1 100 objects from each of the ten dis-tributions (clusters), and made a dataset of 1,000 ob-jects in total.The von Mises-Fisher distribution has two param-eters, the mean direction vector ?, and the concen-tration parameter ?
characterizing how strongly thepopulation is concentrated around the direction ?.We set ?
= 500 for all ten distributions, but the meandirections ?
were made distinct; all mean direction1We used the random sampling code available at http://people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html(Banerjee et al 2005).vectors had 30 components set to 0.5 while the re-maining 270 components were set to 1, but the 30components with value 0.5 were chosen to be dis-tinct among the ten clusters.
This configuration as-sures that all ten mean directions have the same an-gle from the all-ones vector [1, .
.
.
, 1]T, which is thedirection of the mean of the entire data distribution.Note that even though all sampled objects resideon the surface of the unit hypersphere, the data cen-troid lies not on the surface but inside the hyper-sphere.
And after centering, the length of the fea-ture vectors may vary from one another, but we donot normalize these vectors; i.e., object similarity ismeasured by raw inner product, not by cosine.6175.1.2 Correlation between hubness andcentroid similarityThe scatter plot in Figure 1(a) shows the correla-tion between the degree of hubness (N10) of an ob-ject and its inner product similarity to the data cen-troid.
The N10 value of an object is defined as thenumber of times the object appears in the 10 nearestneighbors of other objects in the dataset.
It was usedin (Radovanovic?
et al 2010a) to measure the degreeof hubness of individual objects.The plot clearly shows that the hub objects (i.e.,those with high N10) consist of objects that are simi-lar to the centroid.
Figure 1(d) shows the scatter plotafter the data is centered, created in the same wayas Figure 1(a).
The similarity to the centroid is uni-formly 0 as a result of centering, and no objects havean N10 value greater than 33.5.1.3 Influence of hubs on objects in differentclustersThe kNN matrix of Figure 1(b) depicts the kNNrelations with k = 10 among objects before center-ing.
In this matrix, both the x- and y- axes representthe ID of the objects.
If object x is in the 10 nearestneighbors of object y, a point is plotted at coordi-nates (x, y).
As a result, there are exactly k = 10points in each row.
The color of points indicates thedegree of hubness of object x; warmer color repre-sents higher N10 value of the object.In this matrix, object IDs are sorted by the clus-ter the objects belong to.
Hence in the ideal case inwhich the k nearest neighbors of every object consistgenuinely of objects from the same cluster, only thediagonal blocks would be colored, and off-diagonalareas would be left blank.As Figure 1(b) shows, the actual situation is farfrom ideal, even though ten diagonal blocks are stillidentifiable.
The presence of many warm coloredvertical lines suggests that many hub objects appearin the 10 nearest neighbors of other objects that arenot in the same cluster as the hubs.
Thus these hubsmay have a strong influence on the kNN predictionof other objects.Figure 1(e) shows the kNN matrix after centering.The warm colored lines have disappeared, and thediagonal blocks are now more visible.The bar graphs of Figures 1(c) and (f) plot the N10value of each object (whose ID is on the x-axis).
Re-call that N10 is the number of times an object appearsin the 10 nearest neighbors of other objects.
Thebar for each object is broken down by whether theobject and its neighbors belong to the same cluster(black bar) or in different clusters (magenta bar).
Interms of kNN classification, having a large numberof nearest neighbors with the same class improvesthe classification performance, so longer black barsand shorter magenta bars are more desirable.Before centering (Figure 1(c)), hub objects withlarge N10 values are similar not only to objects be-longing to the same cluster (as indicated by blackbars), but also to objects belonging to different clus-ters (magenta bars).
After centering (Figure 1(f)),the number of tall magenta bars decreases.Before centering, 22.7% of the 10 nearest neigh-bors of an object have the same class label as theobject (as indicated by the ratio of the total height ofblack bars relative to that of all bars in Figure 1(c)).After centering, the percentage increases to 31.6%.5.2 Real datasetWe did the same analysis as Sections 5.1.2?5.1.3to a real dataset with multiple-cluster structure: theReuters Transcribed dataset.
This multi-class docu-ment classification dataset has ten classes, and eachclass roughly forms a cluster.
We will also use thisdataset in an experiment in Section 7.2.The results are shown in Figure 2.
We can ob-serve the same trends as we saw in Figure 1 for thesynthetic data: positive correlation between hubness(N10) and inner product with the data centroid be-fore centering; hubs appearing in the nearest neigh-bors of many objects of different classes; and bothare reduced after centering.The ratio of the height of black bars to that ofall bars in Figure 2(c) is 38.4% before centering,whereas it improves to 41.0% after centering (Fig-ure 2(f)).6 Hubness weighted centeringCentering shifts the origin of the space to the datacentroid, and objects similar to the centroid tend tobecome hubs.
Thus in a sense, centering can beinterpreted as an operation that shifts the origin to-wards hubs.In this section, we extrapolate this interpretation,6180 10 20 30 40 5000.010.020.030.040.050.06N10Similarity with centroid(a) Before centering: N10 vs. innerproduct similarity to the data cen-troid50 100 150 20050100150200Object IDObjectID51015202530(b) Before centering: kNN matrix50 100 150 200402002040Object IDFrequency(c) Before centering: Breakdown ofN10 by class match/mismatch be-tween objects and neighbors0 10 20 30 40 50?0.03?0.02?0.0100.010.020.03N10Similarity with centroid(d) After centering: N10 vs. innerproduct similarity to the data cen-troid50 100 150 20050100150200Object IDObjectID51015202530(e) After centering: kNN matrix50 100 150 200402002040Object IDFrequency(f) After centering: Breakdown of N10by class match/mismatch betweenobjects and neighborsFigure 2: Reuters Transcribed data.and move the origin more actively towards hub ob-jects in the dataset, rather than towards the data cen-troid.
To this end, we consider weighted centering,a variation of centering in which each object is asso-ciated with a weight, and the origin is shifted to theweighted mean of the data.
Specifically, we definethe weight of an object as the sum of the similarities(inner products) between the object and all objects,regarding this sum as the index of how likely the ob-ject can be a hub.6.1 Weighted centeringIn weighted centering, we associate weight wi toeach object i in the dataset, and move the origin tothe weighted centroidx?weighted =n?i=1wixiwhere?ni=1 wi = 1 and 0 ?
wi ?
1 for i = 1, .
.
.
, n.Thus, object x is mapped to a new feature vectorxweighted = x ?
x?weighted = x ?n?i=1wixi.Notice that the original centering formula (2) is re-covered by letting wi = 1/n for all i = 1, .
.
.
, n.Weighted centering can also be kernelized by us-ing the weighted centering matrix H(w) = I ?
1wTin place of H in Eq.
(3).
The resulting Gram matrixisKweighted = H(w)KH(w)T. (10)6.2 Similarity-dependent weightingTo move the origin towards hubs more aggressively,we place more weights on objects that are morelikely to become hubs.
This likelihood is estimatedby the similarity of individual objects to all objectsin the data set.619Let di be the sum of the similarity between objectxi and all objects in the dataset.
So,di =n?j=1?xi, x j?
= n ?xi,1nn?j=1x j?.As seen from the last equation, di is proportional tothe similarity (inner product) between object xi andthe data centroid.Now we define {wi}ni=1 from {di}ni=1 bywi =d?i?nj=1 d?j,where ?
is a parameter controlling how much weemphasize the effect of di.
Setting ?
= 0 results inwi = 1 for every i, and hence is equivalent to normalcentering.
When ?
> 0, weighted centering movesthe origin closer to the objects with a large di thannormal centering would.7 ExperimentsWe evaluated the effect of centering in two naturallanguage tasks: word sense disambiguation (WSD)and document classification.
We are interested inwhether hubs are actually reduced after centering,and whether the performance of kNN classificationis improved.Throughout this section, K denotes cosine simi-larity matrix; i.e., inner product of feature vectorsnormalized to unit length; Kcent denotes the cen-tered similarity matrix computed by Eq.
(3) from K;Kweighted denotes its hubness weighted variant givenby Eq.
(10).
Depending on context, these symbolsare also used to denote kNN classifiers using respec-tive similarity measures.For comparison, we also tested two recently pro-posed approaches to hub reduction: transformationof the base similarity measure (in our case, K) byMutual Proximity (Schnitzer et al 2012)2, and theone (Suzuki et al 2012) based on graph Laplaciankernels.
Since the Laplacian kernels are defined forgraph nodes, we computed them by taking the co-sine similarity matrix K as the weighted adjacency(affinity) matrix of a graph.
For Laplacian kernels,2We used the Matlab script downloaded from http://www.ofai.at/?dominik.schnitzer/mp/.we computed both the regularized Laplacian ker-nel (Chebotarev and Shamis, 1997; Smola and Kon-dor, 2003) with several parameter values, as well asthe commute-time kernel (Saerens et al 2004), butpresent only the best results among these kernels.7.1 Word sense disambiguation7.1.1 Task and datasetIn the WSD experiment, we used the dataset forthe Senseval-3 English Lexical Sample (ELS) task(Mihalcea et al 2004).
It is a collection of sen-tences containing 57 polysemous words, and eachof these sentences is annotated with a gold standardsense of the target word.
The goal of the ELS taskis to build a classifier for each target word, which,given a context around the word, predicts a sensefrom the known set of senses.We used a basic bag-of-words representation forthe context surrounding a target word (Mihalcea,2004; Navigli, 2009).
A context is thus representedas a high-dimensional feature vector holding the tf-idf weighted frequency of words3 in context.7.1.2 Compared methodsWe applied kNN classification using cosine sim-ilarity K, and its four transformed similarity mea-sures: centered similarity Kcent, its weighted vari-ant Kweighted, Mutual Proximity and graph Laplaciankernels.
The sense of a test object was predicted byvoting from the k training objects most similar to thetest object, as measured by the respective similaritymeasures.We used leave-one-out cross validation within thetraining data to tune neighborhood size k for thekNN classification and the voting scheme, i.e., ei-ther (unweighted) majority vote, or weighted vote inwhich votes from individual objects are weighted bytheir similarity score to the test objects.
We also se-lected parameter ?
in Kweighted and the best graphLaplacian kernel among the regularized Laplacianand commute time kernels using the training data.7.1.3 EvaluationWe computed two indices for each similarity mea-sure: (i) skewness of the N10 distribution to evaluate3We removed stop words listed in the on-line appendix of(Lewis et al 2004).620Method F1 score SkewnessK 60.3 4.55Kcent 64.0 1.19Kweighted 64.8 1.02Mutual Proximity 63.0 1.00Graph Laplacian 61.2 4.51GAMBL (Decadt et al 2004) 64.5 ?Table 1: WSD results: Macro-averaged F1 score (points)of the compared methods (larger is better) and empiricalskewness of the N10 distribution for each similarity mea-sure (smaller is better).the emergence of hubs, and (ii) macro-averaged F1score to evaluate the classification performance.Skewness To evaluate the degree of hub emer-gence for each similarity measure, we followed(Radovanovic?
et al 2010a) and counted Nk(x), thenumber of times object x occurs in the kNN listsof other objects in the dataset (we fix k = 10 be-low).
The emergence of hubs in a dataset can thenbe quantified with skewness, defined as follows:S Nk =E[(Nk ?
?Nk)3]?3Nk.In this equation, E[ ? ]
denotes expectation, and ?Nkand ?Nk are the mean and the standard deviation ofthe Nk distribution, respectively.When hubs exist in a dataset, the distribution ofNk is expected to skew to the right, and yields a largeS Nk (Radovanovic?
et al 2010a).
In other words,similarity measures that yield smaller S Nk are moredesirable in terms of hub reduction.Skewness can only be computed for each dataset,and in the WSD task, each target word has its owndataset.
Hence we computed the skewness S N10 foreach word and then took average.Macro-averaged F1 score Classification perfor-mance was measured by the F1 score macro-averaged over all the 57 target words in the Senseval-3 ELS dataset.
The standard Senseval-3 ELS scor-ing method is based on micro average, but we usedmacro average to make the evaluation consistentwith skewness computation, which, as mentionedabove, can only be computed for each dataset (i.e.,word).Dataset #classes #objects #featuresReuters Transcribed 10 201 2730Mini Newsgroups 20 2000 8811Table 2: Document classification datasets: Number ofclasses, data size, and number of features.7.1.4 ResultTable 1 shows the F1 scores and the skewness ofthe N10 distributions, macro averaged over the 57target words.
The table also includes the macro-averaged F1 score4 of the GAMBL system, the bestmemory-based system participated in the Senseval-3 ELS task.
Note however that GAMBL uses moreelaborate features (e.g., part-of-speech of words)than just a plain bag-of-words used by other methodsin this comparison.
GAMBL also employs complexpost-processing of the kNN outputs.After centering (Kcent and Kweighted) skewnessbecame markedly smaller than that of the non-centered cosine K. F1 score also improved with thedecrease in skewness.
In particular, weighted cen-tering (Kweighted) slightly outperformed GAMBL,though the difference was small.
Recall howeverthat Kcent and Kweighted only use naive bag-of-wordsfeatures, unlike GAMBL.7.2 Document classification7.2.1 Task and datasetTwo multiclass document classification datasetswere used: Reuters Transcribed and Mini News-groups, distributed at http://archive.ics.uci.edu/ml/.The properties of the datasets are summarized in Ta-ble 2.7.2.2 EvaluationThe performance was evaluated by the F1 score(equivalent to accuracy in this task) of prediction us-ing leave-one-out cross validation, due to the limitednumber of documents.7.2.3 Compared methodsWe used the cosine similarity as the base sim-ilarity matrix (K).
The centered similarity matrix(Kcent) and its weighted variant (Kweighted), Mutual4The macro-averaged F1 of GAMBL was calculated fromthe per-word F1 scores listed in Table 1 of (Decadt et al 2004).621Method F1 score SkewnessK 56.7 1.61Kcent 61.2 0.11Kweighted 60.2 0.04Mutual Proximity 60.2 ?0.10Graph Laplacian 57.2 0.37(a) Reuters TranscribedMethod F1 score SkewnessK 76.5 4.37Kcent 79.0 1.56Kweighted 79.4 1.68Mutual Proximity 79.0 0.49Graph Laplacian 77.6 2.13(b) Mini NewsgroupsTable 3: Document classification results: F1 score (%)(larger is better) and skewness of the N10 distribution foreach similarity measure (smaller is better).Proximity, and graph Laplacian based kernels werecomputed from K.kNN classification was done in a standard way:The class of object x is predicted by the majorityvote from k = 10 objects most similar to x, mea-sured by a specified similarity measure.
The param-eter k for the kNN classification, the voting scheme(i.e., either unweighted or weighted majority vote),?
in Kweighted, and the best graph Laplacian kernelwere selected by leave-one-out cross validation.7.2.4 ResultTable 3 shows the F1 score and the skewness ofthe N10 distribution of the respective methods indocument classification.
Centered cosine (Kcent)outperformed uncentered cosine similarity K, andachieved an F1 score comparable to Mutual Proxim-ity.
Weighted centering (Kweighted) further improvedF1 on the Mini Newsgroups data.8 ConclusionWe have shown that centering similarity matrices re-duces the emergence of hubs in the data, and conse-quently improves the accuracy of nearest neighborclassification.
We have theoretically analyzed whyobjects most similar to the mean tend to make hubs,and also proved that centering cancels the bias in thedistribution of inner products, and thus is expectedto reduce hubs.In WSD and document classification tasks, kNNclassifiers showed much better performance withcentered similarity measures than non-centeredones.
Weighted centering shifts the origin towardshubs more aggressively, and further improved theclassification performance in some cases.In future work, we plan to exploit the class distri-bution in the dataset to make more effective similar-ity measures; notice that the hubness weighted cen-tering of Section 6 is an unsupervised method, in thesense that class information was not used for deter-mining weights.
We will investigate if more effec-tive weighting can be done using this information.AcknowledgmentsWe thank anonymous reviewers for helpful com-ments.ReferencesArindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh,and Suvrit Sra.
2005.
Clustering on the unit hyper-sphere using von Mises-Fisher distributions.
Journalof Machine Learning Research, 6:1345?1382.P.
Yu.
Chebotarev and E. V. Shamis.
1997.
The matrix-forest theorem and measuring relations in small socialgroups.
Automation and Remote Control, 58(9):1505?1514.Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi,and Luca Cazzanti.
2009.
Similarity-based classifi-cation: Concepts and algorithms.
Journal of MachineLearning Research, 10:747?776.Bart Decadt, Ve?ronique Hoste, Walter Daelemans, andAntal Van den Bosch.
2004.
GAMBL, genetic algo-rithm optimization of memory-based WSD.
In RadaMihalcea and Phil Edmonds, editors, Proceedings ofthe 3rd International Workshop on the Evaluation ofSystems for the Semantic Analysis of Text (Senseval-3), pages 108?112.L.
Eriksson, E. Johansson, N. Kettaneh-Wold, J. Trygg,C.
Wikstro?m, and S. Wold.
2006.
Multi- andMegavariate Data Analysis, Part 1, Basic Principlesand Applications.
Umetrics, Inc.Katrin Erk and Sebastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP ?08),pages 897?906, Honolulu, Hawaii, USA.Douglas H. Fisher and Hans-Joachim Lenz, editors.1996.
Learning from Data: Artificial Intelligence and622Statistics V: Workshop on Artificial Intelligence andStatistics.
Lecture Notes in Statistics 112.
Springer.Ruixin Guo and Sounak Chakraborty.
2010.
Bayesianadaptive nearest neighbor.
Statistical Analysis andData Mining, 3(2):92?105.Daniel Jurafsky and James H. Martin.
2008.
Speech andLanguage Processing.
Prentice Hall, 2nd edition.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
RCV1: a new benchmark collection for textcategorization research.
Journal of Machine LearningResearch, 5:361?397.Colin Mallows.
1991.
Another comment on O?Cinneide.The American Statistician, 45(3):257.K.
V. Mardia and P. Jupp.
2000.
Directional Statistics.John Wiley and Sons, 2nd edition.K.
V. Mardia, J. T. Kent, and J. M. Bibby.
1979.
Multi-variate Analysis.
Academic Press.Brij M. Masand, Gordon Linoff, and David L. Waltz.1992.
Classifying news stories using memory basedreasoning.
In Proceedings of the 15th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval (SIGIR ?92), pages59?65.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
In Rada Mihalcea and Phil Edmonds, editors,Proceedings of the 3rd International Workshop on theEvaluation of Systems for the Semantic Analysis ofText (Senseval-3), pages 25?28, Barcelona, Spain.Rada Mihalcea.
2004.
Co-training and self-training forword sense disambiguation.
In Hwee Tou Ng andEllen Riloff, editors, Proceedings of the 8th Confer-ence on Computational Natural Language Learning(CoNLL ?04), pages 33?40, Boston, Massachusetts,USA.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofthe 46th Annual Meeting of the Association of Compu-tational Linguistics: Human Language Technologies(ACL ?08), pages 236?244, Columbus, Ohio, USA.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys, 41:10:1?10:69.Ali Mustafa Qamar, E?ric Gaussier, Jean-Pierre Cheval-let, and Joo-Hwee Lim.
2008.
Similarity learning fornearest neighbor classification.
In Proceedings of the8th International Conference on Data Mining (ICDM?08), pages 983?988, Pisa, Italy.Milos?
Radovanovic?, Alexandros Nanopoulos, and Mir-jana Ivanovic?.
2010a.
Hubs in space: Popular nearestneighbors in high-dimensional data.
Journal of Ma-chine Learning Research, 11:2487?2531.Milos?
Radovanovic?, Alexandros Nanopoulos, and Mir-jana Ivanovic?.
2010b.
On the existence of obstinateresults in vector space models.
In Proceedings of the33rd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR ?10), pages 186?193, Geneva, Switzerland.Marco Saerens, Franc?ois Fouss, Luh Yen, and PierrDupont.
2004.
The principal components analysisof graph, and its relationships to spectral clustering.In Proceedings of the 15th European Conference onMachine Learning (ECML ?04), Lecture Notes in Ar-tificial Intelligence 3201, pages 371?383, Pisa, Italy.Springer.Dominik Schnitzer, Arthur Flexer, Markus Schedl, andGerhard Widmer.
2012.
Local and global scaling re-duce hubs in space.
Journal of Machine Learning Re-search, 13:2871?2902.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24:97?123.Alexander J. Smola and Risi Kondor.
2003.
Kernels andregularization on graphs.
In Learning Theory and Ker-nel Machines: 16th Annual Conference on LearningTheory and 7th Kernel Workshop, Proceedings, Lec-ture Notes in Artificial Intelligence 2777, pages 144?158.
Springer.Anders S?gaard.
2011.
Semisupervised condensed near-est neighbor for part-of-speech tagging.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics (ACL ?11), pages 48?52, Portland, Oregon, USA.Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Mat-sumoto, and Marco Saerens.
2012.
Investigating theeffectiveness of Laplacian-based kernels in hub reduc-tion.
In Proceedings of the 26th AAAI Conference onArtificial Intelligence (AAAI-12), pages 1112?1118,Toronto, Ontario, Canada.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics (ACL ?10), pages 948?957,Uppsala, Sweden.Jigang Wang, Predrag Neskovic, and Leon N. Cooper.2006.
Neighborhood size selection in the k-nearest-neighbor rule using statistical confidence.
PatternRecognition, 39(3):417?423.Kilian Q. Weinberger and Lawrence K. Saul.
2009.
Dis-tance metric learning for large margin nearest neighborclassification.
Journal of Machine Learning Research,10:207?244.Yiming Yang and Xin Liu.
1999.
A re-examination oftext categorization methods.
In Proceedings of the22nd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR ?99), pages 42?49, Berkeley, California, USA.623
