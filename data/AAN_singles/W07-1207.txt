Proceedings of the 5th Workshop on Important Unresolved Matters, pages 49?56,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDeep Linguistic Processing for Spoken Dialogue SystemsJames AllenDepartment of Computer ScienceUniversity of Rochesterjames@cs.rochester.eduMyroslava DzikovskaICCS-HCRCUniversity of Edinburghmdzikovs@inf.ed.ac.ukMehdi ManshadiDepartment of Computer ScienceUniversity of Rochestermehdih@cs.rochester.eduMary SwiftDepartment of Computer ScienceUniversity of Rochesterswift@cs.rochester.eduAbstractWe describe a framework for deep linguis-tic processing for natural language under-standing in task-oriented spoken dialoguesystems.
The goal is to create domain-general processing techniques that can beshared across all domains and dialoguetasks, combined with domain-specific op-timization based on an ontology mappingfrom the generic LF to the application  on-tology.
This framework has been tested insix domains that involve tasks such as in-teractive planning, coordination operations,tutoring, and learning.1 IntroductionDeep linguistic processing is essential for spokendialogue systems designed to collaborate with us-ers to perform collaborative tasks.
We describe theTRIPS natural language understanding system,which is designed for this purpose.
As we developthe system, we are constantly balancing two com-peting needs: (1) deep semantic accuracy: the needto produce the semantically and pragmatically deepinterpretations for a specific application; and (2)portability: the need to reuse our grammar, lexiconand discourse interpretation processes across do-mains.We work to accomplish portability by using amulti-level representation.
The central componentsare all based on domain general representations,including a linguistically based detailed semanticrepresentation (the Logical Form, or LF), illocu-tionary acts, and a collaborative problem-solvingmodel.
Each application then involves using a do-main-specific ontology and reasoning components.The generic LF is linked to the domain-specificrepresentations by a set of ontology mapping rulesthat must be defined for each domain.
Once theontology mapping is defined, we then can auto-matically specialize the generic grammar to use thestronger semantic restrictions that arise from thespecific domain.
In this paper we mainly focus onthe generic components for deep processing.
Thework on ontology mapping and rapid grammar ad-aptation is described elsewhere (Dzikovska et al2003; forthcoming).2 Parsing for deep linguistic processingThe parser uses a broad coverage, domain-independent lexicon and grammar to produce theLF.
The LF is a flat, unscoped representation thatincludes surface speech act analysis, dependencyinformation, word senses (semantic types) withsemantic roles derived from the domain-independent language ontology, tense, aspect, mo-dality, and implicit pronouns.
The LF supportsfragment and ellipsis interpretation, discussed inSection 5.22.1 Semantic LexiconThe content of our semantic representation comesfrom a domain-independent ontology linked to adomain-independent lexicon.
Our syntax relies ona frame-based design in the LF ontology, a com-mon representation in semantic lexicons (Baker etal., 1998, Kipper et al, 2000).
The LF type hierar-chy is influenced by argument structure, but pro-vides a more detailed level of semantic analysisthan found in most broad coverage parsers as itdistinguishes senses even if the senses take thesame argument structure, and may collapse lexicalentries with different argument structures to thesame sense.
As a very simple example, the genericlexicon includes the senses for the verb take shown49in Figure 1.
Our generic senses have been inspiredby FrameNet (Baker et al, 1998).In addition, types are augmented with semanticfeatures derived from EuroWordNet (Vossen et al,1997) and extended.
These are used to provide se-lectional restrictions, similar to VerbNet (Kipper etal., 2000).
The constraints are intentionally weak,excluding utterances unsuitable in most contexts(the idea slept) but not attempting to eliminateborderline combinations.The generic selectional restrictions are effectivein improving overall parsing accuracy, while re-maining valid across multiple domains.
Anevaluation with an earlier version of the grammarshowed that if generic selectional restrictions wereremoved, full sentence semantic accuracy de-creased from 77.8% to 62.6% in an emergencyrescue domain, and from 67.9 to 52.5% in a medi-cal domain (using the same versions of grammarand lexicon) (Dzikovska, 2004).The current version of our generic lexicon con-tains approximately 6400 entries (excluding mor-phological variants), and the current language on-tology has 950 concepts.
The lexicon can be sup-plemented by searching large-scale lexical re-sources such as WordNet (Fellbaum, 1998) andComlex (Grisham et al, 1994).
If an unknownword is encountered, an underspecified entry isgenerated on the fly.
The entry incorporates asmuch information from the resource as possible,such as part of speech and syntactic frame.
It isassigned an underspecified semantic classificationbased on correspondences between our languageontology and WordNet synsets.2.2 GrammarThe grammar is context-free, augmented with fea-ture structures and feature unification, motivatedfrom X-bar theory, drawing on principles fromGPSG (e.g., head and foot features) and HPSG.
Adetailed description of an early non-lexicalizedversion of the formalism is in (Allen, 1995).
LikeHPSG, our grammar is strongly lexicalized, withthe lexical features defining arguments and com-plement structures for head words.
Unlike HPSG,however, the features are not typed and rather thanmultiple inheritance, the parser supports a set oforthogonal single inheritance hierarchies to capturedifferent syntactic and semantic properties.
Struc-tural variants such as passives, dative shifts, ger-unds, and so on are captured in the context-freerule base.
The grammar has broad coverage ofspoken English, supporting a wide range of con-versational constructs.
It also directly encodesconventional conversational acts, including stan-dard surface speech acts such as inform, requestand question, as well as acknowledgments, accep-tances, rejections, apologies, greetings, corrections,and other speech acts common in conversation.To support having both a broad domain-generalgrammar and the ability to produce deep domain-specific semantic representations, the semanticknowledge is captured in three distinct layers (Fig-ure 2), which are compiled together before parsingto create efficient domain-specific interpretation.The first level is primarily encoded in the gram-mar, and defines an interpretation of the utterancein terms of generic grammatical relations.
The sec-ond is encoded in the lexicon and defines an inter-pretation in terms of a generic language-based on-tology and generic roles.
The third is encoded by aset of ontology-mapping rules that are defined foreach domain, and defines an interpretation in termsof the target application ontology.
While these lev-els are defined separately, the parser can produceall three levels simultaneously, and exploit do-main-specific semantic restrictions to simultane-ously improve semantic accuracy and parsing effi-ciency.
In this paper we focus on the middle level,the generic LF.CONSUME Take an aspirinMOVE Take it to the storeACQUIRE Take a pictureSELECT I?ll take that oneCOMPATIBLEWITHThe projector takes 100 voltsTAKE-TIME It took three hoursFigure 1: Some generic senses of take in lexicon50The rules in the grammar are weighted, andweights are combined, similar to how probabilitiesare computed in a PCFG.
The weights, however,are not strictly probabilities (e.g., it is possible tohave weights greater than 1); rather, they encodestructural preferences.
The parser operates in abest-first manner and as long as weights never ex-ceed 1.0, is guaranteed to find the highest weightedparse first.
If weights are allowed to exceed 1.0,then the parser becomes more ?depth-first?
and itis possible to ?garden-path?
and find globally sub-optimal solutions first, although eventually all in-terpretations can still be found.The grammar used in all our applications usesthese hand-tuned rule weights, which have provento work relatively well across domains.
We do notuse a statistical parser based on a trained corpusbecause in most dialogue-system projects, suffi-cient amounts of training data are not available andwould be too time consuming to collect.
In the onedomain in which we have a reasonable amount oftraining data (about 9300 utterances), we experi-mented with a PCFG using trained probabilitieswith the Collins algorithm, but were not able toimprove on the hand-tuned preferences in overallperformance (Elsner et al, 2005).Figure 3 summarizes some of the most impor-tant preferences encoded in our rule weights.
Be-cause we are dealing with speech, which is oftenungrammatical and fragmented, the grammar in-cludes ?robust?
rules (e.g., allowing dropped de-terminers) that would not be found in a grammar ofwritten English.3 The Logical Form LanguageThe logical form language captures a domain-independent semantic representation of the utter-ance.
As shown later in this paper, it can be seen asa variant of MRS (Copestake et al, 2006) but isexpressed in a frame-like notation rather thanpredicate calculus.
In addition, it has a relativelysimple method of computing possible quantifierscoping, drawing from the approaches by (Hobbs& Shieber, 1987) and (Alshawi, 1990).A logical form is set of terms that can be viewedas a rooted graph with each term being a nodeidentified by a unique ID (the variable).
There arethree types of terms.
The first corresponds to gen-eralized quantifiers, and is on the form (<quant><id> <type> <modifiers>*).
As a simple example,the NP Every dog would be captured by the term(Every d1 DOG).
The second type of term is thepropositional term, which is represented in a neo-Davidsonian representation (e.g., Parsons, 1990)using reified events and properties.
It has the form(F <id> <type> <arguments>*).
The propositionalterms produced from Every dog hates a cat wouldbe (F h1 HATE :Experiencer d1 :Theme c1).
Thethird type of term is the speech act, which has thesame form as propositional terms except for theinitial indicator SA identifying it as a performedspeech act.
The speech act for Every dog hates acat would be (SA sa1 INFORM :content h1).
Put-ting this all together, we get the following (con-densed) LF representation from the parser forEvery large dog hates a cat (shown in graphicalFigure 2: The Levels of Representation computed by the ParserPrefer?
Interpretations without gaps to those with gaps?
Subcategorized interpretations over adjuncts?
Right attachment of PPs and adverbials?
Fully specified constituents over those withdropped or ?implicit?
arguments?
Adjectival modification over noun-noun modifi-cation?
Standard rules over ?robust?
rulesFigure 3: Some Key Preferences used in Parsing51form in Figure 4).
(SA x1 TELL :content x2)(F x2 HATE :experience x3 :theme x5)(Every x3 DOG :mods  (x4))(F x4 LARGE :of x3)(A x5 CAT)4 Comparison of LF and MRSMinimal Recursion Semantics (MRS) (Copestakeet al 2006) is a semantic formalism which hasbeen widely adopted in the last several years.
Thishas motivated some research on how this formal-ism compares to some traditional semantic for-malisms.
For example, Fuchss et al (2004) for-mally show that the translation from MRS toDominance Constraints is feasible.
We have alsofound that MRS is very similar to LF in its de-scriptive power.
In fact, we can convert every LFto an equivalent MRS structure with a simple algo-rithm.First, consider the sentence Every dog hates acat.
Figure 5 shows the LF and MRS representa-tions for this sentence.Figure 5: The LF (left) and MRS (right) representationsfor the sentence ?Every dog hates a cat.
?The first step toward converting LF to MRS is toexpress LF terms as n-ary relationships.
For exam-ple we express the LF term (F v1 Hate:Experiencer x :Theme y) as Hate(x, y).
For quanti-fier terms, we break the LF term into two relations:one for the quantifier itself and one for the restric-tion.
For example (Every x Dog) is converted toEvery(x) and Dog(x).There is a small change in the conversion proce-dure when the sentence contains some modifiers.Consider the modifier large in the sentence Everylarge dog hates a cat.
In the LF, we bring themodifier in the term which defines the semantichead, using a :MODS slot.
In the MRS, however,modifiers are separate EPs labeled with same han-dle as the head?s.
To cover this, for each LF term Twhich has a (:MODS vk) slot,  and the LF term T1which defines the variable vk, we assign the samehandle to both T and T1.
For example for the terms(F x Dog :MODS v2) and (F v2 Large :OF x), weassign the same handle to both Dog(x) andLarge(x).
Similar approach applies when the modi-fier itself is a scopal term, such as in the sentenceEvery cat in a room sleeps.
Figure 7 shows LF andMRS representations for this sentence.
Figure 8,summarizes all these steps as an algorithm whichtakes a LF representation as the input and gener-ates its equivalent MRS.There is a small change in the conversion proce-dure when the sentence contains some modifiers.Consider the modifier large in the sentence Everylarge dog hates a cat.
In the LF, we bring themodifier in the term which defines the semantichead, using a :MODS slot.
In the MRS, however,modifiers are separate EPs labeled with same han-dle as the head?s.
To cover this, for each LF term Twhich has a (:MODS vk) slot,  and the LF term T1which defines the variable vk, we assign the samehandle to both T and T1.
For example for the terms(F x Dog :MODS v2) and (F v2 Large :OF x), weassign the same handle to both Dog(x) andLarge(x).
Similar approach applies when the modi-fier itself is a scopal term, such as in the sentenceEvery cat in a room sleeps.
Figure 7 shows LF andMRS representations for this sentence.
Figure 8,summarizes all these steps as an algorithm whichtakes a LF representation as the input and gener-ates its equivalent MRS.The next step is to bring handles into the repre-Figure 4: The LF in graphical formFigure 6: The steps of converting the LF for?Every cat hates a cat?
to its MRS representation52sentation.
First, we assign a different handle toeach term.
Then, for each quantifier term such asEvery(x), we add two handles as the arguments ofthe relation: one for the restriction and one for thebody as in h2: Every(x, h6, h7).
Finally, we add thehandle constraints to the MRS. We have two typesof handle constraint.
The first type comes from therestriction of each quantifier.
We add a qeq rela-tionship between the restriction handle argument ofthe quantifier term and the handle of the actual re-striction term.
The second type of constraint is theqeq relationship which defines the top handle ofthe MRS.
The speech act term in every LF refers toa formula term as content (:content slot), which isactually the heart of the LF.
We build a qeq rela-tionship between h0 (the top handle) and the han-dle of this formula term.
Figure 6 shows the effectof applying these steps to the above example.Figure 7: The LF and MRS representations for the sen-tence ?Every cat in a room sleeps.
?Another interesting issue about these two formal-isms is that the effect of applying the simple scop-ing algorithms referred in section 3 to generate allpossible interpretations of a LF is the same as ap-plying MRS axioms and handle constraints to gen-erate all scope-resolved MRSs.
For instance, theexample in (Copestake et al 2006), Every nephewof some famous politician saw a pony has the same5 interpretations using either approach.As the last point here, we need to mention thatthe algorithm in Figure 8 does not consider fixed-scopal terms such as scopal adverbials or negation.However, we believe that the framework itself isable to support these types of scopal term and witha small modification, the scoping algorithm willwork well in assigning different possible interpre-tations.
We leave the full discussion about thesedetails as well as the detailed proof of the otherclaims we made here to another paper.5 Generic Discourse InterpretationWith a generic semantic representation, we canthen define generic discourse processing capabili-ties that can be used in any application.
All ofthese methods have a corresponding capability atthe domain-specific level for an application, but wewill not discuss this further here.
We also do notdiscuss the support for language generation whichuses the same discourse context.There are three core discourse interpretation ca-pabilities that the system provides: reference reso-lution, ellipsis processing, and speech act interpre-tation.
All our different dialog systems use thesame discourse processing, whether the task in-volves collaborative problem solving, learningfrom instruction or automated tutoring.5.1 Reference ResolutionOur domain-independent representation supportsreference resolution in two ways.
First, the quanti-fiers and dependency structure extracted from thesentence allow for implementing reference resolu-tion algorithms based on extracted syntactic fea-tures.
The system uses different strategies for re-Figure 8: The LF-MRS conversion algorithm53solving each type of referring expression along thelines described in (Byron, 2002).Second, domain-independent semantic informa-tion helps greatly in resolving pronouns and defi-nite descriptions.
The general capability providedfor resolving referring expressions is to searchthrough the discourse history for the most recententity that matches the semantic requirements,where recency within an utterance may be reor-dered to reflect focusing heuristics (Tetreault,2001).
For definite descriptions, the semantic in-formation required is explicit in the lexicon.
Forpronouns, the parser can often compute semanticfeatures from verb argument restrictions.
For in-stance, the pronoun it carries little semantic infor-mation by itself, but in the utterance Eat it weknow we are looking for an edible object.
Thissimple technique performs well in practice.Because of the knowledge in the lexicon for rolenouns such as author, we can also handle simplebridging reference.
Consider the discourse frag-ment That book came from the library.
The author?.
The semantic representation of the author in-cludes its implicit argument, e.g., (The x1AUTHOR :of b1).
Furthermore, the term b1 hasthe semantic feature INFO-CONTENT, which in-cludes objects that ?contain?
information such asbooks, articles, songs, etc.., which allows the pro-noun to correctly resolve via bridging to the bookin the previous utterance.5.2 EllipsisThe parser produces a representation of fragmen-tary utterances similar to (Schlangen and Las-carides, 2003).
The main difference is that insteadof using a single underspecified unknown_relpredicate to resolve in discourse context, we use aspeech act term as the underspecified relation, dif-ferentiating between a number of common rela-tions such as acknowledgments, politeness expres-sions, noun phrases and underspecified predicates(PP, ADJP and VP fragments).
The representationsof the underspecified predicates also include anIMPRO in place of the unspecified argument.We currently handle only a few key cases of el-lipsis.
The first is question/answer pairs.
By re-taining the logical form of the question in the dis-course history, it is relatively easy to reconstructthe full content of short answers (e.g., in Who atethe pizza?
John?
the answer maps to the represen-tation that John ate the pizza).
In addition, wehandle common follow-up questions  (e.g., DidJohn buy a book?
How about a magazine?)
by per-forming a semantic closeness matching of thefragment into the previous utterance and substitut-ing the most similar terms.
The resulting term canthen be used to update the context.
This process issimilar to the resolution process in (Schlangen andLascarides, 2003), though the syntactic parallelismconstraint is not checked.
It could also be easilyextended to cover other fragment types, as thegrammar provides all the necessary information.5.3 Speech Act InterpretationThe presence of domain-independent semanticclasses allows us to encode a large set of thesecommon conversational pattern independently ofthe application task and domain.
These includerules to handle short answers to questions, ac-knowledgements and common politeness expres-sions, as well as common inferences such as inter-preting I need to do X as please do X.Given our focus on problem solving domains,we are generally interested in identifying morethan just the illocutionary force of an utterance.For instance, in a domain for planning how toevacuate people off an island, the  utterance Canwe remove the people by helicopter?
is not onlyambiguous between being a true Y-N question or asuggestion of a course of action, but at the problemsolving level it might intended to (1) introduce anew goal, (2)  elaborate or extend the solution tothe current problem, or (3) suggest a modificationto an existing solution (e.g., moving them bytruck).
One can only choose between these read-ings using domain specific reasoning about thecurrent task.
The point here is that the interpreta-tion rules are still generic across all domains andexpressed using the generic LF, yet the interpreta-tions produced are evaluated using domain-specificreasoning.
This interleaving of generic interpreta-tion and domain-specific reasoning is enabled byour ontology mappings.Similarly, in tutoring domains students oftenphrase their answers as check questions.
In an an-swer to the question Which components are in aclosed path, the student may say Is the bulb in 3 ina closed path?
The domain-independent represen-tation is used to identify the surface form of thisutterance as a yes-no question.
The dialogue man-ager then formulates two hypotheses: that this is ahedged answer, or a real question.
If a domain-54specific tutoring component confirms the formerhypothesis, the dialogue manager will proceedwith verifying answer correctness and carrying onremediation as necessary.
Otherwise (such as for Isthe bulb in 5 connected to a battery in the samecontext), the utterance is a question that can beanswered by querying the domain reasoner.5.4 A Note on Generic CapabilitiesA key point is that these generic discourse inter-pretation capabilities are enabled because of thedetailed generic semantic interpretation producedby the parser.
If the parser produced a more shal-low representation, then the discourse interpreta-tion techniques would be significantly degraded.On the other hand, if we developed a new repre-sentation for each domain, then we would have torebuild all the discourse processing for the domain.6 EvaluationOur evaluation is aimed at assessing two mainfeatures of the grammar and lexicon: portabilityand accuracy.
We use two main evaluation criteria:full sentence accuracy, that takes into account bothsyntactic and semantic accuracy of the system, andsense tagging accuracy, to demonstrate that theword senses included in the system can be distin-guished with a combination of syntactic and do-main-independent semantic information.As a measure of the breadth of grammaticalcoverage of our system, we have evaluated ourcoverage on the CSLI LKB (Linguistic KnowledgeBuilding) test suite (Copestake, 1999).
The testsuite contains approximately 1350 sentences, ofwhich about 400 are ungrammatical.
We use a full-sentence accuracy measure to evaluate our cover-age, since this is the most meaningful measure interms of what we require as parser output in ourapplications.
For a sentence representation to becounted as correct by this measure, both the syn-tactic structure and the semantic representationmust be correct, which includes the correct as-signment of word senses, dependency relationsamong terms, and speech act type.
Our currentcoverage for the diverse grammatical phenomenain the corpus is 64% full-sentence accuracy.We also report the number of spanning parsesfound, because in our system there are cases inwhich the syntactic parse is correct, but an incor-rect word sense may have been assigned, since wedisambiguate senses using not only syntacticstructure but also semantic features as selectionalrestrictions on arguments.
For example, in Themanager interviewed Browne after working, theparser assigns working the sense LF::FUNCTION,used with non-agentive subjects, instead of the cor-rect sense for agentive subjects, LF::WORKING.For the grammatical utterances in the test suite, ourparser found spanning parses for 80%.While the ungrammatical sentences in the set arean important tool for constraining grammar output,our grammar is designed to find a reasonable inter-pretation for natural speech, which often is lessthan perfect.
For example, we have low preferencegrammar rules that allow dropped subjects, miss-ing determiners, and wrong subject verb agree-ment.
In addition, utterances are often fragmentary,so even those without spanning parses may be con-sidered correct.
Our grammar allows all major con-stituents (NP, VP, ADJP, ADVP) as valid utter-ances.
As a result, our system produces spanningparses for 46% of the ?ungrammatical?
utterances.We have not yet done a detailed error analysis.As a measure of system portability to new do-mains, we have evaluated our system coverage onthe ATIS (Airline Travel Information System)speech corpus, which we have never used before.For this evaluation, the proper names (cities, air-ports, airline companies) in the ATIS corpus wereadded to our lexicon, but no other developmentwork was performed.
We parsed 116 randomlyselected test sentences and hand-checked the re-sults using our full-sentence accuracy measure.Our baseline coverage of these utterances is 53%full-sentence semantic accuracy.
Of the 55 utter-ances that were not completely correct, we foundspanning parses for 36% (20).
Reasons that span-ning parses were marked as wrong include incor-rect word senses (e.g., for stop in I would like it tohave a stop in Phoenix) or PP-attachment.
Reasonsthat no spanning parse was found include missingsenses for existing words (e.g., serve as in Doesthat flight serve dinner).7 DiscussionWe presented a deep parser and semantic inter-preter for use in dialogue systems.
An importantquestion to ask is how it compares to other existingformalisms.
At present there is no easy way tomake such comparison.
One possible criterion isgrammatical coverage.
Looking at the grammarcoverage/accuracy on the TSNLP suite that was55used to evaluate the LINGO ERG grammar, ourgrammar demonstrates 80% coverage (number ofspanning parses).
The reported figure for LINGOERG coverage of CSLI is 77% (Oepen, 1999), butthis number has undoubtedly improved in the  9-year development period.
For example, the currentreported coverage figures on spoken dialogue cor-pora are  close to 90% (Oepen et al, 2002).However, the grammar coverage alone is not asatisfactory measure for a deep NLP system for usein practical applications, because the logical formsand therefore the capabilities of deep NLP systemsdiffer significantly.
A major distinguishing featureof our system is that the logical form it outputsuses semantically motivated word senses.
LINGOERG, in contrast, contains only syntactically moti-vated word senses.
For example, the words end andfinish are not related in any obvious way.
This re-flects a difference in underlying philosophy.LINGO ERG aims for linguistic precision, and ascan be seen from our experiments, requiring theparser to select correct domain-independent wordsenses lowers accuracy.Our system, however, is built with the goal ofeasy portability within the context of dialoguesystems.
The availability of word senses simplifiesthe design of domain-independent interpretationcomponents, such as reference resolution andspeech act interpretation components that use do-main-independent syntactic and semantic informa-tion to encode conventional interpretation rules.If the LINGO ERG grammar were to be put in adialogue system that requires domain interpretationand reasoning, an additional lexical interpretationmodule would have to be developed to performword sense disambiguation as well as interpreta-tion, something that has not yet been done.AcknowledgmentsWe thank 3 reviewers for helpful comments.
Thiswork was supported by NSF IIS-0328811, DARPANBCHD30010 via subcontract to SRI #03-000223and ONR N00014051004-3 and ?8.ReferencesH.
Alshawi.
1990.
Resolving Quasi Logical Forms.Computational Linguistics 16(3):133-144.W.
Baker, C. Fillmore and J.
B. Lowe.
1998.
The Ber-keley FrameNet Project.
COLING-ACL'98, Montr?al.D.
Byron.
2002.
Resolving Pronominal Reference toAbstract Entities.
ACL-02, Philadelphia.A.
Copestake.
1999.
The (New) LKB System.
CSLI.A.
Copestake, D. Flickinger, C. Pollard and I. Sag.2006.
Minimal Recursion Semantics: An Introduc-tion.
Research on Language and Computation,3(4):281-332.M.
Dzikovska.
2004.
A Practical Semantic Representa-tion for Natural Language Parsing.
Ph.D. Thesis,University of Rochester.M.
Dzikovska, J. Allen and M. Swift.
Forthcoming.Linking Semantic and Knowledge Representations ina Multi-domain Dialogue System.
Journal of Logicand Computation.M.
Dzikovska, J. Allen and M. Swift.
2003.
IntegratingLinguistic and Domain Knowledge for Spoken Dia-logue Systems in Multiple Domains.
Workshop onKnowledge and Reasoning in Practical DialogueSystems, IJCAI-2003, Acapulco.M.
Elsner, M. Swift, J. Allen and D. Gildea.
2005.
On-line Statistics for a Unification-based DialogueParser.
IWPT05, Vancouver.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.R.
Fuchss, A. Koller, J. Niehren, S. Thater.
2004.Minimal Recursion Semantics as Dominance Con-straints.
ACL-04, Barcelona.R.
Grisham, C. Macleod and A. Meyers.
1994.
ComlexSyntax: Building a Computational Lexicon.
COLING94, Kyoto.J.
Hobbs and S. Shieber.
1987.
An Algorithm for Gen-erating Quantifier Scopings.
Computational Linguis-tics 13(1-2):47-63.K.
Kipper, H. T. Dang and M. Palmer.
2000.
Class-based Construction of a Verb Lexicon.
AAAI-2000.S.
Oepen, D. Flickinger, K. Toutanova and C. Manning.2002.
Lingo Redwoods: A Rich and Dynamic Tree-bank for HPSG.
First Workshop on Treebanks andLinguistic Theories (TLT2002).S.
Oepen (1999).
[incr tsdb()] User Manual.www.delph-in.net/itsdb/publications/manual.ps.gz.T.
Parsons.
1990.
Events in the Semantics of English.
AStudy in Subatomic Semantics.
MIT Press.D.
Schlangen and A. Lascarides 2003.
The Interpreta-tion of Non-Sentential Utterances in Dialogue.
SIG-DIAL-03, Sapporo.J.
Tetreault.
2001.
A Corpus-Based Evaluation of Cen-tering and Pronoun Resolution.
Computational Lin-guistics.
27(4):507-520.Vossen, P. (1997) EuroWordNet: A Multilingual Data-base for Information Retrieval.
In Proc.
of the Delosworkshop on Cross-language Information Retrieval.56
