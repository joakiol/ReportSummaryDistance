Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 108?118,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsConstructing a Practical Constituent Parser from a Japanese Treebank withFunction LabelsTakaaki Tanaka and Masaaki NagataNTT Communication Science LaboratoriesNippon Telegraph and Telephone Corporation{tanaka.takaaki, nagata.masaaki}@lab.ntt.co.jpAbstractWe present an empirical study on construct-ing a Japanese constituent parser, which canoutput function labels to deal with more de-tailed syntactic information.
Japanese syn-tactic parse trees are usually represented asunlabeled dependency structure between bun-setsu chunks, however, such expression is in-sufficient to uncover the syntactic informationabout distinction between complements andadjuncts and coordination structure, which isrequired for practical applications such as syn-tactic reordering of machine translation.
Wedescribe a preliminary effort on constructinga Japanese constituent parser by a Penn Tree-bank style treebank semi-automatically madefrom a dependency-based corpus.
The eval-uations show the parser trained on the tree-bank has comparable bracketing accuracy asconventional bunsetsu-based parsers, and canoutput such function labels as the grammaticalrole of the argument and the type of adnominalphrases.1 IntroductionIn Japanese NLP, syntactic structures are usuallyrepresented as dependencies between grammaticalchunks called bunsetsus.
A bunsetsu is a grammat-ical and phonological unit in Japanese, which con-sists of an independent-word such as noun, verbor adverb followed by a sequence of zero or moredependent-words such as auxiliary verbs, postposi-tional particles or sentence final particles.
It is oneof main features of Japanese that bunsetsu order ismuch less constrained than phrase order in English.Since dependency between bunsetsus can treat flexi-ble bunsetsu order, most publicly available Japaneseparsers including CaboCha (Kudo et al 2002) andKNP (Kawahara et al 2006) return bunsetsu-baseddependency as syntactic structure.
Such bunsetsu-based parsers generally perform with high accuracyand have been widely used for various NLP applica-tions.However, bunsetsu-based representations alsohave serious shortcomings for dealing with Japanesesentence hierarchy.
The internal structure of a bun-setsu has strong morphotactic constraints in contrastto flexible bunsetsu order.
A Japanese predicatebunsetsu consists of a main verb followed by a se-quence of auxiliary verbs and sentence final parti-cles.
There is an almost one-dimensional order inthe verbal constituents, which reflects the basic hi-erarchy of the Japanese sentence structure includingvoice, tense, aspect and modality.
Bunsetsu-basedrepresentation cannot provide the linguistic structurethat reflects the basic sentence hierarchy.Moreover, bunsetsu-based structures are unsuit-able for representing such nesting structure as co-ordinating conjunctions.
For instance, bunsetsu rep-resentation of a noun phrase ???-?
(technology-GEN) / ??-?
(improvement-CONJ) / ??-?
(economy-GEN) / ??
(growth) ?
technology im-provement and economic growth does not allowus to easily interpret it, which means ((technol-ogy improvement) and (economic growth)) or (tech-nology (improvement and economic growth)), be-cause bunsetsu-based dependencies do not con-vey information about left boundary of each nounphrase (Asahara, 2013).
This drawback complicates108operating syntactically meaningful units in such ap-plications as statistical machine translation, whichneeds to recognize syntactic units in building a trans-lation model (e.g.
tree-to-string and tree-to-tree) andin preordering source language sentences.Semantic analysis, such as predicate-argumentstructure analysis, is usually done as a pipeline pro-cess after syntactic analysis (Iida et al 2011 ;Hayashibe et al 2011 ); but in Japanese, the dis-crepancy between syntactic and semantic units causedifficulties integrating semantic analysis with syn-tactic analysis.Our goal is to construct a practical constituentparser that can deal with appropriate grammaticalunits and output grammatical functions as semi-semantic information, e.g., grammatical or seman-tic roles of arguments and gapping types of relativeclauses.
We take an approach to deriving a grammarfrom manually annotated corpora by training prob-abilistic models like current statistical constituentparsers of de facto standards (Petrov et al 2006;Klein et al 2003 ; Charniak, 2000; Bikel, 2004).We used a constituent-based treebank that Uematsuet al(2013) converted from an existing bunsetsu-based corpus as a base treebank, and retag the non-terminals and transform the tree structures in de-scribed in Section 3.
We will present the results ofevaluations of the parser trained with the treebank inSection 4, and show some analyses in Section 5.2 Related workThe number of researches on Japanese constituent-based parser is quite few compared to that ofbunsetsu-dependency-based parser.
Most of themhave been conducted under lexicalized grammaticalformalism.HPSG (Head-driven Phrase Structure Gram-mar) (Sag et al 2003 ) is a representative one.Gunji et al(1987) proposed JPSG (Japanese PhraseStructure Grammar) that is theoretically precise tohandle the free word order problem of Japanese.
Na-gata et al( 1993 ) built a spoken-style Japanesegrammar and a parser running on it.
Siegel et al2002 ) constructed a broad-coverage linguisticallyprecise grammar JACY, which integrates semantics,MRS (Minimal Recursion Semantics) (Copestake,2005).
Bond et al( 2008 ) built a large-scaleJapanese treebank Hinoki based on JACY and usedit for parser training.Masuichi et al2003) developed a Japanese LFG(Lexicalized-Functional Grammar) (Kaplan et al1982) parser whose grammar is sharing the de-sign with six languages.
Uematsu et al(2013)constructed a CCG (Combinatory Categorial Gram-mar) bank based on the scheme proposed byBekki (2010), by integrating several corpora includ-ing a constituent-based treebank converted from adependency-base corpus.These approaches above use a unification-basedparser, which offers rich information integratingsyntax, semantics and pragmatics, however, gener-ally requires a high computational cost.
We aimat constructing a more light-weighted and practicalconstituent parser, e.g.
a PCFG parser, from PennTreebank style treebank with function labels.
Gab-bard et al(2006) introduced function tags by modi-fying those in Penn Treebank to their parser.
Eventhough Noro et al(2005) built a Japanese corpus forderiving Japanese CFG, and evaluated its grammar,they did not treat the predicate-argument structure orthe distinction of adnominal phrases.This paper is also closely related to the work ofKorean treebank transformations (Choi et al 2012).Most of the Korean corpus was built using grammat-ical chunks eojeols, which resemble Japanese bun-setsus and consist of content words and morphemesthat represent grammatical functions.
Choi et altransformed the eojeol-based structure of Koreantreebanks into entity-based to make them more suit-able for parser training.
We converted an existingbunsetsu-based corpus into a constituent-based oneand integrating other information into it for traininga parser.3 Treebank for parser trainingIn this section, we describe the overview of our tree-bank for training a parser.3.1 Construction of a base treebankOur base treebank is built from a bunsetsu-dependency-based corpus, the Kyoto Corpus (Kuro-hashi et al 2003), which is a collection of news-paper articles, that is widely used for training datafor Japanese parsers and other applications.
We109SIP-MAT[nad]:AVP[nad]:AVP[nad]:AAUX?-PASTVB[nad]??givePP-OBJPCS?-ACCNN?bookPP-OB2PCS?-DATNN??studentSIP-MAT[nad]:AVP[nad]:AAUX?-PASTVB[nad]??givePP-OBJPCS?-ACCNN?bookPP-OB2PCS?-DATNN?
?student(I) gave the student a book.binary tree n-ary (flattened) treeFigure 1: Verb Phrase with subcategorization and voice informationNN General nounNNP Proper nounNPR PronounNV Verbal nounNADJ Adjective nounNADV Adverbial noun (incl.
temporal noun)NNF Formal noun (general)NNFV Formal noun (adverbial)PX PrefixSX SuffixNUM NumeralCL ClassifierVB VerbADJ AdjectiveADNOM Adnominal adjectiveADV AdverbPCS Case particlePBD Binding particlePADN Adnominal particlePCO Parallel particlePCJ Conjunctive particlePEND Sentence-ending particleP Particle (others)AUX Auxiliary verbCONJ ConjunctionPNC PunctuationPAR ParenthesisSYM SymbolFIL FillerTable 1: Preterminal tagsautomatically converted from dependency structureto phrase structure by the previously describedmethod (Uematsu et al 2013), and conversion er-rors of structures and tags were manually corrected.We adopted the annotation schema used inJapanese Keyaki treebank (Butler et al 2012) andAnnotation Manual for the Penn Historical Corporaand the PCEEC (Santorini, 2010) as reference to re-tag the nonterminals and transform the tree struc-tures.The original Kyoto Corpus has fine-grained part-of-speech tags, which we converted into simplerpreterminal tags shown in Table 1 for training bylookup tables.
First the treebank?s phrase tags ex-cept function tags are assigned by simple CFG rulesets, then, function tags are added by integrating theinformation from the other resources or manuallyannotated.
We integrate predicate-argument infor-mation from the NAIST Text Corpus (NTC) (Iida etal., 2007 ) into the treebank by automatically con-verting and adding tag suffixes (e.g.
-SBJ, -ARG0described in section 3.3) to the original tags of theargument phrases.
The structure information aboutcoordination and apposition are manually annotated.3.2 Complementary informationWe selectively added the following information astag suffixes and tested their effectiveness.Inflection We introduced tag suffixes for inflec-tion as clues to identify the attachment position ofthe verb and adjective phrases, because Japaneseverbs and adjectives have inflections, which depends110(no label) base formcont continuative formattr attributive formneg negative formhyp hypothetical formimp imperative formstem stemTable 2: Inflection tag suffixeson their modifying words and phrases (e.g.
nounand verb phrases).
Symbols in Table 2 are attachedto tags VB, ADJ and AUX, based on their inflectionform.
The inflection information is propagated to thephrases governing the inflected word as a head.
Weadopted these symbols from the notation of JapaneseCCG described in (Bekki, 2010).Subcategorization and voice Each verb has asubcategorization frame, which is useful for build-ing verb phrase structure.
For instance, ?
?tsukamu ?grasp?
takes two arguments, nominativeand accusative cases, ???
ataeru ?give?
takesthree arguments: nominative, accusative and dativecases.
We also added suffixes to verb tags to de-note which arguments they require (n:nominative,a:accusative and d: dative).
For instance, theverb???
?give?
takes three arguments (nomina-tive, accusative and dative cases), it is tagged withVB[nad].We retrieve this information from a Japanese caseframe dictionary, Nihongo Goitaikei (Ikehara et al1997), which has 14,000 frames for 6,000 verbs andadjectives.
As an option, we also added voice infor-mation (A:active, P:passive and C:causative) to theverb phrases, because it effectively helps to discrim-inate cases.3.3 Annotation schemaWe introduce phrase and function tags in Table 3 anduse them selectively based on the options describedbelow.Tree Structure We first built a treebank with bi-nary tree structure (except the root and terminalnodes), because it is comparably easy to convertthe existing Japanese dependency-based corpus toit.
We converted the dependency-based corpus bya previously described method in (Uematsu et al2013).
The binary tree?s structure has the follow-NP Noun phrasePP Postposition phraseVP Verb phraseADJP Adjective phraseADVP Adverbial phraseCONJP Conjunction phraseS Sentence (=root)IP Inflectional phraseIP-MAT Matrix clauseIP-ADV Adverb clauseIP-REL Gapping relative clauseIP-ADN Non-gapping adnominal clauseCP Complementizer phraseCP-THT Sentential complementFunction tagssemantic role for mandatory argument (gap notation)-ARG0 ( arg0)-ARG1 ( arg1)-ARG2 ( arg2)grammatical role for mandatory argument (gap notation)-SBJ ( sbj) Subjective case-OBJ ( obj) Objective case-OB2 ( ob2) Indirect object casearbitrary argument-TMP Temporal case-LOC Locative case-COORD Coordination (for n-ary)-NCOORD Left branch of NP coord.
(for binary)-VCOORD Left branch of VP coord.
(for binary)-APPOS Apposition-QUE QuestionTable 3: Phrase tagsing characteristics about verb phrase (VP) and post-position phrase (PP): VP from the same bunsetsuis a left-branching subtree and the PP-VP structure(roughly corresponding to the argument-predicatestructure) is a right-branching subtree.
Pure binarytrees tend to be very deep and difficult to annotateand interpret by humans.
We also built an n-ary treeversion by flattening these structures.The predicate-argument structure, which is usu-ally represented by PPs and a VP in the treebank,particularly tends to be deep in binary trees basedon the number of arguments.
To flatten the structure,we remove the internal VP nodes by intermediatelyre-attaching all of the argument PPs to the VP thatdominates the predicate.
Figure 1 shows an exampleof flattening the PP-VP structure.For noun phrases, since compound nouns and nu-merals cause deep hierarchy, the structure that in-cludes them is flattened under the parent NP.
Thecoordinating structure is preserved, and each NP el-ement of the coordination is flattened111IP-MATVPVPP?-PASTVB???
?chasePP-OBJPCS?-ACCNN?catPP-SBJPCS?-NOMNN?dogNPNPNN?dogIP-REL sbjVPP?-PASTVB????chasePP-OBJPCS?-ACCNN?catNPNPNN??photoIP-ADNVPVPAUX??-PROGVPP?VB???
?chasePP-OBJPCS?-ACCNN?catPP-SBJPCS?-NOMNP?dogThe dog chased the cat.
The dog that chased the cat The photo of a dog chasing a catFigure 2: Leftmost tree shows annotation of grammatical roles in a basic inflectional phrase.
Right two trees showexamples of adnominal phrases.Predicates and arguments The predicate?s argu-ment is basically marked with particles, which rep-resent cases in Japanese; thus, they are representedas a postpositional phrase, which is composed ofa noun phrase and particles.
The leftmost tree inFigure 2 is an example of the parse result of thefollowing sentence: ?-?
inu-ga ?dog-NOM?
?-?
neko-o ?cat-ACC??????
oikaketa ?chased?
(The dog chased the cat.
)We annotated predicate arguments by two dif-ferent schemes (different tag sets) in our treebank:grammatical roles and semantic roles.
In using a tagset based on grammatical roles, the arguments areassigned with the suffixes based on their syntacticroles in the sentence, like Penn Treebank: SBJ (sub-ject), OBJ (direct object), and OB2 (indirect object).Figure 2 is annotated by this scheme.Alternatively, the arguments are labeled based ontheir semantic roles from case frame of predicates,like PropBank (Palmer et al 2005 ): ARG0, ARG1and ARG2.
These arguments are annotated by con-verting semantic roles defined in the case frame dic-tionary Goitaikei into simple labels, the labels arenot influenced by case alternation.In both annotation schemes, we also annotatedtwo types of arbitrary arguments with semantic rolelabels: LOC (locative) and TMP (temporal), whichcan be assigned consistently and are useful for vari-ous applications.Adnominal clauses Clauses modifying nounphrases are divided into two types: (gapping) rela-tive and non-gapping adnominal clauses.
Relativeclauses are denoted by adding function tag -REL tophrase tag IP.
Such a gap is directly attached toIP-REL tag as a suffix consisting of an underscoreand small letters in our treebank, e.g., IP-REL sbjfor a subject-gap relative clause, so that the parsercan learn the type of gap simultaneously, unlikethe Penn Treebank style, where gaps are markedas trace ?*T*?.
For instance, note the structure ofthe following noun phrase, which is shown in themiddle tree in Figure 2: ?-?
neko-o ?cat-ACC??????
oikake-ta ?to chase?
?
inu ?dog?
?neko-o (cat-ACC) oikaketa (chase) inu?
(The dogthat chased the cat.).
We also adopt another type ofgap notation that resembles the predicate-argumentstructure: semantic role notation.
In the exampleabove, tag IP-REL arg0 is attached to the relativeclause instead.We attach tag IP-ADN to another type of ad-nominal clauses, which has no gap, the modifiednoun phrase is not an argument of the predicate inthe adnominal clause.
The rightmost in Figure 2 isan example of a non-gapping clause: ?-?
inu-ga?dog-NOM?
?-?
neko-o ?cat-ACC?
???????
oikake-teiru ?chasing?
??
shashin ?photo?
(A photo of a dog chasing a cat.
), where there is nopredicate-argument relation between the verb ?????
chase and the noun??
photo.112Coordination and apposition The notation ofsuch parallel structure as coordination and apposi-tion differs based on the type of tree structure.
Forbinary trees, the coordination is represented by aleft-branching tree, which is a conjunction or a con-junction particle that first joined a left hand con-stituent; the phrase is marked as a modifier consist-ing of coordination (-NCOORD and -VCOORD forNP and VP coordinations), as shown on the left sideof Figure 3.
On the other hand, in n-ary trees, all thecoordination elements and conjunctions are alignedflatly under the parent phrase with suffix -COORD.The apposition is represented in the same way usingtag -APPOS instead.Phrase and sentential elements Since predicatearguments are often omitted in Japanese, discrimi-nation between the fragment of larger phrases andsentential elements is not clear.
In treebank, we em-ploy IP and CP tags for inflectional and comple-mentizer phrases, assuming that tags with functiontag suffixes to the phrase correspond to the max-imum projection of the predicate (verb or adjec-tive).
The matrix phrase and the adverbial phrasehave IP-MAT and IP-ADV tags respectively.
Thisannotation schema is adopted based on the PennHistorical Corpora (Santorini, 2010) and JapaneseKeyaki treebank (Butler et al 2012) as previouslydescribed, while IP in our treebank is not so flat asthem.Such sentential complements as that-clauses inEnglish are tagged with CP-THT.
In other words,the sentential elements, which are annotated withSBAR, S, and trace *T* in the Penn Treebank, aretagged with CP or IP in our treebank.4 EvaluationThe original Kyoto Corpus has 38,400 sentencesand they were automatically converted to constituentstructures.
The function tags are also added to thecorpus by integrating predicate-argument informa-tion in the NAIST Text corpus.
Since the conver-sion contains errors of structures and tags, about halfof them were manually checked to avoid the effectsof the conversion errors.We evaluated our treebank?s effectiveness forparser training with 18,640 sentences, which weredivided into three sets: 14,895 sentences for a train-Tag set LF1 Comp UF1 Compbinary treeBase 88.4 34.0 89.6 37.9Base inf 88.5?
33.5 90.0?
39.3Fullsr 80.7 13.6 88.4 35.9Fullsr inf 81.1?
15.5 ?
88.7?
36.9Fullsr lex 79.8?
13.1 87.7?
34.3Fullsr vsub 80.3?
12.5 87.9?
35.1Fullsr vsub alt 78.6?
13.3 86.7?
32.5?Fullgr 81.0 15.6 88.5 37.3Fullgr inf 81.3?
15.3 88.8 37.2Fullgr lex 80.3?
14.2 87.9?
33.6?Fullgr vsub 81.2 15.5 88.5 35.2Fullgr vsub alt 77.9?
11.7?
86.0?
29.9?n-ary treeFullsr 76.7 11.4 85.3 28.0Fullsr inf 76.9 11.6 85.4 28.7Fullsr lex 76.5 11.1 84.7?
27.9Fullsr vsub 76.5 10.8 84.9?
26.2Fullsr vsub alt 76.6 11.0 84.8?
27.2Fullgr 77.2 13.2 85.3 29.2Fullgr inf 77.4 12.0?
85.5 28.3Fullgr lex 77.6 12.2?
85.0 28.5Fullgr vsub 77.1 12.7?
84.8?
28.8Fullgr vsub alt 76.9 12.2?
84.7?
26.3?Table 4: Parse results displayed by labeled and unla-beled F1 metrics and proportion of sentences completelymatching gold standard (Comp).
Base contains only ba-sic tags, not grammatical function tags.
Figures with ??
?indicate statistically significant differences (?
= 0.05)from the results without complementary information, i.e.,Fullsr or Fullgr.113NPNN?
?interestPPPADN?-GENNPNNPB ?B CompanyPP-NCOORDPCJ?CONJNNPA ?A CompanyNPNN?
?interestPPPADN?-GENNP-COORDNNPB ?B CompanyPCJ?CONJNNPA ?A Companythe interests of A Company and B CompanyFigure 3: Noun phrase coordinationtag set UASbinary treeBase 89.1Base inf 89.4Fullsr 87.9Fullsr inf 88.3Fullgr 88.0Fullgr inf 88.5?n-ary (flattened) treeFullsr 82.8Fullsr inf 83.3Fullgr 82.9Fullgr inf 83.0Table 5: Dependency accuracies of the results convertedinto bunsetsu dependencies.ing set, 1,860 sentences for a test set, and the re-mainder for a development set.The basic evaluations were under the condition ofusing the original tag sets: the basic set Base, whichcontains all the preterminal tags in Table 1 and thephrase tags in Table Table 3, except the IP and CPtags, and the full set Full, which has Base + IP, CPtags, and all the function tags.
The basic set Baseis provided to evaluate the constituent parser perfor-mance in case that we need better performance at thecost of limiting the information.We used two types of function tag sets: Full sr forsemantic roles and Full gr for grammatical roles.We added the following complementary informa-tion to the tags and named the new tag sets Base orFull and suffix:inf: add inflection information to the POS tag(verbs, adjectives, and auxiliary verbs) and thephrase tags (Table 2).lex: lexicalize the closed words, i.e., auxiliaryverbs and particles.vsub: add verb subcategorization to the verb andverb phrase tags.vsub alt: add verb subcategorization and case al-ternation to the verb and verb phrase tags.In comparing the system output with the gold stan-dard, we remove the complementary information toignore different level of annotation, thus, we do notdiscriminate between VB[na] and VB[nad] forexample.We used the Berkeley parser (Petrov et al 2006)for our evaluation and trained with six iterations forlatent annotations.
In training the n-ary trees, weused a default Markovization parameter (h = 0, v =1), because the parser performed the best with thedevelopment set.Table 4 shows the parsing results of the test sets.On the whole, the binary tree outperformed the n-ary tree.
This indicates that the binary tree struc-ture was converted from bunsetsu-based dependen-cies, whose characteristics are described in Section3.3, and is better for parser training than the partiallyflattened structure.114As for additional information, the inflection suf-fixes slightly improved the F1-metrics.
This ismainly because the inflection information gives thecategory of the attached phrase (e.g., the attributiveform for noun phrases).
The others did not provideany improvement, even though we expected the sub-categorization and case alternation information tohelp the parser detect and discriminate the grammat-ical roles, probably because we simply introducedthe information by concatenating the suffixes to thebase tags to adapt an off-the-shelf parser in our eval-uation.
For instance, VB[n] and VB[na] are rec-ognized as entirely independent categories; a sophis-ticated model, which can treat them hierarchically,would improve the performance.For comparison with a bunsetsu-based depen-dency parser, we convert the parser output into unla-beled bunsetsu dependencies by the following sim-ple way.
We first extract all bunsetsu chunks ina sentence and find a minimum phrase includingeach bunsetsu chunk from a constituent structure.For each pair of bunsetsus having a common parentphrase, we add a dependency from the left bunsetsuto the right one, since Japanese is a head-final lan-guage.The unlabeled attachment scores of the converteddependencies are shown as the accuracies in Table 5,since most bunsetsu-based dependency parsers out-put only unlabeled structure.The Base inf results are comparable with thebunsetsu-dependency results (90.46%) over thesame corpus (Kudo et al 2002)1, which has onlythe same level of information.
Constituent parsingwith treebank almost matched the current bunsetsuparsing.5 AnalysisIn this section, we analyze the error of parse resultsfrom the point of view of the discrimination of gram-matical and semantic roles, adnominal clause andcoordination.Grammatical and semantic roles Predicate argu-ments usually appeared as PP, which is composed ofnoun phrases and particles.
We focus on PPs withfunction labels.
Table 6 shows the PP results with1The division for the training and test sets is different.tag P R F1PP-ARG0 64.9 75.0 69.6PP-ARG1 70.6 80.1 75.1PP-ARG2 60.3 68.5 64.1PP-TMP 40.1 43.6 41.8PP-LOC 23.8 17.2 20.0tag P R F1PP-SBJ 69.6 81.5 75.1PP-OBJ 72.6 83.5 77.7PP-OB2 63.6 71.4 67.3PP-TMP 45.0 48.0 46.5PP-LOC 21.3 15.9 18.2Table 6: Discrimination of semantic role and grammati-cal role labels (upper: semantic roles, lower: grammaticalrole)system \ gold PP-SBJ PP-OBJ PP-OB2PP-SBJ *74.9 6.5 2.3PP-OBJ 5.8 *80.1 0.5PP-OB2 1.7 0.3 *68.5PP-TMP 0.2 0.0 0.5PP-LOC 0.2 0.0 0.4PP 6.5 2.0 16.8other labels 0.5 0.2 0.3no span 10.2 10.9 11.0system \ gold PP-TMP PP-LOCPP-SBJ 4.7 4.1PP-OBJ 0.0 0.0PP-OB2 6.0 13.8PP-TMP *43.6 2.8PP-LOC 2.0 *17.2PP 37.6 49.7other labels 1.4 5.0no span 4.7 7.4Table 7: Confusion matrix for grammatical role labels(recall).
Figures with ?*?
indicate recall.
(binary tree,Fullgr)tag P R F1IP-REL sbj 48.4 54.3 51.1IP-REL obj 27.8 22.7 24.9IP-REL ob2 17.2 29.4 21.7IP-ADN 50.9 55.4 53.1CP-THT 66.1 66.6 66.3Table 8: Results of adnominal phrase and sentential ele-ment (binary tree, Fullgr)115grammatical and semantic labels under the Fullsrand Fullgr conditions respectively.The precision and the recall of mandatory argu-ments did not reach a high level.
The results arerelated to predicate argument structure analysis inJapanese.
But, they cannot be directly compared,because the parser in this evaluation must output acorrect target phrase and select it as an argument, al-though most researches select a word using a goldstandard parse tree.
Hayashibe et al( 2011 ) re-ported the best precision of ARG0 discrimination tobe 88.42 % 2, which is the selection results fromthe candidate nouns using the gold standard parsetree of NTC.
If the cases where the correct candi-dates did not appear in the parser results are ex-cluded (10.8 %), the precision is 72.7 %.
The mainremaining error is to label to non-argument PP withsuffix -ARG0 (17.0%), thus, we must restrain theoverlabeling to improve the precision.The discrimination of grammatical role is higherthan that of semantic role, which is more directly es-timated by case particles following the noun phrases.The confusion matrix for the recall in Table 7 showsmain problem is parse error, where correct phrasespan does not exist (no span), and marks 10-11%.The second major error is discrimination from barePPs (PPs without suffixes), mainly because the cluesto judge whether the arguments are mandatory or ar-bitrary lack in the treebank.
Since even the manda-tory arguments are often omitted in Japanese, it isnot facilitate to identify arguments of predicates byusing only syntactic information.Adnominal phrases We need to discriminate be-tween two types of adnominal phrases as describedin Section 3.3: IP-REL and IP-ADN.
Table 8shows the discrimination results of the adnominalphrase types.
The difference between IP-REL(gapped relative clauses) and IP-ADN is closely re-lated to the discrimination of the grammatical role:whether the antecedent is the argument of the headpredicate of the relative clause.Table 8 shows the discrimination results of theadnominal phrases.
The results indicate the diffi-culties of discriminating the type of gaps of rela-2The figure is calculated only for the arguments that appearas the dependents of predicates, excluding the omitted argu-ments.tive clause IP-REL.
The confusion matrix in Ta-ble 9 shows that the discrimination between gapsand non-gaps, i.e., IP-REL and IP-ADN, is moder-ate as for IP-REL sbj and IP-REL obj.
How-ever, IP-REL ob2 is hardly recognized, becauseit is difficult to determine whether the antecedent,which is marked with particle ?ni?, is a mandatory ar-gument (IP-REL ob2) or not (IP-ADN).
Increas-ing training samples would improve the discrimina-tion, since there are only 290 IP-REL ob2 tags for8,100 IP-ADN tags in the training set.Naturally discrimination only by syntactic infor-mation has limitation; this baseline can be improvedby incorporating semantic information.Coordination Figure 10 shows the coordinationresults, which are considered the baseline for onlyusing syntactic information.
Improvement is possi-ble by incorporating semantic information, since thedisambiguation of coordination structure essentiallyneeds semantic information.6 ConclusionWe constructed a Japanese constituent-based parserto be released from the constraints of bunsetsu-basedanalysis to simplify the integration of syntactic andsemantic analysis.
Our evaluation results indicatethat the basic performance of the parser trained withthe treebank almost equals bunsetsus-based parsersand has the potential to supply detailed syntacticinformation by grammatical function labels for se-mantic analysis, such as predicate-argument struc-ture analysis.Future work will be to refine the annotationscheme to improve parser performance and to eval-uate parser results by adapting them to such NLPapplications as machine translation.AcknowledgmentsWe would like to thank Associate Professor YusukeMiyao, Associate Professor Takuya Matsuzaki ofNational Institute of Informatics and Sumire Ue-matsu of the University of Tokyo for providing usthe language resources and giving us valuable sug-gestions.116system \ gold IP-REL sbj IP-REL objIP-REL sbj *55.0 30.3IP-REL obj 8.5 *33.3IP-REL ob2 0.5 0.0IP-ADN 10.0 9.0IP-ADV 0.3 0.0VP 8.5 7.6other labels 1.2 6.2no span 16.0 13.6system \ gold IP-REL ob2 IP-ADNIP-REL sbj 29.4 7.5IP-REL obj 0.0 0.6IP-REL ob2 *11.8 0.0IP-ADN 23.5 *57.3IP-ADV 0.5 0.3VP 17.6 9.3other labels 5.4 3.0no span 11.8 22.0Table 9: Confusion matrix for adnominal phrases (recall).Figures with ?*?
indicate recall.
(binary tree, Fullgr)tag P R F1NP-COORD 62.6 60.7 61.6VP-COORD 57.6 50.0 53.5NP-APPOS 46.0 40.0 42.8Table 10: Results of coordination and apposition (binarytree, Fullgr)ReferencesMasayuki Asahara.
2013.
Comparison of syntactic de-pendency annotation schemata .
In Proceedings ofthe 3rd Japanese Corpus Linguistics Workshop, InJapanese.Daisuke Bekki.
2010.
Formal theory of Japanese syntax.Kuroshio Shuppan, In Japanese.Daniel M. Bikel.
2004.
A distributional analysis of alexicalized statistical parsing model.
In Proceedingsof Empirical Methods in Natural Language Processing(EMNLP 2004), Vol.4, pp.
182?189.Francis Bond, Sanae Fujita and Takaaki Tanaka.
2008.The Hinoki syntactic and semantic treebank ofJapanese.
In Journal of Language Resources andEvaluation, Vol.42, No.
2, pp.
243?251Alastair Butler, Zhu Hong, Tomoko Hotta, RurikoOtomo, Kei Yoshimoto and Zhen Zhou.
2012.
KeyakiTreebank: phrase structure with functional informa-tion for Japanese.
In Proceedings of Text AnnotationWorkshop.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, (NAACL 2000), pp.
132?139.DongHyun Choi, Jungyeul Park and Key-Sun Choi.2012.
Korean treebank transformation for parser train-ing.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (ACL2012), pp.
78-88.Ann Copestake, Dan Flickinger, Carl Pollard and Ivan A.Sag.
2005.
Minimal recursion semantics: an introduc-tion.
Research on Language and Computation, Vol.
3,No.
4, pp.
281-332.Ryan Gabbard, Mitchell Marcus and Seth Kulick.
2006.Fully parsing the Penn Treebank.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association of Compu-tational Linguistics (HLT-NAACL 2006), pp.
184?191.Takao Gunji.
1987 Japanese phrase structure grammar:a unification-based approach.
D.Reidel.Yuta Hayashibe, Mamoru Komachi and Yujzi Mat-sumoto.
2011.
Japanese predicate argument struc-ture analysis exploiting argument position and type.
InProceedings of the 5th International Joint Conferenceon Natural Language Processing (IJCNLP 2011), pp.201-209.Ryu Iida, Mamoru Komachi Kentaro Inui and Yuji Mat-sumoto.
2007.
Annotating a Japanese text corpus withpredicate-argument and coreference relations.
In Pro-ceedings of Linguistic Annotation Workshop, pp.
132?139.Ryu Iida, Massimo Poesio.
2011.
A cross-lingual ILPsolution to zero anaphora resolution.
In Proceedings117of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies (ACL-HLT 2011), pp.
804-813.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Kentaro Ogura, Yoshifumi Ooyama and Yoshi-hiko Hayashi.
1998.
Nihongo Goitaikei.
IwanamiShoten, In Japanese.Ronald M. Kaplan and Joan Bresnan.
1982.
Lexical-Functional Grammar: a formal system for grammat-ical representation.
In the Mental Representation ofGrammatical Relations (Joan Bresnan ed.
), pp.
173?281.
The MIT Press.Daisuke Kawahara and Sadao Kurohashi.
2006.
Afully-lexicalized probabilistic model for Japanese syn-tactic and case structure analysis.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association of Compu-tational Linguistics (HLT-NAACL 2006), pp.
176?183.Dan Klein and Christopher D. Manning.
2003.
Fast exactinference with a factored model for natural languageprocessing.
Advances in Neural Information Process-ing Systems, 15:3?10.Taku Kudo and Yuji Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
In Pro-ceedings of the 6th Conference on Natural LanguageLearning (CoNLL-2002), Volume 20, pp.
1?7.Sadao Kurohashi and Makoto Nagao.
2003.
Building aJapanese parsed corpus ?
while improving the parsingsystem.
In Abeille (ed.
), Treebanks: Building and us-ing parsed corpora, Chap.
14, pp.
249?260.
KluwerAcademic Publishers.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
In Journal of Com-putational Linguistics.
Vol.19, No.2, pp.
313?330.Hiroshi Masuichi, Tomoko Okuma, Hiroki Yoshimuraand Yasunari Harada.
2003 Japanese parser on the ba-sis of the Lexical-Functional Grammar formalism andits evaluation.
In Proceedings of the 17th Pacific AsiaConference on Language, Information and Computa-tion (PACLIC 17), pp.
298-309.Masaaki Nagata and Tsuyoshi Morimoto,.
1993.A unification-based Japanese parser for speech-to-speech translation.
In IEICE Transaction on Informa-tion and Systems.
Vol.E76-D, No.1, pp.
51?61.Tomoya Noro, Taiichi Hashimoto, Takenobu Tokunagaand Hotsumi Tanaka.
2005.
Building a large-scaleJapanese syntactically annotated corpus for deriving aCFG.
in Proceedings of Symposium on Large-ScaleKnowledge Resources (LKR2005), pp..159 ?
162.Matha Palmer, Daniel Gildea and Paul Kingsbury.
2005.The Proposition Bank: n annotated corpus of semanticroles.
Computational Linguistics, Vol.31 No.
1, pp.71?106.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.. 2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th Annual Meeting of the Associationfor Computational Linguistics (COLING-ACL 2006),pp.
433-440.Ivan A.
Sag, Thomas Wasow and Emily M. Bender,.2003.
Syntactic theory: a formal introduction.
2ndEdition, CSLI Publications.Beatrice Santorini.
2010.
Annotation manual for thePenn Historical Corpora and the PCEEC (Release 2).Department of Linguistics, University of Pennsylva-nia.Melanie Siegel and Emily M. Bender.
2002.
Efficientdeep processing of Japanese.
In Proceedings of the3rd Workshop on Asian Language Resources and In-ternational Standardization at the 19th InternationalConference on Computational Linguistics, Vol.
12, pp.1?8.Sumire Uematsu, Takuya Matsuzaki, Hiroaki Hanaoka,Yusuke Miyao and Hideki Mima.
2013.
Integrat-ing multiple dependency corpora for inducing wide-coverage Japanese CCG resources.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (ACL 2013), pp.
1042?1051.118
