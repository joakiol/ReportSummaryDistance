Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 30?33,Baltimore, Maryland, USA, June 27, 2014.c?2014 Association for Computational LinguisticsActive Learning with Constrained Topic ModelYi YangNorthwestern Universityyiyang@u.northwestern.eduShimei PanIBM T. J. Watson Research Centershimei@us.ibm.comDoug DowneyNorthwestern Universityddowney@eecs.northwestern.eduKunpeng ZhangUniversity of Illinois at Chicagokzhang6@uic.eduAbstractLatent Dirichlet Allocation (LDA) is a topicmodeling tool that automatically discoverstopics from a large collection of documents.It is one of the most popular text analysistools currently in use.
In practice however,the topics discovered by LDA do not al-ways make sense to end users.
In this ex-tended abstract, we propose an active learn-ing framework that interactively and itera-tively acquires user feedback to improve thequality of learned topics.
We conduct exper-iments to demonstrate its effectiveness withsimulated user input on a benchmark dataset.1 IntroductionStatistical topic models such as Latent Dirichlet Al-location (LDA) (Blei et al., 2003) provide powerfultools for uncovering hidden thematic patterns in textand are useful for representing and summarizing thecontents of large document collections.
However,when using topic models in practice, users often faceone critical problem: topics discovered by the modeldo not always make sense.
A topic may contain the-matically unrelated words.
Moreover, two thematicrelated words may appear in different topics.
Thisis mainly because the objective function optimizedby LDA may not reflect human judgments of topicquality (Boyd-Graber et al., 2009).Potentially, we can solve these problems by incor-porating additional user guidance or domain knowl-edge in topic modeling.
With standard LDA how-ever, it is impossible for users to interact with themodel and provide feedback.
(Hu et al., 2011) pro-posed an interactive topic modeling framework thatallows users to add word must-links.
However, ithas several limitations.
Since the vocabulary size ofa large document collection can be very large, usersmay need to annotate a large number of word con-straints for this method to be effective.
Thus, thisprocess can be very tedious.
More importantly, itcannot handle polysemes.
For example, the word?pound?
can refer to either a currency or a unit ofmass.
If a user adds a must-link between ?pound?and another financial term, then he/she cannot adda must-link between ?pound?
and any measurementterms.
Since word must-links are added withoutcontext, there is no way to disambiguate them.
As aresult, word constraints frequently are not as effec-tive as document constraints.Active learning (Settles, 2010) provides a use-ful framework which allows users to iteratively givefeedback to the model to improve its quality.
In gen-eral, with the same amount of human labeling, ac-tive learning often results in a better model than thatlearned by an off-line method.In this extended abstract, we propose an activelearning framework for LDA.
It is based on a newconstrained topic modeling framework which is ca-pable of handling pairwise document constraints.We present several design choices and the pros andcons of each choice.
We also conduct simulated ex-periments to demonstrate the effectiveness of the ap-proach.2 Active Learning With Constrained TopicModelingIn this section, we first summarize our work on con-strained topic modeling.
Then, we introduce anactive topic learning framework that employs con-strained topic modeling.In LDA, a document?s topic distribution~?
isdrawn from a Dirichlet distribution with prior ~?.A simple and commonly used Dirichlet distributionuses a symmetric ~?
prior.
However, (Wallach et al.,2009) has shown that an asymmetric Dirichlet priorover the document-topic distributions~?
and a sym-metric Dirichlet prior over the topic-word distribu-tions~?
yield significant improvements in model per-formance.
Our constrained topic model uses asym-metric priors to encode constraints.To incorporate user feedback, we focus on two130Figure 1: Diagram illustrating the topic model active learning framework.types of document constraints.
A must-link be-tween two documents indicates that they belong tothe same topics, while a cannot-link indicates thatthey belong to different topics.Previously, we proposed a constrained LDAframework called cLDA,1which is capable of incor-porating pairwise document constraints.
Given pair-wise document constraints, the topic distribution ofa document cannot be assumed to be independentlysampled.
More specifically, we denote the collectionof documents as D = {d1, d2, ..., dN}.
We also de-noteMi?
D as the set of documents sharing must-links with document di, and Ci?
D as the set ofdocuments sharing cannot-links with document di.~?iis the topic distribution of di, and ~?
is the globaldocument-topic hyper-parameter shared by all doc-uments.Given the documents inMi, we introduce an aux-iliary variable ~?Mi:~?iM= T ?1|Mi|?j?Mi~?j, (1)where T controls the concentration parameters.
Thelarger the value of T is, the closer~?iis to the averageof~?j?s.Given the documents in Ci, we introduce anotherauxiliary variable:~?iC= T ?
arg~?imaxminj?CiKL(~?i,~?j), (2)whereKL(~?i,~?j) is the KL-divergence between twodistributions~?iand~?j.
This means we choose a vec-tor that is maximally far away from Ci, in terms ofKL divergence to its nearest neighbor in Ci.In such a way, we force documents sharing must-links to have similar topic distributions while docu-ments sharing cannot-links to have dissimilar topicdistributions.
Note that it also encodes constraint assoft preference rather than hard constraint.
We useCollapsed Gibbs Sampling for LDA inference.
Dur-ing Gibbs Sampling, instead of always drawing~?i1currently in submission.from Dirichlet(~?
), we draw~?ibased on the fol-lowing distribution:~?i?
Dir(?~?+?M~?iM+?C~?iC) = Dir(~?i).
(3)Here, ?g, ?Mand ?Care the weights to control thetrade-off among the three terms.
In our experiment,we choose T = 100, ?g= ?M= ?C= 1.Our evaluation has shown that cLDA is effectivein improving topic model quality.
For example, itachieved a significant topic classification error re-duction on the 20 Newsgroup dataset.
Also, top-ics learned by cLDA are more coherent than thoselearned by standard LDA.2.1 Active Learning with User InteractionIn this subsection, we present an active learningframework to iteratively acquire constraints fromusers.
As shown in Figure 1, given a document col-lection, the framework first runs standard LDA witha burnin component.
Since it uses a Gibbs sampler(Griffiths and Steyvers, 2004) to infer topic samplesfor each word token, it usually takes hundreds of it-erations for the sampler to converge to a stable state.Based on the results of the burnt-in model, the sys-tem generates a target document and a set of anchordocuments for a user to annotate.
Target document isa document on which the active learner solicits userfeedback, and anchor documents are representativesof a topic model?s latent topics.
If a large portion ofthe word tokens in a document belongs to topic i, wesay the document is an anchor document for topic i.A user judges the content of the target and theanchor documents and then informs the systemwhether the target document is similar to any of theanchor documents.
The user interface is designedso that the user can drag the target document nearan anchor document if she considers both to be thesame topic.
Currently, one target document can bemust-linked to only one anchor document.
Sinceit is possbile to have multiple topics in one docu-ment, in the future, we will allow user to add mustlinks between one target and mulitple anchor doc-uments.
After adding one or more must-links, the31system automatically adds cannot-links between thetarget document and the rest anchor documents.Given this input, the system adds them to a con-straint pool.
It then uses cLDA to incorporate theseconstraints and generates an updated topic model.Based on the new topic model, the system chooses anew target document and several new anchor docu-ments for the user to annotate.
This process contin-ues until the user is satisfied with the resulting topicmodel.How to choose the target and anchor documentsare the key questions that we consider in the nextsubsections.2.2 Target Document SelectionA target document is defined as a document onwhich the active learner solicits user feedback.
Wehave investigated several strategies for selecting atarget document.Random: The active learner randomly selects a doc-ument from the corpus.
Although this strategy isthe simplest, it may not be efficient since the modelmay have enough information about the documentalready.MaxEntropy: The entropy of a document d is com-puted as Hd= ?
?Ki=1?dklog ?dk, where K is thenumber of topics, and ?
is model?s document-topicdistribution.
Therefore, the system will select a doc-ument about which it is most confused.
A uniform?
implies that the model has no topic informationabout the document and thus assigns equal probabil-ity to all topics.MinLikelihood: The likelihood of a document d iscomputed as Ld= (?Ni=1?Kk=1?ki?dk)/N , whereN is the number of tokens in d, and ?
is model?stopic-word distribution.
Since the overall likeli-hood of the input documents is the objective func-tion LDA aims to maximize, using this criteria, thesystem will choose a document that is most difficultfor which the current model achieves the lowest ob-jective score.2.3 Anchor Documents SelectionGiven a target document d, the active learner thengenerates one or more anchor documents based onthe target document?s topic distribution ?d.
It filtersout topics with trivial value in ?dand extracts an an-chor topic set Tancwhich only contains topics withnon-trivial value in ?d.
A trivial ?dimeans that themass of ith component in ?dis neglectable, whichindicates that the model rarely assign topic i to doc-ument d. For each topic t in Tanc, the active learnerselects an anchor document who has minimum Eu-clidean distance with an ideal anchor ??t.
In the idealanchor ?
?t, all the components are zero except thevalue of the tthcomponent is 1.
For example, if atarget document d?s ?dis {0.5, 0.3, 0.03, 0.02, 0.15}in a K = 5 topic model, the active learner wouldgenerate Tanc= {0, 1, 4} and for each t in Tanc, ananchor document.However, it is possible that some topics learnedby LDA are only ?background?
topics which havesignificant non-trivial probabilities over many doc-uments (Song et al., 2009).
Since background top-ics are often uninteresting ones, we use a weightedanchor topic selection method to filter them.
Aweighted kthcomponent of ?
?dkfor document d isdefined as follows: ?
?dk= ?dk/?Di=0?ik.
There-fore, instead of keeping the topics with non-trivialvalues, we keep those whose weighted values arenon-trivial.3 EvaluationIn this section, we evaluate our active learningframework.
Topic models are often evaluated us-ing perplexity on held-out test data.
However, re-cent work (Boyd-Graber et al., 2009; Chuang et al.,2013) has shown that human judgment sometimesis contrary to the perplexity measure.
Following(Mimno et al., 2011), we employ Topic Coherence,a metric which was shown to be highly consistentwith human judgment, to measure a topic model?squality.
It relies upon word co-occurrence statisticswithin documents, and does not depend on externalresources or human labeling.We followed (Basu et al., 2004) to create a Mix3sub-dataset from the 20 Newsgroups data2, whichconsists of two newsgroups with similar topics(rec.sport.hockey, rec.sport.baseball) and one witha distinctive topic (sci.space).
We use this datasetto evaluate the effectiveness of the proposed frame-work.3.1 Simulated ExperimentsWe first burn-in LDA for 500 iterations.
Then foreach additional iteration, the active learner generatesone query which consists of one target document andone or more anchor documents.
We simulate userfeedback using the documents?
ground truth labels.If a target document has the same label as one ofthe anchor documents, we add a must-link betweenthem.
We also add cannot-links between the targetdocument and the rest of the anchor documents.
Allthese constraints are added into a constraint pool.We also augment the constraint pool with derivedconstraints.
For example, due to transitivity, if thereis a must-link between (a, b) and (b, c), then we add2Available at http://people.csail.mit.edu/jrennie/20Newsgroups32Topic Words1 writes, like, think, good, know, better, even, people, run, hit2 space, nasa, system, gov, launch, orbit, moon, earth, access, data3 game, play, hockey, season, league, fun, wing, cup, shot, score1 baseball, hit, won, shot, hitter, base, pitching, cub, ball, yankee2 space, nasa, system, gov, launch, obit, moon, earth, mission, shuttle3 hockey, nhl, playoff, star, wing, cup, king, detroit, rangerTable 1: Ten most probable words of each topic before (above) and after active learning (below).a must link between (a, c).
We simulate the processfor 100 iterations to acquire constraints.
After that,we keep cLDA running for 400 more iterations withthe acquired constraints until it converges.Figure 2: Topic coherence with different number ofiterations.Figure 2 shows the topic coherence scores for dif-ferent target document selection strategies.
This re-sult indicates 1).
MaxEntropy has the best topic co-herence score.
2).
All active learning strategies out-perform standard LDA, and the results are statisti-cally significant at p = 0.05.
With standard LDA,500 more iterations without any constraints does notimprove the topic coherence.
However, by activelearning with cLDA for 500 iterations, the topic co-herences are significantly improved.Using MaxEntropy target document selectionmethod, we demonstrate the improvement of themost probable topic keywords before and after ac-tive learning.
Table 1 shows that before active learn-ing, topic 1?s most probable words are incoherentand thus it is difficult to determine the meaning ofthe topic .
After active learning, in contrast, topic 1?smost probable words become more consistent witha ?baseball?
topic.
This example suggests that theactive learning framework that interactively and it-eratively acquires pairwise document constraints iseffective in improving the topic model?s quality.4 ConclusionWe presented a novel active learning framework forLDA that employs constrained topic modeling toactively incorporate user feedback encoded as pair-wise document constraints.
With simulated user in-put, our preliminary results demonstrate the effec-tiveness of the framework on a benchmark dataset.In the future, we will perform a formal user studyin which real users will interact with the system toiteratively refine topic models.AcknowledgmentsThis work was supported in part by DARPA contractD11AP00268.ReferencesSugato Basu, A. Banjeree, ER.
Mooney, Arindam Baner-jee, and Raymond J. Mooney.
2004.
Active semi-supervision for pairwise constrained clustering.
InSDM, pages 333?344.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
Journal of MachineLearning Research, 3:993?1022.Jordan Boyd-Graber, Jonathan Chang, Sean Gerrish,Chong Wang, and David Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In NIPS.Jason Chuang, Sonal Gupta, Christopher D. Manning,and Jeffrey Heer.
2013.
Topic model diagnostics:Assessing domain relevance via topical alignment.
InICML.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101(Suppl.
1):5228?5235.Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.2011.
Interactive topic modeling.
In ACL, pages 248?257.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InEMNLP, pages 262?272.Burr Settles.
2010.
Active learning literature survey.Technical report, University of Wisconsin Madison.Yangqiu Song, Shimei Pan, Shixia Liu, Michelle X.Zhou, and Weihong Qian.
2009.
Topic and keywordre-ranking for lda-based topic modeling.
In CIKM,pages 1757?1760.Hanna M. Wallach, David M. Mimno, and Andrew Mc-Callum.
2009.
Rethinking lda: Why priors matter.
InNIPS, pages 1973?1981.33
