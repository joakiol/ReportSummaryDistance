Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 30?38,Dublin, Ireland, August 23 2014.Unsupervised adaptation of supervised part-of-speech taggersfor closely related languagesYves ScherrerLATL-CUIUniversity of GenevaRoute de Drize 7, 1227 Carouge, Switzerlandyves.scherrer@unige.chAbstractWhen developing NLP tools for low-resource languages, one is often confronted with the lack ofannotated data.
We propose to circumvent this bottleneck by training a supervised HMM taggeron a closely related language for which annotated data are available, and translating the words inthe tagger parameter files into the low-resource language.
The translation dictionaries are createdwith unsupervised lexicon induction techniques that rely only on raw textual data.
We obtain atagging accuracy of up to 89.08% using a Spanish tagger adapted to Catalan, which is 30.66%above the performance of an unadapted Spanish tagger, and 8.88% below the performance ofa supervised tagger trained on annotated Catalan data.
Furthermore, we evaluate our model onseveral Romance, Germanic and Slavic languages and obtain tagging accuracies of up to 92%.1 IntroductionRecently, a lot of research has dealt with the task of creating part-of-speech taggers for languages whichlack manually annotated training corpora.
This is usually done through some type of annotation pro-jection from a language for which a tagger or an annotated corpus exists (henceforth called RL forresourced language) towards another language that lacks such data (NRL for non-resourced language).One possibility is to use word-aligned parallel corpora and transfer the tags from the RL to the NRLalong alignment links.
Another possibility is to adapt the parameters of the RL tagger using bilingualdictionaries or manually built transformation rules.In this paper, we argue that neither parallel corpora nor hand-written resources are required if the RLand the NRL are closely related.
We propose a generic method for tagger adaptation that relies on threeassumptions which generally hold for closely related language varieties.
First, we assume that the twolanguages share a lot of cognates, i.e., word pairs that are formally similar and that are translations ofeach other.
Second, we suppose that the word order of both languages is similar.
Third, we assume thatthe set of POS tags is identical.
Under these assumptions, we can avoid the requirements of parallel dataand of manual annotation.Following Feldman et al.
(2006), the reasoning behind our method is that a Hidden Markov Model(HMM) tagger trained in a supervised way on RL data can be adapted to the NRL by translating the RLwords in its parameter files to the NRL.
This requires a bilingual dictionary between RL words and NRLwords.
In this paper, we create different HMM taggers using the bilingual dictionaries obtained with theunsupervised lexicon induction methods presented in our earlier work (Scherrer and Sagot, 2014).The paper is organized as follows.
In Section 2, we present related work on tagger adaptation andlexicon induction.
In Section 3, we review Hidden Markov Models and their relevance for tagging andfor our method of tagger adaptation.
Section 4 presents a set of different taggers in some detail and eval-uates them on Catalan, using Spanish as RL.
In Section 5, we demonstrate the validity of the proposedapproach by performing small-scale evaluations on a number of Romance, Germanic and Slavic lan-guages: we transfer part-of-speech tags from Spanish to Aragonese, from Czech to Slovak and Sorbian,from Standard German to Dutch and Palatine German.
We conclude in Section 6.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/302 Related workThe task of creating part-of-speech taggers (and other NLP tools) for new languages without resorting tomanually annotated corpora has inspired a lot of recent research.
The most popular line of work, initiatedby Yarowsky et al.
(2001), draws on parallel corpora.
They tag the source side of a parallel corpus withan existing tagger, and then project the tags along the word alignment links onto the target side of theparallel corpus.
A new tagger is then trained on the target side, using aggressive smoothing to reduce thenoise caused by alignment errors.In a similar setting, Das and Petrov (2011) use a more sophisticated graph-based projection algorithmwith label propagation to obtain high-precision tags for the target words.
Follow-up work by Li etal.
(2012) uses tag dictionaries extracted from Wiktionary instead of parallel corpora, and T?ckstr?met al.
(2013) attempt to combine these two data sources: the Wiktionary data provides constraints onword types, whereas the parallel data is used to filter these constraints on the token level, depending onthe context of a given word occurrence.
Duong et al.
(2013) show that the original approach of Dasand Petrov (2011) can be simplified by focusing on high-confidence alignment links, thus achievingequivalent performance without resorting to graph-based projection.
The research based on parallelcorpora does not assume any particular etymological relationship between the two languages, but Duonget al.
(2013) note that their approach works best when the source and target languages are closely related.Other approaches explicity model the case of two closely related languages, such as Feldman et al.(2006).
They train a tagger on the source language with standard tools and resources, and then adapt theparameter files of that tagger to the target language using a hand-written morphological analyzer and alist of cognate word pairs.
Bernhard and Ligozat (2013) use a similar approach to adapt a German taggerto Alsatian; they show that manually annotating a small list of closed-class words leads to considerablegains in tagging accuracy.
In a slightly different setting, Garrette and Baldridge (2013) show that taggersfor low-resource languages can be built from scratch with only two hours of manual annotation work.Even though recent work on closely related and low-resource languages presupposes manually an-notated data to some extent, we believe that it is possible to create a tagger for such languages fullyautomatically.
We adopt the general model proposed by Feldman et al.
(2006), but use automaticallyinduced bilingual dictionaries to translate the source language words in the tagger parameter files.
Thebilingual dictionaries are obtained with our unsupervised lexicon induction pipeline (Scherrer and Sagot,2013; Scherrer and Sagot, 2014).
This pipeline is inspired by early work by Koehn and Knight (2002),who propose various methods for inferring translation lexicons using monolingual data.Our lexicon induction pipeline is composed of three main steps.
First, a list of formally similar wordpairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr,2004).
Second, regularities occurring in these word pairs are learned by training and applying a character-level statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009).
Third, cross-lingual contextual similarity measures are used to induce additional word pairs.
The main idea is toextract word n-grams from comparable corpora of both languages and induce word pairs that co-occurin the context of already known word pairs (Fung, 1998; Rapp, 1999; Fi?er and Ljube?i?c, 2011).
In ourpipeline, the already known word pairs are those induced with CSMT.In this paper, we extend our previous work (Scherrer and Sagot, 2014) in two aspects.
First, we usea more powerful HMM tagging model instead of the simple unigram tagger that insufficiently accountsfor the ambiguity in language.
Second, we assess the impact of each lexicon induction step separatelyrather than merely evaluating the final result of the pipeline.3 HMM taggingHidden Markov Models (HMMs) are a simple yet powerful formal device frequently used for part-of-speech tagging.
A HMM describes a process that generates a joint sequence of tags and words bydecomposing the problem into so-called transitions and emissions.
Transitions represent the probabilitiesof a tag given the preceding tag(s), and emissions represent the probabilities of a word given the tagassigned to it (Jurafsky and Martin, 2009).31The main advantage of HMM taggers for our work lies in the independence assumption betweentransitions and emissions: crucially, the emission probability of a word only depends on its tag; it doesnot depend on previous words or on previous tags.
Assuming, as stated in the introduction, that theword order is similar and the tag sets identical between the RL and the NRL, we argue that the transitionprobabilities estimated on RL data are also valid for NRL.
Only the emission probabilities have to beadapted since RL words are formally different from NRL words.Following earlier work (Feldman et al., 2006; Duong et al., 2013), we use the TnT tagger (Brants,2000), an implementation of a trigram HMM tagger that includes smoothing and handling of unknownwords.
In contrast to other implementations that use inaccessible binary files, TnT stores the estimatedparameters in easily modifiable plain text files.3.1 Adapting emission countsThe goal of this work is to adapt an existing RL HMM tagger for a closely related NRL by replacing theRL words in the emission parameters by the corresponding NRL words.
Let us explain this process withan example, using Spanish as RL and Catalan as NRL.The TnT tagger creates an emission parameter file that contains, for each word, the tags and theirfrequencies observed in the training corpus.
For example, a tagger trained on Spanish data may containthe following lines (word on the left, tag in the middle, frequency on the right):(1)intelectual AQ 11intelectual NC 3intelectuales AQ 3intelectuales NC 7Furthermore, suppose that we have a dictionary that associates Catalan words (left) with Spanish words(center), where the weight (right) indicates the ambiguity level of the Catalan word, which is simplydefined as the inverse of the number of its Spanish translations:(2)intel?lectual intelectual 0.5intel?lectual intelectuales 0.5intel?lectuals intelectuales 1A new Catalan emission file is then created by taking, for each Catalan word, the union of the tags ofits Spanish translations and by multiplying the tag weights with the dictionary weights.
This yields thefollowing entries:(3)intel?lectual AQ (0.5 ?11)+(0.5 ?3) = 7intel?lectual NC (0.5 ?3)+(0.5 ?7) = 5intel?lectuals AQ 1 ?3 = 3intel?lectuals NC 1 ?7 = 7Or more formally: for each dictionary triple ?wRL,wNRL, fd?
and each emission triple ?wRL, t, fe?
withmatching wRL, add the new emission triple ?wNRL, t, fd?
fe?.
Merge emission triples with identical wNRLand t and sum their weights.Finally, RL words occurring in the emission file that have not been translated to NRL (because noappropriate word pair existed in the dictionary) are copied without modification to the new emission file.In particular, this allows us to cover punctuation signs and numbers as well as named entities (which aremostly spelled identically in both languages).4 Tagger adaptation for CatalanIn this section, we present seven taggers for Catalan.
Three of them (Sections 4.2 to 4.4) are supervisedtaggers and serve as baseline taggers and as upper bounds.
The four remaining taggers (Sections 4.6 to4.9) are taggers created by adaptation from a Spanish tagger, using the method presented in Section 3.1;32they differ in the lexicons used to translate the emission counts.
These four taggers represent the maincontribution of this paper.
We start by listing the data used in our experiments.4.1 DataMost taggers presented below are initially trained on a part-of-speech annotated corpus of Spanish.
Weuse the Spanish part of the AnCora treebank (Taul?
et al., 2008), which contains about 500 000 words.The AnCora morphosyntactic annotation includes the main category (e.g.
noun), the subcategory(e.g.
proper noun), and several morphological categories (e.g., gender, number, person, tense, mode),yielding about 280 distinct labels.
Since we are mainly interested in part-of-speech information, wesimplified these labels by taking into account the two first characters of each label, corresponding tothe main category and the subcategory.
This simplified tagset contains 42 distinct labels, which is stillconsiderably more than the 12 tags of Petrov et al.
(2012) commonly used in comparable settings.All taggers need to be evaluated on a Catalan gold standard that shares the same tagset as Spanish.
Forthis purpose, we use the Catalan part of AnCora, which also contains about 500 000 words.
We simplifiedthe tags in the same way as above.
The Catalan part of AnCora is also used to train the supervised modelspresented in Sections 4.3 and 4.4.Finally, the lexicon induction algorithms require data on their own, which we present here for com-pleteness.
As in Scherrer and Sagot (2013), we use Wikipedia dumps consisting of 140M words forCatalan and 430M words for Spanish.14.2 Baseline: a Spanish taggerSince Spanish and Catalan are closely related languages, one could presume that a lot of words areidentical, and that a tagger trained on Spanish data would yield acceptable performance on Catalan testdata without modifications.
In order to test this hypothesis, we trained a TnT tagger on Spanish AnCoraand tested it on Catalan AnCora.
We obtained a tagging accuracy of 58.42% only, which suggests thatthis approach is clearly insufficient.
(The results of all experiments are summed up in Table 1.)
Forcomparison, Feldman et al.
(2006) obtain 64.5% accuracy on the same languages with a smaller trainingcorpus (100k instead of 500k words), but also with a smaller tagset (14 instead of 42).We view this model as a baseline that we expect to beat with the adaptation methods.4.3 Upper bound 1: a supervised Catalan taggerThe upper bound of the Catalan tagging experiments is represented by a tagger created under ideal dataconditions: a tagger trained in a supervised way on an annotated Catalan corpus.
We train a TnT taggeron Catalan AnCora and test it on the same corpus, using 10-fold cross-validation to avoid having thesame sentences in the training and the test set.
This yields an averaged accuracy value of 97.96%.For comparison, Feldman et al.
(2006) obtain 97.5% accuracy on their dataset.
More recently, Petrovet al.
(2012) report an accuracy of 98.5% by training on the CESS-ECE corpus, but do not mention thetagging algorithm used.
In any case, our result obtained with TnT can be considered close to state-of-the-art performance on Catalan.4.4 Upper bound 2: a tagger with Spanish transition counts and Catalan emission countsWe introduce a second upper bound that shares the assumption of structural similarity underlying theadaptation-based models.
Concretely, we combine the transition probabilities from the baseline Spanishtagger (Section 4.2) with the emission probabilities of the supervised Catalan tagger (Section 4.3).
Theresulting tagger is evaluated again on Catalan AnCora using 10-fold cross-validation.
We get an accuracyvalue of 97.66%, or just 0.3% absolute below the supervised tagger of Section 4.3.2This suggests thatthe transition probabilities are indeed very similar between the two languages, and that they can safelybe kept constant in the adaptation-based models presented below.1This is not exactly a realistic setting for the intended use for low-resource languages.
However, Section 5 will illustrate theperformance of the proposed models on smaller data sets.
Note also that the lexicon induction methods do not require the twocorpora to be of similar size.2This difference is significant: ?2(1;N = 1064002) = 109.9747799; p < 0.01.33Cognate pair extractionusing BI-SIM scoreCognate pair extractionusing CSMT modelWord pair extractionusing contextual similarityTagger 1 (4.6)Tagger 2 (4.7)Tagger 3 (4.8) /S&S unigram taggerFigure 1: Flowchart of the lexicon induction pipeline and of the resulting taggers.4.5 Lexicon induction methods for adaptation-based taggersThe adaptation-based taggers presented in Sections 4.6 to 4.8 differ in the bilingual dictionaries used toadapt the emission counts.
These dictionaries have been created using the pipeline of Scherrer and Sagot(2014), which we summarize in this section (see Figure 1).The pipeline starts with a cognate pair extraction step that uses the BI-SIM score to identify likelycognate pairs.
The result of this step is used as training data for the second step, in which a CSMT modelis trained to identify likely cognate pairs even more reliably.
The result of the second step is in turn usedas seed data for the third step, in which additional word pairs are extracted on the basis of contextualsimilarity.
Scherrer and Sagot (2014) create a single unigram tagger (abreviated S&S in Figure 1) withthe union of the word pairs obtained in the second and third steps (plus additional clues like word identityand suffix analysis, which are not required here).The three steps are evaluated separately: Tagger 1 relies on the lexicon induced in the first step; Tagger2 relies on the lexicon induced in the second step; Tagger 3 relies on the union of the lexicons inducedin the second and third steps.4.6 Tagger 1: cognate pairs induced with BI-SIM scoreAs first step of the lexicon induction pipeline, word lists are extracted from both Wikipedia corpora,and short words (words with less than 5 characters) as well as rare words (words accounting for thelowest 10% of the frequency distribution) are removed.
Then, the BI-SIM score is computed betweeneach Catalan word wcaand each Spanish word wes.
For each wca, we keep the ?wca,wes?
pair(s) thatmaximize(s) the BI-SIM value, provided it is above the empirically chosen threshold of 0.8.
When a wcais associated with several wes, we keep all of them.
This creates a list of cognate pairs, albeit a rathernoisy one since it does not take into account regular correspondences between languages, but merelycounts letter bigram differences.Tagger 1, the first adaptation-based tagger, is created by replacing the Spanish emission counts withtheir Catalan equivalents using the list of cognate pairs.
Tagger 1 yields an accuracy of 68.32%, which isa full 10% higher than the baseline.
This improvement is surprisingly high, as the cognate list is not onlynoisy, but also incomplete: only 17.91% of the words in the emission file could be translated with it.4.7 Tagger 2: cognate pairs induced with CSMTIn this model, the Spanish emission counts are replaced using the list of cognate pairs obtained in thesecond step of the lexicon induction pipeline.We train a CSMT system on the list of potential cognate pairs of the first step.
We then apply thissystem to translate each Catalan word again into Spanish.
We assume that the CSMT system learnsuseful generalizations about the relationship between Catalan and Spanish words, which the generic BI-SIM measure was not able to make.
Moreover, the CSMT system is able to translate Catalan words even34Baseline Tagger 1 Tagger 2 Tagger 3 Tagger 4Upper Upperbound 2 bound 1Tagging accuracy 58.42% 68.32% 72.32% 88.72% 89.08% 97.66% 97.96%Translated words 17.91% 64.03% 65.62%Table 1: Results of the Catalan tagging experiments.
The first line reports tagging accuracies of thedifferent taggers.
The second line shows ?
where applicable ?
how many words of the emission filescould be translated.if their Spanish translations have not been seen, on the basis of the character correspondences observedin other words.This new dictionary allowed us to translate 64.03% of the words in the emission file.
In consequence,the resulting tagger shows improved performance compared with Tagger 1: its accuracy lies at 72.32%,suggesting that the CSMT system yields a dictionary that is at the same time more precise and morecomplete than the one obtained with BI-SIM in the previous step.4.8 Tagger 3: word pairs induced with CSMT and context similarityIn previous work (Scherrer and Sagot, 2014), we have argued that lexicon induction methods based onformal similarity alone are not sufficient, for the following reasons: (1) even in closely related languages,not all word pairs are cognates; (2) high-frequency words are often related through irregular phoneticcorrespondences; (3) pairs of short words may just be too hard to predict on the basis of formal criteriaalone; (4) formal similarity methods are prone to inducing false friends, i.e., words that are formallysimilar but are not translations of each other.
For these types of words, we have proposed a differentapproach that relies on contextual similarity.We extract 3-gram and 4-gram contexts from both languages and form context pairs whenever the firstand the last word pairs figure in the dictionary obtained with CSMT, allowing the word pair(s) in thecenter to be newly inferred.
Several filters are added in order to remove noise.In order to create Tagger 3, we merge the dictionary induced with CSMT and the dictionary inducedwith context similarity, giving preference to the latter.
Again, the emission parameters of the baselineSpanish tagger are adapted using this dictionary.
65.62% of the words in the emission file could betranslated, i.e.
only 1.59% more than for Tagger 2.
Nevertheless, the accuracy of Tagger 3 (88.72%) liesabout 18% absolute above Tagger 2.
This large gain in accuracy is due to the fact that context similaritymostly adds high-frequency words, which are few but crucial to obtain satisfactory tagging performance.One goal of these experiments was to show whether the improved handling of ambiguity provided byHMMs in comparison with the unigram model used by Scherrer and Sagot (2013) is reflected in betteroverall tagging performance.
This goal has been reached: the unigram model of Scherrer and Sagot(2013) shows a tagging accuracy of 85.1%, which is 3% absolute below Tagger 3, the most directlycomparable HMM-based tagger.34.9 Tagger 4: re-estimate transition probabilitiesIn this last model, we challenge the initial assumption that the Spanish transition probabilities are ?goodenough?
for tagging Catalan.
Concretely, we use Tagger 3 to tag the entire Catalan Wikipedia corpus(the one also used for the lexicon induction tasks) and then train Tagger 4 in a supervised way on thisdata.
The idea behind this additional step is that the transition (and emission) counts estimated on thelarge Catalan corpus are more reliable than those obtained by direct tagger adaptation.Tagger 4 yields an accuracy value of 89.08%, outperforming Tagger 3 by only 0.36%.4This differenceis consistent with the one observed between Upper Bound 1 and Upper Bound 2, suggesting once more3The Catalan results reported in Scherrer and Sagot (2014) are based on a different test set, which is why we rather refer tothe directly comparable Scherrer and Sagot (2013) results in this section.4This difference is significant: ?2(1;N = 1064002) = 35.84835013; p < 0.01.35that transition counts only marginally influence the tagging performance if the former are estimated on alanguage that is structurally similar.5 Multilingual experimentsIn addition to the Spanish?Catalan experiment, we have induced taggers for several closely related lan-guages from Romance, Germanic and Slavic language families and tested them on the multilingual dataset used by Scherrer and Sagot (2014).
Although the results of these additional experiments are lessreliable than the Spanish?Catalan data due to the small test corpus sizes, they allow us to generalize ourfindings to other languages and language families.
The experiments are set up as follows:?
The Aragonese taggers were adapted from a Spanish tagger trained on AnCora.
They are testedon a Wikipedia excerpt of 100 sentences that was manually annotated with the simplified AnCoralabels of Section 4.1.
The Wikipedia corpora used for lexicon induction contained 5.4M words forAragonese, and 431M words for Spanish.?
The Dutch and Palatine German taggers were adapted from a Standard German tagger trainedon the TIGER treebank (900 000 tokens; 55 tags; Brants et al.
(2002)).
The gold standard cor-pora are Wikipedia excerpts of 100 sentences each, manually annotated with TIGER labels.
TheWikipedia corpora used for lexicon induction contained 0.5M words for Dutch, 0.3M words forPalatine German, and 612M words for Standard German.?
The Upper Sorbian, Slovak and Polish taggers were adapted from a Czech Tagger trained on thePrague Dependency Treebank 2.5 (2M tokens; 57 simplified tags).5The gold standard corporaare Wikipedia excerpts of 30 sentences each, manually annotated with simplified PDT labels.
TheWikipedia corpora used for lexicon induction contained 0.9M words for Upper Sorbian, 30M wordsfor Slovak, 206M words for Polish, and 85M words for Czech.The tagging accuracies are reported in the left part of Table 2.
The accuracy values vary widely acrosslanguages, with baseline performances ranging from 24% to 81%.
This variation essentially reflects thelinguistic distance between the RL and the NRL: German and Dutch seem to be particularly distant,while Czech and Slovak are particularly closely related.
In contrast, the overall tendency of the taggingmodels is the same for all languages: there are consistent gradual improvements from the baseline taggerto Tagger 3.
These findings are in line with the Catalan experiments.
The differences between Tagger 3and Tagger 4 are not significant for any language, whereas the Catalan experiment showed a slight butsignificant improvement.
Finally, Taggers 3 and 4 slightly outperform the unigram tagger of Scherrerand Sagot (2014) (S&S in Table 2) on most languages, although the difference is less marked than forCatalan.The right half of Table 2 shows what percentage of the emission files could be translated at each step,analogously to the figures reported for Catalan in Table 1.
The variation observed here mainly dependson the language proximity and on the size of the corpora used for lexicon induction.Globally, the Germanic languages obtain the lowest accuracy scores.
This is due to a combinationof factors.
First, as stated above, the baseline performance is already lower than in the other languagefamilies, which essentially results from a lower number of identical NRL?RL word pairs than in otherlanguage families.
Second, the lexicon induction corpora are much smaller than for the other languagefamilies.6Third, Germanic languages tend to have longer words due to compounding, so that the BI-SIM threshold is more difficult to satisfy.
The combination of the second and third factors lead to poorperformance of the first lexicon induction step: less than 4% of the German words could be translated5Similarly to AnCora, the morphosyntactic labels of the PDT consist of 15 positions that encode the main morphosyntacticcategory, the subcategory as well as various morphological categories.
We simplify the tagset analogously to AnCora, keepingonly the main category and the subcategory, which leads to 57 distinct labels.The PDT is available at http://ufal.mff.cuni.cz/pdt2.5/.6As in our earlier work, we used all of the Palatine German Wikipedia, whereas we reduced the Dutch Wikipedia corpus onpurpose to better simulate the low-resource scenario.36Language Tagging accuracy Translated wordsBaseline T1 T2 T3 T4 S&S T1 T2 T3Aragonese 72% 74% 74% 87% 87% 85% 16.11% 42.65% 43.23%Dutch 24% 30% 39% 60% 62% 59% 3.69% 6.73% 6.79%Palatine German 50% 54% 57% 70% 70% 65% 3.86% 5.52% 5.58%Upper Sorbian 70% 72% 77% 84% 84% 84% 5.70% 11.60% 11.69%Slovak 81% 85% 88% 93% 93% 92% 29.39% 52.40% 54.41%Polish 66% 69% 72% 78% 79% 78% 8.50% 42.27% 42.73%Table 2: Results of the multilingual tagging experiments.
The left half of the table reports taggingaccuracies and compares them with the results reported by Scherrer and Sagot (2014) (S&S column).The right half of the table shows how many words of the emission files could be translated.when building Tagger 1.
This obviously reduces the potential for accuracy gains in Tagger 1, but it alsohampers the training of the CSMT system at the origin of Tagger 2.
However, one should note that goodtagging results can be achieved even with relatively low translation coverage, as shown by the UpperSorbian experiment.6 ConclusionOne goal of the experiments presented here was to validate the pipeline proposed earlier in Scherrerand Sagot (2014).
By showing that there are gradual improvements from the baseline tagger to Tagger3 on a large number of languages, we demonstrate that the overall approach of inducing word pairs insubsequent steps is sound, and that the order of these steps is reasonably chosen.
Furthermore, we findthat re-estimating the tagger parameters on a large monolingual corpus (Tagger 4) does not improve itsperformance substantially, as we have predicted in Section 4.4 on the basis of supervised Catalan taggers.A second goal of these experiments was to show that the HMM taggers offer improved handling ofambiguity compared with the unigram tagger of Scherrer and Sagot (2014).
We have indeed noted anaccuracy gain of 3% on the Catalan data, and the multilingual data set shows similar (yet less marked)tendencies.However, the Catalan experiments show that there still is a gap of about 10% absolute accuracy be-tween the adaptation taggers and fully supervised taggers.
We see two main reasons for this gap.
First,the completely unsupervised lexicon induction algorithms obviously produce a number of erroneousword pairs, which may then result in erroneous tagging.
Second, the lexicon induction algorithms cur-rently do not allow a given NRL word to relate to two different RL words.
As a result, the taggers arenot able to model tagging ambiguities arising from translation ambiguities.
Better ambiguity handling,for instance on the basis of token-level constraints as suggested by T?ckstr?m et al.
(2013), could thusfurther improve tagging accuracy.Finally, discriminative models using Maximum Entropy or Perceptron training have largely supersededHMMs for part-of-speech tagging in the last few years.7Such models take into account a larger set offeatures such as word suffixes, word structure (presence of punctuation signs, numerals, etc.)
and externallexicon information.
Further research will be needed to investigate how our adaptation methods can beapplied to feature-based tagging models.AcknowledgementsThe author would like to thank Beno?t Sagot for his collaboration on earlier versions of this work.
Thiswork was partially funded by the Labex EFL (ANR/CGI), Strand 6, operation LR2.2.7For an overview on recent English taggers, see for example http://aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art).37ReferencesDelphine Bernhard and Anne-Laure Ligozat.
2013.
Hassle-free POS-tagging for the Alsatian dialects.
In MarcosZampieri and Sascha Diwersy, editors, Non-Standard Data Sources in Corpus Based-Research, volume 5 ofZSM Studien, pages 85?92.
Shaker.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith.
2002.
The TIGER Treebank.In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT 2002), pages 24?41.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of ANLP 2000, pages 224?231.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In Proceedings of ACL-HLT 2011, pages 600?609.Long Duong, Paul Cook, Steven Bird, and Pavel Pecina.
2013.
Simpler unsupervised POS tagging with bilingualprojections.
In Proceedings of ACL 2013, pages 634?639.Anna Feldman, Jirka Hana, and Chris Brew.
2006.
A cross-language approach to rapid creation of new morpho-syntactically annotated resources.
In Proceedings of LREC 2006, pages 549?554.Darja Fi?er and Nikola Ljube?i?c.
2011.
Bilingual lexicon extraction from comparable corpora for closely relatedlanguages.
In Proceedings of RANLP 2011, pages 125?131.Pascale Fung.
1998.
A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallelcorpora.
Machine Translation and the Information Soup, pages 1?17.Dan Garrette and Jason Baldridge.
2013.
Learning a part-of-speech tagger from two hours of annotation.
InProceedings of NAACL-HLT 2013, pages 138?147.Daniel Jurafsky and James H. Martin.
2009.
Speech and language processing.
Pearson, 2nd edition.Philipp Koehn and Kevin Knight.
2002.
Learning a translation lexicon from monolingual corpora.
In Proceedingsof the ACL 2002 Workshop on Unsupervised Lexical Acquisition (SIGLEX 2002), pages 9?16.Grzegorz Kondrak and Bonnie Dorr.
2004.
Identification of confusable drug names: A new approach and evalua-tion methodology.
In Proceedings of COLING 2004, pages 952?958.Shen Li, Jo?o Gra?a, and Ben Taskar.
2012.
Wiki-ly supervised part-of-speech tagging.
In Proceedings ofEMNLP-CoNLL 2012, pages 1389?1398.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.
A universal part-of-speech tagset.
In Proceedings ofLREC 2012, pages 2089?2096.Reinhard Rapp.
1999.
Automatic identification of word translations from unrelated English and German corpora.In Proceedings of ACL 1999, pages 519?526.Yves Scherrer and Beno?t Sagot.
2013.
Lexicon induction and part-of-speech tagging of non-resourced languageswithout any bilingual resources.
In Proceedings of the RANLP 2013 Workshop on Adaptation of languageresources and tools for closely related languages and language variants.Yves Scherrer and Beno?t Sagot.
2014.
A language-independent and fully unsupervised approach to lexiconinduction and part-of-speech tagging for closely related languages.
In Proceedings of LREC 2014, pages 502?508.Oscar T?ckstr?m, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre.
2013.
Token and type con-straints for cross-lingual part-of-speech tagging.
Transactions of the Association for Computational Linguistics,1:1?12.Mariona Taul?, M. Ant?nia Mart?, and Marta Recasens.
2008.
Ancora: Multilevel annotated corpora for Catalanand Spanish.
In Proceedings of LREC 2008, pages 96?101.J?rg Tiedemann.
2009.
Character-based PSMT for closely related languages.
In Proceedings of EAMT 2009,pages 12?19.David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007.
Can we translate letters?
In Proceedings of WMT 2007,pages 33?39.David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001.
Inducing multilingual text analysis tools via robustprojection across aligned corpora.
In Proceedings of HLT 2001.38
