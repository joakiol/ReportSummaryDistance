An Efficient Chart-based Algorithmfor Partial-Parsing of Unrestricted TextsDavid D. McDonald14 Brantwood Road, Arlington MA 02174-8004 USAMCDONALD @ BRANDEIS.EDU (617) 646-4124Abstract In the rest of this section we will look at some of theWe present an efficient algorithm for chart-basedphrase structure parsing of natural anguage that is tai-lored to the problem of extracting specific informationfrom unrestricted texts where many of the words are un-known and much of the text is irrelevant to the task.The parser gains algorithmic efficiency through areduc-tion of its search space.
As each new edge is added tothe chart, the algorithm checks only the topmost of theedges adjacent to it, rather than all such edges as in con-ventional treatments.
The resulting spanning edges areinsured to be the correct ones by carefully controllingthe order in which edges are introduced so that everyfinal constituent covers the longest possible span.
Thisis facilitated through the use of phrase boundary heuris-tics based on the placement of function words, and byheuristic rules that permit certain kinds of phrases to bededuced espite the presence of unknown words.
A fur-ther reduction in the search space is achieved by usingsemantic rather than syntactic ategories on the terminaland nonterminal edges, thereby reducing the amount ofambiguity and thus the number of edges, since onlyedges with a valid semantic interpretation are ever intro-daced.1.
IntroductionMuch of the research being done in parsing today isdirected towards the problem of information extraction,sometimes referred to as "message processing" or"populating data-bases".
The goal of this work is to developsystems that can robustly extract information from massivecorpora of unrestricted ("open") texts.
We have developedsuch a system and applied it to the task of extracting infor-mation about changes in employment found in articles fromthe "Who's News" column of the Wall Street Journal.
Wecall the system "Sparser".
There are many possible designgoals for a parser.
For Sparser we have three:?
It must handle unrestricted texts, to be taken directlyfrom online news services without human interven-tion or preprocessing.?
It must operate fficiently and robustly, and be able tohandle articles of arbitrary size using a fixed, rela-tively small set of resources.?
Its purpose is the identification and extraction specifi-cally targeted, literal information i  order to populatea database.
Linguistically motivated structuraldescriptions of the text are a means to that end, not anend in themselves.consequences of these goals for the parser's design.
Themost important of these is how to cope with the fact thatmany of the words in the text will be unknown, which wewill take up first.
We then look at the consequences ofdesigning the parser for the specific purpose of informationextraction.
In section two we will look at how well Sparserhas been able to meet these goals after roughly fifteenmonths of development following more than two years ofexperimentation with other designs, and then go on in thelater sections to situate and describe its phrase structure algo-rithm.1.1 "Partial Parsing"Attempting to understand an entire text, or even to giveall of it a structural description, is well beyond the state ofthe art for today's parser's on unrestricted texts.
Instead, textextraction systems are typically designed to recover only asingle kind of information.
They focus their analyses ononly those portions of the text where this information oc-curs, skipping over the other portions or giving them only aminimal analysis.
Following an emerging convention, wewill call such a system apartialparser.The competence of a partial parser may be compared withthat of people who are learning a second language.
They canbe expected to know most of the language's syntax, functionwords, and morphology, to know certain idioms, the con-ventions for forming common constructions such as names,dates, times, or amounts of money, and the high frequencyopen-class vocabulary such as "said", "make", "new", etc.However, their knowledge of the vocabulary and phrasingsfor specific subjects will be severely limited and particular.A few topics will be understood quite well, others not at all.Their limited comprehension notwithstanding, such peo-ple can nevertheless can entire articles, accurately pickingout and analyzing those parts that are about topics withintheir competence, while ignoring the rest except perhaps forisolated names and phrasal fragments.
They will knowwhen they have correctly understood a portion of the textthat fell within their competence, and can gauge how thor-ough or reliable their understanding of these segments i .1.2 The impact of unknown wordsMirroring this kind of partial but precise competence in aparser is not ~imply a matter of finding the portions of thetext on the m,derstood topics and then proceeding to analyzethem as a ncrmal system would.
Such a strategy will notwork because instances of unknown words and subject mat-ter can occur at any granularity--not just paragraphs andsentences, but appositives, clausal adjuncts, adverbials, allthe way down to adjectives and compound nouns within193otherwise understandable NPs.
For example, an understand-able subject-verb combination may be separated by anappositive outside the system's competence, understandablepp-adjuncts separated by incomprehensible ones, and so on.The example below (from the Wall Street Journal forFebruary 14, 1991) shows a case of a relative clause, off thetopic of employment change, situated between an understoodsubject NP and an understood VP.
(Understood segmentsshown in bold.)...
Robert A.  Beck,  a 65-year-old formerPrudential chairman who originally bought thebrokerage firm, was named chief executive ofPrudential Bache ... .As a result of this and other factors, the design of a par-tial parser must be adapted to a new set of expectations quitedifferent from the customary experience working with care-fully chosen example sentences or even with most question-answering systems.
In particular:(1) Don't expect o complete full sentencesBecause the unknown vocabulary can occur at any point,one cannot assume that the parser will be able to reliablyrecover sentence boundaries, and its grammar should not de-pend on that ability.To this end, Sparser parses opportunistically and bottomup rather than predicting that an S will be completed.
Itsstructural descriptions are typically a "forest" of minimalphrasal trees interspersed with unknown words.
The only re-liable boundaries are those signalled orthographically, suchas paragraphs.
(2) Develop new kinds of algorithms for connectingconstituents separated by unparsed segments of thetext.The standard phrase structure algorithms are based on thecompletion of rewrite rules that are driven by the adjacencyof labeled constituents.
When an off-topic and therefore un-completed text segment intervenes, as in the example justabove, an adjacency-based mechanism will not work, andsome other mechanism will have to be employed.Sparser includes a semantically-driven search mechanismthat scans the forest for compatible phrases whenever a syn-tactically incomplete or unattached phrase is left after con-ventional rules have been applied.
It is sensitive to the kindof grammatical relation that would have to hold between thetwo constituents, e.g.
subject - predicate, and constrains thesearch to be sensitive to the features of the partially parsedtext between them, e.g.
that if in its search it finds evidenceof a tensed verb that is not contained inside a relative clause,then it should abort he search.
(3) Supplement the phrase structure rule backbone ofthe parser with an independent means of identifyingphrase boundaries.Very often, the off-topic, unknown vocabulary is encap-sulated within quite understandable phrasal contexts.Consider the real example "... this gold mining companywas ...".
Here the initial determiner establishes the begin-ning of a noun phrase, and the tensed auxiliary verb estab-lishes that whatever phrase preceded it has finished (barringadverbs).
Forming an NP over the entire phrase is appropri-ate, even when the words "gold" and "mining" are unknownbecause they are part of an open-ended vocabulary.Sparser includes a set of function word-driven phraseboundary rules.
And it has a very successful heuristic forforming and categorizing text segments such as this example("successful" in that it generated no false positives in thetest described in ?2).
Simply stated, if there is a rule in thegrammar that combines the f'n'st and last edges in a boundedsegment (e.g.
a rule that would succeed on the phrase "thiscompany"), then allow that rule to complete, covering theunknown words as well as the known.1.3 Objects rather than expressionsSparser was written to support asks based on populatingdata bases with commercially significant literal informationextracted in real time from online news services, and thisrequirement has permeated nearly every aspect of its design.In particular, it has meant hat it is not adequate to have theoutput of an analysis be just a syntactic structural descrip-tion (a parse tree), or even a logical form or its renderinginto a database access language like SQL, as is done inmany question-answering systems.
Instead, the output mustbe tuples relating individual companies, people, titles, etc.,most of which will have never been seen by the systembefore and which are not available in pre-stored tables.These requirements led to the following features of Sparser'sdesign:?
The system includes a domain model, wherein classesand individuals are represented asunique, first-classobjects and indexed by keys and tables like theywould be in a database, rather than stored as descrip-tive expressions.?
While many individuals will have never been seenbefore, a very significant number will continuallyreoccur: specific titles, months, dates, numbers, etc.,and they should be referenced directly.
The systemincludes facilities for defining rules for the parser as aside-effect of defining object classes or individuals inthe domain.?
Interpretation is done as part of the parsing of linguis-tic form, rather than as a follow-on process as iscustomary.
This is greatly facilitated by the nextpoint:?
semantic ategories are used as the terms in the phrasestructure rules.
1Space limitations do not permit properly describing th~tie-in from the parsing of structural form (the realm of thqparser proper) to the domain model/database.
Briefly, a ruleby-rule correspondence is supported between the syntax am1 This is sometimes referred to as using a "semantic grammar"This nomenclature can be misleading, as the form and use of thqphrase structure grammar is just the same as in a conventionalsyntactically labeled grammar, i.e.
phrases are formed on th~basis of the adjacency of labeled constituents or terminals.
AIthat changes is that most of the terms in rules are now labellike "company" or "year", rather than "NP" or "verb".194the semantics, 2 whereby each rewrite rule is given a corre-sponding interpretation i  the model.
Individuals and typesin the model are structured so that their compositionalitymimics the compositional structure of the correspondingEnglish phrase(s).The correspondence is grounded in the means by whichindividual content words are defined for the parser.
Briefly,the point is that whenever one defines a class of objects orparticular individuals o that they can be represented in one'sdomain model, that same act of definition can be made toresult in a rule(s) being written for the parser so that when-ever that rule completes, the resulting edge can immediatelyinclude a pointer to the domain object.
Compound objectssuch as events are formed through the composition of thesebasic individuals, under the control of the interpretation rulesthat accompany the rules that dictate the composition of thephrases in the English text.
(define-tit le-head "president")(define-tit le-modif ier "assistant")(define-month :name "December":abbreviation "Dec":posit ion-in-the-year 12:number-of-days 31 )2.
Test ResultsWe put SPARSER and its First large grammar through asubstantial test at the end May 1991.
The task was toextract information on people changing jobs.
The articleswere from the Wall Street Journal, as downloaded off theDow Jones News service; the example below is a faithfulreproduction of what one of those articles looks like as thenews service provides them.The test consisted of 203 articles; literally the second halfof all articles that the Journal published in February 1991whose electronic version had the tag "WNEWS".
Theyincluded long columns on advertising and law that men-tioned a job change incidentally, and some feature articles.About two thirds were from the Journal's "Who's News"column, where the article below is a typical example.
It isthe first article from the test set.AN 910214-0090IlL Who's News: Goodyear Tire & Rubber Co.DD 02/14/91SO WALL STREET JOURNAL (J), PAGE B8CO * GTWNEWSIN PIPELINE OPERATORS (PIP) PETROLEUM(PET) AUTO PARTS AND EQUIPMENT INCLUDINGNote that this is now "semantics" in the sense of finding thedenotation of a formula (English phrase) in some model, not inthe sense of the choice of labels in a "semantic grammar".
Forexample, when the parser identifies an NP that is labeled as a"company", that labeling is syntactic and restricts how the NPcan be composed into larger phrases.
The denotation of that NP,which SPARSER constructs on the basis of its rules ofinterpretation, is the particular individual company that the NPrefers to, or more precisely, the representation f that companyin SPARSER's internal data base.TIRES (AUP)TX GOODYEAR TIRE & RUBBER Co. (Akron,Ohio) - George R. Hargreaves, vice president andtreasurer of Goodyear, will become president and chiefexecutive officer of the Celeron Corp. unit, a holdingcompany for Goodyear's All American Pipeline.
Mr.Hargreaves, 61, will assume the post effective March 1and will retain his current posts.
Robert W. Milk,Celeron's current president and chief executive, as well asan executive vice president for Goodyear, will be onspecial assignment until he retires April 30.The task was to extract relations (database tuples), suchas the one below, that give the action, person or personsaffected, the position (title), and the company or subsidiary.In the text, this corresponds to each clause with a relevantverb, and their variants in reduced clauses, conjunctions, rel-atives, lists, etc.
(though by convention it does not includethe appositives, since they give current information ratherthan changes).
It also included redundant instances, uch asnominalizations oranaphoric references (the post).
There arefour instances in this article, of which SPARSER foundthree, missing the meaning of his current posts because ofrule interference with a recently changed efinition for cur-rent.
The example below is the fLrst of those relations.#<edge75 80 Job-event 105event : #<event-type become-tit le>title: (#<title ~president"#<title "chief executive officer">)person: #<person Hargreaves, George R.>company: #<subsidiaryof: #<company Goodyear Tire &Rubber Company>name: #<co-name"Celeron Corporation">>>Overall, SPARSER found 81% of all the possible job-change relations (597/735).
Within the relations that itfound, 81.5% of them had all of their fields fdled with thecorrect values (486/597).
The false positive rate was 3%(19/616).
Given the limited size of the test set (203 articles,735 possible relations), a better way to state these results isthat the system found 4/5ths of the relations, and that 4/5thsof those were correct in all respects.
Most of the deficits inprecision were due to failing to find a value for a field, ratherthan filling it with an incorrect value; the number of rela-tions with an actual mistake in one field was 6% (36/597).Roughly three man months went into preparing thegrammar.
3 The development corpus was the articles fromIt is not very informative to report hat here were 2,092 rewriterules in the grammar on the day of the test, since this numberincludes the definition of 40 individual years, the 12 months andtheir abbreviations, upper and lowercase forms of most of thewords, etc.
The number also omits the grammar for propernames and for numbers, since these are organized on a quitedifferent basis.
To give some idea of its relevant size, we canpoint out that it supported 12 topic-specific verbsubcategorization frames and 31 topic-specific verbs, 25 titleheads and 30 title modifiers, and that about 25% of the 244mistakes counted in the test could be attributed missing some195December 1990 and from the first half of February.Probably an additional two months would have been requiredto bring the grammar up to full competence on that corpus;entire classes of constructions that were known to be rele-vant were not implemented atthe time of the test, includingdefinite descriptions of rifles acting as people (five vice pres-idents were ...), and conjuncts that did not have objects ofidentical type directly on each side of the conjunction.
Thegrammar overall does, however, have reasonable competencein definite references and pronouns, participles, relativeclauses, and appositives.Its accuracy, especially in such areas as pp-attachment,stems from its use of semantic rather than strictly syntacticterms iri its rules.
This means that a non-triviai amount ofextension is required for each new topic area that a grammaris written for, though much of what is needed will be analo-gous to what is already in place, and the syntactic base, withits treatments auxiliaries, determiners, relative pronouns,etc.
can be carried forward unchanged.As a program, Sparser is quite robust.
In other applica-tions the system has been run continuously without error formore than 30 hours, and it has handled magazine articlesmore than forty thousand words long.We will now look at Sparser's algorithm for phrase struc-ture parsing.
We begin by introducing the rationale for thealgorithm by comparing it with other common phrase struc-ture parsing methods.
Then in ?4 we look at the tokenizer,the chart, and the phrase structure rules, finally moving tothe details of the algorithm and examples of its use.Unfortunately space does not permit more than a passingmention of the other parsing algorithms SPARSER uses inconjunction with phrase structure parsing; a brief precis ofthese companion parsing techniques can be found inMcDonald (1990).3.
Placing the phrase structure algorithmin contextSparser forms its analysis in one pass through the textfrom left to right (beginning to end).
The backbone of theanalysis is a set of context free and context sensitive phrasestructure rewrite rules of the usual sort.
These rules are ap-plied to the text to form edges (parse nodes) over the termi-nal words and other edges-their daughter constituents.
Thefinal set of maximal, connected edges constitutes the parser'sanalysis of the text's form and linguistic relations.
A paral-lel set of projected enotations for these edges in the desig-nated domain model constitutes Sparser's analysis of thetext's meaning, as briefly sketched in ?
1.3.3.1 Standard phrase structure algorithmsPhrase structure parsing can be seen as a kind of search.One looks for the best analysis of the text by searching thespace of possible analyses permitted by the grammar to seewhich one best describes the derivation of the text.
To besure of arriving at the correct analysis, the search must bethorough enough to ensure that no valid analysis is missed.At the same time, the search space should be as small aspossible to ensure fficiency.In considering efficiency, we must trade off the simplic-ity of the control structure against the amount or complexityof the state information that the algorithm calls for.
Thesimplest control algorithm is probably the nested loops ofthe CKY algorithm (see, e.g., Aho & Ullman 1972).
Thisalgorithm searches for parse nodes of all PoSsible wordlengths and starting positions.
It looks through all legalvalues of three indices, 0 _< i < j < k _< n (where n is thelength of the input text), to determine whether two adjacentcandidate daughters, one spanning the text from index i toindex j and the other from j to k, can be combined to form anew node from i to k. This algorithm takes only a few linesto write, but since it is driven by the space of index values,it necessarily requires On3 time to complete its search,along with potentially n2 /2  storage cells to record itsintermediate results.Other familiar algorithms reduce the search space by, ineffect, only looking at those points in the space where thereis guaranteed tobe something to see.
They pay for this in amore elaborate control structure.
Using Earley's algorithm(1970) as the model, we can summarize their procedures astypically f'wst predicting, top-down, what constituents couldlegally occur given the rules of the grammar.
Then, as theysequentially scan the terminals of the text, either from theleft end or the right, they incrementally confirm some ofthese predictions by completing hypothesized rules bottomup as all of a rule's daughter constituents are found.
Withcommon grammars (bounded state), Earley's algorithm runsin order of n time, but at a cost in storage potentially asgreat as 62, where G is the number of rules in the grammar.The bulk of this storage cost in Earley's algorithm is dueto its representation f the predictions, i.e.
a listing of all ofthe potentially completable rules that is modified as eachterminal is scanned and edges are completed.
An active-edgechart algorithm (Kay 1980, Kaplan 1973; a good textbooktreatment can be found in Winograd 1983) has a comparablestorage cost because it also maintains an online representa-tion of the production rules that are relevant to the analysis,though the particulars of how it represents these partiallyinstantiated rules are quite different from Earley's given thedifferences in their control structures.In a parser designed for unrestricted, multi-paragraph text,like Sparser, there are problems with using any explicit run-time representation f potentially completable rules.
Thenear-inevitability that the sentences will be broken up byunknown words means that one cannot assume that all otthe root edges in the final forest will be labeled with "S".As a result, one must include in the set of starting labels folthe predictions essentially all of the lefthand-side labels inthe grammar's rule set.
(The treatment in Martin, Church &Patil 1981 handles this "all predictions at all vertexes" prob-lem very elegantly.)
Given that Sparser presently containsapproximately 300 non-terminal labels in its semanticgrammar, any algorithm with an order of G storage cos1would be prohibitively expensive.rewrite rule and 20% to missing vocabulary (8% to the singlecase hold <position>).1963.2 Introduction to SPARSER's algorithmTo predict everything, however, is to constrain othing,and so the natural alternative is of course to form phrasesbottom up, using only the "scan" and "complete" aspects ofthe basic algorithm.
SPARSER uses a bottom-up arsingalgorithm for its phrase structure rules.
All of the edges inits chart are what would be called "inactive" edges in theabove approaches--they all represent actual constituents inthe text rather hypothesized ones.Without he constraint provided by prediction that everyedge will be used in the final analysis, a conventional bot-tom-up algorithm suffers from two kinds of problems.?
Locally correct but globally misaligned edges canresult in additional, unconnected ges that will notbe part of the final, maximal analysis.?
The very same combination of constituents may beparsed in several different orders, resulting in multi-ple, "spurious" edges covering the same span andwith the same meaning, where only one is needed.Sparser addresses the problem of misalignment by forc-ing its its initial, "segment by segment" parsing to conformto a linguistically-motivated "grid" that is formed fromphrase boundary information taken from the location ofclosed-class words; see ?4.4.Sparser addresses the spurious edge problem by drasti-cally restricting its search space of adjacent edges.
When anew edge is entered and checked against i s adjacent neighboredges for possible completion of a new edge, only the single"topmost" neighbor edge at a position is checked, rather thanall of the edges that have accumulated atthat position as iscustomary in a bottom-up algorithm.
This topmost edgewill be the one most recently entered into the chart, and itwill be the longest hus far to start/end at that position.This reduction in the search space by checking againstonly topmost edges can dramatically ower the number ofchecks made and edges entered.
The exact amounts varywith the grammar and the text.
The greatest savings comesin conjunctions or in cases where a head labeled with arecursive category can take several complements from eitherits left or right (e.g.
a verb phrase taking auxiliaries to itsleft and optional adjuncts to its right)--constructions thatare ubiquitous in news articles.
The savings is multiplica-tive: If there are m complements othe left of the head andn complements o the right, and if each composition of acomplement and its accumulated head+complements neigh-bor phrase yields a new edge with the same label as the head,then the number of edges formed if all neighbors are checkedis ra*n. If only the top neighbor is checked the number ism+n.A further reduction in the search space is achievedthrough Sparser's use of a semantic grammar.
Each preter-minal edge for a open class word will have a semantic classi-fication (label) corresponding tothe kind of thing it denotes(see ?
1.3).
If a word is ambiguous it will introduce multi-ple edges, one for each interpretation.
By writing the rulesfor phrasal composition i  terms of these classifications, weinsure that no phrase will be formed by the parser unless ithas a semantic interpretation.
This cuts down dramaticallyon the amount of structural ambiguity that the parser mustcope with; indeed, in nearly all cases examined so far, poly-semous words have been disambiguated bythe first rule thatapplies to them to form a larger phrase.
Prepositional t-tachment, in particular, has not proved a problem since oneis not adding a PP, waiting for a later semantic interpreta-tion process to rule on whether the combination makessense, but instead adding a phrase labeled, e.g., "for-com-pany", whose rules of combination are markedly more spe-cific.4.
The Details of  the Algor i thmWe will now look at particulars of the scan routine, thechart, and the phrase structure rules, and then move on todescribe the phrase structure parsing algorithm in the contextof a short example.
Overall, Sparser is a transducer takingas input a stream of ascii characters and producing as output(a) a recycled chart of completed edges with their denotationsin the domain model, and (b) a sequence of user-specifiedactions triggered at hooks embedded within the core algo-rithms such as the completion of an edge with a given label.One use of these hooks/actions has been to collect, e.g., themaximal job-event edges in an article so that they can bereadout into the data base.
We will not discuss them furtherin this paper.4.1 Operations over terminalsAt the base of the purser's operation is a scan operationthat identifies (delimits) minimal tokens within the inputcharacter stream, consuming it in the process, and addingedges to the chart.
Phrase boundary brackets are also enteredinto the chart when function words are scanned, as describedin ?4.4.As each token is delimited within the stream, it is lookedup in the word list.
If it is known, the prefer'meal object hatrepresents it (a "word") is entered into the chart, along withany edges dictated by the grammar.
If it is unknown--newto the parser--a word object is constructed for it, and itsstring examined to characterize its morphological nd capi-talization properties.
Tokens are minimal sequences of thesame character type, e.g.
the sequence "$43.3 million" isseen as six tokens: "$", "43", ". '
,  "2", <one space>,"million".
All larger combinations are formed throughphrase structure or other sorts of rules.In addition to the introduction of preterminal edges, non-phrase structure rules of various sorts may be associatedwith tokens and are executed as the tokens are scanned.These include?
simple "polyword" rules that interpret a sequence ofterminals as a single, inseparable entity (e.g.
"holdingcompany", Wall S~eet Journal");?
rules for forming constituents on the basis of pairedpunctuation such as parentheses orbrackets, or forspecial conventions such as SGML tags;?
complex rules for the formation of constituents with"flat", Kleene star -style internal structures, inpartic-ular proper names; and197?
arbitrary actions outside the parser's cope, e.g.
to doword counts, or to feed a topic detection algorithmthat does not use Sparser's later stages.A proper discussion of the algorithms for these opera-tions and their integration into the parsing algorithm as awhole is beyond the scope of this paper.
Suffice it to saythat once triggered they execute as self-contained processes,and that their results are always recorded as one or moreedges that they add to the chart, spanning the appropriateamount of text.4.2 The ChartSparser's chart is comprised of three kinds of data struc-tures: positions, edges, and edge-vectors.
Positions provideindices to record the sequence of the terminals and indicatethe spans of the edges.
They correspond to the "vertices"between edges as used in other chart algorithms, but herethey are first class objects with their own primary definitionin terms of the sequence of terminals, rather than beingdependent on the notion of edges.
Following the usual con-vention, positions are located between the terminals.From the point of view of the parsing algorithm, there isunlimited stream of positions, starting with the positionwith index zero that precedes the dummy terminal represent-ing the start of the text, and continuing terminal by terminaluntil the tokenizer has exhausted the input character stream.The implementation is actually in terms of a fixed lengtharray filled with position objects.
This array grounds thenotion of successive positions.
The Scan operation willmake the array wrap around and write over earlier positionobjects as needed when the length of the text exceeds thelength of the array.
The utility of this fixed, recycledresource is that it allows SPARSER to handle texts of arbi-trary length, so long as the array is longer than the longestspan of terminals over which some adjacency-driven phrasestructure rule is expected to apply.
A length of 250 hasproved more than adequate in the Who's News domain.Edges represent the completion of rules, or mark thepresence of terminals that are mentioned as literals in somerule.
An edge has a label, which will be the lefthand-sideterm of the corresponding rule, and records the constituentedges (or single terminal/edge) that it spans.
An edge'sdaughter edges can be readout recursively as a parse treemarking the sequence (derivation) of rules that constitutesthe grammar's analysis of the sequence of terminals the edgespans.
Like positions, edges are implemented asa recycledresource; the customary number of edge objects is 500.Edge-vectors link positions to edges.
Each position hastwo edge vectors, one recording the edges that end at thatposition, the other the edges that begin at that position.The edges in each vector are sorted historically: the first edgein a vector will be the first edge to have been introduced into the chart that ended/started at that position; the last edgewill be the most recent.
The most recently introduced edgeis referred to at the "top" edge to end/start at the position;this edge is pointed to directly by the edge vector objectbecause of its importance to the parsing algorithm.
Giventhe nature of the parsing algorithm it will also be thelongest edge to end/start at the position.4.3 The phrase structure grammarThe phrase structure grammar consists of a set of rewriterules.
The rules define patterns of labeled adjacent immedi-ate constituen~ in the usual manner.
The labels are eitherliteral words (or any other sort of token such as punctuationor in some cases even whitespace), or they are atomic cate-gory symbols.Shown below are some of the rules used in the analysisof the sample article, given in the usual notation as termson the left and righthand-sides of an arrow.
The righthandside terms are the labels on immediate constituents; the left-hand term will be their parent, labeling any edge formed bythe completion of that rule.
(i head-o f - subs id ia ry -phrase  -> "unit"(2 head-o f - subs id ia ry -phrase  ->company head-o f - subs id ia ry -phrase(3 subs id ia ry -company ->"the" head-o f - subs id ia ry -phrase(4 company-possess ive  ->company apost rophe-s(5 name -> company/ company-possess ive(6) for -company -> "for" companyAs part of not needing to support an "active edge" repre-sentation of partially complete rules during runtime,Sparser's basic operation, which we can call "check", isdefined over a pair of adjacent edges.
A table is consulted tosee if there is some rule (or dotted expansion of a rule, seebelow) that lists the labels of those two edges, in order, asits righthand side.
If there is such a rule, a new edge is con-structed and entered into the chart.
If a context free rule isinvolved, then the edge will span both daughter edges and islabeled with the lefthand side term of the rule.
If it is a con-text sensitive rule, then the designated daughter edge will berespanned and given that label.Sparser supports rules with more than two righthand sideterms by converting them to a kind of Chomsky NormalForm using a dotted rule rule convention as described byMartin, Church & Patil (1981).4.4 The parsing algorithmThe phrase structure algorithm divides logically intothree processes: (1) delimiting the next segment, (2) parsingthe new edges within that segment, and (3) parsing edgesacross segment boundaries.
The control structure treatsthese as independent processes that signal events, andswitches between them as the events dictate.
We describeeach of these processes in turn, using as our example theportion of the example article excerpted below.... president and chief executive officer of the CeleronCorp.
unit, a holding company for Goodyear's AllAmerican Pipeline.The notion of a "segment" in SPARSER is a sequence o!terminals between amatching set of phrase boundary brack-ets that are introduced into the chart by closed class words otby known open-class words from the domain vocabulary,198e794President and6chiefcxecutiv~i?ffice~9?~ ~ i ~ 9 5  0tC elero~!Co~0 1 ~  , 3 5 106I " o II IIelO04 e3Just below is the excerpt with its brackets.
The initialbracket was introduced by the just-preceding known verb,"become", the other brackets were introduced by the functionwords/punctuation/affixes: "and", "of', "the", ",", "a'', "for',"'s", and ".
".\[ president \] and \[ chief executive officer \] of  \[ the CeleronCorp.
unit \] , \[ a holding company \] for \[ Goodyear \] 's \[All American Pipeline \] .
\]The idea of segmenting a text on the basis of its closedclass words is an old one.
A recent, comparably systematicsystem where closed class words are used is described byO'Shaughnessy (1989).
And it appears that something likethis scheme is used in Hindle's FIDDITCH parser (partiallydescribed in Hindle 1983).The segment delimiter starts at the last position where asegment terminated (or initially at position 0).
It makessuccessive calls to Scan, adding words and their immediatepre-terminal edges to the chart and running any of the non-phrase structure parsing processes that the words trigger.This processes stops when a word is scanned that introducesa close bracket ("\]").
At this point control is passed thesecond process, to form whatever constituents may be foundwithin the new segment by looking for combinations of thepre-terminal edges.When using a normal "all edges" bottom-up algorithm,the criteria for which of the many trees to select is usuallyto choose the combination that provides the longest consis-tent account of the text and strands the fewest unattachededges.
We mimic that selection criteria online, by havingthe parser first respect he linguistically motivated bound-aries provided by closed class and other known words---pars-ing within a segment before combining any edges across asegment boundary.
And second by respecting the possibilityof that the rightmost edge in a segment may be extended bysome not-yet-formed ge to its right in the adjacent seg-ments-the algorithm does not allow a rightmost edge to becombined with an edge to its left if the resulting edge wouldnot have the same label and consequently does not have thesame possibilities for rightward extensions.In terms of the interaction of the three processes, thismeans first that within-segment parsing is constrained not topermit any combinations of the segment's rightmost edgeand its immediate neighbor edge to its left if that wouldchange the possibilities for extending that rightmost edgelater through a combination with some edge to its right.
(This is a trivial check against the grammar tables.
)Once the within-segment parsing has finished, the result-ing rightmost edge is similarly examined: If it permitsrightward combinations then we return to the segment-delimiting process, and from that to the within-segmentparsing process.
Once there is finally a segment whoserightmost edge does not have a possible rightward extension,then the across-segment parsing process is allowed to startoperating, beginning with the then rightmost edge in thechart overall.
4 As this third process moves leftwards form-ing successively arger edges, the possibility of rightwardextensions is continually checked for, and the segment-delimiting process re-entered as needed.We can see this control structure loop in action by walk-ing through the excerpted text.
Let us assume that havereached the point where the segment containing "the CeleronCorp.
unit" has just been delimited.
The chart will be asshown below.
Positions are indicated by their index numbersbetween and below each of the words.
Edges are indicated byhalf rectangles connecting the positions.
The numbers onthe edges (e.g.
"el") are for expository purposes only; theyreflect he order in which each edge was introduced.
Theedge labels are not shown.
For clarity the edges in the justdelimited segment are shown above the text, and those ofearlier and later segments below.The within-segment parsing process will look for com-binations of the edges between position 100 and 105, work-ing rightwards from edge9.
Edge9, the preterminal edgeover the word "unit", is labeled "head-of-subsidiary-phrase"in this grammar.
There are no rules in the grammar thatwould extend that edge into a larger edge to its right, and sothe process is allowed to look for leftward combinations.There are two edges adjacent o the left of edge9.Following the restricted search space convention of the algo-rithm, only the more recent of these, edge7, is checked.According to rule number two of the set listed earlier, edge7and edge9 combine to form a new edge, which will thencombined with edge5 according to rule three.
There is nowone edge spanning all of the segment; if there was a gap, saydue to the presence of unknown words in the segment, henheuristic rules would be attempted, as briefly mentioned atthe end of ?
1.1.4 In some cases this can mean that an entire sentence is scannedbefore any across-segment dges are formed.
This assumes, ofcourse, that one does not write a grammar rule where period isnot the left term of some rule, in which case the scan wouldcontinue.199The new topmost edge over the segment, labeled"subsidiary-company", does participate in rules that couldcombine it with an edge to its right, and so the delimitingprocess is resumed to scan until the next segment is termi-nated.
That segment will contain the words "a holdingcompany".
Within-segment parsing will span the segmentwith a phrase labeled "company-description", which in thepresent grammar takes rightwards extensions and so the thedelimiting process is run again.
This iterates until theperiod after "pipeline" is reached, at which point across-segment parsing is finally begun.
It rolls up the accumu-lated edges one after the other from the right.
The penulti-mate composition i this example is the title phrase (edge4)and a "subsidiary-company" phrase spanning all the wayfrom position 99 to position 115 just before the period.5.
Conc lus ions :  Why is this efficient ?Given two parsers that employ the same algorithms, themore efficient one will be the one with the most carefullydesigned and optimized implementation.
The two parserswill carry out the same steps (at the aigodthmic level), butone will do them more quickly, consuming less storage, etc.From this mechanical point of view Sparser comes off wellas compared with other parsing systems that the author isfamiliar with: A Lisp program, it uses only preallocatedstorage, which led to a three-fold increase in speed relative toits prior implementation.Holding the quality of the implementation constant (andof course the choice of machine on which any tests aremade), the greatest increase in efficiency comes from im-proving the algorithm so that fewer steps are taken.
Weachieved this in two ways.First, we employed a particular technique for reducing thesearch space through which the parser searched, therebyreducing the number of checks make against the grammar tosee whether two edge could be combined, and also reducingthe number of edges ever entered into the chart.
While wehave not yet made a systematic comparison, this techniqueof checking only the topmost edges at a position appears toresult in three to ten times fewer edges ever being formed(depending on the article and the grammar) when comparedto an earlier variant of Sparser's algorithm that checking allof the edges.Second, we employed a grammar with semanticallylabeled terms, thereby ensuring that only edges that couldreceive a valid semantic interpretation would ever be formed.This does not cut down on the number of edges checkedagainst he grammar, but it has a dramatic effect on thenumber of edges ever allowed to be formed in the first place.While again we do not have systematic counts (which wouldeffectively require having an entirely new grammar that usedonly syntactic labels), our impression is that the reductionin ambiguity that the semantic labels brought about had amore significant effect on the number of edges and checksthan any variation in the algorithm.Earley, J.
(1970) "An Efficient Context-Free ParsingAlgorithm", Communications of the ACM, 13(2),February 1970.Hindle, Don (1983) "Deterministic Parsing of SyntacticNon-fluencies", Proc.
21st Annual Meeting of theAssociation for Computational Linguistics, June 15-171983, MIT, pp.
123-128.Kaplan, Ronald M. (1973) "A General Syntactic Parser"in Rustin (ed.)
Natural Language Processing, AlgorithmicsPress, New York, pp.193-242.Kay, Martin (1980) "Algorithm Schemata and DataStructures in Syntactic Processing", Xerox PARC TechnicalReport CSL-80-12; reprinted in Grosz, Sparck Jones andWebber (eds) Readings in Natural Language Processing,Morgan Kaufmann.
1986.Martin, William A., Kenneth W. Church, Ramesh S. Patil(1981) "preliminary Analysis of a Breadth-First ParsingAlgorithm: Theoretical and Experimental Results" MITLaboratory for Computer Science Technical Report #261.McDonald, David D. (1990) "Robust Partial-Parsingthrough Incremental, Multi-level Processing: rationales andbiases", AAAI Spring Symposium paper eprinted in Jacobs(ed.)
"Text-Based Intelligent Systems: Current Research inText Analysis, Information Extraction, and Retrieval, GER&D Center technical report 90CRD198, Schenectady, NY.O'Shaughnessey, Douglas D. (1989) "Parsing with aSmall Dictionary for Applications uch as Text to Speech"Computational Linguistics 15(2), June 1989, pp.97-108.Winograd, Terry (1983) Language as a Cognitive Process,Addison Wesley.6.
ReferencesAho, Alfred V. & Ullman, Jeffrey D. (1972) The Theoryof Parsing, Translation, and Compiling, Prentice-Hall.200
