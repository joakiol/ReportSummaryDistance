NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 23?30,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsToward Tree Substitution Grammars with Latent AnnotationsFrancis Ferraro and Benjamin Van Durme and Matt PostCenter for Language and Speech Processing, andHuman Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractWe provide a model that extends the split-merge framework of Petrov et al (2006) tojointly learn latent annotations and Tree Sub-stitution Grammars (TSGs).
We then conducta variety of experiments with this model, firstinducing grammars on a portion of the PennTreebank and the Korean Treebank 2.0, andnext experimenting with grammar refinementfrom a single nonterminal and from the Uni-versal Part of Speech tagset.
We present quali-tative analysis showing promising signs acrossall experiments that our combined approachsuccessfully provides for greater flexibilityin grammar induction within the structuredguidance provided by the treebank, leveragingthe complementary natures of these two ap-proaches.1 IntroductionContext-free grammars (CFGs) are a useful tool fordescribing the structure of language, modeling a va-riety of linguistic phenomena while still permittingefficient inference.
However, it is widely acknowl-edged that CFGs employed in practice make unre-alistic independence and structural assumptions, re-sulting in grammars that are overly permissive.
Onesuccessful approach has been to refine the nonter-minals of grammars, first manually (Johnson, 1998;Klein and Manning, 2003) and later automatically(Matsuzaki et al, 2005; Dreyer and Eisner, 2006;Petrov et al, 2006).
In addition to improving pars-ing accuracy, the automatically learned latent anno-tations of these latter approaches yield results thataccord well with human intuitions, especially at thelexical or preterminal level (for example, separatingdemonstrative adjectives from definite articles underthe DT tag).
It is more difficult, though, to extendthis analysis to higher-level nonterminals, where thelong-distance interactions among latent annotationsof internal nodes are subtle and difficult to trace.In another line of work, many researchers have ex-amined the use of formalisms with an extended do-main of locality (Joshi and Schabes, 1997), wherethe basic grammatical units are arbitrary tree frag-ments instead of traditional depth-one context-freegrammar productions.
In particular, Tree Substitu-tion Grammars (TSGs) retain the context-free prop-erties of CFGs (and thus the cubic-time inference)while at the same time allowing for the modeling oflong distance dependencies.
Fragments from suchgrammars are intuitive, capturing exactly the sorts ofphrasal-level properties (such as predicate-argumentstructure) that are not present in Treebank CFGs andwhich are difficult to model with latent annotations.This paper is motivated by the complementarityof these approaches.
We present our progress inlearning latent-variable TSGs in a joint approach thatextends the split-merge framework of Petrov et al(2006).
We present our current results on the Pennand Korean treebanks (Marcus et al, 1993; Han etal., 2001), demonstrating that we are able to learnfragments that draw on the strengths of both ap-proaches.
Table 1 situates this work among othercontributions.In addition to experimenting directly with thePenn and Korean Treebanks, we also conducted twoexperiments in this framework with the Universal23CFG TSGnone Charniak ?97 Cohn et al ?09manual Klein & Manning ?03 Bansal & Klein ?10automatic Matsuzaki et al ?05 This paperPetrov et al ?06Dreyer & Eisner ?06Table 1: Representative prior work in learning refine-ments for context-free and tree substitution grammars,with zero, manual, or automatically induced latent anno-tations.POS tagset (Petrov et al, 2011).
First, we investigatewhether the tagset can be automatically derived af-ter mapping all nonterminals to a single, coarse non-terminal.
Second, we begin with the mapping de-fined by the tagset, and investigate how closely thelearned annotations resemble the original treebank.Together with our TSG efforts, this work is aimed atincreased flexibility in the grammar induction pro-cess, while retaining the use of Treebanks for struc-tural guidance.2 Background2.1 Latent variable grammarsLatent annotation learning is motivated by the ob-served coarseness of the nonterminals in treebankgrammars, which often group together nodes withdifferent grammatical roles and distributions (suchas the role of NPs in subject and object position).Johnson (1998) presented a simple parent-annotationscheme that resulted in significant parsing improve-ment.
Klein and Manning (2003) built on these ob-servations, introducing a series of manual refine-ments that captured multiple linguistic phenomena,leading to accurate and fast unlexicalized parsing.Later, automated methods for nonterminal refine-ment were introduced, first splitting all categoriesequally (Matsuzaki et al, 2005), and later refin-ing nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework.
This lat-ter approach was able to recover many of the splitsmanually determined by Klein and Manning (2003),while also discovering interesting, novel clusterings,especially at the lexical level.
However, phrasal-level analysis of latent-variable grammars is moredifficult.
(2006) observed that these grammars couldlearn long-distance dependencies through sequencesof substates that place all or most of their weight on(a) A TSG fragment.SBARINforSNP VPTOtoVP(b) Equivalent CFG rules.SBAR ?
IN SIN ?
forS ?
NP VPVP ?
TO VPTO ?
toFigure 1: Simple example of a TSG fragment and anequivalent representation with a CFG.particular productions, but such patterns must be dis-covered manually via extensive analysis.2.2 Tree substitution grammarsTree substitution grammars (TSGs) allow for com-plementary analysis.
These grammars employ an ex-tended domain of locality over traditional context-free grammars by generalizing the atomic units of thegrammar from depth-one productions to fragmentsof arbitrary size.
An example TSG fragment alongwith equivalent CFG rules are depicted in Figure 1.The two formalisms areweakly equivalent, and com-puting the most probable derivation of a sentencewith a TSG can be done in cubic time.Unfortunately, learning TSGs is not straight-forward, in large part because TSG-specific re-sources (e.g., large scale TSG-annotated treebanks)do not exist.
One class of existing approaches,known as Data-Oriented Parsing, simply uses all thefragments (Bod, 1993, DOP).
This does not scalewell to large treebanks, forcing the use of implicitrepresentations (Goodman, 1996) or heuristic sub-sets (Bod, 2001).
It has also been generally ob-served that the use of all fragments results in poor,overfit grammars, though this can be addressed withheld-out data (Zollmann and Sima?an, 2005) or sta-tistical estimators to rule out fragments that are un-likely to generalize (Zuidema, 2007).
More recently,a number of groups have found success employingBayesian non-parametric priors (Post and Gildea,2009; Cohn et al, 2010), which put a downwardpressure on fragment size except where the datawarrant the inclusion of larger fragments.
Unfortu-nately, proper inference under these models is in-tractable, and though Monte Carlo techniques can24provide an approximation, the samplers can be com-plex, difficult to code, and slow to converge.This history suggests two approaches to state-splitTSGs: (1) a Bayesian non-parametric sampling ap-proach (incorporate state-splitting into existing TSGwork), or (2) EM (incorporate TSG induction intoexisting state-splitting work).
We choose the latterpath, and in the next section will describe our ap-proach which combines the simplicity of DOP, theintuitions motivating the Bayesian approach, and theefficiency of EM-based state-splitting.In related work, Bansal and Klein (2010) combine(1996)?s implicit DOP representation with a num-ber of the manual refinements described in Klein andManning (2003).
They achieve some of the best re-ported parsing scores for TSGwork and demonstratethe complementarity of the tasks, but their approachis not able to learn arbitrary distributions over frag-ments, and the state splits are determined in a fixedpre-processing step.
Our approach addresses both ofthese limitations.3 State-Split TSG InductionIn this sectionwe describe howwe combine the ideasof dop, Bayesian-induced TSGs and Petrov et al(2006)?s state-splitting framework.1 We are able todo so by adding a coupling step to each iteration.That is, each iteration is of the form:(1) split all symbols in two,(2) merge 50% of the splits, and(3) couple existing fragments.Because every step results in a new grammar, pro-duction probabilities are fit to observed data by run-ning at most 50 rounds of EM after every step listedabove.2 We focus on our contribution ?
the cou-pling step?
and direct those interested in details re-garding splitting/merging to (Petrov et al, 2006).Let T be a treebank and let F be the set of allpossible fragments in T .
Define a tree T ?
Tas a composition of fragments {Fi}ni=1 ?
F , withT = F1 ?
?
?
?
?
Fn.
We use X to refer to an arbi-trary fragment, with rX being the root of X .
Two1Code available at cs.jhu.edu/~ferraro.2We additionally apply Petrov et al (2006)?s smoothing stepbetween split and merge.fragments X and Y may compose (couple), whichwe denote byX ?Y .3 We assume thatX and Y maycouple only if X ?
Y is an observed subtree.3.1 Coupling ProcedureWhile Petrov et al (2006) posit all refinements sim-ulatenously and then retract half, applying this strat-egy to the coupling step would result in a combina-torial explosion.
We control this combinatorial in-crease in three ways.
First, we assume binary trees.Second, we introduce a constraint set C ?
F that dic-tates what fragments are permitted to compose intolarger fragments.
Third, we adopt the iterative ap-proach of split-merge and incrementally make ourgrammar more complex by forbidding a fragmentfrom participating in ?chained couplings:?
X ?Y ?Zis not allowed unless eitherX ?Y or Y ?Z is a validfragment in the previous grammar (and the chainedcoupling is allowed by C).
Note that setting C = ?results in standard split/merge, while C = F resultsin a latently-refined dop-1 model.We say that ?XY?
represents a valid coupling ofXand Y only if X ?
Y is allowed by C, whereas ?XY?represents an invalid coupling ifX?Y is not allowedby C. Valid couplings result in new fragments.
(Wedescribe how to obtain C in ?3.3.
)Given a constraint set C and a current grammar G,we construct a new grammar G?.
For every fragmentF ?
G, hypothesize a fragment F ?
= F ?
C, pro-vided F ?
C is allowed byC.
In order to add F andF ?
to G?, we assign an initial probability to both frag-ments (?3.2), and then use EM to determine appro-priate weights.
We do not explicitly remove smallerfragments from the grammar, though it is possiblefor weights to vanish throughout iterations of EM.Note that a probabilistic TSG fragment may beuniquely represented as its constituent CFG rules:make the root of every internal depth-one subtreeunique (have unit probability) and place the entiretyof the TSG weight on the root depth-one rule.
Thisrepresentation has multiple benefits: it not only al-lows TSG induction within the split/merge frame-work, but it also provides a straight-forward way touse the inside-outside algorithm.3Technically, the composition operator (?)
is ambiguous ifthere is more than one occurrence of rY in the frontier of X .Although notation augmentations could resolve this, we rely oncontext for disambiguation.253.2 Fragment Probability EstimationFirst, we define a count function c over fragments byc(X) =?T?P(T )???T?X,?
, (1)where P(T ) is a parsed version of T , ?
is a subtreeof T and ?X,?
is 1 iff X matches ?
.4 We may thencount fragment co-occurrence by?Yc(X ?
Y ) =?Y :?XY?c(X ?
Y ) +?Y :?XY?c(X ?
Y ).Prior to running inside-outside, we must re-allocate the probability mass from the previous frag-ments to the hypothesized ones.
As this is justa temporary initialization, can we allocate massas done when splitting, where each rule?s mass isuniformly distributed, modulo tie-breaking random-ness, among its refinement offspring?
Split/mergeonly hypothesizes that a node should have a particu-lar refinement, but by learning subtrees our couplingmethod hypothesizes that deeper structure may bet-ter explain data.
This leads to the realization that asymbol may both subsume, and be subsumed by, an-other symbol in the same coupling step; it is not clearhow to apply the above redistribution technique toour situation.However, even if uniform-redistribution couldeasily be applied, we would like to be able to indi-cate how much we ?trust?
newly hypothesized frag-ments.
We achieve this via a parameter ?
?
[0, 1]:as ?
?
1, we wish to move more of P [X | rX ]to P [?XY?
| rX ].
Note that we need to know whichfragmentsL couple below withX (?XL?
), and whichfragments U couple above (?UX?
).For reallocation, we remove a fraction of the num-ber of occurrences of top-couplings of X:c?
(X) = 1 ?
?
?Y :?XY?
c(X ?
Y )?Y c(X ?
Y ), (2)and some proportion of the number of occurrencesof bottom-couplings of X:sub(X) =?U :?UX?
c(U ?X)?U,L:?UL?rX=rLc(U ?
L).
(3)4We use a parsed version because there are no labeled inter-nal nodes in the original treebank.To prevent division-by-zero (e.g., for pre-terminals),(2) returns 1 and (3) returns 0 as necessary.Given any fragmentX in an original grammar, let?
be its conditional probability: ?
= P [X | rX ] .For a new grammar, define the new conditional prob-ability for X to beP [X | rX ] ?
?
?
|c?
(X) ?
sub(X)|, (4)andP [?XY?
| rX ] ?
?
?c(X ?
Y )?Y c(X ?
Y )(5)for applicable Y .Taken together, equations (4) and (5) simply saythat X must yield some percentage of its currentmass to its hypothesized relatives ?XY?, the amountof which is proportionately determined by c?.
But wemay also hypothesize ?ZX?, which has the effect ofremoving (partial) occurrences of X .5Though we would prefer posterior counts of frag-ments, it is not obvious how to efficiently obtain pos-terior ?bigram?
counts of arbitrarily large latent TSGfragments (i.e., c(X ?
Y )).
We therefore obtain, inlinear time, Viterbi counts using the previous bestgrammar.
Although this could lead to count sparsity,in practice our previous grammar provides sufficientcounts across fragments.3.3 Coupling from Common SubtreesWe now turn to the question of how to acquire theconstraint set C. Drawing on the discussion in ?2.2,the constraint set should, with little effort, enforcesparsity.
Similarly to our experiments in classifi-cation with TSGs (Ferraro et al, 2012), we extracta list of the K most common subtrees of size atmost R, which we refer to as F?R,K?.
Note that ifF ?
F?R,K?, then all subtreesF ?
ofF must also be inF?R,K?.6 Thus, we may incrementally build F?R,K?in the following manner: given r, for 1 ?
r ?
R,maintain a ranking S, by frequency, of all fragmentsof size r; the key point is that S may be built fromF?r?1,K?.
Once all fragments of size r have beenconsidered, retain only the top K fragments of theranked set F?r,K?
= F?r?1,K?
?
S.5If c?
(X) = sub(X), then define Eqn.
(4) to be ?.6Analogously, if an n-gram appears K times, then all con-stituentm-grams,m < n, must also appear at leastK times.26This incremental approach is appealing for tworeasons: (1) practically, it helps temper the growthof intermediate rankings F?r,K?
; and (2) it providestwo tunable parametersR andK, which relate to thebase measure and concentration parameter of previ-ous work (Post and Gildea, 2009; Cohn et al, 2010).We enforce sparsity by thresholding at every itera-tion.4 DatasetsWe perform a qualitative analysis of fragmentslearned on datasets for two languages: the Ko-rean Treebank v2.0 (Han and Ryu, 2005) and acomparably-sized portion of the WSJ portion of thePenn Treebank (Marcus et al, 1993).
The KoreanTreebank (KTB) has predefined splits; to be compa-rable for our analysis, from the PTB we used ?2-3for training and ?22 for validation (we refer to thisas wsj2-3).
As described in Chung et al (2010), al-though Korean presents its own challenges to gram-mar induction, the KTB yields additional difficultiesby including a high occurrence of very flat rules (in5K sentences, there are 13 NP rules with at least fourrighthand side NPs) and a coarser nonterminal setthan that of the Penn Treebank.
On both sets, werun for two iterations.Recall that our algorithm is designed to induce astate-split TSG on a binarized tree; as neither datasetis binarized in native form we apply a left-branchingbinarization across all trees in both collections as apreprocessing step.
Petrov et al (2006) found differ-ent binarization methods to be inconsequential, andwe have yet to observe significant impact of this bi-narization decision (this will be considered in moredetail in future work).Recently Petrov et al (2011) provided a set ofcoarse, ?universal?
(as measured across 22 lan-guages), part-of-speech tags.
We explore here theinteraction of this tagset in our model on wsj2-3: callthismodified version uwsj2-3, onwhichwe run threeiterations.
By further coarsening the PTB tags, wecan ask questions such as: what is the refinementpattern?
Can we identify linguistic phenomena in adifferent manner than we might without the univer-sal tag set?
Then, as an extreme, we replace all POStags with the same symbol ?X,?
to investigate whatpredicate/argument relationships can be derived: we(a) Modal construction.S2SNP0 VP0VPMDwillVP0(b) Modifiable NP.NP2NPNNpresidentPP0(c) Nominal-modification.NP0NPNPNNP3 NNP1NNP0NNP0(d) PP construction.PP0INatNPNP0 NNP0(e) Initial Quotation.SINV1SINVSINVSINV0 ,0?0VPVBZ0Figure 2: Example fragments learned on wsj2-3.call this set xwsj2-3 and run four times on it.75 Fragment AnalysisIn this section we analyze hand-selected preliminaryfragments and lexical clusterings our system learns.WSJ, ?2-3 As Figure 2 illustrates, after two iter-ations we learn various types of descriptive lexical-ized and unlexicalized fragments.
For example, Fig-ure 2a concisely creates a four-step modal construc-tion (will), while 2b demonstrates how a potentiallyuseful nominal can be formed.
Further, learned frag-ments may generate phrases with multiple nominalmodifiers (2c), and lexicalized PPs (2d).Note that phrases such as NP0 and VP0 are of-ten lexicalized themselves (with determiners, com-mon verbs and other constructions), though omitteddue to space constraints; these lexicalized phrasescould be very useful for 2a (given the incremental7While the universal tag set has a Korean mapping, the sym-bols do not coincide with the KTB symbols.27(a) Common noun refinements.NNC0 ??
??
?
?case this day at the moment1 ??
??
?
?international economy world2 ??
??
?
?related announcement report(b) Verbal inflection.VV0NNC2 XSV?
(c) Adjectival inflection.VJ0NNC1 XSJ?Figure 3: Clusters and fragments for the KTB.coupling employed, 2a could not have been furtherexpanded in two iterations).
Figure 2c demonstrateshow TSGs and latent annotations are naturally com-plementary: the former provides structure while thelatter describes lexical distributions of nominals.Figure 2e illustrates a final example of syntacticstructure, as we begin to learn how to properly an-alyze a complex quotation.
A full analysis requiresonly five TSG rules while an equivalent CFG-onlyconstruction requires eight.KTB2 To illustrate emergent semantic and syntac-tic patterns, we focus on common noun (NNC) re-finements.
As seen in Table 3a, top words fromNNC0 represent time expressions and planning-related.
As a comparison, two other refinements,NNC1 and NNC2, are not temporally representative.This distinction is important as NNC0 easily yieldsadverbial phrases, while the resultant adverbial yieldfor either NNC1 or NNC2 is much smaller.Comparing NNC1 and NNC2, we see that thehighest-ranked members of the latter, which includereport and announcement, can be verbalized by ap-pending an appropriate suffix.
Nouns under NNC1,such as economy and world, generally are subjectto adjectival, rather than verbal, inflection.
Figures3b and 3c capture these verbal and adjectival inflec-tions, respectively, as lexicalized TSG fragments.WSJ, ?2-3, Universal Tag Set In the preliminarywork done here, we find that after a small number ofiterations we can identify various cluster classifica-tions for different POS tags.
Figures 4a, 4b and 4cprovide examples for NOUN, VERB and PRON, re-spectively.
For NOUNs we found that refinementscorrespond to agentive entities (refinements 0, 1,e.g., corporations or governments), market or stockconcepts (2), and numerically-modifiable nouns (7).Some refinements overlapped, or contained commonnouns usable in many different contexts (3).Similarly for VERBs (4b), we find suggested dis-tinctions among action (1) and belief/cognition (2)verbs.8 Further, some verb clusters are formed ofeventive verbs, both general (3) and domain-specific(0).
Another cluster is primarily of copula/auxiliaryverbs (7).
The remaining omitted categories appearto overlap, and only once we examine the contextsin which they occur do we see they are particularlyuseful for parsing FRAGs.Though NOUN and VERB clusters can be dis-cerned, there tends to be overlap among refinementsthat makes the analysis more difficult.
On the otherhand, refinements for PRON (4c) tend to be fairlyclean and it is generally simple to describe each: pos-sessives (1), personified wh-words (2) and generalwh-words (3).
Moreover, both subject (5) and ob-ject (6) are separately described.Promisingly, we learn interactions among variousrefinements in the form of TSG rules, as illustratedby Figures 4d-4g.
While all four examples involveVERBs it is enlightening to analyze a VERB?s re-finement and arguments.
For example, the refine-ments in 4d may lend a simple analysis of financialactions, while 4e may describe different NP interac-tions (note the different refinement symbols).
Dif-ferent VERB refinements may also coordinate, as in4f, where participle or gerund may help modify amain verb.
Finally, note how in 4g, an object pro-noun correctly occurs in object position.
These ex-amples suggest that even on coarsened POS tags, ourmethod is able to learn preliminary joint syntacticand lexical relationships.WSJ, ?2-3, Preterminals as X In this experiment,we investigate whether the manual annotations ofPetrov et al (2011) can be re-derived through firstreducing one?s non-terminal tagset to the symbolX and splitting until finding first the coarse grain8The next highest-ranked verbs for refinement 1 include re-ceived, doing and announced.28(a) Noun refinements.NOUN0 Corp Big Co.1 Mr. U.S. New2 Bush prices trading3 Japan September Nissan7 year % months(b) Verb refinements.VERB0 says said sell buy rose1 have had has been made2 said says say added believe3 sold based go trading filed7 is are be was will(c) Pronoun refinements.PRON1 its his your2 who whom ?3 what whose What5 it he they6 it them him(d) VP structure.VP0VERB0 NPADJ3 NOUN3(e) Declarative sentence.S0NP4 VPVERB1 NP1(f) Multiple VP interactions.VP0VPVERB7 ADVP0VPVERB0 NP0(g) Accusative use.VP0VERB0 NPPRON6Figure 4: Highest weighted representatives for lexical categories (4a-4c) and learned fragments (4d-4g), for uwsj2-3.X Universal Tag0 two market brain NOUN1 ?s said says VERB2 % company year NOUN3 it he they PRON5 also now even ADV6 the a The DET7 10 1 all NUM9 .
?
... .10 and or but CONJ12 which that who PRON13 is was are VERB14 as of in ADP15 up But billion ADPTable 2: Top-three representatives for various refine-ments of X, with reasonable analogues to Petrov et al(2011)?s tags.
Universal tag recovery is promising.tags of the universal set, followed by finer-grain tagsfrom the original treebank.
Due to the loss of lexi-cal information, we run our system for four iterationsrather than three.As observed in Table 2, there is strong overlapobserved between the induced refinements and theoriginal universal tags.
Though there are 16 refine-ments of X , due to lack of cluster coherence not allare listed.
Those tags and unlisted refinements seemto be interwoven in a non-trivial way.
We also seecomplex refinements of both open- and closed-classwords occurring: refinements 0 and 2 correspondwith the open-class NOUN, while refinements 3 and12, and 14 and 15 both correspond with the closedclasses PRON and ADP, respectively.
Note that 1and 13 are beginning to split verbs by auxiliaries.6 ConclusionWe have shown that TSGs may be encoded and in-duced within a framework of syntactic latent an-notations.
Results were provided for induction us-ing the English Penn, and Korean Treebanks, withfurther experiments based on the Universal Part ofSpeech tagset.
Examples shown suggest the promiseof our approach, with future work aimed at exploringlarger datasets using more extensive computationalresources.Acknowledgements Thank you to the reviewersfor helpful feedback, and to JohnsHopkinsHLTCOEfor providing support.
We would also like to thankByung Gyu Ahn for graciously helping us analyzethe Korean results.
Any opinions expressed in thiswork are those of the authors.ReferencesMohit Bansal and Dan Klein.
2010.
Simple, accurateparsing with an all-fragments grammar.
In Proceed-ings of ACL, pages 1098?1107.
Association for Com-putational Linguistics.Rens Bod.
1993.
Using an annotated corpus as a stochas-29tic grammar.
In Proceedings of EACL, pages 37?44.Association for Computational Linguistics.Rens Bod.
2001.
What is the minimal set of fragmentsthat achievesmaximal parse accuracy?
InProceedingsof ACL, pages 66?73.
Association for ComputationalLinguistics.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of AAAI, pages 598?603.Tagyoung Chung, Matt Post, and Daniel Gildea.
2010.Factors affecting the accuracy of korean parsing.
InProceedings of the NAACL HLT Workshop on Sta-tistical Parsing of Morphologically-Rich Languages(SPMRL), pages 49?57, Los Angeles, California,USA, June.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 548?556, Stroudsburg, PA,USA.
Association for Computational Linguistics.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Journal ofMachine Learning Research, 11:3053?3096, Decem-ber.Markus Dreyer and Jason Eisner.
2006.
Better informedtraining of latent syntactic features.
In Proceedings ofEMNLP, pages 317?326, Sydney, Australia, July.
As-sociation for Computational Linguistics.Francis Ferraro, Matt Post, and Benjamin Van Durme.2012.
Judging Grammaticality with Count-InducedTree Substitution Grammars.
In Proceedings of theSeventh Workshop in Innovated Use of NLP for Build-ing Educational Applications.Joshua Goodman.
1996.
Efficient algorithms for pars-ing the dop model.
In Proceedings of EMNLP, pages143?152.Na-Rae Han and Shijong Ryu.
2005.
Guidelines forPenn Korean Treebank.
Technical report, Universityof Pennsylvania.Chung-hye Han, Na-Rae Han, and Eon-Suk Ko.
2001.Bracketing guidelines for penn korean treebank.
Tech-nical report, IRCS, University of Pennsylvania.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In G. Rozenberg and A. Salo-maa, editors, Handbook of Formal Languages: Be-yond Words, volume 3, pages 71?122.
Springer.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1, pages 423?430, Stroudsburg, PA,USA.
Association for Computational Linguistics.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: The Penn Treebank.
Computationallinguistics, 19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InPro-ceedings of ACL, pages 75?82, Stroudsburg, PA, USA.Association for Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
InProceedings of ACL-ICCL,pages 433?440, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
In ArXiv, April.Matt Post and Daniel Gildea.
2009.
Bayesian learning ofa tree substitution grammar.
In Proceedings of ACL-IJCNLP (short papers), pages 45?48, Stroudsburg, PA,USA.
Association for Computational Linguistics.Andreas Zollmann and Khalil Sima?an.
2005.
A consis-tent and efficient estimator for data-oriented parsing.Journal of Automata Languages and Combinatorics,10(2/3):367.Willem Zuidema.
2007.
Parsimonious data-orientedparsing.
In Proceedings of EMNLP-CoNLL, pages551?560.30
