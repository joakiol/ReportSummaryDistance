Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1055?1065,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsMulti-domain Adaptation for SMT Using Multi-task Learning?Lei Cui1, Xilun Chen2, Dongdong Zhang3, Shujie Liu3, Mu Li3, and Ming Zhou31Harbin Institute of Technology, Harbin, P.R.
Chinaleicui@hit.edu.cn2Cornell University, Ithaca, NY, U.S.xlchen@cs.cornell.edu3Microsoft Research Asia, Beijing, P.R.
China{dozhang,shujliu,muli,mingzhou}@microsoft.comAbstractDomain adaptation for SMT usually adaptsmodels to an individual specific domain.However, it often lacks some correlationamong different domains where commonknowledge could be shared to improve theoverall translation quality.
In this paper, wepropose a novel multi-domain adaptation ap-proach for SMT using Multi-Task Learning(MTL), with in-domain models tailored foreach specific domain and a general-domainmodel shared by different domains.
The pa-rameters of these models are tuned jointly viaMTL so that they can learn general knowledgemore accurately and exploit domain knowl-edge better.
Our experiments on a large-scale English-to-Chinese translation task val-idate that the MTL-based adaptation approachsignificantly and consistently improves thetranslation quality compared to a non-adaptedbaseline.
Furthermore, it also outperforms theindividual adaptation of each specific domain.1 IntroductionDomain adaptation is an active topic in statisti-cal machine learning and aims to alleviate the do-main mismatch between training and testing data.Like many machine learning tasks, Statistical Ma-chine Translation (SMT) assumes that the data dis-tributions of training and testing domains are sim-ilar.
However, this assumption does not hold forreal world SMT systems since training data forSMT models may come from a variety of domains.The translation quality is often unsatisfactory when?This work was done while the first and second authors werevisiting Microsoft Research Asia.translating texts from a specific domain using a gen-eral model that is trained over a hotchpotch of bilin-gual corpora.
Therefore, domain adaptation is cru-cial for SMT systems to achieve better performance.Previous research on domain adaptation for SMTincludes data selection and weighting (Eck et al2004; Lu?
et al 2007; Foster et al 2010; Moore andLewis, 2010; Axelrod et al 2011), mixture mod-els (Foster and Kuhn, 2007; Koehn and Schroeder,2007; Sennrich, 2012; Razmara et al 2012), andsemi-supervised transductive learning (Ueffing etal., 2007), etc.
Most of these methods adapt SMTmodels to a specific domain according to testing dataand have achieved good performance.
It is naturalthat real world SMT systems should adapt the mod-els to multiple domains because the input may beheterogeneous, so that the overall translation qual-ity can be improved.
Although we can easily ap-ply these methods to multiple domains individually,it is difficult to use the common knowledge acrossdifferent domains.
To leverage the common knowl-edge, we need to devise a multi-domain adaptationapproach that jointly adapts the SMT models.Multi-domain adaptation has been proved quiteeffective in sentiment analysis (Dredze and Cram-mer, 2008) and web ranking (Chapelle et al 2011),where the commonalities and differences acrossmultiple domains are explicitly addressed by Multi-task Learning (MTL).
MTL is an approach thatlearns one target problem with other related prob-lems at the same time, using a shared feature repre-sentation.
The key advantage of MTL is to enableimplicit data sharing and regularization.
Therefore,it often leads to a better model for each task.
Anal-ogously, we expect that the overall translation qual-ity can be further improved by using an MTL-based1055TM1S1 S2 S3Multiple In-domainTranslation Models& Language ModelsOne General-domainTranslation Model& Language ModelT1LM1TM2LM2TM3LM3Domain-specificSMT SystemsIn-domainTraining Data T2 T3TMTL-based TuningEntireTraining DataTM-GLM-GTMNLMNTNSN............Figure 1: An example with N pre-defined domains, where T is the entire training corpus.
Ti is the in-domain trainingdata for the i-th domain selected from T using the bilingual cross-entropy based method (Axelrod et al 2011).
Thein-domain TMi and LMi are trained using the in-domain training data Ti.
The general-domain models TM-G and LM-G are trained using the entire training corpus T .
Si is the domain-specific SMT system for the i-th domain, leveragingthe in-domain models and the general-domain models as features.multi-domain adaptation approach.In this paper, we use MTL to jointly adapt SMTmodels to multiple domains.
Specifically, we de-velop multiple SMT systems based on mixture mod-els, where each system is tailored for one specificdomain with an in-domain Translation Model (TM)and an in-domain Language Model (LM).
Mean-while, all the systems share a same general-domainTM and LM.
These SMT systems are considered asseveral related tasks with a shared feature represen-tation, which fits well into a unified MTL frame-work.
With the MTL-based joint tuning, generalknowledge can be better learned by the general-domain models, while domain knowledge can bebetter exploited by the in-domain models as well.By using a distributed stochastic learning approach(Simianer et al 2012), we can estimate the fea-ture weights of multiple SMT systems at the sametime.
Furthermore, we modify the algorithm to treatin-domain and general-domain features separately,which brings regularization to multiple SMT sys-tems in an efficient way.
Experimental results haveshown that our method can significantly improve thetranslation quality on multiple domains over a non-adapted baseline.
Moreover, the MTL-based adap-tation also outperforms the conventional individualadaptation approach towards each domain.The rest of the paper is organized as follows: Theproposed approach is explained in Section 2.
Exper-imental results are presented in Section 3.
Section 4introduces some related work.
Section 5 concludesthe paper and suggests future research directions.2 The Proposed ApproachFigure 1 gives an example with N pre-defined do-mains to illustrate the main idea.
There are threesteps in the training phase.
First, in-domain train-ing data is selected according to the pre-defined do-mains (Section 2.1).
Second, in-domain models andgeneral-domain models are trained to develop thedomain-specific SMT systems (Section 2.2).
Third,multiple domain-specific SMT systems are tunedjointly by using an MTL-based approach (Section2.3).2.1 In-domain Data SelectionIn the first step, in-domain bilingual data is selectedfrom all the bilingual data to train in-domain TMs.We use the bilingual cross-entropy based approach(Axelrod et al 2011) to obtain the in-domain data:[HI?src(s)?HG?src(s)]+[HI?tgt(t)?HG?tgt(t)] (1)1056where {s,t} is a bilingual sentence pair in the entirebilingual corpus.
HI?xxx(?)
and HG?xxx(?)
repre-sent the cross-entropy of a string according to an in-domain LM and a general-domain LM, respectively.?xxx?
denotes either the source language (src) or thetarget language (tgt).
HI?src(s)?HG?src(s) is thecross-entropy difference of string s between the in-domain and general-domain source-side LMs, andHI?tgt(t) ?
HG?tgt(t) is the cross-entropy differ-ence of string t between the in-domain and general-domain target-side LMs.
This criterion biases to-wards sentence pairs that are like the in-domain cor-pus but unlike the general-domain corpus.
There-fore, the sentence pairs with lower scores (larger dif-ferences) are presumed to be better.Now, the question is how to find sufficient mono-lingual data to train in-domain LMs.
A straight-forward solution is to collect the data from the in-ternet.
There are a large number of monolingualwebpages with domain information from web por-tal sites1, which can be collected to train in-domainLMs.
In large-scale real world SMT systems, practi-cal domain adaptation techniques should target moredomains rather than just one due to heterogeneousinput.
Therefore, we use a web crawler to collectmonolingual webpages ofN domains from web por-tal sites, for both the source language and the tar-get language.
The statistics of web-crawled data isgiven in Section 3.1.
We use the web-crawled mono-lingual documents to train N in-domain source-sideLMs and N in-domain target-side LMs.
Addition-ally, we also train the source-side and target-sidegeneral-domain LMs with all the web-crawled doc-uments from different domains.
Finally, these in-domain and general-domain LMs are used to selectin-domain bilingual data for different domains ac-cording to Formula (1).2.2 SMT Systems with Mixture ModelsIn the second step, with the selected in-domain train-ing data, we develop SMT systems based on mix-ture models.
In particular, we use the mixture modelbased approach proposed by Koehn and Schroeder1Many web portal sites contain domain informationfor webpages, such as ?www.yahoo.com?
in English and?www.sina.com.cn?
in Chinese and etc.
The webpages are of-ten categorized by human editors into different domains, suchas politics, sports, business, etc.(2007).
Specifically, we have developed N SMTsystems for N domains respectively, where eachsystem is a typical log-linear model.
For each sys-tem, the best translation candidate f?
is given by:f?
= argmaxf{P (f |e)} (2)where the translation probability P (f |e) is given by:P (f |e) ?
?iwi ?
log ?i(f, e)=?j?Iwj ?
log ?j(f, e)?
??
?In-domain+?k?Gwk ?
log ?k(f, e)?
??
?General domain(3)where ?j(f, e) is the in-domain feature function andwj is the corresponding feature weight.
?k(f, e) isthe general-domain feature function and wk is thefeature weight.
The detailed feature description isas follows:In-domain features?
An in-domain TM, including phrase translationprobabilities and lexical weights for both direc-tions (4 features)?
An in-domain target-side LM (1 feature)?
word count (1 feature)?
phrase count (1 feature)?
NULL penalty (1 feature)?
Number of hierarchical rules used (1 feature)General-domain features?
A general-domain TM, including phrase trans-lation probabilities and lexical weights for bothdirections (4 features)?
A general-domain target-side LM (1 feature)The feature description indicates that each SMTsystem contains two TMs and two LMs.
The in-domain TMs are trained using the selected bilin-gual training data according to Formula (1), and thegeneral-domain TM is trained using the entire bilin-gual training data.
For the LMs, we re-use the target-side in-domain LMs and general-domain LM trained1057for data selection (Section 2.1).
Compared with anormal single-model system, the system with mix-ture models can balance the contributions from thegeneral-domain and in-domain knowledge.
Hence itpotentially benefits from both.2.3 MTL-based TuningIn the third step, the feature weights in multipledomain-specific SMT systems are estimated.
In-stead of tuning each domain-specific system sepa-rately, we treat different systems as related tasks andtune them jointly in an MTL framework.
There aretwo main reasons for MTL-based tuning:1.
Domain-specific translation tasks share thesame general-domain LM and TM.
MTL oftenleads to better performance by leveraging com-monalities among different tasks.2.
By enforcing that the general-domain LM andTM perform equally across different domains,MTL provides a kind of regularization to pre-vent over-fitting.Formally, the objective function of the proposedMTL-based approach is described as follows:minW{N?i=1Loss(Ei, e?
(Fi,wi))}(4)where N is the number of pre-defined domains.
{Fi,Ei} is the in-domain development dataset for thei-th domain.
Fi denotes the source sentences and Eidenotes the reference translations.
wi is a D-lengthfeature weight column vector for the i-th domain,where D is the dimension of the feature space.
W isa N -by-D matrix, representing [w1|w2| .
.
.
|wN ]T .e?
(Fi,wi) are the best translations obtained for Fiwith parameters wi.
Loss(?, ?)
denotes the loss be-tween the system?s output and the reference trans-lations.
The basic idea of the objective function isto minimize the sum of loss functions for all the do-mains, rather than one domain at a time.
Therefore,by adjusting the in-domain and general-domain fea-ture weights, the translation quality is expected to begood across different domains.To effectively tune SMT systems jointly, we mod-ify the asynchronous Stochastic Gradient Descend(SGD) Algorithm (Simianer et al 2012) to optimizeobjective function (4).
We follow the pairwise rank-ing approach with the perceptron algorithm (Shenand Joshi, 2005) to update feature weights.
Let atranslation candidate be denoted by its feature vectorv ?
RD, the pairwise preference for training is con-structed by ranking two candidates according to thesmoothed sentence-level BLEU (Liang et al 2006).For a preference pair v[j]=(v(1), v(2)) where v(1) ispreferred, a hinge loss is used:L(wi) = (?
?wi, v(1) ?
v(2)?
)+ (5)where (x)+ = max(0, x) and ?
?, ??
denotes the in-ner product of two vectors.
With the perceptron al-gorithm (Shen and Joshi, 2005), the gradient of thehinge loss is:?L(wi) ={v(2) ?
v(1) if?wi, v(1) ?
v(2)?
?
00 otherwise(6)The training instances for the discriminativelearning in pairwise ranking are made by comparingthe N-best list of the translation candidates scoredby the smoothed sentence-level BLEU (Liang et al2006).
Following Simianer et al(2012), the N-bestlist is divided into three bins: the top 10% (High),the middle 80% (Middle), and the last 10% (Low).These bins are used for pairwise ranking where thetranslation preference pairs are built between thecandidates in High-Middle, Middle-Low, and High-Low, but not the candidates within the same bin,which is shown in Figure 2.
The idea is to guar-antee that the ranker is more discriminative to preferthe good translations to the bad ones.High: 10%Middle: 80%Low: 10%N-best listFigure 2: Training instances for pairwise ranking.1058Algorithm 1 Modified Asynchronous SGD1: Distribute N domain-specific decoders to N ma-chines2: Initialize w1,w2, .
.
.
,wN ?
03: for epochs t?
0 .
.
.
T ?
1 do4: for all domains d ?
{1 .
.
.
N}: parallel do5: ud,t,0,0 = wd6: S = |Fd|7: for all i ?
{0 .
.
.
S ?
1} do8: Decode i-th sentence with ud,t,i,09: P = No.
of pairs built from the N-best list10: for all pairs v[j], j ?
{0 .
.
.
P ?
1} do11: ud,t,i,j+1 ?
ud,t,i,j ?
?
?L(ud,t,i,j)12: end for13: ud,t,i+1,0 ?
ud,t,i,P14: end for15: end for16: for all domains d ?
{1 .
.
.
N} do17: wd = ud,t,S,018: end for19: WG ?
[wG1 | .
.
.
|wGN ]T20: for all domains d ?
{1 .
.
.
N} do21: for k ?
1 .
.
.
|wGd | do22: wGd [k] =1N?Nn=1 WG[n][k]23: end for24: wd ?
[wIdwGd]25: end for26: end for27: return w1,w2, .
.
.
,wNOur modified algorithm is illustrated in Algorithm1.
Each column vector wi is further split into twoparts wIi and wGi , representing the In-domain andGeneral-domain feature weights respectively.
In Al-gorithm 1, we first distribute the domain-specificSMT decoders to different machines and initializethe feature weights (line 1-2).
Typically, the SGD al-gorithm runs in several iterations (In this study, weset the number of epochs T to 20) (line 3).
Multi-ple SMT decoders run in parallel and each decoderupdates its feature weights individually using its in-domain development data (line 4-15).
For each do-main, the domain-specific decoder translates eachin-domain development sentence and determines theN-best translations (line 4-8).
The preference pairsare built and used to update the parameters by gra-dient descent with ?
= 0.0001 (line 9-13).
Eachdomain-specific decoder translates its in-domain de-velopment data multiple times.
After each itera-tion, feature weights from all decoders are collected(line 16-19).
In contrast to the original algorithm(Simianer et al 2012), we only average the general-domain feature weights wG1 , .
.
.
,wGN , but do not av-erage the in-domain feature weights (line 20-25).The reason is we hope to leverage the commonalitiesamong these systems.
Meanwhile, general knowl-edge is enforced to be conveyed equally across dif-ferent domains.
Finally, the algorithm returns allthe domain-specific feature weights w1,w2, .
.
.
,wNthat are used for testing (line 27).After the joint MTL-based tuning, the featureweights tailored for domain-specific SMT systemsare used to translate the testing data.
We collect in-domain testing data for each domain to evaluate thedomain-specific systems.
Although this is not al-ways the case in real applications where the testingdomain is known, this study mainly focuses on theeffectiveness of the MTL-based tuning approach.3 Experiments3.1 DataWe evaluated our MTL-based domain adaptationapproach on a large-scale English-to-Chinese ma-chine translation task.
The training data consistedof two parts: monolingual data and bilingual data.The monolingual data was used to train the source-side and target-side LMs, both of which were usedfor data selection in Section 2.1.
In addition, thetarget-side LMs were re-used in the SMT systemsas features.
As mentioned in Section 2.1, we built aweb crawler to collect a large number of webpagesfrom web portal sites in English and Chinese respec-tively.
In the experiments, we mainly focused on sixpopular domains, namely Business, Entertainment,Health, Science & Technology, Sports, and Politics.For both English and Chinese webpages, the HTMLtags were removed and the main content was ex-tracted.
The data statistics are shown in Table 1.The bilingual data we used was mainly minedfrom the web using the method proposed by Jianget al(2009), with a post-processing step using ourbilingual data cleaning method (Cui et al 2013).Therefore, the data quality is pretty good.
In addi-tion, we also used the English-Chinese parallel cor-pus released by LDC2.
In total, the bilingual data2LDC2003E07, LDC2003E14, LDC2004E12,LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26,1059DomainEnglish ChineseDocs Words Docs WordsBusiness 21M 10.4B 7.91M 2.73BEnt.
18.3M 8.29B 4.16M 1.31BHealth 8.7M 4.73B 0.9M 0.42BSci&Tech 10.9M 5.33B 5.28M 1.6BSports 18.9M 9.58B 2.49M 0.59BPolitics 10.3M 5.56B 1.67M 0.39BTable 1: Statistics of web-crawled monolingual data, innumbers of documents and words (main content).
?M?refers to million and ?B?
refers to billion.contained around 30 million sentence pairs, with404M words in English and 329M words in Chi-nese.
For each domain, we used the cross-entropybased method in Section 2.1 to rank the entire bilin-gual data, and the top 10% sentence pairs from theranked bilingual data were selected as the in-domaindata to train the in-domain TM.
Moreover, we pre-pared 2,000 in-domain sentences for developmentand 1,000 in-domain sentences for testing in eachdomain.
The details are shown in Table 2.DomainTrain Dev TestEn Ch En Ch En ChBusiness 30M 28M 36K 35K 19K 19KEnt.
25M 22M 21K 18K 13K 12KHealth 23M 20M 33K 33K 21K 22KSci&Tech 28M 26M 46K 45K 27K 27KSports 19M 16M 18K 14K 10K 9KPolitics 28M 24M 19K 17K 13K 12KTable 2: Statistics of in-domain training, developmentand testing data, in number of words.3.2 SetupAn in-house hierarchical phrase-based SMT de-coder was implemented for our experiments.
TheCKY decoding algorithm was used and cube prun-ing was performed with the same default parametersettings as in Chiang (2007).
We used a 100-bestlist from the decoder for the pairwise ranking al-gorithm.
Translation models were trained over thebilingual data that was automatically word-alignedusing GIZA++ (Och and Ney, 2003) in both direc-tions, and the diag-grow-final heuristic was used toLDC2006E34, LDC2006E85, LDC2006E92.refine the symmetric word alignment.
The phrasetables were filtered to retain top-20 translation can-didates for each source phrase for efficiency.
Anin-house language modeling toolkit was used totrain the 4-gram language models with modifiedKneser-Ney smoothing (Kneser and Ney, 1995) overthe web-crawled data.
The evaluation metric forthe overall translation quality was case-insensitiveBLEU4 (Papineni et al 2002).
A statistical sig-nificance test was performed using the bootstrap re-sampling method (Koehn, 2004).3.3 BaselineWe have two baselines.
The first baseline is a non-adapted Hiero using our implementation.
It con-tained the general-domain TM and LM, as well asother standard features.
In addition, the fix-discountmethod (Foster et al 2006) for phrase table smooth-ing was also used.
The system was general-domainoriented and it was tuned by using MERT (Och,2003) with a combination of six in-domain develop-ment datasets.
The second baseline is Google OnlineTranslation Service3.
We obtained the English-to-Chinese translations of the testing data from GoogleTranslation to have a more solid comparison.Moreover, we also compared our method with theadapted systems towards each domain individually(Koehn and Schroeder, 2007).
This is to demon-strate the superiority of our MTL-based tuning ap-proach across different domains.3.4 ResultsThe end-to-end translation performance is shown inTable 3.
We found that the baseline has a similarperformance to Google Translation, with certain do-mains performed even better (Business, Sci&Tech,Sports, Politics).
This demonstrates that the transla-tion quality of our baseline is state-of-the-art.
More-over, we can answer three questions according to theexperimental results as follow:First, is domain mismatch a significant prob-lem for a real world SMT system?
We used thesame system only with general-domain TM and LM,but tuned towards each domain individually usingin-domain dev data.
Table 3 shows that the setting?
[A] G-TM + G-LM?
performs much better than3http://translate.google.com1060Business Ent.
Health Sci&Tech Sports Politics[N] Baseline (G-TM + G-LM) 27.19 17.87 25.79 25.34 25.53 23.01Google Translation 26.01 18.44 27.71 25.07 24.08 22.97[A] G-TM + G-LM 29.58 19.08 28.80 26.84 30.28 25.64[A] I-TM + I-LM 28.20 17.25 27.20 25.41 30.12 22.97[A] (G+I)-TM + G-LM 29.45 19.22 28.93 27.01 31.01 25.40[A] (G+I)-TM + I-LM 29.60 19.43 28.94 27.05 34.36 25.98[A] (G+I)-LM + G-TM 29.66 19.50 29.00 27.10 33.60 26.03[A] (G+I)-LM + I-TM 28.50 17.66 27.58 25.99 30.44 23.30[A] (G+I)-TM + (G+I)-LM 29.82 19.53 29.03 26.94 33.77 26.09[A,MTL] (G+I)-TM + (G+I)-LM 30.26 19.94 29.08 27.17 34.11 26.50Table 3: End-to-end experimental results (BLEU4%) with large-scale training data (p < 0.05).
?[N]?
means the systemis non-adapted and tuned using MERT on general-domain dev data.
?[A]?
denotes that the system is adapted towardseach domain individually using MERT on in-domain dev data.
?[A,MTL]?
indicates that the system was tuned usingour MTL-based approach on in-domain dev data.
?I-TM?
and ?G-TM?
denote the in-domain and general-domaintranslation model.
?I-LM?
and ?G-LM?
denote the in-domain and general-domain language model.
We also obtainedtranslations of the testing data using Google Translation for comparison.the non-adapted baseline across all domains with atleast 1.2 BLEU points.
In addition, the setting ?
[A]G-TM + G-LM?
also outperforms Google Transla-tion on all domains.
Analogous to previous research,this confirms that the domain mismatch indeed ex-ists and the parameter estimation using in-domaindev data is quite useful.Second, does the mixture models based adap-tation work for a variety of domains?
We experi-mented with different settings with multiple TMs orLMs, or both.
It is interesting to note that for large-scale SMT systems, using in-domain models aloneis inferior to using the general models alone.
Thesetting ?
[A] G-TM + G-LM?
is better than the set-ting ?
[A] I-TM + I-LM?
across different domains.The reason is the data for general models has alreadyincluded the in-domain data and the data coverage ismuch larger, thus the probability estimation is morereliable and the translation quality is much better.For the LM, the in-domain LM performs betterthan the general-domain LM because our mono-lingual data (Table 1) for each domain is alreadysufficient for training an in-domain LM with goodperformance.
From Table 3, we observed that thesetting ?
[A] (G+I)-TM + I-LM?
outperforms ?
[A](G+I)-TM + G-LM?, with the ?Sports?
domain be-ing the most significant.
For the TM, the per-formance of the in-domain TM is inferior to thegeneral-domain TM.
The results show that the set-ting ?
[A] (G+I)-LM + G-TM?
is significantly betterthan ?
[A] (G+I)-LM + I-TM?.
The main reason isthe data coverage for in-domain TM is much smallerthan the general model.
When each system uses twoTMs and two LMs, it consistently results in betterperformance, indicating that mixture models are cru-cial for domain adaptation in SMT.Third, can MTL further improve the transla-tion quality?
We used the MTL-based approach tojointly tune multiple domain-specific systems, lever-aging the commonalities among different but relatedtasks.
From Table 3, the MTL-based approach sig-nificantly improve the translation quality over thenon-adapted baseline, and also outperforms conven-tional mixture models based methods.
In particular,the ?Sports?
domain benefits the most from the in-domain knowledge, which confirms that domain dis-crepancy should be addressed and may bring largeimprovements on certain domains.3.5 DiscussionAccording to our experiments, only averaging overthe out-of-domain feature weights returned robustand converged results.
We do not have theoreti-cally grounded guarantee.
However, we observedthat the BLEU score of our method on DEV datawas slightly lower than that in the baseline system,which indicates the out-of-domain features are lessover-fitting on the domain-specific DEV data since1061SOURSE A point begins with a player serving the ball.
This means one player hitsthe ball towards the other player.
(The serve must be played from behind thebaseline and must::::land in the service box .
Players get two attempts to makea good serve.
)REF ?
?
?
?
????
?
?
?
?
?
?
?
?
????
?
?
??????(??????????????????::::?????
???
??????????????
)[N] Baseline (G-TM + G-LM) ????????????????????????????(???????????????:::::::???
???
????????????????
)[A] (G+I)-TM + (G+I)-LM ????????????????
????????(???????????
???
?:::::????????????????????
)[A,MTL](G+I)-TM + (G+I)-LM ?????????????
???????????(??????????????:::::::???
???
????????????????
)Table 4: Examples illustrating some different translations, where the Chinese phrases are translated from the Englishphrases with the same symbols (e.g., underline, wavy-line, and box).
The details are explained in Section 3.5.we enforced them to play the same role across dif-ferent domains.
It seems that averaging the out-of-domain feature weights can be considered as a kindof regularization.An example sentence from the Sports domainwith translations from different methods is shownin Table 4.
In this sentence, the baseline alwaystranslates ?player?
to ????
(game player), whichshould be ????
(ball player).
And, the base-line translates ?serve?
to ????
(work for), whichshould be ????
(put the ball into play).
The phrase?service box?
here means ????
?, which denotesthe zone where the ball is to be served.
However, thebaseline incorrectly splits them into two words, thentranslates ?service?
to ????
and ?box?
to ??
?.In contrast, the approaches with adapted models areable to translate these words very well.Both our MTL-based approach and the conven-tional adaptation methods leverage the mixture mod-els.
A natural question is why our MTL-based ap-proach performs better than the individual adapta-tion.
To answer this question, we looked into thedetails of the tuning and decoding procedures in theMTL-based approach.
We observed that the BLEUscore on the development data for each system waslower than the score when conducting individualadaptation.
Considering that the algorithm enforc-ing the general features play the same role acrossdifferent domains, we suspect that MTL-based ap-proach introduces a kind of regularization for eachdomain-specific system.
The regularization preventsthe general features from biasing towards certain do-mains to the extreme.
This property is quite impor-tant for real world SMT systems.
Usually, a sen-tence is composed of some domain-specific wordsand some general words, so it is often improper totranslate every word in the sentence using the in-domain knowledge.
For the example in Table 4,the individual adaptation method ?
[A] (G+I)-TM +(G+I)-LM?
translates ?land?
to ????
(zone) im-properly, because ????
appears more often in theSports text than the general-domain text.
This showsthat the individual adaptation methods tend to over-fit the in-domain development data.
In contrast, theMTL-based approach ?
[A,MTL](G+I)-TM + (G+I)-LM?
just translates ?land?
to ?????
(fall on),which is more appropriate.4 Related Work4.1 Domain AdaptationOne direction of domain adaptation explored thedata selection and weighting approach to improvethe performance of SMT on specific domains.
Eck1062et al(2004) first decoded the testing data with ageneral TM, and then used the translation resultsto train an adapted LM, which was in turn used tore-decode the testing data.
Lu?
et al(2007) triedto weight the training data according to the similar-ity with test data using information retrieval mod-els, while Foster et al(2010) trained a discrimina-tive model to estimate a weight for each sentencein the training corpus.
Other methods conducteddata selection based on cross-entropy (Moore andLewis, 2010), and Axelrod et al(2011) further ex-tended their cross-entropy based method to the se-lection of bilingual corpus in the hope that more rel-evant corpus to the target domain could yield smallermodels with better performance.
Other methodsincluded using semi-supervised transductive learn-ing techniques to exploit the monolingual in-domaindata (Ueffing et al 2007).Adaptation methods also involved the utiliza-tion of mixture models.
Foster and Kuhn (2007)explored a number of variants of utilizing multi-ple TMs and LMs by interpolation.
Koehn andSchroeder (2007) used MERT to simultaneouslytune two TMs or LMs.
Sennrich (2012) investi-gated the TM perplexity minimization as a methodto set model weights in mixture modeling.
In ad-dition, inspired by system combination approaches,Razmara et al(2012) used the ensemble decodingmethod to mix multiple translation models, whichoutperformed a variety of strong baselines.Generally, most previous methods merely con-ducted domain adaption for a single domain, ratherthan multiple domains at the same time.
One couldalso simply build multiple SMT systems that wereadapted to multiple domains, but they were oftenseparated and not tuned together.
So far, there hasbeen little research into the multi-domain adaptationproblem over mixture models for SMT systems, asproposed in this paper.4.2 Multi-task LearningIn machine learning, MTL is an approach to learnone target problem with other related problems atthe same time.
This often leads to a better model forthe main task because it allows the learner to use thecommonality among the tasks.
MTL is performedby learning tasks in parallel while using a sharedrepresentation.
Therefore, what is learned for eachtask can help other tasks be learned better.MTL was successfully applied in some Natu-ral Language Processing (NLP) tasks.
For exam-ple, Blitzer et al(2006) extended the MTL ap-proach (Ando and Zhang, 2005) to domain adapta-tion tasks in part-of-speech tagging.
Collobert andWeston (2008) proposed using deep neural networksto train a set of tasks, including part-of-speech tag-ging, chunking, named entity recognition, and se-mantic roles labeling.
They reported that jointlylearning these tasks led to superior performance.MTL was also applied in sentiment analysis (Dredzeand Crammer, 2008) and web ranking (Chapelleet al 2011) to address the multi-domain learningand adaptation.
In SMT, Duh et al(2010) pro-posed using MTL for N-best re-ranking on sparsefeature sets, where each N-best list corresponded toa distinct task.
Simianer et al(2012) proposed dis-tributed stochastic learning with feature selection in-spired by MTL.
The distributed learning approachoutperformed several other training methods includ-ing MIRA and SGD.Inspired by these methods, we used MTL to tunemultiple SMT systems at the same time, where eachsystem was composed of in-domain and general-domain models.
Through a shared feature represen-tation, the commonalities among the SMT systemswere better learned by the general models.
In ad-dition, domain-specific translation knowledge wasalso better characterized by the in-domain models.5 Conclusion and Future WorkIn this paper, we propose an MTL-based approach toaddress multi-domain adaptation for SMT.
We firstuse the cross-entropy based data selection methodto obtain in-domain bilingual data.
After that, in-domain TMs and LMs are trained for each domain-specific SMT system.
In addition, the general-domain TM and LM are also trained and sharedacross different systems.
Finally, MTL is lever-aged to tune multiple systems jointly.
Experimen-tal results have shown that our approach is quitepromising for the multi-domain adaptation problem,and it brings significant improvement over both thenon-adapted baselines and the conventional domainadaptation methods with mixture models.We assume the domain information for testing1063data is known beforehand in this study.
However,this is not always the case for real world SMT sys-tems.
Therefore, to apply our approach in real appli-cations, the domain information needs to be identi-fied automatically.
In the future, we will pre-definemore popular domains and develop automatic do-main classifiers.
For those domains that are iden-tified with high confidence, we use the domain-specific system to translate the texts.
For other texts,we use the general system to translate them.
Fur-thermore, since our approach is a general trainingmethod, we may also combine this approach withother domain adaptation methods to get more per-formance improvement.AcknowledgmentsWe are especially grateful to Nan Yang, YajuanDuan, Hong Sun and Danran Chen for the helpfuldiscussions.
We also thank the anonymous review-ers for their insightful comments.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
The Journal of Machine LearningResearch, 6:1817?1853.Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011.Domain adaptation via pseudo in-domain data selec-tion.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 355?362, Edinburgh, Scotland, UK., July.
As-sociation for Computational Linguistics.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 120?128, Sydney, Australia, July.
As-sociation for Computational Linguistics.Olivier Chapelle, Pannagadatta Shivaswamy, SrinivasVadrevu, Kilian Weinberger, Ya Zhang, and BelleTseng.
2011.
Boosted multi-task learning.
Machinelearning, 85(1-2):149?173.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference onMachine learn-ing, pages 160?167.
ACM.Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, and MingZhou.
2013.
Bilingual data cleaning for smt usinggraph-based random walk.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 340?345,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Mark Dredze and Koby Crammer.
2008.
Online methodsfor multi-domain learning and adaptation.
In Proceed-ings of the 2008 Conference on Empirical Methods inNatural Language Processing, pages 689?697, Hon-olulu, Hawaii, October.
Association for ComputationalLinguistics.Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, HidekiIsozaki, and Masaaki Nagata.
2010.
N-best rerankingby multitask learning.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation and Met-ricsMATR, pages 375?383, Uppsala, Sweden, July.Association for Computational Linguistics.Matthias Eck, Stephan Vogel, and Alex Waibel.
2004.Language model adaptation for statistical machinetranslation based on information retrieval.
In In Proc.of LREC.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages128?135, Prague, Czech Republic, June.
Associationfor Computational Linguistics.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable smoothing for statistical machinetranslation.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 53?61, Sydney, Australia, July.
Association forComputational Linguistics.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in statistical machine translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 451?459, Cambridge,MA, October.
Association for Computational Linguis-tics.Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, andQingsheng Zhu.
2009.
Mining bilingual data from theweb with adaptively learnt patterns.
In Proceedingsof the Joint Conference of the 47th Annual Meetingof the ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages870?878, Suntec, Singapore, August.
Association forComputational Linguistics.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Acous-tics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, volume 1,pages 181?184.
IEEE.1064Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 224?227, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 761?768, Syd-ney, Australia, July.
Association for ComputationalLinguistics.Yajuan Lu?, Jin Huang, and Qun Liu.
2007.
Improvingstatistical machine translation performance by train-ing data selection and optimization.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages343?350, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 220?224, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 160?167, Sapporo, Japan,July.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Majid Razmara, George Foster, Baskaran Sankaran, andAnoop Sarkar.
2012.
Mixing multiple translationmodels in statistical machine translation.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 940?949, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Rico Sennrich.
2012.
Perplexity minimization for trans-lation model domain adaptation in statistical machinetranslation.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 539?549, Avignon, France,April.
Association for Computational Linguistics.Libin Shen and Aravind K Joshi.
2005.
Ranking andreranking with perceptron.
Machine Learning, 60(1-3):73?96.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint feature selection in distributed stochastic learn-ing for large-scale discriminative training in smt.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 11?21, Jeju Island, Korea, July.
Asso-ciation for Computational Linguistics.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Transductive learning for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 25?32, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.1065
