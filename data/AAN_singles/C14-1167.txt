Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1773?1782, Dublin, Ireland, August 23-29 2014.Morphological Analysis for Japanese Noisy TextBased on Character-level and Word-level NormalizationSAITO Itsumi, SADAMITSU Kugatsu, ASANO Hisako and MATSUO YoshihiroNTT Media Intelligence Laboratories{saito.itsumi, sadamitsu.kugatsu,asano.hisako, matsuo.yoshihiro}@lab.ntt.co.jpAbstractSocial media texts are often written in a non-standard style and include many lexical variantssuch as insertions, phonetic substitutions, abbreviations that mimic spoken language.
The nor-malization of such a variety of non-standard tokens is one promising solution for handling noisytext.
A normalization task is very difficult to conduct in Japanese morphological analysis becausethere are no explicit boundaries between words.
To address this issue, in this paper we propose anovel method for normalizing and morphologically analyzing Japanese noisy text.
We generateboth character-level and word-level normalization candidates and use discriminative methods toformulate a cost function.
Experimental results show that the proposed method achieves accept-able levels in both accuracy and recall for word segmentation, POS tagging, and normalization.These levels exceed those achieved with the conventional rule-based system.1 IntroductionSocial media texts attract a lot of attention in the fields of information extraction and text mining.
Al-though texts of this type contain a lot of information, such as one?s reputation or emotions, they oftencontain non-standard tokens (lexical variants) that are considered out-of-Vocabulary (OOV) terms.
Wedefine an OOV as a word that does not exist in the dictionary.
Texts in micro-blogging services suchas Twitter are particularly apt to contain words written in a non-standard style, e.g., by lengtheningthem (?goooood?
for ?good?)
or abbreviating them (?thinkin?
?
for ?thinking?).
This is also seen in theJapanese language, which has standard word forms and variants of them that are often used in socialmedia texts.
To take one word as an example, the standard form is????
(oishii, ?It is delicious?)
andits variants include ???????
(oishiiiii), ???
(?oishii), and ????
(oishii), where the un-derlined characters are the differences from the standard form.
Such non-standard tokens often degradethe accuracy of existing language processing systems, which are trained using a clean corpus.Almost all text normalization tasks for languages other than Japanese (e.g., English), aim to replacethe non-standard tokens that are explicitly segmented using the context-appropriate standard words (Hanet al.
(2012), Han and Baldwin (2011), Hassan and Menezes (2013), Li and Liu (2012), Liu et al.
(2012),Liu et al.
(2011), Pennell and Liu (2011), Cook and Stevenson (2009), Aw et al.
(2006)).
On the otherhand, the problem is more complicated in Japanese morphological analysis because Japanese words arenot segmented by explicit delimiters.
In traditional Japanese morphological analysis, word segmentationand part-of-speech (POS) tagging are simultaneously estimated.
Therefore, we have to simultaneouslyanalyze normalization, word segmentation, and POS tagging to estimate the normalized form using thecontext information.
For example, the input ???????
???
(pan-keiki oishiiii, ?This pancaketastes good?)
written in the standard form is?????????
(pan-keiki oishii).
The result obtainedwith the conventional Japanese morphological analyzer MeCab (Kudo (2005)) for this input is?????
(pancake, noun)/?
??
(unk)/?
(unk)/?
(unk)/, where slashes indicate the word segmentations and?unk?
means an unknown word.
As this result shows, Japanese morphological analyzers often fail toThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1773correctly estimate the word segmentation if there are unknown words, so the pipeline method (e.g., firstestimating the word segmentations and then estimating the normalization forms) is unsuitable.Moreover, Japanese has several writing scripts, the main ones being Kanji, Hiragana, and Katakana.Each word has its own formal written script (e.g., ???
(kyoukasyo, ?textbook?)
as formally writtenin Kanji), but in noisy text, there are many words that are intentionally written in a different script(e.g., ??????
(kyoukasyo, ?textbook?)
is the Hiragana form of???).
These tokens written indifferent script also degrade the performance of existing systems because dictionaries basically includeonly the standard script.
Unlike the character-level variation we described above, this type of variationoccurs on a word?level one.
Therefore, there are both character-level and word-level non-standardtokens in Japanese informal written text.
Several normalization approaches have been applied to Japanesetext.
Sasano et al.
(2013) and Oka et al.
(2011) introduced simple character level derivational rules forJapanese morphological analysis that are used to normalize specific patterns of non-standard tokens, suchas for word lengthening and lower-case substitution.
Although these approaches handle Japanese noisytext fairly effectively, they can handle only limited kinds of non-standard tokens.We propose a novel method of normalization in this study that can handle both character- and word-level lexical variations in one model.
Since it automatically extracts character-level transformation pat-terns in character-level normalization, it can handle many types of character-level transformations.
Ituses two steps (character- and word-level) to generate normalization candidates, and then formulates acost function of the word sequences as a discriminative model.
The contributions this research makescan be summarized by citing three points.
First, the proposed system can analyze a wider variety ofnon-standard token patterns than the conventional system by using our two-step normalization candidategeneration algorithms.
Second, it can largely improve the accuracy of Japanese morphological analysisfor non-standard written text by simultaneously performing the normalization and morphological analy-ses.
Third, it can automatically extract character alignments and in so doing reduces the cost of manuallycreating many types of transformation patterns.
The rest of this paper is organized as follows.
Section 2describes the background to our research, including Japanese traditional morphological analysis, relatedwork, and data collection methods.
Section 3 introduces the proposed approach, which includes latticegeneration and formulation, as a discriminative model.
Section 4 discusses experiments we performedand our analyses of the experimental results.
Section 5 concludes the paper with a brief summary and amention of future work.2 Background2.1 Japanese Morphological AnalysisMany approaches to joint word segmentation and POS tagging including Japanese Morphological anal-ysis can be interpreted as re-ranking while using a word lattice (Kaji and Kitsuregawa (2013)).
Thereare two points to consider in the analysis procedure: how to generate the word lattice and how to formu-late the cost of each path.
In Japanese morphological analysis, the dictionary-based approach has beenwidely used to generate the word lattice (Kudo et al.
(2004), Kurohashi et al.
(1994)).
In a traditionalapproach, an optimal path is sought by using the sum of the two types of costs for the path: the costfor a candidate word that reflects the word?s occurrence probability, and the cost for a pair of adjacentPOS that reflects the probability of an adjacent occurrence of the pair (Kudo et al.
(2004), Kurohashi etal.
(1994)).
A greater cost means less probability.
The Viterbi algorithm is usually used for finding theoptimal path.2.2 Related WorkSeveral studies have been conducted on Japanese morphological analysis in the normalized form.
Theapproach proposed by Sasano et al.
(2013) aims to develop heuristics to flexibly search by using a simple,manually created derivational rule.
Their system generates normalized character sequence based on thederivational rule, and adding new nodes that are generated from normalized character sequence whengenerating the word lattice using dictionary lookup.
Figure 1 presents an example of this approach.If the non-standard written sentence ???????
(suugoku tanoshii, ?It is such fun?)
is input, the1774Figure 1: Example of Japanese morphological analysis and normalizationtype non-standard form standard form(1) Insertion ???????
(arigatoou) ?????
(arigatou, ?Thank you?
)(2) Deletion ??
(samu) ???
(samui, ?cold?
)(3) Substitution with phonetic variation ????
(kawaee) ????
(kawaii, ?cute?
)(4) Substitution with lowercases and uppercases ?????
(arigatou) ?????
(arigatou, ?Thank you?
)(5) Hiragana substitution ?????
(aidei) ID (aidei, ?identification card?
)(6) Katakana substitution ?????
(arigatou) ?????
(arigatou, ?Thank you?
)(7) Any combination of (1) to (6) ?????
(kaunta) ?????
(kaunta, ?counter?)????
(attsui) ???
(atsui, ?hot?
)Table 1: Types of non-standard tokens and examples of annotated datatraditional dictionary-based system generates Nodes that are described using solid lines, as shown in Fig.1.
Since ??????
(suugoku, ?such?)
and ?????
(tanoshii, ?fun?)
are OOVs, the traditional systemcannot generate the correct word segments or POS tags.
However, their system generates additionalnodes for the OOVs, shown as broken line rectangles in Fig.
1.
In this case, derivational rules thatsubstitute ???
with ?null?
and ???
(i) with ???
(i) are used and the system can generate the standardforms ?????
(sugoku, ?such?)
and ?????
(tanoshii, ?fun?)
and their POS tags.
If we can generatesufficiently appropriate rules, these approaches seem to be effective.
However, there are many types ofderivational patterns in SNS text and it is difficult to cover all of them by hand.
Moreover, it becomes aserious problem how to set the path cost for appropriately re-ranking the word lattice when the numberof candidates increases.
Our approach is also based on the dictionary-based approach, however, ourapproach is significantly dissimilar from their approach in two ways.
First, we automatically generatederivational patterns (we call them transformation tables) based on the character-level alignment betweennon-standard tokens and their standard forms.
Compared to generating the rules by hand, our approachcan generate broad coverage rules.
Second, we use discriminative methods to formulate a cost function.Jiang et al.
(2008), Kaji and Kitsuregawa (2013) introduce several features to appropriately re-rank theadded nodes.
This enables our system to perform well even when the number of candidates increases.On the other hand, several studies have applied a statistical approach.
For example, Sasaki et al.
(2013) proposed a character-level sequential labeling method for normalization.
However, it handlesonly one-to-one character transformations and does not take the word-level context into account.
Theproposed method can handle many-to-many character transformations and takes word-level context intoaccount, so the scope for handling non-standard tokens is different.
Many studies have been done on textnormalization for English; for example Han and Baldwin (2011) classifies whether or not OOVs are non-standard tokens and estimates standard forms on the basis of contextual, string, and phonetic similarities.In these studies it was assumed that clear word segmentations existed.
However, since Japanese is anunsegmented language the normalization problem needs to be treated as a joint normalization, wordsegmentation, and POS tagging problem.2.3 Data Collection and Analysis of Non-standard TokensIn previous studies (Hassan and Menezes (2013), Ling et al.
(2013), Liu et al.
(2011)), the researchersproposed unsupervised ways to extract non-standard tokens and their standard forms.
For Japanese text,however, it is very difficult to extract word pairs in an unsupervised way because there is no clear wordsegmentation.
To address this problem we first extracted non-standard tokens from Twitter text and blog1775Figure 2: Structure of proposed systemFigure 3: Example of candidate generationtext and manually annotated their standard (dictionary) forms.
In total, we annotated 4808 tweets and8023 blog text sentences.
Table 1 lists the types of non-standard tokens that we targeted in this studyand examples of the annotated data.
Types (1), (2), (3) and (4) are similar to English transform patterns.Types (5) and (6) are distinctive patterns in Japanese.
As previously mentioned Japanese has severalkinds of scripts, the main ones being Kanji, Hiragana, and Katakana.
These scripts can be used to writethe same word in several ways.
For example, the dictionary entry ??
(sensei, ?teacher?)
can alsobe written in Hiragana form ????
(sensei) or Katakana form ????
(sensei).
Most words arenormally written in the standard form, but in informal written text (e.g., Twitter text), these same wordsare often written in a non-standard form.
In examining Twitter data for such non-standard tokens, wefound that 55.0% of them were types (1) to (3) in Table 1, 4.5% were type (4), 20.1% were types (5)to (6), 2.7% were type (7), and the rest did not fall under any of these types since they were the resultof dialects, typos, and other factors.
In other words, a large majority of the non-standard tokens fellunder types (1) to (7).
We excluded those that did not as targets in this study because our proposedmethod cannot easily handle them.
Types (1) to (4) occur at character-level and so can be learned fromcharacter-level alignment, but types (5) to (6) occur at word-level and it is inefficient to learn them ona character?level basis.
Accordingly, we considered generating candidates and features on two levels:character-level and word-level.3 Proposed Method3.1 Overview of Proposed SystemWe showed the structure of the proposed system in Fig.
2.
Our approach adds possible normalizationcandidates to a word lattice and finds the best sequence using a Viterbi decoder based on a discriminativemodel.
We introduced several features that can be used to appropriately evaluate the confidence of theadded nodes as normalization candidates.
We generate normalization candidates as indicated in Fig.
3.1776Figure 4: Example of character alignmentWe describe the details in the following section.3.2 Character-level Lattice3.2.1 Character Alignment between Non-standard Tokens and Their Normalized FormsWe have to create a character-level transformation table to generate the character-level lattice.
We usedthe joint multigram model proposed by Sittichai et al.
(2007) to create the transformation table becausethis model can handle many-to-many character alignments between two character sequences.
In ob-serving non-standard tokens and their standard forms, we find there are not only one-to-one charactertransformations but also many-to-many character transformations.
Furthermore, unlike in translation,there is no character reordering so the problems that arise are similar to those in transliteration.
Accord-ingly, we adopted a joint multigram model that is widely used for transliteration problems.
The optimalalignment can be formulated as q?
= arg maxq?Kd?q?qp(q) , where d is a pair of non-standard tokensand its standard form (e.g., d is??????
(arigatoou), ?????
(arigatou).
Here, q is a partialcharacter alignment in d (e.g., q is ???
?, ???
), q is the character alignment q set in d (e.g., q ofpath 1 in Fig.
4 is {(?
?, ??
), (?
?, ??
), (?
?, ??
), (???
?, ???)}.
Kdis the possible characteralignment sequence candidates generated from d. We generate n-best optimal path for Kdin this study.The maximum likelihood training can be performed using the EM algorithm derivated in Bisani and Ney(2008) and Kubo et al.
(2011) to estimate p(q).
p(q) can be formulated as follow:p(q) = ?q/?q?Q?q(1)?q=?d?D?q?Kdp(q)nq(q) =?d?D?q?Kd?q?qp?(q)?q?Kd?q?qp?
(q)nq(q),and where D is the number of the d pair, Q is the set of q, and nq(q) is the count of q that occurred inq.
In our system, we allow for standard form deletions (i.e., mapping of a non-standard character to anull standard character) but not non-standard token deletions.
Since we use this alignment as the trans-formation table when generating a character-level lattice, the lattice size becomes unnecessarily largeif we allow for non-standard form deletions.
In the calculation step of the EM algorithm, we calculatethe expectation (partial counts) ?qof each alignment in the E-step, calculate the joint probability p(q)that maximizes the likelihood function in the M-step as described before, and repeat these steps untilconvergence occurs.
p?
(q) indicates the result of p(q) calculated in the previous step over the iteration.When generating the character-level lattice, we used alignments that were expected to exceed a prede-fined threshold.
We used ?q(q = (ct, cv)) and r(ct, cv) as thereshold, where ctand cvare the partialcharacter sequence of non-standard token and it?s standard form respectively.
r(ct, cv) is calculated byr(ct, cv) = ?q/ncv., where ncvis the number of occurrences of cvin the training data.
We set the thresh-old ?q thres= 0.5 , and r(ct, cv)thres= 0.0001 in this study.
We also used r(ct, cv) as a feature of cost1777function in subsection.
3.4.2.
When calculating initial value, we set p(ct, cv) high if the character ctandcvare the same character and the length of each character is 1.
We also give the limitation that a Kanjicharacter does not change to a different character and is aligned with same character in the calculationstep of the character alignment.3.2.2 Generation of Character-level Lattice Based on Transformation TableFirst, repetitions of more than one letter of ??
?, ??
?, ?-?, and ???
are reduced back to one letter (e.g.,????????
(arigatooooou, ?Thank you?)
is reduced to ??????
(arigatoou)) for theinput text.
In addition, repetitions of more than three letters other than ??
?, ??
?, ?-?, and ???
arereduced back to three letters (e.g.,????????
(uresiiiiiii, ?I?m happy?)
is reduced back to??????
(uresiiii)).
These preprocessing rules are inspired by Han and Baldwin (2011) and determinedby taking the Japanese characteristics into consideration.
We also used these rules when we estimated thealignments of the non-standard tokens and their standard forms.
Next, we generate the character-levelnormalization candidates if they match the key transformation table in the input text.
For example, if thetransformation table contains (q, logp(q))= (???
(yoo), ??
(you)?, -8.39), (??
(o), ?
(o)?, -7.56),and the input text includes the character sequence ????
?
(tyoo), we generate a new sequence ?????
(tyou) and ?????
(tyoo).
In other words, we add new nodes ????
(you) and ???
(o) in the positionof ???
?
(yoo) and ???
(o), respectively (see Fig.
3).3.3 Generation of Word-level LatticeWe generate the word lattice based on the generated character-level lattice using dictionary lookup.
Weexploit dictionary lookup by using the possible character sequence of the character-level lattice whilethe traditional approach exploits it by using only the input character sequence.
For example, we exploitdictionary lookup for character sequences such as ????
?????
(tyoo kawaii) and ?????????
(tyou kawaii) and ?????????
(chiyou kawaii) and ????
?????
(tyoo kawaii) (see Fig.
3)Furthermore, we use the phonetic information of the dictionary to generate the normalization candi-dates for Hiragana and Katakana substitution.
For example, assume ???
(tyou, ?super?)
and ??????
(kawaii, ?cute?)
are the dictionary words.
Then, if the input text contains the character sequences ?????
(tyo) (which is written in Hiragana) and ??????
(kawaii) (which is written in Katakana), we add???
(tyo, ?super?)
and ??????
(kawaii, ?cute?)
to the word lattice as the normalization candidatessince the two character sequences are pronounced identically.
By using this two-step algorithm, we canhandle any combinational derivational patterns, such as Katakana substitutions or substitutions of lower-cases like ??????
(kawaii)?
??????
(kawaii)?
??????
(kawaii, ?cute?)
(see Fig.
3).
Notethat we filtered candidates on the basis of a predefined threshold to prevent the generation of unneces-sary candidates.
The threshold was defined on the basis of the character sequence cost of normalization,which is described in subsection 3.4.2.
Furthermore, we limited the number of character transformationsto two per word.3.4 Decoder3.4.1 Objective FunctionThe decoder selects the optimal sequence y?
from L(s) when given the candidate set L(s) for sentences.
This is formulated as y?
= arg miny?L(s)w ?
f(y) (Jiang et al.
(2008), Kaji and Kitsuregawa (2013)), wherey?
is the optimal path, L(s) is the lattice created for sentence s, and w ?
f(y) is the dot product betweenweight vector w and feature vector f(y).
The optimal path is selected according to the w ?
f(y) value.3.4.2 FeaturesThe proposed lattice generation algorithm generates a lattice larger than that generated in traditionaldictionary-based lattice generation.
Therefore, we need to introduce an appropriate normalization costinto the objective function.
We listed the features we used in Table 2.
Let wibe the ith word candidateand pibe the POS tag of wi.
pi?1andwi?1are adjacent POS tag and word respectively.
We also used theword unigram cost fwipi, the cost for a pair of adjacent POS fpi?1,pithat are quoted from MeCab (Kudo,1778Name FeatureWord unigram cost fwipiPOS bi-gram cost fpi?1,piWord-POS bi-gram cost ?logpwi?1pi?1,wipiCharacter sequence cost log(p?s/p?ti)where, p?x= p1/length(x)x, px=?nj=1p(cj|cj?1j?5), x ?
{s, ti}Character transformation cost ?transi?
(?logr(ct, cv))Hiragana substitution cost ?hi?
fwipiKatakana substitution cost ?ki?
fwipiTable 2: Feature list of the decoder.
?transiis 1 if wiis generated by character transformation, otherwise0.
?hiis 1 ifwiis generated by Hiragana substitution, otherwise 0.
?kiis 1 ifwiis generated by Katakanasubstitution, otherwise 0.2005), and five additional types of costs.
These are the word-pos bi-gram cost ?logpwi?1pi?1,wipiof ablog corpus; the character transformation cost ?transi?
(?logr(ct, cv)), which is calculated in Section3.2,for nodes generated by character transformation; the Hiragana substitution cost ?hi?
fwipifor nodesgenerated by Hiragana substitution; the Katakana substitution cost ?ki?
fwipifor nodes generated byKatakana substitution; and the character sequence cost log(p?s/p?ti) for all the normalized nodes.
Thecharacter sequence cost reflects the character sequence probability of the normalization candidates.
Here,s and tiare input string and transformed string respectively.
(e.g., In Fig.
3, for the normalized node??????
(cute, adjective), s is ????
?????
and tiis ????
?????).
Then psand ptiarecalculated by using the character 5-gram of a blog corpus, which is formulated by ps= p(c1?
?
?
cn) =?nj=1p(cj|cj?1j?5), where cjis the j th character of character sequence s. p?tiand p?sare normalized byusing the length of each string s and tias p?ti= p1/length(ti)ti.
We set the threshold (p?s/p?ti)thres= 1.5for generating a Hiragana or Katakana normalization candidate in this study.
Since all those features canbe factorized, the optimal path is searched for by using the Viterbi algorithm.3.4.3 TrainingWe formulated the objective function for tuning weights w by using Eq.
2.
The weights w are trainedby using the minimum error rate training (MERT) Machery et al.
(2008).
We defined the error functionas the differences between the reference word segmentations and the POS tags of the reference sequenceyrefand the system output arg miny?L(s)w ?
f(y).w?
= arg minw?WN?i=1error(yref, arg miny?L(s)w ?
f(y)) (2)4 Experiments4.1 Dataset and Estimated Transformation TableWe conducted experiments to confirm the effectiveness of the proposed method, in which we annotatedcorpora of a Japanese blog and Twitter.
The Twitter corpus was split into three parts: the training, devel-opment, and test sets.
The test data comprised 300 tweets, development data comprised 500 sentencesand the training data comprised 4208 tweets.
We randomly selected the test data which contained at leastone non-standard token.
The test data comprised 4635 words, 403 words of them are non-standard tokenand are orthographically transformed into normalized form and POS tags.
The blog corpus comprised8023 sentences and all of them were used as training data.
Training data was used for extracting char-acter transformation table and development data was used for estimating parameters of discriminativemodel.
We used the IPA dictionary provided by MeCab to generate the word-level lattice and extractedthe dictionary-based features.
We itemized the estimated character transformation patterns in Table 3.There were 5228 transformation patterns that were learned from the training data and we used 3268 ofthem, which meets the predefined condition.
The learned patterns cover most of the previously pro-1779non-standardcharacter ctstandardcharacter cvlogp(q)non-standardcharacter ctstandardcharacter cvlogp(q)?
null -4.233 ??
(ssu) ??
(desu) -5.999??
(maa) ??
(maa) -5.059 ??
(doo) ??
(dou) -6.210??
(syo) ???
(syou) -5.211 ??
(nee) ??
(nai) -6.232??
(daro) ???
(darou) -5.570 ??
(rya) ??
(reha) -6.492?
(ttsu) null -5.648 ??
(ten) ??
(teru) -6.633??
(nto) ???
(ntou) -5.769 ??
(yuu) ??
(iu) -6.660?
(wa) ?
(wa) -5.924 ??
(nan) ??
(nano) -6.706Table 3: Example of character-level transformation tableposed rules.
In addition, our method can learn more of the variational patterns that are difficult to createmanually.4.2 Baseline and Evaluation MetricsWe compared the five methods listed in Table 4 in our experiments.
Traditional means that which gen-erates no normalization candidates and only uses the word cost and the cost for a pair of adjacent POS,so we can consider it as a traditional Japanese morphological analysis.
We compared three baselines,Baseline1, Baseline2 and Baseline3.
Baseline1 is the conventional rule-based method (considering in-sertion of long sound symbols and lowercases, and substitution with long sound symbols and lower-cases), which was proposed by Sasano et al.
(2013).
In Baseline2, 3, and Proposed, we basically usethe proposed discriminative model and features, but there are several differences.
Baseline2 only gen-erates character-level normalization candidates.
Baseline3 uses our two-step normalization candidategeneration algorithms, but the character transformation cost of all the normalization candidates that aregenerated by character normalization is the same.
Proposed generates the character-level and Hiraganaand Katakana normalization candidates and use all features we proposed.We evaluated each method on the basis of precision and recall and the F-value for the overall systemaccuracy.
Since Japanese morphological analysis simultaneously estimates the word segmentation andPOS tagging, we have to check whether or not our system is negatively affected by anything other than thenon-standard tokens.
We also evaluated the recall with considering only normalized words.
That valuedirectly reflects the performance of our normalization method.
We registered emoticons that occurred inthe test data in the dictionary so that they would not negatively affect the systems?
performance.4.3 Results and DiscussionThe results are classified in Table 4.
As the table shows, the proposed methods performed statisticallysignificantly better than the baselines and the traditional method in both precision and recall (p < 0.01),where the precision was greatly improved.
This indicates that our method can not only correctly analyzethe non-standard tokens, but can also reduce the number of wrong words generated.
Baseline1 alsoimproved the accuracy and recall compared to the traditional method, but the effect was limited.
Whenwe compare Proposed with Baseline2, we find the F-value is improved when we take the Hiraganaand Katakana substitution into consideration.
Baseline3 also improved the F-value but its performance isinferior to proposed method.This proves that even if we can generate sufficient normalization candidates,the results worsen if the weight parameter of each normalization candidate is not appropriately tuned.
Thecolumn of ?recall??
in Table 4 specifies the improvement rates of the non-standard tokens.
The proposedmethods improve about seven times when using Baseline1 while preventing degradation.
These resultsprove that we have to generate appropriate and sufficient normalization candidates and appropriately tunethe cost of each candidate to improve both the precision and recall.We show examples of the system output in Table 5.
In the table, slashes indicate the position of theestimated word segmentations and the words that were correctly analyzed are written in bold font.
Exam-ples (1) to (5) are examples improved by using the proposed method.
Examples (6) to (7) are examplesthat were not improved and example (8) is an example that was degraded.
Examples (1) to (3) includephonetic variations and example (4) is a Hiragana substitution.
Example (5) is a combinational trans-1780word segmentation word segmentation and POS tagmethod precision recall F-value precision recall F-value recall?Traditional 0.716 0.826 0.767 0.683 0.788 0.732 -Rule based (BL1??)
0.753 0.833 0.791 0.717 0.794 0.754 0.092Proposed 0.856 0.883 0.869 0.822 0.849 0.835 0.667- without Hiragana and Katakana normalization (BL2) 0.834 0.875 0.854 0.798 0.838 0.818 0.509- character transformation cost is fixed (BL3) 0.838 0.865 0.851 0.807 0.834 0.821 0.533?
considering only normalized words, ??
BL:baselineTable 4: Results of precision and recall of test datainput traditional proposed gold standard(1)???
(adii) ?
(a)/?
(di)/?
???
(atsui) ???
(atsui, ?hot?)(2)???
(sugee) ??
(suge)/?
???
(sugoi) ???
(sugoi, ?great?)(3)?????
(gommeen) ?
(go)/?/?
(me)/?/?
(n)/ ???
(gomen) ???
(gomen, ?I?m sorry?)(4)????
(hitsuyou) ??
(hitsu)/??
(you) ??
(hitsuyou) ??
(hitsuyou, ?necessary?)(5)?????
(daichuki) ?
(da)/??
(ichi)/?(yu)/?
(ki)/ ???
(daisuki) ???
(daisuki, ?like very much?)(6)?????
(oseee) ??
(ose)/??
(ee)/?
(e) ??
(ose) ???
(osoi, ?slow?)(7)?????
(kanwaii) ??
(kan)/?
(wa)/??
(ii) ??
(kanwa)/??
(ii) ????
(kawaii, ?cute?)(8)???
(inai) ?
(i)/??
(nai) ??
(inai) ?/??
(i/nai, ?absent?
)Table 5: System output examplesformation pattern of a phonetic variation and Hiragana substitution.
We can see our system can analyzesuch variational non-standard tokens for all these examples.
Two types of errors were identified.
The firstoccurred as the result of a lack of a character transformation pattern and the second was search errors.Example (6) shows an example of a case in which our system couldn?t generate correct normalizationcandidate because there was not corresponding character transformation pattern, even though there wasa similar phonetic transformation pattern.
To ensure there will be no lack of transformation patterns,we should either increase the parallel corpus size to enable the learning of more patterns or derive newtransformation patterns from the learned patterns.
Example (7) shows an example of a case in which anormalized candidate was generated but a search failed to locate it.
Example (8) shows an example of acase in which the result was degraded.
Our system can control the degradation well, but there are severaldegradation caused by normalization.
We will need to develop a more complicated model or introduceother features into the current model to reduce the number of search errors.5 Conclusion and Future WorkWe introduced a text normalization approach into joint Japanese morphological analysis and showed thatour two-step lattice generation algorithm and formulation using discriminative methods outperforms theprevious method.
In future work, we plan to extend this approach by introducing an unsupervised orsemi-supervised parallel corpus extraction for learning character alignments to generate more patternsat a reduced cost.
We also plan to improve our model?s structure and features and implement it with adecoding method to reduce the number of search errors.
In addition, we should consider adding othertypes of unknown words (such as named entities) to the morphological analysis system to improve itsoverall performance.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
A phrase-based statistical model for sms text normalization.Proceedings of the COLING/ACL on Main Conference Poster Sessions, pages 33?40.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conversion.Speech Commun., 50(5):434?451, May.Paul Cook and Suzanne Stevenson.
2009.
An unsupervised model for text message normalization.
Proceedingsof the Workshop on Computational Approaches to Linguistic Creativity, pages 71?78.1781Bo Han and Timothy Baldwin.
2011.
Lexical normalisation of short text messages: Makn sens a #twitter.
Pro-ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-nologies - Volume 1, pages 368?378.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Automatically constructing a normalisation dictionary formicroblogs.
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processingand Computational Natural Language Learning, pages 421?432.Hany Hassan and Arul Menezes.
2013.
Social text normalization using contextual graph random walks.
Proceed-ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),pages 1577?1586, August.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008.
Word lattice reranking for chinese word segmentation and part-of-speech tagging.
Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1,pages 385?392.Nobuhiro Kaji and Masaru Kitsuregawa.
2013.
Efficient word lattice generation for joint word segmentationand pos tagging in japanese.
Proceedings of the Sixth International Joint Conference on Natural LanguageProcessing, pages 153?161.Keigo Kubo, Hiromichi Kawanami, Hiroshi Saruwatari, and Kiyohiro Shikano.
2011.
Unconstrained many-to-many alignment for automatic pronunciation annotation.
In Proc.
of APSIPA ASC.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004.
Applying conditional random fields to japanesemorphological analysis.
In Proc.
of EMNLP, pages 230?237.T.
Kudo.
2005.
Mecab : Yet another part-of-speech and morphological analyzer.
http://mecab.sourceforge.net/.Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao.
1994.
Improvements of japanesemorphological analyzer juman.
In Proc.
of The International Workshop on Sharable Natural Language Re-sources, page 22?38.Chen Li and Yang Liu.
2012.
Improving text normalization using character-blocks based models and systemcombination.
Proceedings of COLING 2012, pages 1587?1602.Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso.
2013.
Paraphrasing 4 microblog normalization.Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,October.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011.
Insertion, deletion, or substitution?
normaliz-ing text messages without pre-categorization nor supervision.
Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: Human Language Technologies, pages 71?76, June.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
A broad-coverage normalization system for social media language.Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pages 1035?1044.W Machery, F J Och, and I Uszkoreit J Thayer.
2008.
Lattice-based minimum error rate training for statisticalmachine translation.
In Proc.
of EMNLP, 1:725?734.Teruaki Oka, Mamoru Komachi, Toshinobu Ogiso, and Yuji Matsumoto.
2011.
Handling orthographic variationsin morphological analysis for near-modern japanese (in japanese).
In Proc.
of The 27th Annual Conference ofthe Japanese Society for Articial Intelligence.Deana Pennell and Yang Liu.
2011.
A character-level machine translation approach for normalization of smsabbreviations.
Proceedings of 5th International Joint Conference on Natural Language Processing, pages 974?982, November.Akira Sasaki, Junta Mizuno, Naoaki Okazaki, and Kentaro Inui.
2013.
Normalization of text in microbloggingbased on machine learning(in japanese) (in japanese).
In Proc.
of The 27th Annual Conference of the JapaneseSociety for Articial Intelligence.Ryohei Sasano, Sadao Kurohashi, and Manabu Okumura.
2013.
A simple approach to unknown word process-ing in japanese morphological analysis.
Proceedings of the Sixth International Joint Conference on NaturalLanguage Processing, pages 162?170.Jiampojamarn Sittichai, Kondrak Grzegorz, and Sherif Tarek.
2007.
Applying many-to-many alignments andhidden markov models to letter-to-phoneme conversion.
In Proc.
of The Conference of the North AmericanChapter of the Association for Computational Linguistics, pages 372?379.1782
