Combining Lexical and Syntactic Features forSupervised Word Sense DisambiguationSaif MohammadUniversity of TorontoToronto, ON M4M2X6 Canadasmm@cs.toronto.eduhttp://www.cs.toronto.edu/?smmTed PedersenUniversity of MinnesotaDuluth, MN 55812 USAtpederse@d.umn.eduhttp://www.d.umn.edu/?tpederseAbstractThe success of supervised learning approachesto word sense disambiguation is largely de-pendent on the features used to represent thecontext in which an ambiguous word occurs.Previous work has reached mixed conclusions;some suggest that combinations of syntacticand lexical features will perform most effec-tively.
However, others have shown that sim-ple lexical features perform well on their own.This paper evaluates the effect of using differ-ent lexical and syntactic features both individu-ally and in combination.
We show that it is pos-sible for a very simple ensemble that utilizes asingle lexical feature and a sequence of part ofspeech features to result in disambiguation ac-curacy that is near state of the art.1 IntroductionMost words in natural language exhibit polysemy, thatis, they have multiple possible meanings.
Each of thesemeanings is referred to as a sense, and word sense disam-biguation is the process of identifying the intended senseof a target word based on the context in which it is used.The context of the target word consists of the sentencein which it occurs, and possibly one or two surroundingsentences.
Consider the following sentence:Harry cast a bewitching spell (1)The target word spell has many possible senses, such as,a charm or incantation, to read out letter by letter, anda period of time.
The intended sense, a charm or incan-tation, can be identified based on the context, which inthis case includes bewitching and a reference to a famousyoung wizard.Word sense disambiguation is often approached by su-pervised learning techniques.
The training data consistsof sentences which have potential target words taggedby a human expert with their intended sense.
Numerouslearning algorithms, such as, Naive Bayesian classifiers,Decision Trees and Neural Networks have been used tolearn models of disambiguation.
However, both (Peder-sen, 2001a) and (Lee and Ng, 2002) suggest that differentlearning algorithms result in little change in overall dis-ambiguation results, and that the real determiner of accu-racy is the set of features that are employed.Previous work has shown that using different combi-nations of features is advantageous for word sense dis-ambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996),(Stevenson and Wilks, 2001), (Yarowsky and Florian,2002)).
However, less attention is paid to determiningwhat the minimal set of features necessary to attain highaccuracy disambiguation are.
In this paper we presentexperiments that measure the redundancy in disambigua-tion accuracy achieved by classifiers using two differentsets of features, and we also determine an upper bound onthe accuracy that could be attained via the combination ofsuch classifiers into an ensemble.We find that simple combinations of lexical and syn-tactic features can result in very high disambiguationaccuracy, via an extensive set of experiments using theSENSEVAL-1, SENSEVAL-2, line, hard, serve and inter-est data.
Together, this consists of more than 50,000sense-tagged instances.
This paper also introduces a tech-nique to quantify the optimum gain that is theoreticallypossible when two feature sets are combined in an ensem-ble.
In the process, we identify some of the most usefulpart of speech and parse features.2 Feature SpaceWe employ lexical and syntactic features in our wordsense disambiguation experiments.
The lexical featuresare unigrams, bigrams, and the surface form of the targetword, while the syntactic features are part of speech tagsand various components from a parse tree.2.1 Lexical FeaturesThe surface form of a target word may restrict its possiblesenses.
Consider the noun case which has the surfaceforms: case, cases and casing.
These have the followingsenses: object of investigation, frame or covering and aweird person.
Given an occurrence of the surface formcasing, we can immediately conclude that it was used inthe sense of a frame or covering and not the other two.Each possible surface form as observed in the trainingdata is represented as a binary feature, and indicates ifthat particular surface form occurs (or not).Unigrams are individual words that appear in the text.Consider the following sentence:the judge dismissed the case (2)Here the, judge, dismissed, the and case are unigrams.Both judge and dismissed suggest that case has been usedin the judicial sense and not the others.
Every unigramthat occurs above a certain frequency threshold in thetraining corpus is represented as a binary feature.
Forexample, there is a feature that represents whether or notjudge occurs in the context of a target word.Bigrams are pairs of words that occur in close proxim-ity to each other, and in a particular order.
For example,in the following sentence:the interest rate is lower in state banks (3)the interest, interest rate, rate is, is lower, lower in, instate and state banks are bigrams, where interest ratesuggests that bank has been used in the financial insti-tution sense and not the river bank sense.
Every bigramthat reaches a given frequency and measure of associationscore threshold is represented as a binary feature.
For ex-ample, the bigram feature interest rate has value of 1 if itoccurs in the context of the target word, and 0 if it doesnot.We use the Ngram Statistics Package1 to identify fre-quent unigrams and statistically significant bigrams in thetraining corpus for a particular word.
However, unigramsor bigrams that occur commonly in text are ignored byspecifying a stop list composed mainly of prepositions,articles and conjunctions.2.2 Part of Speech FeaturesThe parts of speech of words around the target wordare also useful clues for disambiguation.
It is likelythat when used in different senses, the target wordwill have markedly different configuration of parts ofspeech around it.
The following sentences have theword turn in changing sides/parties sense and changingcourse/direction senses, respectively:1http://ngram.sourceforge.netDid/VBD Jack/NNP turn/VB against/INhis/PRP$ team/NN ?/.
(4)Did/VBD Jack/NNP turn/VB left/NNat/IN the/DT crossing/NN ?/.
(5)Observe that the parts of speech following each occur-rence of turn are significantly different, and that this dis-tinction can be captured both by individual and combina-tions of part of speech features.The parts of speech of individual words at particularpositions relative to the target word serve as features.
Thepart of speech of the target word is P0.
The POS of wordsfollowing the target are denoted by P1, P2and so on.
ThePOS of words to the left of the target word are P?1, P?2,etc.
There is a binary feature for each part of speech tagobserved in the training corpus at the given position orpositions of interest.Suppose we would like to use part of speech featuresfor the target word and one word to the right of the target.If the target word has 3 different parts of speech observedin the training data, and the word to the right (withoutregard to what that word is) has 32 different part of speechtags, then there will be 35 binary features that representthe occurrence of those tags at those positions.We also consider combinations of part of speech tagsas features.
These indicate when a particular sequenceof part of speech tags occurs at a given set of positions.These features are boolean, and indicate if a particular se-quence of tags has occurred or not.
In the scenario above,there would be 96 different binary features represented,each of which indicates if a particular combination of val-ues for the two positions of interest, occurs.2.3 Parse FeaturesA sentence is made up of multiple phrases and eachphrase, in turn, is made of phrases or words.
Each phrasehas a head word which may have strong syntactic re-lations with other words in the sentence.
Consider thephrases, her hard work and the hard surface.
The headwords work and surface are indicative of the calling forstamina/endurance and not easily penetrable senses ofhard.Thus, the head word of the phrase housing the targetword is used as a feature.
The head word of its parentphrase is also suggestive of the intended sense of the tar-get word.
Consider the sentence fragments fasten the lineand cross the line.
The noun phrases (the line) have theverbs fasten and cross as the head of parent phrases.
Verbfasten is indicative of the cord sense of line while crosssuggests the division sense.The phrase housing the target word and the parentphrase are also used as features.
For example, phrasehousing the target word is a noun phrase, parent phraseis a verb phrase and so on.
Similar to the part of speechfeatures, all parse features are boolean.3 Experimental DataWe conducted experiments using part of speech taggedand parsed versions of the SENSEVAL-2, SENSEVAL-1, line, hard, serve and interest data.
The packagesposSenseval and parseSenseval part of speechtagged and parsed the data, respectively.
posSensevaluses the Brill Tagger while parseSenseval employsthe Collins Parser.
We used the training and test datadivisions that already exist in the SENSEVAL-2 andSENSEVAL-1 data.
However, the line, hard, serve andinterest data do not have a standard division, so we ran-domly split the instances into test (20%) and training(80%) portions.The SENSEVAL-2 and SENSEVAL-1 data were cre-ated for comparative word sense disambiguation exer-cises held in the summers of 2001 and 1998, respectively.The SENSEVAL-2 data consists of 4,328 test instancesand 8,611 training instances and include a total of 73nouns, verbs and adjectives.
The training data has thetarget words annotated with senses from WordNet.
Thetarget words have a varied number of senses ranging fromtwo for collaborate, graceful and solemn to 43 for turn.The SENSEVAL-1 data has 8,512 test and 13,276 traininginstances, respectively.
The number of possible sensesfor these words range from 2 to 15, and are tagged withsenses from the dictionary Hector.The line data (Leacock, 1993) consists of 4,149 in-stances where the noun line is used in one of six possibleWordNet senses.
This data was extracted from the 1987-1989 Wall Street Journal (WSJ) corpus, and the AmericanPrinting House for the Blind (APHB) corpus.
The distri-bution of senses is somewhat skewed with more than 50%of the instances used in the product sense while all theother instances more or less equally distributed amongthe other five senses.The hard data (Leacock, 1998) consists of 4,337 in-stances taken from the San Jose Mercury News Corpus(SJM) and are annotated with one of three senses of theadjective hard, from WordNet.
The distribution of in-stances is skewed with almost 80% of the instances usedin the not easy - difficult sense.The serve data (Leacock, 1998) consists of 5,131 in-stances with the verb serve as the target word.
They areannotated with one of four senses from WordNet.
Likeline it was created from the WSJ and APHB corpora.The interest data (Bruce, 1994) consists of 2,368 in-stances where the noun interest is used in one of sixsenses taken from the Longman Dictionary of Contempo-rary English (LDOCE).
The instances are extracted fromthe part of speech tagged subset of the Penn TreebankWall Street Journal Corpus (ACL/DCI version).4 Experiments and DiscussionThe SyntaLexword sense disambiguation package wasused to carry out our experiments.
It uses the C4.5 algo-rithm, as implemented by the J48 program in the WaikatoEnvironment for Knowledge Analysis (Witten and Frank,2000) to learn a decision tree for each word to be disam-biguated.We use the majority classifier as a baseline point ofcomparison.
This is a classifier that assigns all instancesto the most frequent sense in the training data.
Our sys-tem defaults to the majority classifier if it lacks any otherrecourse, and therefore it disambiguates all instances.
Wethus, report our results in terms of accuracy.
Table 1shows our overall experimental results, which will be dis-cussed in the sections that follow.
Note that the results ofthe majority classifier appear at the bottom of that table,and that the most accurate result for each set of of data isshown in bold face.4.1 Lexical FeaturesWe utilized the following lexical features in our experi-ments: the surface form of the target word, unigrams andbigrams.
The entries under Lexical in Table 1 show dis-ambiguation accuracy when using those features individ-ually.It should be noted that the experiments for theSENSEVAL-2 and SENSEVAL-1 data using unigrams andbigrams are re-implementations of (Pedersen, 2001a),and that our results are comparable.
However, the exper-iments on line, hard, serve and interest have been carriedout for the first time.We observe that in general, surface form does notimprove significantly on the baseline results providedby the majority classifier.
While in most of the data(SENSEVAL-2, line, hard and serve data) there is hardlyany improvement, we do see noticeable improvements inSENSEVAL-1 and interest data.
We believe that this isdue to the nature of the feature.
Certain words have manysurface forms and senses.
In many such cases, certainsenses can be represented by a restricted subset of possi-ble surface forms.
Such words are disambiguated betterthan others using this feature.4.2 Part of Speech FeaturesWord sense disambiguation using individual part ofspeech features is done in order to compare the effect ofsingle POS features versus possibly more powerful com-bination part of speech features.
They are not expectedto be powerful enough to do very good classification butmay still capture certain intuitive notions.
For example,it is very likely that if the noun line is preceded by a whTable 1: Supervised WSD Accuracy by Feature TypeFeatures SENSEVAL-2 SENSEVAL-1 line hard serve interestLexicalSurface Form 49.3% 62.9% 54.3% 81.5% 44.2% 64.0%Unigrams 55.3% 66.9% 74.5% 83.4% 73.3% 75.7%Bigrams 55.1% 66.9% 72.9% 89.5% 72.1% 79.9%POSP?247.1% 57.5% 54.9% 81.6% 52.1% 56.0%P?149.6% 59.2% 56.2% 82.1% 54.8% 62.7%P049.9% 60.3% 54.3% 81.6% 47.4% 64.0%P153.1% 63.9% 54.2% 81.6% 55.6% 65.3%P248.9% 59.9% 54.3% 81.7% 48.9% 62.3%POS CombosP?1, P050.8% 62.2% 56.5% 82.3% 60.3% 67.7%P0, P154.3% 66.7% 54.1% 81.9% 60.2% 70.5%P1, P253.2% 64.0% 55.9% 82.2% 58.0% 68.6%P?1, P0, P154.6% 68.0% 60.4% 84.8% 73.0% 78.8%P?2, P?1, P0, P1, P254.6% 67.8% 62.3% 86.2% 75.7% 80.6%ParseHead (H) 51.7% 64.3% 54.7% 87.8% 47.4% 69.1%Head of Parent (HP) 50.0% 60.6% 59.8% 84.5% 57.2% 67.8%Phrase POS (P) 52.9% 58.5% 54.3% 81.5% 41.4% 54.9%Parent Phrase POS (PP) 52.7% 57.9% 54.3% 81.7% 41.6% 54.9%Parse CombosH + HP 52.6% 65.1% 60.4% 87.7% 58.1% 73.2%H + P 51.9% 65.1% 54.7% 87.8% 45.9% 69.1%H + HP + P 52.9% 65.5% 60.4% 87.7% 57.6% 73.2%H + P + HP + PP 52.7% 65.6% 60.5% 87.7% 56.7% 73.5%Majority Classifier 47.7% 56.3% 54.3% 81.5% 42.2% 54.9%word such as whose or which, it is used in the phone linesense.
If the noun line is preceded by a preposition, say inor of, then there is a good chance that line has been usedin the formation sense.
The accuracies achieved by partof speech features on SENSEVAL-2, SENSEVAL-1, line,hard, serve and interest data are shown in Table 1.
Theindividual part of speech feature results are under POS,and the combinations under POS Combos.We observe that the individual part of speech featuresresult in accuracies that are significantly better than themajority classifier for all the data except for the line andhard.
Like the surface form, we believe that the partof speech features are more useful to disambiguate cer-tain words than others.
We show averaged results for theSENSEVAL-2 and SENSEVAL-1, and even there the partof speech features fare well.
In addition, when lookingat a more detailed breakdown of the 73 and 36 words in-cluded in these samples respectively, a considerable num-ber of those words experience improved accuracy usingpart of speech features.In particular, we observed that while verbs and adjec-tives are disambiguated best by part of speech of wordsone or two positions on their right (P1, P2), nouns in gen-eral are aided by the part of speech of immediately adja-cent words on either side (P?1, P1).
In the case of tran-sitive verbs (which are more frequent in this data thanintransitive verbs), the words at positions P1and P2areusually the objects of the verb (for example, drink water).Similarly, an adjective is usually immediately followedby the noun which it qualifies (for example, short discus-sion).
Thus, in case of both verbs and adjectives, the wordimmediately following (P1) is likely to be a noun havingstrong syntactic relation to it.
This explains the higheraccuracies for verbs and adjectives using P1and wouldimply high accuracies for nouns using P?1, which too weobserve.
However, we also observe high accuracies fornouns using P1.
This can be explained by the fact thatnouns are often the subjects in a sentence and the wordsat positions P1and P2may be the syntactically relatedverbs, which aid in disambiguation.To summarize, verbs are aided by P1and P2, adjectivesby P1and nouns by P?1and P1.
Thus, P1is the the mostpotent individual part of speech feature to disambiguate aset of noun, verb and adjective target words.4.2.1 Combining Part of Speech featuresA combination of parts of speech of words surround-ing (and possibly including) the target word may bettercapture the overall context than single part of speech fea-tures.
Following is an example of how a combinationof part of speech features may help identify the intendedsense of the noun line.
If the target word line is used inthe plural form, is preceded by a personal pronoun andthe word following it is not a preposition, then it is likelythat the intended sense is line of text as in the actor forgothis lines or they read their lines slowly.
However, if theword preceding line is a personal pronoun and the wordfollowing it is a preposition, then it is probably used inthe product sense, as in, their line of clothes.
POS Com-bos in Table 1 shows the accuracies achieved using suchcombinations with the SENSEVAL-2, SENSEVAL-1, line,hard, serve and interest data.
Again due to space con-straints we do not give a break down of the accuracies forthe SENSEVAL-2 and SENSEVAL-1 data for noun, verband adjective target words.We note that decision trees based on binary featuresrepresenting the possible values of a given sequence ofpart of speech tags outperforms one based on individ-ual features.
The combinations which include P1obtainhigher accuracies.
In the the case of the verbs and ad-jectives in SENSEVAL-2 and SENSEVAL-1 data, the bestresults are obtained using the parts of speech of wordsfollowing the target word.
The nouns are helped by partsof speech of words on both sides.
This is in accordancewith the hypothesis that verbs and adjectives have strongsyntactic relations to words immediately following whilenouns may have strong syntactic relations on either side.However, the hard and serve data are found to be helpedby features from both sides.
We believe this is becauseof the much larger number of instances per task in caseof hard and serve data as compared to the adjectives andverbs in SENSEVAL-1 and SENSEVAL-2 data.
Due to thesmaller amount of training data available for SENSEVAL-2 and SENSEVAL-1 words, only the most potent featureshelp.
The power of combining features is highlighted bythe significant improvement of accuracies above the base-line for the line and hard data, which was not the caseusing individual features (Table 1).4.3 Parse FeaturesWe employed the following parse features in these exper-iments: the head word of the phrase housing the targetword, the type of phrase housing the target word (Nounphrase, Verb Phrase, etc), the head of the parent phrase,and the type of parent phrase.
These results are shownunder Parse in Table 1.The head word feature yielded the best results in all thedata except line, where the head of parent phrase is mostpotent.
Further, the nouns and adjectives benefit most bythe head word feature.
We believe this the case becausethe head word is usually a content word and thus likelyto be related to other nouns in the vicinity.
Nouns areusually found in noun phrases or prepositional phrases.When part of a noun phrase, the noun is likely to be thehead and thus does not benefit much from the head wordfeature.
In such cases, the head of the parent phrase mayprove to be more useful as is the case in the line data.In case of adjectives, the relation of the head word to thetarget word is expected to be even stronger as it is likely tobe the noun modified by the adjective (target word).
Theverb is most often found in a verb phrase and is usuallythe head word.
Hence, verb target words are not expectedto be benefited by the head word feature, which is whatwe find here.
The phrase housing the target word and theparent phrase were not found to be beneficial when usedindividually.4.3.1 Combining Parse FeaturesCertain parse features, such as, the phrase of the targetword, take very few distinct values.
For example, the tar-get word shirt may occur in at most just two distinct kindsof phrases: noun phrase and prepositional phrase.
Suchfeatures are not expected to perform much better than themajority classifier.
However, when used in combinationwith other features, they may be useful.
Thus, like partof speech features, experiments were conducted using acombination of parse features in an effort to better capturethe context and to identify sets of features which workwell together.
Consider the parse features head word andparent word.
Head words such as magazine, situation andstory are indicative of the quality of causing attention tobe given sense of interest while parent words such as ac-crue and equity are indicative of the interest rate sense.A classifier based on both features can confidently clas-sify both kinds of instances.
Table 1 has the results underParse Combos.
The Head and Head of Parent combina-tions have in general yielded significantly higher accura-cies than simply the head word or any other parse featureused individually.
The improvement is especially note-worthy in case of line, serve and interest data.
The in-clusion of other features along with these two does nothelp much more.
We therefore find the Head and Headof Parent combination to be the most potent parse featurecombination.
It may be noted that a break down of ac-curacies (not shown here for sake of brevity) for noun,verb and adjective target words, of the SENSEVAL-1 andSENSEVAL-2 data revealed that the adjectives were dis-ambiguated best using the Head word and Phrase combi-nation.
This is observed in the hard data results as well,albeit marginally.Table 2: The Best Combinations of Syntactic and Lexical FeaturesFeature-Set Pair Baseline Maj.
Simple Optimal BestData Set 1 Acc.
Set2 Acc.
Ens.
Class.
Ens.
Ens.SVAL-2 Unigram 55.3% P?1, P0, P154.6% 43.6% 47.7% 57.0% 67.9% 66.7%SVAL-1 Unigram 66.9% P?1, P0, P168.0% 57.6% 56.3% 71.1% 78.0% 81.1%line Unigram 74.5% P?1, P0, P160.4% 55.1% 54.3% 74.2% 82.0% 88.0%hard Bigram 89.5% Head, Parent 87.7% 86.1% 81.5% 88.9% 91.3% 83.0%serve Unigram 73.3% P?1, P0, P173.0% 58.4% 42.2% 81.6% 89.9% 83.0%interest Bigram 79.9% P?1, P0, P178.8% 67.6% 54.9% 83.2% 90.1% 89.0%5 Complementary/Redundant FeaturesAs can be observed in the previous results, many differentkinds of features can lead to roughly comparable wordsense disambiguation results.Different types of features are expected to be redun-dant to a certain extent.
In other words, the featureswill individually classify an identical subset of the in-stances correctly.
Likewise, the features are expected tobe complementary to some degree, that is, while one setof features correctly disambiguates a certain subset of in-stances, use of another set of features results in the cor-rect disambiguation of an entirely distinct subset of theinstances.The extent to which the feature sets are complementaryand redundant justify or obviate the combining of the fea-ture sets.
In order to accurately capture the amount of re-dundancy and complementarity among two feature sets,we introduce two measures: the Baseline Ensemble andthe Optimal Ensemble.
Consider the scenario where theoutputs of two classifiers based on different feature setsare to be combined using a simple voting or ensembletechnique for word sense disambiguation.The Baseline Ensemble is the accuracy attained by ahypothetical ensemble technique which correctly disam-biguates an instance only when both the classifiers iden-tify the intended sense correctly.
In effect, the BaselineEnsemble quantifies the redundancy among the two fea-ture sets.
The Optimal Ensemble is the accuracy of ahypothetical ensemble technique which accurately dis-ambiguates an instance when any of the two classifierscorrectly disambiguates the intended sense.
We say thatthese are hypothetical in that they can not be imple-mented, but rather serve as a post disambiguation anal-ysis technique.Thus, the Optimal Ensemble is the upper bound to theaccuracy achievable by combining the two feature setsusing an ensemble technique.
If the accuracies of indi-vidual classifiers is X and Y, the Optimal Ensemble canbe defined as follows:OptimalEnsemble = (X ?BaselineEnsemble) +(Y ?BaselineEnsemble) + BaselineEnsembleWe use a simple ensemble technique to combine someof the best lexical and syntactic features identified in theprevious sections.
The probability of a sense to be theintended sense as identified by lexical and syntactic fea-tures is summed.
The sense which attains the highestscore is chosen as the intended sense.
Table 2 showsthe best results achieved using this technique along withthe baseline and optimal ensembles for the SENSEVAL-2, SENSEVAL-1, line, hard, serve and interest data.
Thetable also presents the feature sets that achieved these re-sults.
In addition, the last column of this table shows rep-resentative values for some of the best results attained inthe published literature for these data sets.
Note that theseare only approximate points of comparison, in that thereare differences in how individual experiments are con-ducted for all of the non?SENSEVAL data.From the Baseline Ensemble we observe that there is alarge amount of redundancy across the feature sets.
Thatsaid, there is still a significant amount of complementar-ity as may be noted by the difference between the OptimalEnsemble and the greater of the individual accuracies.For example, in the SENSEVAL-2 data, unigrams aloneachieve 55.3% accuracy and part of speech features attainan accuracy of 54.6%.
The Baseline Ensemble attains ac-curacy of 43.6%, which means that this percentage of thetest instances are correctly tagged, independently, by bothunigrams and part of speech features.
The unigrams getan additional 11.7% of the instances correct which thepart of speech features tag incorrectly.Similarly, the part of speech features are able to cor-rectly tag an additional 11% of the instances which aretagged erroneously when using only bigrams.
The abovevalues suggest a high amount of redundancy among theunigrams and part of speech features but not high enoughto suggest that there is no significant benefit in combin-ing the two kinds of features.
The difference between theOptimal Ensemble and the accuracy attained by unigramsis 12.6% (67.9% - 55.3%).
This is a significant improve-ment in accuracy which may be achieved by a suitableensemble technique.
The difference is a quantification ofthe complementarity between unigram and part of speechfeatures based on the data.
Further, we may conclude thatgiven these unigram and part of speech features, the bestensemble techniques will not achieve accuracies higherthan 67.9%.It may be noted that a single unified classifier basedon multiple features may achieve accuracies higher thanthe Optimal Ensemble.
However, we show that an ac-curate ensemble method (Optimal Ensemble), based onsimple lexical and syntactic features, achieves accuraciescomparable or better than some of the best previous re-sults.
The point here is that using information from twodistinct feature sets (lexical features and part of speech)could lead to state of the art results.
However, it is asyet unclear how to most effectively combine such simpleclassifiers to achieve these optimal results.Observation of the pairs of lexical and syntactic fea-tures which provide highest accuracies for the variousdata suggest that the part of speech combination feature -P?1, P0, P1, is likely to be most complementary with thelexical features (bigrams or unigrams).The hard data did particularly well with combinationsof parse features, the Head and Parent words.
The Op-timal Ensemble attains accuracy of over 91%, while thebest previous results were approximately 83%.
This indi-cates that not only are the Head and Parent word featuresvery useful in disambiguating adjectives but are also asource of complementary information to lexical features.6 Related Work(McRoy, 1992) was one of the first to use multiple kindsof features for word sense disambiguation in the semanticinterpretation system, TRUMP.
The system aims at dis-ambiguating all words in the text and relies extensivelyon dictionaries and is not corpus based.
Scores are as-signed based on morphology, part of speech, collocationsand syntactic cues.
The sense with the highest score ischosen as the intended sense.
TRUMP was used to tag asubset of the Wall Street Journal (around 2500 words) butwas not evaluated due to lack of gold standard.The LEXAS system of (Ng and Lee, 1996) uses partof speech, morphology, co-occurrences, collocations andverb object relation in nearest neighbor implementation.The system was evaluated using the interest data onwhich it achieved an accuracy of 87.3%.
They studiedthe utility of individual features and found collocationsto be most useful, followed by part of speech and mor-phological form.
(Lin, 1997) takes a supervised approach that is uniqueas it did not create a classifier for every target word.
Thesystem compares the context of the target word with thatof training instances which are similar to it.
The sense ofthe target word most similar to these contexts is chosenas the intended sense.
Similar to McRoy, the system at-tempts to disambiguate all words in the text.
Lin relies onsyntactic relations, such as, subject-verb agreement andverb object relations to capture the context.
The systemachieved accuracies between 59% and 67% on the Sem-Cor corpus.
(Pedersen, 2001b) compares decision trees, decisionstumps and a Naive Bayesian classifier to show that bi-grams are very useful in identifying the intended senseof a word.
The accuracies of 19 out of the total 36 tasksin SENSEVAL-1 data were greater than the best reportedresults in that event.
Bigrams are easily captured fromraw text and the encouraging results mean that they canact as a powerful baseline to build more complex systemsby incorporating other sources of information.
Pedersenpoints out that decision trees can effectively depict the re-lations among the various features used.
With the use ofmultiple sources of information this quality of decisiontrees gains further significance.
(Lee and Ng, 2002) compare the performances of Sup-port Vector Machines, Naive Bayes, AdaBoost and De-cision Trees using unigrams, parts of speech, colloca-tions and syntactic relations.
The experiments were con-ducted on SENSEVAL-2 and SENSEVAL-1 data.
Theyfound the combination of features achieved highest ac-curacy (around 73%) in SENSEVAL-1 data, irrespectiveof the learning algorithm.
Collocations(57.2%), part ofspeech tags(55.3%) and syntactic relations(54.2%) per-formed better than decision trees using all features in theSENSEVAL-2 data.
(Yarowsky and Florian, 2002) performed experimentswith different learning algorithms and multiple features.Three kinds of Bayes Classifier, Decision lists and Trans-formation Based Learning Model (TBL) were used withcollocations, bag of words and syntactic relations as fea-tures.
Experiments on SENSEVAL-2 data revealed thatthe exclusion of any of the three kinds of features resultedin a significant drop in accuracy.
Lee and Ng as well asYarowsky and Florian conclude that the combination offeatures is beneficial.
(Pedersen, 2002) does a pairwise study of the systemsthat participated in SENSEVAL-2 English and Spanishdisambiguation exercises.
The study approaches the sys-tems as black boxes, looking only at the assigned tagswhatever the classifier and sources of information maybe.
He introduces measures to determine the similarityof the classifications and optimum results obtainable bycombining the systems.
He points out that pairs of sys-tems having low similarity and high optimal accuraciesare of interest as they are markedly complementary andthe combination of such systems is beneficial.There still remain questions regarding the use of mul-tiple sources of information, in particular which featuresshould be combined and what is the upper bound on theaccuracies achievable by such combinations.
(Pedersen,2002) describes how to determine the upper bound whencombining two systems.
This paper extends that idea toprovide measures which determine the upper bound whencombining two sets of features in a single disambiguationsystem.
We provide a measure to determine the redun-dancy in classification done using two different featuresets.
We identify particular part of speech and parse fea-tures which were found to be very useful and the com-binations of lexical and syntactic features which workedbest on SENSEVAL-2, SENSEVAL-1, line, hard, serve andinterest data.7 ConclusionsWe conducted an extensive array of word sense disam-biguation experiments using a rich set of lexical and syn-tactic features.
We use the SENSEVAL-2, SENSEVAL-1,line, hard, serve and interest data which together havemore than 50,000 sense tagged instances.
We show thatboth lexical and syntactic features achieve reasonablygood accuracies when used individually, and that the partof speech of the word immediately following the targetword is particularly useful in disambiguation as com-pared to other individual part of speech features.
A com-bination of part of speech features attains even better ac-curacies and we identify (P0, P1) and (P?1, P0, P1) as themost potent combinations.
We show that the head wordof a phrase is particularly useful in disambiguating adjec-tives and nouns.
We identify the head and parent as themost potent parse feature combination.We introduce the measures Baseline Ensemble and Op-timal Ensemble which quantify the redundancy amongtwo feature sets and the maximum accuracy attainable byan ensemble technique using the two feature sets.
Weshow that even though lexical and syntactic features areredundant to a certain extent, there is a significant amountof complementarity.
In particular, we showed that sim-ple lexical features (unigrams and bigrams) used in con-junction with part of speech features have the potential toachieve state of the art results.8 AcknowledgmentsThis work has been partially supported by a National Sci-ence Foundation Faculty Early CAREER Developmentaward (#0092784).ReferencesR.
Bruce and L. Wiebe.
1994 Word-Sense Disambigua-tion using Decomposable Models In Proceedings ofthe 32nd Annual Meeting of the Association for Com-putational Linguistics.C.
Leacock and M. Chodorow and G. Miller.
1998 Us-ing Corpus Statistics and WordNet Relations for SenseIdentification Computational Linguistics, 24(1):147?165.C.
Leacock and E. Voorhees.
1993 Corpus-Based Sta-tistical Sense Resolution In Proceedings of the ARPAWorkshop on Human Language Technology.K.L.
Lee and H.T.
Ng.
2002.
An empirical evaluation ofknowledge sources and learning algorithms for wordsense disambiguation.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 41?48.D.
Lin.
1997.
Using syntactic dependency as a localcontext to resolve word sense ambiguity.
In Proceed-ings of the 35th Annual Meeting of the Association forComputational Linguistics, pages 64?71, Madrid, July.S.
McRoy.
1992.
Using multiple knowledge sources forword sense discrimination.
Computational Linguis-tics, 18(1):1?30.H.T.
Ng and H.B.
Lee.
1996.
Integrating multipleknowledge sources to disambiguate word sense: Anexemplar-based approach.
In Proceedings of the 34thAnnual Meeting of the Association for ComputationalLinguistics, pages 40?47.T.
Pedersen.
2001a.
A decision tree of bigrams is an ac-curate predictor of word sense.
In Proceedings of theSecond Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics,pages 79?86, Pittsburgh, July.T.
Pedersen.
2001b.
Machine learning with lexical fea-tures: The duluth approach to senseval-2.
In Pro-ceedings of the Senseval-2 Workshop, pages 139?142,Toulouse, July.T.
Pedersen.
2002.
Assessing system agreement andinstance difficulty in the lexical samples tasks ofsenseval-2.
In Proceedings of the ACL Workshop onWord Sense Disambiguation: Recent Successes andFuture Directions, pages 40?46, Philadelphia.M.
Stevenson and Y. Wilks.
2001.
The interactionof knowledge sources in word sense disambiguation.Computational Linguistics, 27(3):321?349, Septem-ber.I.
Witten and E. Frank.
2000.
Data Mining - Practi-cal Machine Learning Tools and Techniques with JavaImplementations.
Morgan?Kaufmann, San Francisco,CA.D.
Yarowsky and R. Florian.
2002.
Evaluating sensedisambiguation performance across diverse parameterspaces.
Journal of Natural Language Engineering,8(2).D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA.
