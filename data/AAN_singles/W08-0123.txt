Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 138?147,Columbus, June 2008. c?2008 Association for Computational LinguisticsArgumentative Human Computer Dialogue for Automated PersuasionPierre Andrews* and Suresh Manandhar* and Marco De Boni*** Department of Computer ScienceUniversity of YorkYork YO10 5DDUK{pandrews,suresh}@cs.york.ac.uk** Unilever Corporate ResearchBedford MK44 1LQUKMarco.De-Boni@unilever.comAbstractArgumentation is an emerging topic in thefield of human computer dialogue.
In thispaper we describe a novel approach to dia-logue management that has been developed toachieve persuasion using a textual argumen-tation dialogue system.
The paper introducesa layered management architecture that mixestask-oriented dialogue techniques with chat-bot techniques to achieve better persuasive-ness in the dialogue.1 IntroductionHuman computer dialogue is a wide research areain Artificial Intelligence.
Computer dialogue isnow used at production stage for applications suchas tutorial dialogue ?
that helps teaching students(Freedman, 2000) ?
task-oriented dialogue ?
thatachieves a particular, limited task, such as book-ing a trip (Allen et al, 2000) ?
and chatbot dialogue(Levy et al, 1997) ?
that is used within entertain-ment and help systems.None of these approaches use persuasion as amechanism to achieve dialogue goals.
However,research towards the use of persuasion in Hu-man Computer Interactions has spawned around thefield of natural argumentation (Norman and Reed,2003).
Similarly research on Embodied Con-versational Agents (ECA) (Bickmore and Picard,2005) is also attempting to improve the persuasive-ness of agents with persuasion techniques; how-ever, it concentrates on the visual representationof the interlocutor rather than the dialogue man-agement.
Previous research on human computerdialogue has rarely focused on persuasive tech-niques (Guerini, Stock, and Zancanaro, 2004, initi-ated some research in that field).
Our dialogue man-agement system applies a novel method, taking ad-vantage of persuasive and argumentation techniquesto achieve persuasive dialogue.According to the cognitive dissonance theory(Festinger, 1957), people will try to minimise thediscrepancy between their behaviour and their be-liefs by integrating new beliefs or distorting existingones.
In this paper, we approach persuasion as a pro-cess shaping user?s beliefs to eventually change theirbehaviour.The presented dialogue management system hasbeen developed to work on known limitations of cur-rent dialogue systems:The impression of lack of control is an issue whenthe user is interacting with a purely task-oriented di-alogue system (Farzanfar et al, 2005).
The systemfollows a plan to achieve the particular task, and theuser?s dialogue moves are dictated by the plannerand the plan operators.The lack of empathy of computers is also aproblem in human-computer interaction for applica-tions such as health-care, where persuasive dialoguecould be applied (Bickmore and Giorgino, 2004).The system does not respond to the user?s personaland emotional state, which sometimes lowers theuser?s implication in the dialogue.
However, exist-ing research (Klein, Moon, and Picard, 1999) showsthat a system that gives appropriate response to theuser?s emotion can lower frustration.In human-human communication, these lim-itations reduce the effectiveness of persuasion138(Stiff and Mongeau, 2002).
Even if the response to-wards the computer is not always identical to theone to humans, it seems sensible to think that per-suasive dialogue systems can be improved by apply-ing known findings from human-human communi-cation.The dialogue management architecture describedin this paper (see Figure 1) addresses these dialoguemanagement issues by using a novel layered ap-proach to dialogue management, allowing the mix-ing of techniques from task-oriented dialogue man-agement and chatbot techniques (see Section 4).Figure 1: Layered Management ArchitectureThe use of a planner guarantees the consistencyof the dialogue and the achievement of persuasivegoals (see Section 4.2).
Argumentative dialogue canbe seen as a form of task-oriented dialogue wherethe system?s task is to persuade the user by present-ing the arguments.
Thus, the dialogue manager firstuses a task-oriented dialogue methodology to cre-ate a dialogue plan that will determine the contentof the dialogue.
The planning component?s role isto guarantee the consistency of the dialogue and theachievement of the persuasive goals.In state-of-the-art task-oriented dialogue manage-ment systems, the planner provides instructions fora surface realizer (Green and Lehman, 2002), re-sponsible of generating the utterance correspondingto the plan step.
Our approach is different to al-low more reactivity to the user and give a feelingof control over the dialogue.
In this layered ap-proach, the reactive component provides a direct re-action to the user input, generating one or more ut-terances for a given plan step, allowing for reactionsto user?s counter arguments as well as backchanneland chitchat phases without cluttering the plan.Experimental results show that this layered ap-proach allows the user to feel more comfortable inthe dialogue while preserving the dialogue consis-tency provided by the planner.
Eventually, this trans-lates into a more persuasive dialogue (see Section 6).2 Related WorkPersuasion through dialogue is a novelfield of Human Computer Interaction.Reiter, Robertson, and Osman (2003),Reed (1998)and Carenini and Moore (2000) apply persuasivecommunication principles to natural languagegeneration, but only focus on monologue.The 3-tier planner for tutoring dialogue byZinn, Moore, and Core (2002) provides a di-alogue management technique close to ourapproach: a top-tier generates a dialogue plan,the middle-tier generates refinements to theplan and the bottom-tier generates utterances.Mazzotta, de Rosis, and Carofiglio (2007) alsopropose a planning framework for user-adaptedpersuasion where the plan operators are mappedto natural language (or ECA) generation.
How-ever, these planning approaches do not include amechanism to react to user?s counter argumentsthat are difficult to plan beforehand.
This paperpropose a novel approach that could improvethe user?s comfort in the dialogue as well as itspersuasiveness.3 Case StudyPart of the problem in evaluating persuasive dia-logue is using an effective evaluation framework.Moon (1998) uses the Desert Survival Scenario toevaluate the difference of persuasion and trust ininteraction between humans when face-to-face orwhen mediated by a computer system (via an instantmessaging platform).The Desert Survival Scenario(Lafferty, Eady, and Elmers, 1974) is a negoti-ation scenario used in team training.
The team isput in a scenario where they are stranded in thedesert after a plane crash.
They have to negotiate aranking of the most eligible items (knife, compass,map, etc.)
that they should keep for their survival.For the evaluation of the dialogue system, a simi-lar scenario is presented to the participants.
The userhas to choose an initial preferred ranking of items139and then engages in a discussion with the dialoguesystem that tries to persuade the user to change theranking.
At the end of the dialogue, the user has theopportunity to either change or keep the ranking.The architecture of the dialogue system is de-scribed throughout this paper using examples fromthe Desert Scenario.
The full evaluation protocol isdescribed in Section 5 and 6.4 Dialogue Management ArchitectureThe following sections provide a description ofthe dialogue management architecture introduced inFigure 1.4.1 Argumentation ModelThe Argumentation model represents the differentarguments (conclusions and premises) that can beproposed by the user or by the system.
Figure 2gives a simplified example of the Desert Scenariomodel.Figure 2: Argumentation Model SampleThis model shows the different facts that areknown by the system and the relations be-tween them.
Arrows represent the support re-lation between two facts.
For example, res-cue knows where you are is a support to the factgoal(signal) (the user goal is to signal presence tothe rescue) as well as a support to goal(stay put) (theuser goal is to stay close to the wreckage).
Thisrelational model is comparable to the argumenta-tion framework proposed by Dung (1995), but storesmore information about each argument for reason-ing within the planning and reactive component (seeSection 4.2).Each fact in this model represents a belief to beintroduced to the user.
For example, when the dia-logue tries to achieve the goal reorder(flashlight >air map): the system wants the user to believe thatthe ?flashlight?
item should be ranked higher thanthe ?air map?
item.
The argumentation model de-scribes the argumentation process that is requiredto introduce this new belief: the system first has tomake sure the user believes in rate lower(air map)and rate higher(flashlight).Lower level facts (see Figure 2) are the goal factsof the dialogue, the ones the system chooses as di-alogue goals, according to known user beliefs andthe system?s goal beliefs (e.g.
according to the rank-ing the system is trying to defend).
The facts in themiddle of the hierarchy are intermediate facts thatneed to be asserted during the dialogue.
The top-level facts are world knowledge: facts that requireminimum defense and can be easily grounded in thedialogue.4.2 Planning ComponentThe planning component?s task is to find a plan us-ing the argumentation model to introduce the re-quired facts in the user?s belief to support the per-suasive goals.
The plan is describes a path in the ar-gumentation model beliefs hierarchy that translatesto argumentation segments in the dialogue.In our current evaluation method, the goal of thedialogue is to change the user?s beliefs about theitems so that the user eventually changes the rank-ing.
At the beginning of the dialogue, the ranking ofthe system is chosen and persuasive goals are com-puted for the dialogue.
These persuasive goals cor-respond to the lower level facts in the argumentationmodel ?
like ?reorder(flashlight > air map)?
in ourprevious example.
The available planning operatorsare:use world(fact) describes a step in the dialoguethat introduces a simple fact to the user.ground(fact) describes a step in the dialogue thatgrounds a fact in the user beliefs.
Grounding a factis a different task from the use world operator as itwill need more support during the dialogue.do support([fact0, fact1, .
.
.
], fact2) describes acomplex support operation.
The system will initiatea dialogue segment supporting fact2 with the factsfact1 and fact0, etc.
that have previously been intro-duced in the user beliefs.The planning component can also use twonon-argumentative operators, do greetings and140do farewells, that are placed respectively at the be-ginning and the end of the dialogue plan to open andclose the session.Here is an example plan using the two argu-ments described in Figure 2 to support the goal re-order(flashlight > air map):Step 1 do greetingsStep 2 use world(goal(be found))ground(rescue knows where you are)ground(can(helpatnight,item(flashlight)))Step 3 do support([can(helpatnight,item(flashlight))],rate higher(item(flashlight)))do support([rescue knows where you are,goal(be found)],goal(stay put))Step 4 do support([goal(stay put)],rate lower(item(air map)))Step 5 do support(...,reorder(item(flashlight),item(air map)))Step 6 do farewellsThe plan is then interpreted by the reactive com-ponent that is responsible for realizing each step ina dialogue segment.4.3 The Reactive ComponentThe reactive component?s first task is to realize theoperators chosen by the planning component into di-alogue utterance(s).
However, it should not be mis-taken for a surface language realizer.
The reactivecomponent?s task, when realizing the operator, is todecide how to present the particular argumentationoperator and its parameters to the user according tothe dialogue context and the user?s reaction to theargument.
This reactive process is described in thefollowing sections.4.3.1 Realization and Reaction StrategiesEach step of the plan describes the general topicof a dialogue segment1.
A dialogue segment isa set of utterances from the system and from1i.e.
it is not directly interpreted as an instruction to generateone unique utterance.the user that are related to a particular argument.For example, in the Desert Scenario, the operatorground(can(helpatnight, item(flashlight))) may re-sult in the following set of utterances:S(ystem) I think the flashlight couldbe useful as it could help us atnight,U(ser) How is that?
We are not goingto move during the night.S well, if we want to collect water,it will be best to do things atnight and not under the burningsun.U I see.
It could be useful then.In this example, the ground operator has been re-alized by the reactive component in two different ut-terances to react to the user?s interaction.The goal of the reactive component is to make theuser feel that the system understands what has beensaid.
It is also important to avoid replanning as ittries to defend the arguments chosen in the plan.As described in Section 4.2, the planner relies onthe argumentation model to create a dialogue plan.Encoding all possible defenses and reactions to theuser directly in this model will explode the searchspace of the planner and require careful authoringto avoid planning inconsistencies2 .
In addition, pre-dicting at the planning level what counter argumentsa user is likely to make requires a prior knowledgeof the user?s beliefs.
At the beginning of a one-offdialogue, it is not possible to make prior assump-tions on the user?s beliefs; the system has a shal-low knowledge of the user?s beliefs and will discoverthem as the dialogue goes.Hence, it is more natural to author a reactive di-alogue that will respond to the user?s counter ar-guments as they come and extends the user beliefsmodel as it goes.
In our architecture if the user isdisagreeing with an argument, the plan is not reviseddirectly; if possible, the reactive component selectsnew, contextually appropriate, supporting facts forthe current plan operator.
It can do this multipleconsecutive local repairs if the user needs more con-vincing and the domain model provides enough de-fenses.
This allows for a simpler planning frame-work.2a new plan could go against the previously used arguments.141In addition, when available, and even if the useragrees with the current argument, the reactive com-ponent can also choose from a set of ?dialoguesmoothing?
or backchannel utterances to make thedialogue feel more natural.
Here is an example fromthe Desert Scenario:S We don?t have much water, we need tobe rescued as soon as possible.
(from plan step: user world( goal(be found)))U rightS I am glad we agree.
(backchannel)S There is a good chance that therescue team already knows ourwhereabouts.
We should beoptimistic and plan accordingly,don?t you think?
(from plan step:use world( rescue knows where you are))4.3.2 Detecting user reactionsThe reactive component needs to detect if the useris agreeing to its current argument or resisting thenew fact that is presented.
Because the dialoguemanagement system was developed from the per-spective of a system that could be easily ported todifferent domains, choice was made to use a domainindependent and robust agreement/disagreement de-tection.The agreement/disagreement detection is basedon an utterance classifier.
The classifier is a cas-cade of binary Support Vector Machines (SVM)(Vapnik, 2000) trained on the ICSI Meeting cor-pus (Janin et al, 2003).
The corpus contains 8135spurts3 annotated with agreement/disagreement in-formation Hillard, Ostendorf, and Shriberg (2003).A multi-class SVM classifier is trained on localfeatures of the spurts such as a) the length of thespurt, b) the first word of the spurt, c) the bigrams ofthe spurts, and d) part of speech tags.
The classifica-tion achieves an accuracy of 83.17% with an N-Fold4 ways split cross validation.
Additional results andcomparison with state-of-the-art are available in Ap-pendix A.During the dialogue, the classifier is applied oneach of the user?s utterances, trying to determine ifthe user is agreeing or disagreeing with the system.3speech utterances that have no pauses longer than .5 sec-onds.According to this labelling, the strategies describedin section 4.3.1 and 4.3.3 are applied.4.3.3 Revising the planThe reactive component will attempt local repairsto the plan by defending the argumentation movechosen by the planning component.
However, thereare cases when the user will still not accept an ar-gument.
In these cases, imposing the belief to theuser is counter-productive and the current goal be-lief should be dropped from the plan.For each utterance chosen by the reactive com-ponent, the belief model of the user is updated torepresent the system knowledge of the user?s be-liefs.
Every time the user agrees to an utterancefrom the system, the belief model is extended witha new belief; in the previous example, when theuser says ?I see, it could be useful then.
?, the sys-tem detects an agreement (see the Section 4.3.2)and extends the user?s beliefs model with the be-lief: can(helpatnight, item(flashlight)).
The agree-ment is then followed by a local repair, since theuser doesn?t disagree with the statement made, thesystem also extends the belief model with beliefs rel-evant to the content of the local repair, thus learningmore about the user?s belief model.As a result of this process, when the system de-cides to revise the plan, the planning componentdoes not start from the same beliefs state as previ-ously.
In effect, the system is able to learn user?s be-liefs based on the agreement/disagreement with theuser, it can therefore make a more effective use ofthe argumentation hierarchy to find a better plan toachieve the persuasive goals.Still, there are some cases when the planningcomponent will be unable to find a new plan fromthe current belief state to the goal belief state ?
thiscan happen when the planner has exhausted all itsargumentative moves for a particular sub-goal.
Inthese cases, the system has to make concessions anddrop the persuasive goals that it cannot fulfil.
Bydropping goals, the system will lower the final per-suasiveness, but guarantees not coercing the user.4.3.4 GenerationUtterance generation is made at the reactive com-ponent level.
In the current version of the dia-logue management system, the utterance generation142is based on an extended version of Alicebot AIML4.AIML is an XML language that provides a pat-tern/template generation model mainly used forchatbot systems.
An AIML bot defines a set ofcategories that associate a topic, the context of theprevious bot utterance (called that in the AIML ter-minology), a matching pattern that will match thelast user utterance and a generation template.
Thetopic, matching and that field define matching pat-terns that can contain * wildcards accepting any to-ken(s) of the user utterance (e.g.
HELLO * wouldmatch any utterance starting by ?Hello?).
They arelinked to a generation template that can reuse the to-kens matched by the patterns wildcards to generatean utterance tailored to the user input and the dia-logue context.For the purpose of layered dialogue management,the AIML language has been extended to includemore features: 1) A new pattern slot has been in-troduced to link a set of categories to a particular ar-gumentation operator; 2) Utterances generations arelinked to the belief they are trying to introduce tothe user and if an agreement is detected, this beliefis added to the user belief model.For example, a set of matching categories for theDesert Scenario could be:Plan operator: use world(goal(survive))Category 1 :Pattern *Template Surviving is ourpriority, do you wantto hear about my desertsurvival insights?Category 2 :Pattern * insightsThat * survival insightsTemplate I mean, I had a fewideas ...common knowledge Isuppose.Category 3 :Pattern *That * survival insightsTemplate Well, we are in thistogether.
Let me tell youof what I think of desertsurvival, ok?4http://www.alicebot.org/These three categories can be used to matchthe user reaction during the dialogue seg-ment corresponding to the plan operator:use world(goal(survive)).
Category 1 is usedas the initiative taking generation.
It will be thefirst one to be used when the system comes froma previously finished step.
Categories 2-3 are all?defenses?
that support Category 1.
They will beused to react to the user if no agreement is detectedfrom the last utterances.
For example, if the usersays ?what kind of survival insights???
as a replyto the generation from Category 1, a disagreementis detected and the reactive component will have acontextualised answer as given by category 2 whosethat pattern matches the last utterance from thesystem, the pattern pattern matches the userutterance.The dialogue management system uses 187 cate-gories tailored to the Desert Scenario as well as 3737general categories coming from the Alice chatbotand used to generate the dialogue smoothing utter-ances.
Developing domain specific reactions is a te-dious and slow process that was iteratively achievedwith Wizard of OZ experiments with real users.
Inthese experiments, users were told they were goingto have a dialogue with another human in the DesertScenario context.
The dialogue system manages thewhole dialogue, except for the generation phase thatis mediated by an expert that can either choose thereaction of the system from an existing set of utter-ances, or type a new one.5 Persuasiveness MetricEvaluating a behavior change would require a long-term observation of the behavior that would be de-pendent to external elements (Bickmore and Picard,2005).
To evaluate our system, an evaluation proto-col measuring the change in the beliefs underlyingthe behavior was chosen.
As explained in Section 3,the Desert Scenario is used as a base for the evalu-ation.
Each participant is told that he is stranded inthe desert.
The user gives a preferred initial rank-ing Ri of the items (knife, compass, map, etc.).
Theuser then engages in a dialogue with the system.
Thesystem then attempts to change the user?s ranking toa different ranking Rs through persuasive dialogue.At the end of the dialogue, the user can change this143choice to arrive at a final ranking Rf .The persuasiveness of the dialogue can be mea-sured as the evolution of the distance betweenthe user ranking (Ri, Rf ) and the system ranking(Rs).
The Kendall ?
distance (Kendall, 1938) isused to compute the pairwise disagreement betweentwo rankings.
The change of the Kendall ?
dis-tance during the dialogue gives an evaluation of thepersuasiveness of the dialogue: Persuasiveness =K?
(Ri, Rs) ?
K?
(Rf , Rs).
In the current evalu-ation protocol, the Rs is always the reverse of theRi, so K?
(Ri, Rs) is always the maximum distancepossible: n?
(n?1)2 where n is the number of items torank.
The minimum Kendall tau distance is 0.
If thesystem was persuasive enough to make the user in-vert the initial ranking, Persuasiveness of the systemis maximum and equal to: n?
(n?1)2 .
If the systemdoes not succeed in changing the user ranking, thenPersuasiveness is zero.6 Evaluation Results and Discussion16 participants have been recruited from a variety ofages (from 20 to 59) and background.
They wereall told to use a web application that describes theDesert Scenario (see Section 3) and proposes to un-dertake two instant messaging chats with two humanusers5.
However, both discussions are managed bydifferent versions of the dialogue system, followinga similar protocol:?
one version of the dialogue is managed by alimited version of the dialogue system, with noreactive component.
This version is similar toa purely task-oriented system, planning and re-vising the plan directly on dialogue failures,?
the second version is the full dialogue systemas described in this paper.Each participant went through one dialogue witheach system, in a random order.
This comparisonshows that the dialogue flexibility provided by thereactive component allows a more persuasive dia-logue.
In addition, when faced with the second dia-logue, the participant has formed more beliefs aboutthe scenario and is more able to counter argue.5The evaluation is available Online athttp://www.cs.york.ac.uk/aig/edenFigure 3: Comparative Results.
interpret, not coercive,perceived persuasion are on a scale of [0 ?
4] (see Ap-pendix B).
Persuasiveness is on a scale of [?10, 10].Figure 3 reports the independent Persuasivenessmetric results as well as interesting answers to aquestionnaire that the participants filled after eachdialogue (see the Appendix B for detailed resultsand questionnaire).Over all the dialogues, the full system is 18%more persuasive than the limited system.
This ismeasured by the Persuasiveness metric introduced inSection 5.
With the full system, the participants didan average of 1.33 swaps of items towards the sys-tem?s ranking.
With the limited system, the partic-ipants did an average of 0.47 swaps of items awayfrom the system?s ranking.
However, the answersto the self evaluated perceived persuasion questionshow that the participants did not see any significantdifference in the ability to persuade of the limitedand the full systems.According to the question interpret, the partici-pants found that the limited system understood bet-ter what they said.
This last result might be ex-plained by the behavior of the systems: the limitedsystem drops an argument at every user disagree-ment, making the user believe that the disagreementwas understood.
The full system tries to defend theargument; if possible with a contextually tailoredsupport, however, if this is not available, it may use ageneric support, making the user believe he was notfully understood.Our interpretation of the fact that the discrepancybetween user self evaluation of the interaction withthe system and the measured persuasion is that, evenif the full system is more argumentative, the user144didn?t feel coerced6.
These results show that a morepersuasive dialogue can be achieved without deteri-orating the user perception of the interaction.7 ConclusionOur dialogue management system introduces anovel approach to dialogue management by usinga layered model mixing the advantages of state-of-the-art dialogue management approaches.
A plan-ning component tailored to the task of argumenta-tion and persuasion searches the ideal path in an ar-gumentation model to persuade the user.
To give areactive and natural feel to the dialogue, this task-oriented layer is extended by a reactive componentinspired from the chatbot dialogue management ap-proach.
The Desert Scenario evaluation, providinga simple and independent metric for the persuasive-ness of the dialogue system provided a good proto-col for the evaluation of the dialogue system.
Thisone showed to be 18% more persuasive than a purelytask-oriented system that was not able to react to theuser interaction as smoothly.Our current research on the dialogue managementsystem consists in developing another evaluation do-main where a more complex utterance generationcan be used.
This will allow going further than thesimple template based system, offering more diverseanswers to the user and avoiding repetitions; it willalso allow us to experiment textual persuasion tai-lored to other parameters of the user representation,such as the user personality.ReferencesAllen, J. F., G. Ferguson, B. W. Miller, E. K. Ringger, andT.
Sikorski.
2000.
Dialogue Systems: From Theory toPractice in TRAINS-96, chapter 14.Bickmore, T. and T. Giorgino.
2004.
Some novel aspectsof health communication from a dialogue systems per-spective.
In AAAI Fall Symposium.Bickmore, T. W. and R. W. Picard.
2005.
Establishing andmaintaining long-term human-computer relationships.ACM Trans.
Comput.-Hum.
Interact., 12(2):293?327.Carenini, G. and J. Moore.
2000.
A strategy for generat-ing evaluative arguments.
In International Conferenceon Natural Language Generation.Dung, P. M. 1995.
On the acceptability of argumentsand its fundamental role in nonmonotonic reasoning,6The answers to the not coercive question do not show anysignificant difference in the perception of coercion of the twosystem.logic programming and n-person games.
Artif.
Intell.,77(2):321?357.Farzanfar, R., S. Frishkopf, J. Migneault, and R. Fried-man.
2005.
Telephone-linked care for physical ac-tivity: a qualitative evaluation of the use patterns ofan information technology program for patients.
J. ofBiomedical Informatics, 38(3):220?228.Festinger, Leon.
1957.
A Theory of Cognitive Disso-nance.
Stanford University Press.Freedman, R. 2000.
Plan-based dialogue management ina physics tutor.
In Proceedings of ANLP ?00.Galley, M., K. Mckeown, J. Hirschberg, and E. Shriberg.2004.
Identifying agreement and disagreement in con-versational speech: use of bayesian networks to modelpragmatic dependencies.
In Proceedings of ACL?04.Green, N. and J. F. Lehman.
2002.
An integrated dis-course recipe-based model for task-oriented dialogue.Discourse Processes, 33(2):133?158.Guerini, M., O.
Stock, and M. Zancanaro.
2004.
Per-suasive strategies and rhetorical relation selection.
InProceedings of ECAI-CMNA.Hillard, D., M. Ostendorf, and E. Shriberg.
2003.
Detec-tion of agreement vs. disagreement in meetings: train-ing with unlabeled data.
In Proceedings of NAACL?03.Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C. Wooters.
2003.
The ICSI meeting corpus.In Proceedings of ICASSP?03.Kendall, M. G. 1938.
A new measure of rank correlation.Biometrika, 30(1/2):81?93.Klein, J., Y.
Moon, and R. W. Picard.
1999.
This com-puter responds to user frustration.
In CHI?99.Lafferty, J. C., Eady, and J. Elmers.
1974.
The desertsurvival problem.Levy, D., R. Catizone, B. Battacharia, A. Krotov, andY.
Wilks.
1997.
Converse:a conversational compan-ion.
In Proceedings of 1st International Workshop onHuman-Computer Conversation.Mazzotta, I., F. de Rosis, and V. Carofiglio.
2007.
Por-tia: A user-adapted persuasion system in the healthy-eating domain.
Intelligent Systems, IEEE, 22(6).Moon, Y.
1998.
The effects of distance in local versusremote human-computer interaction.
In Proceedingsof SIGCHI?98.Norman, Timothy J. and Chris Reed.
2003.
Argumenta-tion Machines : New Frontiers in Argument and Com-putation (Argumentation Library).
Springer.Reed, C. 1998.
Generating Arguments in Natural Lan-guage.
Ph.D. thesis, University College London.Reiter, E., R. Robertson, and L. M. Osman.
2003.Lessons from a failure: generating tailored smokingcessation letters.
Artif.
Intell., 144(1-2):41?58.Stiff, J.
B. and P. A. Mongeau.
2002.
Persuasive Commu-nication, second edition.Vapnik, V. N. 2000.
The Nature of Statistical LearningTheory.Zinn, C., J. D. Moore, and M. G. Core.
2002.
A 3-tierplanning architecture for managing tutorial dialogue.In Proceedings of ITS ?02.145A Agreement/Disagreement ClassificationSetup 1 Setup 2Galley et al, global features 86.92% 84.07%Galley et al, local features 85.62% 83.11%Hillard et al 82% NASVM 86.47% 83.17%Table 1: Accuracy of different agreement/disagreementclassification approaches.The accuracy of state-of-the-art techniques(Hillard, Ostendorf, and Shriberg (2003) andGalley et al (2004)) are reported in Table 1 andcompared to our SVM classifier.
Two experimentalsetups were used:Setup 1 reproduces Hillard, Ostendorf, and Shriberg(2003) training/testing split between meetings;Setup 2 reproduces the N-Fold, 4 ways split used byGalley et al (2004).The SVM results are arguably lower than Galley et alsystem with labeled dependencies.
However, this is be-cause our system only relies on local features of eachutterance, while Galley et al (2004) use global features(i.e.
features describing relations between consecutive ut-terances) suggest that adding global features would alsoimprove the SVM classifier.B Evaluation QuestionnaireIn the evaluation described in section 6, the participantswere asked to give their level of agreement with eachstatement on the scale: Strongly disagree (0), Disagree(1), Neither agree nor disagree (2), Agree (3), StronglyAgree(4).
Table 2 provides a list of questions with theaverage agreement level and the result of a paired t-testbetween the two system results.146label question full system limited system ttestinterpret ?In the conversation, the other user inter-preted correctly what you said?1.73 2.13 0.06perceived persuasion ?In the conversation, the other user waspersuasive?2.47 2.53 0.44not coercive ?The other user was not forceful inchanging your opinion?2.4 2.73 0.15sluggish ?The other user was sluggish and slow toreply to you in this conversation?1.27 1.27 0.5understand ?The other user was easy to understandin the conversation?3.2 3.13 0.4pace ?The pace of interaction with the otheruser was appropriate in this conversa-tion?2.73 3.07 0.1friendliness ?The other user was friendly?
2.93 2.87 0.4length length of the dialogue 12min 19s 08min 33s 0.07persuasiveness Persuasiveness 1.33 -0.47 0.05Table 2: Results from the evaluation questionnaire.147
