Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145?150,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsOnline Plagiarism Detection Through Exploiting Lexical, Syntactic, andSemantic InformationWan-Yu Lin Nanyun Peng Chun-Chao Yen Shou-de LinGraduate Institute ofNetworking andMultimedia, NationalTaiwan UniversityInstitute ofComputationalLinguistic, PekingUniversityGraduate Institute ofNetworking andMultimedia, NationalTaiwan UniversityGraduate Institute ofNetworking andMultimedia, NationalTaiwan Universityr99944016@csie.ntu.edu.twpengnanyun@pku.edu.cnr96944016@csie.ntu.edu.twsdlin@csie.ntu.edu.twAbstractIn this paper, we introduce a framework thatidentifies online plagiarism by exploiting lexical,syntactic and semantic features that includesduplication-gram, reordering and alignment ofwords, POS and phrase tags, and semanticsimilarity of sentences.
We establish an ensembleframework to combine the predictions of eachmodel.
Results demonstrate that our system cannot only find considerable amount of real-worldonline plagiarism cases but also outperformsseveral state-of-the-art algorithms and commercialsoftware.KeywordsPlagiarism Detection, Lexical, Syntactic, Semantic1.
IntroductionOnline plagiarism, the action of trying to create anew piece of writing by copying, reorganizing orrewriting others?
work identified through searchengines, is one of the most commonly seenmisusage of the highly matured web technologies.As implied by the experiment conducted by(Braumoeller and Gaines, 2001), a powerfulplagiarism detection system can effectivelydiscourage people from plagiarizing others?
work.A common strategy people adopt for online-plagiarism detection is as follows.
First theyidentify several suspicious sentences from thewrite-up and feed them one by one as a query to asearch engine to obtain a set of documents.
Thenhuman reviewers can manually examine whetherthese documents are truly the sources of thesuspicious sentences.
While it is quitestraightforward and effective, the limitation of thisstrategy is obvious.
First, since the length of searchquery is limited, suspicious sentences are usuallyqueried and examined independently.
Therefore, itis harder to identify document level plagiarismthan sentence level plagiarism.
Second, manuallychecking whether a query sentence plagiarizescertain websites requires specific domain andlanguage knowledge as well as considerableamount of energy and time.
To overcome theabove shortcomings, we introduce an onlineplagiarism detection system using natural languageprocessing techniques to simulate the abovereverse-engineering approach.
We develop anensemble framework that integrates lexical,syntactic and semantic features to achieve this goal.Our system is language independent and we haveimplemented both Chinese and English versionsfor evaluation.2.
Related WorkPlagiarism detection has been widely discussed inthe past decades (Zou et al, 2010).
Table 1.summarizes some of them:AuthorComparisonUnitSimilarity FunctionBrin et al,1995Word +SentencePercentage of matchingsentences.White andJoy, 2004SentenceAverage overlap ratio ofthe sentence pairs using 2pre-defined thresholds.Niezgodaand Way,2006A humandefinedslidingwindowSliding windows rankedby the average length perword.Cedeno andRosso,  2009Sentence +n-gramOverlap percentage of n-gram in the sentence pairs.145Pera and Ng,2010SentenceA pre-definedresemblance functionbased on word correlationfactor.Stamatatos,2011PassageOverlap percentage ofstopword n-grams.Grman andRavas, 2011PassageMatching percentage ofwords with giventhresholds on both ratioand absolute number ofwords in passage.Table 1.
Summary of related worksComparing to those systems, our system exploitsmore sophisticated syntactic and semanticinformation to simulate what plagiarists are tryingto do.There are several online or charged/freedownloadable plagiarism detection systems such asTurnitin, EVE2, Docol?
c, and CATPPDS whichdetect mainly verbatim copy.
Others such asMicrosoft Plagiarism Detector (MPD), Safeassign,Copyscape and VeriGuide, claim to be capable ofdetecting obfuscations.
Unfortunately thosecommercial systems do not reveal the detailstrategies used, therefore it is hard to judge andreproduce their results for comparison.3.
MethodologyFigure 1.
Detection FlowThe data flow is shown above in Figure 1.3.1 Query a Search EngineWe first break down each article into a series ofqueries to query a search engine.
Several systemssuch as (Liu at al., 2007) have proposed a similaridea.
The main difference between our method andtheirs is that we send unquoted queries rather thanquoted ones.
We do not require the search resultsto completely match to the query sentence.
Thisstrategy allows us to not only identify thecopy/paste type of plagiarism but also re-write/edittype of plagiarism.3.2 Sentence-based Plagiarism DetectionSince not all outputs of a search engine contain anexact copy of the query, we need a model toquantify how likely each of them is the source ofplagiarism.
For better efficiency, our experimentexploits the snippet of a search output to representthe whole document.
That is, we want to measurehow likely a snippet is the plagiarized source of thequery.
We designed several models which utilizedrich lexical, syntactic and semantic features topursue this goal, and the details are discussedbelow.3.2.1 Ngram Matching (NM)One straightforward measure is to exploit the n-gram similarity between source and target texts.We first enumerate all n-grams in source, and thencalculate the overlap percentage with the n-gramsin the target.
The larger n is, the harder for thisfeature to detect plagiarism with insertion,replacement, and deletion.
In the experiment, wechoose n=2.3.2.2 Reordering of Words (RW)Plagiarism can come from the reordering of words.We argue that the permutation distance between S1and S2 is an important indicator for reorderedplagiarism.
The permutation distance is defined asthe minimum number of pair-wise exchanging ofmatched words needed to transform a sentence, S2,to contain the same order of matched words asanother sentence, S1.
As mentioned in (S?rensenaand Sevaux, 2005), the permutation distance canbe calculated by the following expression?
?1, ?2 =   ?????=?+1??1?=1where???
=1, ??
?1 ?
> ?1 ?
???
?2 ?
< ?2 ?0, ????????
?S1(i) and S2(i) are indices of the ith matchedword in sentences S1 and S2 respectively and n isthe number of matched words between  thesentences S1 and S2.
Let ?
=n2?
n2be thenormalized term, which is the maximum possibledistance between S1 and S2, then the reordering146score of the two sentences, expressed as s(S1, S2),will be s S1 , S2  = 1 ?d S1 ,S2?3.2.3 Alignment of Words (AW)Besides reordering, plagiarists often insert ordelete words in a sentence.
We try to model suchbehavior by finding the alignment of two wordsequences.
We perform the alignment using adynamic programming method as mentioned in(Wagner and Fischer, 1975).However, such alignment score does not reflectthe continuity of the matched words, which can bean important cue to identify plagiarism.
Toovercome such drawback, we modify the score asbelow.New Alignment Score =??|?
|?1?=1|?|?1where    ??
=1# ??
?????
???????
??
,?
?+1 +1M is the list of matched words, and Mi is the ithmatched word in M. This implies we prefer fewerunmatched words in between two matched ones.3.2.4 POS and Phrase Tags of Words (PT, PP)Exploiting only lexical features can sometimesresult in some false positive cases because two setsof matched words can play different roles in thesentences.
See S1 and S2 in Table 2. as a possiblefalse positive case.S1: The man likes the womanS2: The woman is like the manWord S1: Tag S2: Tag S1: Phrase S2: Phraseman NN NN NP PPlike VBZ IN VP PPwoman NN NN VP NPTable 2.
An example of matched words with differenttags and phrasesTherefore, we further explore syntactic featuresfor plagiarism detection.
To achieve this goal, weutilize a parser to obtain POS and phrase tags ofthe words.
Then we design an equation to measurethe tag/phrase similarity.Sim =???
????
???
?????
???
?
?????????
??????
????
???
????
?We paid special attention to the case thattransforms a sentence from an active form to apassive-form or vice versa.
A subject originally ina Noun Phrase can become a Preposition Phrase,i.e.
?by ?
?, in the passive form while the object ina Verb Phrase can become a new subject in a NounPhrase.
Here we utilize the Stanford Dependencyprovided by Stanford Parser to match thetag/phrase between active and passive sentences.3.2.5 Semantic Similarity (LDA)Plagiarists, sometimes, change words or phrases tothose with similar meanings.
While previous works(Y. Lin et al, 2006) often explore semanticsimilarity using lexical databases such as WordNetto find synonyms, we exploit a topic model,specifically latent Dirichlet alocation (LDA, D. M.Blei et al, 2003), to extract the semantic featuresof sentences.
Given a set of documents representedby their word sequences, and a topic number n,LDA learns the word distribution for each topicand the topic distribution for each document whichmaximize the likelihood of the word co-occurrencein a document.
The topic distribution is often takenas semantics of a document.
We use LDA to obtainthe topic distribution of a query and a candidatesnippet, and compare the cosine similarity of themas a measure of their semantic similarity.3.3 Ensemble Similarity ScoresUp to this point, for each snippet the systemgenerates six similarity scores to measure thedegree of plagiarism in different aspects.
In thisstage, we propose two strategies to linearlycombine the scores to make better prediction.
Thefirst strategy utilizes each model?s predictability(e.g.
accuracy) as the weight to linearly combinethe scores.
In other words, the models that performbetter individually will obtain higher weights.
Inthe second strategy we exploit a learning model (inthe experiment section we use Liblinear) to learnthe weights directly.3.4 Document Level Plagiarism DetectionFor each query from the input article, our systemassigns a degree-of-plagiarism score to someplausible source URLs.
Then, for each URL, thesystem sums up all the scores it obtains as the finalscore for document-level degree-of-plagiarism.
Weset up a cutoff threshold to obtain the mostplausible URLs.
At the end, our system highlightsthe suspicious areas of plagiarism for display.1474.
EvaluationWe evaluate our system from two different angles.We first evalaute the sentence level plagirismdetection using the PAN corpus in English.
Wethen evaluate the capability of the full system todetect on-line plagiarism cases using annotatedresults in Chinese.4.1 Sentence-based EvaluationsWe want to compare our model with the state-of-the-art methods, in particular the winning entries inplagiarism detection competition in PAN 1 .However, the competition in PAN is designed foroff-line plagiarism detection; the entries did notexploit an IR system to search the Web like we do.Nevertheless, we can still compare the corecomponent of our system, the sentence-basedmeasuring model with that of other systems.
Toachieve such goal, we first randomly sampled 370documents from PAN-2011 external plagiarismcorpus (M. Potthast et al, 2010) containing 2882labeled plagiarism cases.To obtain high-quality negative examples forevaluation, we built a full-text index on the corpususing Lucene package.
Then we use the suspiciouspassages as queries to search the whole datasetusing Lucene.
Since there is length limitation inLucene (as well as in the real search engines), wefurther break the 2882 plagiarism cases into 6477queries.
We then extract the top 30 snippetsreturned by the search engine as the potentialnegative candidates for each plagiarism case.
Notethat for each suspicious passage, there is only onetarget passage (given by the ground truth) that isconsidered as a positive plagiarism case in this data,and it can be either among these 30 cases or not.However, we union these 30 cases with the groundtruth as a set, and use our (as well as thecompetitors?)
models to rank the degree-of-plagiarism for all the candidates.
We then evaluatethe rank by the area-under-PR-curve (AUC) score.We compared our system with the winning entry ofPAN 2011 (Grman and Ravas, 2011) and thestopword ngram model that claims to performbetter than this winning entry by Stamatatos (2011).The results of each individual model and ensembleusing 5-fold cross validation are listed in Table 3.It shows that NM is the best individual model, and1 The website of PAN-2011 is http://pan.webis.de/an ensemble of three features outperforms thestate-of-the-art by 26%.NM RW AW PT PP LDA0.876 0.596 0.537 0.551 0.521 0.596(a)Ours ensemblePan-11ChampionStopwordNgramAUC0.882(NM+RW+PP)0.620 0.596(b)Table 3.
(a) AUC for each individual model (b) AUC ofour ensemble and other state-of-the-art algorithms4.2 Evaluating the Full SystemTo evaluate the overall system, we manuallycollect 60 real-world review articles from theInternet for books (20), movies (20), and musicalbums (20).
Unfortunately for an online systemlike ours, there is no ground truth available forrecall measure.
We conduct two differementevalautions.
First we use the 60 articles as inputs toour system, ask 5 human annotators to checkwhether the articles returned by our system can beconsidered as plagiarism.
Among all 60 reviewarticles, our system identifies a considerablely highnumber of copy/paste articles, 231 in total.However, identifying this type of plagiarism istrivial, and has been done by many similar tools.Instead we focus on the so-called smart-plagiarismwhich cannot be found through quoting a query ina search engine.
Table 4. shows the precision ofthe smart-plagiarism articles returned by oursystem.
The precision is very high and outperformsa commertial tool Microsoft Plagiarism Detector.Book Movie MusicOurs280/288(97%)88/110(80%)979/1033(95%)MPD44/53(83%)123/172(72%)120/161(75%)Table 4.
Precision of Smart PlagiarismIn the second evaluation, we first choose 30reviews randomly.
Then we use each of them asqueries into Google and retrieve a total of 5636pieces of snippet candidates.
We then ask 63human beings to annotate whether those snippetsrepresent plagiarism cases of the original reviewarticle.
Eventually we have obtained an annotated148dataset and found a total of 502 plagiarizedcandidates with 4966 innocent ones for evalaution.Table 5. shows the average AUC of 5-fold crossvalidation.
The results show that our methodoutperforms the Pan-11 winner slightly, and muchbetter than the Stopword Ngram.NM RW AW PT PP LDA0.904 0.778 0.874 0.734 0.622 0.581(a)Ours ensemblePan-11ChampionStopwordNgramAUC0.919(NM+RW+AW+PT+PP+LDA)0.893 0.568(b)Table 5.
(a) AUC for each individual model (b) AUC ofour ensemble and other state-of-the-art algorithms4.3 DiscussionThere is some inconsistency of the performance ofsingle features in these two experiments.
The mainreason we believe is that the plagiarism cases werecreated in very different manners.
Plagiarism casesin PAN external source are created artificiallythrough word insertions, deletions, reordering andsynonym substitutions.
As a result, features such asword alignment and reordering do not performwell because they did not consider the existence ofsynonym word replacement.
On the other hand,real-world plagiarism cases returned by Google arethose with matching-words, and we can find betterperformance for AW.The performances of syntactic and semanticfeatures, namely PT, PP and LDA, are consistentlyinferior than other features.
It is because they oftenintroduce false-positives as there are some non-plagiarism cases that might have highly overlappedsyntactic or semantic tags.
Nevertheless,experiments also show that these features canimprove the overall accuracy in ensemble.We also found that the stopword Ngram modelis not applicable universally.
For one thing, it isless suitable for on-line plagiarism detection, as thelength limitation for queries diminishes theusability of stopword n-grams.
For another,Chinese seems to be a language that does not relyas much on stopwords as the latin languages do tomaintain its syntax structure.Samples of our system?s finding can be foundhere, http://tinyurl.com/6pnhurz5.
Online Demo SystemWe developed an online demos system usingJAVA (JDK 1.7).
The system currently supportsthe detection of documents in both English andChinese.
Users can either upload the plain text fileof a suspicious document, or copy/paste thecontent onto the text area, as shown below inFigure 2.Figure 2.
Input Screen-ShotThen the system will output some URLs andsnippets as the potential source of plagiarism.
(seeFigure 3.
)Figure 3.
Output Screen-Shot6.
ConclusionComparing with other online plagiarismdetection systems, ours exploit more sophisticatedfeatures by modeling how human beings plagiarizeonline sources.
We have exploited sentence-levelplagiarism detection on lexical, syntactic andsemantic levels.
Another noticeable fact is that ourapproach is almost language independent.
Given aparser and a POS tagger of a language, ourframework can be extended to support plagiarismdetection for that language.1497.
ReferencesSalha Alzahrani, Naomie Salim, and Ajith Abraham,?Understanding Plagiarism Linguistic Patterns,Textual Features and Detection Methods ?
in IEEETransactions on systems , man and cyberneticsPart C:Applications and reviews, 2011D.
M.  Blei,  A.  Y. Ng, M. I. Jordan,  and J. Lafferty.Latent dirichlet alocation.
Journal of MachineLearning Research, 3:2003, 2003.Bear F. Braumoeller and Brian J. Gaines.
2001.
ActionsDo Speak Louder Than Words: Deterring Plagiarismwith the Use of Plagiarism-Detection Software.
InPolitical Science & Politics, 34(4):835-839.Sergey Brin, James Davis, and Hector Garcia-molina.1995.
Copy Detection Mechanisms for DigitalDocuments.
In Proceedings of the ACM SIGMODAnnual Conference, 24(2):398-409.Alberto Barr?n Cede?o and Paolo Rosso.
2009.
OnAutomatic Plagiarism Detection based on n-gramsComparison.
In Proceedings of the 31th EuropeanConference on IR Research on Advances inInformation Retrieval, ECIR 2009, LNCS 5478:696-700, Springer-Verlag, and Berlin Heidelberg,Jan Grman and Rudolf Ravas.
2011.
Improvedimplementation for finding text similarities in largecollections of data.In Proceedings of PAN 2011.NamOh Kang, Alexander Gelbukh, and SangYong Han.2006.
PPChecker: Plagiarism Pattern Checker inDocument Copy Detection.
In Proceedings of TSD-2006, LNCS, 4188:661-667.Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?
Shea, and Keeley Crockett.
2006.
SentenceSimilarity Based on Semantic Nets and CorpusStatistics.
In Proceedings of the IEEE Transactionson Knowledge and Data Engineering, 18(8):1138-1150.Yi-Ting Liu, Heng-Rui Zhang, Tai-Wei Chen, and Wei-Guang Teng.
2007.
Extending Web Search forOnline Plagiarism Detection.
In Proceedings of theIEEE International Conference on Information Reuseand Integration, IRI 2007.Caroline Lyon, Ruth Barrett, and James Malcolm.
2004.A Theoretical Basis to the Automated Detection ofCopying Between Texts, and its PracticalImplementation in the Ferret Plagiarism andCollusion Detector.
In Proceedings of Plagiarism:Prevention, Practice and Policies 2004 Conference.Sebastian Niezgoda and Thomas P. Way.
2006.SNITCH: A Software Tool for Detecting Cut andPaste Plagiarism.
In Proceedings of the 37th SIGCSETechnical Symposium on Computer ScienceEducation, p.51-55.Maria Soledad Pera and Yiu-kai Ng.
2010.
IOS PressSimPaD: A Word-Similarity Sentence-BasedPlagiarism Detection Tool on Web Documents.
InJournal on Web Intelligence and Agent Systems, 9(1).Xuan-Hieu Phan and Cam-Tu Nguyen.
GibbsLDA++:A C/C++ implementation of latent Dirichletallocation (LDA), 2007Martin Potthast, Benno Stein, Alberto Barr?n Cede?o,and Paolo Rosso.
An Evaluation Framework forPlagiarism Detection.
In 23rd InternationalConference on Computational Linguistics (COLING10), August 2010.
Association for ComputationalLinguistics.Kenneth S?rensena and Marc Sevaux.
2005.Permutation Distance Measures for MemeticAlgorithms with Population Management.
InProceedings of 6th Metaheuristics InternationalConference.Efstathios Stamatatos, "Plagiarism Detection Based onStructural Information" in Proceedings of the 20thACM international conference on Information andknowledge management, CIKM'11Robert A. Wagner and Michael J. Fischer.
1975.
TheString-to-string correction problem.
In Journal of theACM, 21(1):168-173.Daniel R. White and Mike S. Joy.
2004.
Sentence-BasedNatural Language Plagiarism Detection.
In Journalon Educational Resources in Computing JERICHomepage archive, 4(4).Du Zou, Wei-jiang Long, and Zhang Ling.
2010.
ACluster-Based Plagiarism Detection Method.
In LabReport for PAN at CLEF 2010.150
