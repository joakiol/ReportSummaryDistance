Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 34?42,COLING 2010, Beijing, August 2010.Phrase Based Decoding using a Discriminative ModelPrasanth KolachinaLTRC, IIIT-Hyderabad{prasanth k}@research.iiit.ac.inSriram VenkatapathyLTRC, IIIT-Hyderabad{sriram}@research.iiit.ac.inSrinivas BangaloreAT&T Labs-Research, NY{srini}@research.att.comSudheer KolachinaLTRC, IIIT-Hyderabad{sudheer.kpg08}@research.iiit.ac.inAvinesh PVSLTRC, IIIT-Hyderabad{avinesh}@research.iiit.ac.inAbstractIn this paper, we present an approach tostatistical machine translation that com-bines the power of a discriminative model(for training a model for Machine Transla-tion), and the standard beam-search baseddecoding technique (for the translation ofan input sentence).
A discriminative ap-proach for learning lexical selection andreordering utilizes a large set of featurefunctions (thereby providing the power toincorporate greater contextual and linguis-tic information), which leads to an effec-tive training of these models.
This modelis then used by the standard state-of-artMoses decoder (Koehn et al, 2007) for thetranslation of an input sentence.We conducted our experiments onSpanish-English language pair.
We usedmaximum entropy model in our exper-iments.
We show that the performanceof our approach (using simple lexicalfeatures) is comparable to that of thestate-of-art statistical MT system (Koehnet al, 2007).
When additional syntacticfeatures (POS tags in this paper) are used,there is a boost in the performance whichis likely to improve when richer syntacticfeatures are incorporated in the model.1 IntroductionThe popular approaches to machine translationuse the generative IBM models for training(Brown et al, 1993; Och et al, 1999).
The param-eters for these models are learnt using the stan-dard EM Algorithm.
The parameters used in thesemodels are extremely restrictive, that is, a simple,small and closed set of feature functions is usedto represent the translation process.
Also, thesefeature functions are local and are word based.
Inspite of these limitations, these models performvery well for the task of word-alignment becauseof the restricted search space.
However, they per-form poorly during decoding (or translation) be-cause of their limitations in the context of a muchlarger search space.To handle the contextual information, phrase-based models were introduced (Koehn et al,2003).
The phrase-based models use the wordalignment information from the IBM models andtrain source-target phrase pairs for lexical se-lection (phrase-table) and distortions of sourcephrases (reordering-table).
These models are stillrelatively local, as the target phrases are tightly as-sociated with their corresponding source phrases.In contrast to a phrase-based model, a discrim-inative model has the power to integrate muchricher contextual information into the trainingmodel.
Contextual information is extremely use-ful in making lexical selections of higher quality,as illustrated by the models for Global Lexical Se-lection (Bangalore et al, 2007; Venkatapathy and34Bangalore, 2009).However, the limitation of global lexical se-lection models has been sentence construction.In global lexical selection models, lattice con-struction and scoring (LCS) is used for the pur-pose of sentence construction (Bangalore et al,2007; Venkatapathy and Bangalore, 2009).
In ourwork, we address this limitation of global lexi-cal selection models by using an existing state-of-art decoder (Koehn et al, 2007) for the purposeof sentence construction.
The translation modelused by this decoder is derived from a discrimina-tive model, instead of the usual phrase-table andreordering-table construction algorithms.
This al-lows us to use the effectiveness of an existingphrase-based decoder while retaining the advan-tages of the discriminative model.
In this paper,we compare the sentence construction accuraciesof lattice construction and scoring approach (seesection 4.1 for LCS Decoding) and the phrase-based decoding approach (see section 4.2).Another advantage of using a discriminative ap-proach to construct the phrase table and the re-ordering table is the flexibility it provides to in-corporate linguistic knowledge in the form of ad-ditional feature functions.
In the past, factoredphrase-based approaches for Machine Translationhave allowed the use of linguistic feature func-tions.
But, they are still bound by the local-ity of context, and definition of a fixed struc-ture of dependencies between the factors (Koehnand Hoang, 2007).
Furthermore, factored phrase-based approaches place constraints both on thetype and number of factors that can be incorpo-rated into the training.
In this paper, though we donot extensively test this aspect, we show that us-ing syntactic feature functions does improve theperformance of our approach, which is likely toimprove when much richer syntactic feature func-tions (such as information about the parse struc-ture) are incorporated in the model.As the training model in a standard phrase-based system is relatively impoverished with re-spect to contextual/linguistic information, integra-tion of the discriminative model in the form ofphrase-table and reordering-table with the phrase-based decoder is highly desirable.
We propose todo this by defining sentence specific tables.
Forexample, given a source sentence s, the phrase-table contains all the possible phrase-pairs condi-tioned on the context of the source sentence s.In this paper, the key contributions are,1.
We combine a discriminative training modelwith a phrase-based decoder.
We ob-tained comparable results with the state-of-art phrase-based decoder.2.
We evaluate the performance of the latticeconstruction and scoring (LCS) approach todecoding.
We observed that even though thelexical accuracy obtained using LCS is high,the performance in terms of sentence con-struction is low when compared to phrase-based decoder.3.
We show that the incorporation of syntacticinformation (POS tags) in our discriminativemodel boosts the performance of translation.In future, we plan to use richer syntactic fea-ture functions (which the discriminative ap-proach allows us to incorporate) to evaluatethe approach.The paper is organized in the following sec-tions.
Section 2 presents the related work.
Insection 3, we describe the training of our model.In section 4, we present the decoding approaches(both LCS and phrase-based decoder).
We de-scribe the data used in our experiments in section5.
Section 6 consists of the experiments and re-sults.
Finally we conclude the paper in section 7.2 Related WorkIn this section, we present approaches that are di-rectly related to our approach.
In Direct Trans-lation Model (DTM) proposed for statistical ma-chine translation by (Papineni et al, 1998; Ochand Ney, 2002), the authors present a discrimi-native set-up for natural language understanding(and MT).
They use a slightly modified equation(in comparison to IBM models) as shown in equa-tion 1.
In equation 1, they consider the translationmodel from f ?
e (p(e|f)), instead of the the-oretically sound (after the application of Bayes?rule), e ?
f (p(f |e)) and use grammatical fea-tures such as the presence of equal number of35verbs forms etc.e?
= argmaxepTM (e|f) ?
pLM (e) (1)In their model, they use generic feature func-tions such as language model, cooccurence fea-tures such as presence of a lexical relationship inthe lexicon.
Their search algorithm limited the useof complex features.Direct Translation Model 2 (DTM2) (Itty-cheriah and Roukos, 2007) expresses the phrase-based translation task in a unified log-linear prob-abilistic framework consisting of three compo-nents:1. a prior conditional distribution P02.
a number of feature functions ?i() that cap-ture the effects of translation and languagemodel3.
the weights of the features ?i that are esti-mated using MaxEnt training (Berger et al,1996) as shown in equation 2.Pr(e|f) = P0(e, j|f)Z exp?i?i?i(e, j, f) (2)In the above equation, j is the skip reorderingfactor for the phrase pair captured by?i() and rep-resents the jump from the previous source word.Z represents the per source sentence normaliza-tion term (Hassan et al, 2009).
While a uni-form prior on the set of futures results in a max-imum entropy model, choosing other priors out-put a minimum divergence models.
Normalizedphrase count has been used as the prior P0 in theDTM2 model.The following decision rule is used to obtain opti-mal translation.e?
= argmaxePr(e|f)= argmaxeM?m=1?m?m(f, e)(3)The DTM2 model differs from other phrase-based SMT models in that it avoids the redun-dancy present in other systems by extracting froma word aligned parallel corpora a set of minimalphrases such that no two phrases overlap witheach other (Hassan et al, 2009).The decoding strategy in DTM2 (Ittycheriahand Roukos, 2007) is similar to a phrase-based de-coder except that the score of a particular transla-tion block is obtained from the maximum entropymodel using the set of feature functions.
In ourapproach, instead of providing the complete scor-ing function ourselves, we compute the parame-ters needed by a phrase based decoder, which inturn uses these parameters appropriately.
In com-parison with the DTM2, we also use minimal non-overlapping blocks as the entries in the phrase ta-ble that we generate.Xiong et al (2006) present a phrase reorderingmodel under the ITG constraint using a maximumentropy framework.
They model the reorderingproblem as a two-class classification problem, theclasses being straight and inverted.
The model isused to merge the phrases obtained from trans-lating the segments in a source sentence.
Thedecoder used is a hierarchical decoder motivatedfrom the CYK parsing algorithm employing abeam search algorithm.
The maximum entropymodel is presented with features extracted fromthe blocks being merged and probabilities are es-timated using the log-linear equation shown in(4).
The work in addition to lexical features andcollocational features, uses an additional metriccalled the information gain ratio (IGR) as a fea-ture.
The authors report an improvement of 4%BLEU score over the traditional distance baseddistortion model upon using the lexical featuresalone.p?
(y|x) =1Z?
(x)exp(?i?i?i(x, y)) (4)3 TrainingThe training process of our approach has twosteps:1. training the discriminative models for trans-lation and reordering.2.
integrating the models into a phrase baseddecoder.36The input to our training step are the word-alignments between source and target sentencesobtained using GIZA++ (implementation of IBM,HMM models).3.1 Training discriminative modelsWe train two models, one to model the transla-tion of source blocks, and the other to model thereordering of source blocks.
We call the transla-tion model a ?context dependent block translationmodel?
for two reasons.1.
It is concerned with the translation of mini-mal phrasal units called blocks.2.
The context of the source block is used dur-ing its translation.The word alignments are used to obtain the setof possible target blocks, and are added to the tar-get vocabulary.
A target block b is a sequence of nwords that are paired with a sequence ofm sourcewords (Ittycheriah and Roukos, 2007).
In our ap-proach, we restrict ourselves to target blocks thatare associated with only one source word.
How-ever, this constraint can be easily relaxed.Similarly, we call the reordering model, a ?con-text dependent block distortion model?.
For train-ing, we use the maximum entropy software libraryLlama presented in (Haffner, 2006).3.1.1 Context Dependent Block TranslationModelIn this model, the goal is to predict a targetblock given the source word and contextual andsyntactic information.
Given a source word and itslexical context, the model estimates the probabil-ities of the presence or absence of possible targetblocks (see Figure 1).The probabilities of the candidate target blocksare obtained from the maximum entropy model.The probability pei of a candidate target block eiis estimated as given in equation 5pei = P (true|ei, fj , C) (5)where fj is the source word corresponding to eiand C is its context.Using the maximum entropy model, binaryclassifiers are trained for every target block in thecontext windowsource wordword syntactically dependentSOURCE SENTENCEtarget word 1 prob p1............target word 2 prob p2prob pKtarget word Kon source wordFigure 1: Word prediction modelvocabulary.
These classifiers predict if a particu-lar target block should be present given the sourceword and its context.
This model is similar to theglobal lexical selection (GLS) model described in(Bangalore et al, 2007; Venkatapathy and Banga-lore, 2009) except that in GLS, the predicted tar-get blocks are not associated with any particularsource word unlike the case here.For the set of experiments in this paper, we useda context of size 6, containing three words to theleft and three words to the right.
We also usedthe POS tags of words in the context window asfeatures.
In future, we plan to use the words syn-tactically dependent on a source word as globalcontext(shown in Figure 1).3.1.2 Context Dependent Block DistortionModelAn IBM model 3 like distortion model istrained to predict the relative position of a sourceword in the target given its context.
Given asource word and its context, the model estimatesthe probability of particular relative position be-ing an appropriate position of the source word inthe target (see Figure 2).context windowsource wordSOURCE SENTENCE0p01p1 2p2 wpw?1p?1?2p?2?wp?w ......word syntactically dependenton source wordFigure 2: Position prediction modelUsing a maximum entropy model similar to37the one described in the context dependent blocktranslation model, binary classifiers are trainedfor every possible relative position in the target.These classifiers output a probability distributionover various relative positions given a source wordand its context.The word alignments in the training corpus areused to train the distortion model.
While comput-ing the relative position, the difference in sentencelengths is also taken into account.
Hence, the rela-tive position of the target block located at positioni corresponding to the source word located at po-sition j is given in equation 6.r = round(i ?
mn ?
j) (6)where, m is the length of source sentence and n isthe number of target blocks.
round is the functionto compute the nearest integer of the argument.
Ifthe source word is not aligned to any target word,a special symbol ?INF?
is used to indicate such acase.
In our model, this symbol is also a part ofthe target distribution.The features used to train this model are thesame as those used for the block translationmodel.
In order to use further lexical information,we also incorporated information about the targetword for predicting the distribution.
The informa-tion about possible target words is obtained fromthe ?context dependent block translation model?.The probabilities in this case are measured asshown in equation 7pr,ei = P (true|r, ei, fj , C) (7)3.2 Integration with phrase-based decoderThe discriminative models trained are sentencespecific, i.e.
the context of the sentence is usedto make predictions in these models.
Hence,the phrase-based decoder is required to use in-formation specific to a source sentence.
In orderto handle this issue, a different phrase-table andreordering-table are constructed for every inputsentence.
The phrase-table and reordering-tableare constructed using the discriminative modelstrained earlier.In Moses (Koehn et al, 2007), the phrase-table contains the source phrase, the target phraseand the various scores associated with the phrasepair such as phrase translation probability, lexicalweighting, inverse phrase translation probability,etc.1In our approach, given a source sentence, thefollowing steps are followed to construct thephrase table.1.
Extract source blocks (?words?
in this work)2.
Use the ?context dependent block translationmodel?
to predict the possible target blocks.The set of possible blocks can be predictedusing two criteria, (1) Probability threshold,and (2) K-best.
Here, we use a thresholdvalue to prune the set of possible candidatesin the target vocabulary.3.
Use the prediction probabilities to assignscores to the phrase pairs.A similar set of steps is used to construct thereordering-table corresponding to an input sen-tence in the source language.4 Decoding4.1 Decoding with LCS DecoderThe lattice construction and scoring algorithm, asthe name suggests, consists of two steps,1.
Lattice constructionIn this step, a lattice representing variouspossible target sequences is obtained.
In theapproach for global lexical selection (Banga-lore et al, 2007; Venkatapathy and Banga-lore, 2009), the input to this step is a bag ofwords.
The bag of words is used to constructan initial sequence (a single path lattice).
Tothis sequence, deletion arcs are added to in-corporate additional paths (at a cost) that fa-cilitate deletion of words in the initial se-quence.
This sequence is permuted using apermutation window in order to construct alattice representing possible sequences.
Thepermutation window is used to control thesearch space.In our experiments, we used a similar processfor sentence construction.
Using the con-text dependent block translation algorithm,1http://www.statmt.org/moses/?n=FactoredTraining.ScorePhrases38we obtain a number of translation blocks forevery source word.
These blocks are inter-connected in order to obtain the initial lattice(see figure 3).f_(i?1) f_(i) f_(i+1)t_(i?1,1)t_(i?1,2)t_(i?1,3)t_(i,2)t_(i,1) t_(i+1,1)t_(i+1,2)t_(i+1,3)....
...............SOURCE SENTENCEINTIAL TARGET LATTICEFigure 3: Lattice ConstructionTo control deletions at various source posi-tions, deletion nodes may be added to theinitial lattice.
This lattice is permuted us-ing a permutation window to construct a lat-tice representing possible sequences.
Hence,the parameters that dictate lattice construc-tion are, (1) Threshold for lexical selection,(2) Using deletion arcs or not, and (3) Per-mutation window.2.
ScoringIn this step, each of the paths in the latticeconstructed in the earlier step is scored us-ing a language model (Haffner, 2006), whichis same as the one used in the sentence con-struction in global lexical selection models.It is to be noted that we do not use the dis-criminative reordering model in this decoder,and only the language model is used to scorevarious target sequences.The path with the lowest score is consideredthe best possible target sentence for the givensource sentence.
Using this decoder, we con-ducted experiments on the development set byvarying threshold values and the size of the per-mutation window.
The best parameter values ob-tained using the development set were used for de-coding the test corpus.4.2 Decoding with Moses DecoderIn this approach, the phrase-table and thereordering-table are constructed using the dis-criminative model for every source sentence (seesection 3.2).
These tables are then used by thestate-of-art Moses decoder to obtain correspond-ing translations.The various training and decoding parametersof the discriminative model are computed by ex-haustively exploring the parameter space, and cor-respondingly measuring the output quality on thedevelopment set.
The best set of parameters wereused for decoding the sentences in the test corpus.We modified the weights assigned by MOSES tothe translation model, reordering model and lan-guage model.
Experiments were conducted byperforming pruning on the options in the phrasetable and by using the word penalty feature inMOSES.We trained a language model of order 5 built onthe entire EUROPARL corpus using the SRILMpackage.
The method uses improved Kneser-Neysmoothing algorithm (Chen and Goodman, 1999)to compute sequence probabilities.5 DatasetThe experiments were conducted on the Spanish-English language pair.
The latest version of theEuroparl corpus(version-5) was used in this work.A small set of 200K sentences was selected fromthe training set to conduct the experiments.
Thetest and development sets containing 2525 sen-tences and 2051 sentences respectively were used,without making any changes.Corpus No.
of sentences Source TargetTraining 200000 59591 36886Testing 2525 10629 8905Development 2051 8888 7750Monolingual 200000 n.a 36886English (LM)Table 1: Corpus statistics for Spanish-English cor-pus.6 Experiments and ResultsThe output of our experiments was evaluated us-ing two metrics, (1) BLEU (Papineni et al, 2002),and (2) Lexical Accuracy (LexAcc).
Lexical ac-curacy measures the similarity between the un-ordered bag of words in the reference sentence39against the unordered bag of words in the hypoth-esized translation.
Lexical accuracy is a measureof the fidelity of lexical transfer from the sourceto the target sentence, independent of the syntaxof the target language (Venkatapathy and Banga-lore, 2009).
We report lexical accuracies to showthe performance of LCS decoding in comparisonwith the baseline system.We first present the results of the state-of-artphrase-based model (Moses) trained on a paral-lel corpus.
We treat this as our baseline.
The re-ordering feature used is msd-bidirectional, whichallows for all possible reorderings over a speci-fied distortion limit.
The baseline accuracies areshown in table 2.Corpus BLEU Lexical AccuracyDevelopment 0.1734 0.448Testing 0.1823 0.492Table 2: Baseline AccuracyWe conduct two types of experiments to test ourapproach.1.
Experiments using lexical features (see sec-tion 6.1), and2.
Experiments using syntactic features (seesection 6.2).6.1 Experiments using Lexical FeaturesIn this section, we present results of our exper-iments that use only lexical features.
First, wemeasure the translation accuracy using LCS de-coding.
On the development set, we explored theset of decoding parameters (as described in sec-tion 4.1) to compute the optimal parameter val-ues.
The best lexical accuracy obtained on the de-velopment set is 0.4321 and the best BLEU scoreobtained is 0.0923 at a threshold of 0.17 and a per-mutation window size of value 3.
The accuraciescorresponding to a few other parameter values areshown in Table 3.On the test data, we obtained a lexical accu-racy of 0.4721 and a BLEU score of 0.1023.
Aswe can observe, the BLEU score obtained usingthe LCS decoding technique is low when com-pared to the BLEU score of the state-of-art sys-tem.
However, the lexical accuracy is comparableThreshold Perm.
Window LexAcc BLEU0.16 3 0.4274 0.09140.17 3 0.4321 0.09230.18 3 0.4317 0.09180.16 4 0.4297 0.09120.17 4 0.4315 0.0915Table 3: Lexical Accuracies of Lattice-Output us-ing lexical features alone for various parametervaluesto the lexical accuracy of Moses.
This shows thatthe discriminative model provides good lexical se-lection, while the sentence construction techniquedoes not perform as expected.Next, we present the results of the Moses baseddecoder that uses the discriminative model (seesection 3.2).
In our experiments, we did not useMERT training for tuning the Moses parameters.Rather, we explore a set of possible parameter val-ues (i.e.
weights of the translation model, reorder-ing model and the language model) to check theperformance.
We show the BLEU scores obtainedon the development set using Moses decoder inTable 4.Reordering LM Translation BLEUweight(d) weight(l) weight(t)0 0.6 0.3 0.13470 0.6 0.6 0.13540.3 0.6 0.3 0.14410.3 0.6 0.6 0.1468Table 4: BLEU for different weight values usinglexical features onlyOn the test set, we obtained a BLEU score of0.1771.
We observe that both the lexical accuracyand the BLEU scores obtained using the discrim-inative training model combined with the Mosesdecoder are comparable to the state-of-art results.The summary of the results obtained using threeapproaches and lexical feature functions is pre-sented in Table 5.6.2 Experiments using Syntactic FeaturesIn this section, we present the effect of incorpo-rating syntactic features using our model on the40Approach BLEU LexAccState-of-art(MOSES) 0.1823 0.492LCS decoding 0.1023 0.4721Moses decoder trainedusing a discriminative 0.1771 0.4841modelTable 5: Translation accuracies using lexical fea-tures for different approachestranslation accuracies.
Table 6 presents the resultsof our approach that uses syntactic features at dif-ferent parameter values.
Here, we can observethat the translation accuracies (both LexAcc andBLEU) are better than the model that uses onlylexical features.Reordering LM Translation BLEUweight(d) weight(l) weight(t)0 0.6 0.3 0.16610 0.6 0.6 0.17240.3 0.6 0.3 0.17800.3 0.6 0.6 0.1847Table 6: BLEU for different weight values usingsyntactic featuresTable 7 shows the comparative performance ofthe model using syntactic as well as lexical fea-tures against the one with lexical features func-tions only.Model BLEU LexAccLexical features 0.1771 0.4841Lexical+Syntactic 0.201 0.5431featuresTable 7: Comparison between translation accura-cies from models using syntactic and lexical fea-turesOn the test set, we obtained a BLEU score of0.20 which is an improvement of 2.3 points overthe model that uses lexical features alone.
We alsoobtained an increase of 6.1% in lexical accuracyusing this model with syntactic features as com-pared to the model using lexical features only.7 Conclusions and Future WorkIn this paper, we presented an approach to statisti-cal machine translation that combines the powerof a discriminative model (for training a modelfor Machine Translation), and the standard beam-search based decoding technique (for the transla-tion of an input sentence).
The key contributionsare:1.
We incorporated a discriminative model ina phrase-based decoder.
We obtained com-parable results with the state-of-art phrase-based decoder (see section 6.1).
The ad-vantage in using our approach is that it hasthe flexibility to incorporate richer contextualand linguistic feature functions.2.
We show that the incorporation of syntac-tic information (POS tags) in our discrimina-tive model boosted the performance of trans-lation.
The lexical accuracy using our ap-proach improved by 6.1% when syntacticfeatures were used in addition to the lexi-cal features.
Similarly, the BLEU score im-proved by 2.3 points when syntactic featureswere used compared to the model that useslexical features alone.
The accuracies arelikely to improve when richer linguistic fea-ture functions (that use parse structure) areincorporated in our approach.In future, we plan to work on:1.
Experiment with rich syntactic and structuralfeatures (parse tree-based features) using ourapproach.2.
Experiment on other language pairs such asArabic-English and Hindi-English.3.
Improving LCS decoding algorithm usingsyntactic cues in the target (Venkatapathyand Bangalore, 2007) such as supertags.ReferencesBangalore, S., P. Haffner, and S. Kanthak.
2007.
Statistical machine transla-tion through global lexical selection and sentence reconstruction.
In An-nual Meeting-Association for Computational Linguistics, volume 45, page152.Berger, A.L., V.J.D.
Pietra, and S.A.D.
Pietra.
1996.
A maximum en-tropy approach to natural language processing.
Computational linguistics,22(1):39?71.41Brown, P.F., V.J.D.
Pietra, S.A.D.
Pietra, and R.L.
Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter estimation.
Computa-tional linguistics, 19(2):263?311.Chen, S.F.
and J. Goodman.
1999.
An empirical study of smoothingtechniques for language modeling.
Computer Speech and Language,13(4):359?394.Haffner, P. 2006.
Scaling large margin classifiers for spoken language under-standing.
Speech Communication, 48(3-4):239?261.Hassan, H., K. Sima?an, and A.
Way.
2009.
A syntactified direct translationmodel with linear-time decoding.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Processing: Volume 3-Volume3, pages 1182?1191.
Association for Computational Linguistics.Ittycheriah, A. and S. Roukos.
2007.
Direct translation model 2.
In Proceed-ings of NAACL HLT, pages 57?64.Koehn, P. and H. Hoang.
2007.
Factored translation models.
In Pro-ceedings of the 2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natural Language Learning(EMNLP-CoNLL), pages 868?876.Koehn, P., F.J. Och, and D. Marcu.
2003.
Statistical phrase-based transla-tion.
In Proceedings of the 2003 Conference of the North American Chap-ter of the Association for Computational Linguistics on Human LanguageTechnology-Volume 1, pages 48?54.
Association for Computational Lin-guistics.Koehn, P., H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi,B.
Cowan, W. Shen, C. Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Annual meeting-associationfor computational linguistics, volume 45, page 2.Och, F.J. and H. Ney.
2002.
Discriminative training and maximum entropymodels for statistical machine translation.
In Proceedings of ACL, vol-ume 2, pages 295?302.Och, F.J., C. Tillmann, H. Ney, et al 1999.
Improved alignment modelsfor statistical machine translation.
In Proc.
of the Joint SIGDAT Conf.on Empirical Methods in Natural Language Processing and Very LargeCorpora, pages 20?28.Papineni, KA, S. Roukos, and RT Ward.
1998.
Maximum likelihood anddiscriminative training of directtranslation models.
In Acoustics, Speechand Signal Processing, 1998.
Proceedings of the 1998 IEEE InternationalConference on, volume 1.Papineni, K., S. Roukos, T. Ward, and W.J.
Zhu.
2002.
BLEU: a method forautomatic evaluation of machine translation.
In Proceedings of the 40thannual meeting on association for computational linguistics, pages 311?318.
Association for Computational Linguistics.Venkatapathy, S. and S. Bangalore.
2007.
Three models for discriminativemachine translation using Global Lexical Selection and Sentence Recon-struction.
In Proceedings of the NAACL-HLT 2007/AMTA Workshop onSyntax and Structure in Statistical Translation, pages 96?102.
Associationfor Computational Linguistics.Venkatapathy, Sriram and Srinivas Bangalore.
2009.
Discriminative MachineTranslation Using Global Lexical Selection.
ACM Transactions on AsianLanguage Information Processing, 8(2).Xiong, D., Q. Liu, and S. Lin.
2006.
Maximum entropy based phrase reorder-ing model for statistical machine translation.
In Proceedings of the 21stInternational Conference on Computational Linguistics and the 44th an-nual meeting of the Association for Computational Linguistics, page 528.Association for Computational Linguistics.42
