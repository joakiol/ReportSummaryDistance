Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 297?307, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExploiting Reducibility in Unsupervised Dependency ParsingDavid Marec?ek and Zdene?k Z?abokrtsky?Charles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostranske?
na?me?st??
25, 11800 Prague, Czech Republic{marecek,zabokrtsky}@ufal.mff.cuni.czAbstractThe possibility of deleting a word from a sen-tence without violating its syntactic correct-ness belongs to traditionally known manifes-tations of syntactic dependency.
We introducea novel unsupervised parsing approach that isbased on a new n-gram reducibility measure.We perform experiments across 18 languagesavailable in CoNLL data and we show thatour approach achieves better accuracy for themajority of the languages then previously re-ported results.1 IntroductionThe true nature of the notion of dependency (af-ter removing sedimentary deposits of rules imposedonly by more or less arbitrary conventions) remainsstill somewhat vague and elusive.
This holds in spiteof a seemingly strong background intuition and evenafter a decade of formalized large-scale dependency-based resources being available to the research com-munity.
It is undeniable that a huge progress hasbeen reached in the field of supervised dependencyparsing, especially due to the CoNLL shared task se-ries.
However, when it comes to unsupervised pars-ing, there are surprisingly few clues we could relyon.As mentioned e.g.
by Ku?bler et al2009), one ofthe traditional linguistic criteria for recognizing de-pendency relations (including their head-dependentorientation) is that a head H of a construction C de-termines the syntactic category of C and can oftenreplace C. Or, in words of Dependency Analysis byReduction (Lopatkova?
et al2005), stepwise dele-tion of dependent elements within a sentence pre-serves its syntactic correctness.
A similar idea ofdependency analysis by splitting a sentence into allpossible acceptable fragments is used by Gerdes andKahane (2011).Of course, all the above works had to respond tothe notorious fact that there are many language phe-nomena precluding the ideal (word by word) sen-tence reducibility (e.g.
in the case of prepositionalgroups, or in the case of subjects in English finiteclauses).
However, we disregard their solutions ten-tatively and borrow only the very core of the re-ducibility idea: if a word can be removed from asentence without damaging it, then it is likely to bedependent on some other (still present) word.As it is usual with dichotomies in natural lan-guages, it seems more adequate to use a continuousscale instead of the reducible-irreducible opposition.That is why we introduce a simple reducibility mea-sure based on n-gram corpus statistics.
We employthis reducibility measure as the main feature in ourunsupervised parsing procedure.The procedure is based on a commonly usedBayesian inference technique called Gibbs sampling(Gilks et al1996).
In our sampler, the more re-ducible a given token is, the more likely it is tobe sampled as a dependant and not as a head.
Af-ter certain number of sampling iterations, for eachsentence a final dependency tree is created (one to-ken per node, including punctuation) that maximizesthe product of edge probabilities gathered along thesampling history.Our approach allows to utilize information from297very large corpora.
While the computationally de-manding sampling procedure can be applied onlyon limited data, the unrepeated precomputation ofstatistics for reducibility estimates can easily exploitmuch larger data.We are not aware of any other published workon unsupervised parsing employing reducibility ora similar idea.
Dominating approaches in unsuper-vised parsing are typically based on repeated pat-terns, and not on the possibility of a deletion insidea pattern.
It seems that the two views of depen-dency (frequent co-occurrence of head-dependantpair, versus reducibility of the dependant) are rathercomplementary, so fruitful combinations can behopefully expected in future.The remainder of this paper is structured as fol-lows.
Section 2 briefly outlines the state of the art inunsupervised dependency parsing.
Our measure ofreducibility based on a large monolingual corpus ispresented in Section 3.
Section 4 shows our modelswhich serve for generating probability estimates foredge sampling described in Section 5.
Experimen-tal parsing results for languages included in CoNLLshared task treebanks are summarized in Section 6.Section 7 concludes this article.2 Related WorkThe most popular approach in unsupervised de-pendency parsing of the recent years is to employDependency Model with Valence (DMV), whichwas introduced by Klein and Manning (2004).The inference algorithm was further improved bySmith (2007) and Cohen et al2008).
Headden,Johnson and McClosky (2009) introduced the Ex-tended Valence Grammar (EVG) and added lexical-ization and smoothing.
Blunsom and Cohn (2010)use tree substitution grammars, which allow learn-ing larger dependency fragments.Unfortunately, many of these works show resultsonly for English.1 However, the main feature ofunsupervised methods should be their applicabil-ity across a wide range of languages.
Such exper-iments were done by Spitkovsky (2011b; 2011c),where the parsing algorithm was evaluated on all 19languages included in CoNLL 2006 (Buchholz and1The state-of-the-art unsupervised parsers achieve morethan 50% of attachment score measured on the Penn Treebank.Marsi, 2006) and 2007 (Nivre et al2007) sharedtasks.The fully unsupervised linguistic analysis(Spitkovsky et al2011a) shows that the unsuper-vised part-of-speech tags may be more useful forthis task than the supervised ones.Another possibility for obtaining dependencystructures for languages without any linguisticallyannotated resources can be the projection using aparallel treebank with a resource-rich language (typ-ically English).
McDonald et al2011) showed thatsuch projection produce better structures than thecurrent unsupervised parsers do.
However, our taskis different.
We would like to produce structures thatare not burdened by any linguistic conventions.In this paper, we describe a novel approach to un-supervised dependency parsing.
Our model differsfrom DMV, since we employ the reducibility featureand use fertility of nodes instead of generating STOPsigns.We use Gibbs sampling procedure for inferenceinstead of Variational Bayes, which has been morecommon for induction of linguistic strucures.
Gibbssampling algorithm for grammar induction was usedalso by Marec?ek and Z?abokrtsky?
(2011).
However,their sampling algorithm produces generally non-projective trees.
Our sampler, which is described inSection 5, introduces a completely different small-change operator that guarantees projective edges.3 Computing Reducibility scoresWe call a word (or a sequence of words) in a sen-tence reducible, if the sentence after removing theword remains grammatically correct.
Although wecannot automatically recognize grammaticality ofsuch newly created sentence, we can search for itin a large corpus.
If we find it, we assume the wordwas reducible in the original sentence.Since the number of such reducible word se-quences found in any corpus will be low, we de-termine the reducibility scores from their individualtypes (part-of-speech tags).
This then implicitly al-lows some sharing of the scores between differentword sequences.The necessity to search for the whole sentencesin the corpus and not only for some smaller context(considering, for example, just left and right neigh-298bor), which would lead to lower sparsity, is rational-ized by the following example:Their children went to school.I took their children to school.The verb ?went?
would be reducible in the context?their children went to school?, because the sequence?their children to school?
occurs in the second sen-tence.
One could find such examples frequently evenfor large contexts.
For instance, verbs in free word-order languages can be placed almost at any posi-tion in a sentence; therefore, without the full sen-tence context, they would have to be considered asreducible.
To prevent this, we decided to work ex-clusively with the full sentence context instead ofshorter contexts.Other way that would lead to lower sparsity wouldbe searching for sequences of part-of-speech tags in-stead of sequences of word forms.
However, thisalso does not bring desired results.
For instance, thetwo following sentence patternsDT NNS VBD IN DT NN .DT NNS VBD DT NN .are quite frequent in English and we can deducefrom them that the preposition IN is reducible.
Butthis is of course a wrong deduction, since the prepo-sition cannot be removed from the prepositionalphrase.
Using part-of-speech tags instead of wordforms is thus not suitable for computing reducibilityscores.Although we search for reducible sequences ofword forms in the corpus, we compute reducibil-ity scores for sequences of part-of-speech tags.
Thisrequires to have the corpus morphologically disam-biguated.
A sequences of part-of-speech tags will bedenoted as ?PoS n-gram?
in the following text.Assume a PoS n-gram g = [t1, .
.
.
, tn].
We gothrough the corpus and search for all its occurrences.For each such occurrence, we remove the respec-tive words from the current sentence and check inthe corpus whether the rest of the sentence occurs atleast once elsewhere in the corpus.2 If so, then suchoccurrence of PoS n-gram is reducible, otherwise itis not.
We denote the number of such reducible oc-2We do not take into account sentences with less then 10words, because they could be nominal (without any verb) andmight influence the reducibility scores of verbs.unigrams R bigrams R trigrams RVB 0.04 VBN IN 0.00 IN DT JJ 0.00TO 0.07 IN DT 0.02 JJ NN IN 0.00IN 0.11 NN IN 0.04 NN IN NNP 0.00VBD 0.12 NNS IN 0.05 VBN IN DT 0.00CC 0.13 JJ NNS 0.07 JJ NN .
0.00VBZ 0.16 NN .
0.08 DT JJ NN 0.04NN 0.22 DT NNP 0.09 DT NNP NNP 0.05VBN 0.24 DT NN 0.09 NNS IN DT 0.14.
0.32 NN , 0.11 NNP NNP .
0.15NNS 0.38 DT JJ 0.13 NN IN DT 0.23DT 0.43 JJ NN 0.14 NNP NNP , 0.46NNP 0.78 NNP .
0.15 IN DT NNP 0.55JJ 0.84 NN NN 0.22 DT NN IN 0.59RB 2.07 IN NN 0.67 NNP NNP NNP 0.64, 3.77 NNP NNP 0.76 IN DT NN 0.80CD 55.6 IN NNP 1.81 IN NNP NNP 4.27Table 1: Reducibility scores of the most frequentEnglish n-grams.
(V* are verbs, N* are nouns, DETare determiners, IN are prepositions, JJ are adjec-tives, RB are adverbs, CD are numerals, and CC arecoordinating conjunctions)currences of PoS n-gram g by r(g).
The number ofall its occurrences is c(g).The relative reducibility R(g) of a PoS n-gram gis then computed asR(g) =1Nr(g) + ?1c(g) + ?2, (1)where the normalization constant N , which ex-presses relative reducibility over all the PoS n-grams(denoted by G), causes the scores are concentratedaround the value 1.N =?g?G(r(g) + ?1)?g?G(c(g) + ?2)(2)Smoothing constants ?1 and ?2, which prevent re-ducibility scores from being equal to zero, are setto?1 =?g?G r(g)?g?G c(g), ?2 = 1 (3)This setting causes that even if a given PoS n-gram isnot reducible anywhere in the corpus, its reducibilityscore is 1/(c(g) + 1).Tables 1, 2, and 3 show reducibility scores of themost frequent PoS n-grams of three selected lan-guages: English, German, and Czech.
If we consideronly unigrams, we can see that the scores for verbsare often among the lowest.
Verbs are followed byprepositions and nouns, and the scores for adjectives299unigrams R bigrams R trigrams RVVPP 0.00 NN APPR 0.00 NN APPR NN 0.01APPR 0.27 APPR ART 0.00 ADJA NN APPR 0.01VVFIN 0.28 ART ADJA 0.00 APPR ART ADJA 0.01APPRART 0.32 NN VVPP 0.00 NN KON NN 0.01VAFIN 0.37 NN $( 0.01 ADJA NN $.
0.01KON 0.37 NN NN 0.01 NN ART NN 0.32NN 0.43 NN ART 0.21 ART NN ART 0.49ART 0.49 ADJA NN 0.28 NN ART ADJA 0.90$( 0.57 NN $, 0.67 ADJA NN ART 0.95$.
1.01 NN VAFIN 0.85 NN APPR ART 0.95NE 1.14 NN VVFIN 0.89 NN VVPP $.
1.01CARD 1.38 NN $.
0.95 ART NN APPR 1.35ADJA 2.38 ART NN 1.07 ART ADJA NN 1.58$, 2.94 NN KON 2.41 APPR ART NN 2.60ADJD 3.54 APPR NN 2.65 APPR ADJA NN 2.65ADV 7.69 APPRART NN 3.06 ART NN VVFIN 9.51Table 2: Reducibility scores of the most frequentGerman n-grams.
(V* are verbs, N* are nouns, ARTare articles, APPR* are prepositions, ADJ* are ad-jectives, ADV are adverbs, CARD are numerals, andKON are conjunctions)and adverbs are very high for all three examined lan-guages.
That is desired, because the reducible uni-grams will more likely become leaves in dependencytrees.
Considering bigrams, the couples [determiner?
noun], [adjective ?
noun], and [preposition ?
noun]obtained reasonably high scores.
However, thereare also n-grams such as the German trigram [de-terminer ?
noun ?
preposition] (ART-NN-APPR)whose reducibility score is undesirably high.34 ModelsWe introduce a new generative model that is dif-ferent from the widely used Dependency Modelwith Valence (DMV).
In DMV (Klein and Manning,2004) and in the extended model EVG (Headden IIIet al2009), there is a STOP sign indicating that nomore dependents in a given direction will be gener-ated.
Given a certain head, all its dependents in leftdirection are generated first, then the STOP sign inthat direction, then all its right dependents and thenSTOP in the other direction.
This process continuesrecursively for all generated dependents.Our model introduces fertility of a node, whichsubstitutes the STOP sign.
For a given head, we firstgenerate the number of its left and right children3The high reducibility score of ART-NN-APPR was proba-bly caused by German particles, which have the same PoS tagas prepositions.unigrams R bigrams R trigrams RP4 0.00 RR AA 0.00 RR NN Z: 0.00RV 0.00 Z: J, 0.00 NN RR AA 0.00Vp 0.06 Vp NN 0.00 NN AA NN 0.16Vf 0.06 VB NN 0.12 AA NN RR 0.23P7 0.16 NN Vp 0.13 NN RR NN 0.46J, 0.24 NN VB 0.18 NN J?
NN 0.46RR 0.28 NN RR 0.22 AA NN NN 0.47VB 0.33 NN AA 0.23 NN Z: Z: 0.48NN 0.72 NN J?
0.62 NN Z: NN 0.52J?
1.72 AA NN 0.62 NN NN NN 0.70C= 1.85 NN NN 0.70 AA AA NN 0.72PD 2.06 NN Z: 0.97 AA NN Z: 0.86AA 2.22 Z: NN 1.72 NN NN Z: 1.38Dg 3.21 Z: Z: 1.97 RR NN NN 2.26Z: 4.01 J?
NN 2.05 RR AA NN 2.65Db 4.62 RR NN 2.20 Z: NN Z: 8.32Table 3: Reducibility scores of the most frequentCzech n-grams.
(V* are verbs, N* are nouns, P* arepronouns, R* are prepositions, A* are adjectives, D*are adverbs, C* are numerals, J* are conjunctions,and Z* is punctuation)(fertility model) and then we fill these positions bygenerating its individual dependents (edge model).If a zero fertility is generated in both the directions,the head becomes a leaf.Besides the fertility model and the edge model,we use two more models (subtree model and dis-tance model), which force the generated trees tohave more desired shape.44.1 Fertility ModelWe express a fertility of a node by a pair of num-bers: the number of its left dependents and the num-ber of its right dependents.
For example, fertility?1-3?
means that the node has one left and threeright dependents, fertility ?0-0?
indicates that it isa leaf.
Fertility is conditioned by part-of-speech tagof the node and it is computed following the Chi-nese restaurant process.
This means that if a specificfertility has been frequent for a given PoS tag in thepast, it is more likely to be generated again.
Theformula for computing probability of fertility fi of aword on the position i in the corpus is as follows:Pf (fi|ti) =c?i(?ti, fi?)
+ ?P0(fi)c?i(?ti?)
+ ?, (4)4In fact, the subtree model and the distance model disrupt abit the generative story, because the probabilites do not sum upto one when they are used.
However, they proved to help withinducing better linguistic structures.300where ti is part-of-speech tag of the word on the po-sition i, c?i(?ti, fi?)
stands for the count of wordswith PoS tag ti and fertility fi in the history, andP0 is a prior probability for the given fertility whichdepends on the total number of node dependents de-noted by |fi| (the sum of numbers of left and rightdependents):P0(fi) =12|fi|+1(5)This prior probability has a nice property: for agiven number of nodes, the product of fertility prob-abilities over all the nodes is equal for all possibledependency trees.
This ensures the stability of thismodel during the inference.Besides the basic fertility model, we introducealso an extended fertility model, which uses fre-quency of a given word form for generating numberof children.
We assume that the most frequent wordsare mostly function words (e.g.
determiners, prepo-sitions, auxiliary verbs, conjunctions).
Such wordstend to have a stable number of children, for exam-ple (i) some function words are exclusively leaves,(ii) prepositions have just one child, and (iii) attach-ment of auxiliary verbs depends on the annotationstyle, but number of their children is also not veryvariable.
The higher the frequency of a word form,the higher probability mass is concentrated on onespecific number of children and the lower Dirichlethyperparameter ?
in Equation 4 is needed.
The ex-tended fertility is described by equationP ?f (fi|ti, wi) =c?i(?ti, fi?)
+ ?eF (wi)P0(fi)c?i(?ti?)
+ ?eF (wi), (6)where F (wi) is a frequency of the word wi, whichis computed as a number of words wi in our corpusdivided by number of all words.4.2 Edge ModelAfter the fertility (number of left and right depen-dents) is generated, the individual slots are filled us-ing the edge model.
A part-of-speech tag of each de-pendent is conditioned by part-of-speech tag of thehead and the edge direction (position of the depen-dent related to the head).55For the edge model purposes, the PoS tag of the technicalroot is set to ?<root>?
and it is in the zero-th position in theSimilarly as for the fertility model, we employChinese restaurant process to assign probabilities ofindividual dependent.Pe(tj |ti, dj) =c?i(?ti, tj , dj?)
+ ?c?i(?ti, dj?)
+ ?|T |, (7)where ti and tj are the part-of-speech tags of thehead and the generated dependent respectively; dj isa direction of edge between the words i and j, whichcan have two values: left and right.
c?i(?ti, tj , dj?
)stands for the count of edges ti ?
tj with the direc-tion dj in the history, |T | is a number of unique tagsin the corpus and ?
is a Dirichlet hyperparameter.4.3 Distance ModelDistance model is an auxiliary model that preventsthe resulting trees from being too flat.
Ideally, itwould not be needed, but experiments showed thatit helps to infer better trees.
This simple model saysthat shorter edges are more probable than longerones.
We define probability of a distance betweena word and its parent as its inverse value,6 which isthen normalized by the normalization constant d.Pd(i, j) =1d(1|i?
j|)?
(8)The hyperparameter ?
determines the weight of thismodel.4.4 Subtree ModelThe subtree model uses the reducibility measure.
Itplays an important role since it forces the reduciblewords to be leaves and reducible n-grams to be sub-trees.
Words with low reducibility are forced to-wards the root of the tree.
We define desc(i) as asequence of tags [tl, .
.
.
, tr] that corresponds to allthe descendants of the word wi including wi, i.e.
thewhole subtree of wi.
The probability of such sub-tree is proportional to its reducibility R(desc(i)).The hyperparameter ?
determines the weight of themodel; s is a normalization constant.Ps(i) =1sR(desc(i))?
(9)sentence, so the head word of the sentence is always its rightdependent.6Distance between any word and the technical root of thedependency tree was set to 10.
Since each technical root hasonly one dependent, this value does not affect the model.3014.5 Probability of the Whole TreebankWe want to maximize the probability of the wholegenerated treebank, which is computed as follows:Ptreebank =n?i=1(P ?f (fi|ti, wi) (10)Pe(ti|tpi(i), di) (11)Pd(i, pi(i)) (12)Ps(i)), (13)where pi(i) denotes the parent of the word on theposition i.
We multiply the probabilities of fertil-ity, edge, distance from parent, and subtree over allwords (nodes) in the corpus.
The extended fertilitymodel P ?f can be substituted by its basic variant Pf .5 Sampling AlgorithmFor stochastic searching for the most probable de-pendency trees, we employ Gibbs sampling, a stan-dard Markov Chain Monte Carlo technique (Gilks etal., 1996).
In each iteration, we loop over all wordsin the corpus in a random order and change the de-pendencies in their neighborhood (a small changedescribed in Section 5.2).
In the end, ?average?
treesbased on the whole sampling are built.5.1 InitializationBefore the sampling starts, we initialize the projec-tive trees randomly.
For doing so, we tried the fol-lowing two initializers:?
For each sentence, we choose randomly oneword as the head and attach all other words toit.?
We are picking one word after another in a ran-dom order and we attach it to the nearest left (orright) neighbor that has not been attached yet.The left-right choice is made by a coin flip.
If itis not possible to attach a word to one side, weattach it to the other side.
The last unattachedword becomes the head of the sentence.While the first method generates only flat trees,the second one can generate all possible projectivetrees.
However, the sampler converges to similar re-sults for both the initializations.
Therefore we con-clude that the choice of the initialization mechanismThe   dog   was   in   the   park  .
(((The) dog) was (in ((the) park)) (.
))Figure 1: Arrow and bracketing notation of a projec-tive dependency tree.TTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddTTd dTTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddT Td dFigure 2: An example of small change in a projec-tive tree.
The bracket (in the park) is removed andthere are five possibilities how to replace it.is not so important here and we choose the first onedue to its simplicity.5.2 Small Change OperatorWe use the bracketing notation for illustrating thesmall change operator.
Each projective dependencytree consisting of n words can be expressed by npairs of brackets.
Each bracket pair belongs to onenode and delimits its descendants from the rest ofthe sentence.
Furthermore, each bracketed segmentcontains just one word that is not embedded deeper;this node is the segment head.
An example of thisnotation is in Figure 1.The small change is then very simple.
We removeone pair of brackets and add another, so that the con-ditions defined above are not violated.
An exampleof such change is in Figure 2.From the perspective of dependency structures,the small change can be described as follows:1.
Pick a random non-root word w (the word inin our example) and find its parent p (the wordwas).2.
Find all other children of w and p (the wordsdog, park, and .)
and denote this set by C.3.
Choose the new head out of w and p. Mark thenew head as g and the second candidate as d.Attach d to g.3024.
Select a neighborhoodD adjacent to the word das a continuous subset ofC and attach all wordsfrom D to d. D may be also empty.5.
Attach the remaining words from C that werenot in D to the new head g.5.3 Building ?Average?
TreesThe ?burn-in?
period is set to 10 iterations.
Afterthis period, we begin to count how many times anedge occurs at a particular location in the corpus.These counts are collected over the whole corpuswith the collection-rate 0.01.7When the samling is finished, we build final de-pendency trees based on the edge counts obtainedduring the sampling.
We employ the maximumspanning tree (MST) algorithm (Chu and Liu, 1965)to find them; the weights of edges for computingMST correspond to the number of times they werepresent during the sampling.
This averaging methodwas used also by Marec?ek and Z?abokrtsky?
(2011).Other possibilities for obtaining final depen-dency trees would be using Eisner?s projective al-gorithm (Eisner, 1996) or using annealing method(favoring more likely changes) at the end of the sam-pling.
However, the general non-projective MST al-gorithm enable non-projective edges, which are byno means negligible in treebanks (Havelka, 2007).6 Experiments and EvaluationWe evaluate our parser on 20 treebanks (18languages) included in CoNLL shared tasks2006 (Buchholz and Marsi, 2006) and 2007 (Nivreet al2007).Similarly to some previous papers on unsuper-vised parsing (Gillenwater et al2011; Spitkovskyet al2011b), the tuning experiments were per-formed on English only.
We used English for check-ing functionality of the individual models and foroptimizing hyperparameter values.
The best config-uration of the parser achieved on English develop-ment data was then used for parsing all other lan-guages.
This simulates the situation in which wehave only one treebank (English) on which we cantune our parser and we want to parse other languagesfor which we have no manually annotated treebanks.7After each small change is made, the edges from the wholecorpus are collected with a probability 0.01.language tokens (mil.)
language tokens (mil.
)Arabic 19.7 Greek 20.9Basque 14.1 Hungarian 26.3Bulgarian 18.8 Italian 39.7Catalan 27.0 Japanese 2.6Czech 20.3 Portuguese 31.7Danish 15.9 Slovenian 13.7Dutch 27.1 Spanish 53.4English 85.0 Swedish 19.2German 56.9 Turkish 16.5Table 4: Wikipedia texts statistics6.1 DataWe need two kinds of data for our experiments: asmaller treebank, which is used for sampling and forevaluation, and a large corpus, from which we com-pute n-gram reducibility scores.The treebanks are taken from the CoNLL sharedtask 2006 and 2007.
The experiments are per-formed for all languages except for Chinese.8 Weuse only the testing parts of the treebanks (the filestest.conll) for the dependency tree induction.As a source of the part-of-speech tags, we use thefine-grained gold PoS tags, which are in the fifth col-umn in the CoNLL format.For obtaining reducibility scores, we used theW2C corpus9 of Wikipedia articles, which wasdownloaded by Majlis?
and Z?abokrtsky?
(2012).
Theirstatistics across languages are shown in Table 4.
Tomake them useful, the necessary preprocessing stepsmust have been done.
The texts were first automati-cally segmented and tokenized10 and then they werepart-of-speech tagged by TnT tagger (Brants, 2000),which was trained on the respective CoNLL train-ing data (the files train.conll).
The quality ofsuch tagging is not very high, since we do not useany lexicons11 or pretrained models.
However, it issufficient for obtaining good reducibility scores.8We do not have appropriate Chinese segmenter that wouldsegment Chinese texts in the same way as in CoNLL.9http://ufal.mff.cuni.cz/?majlis/w2c/10The segmentation to sentences and tokenization was per-formed using the TectoMT framework (Popel and Z?abokrtsky?,2010).11Using lexicons or another pretrained models for taggingmeans using other sources of human annotated data, which isnot allowed if we want to compare our results with others.3036.2 Setting the HyperparametersThe applicability of individual models and their pa-rameters were tested on development data set ofEnglish (the file en/dtest.conll in CoNLLshared task 2007).After several experiments, we have observed thatthe extended fertility model provides better resultsthan the basic fertility model; the parser using thebasic fertility model achieved 44.1% attachmentscore for English, whereas the extended fertilitymodel increased the score to 46.8%.
The four hy-perparameters ?e (extended fertility model), ?
(edgemodel), ?
(distance model), and ?
(subtree model),were set by a grid search algorithm,12 which foundthe following optimal values:?e = 0.01, ?
= 1, ?
= 1.5, ?
= 1In informal experiments, parameters were tunedalso for other treebanks and we found out that theyvary across languages.
Therefore, adjusting the hy-perparameters on another language would probablychange the scores significantly.6.3 EvaluationThe best setting from the experiments on English isnow used for evaluating our parser on all CoNLLlanguages.
To be able to compare our parser attach-ment score to previously published results, the fol-lowing steps must be done:?
We take the testing part of each treebank (thefile test.conll) and remove all the punctu-ation marks.
If the punctuation node is not aleaf, its children are attached to the parent ofthe removed node.?
Some previous papers report results on up-to-10-words sentences only.
Therefore we extractsuch sentences from the test data and evaluateon this subsets as well.12Here we make use of manually annotated trees.
However,we use only English treebank an we are setting only four num-bers out of several previously given values (e.g ?e out of 0.01,0.1, 1, 10).
These numbers could be tuned also by inspectingthe outputs.
So we believe this method can be treated as unsu-pervised.CoNLL ?
10 tokens all sentenceslanguage year gil11 our spi11 ourArabic 06 ?
40.5 16.6 26.5Arabic 07 ?
48.0 49.5 27.9Basque 07 ?
30.8 24.0 26.8Bulgarian 06 58.3 53.2 43.9 46.0Catalan 07 ?
63.5 59.8 47.0Czech 06 53.2 58.9 27.7 49.5Czech 07 ?
63.7 28.4 48.0Danish 06 45.9 49.5 38.3 38.6Dutch 06 33.5 48.8 27.8 44.2English 07 ?
64.1 45.2 49.2German 06 46.7 60.8 30.4 44.8Greek 07 ?
30.2 13.2 20.2Hungarian 07 ?
61.8 34.7 51.8Italian 07 ?
50.5 52.3 43.3Japanese 06 57.7 65.4 50.2 50.8Portuguese 06 54.0 62.3 36.7 50.6Slovenian 06 50.9 21.0 32.2 18.1Spanish 06 57.9 67.3 50.6 51.9Swedish 06 45.0 60.5 50.0 48.2Turkish 07 ?
13.0 35.9 15.7Average: 50.3?
54.7?
37.4 40.0Table 5: Comparison of directed attachment scoreswith previously reported results on CoNLL tree-banks.
The column ?gil11?
contains results reportedby Gillenwater et al011) (see the best configura-tion in Table 7 in their paper).
They provided onlyresults on sentences of up to 10 tokens from CoNLL2006 treebanks.
Results in the column ?spi11?
aretaken from Spitkovsky et al011b), best configu-ration in Table 6 in their paper.
The average scorein the last line is computed across all comparableresults, i.e.
for comparison with ?gil11?
only theCoNLL?06 results are averaged (?).
Our parser wasnot evaluated on Turkish CoNLL?06 data and Chi-nese data, because we have not them available.The resulting scores are given in Table 5.
Wecompare our results with results previously reportedby Gillenwater (2011) and Spitkovsky (2011b), whoused the CoNLL data for evaluation too.
Since theyprovide results for several configurations of theirparsers, we choose only the best one from each thepaper.
We define the best configuration as the one304with the highest average attachment score across allthe tested languages.We can see that our parser outperforms the pre-viously published ones.
In one case, it is better for8 out of 10 data sets, in the other case, it is betterfor 14 out of 20 data sets.
The average attachmentscores, which are computed only from the resultspresent for both compared parsers, also confirm theimprovement.However, it is important to note that we used anadditional source of information, namely large unan-notated corpora for computing reducibility scores,while the others used only the CoNLL data.6.4 Error AnalysisOur main motivation for developing an unsuperviseddependency parser was that we wanted to be ableto parse any language.
However, the experimentsshow that our parser fails for some languages.
Inthis section, we try to analyze and explain some ofthe most substantial types of errors.Auxiliary verbs in Slovenian ?
In the Sloveniantreebank, many verbs are composed of two words:main verb (marked as Verb-main) and auxiliaryverb (Verb-copula).
Our parser choose the aux-iliary verb as the head and the main verb and all itsdependants become its children.
That is why the at-tachment score is so poor (only 18.1%).
In fact, theinduced structure is not so bad.
The main verb isswitched with the auxiliary one which causes alsothe wrong attachment of all its dependants.Articles in German ?
Attachment of about onehalf of German articles is wrong.
Instead of the ar-ticle being attached below the appropriate noun, thenoun is attached below the article.
It is a similarproblem as the aforementioned Slovenian auxiliaryverbs.
The dependency between content and func-tion word is switched and the dependants of the con-tent word are attached to the function word.
Kleinand Manning (2004) observed a similar behavior intheir experiments with DMV.Noun phrases in English ?
The structure ofphrases that consist of more nouns are often inducedbadly.
This is caused probably by ignoring wordforms.
For example, the structure of the sequence?NN NN NN?
can be hardly recognized by our parser.fert.
edge dist.
subtr.
en de cs(random baseline) 19.8 18.4 26.7X 8.71 13.7 14.9X 18.9 20.2 26.5X 23.6 19.5 25.3X 28.2 23.7 33.5X X 21.2 22.9 23.5X X 19.9 19.7 25.5X X 7.8 17.5 22.7X X 24.1 19.5 27.1X X 25.5 27.5 40.7X X 31.2 25.2 33.1X X X 30.7 26.2 22.0X X X 14.1 18.1 34.6X X X 36.1 32.2 38.9X X X 34.8 26.7 42.4X X X X 46.8 36.5 47.2Table 6: Ablation analysis.
Unlabeled attachmentscores for different combinations of model compo-nents (fertility model, edge model, distance modeland subtree model).
The scores are computed on allsentences of the development data.
Punctuation isincluded into the evaluation.6.5 Ablation AnalysisTo investigate the impact of individual componentsof the model, we run the parser for all possible com-ponent combinations.
We choose three languagesalong the scale of word order freedom: English(very rigid word order), Czech (relatively free wordorder), and German (somewhere in the middle).
Theattachment scores are shown in Table 6.
If no modelis used for the inference and the sampling algorithmsamples completely random trees, we get the ran-dom baseline score, which is 19.8% for English13.From the perspective of the subtree model, whichimplements the reducibility feature, we can see thatit is the most useful model here.
Alone, it improvesthe score for English to 28.2%.
If we do not useit, the score decreases from 46.8% (when all mod-els are used) to 30.7%.
Very important is also thedistance model which eliminates the possibility ofattaching all words to one head word.
If we omit13This relatively high baseline scores are caused by the MSTalgorithm, which chooses the most frequent edges from randomtrees i.e.
the shortest ones.305it, the score for English falls drastically to 14.1%.Some combinations of models have their scores farbelow the baseline.
This is caused by the fact thatsome regularities have been found but the structuresare induced differently and thus all attachments arewrong.6.6 Induction without Wikipedia CorpusWe have performed also experiments using exclu-sively the CoNLL data.
However, the numbers ofreducible words in CoNLL training set were verylow (50 words at maximum in CoNLL 2006 train-ing data and 10 words at maximum in CoNLL 2007training data).
This led to completely unreliable re-ducibility scores and the consequent poor results.7 Conclusions and Future WorkWe have shown that employing the reducibility fea-ture is useful in unsupervised dependency parsingtask.
We extracted the n-gram reducibility scoresfrom a large corpus, and then made the computation-ally demanding inference on smaller data using onlythese scores.
We evaluated our parser on 18 lan-guages included in CoNLL and for 14 of them, weachieved higher attachment scores than previouslypublished results.The most errors were caused by function words,which sometimes take over the dependents of adja-cent content words.
This can be caused by the factthat the reducibility cannot handle function wordscorrectly, because they must be reduced togetherwith a content word, not one after another.In future work, we would like to estimate thehyperparameters automatically.
Furthermore, wewould like to get rid of manually designed PoS tagsand use some kind of unsupervised clusters in orderto have all the annotation process completely unsu-pervised.
We would also like to employ lexicalizedmodels that should help in situations in which thePoS tags are too coarse.Finally, we would like to move towards deepersyntactic structures, where the tree would be formedonly by content words and the function words wouldbe treated in a different way.SoftwareThe source code of our unsupervised dependencyparser including the script for computing reducibil-ity scores from large corpora is available athttp://ufal.mff.cuni.cz/?marecek/udp.AcknowledgementThis research was supported by the grantsGAUK 116310, GA201/09/H057 (Res Infor-matica), LM2010013, MSM0021620838, andby the European Commission?s 7th FrameworkProgram (FP7) under grant agreement n?
247762(FAUST).
We thank anonymous reviewers for theirvaluable comments and suggestions.ReferencesPhil Blunsom and Trevor Cohn.
2010.
Unsupervisedinduction of tree substitution grammars for depen-dency parsing.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?10, pages 1204?1213, Stroudsburg, PA,USA.
Association for Computational Linguistics.Thorsten Brants.
2000.
TnT - A Statistical Part-of-Speech Tagger.
Proceedings of the sixth conferenceon Applied natural language processing, page 8.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the Tenth Conference on Compu-tational Natural Language Learning, CoNLL-X ?06,pages 149?164, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Y.
J. Chu and T. H. Liu.
1965.
On the Shortest Arbores-cence of a Directed Graph.
Science Sinica, 14:1396?1400.Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.2008.
Logistic normal priors for unsupervised prob-abilistic grammar induction.
In Neural InformationProcessing Systems, pages 321?328.Jason Eisner.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
In Proceed-ings of the 16th International Conference on Com-putational Linguistics (COLING-96), pages 340?345,Copenhagen, August.Kim Gerdes and Sylvain Kahane.
2011.
Defining depen-dencies (and constituents).
In Proceedings of Depen-dency Linguistics 2011, Barcelona.Walter R. Gilks, S. Richardson, and David J. Spiegelhal-ter.
1996.
Markov chain Monte Carlo in practice.
In-terdisciplinary statistics.
Chapman & Hall.306Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-nando Pereira, and Ben Taskar.
2011.
PosteriorSparsity in Unsupervised Dependency Parsing.
TheJournal of Machine Learning Research, 12:455?490,February.Jir???
Havelka.
2007.
Beyond Projectivity: Multilin-gual Evaluation of Constraints and Measures on Non-Projective Structures.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics, pages 608?615.William P. Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 101?109, Stroudsburg, PA, USA.Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of de-pendency and constituency.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, ACL ?04, Stroudsburg, PA, USA.Association for Computational Linguistics.Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectures onHuman Language Technologies.
Morgan & ClaypoolPublishers.Marke?ta Lopatkova?, Martin Pla?tek, and Vladislav Kubon?.2005.
Modeling syntax of free word-order languages:Dependency analysis by reduction.
In Va?clav Ma-tous?ek, Pavel Mautner, and Toma?s?
Pavelka, editors,Lecture Notes in Artificial Intelligence, Proceedings ofthe 8th International Conference, TSD 2005, volume3658 of Lecture Notes in Computer Science, pages140?147, Berlin / Heidelberg.
Springer.Martin Majlis?
and Zdene?k Z?abokrtsky?.
2012.
Languagerichness of the web.
In Proceedings of the Eight In-ternational Conference on Language Resources andEvaluation (LREC 2012), Istanbul, Turkey, May.
Eu-ropean Language Resources Association (ELRA).David Marec?ek and Zdene?k Z?abokrtsky?.
2011.
GibbsSampling with Treeness constraint in UnsupervisedDependency Parsing.
In Proceedings of RANLP Work-shop on Robust Unsupervised and SemisupervisedMethods in Natural Language Processing, pages 1?8,Hissar, Bulgaria.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 62?72, Edinburgh, Scotland, UK., July.
Associ-ation for Computational Linguistics.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 Shared Task on Depen-dency Parsing.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL 2007, pages 915?932,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Martin Popel and Zdene?k Z?abokrtsky?.
2010.
TectoMT:modular NLP framework.
In Proceedings of the 7thinternational conference on Advances in natural lan-guage processing, IceTAL?10, pages 293?304, Berlin,Heidelberg.
Springer-Verlag.Noah Ashton Smith.
2007.
Novel estimation methodsfor unsupervised discovery of latent structure in natu-ral language text.
Ph.D. thesis, Baltimore, MD, USA.AAI3240799.Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang,and Daniel Jurafsky.
2011a.
Unsupervised depen-dency parsing without gold part-of-speech tags.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2011).Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2011b.
Lateen EM: Unsupervised training withmultiple objectives, applied to dependency grammarinduction.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2011).Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2011c.
Punctuation: Making a point in unsu-pervised dependency parsing.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning (CoNLL-2011).307
