Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing, pages 19?26,Avignon, France, April 23, 2012. c?2012 Association for Computational LinguisticsAggregated Assessment and ?Objectivity 2.0?Joseph M. MoxleyUniversity of South Florida4202 East Fowler AvenueTampa, FL, USA  33620moxley@usf.eduAbstractThis essay provides a summary of researchrelated to My Reviewers, a web-based appli-cation that can be used for teaching and as-sessment purposes.
The essay concludeswith speculation about ongoing develop-ment efforts, including a social helpfulnessalgorithm, a badging system, and NaturalLanguage Processing (NLP) features.1 IntroductionThe essay summarizes research that has identi-fied ways My Reviewers can be used to:?
integrate formative with summativeevaluations, thereby enabling universi-ties and teachers to alter curriculum ap-proaches in real time in response toongoing assessment information,?
assess students?
critical thinking, re-search, and writing skills?aggregatingnot a small percentage but all of themarked up documents (in our case about16,000 evaluations by teachers of stu-dents?
intermediate and final drafts of es-says/semester),?
enable reviewers (teachers and students)to provide more objective feedback, fa-cilitating ?Objectivity 2.0,?
a form ofevaluative consensus mediated after ex-tensive crowdsourcing of standards,?
provide conclusive evidence that can beused to compare the efficacy of particu-lar curricular approaches,?
enable students and writing programs totrack progress related to specific learningoutcomes (from project to project, courseto course, year to year),?
inform faculty development and teacherresponse, and?
create an e-portfolio of students?
workthat reflects their ongoing progress.2 What is My Reviewers?My Reviewers is a web-based application thatenables students, teachers, and universities to?
aggregate assessment information aboutstudents?
critical thinking and writingskills,?
mark up PDF documents (with stickynotes, text box notes, drawing tools,etc.),?
grade documents according to a rubric,?
assign and conduct or grade peer re-views.
(My Reviewers enables teachersto see at a glance each student?s in-textannotations, end-note comments, and ru-bric scores),?
use a library of comments and resourcestailored to address common writingproblems, and?
crowdsource comments and resources.The permissions-based workflow features ofMy Reviewers enable teachers and students to usea rubric and commenting tools to review andgrade student writing while protecting studentconfidentiality behind a Net ID.My Reviewers is founded on the assumptionsthat language and learning are social practices,and that students can provide valuable feedbackto one another based on their backgrounds asreaders and critical thinkers.By enabling students to track their progress (orlack of progress) according to various evaluativecriteria (such as focus, evidence, organization,style, and format), My Reviewers clarifies aca-demic expectations and facilitates reflection andawareness of teachers?
evaluations and concerns,thereby helping students grow as writers, editors,and collaborators.
Furthermore, the pedagogicalmaterials embedded into the tool?videos, ex-planatory materials, exercises, library of com-ments with supporting hyperlinks?clarify19grading criteria for both students and teachers.In summary, by aggregating assessment results ininnovative new ways, My Reviewers reshapeshow teachers respond to writing, how studentsconduct peer reviews, how students track theirdevelopment as writers and reader feedback, andhow universities can conduct assessments ofstudents?
development as critical thinkers andwriters.3 Context and MethodsThe FYC (First-Year Composition) Program atthe University of South Florida is one of the lar-gest writing programs in the U.S, serving ap-proximately 7,500 students in two compositioncourses each year, ENC 1101 and ENC1102.
Thanks to funding from USF Tech FeeFunds and CTE21, we have piloted use of MyReviewers for the past three years, using My Re-viewers to assess over 30,000 student documents.Last semester (Fall 2011), approximately 70first-year composition instructors assessed16,000 essays (including early, intermediate, andfinal drafts)?not counting student peer re-views.
This semester (Spring 2012), we are oncourse for reviewing another 16,000 essays.
TheNational Council of Teachers of English awardedthe FYC Program the 2011-12 CCCC (Confer-ence on College Composition and Communica-tion) Writing Program Certificate of ExcellenceAward based in part on its development of MyReviewers.Over the past eight years, our teachers andwriting program administrators have crowd-sourced a community rubric by employing vari-ous peer-production technologies and face-to-face meetings (see Table 1).
The early stages ofour development process are reported inVieregge, Stedman, Mitchell, & Moxley?s (2012)Agency in the Age of Peer Production, an ethno-graphic monograph published by NCTE?s serieson Studies in Writing and Rhetoric.Since moving from a requirement for our in-structors to use a printed version of the commu-nity rubric to using My Reviewers, which enablesteachers to view the rubric while grading andassociates rubric scores with marked-up texts, wehave observed some benefits: While we mayhave 500 sections of the 1101 and 1102 courses,we want all of these sections to focus on sharedoutcomes.
We have found our use of My Re-viewers helps ensure students have a more com-parable experience than when paper rubrics wereused.
Back in the days of the printed version ofthe rubric, at the end of the semester when wesurveyed students about usage, about half of ourstudents reported they were unfamiliar with therubric.
One of the advantages of an online toollike My Reviewers for universities is that it en-ables writing program administrators to betterensure instructors and students are keeping upwith our shared curriculum.
Also, by using asingle analytic rubric tool across sections, we canassess progress by student, teacher, section, andrubric criteria.Figure 1: Sample Document Markup and Rubric20As rhetoricians, we understand the value ofusing rubrics that address the demands of spe-cific rhetorical contexts.
When addressing dif-ferent genres, audiences, disciplines and whenusing multiple media to remediate texts (Twitter,podcasts, movies, print documents), studentsclearly benefit from receiving feedback related toconventions in those genres, disciplines, andmedia.
Given this, we clearly understand whyPeter Elbow, Chris Anson, William Condon,among other assessment leaders, fault universi-ties for employing a generic rubric like ourcommunity rubric to assess texts across projects,genres, courses, media and so on.
Like Elbow(2006), Anson (2011), and Condon (2011), wesee enormous value in clarifying specific gradingCriteria Level Emerging01 Developing23 Mastering4Focus Basics Does not meet assign-mentrequirementsPartially meets assignmentrequirementsMeets assignment re-quirementsCriticalThink-ingAbsent or weak thesis;ideas are underdevel-oped, vague orunrelated to thesis;poor analysis of ideasrelevant to thesisPredictable or unoriginal the-sis; ideas are partially devel-oped and related to thesis;inconsistent analysis of subjectrelevant to thesisInsightful/intriguing the-sis; ideas are convincingand compelling; cogentanalysis of subject rele-vant to thesisEvidence CriticalThink-ingSources and supportingdetails lack credibility;poor synthesis of pri-mary and secondarysources/evidence rele-vant to thesis; poorsynthesis ofvisuals/personal ex-perience/anecdotesrelevant to thesis;rarely distinguishesbetween writer?
?s ideasand source?
?s ideasFair selection of crediblesources and supporting de-tails; unclear relationshipbetween thesis and primaryand secondarysources/evidence; ineffectivesynthesis of sources/evidencerelevant to thesis; occasionallyeffective synthesis ofvisuals/personal experi-ence/anecdotes relevant tothesis; inconsistently distin-guishes between writer?
?s ideasand source?
?s ideasCredible and usefulsources and supportingdetails; cogent synthesisof primary and secondarysources/evidence relevantto thesis; clever synthesisof visuals/personal ex-perience/anecdotes rele-vant to thesis;distinguishes betweenwriter?
?s ideas and source'sideas.Organization Basics Confusing opening;absent, inconsistent, ornon-relevant topicsentences; few transi-tions and absent orunsatisfying conclusionUninteresting or somewhattrite introduction, inconsistentuse of topics sentences, se-gues, transitions, and medio-cre conclusionEngaging introduction,relevant topic sentences,good segues, appropriatetransitions, and compel-ling conclusionCriticalThink-ingIllogical progression ofsupportingpoints; lacks cohesive-nessSupporting points follow asomewhat logical progression;occasional wandering of ideas;some interruption of cohesive-nessLogical progression ofsupporting points; verycohesiveStyle Basics Frequent gram-mar/punctuation er-rors; inconsistent pointof viewSome grammar/punctuationerrors occur in some places;somewhat consistent point ofviewCorrect grammar andpunctuation; consistentpoint of viewCriticalThink-ingSignificant problemswith syntax,diction, word choice,and vocabularyOccasional problems withsyntax, diction, word choice,and vocabularyRhetorically-sound syntax,diction, word choice, andvocabulary; effective useof figurative languageFormat Basics Little compliance withaccepted documenta-tion style (i.e., MLA,APA) for paper format-ting, in-text citations,annotatedbibliographies, andworks cited; minimalattention to documentdesignInconsistent compliance withaccepted documentation style(i.e., MLA, APA) for paperformatting, in-text citations,annotated bibliographies, andworks cited; some attention todocument designConsistent compliancewith accepted documenta-tion style (i.e., MLA, APA)for paper formatting, in-text citations, annotatedbibliographies, and workscited; strong attention todocument designTable 1: Community Assessment Rubric21criteria for specific projects, and we understandgrading criteria change along with changes indifferent rhetorical situations.
Plus, as composi-tionists, we understand that writers need differentkinds of feedback when they are in differentstages of the composing process.
Using a rubriclike our community rubric early in the writingprocess can clearly be overkill.
There is no pointin discussing style, for example, when the writerneeds to be told that his or her purpose is unclearor not satisfactory given the assignment specifi-cations.
Nonetheless, we have found?as wediscuss below?some benefits for using ourcommunity rubric to assess multiple projects,even ones that address different audiences, gen-res, and media.4 Independent Validation of the Com-munity Rubric by the USF Office ofInstitutional EffectivenessWhile we are currently seeking funding to addadministration features that would enable usersto write their own rubrics or import rubrics, MyReviewers employs a single community rubric(see Table 1) that has been validated by an inde-pendent assessment conducted by the Office ofInstitutional Effectiveness at the University ofSouth Florida in the spring of 2010.To conduct the assessment, 10 independentscorers reviewed the third/final drafts of 249students?
ENC 1101 Project 2 essays and thesesame students?
ENC 1102 Project 2 essays.
TheOffice of Institutional Effectiveness settled onthis odd number?249?because it represented5% of our total unique student head count (4,980students) for the 2009/2010 academic year.
Thescorers used the same scoring rubric to evaluateall 498 essays according to eight criteria deline-ated in our community rubric.
Scorers did notprovide comments nor did they have access tothe markup and grading provided by the stu-dents?
classroom instructors.Before the raters scored the randomly chosenstudent essays, an assessment expert from theOffice of Institutional Effectiveness led a briefdiscussion of the rubric and asked the scorers toread sample essays.
He then computed an inter-rater agreement of .93.
Confident our scorersunderstood our rubric and encouraged by ourinter-rater reliability, raters subsequently scoredthe 498 essays over a three-day period.Naturally, we were pleased to see that our as-sessment results suggested students were makingsome progress on all measures of writing andcritical thinking, that their 1102 Project 2 scoreswere higher than their Project 2 scores in 1101,although we were underwhelmed by the degreeof improvement.
We also were not really sur-prised that we were able to reach a high level ofinter-rater reliability among raters.However, this study did reveal a counterintui-tive and remarkable result: by comparing therankings of the independent scorers with therankings of these students?
classroom teachers,we found no statistical difference on seven of theeight rubric criteria.
In other words, when itcame to scoring eight criteria, the only differencebetween the independent scorers and the class-room teachers was ?Style (Basics),?
a criterionthat represents a 5% grade weight when the ru-bric was used to grade student papers.
This dis-crepancy may suggest that the independentscorers were being more lenient regarding thestudents?
grammatical and stylistic infelicitiesthan the students?
classroom teachers.Overall, the high level of agreement amongthe classroom teachers and the independent scor-ers suggests My Reviewers (perhaps by clarifyingthe grading criteria for teachers and students)enables diverse reviewers to mediate a sharedevaluation of texts, to reach an unprecedentedlevel of inter-rater reliability among large groupsof readers?what we might call ?Objectivity2.0.
?In a recent exchange on the Writing ProgramAdministrator Listserv, Chris Anson, this year?sChair of the Conference on College Compositionand past president of the Writing Program Ad-ministrators writes: ?
[the] Problem with [generic]rubrics is their usual high level of generalization(which makes them worthless).?
In a subsequentco-authored essay, ?Big Rubrics and Weird Gen-res:  The Futility of Using Generic AssessmentTools Across Diverse Instructional Contexts,?Anson et.
al.
(in press) write: ?Put simply, ge-neric, all-purpose criteria for evaluating writingand oral communication fail to reflect the lin-guistic, rhetorical, relational, and contextualcharacteristics of specific kinds of writing orspeaking that we find in higher education.
?While we share Anson?s preferences for ru-brics that are designed to address the particularconventions of specific genres, audiences andmedia, and while we hope to secure the fundingwe need to add greater flexibility to My Review-ers?so we can better account for different rhe-torical situations and media?, our researchdemonstrates the value and credibility of using acommunity rubric to assess multiple genres, even22ones that are quite distinct, such as the personalnarrative essays versus third-person based re-search reports.
Perhaps our results suggest thatthe eight criteria defined by our rubric are gener-alizable enough across disciplines, genres, andmedia that university faculty can recognize themand employ them in meaningful ways to reachObjectivity 2.0.To be completely frank, we are somewhat as-tounded by the inter-rater reliability we havebeen able to achieve among such diverse readers,and we wonder whether a rubric such as ourcommunity rubric can be used meaningfully toovercome the ?courseocentrism?
that GeraldGraff (2010) has described as undermining edu-cation in the U.S.  Perhaps a tool such as MyReviewers can be used to leverage communica-tion across departments, perhaps general-education wide, to address the common charac-teristics of academic prose that faculty acrossdisciplines value.5 Assess Undergraduate LearningRichard Arum and Josipa Roksa have receivedworldwide attention for their evidence and argu-ment in Academically Adrift (2011) that under-graduates fail to learn much despite theircoursework.
In contrast, by comparing students?scores from project to project, we have been ableto demonstrate students?
development as writers,researchers, and critical thinkers.
Note, for ex-ample, our evidence, shown in Figure 2, of stu-dent development over one academic semester?based not on a small sample size but on all stu-dents in ENC 1102 that semester.Figure 2: 1102 Final Project Scores6 Make Evidence-Based CurriculumChangesAs any seasoned teacher or administrator knows,not all curricular materials are equivalent.
Onoccasion, students perform poorly not because ofa lack of innate inability but because of poorcurricular planning on the part of the teachers(e.g., inadequate scaffolding of projects).
Figure3 illustrates ways My Reviewers can be used toimprove the curriculum in light of evidence?illustrating ways assessment results can be usedto inform curriculum changes.
In this example,program administrators made changes to thehistoriography project (Project 2) from theSpring 2010 semester, and, subsequently, in theFall 2011 semester students scored significantlybetter on most measures (Langbehn, McIntyre,Moxley, 2012).Figure 3: Comparison of Project 2 for the Spring2010 vs. Fall 2011 Semesters7 Compare Alternative Curricular Ap-proachesUse of a community rubric across genres, cour-ses and disciplines can also be used to chart stu-dent progress, or lack of progress, or to indicatedistinctions between the levels of difficulty im-posed by unique projects/genres.
On occasion,the lack of student success can be linked to issuespertaining to curriculum design as opposed to aparticular student deficit.
Figure 4 shows thecomparison of student scores in two alternativecourses, taken in succession by students at ouruniversity?results that suggest we need to onceagain rethink our curriculum for 1101 despite ourintuition that the course was well designed andwell received:238 Develop and Compare New Modelsfor Teaching and LearningWriting programs can use tools such as My Re-viewers to compare alternative curriculums.
Weare currently providing three alternative approa-ches to teaching writing in university settings?the traditional approach, where students meetthree hours each week in class; an online model;and a collaborative model, which requires stu-dents to use My Reviewers to conduct two cyclesof peer review and two cycles of teacher feed-back?as illustrated partially in Figure 5.9 NLP Features Under DevelopmentWe are currently implementing a library ofcomments, which we developed by analyzingapproximately  30,000 annotations and 20,000endnotes; we are in the process of developingresources to help students better understand tea-cher and peer comments.We are seeking additional funding to developan algorithm and badging system to inspire moreeffective peer-review.
By enabling students toearn badges according to the quality of theirfeedback, as measured by their peers and stu-dents, we are hoping to provide a further incen-tive for quality feedback.
We would like to tiethe badges to the number of substantive and edi-torial critiques that the document authors accountfor when revising, by endorsements by teachersfor peer feedback, and by overall rankings ofpeer reviews.Eventually we hope to add NLP (Natural Lan-guage Processing) tools that identify repeatedpatterns of error?as identified by past and pre-sent teachers who have used the tool.
For exam-ple, students could be informed when they havereceived similar feedback in the past, and theycould be offered hyperlinks back to past, similarcomments.
We can imagine features that high-light for teachers common comments on specificsets of papers or projects.
Perhaps OER (OpenEducation Resources) such as Writing Com-mons, http://writingcommons.org, could be sug-gested as teachers and peers make comments.10 ConclusionsIn his seminal work, The Wealth of Networks,Yochai Benkler wisely remarks,Different technologies make different kindsof human action and interaction easier orharder to perform.
All other things beingequal, things that are easier to do are morelikely to be done, and things that are harderto do are less likely to be done.
(17)My Reviewers, and other tools like it that are indevelopment, shatter pedagogical practices bymaking it easier to provide comments, easier toorganize and grade peer reviews, and easier toconduct assessments based on whole populationsrather than randomly selected groups.
The Lear-ning Analytics embedded in tools like My Re-viewers can empower students, teachers, andadministrators in meaningful ways.Figure 4: 1101 (left) vs. 1102 Final Project Results24AcknowledgmentsProject Development has been a deeply collabo-rative effort.
Terry Beavers, Mike Shuman, andI?the chief architects of My Reviewers?havebenefitted from the contributions of many col-leagues.
We thank Michelle Flanagan, for herongoing development work; Dianne Donnelly;Hunt Hawkins; Janet Moore; Steve RiCharde;Dianne Williams; Nancy Serrano, Megan McIn-tyre; Nancy Lewis; Brianna Jerman; Erin Trauth.Finally, we thank the University of South FloridaTechnology Fee Grant Program and the Centerfor 21st Century Teaching Excellence for fun-ding our project.ReferencesChris M. Anson.
2011.
Re: Rubrics and writingassessment.
In WPA-L Archives.
Council ofWriting Program Administrators.
Message postedto http://wpacouncil.org/wpa-lFigure 5: Cycle 1 for Peer Review Process25Chris M. Anson, Deanna P. Dannels, Pamela Flash, &A.L.H.
Gaffney.
In press.
Big Rubrics and WeirdGenres: The Futility of Using Generic AssessmentTools Across Diverse Instructional Contexts.Journal of Writing Assessment.Richard Arum & Josipa Roksa.
2011.
AcademicallyAdrift.
University of Chicago Press, Chicago.Yochai Benkler.
2006.
The Wealth of Networks.Yale University Press, New Haven and London.William F. Condon.
2011.
Re: Rubrics and writingassessment.
In WPA-L Archives.
Council ofWriting Program Administrators.
Message postedto http://wpacouncil.org/wpa-lPeter Elbow.
2006.
Do We Need a Single Standardof Value for Institutional Assessment?
An EssayResponse to Asao Inoue?s ?Community-Based As-sessment Pedagogy?.
Assessing Writing, 11:81?99.Gerald Graff.
2010.
Why Assessment?
Pedagogy,12(1):153-165.Karen Langbehn, Megan McIntyre & Joseph Moxley.Under review.
Using Real-Time Formative As-sessments to Close the Assessment Loop.
In HeidiMcKee & Danielle Nicole DeVoss (Eds.
), DigitalWriting Assessment.Quentin Vieregge, Kyle Stedman, Taylor Mitchell,and Joseph Moxley..
In press.
Agency in the Ageof Peer Production.
Studies in Writing and Rheto-ric Series.
National Council of Teachers of Eng-lish, Urbana, IL.26
