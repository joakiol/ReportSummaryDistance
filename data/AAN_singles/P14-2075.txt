Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 458?463,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning a Lexical Simplifier Using WikipediaColby Horn, Cathryn Manduca and David KauchakComputer Science DepartmentMiddlebury College{chorn,cmanduca,dkauchak}@middlebury.eduAbstractIn this paper we introduce a new lexicalsimplification approach.
We extract over30K candidate lexical simplifications byidentifying aligned words in a sentence-aligned corpus of English Wikipedia withSimple English Wikipedia.
To apply theserules, we learn a feature-based ranker us-ing SVMranktrained on a set of labeledsimplifications collected using Amazon?sMechanical Turk.
Using human simplifi-cations for evaluation, we achieve a preci-sion of 76% with changes in 86% of theexamples.1 IntroductionText simplification is aimed at reducing the read-ing and grammatical complexity of text while re-taining the meaning (Chandrasekar and Srinivas,1997).
Text simplification techniques have a broadrange of applications centered around increasingdata availability to both targeted audiences, suchas children, language learners, and people withcognitive disabilities, as well as to general readersin technical domains such as health and medicine(Feng, 2008).Simplifying a text can require a wide rangeof transformation operations including lexicalchanges, syntactic changes, sentence splitting,deletion and elaboration (Coster and Kauchak,2011; Zhu et al, 2010).
In this paper, we ex-amine a restricted version of the text simplifica-tion problem, lexical simplification, where text issimplified by substituting words or phrases withsimpler variants.
Even with this restriction, lexi-cal simplification techniques have been shown topositively impact the simplicity of text and to im-prove reader understanding and information reten-tion (Leroy et al, 2013).
Additionally, restrict-ing the set of transformation operations allows formore straightforward evaluation than the generalsimplification problem (Specia et al, 2012).Most lexical simplification techniques rely ontransformation rules that change a word or phraseinto a simpler variant with similar meaning (Bi-ran et al, 2011; Specia et al, 2012; Yatskar etal., 2010).
Two main challenges exist for this typeof approach.
First, the lexical focus of the trans-formation rules makes generalization difficult; alarge number of transformation rules is required toachieve reasonable coverage and impact.
Second,rules do not apply in all contexts and care must betaken when performing lexical transformations toensure local cohesion, grammaticality and, mostimportantly, the preservation of the original mean-ing.In this paper, we address both of these issues.We leverage a data set of 137K aligned sentencepairs between English Wikipedia and Simple En-glish Wikipedia to learn simplification rules.
Pre-vious approaches have used unaligned versions ofSimple English Wikipedia to learn rules (Biran etal., 2011; Yatskar et al, 2010), however, by usingthe aligned version we are able to learn a muchlarger rule set.To apply lexical simplification rules to a newsentence, a decision must be made about which, ifany, transformations should be applied.
Previousapproaches have used similarity measures (Biranet al, 2011) and feature-based approaches (Speciaet al, 2012) to make this decision.
We take the lat-ter approach and train a supervised model to rankcandidate transformations.2 Problem SetupWe learn lexical simplification rules that consistof a word to be simplified and a list of candidatesimplifications:w ?
c1, c2, ..., cmConsider the two aligned sentence pairs in Table458The first school was established in 1857.The first school was started in 1857.The district was established in 1993 by mergingthe former districts of Bernau and Eberswalde.The district was made in 1993 by joining theold districts of Bernau and Eberswalde.Table 1: Two aligned sentence pairs.
The bottomsentence is a human simplified version of the topsentence.
Bold words are candidate lexical simpli-fications.1.
The bottom sentence of each pair is a simpli-fied variant of the top sentence.
By identifyingaligned words within the aligned sentences, can-didate lexical simplifications can be learned.
Thebold words show two such examples, though othercandidates exist in the bottom pair.
By examiningaligned sentence pairs we can learn a simplifica-tion rule.
For example, we might learn:established?
began,made, settled, startedGiven a sentence s1, s2, ..., sn, a simplificationrule applies if the left hand side of the rule can befound in the sentence (si= w, for some i).
If arule applies, then a decision must be made aboutwhich, if any, of the candidate simplificationsshould be substituted for the word w to simplifythe sentence.
For example, if we were attemptingto simplify the sentenceThe ACL was established in 1962.using the simplification rule above, some of thesimplification options would not apply becauseof grammatical constraints, e.g.
began, whileothers would not apply for semantic reasons, e.g.settled.
This does not mean that these are notgood simplifications for established since in othercontexts, they might be appropriate.
For example,in the sentenceThe researcher established a new paperwriting routine.began is a reasonable option.3 Learning a Lexical SimplifierWe break the learning problem into two steps: 1)learn a set of simplification rules and 2) learn aranking function for determining the best simpli-fication candidate when a rule applies.
Each ofthese steps are outlined below.3.1 Rule ExtractionTo extract the set of simplification rules, we usea sentence-aligned data set of English Wikipediasentences (referred to as normal) aligned to Sim-ple English Wikipedia sentences (referred to assimple) (Coster and Kauchak, 2011).
The data setcontains 137K such aligned sentence pairs.Given a normal sentence and the correspondingaligned simple sentence, candidate simplificationsare extracted by identifying a word in the simplesentence that corresponds to a different word in thenormal sentence.
To identify such pairs, we au-tomatically induce a word alignment between thenormal and simple sentence pairs using GIZA++(Och and Ney, 2000).
Words that are aligned areconsidered as possible candidates for extraction.Due to errors in the sentence and word alignmentprocesses, not all words that are aligned are actu-ally equivalent lexical variants.
We apply the fol-lowing filters to reduce such spurious alignments:?
We remove any pairs where the normal wordoccurs in a stoplist.
Stoplist words tend to besimple already and stoplist words that are be-ing changed are likely either bad alignmentsor are not simplifications.?
We require that the part of speeches (POS)of the two words be the same.
The parts ofspeech were calculated based on a full parseof the sentences using the Berkeley parser(Petrov and Klein, 2007).?
We remove any candidates where the POSis labeled as a proper noun.
In most cases,proper nouns should not be simplified.All other aligned word pairs are extracted.
Togenerate the simplification rules, we collect allcandidate simplifications (simple words) that arealigned to the same normal word.As mentioned before, one of the biggest chal-lenges for lexical simplification systems is gen-eralizability.
To improve the generalizability ofthe extracted rules, we add morphological variantsof the words in the rules.
For nouns, we includeboth singular and plural variants.
For verbs, weexpand to all inflection variants.
The morpholog-ical changes are generated using MorphAdorner(Burns, 2013) and are applied symmetrically: anychange to the normal word is also applied to thecorresponding simplification candidates.4593.2 Lexical Simplification as a RankingProblemA lexical simplification example consists of threeparts: 1) a sentence, s1, s2, ..., sn, 2), a word inthat sentence, si, and 3) a list of candidate sim-plifications for si, c1, c2, ..., cm.
A labeled exam-ple is an example where the rank of the candidatesimplifications has been specified.
Given a set oflabeled examples, the goal is to learn a rankingfunction that, given an unlabeled example (exam-ple without the candidate simplifications ranked),specifies a ranking of the candidates.To learn this function, features are extractedfrom a set of labeled lexical simplification exam-ples.
These labeled examples are then used to traina ranking function.
We use SVMrank(Joachims,2006), which uses a linear support vector machine.Besides deciding which of the candidates ismost applicable in the context of the sentence,even if a rule applies, we must also decide ifany simplification should occur.
For example,there may be an instance where none of the can-didate simplifications are appropriate in this con-text.
Rather than viewing this as a separate prob-lem, we incorporate this decision into the rankingproblem by adding w as a candidate simplifica-tion.
For each rule, w ?
c1, c2, ..., cmwe add oneadditional candidate simplification which does notchange the sentence, w ?
c1, c2, ..., cm, w. If w isranked as the most likely candidate by the rankingalgorithm, then the word is not simplified.3.2.1 FeaturesThe role of the features is to capture informationabout the applicability of the word in the contextof the sentence as well as the simplicity of theword.
Many features have been suggested previ-ously for use in determining the simplicity of aword (Specia et al, 2012) and for determining ifa word is contextually relevant (Biran et al, 2011;McCarthy and Navigli, 2007).
Our goal for thispaper is not feature exploration, but to examinethe usefulness of a general framework for feature-based ranking for lexical simplification.
The fea-tures below represent a first pass at candidate fea-tures, but many others could be explored.Candidate Probabilityp(ci|w): in the sentence-aligned Wikipedia data,when w is aligned to some candidate simplifica-tion, what proportion of the time is that candidateci.FrequencyThe frequency of a word has been shown to cor-relate with the word?s simplicity and with peo-ple?s knowledge of that word (Leroy and Kauchak,2013).
We measured a candidate simplification?sfrequency in two corpora: 1) Simple EnglishWikipedia and 2) the web, as measured by the un-igram frequency from the Google n-gram corpus(Brants and Franz, 2006).Language Modelsn-gram language models capture how likely a par-ticular sequence is and can help identify candidatesimplifications that are not appropriate in the con-text of the sentence.
We included features fromfour different language models trained on four dif-ferent corpora: 1) Simple English Wikipedia, 2)English Wikipedia, 3) Google n-gram corpus and4) a linearly interpolated model between 1) and2) with ?
= 0.5, i.e.
an even blending.
Weused the SRI language modeling toolkit (Stolcke,2002) with Kneser-Kney smoothing.
All modelswere trigram language models except the Googlen-gram model, which was a 5-gram model.Context FrequencyAs another measure of the applicability of a can-didate in the context of the sentence, we also cal-culate the frequency in the Google n-grams of thecandidate simplification in the context of the sen-tence with context windows of one and two words.If the word to be substituted is at position i in thesentence (w = si), then the one word windowfrequency for simplification cjis the trigram fre-quency of si?1cjsi+1and the two word windowthe 5-gram frequency of si?2si?1cjsi+1si+2.4 DataFor training and evaluation of the models, we col-lected human labelings of 500 lexical simplifica-tion examples using Amazon?s Mechanical Turk(MTurk)1.
MTurk has been used extensively forannotating and evaluating NLP tasks and has beenshown to provide data that is as reliable as otherforms of human annotation (Callison-Burch andDredze, 2010; Zaidan and Callison-Burch, 2011).Figure 1 shows an example of the task we askedannotators to do.
Given a sentence and a wordto be simplified, the task is to suggest a simplervariant of that word that is appropriate in the con-text of the sentence.
Candidate sentences were se-1https://www.mturk.com/460Enter a simpler word that could be substituted for the red, bold word in the sentence.
A simplerword is one that would be understood by more people or people with a lower reading level (e.g.children).Food is procured with its suckers and then crushed using its tough ?beak?
of chitin.Figure 1: Example task setup on MTurk soliciting lexical simplifications from annotators.lected from the sentence-aligned Wikipedia cor-pus where a word in the normal sentence is be-ing simplified to a different word in the simplesentence, as identified by the automatically in-duced word alignment.
The normal sentence andthe aligned word were then selected for annota-tion.
These examples represent words that otherpeople (those that wrote/edited the Simple En-glish Wikipedia page) decided were difficult andrequired simplification.We randomly selected 500 such sentences andcollected candidate simplifications from 50 peopleper sentence, for a total of 25,000 annotations.
Toparticipate in the annotation process, we requiredthat the MTurk workers live in the U.S. (for En-glish proficiency) and had at least a 95% accep-tance rate on previous tasks.The simplifications suggested by the annotatorswere then tallied and the resulting list of simpli-fications with frequencies provides a ranking fortraining the candidate ranker.
Table 2 shows theranked list of annotations collected for the exam-ple in Figure 1.
This data set is available online.2Since these examples were selected from En-glish Wikipedia they, and the correspondingaligned Simple English Wikipedia sentences, wereremoved from all resources used during both therule extraction and the training of the ranker.5 Experiments5.1 Other ApproachesWe compared our lexical simplification approach(rank-simplify) to two other approaches.
To un-derstand the benefit of the feature-based rankingalgorithm, we compared against a simplifier thatuses the same rule set, but ranks the candidatesonly based on their frequency in Simple EnglishWikipedia (frequency).
This is similar to base-lines used in previous work (Biran et al, 2011).To understand how our extracted rules com-pared to the rules extracted by Biran et al, we2http://www.cs.middlebury.edu/?dkauchak/simplification/used their rules with our ranking approach (rank-Biran).
Their approach also extracts rules froma corpus of English Wikipedia and Simple En-glish Wikipedia, however, they do not utilize asentence-aligned version and instead rely on con-text similarity measures to extract their rules.5.2 EvaluationWe used the 500 ranked simplification examples totrain and evaluate our approach.
We employed 10-fold cross validation for all experiments, trainingon 450 examples and testing on 50.We evaluated the models with four differentmetrics:precision: Of the words that the system changed,what percentage were found in any of the humanannotations.precision@k: Of the words that the systemchanged, what percentage were found in the topk human annotations, where the annotations wereranked by response frequency.
For example, if wewere calculating the precision@1 for the examplein Table 2, only ?obtained?
would be consideredcorrect.accuracy: The percentage of the test exampleswhere the system made a change to one of theannotations suggested by the human annotators.Note that unlike precision, if the system does notsuggest a change to a word that was simplified itstill gets penalized.changed: The percentage of the test exampleswhere the system suggested some change (even ifit wasn?t a ?correct?
change).5.3 ResultsTable 3 shows the precision, accuracy and percentchanged for the three systems.
Based on all threemetrics, our system achieves the best results.
Al-though the rules generated by Biran et al have rea-sonable precision, they suffer from a lack of cov-erage, only making changes on about 5% of the461word frequency word frequency word frequencyobtained 17 made 2 secured 1gathered 9 created 1 found 1gotten 8 processed 1 attained 1grabbed 4 received 1 procured 1acquired 2 collected 1 aquired 1Table 2: Candidate simplifications generated using MTurk for the examples in Figure 1.
The frequencyis the number of annotators that suggested that simplification.precision accuracy changedfrequency 53.9% 46.1% 84.9%rank-Biran 71.4% 3.4% 5.2%rank-simplify 76.1% 66.3% 86.3%Table 3: Precision, accuracy and percent changedfor the three systems, averaged over the 10 folds.examples.
For our approach, the extracted ruleshad very good coverage, applying in over 85% ofthe examples.This difference in coverage can be partially at-tributed to the number of rules learned.
We learnedsimplifications for 14,478 words with an averageof 2.25 candidate simplifications per word.
In con-trast, the rules from Biran et al only had simpli-fications for 3,598 words with an average of 1.18simplifications per word.The precision of both of the approaches thatutilized the SVM candidate ranking were sig-nificantly better than the frequency-based ap-proach.
To better understand the types of sug-gestions made by the systems, Figure 2 shows theprecision@k for increasing k. On average, overthe 500 examples we collected, people suggested12 different simplifications, though this varied de-pending on the word in question and the sentence.As such, at around k=12, the precision@k of mostof the systems has almost reached the final preci-sion.
However, even at k = 5, which only countscorrect an answer in the top 5 human suggestedresults, our system still achieved a precision ofaround 67%.6 Future WorkIn this paper we have introduced a new rule ex-traction algorithm and a new feature-based rank-ing approach for applying these rules in the con-text of different sentences.
The number of ruleslearned is an order of magnitude larger than anyprevious lexical simplification approach and the0.30.350.40.450.50.550.60.650.70.750.80  5  10  15  20  25  30  35  40  45  50precisionkrank-simplifyrank-BiranfrequencyFigure 2: Precision@k for varying k for the threedifferent approaches averaged over the 10 folds.quality of the resulting simplifications after apply-ing these rules is better than previous approaches.Many avenues exist for improvement and forbetter understanding how well the current ap-proach works.
First, we have only explored asmall set of possible features in the ranking algo-rithm.
Additional improvements could be seen byincorporating a broader feature set.
Second, moreanalysis needs to be done to understand the qualityof the produced simplifications and their impact onthe simplicity of the resulting sentences.
Third, theexperiments above assume that the word to be sim-plified has already been identified in the sentence.This identification step also needs to be exploredto implement a sentence-level simplifier using ourapproach.
Fourth, the ranking algorithm can beapplied to most simplification rules (e.g.
we ap-plied the ranking approach to the rules obtainedby Biran et al (2011)).
We hope to explore otherapproaches for increasing the rule set by incorpo-rating other rule sources and other rule extractiontechniques.462ReferencesOr Biran, Samuel Brody, and Noe ?mie Elhadad.
2011.Putting it simply: A context-aware approach to lexi-cal simplification.
In Proceedings of ACL.Thorsten Brants and Alex Franz.
2006.
Web 1T5-gram version 1.
Linguistic Data Consortium,Philadelphia.Philip R. Burns.
2013.
Morphadorner v2: A Java li-brary for the morphological adornment of englishlanguage texts.Chris Callison-Burch and Mark Dredze.
2010.
Cre-ating speech and language data with Amazon?s Me-chanical Turk.
In Proceedings of NAACL-HLT 2010Workshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk.Raman Chandrasekar and Bangalore Srinivas.
1997.Automatic induction of rules for text simplification.In Knowledge Based Systems.William Coster and David Kauchak.
2011.
Simple En-glish Wikipedia: A new text simplification task.
InProceedings of ACL.Lijun Feng.
2008.
Text simplification: A survey.CUNY Technical Report.Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In Proceedings of KDD.Gondy Leroy and David Kauchak.
2013.
The effectof word familiarity on actual and perceived text dif-ficulty.
Journal of American Medical InformaticsAssociation.Gondy Leroy, James E. Endicott, David Kauchak,Obay Mouradi, and Melissa Just.
2013.
User evalu-ation of the effects of a text simplification algorithmusing term familiarity on perception, understanding,learning, and information retention.
Journal of Med-ical Internet Research (JMIR).Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of SEMEVAL.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In ACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HTL-NAACL.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-cea.
2012.
Semeval-2012 task 1: English lexicalsimplification.
In Joint Conference on Lexical andComputerational Semantics (*SEM).Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the In-ternational Conference on Statistical Language Pro-cessing.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifi-cations from Wikipedia.
In NAACL/HLT.Omar F. Zaidan and Chris Callison-Burch.
2011.Crowdsourcing translation: Professional qualityfrom non-professionals.
In Proceedings of ACL.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of ICCL.463
