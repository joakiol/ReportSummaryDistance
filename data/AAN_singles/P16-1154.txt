Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1631?1640,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsIncorporating Copying Mechanism in Sequence-to-Sequence LearningJiatao Gu?Zhengdong Lu?Hang Li?Victor O.K.
Li?
?Department of Electrical and Electronic Engineering, The University of Hong Kong{jiataogu, vli}@eee.hku.hk?Huawei Noah?s Ark Lab, Hong Kong{lu.zhengdong, hangli.hl}@huawei.comAbstractWe address an important problem insequence-to-sequence (Seq2Seq) learningreferred to as copying, in which cer-tain segments in the input sequence areselectively replicated in the output se-quence.
A similar phenomenon is ob-servable in human language communica-tion.
For example, humans tend to re-peat entity names or even long phrasesin conversation.
The challenge with re-gard to copying in Seq2Seq is that newmachinery is needed to decide when toperform the operation.
In this paper, weincorporate copying into neural network-based Seq2Seq learning and propose a newmodel called COPYNET with encoder-decoder structure.
COPYNET can nicelyintegrate the regular way of word gener-ation in the decoder with the new copy-ing mechanism which can choose sub-sequences in the input sequence and putthem at proper places in the output se-quence.
Our empirical study on both syn-thetic data sets and real world data setsdemonstrates the efficacy of COPYNET.For example, COPYNET can outperformregular RNN-based model with remark-able margins on text summarization tasks.1 IntroductionRecently, neural network-based sequence-to-sequence learning (Seq2Seq) has achieved re-markable success in various natural language pro-cessing (NLP) tasks, including but not limited toMachine Translation (Cho et al, 2014; Bahdanauet al, 2014), Syntactic Parsing (Vinyals et al,2015b), Text Summarization (Rush et al, 2015)and Dialogue Systems (Vinyals and Le, 2015).Seq2Seq is essentially an encoder-decoder model,in which the encoder first transform the input se-quence to a certain representation which can thentransform the representation into the output se-quence.
Adding the attention mechanism (Bah-danau et al, 2014) to Seq2Seq, first proposedfor automatic alignment in machine translation,has led to significant improvement on the perfor-mance of various tasks (Shang et al, 2015; Rush etal., 2015).
Different from the canonical encoder-decoder architecture, the attention-based Seq2Seqmodel revisits the input sequence in its raw form(array of word representations) and dynamicallyfetches the relevant piece of information basedmostly on the feedback from the generation of theoutput sequence.In this paper, we explore another mechanismimportant to the human language communication,called the ?copying mechanism?.
Basically, itrefers to the mechanism that locates a certain seg-ment of the input sentence and puts the segmentinto the output sequence.
For example, in thefollowing two dialogue turns we observe differ-ent patterns in which some subsequences (coloredblue) in the response (R) are copied from the inpututterance (I):I: Hello Jack, my name is Chandralekha.R: Nice to meet you, Chandralekha.I: This new guy doesn?t perform exactlyas we expected.R: What do you mean by "doesn?t performexactly as we expected"?Both the canonical encoder-decoder and itsvariants with attention mechanism rely heavilyon the representation of ?meaning?, which mightnot be sufficiently inaccurate in cases in whichthe system needs to refer to sub-sequences of in-put like entity names or dates.
In contrast, the1631copying mechanism is closer to the rote memo-rization in language processing of human being,deserving a different modeling strategy in neuralnetwork-based models.
We argue that it will ben-efit many Seq2Seq tasks to have an elegant unifiedmodel that can accommodate both understandingand rote memorization.
Towards this goal, we pro-pose COPYNET, which is not only capable of theregular generation of words but also the operationof copying appropriate segments of the input se-quence.
Despite the seemingly ?hard?
operationof copying, COPYNET can be trained in an end-to-end fashion.
Our empirical study on both syntheticdatasets and real world datasets demonstrates theefficacy of COPYNET.2 Background: Neural Models forSequence-to-sequence LearningSeq2Seq Learning can be expressed in a prob-abilistic view as maximizing the likelihood (orsome other evaluation metrics (Shen et al, 2015))of observing the output (target) sequence given aninput (source) sequence.2.1 RNN Encoder-DecoderRNN-based Encoder-Decoder is successfully ap-plied to real world Seq2Seq tasks, first by Cho etal.
(2014) and Sutskever et al (2014), and thenby (Vinyals and Le, 2015; Vinyals et al, 2015a).In the Encoder-Decoder framework, the source se-quence X = [x1, ..., xTS] is converted into a fixedlength vector c by the encoder RNN, i.e.ht= f(xt,ht?1); c = ?
({h1, ...,hTS}) (1)where {ht} are the RNN states, c is the so-calledcontext vector, f is the dynamics function, and ?summarizes the hidden states, e.g.
choosing thelast state hTS.
In practice it is found that gatedRNN alternatives such as LSTM (Hochreiter andSchmidhuber, 1997) or GRU (Cho et al, 2014) of-ten perform much better than vanilla ones.The decoder RNN is to unfold the context vec-tor c into the target sequence, through the follow-ing dynamics and prediction model:st= f(yt?1, st?1, c)p(yt|y<t, X) = g(yt?1, st, c)(2)where stis the RNN state at time t, ytis the pre-dicted target symbol at t (through function g(?
))with y<tdenoting the history {y1, ..., yt?1}.
Theprediction model is typically a classifier over thevocabulary with, say, 30,000 words.2.2 The Attention MechanismThe attention mechanism was first introduced toSeq2Seq (Bahdanau et al, 2014) to release theburden of summarizing the entire source into afixed-length vector as context.
Instead, the atten-tion uses a dynamically changing context ctin thedecoding process.
A natural option (or rather ?softattention?)
is to represent ctas the weighted sumof the source hidden states, i.e.ct=TS??=1?t?h?
; ?t?=e?(st?1,h?)???e?(st?1,h??
)(3)where ?
is the function that shows the correspon-dence strength for attention, approximated usuallywith a multi-layer neural network (DNN).
Notethat in (Bahdanau et al, 2014) the source sen-tence is encoded with a Bi-directional RNN, mak-ing each hidden state h?aware of the contextualinformation from both ends.3 COPYNETFrom a cognitive perspective, the copying mech-anism is related to rote memorization, requiringless understanding but ensuring high literal fi-delity.
From a modeling perspective, the copyingoperations are more rigid and symbolic, makingit more difficult than soft attention mechanism tointegrate into a fully differentiable neural model.In this section, we present COPYNET, a differen-tiable Seq2Seq model with ?copying mechanism?,which can be trained in an end-to-end fashion withjust gradient descent.3.1 Model OverviewAs illustrated in Figure 1, COPYNET is still anencoder-decoder (in a slightly generalized sense).The source sequence is transformed by Encoderinto representation, which is then read by Decoderto generate the target sequence.Encoder: Same as in (Bahdanau et al, 2014), abi-directional RNN is used to transform the sourcesequence into a series of hidden states with equallength, with each hidden state htcorresponding toword xt.
This new representation of the source,{h1, ...,hTS}, is considered to be a short-termmemory (referred to as M in the remainder of thepaper), which will later be accessed in multipleways in generating the target sequence (decoding).1632hello    ,     my     name   is    Tony  Jebara   .Attentive	Readhi     ,     Tony  Jebara<eos>   hi     ,     Tonyh1 h2 h3 h4 h5s1 s2 s3 s4h6 h7 h8 ?Tony?DNNEmbeddingfor ?Tony?Selective Readfor ?Tony?
(a) Attention-based Encoder-Decoder (RNNSearch)(c) State Updates4SourceVocabularySoftmaxProb(?Jebara?)
= Prob(?Jebara?, g) + Prob(?Jebara?, c)?
...(b) Generate-Mode & Copy-Mode?MMFigure 1: The overall diagram of COPYNET.
For simplicity, we omit some links for prediction (seeSections 3.2 for more details).Decoder: An RNN that reads M and predictsthe target sequence.
It is similar with the canoni-cal RNN-decoder in (Bahdanau et al, 2014), withhowever the following important differences?
Prediction: COPYNET predicts words basedon a mixed probabilistic model of two modes,namely the generate-mode and the copy-mode, where the latter picks words from thesource sequence (see Section 3.2);?
State Update: the predicted word at time t?1is used in updating the state at t, but COPY-NET uses not only its word-embedding butalso its corresponding location-specific hid-den state in M (if any) (see Section 3.3 formore details);?
ReadingM: in addition to the attentive readto M, COPYNET also has?selective read?to M, which leads to a powerful hybrid ofcontent-based addressing and location-basedaddressing (see both Sections 3.3 and 3.4 formore discussion).3.2 Prediction with Copying and GenerationWe assume a vocabulary V = {v1, ..., vN}, anduse UNK for any out-of-vocabulary (OOV) word.In addition, we have another set of words X , forall the unique words in source sequence X ={x1, ..., xTS}.
Since X may contain words notin V , copying sub-sequence in X enables COPY-NET to output some OOV words.
In a nutshell,the instance-specific vocabulary for source X isV ?
UNK ?
X .Given the decoder RNN state stat time t to-gether with M, the probability of generating anytarget word yt, is given by the ?mixture?
of proba-bilities as followsp(yt|st, yt?1, ct,M) = p(yt, g|st, yt?1, ct,M)+ p(yt, c|st, yt?1, ct,M) (4)where g stands for the generate-mode, and c thecopy mode.
The probability of the two modes aregiven respectively byp(yt, g|?)=??????
?1Ze?g(yt), yt?
V0, yt?
X ??V1Ze?g(UNK)yt6?
V ?
X(5)p(yt, c|?
)={1Z?j:xj=yte?c(xj), yt?
X0 otherwise(6)where ?g(?)
and ?c(?)
are score functions forgenerate-mode and copy-mode, respectively, andZ is the normalization term shared by the twomodes, Z =?v?V?
{UNK}e?g(v)+?x?Xe?c(x).Due to the shared normalization term, the twomodes are basically competing through a softmaxfunction (see Figure 1 for an illustration with ex-ample), rendering Eq.
(4) different from the canon-ical definition of the mixture model (McLachlanand Basford, 1988).
This is also pictorially illus-trated in Figure 2.
The score of each mode is cal-culated:1633unk?
??
?
???
exp ??
??
|	??
= ????
?
exp ??
??
| 	?
?= ??
????
?
exp ??
??
+??
exp ??
??
|	??
= ??,??
= ??
??
exp	[??
unk ]*Z	is	the	normalization	term.Figure 2: The illustration of the decoding proba-bility p(yt|?)
as a 4-class classifier.Generate-Mode: The same scoring function asin the generic RNN encoder-decoder (Bahdanau etal., 2014) is used, i.e.
?g(yt= vi) = v>iWost, vi?
V ?
UNK (7)where Wo?
R(N+1)?dsand viis the one-hot in-dicator vector for vi.Copy-Mode: The score for ?copying?
the wordxjis calculated as?c(yt= xj) = ?
(h>jWc)st, xj?
X (8)where Wc?
Rdh?ds, and ?
is a non-linear ac-tivation function, considering that the non-lineartransformation in Eq.
( 8) can help project stand hjin the same semantic space.
Empirically, we alsofound that using the tanh non-linearity workedbetter than linear transformation, and we used thatfor the following experiments.
When calculatingthe copy-mode score, we use the hidden states{h1, ...,hTS} to ?represent?
each of the word inthe source sequence {x1, ..., xTS} since the bi-directional RNN encodes not only the content, butalso the location information into the hidden statesin M. The location informaton is important forcopying (see Section 3.4 for related discussion).Note that we sum the probabilities of all xjequalto ytin Eq.
(6) considering that there may be mul-tiple source symbols for decoding yt.
Naturallywe let p(yt, c|?)
= 0 if ytdoes not appear in thesource sequence, and set p(yt, g|?)
= 0 when ytonly appears in the source.3.3 State UpdateCOPYNET updates each decoding state stwiththe previous state st?1, the previous symbol yt?1and the context vector ctfollowing Eq.
(2) for thegeneric attention-based Seq2Seq model.
However,there is some minor changes in the yt?1?
?stpathfor the copying mechanism.
More specifically,yt?1will be represented as [e(yt?1); ?
(yt?1)]>,where e(yt?1) is the word embedding associatedwith yt?1, while ?
(yt?1) is the weighted sum ofhidden states in M corresponding to yt?
(yt?1) =?TS?=1?t?h?
?t?={1Kp(x?, c|st?1,M), x?= yt?10 otherwise(9)where K is the normalization term which equals???:x??=yt?1p(x?
?, c|st?1,M), considering theremay exist multiple positions with yt?1in thesource sequence.
In practice, ?t?is often con-centrated on one location among multiple appear-ances, indicating the prediction is closely boundedto the location of words.In a sense ?
(yt?1) performs a type of read toM similar to the attentive read (resulting ct) withhowever higher precision.
In the remainder ofthis paper, ?
(yt?1) will be referred to as selectiveread.
?
(yt?1) is specifically designed for the copymode: with its pinpointing precision to the cor-responding yt?1, it naturally bears the location ofyt?1in the source sequence encoded in the hiddenstate.
As will be discussed more in Section 3.4,this particular design potentially helps copy-modein covering a consecutive sub-sequence of words.If yt?1is not in the source, we let ?
(yt?1) = 0.3.4 Hybrid Addressing of MWe hypothesize that COPYNET uses a hybridstrategy for fetching the content in M, which com-bines both content-based and location-based ad-dressing.
Both addressing strategies are coordi-nated by the decoder RNN in managing the atten-tive read and selective read, as well as determiningwhen to enter/quit the copy-mode.Both the semantics of a word and its locationin X will be encoded into the hidden states in Mby a properly trained encoder RNN.
Judging fromour experiments, the attentive read of COPYNET isdriven more by the semantics and language model,therefore capable of traveling more freely on M,even across a long distance.
On the other hand,once COPYNET enters the copy-mode, the selec-tive read of M is often guided by the location in-formation.
As the result, the selective read oftentakes rigid move and tends to cover consecutivewords, including UNKs.
Unlike the explicit de-sign for hybrid addressing in Neural Turing Ma-chine (Graves et al, 2014; Kurach et al, 2015),COPYNET is more subtle: it provides the archi-1634tecture that can facilitate some particular location-based addressing and lets the model figure out thedetails from the training data for specific tasks.Location-based Addressing: With the locationinformation in {hi}, the information flow?(yt?1)update????
stpredict????
ytsel.
read?????
?
(yt)provides a simple way of ?moving one step to theright?
on X .
More specifically, assuming the se-lective read ?
(yt?1) concentrates on the `thwordin X , the state-update operation ?(yt?1)update???
?stacts as ?location ?
location+1?, making stfavor the (`+1)thword in X in the predictionstpredict????
ytin copy-mode.
This again leads tothe selective read?htsel.
read??????
(yt) for the state up-date of the next round.Handling Out-of-Vocabulary Words Althoughit is hard to verify the exact addressing strategy asabove directly, there is strong evidence from ourempirical study.
Most saliently, a properly trainedCOPYNET can copy a fairly long segment full ofOOV words, despite the lack of semantic infor-mation in its M representation.
This provides anatural way to extend the effective vocabulary toinclude all the words in the source.
Although thischange is small, it seems quite significant empiri-cally in alleviating the OOV problem.
Indeed, formany NLP applications (e.g., text summarizationor spoken dialogue system), much of the OOVwords on the target side, for example the propernouns, are essentially the replicates of those on thesource side.4 LearningAlthough the copying mechanism uses the ?hard?operation to copy from the source and choose topaste them or generate symbols from the vocab-ulary, COPYNET is fully differentiable and canbe optimized in an end-to-end fashion using back-propagation.
Given the batches of the source andtarget sequence {X}Nand {Y }N, the objectivesare to minimize the negative log-likelihood:L = ?1NN?k=1T?t=1log[p(y(k)t|y(k)<t, X(k))], (10)where we use superscripts to index the instances.Since the probabilistic model for observing anytarget word is a mixture of generate-mode andcopy-mode, there is no need for any additionallabels for modes.
The network can learn to co-ordinate the two modes from data.
More specif-ically, if one particular word y(k)tcan be foundin the source sequence, the copy-mode will con-tribute to the mixture model, and the gradient willmore or less encourage the copy-mode; otherwise,the copy-mode is discouraged due to the compe-tition from the shared normalization term Z. Inpractice, in most cases one mode dominates.5 ExperimentsWe report our empirical study of COPYNET on thefollowing three tasks with different characteristics1.
A synthetic dataset on with simple patterns;2.
A real-world task on text summarization;3.
A dataset for simple single-turn dialogues.5.1 Synthetic DatasetDataset: We first randomly generate transforma-tion rules with 5?20 symbols and variables x &y, e.g.a b x c d y e f ??
g h x m,with {a b c d e f g h m} being regular symbolsfrom a vocabulary of size 1,000.
As shown in thetable below, each rule can further produce a num-ber of instances by replacing the variables withrandomly generated subsequences (1?15 sym-bols) from the same vocabulary.
We create fivetypes of rules, including ?x ?
??.
The task isto learn to do the Seq2Seq transformation fromthe training instances.
This dataset is designed tostudy the behavior of COPYNET on handling sim-ple and rigid patterns.
Since the strings to repeatare random, they can also be viewed as some ex-treme cases of rote memorization.Rule-type Examples (e.g.
x = i h k, y = j c)x?
?
a b c d x e f?
c d gx?
x a b c d x e f?
c d x gx?
xx a b c d x e f?
x d x gxy?
x a b y d x e f?
x d i gxy?
xy a b y d x e f?
x d y gExperimental Setting: We select 200 artificialrules from the dataset, and for each rule 200 in-stances are generated, which will be split intotraining (50%) and testing (50%).
We comparethe accuracy of COPYNET and the RNN Encoder-Decoder with (i.e.
RNNsearch) or without atten-tion (denoted as Enc-Dec).
For a fair compari-son, we use bi-directional GRU for encoder andanother GRU for decoder for all Seq2Seq models,with hidden layer size = 300 and word embeddingdimension = 150.
We use bin size = 10 in beamsearch for testing.
The prediction is considered1635Rule-type x x x xy xy?
?
?
x ?
xx ?
x ?
xyEnc-Dec 100 3.3 1.5 2.9 0.0RNNSearch 99.0 69.4 22.3 40.7 2.6COPYNET 97.3 93.7 98.3 68.2 77.5Table 1: The test accuracy (%) on synthetic data.correct only when the generated sequence is ex-actly the same as the given one.It is clear from Table 1 that COPYNET signifi-cantly outperforms the other two on all rule-typesexcept ?x?
?
?, indicating that COPYNET can ef-fectively learn the patterns with variables and ac-curately replicate rather long subsequence of sym-bols at the proper places.This is hard to Enc-Decdue to the difficulty of representing a long se-quence with very high fidelity.
This difficulty canbe alleviated with the attention mechanism.
How-ever attention alone seems inadequate for handlingthe case where strict replication is needed.A closer look (see Figure 3 for example) re-veals that the decoder is dominated by copy-modewhen moving into the subsequence to replicate,and switch to generate-mode after leaving thisarea, showing COPYNET can achieve a rather pre-cise coordination of the two modes.Pattern: 705 502 X 504 339 270 584 556?510 771 581 557 022 230 X 115 102 172 862 X 950* Symbols are represented by their indices from 000 to 999** Dark color represents large value.
510 771 581 557 263 022 230 970 991 575 325 688 115 102 172 862 970 991 575 325 688 950705 502 970 991 575 325 688504 339 270 584 556The SourceSequence<0> 510 771 581 557 263 022 230 970 991 575 325 688 115 102 172 862 970 991 575 325 688Input:The Target SequencePredict:Figure 3: Example output of COPYNET on thesynthetic dataset.
The heatmap represents the ac-tivations of the copy-mode over the input sequence(left) during the decoding process (bottom).5.2 Text SummarizationAutomatic text summarization aims to find a con-densed representation which can capture the coremeaning of the original document.
It has beenrecently formulated as a Seq2Seq learning prob-lem in (Rush et al, 2015; Hu et al, 2015), whichessentially gives abstractive summarization sincethe summary is generated based on a represen-tation of the document.
In contrast, extractivesummarization extracts sentences or phrases fromthe original text to fuse them into the summaries,therefore making better use of the overall struc-ture of the original document.
In a sense, COPY-NET for summarization lies somewhere betweentwo categories, since part of output summary is ac-tually extracted from the document (via the copy-ing mechanism), which are fused together possi-bly with the words from the generate-mode.Dataset: We evaluate our model on the recentlypublished LCSTS dataset (Hu et al, 2015), a largescale dataset for short text summarization.
Thedataset is collected from the news medias on SinaWeibo1including pairs of (short news, summary)in Chinese.
Shown in Table 2, PART II and III aremanually rated for their quality from 1 to 5.
Fol-lowing the setting of (Hu et al, 2015) we use PartI as the training set and and the subset of Part IIIscored from 3 to 5 as the testing set.Dataset PART I PART II PART IIIno.
of pairs 2,400,591 10,666 1106no.
of score ?
3 - 8685 725Table 2: Some statistics of the LCSTS dataset.Experimental Setting: We try COPYNET that isbased on character (+C) and word (+W).
For theword-based variant the word-segmentation is ob-tained with jieba2.
We set the vocabulary size to3,000 (+C) and 10,000 (+W) respectively, whichare much smaller than those for models in (Huet al, 2015).
For both variants we set the em-bedding dimension to 350 and the size of hiddenlayers to 500.
Following (Hu et al, 2015), weevaluate the test performance with the commonlyused ROUGE-1, ROUGE-2 and ROUGE-L (Lin,2004), and compare it against the two models in(Hu et al, 2015), which are essentially canonicalEncoder-Decoder and its variant with attention.Models ROUGE scores on LCSTS (%)R-1 R-2 R-LRNN +C 21.5 8.9 18.6(Hu et al, 2015) +W 17.7 8.5 15.8RNN context +C 29.9 17.4 27.2(Hu et al, 2015) +W 26.8 16.1 24.1COPYNET+C 34.4 21.6 31.3+W 35.0 22.3 32.0Table 3: Testing performance of LCSTS, where?RNN?
is canonical Enc-Dec, and ?RNN context?its attentive variant.It is clear from Table 3 that COPYNET beatsthe competitor models with big margin.
Huet al (2015) reports that the performance of aword-based model is inferior to a character-based1www.sina.com2https://pypi.python.org/pypi/jieba1636Input(1):  ????
9 ???????????????????????????????????????????????????????????
92 ????????????
4 ??????????????????????????????????
?Today 9:30, the Fudan poisoning case will be will on public trial at the Shanghai Second Intermediate Court.
The relatives of the murdered student Huang Yang has arrived at Shanghai from Sichuan.
His father said that they will startthe lawsuit for civil compensation after the criminal section.
HuangYang 92-year-old grandmother is still unaware of his death.
In April, a graduate student at Fudan University Shanghai Medical College, Huang Yang is allegedlypoisoned and killed by his roommate Lin Senhao.
Reported by Xinmin______________________________________________________________Golden: ??????????
92 ??????
?the case of Lin Senhao poisoning is on trial today, his 92-year-old grandmother is still unaware of thisRNN context:  ???????????????????????????????4????????
?CopyNet:  ????????????
?the Fudan poisoning case is on public trial today in ShanghaiInput(2):  ?????
300027?????????????????????
3.978 ???????????????????????????
51 % ?????????????????????????????????????????????????
?Huayi Brothers (300027) announced that the company intends to buy with its own fund 397.8 million 51% of Zhejiang Yongle Film LTD's stake owned by a number of shareholders of Yongle Film LTD. For this acquisition, thesecretary of the board, Hu Ming, said yesterday: "the merging with Yongle Film is to strengthen Huayi Brothers on TV business".______________________________________________________________Golden: ???????????????
?Huayi Brothers intends to acquire 51% stake of Zhejiang Yongle FilmRNN context:  ??????????51%?????????????????????UNK???UNK????
?CopyNet:  ?????
3.978 ????????????????
?Huayi Brothers is intended to 397.8 million acquisition of Yongle Film secretaries called to strengthen the TV businessInput(3):  ?????????
20 ???????????????????????????????????
7 ?
4 ???????????????????????????????????????????????????????
?The door of factory is locked.
About 20 workers are scattered to sit under the shade.
?We are ordinary workers, waiting for our salary?
one of them said.
In the morning of July 4th, reporters arrived at Yuanjing PhotoelectronCorporation located at Qinghu Road, Longhua District, Shenzhen.
Just as the rumor, Yuanjing Photoelectron Corporation is closed down and the big shareholder Xing Yi is missing.______________________________________________________________Golden: ?????
LED ????????????
?Hundred-million CNY worth LED enterprise is closed down and workers wait for the boss under the scorching sunRNN context:  ???<UNK>???
?<UNK><UNK>?<UNK>?<UNK>?<UNK>CopyNet: ???????
20 ????????
?Yuanjing Photoelectron Corporation is closed down, 20 workers are scattered to sit under the shadeInput(4):  ??
2012 ?
10 ????????????????????
492191?????????????????????????
2011 ?
9 ????????????????????
6 ?????????????????????????????
75.8 % ?
?At the end of October 2012, the national total of reported HIV infected people and AIDS patients is 492,191 cases.
The Health Ministry saids exual transmission has become the main route of transmission of AIDS.
To September2011, the six provinces with the most reported HIV infected people and AIDS patients were Yunnan, Guangxi, Henan,Sichuan, Xinjiang and Guangdong, accounting for 75.8% of the country.______________________________________________________________Golden: ????????????????
?Ministry of Health: Sexually transmission became the main route of transmission of AIDSRNN context:  ??????????????<UNK>?????????<UNK>%????????????
?CopyNet:  ??????????????????
?Ministry of Health: Sexually transmission has become the main route of transmission of AIDSInput(5):  ???????????????????????????????????????????
12 ????????????????????????????????????????
?Chinese antitrust investigation continues to sweep the automotive industry.
After Germany Audi car and the US Chrysler "fell", there are 12 Japanese car companies involved in the whirlpool.
Reporters learned from the insidersthat Toyota's Lexus has been asked to report to the Development and Reform Commission recently.______________________________________________________________Golden: ???????????????????????
?the investigation by Development and Reform Commission: Toyota's Lexus has been asked to reportRNN context:  ????????????????????????????????????
?CopyNet:  ?????????????
12 ????????????
?Chinese antitrust investigation continues to sweep the automotive industry.
12 Japanese car companies are asked to report to he Development and Reform CommissionInput(6):  ???????????????????????????????????????????????????????????????????????????????
?The energy density of Magnesium ion batteries almost doubles that of lithium battery, which means that for the electric vehicles using of magnesium batteries will last longer even at pure electric power.
But currently due to thetechnical barriers to the electrolyte, it is still too early for the mass production of it and replacing lithium batteries..______________________________________________________________Golden: ????????
??????????????
?Lithium batteries will be phased out, magnesium battery with energy density higher will be the future trendRNN context:  <UNK>?<UNK>?<UNK>?<UNK>?<UNK>?<UNK>?<UNK>?<UNK>?<UNK>?<UNK>?<UNK>?<UNK>??
?CopyNet:  ?????????????????
?Magnesium ion battery is developed : mass production of it will replace lithium batteriesInput(7):  1 .
?????????
2 .
?????
3 .
????
4 .
?????
5 .
??
+ ???
6 .
????????
7 .
??
+ ??
+ ???
8 .
??????
bug???????
9 .???????????1.
master the skills; 2 Learn to finance ; 3. understand the law; 4.
Be confident; 5. test+ trial; 6. understand the need of customers; 7 forecast + measure + ensure; 8. mentally prepared to fight all kinds of small bugs ; 9 discoveropportunities and keep the passion of start-up.______________________________________________________________Golden: ????????
10 ??
?The 10 tips for the first time start-upsRNN context:  6????????6?<UNK>?<UNK>???????????????6?
?CopyNet:  ?????
9 ??
?The 9 tips for success in start-upInput(8):  9 ?
3 ????????????????????
2014 - 2015 ???????????????????????????????????????????????????????
28 ????????????
?On September 3, the Geneva based World Economic Forum released ?
The Global Competitiveness Report 2014-2015?.
Switzerland topped the list for six consecutive years , becoming the world?s most competitive country.
Singaporeand the United States are in the second and third place respectively.
China is in the 28th place, ranking highest among the BRIC countries.______________________________________________________________Golden: ???????????
28 ???????
?The Global competitiveness ranking list, China is in the 28th place, the highest among BRIC countries.RNN context:  2014-2015?????????????6???????28?(?/3??????)????
?28?CopyNet:  2014 - 2015 ????????????????
282014--2015 Global Competitiveness Report: Switzerland topped and China the 28thFigure 4: Examples of COPYNET on LCSTS compared with RNN context.
Word segmentation isapplied on the input, where OOV words are underlined.
The highlighted words (with different colors)are those words with copy-mode probability higher than the generate-mode.
We also provide literalEnglish translation for the document, the golden, and COPYNET, while omitting that for RNN contextsince the language is broken.1637one.
One possible explanation is that a word-based model, even with a much larger vocabulary(50,000 words in Hu et al (2015)), still has a largeproportion of OOVs due to the large number of en-tity names in the summary data and the mistakesin word segmentation.
COPYNET, with its abilityto handle the OOV words with the copying mech-anism, performs however slightly better with theword-based variant.5.2.1 Case StudyAs shown in Figure 4, we make the followinginteresting observations about the summary fromCOPYNET: 1) most words are from copy-mode,but the summary is usually still fluent; 2) COPY-NET tends to cover consecutive words in the orig-inal document, but it often puts together seg-ments far away from each other, indicating a so-phisticated coordination of content-based address-ing and location-based addressing; 3) COPYNEThandles OOV words really well: it can gener-ate acceptable summary for document with manyOOVs, and even the summary itself often con-tains many OOV words.
In contrast, the canonicalRNN-based approaches often fail in such cases.It is quite intriguing that COPYNET can oftenfind important parts of the document, a behav-ior with the characteristics of extractive summa-rization, while it often generate words to ?con-nect?
those words, showing its aspect of abstrac-tive summarization.5.3 Single-turn DialogueIn this experiment we follow the work on neuraldialogue model proposed in (Shang et al, 2015;Vinyals and Le, 2015; Sordoni et al, 2015), andtest COPYNET on single-turn dialogue.
Basically,the neural model learns to generate a response touser?s input, from the given (input, response) pairsas training instances.Dataset: We build a simple dialogue datasetbased on the following three instructions:1.
Dialogue instances are collected from BaiduTieba3with some coverage of conversationsof real life e.g., greeting and sports, etc.2.
Patterns with slots likehi, my name is x?
hi, xare mined from the set, with possibly multi-ple responding patterns to one input.3http://tieba.baidu.com3.
Similar with the synthetic dataset, we enlargethe dataset by filling the slots with suitablesubsequence (e.g.
name entities, dates, etc.
)To make the dataset close to the real conversations,we also maintain a certain proportion of instanceswith the response that 1) do not contain entities or2) contain entities not in the input.Experimental Setting: We create two datasets:DS-I and DS-II with slot filling on 173 collectedpatterns.
The main difference between the twodatasets is that the filled substrings for training andtesting in DS-II have no overlaps, while in DS-Ithey are sampled from the same pool.
For eachdataset we use 6,500 instances for training and1,500 for testing.
We compare COPYNET withcanonical RNNSearch, both character-based, withthe same model configuration in Section 5.1.DS-I (%) DS-II (%)Models Top1 Top10 Top1 Top10RNNSearch 44.1 57.7 13.5 15.9COPYNET 61.2 71.0 50.5 64.8Table 4: The decoding accuracy on the two testingsets.
Decoding is admitted success only when theanswer is found exactly in the Top-K outputs.We compare COPYNET and RNNSearch onDS-I and DS-II in terms of top-1 and top-10 ac-curacy (shown in Table 4), estimating respectivelythe chance of the top-1 or one of top-10 (frombeam search) matching the golden.
Since thereare often many good responses to an input, top-10 accuracy appears to be closer to the real worldsetting.As shown in Table 4, COPYNET significantlyoutperforms RNNsearch, especially on DS-II.
Itsuggests that introducing the copying mechanismhelps the dialogue system master the patterns indialogue and correctly identify the correct parts ofinput, often proper nouns, to replicate in the re-sponse.
Since the filled substrings have no over-laps in DS-II, the performance of RNNSearchdrops significantly as it cannot handle words un-seen in training data.
In contrast, the performanceof COPYNET only drops slightly as it has learnedto fill the slots with the copying mechanism andrelies less on the representation of the words.5.3.1 Case StudyAs indicated by the examples in Figure 5, COPY-NET accurately replicates the critical segmentsfrom the input with the copy-mode, and generates1638Input(1):  ??????
?/I was born in ShanghaiGolden:   ?????????????!
/What a coincidence, I also come from Shanghai, we are fellow townsman!RNNSearch:   ?????????????!
/What a coincidence, I also come from Hainan, we are fellow townsman!CopyNet:   ?????????????!
/What a coincidence, I also come from Shanghai, we are fellow townsman!Input(2):  ???????
?/Shall we climbing Huangshan?Golden:   ???????????!
/Climb Taishan.
Huangshan is boring!RNNSearch:   ????????????!
/Climb Changbaishan, Hengshan is boring!CopyNet:   ???????????!
/Climb Songshan, Huangshan is boring!Input(3):  ????????????
?/Would you like to drink Sprite or Coke?Golden:    ???????!
/Not at all, thanks!RNNSearch:    ??????!
/I like to drink honey!CopyNet:    ??????!
/I like to drink coke!Input(4):  ???????
?/Our great Qingdao has a good weather!Golden:   ????????
/Our Qingdao is great~~~RNNSearch:   ?????????
/Our Harbin is great~~~CopyNet:   ????????
/Our Qingdao is great~~~Input(5):  ?????
?/Daughter can swim butterfly?Golden:   ??????
?T^T                      /I cannot swim at all T^TRNNSearch:   ??????
?T^T                 /I cannot swim at all T^TCopyNet:   ??????
?T^T                  /I cannot swim at all T^TInput(6):  ?????
?/Who is Su You Peng?Golden:   ??????????
/Su You Peng is a male star.CopyNet:   ?????????
/Su You is a male starRNNSearch: ??????????
/Wu Yifan is a male star.Figure 5: Examples from the testing set of DS-II shown as the input text and golden, with the outputsof RNNSearch and CopyNet.
Words in red rectangles are unseen in the training set.
The highlightedwords (with different colors) are those words with copy-mode probability higher than the generate-mode.Green cirles (meaning correct) and red cross (meaning incorrect) are given based on human judgment onwhether the response is appropriate.the rest of the answers smoothly by the generate-mode.
Note that in (2) and (3), the decoding se-quence is not exactly the same with the standardone, yet still correct regarding to their meanings.In contrast, although RNNSearch usually gener-ates answers in the right formats, it fails to catchthe critical entities in all three cases because of thedifficulty brought by the unseen words.6 Related WorkOur work is partially inspired by the recent workof Pointer Networks (Vinyals et al, 2015a), inwhich a pointer mechanism (quite similar with theproposed copying mechanism) is used to predictthe output sequence directly from the input.
In ad-dition to the difference with ours in application,(Vinyals et al, 2015a) cannot predict outside ofthe set of input sequence, while COPYNET cannaturally combine generating and copying.COPYNET is also related to the effort to solvethe OOV problem in neural machine translation.Luong et al (2015) introduced a heuristics to post-process the translated sentence using annotationson the source sentence.
In contrast COPYNET ad-dresses the OOV problem in a more systemic waywith an end-to-end model.
However, as COPY-NET copies the exact source words as the output, itcannot be directly applied to machine translation.However, such copying mechanism can be natu-rally extended to any types of references exceptfor the input sequence, which will help in appli-cations with heterogeneous source and target se-quences such as machine translation.The copying mechanism can also be viewed ascarrying information over to the next stage withoutany nonlinear transformation.
Similar ideas areproposed for training very deep neural networks in(Srivastava et al, 2015; He et al, 2015) for clas-sification tasks, where shortcuts are built betweenlayers for the direct carrying of information.Recently, we noticed some parallel efforts to-wards modeling mechanisms similar to or relatedto copying.
Cheng and Lapata (2016) devised aneural summarization model with the ability to ex-tract words/sentences from the source.
Gulcehreet al (2016) proposed a pointing method to han-dle the OOV words for summarization and MT.
Incontrast, COPYNET is more general, and not lim-ited to a specific task or OOV words.
Moreover,the softmaxCOPYNET is more flexible than gatingin the related work in handling the mixture of twomodes, due to its ability to adequately model thecontent of copied segment.7 Conclusion and Future WorkWe proposed COPYNET to incorporate copy-ing into the sequence-to-sequence learning frame-work.
For future work, we will extend this idea tothe task where the source and target are in hetero-geneous types, for example, machine translation.AcknowledgmentsThis work is supported in part by the China Na-tional 973 Project 2014CB340301.1639ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Jianpeng Cheng and Mirella Lapata.
2016.
Neuralsummarization by extracting sentences and words.arXiv preprint arXiv:1603.07252.Kyunghyun Cho, Bart Van Merri?enboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder-decoderfor statistical machine translation.
arXiv preprintarXiv:1406.1078.Alex Graves, Greg Wayne, and Ivo Danihelka.2014.
Neural turing machines.
arXiv preprintarXiv:1410.5401.Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-ati, Bowen Zhou, and Yoshua Bengio.
2016.Pointing the unknown words.
arXiv preprintarXiv:1603.08148.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun.
2015.
Deep residual learning for image recog-nition.
arXiv preprint arXiv:1512.03385.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Baotian Hu, Qingcai Chen, and Fangze Zhu.
2015.
Lc-sts: a large scale chinese short text summarizationdataset.
arXiv preprint arXiv:1506.05865.Karol Kurach, Marcin Andrychowicz, and IlyaSutskever.
2015.
Neural random-access machines.arXiv preprint arXiv:1511.06392.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Stan SzpakowiczMarie-Francine Moens, editor, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,and Wojciech Zaremba.
2015.
Addressing the rareword problem in neural machine translation.
In Pro-ceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th In-ternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 11?19,Beijing, China, July.
Association for ComputationalLinguistics.Geoffrey J McLachlan and Kaye E Basford.
1988.Mixture models.
inference and applications to clus-tering.
Statistics: Textbooks and Monographs, NewYork: Dekker, 1988, 1.Alexander M Rush, Sumit Chopra, and Jason We-ston.
2015.
A neural attention model for ab-stractive sentence summarization.
arXiv preprintarXiv:1509.00685.Lifeng Shang, Zhengdong Lu, and Hang Li.
2015.Neural responding machine for short-text conversa-tion.
arXiv preprint arXiv:1503.02364.Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, HuaWu, Maosong Sun, and Yang Liu.
2015.
Minimumrisk training for neural machine translation.
CoRR,abs/1512.02433.Alessandro Sordoni, Michel Galley, Michael Auli,Chris Brockett, Yangfeng Ji, Margaret Mitchell,Jian-Yun Nie, Jianfeng Gao, and Bill Dolan.
2015.A neural network approach to context-sensitive gen-eration of conversational responses.
arXiv preprintarXiv:1506.06714.Rupesh Kumar Srivastava, Klaus Greff, and J?urgenSchmidhuber.
2015.
Highway networks.
arXivpreprint arXiv:1505.00387.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in neural information process-ing systems, pages 3104?3112.Oriol Vinyals and Quoc Le.
2015.
A neural conversa-tional model.
arXiv preprint arXiv:1506.05869.Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.2015a.
Pointer networks.
In Advances in NeuralInformation Processing Systems, pages 2674?2682.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015b.
Gram-mar as a foreign language.
In Advances in NeuralInformation Processing Systems, pages 2755?2763.1640
