Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 115?120,Sydney, July 2006. c?2006 Association for Computational LinguisticsSemTAG, the LORIA toolbox for TAG-based Parsing and GenerationEric KowINRIA / LORIAUniversite?
Henri Poincare?615, rue du Jardin BotaniqueF-54 600 Villers-Le`s-Nancykow@loria.frYannick ParmentierINRIA / LORIAUniversite?
Henri Poincare?615, rue du Jardin BotaniqueF-54 600 Villers-Le`s-Nancyparmenti@loria.frClaire GardentCNRS / LORIA615, rue du Jardin BotaniqueF-54 600 Villers-Le`s-Nancygardent@loria.frAbstractIn this paper, we introduce SEMTAG, atoolbox for TAG-based parsing and gen-eration.
This environment supports thedevelopment of wide-coverage grammarsand differs from existing environmentsfor TAG such as XTAG, (XTAG-Research-Group, 2001) in that it includes a semanticdimension.
SEMTAG is open-source andfreely available.1 IntroductionIn this paper we introduce a toolbox that allows forboth parsing and generation with TAG.
This tool-box combines existing software and aims at facili-tating grammar development, More precisely, thistoolbox includes1:?
XMG: a grammar compiler which supports thegeneration of a TAG from a factorised TAG(Crabbe?
and Duchier, 2004),?
LLP2 and DyALog: two chart parsers, onewith a friendly user interface (Lopez, 2000)and the other optimised for efficient parsing(Villemonte de la Clergerie, 2005)2?
GenI: a chart generator which has beentested on a middle size grammar for French(Gardent and Kow, 2005)1All these tools are freely available, more information andlinks at http://trac.loria.fr/?semtag.2Note that DyALog refers in fact to a logic program-ming language, and a tabular compiler for this language.
TheDyALog system is well-adapted to the compilation of effi-cient tabular parsers.2 XMG, a grammar writing environmentfor Tree Based GrammarsXMG provides a grammar writing environment fortree based grammars3 with three distinctive fea-tures.
First, XMG supports a highly factorised andfully declarative description of tree based gram-mars.
Second, XMG permits the integration in aTAG of a semantic dimension.
Third, XMG is basedon well understood and efficient logic program-ming techniques.
Moreover, it offers a graphicalinterface for exploring the resulting grammar (seeFigure 1).Factorising information.
In the XMG frame-work, a TAG is defined by a set of classes organisedin an inheritance hierarchy where classes definetree fragments (using a tree logic) and tree frag-ment combinations (by conjunction or disjunc-tion).
XMG furthermore integrates a sophisticatedtreatment of names whereby variables scope canbe local, global or user defined (i.e., local to partof the hierarchy).In practice, the resulting framework supports avery high degree of factorisation.
For instance, afirst core grammar (FRAG) for French comprising4 200 trees was produced from roughly 300 XMGclasses.Integrating semantic information.
In XMG,classes can be multi-dimensional.
That is, theycan be used to describe several levels of linguis-tic knowledge such as for instance, syntax, seman-tics or prosody.
At present, XMG supports classesincluding both a syntactic and a semantic dimen-sion.
As mentioned above, the syntactic dimen-3Although in this paper we only mention TAG, the XMGframework is also used to develop so called Interaction Gram-mars i.e., grammars whose basic units are tree descriptionsrather than trees (Parmentier and Le Roux, 2005).115Figure 1: XMG?s graphical interfacesion is based on a tree logic and can be used todescribe (partial) tree fragments.
The semantic di-mension on the other hand, can be used to asso-ciate with each tree a flat semantic formula.
Such aformula can furthermore include identifiers whichcorefer with identifiers occurring in the associatedsyntactic tree.
In other words, XMG also providessupport for the interface between semantic formu-lae and tree decorations.
Note that the inclusion ofsemantic information remains optional.
That is, itis possible to use XMG to define a purely syntacticTAG.XMG was used to develop a core grammar forFrench (FRAG) which was evaluated to have 75%coverage4 on the Test Suite for Natural LanguageProcessing (TSNLP, (Lehmann et al, 1996)).
TheFRAG grammar was furthermore enriched withsemantic information using another 50 classes de-scribing the semantic dimension (Gardent, 2006).The resulting grammar (SEMFRAG) describesboth the syntax and the semantics of the Frenchcore constructions.Compiling an XMG specification.
By build-ing on efficient techniques from logic program-ming and in particular, on the Warren?s Abstract4This means that for 75 % of the sentences, a TAG parsercan build at least one derivation.Figure 2: The LLP2 parser.Machine idea (Ait-Kaci, 1991), the XMG com-piler allows for very reasonable compilation times(Duchier et al, 2004).
For instance, the compila-tion of a TAG containing 6 000 trees takes about 15minutes with a Pentium 4 processor 2.6 GHz and1 GB of RAM.3 Two TAG parsersThe toolbox includes two parsing systems: theLLP2 parser and the DyALog system.
Both ofthem can be used in conjunction with XMG.
Firstwe will briefly introduce both of them, and thenshow that they can be used with a semantic gram-mar (e.g., SEMFRAG) to perform not only syntac-tic parsing but also semantic construction.LLP2 The LLP2 parser is based on a bottom-up algorithm described in (Lopez, 1999).
It hasrelatively high parsing times but provides a userfriendly graphical parsing environment with muchstatistical information (see Figure 2).
It is wellsuited for teaching or for small scale projects.DyALog The DyALog system on the otherhand, is a highly optimised parsing system basedon tabulation and automata techniques (Ville-monte de la Clergerie, 2005).
It is implementedusing the DyALog programming language (i.e.,it is bootstrapped) and is also used to compileparsers for other types of grammars such as TreeInsertion Grammars.The DyALog system is coupled with a seman-tic construction module whose aim is to associatewith each parsed string a semantic representation5.This module assumes a TAG of the type describedin (Gardent and Kallmeyer, 2003; Gardent, 2006)5The corresponding system is called SemConst (cf section6).116Figure 3: The SemConst systemwhere initial trees are associated with semantic in-formation and unification is used to combine se-mantic representations.
In such a grammar, the se-mantic representation of a derived tree is the unionof the semantic representations of the trees enter-ing in the derivation of that derived tree modulothe unifications entailed by analysis.
As detailedin (Gardent and Parmentier, 2005), such grammarssupport two strategies for semantic construction.The first possible strategy is to use the fullgrammar and to perform semantic constructionduring derivation.
In this case the parser must ma-nipulate both syntactic trees and semantic repre-sentations.
The advantage is that the approach issimple (the semantic representations can simplybe an added feature on the anchor node of eachtree).
The drawback is that the presence of seman-tic information might reduce chart sharing.The second possibility involves extracting thesemantic information contained in the grammarand storing it into a semantic lexicon.
Parsing thenproceeds with a purely syntactic grammar and se-mantic construction is done after parsing on thebasis of the parser output and of the extracted se-mantic lexicon.
This latter technique is more suit-able for large scale semantic construction as it sup-ports better sharing in the derivation forests.
Itis implemented in the LORIA toolbox where amodule permits both extracting a semantic lexi-con from a semantic TAG and constructing a se-mantic representation based on this lexicon and onthe derivation forests output by DyALog (see Fig-ure 3).The integration of the DyALog system into thetoolbox is relatively new so that parsing evaluationFigure 4: The GenI debuggeris still under progress.
So far, evaluation has beenrestricted to parsing the TSNLP with DyALogwith the following preliminary results.
On sen-tences ranging from 1 to 18 words, with an aver-age of 7 words per sentence, and with a grammarcontaining 5 069 trees, DyALog average parsingtime is of 0.38 sec with a P4 processor 2.6 GHzand 1 GB of RAM6.4 A TAG-based surface realiserThe surface realiser GenI takes a TAG and a flatsemantic logical form as input, and produces allthe sentences that are associated with that logi-cal form by the grammar.
It implements two bot-tom up algorithms, one which manipulates derivedtrees as items and one which is based on Earley forTAG.
Both of these algorithms integrate a numberof optimisations such as delayed adjunction andpolarity filtering (Kow, 2005; Gardent and Kow,2005).GenI is written in Haskell and includes agraphical debugger to inspect the state of the gen-erator at any point in the surface realisation pro-cess (see Figure 4).
It also integrates a test harnessfor automated regression testing and benchmark-ing of the surface realiser and the grammar.
Theharness gtester is written in Python.
It runs thesurface realiser on a test suite, outputting a singledocument with a table of passes and failures andvarious performance charts (see Figures 5 and 6).Test suite and performance The test suite isbuilt with an emphasis on testing the surface re-6These features only concern classic syntactic parsing asthe semantic construction module has not been tested on realgrammars yet.117test expected simple earleyt1 il le accepter pass passt32 il nous accepter pass passt83 le ingnieur le lui apprendre pass DIEDt114 le ingnieur nous le prsenter pass passt145 le ingnieur vous le apprendre pass passt180 vous venir pass passFigure 5: Fragment of test harness output - TheEarley algorithm timed out.01000200030004000500060000  20  40  60  80  100  120  140  160  180  200chart_sizelex_foot_nodeschart_size for lex_foot_nodessimpleearleyFigure 6: Automatically generated graph of per-formance data by the test harness.aliser?s performance in the face of increasing para-phrastic power i.e., ambiguity.
The suite consistsof semantic inputs that select for and combinesverbs with different valencies.
For example, givena hypothetical English grammar, a valency (2,1)semantics might be realised in as Martin thinksFaye drinks (thinks takes 2 arguments and drinkstakes 1), whereas a valency (2,3,2) one would beDora says that Martin tells Bob that Faye likesmusic.
The suite also adds a varying number ofintersective modifiers into the mix, giving us forinstance, The girl likes music, The pretty scary girllikes indie music.The sentences in the suite range from 2 to 15words (8 average).
Realisation times for the coresuite range from 0.7 to 2.84 seconds CPU time(average 1.6 seconds).We estimate the ambiguity for each test casein two ways.
The first is to count the number ofparaphrases.
Given our current grammar, the testcases in our suite have up to 669 paraphrases (av-erage 41).
The second estimate for ambiguity isthe number of combinations of lexical items cov-ering the input semantics.This second measure is based on optimisationknown as polarity filtering (Gardent and Kow,2005).
This optimisation detects and eliminatescombinations of lexical items that cannot be usedto build a result.
It associates the syntactic re-sources (root nodes) and requirements (substitu-tion nodes) of the lexical items to polarities, whichare then used to build ?polarity automata?.
Theautomata are minimised to eliminate lexical com-binations where the polarities do not cancel out,that is those for which the number of root and sub-stitution nodes for any given category do not equaleach other.Once built, the polarity automata can also serveto estimate ambiguity.
The number of paths in theautomaton represent the number of possible com-binations of lexical items.
To determine how ef-fective polarity filtering with respect to ambiguity,we compare the combinations before and after po-larity filtering.
Before filtering, we start with aninitial polarity automaton in which all items areassociated with a zero polarity.
This gives us thelexical ambiguity before filtering.
The polarity fil-ter then builds upon this to form a final automatonwhere all polarities are taken into account.
Count-ing the paths on this automaton gives us the am-biguity after filtering, and comparing this numberwith the lexical initial ambiguity provides an es-timate on the usefulness of the polarity filter.
Inour suite, the initial automata for each case have1 to 800 000 paths (76 000 average).
The fi-nal automata have 1 to 6000 paths (192 average).This can represent quite a large reduction in searchspace, 4000 times in the case of the largest au-tomaton.
The effect of this search space reduc-tion is most pronounced on the larger sentences orthose with the most modifiers.
Indeed, realisationtimes with and without filtering are comparable formost of the test suite, but for the most complicatedsentence in the core suite, polarity filtering makessurface realisation 94% faster, producing a resultin 2.35 seconds instead of 37.38.5 Benefits of an integrated toolsetAs described above, the LORIA toolbox for TAGbased semantic processing includes a lexicon, agrammar, a parser, a semantic construction mod-ule and a surface realiser.
Integrating these intoa single platform provides some accrued benefitswhich we now discuss in more details.Simplified resource management The first ad-vantage of an integrated toolkit is that it facilitates118the management of the linguistic resources usednamely the grammar and the lexicon.
Indeed it iscommon that each NLP tool (parser or generator)has its own representation format.
Thus, manag-ing the resources gets tiresome as one has to dealwith several versions of a single resource.
Whenone version is updated, the others have to be re-computed.
Using an integrated toolset avoid sucha drawback as the intermediate formats are hiddenand the user can focus on linguistic description.Better support for grammar developmentWhen developing parsers or surface realisers, it isuseful to test them out by running them on large,realistic grammars.
Such grammars can explorenooks and crannies in our implementations thatwould otherwise have been overlooked by a toygrammar.
For example, it was only when we ranGenI on our French grammar that we realised ourimplementation did not account for auxiliary treeswith substitution nodes (this has been rectified).In this respect, one could argue that XMG could al-most be seen as a parser/realiser debugging utilitybecause it helps us to build and extend the largegrammars that are crucial for testing.This perspective can also be inverted; parsersand surface realiser make for excellent grammar-debugging devices.
For example, one possibleregression test is to run the parser on a suite ofknown sentences to make sure that the modifiedgrammar still parses them correctly.
The exactreverse is useful as well; we could also run thesurface realiser over a suite of known semanticinputs and make sure that sentences are gener-ated for each one.
This is useful for two reasons.First, reading surface realiser output (sentences)is arguably easier for human beings than readingparser output (semantic formulas).
Second, thesurface realiser can tell us if the grammar overgen-erates because it would output nonsense sentences.Parsers, on the other hand, are much better adaptedfor testing for undergeneration because it is easierto write sentences than semantic formulas, whichmakes it easier to test phenomena which might notalready be in the suite.Towards a reversible grammar Another ad-vantage of using such a toolset relies on the factthat we can manage a common resource for bothparsing and generation, and thus avoid inconsis-tency, redundancy and offer a better flexibility asadvocated in (Neumann, 1994).On top of these practical questions, having aunique reversible resource can lead us further.For instance, (Neumann, 1994) proposes an inter-leaved parsing/realisation architecture where theparser is used to choose among a set of para-phrases proposed by the generator; paraphraseswhich are ambiguous (that have multiple parses)are discarded in favour of those whose meaning ismost explicit.
Concretely, we could do this with asimple pipeline using GenI to produce the para-phrases, DyALog to parse them, and a small shellscript to pick the best result.
This would only bea simulation, of course.
(Neumann, 1994) goesas far as to interleave the processes, keeping theshared chart and using the parser to iterativelyprune the search space as it is being explored bythe generator.
The version we propose would nothave such niceties as a shared chart, but the pointis that having all the tools at our disposable makessuch experimentation possible in the first place.Moreover, there are several other interest-ing applications of the combined toolbox.
Wecould use the surface realiser to build artifi-cial corpora.
These can in turn be parsed tosemi-automatically create rich treebanks contain-ing syntactico-semantic analyses a` la Redwoods(Oepen et al, 2002).Eventually, another use for the toolbox might bein components of standard NLP applications suchas machine translation, questioning answering, orinteractive dialogue systems.6 AvailabilityThe toolbox presented here is open-source andfreely available under the terms of the GPL7.
Moreinformation about the requirements and installa-tion procedure is available at http://trac.loria.fr/?semtag.
Note that this toolbox ismade of two main components: the GenI8 sys-tem and the SemConst9 system, which respec-tively performs generation and parsing from com-mon linguistic resources.
The first is written inHaskell (except the XMG part written in Oz) and ismulti-platform (Linux, Windows, Mac OS).
Thelatter is written in Oz (except the DyALog partwhich is bootstrapped and contains some Intel as-sembler code) and is available on Unix-like plat-7Note that XMG is released under the terms of theCeCILL license (http://www.cecill.info/index.en.html), which is compatible with the GPL.8http://trac.loria.fr/?geni9http://trac.loria.fr/?semconst119forms only.7 ConclusionThe LORIA toolbox provides an integrated envi-ronment for TAG based semantic processing: ei-ther to construct the semantic representation of agiven sentence (parsing) or to generate a sentenceverbalising a given semantic content (generation).Importantly, both the generator and the parsersuse the same grammar (SEMFRAG) so that bothtools can be used jointly to improve grammar pre-cision.
All the sentences outputted by the surfacerealiser should be parsed to have at least the se-mantic representation given by the test suite, andall parses of a sentence should be realised into atleast the same sentence.Current and future work concentrates on de-veloping an automated error mining environmentfor both parsing and generation; on extending thegrammar coverage; on integrating further optimi-sations both in the parser (through parsing withfactorised trees) and in the generator (throughpacking and accessibility filtering cf.
(Carroll andOepen, 2005); and on experimenting with differ-ent semantic construction strategies (Gardent andParmentier, 2005).ReferencesH.
Ait-Kaci.
1991.
Warren?s Abstract Machine: A Tu-torial Reconstruction.
In K. Furukawa, editor, Proc.of the Eighth International Conference of Logic Pro-gramming.
MIT Press, Cambridge, MA.J.
Carroll and S. Oepen.
2005.
High efficiency re-alization for a wide-coverage unification grammar.In R. Dale and K-F. Wong, editors, Proceedings ofthe Second International Joint Conference on Natu-ral Language Processing, volume 3651 of SpringerLecture Notes in Artificial Intelligence, pages 165?176.B.
Crabbe?
and D. Duchier.
2004.
Metagrammar Re-dux.
In Proceedings of CSLP 2004, Copenhagen.D.
Duchier, J.
Le Roux, and Y. Parmentier.
2004.
TheMetagrammar Compiler: An NLP Application witha Multi-paradigm Architecture.
In 2nd InternationalMozart/Oz Conference (MOZ?2004), Charleroi.C.
Gardent and L. Kallmeyer.
2003.
Semantic con-struction in FTAG.
In Proceedings of EACL?03, Bu-dapest.C.
Gardent and E. Kow.
2005.
Generating and select-ing grammatical paraphrases.
ENLG, Aberdeen.C.
Gardent and Y. Parmentier.
2005.
Large scalesemantic construction for tree adjoining grammars.In Proceedings of The Fifth International Confer-ence on Logical Aspects of Computational Linguis-tics (LACL05).C.
Gardent.
2006.
Inte?gration d?une dimensionse?mantique dans les grammaires d?arbres adjoints.In Actes de la confe?rence TALN?2006 Leuven.E.
Kow.
2005.
Adapting polarised disambiguationto surface realisation.
In 17th European SummerSchool in Logic, Language and Information - ESS-LLI?05, Edinburgh, UK, Aug.S.
Lehmann, S. Oepen, S. Regnier-Prost, K. Netter,V.
Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,E.
Dauphin, H. Compagnion, J. Baur, L. Balkan, andD.
Arnold.
1996.
TSNLP ?
Test Suites for NaturalLanguage Processing.
In Proceedings of COLING1996, Kopenhagen.P.
Lopez.
1999.
Analyse d?e?nonce?s oraux pour le dia-logue homme-machine a` l?aide de grammaires lex-icalise?es d?arbres.
Ph.D. thesis, Universite?
HenriPoincare?
?
Nancy 1.P.
Lopez.
2000.
Extended Partial Parsing forLexicalized Tree Grammars.
In Proceedings ofthe International Workshop on Parsing Technology(IWPT2000), Trento, Italy.G.
Neumann.
1994.
A Uniform ComputationalModel for Natural Language Parsing and Gener-ation.
Ph.D. thesis, University of the Saarland,Saarbru?cken.S.
Oepen, E. Callahan, C. Manning, and K. Toutanova.2002.
Lingo redwoods?a rich and dynamic tree-bank for hpsg.Y.
Parmentier and J.
Le Roux.
2005.
XMG: an Exten-sible Metagrammatical Framework.
In Proceedingsof the Student Session of the 17th European SummerSchool in Logic, Language and Information, Edin-burg, Great Britain, Aug.E.
Villemonte de la Clergerie.
2005.
DyALog: a tabu-lar logic programming based environment for NLP.In Proceedings of CSLP?05, Barcelona.XTAG-Research-Group.
2001.
A lexical-ized tree adjoining grammar for english.Technical Report IRCS-01-03, IRCS, Uni-versity of Pennsylvania.
Available athttp://www.cis.upenn.edu/?xtag/gramrelease.html.120
