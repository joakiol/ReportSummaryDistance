Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301?304,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPValidating the web-based evaluation of NLG systemsAlexander KollerSaarland U.koller@mmci.uni-saarland.deKristina StriegnitzUnion Collegestriegnk@union.eduDonna ByronNortheastern U.dbyron@ccs.neu.eduJustine CassellNorthwestern U.justine@northwestern.eduRobert DaleMacquarie U.Robert.Dale@mq.edu.auSara Dalzel-JobU.
of EdinburghS.Dalzel-Job@sms.ed.ac.ukJon OberlanderU.
of EdinburghJohanna MooreU.
of Edinburgh{J.Oberlander|J.Moore}@ed.ac.ukAbstractThe GIVE Challenge is a recent sharedtask in which NLG systems are evaluatedover the Internet.
In this paper, we validatethis novel NLG evaluation methodology bycomparing the Internet-based results withresults we collected in a lab experiment.We find that the results delivered by bothmethods are consistent, but the Internet-based approach offers the statistical powernecessary for more fine-grained evaluationsand is cheaper to carry out.1 IntroductionRecently, there has been an increased interest inevaluating and comparing natural language gener-ation (NLG) systems on shared tasks (Belz, 2009;Dale and White, 2007; Gatt et al, 2008).
However,this is a notoriously hard problem (Scott and Moore,2007): Task-based evaluations with human experi-mental subjects are time-consuming and expensive,and corpus-based evaluations of NLG systems areproblematic because a mismatch between human-generated output and system-generated output doesnot necessarily mean that the system?s output isinferior (Belz and Gatt, 2008).
This lack of evalua-tion methods which are both effective and efficientis a serious obstacle to progress in NLG research.The GIVE Challenge (Byron et al, 2009) is arecent shared task which takes a third approach toNLG evaluation: By connecting NLG systems toexperimental subjects over the Internet, it achievesa true task-based evaluation at a much lower cost.Indeed, the first GIVE Challenge acquired datafrom over 1100 experimental subjects online.
How-ever, it still remains to be shown that the resultsthat can be obtained in this way are in fact com-parable to more established task-based evaluationefforts, which are based on a carefully selected sub-ject pool and carried out in a controlled laboratoryenvironment.
By accepting connections from arbi-trary subjects over the Internet, the evaluator givesup control over the subjects?
behavior, level of lan-guage proficiency, cooperativeness, etc.
; there isalso an issue of whether demographic factors suchas gender might skew the results.In this paper, we provide the missing link byrepeating the GIVE evaluation in a laboratory en-vironment and comparing the results.
It turns outthat where the two experiments both find a signif-icant difference between two NLG systems withrespect to a given evaluation measure, they alwaysagree.
However, the Internet-based experimentfinds considerably more such differences, perhapsbecause of the higher number of experimental sub-jects (n = 374 vs. n = 91), and offers other oppor-tunities for more fine-grained analysis as well.
Wetake this as an empirical validation of the Internet-based evaluation of GIVE, and propose that it canbe applied to NLG more generally.
Our findingsare in line with studies from psychology that indi-cate that the results of web-based experiments aretypically consistent with the results of traditionalexperiments (Gosling et al, 2004).
Nevertheless,we do find and discuss some effects of the uncon-trolled subject pool that should be addressed infuture Internet-based NLG challenges.2 The GIVE ChallengeIn the GIVE scenario (Byron et al, 2009), userstry to solve a treasure hunt in a virtual 3D worldthat they have not seen before.
The computer hascomplete information about the virtual world.
Thechallenge for the NLG system is to generate, in realtime, natural-language instructions that will guidethe users to the successful completion of their task.From the perspective of the users, GIVE con-sists in playing a 3D game which they start froma website.
The game displays a virtual world andallows the user to move around in the world andmanipulate objects; it also displays the generated301instructions.
The first room in each game is a tuto-rial room in which users learn how to interact withthe system; they then enter one of three evaluationworlds, where instructions for solving the treasurehunt are generated by an NLG system.
Playerscan either finish a game successfully, lose it bytriggering an alarm, or cancel the game at any time.When a user starts the game, they are randomlyconnected to one of the three worlds and one of theNLG systems.
The GIVE-1 Challenge evaluatedfive NLG systems, which we abbreviate as A, M,T, U, and W below.
A running GIVE NLG systemhas access to the current state of the world and toan automatically computed plan that tells it whatactions the user should perform to solve the task.
Itis notified whenever the user performs some action,and can generate an instruction and send it to theclient for display at any time.3 The experimentsThe web experiment.
For the GIVE-1 challenge,1143 valid games were collected over the Internetover the course of three months.
These were dis-tributed over three evaluation worlds (World 1: 374,World 2: 369, World 3: 400).
A game was consid-ered valid if the game client didn?t crash, the gamewasn?t marked as a test run by the developers, andthe player completed the tutorial.Of these games, 80% were played by males and10% by females (the remaining 10% of the partic-ipants did not specify their gender).
The playerswere widely distributed over countries: 37% con-nected from IP addresses in the US, 33% fromGermany, and 17% from China; the rest connectedfrom 45 further countries.
About 34% of the par-ticipants self-reported as native English speakers,and 62% specified a language proficiency level ofat least ?expert?
(3 on a 5-point scale).The lab experiment.
We repeated the GIVE-1evaluation in a traditional laboratory setting with91 participants recruited from a college campus.In the lab, each participant played the GIVE gameonce with each of the five NLG systems.
To avoidlearning effects, we only used the first game runfrom each subject in the comparison with the webexperiment; as a consequence, subjects were dis-tributed evenly over the NLG systems.
To accom-modate for the much lower number of participants,the laboratory experiment only used a single gameworld ?
World 1, which was known from the onlineversion to be the easiest world.Among this group of subjects, 93% self-ratedtheir English proficiency as ?expert?
or better; 81%were native speakers.
In contrast to the online ex-periment, 31% of participants were male and 65%were female (4% did not specify their gender).Results: Objective measures.
The GIVE soft-ware automatically recorded data for five objec-tive measures: the percentage of successfully com-pleted games and, for the successfully completedgames, the number of instructions generated bythe NLG system, of actions performed by the user(such as pushing buttons), of steps taken by theuser (i.e., actions plus movements), and the taskcompletion time (in seconds).Fig.
1 shows the results for the objective mea-sures collected in both experiments.
To make theresults comparable, the table for the Internet ex-periment only includes data for World 1.
The tasksuccess rate is only evaluated on games that werecompleted successfully or lost, not cancelled, aslaboratory subjects were asked not to cancel.
Thisbrings the number of Internet subjects to 322 forthe success rate, and to 227 (only successful games)for the other measures.Task success is the percentage of successfullycompleted games; the other measures are reportedas means.
The chart assigns systems to groups Athrough C or D for each evaluation measure.
Sys-tems in group A are better than systems in groupB, and so on; if two systems have no letter in com-mon, the difference between them is significantwith p < 0.05.
Significance was tested using a ?2-test for task success and ANOVAs for instructions,steps, actions, and seconds.
These were followedby post hoc tests (pairwise ?2and Tukey) to com-pare the NLG systems pairwise.Results: Subjective measures.
Users wereasked to fill in a questionnaire collecting subjec-tive ratings of various aspects of the instructions.For example, users were asked to rate the overallquality of the direction giving system (on a 7-pointscale), the choice of words and the referring ex-pressions (on 5-point scales), and they were askedwhether they thought the instructions came at theright time.
Overall, there were twelve subjectivemeasures (see (Byron et al, 2009)), of which weonly present four typical ones for space reasons.For each question, the user could choose not toanswer.
On the Internet, subjects made consider-able use of this option: for instance, 32% of users302Objective Measures Subjective Measurestasksuccessinstructions steps actions seconds overallchoiceof wordsreferringexpressionstimingA 91% A 83.4 B 99.8 A 9.4 A 123.9 A 4.7 A 4.7 A 4.7 A 81% AM 76% B 68.1 A 145.1 B 10.0 AB 195.4 BC 3.8 AB 3.8 B 4.0 B 70% ABCT 85% AB 97.8 C 142.1 B 9.7 AB 174.4 B 4.4 B 4.4 AB 4.3 AB 73% ABU 93% AB 99.8 C 142.6 B 10.3 B 194.0 BC 4.0 B 4.0 B 4.0 B 51% CW 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BCA 100% A 78.2 AB 93.4 A 9.9 A 143.9 A 5.7 A 4.7 A 4.8 A 92% A BM 95% A 66.3 A 141.8 B 10.5 A 211.8 B 5.4 A 3.8 B 4.3 A 95% A BT 93% A 107.2 CD 134.6 B 9.6 A 205.6 B 4.9 A 4.5 A B 4.4 A 64% A BU 100% A 88.8 BC 128.8 B 9.8 A 195.1 AB 5.7 A 4.7 A 4.3 A 100% AW 17% B 134.5 D 213.5 C 10.0 A 252.5 B 5.0 A 4.5 A B 4.0 A 100% BFigure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).didn?t fill in the ?overall evaluation?
field of thequestionnaire.
In the laboratory experiment, thesubjects were asked to fill in the complete question-naire and the response rate is close to 100%.The results for the four selected subjective mea-sures are summarized in Fig.
1 in the same way asthe objective measures.
Also as above, the tableis based only on successfully completed games inWorld 1.
We will justify this latter choice below.4 DiscussionThe primary question that interests us in a compar-ative evaluation is which NLG systems performedsignificantly better or worse on any given evalua-tion measure.
In the experiments above, we findthat of the 170 possible significant differences (=17 measures ?
10 pairs of NLG systems), the labo-ratory experiment only found six that the Internet-based experiment didn?t find.
Conversely, thereare 26 significant differences that only the Internet-based experiment found.
But even more impor-tantly, all pairwise rankings are consistent acrossthe two evaluations: Where both systems found asignificant difference between two systems, they al-ways ranked them in the same order.
We concludethat the Internet experiment provides significancejudgments that are comparable to, and in fact moreprecise than, the laboratory experiment.Nevertheless, there are important differences be-tween the laboratory and Internet-based results.
Forinstance, the success rates in the laboratory tendto be higher, but so are the completion times.
Webelieve that these differences can be attributed tothe demographic characteristics of the participants.To substantiate this claim, we looked in some detailat differences in gender, language proficiency, andquestionnaire response rates.First, the gender distribution differed greatly be-Webgames reported meansuccess 227 = 61% 93% 4.9lost 92 = 24% 48% 3.4cancelled 55 = 15% 16% 3.3Lab# games reported meansuccess 73 = 80% 100% 5.4lost 18 = 20% 94% 3.3cancelled 0 ?
?Figure 2: Skewed results for ?overall evaluation?.tween the Internet experiment (10% female) andthe laboratory experiment (65% female).
This isrelevant because gender had a significant effecton task completion time (women took longer) andon six subjective measures including ?overall eval-uation?
in the laboratory.
We speculate that thedifference in task completion time may be relatedto well-known gender differences in processingnavigation instructions (Moffat et al, 1998).Second, the two experiments collected data fromsubjects of different language proficiencies.
While93% of the participants in the laboratory experi-ment self-rated their English proficiency as ?expert?or better, only 62% of the Internet participants did.This partially explains the lower task success rateson the Internet, as Internet subjects with Englishproficiencies of 3?5 performed significantly betteron ?task success?
than the group with proficiencies1?2.
If we only look at the results of high-English-proficiency subjects on the Internet, the successrates for all NLG systems except W rise to at least86%, and are thus close to the laboratory results.Finally, the Internet data are skewed by the ten-dency of unsuccessful participants to not fill in thequestionnaire.
Fig.
2 summarizes some data aboutthe ?overall evaluation?
question.
Users who didn?tcomplete the task successfully tended to judge the303systems much lower than successful users, but atthe same time tended not to answer the questionat all.
This skew causes the mean subjective judg-ments across all Internet subjects to be artificiallyhigh.
To avoid differences between the laboratoryand the Internet experiment due to this skew, Fig.
1includes only judgments from successful games.In summary, we find that while the two experi-ments made consistent significance judgments, andthe Internet-based evaluation methodology thusproduces meaningful results, the absolute valuesthey find for the individual evaluation measuresdiffer due to the demographic characteristics of theparticipants in the two studies.
This could be takenas a possible deficit of the Internet-based evalua-tion.
However, we believe that the opposite is true.In many ways, an online user is in a much morenatural communicative situation than a laboratorysubject who is being discouraged from cancellinga frustrating task.
In addition, every experiment ?whether in the laboratory or on the Internet ?
suf-fers from some skew in the subject population dueto sampling bias; for instance, one could argue thatan evaluation that is based almost exclusively on na-tive speakers in universities leads to overly benignjudgments about the quality of NLG systems.One advantage of the Internet-based approachto data collection over the laboratory-based one isthat, due to the sheer number of subjects, we can de-tect such skews and deal with them appropriately.For instance, we might decide that we are onlyinterested in the results from proficient Englishspeakers and ignore the rest of the data; but weretain the option to run the analysis over all partici-pants, and to analyze how much each system relieson the user?s language proficiency.
The amountof data also means that we can obtain much morefine-grained comparisons between NLG systems.For instance, the second and third evaluation worldspecifically exercised an NLG system?s abilities togenerate referring expressions and navigation in-structions, respectively, and there were significantdifferences in the performance of some systemsacross different worlds.
Such data, which is highlyvaluable for pinpointing specific weaknesses of asystem, would have been prohibitively costly andtime-consuming to collect with laboratory subjects.5 ConclusionIn this paper, we have argued that carrying out task-based evaluations of NLG systems over the Internetis a valid alternative to more traditional laboratory-based evaluations.
Specifically, we have shownthat an Internet-based evaluation of systems in theGIVE Challenge finds consistent significant differ-ences as a lab-based evaluation.
While the Internet-based evaluation suffers from certain skews causedby the lack of control over the subject pool, it doesfind more differences than the lab-based evaluationbecause much more data is available.
The increasedamount of data also makes it possible to comparethe quality of NLG systems across different evalua-tion worlds and users?
language proficiency levels.We believe that this type of evaluation effortcan be applied to other NLG and dialogue tasksbeyond GIVE.
Nevertheless, our results also showthat an Internet-based evaluation risks certain kindsof skew in the data.
It is an interesting question forthe future how this skew can be reduced.ReferencesA.
Belz and A. Gatt.
2008.
Intrinsic vs. extrinsic eval-uation measures for referring expression generation.In Proceedings of ACL-08:HLT, Short Papers, pages197?200, Columbus, Ohio.A.
Belz.
2009.
That?s nice ... what can you do with it?Computational Linguistics, 35(1):111?118.D.
Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,J.
Moore, and J. Oberlander.
2009.
Report on theFirst NLG Challenge on Generating Instructions inVirtual Environments (GIVE).
In Proceedings of the12th European Workshop on Natural Language Gen-eration (Special session on Generation Challenges).R.
Dale and M. White, editors.
2007.
Proceedingsof the NSF/SIGGEN Workshop for Shared Tasks andComparative Evaluation in NLG, Arlington, VA.A.
Gatt, A. Belz, and E. Kow.
2008.
The TUNAchallenge 2008: Overview and evaluation results.In Proceedings of the 5th International NaturalLanguage Generation Conference (INLG?08), pages198?206.S.
D. Gosling, S. Vazire, S. Srivastava, and O. P. John.2004.
Should we trust Web-based studies?
A com-parative analysis of six preconceptions about Inter-net questionnaires.
American Psychologist, 59:93?104.S.
Moffat, E. Hampson, and M. Hatzipantelis.
1998.Navigation in a ?virtual?
maze: Sex differences andcorrelation with psychometric measures of spatialability in humans.
Evolution and Human Behavior,19(2):73?87.D.
Scott and J. Moore.
2007.
An NLG evaluation com-petition?
Eight reasons to be cautious.
In (Dale andWhite, 2007).304
