TOKENIZAT ION AS THE IN IT IAL  PHASE IN  NLPJonathan J. Webster & Chunyu KitCity Polytechnic of Hong Kong83 Tat Chee Avenue, Kowloon, Hong KongE-mail: ctwebste@cphkvx.bitnetABSTRACTIn this paper, the authors address the significance andcomplexity of tokenization, the beginning step of NLP.Notions of word and token are discussed and definedfrom the viewpoints of lexicography and pragmaticimplementation, respectively.
Automatic segmentationof Chinese words is presented as an illustration oftokenization.
Practical approaches to identification ofcompound tokens in English, such as idioms, phrasalverbs and fixed expressions, are developed.1.
Introduct ion:  Tokenizat ion in NLPIn NLP studies, it is conventional to concentrate onpure analysis or generation while taking the basic units,namely words, for granted.
It ks an obvious truth, how-ever, that without hese basic units clearly segregated,it is impossible to carry out any analysis or generation.But too little attention has so far been paid to theprocess, a kind of preproeessing in a sense, of iden-tifying basic units to be processed.
The simplicity ofrecognizing words in English, resulting from the exis-tence of space marks as explicit delimiters, has mostlikely misled us into overlooking the complexity ofdistinguishing other units in English, such as idiomsand fixed expressions, not to mention the difficulty inidentifying words in other languages, like Chinese,resulting from the absence of delimiters.In this paper, we define this preprocessing astoken-ization.
The In'st step in NLP is to identify tokens, orthose basic units which need not be decomposed in asubsequent processing.
The entity word is one kind oftoken for NLP, the most basic one.
Our concern, how-ever, is with using the computer to recognize thosetokens without distinct delimiters, such as Chinesewords, English idioms and fixed expressions.So far, there exists very little research adopting thenotion of tokenization we put forward here.
Santos(1990) explored a pragmatic way to transfer Englishidioms and fixed expressions in the domain of machinetranslation.
Linden et al(1990) focused on determiningthe idiomatic or non-idiomatic meaning of idioms.
It isbelieved that, by taking idioms and fixed expressionsas a kind of basic unit at the same level as words,tokeulzation should take on a more generalized andrealistic significance making NLP and MT systemsmore robust and practical.Before we can achieve the identification of suchtokens by computational means, many fundamentalissues need to be resolved.
Among these the nmstimportant is clearly the definition of token.2.
Defining the entity wordThere are a number of notions of what counts as atoken in NLP.
Different notions depend on differentobjectives (e.g.
parsing, MT) and often differentlanguage backgrounds.
To arrive at a definition oftoken, which is at once linguistically significant andmethodologically useful, we propose to first addressthe issue of what is a word frQm ~t l~xicograoher'sSpeaking as a lexicographer, J. McH.
Sinclair pro-poses to define a lexical item as "a formal item (atleast one morpheme long) whose pattern of occurrencecan be described in terms of a uniquely ordered seriesof other lexical items occurring in its environment"(1966:412).
For the lexicographer, it is simply aquestion of finding significant collocations.Sinclair differentiates betweeu what he calls 'casual'and 'significant' collocation.
Casual collocationincludes items which have no bearing on the node, andas Sinclair explains "may be accidental, reflecting theplace, perhaps, where someone breaks into a commit-tee meeting with the coffee; or they may include themagnificent images of some of our greatest poetry"(1966:418).
The larger the corpus, the more casualcollocates will be netted, but at the same time theirsignificance will steadily decrease.
While, on the otherhand, 'collocates typical of the item in question willimpress their pattern more and more strongly untilthe pattern is broadly speaking complete and theevidence of further occurrence does not materiallyalter the pattern" (1966:418).The lexicographer's approach to identifying wordshas significance for tokenization.
By comparingobserved collocation patterns of strings with storedpatterns we can proceed to segment he text intowords.
Finding significant tokens depends on theability to recognize patterns displaying significantcollocation.
Rather than simply relying on whether astring is bounded by delimiters on either side, segmen-tation into significant token relies on a kind of patternrecognition i volving collocational patterns.While suggesting that the search for lexical itemsbegin with those units "which we widely call mor-phemes', Sinclair acknowledges those problems whichAcrEs DE COLING-92.
NANTE'S.
23-28 AOt'rr 1992 1 1 0 6 PROC.
OV COLING-92, NANTES, AUG. 23 28, 1992are likely to complicate matters for the lexicographer:(i) homographs; and(it) compounds or multi-morpheme it ms.Both problems are likely also to affect, perhaps evenfrustrate attempts at automatic segmentation f stringsinto meaningful units, ltomographs, for instance,possess multiple collocational patterns.
It becomes aquestion of not simply finding a match, but evaluatingbetween patterns to lind the one with the best fit.Taking the complex homograph, and, as an example,Sinclair writes, "Grammar is hardly any help at all, andthe distinctions gained by a word-class division makelittle inroads on the complexity of Ihis form.
Thelexicographer is forced to examine the blternalconsistency of the cluster" (1966:42.5).
What onediscovers is that some occurrences of hand are collo-cationally distinct from other nceurrences of the sameform.
Sinclair cites two instances of hand.
One havingcollocates like marriage, daughter, and engagement.
Theother with collocates which include words we as-sociate with card games like whist, ntmmy, ace, andflush.
Both grouping~s are qnlte distinct.
As Sinclairputs it, "tbe chances of whist and marriage coooccuringare as poor as those of archbishop and fish."
Thissuggests we are dealing with difierent lexical items, f inthe other hand, "groupings which shade into eachother, even though opposite nds do not intercollocate,suggest one item with a wide range.
"Polymorphemic tems further complicate the situa-tion.
Richard Hudson, in his Word Gr~mm~r, treatscompounds as a single word whose composition con-sists of a string of two words (1984:50).
Citing theexample offi~miture shop, he explains, "the word shopprovides the link between furniture and the rest of thesentence - it is because shop is a noun that the wholecan occur where nouns can occur; if the whole wereplural, the suffix would be added to shop, not tofurniture; and semantically, afarniture shop is a kind ofshop, and not a kind of furniture"(1984:87).Hudson goes so far as to float the idea that we treatexpressions like London shop, expensive ship, or evensoon left (as in John soon left) in the same manner assingle words consisting of a modifier followed by itshead (1984:89).
While he admits this rcanalysis "mayseem perverse", nevertheless he believes there arearguments in its favour.
For one thing, the word-orderrules for English would be simplified (i.e.
a modifierfollows its head unless both modifier and head are partof a word, then the modifier comes first).
Also, hisreanalysis would help to explain why premodifierscannot hemselves have postmodifiers.Sinclair, on the other hand, would not regard aparticular combination of words as a separate poly-morphemic item unless its cluster cannot be predictedfrom the clusters of its components (1966:423).
Thus,while some occurrences ofcold + feet are regarded asa separate polymorphemic item, cold + hands wouldnot be treated as such.Sinclair fixes no limit on the size of a polymor-phemic item.
Moreover, contrary to a claim made byHudson "that the parts of a word cannot be separatedby other words which are not part of the same wurd"(1984:89), Sinclair argues that the components of apolymorphemic item may in tact be discontinuous.
Sin-clair cites examples like you must cat your coat, I'mafrai~ according to your ch~th, and from a Sundaynewspaper, put all his nuclear eg~" in the West Germanbasket.The possibility of achieving word recognition throughmapping collocations in the text to stored collocafionalpatterus uggests a common-sense, practical approachto tokenization and dismnbiguation.3.
Automat ic  word  segmentat ion  i  Clt ineseNI ,P  - An  example of  tokenizat ionldentificatinn of words is still a perl,lexing problemin Chinese NLP.
As with English words, i)articularlyidioms aml compounds, the source of dilficulty has todo wilh the absence of dclinfiters between tokens.3.1 BackgroundAs we know, a Chinese character is compntatioitallyrepresented by an internal code.
Words, however, eachof which may consist of one or more characters, donot have any obvious indicators to mark their boun.-dories.
Tokenizafion of Chinese words, includingidioms and fixed expressions which are, of conrse,phrases containing words as their constituents butused as words, is generally regarded as anotherbottleneck following "Chinese character coding".
It ksknown in formal terms as automatic word segmentationin China mainland and as word identification abroad.
Inrecent years, it has become a very important opic inChinese NLP.
Without coding, it is impossible to inputcharacters into computer.
Without word identification,we cannot hope to achieve text processing.This topic has been approached flora two sides.
Onthe theoretical side, researchers have sought an explicitspecification of the entity word.
The difficulty of wordidentification has rcsnlted fi'om a confusion ofcharacter, word and phrase in Chinese finguistk's.Because the construction of words, phrases andsel).tences arc so similar, some scholars even believethey are identical.
In an attempt to bring this debate tosome conclusion, a standard was introdnced by theChinese State Bureau of Standardization t0r wordsegmentation.
The term segrnentation unit was em-ployed to refer to words, idioms, fixed expressious,terminology as long as two or three dozen charactersand even any entities which can be treated as anundivided unit in a processing (Kit 1989).
This term,as a prototype of token, indicates the appearance oftokenization otion in Chinese computing.3_2 Basic methodsOn the practical side, studies have concentrated onAcrEs DECOLING-92, NANTES, 23 28 aO(J'r 1992 I l 0 7 PICOt:.
0~: COLING-92, NANrES, AUC;.
23 28, 1992two aspects: 1) the implementation of mechanicalsegmentation with fundamental supports, such as theConstruction of a dictionary wlfich permits quick andefficient access; 2) strategies for disambiguation.At the outset, segmentation methods were inventedone after another and seemed inexhaustible.
But aftersystematic study, a structural model was finally built(Kit 1988; Kit et al1989).
In essence, word segmen-tation involves table-look-up and string matching suchthat character string of the input text is compared withentities in an existing word list, i.e., the dictionary.Every automatic segmenting method of this kind isproven to be decided by three factors, as shown belowin the structural model ASM(d,a,m), in whichASM stands for Automatic Segmenting Method;d~{+l,-1}, indicates the scanning directions inmatching, scanning from left to right is forward andthe opposite is backward, respectively;,a~ { + 1,-1}, indicates character addition or omissionin each round of string matching that finds a word,respectively;m~{+l, - l} ,  indicates the usage of maximum orminimum matching, respectively.It is believed that all elemental methods are includedin this model.
Furthermore, it can be viewed as theultimate model for methods of string matching of anyelements, including methods for finding English idioms.The minimum match methods are not appropriatefor Chinese word segmentation because almost everyChinese character can be used as a token - a word ora single morpheme.
By contrast, however, a maximummatch method can obtain an identification rate as highas around 98 per cent, with an adequately largedictionary.
The earliest and most influential implemenotation was the CWDS system (Liang 1984; Liu & Liang1986), which processed a corpus of 200 millioncharacters in practical use.A segmenting strategy may integrate more than onebasic method to achieve a special task, e.g., forwardand backward scanning methods are often employedtogether to check segmentation ambiguities.
Such hasbeen proven an efficient approach, though not perfect.3.3 Handling ambiguitiesMost research today on Chinese word segmentationhas shifted to handling ambiguities in order to achievea higher identification rate.
There are two types ofambiguities at the level of word segmentation:Type I: In a sequence of chinese characters S = av..a ibt...bi, if at...ai, bt...b I and S are each a word, thenthere is conjunctive ambiguity in S. The segment Swhich is itself a word contains other words.
It is alsoknown as multi-combinational ambiguity.Type 1I: In a sequence of Chinese characters S = a...a ib~...bjct...q, if at...a~bt...b i and b~...b~q...cx are each aword, then S is an overlapping ambiguous segment, orin other words the segment S displays disjunctiveambiguity.
The segment bl...bt is known as an overlap,which is usually one character long.3.3.1 Ambiguity checkingThe first step toward resolving segmentation ambi-guities is to find them.
Bidirectional scanning is onesimple and powerful method.
Differences in segmen-tation resulting fi'om the two methods reveal thepresence of ambiguities.
But there still remain manyambiguities not found using this method.
An integralapproach to checking segmentation ambiguities wasdeveloped as follows:1.
Find all possible words from the beginning of thestring and record their end positions;2.
Redo step 1 from those end positions, rather thanfrom the beginning, if there is any new end positionequal to or exceeding previous greatest one, a type I ortype I1 ambiguity, respectively, is found.It is a very simple and efficient strategy for findingany ambiguity and prevent all unnecessary operationson false ambiguities (Kit 1988 & 1992).3.3.2 Approaches to disamhiguationNormally, tile disambiguation stage follows themechanical segmentation a d the ambiguity checking.Two distinct approaches to disambignation are theknowledge-based and the statistic'd-based.Tile former is to discriminate all ambiguities bymeans of a built-in knowledge base, including rules,which are applied to a series of similar ambiguities,and special case knowledge for particular cases ofambiguities (Liang 1984 & 1990; Ho et al1991).
Alarge number of uncertainties are settled in this way,but there is a side-affect: he rules may result in somemistakes that even a mechanical segmenting methodcan handle properly (Kit 1988).
This may be partiallydue to the complexity of language, but a more sophis-ticated approach to organizing and applying knowledgeis still needed.As for the latter, deriving from corpus linguistics,general techniques in tagging are employed and someadvances have been reported (Lai et al1991).
But thedesign of a comprehensive and efficient agging systemis still, however, a big problem.
Besides, a relaxationapproach, which skips the mechanical segmentationand entirely relies on calculation of possibility, istheoretically sound, but practically, its identificationrate is just about 95% (Fan & Tsai 1988), lower thanthat of mechanical methods.
An appropriate combina-tion of relaxation and mechanical means is expected toachieve a better esult.4.
English compound tokens in NLPIn previous ections, we concentrated on words, inboth English and Chinese: In fact, there are still alarge number of compound tokens that take simpletokens, like words, as their constituents.
They arecritical to many processes in NLP and machine trans-lation so that their identification is of greatsignificance.AcrEs DE COLING-92, NANI'ES, 23-28 AO(n 1992 1 l 0 8 PROC.
OV COLING-92, NANTES, AUG. 23-28, 19924.1 Ratiouale for the notion of tokenConcepts such as word, collocation, and multi-morpheme item are important o lexicographers andlinguists; whereas the concept of token is specific tocertain processes in NLP and MT.
A tokcn will not bebroken down into smaller parts, lit other words, for tbepurpose of computational processing, it can be treatedas an atom.There are many compound tokens, composed of anumber of words, to be trans-lerred as a whole in MT.In syntactic analysis, if it is decided to treat them asindivisible units, with no care as to their innerstructurc, then they become tokens for syntacticanalysis.
Token, then is a terminal node ill processing.This is the essence, and also the importance, of theconcept of token.4.2 Decomposition versus IdeutificatiouThere are mainly two opposing views on how oneshould deal with English idioms, which have beenidentified as compound tokens in our framework: onestresses the decomposltlonality of idioms intoconstituent parts (Wasow, Sag, and Nunberg 1983;Gazxlar et al1985; Stock 19891; another considersidioms as units of language as basic as word andwholly non-compositional In meaning (Wood 1986;Linden et al1990; Santos 1990).The concept of token may offer a possible solutionto this debate.
To what degree a linguistic unit requiresdecomposition will depend on the nature of the task tobe performed.
In the case of lexieal transfer in MT,there is no need to decompose an idiom into itsconstituent parts.
However as noted below in ourdiscussion of idioms and fixed expressions in dis-continuous co-occurrence, structural analysis issometimes necessary.
In either ease, priority must begiven to the recognition of compound tokens.
Thewhole must first be ascertained before one can evenconsider what are its constituents.5.
Tokenizat ion and lexical information retr ievalThere are a number of approaches to recognizingcompound token.s.
In this section we discuss two inparticular.
In the first, recognition is achieved bymeans of accessing lexical knowledge represented as anetwork of associations.
The second adopts anapproach combining table-look-up matching andknowledge processing.5.1 l~xical Information retrieval as a basis for tokenrecognitionAs noted above, the lexicographer's notion of wordcorresponds closely to the notion of token we haveadopted here.
We saw that what the lexicographertakes to be a word is an entity for which there existssome distinctive and significant collocation pattern.This bidirectional ssociation between a word and itscompanions i itself evidence of that word's integrityand offcrs insight into its interp~'etation* The lexico-grapher's discovery procedure offers a useful model forachieving token identification.
We are proposing totrain a neural network to recognize tokens on the basisof their compauion relations.
Once the training processis completed, the neural network will be enabled toperform the tasks of tokeni~'ation a d disambignationby matching input with learned patterns of companionrelalimls.The network might also have to include informationabout other kinds of relatinns as well.
The basicpremise of Richard \]Iudson's Word Grammar is thatthe entity word can be realized as part of a system ornetwork of relations.
Entities in the lexicon, heexplains, include words and their parts, their models,their companions, their referents and their contexts ofuse.
Lexemes arc emic units joined systematically toone another along vertical and horizontal dimensions.Every entity in the lexicon is at once a whole arid aninstance.
It is the composite realization of its parts aswell as the reali:,atinn of some model.
Along thisw:rtical dimcnsion~ information flows from the moregeucral to the more specific.
The horizontal dimension,on the other hand, includes the lexical constraintsimposed by heads tm modifiers as well as vice versa.
"l'hcse lludson refers to as an entity's companinnrelations.
Such are the relations between collocates.Hudson's network approach accounts for the variousrealizations of entities as they occur in context in termsof the eom~ectinns drawn between an entity and itsrcfcrent(s), utterance-event(s), and companinn(s).
Ina previous implementation of Hudson's networkapproach, we represented each lexical entry by meansof a frmne whose slots coH'espondcd to Hudson's fiverelations (Webster,1987).52 Table-look-up matchingThe simplest approach to identification ofcompoundtokens is obvinnsly table-look-up matching.
Admittedly,it presumes that a list of sample tokens in sufficientnmnber already exists.
The basic steps of this approacharc, first, tokcnize each single word, then continuematchi~ to find whether there are any compoundtokens among these single words.
Such an approachis very similar to the basic method of automaticsegmentation of Chinese words.
This method canrecoglfize English idioIas and other compound tokenswhose constituents are continuous, but has no ability tohandle ambiguities and catch variaut forms of idiomsand lixed expressions in discontinuous co-occurrence.5.3 Generalized Table-look-upThis is an adjttsted table~hmk-up method designedto deal with idioms and ft~ed expressions in discon-tinuous co.occurrence, e.g., keep INP\] in mind in wlfichkeep plus/n mind constitutes a fixed expression and inrabid, a preposifional phrase, Ls merely part of a biggertoken.
Between these two parts, there is a noun phraseACIES DE COL1NG-92.
NANq'ES, 23-28 Aof;i 1992 l I 0 9 lhz~c., ol; C(iI.lNG.92, NANrES, AUti.
23-28.
1992which is usually not too long.
If it is long, we have avariant form of it, i.e., keep in mind \[NP\].
Of course,the \[NP\] can be substituted with a \[Subclause\].In order to identify a compound token like this, wehave to determine the NP and Subclause.
Operationsof this type need to, in part, make use of syntacticanalysis.
Thus, partialpwxing needs to be incorporatedinto the table-look-up.
Notice that the parsing maytake every word as its token in order to find compoundtokens.
From this, one can see the importance ofstruc-tural analysis to the identification of compound tokens.Besides tructural analysis, knowledge about com-pound tokens, such as where the INP\] and \[Subclause\]should be put in the discontinuous token keep ... inmind, is also required.
Discontinuous idioms, phrasalverbs such as figure out \[NP\] and figure \[it\] out, andfixed expressions, allhave to be processed with the aidof knowledge.
By now it is dear that the generalizedtable-lookoup is an approach combining parsing andknowledge processing.
With adequate knowledge aboutdiscontinuous compound tokens, itmay prove effectivein their identification.6.
ConclusionThe notion of token must first be defined beforecomputational processing can proceed.
Obviously thereis more to the issue than simply identifying stringsdelimited on both sides by spaces or punctuation.
Wehave considered what constitutes a token from twoperspectives: one from the lexicographer's experiencewith identifying words, the second from the experienceof researchers in the area of Chinese NLP.
From thework on automatic word segmentation i  ChineseNLP, we have noted some valuable lessons which canbe applied to the recognition of idioms and othertixed-expressions in English.
The lexicographer'sdiscovery procedures, informed with the knowledge oflexical relations implemented either as a neural net-work or in lcxical frames, also provide a useful modelfor the construction of a practical, knowledge-basedapproach to tokenization and disambiguation.Main references\[1\] Fan, C. K., and Tsai, W. H. 1988.
Automatic wordidentification in Chinese sentences by the relaxationtechnique, Computer Processing of Chinese & OrientalLanguages, V.4, No.1.\[2\] Ga:,xiar, G., Klein, E., PuUum, G., and Sag, 1.
1985.Generalized Phrase Structure Grammar.
Cambridge,Mass.
: Harvard University Press\[3\] He, K., Xn, H., and Sun, B.
1991.
Expert system forautomatic word segmentation f written Chinese.
Pro-ceedings of 1991 International Conference on ComputerProcessing of Chinese and Oriental Languages, Taiwan.\[41 Hudson, R.A. 1984.
Word Grammar.
Oxford: BasilBlackweU.\[5\] Kit(=Jie), C. 1988.
Methods of Chinese automaticword segmentation a d the design and implementationof a practical system.
Master's Thesis, Graduate School,Chinese Academy of Social Sciences, Beijing.\[6\] Kit(=Jie), C., Liu, Y., and Liang, N. 1989.
Onmethods of Chinese automatic word segmentation.Journal of Chinese Information Processing, Vol.3, No.1.\[8\] Kit(=Jie), C. 1989.
Some key issues on theContemporary Chinese language Word SegmentationStandard Used for Information Processing, Pro-ceedings of 1989 International Symposium on Standar-dization of Chinese Infomzation Processing, Beijing,\[9\] Kit, C. 1992.
Practical techniques of chineseautomatic word segmentation in the applied systemCASS, Proceedings of PAN-ASIATIC LINGUISTICS-92, Bangkok.\[10\] Kramsky, Jiri.
1%9.
The Wordas a Linguistic Unit.The Hague: Mouton.\[11\] lai, T., Lun, S., Sun, C., and Sun, M. 1991.
Amaximal match Chinese text segmentation algorithmusing mainly tagrs for resolution of ambiguities,Proceedings of ROCLING IV, Taiwan..\[12\] Liang, N. 1984.
Chinese automatic word segmen-tation system CDWS, Journal of Beijing University ofAeronautics and Astronautics, 1984, No.4.\[13\] Liang, N. 1990.
The knowledge for Chinese wordsegmentation, Journal of Chinese InformationProcessing, Vol.
4, No.
2.\[14\] Linden, E. van der, and Kraaij, W. 1990.Ambiguity resolution and the retrieval of idioms: twoapproaches.
Proceedings of COLING-90.
Helsinki,Finland.\[15\] Lin, Y., and Liang, N. 1986.
Basic engineering forChinese processing - modem Chinese word frequencycount, Journal of Chinese Information Processing, Vol.1,No.1.\[16\] Santos, D. 1990.
Lexlcal gaps and idioms inmachine translation, Proceedings of COLING-90,Helsinki, Finland.\[17\] Sinclair, John McH.
1966.
Beginning the study oflexis (1966:410-429), in Bazell, et al (eds.)
In Memoryof J R Firth.
Longmans: London.\[18\] Stock, O.
1989.
Parsing with flexibility, dynamicstrategies and idioms in mind.
Journal of Computa-tional Linguistics, Vol.
15, No.
1.\[19\] Wasow, T., Sag, 1., and Nunberg, G. 1983.
Idioms:an interim report.
In S. Hattori and K. Inoue (eds.
)Proceedings of the 13th international congress oflinguistics, Tokyo, Japan.\[20\] Webster, Jonathan J.
1987.
A computationalmodel for representing WORD knowledge (1987:432-442) in the Thirteenth LACUS Forum 1986, Chicago,Illinois: LINGUISTIC ASSOCIATION OF CANADAAND THE US.\[21\] Wood, M. McGee.
1986.
A Definition of Idiom.Master's Thesis, University of Manchester (1981).Reproduced by the Indiana University Linguistics Club.A(:tES DE COLING-92, NArZlJ:.S, 23-28 ^O(n 1992 1 1 1 0 PRO(:.
o\[: COLING-92, NANTES.
Auo.
23-28, 1992
