Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2019?2027,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsClassifying Idiomatic and Literal Expressions Using Topic Models andIntensity of EmotionsJing Peng & Anna FeldmanComputer Science/LinguisticsMontclair State UniversityMontclair, New Jersey, USA{pengj,feldmana}@mail.montclair.eduEkaterina VylomovaComputer ScienceBauman State Technical UniversityMoscow, Russiaevylomova@gmail.comAbstractWe describe an algorithm for automatic clas-sification of idiomatic and literal expressions.Our starting point is that words in a given textsegment, such as a paragraph, that are high-ranking representatives of a common topic ofdiscussion are less likely to be a part of an id-iomatic expression.
Our additional hypothesisis that contexts in which idioms occur, typi-cally, are more affective and therefore, we in-corporate a simple analysis of the intensity ofthe emotions expressed by the contexts.
Weinvestigate the bag of words topic represen-tation of one to three paragraphs containingan expression that should be classified as id-iomatic or literal (a target phrase).
We ex-tract topics from paragraphs containing idiomsand from paragraphs containing literals us-ing an unsupervised clustering method, LatentDirichlet Allocation (LDA) (Blei et al., 2003).Since idiomatic expressions exhibit the prop-erty of non-compositionality, we assume thatthey usually present different semantics thanthe words used in the local topic.
We treatidioms as semantic outliers, and the identifi-cation of a semantic shift as outlier detection.Thus, this topic representation allows us to dif-ferentiate idioms from literals using local se-mantic contexts.
Our results are encouraging.1 IntroductionThe definition of what is literal and figurative is stillobject of debate.
Ariel (2002) demonstrates that lit-eral and non-literal meanings cannot always be distin-guished from each other.
Literal meaning is originallyassumed to be conventional, compositional, relativelycontext independent, and truth conditional.
The prob-lem is that the boundary is not clear-cut, some figu-rative expressions are compositional ?
metaphors andmany idioms; others are conventional ?
most of the id-ioms.
Idioms present great challenges for many Natu-ral Language Processing (NLP) applications.
They canviolate selection restrictions (Sporleder and Li, 2009)as in push one?s luck under the assumption that onlyconcrete things can normally be pushed.
Idioms candisobey typical subcategorization constraints (e.g., inline without a determiner before line), or change thedefault assignments of semantic roles to syntactic cate-gories (e.g., in X breaks something with Y, Y typicallyis an instrument but for the idiom break the ice, it ismore likely to fill a patient role as in How to break theice with a stranger).
In addition, many potentially id-iomatic expressions can be used either literally or fig-uratively, depending on the context.
This presents agreat challenge for machine translation.
For example,a machine translation system must translate held firedifferently in Now, now, hold your fire until I?ve had achance to explain.
Hold your fire, Bill.
You?re too quickto complain.
and The sergeant told the soldiers to holdtheir fire.
Please hold your fire until I get out of theway.
In fact, we tested the last two examples using theGoogle Translate engine and we got proper translationsof the two neither into Russian nor into Hebrew, Span-ish, or Chinese.
Most current translation systems relyon large repositories of idioms.
Unfortunately, thesesystems are not capable to tell apart literal from figura-tive usage of the same expression in context.
Despitethe common perception that phrases that can be idiomsare mainly used in their idiomatic sense, Fazly et al.
(2009)?s analysis of 60 idioms has shown that close tohalf of these also have a clear literal meaning; and ofthose with a literal meaning, on average around 40% oftheir usages are literal.In this paper we describe an algorithm for automaticclassification of idiomatic and literal expressions.
Ourstarting point is that words in a given text segment,such as a paragraph, that are high-ranking representa-tives of a common topic of discussion are less likelyto be a part of an idiomatic expression.
Our additionalhypothesis is that contexts in which idioms occur, typ-ically, are more affective and therefore, we incorpo-rate a simple analysis of the intensity of the emotionsexpressed by the contexts.
We investigate the bag ofwords topic representation of one to three paragraphscontaining an expression that should be classified asidiomatic or literal (a target phrase).
We extract top-ics from paragraphs containing idioms and from para-graphs containing literals using an unsupervised clus-tering method, Latent Dirichlet Allocation (LDA) (Bleiet al., 2003).
Since idiomatic expressions exhibit theproperty of non-compositionality, we assume that theyusually present different semantics than the words used2019in the local topic.
We treat idioms as semantic outliers,and the identification of semantic shift as outlier detec-tion.
Thus, this topic representation allows us to differ-entiate idioms from literals using the local semantics.The paper is organized as follows.
Section 2 brieflydescribes previous approaches to idiom recognition orclassification.
In Section 3 we describe our approach indetail, including the hypothesis, the topic space repre-sentation, and the proposed algorithm.
After describingthe preprocessing procedure in Section 4, we turn to theactual experiments in Sections 5 and 6.
We then com-pare our approach to other approaches (Section 7) anddiscuss the results (Section 8).2 Previous WorkPrevious approaches to idiom detection can be classi-fied into two groups: 1) Type-based extraction, i.e., de-tecting idioms at the type level; 2) token-based detec-tion, i.e., detecting idioms in context.
Type-based ex-traction is based on the idea that idiomatic expressionsexhibit certain linguistic properties that can distinguishthem from literal expressions (Sag et al.
(2002); Fa-zly et al.
(2009)), among many others, discuss variousproperties of idioms.
Some examples of such proper-ties include 1) lexical fixedness: e.g., neither ?shootthe wind?
nor ?hit the breeze?
are valid variations ofthe idiom shoot the breeze and 2) syntactic fixedness:e.g., The guy kicked the bucket is potentially idiomaticwhereas The bucket was kicked is not idiomatic any-more; and of course, 3) non-compositionality.
Thus,some approaches look at the tendency for words to oc-cur in one particular order, or a fixed pattern.
Hearst(1992) identifies lexico-syntactic patterns that occurfrequently, are recognizable with little or no precodedknowledge, and indicate the lexical relation of interest.Widdows and Dorow (2005) use Hearst?s concept oflexicosyntactic patterns to extract idioms that consistof fixed patterns between two nouns.
Basically, theirtechnique works by finding patterns such as ?thrills andspills?, whose reversals (such as ?spills and thrills?)
arenever encountered.While many idioms do have these properties, manyidioms fall on the continuum from being composi-tional to being partly unanalyzable to completely non-compositional (Cook et al.
(2007)).
Fazly et al.
(2009);Li and Sporleder (2010), among others, notice thattype-based approaches do not work on expressions thatcan be interpreted idiomatically or literally dependingon the context and thus, an approach that considers to-kens in context is more appropriate for the task of idiomrecognition.A number of token-based approaches have beendiscussed in the literature, both supervised (Katzand Giesbrech (2006)), weakly supervised (Birke andSarkar (2006)) and unsupervised (Sporleder and Li(2009); Fazly et al.
(2009)).
Fazly et al.
(2009) de-velop statistical measures for each linguistic propertyof idiomatic expressions and use them both in a type-based classification task and in a token identificationtask, in which they distinguish idiomatic and literal us-ages of potentially idiomatic expressions in context.Sporleder and Li (2009) present a graph-based modelfor representing the lexical cohesion of a discourse.Nodes represent tokens in the discourse, which are con-nected by edges whose value is determined by a seman-tic relatedness function.
They experiment with two dif-ferent approaches to semantic relatedness: 1) Depen-dency vectors, as described in Pado and Lapata (2007);2) Normalized Google Distance (Cilibrasi and Vit?anyi(2007)).
Sporleder and Li (2009) show that this methodworks better for larger contexts (greater than five para-graphs).
Li and Sporleder (2010) assume that literaland figurative data are generated by two different Gaus-sians, literal and non-literal and the detection is done bycomparing which Gaussian model has a higher prob-ability to generate a specific instance.
The approachassumes that the target expressions are already knownand the goal is to determine whether this expression isliteral or figurative in a particular context.
The impor-tant insight of this method is that figurative languagein general exhibits less semantic cohesive ties with thecontext than literal language.Feldman and Peng (2013) describe several ap-proaches to automatic idiom identification.
One ofthem is idiom recognition as outlier detection.
Theyapply principal component analysis for outlier detec-tion ?
an approach that does not rely on costly an-notated training data and is not limited to a specifictype of a syntactic construction, and is generally lan-guage independent.
The quantitative analysis providedin their work shows that the outlier detection algorithmperforms better and seems promising.
The qualitativeanalysis also shows that their algorithm has to incor-porate several important properties of the idioms: (1)Idioms are relatively non-compositional, comparing toliteral expressions or other types of collocations.
(2)Idioms violate local cohesive ties, as a result, they aresemantically distant from the local topics.
(3) Whilenot all semantic outliers are idioms, non-compositionalsemantic outliers are likely to be idiomatic.
(4) Id-iomaticity is not a binary property.
Idioms fall on thecontinuum from being compositional to being partlyunanalyzable to completely non-compositional.The approach described below is taking Feldmanand Peng (2013)?s original idea and is trying to address(2) directly and (1) indirectly.
Our approach is alsosomewhat similar to Li and Sporleder (2010) because italso relies on a list of potentially idiomatic expressions.3 Our HypothesisSimilarly to Feldman and Peng (2013), out startingpoint is that idioms are semantic outliers that violatecohesive structure, especially in local contexts.
How-ever, our task is framed as supervised classification andwe rely on data annotated for idiomatic and literal ex-pressions.
We hypothesize that words in a given text2020segment, such as a paragraph, that are high-rankingrepresentatives of a common topic of discussion areless likely to be a part of an idiomatic expression inthe document.3.1 Topic Space RepresentationInstead of the simple bag of words representation of atarget document (segment of three paragraphs that con-tains a target phrase), we investigate the bag of wordstopic representation for target documents.
That is, weextract topics from paragraphs containing idioms andfrom paragraphs containing literals using an unsuper-vised clustering method, Latent Dirichlet Allocation(LDA) (Blei et al., 2003).
The idea is that if the LDAmodel is able to capture the semantics of a target docu-ment, an idiomatic phrase will be a ?semantic?
outlierof the themes.
Thus, this topic representation will al-low us to differentiate idioms from literals using thesemantics of the local context.Let d = {w1, ?
?
?
, wN}tbe a segment (document)containing a target phrase, where N denotes the num-ber of terms in a given corpus, and t represents trans-pose.
We first compute a set of m topics from d. Wedenote this set byT (d) = {t1, ?
?
?
, tm},where ti= (w1, ?
?
?
, wk)t. Here wjrepresents a wordfrom a vocabulary of W words.
Thus, we have tworepresentations for d: (1) d, represented by its originalterms, and (2)?d, represented by its topic terms.
Twocorresponding term by document matrices will be de-noted by MDand M?D, respectively, where D denotesa set of documents.
That is, MDrepresents the original?text?
term by document matrix, while M?Drepresentsthe ?topic?
term by document matrix.Figure 1 shows the potential benefit of topic spacerepresentation.
In the figure, text segments containingtarget phrase ?blow whistle?
are projected on a two di-mensional subspace.
The left figure shows the projec-tion in the ?text?
space, represented by the term by doc-ument matrixMD.
The middle figure shows the projec-tion in the topic space, represented by M?D.
The topicspace representation seems to provide a better separa-tion.We note that when learning topics from a small datasample, learned topics can be less coherent and inter-pretable, thus less useful.
To address this issue, regu-larized LDA has been proposed in the literature (New-man et al., 2011).
A key feature is to favor words thatexhibit short range dependencies for a given topic.
Wecan achieve a similar effect by placing restrictions onthe vocabulary.
For example, when extracting topicsfrom segments containing idioms, we may restrict thevocabulary to contain words from these segments only.The middle and right figures in Figure 1 illustrate a casein point.
The middle figure shows a projection onto thetopic space that is computed with a restricted vocabu-lary, while the right figure shows a projection when weplace no restriction on the vocabulary.
That is, the vo-cabulary includes terms from documents that containboth idioms and literals.Note that by computing M?D, the topic term by doc-ument matrix, from the training data, we have createda vocabulary, or a set of ?features?
(i.e., topic terms)that is used to directly describe a query or test segment.The main advantage is that topics are more accuratewhen computed by LDA from a large collection of id-iomatic or literal contexts.
Thus, these topics capturemore accurately the semantic contexts in which the tar-get idiomatic and literal expressions typically occur.
Ifa target query appears in a similar semantic context, thetopics will be able to describe this query as well.
On theother hand, one might similarly apply LDA to a givenquery to extract query topics, and create the query vec-tor from the query topics.
The main disadvantage isthat LDA may not be able to extract topic terms thatmatch well with those in the training corpus, when ap-plied to the query in isolation.3.2 AlgorithmThe main steps of the proposed algorithm, calledTopSpace, are shown below.Input: D = {d1, ?
?
?
, dk, dk+1, ?
?
?
, dn}: trainingdocuments of k idioms and n?
k literals.Q = {q1, ?
?
?
, ql}: l query documents.1.
Let DicI be the vocabulary determined solelyfrom idioms {d1, ?
?
?
, dk}.
Similarly, let DicLbe the vocabulary obtained from literals{dk+1, ?
?
?
, dn}.2.
For a document diin {d1, ?
?
?
, dk}, apply LDAto extract a set of m topics T (di) = {t1, ?
?
?
, tm}using DicI .
For di?
{dk+1, ?
?
?
, dn}, DicL isused.3.
Let?D = {?d1, ?
?
?
,?dk,?dk+1, ?
?
?
,?dn} be theresulting topic representation of D.4.
Compute the term by document matrix M?Dfrom?D, and let DicT and gw be the resultingdictionary and global weight (idf ), respectively.5.
Compute the term by document matrix MQfromQ, using DicT and gw from the previous step.Output: M?Dand MQTo summarize, after splitting our corpus (see section4) into paragraphs and preprocessing it, we extract top-ics from paragraphs containing idioms and from para-graphs containing literals.
We then compute a term bydocument matrix, where terms are topic terms and doc-uments are topics extracted from the paragraphs.
Ourtest data are represented as a term-by-document matrixas well (See the details in section 5).2021?100 ?80 ?60 ?40 ?20 0 20?20020406080100 2D Text Space: Blow WhistleIdiomsLiterals?20 ?15 ?10 ?5 0 5 10 15?505101520 2D Topic Space: Blow WhistleIdiomsLiterals?12 ?10 ?8 ?6 ?4 ?2 0 2 4 6 8?10?50510152025 2D Topic Space: Blow WhistleIdiomsLiteralsFigure 1: 2D projection of text segments containing ?blow whistle.?
Left panel: Original text space.
Middle panel:Topic space with restricted vocabulary.
Right panel: Topic space with enlarged vocabulary.3.3 Fisher Linear Discriminant AnalysisOnce M?Dand MQare obtained, a classification rulecan be applied to predict idioms vs. literals.
The ap-proach we are taking in this work for classifying id-ioms vs. literals is based on Fisher?s discriminant anal-ysis (FDA) (Fukunaga, 1990).
FDA often significantlysimplifies tasks such as regression and classification bycomputing low-dimensional subspaces having statisti-cally uncorrelated or discriminant variables.
In lan-guage analysis, statistically uncorrelate or discriminantvariables are extracted and utilized for description, de-tection, and classification.
Woods et al.
(1986), for ex-ample, use statistically uncorrelated variables for lan-guage test scores.
A group of subjects is scored on abattery of language tests, where the subtests measuredifferent abilities such as vocabulary, grammar or read-ing comprehension.
Horvath (1985) analyzes speechsamples of Sydney speakers to determine the relativeoccurrence of five different variants of each of fivevowels sounds.
Using this data, the speakers clusteraccording to such factors as gender, age, ethnicity andsocio-economic class.A similar approach has been discussed in Peng et al.(2010).
FDA is a class of methods used in machinelearning to find the linear combination of features thatbest separate two classes of events.
FDA is closelyrelated to principal component analysis (PCA), wherea linear combination of features that best explains thedata.
Discriminant analysis explicitly exploits class in-formation in the data, while PCA does not.Idiom classification based on discriminant analysishas several advantages.
First, as has been mentioned,it does not make any assumption regarding data distri-butions.
Many statistical detection methods assume aGaussian distribution of normal data, which is far fromreality.
Second, by using a few discriminants to de-scribe data, discriminant analysis provides a compactrepresentation of the data, resulting in increased com-putational efficiency and real time performance.In FDA, within-class, between-class, and mixturescatter matrices are used to formulate the criteria ofclass separability.
Consider a J class problem, wherem0is the mean vector of all data, and mjis the meanvector of jth class data.
A within-class scatter ma-trix characterizes the scatter of samples around theirrespective class mean vector, and it is expressed bySw=J?j=1pjlj?i=1(xji?mj)(xji?mj)t, (1)where ljis the size of the data in the jth class, pj(?jpj= 1) represents the proportion of the jth classcontribution, and t denotes the transpose operator.
Abetween-class scatter matrix characterizes the scatter ofthe class means around the mixture mean m0.
It is ex-pressed bySb=J?j=1pj(mj?m0)(mj?m0)t. (2)The mixture scatter matrix is the covariance matrix ofall samples, regardless of their class assignment, and itis given bySm=l?i=1(xi?m0)(xi?m0)t= Sw+ Sb.
(3)The Fisher criterion is used to find a projection matrixW ?
<q?dthat maximizesJ(W ) =|WtSbW ||WtSwW |.
(4)In order to determine the matrix W that maximizesJ(W ), one can solve the generalized eigenvalue prob-lem: Sbwi= ?iSwwi.
The eigenvectors correspondingto the largest eigenvalues form the columns ofW .
For atwo class problem, it can be written in a simpler form:Sww = m = m1?
m2, where m1and m2are themeans of the two classes.4 Data preprocessing4.1 Verb-noun constructionsFor our experiments we use the British National Cor-pus (BNC, Burnard (2000)) and a list of verb-noun con-structions (VNCs) extracted from BNC by Fazly et al.2022(2009); Cook et al.
(2008) and labeled as L (Literal),I (Idioms), or Q (Unknown).
The list contains onlythose VNCs whose frequency was greater than 20 andthat occurred at least in one of two idiom dictionaries(Cowie et al., 1983; Seaton and Macaulay, 2002).
Thedataset consists of 2,984 VNC tokens.
For our experi-ments we only use VNCs that are annotated as I or L.4.2 LemmatizationInstead of dealing with various forms of the same root,we use lemmas provided by the BNC XML annotation,so our corpus is lemmatized.
We also apply the (modi-fied) Google stop list before extracting the topics.
Thereason we modified the stop list is that some functionwords can potentially be idiom components (e.g., cer-tain prepositions).4.3 ParagraphsWe use the original SGML annotation to extract para-graghs from BNC.
We only kept the paragraphs thatcontained VNCs for our experiments.
We experi-mented with texts of one paragraph length (single para-graph contexts) and of three-paragraph length (multi-paragraph contexts).
An example of multi-paragraphcontexts is shown below:So, reluctantly, I joined Jack Hobbs in not rockingthe boat, reporting the play and the general uproar withperhaps too much impartiality.
My reports went to allBritish newspapers, with special direct services by meto India, South Africa and West Indies; even to KingGeorge V in Buckingham Palace, who loved his cricket.In other words, I was to some extent leading the Britishpublic astray.I regret I can shed little new light on the mystery ofwho blew the whistle on the celebrated dressing-roomscene after Woodfull was hit.
while he was lying on themassage table after his innings waiting for a doctor,Warner and Palairet called to express sympathy.Most versions of Woodfull?s reply seem to agree thathe said.
There are two teams out there on the oval.One is playing cricket, the other is not.
This game istoo good to be spoilt.
It is time some people got out ofit.
Warner and Palairet were too taken aback to reply.They left the room in embarrassment.Single paragraph contexts simply consist of the mid-dle paragraph.5 Experiments5.1 MethodsWe have carried out an empirical study evaluating theperformance of the proposed algorithm.
For compar-ison, the following methods are evaluated.
(1) Theproposed algorithm TopSpace (1), where the data arerepresented in topic space.
(2) TexSpace algorithm,where the data are represented in original text space.For each representation, two classification schemes areapplied: a) FDA (Eq.
4), followed by the nearest neigh-bor rule.
b) SVMs with Gaussian kernels (Cristianiniand Shawe-Taylor (2000)).
For the nearest neighborrule, the number of nearest neighbors is set to dn/5e,where n denotes the number of training examples.
ForSVMs, kernel width and soft margin parameters are setto default values.5.2 Data SetsThe following data sets are used to evaluate the perfor-mance of the proposed technique.
These data sets haveenough examples from both idioms and literals to makeour results meaningful.
On average, the training data is6K word tokens.
Our test data is of a similar size.BlowWhistle: This data set has 78 examples, 27 ofwhich are idioms and the remaining 51 are literals.
Thetraining data for BlowWhistle consist of 40 randomlychosen examples (20 paragraphs containing idioms and20 paragraphs containing literals).
The remaining 38examples (7 idiomatic and 31 literals) are used as testdata.MakeScene: This data set has 50 examples, 30 ofwhich are paragraphs containing idioms and the re-maining 20 are paragraphs containing literals.
Thetraining data for MakeScene consist of 30 randomlychosen examples, 15 of which are paragraphs contain-ing make scene as an idiom and the rest 15 are para-graphs containing make scene as a literal.
The remain-ing 20 examples (15 idiomatic paragraphs and 5 liter-als) are used as test data.LoseHead: This data set has 40 examples, 21 ofwhich are idioms and the remaining 19 are literals.The training data for LoseHead consist of 30 randomlychosen examples (15 idiomatic and 15 literal).
Theremaining 10 examples (6 idiomatic and 4 literal) areused as test data.TakeHeart: This data set has 81 examples, 61 ofwhich are idioms and the remaining 20 are literals.
Thetraining data for TakeHeart consist of 30 randomlychosen examples (15 idiomatic and 15 literals).
Theremaining 51 examples (46 idiomatic and 5 literals) areused as test data.5.3 Adding affectNunberg et al.
(1994) notice that ?idioms are typicallyused to imply a certain evaluation or affective stancetoward the things they denote?.
Language users usu-ally choose an idiom in non-neutral contexts.
The situ-ations that idioms describe can be positive or negative;however, the polarity of the context is not as impor-tant as the strength of the emotion expressed.
So, wedecided to incorporate the knowledge about the emo-tion strength into our algorithm.
We use a database ofword norms collected by Warriner et al.
(2013).
Thisdatabase contains almost 14,000 English lemmas an-notated with three components of emotions: valence(the pleasantness of a stimulus), arousal (the intensityof emotion provoked by a stimulus), and dominance2023Table 1: Average accuracy of competing methods on four datasets in single paragraph contexts: A = ArousalModel BlowWhistle LoseHead MakeScene TakeHeartPrec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall AccFDA-Topics 0.44 0.40 0.79 0.70 0.90 0.70 0.82 0.97 0.81 0.91 0.97 0.89FDA-Topics+A 0.51 0.51 0.75 0.78 0.68 0.66 0.80 0.99 0.80 0.93 0.84 0.80FDA-Text 0.37 0.81 0.63 0.60 0.88 0.58 0.82 0.89 0.77 0.36 0.38 0.41FDA-Text+A 0.42 0.49 0.76 0.64 0.92 0.63 0.83 0.95 0.82 0.75 0.53 0.53SVMs-Topics 0.08 0.39 0.59 0.28 0.25 0.45 0.59 0.74 0.61 0.91 1.00 0.91SVMs-Topics+A 0.06 0.21 0.69 0.38 0.18 0.44 0.53 0.40 0.44 0.91 1.00 0.91SVMs-Text 0.08 0.39 0.59 0.36 0.60 0.52 0.23 0.30 0.40 0.42 0.16 0.22SVMs-Text+A 0.15 0.51 0.60 0.31 0.38 0.48 0.37 0.40 0.45 0.95 0.48 0.50(the degree of control exerted by a stimulus).
Thesecomponents were elicited from human subjects via anAmazon Mechanical Turk crowdsourced experiment.We only used the arousal feature in our experimentsbecause we were interested in the intensity of the emo-tion rather than its valence.For a document d = {w1, ?
?
?
, wN}t, we calculatethe corresponding arousal value aifor each wi, ob-taining dA= {a1, ?
?
?
, aN}t. Let mAbe the aver-age arousal value calculated over the entire trainingdata.
The centered arousal value for a training docu-ment is obtained by subtractingmAfrom dA, i.e.,?dA=dA?mA= {a1?mA, ?
?
?
, aN?mA}t. Similarly, thecentered arousal value for a query is computed accord-ing to q?A= qA?mA= {q1?mA, ?
?
?
, qN?mA}t.That is, the training arousal mean is used to center bothtraining and query arousal values.
The correspondingarousal matrices for D,?D, and Q are AD, A?D, AQ, re-spectively.
To incorporate the arousal feature, we sim-ply compute?D= MD+AD, (5)and?
?D= M?D+A?D.
(6)The arousal feature can be similarly incoporated intoquery ?Q= MQ+AQ.6 ResultsTable 1 shows the average precision, recall, and ac-curacy of the competing methods on the four datasets over 10 runs in simple paragraph contexts.
Table2 shows the results for the multi-paragraph contexts.Note that for single paragraph contexts, we chose twotopics, each having 10 terms.
For multi-paragrah con-texts, we had four topics, with 10 terms per topic.
Nooptimization was made for selecting the number of top-ics as well as the number of terms per topic.
In thetables, the best performance in terms of the sum of pre-cision, recall and accuracy is given in boldface.The results show that the topic representationachieved the best performance in 6 out of 8 cases.
Fig-ure 2 plots the overall aggregated performance in termsof topic vs text representations across the entire datasets, regardless of the classifiers used.
Everything elsebeing equal, this clearly shows the advantage of topicsover simple text representation.Precision Recall Accuracy00.10.20.30.40.50.60.70.8TopicsTextFigure 2: Aggregated performance: Topic vs text rep-resentations.The arousal feature (Eqs 5 and 6) also improved theoverall performance, particularly in text representation(Eq.
5).
This can be seen in the top panel in Figure 3.In fact, in 2/8 cases, text representation coupled withthe arousal feature achieved the best performance.
Onepossible explanation is that the LDA model already per-formed ?feature?
selection (choosing topic terms), tothe extent possible.
Thus, any additional informationsuch as arousal only provides marginal improvementat the best (bottom panel in Figure 3).
On the otherhand, original text represents ?raw?
features, wherebyarousal information helps provide better contexts, thusimproving overall performance.Figure 4 shows a case in point: the average (sorted)arousal values of idioms and literals of the target phrase?lose head.?
The upper panel plots arousal values inthe text space, while lower panel plots arousal valuesin the topic space.
The plot supports the results shownin Tables 1 and 2, where the arousal feature generallyimproves text representation.7 Comparisons with other approachesEven though we used Fazly et al.
(2009)?s dataset forthese experiments, the direct comparison with theirmethod is impossible here because our task is formu-lated differently and we do not use the full dataset forthe experiments.
Fazly et al.
(2009)?s unsupervised2024Table 2: Average accuracy of competing methods on four datasets in multiple paragraph contexts: A = ArousalModel BlowWhistle LoseHead MakeScene TakeHeartPrec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall AccFDA-Topics 0.62 0.60 0.83 0.76 0.97 0.78 0.79 0.95 0.77 0.93 0.99 0.92FDA-Topics+A 0.47 0.44 0.79 0.74 0.93 0.74 0.82 0.69 0.65 0.92 0.98 0.91FDA-Text 0.65 0.43 0.84 0.72 0.73 0.65 0.79 0.95 0.77 0.46 0.40 0.42FDA-Text+A 0.45 0.49 0.78 0.67 0.88 0.65 0.80 0.99 0.80 0.47 0.29 0.33SVMs-Topics 0.07 0.40 0.56 0.60 0.83 0.61 0.46 0.57 0.55 0.90 1.00 0.90SVMs-Topics+A 0.21 0.54 0.55 0.66 0.77 0.64 0.42 0.29 0.41 0.91 1.00 0.91SVMs-Text 0.17 0.90 0.25 0.30 0.50 0.50 0.10 0.01 0.26 0.65 0.21 0.26SVMs-Text+A 0.24 0.87 0.41 0.66 0.85 0.61 0.07 0.01 0.26 0.74 0.13 0.20Precision Recall Accuracy00.10.20.30.40.50.60.7TextText+APrecision Recall Accuracy00.10.20.30.40.50.60.70.8TopicsTopics+AFigure 3: Aggregated performance: Textvs.
text+Arousal representations (top) and Top-ics vs. Topics+Arousal representations (bottom).model that relies on the so-called canonical forms gives72.4% (macro-)accuracy on the extraction of idiomatictokens when evaluated on their test data.We cannot compare our method directly with theother methods discussed in section 2 either becauseeach uses a different dataset or formulates the taskdifferently (detection vs. recognition vs. identifica-tion).
However, we can compare the method presentedhere with Feldman and Peng (2013) who also experi-ment with LDA, use similar data, and frame the prob-lem as classification.
Their goal, however, is to clas-sify sentences as either idiomatic or literal.
To obtaina discriminant subspace, they train their model on asmall number of randomly selected idiomatic and non-idiomatic sentences.
They then project both the train-ing and the test data on the chosen subspace and usethe three nearest neighbor (3NN) classifier to obtainaccuracy.
The average accuracy they report is 80%.0 5 10 15?1.4?1.2?1?0.8?0.6?0.4?0.200.2Text TermsArousal ValuesText RepresentationIdiomsLiterals0 5 10 15?1.5?1?0.500.51Arousal ValuesTopic TermsTopic RepresentationIdiomsLiteralsFigure 4: Average arousal values?Upper panel: Textspace.
Lower panel: Topic space.Our method clearly outperforms the Feldman and Peng(2013) approach (at least on the dataset we use).8 Discussion and ConclusionWe have described an algorithm for automatic classi-fication of idiomatic and literal expressions.
We haveinvestigated the bag of words topic representation fortarget documents (segments of one or three paragraphsthat contains a target phrase).
The approach definitelyoutperforms the baseline model that is based on thesimple bag of words representation, but it also outper-forms approaches previously discussed in the literature.Our model captures the local semantics and thus is ca-pable to identify semantic outliers (=idioms).While we realize that the data set we use is small, theresults are encouraging.
We notice that using 3 para-graphs for local contexts improves the performance ofthe classifiers.
The reason is that some paragraphs are2025relatively short.
A larger context provides more relatedterms, which gives LDA more opportunities to samplethese terms.Idioms are also relatively non-compositional.
Whilewe do not measure their non-compositionality in thisapproach, we indirectly touch upon this property by hy-pothesizing that non-compositional idiomatic expres-sions are likely to be far from the local topics.We feel that incorporating the intensity of emotionexpressed by the context into our model improves per-formance, in particular, in text representation.
Whenwe performed a qualitative analysis of the results try-ing to determine the causes of false positives and neg-atives, we noticed that there were quite a number ofcases that improved after incorporating the arousal fea-ture into the model.
For example, the FDA:topic classi-fier labels ?blow the whistle?
as literal in the followingcontext, but FDA:topics+A marks this expression as id-iomatic (italicized words indicate words with relativelyhigh arousal values):Peter thought it all out very carefully.
He decided the wis-est course was to pool all he had made over the last two years,enabling Julian to purchase the lease of a high street property.This would enable them to set up a business on a more set-tled and permanent trading basis.
Before long they opened agrocery-cum-delicatessen in a good position as far as passingtrade was concerned.
Peter?s investment was not misplaced.The business did very well with the two lads greatly appreci-ated locally for their hard work and quality of service.
Therange of goods they were able to carry was welcomed in thearea, as well as lunchtime sandwich facilities which had pre-viously been missing in the neighbourhood.Success was the fruit of some three years?
strenuous work.But it was more than a shock when Julian admitted to Pe-ter that he had been running up huge debts with their bank.Peter knew that Julian gambled, but he hadn?t expected himto gamble to that level, and certainly not to use the shop assecurity.
With continual borrowing over two years, the bankhad blown the whistle.
Everything was gone.
Julian wasbankrupt.
Even if they?d had a formal partnership, whichthey didn?t, it would have made no difference.
Peter lost allhe?d made, and with it his chance to help his parents and hisyounger brother and sister, Toby and Laura.Peter was heartbroken.
His father had said all along: nei-ther a lender nor a borrower.
Peter had found out the hardway.
But as his mother observed, he was the same Peter, he?dpick himself up somehow.
Once again, Peter was resolute.
Hemade up his mind he?d never make the same mistake twice.
Itwasn?t just the money or the hard work, though the waste ofthat was difficult enough to accept.
Peter had been workinga debt of love.
He?d done all this for his parents, particularlyfor his father, whose dedication to his children had alwaysimpressed Peter and moved him deeply.
And now it had allcome to nothing.Therefore, we think that idioms have the tendency toappear in more affective contexts; and we think that in-corporating more sophisticated sentiment analysis intoour model will improve the results.AcknowledgmentsThis material is based upon work supported by the Na-tional Science Foundation under Grant No.
1319846.We also thank the anonymous reviewers for usefulcomments.
The third author thanks the Fulbright Foun-dation for giving her an opportunity to conduct this re-search at Montclair State University (MSU).ReferencesAriel, M. (2002).
The demise of unique concept ofliteral meaning.
Journal of Pragmatics 34, 361?402.Birke, J. and A. Sarkar (2006).
A clustering approachto the nearly unsupervised recognition of nonliterallanguage.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL?06), Trento, Italy, pp.329?226.Blei, D., A. Ng, and M. Jordan (2003).
Latent Dirich-let Allocation.
Journal of Machine Learning Re-search 3, 993?1022.Burnard, L. (2000).
The British National Corpus UsersReference Guide.
Oxford University Computing Ser-vices.Cilibrasi, R. and P. M. B. Vit?anyi (2007).
Thegoogle similarity distance.
IEEE Trans.
Knowl.
DataEng.
19(3), 370?383.Cook, P., A. Fazly, and S. Stevenson (2007).
Pullingtheir weight: Exploiting syntactic forms for the auto-matic identification of idiomatic expressions in con-text.
In Proceedings of the ACL 07 Workshop on ABroader Perspective on Multiword Expressions, pp.41?48.Cook, P., A. Fazly, and S. Stevenson (2008, June).
TheVNC-Tokens Dataset.
In Proceedings of the LRECWorkshop: Towards a Shared Task for MultiwordExpressions (MWE 2008), Marrakech, Morocco.Cowie, A. P., R. Mackin, and I. R. McCaig (1983).
Ox-ford Dictionary of Current Idiomatic English, Vol-ume 2.
Oxford University Press.Cristianini, N. and J. Shawe-Taylor (2000).
An In-troduction to Support Vector Machines and otherkernel-based learning methods.
Cambridge, UK:Cambridge University Press.Fazly, A., P. Cook, and S. Stevenson (2009).
Unsu-pervised Type and Token Identification of IdiomaticExpressions.
Computational Linguistics 35(1), 61?103.Feldman, A. and J. Peng (2013).
Automatic detec-tion of idiomatic clauses.
In Computational Linguis-tics and Intelligent Text Processing, pp.
435?446.Springer.Fukunaga, K. (1990).
Introduction to statistical patternrecognition.
Academic Press.Hearst, M. A.
(1992).
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th Conference on Computational Linguistics- Volume 2, COLING ?92, Stroudsburg, PA, USA,pp.
539?545.
Association for Computational Lin-guistics.2026Horvath, B. M. (1985).
Variation in Australian English.Cambridge: Cambridge University PRess.Katz, G. and E. Giesbrech (2006).
Automatic Iden-tification of Non-compositional Multiword Expres-sions using Latent Semantic Analysis.
In Proceed-ings of the ACL/COLING-06 Workshop on Multi-word Expressions: Identifying and Exploiting Un-derlying Properties, pp.
12?19.Li, L. and C. Sporleder (2010).
Using Gaussian Mix-ture Models to Detect Figurative Language in Con-text.
In Proceedings of NAACL/HLT 2010.Newman, D., E. V. Bonilla, and W. L. Buntine (2011).Improving topic coherence with regularized topicmodels.
In NIPS, pp.
496?504.Nunberg, G., I.
A.
Sag, and T. Wasow (1994).
Idioms.Language 70(3), 491?538.Pado, S. and M. Lapata (2007).
Dependency-basedconstruction of semantic space models.
Computa-tional Linguistics 33(2), 161?199.Peng, J., A. Feldman, and L. Street (2010).
Comput-ing linear discriminants for idiomatic sentence de-tection.
Research in Computing Science, Special is-sue: Natural Language Processing and its Applica-tions 46, 17?28.Sag, I.
A., T. Baldwin, F. Bond, A. Copestake, andD.
Flickinger (2002).
Multiword expressions: APain in the Neck for NLP.
In Proceedings of the3rd International Conference on Intelligence TextProcessing and Computational Linguistics (CICLing2002), Mexico City, Mexico, pp.
1?15.Seaton, M. and A. Macaulay (Eds.)
(2002).
CollinsCOBUILD Idioms Dictionary (second ed.).
Harper-Collins Publishers.Sporleder, C. and L. Li (2009).
Unsupervised Recogni-tion of Literal and Non-literal Use of Idiomatic Ex-pressions.
In EACL ?09: Proceedings of the 12thConference of the European Chapter of the Associa-tion for Computational Linguistics, Morristown, NJ,USA, pp.
754?762.
Association for ComputationalLinguistics.Warriner, A.
B., V. Kuperman, and M. Brysbaert(2013).
Norms of valence, arousal, and dominancefor 13,915 english lemmas.
Behavior ResearchMethods 44(4).Widdows, D. and B. Dorow (2005).
Automatic extrac-tion of idioms using graph analysis and asymmet-ric lexicosyntactic patterns.
In Proceedings of theACL-SIGLEX Workshop on Deep Lexical Acquisi-tion, DeepLA ?05, Stroudsburg, PA, USA, pp.
48?56.
Association for Computational Linguistics.Woods, A., P. Fletcher, and A. Hughes (1986).
Statis-tics in Language Studies.
Cambridge: CambridgeUniversity Press.2027
