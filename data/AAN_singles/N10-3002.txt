Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 7?12,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsOn Automated Evaluation of Readability of Summaries:Capturing Grammaticality, Focus, Structure and CoherenceRavikiran VadlapudiLanguage Technologies Research CenterIIIT Hyderabadravikiranv@research.iiit.ac.inRahul KatragaddaLanguage Technologies Research CenterIIIT Hyderabadrahul k@research.iiit.ac.inAbstractReadability of a summary is usually gradedmanually on five aspects of readability: gram-maticality, coherence and structure, focus,referential clarity and non-redundancy.
In thecontext of automated metrics for evaluationof summary quality, content evaluations havebeen presented through the last decade andcontinue to evolve, however a careful exami-nation of readability aspects of summary qual-ity has not been as exhaustive.
In this paperwe explore alternative evaluation metrics for?grammaticality?
and ?coherence and struc-ture?
that are able to strongly correlate withmanual ratings.
Our results establish that ourmethods are able to perform pair-wise rank-ing of summaries based on grammaticality, asstrongly as ROUGE is able to distinguish forcontent evaluations.
We observed that noneof the five aspects of readability are indepen-dent of each other, and hence by addressingthe individual criterion of evaluation we aim toachieve automated appreciation of readabilityof summaries.1 IntroductionAutomated text summarization deals with both theproblem of identifying relevant snippets of informa-tion and presenting it in a pertinent format.
Auto-mated evaluation is crucial to automatic text sum-marization to be used both to rank multiple partic-ipant systems in shared tasks1, and to developerswhose goal is to improve the summarization sys-tems.
Summarization evaluations help in the cre-ation of reusable resources and infrastructure; it setsup the stage for comparison and replication of re-sults by introducing an element of competition toproduce better results (Hirschman and Mani, 2001).1The summarization tracks at Text Analysis Conference(TAC) 2009, 2008 and its predecessors at Document Under-standing Conferences (DUC).Readability or Fluency of a summary is categor-ically measured based on a set of linguistic qual-ity questions that manual assessors answer for eachsummary.
The linguistic quality markers are: gram-maticality, Non-Redundancy, Referential Clarity,Focus and Structure and Coherence.
Hence read-ability assessment is a manual method where ex-pert assessors give a rating for each summary onthe Likert Scale for each of the linguistic qualitymarkers.
Manual evaluation being time-consumingand expensive doesn?t help system developers ?who appreciate fast, reliable and most importantlyautomated evaluation metric.
So despite having asound manual evaluation methodology for readabil-ity, there is an need for reliable automatic metrics.All the early approaches like Flesch Reading Ease(Flesch, 1948) were developed for general texts andnone of these techniques have tried to character-ize themselves as approximations to grammatical-ity or structure or coherence.
In assessing read-ability of summaries, there hasn?t been much ofdedicated analysis with text summaries, except in(Barzilay and Lapata, 2005) where local coherencewas modeled for text summaries and in (Vadlapudiand Katragadda, 2010) where grammaticality of textsummaries were explored.
In a marginally relatedwork in Natural Language Generation, (Mutton etal., 2007) addresses sentence level fluency regard-less of content, while recent work in (Chae andNenkova, 2009) gives a systematic study on howsyntactic features were able to distinguish machinegenerated translations from human translations.
Inanother related work, (Pitler and Nenkova, 2008)investigated the impact of certain surface linguisticfeatures, syntactic, entity coherence and discoursefeatures on the readability of Wall Street Journal(WSJ) Corpus.
We use the syntactic features usedin (Pitler and Nenkova, 2008) as baselines for ourexperiments on grammaticality in this paper.7While studying the coherence patterns in studentessays, (Higgins et al, 2004) identified that gram-matical errors affect the overall expressive qualityof the essays.
In this paper, due to the lack of an ap-propriate baseline and due to the interesting-ness ofthe above observation we use metrics for grammat-icality as a baseline measure for structure and co-herence.
Focus of a summary, is the only aspect ofreadability that relies to a larger extent on the contentof the summary.
In this paper, we use Recall Ori-ented Understudy of Gisting Evaluation (ROUGE)(Lin, 2004) based metrics as one of the baselines tocapture focus in a summary.2 Summary GrammaticalityGrammaticality of summaries, in this paper, is de-fined based on the grammaticality of its sentences,since it is more a sentence level syntactic property.A sentence can either be grammatically correct orgrammatically incorrect.
The problem of grammati-cal incorrectness should not occur in summaries be-ing evaluated because they are generated mostly byextract based summarization systems.But as the distribution of grammaticality scoresin Table 1 shows, there are a lot of summaries thatobtain very low scores.
Hence, We model the prob-lem of grammaticality as ?how suitable or accept-able are the sentence structures to be a part of asummary?
?.The acceptance or non acceptance of sentencestructures varies across reviewers because of vari-ous factors like usage, style and dialects.
Hence, wedefine a degree to which a sentence structure is ac-ceptable to the reviewers, this is called the degree ofacceptance throughout this paper.Grammaticality Score 1 2 3 4 5Percentage Distribution (in %) 10 13 15 37 25Table 1: Percentage distribution of grammaticality scoresin system summariesIn this paper, the degree of acceptance of sen-tence structures is estimated using language mod-els trained on a corpus of human written sum-maries.
Considering the sentence structures in ref-erence summaries as the best accepted ones (withhighest degree of acceptance), we estimate the de-gree of acceptance of sentences in system gener-ated summaries by quantifying the amount of sim-ilarity/digression from the references using the lan-guage models.The structure of the sentences can be representedby sequences of parts-of-speech (POS) tags andchunk tags.
Our previous observations (Vadlapudiand Katragadda, 2010) show that the tagset sizeplays an important role in determining the degreeof acceptance.
In this paper, we combine the twofeatures of a sentence ?
the POS-tag sequence andchunk-tag sequence ?
to generate the POS-Chunk-tag training corpus.Some aspects of grammatical structure are wellidentifiable at the level of POS tags, while someother aspects (such as distinguishing between appos-itives and lists for eg.)
need the power of chunk tags,the combination of these two tag-sequences providesthe power of both.Hence, the following approaches use probabilis-tic models, learned on POS tag corpus and POS-Chunk tag corpus, in 3 different ways to determinethe grammaticality of a sentence.2.1 Enhanced Ngram modelAs described in our previous work, the Ngrammodel estimates the probability of a sentence to begrammatically acceptable with respect to the corpususing language models.
Sentences constructed us-ing frequent grammar rules would have higher prob-ability and are said to have a well accepted sentencestructure.
The grammaticality of a summary is com-puted asG(Sum) = AV G(P (Seqi)) ; P (Seqi) = log( n???
?n?j=1P (Kj))P (Kj) = P (tj?2tj?1tj)P (t1t2t3) = ?1 ?
P (t3|t1t2) + ?2 ?
P (t3|t2) + ?3 ?
P (t3)where G(Sum) is grammaticality score of a sum-mary Sum and G(Si) is grammaticality of sentenceSi which is estimated by the probability (P (Seqi))of its POS-tag sequence (Seqi).
P (Kj) is proba-bility of POS-tag trigram Kj which is tj?2tj?1tjand ?tj , tj ?
POS tags.
The additional tagst?1, t0 and tn+1 are the beginning-of-sequence andend-of-sequence markers.
The average AV G ofthe grammaticality scores of sentences P (Seqi) ina summary gives the final grammaticality score ofthe summary.
In the prior work, arithmetic meanwas used as the averaging technique, which per-forms consistently well.
However, here two otheraveraging techniques namely geometric mean and8harmonic mean are experimented and based on ourexperiments, we found geometric mean perform-ing better than the other two averaging techniques.All the results reported in this paper are based ongeometric mean.
The above procedure estimatesgrammaticality of sentence using its POS tags andwe call this run ?Ngram (POS)?.
A similar proce-dure is followed to estimate grammaticality using itsPOS-Chunk tags (language models trained on POS-chunk-tag training corpus).
The corresponding runis called ?Ngram (POS-Chunk)?
in the results.2.2 Multi-Level Class modelIn this model, we view the task of scoring grammati-cality as a n-level classification problem.
Grammat-icality of summaries is manually scored on a scaleof 1 to 5, which means the summaries are classi-fied into 5 classes.
We assume that each sentence ofthe summary is also rated on a similar scale whichcumulatively decides to which class the summarymust belong.
In our approach, sentences are classi-fied into 5 classes on the basis of frequencies of un-derlying grammar rules (trigram) by defining classboundaries on frequencies.
Hence, the cumulativescore of the rules estimate the score of grammatical-ity of a sentence and inturn the summary.Similar to (Vadlapudi and Katragadda, 2010), tri-grams are classified into 5 classes C1, C2, C3, C4and C5 and each class is assigned a score on a sim-ilar scale (?jscore(Cj) = j) and class boundariesare estimated using the frequencies of trigrams inthe training corpus.
The most frequent trigram, forexample, would fall into class C5.
POS-Class se-quences are generated from POS-tag sequences us-ing class boundaries as shown in Figure 1.
This isthe first level of classification.Figure 1: Two-level class modelLike the first level of classification, a series ofclassifications are performed upto ?k?
levels.
At eachlevel we apply the scoring method described belowto evaluate the grammaticality of summaries.
Weobserved that from 3rd level onwards the structuraldissimilarity disappears and the ability to distinguishdifferent structures is lost.
Hence, we report on thesecond level of classification, that captures the gram-matical acceptability of summaries very well, andFigure 1 explains the two level classification.G(Si) = AV G(H(Cw1),H(Cw2), ....,H(Cwn)) (1)AVG is the average of H(Cwi), where w1, w2,.
.
.
wn are class trigrams, Cwi is the class into whichclass trigram wi falls into and H(Cwi) is score as-signed to the class Cwi.
The AV G is computed us-ing geometric mean and this run is referred as ?Class(POS 2 level)?
in the results.Similar to above approach, the grammaticalityof a sentence can also be estimated using POS-Chunk tag sequence and POS-Chunk Class trainingdata, and the corresponding run is referred as ?Class(POS-Chunk 2 level)?.2.3 Hybrid ModelAs would be later seen in Table 2, the Ngram (POS)and Class (POS 2 level) runs are able to distin-guish various systems based on grammaticality.
Wealso note that these runs are able to very finelydistinguish the degree of grammaticality at sum-mary level.
This is a very positive result, one thatshows the applicability of applying these methodsto any test summaries in this genre.
To fully uti-lize these methods we combine the two methodsby a linear combination of their scores to form a?hybrid model?.
As seen with earlier approaches,both the POS-tag sequences and POS-Chunk-tag se-quences could be used to estimate the grammatical-ity of a sentence, and hence the summary.
These tworuns are called ?Hybrid (POS)?
and ?Hybrid (POS-Chunk)?, respectively.3 Structure and CoherenceMost automated systems generate summaries frommultiple documents by extracting relevant sentencesand concatenating them.
For these summaries to becomprehensible they must also be cohesive and co-herent, apart from being content bearing and gram-matical.
Lexical cohesion is a type of cohesion thatarises from links between words in a text (Hallidayand Hasan, 1976).A Lexical chain is a sequence of9such related words spanning a unit of text.
Lexi-cal cohesion along with presuppositions and impli-cations with world knowledge achieves coherence intexts.
Hence, coherence is what makes text semanti-cally meaningful, and in this paper, we also attemptto automate the evaluation of the ?structure and co-herence?
of summaries.We capture the structure or lexical cohesion of asummary by constructing a lexical chain that spansthe summary.
The relation between entities (nounphrases) in adjacent sentences could be of typecenter-reference (pronoun reference or reiteration),or based on semantic relatedness (Morris and Hirst,1991).
A center-reference relation exists if an en-tity in a sentence is a reference to center in adjacentsentence.
Identifying centers of reference expres-sions can be done using a co-reference resolutiontool.
Performance of co-reference resolution tools insummaries, being evaluated, is not as good as theirperformance on generic texts.
Semantic relatednessrelation cannot be captured by using tools likeWord-net because they are not very exhaustive and henceare not effective.
We use a much richer knowledgebase to define this relation ?
Wikipedia.Coherence of a summary is modelled by its struc-ture and content together.
Structure is captured bylexical chains which also give information about fo-cus of each sentence which inturn contribute to thetopic focus of the summary.
Content presented inthe summary must be semantically relevant to thetopic focus of the summary.
If the content presentedby each sentence is semantically relevant to the fo-cus of the sentence, then it would be semanticallyrelevant to the topic focus of the summary.
As thefoci of sentences are closely related, a prerequisitefor being a part of a lexical chain, the summary issaid to be coherent.
In this paper, the semantic relat-edness of topic focus and content is captured usingWikipedia as elaborated in Section 3.1 of this paper.3.1 Construction of lexical chainsIn this approach, we identify the strongest lexicalchain possible which would capture the structure ofthe summary.
We define this problem of finding thestrongest possible lexical chain as that of finding thebest possible parts-of-speech tag sequence for a sen-tence using the Viterbi algorithm shown in (Brants,2000).
The entities (noun phrases) of each sentenceare the nodes and transition probabilities are definedas relatedness score (Figure 2).
The strongest lex-ical chain would have the highest score than otherpossible lexical chains obtained.Consider sentence Sk with entity set (e11, e12,e13, .
.
.
e1n) and sentence Sk+1 with entity set (e21,e22, e23, .
.
.
e2m).
Sentences Sk and Sk+1 are saidto be strongly connected if there exists entities e1i ?Sk and e2j ?
Sk+1 that are closely related.
e1i ande2j are considered closely related if?
e2j is a pronoun reference of the center e1i?
e2j is a reiteration of e1i?
e2j and e1i are semantically relatedPronoun reference In this approach, we resolvethe reference automatically by finding more thanone possible center for the reference expression us-ing Wikipedia.
Since the summaries are generatedfrom news articles, we make a fair assumption thatrelated articles are present in Wikipedia.
We en-sure that the correct center is one among the pos-sible centers through which Sk+1 and Sk+2 mightbe strongly connected.
Entities with query hits ra-tio ?
?
are considered as possible centers and entitye2j is replaced by entities that act as the possiblecenters.
Since the chain with the identified correctcenter is likely to have the highest score, our finallexical chain would contain the correct center.Query hit ratio =Query hits(e1i and e2j)Query hits(e1i)Reiteration Generally, an entity with a determinercan be treated as reiteration expression but not viceversa.
Therefore, we check whether e2j is actuallya reiteration expression or not, using query hits onWikipedia.
If Query hits (e2j) ?
?
then we con-sider it to be a reiteration expression.
A reiteratingexpression of a named entity is generally a commonnoun that occurs in many documents.
After identify-ing a reiteration expression we estimate relatednessusing semantic relatedness approach.Figure 2: Viterbi trace for identifying lexical chain10Semantic relatedness By using query hits overWikipedia we estimate the semantic relatedness oftwo entities.
Such an approach has been previouslyattempted in (Strube and Ponzetto, 2006).
Based onour experiments on grammaticality 2.2, classifyinginto 5 classes is better suited for evaluation tasks,hence we follow suit and classify semantic related-ness into 5 classes.
These classes indicate how se-mantically related the entities are.
Each class is as-signed a value that is given to the hits which fall intothe class.
For example, if query hits lie in the range(?1, ?2) or if query hit ratio is ?
?
then it falls intoclass k and is assigned a score equal to k.Now that we have computed semantic connect-edness between adjacent sentences using the meth-ods explained above, we identify the output nodewith maximum score (node V2 in Figure 2).
Thisnode with best score is selected and by backtack-ing the Viterbi path we generate the lexical chain forthe summary.
The constants ?, ?1, ?2and?
are deter-mined based on empirical tuning.3.2 CoherenceWe estimate coherence of the summary by estimat-ing how the sentences stick together and the seman-tic relevance of their collocation.
In a sentence, thesemantic relatedness of entities with the focus esti-mates score for the meaningfulness of the sentence,and the average score of all the sentences estimatesthe coherence of the summary.C(Summary) =?Ni=1G(si)NG(si) =?k?1j=1H(Q(F and eij))kWhere C(Summary) is the coherence of sum-mary Summary, and G(si) is the semantic relat-edness of a sentence si in Summary, while Q(q)denotes the number of query hits of query q. F isthe focus of si and eij is an entity in si, and H(Q)is the score of class into which query falls.4 EvaluationThis paper deals with methods that imitate manualevaluation metric for grammaticality and structureand coherence by producing a score for each sum-mary.
An evaluation of these new summarizationevaluation metrics is based on how well the systemrankings produced by them correlate with manualevaluations.
We use 3 types of correlation evalu-ations ?
Spearman?s Rank Correlation, Pearson?sCorrelation and Kendall?s Tau ?
each describingsome aspect of ordering problems.We used reference summaries from TAC 2008,2009 for the reference corpus and the experimentsdescribed were tested on DUC 2007 query-focusedmulti-document summarization datasets which have45 topics and 32 system summaries for each topicapart from 4 human reference summaries.Table 2 shows the system level correlations ofour approaches to grammaticality assessment withthat of human ratings.
We have used four base-line approaches: AverageNPs, AverageVPs, Aver-ageSBARs and AverageParseTreeHeight.
Our ap-proaches constitute of the following runs: Ngram(POS), Ngram (POS-Chunk), Class (POS 2 level),Class (POS-Chunk 2 level), Hybrid (POS), Hybrid(POS-Chunk).RUN Spearman?s ?
Pearson?s r Kendall?s ?BaselinesAverageNPs 0.1971 0.2378 0.1577AverageSBARs 0.2923 0.4167 0.2138AverageVPs 0.3118 0.3267 0.2225ParseTreeHeight 0.2483 0.3759 0.1922Our experimentsNgram (POS) 0.7366 0.7411 0.5464Ngram (POS+Chunk) 0.7247 0.6903 0.5421Class (POS 2 level) 0.7168 0.7592 0.5464Class (POS+Chunk 2 level) 0.7061 0.7409 0.5290Hybrid (POS) 0.7273 0.7845 0.5205Hybrid (POS+Chunk) 0.7733 0.7485 0.5810Table 2: System level correlations of automated and man-ual metrics for grammaticality.RUN Spearman?s ?
Pearson?s r Kendall?s ?ExperimentsNgram (POS) 0.4319 0.4171 0.3165Ngram (POS+Chunk) 0.4132 0.4086 0.3124Class (POS 2 level) 0.3022 0.3036 0.2275Class (POS+Chunk 2 level) 0.2698 0.2650 0.2015Hybrid (POS) 0.3652 0.3483 0.2747Hybrid (POS+Chunk) 0.3351 0.3083 0.2498Table 3: Summary level correlations of automated and manual met-rics for grammaticality .RUN Spearman?s ?
Pearson?s r Kendall?s ?BaselinesHuman Grammaticality rating 0.5546 0.6034 0.4152Ngram(POS) 0.3236 0.4765 0.2229ExperimentsOur coherence model 0.7133 0.5379 0.5173Table 4: System level correlations of automated and manual metricsfor coherence .Table 4 shows the system level correlations ofour approach to structure and coherence assessmentwith that of human ratings.
As mentioned earlier inSection 1, human ratings for grammaticality and our11RUN Spearman?s ?
Pearson?s r Kendall?s ?BaselinesHuman Grammaticality rating 0.5979 0.6463 0.4360Human Coherence rating 0.9400 0.9108 0.8196Ngram(POS) 0.4336 0.6578 0.3175Our coherence model 0.5900 0.5331 0.4125ROUGE-2 0.3574 0.4237 0.2681Table 5: System level correlations of automated and manual metricsfor focusbest performing system for grammaticality are usedas baselines for structure and coherence assessment.Again, like we previously mentioned, focus can beeasily characterized using structure and coherence,and to an extent the grammatical well-formedness.Also the focus of a summary is also dependent oncontent of the summary.
Hence, we use ROUGE-2, manual rating for grammaticality, manual ratingfor coherence, and our approaches to both grammat-icality and structure and coherence as baselines asshown in Table 5.5 Discussion and ConclusionIn this paper, we addressed the problem of identi-fying the degree of acceptance of grammatical for-mations at sentence level using surface features likeNgrams probabilities (in Section 2.1), and trigramsbased class Ngrams (in Section 2.2) and a hybridmodel using both Ngram and Class model (in Sec-tion 2.3), on the POS-tag sequences and POS-chunk-tag sequences which have produced impressive re-sults improving upon our previous work.Our approaches have produced high correlationsto human judgment on grammaticality.
Results inTable 2 show that the Hybrid approach on the POS-Chunk tag sequences outperforms all the other ap-proaches.
Our approaches to grammaticality assess-ment have performed decently at pair-wise rankingof summaries, shown by correlations of the order of0.4 for many runs.
This correlation is of the sameorder as that of similar figure for content evaluationsusing ROUGE and Basic Elements.Table 4 shows that our approach to the ?structureand coherence?
assessment outperforms the base-lines set and has an impressive correlation with man-ual ratings.
From Table 5 we found that grammati-cality is a good indicator of focus while we also ob-serve that structure and coherence forms a strongalternative to focus.The focus of this paper was on providing a com-plete picture on capturing the grammaticality as-pects of readability of a summary using relativelyshallow features as POS-tags and POS-Chunk-tags.We used lexical chains to capture structure and co-herence of summaries, whose performance also cor-related with focus of summaries.
None of the fiveaspects of readability are completely independent ofeach other, and by addressing the individual criteriafor evaluation we aim to achieve overall appreciationof readability of summary.ReferencesRegina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In ACL.Thorsten Brants.
2000.
Tnt: a statistical part-of-speechtagger.
In Proceedings of the sixth conference onApplied natural language processing, pages 224?231,Morristown, NJ, USA.
Association for ComputationalLinguistics.Jieun Chae and Ani Nenkova.
2009.
Predicting thefluency of text with shallow structural features: Casestudies of machine translation and human-written text.In EACL, pages 139?147.
The Association for Com-puter Linguistics.Rudolf Flesch.
1948.
A new readability yardstick.
Jour-nal of Applied Psychology, 32:221?233.M.A.K Halliday and Ruqayia Hasan.
1976.
Longmanpublishers.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspects of co-herence in student essays.
In HLT-NAACL 2004: MainProceedings, pages 185?192, Boston, Massachusetts,USA, May 2 - May 7.
Association for ComputationalLinguistics.Lynette Hirschman and Inderjeet Mani.
2001.
Evalua-tion.Chin-Yew Lin.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In the proceedings of ACLWorkshop on Text Summarization Branches Out.
ACL.Jane Morris and Graeme Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indicator of thestructure of text.
Comput.
Linguist., 17(1):21?48.Andrew Mutton, Mark Dras, Stephen Wan, and RobertDale.
2007.
Gleu: Automatic evaluation of sentence-level fluency.
In ACL.
The Association for ComputerLinguistics.Emily Pitler and Ani Nenkova.
2008.
Revisiting read-ability: A unified framework for predicting text qual-ity.
In EMNLP, pages 186?195.
ACL.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
computing semantic relatedness usingwikipedia.
In 21.
AAAI / 18.
IAAI 2006.
AAAI Press,july.Ravikiran Vadlapudi and Rahul Katragadda.
2010.Quantitative evaluation of grammaticality of sum-maries.
In CICLing.12
