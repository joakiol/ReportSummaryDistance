Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 168?173,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsComputational Exploration of the Linguistic Structures of Future-OrientedExpression: Classification and CategorizationAiming Nie1,2, Jason Shepard2, Jinho Choi1, Bridget Copley3, Phillip Wolff21Dept.
of Computer Science, Emory University,2Dept.
of Psychology, Emory University3Structures Formelles du Language, 4CNRS / Universite Paris 8, Paris, France 30322{anie, jason.s.shepard, jinho.choi, pwolff}@emory.edu, bridget.copley@sfl.cnrs.frAbstractEnglish, like many languages, uses a wide va-riety of ways to talk about the future, whichmakes the automatic identification of futurereference a challenge.
In this research we ex-tend Latent Dirichlet alocation (LDA) for usein the identification of future-referring sen-tences.
Building off a set of hand-designedrules, we trained a ADAGRAD classifier to beable to automatically detect sentences refer-ring to the future.
Uni-bi-trigram and syntac-tic rule mixed feature was found to provide thehighest accuracy.
Latent Dirichlet Allocation(LDA) indicated the existence of four majorcategories of future orientation.
Lastly, the re-sults of these analyses were found to correlatewith a range of behavioral measures, offeringevidence in support of the psychological real-ity of the categories.1 IntroductionEarly formal work on tense such as (Prior, 1967)treated tenses as logical operators; this approach,however, could not correctly account for com-plex tenses, and was superseded by relational ac-counts (Reichenbach, 1947; Hornstein, 1990; Klein,1997).
However, these frameworks too fall short tothe extent that they only posit three times (corre-sponding to the speech time, a reference time, anda time at which an event happens (Reichenbach?s S,R, and T respectively).
Natural language, however,can accommodate more than three times, as in Be-fore yesterday, Mary had been going to go to Parison Friday.
In a Reichenbachian system, the refer-ence time referred to by this sentence, would be yes-terday, but then not only is there the event time ofher going to Paris, but a time before yesterday isneeded for Mary?s plan as well.
The future orien-tation (that is, the future relationship between refer-ence time and event time) of such a sentence cannotbe modeled in Reichenbach?s system.
Such exam-ples indicate that a analysis with greater sensitivityto linguistic structure is needed if reference to thefuture is to be identified and modeled.In this paper we use the syntactic properties of asentence to identify references to the future.
We alsoexamine how references to the future might be diag-nostic of a person?s psychological wellbeing.
In par-ticular, we hypothesize that references to the futurereflect, in part, a person?s future-orientation, that isthe proportion of time a person?s thoughts concernthe future.Apparently, reference to future has sparked the in-terests of many Psychologists.
Recent researchessuggest that future-oriented thinking is linked tophysical and mental health, academic achieve-ment, increased social involvement, and lower dis-tress (Kahana et al, 2005; Aspinwall, 2005; Simonset al, 2004).While future-oriented thought appears to play acentral role in cognition, it?s identification in lan-guages such as English is not easily accomplished.As pointed out earlier, the absence of explicit andnecessary morphology for the encoding of futurereference often makes distinguish references to thefuture or present difficult to determine.The goal of this research is to develop proceduresfor the automated detection of references to the fu-ture, even in the context of a mix of verbs with differ-168ent tenses.
Such procedures will allow linguists andpsychologists to more effectively mine text from so-cial media to better extract chains and causation, aswell as, potentially determine a person?s or group?sstate of wellbeing.
To the best of our knowledge,this is the first time that a project of this kind hasbeen done in English, though similar research hasbeen conducted in Japanese (Nakajima et al, 2014).2 Related workDocument classification has been a long researchedtopic.
Tools and algorithms have been developedto enable people to classify pre-labeled documents.The approach in this paper is single-label text clas-sification using ADAGRAD (Duchi et al, 2011a).Later on, we explored Latent Dirichlet Model-ing (Blei et al, 2003) on the basis of induced sub-trees, which are commonly used in data mining, butnot frequently seen in Natural Language Processing.Frequent Subtree Mining is a common data min-ing topic.
Related algorithms such as TreeMiner,FreeQT have been developed to find most frequentstructure in a given tree bank (Chi et al, 2005).Similar approaches have been explored in Mos-chitti (2006)?s work on using subtrees as featuresfor Support Vector Machine.
We did not use his ap-proach because were were not interested in the sim-ilarity between tree structures, but rather in the lin-guistic regularities implicit in the text.
For this rea-son, we chose to use Varro algorithm developed byMartens (2010), to exhaustively generate subtrees.3 DataWe used data collected through Amazon Mechani-cal Turk (MTurk).
Participants were asked to writedown their mind wanderings as follows:Please think back to the last time you werethinking about something other than whatyou were currently doing.
Please sharewith us what you were thinking about.
Ifyou found yourself thinking about manydifferent things, please share with us asmany of these things that you can remem-ber.In addition to writing down their mind wanderings,participants (N = 795) also answered a series of be-havioral survey questions related to anxiety, health,happiness, life and financial satisfaction.
The taskresulted in a total of 2007 sentences.
Table 1 de-scribes the distribution of our data.The sentences were rated by three human raters.For each sentence, raters indicated whether the ex-pression referred to the future and their level of con-fidence of their decision.Sentence Subtree TokenFuture 867 164,772 11,910Not Future 1140 196,049 15,228Table 1: Total number of sentences, subtrees and tokensWe used the Stanford factored parser (Klein andManning, 2002) to parse sentences into constituencygrammar tree representations.
Tokens were gen-erated by a uni-bi-trigram mixed model.
Subtreestructures were generated using the Varro algo-rithm (Martens, 2010) with threshold k = 1 to in-clude lexicons.
For the future corpus, 2,529,040subtrees were processed while for the non-futurecorpus 2,792,875 were processed.
A subset of thesubtrees were selected as words for the LDA analy-sis, as described in Martens (2009).4 ExamplesWhile there are many cases of grammatical futuremarking (i.e., will, be going to) and lexical futuremeaning (e.g., plan, want, need, tomorrow, goal,ambition), many of the ways people use to refer tothe future do not fall into one of these two types oflinguistic categories.For example, as we have seen, it?s possible to havefuture reference without an obvious grammatical orlexical way of referring to the future.
One way ofdoing this is with so-called futurate sentences (Cop-ley, 2009; Kaufmann, 2005), such as Mary is goingto Paris, which can refer to a contextually-providedfuture time (e.g., tomorrow).
Another way to referto the future without grammatical or lexial means isto use a wh-question word with an infinitive, such asin I?m thinking about what to eat.
Such cases will bemissed by ngram approaches.Secondly, relying purely on lexical targets willnot work well when sense disambiguation is re-quired.
Modals in English can have multiple mean-ings (Palmer, 1986):169I was thinking about the local news be-cause they were showing what the weatherwould be like.I was thinking about my life and marriageand how much money or lack of plays arole in my obligations, and what my hus-band would do if I died.Both sentences have the modal word would.
Manycases of would are ?sequence-of-tense?
woulds, as inthe first sentence above.
That is, they should reallybe seen as will in the past; the past-tense markinginherent to would is functioning as a kind of tenseagreement with the main clause past.
The futureorientation provided by would is future with respectto the past reference time.
However, the would inthe second sentence is not a will of a past referencetime, but picks out a ?less-vivid?
future relative tothe present reference time (Iatridou, 2000).5 Classification5.1 Syntactic structural rulesWe used the constituency grammar rules generatedby Wolff and Copley.
Rules were generated onthe basis of linguistic theory, and then later refinedon the basis of analyses of the false positives andmisses.The rules were instantiated in the Tregex patternlanguage (Levy and Andrew, 2006), which couldthen be used to find matching structures in theparsed sentences.
There were 39 future-relatedrules, 16 past-related rules, and 3 present-relatedrules.
The rules varied from the purely syntacticto the lexical, with a number of rules containingof mix of both.
Syntactic information helpedto disambiguate the senses of the modal verbs.Fourteen of the future-related rules empha-sized the modal verbs.
Rules are released online athttps://github.com/clir/time-perception.5.2 Adaptive sub-gradient descentTo build statistical models, we used a stochasticadaptive subgradient algorithm called ADAGRADthat uses per-coordinate learning rates to exploitrarely seen features while remaining scalable (Duchiet al, 2011b).
This is suitable for NLP tasks whererarely seen features often play an important roleand training data consists of a large number of in-stances with high dimensional features.
We use theimplementation of ADAGRAD in ClearNLP (Choi,2013) using the hinge-loss, and the default hyper-parameters (learning rate: a = 0.01, terminationcriterion: r = 0.1).5.3 ExperimentsOur experiment consists of four parts.
First, we usedthe Tregex-based rule discussed in section 5.1 to de-termine whether the sentences referred to the future.Each sentence was matched against all rules, and anodd ratio score was calculated on the basis of theequation in (1).FutureFuture + Past + Present(1)We used this as our baseline classifier.
In the sec-ond part of the experiment, we converted the rulematches into vector: matches were coded as 1?s, ab-sences as 0?s.In the third part of the experiment, we used a moretraditional uni-bi-trigram mixed model as featuresfor ADAGRAD.
The extracted number of tokensfrom the corpus are represented in Table 1.
Finally,we mixed the ngram features with rule-based fea-tures to train the final classifier.
All classifiers weretrained through a 5-fold cross-validation process.
Inthe case of the human raters, we selected the labelthat was selected by 2 of the 3 raters.
Table 3 showsthe results of our classification.odd-ratio humanaccuracy 70.75 87.381Table 2: Simple Rule and Human Performance6 Categorization6.1 Induced subtreeThree types of subtrees are generally researched insubtree mining: bottom-up subtrees, induced sub-trees, and embedded subtrees.
They are rankedin order from the most restrictive to the most free1Due to the fact that the corpus was slowly built over a year,and confidence rating task was later added to the rating task,thus only tested over 1034 sentences.170rules ngram ngram + rules75.12 77.61 83.3371.14 81.09 78.8675.56 83.54 83.2974.81 79.30 82.0474.81 80.55 84.7974.29 80.42 82.46Table 3: 5-fold Cross-Validation: ADAGRAD ClassifierPerformance in Accuracyform.
Bottom-up subtree mining does not capturethe transformations of a sentence, while embeddedtree mining breaks a sentence structure down intounits that are often unhelpful.
Given these lim-itations, we used induced subtree mining, as re-comended in (Martens, 2009).After the initial extraction, we combined subtreesfrom the future, past, and present corpora to produce322,691 subtrees.
Each subtree?s weights were cal-culated using the frequency of the subtree appearingin the future corpus divided by total number of sen-tence in future corpus minus the same subtree ap-pearing in non-future corpora divided by total num-ber of sentences in non-future corpus.Linguists have long argued that syntactic con-structions encode meaning (Grimshaw, 1990; Levinand Hovav, 1995).
We argue that by using the sub-tree structures to represent a sentence, the compo-nents of meaning associated with a syntactic con-struction can be teased apart.
The components ofmeaning associated with these subtrees can then beinferred using procedures such as latent dirichlet allocation (LDA).6.2 Recursive LDAWe implemented a procedure called recursive LDAin which LDA was performed iteratively within newtopics.
One of the obstacles of modelling data usingLDA is that the number of topics must be chosenin advance.
Therefore it is very necessary to under-stand the properties of the data being modelled andchoose a number of categories appropriately.
Vari-ations and extensions of LDA should also be mod-elled to reflect the characteristics of the space andthe categories being modelled.
With this in mind, wehypothesize that the total future-oriented referencespace could be divided into a small number of cat-egories and within each semantic category, future-oriented reference relate to each other will formmore specific categories.
In comparison to a similarextension: hLDA (Griffiths and Tenenbaum, 2004),rLDA provides better control to researchers, and ismore suitable to discover categories on well-studiedproblems.To run rLDA, we selected subtrees with weightslarger than 0 (N = 21,156; 6.56% of the total gen-erated subtree structures) as our features (words)and sentences identified as referring to the future asour collections (N = 867)(documents).
Specifically,LDA was run on all of the subtrees with the goal ofdiscovering 2 topics.
The solution from this analysiswas then used to divide the subtrees into two groups,and LDA was subsequently run again on each set ofsubtrees.6.3 ExperimentsWe obtained 4 topics through two recursive run withLDA.
All of which have significant statistical corre-lations with behavioral data.
Two topics on the firstlevel are labeled as topic A and topic B.Figure 1: Recursive LDA Topic HierarchyThe main semantic difference between A and Bseemed to concern the distinction between open andfixed futures.
Sentences in topic A indicate far feweror more fixed choices, normally between just twochoices.
Sentences in topic B tend to include open-ended questions.
Example sentences from these twosub-types are shown below:Topic A - Fixed future:I was thinking that I should not be playingHay Day and I should do my work.Last night I decided that I should travel tomeet my aunt in Rhode Island as I haven?t171Topic AA Topic AB Topic BA Topic BBAge .055 .397** -.286** -.167Vividness .157 .199 -.266* -.100Anxiety-State .105 -.383** .260 -.041Anxiety-Trait .050 -.342* .247 -.008Financial Satisfaction .114 .326* -.364** -.032Control over Life .107 -.299** .149 .039Table 4: Correlation Table Between LDA Topics and Behavioral Data.
Due to the iterative design of our survey, we didnot have a complete behavioral question section till the end of our data collection.
146 people accounting for 18.36%of the total sample participated in the behavioral question research, and a subset of 81 people had future sentences intheir response.
Only content items that correlated with at least one category reported.
*p < .01, **p < .002seen her in a long time.Topic B - Open Future:At the same time I was thinking aboutwhat I was going to have for breakfast.I was thinking about what I would cook fordinner tonight.From the second level, more fine-grained topicsemerged.
Descending from topic A (fixed future),the two sub-types seemed to differ with respect tolevel of certainty: Topic AA tended to involve sen-tences conveying the notion of uncertainty, whileTopic AB tended to involve sentences implying cer-tainty.
From Table 4 People, who construct futuresentences with high certainty, have less control overlife, scored lower on the trait and state anxiety in-ventory (Spielberger, 2010).Topic AA - Uncertainty:I was thinking about a trip that I may takeat the end of the summer.I was wondering if we would end up to-gether and thinking about the fact thatsomething that can seem so certain nowmay not be in the future.Topic AB - Certainty:I was making my wife ?s lunch to take towork , and I was thinking about playinggolf this weekend .I am getting married in April , and there isa bunch of stuff left to be done .Topic B appeared to be mostly about an open future.Its sub-types seemed to differ with respect to the no-tion of constraint: Topic BA seemed to consist ofsentences about an unconstrained future while TopicBB seemed to concern sentences implying a con-strained future.
Our categorization matches with be-havioral data in Table 4.
People using unconstrainedfuture sentence constructs rated their future as lessvivid.
They also were younger and had lower finan-cial satisfaction.Topic BA - Unconstrained:I was thinking about what I should do forthe rest of the day.I was thinking about what I should ani-mate for my next cartoon.Topic BB - Constrained:Two hours ago I was debating what Ishould have for lunch and what I shouldwatch while I was eating.I was thinking about a girl I would like tomeet , what we would do , and how longwe would do it.7 ConclusionIn this research we leveraged recent developments inlinguistic theory (Iatridou, 2000; Condoravdi, 2002;Copley and Martin, 2014) to build an automatedsystem capable of discovering different ways of ex-pressing the future.
Specifically, we trained a ADA-GRAD classifier to a relatively high level of accuracyand examined the number of topics associated withreferences to the future through the use of recursive172LDA.
Finally, we established the psychological real-ity of our topics via comparisons to behavioral mea-sures.8 AcknowledgementsThis research was supported by a grant from the UPenn / John Templeton Foundation to B. Copley andP.
Wolff.ReferencesL.
G. Aspinwall.
2005.
The psychology of future-oriented thinking: From achievement to proactive cop-ing, adaptation, and aging.
Motivation and Emotion,29(4):203?235.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Yun Chi, Richard R Muntz, Siegfried Nijssen, andJoost N Kok.
2005.
Frequent subtree mining-anoverview.
Fundamenta Informaticae, 66(1):161?198.Jinho D Choi.
2013.
Clearnlp.Cleo Condoravdi.
2002.
Temporal interpretation ofmodals.
In David Beaver, Stefan Kaufmann, BradyClark, and Luis Casillas, editors, Stanford Papers onSemantics.
CSLI Publications, Palo Alto.Bridget Copley and Fabienne Martin, editors.
2014.Causation in Grammatical Structures.
Oxford Uni-versity Press.Bridget Copley.
2009.
The semantics of the future.
Rout-ledge.John Duchi, Elad Hazan, and Yoram Singer.
2011a.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.John Duchi, Elad Hazan, and Yoram Singer.
2011b.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
The Journal of MachineLearning Research, 12(39):2121?2159.DMBTL Griffiths and MIJJB Tenenbaum.
2004.
Hierar-chical topic models and the nested chinese restaurantprocess.
Advances in neural information processingsystems, 16:17.Jane Grimshaw.
1990.
Argument structure.
the MITPress.Norbert Hornstein.
1990.
As Time Goes By.
MIT Press.Sabine Iatridou.
2000.
The grammatical ingredients ofcounterfactuality.
LI, 31:231?270.E.
Kahana, B. Kahana, and J. Zhang.
2005.
Motivationalantecedents of preventive proactivity in late life: Link-ing future orientation and exercise.
Motivation andemotion, 29(4):438?459.Stefan Kaufmann.
2005.
Conditional truth and futurereference.
Journal of Semantics, 22(3):231?280, Au-gust.Dan Klein and Christopher D Manning.
2002.
Fast exactinference with a factored model for natural languageparsing.
In Advances in neural information processingsystems, pages 3?10.Wolfgang Klein.
1997.
Time in Language.
Routledge,New York.Beth Levin and Malka Rappaport Hovav.
1995.
Unac-cusativity: At the syntax-lexical semantics interface,volume 26.
MIT press.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In Proceedings of the fifth internationalconference on Language Resources and Evaluation,pages 2231?2234.
Citeseer.Scott Martens.
2009.
Quantitative analysis of treebanksusing frequent subtree mining methods.
In Proceed-ings of the 2009 Workshop on Graph-based Methodsfor Natural Language Processing, pages 84?92.
Asso-ciation for Computational Linguistics.Scott Martens.
2010.
Varro: an algorithm and toolkit forregular structure discovery in treebanks.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics: Posters, pages 810?818.
Associ-ation for Computational Linguistics.Alessandro Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In EACL, volume113, page 24.Yoko Nakajima, Michal Ptaszynski, Hirotoshi Honma,and Fumito Masui.
2014.
Investigation of future refer-ence expressions in trend information.
In Proceedingsof the 2014 AAAI Spring Symposium Series,Big databecomes personal: knowledge into meaning?For bet-ter health, wellness and well-being, pages 31?38.F.
R. Palmer.
1986.
Mood and modality.
CambridgeUniversity Press, Cambridge.Arthur Prior.
1967.
Past, Present, and Future.
OxfordUniversity Press, Oxford.Hans Reichenbach.
1947.
The tenses of verbs.
na.J.
Simons, M. Vansteenkiste, W. Lens, and M. Lacante.2004.
Placing motivation and future time perspectivetheory in a temporal perspective.
Educational Psy-chology Review, 16(2):121?139.Charles D Spielberger.
2010.
State-Trait Anxiety Inven-tory.
Wiley Online Library.173
