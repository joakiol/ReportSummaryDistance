Word Sense Disambiguation for Acquisition of SelectionalPreferencesDiana McCarthyCognitive & Computing SciencesUniversity of SussexBrighton BN1 9QH, UKdiana, mccarth y~ cogs.susx, ac.
uk7th May 1997AbstractThe selectional preferences of verbal predicates arean important component of lexical information use-ful for a number of NLP tasks including disambiglia-tion of word senses.
Approaches to selectional pref-erence acquisition without word sense disambigua-tion are reported to be prone to errors arising fromerroneous word senses.
Large scale automatic se-mantic tagging of texts in sufficient quantity forpreference acquisition has received little attention asmost research in word sense disambiguation has con-centrated on quality word sense disambiguation fa handful of target words.
The work described hereconcentrates on adapting semantic tagging methodsthat do not require a massive overhead of manualsemantic tagging and that strike a reasonable com-promise between accuracy and cost so that largeamounts of text can be tagged relatively quickly.The results of some of these adaptations are de-scribed here along with a comparison of the selec-tional preferences acquired with and without one ofthese methods.
Results of a bootstrapping approachare also outlined in which the preferences obtainedare used for coarse grained sense disambiguation a dthen the partially disambiguated data is fed backinto the preference acquisition system.
11This work was supported by CEC Telematics Appli-cations Programme project LE1-2111 "SPARKLE: Shal-low PARsing and Knowledge xtraction for LanguageEngineering".1 IntroductionThe automatic acquisition of lexical information iswidely seen as a way of overcoming the bottleneckof producing NLP applications (Zernik, 1991).
Theselectional preferences that predicates have for theirarguments provide useful information that can helpwith resolution of both lexical and structural ambi-guity and anaphora s well as being important foridentifying the underlying semantic roles in argu-ment slots.The work reported here concentrates on acquisi-tion for verbal predicates since verbs are of such ob-vious importance for the lexicon.
However it couldalso be applied to any other type of predication.The main contribution of this work is that it usesshallow parses produced by a fully automatic parserand that some word sense disambiguation (WSD)is performed on the heads collected from theseparses.
Most current research on selectional prefer-ence acquisition has used the Penn Treebank parses(Resnik, 1993a, 1993b; Ribas, 1995; Li & Abe, 1995;Abe & Li, 1996) These are obtained semi- automati-cally with a deterministic parser and manual correc-tion.
Additionally the other approaches do not per-form any WSD on the input data and most reporta major source of error arising from the contribu-tion of erroneous senses ometimes giving incorrectpreferences and at other times a noticeable ffect ofover-generalisation (Ribas, 1995; Resnik, 1993a).The relationship between selectional preferenceacquisition and WSD is a circular one.
One poten-tial use of selectional preferences i  WSD yet theiracquisition appears to require disarabiguated data.There are ways of breaking this circle.
This pa-52per describes work comparing the preferences ac-quired with and without semantic tagging of the in-put data.
The cost of word sense disambiguationis kept low enough to permit tagging of a sufficientquantity of data.
An iterative approach is also out-lined whereby the preferences so acquired are usedto disambiguate he argument heads which are thenfed back into the preference acquisition system.It is hoped that with enough data erroneoussenses can be filtered out as noise (Li & Abe, 1995).However tagged data should produce more appro-priate selectional restrictions provided the taggingis sufficiently accurate.
Tagging should be particu-larly useful in cases where the data is scarce.2 Previous Work2.1 Se lec t iona l  p re ference  acqu is i t ionOther approaches to selectional preference ac-quisition closely related to this are those ofResnik (Resnik, 1993b, 1993a) Ribas (1994, 1995),and Li and Abe (Li & Abe, 1995; Abe & Li, 1996) 2.All use a class based approach using the WordNethypernym hierarchy (Beckwith, Felbaum, Gross, &Miller, 1991) as the noun classification and find-ing selectional preferences as sets of disjoint nounclasses (not related as descendants of one another)within the hierarchy.
The key to obtaining goodselectional preferences is obtaining classes at an ap-propriate level of generalisation.
These researchersalso use variations on the association score given inequation 1, the log of which gives the measure frominformation theory known as mutual information.This measures the "relatedness" between two words,or in the class-based work on selectional preferencesbetween a class (c) and the predicate (v).A(c, v) = P(clv) p(c) (1)In comparison to the conditional distributionp(clv ) of the predicate (v) and noun class (c) theassociation score takes into account the marginaldistribution of the noun class so that higher valuesare not obtained because the noun happens to occur2I shall refer to the work in papers (Li & Abe, 1995) and(Abe & Li, 1996) as "Li and Abe" throughout, since the twopieces of work relate to each other and both involve the sametwo authorsmore frequently irrespective of context.
The condi-tional distribution might, for example, bias a classcontaining "people" as the direct object of "fly" incomparison to the class of "BIRDS" simply becausethe "PEOPLE" class occurs more in the corpus tostart with.l~esnik and Pdbas both search for disjoint classeswith the highest score.
Since smaller classes will fitthe predicate better and will hence have a higherassociation value they scale up the mutual informa-tion value by the conditional distribution giving theassociation score in equation 2.
The conditional dis-tribution will be larger for larger classes and so inthis way they hope to obtain an appropriate l vel ofgenerallsation.A(v,c) P(clv) = e(clvl log 1- (21The work described here uses the approach of Liand Abe who rather than modifying the associationscore use a principle of data compression from infor-mation theory to find the appropriate l vel of gen-eralisation.
This principle is known as the Mini-mum Description Length (MDL) principle.
In theirapproach selectional preferences are represented asa set of classes or a "tree cut" across the hierar-chy which dominates all the leaf nodes exhaustivelyand disjointly.
The tree cut features in a model,termed an "association tree cut model" (ATCM)which identifies an association score for each of theclasses in the cut.
In the MDL principle the bestmodel is taken to be the one that minimises thesum of:1.
The Model Description Length - the number ofbits to encode the model2.
The Data Description Length - the number ofbits to encode the data in the model.In this way, rather than searching for the classeswith the highest association score, MDL searchesfor the classes which make the best compromise be-tween explaining the data well by having a high as-sociation score and providing as simple (general) amodel as possible and so minimising the model de-scription length.In all the systems described above the input is notdisambiguated with respect o word senses.
Resnikand Ribas both report erroneous word senses beinga major source of error.
Ribas explains that this53occurs because some individual nouns occur partic-ularly frequently as complements to a given verb andso all senses of these nouns also get unusually highfrequencies.Li and Abe place a threshold on class frequenciesbefore consideration fa class.
In this way they hopeto avoid the noise from erroneous enses.
In thispaper some modifications to Li and Abe's systemare described and a comparison is made of the useof some word sense disambiguation.2.2 Word  Sense D isambiguat ionSince the literature on WSD is vast there will be noattempt to describe the variety of current work here.Two approaches were investigated as possibleways for pretagging the head nouns that are used asinput to the preference acquisition system.
Thesewere selected for having a low enough cost to enabletagging of a sufficient amount of text.One strategy has been suggested by Wilks andStevenson i which the most frequent sense is pickedregardless of context.
In this work they distinguishsenses to the homograph level given the correct partof speech and report a 95% accuracy using the mostfrequent sense specified by LDOCE ranking.
Thisapproach as the advantage of simplicity and train-ing data is only needed for the estimation of oneparameter, the sense frequencies.The other approach selected was Yarowsky's un-supervised algorithm (1995).
This has the advan-tage that it does not require any manually taggeddata.
His approach relies on initial seed collocationsto discriminate senses that can be observed in a por-tion of the data.
This portion is then labelled ac-cordingly.
New collocations are extracted from thelabeUed sample and ordered by log-likelihood as inequation 3.
The new ordered collocations are thenused to relable the data and the system iterates be-tween observing and ordering new collocations andre-labelling the data until the stopping condition ismet.
The final decision list of collocations can thenbe used at run-time.p(senseAlcoll,)log-likelihood = log p(other_sensesicoll, ) (3)3 Experiments3.1 Word  Sense D isambiguat ionPreliminary experiments have been carried out us-ing adaptations of the two approaches mentionedabove.3.1.1 Experiment 1This experiment followed the approach of using thefirst sense regardless of context.
Wilks and Steven-son did this in order to disambiguate LDOCE ho-mographs.
Distinguishing between WordNet sensesis a much harder problem and so performance wasnot expected to be as good.The only frequency information available forWordNet senses, assuming large scale manual tag-ging is out of the question, is the portion of theBrown corpus that has been semantically taggedwith WordNet senses for the SemCor project (Miller,Leacock, Tengi, & Bunker, 1993).
Criteria were usedalongside this frequency data specifying when to usethe first sense and when to leave the ambiguity un-touched.
Two criteria were used initially:1.
Fi~EQ - a threshold on the frequency of the firstsense.
RATIO - a threshold ratio between the frequen-cies of the first sense and next most frequentsense.At first FREQ was set at 5 and RATIO at 2.The method was then evaluated against he man-ually tagged sample of the Brown corpus (200,000words of text) from which the frequency data wasobtained and two small manually tagged samplesfrom the LOB corpus (sample size of nouns 179)and the Wall Street Journal corpus (size 191 nouns).The results are shown in table 1.
As expected theperformance was superior when scored against hesame data from which the frequency estimates hadbeen taken.Further experimentation was performed using theLOB sample and varying FREQ and RATIO.
Ad-ditionally a third constraint was added (D).
In thisnouns identified on the SemCor project as being dif-ficult for humans to tag were ignored.The results are shown in table 2.
Although the re-suits indicate this is rather a limited method of dis-ambiguating it was hoped that it would improve the54Table 1: Threshold 5 ratio 2DATA RECALL PRECISIONBrown 61 86LOB(179)WSJ (191)44416968Table 2: Variation of thresholds on the LOB dataCRITERIA "l RECALL PRECISIONfreq 5 ratio 2 44 69freq 3 ratio 2 47 69freq 1 ratio 2 49 67freq 3 ratio 1.5 50 67freq 3 ratio 3 39 76freq 3 ratio 2 (D) 45 71selectional preference acquisition process whilst alsoavoiding a heavy cost in terms of human time (formanual tagging) or computer time (for unsupervisedtraining).
For the selectional preference acquisitionexperiments 4 and 6 described below it was decidedto use the criteria FREQ 3, RATIO 2 and D (ignoredifficult nouns).3.1.2 Experiment 2Yarowksy's unsupervised algorithm (1995) was alsoinvestigated using WordNet to generate the initialseed collocations.
This has the advantage that itdoes not rely on a quantity of handtagged data how-ever the time taken for training remains an issue.Without optimisation the algorithm took 15 min-utes of elapsed time for 710 citations of the word"plant".
Accuracy was reasonable considering a)the quantity of data used (a corpus of 90 millionwords compared with Yarowsky's 460 million) andb) the simplifications made, imparticular the use ofonly one type of collocation.
3On initial experimentation it was evident thatpredominant senses quickly became favoured.
Forthis reason the measure to order the decision list3The only collocation used was within a window of 10words either side of the target.
Other simplifications includethe use of a constant for smoothing, a rudimentary stoppingcondition, no use of the one sense per discourse strategy andno alteration of the parameters at run time.was changed from log-likelihood to a log of the ratioof association scores as show in equation 4WhereA( senseA,~o=n , collocation,)log A( other_senses~o=~ , collocation,) (4)A(  ollo ation,) = prob(   n  lcoUo ation,)prob( sense)(5)This helped overcome the bias of conditionalprobabilities towards the most frequent sense.
Re-call is 71% and precision is 72% when using the log-likelihood to order the decision list with a stoppingcondition that the tagged portion exceeds 95% of thetarget data.
The ratio of association scores compen-sates for the relative frequencies of the senses andon stopping the recall is 76% and precision is 78%Unfortunately evaluation on the target word"plant" was rather optimistic when contrasted withan evMuation on randomly selected targets involvingfiner word sense distinctions.
In a experiment 390mid-frequency nouns were trained and the algorithmused to disambiguate he same nouns appearing inthe SemCor files of the Brown corpus.
This pro-duced only 29% for both recall and precision whichwas only just better than chance.
An importantsource of error seems to have been the poor qualityof the automatically derived seeds.On account of the training time that would berequired Yarowsky's unsupervised algorithm wasabandoned for the purpose of tagging the argumentheads.
The Wilks and Stevenson style strategy waschosen instead because it requires torage of one pa-rameter only and is exceptionally easy to apply.
Amajor disadvantage for this approach is that lowerrank senses do not feature in the data at all.
It ishoped that this will not matter where we are col-lecting information from many heads in a particu-lar slot because any mistagging will be outweighedby correct aggings overall.
However this approachwould be unhelpful where we want to distinguishbehaviour for different word senses.
A potential useof Yarowky's algorithm might be verb sense distinc-tion.
The experiments outlined in the next sectionhave been conducted using verb form rather thansense.
If verbs sense distinction were to be per-formed it would be important o obtain the pref-erences for the different senses and would not beappropriate to lump the preferences together underthe predominant sense.
It is hoped that with some55alteration to the automatic seed derivation and al-lowance for a coarser grained distinction this wouldbe viable.a threshold of 0.1 was adhered to as this not onlyavoided noise but also reduced the search space.3.2 Acqu is i t ion  of  Select ional  Prefer-encesRepresentation a d acquisition of selectional pref-erences is based on Li and Abe's concept of anATCM.
The details of how such a model is acquiredfrom corpus data using WordNet and the MDL prin-ciple is detailed in the papers (Li & Abe, 1995; Abe& Li, 1996).The WordNet hypernym noun hierarchy is usedhere as it is available and ready made.
Using a re-source produced by humans has its drawbacks, par-ticularly that the classification is not tailored to thetask and data at hand and is prone to the inconsis-tencies and errors that beset any man-made l xicalresource.
Still the alternative of using an automati-cally clustered hierarchy has other disadvantages, aparticular problem being that techniques so far de-veloped often give rise to semantically incongruousclasses (Pereira, Tishby, & Lee, 1993).Calculation of the class frequencies i key to theprocess of acquisition of selectional preferences.
Liand Abe estimate class frequencies by dividing thefrequencies of nouns occurring in the set of syn-onyms of a class between all the classes in whichthey appear.
Class frequencies are then inheritedup the hierarchy.
In order to keep to their definitionof a "tree cut" all nouns in the hierarchy need tobe positioned at leaves.
WordNet does not adhereto this stipulation and so they prune the hierarchyat classes where a noun featured in the set of syn-onyms has occurred in the data.
This strategy wasabandoned in the work described here because somewords in the data belonged at root classes.
For ex-ample in the direct object of "build" one instanceof the word "entity" occurred which appears at oneof the roots in WordNet.
If the tree were prunedat the "ENTITY" class there would be no possibil-ity for the preference of "build" to distinguish be-tween the subclasses "LIFE FORM" and "PHYSI-CAL OBJECT".As an alternative strategy in this work, new leafclasses were created for every internal class in theWordNet hierarchy so that terminals only occurredat leaves but the detail of WordNet was left intact.Li and Abe's strategy of pruning at classes less than3.2.1 Experiments 3 and 4The input data was produced by the system de-scribed in (Briscoe ~ Carroll, 1997) and comprised2 million words of parsed text with argument headsand subcategorisation frames identified.
Only argu-ment heads consisting of common ouns, days of theweek and months and personal pronouns with theexception of "it" were used.
The personal pronounswere all tagged with the "SOMEONE" class whichis unambiguous in WordNet.
Selectional preferenceswere acquired for a handful of verbs using eithersubject or object position.
In experiment 3 classfrequencies were calculated in much the same wayas in Li and Abe's original experiments, dividing fre-quencies for each noun between the set of classes inwhich they featured as synonyms.
In experiment 4the nouns in the target slots were disambiguated us-ing the approach outlined in experiment 1.
Wherefrequency data was not available for the target wordthe word was simply treated as ambiguous and classfrequencies were calculated as in experiment 3.Since ATCMs have only been obtained for thesubject and object slot and for 10 target verbs noformal evaluation has been performed as yet.
In-stead the ATCMs were examined and some observa-tions are given below along with diagrams howingsome of the models obtained.
For clarity only someof the nodes have been shown and classes are la-belled with some of the synonyms belonging to thatclass in WordNet.In order to obtain the ATCMs "tree cut models"(TCMs) for the target slot, irrespective of verb areobtained.
A TCM is similar to an ATCM exceptthat the scores associated with each class on the cutare probabilities and should sum to 1.
The TCMsobtained for a given slot with and without WSDwere similar.In contrast ATCMs are produced with a smalldata set specific to the target verb.
The verbs inour target set having between 32 ('clean') and 2176('make') instances.
Because of this the noise fromerroneous enses is not as easily filtered and WSDdoes seem to make a difference although this de-pends on the verb and the degree of polysemy ofthe most common arguments.
"Eat" is a verb which selects trongly for its ob-56i .
.
.
.
"l , ~ Shaded boxes.
.
.
.
.
.
.
for new leaf created I ent i ty  1for internal nodeATCM no WED .
.
.
.
.
.
.
.
.
.
.
.
.
.
.  "
-"  - 2:" '/ATCM with WSIT .
.
.
.
.
.
.
.
i , '" ",Figure 1: ATCM for 'eat' Object slot.
.
.
.
.
.
.
.
.
.
.
ATCM without WaD.
.
.
.
.
1...8 .
.
.
.
.
.
_ .
.
.
.
.
.
.
ATCM with WaD<~.
.
.
.~  ~xmple  nouns undw lhe node In the cut~,,.~t, .?
link'* |cen~u hoAme " '~:~ '~ ' /  ' flgh:.
.
.
.
.  "
-  - - -  .
.
, .h ,A*Figure 2: ATCM for 'establish' Object slotject slot.
The ATCMs with and without WSD arepictured in figure I.
The ATCMs are similar butWSD gives slightly stronger scores to the appropri-ate nodes.
Additionally the NATURAL OBJECTclass changes from a slight preference in the ATCMwithout WSD to a score below 1 (indicating no ev-idence for a preference) with WSD.
WSD appearsto slightly improve the preferences acquired but thedifference is small.
The reasons are that there is apredominant sense of "eat" which selects stronglyfor its direct object and many of the heads in thedata were monosemous (e.g.
food, sandwich andpretzel).In contrast "establish" only has 79 instances andwithout any WSD the ATCM consists of the rootnode with a score of 1.8.
This shows that withoutWSD the variety of erroneous enses causes grossover-generalisation when compared to the cut withWSD as pictured in figure 2.
There are cases wherethe WSD is faulty and many heads are not coveredby the criteria outlined in experiment 1.
The head"right" for example contributes to a higher associa-tion score at the LOCATION node though its cor-rect sense really falls under the ABSTRACTIONnode.
However even with these inadequacies thecut with WSD appears to provide a reasonable setof preferences a opposed to the cut at the root nodewhich is uninformative.There was no distinction of verb senses for thepreferences acquired and the data and ATCM for"serve" highlights this.
"Serve" has a number ofsenses including the sense of "meet the needs of"or "set food on the table" or "undergo a due pe-riod'.
The heads in direct object position could onthe whole be identified as belonging to one or otherof these senses.
The ATCM with WSD is illustratedin figure 3 The GROUP, RELATION and MENTALOBJECT nodes relate to the first sense, the SUB-57STANCE to the second and the third sense to theSTATE and RELATION nodes.
The ATCM with-out WSD was again an uninformative cut at theroot.
Ideally preferences should be acquired respec-tive to verb sense otherwise the preferences for thedifferent predicates will be confused.Although formal evaluation has as yet to be per-formed the models examined so far with the crudeWSD seem to improve on those without.
This isespecially so in cases of sparse data.Some errors were due to the parser.
For exampletime adverbials uch as "the night before" weremistaken as direct objects when the parser failed toidentify the passive as in :-"... presented a lamb, killed the night before".Errors also arose because collocations such as"post office" were not recognised~as such.
Despitethese errors the advantages Of using automaticparsing are significant in terms of the quantity ofdata thereby made available and portability to newdomains.3.3 Word Sense  D isambiguat ion  us ingSe lec t ion  P re ferencesThe tree cuts obtained in experiments 3 and 4 havebeen used for WSD in a bootstrapping approachwhere heads, disambiguated by selectional prefer-ences, are then used as input data to the preferenceacquisition system.
WSD using the ATCMs sim-ply selects all senses for a noun that fall under thenode in the cut with the highest association scorewith senses for this word.
For example the senseof "chicken" under VICTUALS would be preferredover the senses under LIFE FORM when occurringas the direct object of "eat".
The granularity of theWSD depends on how specific the cut is.
The ap-proach has not been evaluated formally although wehave plans to so with SemCor.
A small evaluationhas been performed comparing the manually taggeddirect objects of "eat" with those selected using thecuts from experiment 3.
The coarse tagging is con-sidered correct when the identified senses containthe manually selected one.
This provides a recall of62% and precision of 93% which can be comparedto a baseline precision of 55% which is calculated asin equation 6Number.-Sensesu Under_Cut ~neHeads Number_SensesnTotal.Heads_Covered (6)Naturally this approach will work better for verbswhich select more strongly for their arguments.Further experiments have been conducted whichfeed the disambiguated heads back into the selec-tional preference acquisition system.3.3.1 Exper iments 5 and 6In experiment 5 cuts obtained in experiment 3,with-out any initial WSD, are used to disambiguate heheads before these are then fed back in.
In contrastexperiment 6 uses the cuts obtained with Wilks andStevenson style WSD from experiment 4 to disam-biguate the heads.
In both cases the cuts are onlyused to dis ambiguate the heads appearing with thetarget verb and the full data sample required for theprior distribution TCM is left as in experiments 3and 4.Where the verb selects trongly for its arguments,for example "eat", the cuts obtained in experiments5 and 6 were similar to those achieved with initialWilks and Stevenson WSD, for example both havethe effect of taking the class NATURAL OBJECTbelow 1 (i.e.
removing the weak indication of a pref-erence).In contrast where the quantity of data is sparseand the verb selects less strongly the cut obtainedfrom fully ambiguous data (experiment 5) is unhelp-ful for WSD.
However if the Wilks and Stevensonstyle disambiguation is performed on the initial datathe cuts in experiment 6 show great improvement onthose from experiment 4.
For example the ATCM inexperiment 6 for "establish" showed no preferencesfor the LOCATION and POSSESSION nodes wherepreferences in experiment 4 had arisen because of er-roneous word senses.4 ConclusionsFrom inspection of the ATCMs obtained so far itappears that even crude WSD does help the selec-tional preference acquisition especially in cases ofsparse data, however this still needs formal evalua-tion to verify whether the difference is significant.WSD is particularly useful when the quantity ofdata is small as is the case when collecting data for a58.
.
.
.
.
.
.
.
.
.
.
ATCM without WSD1.9 .
.
.
.
.
.
.
.
ATCM with WSD<ROOT> * Example nouns under the node In the nutf ~ 0 b ~ l  J ~ ~ubstan?e J ~,;:;'rf;;le~tA b 4bls?uet memoryskate purposeFigure 3: ATCM for 'serve' Object slotspecific predicate.
WSD selecting the most frequentsense regardless of context certainly seems to helpoverall despite mistakes.
The preferences are im-proved still further if art iterative approach is takenand the preferences produced with initial WSD areused to disambiguate he heads which cart then befed back into the preference acquisition system.
Thishas the effect of removing preferences caused by er-roneous enses.So far experiments u ing Yarowsky's unsupervisedalgorithm take too long for training each word toproduce semantic tagging of sufficient quantity oftext for preference acquisition but may be usefulfor disambiguation f target verbs, particularly withadaptations to aLlow a coarser grarmlarity than theexact WordNet sense.5 FutureThe importance of word sense disambiguation onthe input data needs to be subjected to formal eval-uation.Undoubtedly different underlying semantic roleswill occur in a specified argument slot and this willconfuse the issue.
It would be interesting to exam-ine the preferences acquired where the data used isspecific to the subca.tegorisation frame as well as tothe argument slot.
Naturally this cart only be doneif we have sufficient data to start with.Where there is insufficient data for a target verbit may be worth merging the data for similar verbs.The WordNet verb hierarchies might provide a use-ful classification for this purpose.Although WordNet is used here, the classificationcould easily be changed.
It would be interesting tocompare results from a re-implementation using analternative hierarchy more in tune with the corpusdata.
The hierarchy could be produced by humansor automatically.The encoding of the model and data for the MDLprinciple needs attention as this will affect the levelof generalisation.
As yet the description length hasassumed a tree rather than a DAG and it is apparentthat cuts at nodes with shared daughters will bepenalised in the current scheme.ReferenceAbe, N., & Li, H. (1996).
Learning word associationnorms using tree cut pair models..
Unpublished.
cmp-1g/9605029.Beckwith, It., Felbaum, C., Gross, D., & Miller, G. A.
(1991).WordNet: A \]exical database organised on psycholin-guistic principles.
In Zernik, U.
(Ed.
), Lexical Acqmss-twn: Exploltmg On-Line Resources to Bmld a Lexwon,pp.
211-232.
Lawrence Erlbaum Associates., HillsdaleNJ.Briscoe, T., & Caxroll, J.
(1997).
Automatic extraction of sub-categorization from corpora..
In Fifth Apphed NaturalLanguage Processing Con\]erence.Li, H., & Abe, N. (1995).
Generalizing case frames using athesaurus and the MDL principle.
In Proceedings ofRecent Advances m Natural Language Processing, pp.239-248.Miller, George, A., Leacock, C., Tengi, It., & Bunker, R. T.(1993).
A semantic oncordance..
In Proceedings ofthe ARPA Workshop on Human Language Technology.,pp.
303-308.
Morgan Kaufman.Pereira, F., Tishby, N., & Lee, L. (1993).
Distributional clus-tering of English words.
In Praceedmgs of the 31st59Annual Meeting of the Association for ComputationalLinguists., pp.
183-190.Resnik, P. (1993a).
Selection and Information: A Glass-BasedApproach to Lezical Relationships.
Ph.D. thesis, Uni-versity of Pennsylvania.Resnik, P. (1993b).
Semantic lasses and syntactic ambigu-ity.
In Proceedings of the ARPA Workshop on HumanLanguage Technology., pp.
278-283.
Morgan Kanfman.Rib,s, F. (1994).
Learning more appropriate selectional re-strictions.
Tech.
rep. 41, ACQUILEX-II.Ribas, F. (1995).
On learning more appropriate selectionalrestrictions..
In Proceedings of the Seventh Conferenceof the European Chapter of the Association \]or Com-putational Linguistics., pp.
112-118.Wilks, Y., & Stevenson, M. (1996).
The grammar of sense -is word-sense tagging much more than part-of-speechtagging?, cmp-lg/9607028.Yarowsky~ D. (1995).
Unsupervised word sense disambigua-tion rivaling supervised methods..
In Proceedings ofthe 83rd Annual Meeting of the Association for Com-putational Linguists., pp.
189-196.Zernik, U.
(1991).
Introduction..
In Zernik, U.
(Ed.
), LexicalAcquisition : Ezploiting On.Line Resources to Builda Lezicon., pp.
1-26.
Lawrence Erlbaum Associates.,Hillsdale NJ.60
