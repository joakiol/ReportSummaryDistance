Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9?12,New York, June 2006. c?2006 Association for Computational LinguisticsMuseli: A Multi-Source Evidence Integration Approach to Topic Seg-mentation of Spontaneous DialogueJaime Arguello Carolyn Ros?Language Technologies Institute Language Technologies InstituteCarnegie Mellon University Carnegie Mellon UniversityPittsburgh, PA 15213 Pittsburgh, PA 15213jarguell@andrew.cmu.edu cprose@cs.cmu.eduAbstractWe introduce a novel topic segmentationapproach that combines evidence of topicshifts from lexical cohesion with linguisticevidence such as syntactically distinct fea-tures of segment initial contributions.
Ourevaluation demonstrates that this hybridapproach outperforms state-of-the-art algo-rithms even when applied to loosely struc-tured, spontaneous dialogue.1 IntroductionUse of topic-based models of dialogue hasplayed a role in information retrieval (Oard et al,2004), information extraction (Baufaden, 2001),and summarization (Zechner, 2001).
However,previous work on automatic topic segmentation hasfocused primarily on segmentation of expositorytext.
We present Museli, a novel topic segmenta-tion approach for dialogue that integrates evidenceof topic shifts from lexical cohesion with linguisticindicators such as syntactically distinct features ofsegment initial contributions.Our evaluation demonstrates that approaches de-signed for text do not generalize well to dialogue.We demonstrate a significant advantage of Museliover competing approaches.
We then discuss whymodels based entirely on lexical cohesion fail ondialogue and how our algorithm compensates withother topic shift indicators.2 Previous WorkExisting topic segmentation approaches can beloosely classified into two types: (1) lexical cohe-sion models, and (2) content-oriented models.
Theunderlying assumption in lexical cohesion modelsis that a shift in term distribution signals a shift intopic (Halliday and Hassan, 1976).
The best knownalgorithm based on this idea is TextTiling (Hearst,1997).
In TextTiling, a sliding window is passedover the vector-space representation of the text.
Ateach position, the cosine correlation between theupper and lower region of the sliding window iscompared with that of the peak cosine correlationvalues to the left and right of the window.
A seg-ment boundary is predicted when the magnitude ofthe difference exceeds a threshold.One drawback to relying on term co-occurrenceto signal topic continuity is that synonyms or re-lated terms are treated as thematically-unrelated.One solution to this problem is using a dimension-ality reduction technique such as Latent SemanticAnalysis (LSA) (Landauer and Dumais, 1997).Two such algorithms for segmentation are de-scribed in (Foltz, 1998) and (Olney and Cai, 2005).Both TextTiling and Foltz?s approach measurecoherence as a function of the repetition of the-matically-related terms.
TextTiling looks for co-occurrences of terms or term-stems and Foltz usesLSA to measure semantic relatedness betweenterms.
Olney and Cai?s orthonormal basis ap-proach also uses LSA, but allows a richer represen-tation of discourse coherence, which is that coher-ence is a function of how much new information adiscourse unit (e.g.
a dialogue contribution) adds(informativity) and how relevant it is to the localcontext (relevance) (Olney and Cai, 2005).Content-oriented models, such as (Barzilay andLee, 2004), rely on the re-occurrence of patterns oftopics over multiple realizations of thematicallysimilar discourses, such as a series of newspaperarticles about similar events.
Their approach util-izes a hidden Markov model where states corre-spond to topics, and state transition probabilitiescorrespond to topic shifts.
To obtain the desired9number of topics (states), text spans of uniformlength (individual contributions, in our case) areclustered.
Then, state emission probabilities areinduced using smoothed cluster-specific languagemodels.
Transition probabilities are induced byconsidering the proportion of documents in whicha contribution assigned to the source cluster (state)immediately precedes a contribution assigned tothe target cluster (state).
Using an EM-like Viterbiapproach, each contribution is reassigned to thestate most likely to have generated it.3 Overview of Museli ApproachWe will demonstrate that lexical cohesion alonedoes not adequately mark topic boundaries in dia-logue.
Nevertheless, it can provide one meaning-ful source of evidence towards segmenting dia-logue.
In our hybrid Museli approach, we com-bined lexical cohesion with features that have thepotential to capture something about the linguisticstyle that marks shifts in topic: word-unigrams,word-bigrams, and POS-bigrams for the currentand previous contributions; the inclusion of at leastone non-stopword term (contribution of content);time difference between contributions; contributionlength; and the agent role of the previous and cur-rent contribution.We cast the segmentation problem as a binaryclassification problem where each contribution isclassified as NEW_TOPIC if the contribution in-troduces a new topic and SAME_TOPIC other-wise.
We found that using a Na?ve Bayes classifier(John & Langley, 1995) with an attribute selectionwrapper using the chi-square test for ranking at-tributes performed better than other state-of-the-artmachine learning algorithms, perhaps because ofthe evidence integration oriented nature of theproblem.
We conducted our evaluation using 10-fold cross-validation, being careful not to includeinstances from the same dialogue in both the train-ing and test sets on any fold so that the results wereport would not be biased by idiosyncratic com-municative patterns associated with individualconversational participants picked up by thetrained model.Using the complete set of features enumeratedabove, we perform feature selection on the trainingdata for each fold of the cross-validation sepa-rately, training a model with the top 1000 features,and applying that trained model to the test data.Examples of high ranking features confirm ourintuition that contributions that begin new topicsegments are syntactically marked.
For example,many typical selected word bigrams were indica-tive of imperatives, such as lets-do, do-the, ok-lets,ok-try, lets-see, etc.
Others included time orienteddiscourse markers such as now, then, next, etc.To capitalize on differences in conversationalbehavior between participants assigned to differentroles in the conversation (i.e., student and tutor inour evaluation corpora), we learn separate modelsfor each role in the conversation1.
This decision isbased on the observation that participants with dif-ferent agent-roles introduce topics with a differentfrequency, introduce different types of topics, andmay introduce topics in a different style that dis-plays their status in the conversation.
For instance,a tutor may introduce new topics with a contribu-tion that ends with an imperative.
A student mayintroduce new topics with a contribution that endswith a wh-question.4 EvaluationIn this section we evaluate Museli in comparisonto the best performing state-of-the-art approaches,demonstrating that our hybrid Museli approachout-performs all of these approaches on two differ-ent dialogue corpora by a statistically significantmargin (p < .01), in one case reducing the prob-ability of error as measured by Beeferman's Pk toonly 10% (Beeferman et al, 1999).4.1 Experimental CorporaWe used two different dialogue corpora for ourevaluation.
The first corpus, which we refer to as theOlney & Cai corpus, is a set of dialogues selected ran-domly from the same corpus Olney and Cai selectedtheir corpus from (Olney and Cai, 2005).
The secondcorpus is a locally collected corpus of thermodynamicstutoring dialogues, which we refer to as the Thermocorpus.
This corpus is particularly appropriate for ad-dressing the research question of how to automaticallysegment dialogue for two reasons: First, the explora-tory task that students and tutors engaged in together ismore loosely structured than many task oriented do-mains typically investigated in the dialogue commu-nity, such as flight reservation or meeting scheduling.Second, because the tutor and student play asymmetricroles in the interaction, this corpus allows us to explore1 Dissimilar agent-roles occur in other domains as well (e.g.Travel Agent and Customer)10how conversational role affects how speakers marktopic shifts.Table 1 presents statistics describing characteris-tics of these two corpora.
Similar to (Passonneauand Litman, 1993), we adopt a flat model of topic-segmentation for our gold standard based on dis-course segment purpose, where a shift in topic cor-responds to a shift in purpose that is acknowledgedand acted upon by both conversational agents.
Weevaluated inter-coder reliability over 10% of theThermo corpus mentioned above.
3 annotatorswere given a 10 page coding manual with explana-tion of our informal definition of shared discoursesegment purpose as well as examples of segmenteddialogues.
Pairwise inter-coder agreement wasabove 0.7 kappa for all pairs of annotators.Olney & CaiCorpusThermoCorpus# Dialogues 42 22Contributions/Dialogue195.40 217.90Contributions/Topic24.00 13.31Topics/Dialogue 8.14 16.36Words/Contribution28.63 5.12Table 1: Evaluation Corpora Statistics4.2 Baseline ApproachesWe evaluate Museli against the following algo-rithms: (1) Olney and Cai (Ortho), (2) Barzilay andLee (B&L), (3) TextTiling (TT), and (4) Foltz.As opposed to the other baseline algorithms,(Olney and Cai, 2005) applied their orthonormalbasis approach specifically to dialogue, and priorto this work, report the highest numbers for topicsegmentation of dialogue.
Barzilay and Lee?s ap-proach is the state of the art in modeling topicshifts in monologue text.
Our application of B&Lto dialogue attempts to harness any existing andrecognizable redundancy in topic-flow across ourdialogues for the purpose of topic segmentation.We chose TextTiling for its seminal contributionto monologue segmentation.
TextTiling and Foltzconsider lexical cohesion as their only evidence oftopic shifts.
Applying these approaches to dialoguesegmentation sheds light on how term distributionin dialogue differs from that of expository mono-logue text (e.g.
news articles).The Foltz and Ortho approaches require atrained LSA space, which we prepared as de-scribed in (Olney and Cai, 2005).
Any parametertuning for approaches other than our hybrid ap-proach was computed over the entire test set, giv-ing competing algorithms the maximum advantage.In addition to these approaches, we includesegmentation results from three degenerate ap-proaches: (1) classifying all contributions asNEW_TOPIC (ALL), (2) classifying no contribu-tions as NEW_TOPIC (NONE), and (3) classifyingcontributions as NEW_TOPIC at uniform intervals(EVEN), corresponding to the average referencetopic length (see Table 1).As a means for comparison, we adopt two evalua-tion metrics: Pk and f-measure.
An extensive argu-ment of Pk?s robustness (if k is set to ?
the averagereference topic length) is present in (Beeferman, et al1999).
Pk measures the probability of misclassifyingtwo contributions a distance of k contributions apart,where the classification question is are the two con-tributions part of the same topic segment or not?Lower Pk values are preferred over higher ones.
Itequally captures the effect of false-negatives andfalse-positives and it favors near misses.
F-measurepunishes false positives equally, regardless of thedistance to the reference boundary.4.3 ResultsResults for all approaches are displayed in Table2.
Note that lower values of Pk are preferred overhigher ones.
The opposite is true of F-measure.
Inboth corpora, Museli performed significantly betterthan all other approaches (p <  .01).Olney & Cai Corpus Thermo CorpusPk F Pk FNONE 0.4897 -- 0.4900 --ALL 0.5180 -- 0.5100 --EVEN 0.5117 -- 0.5132 --TT 0.6240 0.1475 0.5353 0.1614B&L 0.6351 0.1747 0.5086 0.1512Foltz 0.3270 0.3492 0.5058 0.1180Ortho 0.2754 0.6012 0.4898 0.2111Museli 0.1051 0.8013 0.4043 0.3693Table 2: Results on both corpora4.4 Error AnalysisResults for all approaches are better on the Ol-ney and Cai corpus than the Thermo corpus.
TheThermo corpus differs profoundly from the Olneyand Cai corpus in ways that very likely influencedthe performance.
For instance, in the Thermo cor-pus each dialogue contribution is an average of 5words long, whereas in the Olney and Cai corpus11each dialogue contribution contains an average of28 words.
Thus, the vector space representation ofthe dialogue contributions is much more sparse inthe Thermo corpus, which makes shifts in lexicalcoherence less reliable as topic shift indicators.In terms of Pk, TextTiling (TT) performed worsethan the degenerate algorithms.
TextTiling meas-ures the term-overlap between adjacent regions inthe discourse.
However, dialogue contributions areoften terse or even contentless.
This producesmany islands of contribution-sequences for whichthe local lexical cohesion is zero.
TextTilingwrongfully classifies all of these as starts of newtopics.
A heuristic improvement to preventTextTiling from placing topic boundaries at everypoint along a sequence of contributions failed toproduce a statistically significant improvement.The Foltz and the orthonormal basis approachesrely on LSA to provide strategic semantic gener-alizations.
Following (Olney and Cai, 2005), webuilt our LSA space using dialogue contributionsas the atomic text unit.
However, in corpora suchas the Thermo corpus, this may not be effectivebecause of the brevity of contributions.Barzilay and Lee?s algorithm (B&L) did notgeneralize well to either dialogue corpus.
One rea-son could be that such probabilistic methods re-quire that reference topics have significantly dif-ferent language models, which was not true in ei-ther of our evaluation corpora.
We also noticed anumber of instances in the dialogue corpora whereparticipants referred to information from previoustopic segments, which consequently may haveblurred the distinction between the language mod-els assigned to different topics.5 Current DirectionsIn this paper we address the problem of auto-matic topic segmentation of spontaneous dialogue.We demonstrated with an empirical evaluation thatstate-of-the-art approaches fail on spontaneous dia-logue because word-distribution patterns alone areinsufficient evidence of topic shifts in dialogue.We have presented a supervised learning algorithmfor topic segmentation of dialogue that combineslinguistic features signaling a contribution?s func-tion with lexical cohesion.
Our evaluation on twodistinct dialogue corpora shows a significant im-provement over the state of the art approaches.The disadvantage of our approach is that it re-quires hand-labeled training data.
We are currentlyexploring ways of bootstrapping a model from asmall amount of hand labeled data in combinationwith lexical cohesion (tuned for high precision andconsequently low recall) and some reliable dis-course markers.AcknowledgmentsThis work was funded by Office of Naval Re-search, Cognitive and Neural Science Division,grant number N00014-05-1-0043.ReferencesRegina Barzilay and Lillian Lee (2004).
Catching thedrift: Probabilistic Content Models, with Applicationsto Generation and Summarization.
In Proceedings ofHLT-NAACL 2004.Doug Beeferman, Adam Berger, John D. Lafferty(1999).
Statistical Models for Text Segmentation.Machine Learning 34 (1-3): 177-210.Narj?s Boufaden, Guy Lapalme, Yoshua Bengio (2001).Topic Segmentation: A first stage to Dialog-based In-formation Extraction.
In Proceedings of NLPRS 2001.P.W.
Foltz, W. Kintsch, and Thomas Landauer (1998).The measurement of textual cohesion with latent se-mantic analysis.
Discourse Processes, 25, 285-307.M.
A. K. Halliday and Ruqaiya Hasan (1976).
Cohesionin English.
London: Longman.Marti Hearst.
1997.
TextTiling: Segmenting Text intoMulti-Paragragh Subtopic Passages.
ComputationalLinguistics, 23(1), 33 ?
64.George John & Pat Langley (1995).
Estimating Con-tinuous Distributions in Bayesian Classifiers.
In Pro-ceedings of UAI 2005.Thomas Landauer, & Susan Dumais (1997).
A Solutionto Plato?s Problem: The Latent Semantic Analysis ofAcquisition, Induction, and Representation of Knowl-edge.
Psychological Review, 104, 221-240.Douglas Oard, Bhuvana Ramabhadran, and SamuelGustman (2004).
Building an Information RetrievalTest Collection for Spontaneous ConversationalSpeech.
In Proceedings of SIGIR 2004.Andrew Olney and Zhiqiang Cai (2005).
An Orthonor-mal Basis for Topic Segmentation of Tutorial Dia-logue.
In Proceedings of HLT-EMNLP 2005.Rebecca Passonneau and Diane Litman (1993).
Inten-tion-Based Segmentation: Human Reliability andCorrelation with Linguistic Cues.
In ProceedingsACL 2003.Klaus Zechner (2001).
Automatic Generation of Con-cise Summaries of Spoken Dialogues in UnrestrictedDomains.
In Proceedings of SIGIR 2001.12
