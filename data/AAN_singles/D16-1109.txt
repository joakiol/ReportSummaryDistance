Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1036?1041,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsJoint Transition-based Dependency Parsing and Disfluency Detection forAutomatic Speech Recognition TextsMasashi Yoshikawa and Hiroyuki Shindo and Yuji MatsumotoGraduate School of Information and ScienceNara Institute of Science and Technology8916-5, Takayama, Ikoma, Nara, 630-0192, Japan{ masashi.yoshikawa.yh8, shindo, matsu }@is.naist.jpAbstractJoint dependency parsing with disfluency de-tection is an important task in speech lan-guage processing.
Recent methods show highperformance for this task, although most au-thors make the unrealistic assumption that in-put texts are transcribed by human annota-tors.
In real-world applications, the input textis typically the output of an automatic speechrecognition (ASR) system, which implies thatthe text contains not only disfluency noises butalso recognition errors from the ASR system.In this work, we propose a parsing method thathandles both disfluency and ASR errors us-ing an incremental shift-reduce algorithm withseveral novel features suited to ASR outputtexts.
Because the gold dependency informa-tion is usually annotated only on transcribedtexts, we also introduce an alignment-basedmethod for transferring the gold dependencyannotation to the ASR output texts to con-struct training data for our parser.
We con-ducted an experiment on the Switchboard cor-pus and show that our method outperformsconventional methods in terms of dependencyparsing and disfluency detection.1 IntroductionSpontaneous speech is different from written text inmany ways, one of which is that it contains disfluen-cies, that is, parts of the utterance that are correctedby the speaker during the utterance.
NLP systemperformance is reported to deteriorate when thereare disfluencies, for example, with SMT (Cho et al,2014).
Therefore, it is desirable to preprocess thespeech before passing it to other NLP tasks.There are a number of studies that address theproblem of detecting disfluencies.
Some of thesestudies include dependency parsing (Honnibal andJohnson, 2014; Wu et al, 2015; Rasooli andTetreault, 2014), whereas others are dedicated sys-tems (Qian and Liu, 2013; Ferguson et al, 2015;Hough and Purver, 2014; Hough and Schlangen,2015; Liu et al, 2003).
Among these studies, Hon-nibal (2014) and Wu (2015) address this problem byadding a new action to transition-based dependencyparsing that removes the disfluent parts of the in-put sentence from the stack.
Using this approach,they achieved high performance in terms of both de-pendency parsing and disfluency detection on theSwitchboard corpus.However, the authors assume that the input textsto parse are transcribed by human annotators, which,in practice, is unrealistic.
In real-world applications,in addition to disfluencies, the input texts containASR errors; these issues might degrade the parsingperformance.
For example, proper nouns that are notcontained in the ASR system vocabulary may breakup into smaller pieces, yielding a difficult problemfor the parsing unit (Cheng et al, 2015):REF: what can we get at LitanfeethHYP: what can we get it leaks on feetIn this work, we propose a method for joint de-pendency parsing and disfluency detection that canrobustly parse ASR output texts.
Our parser handlesboth disfluencies and ASR errors using an incremen-tal shift-reduce algorithm, with novel features thatconsider recognition errors of the ASR system.Furthermore, to evaluate dependency parsing per-1036they  may      flip      flop  when  they  get  to  be  uh    N     N   olderthey  made  slipped  flop  when  they  get  to  be  uh  old  way  olderROOTTrans:error errorerrorASR: ROOTerror ASR-to-NULL:ASR output token aligns to NULL in gold transcription.NOT MATCH:Aligned tokensdoes not match on character.what  age  are  your  childrenwhat  age   N   your  childrenerror errorROOTROOTROOTTrans-to-NULL:Transcription token aligns to NULL in ASR output text.
(a) (b)Figure 1: Examples of three problematic cases.
Above shows the gold transcription and its tree, below shows the aligned ASRoutput and its newly transferred tree, where the dotted edges are ASR error edges.formance on real human utterances, we create a tree-annotated corpus that contains ASR errors.
12 Data CreationTo evaluate dependency parsing performance on realspeech texts, we must create a tree-annotated corpusof ASR output texts.Given a corpus that consists of speech data, tran-scription text and its syntactic annotation (e.g., theSwitchboard corpus), we first apply the ASR sys-tem to the speech data.
Next, we perform alignmentbetween the ASR output texts and the transcription.Then, we transfer the gold syntactic annotation tothe ASR output texts based on this alignment (Fig-ure 1).
The alignment is performed by minimizingthe edit distance between the two sentences.
We in-clude ?NULL?
tokens in this alignment to allow forsome tokens not having an aligned conterpart (?N?tokens in the Figure 1).In the constructed trees, there are three problem-atic cases based on how an ASR output text and itstranscription are aligned with each other: (1) a wordin the ASR output text aligns with a NULL tokenin the transcription (ASR-to-NULL), (2) a word inthe gold transcription aligns with a NULL in theASR output (Trans-to-NULL), and (3) two wordsalign, but do not match exactly in terms of characters(NOT MATCH).
To create a consistent dependencytree that spans the entire sentence, we must addresseach of these cases.1There are also studies that tackle the problem of disfluencydetection in the context of speech recognition such as (Liu et al,2003).
Our work is novel in that our aim is to extend the jointmethod of disfluency detection with dependency parsing so thatit can be applicable to the output of ASR system.2.1 ASR-to-NULLIn the case of ASR-to-NULL, a token from the ASRsystem has no corresponding token in the gold tran-scription.
In this case, we automatically annotate adependency relation with an ?error?
label such thatthe token?s head becomes the previous word token.Figure 1(a) shows an example of this case.
Inthe figure, the words ?old?
and ?way?
have no cor-responding words in the gold transcription.
Thus,we automatically annotate the dependency relationsbetween (?old?, ?uh?)
and (?way?, ?old?
), respec-tively, with the ?error?
label.2.2 Trans-to-NULLAlthough NULL tokens are introduced to facilitatealignment, as these tokens in the ASR output are notactual words, we must remove them in the final tree.Without any treatment, the gold transcription tokensaligned to these tokens are also deleted along withthem.
This causes the child tokens in the sentencenot to have heads; consequently, these child tokensare not included in the syntactic tree.
To avoid thisproblem, we instead attach them to the head of thedeleted token.For example, in Figure 1(b), the word ?are?
ismissing in the ASR hypothesis.
Then, this token?schildren lose their head in the transfer process.
Thus,we rescue these children by attaching them to thehead of ?are?, which, in this case, is ROOT token.If the head of the removed token is also of theTrans-to-NULL type, then we look for an alternativehead by climbing the tree in a recursive manner, un-til reaching ROOT.
We also label the newly creatededges in this process as ?error?.10372.3 NOT MATCHIn cases in which two aligned tokens do not matchexactly on the character level, the mismatch is re-garded as an instance of a substitution type of ASRerror.
Therefore, we encode this fact in the label ofthe arc from the token to its head.In Figure 1(a), the words ?made?
and ?slipped?
inthe ASR hypothesis do not match the gold transcrip-tion tokens, ?may?
and ?flip?, respectively.
There-fore, we automatically re-label the arc from each to-ken to its head as ?error?.3 Transition-based Dependency ParsingTo parse texts that contain disfluencies and ASR er-rors, we extend the ArcEager shift-reduce depen-dency parser of (Zhang and Nivre, 2011).
Our pro-posed parser adopts the same Shift, Reduce, LeftArc,and RightArc actions as ArcEager.
To this parser weadd three new actions, i.e., Edit, LeftArcError, andRightArcError, to handle disfluencies and ASR er-rors.Edit action removes a disfluent token when it isthe first element of the stack.
This is different fromHonnibal (2014)?s Edit action: theirs accumulatesconsecutive disfluent tokens on the top of the stackand removes them all at once, whereas our methodremoves this kind of token one-by-one.
Use of thisEdit action guarantees that the length of the actionsequence is always 2n?1.
This property is advanta-geous because the parser can use the standard beamsearch and does not require normalization, such asthose adopted in (Honnibal and Johnson, 2014) and(Zhu et al, 2013).LeftArcError and RightArcError act in the sameway as LeftArc and RightArc, except that these actonly on ASR error tokens, whereas the original Left-Arc and RightArc are reserved for non ASR error to-kens.
Using two different kinds of Arc actions forthe two types of tokens (ASR error or not) allowsfor the weights not to be shared between them, andis expected to yield improved performance.In the experiment below, we train all of the mod-els using structured perceptron with max violation(Huang et al, 2012).
The feature set is mainly basedon (Honnibal and Johnson, 2014), such as the dis-fluency capturing features to inquire whether the to-ken sequence inside the two specific spans match onword forms or POS tags.
We adjusted these featuresto inspect the content of the buffer more carefully,because our parser decides if the word token is dis-fluent or not every time new token is shifted andhints for the decision lies much more in the buffer.3.1 Backoff Action FeatureWith the newly proposed LeftArcError andRightArcError actions, we fear that the relativelylow frequency of ?error?
tokens may cause theweights for these actions to be updated too infre-quently to be accurately generalized.
We resortto using the ?backoff action feature?
to avoidthis situation.
This means that, for each actiona ?
{LeftArc, LeftArcError}, the score ofperforming it in a state s is calculated as follow:SCORE(a, s) = w ?
f(a, s) +w ?
f(a?, s) (1)where a?
= LeftArcBackoff, w is the weight vec-tor and f(?, ?)
is the feature representation, respec-tively.
LeftArcBackoff is not actual action per-formed by our parser, rather it is used to provide thecommon feature representation which both LeftArcand LeftArcError can ?back off?
to.
RightArc andRightArcError actions also calculate their scores asin Eq.
(1), with a?
= RightArcBackoff.
The scoresfor all the other actions are calculated in the normalway: SCORE(a, s) = w ?
f(a, s).3.2 WCN FeatureTo better capture which parts of the texts are likelyto be ASR errors, we use additional features ex-tracted from a word confusion network (WCN) gen-erated by ASR models.
Marin (2015) reports hisobservation that WCN slots with more arcs tend tocorrespond to erroneous region.
Following (Marin,2015), we use mean and standard deviation of arcposteriors and the highest arc posterior in eachWCNslot corresponding to each word token.
We includein the feature vector these real-valued features for to-kens on top of the stack and the first three elementsof the buffer.4 ExperimentWe conducted experiments using both the proposedparsing method and the tree-annotated corpus basedon the ASR output texts.
Our experiments were per-formed using the Switchboard corpus (Godfrey et1038al., 1992).
This corpus consists of speech data andits transcription texts, and subset of which is anno-tated with POS tags, syntactic trees and disfluencyinformation (repair, discourse marker and interjec-tion) based on (Shriberg, 1994).
24.1 ASR SettingsTo obtain the ASR output texts of the corpus,we used the off-the-shelf NeuralNet recipe (Zhanget al, 2014) presented by Kaldi.3 We used thejackknife method to obtain the ASR output textsthroughout the syntactically annotated part of thecorpus.
4From these ASR output texts, we created thetree-annotated corpus by applying the data creationmethod introduced in ?2.
Out of all 857,493 wordtokens, there are 32,606 ASR-to-NULL, 34,952Trans-to-NULL, and 93,138 NOT MATCH cases,meaning 15.6% of all word tokens had ?error?
la-beled arcs.4.2 Parsing SettingsWe assigned POS tags to the created corpus usingthe Stanford POS tagger (Toutanova et al, 2003)trained on a part of the gold Switchboard corpus.
5We adopt the same train/dev/test split as in (Hon-nibal and Johnson, 2014), although the data size re-duces slightly during the process of data creation.We report the unlabeled attachment score (UAS),which indicates how many heads of fluent tokens arecorrectly predicted.
As for disfluency detection, wereport precision/recall/F1-score values following theprevious work in the literature.As a baseline (To which we refer as Base in thefollowing), we use an ArcEager parser with our pro-posed Edit action and the disfluency capturing fea-tures, trained on the train part of the gold Switch-board corpus.
Using this parser on ASR output testdata can be seen as reproducing the typical situation,2We converted the phrase structure trees to dependency onesusing the Stanford converter (de Marneffe et al, 2006).3http://kaldi-asr.org/4The average Word Error Rate of resulting models were13.9 % on the Switchboard part of HUB5 evaluation dataset:https://catalog.ldc.upenn.edu/LDC2002S095We used a part of the corpus that is annotated with POS in-formation but not syntactic one.
The performance of the taggeris evaluated on the syntactically annotated part of the corpus;the tagger has an accuracy score of 95.0%.Model Dep DisflUAS Prec.
Rec.
F1Base 72.7 58.6 62.2 60.3+ ErrorAct 76.3 66.0 57.6 61.5+ Backoff 76.4 65.6 57.3 61.1+ WCN 76.2 67.9 57.9 62.5Table 1: Dependency parsing and disfluency detection resultsof the proposed methods.
We used our created corpus as bothtrain and test data.Train Test Model Dep DisflUAS Prec.
Rec.
F1Trans Trans Base 89.7 90.4 76.8 83.1Trans ASR Base 74.7 58.5 65.6 61.8ASR ASR Base 72.7 58.6 62.2 60.3ASR ASR Ours 76.2 67.9 57.9 62.5Table 2: Parsing result on different train-test settings.
Transrefers to original Switchboard transcription text, ASR the textcreated through the data creation in ?4.1.
Ours is our proposedparser: Base + ErrorAct + Backoff + WCN.in which a parser is trained on ASR-error-free texts,but nevertheless needs to parse the ASR output texts.4.3 Results and AnalysisIn Table 1, based on the baseline Base parser, wereport scores with the additional (and additive) useof Left/RightArcError actions (ErrorAct), the WCNfeature (WCN), and the backoff action feature (Back-off), on our created corpus.
Using ErrorAct resultedin 3.6% and 1.2% improvement in UAS and disflu-ency detection F1, respectively.
Backoff contributesto further improved UAS, whereas WCN cause anincrease in disfluency detection accuracy.Table 2 reports performance on various train andtest data settings.
In Table 2, the Train and Testcolumns represent which data to use in trainingand testing; Trans refers to the gold transcriptiontext of the Switchboard corpus, and ASR the textcreated through the data creation in ?4.1.
Whenevaluated on the ASR texts, the parser trained onthe ASR texts showed degraded performance com-pared to the parser trained on the gold transcription((Train,Test) = (ASR,ASR)).
Although both thetrain and test data are ASR texts and share character-istics, we did not observe domain adaptation effect.We hypothesized that the drop in the performance isdue to the noisy nature of our corpus, which is cre-ated from the texts with ASR errors.
Having ASR-1039error-specific actions, Left/RightArcError mitigatesthis problem by separately treating the ASR errortokens and non ASR error tokens.
Finally, with thenewly proposed features, the parser trained on ASRtexts outperforms the parser trained on the transcrip-tion texts with the improvement of 1.5% and 0.7%for UAS and disfluency detection, respectively.However, when compared with the case of(Train,Test) = (Trans, Trans), we observe sig-nificant decreases in performance in both of thetasks conducted on ASR texts.
This result clearlyposes a new challenge for the disfluency detectioncommunity.5 ConclusionIn this work, we have proposed a novel jointtransition-based dependency parsing method withdisfluency detection.
Using new actions, and newfeature set, the proposed parser can parse ASR out-put texts robustly.
We have also introduced a dataconstruction method to evaluate dependency parsingand disfluency detection performance for real speechdata.
As the experimental results for ASR texts issignificantly lower than that achieved for the goldtranscription texts, we have clarified the need to de-velop a method that is robust to recognition errors inthe ASR system.6 AcknowledgementsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlier draftof this paper.
This work was supported by JSPSKAKENHI Grant Number 15K16053, 26240035.ReferencesHao Cheng, Hao Fang, andMari Ostendorf.
2015.
Open-domain name error detection using a multi-task rnn.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages 737?746.
Association for Computational Linguistics.Eunah Cho, Jan Niehues, and Alex Waibel.
2014.
Tightintegration of speech disfluency removal into smt.
InProceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, volume 2: Short Papers (EACL), pages 43?47.Association for Computational Linguistics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InIn Proceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC).James Ferguson, Greg Durrett, and Dan Klein.
2015.Disfluency detection with a semi-markov model andprosodic features.
In Proceedings of the 2015 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (NAACL), pages 257?262.
Asso-ciation for Computational Linguistics.J.
J. Godfrey, E. C. Holliman, and J. McDaniel.
1992.?
switchboard: Telephone speech corpus for researchand development?.
In Acoustics, Speech, and Sig-nal Processing, 1992.
ICASSP-92., 1992 IEEE Inter-national Conference on (Volume:1 ).
Proc.
IEEE Int.Conf.
Acoust.
Speech Sig.
Proc.Matthew Honnibal and Mark Johnson.
2014.
Joint incre-mental disfluency detection and dependency parsing.In Transactions of the Association of ComputationalLinguistics Volume 2, Issue 1 (TACL), pages 131?142.Association for Computational Linguistics.Julian Hough and Matthew Purver.
2014.
Strongly in-cremental repair detection.
In Proceedings of the 2014Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 78?89.
Associa-tion for Computational Linguistics.Julian Hough and David Schlangen.
2015.
Recurrentneural networks for incremental disfluency detection.Interspeech 2015.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies.
Association forComputational Linguistics.Yang Liu, Elizabeth Shriberg, and Andreas Stolcke.2003.
Automatic disfluency identification in coversa-tional speech using multiple knowledge sources.
In InProceedings of the 8th Eurospeech Conference.Marius Alexandru Marin.
2015.
In Effective Use ofCross-Domain Parsing in Automatic Speech Recogni-tion and Error Detection.
Ph.D. thesis.
University ofWashington.Xian Qian and Yang Liu.
2013.
Disfluency detectionusing multi-step stacked learning.
In Proceedings ofthe 2013 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies.
Association for Compu-tational Linguistics.Mohammad Sadegh Rasooli and Joel Tetreault.
2014.Non-monotonic parsing of fluent umm i mean disfluentsentences.
In Proceedings of the 14th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, volume 2: Short Papers (EACL),1040pages 48?53.
Association for Computational Linguis-tics.Elizabeth Shriberg.
1994.
In Preliminaries to a The-ory of Speech Disfluencies.
Ph.D. thesis.
University ofCalifornia, Berkeley.Kristina Toutanova, Dan Klein, Christopher Manning,and Singer Yoram.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In In Pro-ceedings of the 2003 Human Language TechnologyConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.Shuangzhi Wu, Dongdong Zhang, Ming Zhou, andTiejun Zhao.
2015.
Efficient disfluency detection withtransition-based parsing.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: Long Pa-pers) (ACL), pages 495?503.
Association for Compu-tational Linguistics.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL), pages 188?193.
Associa-tion for Computational Linguistics.Xiaohui Zhang, Jan Trmal, Daniel Povey, and San-jeev Khudanpur.
2014.
Improving deep neural net-work acoustic models using generalized maxout net-works.
In IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP).Muhua Zhu, Yue Zhang, Wenliang Chen, Miu Zhang, andJingbo Zhu.
2013.
Fast and accurate shift-reduce con-stituent parsing.
In In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 434?443.
Association for Computa-tional Linguistics.1041
