61A Computational Model of Emergent Simple Syntax: Supporting the NaturalTransition from the One-Word Stage to the Two-Word Stage.Kris Jack, Chris Reed, and Annalu WallerDivision of Applied Computing,University of Dundee,Dundee, Scotland, DD1 4HN[kjack | creed | awaller]@computing.dundee.ac.ukAbstractThis paper introduces a system that simulatesthe transition from the one-word stage to thetwo-word stage in child language production.Two-word descriptions are syntacticallygenerated and compete against one-worddescriptions from the outset.
Two-worddescriptions become dominant as wordcombinations are repeatedly recognised,forming syntactic categories; resulting in anemergent simple syntax.
The systemdemonstrates a similar maturation as childrenas evidenced by phenomena such asoverextensions and mismatching, and the useof one-word descriptions being replaced bytwo-word descriptions over time.1 IntroductionStudies of first language acquisition in childrenhave documented general stages in linguisticdevelopment.
Neither the trigger nor themechanism that takes a child from one stage to thenext are known.
Stages arise gradually with noprecise start  or end points, overlapping oneanother (Ingram, 1989).The aim of this research is to develop a systemthat autonomously acquires conceptualrepresentations of individual words (the ?one-wordstage?)
and also, simultaneously, is capable ofdeveloping representations of valid multi-wordstructures i.e.
simple syntax (the ?two-wordstage?).
Two-word descriptions are expected toemerge as a result of the system state and not beartificially triggered.The system accepts sentences containing amaximum of two words.
It is designed to bescalable, allowing larger, more natural sentencesizes also.
System input is therefore a mixture ofboth one-word and two-word sentences.
Thesystem is required to produce valid descriptions,particularly in the two-word stage.
Rules thatenforce syntactic order, and allow for theproduction of semantically correct descriptionsfrom novel concepts, are desirable.This paper is sectioned as follows;  pre-one-word stage linguistic abilities in children arebriefly discussed to explain why initial systemfunctionality assumptions are made; the definingcharacteristics of both the one-word stage and two-word stage in children are introduced as possiblebenchmarks for the system; a detailed descriptionof system design and implementation withexamples of the learning process and games playedby the system are presented; a discussion of currentresults along with their possible implicationsfollows; a brief review of related works that haveinfluenced this research, citing major influences;the direction and aims of future research isdescribed briefly; and finally, conclusions aredrawn.2 Pre-One-Word Stage ChildrenLinguistic abilities can be found in children priorto word production.
In terms of comprehension,children can distinguish between their mother?svoice and a stranger?s voice, male and femalevoices, and sentences spoken in their mother?snative language and sentences spoken in a differentlanguage.
They also show categorical perceptionto voice, can use formant transition information tomark articulation, and show intonation sensitivity(Pinker, 1994, Jusczyk, 1999).In terms of production, children produce noises,such as discomfort noises (0-2 months), comfortnoises (2-4 months), and ?play?
vocally with pitchand loudness variations (4-7 months) (Pinker,1994).
The babbling stage (6-8 months) ischaracterised with the production of recognisablesyllables.
The syllables are often repeated, such as[mamama] and [papapa], with the easiest toproduce sounds often being associated withmembers of the family (Jakobson, 1971).From this evidence it is reasonable to drawconclusions about linguistic abilities in the youngchild that can be used to frame assumptions for usein the system.
It is assumed that the system canreceive and produce strings that can be brokendown into their component words.
These wordscan be compared and equalities can be detected.623 One-Word Stage and Two-Word StagesThe system is required to produce one-worddescriptions in early stages that develop into two-word descriptions, where appropriate, in latterstages..
The recognition of each stage is based onthe number of words that the system uses at aparticular point.
In children, the one and two-wordstages have notable features.The one-word, or holophrastic, stage (9-18months), is characterised by one-wordvocalisations that are consistently associated withconcepts.
These concepts can be either concrete orabstract, such as ?mama?, referring to the concreteconcept of the child?s mother, and ?more?, anabstract concept which can be applied in a varietyof situations (Piaget, 1960).Two phenomena that occur during this stage areunderextensions and overextensions.
Anunderextension is the formation of a word toconcept association that is too narrow, such as?dog?
referring to only the family dog.Overextension, similarly, is an association that istoo broad, such as ?dog?
referring to all fourlegged animals.
Mismatches, or idiosyncraticreferencing also occur, resulting in a word beingassociated with an unrelated concept, such as?dog?
referring to a table (Pinker, 1994).
Theseassociations change over time.The two-word stage (18-24 months) introducessimple syntax into the child?s language faculty.Children appear to determine the most importantwords in a sentence and, almost all of the time, usethem in the same order as an adult would(Gleitman and Newport, 1995).
Brown (1973)defines a typology to express semantic relations inthe two-word stage.
It contains ten sets ofrelations, but only one will be considered in thispaper; attribute  + entity (?red circle?).
During thisstage, children already demonstrate a three wordcomprehension level (Tomasello and Kruger,1992).
The concepts relating to their sentencesmay therefore be more detailed than the phrasesthemselves.The system is expected to make the transitionfrom the one-word stage to the two-word stagewithout changes to the functionality of the system.Once the system begins to run, input is restricted tothat of sensory (concept based) and vocal (stringrepresentation) data.4 System Design and Implementation4.1 IntroductionThe system is designed to learn phrase-to-concept associations and demonstrate it throughplaying games: a guessing game and a naminggame.
Games are often used to test, and encouragesystem learning (Steels and Kaplan, 2001).
Thelearning process involves a user selecting an objectin a scene and naming it.
The guessing gameinvolves a user saying a phrase, and the systempointing to the object that the phrase refers to.
Thenaming game involves a user pointing to an objectand the system naming it  The system is notphysically grounded, so all games are simulated.The learning process allows the system toacquire associations between phrases and conceptswhile the games test system comprehension andsystem production respectively.
The learningprocess takes a string and concept as input, andproduces no output.
Comprehension takes a stringas input, and produces a concept as output,whereas production takes a concept as input, andproduces a string as output.4.2 Strings and ConceptsA string is a list of characters with a fixed order.A blank space is used to separate words within thestring, of which there can be either one or two.The system can break strings down into theircomponent words.A concept is a list of feature values.
Thesystem recognises six feature values; red, blue,green, white, circle, and square.
There are no in-built associations between any of the featurevalues.
This form of learning is supported by theimageability theory (Paiviom 1971).
No claimsconcerning concept acquisition and formation aremade in this paper.
All concepts are hard codedfrom the outset.The full list of objects used in the games arederived from shape and colour combinations; redsquare, red circle, blue square, blue circle, greensquare, green circle, white square, and whitecircle.
Individual feature values can also act asconcepts, therefore the full list is concepts is thelist of object plus the list of feature values.4.3 GroupsTo associate a string with a concept, the systemstores a list of groups.
Each group contains an ID,one or more description pairs, an observedfrequency, and zero or more occurrencesupporter links.The ID acts as a unique identifier, allowing thegroup to be found.
A description pair is a stringand a concept.
Groups must have at least onedescription pair since their primary function is torelate a string to a concept.
The observedfrequency represents the number of times that thedescription pair?s components have beenassociated through system input.The occurrence supporter links are a set of groupIDs.
Each ID in the set refers to a group that63contains a superset of either the description pair, orthe same value for one component of thedescription pair and a superset of the other e.g.
Thedescription pair [?red?
; red] 1 would be supportedby the description pair [?red square?
; red square].A worked example is provided in the next section.The links therefore record the number ofoccurrences of the group?s description pair.
Theoccurrence supporter link reinforces thedescription pair?s association and increases thetotal frequency of the group.
The total frequencyis the group?s observed frequency plus theobserved frequency of all of its supporters, neverincluding a supporter more than once.Finally, group equality is defined by groupssharing the same description pair.4.4 The Learning ProcessAt each stage in the learning process, adescription pair is entered into the system.
Thesystem does not attempt to parse the correctness ofthe description.
All data is considered to bepositive.
The general learning process algorithm isdetailed in the rest of this section.
Specificexamples are also provided in Table 1, showing thegroups?
values; ID, description pair, occurrencefrequency (OF), occurrence supporter links(OSLs), and total frequency (TF).
Five steps arefollowed to incorporate the new data:1.
Identify the description pair.2.
Find equal and unequal parts.3.
Update system based on equal parts..4.
Update system based on unequal parts.5.
Re-enter new groups into the system.4.4.1 Identify the description pairIf the description pair exists in a group that isalready in the system, then that group?s observedfrequency is incremented.
Otherwise, the systemcreates a new group containing the newdescription.
It is given a unique ID and anobserved frequency of one.
Assume that thesystem already contains a group based on thedescription pair [?red circle?
; red circle].
This hasan ID of one.
Assume also that the newdescription pair entered is [?red square?
; redsquare].
Its group has an ID of two (group #2).All description pairs entered into the system arecalled concrete description pairs, this is, thesystem has encountered them directly as input.The new group is referred to as a concrete group,since it contains a concrete description pair.1The convention of strings appearing in quotes(?
red?
), and concepts appearing in italics (red) isadopted throughout this paper.ID Description Pair OF OSLs TF#1 [?
red circle?
; red circle] 1 [] 1#2 [?
red square?
; redsquare]1 [] 1#3 [?
red?
; red] 0 [#1,# 2] 2#4 [?
#3 circle?
; #3 circle] 0 [#1] 1#5 [?
#3 square?
; #3square]0 [#2] 1#6 [?
circle?
; circle],[?
square?
; square]0 [] 0#7 [?
#3 #6?
; #3 #6] 0 [#2] 1Table 1: Sample data4.4.2 Find equal and unequal partsThe new group is compared to all of the groupsin the system.
Comparisons are based on thegroups?
description pairs alone.
Strings arecompared separately from concepts.
A stringmatch is found if one of the strings is a subset, orexact match, of the other.
Subsets of strings mustcontain complete words.
Words are regarded asatomic units.
Concepts are compared in the samefashion as strings, where feature values are theatomic units.
Successful comparisons create a setof equal parts and unequal parts.
Comparisonresults are only used when equal parts exist.
Thisapproach is similar to alignment based learning,but with the additional component of concepts (vanZaanen, 2000).In comparing the new group, group #2, to theexisting group, group #1, the equal part [?red?
;red] and the unequal part [?circle?
; circle],[?square?
; square] are found.
The comparisonalgorithm is essential to the operation of thesystem.
It is used in the learning process and in thegames.
Without it, no string or concept relationscould be drawn2.4.4.3 Update system based on equal partsWhen an equal part is found, a new group iscreated.
In the example, an equal part is foundbetween group #1 and group #2.
Group #3 iscreated as a result.
The new group is given anobserved frequency of zero.
The IDs of the groupsthat were compared (group #1 and group #2) areadded to the new group?s (group #3) occurrencesupporter links.
If the group already exists, then aswell as the existing group?s observed frequencybeing incremented, the IDs of the groups that werecompared are added to the occurrence supporterlinks.
IDs can only appear once in the set ofoccurrence supporters links, so if an ID is alreadyin it, then it is not added.2The system assumes full compositionality.
Idiomsand metaphors are not considered at this stage.64Up until this point, all groups?
description pairshave contained a string and concept.
Descriptionpairs can also contain links to other groups?
stringsand groups?
concepts.
These description pairs arereferred to as abstract description pairs.
If allelements of the abstract description pair are linksto other groups then it is fully abstract, else it ispartially abstract.
A group that contains anabstract description pair is called an abstractgroup.
The group is fully abstract if its abstractdescription pair is fully abstract, else it is apartially abstract group.
Once a group has beencreated (as group #3 was), based on a descriptioncomparison, the system attempts to make twoabstract groups.The new abstract groups (group #4 and group#5) are based on substitutions of the new group?sID (group #3) into each of the groups that wereoriginally compared.
Group #4 is therefore createdby substituting group #3 into group #1.
Similarly,group #5 is created by substituting group #3 intogroup #2.The new abstract groups are given an observedfrequency of zero (ID?s equal four and five).
Notethat abstract groups always have an observedfrequency of zero as they can never been directlyobserved.
The ID of the appropriate group used incomparison and later creation is added to theoccurrence supporters links.
Each abstract grouptherefore has a total frequency equal to that of thegroup of which it is an abstract form.4.4.4 Update system based on unequal partsUnequal parts are only considered if equal partsare found in the comparison.
Otherwise, theunequal parts would be the complete set of datafrom both groups, which does not provide usefulinformation for comparisson.
For every set ofunequal parts that is found, a new group is created.If there is more than one unequal part then thegroup will contain more than one description pair.Such a group is referred to as a multi-group.
Twounequal parts were found earlier in comparinggroup #1 and group #2.
They are [?circle?
; circle]and [?square?
; square].
Group #6 is thereforecreated using these two description pairs.The creation of a multi-group allows for a fullyabstract group to be created.
The system uses thedata from the new multi-group (group #6) and thegroup created through equal parts (group #3).Both groups are substituted back into the groupthat was originally being compared (group #1).The resulting group (group #7) is fully abstract asboth equal parts and unequal parts have been usedto reconstruct the original group (group #1).4.4.5 Re-enter new groups into the systemAll groups that have been created through steps3 and 4 are compared to all other groups in thesystem.
Results of comparisons are dealt with byrepeating steps 3-5 with the new results.
By use arecursive step like this, all groups are compared toone another in the system.
All group equalities aretherefore created when the round is complete.
Theamount of information available from every newgroup entered into the system is thereforemaximised.4.5 The Significance of Groups TypesFour different types of group have beenidentified in the previous section.
Although allgroups share the same properties, they can be seento represent difference aspects of language.
It isthe combination and interaction of these groupsthat gives rise to emergent simple syntax.
Thissyntax is bi-gram collocations, but since the systemis scalable, it is referred to as simple syntax.4.5.1 Concrete GroupsConcrete groups acquire the meaning ofindividual lexemes (associate concepts withstrings).
They are verifiable in the real worldthrough the use of scene based games.4.5.2 Multi-GroupsMulti-groups form syntactic categories based onsimilarities between description pair usage.
Underthe current system, groups can only have amaximum of two description pairs.
If this were tobe expanded, it is clear that large syntacticcategories such as noun and verb equivalentswould arise.4.5.3 Partially and Fully Abstract GroupsPartially and fully abstract groups act as phrasalrules in the system.
Abstract values containedwithin the group?s description pairs can relate toboth concrete groups and multi-groups.
Abstractgroups that relate to multi-groups offer a choice ofsubstitutions.For example, group #7 (Table 1) relates a singlegroup to a multi-group.
By substitution of groups#3 and #6 into group #7, the concrete pairings of[?red circle?
; red circle] and [?red square?
; redsquare] are produced.
The string data are directlyequivalent to:S -> Adj.
N,where Adj.
= {?red?
}and N = {?circle?, ?square?
}When a description pair is entered into thesystem, the process of semantic bootstrappingtakes place.
Lexical items (strings) are associatedwith their meanings (concepts).
When group65comparisons are made, syntactic bootstrappingbegins.
Associations are made between allcombinations of lexical items throughout thesystem, and all combinations of meaningsthroughout the system.The system stores lexical item-meaningassociations, lexical item-lexical item associationsand meaning-meaning associations.
This basicframework allows for the production of complexphrasal rules.4.6 Comprehension and Production ThroughGamesThe guessing game tests comprehension whilethe naming game test production.
Comprehensiontakes a string as input, and produces a concept asoutput, whereas production takes a concept asinput, and produces a string as output.
Thecomprehension and the production algorithms arethe same, except the first is string based, and thesecond is concept based.The algorithm performs two tasks: findingconcrete groups with exact matches to the input,and finding abstract groups with possible matchesto the input.
Holophrastic matching uses onlyconcrete groups.
Syntactic matching performsholophrastic matching, followed by furthermatches using abstract groups.
Note that thesystem only performs syntactic matching, whichincludes holophrastic matching.
Holophrasticmatching is never performed alone, unless intesting stages.For holophrastic matches, the system searchesthrough its list of groups.
Their description pairsare compared to the input being searched for.There is therefore re-use of the comparisonalgorithm introduced in the learning process.When a match is found, the group is added to a listof possible results.If holophrastic matching is being performedalone, then this list of possible results is sorted bytotal frequency.
The group with the highest totalfrequency is output by the system.Syntactic matching begins by performingholophrastic matching, but does not output a resultuntil all abstract groups have been matched too.
Itis therefore an extension of holophrastic matching.Once a first fun of holophrastic matching isperformed, the input is converted into abstractform.
This is performed at the word/feature valuelevel.
The most likely element is found bysearching through the groups, comparing it to thedescription pair, and selecting the group with thehighest total frequency from those found.The group IDs replace the appropriate element inthe input (just as substitutions were made duringthe learning process).
All multi-groups thatcontain any of the abstract forms are found.
Eachmulti-group?s description pair becomes areplacement for the appropriate input?s abstractvalue.The new input, which is still in abstract form, issearched for, using holophrastic matching again.Since the groups found are not exact matches ofthe original input, their total frequency ismultiplied by an abstract factor.
The abstractfactor is a value between zero and one inclusive.The higher the factor, the greater the effect thatabstract groups have on the results.
Syntacticmatches can therefore  produce different resultsbased on the value of abstract factor.
The abstractfactor is not changed from the initiation totermination of the system.Groups found during the search are added to anew list of possible results.
The appropriateelements are substituted into the groups abstractvalues to make them concrete.
If an abstract valueis acting as a substitute (by being found originallyin a multi-group) then the original input value isused, not the replacement element.
This allows theabstract group to act as a syntactic rule, but it ispenalised by the abstract factor so it does not haveas much influence as concrete groups, that havebeen found to occur through direct inputassociations.The groups found throughout the entire syntacticsearch are now contained in a second list ofpossible results.
This list is reduced by removingduplicate groups.
For each group that is removed,its observed frequency and occurrence supporterlinks are added to the duplicate that is kept in thelist.The two lists from each matching routine aremerged and sorted by total frequency.
Thestring\concept of the group with the highest totalfrequency is outputted by the system.5 Testing and ResultsThe system is tested within the following areas:1.
Comprehension and production of allfourteen concepts.
The rate at which fullcomprehension and full production areachieved is compared.2.
Correctness of production matches forcompound concepts.
The correctness ofproduction matches are studied over anumber of rounds.3.
Type of production matches for compoundconcepts.
The type of production matchesfavoured, holophrastic or syntactic, arecompared over a number of roundsA match of concept to word or word to conceptis considered correct if the string describes theconcept fully.
For example, [?red?
; red] and [?red66square?
;  red square] are correct, but [?red?
; redsquare] and [?red square?
; red] are incorrect.
Onepoint is given for each correct match, zero for eachincorrect match.Note that all test results are based on the averageof ten different system trials.
Each result shows abroad tendency that will likely be smoothed ifmore trials are run.
All input is randomlygenerated.
The abstract factor is set to 0.4 for alltests.5.1 Comprehension Vs. ProductionFull comprehension occurs much sooner (seeFigure 1), on average, than full production.
Thisresult is found in children also.
Althoughproduction and comprehension compete quitesteadily in early stages of the system,comprehension reaches its maximum, on average,in 20% of the time that production takes to reachits maximum.024681012141 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49ProductionComprehensionFigure 1: Shows number of correctcomprehension and production matchesFull comprehension (fourteen points) isachieved, on average, by round 50, while fullproduction comes at round 250.
Both holophrasticdata and syntactic data contribute to the successes.Underextensions are found during comprehension.For example,  in early rounds, ?green?
is used todescribe only green squares.
This phenomena isquickly eliminated in the trials but with a larger setof concepts and vocabulary, it is likely to persistfor more than a few rounds.5.2 Correctness of Holophrastic Vs SyntacticMatchesAt the end of each round, production is testedusing the eight compound concepts alone.
Theseare based on the eight observable objects in thesimulated scene.
Only compound concepts candemonstrate simple syntax in this system, assingular concepts have associations to single wordstrings.The system uses syntactic matching alone, butsyntactic matching includes holophrastic matching,as discussed earlier.
To determine whetherholophrastic data is being used, or syntactic datawhen a syntactic match is run, the matchingalgorithm has been split.
The number of correctstrings produced using holophrastic data and thenumber of correct strings produced using syntacticdata alone are compared (see Figure 2).The data demonstrate that the system usesmostly holophrastic matches in early rounds(comparable to the one-word stage).
This iseliminated in further rounds, in favour or syntacticmatches alone (the two-word stage).
Note thatalthough the holophrastic stage may appear to beproducing two-words, these words are consideredto be one-word.
For example, ?allgone?
isconsidered to be one-word in early stages oflinguistic development, as opposed to ?all gone?
(Ingram, 1989).01234561 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73HolophrasticSyntacticFigure 2: Shows number of correct holophrasticand syntactic matches.The syntactic data continues to rise, until itachieves full production.
The holophrastic stagenever achieves full production, but peaks, thenreduces to zero.
This trend occurs as holophrasticunderextensions such as ?red?
representing redsquare become more likely than ?red square?representing red square.Early syntactic matches are based on novelstring productions for novel string concepts.Holophrastic matching is incapable of producingnovel strings from novel concepts, as it deals withconcrete concepts.
Abstract concepts however,allow new string combinations to be produced,such as ?blue square?, from blue square eventhough neither then string nor concept have beenencountered before.
Such an abstraction maycome from a multi-group that associates ?blue?with ?red?, while containing a group that contains?red square?
also.
The novel string ?blue square?is therefore abstracted.5.3 Use of Holophrastic Vs Syntactic MatchesThe system does not always produce the correctstrings when a concept is entered.
The strings thatare produced are a result of either holophrastic orsyntactic matching.
Regardless of correctness, theamount of times that holophrastic matches aremade over syntactic matches can be compared (seeFigure 3).670%10%20%30%40%50%60%70%80%90%100%1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73SyntacticHolophrasticFigure 3: Shows distribution of holophrastic andsyntactic matches.The system relies completely on one-worddescriptions at the outset, but soon syntacticallyderived two-word descriptions become prevalent.It is likely that the one-word stage will last longerif larger concept and vocabulary sets are in use.The system shows the same form of transition ascan be seen in children from the one-word stage tothe two-word stage, without the use of an artificialtrigger.
The shift is gradual although the use oflarger concept and vocabulary sets, plus differentabstract factor values will affect the transition.The greater the number of words in multi-groups(the greater the size of syntactic categories), thelower the abstract factor is required to encouragethe emergence of simple syntax.6 Related WorksSupporters of computational modelling inlanguage acquisition, often promote the practicalimportance of running simulations, whereevolutionary effects can be recreated in short timeperiods (Zuidema, 2001).Although this paper is focussed on an individualsystem, or agent, acquiring language, it is beeninfluenced by research into social learning(Oliphant and Batali, 1997; Kirby, 1999; Steelsand Kaplan, 2002).
Social learning demonstratesthe convergence upon a common language, or setof languages, from an uncoordinated proto-language, within a population of agents.
Sociallearning allows for the playing of games betweenagents, similar to those in this paper, with theresults being used as further system input, tosupport, or deny associations.
This research can beviewed as a form of social learning with one agent(string and concept generator) performing theteacher role, and the other agent (the system)performing the learner role.Simulations of both the babbling stage and theone-word stage have been developed (Scheler,1997; Abidi, 1999).
ACCLAIM, a one-word stagesimulator, demonstrates that systems can reactappropriately to changes in situations.
Forexample, when a cessation event is triggered, itproduces ?Stop?, and when an object is requested,it produces ?More?.
Both examples are typical ofchildren during the one-word stage (Bloom, 1973).Several systems exist that use perceptions toencourage language acquisition (Howell, Becker,and Jankowicz,, 2001; Roy, 2001).
ELBA learnsboth nouns and verbs from video scenes, startingwith a blank lexicon.
Such systems have helped inthe selection of both appropriate input sources andfeature values to use in this research.
This systemwill also be physically grounded in future.The research presented in this paper describes asystem that drives linguistic development.
Othersystems have used similar techniques, based onsyntactic and semantic bootstrapping (Howell andBecker, 2001), but have not explained howmultiple word acquisition is achieved from a singleword basis.Steels (1998) introduces frames that grouplexical elements together by the roles that theyplay, very similar to groups in this paper.
Framesare more dynamic than groups however,structurally adapting when words reoccur.
Groupsdo not adapt in this way.
New groups are createdto describe similarities rather than adaptingexisting ones.
Steels also introduces multiple wordsentences, but it is unclear as to why agents inventa multiple word description over creating a newsingle word description.
The invention is triggeredand does not emerge.
This research is based onreal multiple word inputs, so the reason forinvention is not necessary, unlike the reason foradoption i.e.
why the system adopts two-worddescriptions.The comparison algorithm, as previously noted,is similar to alignment based learning (van Zaanen,2000).
The system in this research performsperfect alignment requiring exact word matcheswhen finding equal parts and unequal parts.
Thissystem also uses concepts, reducing the number ofincorrect groupings, or constituents, when there isambiguity in text.
Unsupervised grammarinduction can also be found in EMILE (van Zaanenand Adriaans, 2001).
EMILE identifiessubstitution classes by means of clustering.
Theseclasses are comparable to this system?s groupsalthough no concepts are used.7 Future ResearchAs the system stands, it uses a small input set.Further developments are focussed on expandingthe system.
All ten of Brown?s relations should beimplemented.
Larger concept and vocabulary setsare therefore required.
Extensions to these sets arelikely to affect underextensions, mismatches, thelength of pre-syntactic usage time, and the overallgrowth pattern of simple syntax.688 ConclusionThis paper offers a potential explanation of themechanism by which the two-word stage emergesfrom the one-word stage.
It suggests that syntacticdata is sought out from the beginning of languageacquisition.
This syntactic data is alwayscompeting with the associations of holophrasticdata.
Syntax is strengthened when patterns areconsistently found between strings and concepts,and is used in favour of holophrastic data when itis sufficiently frequent.
The simple syntaxcontinues to grow in strength, ultimately beingused in favour of holophrastic data in allproduction and comprehension tasks.This system provides the foundation for morecomplex, hierarchical, syntax to emerge.
The typeand volume of input is the only constraint upon thesystem.
The entry into post two-word stages ispredicted from the system?s robust architecture.9 AcknowledgementsThe first author is sponsored by a studentshipfrom the EPSRC.Thanks to the workshop reviewers for theirhelpful and much appreciated advice.ReferencesS.
Abidi, 1996.
A Neural Network Simulation of ChildLanguage Development at the One-word Stage.
Inproceedings of IASTED Int.
Conf.
on Modelling,Simulation and Optimization, Gold Coast, Australia.L.
Bloom, 1973.
One Word at a Time.
The use ofsingle-word utterances before syntax  The Hague,Mouton.R.W.
Brown, 1986.
Language and categories.
In ?
AStudy of Thinking?
, ed.
J.S.
Bruner, J.J. Goodnow,and G.A.
Austin, pages 247-312.
New York: JohnWiley, 1956.
Reprint, New Brunswick: Transaction.L.R.. Gleitman and Elissa L. Newport, 1995.
TheInvention of Language by Children: Environmentaland Biological Influences on the AcquisitionLanguage.
In ?
An Invitation to Cognitive Science?
,L.R.
Gleitman and M. Liberman, 2nd ed., Vol.1,Cambridge, Mass., London, MIT Press.S.R.
Howell and S. Becker, 2001.
Modelling languageacquisition: Grammar from the Lexicon?
InProceedings of the Cognitive Science Society..S.R.
Howell, S. Becker, and D. Jankowicz, 2001.Modelling Language Acquisition: Lexical GroundingThrough Perceptual Features.
In Proceedings of the2001 Workshop on Developmental EmbodiedCognitionJ.R.
Hurford, M. Studdert-Kennedey, and C. Knight,1998.
The Emergence of Syntax.
In ?
Approaches tothe evolution of language: social and cognitivebases?
, Cambridge, Cambridge University Press.D.
Ingram, 1989.
First Language Acquisition.
Method,Description and Explanation.
Cambridge: CambridgeUniversity Press.R.
Jakobson, 1971.
Why ?mama?
and ?papa??
In?
Child Language: A Book of Readings?
, by A. Bar-Adon and W. F. Leopold, ed., pages 213-217.Englewood Cliffs, NJ:Prentice-Hall.P.W.
Jusczyk, 1999  How infants begin to extract wordsfrom speech.
Trends in Cognitive Science, 3 (9,September):323-328.S.
Kirby, 1999.
Syntax out of learning: The culturalevolution of structured communication in apopulation of induction algorithms.
In Proceedings ofECAL99 European Conference on Artificial Life, D.Floreano et al ed.
pages 694-703, Berlin: Springer-Verlag,M.
Oliphant and J. Batali 1997.
Learning and theemergence of coordinated communication.
Centre forResearch in Language Newsletter, 11(1).A.
Paivio, 1971, Imagery and Verbal Processes.
NewYork: Holt, Rinehart & Winston.J.
Piaget, 1960.
The Language and Thought of theChild.
Routledge and K. Paul, 3rd ed.,.
RoutledgePaperbacks.S.
Pinker, 1994.
The Language Instinct.
The NewScience of Language and Mind.
Allen Lane, PenguinPress.D.
Roy, 2001.
Grounded spoken language acquisition:Experiments in word learning.
IEEE Transactions onMultimedia.G.
Scheler, 1997d.
The transition from babbling to theone-word stage: A computational model.
InProceedings of GALA '97.L.
Steels and F. Kaplan, 2001.
AIBO's first words: Thesocial learning of language and meaning.
Evolution ofCommunication, vol.
4(1):3-32.
John Benjamin?sPublishing Company, Amsterdam, Holland.L.
Steels, 1998.
The Origins of Syntax in visuallygrounded robotic agents.
AI 103, 1-24.M.
Tomasello, and A.C. Kruger, 1992.
Joint attention inaction: Acquiring verbs in ostensive and non-ostensive contexts.
Journal of Child Language19:311-333.M.
van Zaanen, 2000.
Learning structure usingalignment based learning.
In Proceedings of theThird Annual Doctoral Research Colloquium(CLUK), pages 75-82.M.
van Zaanen and P. Adriaans, 2001.
Alignment-based learning versus EMILE: A comparison.
InProceedings of the Belgian-Dutch Conference on AI(BNAIC).W.H.
Zuidema, 2001.
Emergent syntax: the unremittingvalue of computational modelling for understandingthe origins of complex language.
ECAL01, 641-644.Springer, Prague, Sept. 10-14, 2001.
