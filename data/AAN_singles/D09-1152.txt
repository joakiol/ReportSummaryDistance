Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1465?1474,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPQuantifier Scope Disambiguation Using Extracted Pragmatic Knowledge:Preliminary ResultsPrakash SrinivasanTemple University1805 N. Broad St.Wachman Hall 324Philadelphia, PA 19122prakash.srinivasan@temple.eduAlexander YatesTemple University1805 N. Broad St.Wachman Hall 324Philadelphia, PA 19122yates@temple.eduAbstractIt is well known that pragmatic knowl-edge is useful and necessary in many dif-ficult language processing tasks, but be-cause this knowledge is difficult to acquireand process automatically, it is rarely used.We present an open information extrac-tion technique for automatically extractinga particular kind of pragmatic knowledgefrom text, and we show how to integratethe knowledge into a Markov Logic Net-work model for quantifier scope disam-biguation.
Our model improves quantifierscope judgments in experiments.1 IntroductionIt has long been a goal of the natural languageprocessing (NLP) community to be able to inter-pret language utterances into logical representa-tions of their meaning.
Quantifier scope ambigu-ity has been recognized as one particularly chal-lenging aspect of this problem.
For example, thefollowing sentence has two possible readings, de-pending on the scope of its quantifiers:Every boy wants a dog.One reading of this sentence is that there exists asingle dog in the world which all boys want.
Thesecond, and usually preferred, reading is that thesentence is describing a separate ?wanting?
rela-tion for each boy, and that the dog in question isa function of the boy who wants it.
In this read-ing, there may be as many different dogs as boys,although it leaves open the possibility that severalof the boys want the same dog.
In logic, these tworeadings can be represented as follows:1.
?d?Dogs?b?Boyswants(b, d)2.
?b?Boys?d?Dogswants(b, d)The readings differ only in the order of the quanti-fiers.
The quantifier that comes first in each ex-pression is said to have wide scope; the secondquantifier has narrow scope.Linguists and NLP researchers have come upwith several theories and mechanisms for automat-ically determining the scope of quantified linguis-tic expressions.
Despite a long history of proposedsolutions, however, researchers have for the mostpart abandoned this task as hopeless because of?overwhelming evidence suggesting that quanti-fier scope is a phenomenon that must be treated atthe pragmatic level?
(Saba and Corriveau, 2001).For example, in active voice clauses, the quantifierfor the subject noun is usually preferred for widescope over the quantifier of the predicate noun(Kurtzman and MacDonald, 1993).
But such pref-erences can easily be overruled by world knowl-edge:A doctor lives in every city.1.
?d?Docs?c?Citieslives in(d, c)(A single doctor lives in all cities.)2.
?c?Cities?d?Docslives in(d, c)(Each city has a different doctor living there.
)Syntactic preferences would normally indicatethat reading 1 is better, but in this particular casecommon-sense knowledge of the world overrulesthat preference and makes reading 2 far moreprobable.Open-domain pragmatic knowledge is usuallynot available to language processing systems, butthat is beginning to change.
Recent research inopen information extraction (Banko and Etzioni,2008; Davidov and Rappaport, 2008) has shownthat we can extract large amounts of relational datafrom open-domain text with high accuracy.
Here,we show how we can connect the two fields, by ex-tracting a targeted form of pragmatic knowledgefor use in quantifier scope disambiguation.
Ourcontributions are:1) We build an extraction mechanism for extract-ing pragmatic knowledge about relations.
In par-1465ticular, we extract knowledge about the expectedsizes of the sets of objects that participate in therelations.
The task of identifying functional re-lationships is a subtask of our extraction problemthat has received recent attention in the literature(Ritter et al, 2008).2) We devise a novel probabilistic model in theMarkov Logic Network framework for reasoningover possible readings of sentences that involvequantifier scope ambiguities.
The model is ableto assign a probability that a particular reading isplausible, given the pragmatic knowledge we ex-tract.3) We provide an empirical demonstration that oursystem is able to resolve quantifier scope ambigu-ities in cases where the syntactic and lexical fea-tures used by previous systems are of no help.The remainder of this paper is organized as fol-lows.
The next section describes previous work.Section 3 shows how the problem can be formu-lated as a task of assigning probabilities to possi-ble worlds, and that the crucial difference betweenthem has to do with the number of objects partic-ipating in individual relationships.
Section 4 dis-cusses our techniques for extracting the pragmaticknowledge that allows us to make judgments aboutquantifier scope.
Section 5 presents our proba-bilistic model for resolving scope ambiguities.
Wepresent an empirical study in section 6, and section7 concludes and suggests items for future work.2 Related WorkQuantifier scope disambiguation has received at-tention in linguistics and computational linguis-tics since at least the 1970s.
Montague (1973)gave a seminal treatment of quantifier ambiguities,and argued that a particular syntax-based mech-anism known as ?quantifying-in?
could resolvescope ambiguities.
Since then, most work on dis-ambiguation has focused on syntactic clues for de-termining which readings of an ambiguous state-ment are possible, and of the set of possible read-ings, which ones are preferred (Van Lehn, 1978;Hobbs and Shieber, 1987; Poesio, 1993a; Hurum,1988; Moran, 1988).
For instance, one linguis-tic study (Kurtzman and MacDonald, 1993) deter-mined that in active voice sentences where quan-tifiers in the subject and object give rise to scopeambiguity, there is a preference for the reading inwhich the subject quantifier has wide scope ?
thedirect reading is acceptable 70-80% of the time,whereas the indirect reading is acceptable 30-40%of the time.
Sentences that are similar in all re-spects except that they are passive voice have nosuch preference.
Nevertheless, in these studiesboth readings are often quite plausible.
In additionto syntactic clues, other studies have noted thatthe choice of quantifier has a significant effect onscope disambiguation (e.g., ?each?
has a greatertendency for wide scope than ?every?)
(Van Lehn,1978; Alshawi, 1990).
Most authors have notedthat both syntactic and lexical evidence fall shortof a full solution, and that pragmatic knowledge(knowledge about the world) is necessary for thistask (Van Lehn, 1978; Saba and Corriveau, 1997;Moran, 1988).
Saba and Corriveau (2001) recentlyproposed a test for quantifier scope disambigua-tion using pragmatic knowledge.
However, theydo not show how to extract the necessary infor-mation, nor do they implement or evaluate theirproposed test.Due to the difficulty of the problem, severalauthors have devised techniques for ?underspec-ified?
logical representations that can efficientlystore multiple ambiguous readings, and they de-vise techniques for automated reasoning using un-derspecified representations (Reyle, 1995; Late-cki, 1992; Poesio, 1993b).
Others (Hobbs andShieber, 1987; Park, 1988) have devised compu-tational mechanisms for generating all of the pos-sible readings of statements exhibiting quantifierambiguity, especially in cases involving more thantwo quantifiers.Detecting functions in extracted relational datahas been studied in several contexts.
Ritter etal.
(2008) use knowledge of functions to determinewhen two extracted relationships contradict oneanother.
Knowledge of functions has also beenimportant in finding synonyms (Yates and Etzioni,2009) and in review mining (Popescu, 2007).
Weextend this work by extracting not just a binarydetermination of whether a relation is functional,but a distribution over the expected number of ar-guments for that relation.
Our technique also dif-fers from previous work based on extracted rela-tionships between named entities.
We leveragedomain-independent extraction patterns involvingnumeric phrases, as discussed below; our tech-nique is complementary to existing approachesand could in fact be combined with them for evengreater accuracy.
Finally, we apply the extractedknowledge in a novel way to quantifier scope dis-ambiguation.Our work is similar in spirit to several recent1466projects that use semantic reasoning over extractedknowledge for a novel approach to well-knowntasks.
For example, Schoenmackers et al(2008)have recently used extracted knowledge for thetask of predicting whether a new extracted fact iscorrect.
Yates et al(2006) use extracted knowl-edge to determine whether a parse of a sentencehas a plausible semantic interpretation.
We extendthis new line of attack to a hard problem in lan-guage understanding.3 Possible Worlds FrameworkWe now present a framework for reasoning aboutquantifier scope ambiguities, and for choosingamong possible readings based on pragmaticknowledge (or world knowledge ?
we use theterms interchangeably).
We first present a formaldescription of the quantifier scope disambiguation(QSD) problem.
We then describe the crucial dif-ferences between the ?possible worlds?
evoked bydifferent readings of an ambiguous statement.3.1 Representation of ReadingsWe follow Copestake et al (2005), among oth-ers, in representing quantifiers as modal oper-ators with three arguments: a variable namefor the variable being quantified; a logical for-mula, called the restriction, which defines the setof objects over which the variable may range;and a second logical formula, called the body,which defines the expression in which the quan-tified variable takes part.
For example, werepresent the sentence ?Every dog barks?
as:every(x,dog(x),barks(x)).For the sake of clarity and convenience, we re-strict our attention to a common syntactic formof sentences, where the semantic representationis relatively well-understood: active-voice Englishsentences in which the subject noun phrase isquantified, and a noun phrase in the predicate (ei-ther an object of the verb, or an object of a prepo-sition attached to the verb) is also quantified.
Fora sentence with the following structure, in whichpiand qjrepresent predicates introduced by mod-ifiers like adjectives and prepositional phrases,(S(NP(DETQ1)(N[p1, .
.
.
, pn]C1))(V P(VR)(NP(DETQ2)(N[q1, .
.
.
, qm]C2))we can represent the two possible readings of thesentence as:direct reading:Q1(x,C1(x) ?
p1(x) ?
.
.
.
?
pn(x),Q2(y, C2(y) ?
q1(y) ?
.
.
.
?
qm(y),R(x, y)))(1)indirect reading:Q2(y, C2(y) ?
q1(y) ?
.
.
.
?
qm(y),Q1(x,C1(x) ?
p1(x) ?
.
.
.
?
pn(x),R(x, y)))(2)By making the restriction to this type of sen-tences, we can isolate the effects of pragmatics onscope disambiguation decisions from the effectsof syntax, since all test cases have essentially thesame syntax.
As we show below, for certain typesof relations, the preference for interpretations maybe drastically different from the general preferencefor the direct reading, even though the syntax ofthe sentences we investigate matches the syntaxstudied by Kurtzman and MacDonald (1993).3.2 Readings, Possible Worlds, and WorldKnowledgeThe different logical forms for the direct and indi-rect readings describe different ?possible worlds.
?For instance, the direct reading of ?A doctor livesin every city?
describes worlds in which there is asingle doctor who manages to reside in each cityof the world simultaneously.
This reading is ?pos-sible?
in the sense that it does not contradict itself.In logical terms, if ?
represents the direct readingof this sentence, ?
0 ?.
Using some imaginationone could devise a scenario, perhaps in an onlinegame world, that satisfies ?.Nevertheless, the indirect reading is stronglypreferred for this statement in the absence of anycontext that indicates an abnormal world.
Theindirect reading ??
describes worlds where everycity is inhabited by some doctor, but potentially adifferent doctor per city.
Using pragmatic knowl-edge, the reader can easily deduce that this logicalstatement is a much more likely reading than ?.Let B represent the reader?s pragmatic knowledge,including facts like ?People don?t simultaneouslylive in more than one city,?
and, ?There are at leasthundreds of cities in the world.?
The reader caneasily deduce that B  ??.
We now turn to meth-ods for extracting the necessary pragmatic knowl-edge B from text.14674 Extraction Techniques to Support QSDDecisionsSaba and Corriveau (2001) point out that there isa restricted form of pragmatic knowledge that canbe used in many instances of QSD.
Consider thefacts that were used above to determine that ?
?is preferable to ?.
The facts fall into two basiccategories of knowledge: 1) the size of class C(e.g., how many cities are there?
), and 2) the ex-pected number of Y participants in a relationshipR, given that there is exactly 1 X participant (e.g.,how many cities does 1 doctor live in?).
In bothcases, we are concerned with extracting sizes ofsets.Previous extraction systems have attempted toestimate set sizes based on extracted named en-tities.
Downey et al (2005)estimate the size ofclasses based on the number of named-entities ex-tracted for the class.
As far as we are aware, find-ing the expected size of an argument set for a rela-tion is a novel task for information extraction, butseveral researchers (Ritter et al, 2008; Yates andEtzioni, 2009) have investigated the special caseof detecting functional relations ?
those relationswhere the expected size of the Y argument set isprecisely 1.
As with class size extraction, theyuse extractions involving named-entity argumentsto find functional relations.Approaches that depend on named-entity ex-tractions have several disadvantages: they mustfind a large set of named-entities for every set,which can be time-consuming and difficult.
Also,many classes, like ?trees?
and ?hot dogs,?
have noor very few named instances, but many un-namedinstances, so approaches based on named entitieshave little hope.
In fact, besides classes like peo-ple, locations, and organizations (and their sub-classes), there are few classes that have a largenumber of named instances.
For classes that dohave named instances, synonymy, polysemy, andextraction errors are common problems that canall affect estimates of size (Ritter et al, 2008).Rather than indirectly determining set sizesfrom extracted instances, our system directly ex-tracts estimates of set sizes.
It uses numericphrases, like ?two trees,?
?hundreds of students,?or ?billions of stars,?
to associate numeric valueswith sets.
Table 1 lists the numeric phrases we use.Currently, we use only numeric phrases with ex-plicit values or ranges of values, but it may be pos-sible to increase the recall of our extraction tech-nique by incorporating more approximate phrasesNumeric Phrase Valueno | none | zero 0a | one | this | the 1two 2......one hundred | a hundred 100......hundreds of 100thousands of 1,000tens of thousands of 10,000......Table 1: Numeric phrases used in our extraction pat-terns.
For the word ?the?, we require that it be followed di-rectly by a singular noun, to try to weed out plural usages.like ?several,?
?many,?
or even bare plurals.
Wedo not match numbers expressed in digits (e.g.,1234) because we found that they produced toomany noisy extractions, such as dates and times.For words like ?hundreds,?
we set the value of theword to be the lower limit (i.e., 100).
This givesa conservative estimate of the value, but our tech-niques described below can help to compensate forthis bias.Table 2 lists examples of the hand-crafted,domain-independent extraction patterns we use.Our extraction patterns generate two types of ex-tractions, one for classes and one for relations.
Forclasses, each extraction E consists of a class nameEcand a number Enindicating the size of somesubset S ?
Ec.
For instance, the 4gram ?hun-dreds of students are?
matches our first pattern.The numeric phrase ?hundreds of?
here indicatesthat some subset S ?
Ec= students has a size inthe hundreds.
After processing a large corpus, oursystem can determine a probability distribution forthe size of a class given by:PC(size(C) = N) =|{E | Ec= C ?
En= N}||{E | Ec= C}|In practice, we only include the largest 20% of thenumbers N in the set of extractions for a class toestimate that class?s size.The second type of extraction we get from ourpatterns are relational extractions.
Each relationalextraction F consists of a relation name Fr, andpossibly names for the classes of its two argu-ments, Fc1, Fc2.
In addition, the extraction con-tains values for the size of both arguments, Fn11468Pattern Extraction<numeric> <word>+ (of | are | have) Ec= <word>+, En= value(<numeric>)(I | he | she) <word>+ <numeric> <noun> Fr= <word>+, Fc1= people, Fc2= <noun>,Fn1= 1, Fn2= value(<numeric>)it is pastParticiple(<verb>) by <numeric> Fr= <verb>,Fc2= thing,Fn1= value(<numeric>), Fn2= 1is the <word> of <numeric> Fr= is the <word> of,Fn1= 1, Fn2= value(<numeric>)Table 2: Sample extraction patterns for discovering classes (Ec) and their sizes (En), or relations (Fr) and the expectedset size of their arguments (Fn1and Fn2).and Fn2respectively.
For example, the fragment?she visited four countries?
matches the secondpattern in Table 2, with Fr= visited, Fn1= 1,and Fn2= 4.
Note that in our extraction patterns,one of of the arguments is always constrained to bea singleton set, like ?he?
or ?it.?
This restrictionallows us to avoid quantifier scope ambiguity inthe extraction process: if we extracted phrases like?Two men married two women,?
it would be un-clear which quantifier has wide scope, and there-fore how many men and women are participatingin each ?married?
relationship.
By using singularpronouns, we avoid this confusion; in almost allcases, these pronouns have wide scope, and indi-cate a single element.1Based on these extractions, our system de-termines two distributions for each relation:PLeftR(n) and PRightR(n).
The PLeftRdistributionrepresents the probability that the left argument ofR is a set of size n, given that the right argumentis a singleton set, and likewise for PRightR.
We de-termine the distributions from the extractions bymaximum likelihood estimation:PLeftR(n) =|{F | Fr= R,Fn1= n, Fn2= 1}||{F | Fr= R,Fn2= 1}|PRightR(n) =|{F | Fr= R,Fn2= n, Fn1= 1}||{F | Fr= R,Fn1= 1}|For example, for the relation is the fatherof, we might see the fragment ?he is the fatherof two children?
far more often than ?he is thefather of twenty children.?
PRightis the father ofwould therefore have a relatively low probabilityfor n = 20.
As one would expect, the relation1An example of an exception to this rule from our data setis the sentence ?It is worn by millions of women.?
Here, ?it?refers to a class of items such as a brand, and thus may referto a different item for each of the ?millions of women.
?visited appears more often with ?twenty,?
andthe relation married never does.
Their PRightdistributions are comparatively higher and lower,respectively than the one for is the fatherof at n = 20.In practice, we create histograms of the ex-tracted counts for both our E and F extractions,and our probability distributions are really dis-tributions over the buckets in these histograms,rather than over all possible set sizes.
To helpcombat sparse counts for large numeric values, weuse buckets of exponentially increasing width forlarger numeric values.
Thus between n = 0 and10, buckets have size 1, between 10 and 100 theyhave size 10, and so on.We also create distributions in the same way forrelations together with their extracted argumentclasses.
Since counts for these extractions tend tobe much more sparse, we interpolate these distri-butions with the distribution for just the relation,and with the distribution for the relation and justone class.
We use equal weights for all interpo-lated distributions.5 A Probabilistic Model for QuantifierScope DisambiguationQSD requires reasoning about different possiblestates of the world.
This involves logical reason-ing, since the direct and indirect readings differ inthe number of objects that exist in models satis-fying each reading, and the number of relation-ships between those objects.
QSD also involvesprobabilistic reasoning, since none of the extractedknowledge is certain.
We leverage recent work onMarkov Logic Networks (MLNs) (Richardson andDomingos, 2006) to incorporate both types of rea-soning into our technique for QSD.
We next brieflyreview MLNs, before describing our model and1469methods for training it.5.1 Markov Logic NetworksSyntactically, an MLN consists of a set of first-order logical formulas F and a real-valued weightwFfor each F ?
F. Semantically, an MLNdefines a probability distribution over possiblegroundings of the logical formulas.
That is, if Udenotes the set of all objects in the universe, andGdenotes the set of all possible ways to ground ev-ery F ?
F (i.e., substitute an element from U forevery variable in F ), then an MLN defines a distri-bution over truth assignments to the grounded for-mulas G ?
G. Let I denote the set of all possibleinterpretations of G ?
that is, each I ?
I assignstrue or false to every G ?
G. The probabil-ity of a particular interpretation I according to theMLN is given by :P (I) =1Zexp(?F?FwF?
n(F, I))Z =?I?Iexp(?F?FwF?
n(F, I))where n(F, I) gives the number of groundings ofF that are true in interpretation I .The equation above provides an expression forP (I) when U , or at least the size of U , is knownand fixed.
When we are interpreting expressionslike ?every city?
or ?every doctor?, however, werequire extracted knowledge to inform the systemof the correct number of ?city?
or ?doctor?
objects.Since our extractions are uncertain, they provide adistribution P (|U | = n) for the size of a class.Using P (|U |), we can still calculate P (I), evenwithout knowing the exact size of U :P (I) =?nP (|U | = n)P (I | |U | = n)5.2 MLN Classifier for QSDLet Q be a QSD problem, consisting of a rela-tion Qr, a class for the first argument of the re-lation Qc1, a class for the second argument Qc2,and quantifiiers Qq1, Qq2for each argument.
Weconstruct an MLN model for Q using the follow-ing logical formulas:1) Clustering: We allow members of each classto belong to clusters denoted by ?, but each ele-ment can belong to no more than one cluster.
Thisis represented by the following formula, which hasinfinite weight.?x?Qc1?Qc2,?,?
?x ?
?
?
x ?
???
?
= ?
?2) Relation between clusters: Every cluster ofclass 1 elements must participate in the relationQrwith exactly one cluster of class 2 elements,and vice versa.
We represent this participation inQrwith a series of logical relations Rm,n, each ofwhich indicates that a cluster of size m is partic-ipating in Qrwith a cluster of size n. We use aset of formulas for each setting of m and n, eachhaving infinite weight.???Qc1?!??
?Qc2,m,nRm,n(?, ??)????Qc2?!?
?Qc1,m,nRm,n(?, ??)??,?
?Rm,n(?, ??)
?
|?| = m ?
|?
?| = n3) Prefer relations between clusters of the ap-propriate size: We include a set of formulas withfinite weight that express the preference for a par-ticular relation to have arguments of a certain size.There is a separate formula for each setting of mand n, with a separate weight wm,nfor each.??,?
?Rm,n(?, ??
)This formula does most of the work of our clas-sifier.
For a given relation, such as the livesin(Person, City) relation, we can set theweights wm,nso that the model prefers worldswhere each person lives in just one place.
For in-stance, we can set the weight w1,1relatively high,so that the model is more likely to make clusters ofsize 1, which then participate in the R1,1relation.We describe how we choose the wm,nweightsbelow, but first we explain how to incorporate thequantifiers Qq1and Qq2into the model.
Unfortu-nately, every natural language quantifier has dif-ferent semantics (Barwise and Cooper, 1981), andthus they affect our model in different ways.
Here,we restrict our attention to the two common quan-tifiers ?a?
and ?every?, but note that the MLNframework is a powerful tool for incorporatingthe logical semantics and statistical preferences ofother quantifiers.For the quantifier ?a?, we require that the rela-tion have no argument clusters with size more than1 for that class.
Thus if Qq1= ?a?, we restrictRm,nto R1,n, and vice versa if Qq2= ?a.?
Fur-thermore, we require that at least one element ofthe class belong to a cluster: ?x,?x ?
?
has infi-nite weight.
For ?every,?
we require that every el-ement of the class that ?every?
modifies to be part1470of some cluster.
To effect this change, we simplyput an infinite weight on the formula ?x?
?x ?
?.Our MLN model is general in the sense thatfor any QSD problem Q, it can determine prob-abilities for any possible world corresponding toa reading of Q.
For our purposes, we are primar-ily interested in the direct and indirect readings ofany Q involving ?a?
and ?every.?
To predict thecorrect reading for a given Q, we simply check tosee which has the higher probability according toour MLN model.5.3 Parameter EstimationOur MLN model for QSD requires settings forthe wm,nparameters for each QSD problem Q.The standard approach to this problem would beto estimate these parameters from labeled train-ing data.
We reject the standard supervised frame-work, however, because each distinct relation Qrrequires different settings of the parameters, andtherefore a standard supervised approach wouldrequire manually labeled training data for everyrelation Qr.A second approach that is made possible byour extraction technique is to set the parametersusing the extracted distributions.
We tried thisapproach by setting w1,n= logPRightQr(n) andwm,1= logPLeftQr(m); since we only considersentences containing the quantifier ?a?, one of mand n will always be 1.
Unfortunately, in our ex-periments we found that this setting for the param-eters often gave far too little weight for large val-ues of m and n, and as a consequence, the classi-fier would systematically judge one reading to bemore likely than another.To counteract this problem, we take a hybrid ap-proach to parameter estimation, informed by bothlabeled training data and the extracted distribu-tions.
Crucially, our approach, which we call ZIPFFLATTENING, has only two parameters that needto be trained using a supervised approach, andthese parameters do not depend on the relation R.Thus, the approach minimizes the amount of train-ing data we need to a practical level.ZIPF FLATTENING works by correcting the PRdistributions to give higher weight to larger valuesof m and n. First, we estimate a Zipf distributionfrom the raw extracted counts for each argumentof relation R. To fit a Zipf curve, we use least-squares linear regression on the log-log plot of theextracted counts to find parameters zRand cRsuchthatlog (count) = zR?
log (argSize) + cR?
count = ecR?
argSizezRWe can perform this part automatically, using onlythe extraction data and no manually labeled train-ing data, for every relation.
However, the fittedZipf distribution needs to be corrected for the sys-tematic bias in the extracted counts.
To do this, weintroduce two parameters, ?1and ?2, that we useto scale back the sharp falloff in the Zipf distribu-tion.
Our flattened distribution has the form:count = e?1cR?
argSize?2zRWhen ?2is less than 1, the resulting curve has aless steep slope, and greater weight is placed onthe large values of m and n, as desired.
Our laststep is to interpolate the PRightRand PLeftRdistri-butions with the flattened Zipf distribution to comeup with corrected distributions for the right andleft argument sizes of R. We use equal weightson the two distributions to interpolate.
Note that ifthe original counts from the extraction system in-clude counts for only one argument size, then it isimpossible to estimate a Zipf distribution, and wesimply fall back on the extracted distribution.
Wedo not include counts for an argument size of zeroin this process.To estimate the parameters ?i, we collect atraining set of QSD problems Q, labeled with thecorrect reading for each (direct or indirect), andrun the extractor for the relations Qrappearing inthe training set.
We then perform a gradient de-scent search to find optimal settings for the ?ionthe training data.6 ExperimentsWe report on two sets of experiments.
The firsttests our extraction technique on its own, and thesecond tests the accuracy of our complete QSDsystem, including the extraction mechanisms andthe prediction model, on a quantifier scope disam-biguation task.6.1 Function Detection ExperimentFunction detection is an important task in its ownright, and has been used in several previous ap-plications (Ritter et al, 2008; Yates and Etzioni,2009; Popescu, 2007).
To turn our extractionsystem into a classifier for functions vs. non-functions, we simply checked whether there were1471Num Precision Recall F1Functions 54 .79 .76 .77Non-functions 74 .83 .85 .84Table 3: Precision and recall for detecting functions us-ing the numeric extraction technique.any extractions for R with Fn2> 1.
If so, we pre-dicted that the R was nonfunctional, and otherwisewe predicted it was functional.We used the Web1Tgram Corpus of n-gramsprovided by Google, Inc to extract classes, rela-tions, and counts.
This corpus contains counts for2- through 5-grams that appear on the Web pagesindexed by Google.
Counts are included in thisdata set for all n-grams that appeared at least 40times in their text.
We ran our extraction tech-niques on the 3-, 4- and 5-grams.
To create a testset, we sampled a set of 200 relations from our ex-tractions, removed any relations that consisted ofpunctuations, stopwords, or other non-relationalitems.
We then manually labeled the remainderas functions or non-functions.Table 3 shows our results.
A baseline sys-tem that simply predicts the majority class (non-functions) on this data set would achieve an ac-curacy of 56%, well below the 81% accuracyof our classifier.
Many of the relations in ourtest set, like built(Person, House) and isriding(Person,Animal), do not ordinarilyhave named-entity extractions for both arguments,and would therefore not be amenable to previousfunction detection approaches.Some of our technique?s errors highlight inter-esting difficulties with function detection.
For in-stance, while we labeled the is capital ofrelation as a function, our technique predicted thatit was not.
It turns out that the country of Boliviahas two capitals, and the South Asian region ofJammu and Kashmir also has two capitals.
Bothof these facts are prominent enough on the Webto cause our system to detect a small probabilityfor PRightcapital of(2).
Thus any label for this rela-tion is somewhat unsatisfying: it is almost entirelyfunctional, but not strictly so.
By generalizing theproblem to one of determining a distribution forthe size of the argument, we can handle these bor-der cases in a useful way for QSD, as discussedbelow.6.2 Preliminary QSD ExperimentsWe test our complete QSD system on two impor-tant tasks.
In the first, the system is presentedwith a series of QSD problems Q in which thefirst quantifier Qq1is always ?a,?
and the second(Qq2) is always ?every.?
Each example is manu-ally labeled to indicate whether a direct or indirectreading of the sentence is preferred, and the sys-tem is charged with predicting the preferred read-ing.
In the second task, each Q has ?every?
asthe first quantifier, and ?a?
as the second quanti-fier.
Since indirect readings are very rarely pre-ferred for active-voice sentences of this form,wecharge the system with making a different type ofprediction: determine whether the indirect readingis plausible or not.
The system assumes that ev-ery sentence has a plausible direct reading, but bydetermining whether the indirect reading is plau-sible, it can determine whether the sentence is am-biguous between the two readings.We created data sets for these tasks by samplingour 5grams for examples containing the relationsin our function experiment.
From this set, we se-lected phrases that involved named classes for thearguments to the relation.
When a class was miss-ing, we either manually supplied one, or discardedthe example.
We then constructed two examplesfrom each combination of relation and argumentclasses: one example in which the first argumentis constrained by the quantifier ?a?
and the sec-ond by ?every,?
and a second example in whichthe quantifiers are reversed.
Finally, we manuallylabeled every example with a preference for director indirect reading (in the case of ?a/every?
exam-ples) or with a plausibility judgment for the indi-rect reading (in the case of ?every/a?
examples).Our final test sets included 46 labeled examplesfor each task.
Further experiments involving mul-tiple annotators, as in the experiments of Kurtz-man and MacDonald (1993), are of course desir-able, but note that even their experiments includedjust 32 labeled examples.Table 4 shows our results for the first QSD task,and Table 5 shows our results for the second one.In each case, we compare our supervised Cor-rected MLN model against an Uncorrected MLNmodel that uses no supervised data, and simplytakes its weights straight from our extracted distri-butions.
The supervised model uses a training cor-pus of 10 manually labeled examples for each task,five from each class.
We also compare against amajority class baseline.
Note that the Corrected1472Direct IndirectSystem Acc.
P R P RAll-Direct BL .53 .53 1.0 0.0 0.0Uncorrected MLN .58 .78 .30 .53 .90Corrected MLN .74 .77 .74 .71 .75Table 4: Our trained MLN outperforms two other sys-tems at predicting whether sentences of the form ?A/some<class 1> <relation> every <class 2>?
should have di-rect or indirect readings.
We measure accuracy over thewhole dataset, as well as precision and recall for the two sub-sets labeled with direct and indirect readings, respectively.Plausible Implaus.System Acc.
P R P RAll-Plausible BL .67 .67 1.0 0.0 0.0Uncorrected MLN .49 .89 .28 .38 .93Corrected MLN .72 .76 .86 .60 .43Table 5: Our trained MLN outperforms two other sys-tems at predicting whether sentences of the form ?Every<class 1> <relation> a/some <class 2>?
have a plausi-ble indirect reading or not.
We measure accuracy over thewhole dataset, as well as precision and recall for the two sub-sets labeled with plausible and implausible indirect readings.MLN model has balanced recall numbers for thetwo classes in both of our tasks, compared with theUncorrected MLN.
This indicates that our ZIPFFLATTENING technique is accurately learning bet-ter weights to remove the systematic bias in theUncorrected MLN.Our results demonstrate the utility of our ex-tracted distributions for these difficult tasks.
Al-though the extracted data prevents us from deter-mining that is capital of should be classi-fied as a function, since almost all of the prob-ability mass in PRight is still on n ?
{0, 1}.Thus, the probability for the direct reading ofa sentence like ?Some city is the capital of ev-ery country?
is still very low.
Likewise, eventhough our system (correctly) determines that therelation is a parent of is non-functional,it does not therefore group it with other non-functional relations like visited.
The dis-tribution PRightis parent of(n) is skewed to muchsmaller numbers for n than is the distribution forvisited, and thus the indirect reading for ?Aperson is the parent of every child?
is much morelikely than the indirect reading of ?A person vis-ited every country.
?The biggest hurdle for better performance isnoise in our extraction technique.
Polysemous re-lations sometimes have large counts for large ar-gument sizes in one sense, but not another.
Us-ing argument classes to disambiguate relations canhelp, but extractions for relations in combinationwith argument classes are much more sparse.
Im-proved extraction techniques could directly impactperformance on the QSD task.7 Conclusion and Future WorkWe have demonstrated targeted methods for ex-tracting world knowledge that is necessary formaking quantifier scope disambiguation deci-sions.
We have also demonstrated a novel,minimally-supervised, statistical relational modelin the Markov Logic Network framework for mak-ing QSD decisions based on extracted pragmatics.While our preliminary results for QSD arepromising, there are clearly many areas for im-provement.
We will need to handle more kinds ofquantifiers in our MLN model.
Our current systemis biased towards using purely pragmatic knowl-edge, but a complete system should also integratesyntactic and lexical constraints and preferences.Also, discourses can introduce knowledge that di-rectly affects QSD problems, such as constraintson the size of a particular set that is discussed inthe discourse.
Integrating our technique for QSDwith discourse processing is a major challenge thatwe hope to address.ReferencesHiyan Alshawi.
1990.
Resolving quasi logical forms.Computational Linguistics, 16(3):133?144.Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional information extraction.In Proceedings of the ACL.J.
Barwise and R. Cooper.
1981.
Generalized quanti-fiers and natural language.
Linguistics and Philoso-phy, 4(2):150?219.Ann Copestake, Dan Flickinger, Carl Pollard, andIvan A.
Sag.
2005.
Minimal recursion semantics:An introduction.
Research on Language and Com-putation, 3:281?332.D.
Davidov and A. Rappaport.
2008.
Unsuperviseddiscovery of generic relationships using pattern clus-ters and its evaluation by automatically generatedSAT analogy questions.
In Proceedings of the ACL.Doug Downey, Oren Etzioni, and Stephen Soderland.2005.
A Probabilistic Model of Redundancy in In-formation Extraction.
In IJCAI.1473Jerry R. Hobbs and Stuart M. Shieber.
1987.
An algo-rithm for generating quantifier scopings.
Computa-tional Linguistics, 13(1-2):47?63.Sven Hurum.
1988.
Handling scope ambiguities inEnglish.
In Proceedings of the Second Conferenceon Applied Natural Language Processing, pages 58?65.Howard S. Kurtzman and Maryellen C. MacDonald.1993.
Resolution of quantifier scope ambiguities.Cognition, 48:243?279.Longin Latecki.
1992.
Connection relations and quan-tifier scope.
In Proceedings of the ACL.Richard Montague.
1973.
The proper treatment ofquantification in ordinary English.
In Jaakko Hin-tikka, Julius Moravcsik, and Patrick Suppes, editors,Approaches to Natural Languages, pages 221?242.Reidel, Dordrecht.Douglas B. Moran.
1988.
Quantifier scoping in theSRI core language engine.
In Proceedings of the26th Annual Meeting of the Assoc.
for Comp.
Lin-guistics, pages 33?40.Jong C. Park.
1988.
Quantifier scope and constituency.In Proceedings of the 26th Annual Meeting of theAssoc.
for Comp.
Linguistics, pages 33?40.Massimo Poesio.
1993a.
Assigning a semantic scopeto operators.
In Proceedings of the ACL.Massimo Poesio.
1993b.
Assigning a semantic scopeto operators.
In Proceedings of the Second Confer-ence on Situation Theory and Its Applications.Ana-Maria Popescu.
2007.
Information Extractionfrom Unstructured Web Text.
Ph.D. thesis, Univer-sity of Washington.Uwe Reyle.
1995.
On reasoning with ambiguities.
InProceedings of the EACL, pages 1?8.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning,62:107?136.Alan Ritter, Doug Downey, Stephen Soderland, andOren Etzioni.
2008.
It?s a contradiction ?
No, it?snot: A case study using functional relations.
In Em-pirical Methods in Natural Language Processing.Walid S. Saba and Jean-Pierre Corriveau.
1997.
Apragmatic treatment of quantification in natural lan-guage.
In Proceedings of the National Conferenceon Artificial Intelligence.Walid S. Saba and Jean-Pierre Corriveau.
2001.
Plau-sible reasoning and the resolution of quantifier scopeambiguities.
Studia Logica, 67:271?289.Stefan Schoenmackers, Oren Etzioni, and Dan Weld.2008.
Scaling textual inference to the web.
In Pro-ceedings of EMNLP.Kurt Van Lehn.
1978.
Determining the scope of En-glish quantifiers.
Technical Report AI-TR-483, AILab, MIT.Alexander Yates and Oren Etzioni.
2009.
Unsuper-vised methods for determining object and relationsynonyms on the web.
Journal of Artificial Intelli-gence Research (JAIR), 34:255?296, March.Alexander Yates, Stefan Schoenmackers, and Oren Et-zioni.
2006.
Detecting parser errors using web-based semantic filters.
In Proceedings of EMNLP.1474
