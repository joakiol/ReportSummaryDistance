Multilingual Sentence GenerationTakako AikawaMaite MeleroLee SchwartzAndi WuMicrosoft ResearchOne Microsoft WayRedmond, WA 98008, USAtakakoa@microsoft.commaitem@microsoft.comleesc@micorosft.comandiwu@microsoft.comAbstractThis paper presents an overview of arobust, broad-coverage, andapplication-independent naturallanguage generation system.
Itdemonstrates how the differentlanguage generation componentsfunction within a multilingualMachine Translation (MT) system,using the languages that we arecurrently working on (English,Spanish, Japanese, and Chinese).Section 1 provides a systemdescription.
Section 2 focuses on thegeneration components and their coreset of rules.
Section 3 describes anadditional layer of generation rulesincluded to address application-specific issues.
Section 4 provides abrief description of the evaluationmethod and results for the MT systemof which our generation componentsare a part.1 System DescriptionWe present a natural language generationmethod in the context of a multi-lingual MTsystem.
The system that we have beendeveloping is a hybrid system with rule-based,example-based, and statistical components.Analysis and generation are performed withlinguistic parsers and syntactic realizationmodules, the rules of which are coded by hand.Transfer is accomplished using transferrules/mappings automatically extracted fromaligned corpora.The MT process starts with a source sentencebeing analyzed by the source-language parser,which produces as output a syntactic tree.
Thistree is input to the Logical Form module, whichproduces a deep syntactic representation of theinput sentence, called the LF (Heidorn, G. E.,2000).
The LF uses the same basic set ofrelation types for all languages.
Figure 1 givesthe syntactic tree and LF for the simple Englishsentence, ?I gave the pencils to John?.TreeLFFigure 1The LF is the final output of the analysis phaseand the input to the transfer phase.Transfer extracts a set of mappings from thesource-target language MindNet (Richardson,2000), a translation knowledge database, andapplies these mappings to the LF of the sourcesentence to produce a target LF.
The translationMindNet for a language pair is a repository ofaligned LFs and portions of LFs (produced byanalyzing sentence-aligned corpora).
Analignment of two LFs is a set of mappingsbetween a node or set of nodes (and the relationsbetween them) in the source LF and a node orset of nodes (and the relations between them) inthe target LF (Menezes & Richardson, 2001).In the translation process, the transfercomponent searches the alignments in theMindNet for those that match portions of the LFof the sentence being translated.
Mappings withlarger context are preferred to mappings withsmaller context and higher frequency mappingsare preferred to lower frequency mappings.
Thelemmas in any portion of the LF of the inputsentence that do not participate in a mapping aremapped to a target lemma using a bilingualdictionary.
The target LF fragments from thetransfer mappings and dictionary mappings arestitched together to produce the target LF(Menezes & Richardson, 2001).
For ourexample in Figure 1, the transfer componentproduces the following target LFs for Spanish,Japanese, and Chinese (Figure 2).1Source sentence: I gave the pencils to John.Transferred Spanish LF:Transferred Japanese LF:Transferred Chinese LF:Figure 2The transferred LF is the input to the generationcomponent, which we will discuss in detailbelow.2 Syntactic Generation ComponentThe different language generation modules inour system are syntactic realization componentsthat take as input an LF characteristic of thelanguage to be generated and produce asyntactic tree and surface string for thatlanguage.
In this sense, they are functionallysimilar to the REALPRO system (Lavoie andRambow, 1997).1 English gloss is provided in Figure 2 for readabilitypurposes only.The generation modules are not designedspecifically for MT, but rather are application-independent.
They can take as input an LFproduced by a dialog application, a critiquingapplication, a database query application, an MTapplication, etc.
They only require amonolingual dictionary for the language beinggenerated and an input LF that is characteristicof that language.
For each language there isonly one generation component that is used forall applications, and for MT, it is used fortranslation from all languages to that language.At the beginning of generation, the input LFis converted into a basic syntactic tree thatconforms to the tree geometry of the NLPsystem.
The nodes in LF become subtrees of thistree and the LF relations becomecomplement/adjunct relationships between thesubtrees.
This basic tree can be set up indifferent ways.
For English, Spanish, andChinese, we set it up as strictly head-initial withall the complements/adjuncts following thehead, resembling the tree of a VSO language.For Japanese, we set it up as strictly head-final,with all the complements/adjuncts preceding thehead.
Figure 3 gives the basic Spanishgeneration tree produced from the Spanishtransferred LF in Figure 2.Figure 3The generation rules apply to the basic tree,transforming it into a target language tree.
In theapplication of the rules, we traverse the tree in atop-down, left-to-right, depth-first fashion,visiting each node and applying the relevantrules.
Each rule can perform one or more of thefollowing operations:(1) Assign a syntactic label to the node.
Forexample, the ?DECL?
label will be assignedto the root node of a declarative sentence.
(2) Modify a node by changing someinformation within the node.
For example, apronoun might be marked as reflexive if it isfound to be co-referential with the subject ofthe clause it is in.
(3) Expand a node by introducing new node(s)into the tree.
For example, the ?Definite?
(+Def) feature on a node may become adeterminer phrase attached to the syntacticsubtree for that node.
(4) Delete a node.
For example, for a pro-droplanguage, a pronominal subject may beremoved from the tree.
(5) Move a node by deleting it from Position Aand inserting it in Position B.
For example,for an SVO language, the subject NP of asentence may be moved from a post-verbalposition to a pre-verbal position.
(6) Ensure grammatical agreement betweennodes.
For example, if the subject of asentence is first person singular, thosenumber and person features will be assignedto the main verb.
(7) Insert punctuation and capitalization.The nodes in the generated tree are linked toeach other by relations such as ?head?, ?parent?and ?sibling?.
The entire tree is thus visiblefrom any given node via these relations.
Whena rule is applied to a node, the decisions made inthat rule can be based not just on features of thatnode, but also on features of any other node inthe tree.
This basically eliminates the need forbacktracking, which would be necessary only ifthere were local ambiguities resulting from theabsence of global information.
In this sense, ourapproach is similar to that of other large-scalegenerators (Tomita and Nyberg, 1988).The generation rules operate on a single tree.Rule application is deterministic and thus veryefficient.
If necessary, the tree can be traversedmore than once, as is the case in the generationmodules for the languages we are currentlyworking on.
There is a ?feeding?
relationshipamong the rules.
The rules that assignpunctuation and capitalization, for example, donot apply until all the movement rules haveapplied, and movement rules do not apply untilnodetypes and functional roles are assigned.To improve efficiency and to prevent a rulefrom applying at the wrong time or to the wrongstructure, the rules are classified into differentgroups according to the passes in which they areapplied.
Each traversal of the tree activates agiven group of rules.
The order in which thedifferent groups of rules are applied depends onthe feeding relations.For the simple example in Figure 2 above,the Spanish, Chinese, and Japanese generationcomponents all have an initial pass that assignsnodetypes and functional roles and a final passthat inserts punctuation marks.In addition, the Spanish component, in a firstpass that identifies syntactic functions, deletesthe pronominal subject and inserts a dative cliticpronoun.
It also inserts the definite article andthe personal marker ?a?.
In a second pass, itchecks agreement between indirect object anddoubled clitic as well as between subject andverb, assigning the appropriate person, number,and gender agreement information to theterminal nodes.Reordering operations, such as moving theclitic in front of the verb, if the verb is finite, orafter, if it is non-finite, come later.
The last passtakes care of euphonic issues, such ascontractions or apocopated adjectives.
Figure 4ashows the resulting tree.Figure 4aThe Chinese component has a node-modification pass, which adds the FUNCWnode headed by (le) to indicate past tense.
Inthis pass the direct object is also turned into aprepositional phrase introduced by (ba) toshow the definiteness of the NP.
Following thispass, a movement pass moves the subject infront of the verb.Figure 4bThe Japanese component has a pass in whichcase-markers or modifiers are inserted.
InFigure 4c, the nominative, the accusative, andthe dative case markers are inserted in thesubject, direct object, and indirect object NPs,respectively.
Also, the demonstrativecorresponding to English "that" is inserted at thebeginning of the definite NP (pencil).Figure 4cAfter the grammatical rules apply, themorphological rules apply to the leaf nodes ofthe tree.
Since each node in the tree is a featurematrix and agreement information has alreadybeen assigned by the generation rules,morphological processing simply turns thefeature matrices into inflected forms.
Forinstance, in our Spanish example, the verb ?dar?with the ?past?, ?singular?
and ?1st person?features is spelled out as ?di?.
Once all thewords are inflected, the inflected form of eachleaf node is displayed to produce the surfacestring.
This completes the generation process,as exemplified for Spanish in Figure 5.Figure 53 Application-Driven GenerationThe example used in the previous sections isquite simple, and not representative of the actualproblems that arise in MT.
Applications, suchas MT, that automatically create input for thegeneration component for a language will notalways produce ideal LFs for that language, i.e.,LFs that could have been produced by theanalysis modules for that language.We have designed the generationcomponents, therefore, to add a degree ofrobustness to our applications.
To some extent,and based only on information about thelanguage being generated, the generationcomponents will fix incomplete or inconsistentLFs and will verify that the structures theygenerate comply with the constraints imposedby the target language.The core generation rules are designed to beapplication-independent and source-language-independent.
Expanding the rule base to coverall the idiosyncrasies of the input wouldcontaminate the core rules and result in loss ofgenerality.
In order to maintain the integrity ofthe core rules while accommodating imperfectinput, we have opted to add a pre-generationlayer to our generation components.Pre-generation rules apply before the basicsyntactic tree is built.
They can modify theinput LF by adding or removing features,changing lemmas, or even changing structuralrelations.
Below we give examples of problemssolved in the pre-generation layers of ourdifferent language generation modules.
Theseillustrate not just the source-languageindependence, but also the application-independence of the generation modules.We start with the English generationcomponent, which was used in experimentalquestion-answering applications before beingused in MT.
Among the pre-generation rules inthis component is one that removes the markerindicating non-restrictive modification (Nonrest)from LF nodes that are not in a modificationrelationship to another LF node.
So, forexample, when the question-answeringapplication is presented with the query ?Whendid Hitler come to power,?
the NLP systemanalyzes the question, produces an LF for it,searches its Encarta Mindnet (which containsthe LFs for the sentences in the Encartaencyclopedia), retrieves the LF fragment inFigure 6, and sends it to the English generationcomponent.Figure 6The LF that is the input to generation in thisexample is a portion of the LF representation ofa complete sentence that includes the phrase?Hitler, who came to power in 1933.?
The partof that sentence that answers the question is thenonrestrictive relative clause ?who came topower in 1933.?
Yet, we do not want togenerate the answer as a non-restrictive relativeclause (as indicated by Nonrest in the LF), butas a declarative sentence.
So, rather than pollutethe core generation rules by including checks forimplausible contexts in the rule for generatingnonrestrictive modifiers, a pre-generation rulesimply cleans up the input.
The rule isapplication-independent (though motivated by aparticular application) and can only serve toclean up bad input, whatever its source.An example of a rule motivated by MT, butuseful for other applications, is the pre-generation rule that changes the quantifier ?less?to ?fewer?, and vice versa, in the appropriatesituations.
When the LF input to the Englishgeneration component specifies ?less?
as aquantifier of a plural count noun such as ?car,?this rule changes the quantifier to ?fewer?.Conversely, when an input LF has ?fewer?specified as a quantifier of a mass noun such as?luck?, the rule changes it to ?less.?
This rulemakes no reference to the source of the input togeneration.
This has the advantage that it willapply in a grammar-checking application as wellas in an MT application (or any otherapplication).
If the input to English generationwere the LF produced for the ungrammaticalsentence ?He has less cars,?
the generationcomponent would produce the correct ?He hasfewer cars,?
thereby effectively grammarchecking the sentence.
And, if the ultimatesource of the same input LF were the Spanishsentence ?Juan tiene menos coches, ?
the resultwould be the same, even if ?menos?
whichcorresponds to both ?less?
and ?fewer?
inEnglish, were not transferred correctly.
Anothertype of problem that a generation componentmight encounter is the absence of necessaryinformation.
The Spanish generationcomponent, for instance, may receive as inputunderspecified nominal relations, such as theone exemplified in Figure 7, in which a noun(registro) is modified by another noun(programa).
The relationship between the twonouns needs to be made explicit, in Spanish, bymeans of a preposition when the modifyingnoun is not a proper noun.
Absent the necessaryinformation in the incoming LF, a pre-generation rule introduces the defaultpreposition ?de?
to specify this relationship.Figure 7Another example of a pre-generation rule, thistime from Japanese, deals with the unspecified1st/2nd person pronominal subject for particulartypes of predicates.
The 1st/2nd person pronoun( ) is not used as the subject insentences that express the speaker?s/thelistener?s desire (unless there is somefocus/contrast on the subject).
So, one of theJapanese pre-generation rules deletes the subjectin the input LF that involves such a predicate.For instance, below is the input LF, the modifiedLF, and the string produced from the Englishsentence ?I want to read the book.
?Figure 8From Chinese, we give an example of a rule thatactually changes the structure of an LF.
In oursystem, it is possible for the source and targetlanguages to have different LF representationsfor similar structures.
In English and otherEuropean languages, for example, the verb ?BE?is required in sentences like ?He is smart?.
InChinese, however, no copula is used.
Instead,an adjectival predicate is used.
While we mightattempt at the LF level to unify theserepresentations, we have not yet done so.Moreover, the LF in our system is not intendedto be an interlingua representation.
Differencesbetween languages and their LFs are tolerated.Therefore, Chinese uses a pre-generation rule totransform the be-predicate adjective LF into itsChinese equivalent as shown in Figure 9, thoughwe soon expect transfer to automatically do this.Figure 94 EvaluationThe generation components described in theprevious sections are part of an MT system thathas been run on actual Microsoft technicaldocumentation.
The system is frequentlyevaluated to provide a measure of progress andto yield feedback on its design and development.In evaluating our progress over time andcomparing our system with others, we haveperformed several periodic, blind humanevaluations.
We focus here on the evaluation ofour Spanish-English and English-Spanishsystems.For each evaluation, several human ratersjudge the same set of 200-250 sentencesrandomly extracted from our technical corpora(150K sentences).2 The raters are not shown thesource language sentence; instead, they arepresented with a human translation, along withtwo machine-generated translations.
Their taskis to choose between the alternatives, using thehuman translation as a reference.Table 1 summarizes a comparison of theoutput of our Spanish-English system with thatof Babelfish (http://world.altavista.com/).Table 2 does the same for our English-Spanishsystem and Lernout & Hauspie?s English-Spanish system (http://officeupdate.lhsl.com/).In these tables, a rating of 1 means that ratersuniformly preferred the translation produced byour system; a rating of 0 means that they did notuniformly prefer either translation; a rating of -1means that they uniformly preferred thetranslation produced by the alternative system.3Beside each rating is a confidence measure forthe mean preference at the .99 level (Richardson,S., et al(2001)).Spanish-EnglishSystemsMean preferencescore (7 raters)SamplesizeOur 4/01 (2001)MT vs. Babelfish0.32 ?
0.11(at .99)250sentencesTable 1.
Our Spanish-English MT vs. BabelfishEnglish-SpanishSystemsMean preferencescore (5 raters)SamplesizeOur 4/01 (2001)MT vs. L&H0.19 ?
0.14(at 0.99)250sentencesTable 2.
Our English-Spanish MT vs. Lernout &Hauspie2 The human raters used for these evaluations work for anindependent agency and played no development rolebuilding the systems they test.3 In interpreting our results, it is important to keep in mindthat our MT system has been customized to the test domain,while the Babelfish and Lernout & Hauspie systems havenot.5 ConclusionIn this paper we have presented an overview ofthe natural language generation componentdeveloped at Microsoft Research and havedemonstrated how this component functionswithin a multilingual Machine Translationsystem.
We have provided motivation for thegeneration architecture, which consists of a setof core rules and a set of application-driven pre-generation rules, within a wide-coverage, robust,application-independent, multilingual naturallanguage processing system.
In addition wehave presented evaluation figures for Spanish-English and English-Spanish, two of thelanguage pairs of the MT system in which ourgeneration components are used.6 ReferencesHeidorn, G. E. (2000): Intelligence WritingAssistance.
In Dale R., Moisl H., and SomersH.
(eds.
), A Handbook of Natural LanguageProcessing: Techniques and Applications forthe Processing of Language as Text.
MarcelDekker, New York, 1998 (published inAugust 2000), pages 181-207.Jensen, K., Heidorn G., and Richardson S.(eds.)
(1993): Natural Language Processing:The PLNLP Approach, Boston, Kluwer.Lavoie, Benoit and Owen Rambow.
(1997): Afast and portable realizer for text generation.In Proceedings of the Fifth Conference onApplied Natural-Language Processing(ANLP-1997), pages 265-268.Melero, M. and Font-Llitjos, A.
(2001):Construction of a Spanish Generation modulein the framework of a General-Purpose,Multilingual Natural Language ProcessingSystem.
In Proceedings of the VIIInternational Symposium on SocialCommunication, Santiago de Cuba.Reiter, E. and Dale, R. (2000): Building NaturalLanguage Generation Systems, CambridgeUniversity Press.Richardson, S., et al(2001): Overcoming thecustomization bottleneck using example-based MT, Paper submitted for Data-drivenMT Workshop at ACL 2001, Toulouse,France.Richardson, S. (2000): The evolution of an NLPSystem.
NLP Group Microsoft Research,Presentation at the LREC?2000 Athens,Greece.Tomita, M. and Nyberg E. (1988): The GenKitand Transformation Kit User?s Guide.Technical Report CMU-CMT-88-MEMO,Centre for Machine Translation, CarnegieMellon University.
