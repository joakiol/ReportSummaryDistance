Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 939?946, Vancouver, October 2005. c?2005 Association for Computational LinguisticsIntegrating linguistic knowledge in passage retrieval for question answeringJo?rg TiedemannAlfa Informatica, University of GroningenOude Kijk in ?t Jatstraat 269712 EK Groningen, The Netherlandsj.tiedemann@rug.nlAbstractIn this paper we investigate the use of lin-guistic knowledge in passage retrieval aspart of an open-domain question answer-ing system.
We use annotation producedby a deep syntactic dependency parser forDutch, Alpino, to extract various kinds oflinguistic features and syntactic units tobe included in a multi-layer index.
Sim-ilar annotation is produced for natural lan-guage questions to be answered by thesystem.
From this we extract query termsto be sent to the enriched retrieval index.We use a genetic algorithm to optimize theselection of features and syntactic unitsto be included in a query.
This algo-rithm is also used to optimize further pa-rameters such as keyword weights.
Thesystem is trained on questions from thecompetition on Dutch question answeringwithin the Cross-Language Evaluation Fo-rum (CLEF).
We could show an improve-ment of about 15% in mean total recip-rocal rank compared to traditional infor-mation retrieval using plain text keywords(including stemming and stop word re-moval).1 IntroductionImproving information retrieval (IR) through natu-ral language processing (NLP) has been the goalfor many researchers.
NLP techniques such aslemmatization and compound splitting have beenused in several studies (Krovetz, 1993; Hollink et al,2003).
Linguistically motivated syntactic units suchas noun phrases (Zhai, 1997), head-modifier pairs(Fagan, 1987; Strzalkowski et al, 1996) and subject-verb-object triples (Katz and Lin, 2003) have alsobeen integrated in information retrieval.
However,most of these studies resulted in only little successor even decreasing performance.
It has been arguedthat NLP and especially deep syntactic analysis isstill too brittle and ineffective (Katz and Lin, 2003).Integrating NLP in information retrieval seemsto be very hard because the task here is to matchplain text keywords to natural language documents.In question answering (QA), however, the task isto match natural language questions to relevant an-swers within document collections.
For this, wehave to analyze the question in order to determinewhat kind of answer the user is expecting.
Tradi-tional information retrieval is used in QA systems tofilter out relevant passages from the document col-lection which are then processed to extract possibleanswers.
Hence, the performance of this passage re-trieval component (especially in terms of recall) iscrucial for the success of the entire system.
NLPtools and linguistic resources are frequently used inQA systems, e.g.
(Bernardi et al, 2003; Moldovanet al, 2002), although not very often for passageretrieval (some exceptions are (Strzalkowski et al,1996; Katz and Lin, 2003; Neumann and Sacaleanu,2004)).Our goal is to utilize information that can be ex-tracted from the analyzed question in order to matchlinguistic features and syntactic units in analyzed939documents.
The main research question is to findappropriate units and features that actually help toimprove the retrieval component.
Furthermore, wehave to find an appropriate way of combining queryterms to optimize IR performance.
For this, we ap-ply an iterative learning approach based on examplequestions annotated with their answers.In the next section we will give a brief descriptionof our question answering system with focus on thepassage retrieval component.
Thereafter we will dis-cuss the query optimization algorithm followed by asection on experimental results.
The final sectioncontains our conclusions.2 Question answering with dependencyrelationsOur Dutch question answering system, Joost(Bouma et al, 2005), consists of two streams: a tablelook-up strategy using off-line information extrac-tion and an on-line strategy using passage retrievaland on-the-fly answer extraction.
In both strate-gies we use syntactic information produced by awide-coverage dependency parser for Dutch, Alpino(Bouma et al, 2001).
In the off-line strategy we usesyntactic patterns to extract information from unre-stricted text to be stored in fact tables (Jijkoun etal., 2004).
For the on-line strategy, we assume thatthere is a certain overlap between syntactic relationsin the question and in passages containing the an-swers.
Furthermore, we also use strategies for rea-soning over dependency rules to capture semanticrelationships that are expressed by different syntac-tic patterns (Bouma et al, 2005).Our focus is set on open-domain question an-swering using data from the CLEF competition onDutch QA.
We have parsed the entire corpus pro-vided by CLEF with about 4,000,000 sentences inabout 190,000 documents.
The dependency trees arestored in XML and are directly accessible from theQA system.
Syntactic patterns for off-line informa-tion extraction are run on the entire corpus.
For theon-line QA strategy we use traditional informationretrieval to select relevant passages from the corpusto be processed by the answer extraction modules.This step is necessary to reduce the search space forthe QA system to make it feasible to run on-line QA.As segmentation level we used paragraphs markedin the corpus (about 1.1 million).Questions are parsed within the QA system usingthe same parser.
Using their analysis, the system de-termines the question type and, hence, the expectedanswer type.
According to the type, we try to findthe answer first in the fact database (if an appropri-ate table exists) and then (as fallback) in the corpususing the on-line QA strategy.2.1 Passage retrieval in JoostInformation retrieval is one of the bottle-necks in theon-line strategy of our QA system.
The system re-lies on the passages retrieved by this component andfails if IR does not provide relevant documents.
Tra-ditional IR uses a bag-of-words approach using plaintext keywords to be matched with word-vectors de-scribing documents.
The result is usually a rankedlist of documents.
Simple techniques such as stem-ming and stop word removal are used to improve theperformance of such a system.
This is also the base-line approach for passage retrieval in our QA sys-tem.The passage retrieval component in Joost includesan interface to seven off-the shelf IR systems.
Oneof the systems supported is Lucene from the ApacheJakarta project (Jakarta, 2004).
Lucene is a widely-used open-source Java library with several exten-sions and useful features.
This was the IR engine ofour choice in the experiments described here.
Forthe base-line we use standard settings and a pub-lic Dutch text analyzer for stemming and stop wordremoval.
Now, the goal is to extend the base-lineby incorporating linguistic information produced bythe syntactic analyzer.
Figure 1 shows a dependencytree produced for one of the sentences in the CLEFcorpus.
We like to include as much information fromthe parsed data as possible to find better matches be-tween an analyzed question and passages that con-tain answers.
From the parse trees, we extract vari-ous kinds of linguistic features and syntactic units tobe stored in the index.
Besides the dependency rela-tions the parser also produces part-of-speech (POS)tags, named entity labels and linguistic root forms.
Italso recognizes compositional compounds and par-ticle verbs.
All this information might be useful forour passage retrieval component.Lucene supports multiple index fields that can befilled with different types of data.
This is a useful940topsmainsu1npdetdethet0hdnounembargo1modpphdpreptegen2obj1nameIrak3vcppartobj11hdverbstel in5modpphdprepna6obj1npdetdetde7hdnouninval8modpphdprepin9obj1nameKoeweit10modpphdprepin11obj1noun199012hdverbword4Figure 1: A dependency tree produced by Alpino:Het embargo tegen Irak werd ingesteld na de invalin Koeweit in 1990.
(The embargo against Iraq hasbeen declared after the invasion of Kuwait in 1990.
)feature since it allows one to store various kinds ofinformation in different fields in the index.
Hence-forth, we will call these data fields index layers and,thus, the index will be called a multi-layer index.
Wedistinguish between token layers, type layers and an-notation layers.
Token layers include one item pertoken in the corpus.
Table 1 lists token layers de-fined in our index.Table 1: Token layerstext plain text tokensroot root formsRootPOS root form + POS tagRootHead root form + headRootRel root form + relation nameRootRelHead root form + relation + headWe included various combinations of features de-rived from the dependency trees to make it possi-ble to test their impact on IR.
Features are simplyconcatenated (using special delimiting symbols be-tween the various parts) to create individual items inthe layer.
For example, the RootHead layer containsconcatenated dependent-head bigrams taken fromthe dependency relations in the tree.
Tokens in thetext layer and in the root layer have been split at hy-phens and underscores to split compositional com-pounds and particle verbs (Alpino adds underscorestopwhqwhd1advwanneer0bodysv1mod1hdverbstel in1sunpdetdetde2hdnameVerenigde Naties3obj1npdetdeteen5hdnounembargo6svppartin7modpphdpreptegen8obj1nameIrak9Figure 2: A dependency tree for a question: Wan-neer stelde de Verenigde Naties een embargo integen Irak?
(When did the United Nations declarethe embargo against Iraq?
)between the compositional parts).
Type layers in-clude only specific types of tokens in the corpus, e.g.named entities or compounds (see table 2).Table 2: Type layerscompound compoundsne named entitiesneLOC location namesnePER person namesneORG organization namesAnnotation layers include only the labels of (certain)token types.
So far, we defined only one annotationlayer for named entity labels.
This layer may containthe items ?ORG?, ?PER?
or ?LOC?
if such a namedentity occurs in the text passage.3 Query formulationQuestions are analyzed in the same way as sentencesin documents.
Hence, we can extract appropriateunits from analyzed questions to be matched withthe various layers in the index.
For example, wecan extract root-head word pairs to be matched withthe RootHead layer.
In this way, each layer can bequeried using keywords of the same type.
Further-more, we can also use linguistic labels to restrict ourquery terms in several ways.
For example, we canuse part-of-speech labels to exclude keywords of acertain word class.
We can also use the syntactic re-lation name to define query constraints.
Each tokenlayer can be restricted in this way (even if the featureused for the restriction is not part of the layer).
For941example, we can limit our set of root keywords tonouns even though part-of-speech labels are not partof the root layer.
We can also combine constraints,for example, RootPOS keywords can be restricted tonouns that are in an object relation within the ques-tion.Another feature of Lucene is the support of key-word weights.
Keywords can be ?boosted?
using so-called ?boost factors?.
Furthermore, keywords canalso be marked as ?required?.
These two featurescan be applied to all kinds of keywords (token layer,type layer, annotation layer keywords, and restrictedkeywords).The following list summarizes possible keywordtypes in our passage retrieval component:basic: a keyword in one of the index layersrestricted: token-layer keywords can be restricted to a certainword class and/or a certain relation type.
We use only thefollowing word class restrictions: noun, name, adjective,verb; and the following relation type restrictions: directobject, modifier, apposition and subjectweighted: keywords can be weighted using a boost factorrequired: keywords can be marked as requiredQuery keywords from all types can be combined intoa single query.
We connect them in a disjunctive waywhich is the default operation in Lucene.
The queryengine provides ranked query results and, therefore,each disjunction may contribute to the ranking of theretrieved documents but does not harm the query ifit does not produce any matching results.
We may,for example, form a query with the following ele-ments: (1) all plain text tokens; (2) named entities(ne) boosted with factor 2; (3) RootHead bigramswhere the root is in an object relation; (4) RootRelkeywords for all nouns.
Applying these parame-ters to the question in figure 2 we get the followingquery:1text:(Irak embargo Verenigde Naties stelde)ne:(Irak?2 Verenigde_Naties?2)RootHead:(Irak/tegen embargo/stel_in)RootRel:(embargo/obj1)Now, query terms from various keyword types mayrefer to the same index layer.
For example, we mayuse weighted plain text keywords restricted to nounstogether with unrestricted plain text keywords.
To1Note that stop words have been removed.combine them we use a preference mechanism tokeep queries simple and to avoid disjunctions withconflicting keyword parameters: (a) Restricted key-word types are more specific than basic keywords;(b) Keywords restricted in relation type and POS aremore specific than keywords with only one restric-tion; (c) Relation type restrictions are more specificthan POS restrictions.
Using these rules we definethat weights of more specific keywords overwriteweights of less specific ones.
Furthermore, we de-fine that the ?required-marker?
(?+?)
overwrites key-word weights.
Using these definitions we would getthe following query if we add two elements to thequery from above: (5) plain text keywords in an ob-ject relation with boost factor 3 and (6) plain textkeywords labeled as names marked as required.text:(Irak?3 embargo?3 +Verenigde +Natiesstelde)ne:(Irak?2 Verenigde_Naties?2)RootHead:(Irak/tegen embargo/stel_in)RootRel:(embargo/obj1)Finally, we can also use the question type deter-mined by question analysis in the retrieval compo-nent.
The question type corresponds to the expectedanswer type, i.e.
we expect an entity of that type inthe relevant text passages.
In some cases, the ques-tion type can be mapped to one of the named entitylabels assigned by the parser, e.g.
a name question islooking for names of persons (ne = PER), a questionfor a capital is looking for a location (ne = LOC) anda question for organizations is looking for the nameof an organization (ne = ORG).
Hence, we can addanother keyword type, the expected answer type tobe matched with named entity labels in the ne layer,cf.
(Prager et al, 2000).There are many possible combinations of restric-tions even with the small set of POS labels and rela-tion types listed above.
However, many of them areuseless because they cannot be instantiated.
For ex-ample, an adjective cannot appear in subject relationto its head.
For simplicity we limit ourselves to thefollowing eight combined restrictions (POS + rela-tion type): names + {direct object, modifier, apposi-tion, subject} and nouns + {direct object, modifier,apposition, subject}.
These can be applied to all to-ken layers in the same way as the other restrictionsusing single constraints.Altogether we have 109 different keyword types942using the layers and the restrictions defined above.Now the question is to select appropriate keywordtypes among them with the optimal parameters(weights) to maximize retrieval performance.
Thefollowing section describes the optimization proce-dure used to adjust query parameters.4 Optimization of query parametersIn the previous sections we have seen the internalstructure of the multi-layer index and the queries weuse in our passage retrieval component.
Now wehave to address the question of how to select layersand restrict keywords to optimize the performanceof the system according to the QA task.
For thiswe employ an automatic optimization procedure thatlearns appropriate parameter settings from exampledata.
We use annotated training material that is de-scribed in the next section.
Thereafter, the optimiza-tion procedure is introduced.4.1 CLEF questions and evaluationWe used results from the CLEF competition onDutch QA from the years 2003 and 2004 for train-ing and evaluation.
They contain natural languagequestions annotated with their answers found in theCLEF corpus (answer strings and IDs of documentsin which the answer was found).
Most of the ques-tions are factoid questions such as ?Hoeveel inwon-ers heeft Zweden??
(How many inhabitants doesSweden have?).
Altogether there are 631 questionswith 851 answers.2Standard measures for evaluating information re-trieval results are precision and recall.
However,for QA several other specialized measures havebeen proposed, e.g.
mean reciprocal rank (MRR)(Vorhees, 1999), coverage and redundancy (Robertsand Gaizauskas, 2004).
MRR accounts only for thefirst passage retrieved containing an answer and dis-regards the following passages.
Coverage and re-dundancy on the other hand disregard the rankingcompletely and focus on the sets of passages re-trieved.
However, in our QA system, the IR score2Each question may have multiple possible answers.
Wealso added some obvious answers which were not in the originaltest set when encountering them in the corpus.
For example,names and numbers can be spelled differently (Kim Jong Il vs.Kim Jong-Il, Saoedi-Arabie?
vs.
Saudi-Arabie?, bijna vijftig jaarvs.
bijna 50 jaar)(on which the retrieval ranking is based) is one ofthe clues used by the answer identification modules.Therefore, we use the mean of the total reciprocalranks (MTRR), cf.
(Radev et al, 2002), to combinefeatures of all three measures:MTRR = 1xx?i=1?d?Ai1rankRi(d)Ai is the set of retrieved passages containing ananswer to question number i (subset of Ri) andrankRi(d) is the rank of document d in the list ofretrieved passages Ri.In our experiments we used the provided answerstring rather than the document ID to judge if a re-trieved passage was relevant or not.
In this way,the IR engine may provide passages with correct an-swers from other documents than the ones marked inthe test set.
We do simple string matching betweenanswer strings and words in the retrieved passages.Obviously, this introduces errors where the match-ing string does not correspond to a valid answer inthe context.
However, we believe that this does notinfluence the global evaluation figure significantlyand therefore we use this approach as a reasonablecompromise when doing automatic evaluation.4.2 Learning query parametersAs discussed earlier, there is a large variety of possi-ble keyword types that can be combined to query themulti-layer index.
Furthermore, we have a numberof parameters to be set when formulating a query,e.g.
the keyword weights.
Selecting the appropri-ate keywords and parameters is not straightforward.We like to carry out a systematic search for optimiz-ing parameters rather than using our intuition.
Here,we use the information retrieval engine as a blackbox with certain input parameters.
We do not knowhow the ranking is done internally or how the outputis influenced by parameter changes.
However, wecan inspect and evaluate the output of the system.Hence, we need an iterative approach for testing sev-eral settings to optimize query parameters.
The out-put for each setting has to be evaluated according toa certain objective function.
For this, we need an au-tomatic procedure because we want to check manydifferent settings in a batch run.
The performance ofthe system can be measured in several ways, e.g.
us-943ing the MTRR scores described in the previous sec-tion.
We have chosen to use this measure and theannotated CLEF questions to evaluate the retrievalperformance automatically.We decided to use a simplified genetic algorithmto optimize query parameters.
This algorithm isimplemented as an iterative ?trial-and-error beamsearch?
through possible parameter settings.
Theoptimization loop works as follows (using a sub-setof the CLEF questions):1.
Run initial queries (one keyword type per IR run) withdefault weights.2.
Produce a number of new settings by combining two pre-vious ones (= crossover).
For this, select two settingsfrom an N-best list from the previous IR runs.
Apply mu-tation operations (see next step) until the new settings areunique (among all settings we have tried so far).3.
Change some of the new settings at random (= mutation)using pre-defined mutation operations.4.
Run the queries using the new settings and evaluate theretrieval output (determine fitness).5.
Continue with 2 until some stop condition is satisfied.This optimization algorithm is very simple but re-quires some additional parameters.
First of all, wehave to set the size of the population, i.e.
the num-ber of IR runs (individuals) to be kept for the nextiteration.
We decided to keep the population smallwith only 25 individuals.
Then we have to decidehow to evaluate fitness to rank retrieval results.
Thisis done using the MTRR measure.
Natural selectionusing these rankings is simplified to a top-N searchwithout giving individuals with lower fitness valuesa chance to survive.
This also means that we canupdate the population directly when a new IR run isfinished.
We also have to set a maximum number ofnew settings to be created.
In our experiments welimit the process to a maximum of 50 settings thatmay be tried simultaneously.
A new setting is cre-ated as soon as there is a spot available.An important part of the algorithm is the com-bination of parameters.
We simply merge the set-tings of two previous runs (parents) to produce anew setting (a child).
That means that all keywordtypes (with their restrictions) from both parents areincluded in the child?s setting.
Parents are selected atrandom without any preference mechanism.
We alsouse a very simple strategy in cases where both par-ents contain the same keyword type.
In these caseswe compute the arithmetic mean of the weight as-signed to this type in the parents?
settings (defaultweight is one).
If the keyword type is marked as re-quired in one of the parents, it will also be marked asrequired in the child?s setting (which will overwritethe keyword weight if it is set in the other parent).Another important principle in genetic optimiza-tion is mutation.
It refers to a randomized modifi-cation of settings when new individuals are created.First, we apply mutation operations where new set-tings are not unique.3 Secondly, mutation operationsare applied with fixed probabilities to new settings.In most genetic algorithms, settings are convertedto genes consisting of bit strings.
A mutation op-eration is then defined as flipping the value of onerandomly chosen bit.
In our approach, we do notuse bit strings but define several mutation operationsto modify parameters directly.
The following opera-tions have been defined:?
a new keyword type is added to new settingswith a chance of 0.2?
a keyword type is removed from the settingswith a chance of 0.1?
a keyword weight (boost factor) is modified bya random value between -5 and 5 with a chanceof 0.2 (but only if the weight remains a positivevalue)?
a keyword type is marked as required with achance of 0.01All these parameters are intuitively chosen.
We as-signed rather high probabilities to the mutation op-erations to reduce the risk of local maximum traps.Note that there is no obvious condition for termi-nation.
In randomized approaches like this one thedevelopment of the fitness score is most likely notmonotonic and therefore, it is hard to predict whenwe should stop the optimization process.
However,we expect the scores to converge at some point andwe may stop if a certain number of new settings doesnot improve the scores anymore.3We require unique settings in our implementation becausewe want to avoid re-computation of fitness values for settingsthat have been tried already.
?Good?
settings survive anywayusing our top-N selection approach.9445 ExperimentsWe selected a random set of 420 questions from theCLEF data for training and used the remaining 150questions for evaluation.
We used the optimizationalgorithm with the settings as described above.
IRwas run in parallel on 3-7 Linux workstations on alocal network.
We retrieved a maximum of 20 pas-sages per question.
For each setting we computedthe fitness scores for the training set and the eval-uation set using MTRR.
The top scores have beenprinted after each 10 runs and compared to the eval-uation scores.
Figure 3 shows a plot of the fitnessscore development throughout the optimization pro-cess in comparison with the evaluation scores.0.850.90.9511.05400 800 1200 1600 2000 2400 2800 3200answerstringMTRRnumber of settingsevaluation base-line: 0.8799trainingevaluationFigure 3: Parameter optimizationThe base-line of 0.8799 refers to the retrieval re-sult on evaluation data when using traditional IRwith plain text keywords only (i.e.
using the textlayer, Dutch stemming and stop word removal).
Thebase-line performance on training data is slightlyworse with 0.8224 MTRR.
After 1130 settings theMTRR scores increased to 0.9446 for training dataand 1.0247 for evaluation data.
Thereafter we canobserve a surprising drop in evaluation scores toaround 0.97 in MTRR.
This might be due to over-fitting although the drop seems to be rather radi-cal.
After that the curve of the evaluation scoresgoes back to about the same level as achieved be-fore and the training curve seems to level out.
TheMTRR score after 3200 settings is at 1.0169 on eval-uation data which is a statistically significant im-provement of the baseline score (tested using theWilcoxon matched-pairs signed-ranks test at p <0.01).
MTRR measured on document IDs and eval-uation data did also increase from 0.5422 to 0.6215which is statistically significant at p?0.02.
Coveragewent up from 78.68% to 81.62% on evaluation dataand the redundancy was improved from 3.824 to4.272 (significance tests have not been carried out).Finally, the QA performance using Joost with onlythe IR based strategy was increased from 0.289 (us-ing CLEF scores) to 0.331.
This, however, is not sta-tistically significant according to the Wilcoxon testand may be due to chance.Table 3: Optimized parameters (3200 settings)weighted keywords required keywordslayer restriction weight layer restrictiontext 7.43 root nametext name 11.94text adj 9.14 RootPOStext mod 5.83 RootPOS obj1text verb 4.33 RootPOS noun-modtext noun-app 3.70root 4.45 RootRelroot noun-su 2.65 RootRel approot name-mod 9.71 RootRel noun-approot noun-obj1 0.09 RootRel noun-modroot mod 0.81 RootRel noun-obj1root verb 0.01RootHead noun-app 7.65 RootRelHead suRootHead noun-mod 5.24 RootRelHead adjRootHead name-su 1 RootRelHead name-appRootRel mod 4.45RootRel name-app 2.17 Q-typeRootRel noun 2.49RootRelHead obj1 1.60RootRelHead name-su 1nePER 0.91Table 3 shows the features and weights selected inthe training process.
The largest weights are givento names in the text layer, to root forms of names inmodifier relations and to plain text adjectives.
Manykeyword types use ?name?
or ?noun?
as POS restric-tion.
A surprisingly large number of keyword typesare marked as required.
Some of them overlap witheach other and are therefore redundant.
For exam-ple, all RootPOS keywords are marked as requiredand therefore, the restrictions of RootPOS keywordsare useless because they do not alter the query.
How-ever, in other cases overlapping keyword type defini-tions do influence the query.
For example, RootRelkeywords in general are marked as required.
How-ever, other type definitions replace some of themwith weighted keywords, e.g., RootRel noun key-945words.
Finally, some of them may be changed backto required keywords, e.g., RootRel keywords ofnouns in a modifier relation.6 ConclusionsIn this paper we describe an approach for integrat-ing linguistic information derived from dependencyanalyses in passage retrieval for question answer-ing.
Our retrieval component uses a multi-layer in-dex containing various combinations of linguisticfeatures and syntactic units extracted from a fullyanalyzed corpus of unrestricted Dutch text.
Natu-ral language questions are parsed in the same way.Their analyses are used to build complex queries toour extended index.
We demonstrated a genetic al-gorithm for optimizing query parameters to improvethe retrieval performance.
The system was trainedon questions from the CLEF competition on open-domain question answering for Dutch which are an-notated with corresponding answers in the corpus.We could show a significant improvement of about15% in mean total reciprocal rank using extendedqueries with optimized parameters compared withthe base-line of traditional information retrieval us-ing plain text keywords.ReferencesRaffaella Bernardi, Valentin Jijkoun, Gilad Mishne, andMaarten de Rijke.
2003.
Selectively using linguisticresources throughout the question answering pipeline.In Proceedings of the 2nd CoLogNET-ElsNET Sympo-sium.Gosse Bouma, Gertjan van Noord, and Robert Malouf.2001.
Alpino: Wide coverage computational analysisof Dutch.
In Computational Linguistics in the Nether-lands CLIN, 2000.
Rodopi.Gosse Bouma, Jori Mur, and Gertjan van Noord.
2005.Reasoning over dependency relations for QA.
InKnowledge and Reasoning for Answering Questions(KRAQ?05), IJCAI Workshop, Edinburgh, Scotland.Joel L. Fagan.
1987.
Automatic phrase indexing fordocument retrieval.
In SIGIR ?87: Proceedings ofthe 10th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 91?101, New York, NY, USA.
ACM Press.Vera Hollink, Jaap Kamps, Christof Monz, and Maartende Rijke.
2003.
Monolingual document retrieval forEuropean languages.
Information Retrieval, (6).Apache Jakarta.
2004.
Apache Lucene - a high-performance, full-featured text search engine library.http://lucene.apache.org/java/docs/index.html.Valentin Jijkoun, Jori Mur, and Maarten de Rijke.
2004.Information extraction for question answering: Im-proving recall through syntactic patterns.
In Proceed-ings of COLING-2004.Boris Katz and Jimmy Lin.
2003.
Selectively using re-lations to improve precision in question answering.
InProceedings of the EACL-2003 Workshop on NaturalLanguage Processing for Question Answering.Robert Krovetz.
1993.
Viewing morphology as an infer-ence process,.
In Proceedings of the Sixteenth AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages 191?203.Dan Moldovan, Sanda Harabagiu, Roxana Girju, PaulMorarescu, Finley Lacatusu, Adrian Novischi, Adri-ana Badulescu, and Orest Bolohan.
2002.
LCC toolsfor question answering.
In Proceedings of TREC-11.Gu?nter Neumann and Bogdan Sacaleanu.
2004.
Experi-ments on robust NL question interpretation and multi-layered document annotation for a cross-languagequestion/answering system.
In Proceedings of theCLEF 2004 working notes of the QA@CLEF, Bath.John Prager, Eric Brown, Anni Cohen, Dragomir Radev,and Valerie Samn.
2000.
Question-answering bypredictive annotation.
In In Proceedings of the 23rdAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval,Athens, Greece, July.Dragomir R. Radev, Hong Qi, Harris Wu, and WeiguoFan.
2002.
Evaluating web-based question answeringsystems.
In Proceedings of LREC, Las Palmas, Spain.Ian Roberts and Robert Gaizauskas.
2004.
Evaluatingpassage retrieval approaches for question answering.In Proceedings of the 26th European Conference onInformation Retrieval (ECIR), pages 72?84.Tomek Strzalkowski, Louise Guthrie, Jussi Karlgren, JimLeistensnider, Fang Lin, Jose?
Pe?rez-Carballo, TroyStraszheim, Jin Wang, and Jon Wilding.
1996.
Nat-ural language information retrieval: TREC-5 report.Ellen M. Vorhees.
1999.
The TREC-8 question answer-ing track report.
In Proceedings of TREC-8, pages 77?82.Chengxiang Zhai.
1997.
Fast statistical parsing of nounphrases for document indexing.
In Proceedings of thefifth conference on Applied natural language process-ing, pages 312?319, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.946
