Probabilistic Model of Acoustic/Prosody/ Concept Relationshipsfor Speech SynthesisNanet te  M.  Ve i l l euxBoston UniversityAbst rac tThis paper describes the formalism forincorporating emerging linguistic the-ory in a joint model of the acous-tic/prosody/concept relationships.
Itmakes use of binary decision trees to es-timate model parameters, the conditionalprobabilities.
In doing so, the model re-mains general, and can accommodate heresults of our evolving understanding of theinteraction between factors that determineprosody.
While this model has been suc-cessful in both speech synthesis and anal-ysis applications, it has made use of syn-tactic and pragmatic information alone.Extension of this model to map prosodicstructure to other higher order linguis-tic structures that more fully describe themeaning that an utterance is straightfor-ward.
As hypotheses are developed in theranking of competing constraints, includ-ing focus structure, and in the role of dis-course history, they can be integrated intothe model as features in the binary decisiontree.1 In t roduct ionProsody, particularly the placement of phrasing andrelatively prominent syllables within an utterance,is important in human understanding of speech.
(Boogaartand Silverman, 1992; Price et.al., 1991)While great improvements in the prosody of syn-thetic speech ave been made over the past decade,naturalness itself has proved elusive (BoogaartandSilverman, 1992; Veilleux, 1994).
One reason for theremaining diflhrences between synthetic and humanspeech is an incomplete understanding of the map-ping between the speaker's intended meaning andthat meaning's acoustic onsequences, which are, inpart, encoded in the prosodic structure (Price et.al.,1991).
Prosody, therefore, is an important part ofthe route from meaning to speech.
In order to seehow prosody can be improved in automatic speechsynthesis ystems, it is useful to examine what isknown about the relationship between prosody andthe acoustic speech signal on the one hand and be-tween prosody and the meaning embedded in thatspeech on the other.Clearly, prosody is related to the acoustic speechsignal.
In human speech, prosodic phrases andprominence are cued by acoustic features uch asf0 contour and duration.
For example, final syllablelengthening and a descending f0pattern on the wordthink (as well as know) will lead the listener to theperceive a phrase break between know and I in theunderlined utterance:Don't you think Michael Jordan is great?I don't think; I know.Furthermore, this sentence might reasonably beproduced with a pitch accent (e.g.
an H*, or rise/fallf0 pattern) on the words think and know, lending theperception that these two words are more prominentthan other words in the utterance and that they arebeing contrasted (Prevost, 1996).Prosody is also related to higher order linguis-tic structures uch as the syntax of an utterance.In the above example, the main clauses \[S I don'tthink\] and \[S I know\] align with major prosodicphrase breaks.
Other researchers, aswell as myself,have gainfully used this prosody/syntax relationshipin both speech synthesis and speech analysis (auto-matic recognition and understanding) applications.(e.g.
(Veilleux, 1996; Wang and Hirschberg, 1992)).Also note that the same word string could be usedto convey the opposite meaning, as inWhat's the capital of Sri Lanka?I don't think I know.However, this sentence does not have the same syn-tactic structure and one would not expect he sameprosodic structure (e.g.
there would be less of a"break" after th ink in the second example).However, just as syntax is not fully determined byword choice or order, syntax is not the only factorthat determines the prosodic structure.
For example(from (Steedman, 1991)), the sentence\[S \[NP Mary\] \[VP prefers \[NP corduroy\]\]\]can be naturally produced with a major phrasebreak bisecting the verb phrase:(Mary prefers) (corduroy).It is reasonable to conclude that prosody is con-strained by factors in addition to (and possibly inconflict with) syntax.
The following example showsthat semantic issues also play a role in determiningprosody.Did she come with Bill's wife??
No, she came with BILL's sister.Bill is inappropriately produced with prosodicprominence (denoted by the use of capital etters)given the intention of the speaker to directly answerthe question.
Notice that the following pair is per-ceived as appropriate:Did she come with John's sister?No, she came with BILL's sister.Considerations of this kind suggest that prosody istherefore linked to the syntax and semantics of anutterance, which in turn are related to the speaker'sintentions, or the concept that the speaker wishes toconvey.
Collectively, syntax and semantic structurewill be referred to here as the information structureof an utterance.
However, we purposely leave infor-mation structure as a loosely defined term that canbe expanded as our understanding of the relation-ship between prosody and the meaning embeddedin speech evolves.Therefore, we find that prosody is related to theinformation structure on one hand and to the acous-tic signal on the other.
Consequently, prosody canbe useful as a bridge, or intermediary epresentationbetween concept and speech.This work presents a general methodology forderiving a formalism to describe the acoustic/prosody/concept relationships for use in automaticspoken language systems, by generating a mappingbetween the acoustics of speech on one hand, anda syntactic/semantic representation f the speaker'sintentions on the other.
This computational map-ping serves to create an acoustic/ prosody/ con-cept model.
Most recent applications of this modelhave been in the area of automatic speech recogni-tion where the acoustic/prosody/syntax m pping 1was used to decrease word error (Veilleux, 1996).However, the mapping is bi-directional, and thismodel can be used for speech synthesis as well asfor speech analysis (recognition and understanding).The method for doing so will be presented after themodel is described in more detail.The consequences of the analysis of the relation-ships between information structure, prosody andspeech described above show that the use of prosodyin concept-to-speech synthesis (CTS) can not beachieved merely as plug-in extension of prosodymodels that have been developed for text-to-speech(TTS) systems (e.g.
(Veilleux, 1994; Wang andHirschberg, 1992)).
Although prosody models couldbe applied after the generation of the word string,as in TTS applications, prosodic structure must bedetermined not only by the word string, but alsoby the meaning-specific syntax and semantic struc-ture.
The lack of mapping between this informationstructure and prosody is partly responsible for thepersistent unnaturalness in synthetic speech.Clearly, there are many aspects of the factorsthat determine prosodic structure that are still tobe understood.
While the acoustic correlates ofprosody(Pierrehumbert80, 1980; t'Hart et.al., 1990)and the relationships between prosody and syntax(e.g.
(Selkirk, 1984; BachenkoFitzpatrick, 1990;Ostendorf and Veilleux, 1993; Gee and Grosjean,1983; Wang and Hirschberg, 1992; Terken and Hirsh-berg, 1994)), have been investigated in some length,the mapping and interactions between these do-mains has not been completely quantified.
More-over, the effect of other factors such as focus anddiscourse structure on prosody, have not been stud-ied as extensively.
A strength of the model presentedhere is that it is general, and therefore adaptableas our understanding of the prosody/concept andprosody/speech relationships improves.
This gen-erality in the formalism will be pointed out as themodel is described.One more general comment about this model isin order before we turn to the details.
The formal-ism presented here is probabilistic.
It is data-driven(labeled training data is used to derive model pa-rameters, i.e.
the probabilities).
It therefore has1The concept domain was primarily represented bysyntax, because syntactic structure was readily availableand its role in constraining prosodic structure is betterunderstood.2the strengths of corpus-driven approaches in that itis informed by a large body of example.
It also hasthe characteristic weakness of probabilistic models inthat it tends to generalize unless otherwise directed(e.g.
by manipulating misclassification costs, or pro-viding constraints to distinguish different events.
)One other feature of the model presented here isthat it explicitly incorporates linguistic knowledgein the design of decision trees used as probabilityestimators.
This serves to cluster data according tolinguistic ontext, decreasing the bias towards gen-eralizing the most prevalent.2 P robab i l i s t i c  Mapp ing  o fAcoust i c /P rosody /ConceptRe la t ionsh ips2.1 Model  Formal ismIf the task of automatic speech synthesis can beframed as that of selecting the most probable acous-tic production associated with a text string anno-tated for intended meaning, it can be achieved byfindingar gmaxxp( x\]meaning ) .
(1)That is, by finding the acoustic sequence x that max-imizes the joint probability of the acoustic sequencegiven the meaning or concept he speaker wishes toconvey.
For this work, the word string itself is as-sumed to have been generated in a prior step.
There-fore, the acoustic sequence would be a sequence ofprosody related supra-segmental features.
The termmeaning is quite open to interpretation.
For the pur-poses of this work, meaning will be assumed to bestraightforwardly encoded in the information struc-ture, that is, the syntax and semantic structure ofthe utterance.
For the present, information struc-ture (denoted by I in the equations below) repre-sents some aspects of the underlying concept hatare covered by theory (such as syntax) or description(such as an instantiation of a focussed constituent).This structure will be represented asannotations onthe text string, which can serve as input to a speechsynthesis system.
What features, beyond syntax, arerelevant and can be reliably annotated is, of course,an open research question, that will probably be an-swered over time.
Some suggestions for features anda method for incorporating them in this probabilis-tic model are described in Section 3.
So, returningto the model derivation, we see that we wish to findargmaxxp(x\]I), (2)that is, the acoustic sequence x that maximizesthe probability of the acoustics given the informa-tional annotation.
Inserting prosody as an inter-mediary representation, wecan re-write this condi-tional probability of the acoustics given the informa-tion structure in terms of the sequence of abstractprosodic labels a.p(xlz) =  p(x, alZ) (3)aThis sequence of abstract prosodic labels describe anordered set of prosodic events, such as prominenceand phrasing.In order to capture these prosodic events in acomputational model, this work uses prosodic labelsbased on the ToBI transcription convention (--Tonesand Break Indices, see (Silverman et.al., 1992)).
TheToBI system labels prosodic prominence with a pitchaccent type (Tone), from a subset of Pierrehum-bert's inventory (Pierrehumbert80, 1980).
In pre-vious work, data with this level of detail was notavailable so the simple label of ?
prominence oneach syllable was used.
Prosodic phrasing is cap-tured by placing a break index (BI: 0 to 4) at eachword juncture, to indicate the level of de-couplingbetween the words.
For example, the juncture be-tween two words in a clitic group would be labeledwith a 0 break index.
At the other end of the spec-trum, the junction between two words separated bya major prosodic phrase would be labeled with a4 break index.
Moreover, the ToBI system allowsone to label prosodic events that are conspicuous inspontaneous speech.
For example, significant length-ening of a final syllable, without he comcommittantintonational cues associated with prosodic phrasingcan be labeled with a diacritic.
If these events ervea communicative purpose in informal speech (mark-ing focus, holding the floor) then future models maymake use of these labels.Applying Bayes' Law to Equation 3:p(xl~ = ~p(x /a )p(a /Z)  (4)aThis form of the equation more clearly reflectsthe use of prosody as an intermediate r presenta-tion, relating the acoustic sequence to the informa-tion structure by modeling p(xl I  ) in terms of theconditional probability of acoustic sequence giventhe prosodic structure (p(xla)) and the conditionalprobability of prosody given the information struc-ture (p(alI)).
The model parameters can be esti-mated using statistical methods: in this work, usingan automatically derived binary decision trees.In speech synthesis applications, each prosodicevent can be realized by manipulating the acous-tic signal according to a set of context-based f0and duration rules (vanSanten, 1993).
For exam-ple, a syllable labeled with a high pitch accent (H*or simply +prominence from this work), would begiven a rise/fall f0 contour, adjusted according toe.g.
the duration of the vowel, etc.
In such ap-plications, p(x/a), the probability of the acousticparameters given the prosodic labels, is fully deter-mined by these rules alone.
Although a stochasticmodel of prosody and acoustic features could be use-ful in more specifically determining the correlates ofprosody in f0, duration and vowel quality, it is leftto future work to incorporate a probabilistic acousticmodel of prosody in the synthesis algorithm.
There-fore, with this simplification, the model equation be-comes:argmaxxp(x\[Z) ---- argmaxap(a\[Z) (5)np(alz) = IIp(a,/  (6)i= lassuming prosodic labels are independent.Or,n= p(allZ) ~IP(al/al...ai_l,~ ) (7)i=2assuming a Markov dependency.Here \[al ...an\] is the sequence of n prosodic labelsin the utterance.
The task remains, therefore, tofind the sequence of prosodic labels with the high-est probability given the underlying concept to beconveyed I.2.2 Decoding AlgorithmsSeveral decoding algorithms have been proposed tofind the best sequence a.
If prosodic labels are as-sumed to be independent, as in Equation 6, the high-est probability sequence will be the sequence of high-est probability elements p(ai\]I).Note that, until this point, no assumptions havebeen made about the dependence or independenceof the sequences of acoustic, prosodic or informa-tional events.
Such assumptions are certainly in-correct.
For example, both Prevost(Prevost, 1996)and Selkirk(Selkirk, 1997) propose prosodic struc-ture that involves a combination of prominence andphrase boundary placement to cue meaning-specificspeech renditions.
Furthermore, though useful tosimplify the decoding problem, independence as-sumptions are probably not viable once demandson spoken language systems become more sophis-ticated.Several prosodic models have been developed thatrelax this assumption.
For example, work in predict-ing prominence by (Ross et.al., 1992) makes use ofa Markov assumption.
Also, a hierarchical model(Ostendorf and Veilleux, 1993), makes use of thestrict layer hypothesis of prosodic phrase structure,assuming that a well-formed utterance is comprisedof major phrases, which are in turn comprised of mi-nor phrases.
In that work, a dynamic programmingalgorithm (see Figure 2.2) proposes a major phrasewithin an utterance, uses hypothetical minor phraseswithin that major phrase to estimate its probabilityand then, finally, choses the most likely sequence ofmajor phrases.
The most likely hypothesis of mi-nor phrases within each major phrase is determinedby using probability estimates from a previously de-rived binary decision tree.
As we will see in thenext section, the binary decision tree provides esti-mates of (p(ak\[T(Wi,mprev))), that is, the proba-bility of a particular prosodic event (phrase breakindices here), given the word sequence and the pre-vious minor phrase.Perceptual experiments have been performed totry to investigate whether the hierarchical model de-scribed above improves the intelligibility of synthe-sized speech.
Evaluations of this sort are notablydifficult, but necessary.
Instead of trying to evalu-ate the "naturalness" of synthetic speech, Silvermanet.al.
(Silverman, 1993; Boogaartand Silverman,1992) have suggested a transcription or response-type task to evaluate comprehensibility.
Improvedcomprehensibility should manifest i self as improvedtranscription performance.
However, in the percep-tual experiment used to evaluate the hierarchicalmodel, subjects performed similarly well on a tran-scription task designed to compare three differentprosodic phrase break models on an AT&T TTSsystem (the AT&T default, the hierarchical modeland a random generator)2(.62-.67 correct, a2 ,~ 0.5),including randomly placed breaks.
Informal discus-sion with human subjects revealed that the task wasconsidered very difficult.
Although several subjectsclaimed to have understood the sentences, they saidthat they didn't have enough time to transcribe thesentence.In addition to transcribing the synthetic sen-tences, subjects were also asked to check which, offive adjectives ("choppy", "okay", had "not enoughpauses", or "unnatural"), best described the phras-ing of the sentences.
The results of this experimentfor twenty subjects are tabulated on Table 1.
Over-all, more hierarchical model sentences were judgedto be "okay".
However, hierarchical model sentenceswere judged to be "choppy" more often than AT&T2Details of the hierarchical model or this experimentappear in (Veilleux, 1994), also available as a postscriptfile via http://raven.bu.edu/fimv)Dynamic P rogramming Routine forProsodic Parse Predict ionFor each word t in unit Ui (t = 1,..-,li):Compute log pt ( uil (1, t ) \[\]/Vi, Ui_ l ).For each n-length sequence of subunits panning \[1,t\] (n = 2, .
.
.
,  t):logpt(uil ...Uin\[YVi, Ui-1) = maxs<t logps(Uix, .
.
.
, Ui,n-1 \[VI2i, Ui-I )+ logp(uin(s + 1, t)\[)4)i, ui(n-D)(Computing logp(uin(S + 1, t)l~V~, u~(n_a)) with a recursive call to this routine.
)Save pointers to best previous break location s.To find the most likely sequence,p(Ui\[)/~)i, Ui_l ) --- maxn logpt, (U/I,---, Uin\[~4)i, Ui-1 ) + log q(nlli ).Here, the probability q(n\[li) provides a lengthconstraint.sentences, which in turn were more often consideredto have "not enough pauses".
Interestingly, the hi-erarchical model sentences were not considered morechoppy than the random model's entences, despitethe minor phrase breaks generated by the hierarchi-cal model in addition to major breaks.
Moreover,the hierarchical model was given significantly more"Okay" ratings than the other two models.Based on other evaluation metrics, the perfor-mance of the hierarchical model is believed to be ofhigher quality in general text-to-speech tasks.
Al-though there are specific syntactic structures thatare consistently problematic (e.g.
particles andprepositions), improved POS labeling and additionalrules in text-processing can easily reduce these prob-lems.
Furthermore, this work did not take into con-sideration discourse or non-syntax semantic infor-mation, and did not alter the AT&T defaults forplacing prominences (pitch accents).2.3 Parameter  Est imat ion Using BinaryDecision TreesFor the hierarchical model, or any future modelbased on the more general acoustic/prosody/syn-tax formalism presented here, we need to have esti-mates for p(ai/Z) in order to decode the most prob-able prosodic sequence.
Binary decision trees areused in this work to estimate p(a jZ)  for severalreasons, and are a main feature supporting the gen-erality and adaptability of the overall model.
First,binary trees can be used to map heterogeneous fea-tures to prosodic labels.
Features can be continuous(e.g.
degree of syntactic bracketing) or discrete (e.g.classes of function word types).
Furthermore, thefeatures can be inter-dependent, such as location ofthe last predicted phrase boundary or prominence.The automatic algorithm which determines the treestructure typically chooses features to minimize mis-classification of prosodic labels, and as such, canindicate how relevant a feature is in the choice ofa particular prosodic label.
Finally, and most im-portantly from a theoretical standpoint, a decisiontree presents a model of the relationship betweenone domain, e.g.
information structure (or alterna-tively acoustics) and another domain, e.g.
prosody.As shown below, the path from root node to leafis unique for each token in the input sequence anddescribes a prosodic label as a function of the treefeatures.
(Let T(Z) represent a function T of the in-formation structure Z).
This model therefore allowsus to map information structure onto prosodic struc-ture, or, alternatively, to map acoustic parametersonto prosodic structure.Binary decision trees, like the one shown in Fig-ure 1, are a series of binary questions about features(text-derived syntactic, grammatical nd pragmaticproperties in this case).
Each data token (a datatoken would be a word pair in trees designed topredict prosodic phrase breaks and a single sylla-ble to predict :i: prominence), is "dropped" throughthe tree, starting with the root question.
In the ex-ample shown here, a binary decision tree has beengrown to predict the phrase break index betweeneach word pair.
The root node represents he ques-tion Is the left word a content word and the rightword a function word?
If the answer is no, the wordpair is dropped to the lower right node, and exam-ined in the light of that node's question (What func-tion word class is the right word?).
The process isrepeated until the data token reaches a leaf (termi-nal node).
In some cases (Wang and Hirschberg,1992) the leaf node would be associated with a spe-cific prediction, e.g.
phrase break index 1, and theword pairs shunted to this leaf node would be pre-5noyes ao other content word / ~- -~ or default fw<=1<= 1>113other S, NP or\ADJP/ADVPwon/over(prep)S,NP oraux verb, prep,\ or default fwo ther  rst,sec?nd,\third or sixth\eighthotherother'over/their Iotherfourth, fifth or\~eventh eighthdet or\prep <=1/~--"~\ >1)first orixth eighthFigure 1: Binary decision tree designed to generate the probability of break indices given syntax, p(bilsyntax)./~acb data token (a word pair) is shunted along a path from the root to a leaf node.
Features include syntazticderivations (e.g.
dom_Ift means "what is the highest syntactic onstituency that dominates the left wordbut not the right word"), word class type, degree of syntactic bracketing, relative position in the syntax treeand relative position in the sentence.
The word pairs from the sentence The men won over their enemies areshown under their destination leaves.Table h Results for subjective judgments by 20 human subjects for nine sentences synthesized using each ofthree prosodic phrase break prediction algorithms (AT~T default, hierarchical model, and randomly placedbi'eaks).Phrase Break Model choppyAT&T default 7Hierarchical Model 12Random 14okay162316not enoughpauses unnatural23 148 1717 13dicted to be separated by a typical intra-phrase wordbreak.
However, the approach taken here differs inthat trees are not used directly to predict phrasebreaks or prominences.
Instead the trees are usedto generate conditional probability distributions (inthis example, p(break indexi\[T(syntax))).
The dis-tributions are then used in the computational modeldescribed above to find the most likely sequence ofprosodic labels.
Again, the advantage of decodingthe entire sequence is that one is able to explic-itly make use of the inter-dependencies in the as-signment of specific prosodic labels as in (Ostendorfand Veilleux, 1993).
Furthermore, focus might bemarked by a confluence of prominence and phrasing(Selkirk, 1997), requiring arelaxation of the assump-tion that these events are independent.In order to see how a decision tree is used as aconditional probability estimator ather than as apredictor, recall how such a tree is originally con-structed.
Binary decision trees, like most statisticalmodels, are derived using labeled training data.
Inthis case, the training data would be hand-labeledprosodically (ToBI) and would be associated withfeatures automatically extracted for each data to-ken.
The tree in the example above was derived toestimate the conditional probability of the prosodicbreak indices (bi), given?
syntactic onstituency and derivation rules,?
part-of-speech lasses,?
and pragmatic rules such as relative location inthe utterance.To generate this p(bi\[T(syntax)) tree, data to-kens (word pairs) were hand-labeled for prosodicbreak indices, analyzed by an automatic parser andother feature xtraction programs equipped with e.g.function word tables and dictionaries, to produce alabeled database.
A similar tree was also generatedto estimate the probability of =k prominence, givensimilar syntactic/pragmatic features.
To estimatethe conditional probabilities between prosodic labelsand acoustic signal, trees have also been derived us-ing acoustic features such as normalized f0, duration,and vowel quality (Wightman, 1991).After the database is fully labeled, an automatictree-growing algorithm partitions this training databased on one of these extracted features (in Figure 1the cw-fw feature) into two subsets, each more ho-mogeneous with respect o break indices than thewhole set 3.
The partitioning process is repeated oneach of the subsets, using one of the extracted fea-tures and each time producing children subsets thatare more homogeneous than their parents.
Ideally,the final subsets, which share a particular syntac-tic/ pragmatic ontext, would contain data whereall word pairs had been labeled with the same breakindex, prompting only a single prediction.
How-ever, this is unlikely, since all factors that determineprosody are either not yet understood, such as focusstructure, or can not be known from text, such asspeaking rate, and thus are not used in the partition-ing process.
Instead, each of the final subsets has adistribution of prosodic labels that can be used to es-timate a probability distribution p(a\[leafi ).
In thisexample, each leaf represents a unique path whichserves to describe a distribution of prosodic labels asa function of the syntax and related features.
There-fore p(alleaf i ) = p(alW(syntax)).One interesting observation from the automaticdesign of the decision tree shown in Figure 1 is theselection of the cw-fw feature at the root.
Sorin(Sorin et.al, 1987) has shown that prosodic phrasebreaks in French tend to correspond with just cw-fwjunctions.
Although this rule over-generates phraseboundaries in English, the choice of this feature inthe decision tree indicates a persistent correlation.~In this case, words pairs spanning acontent-functionword boundary have been found to be associated withintonational phrase breaks (indices 3 and 4) and the dataset that contains word pairs that are cw-~T pairs hasa higher percentage of 3/4 break index labels than thewhole data set.3 Use  o f  semant ic  features  in  theprosody /concept  mapp ingPrevious work has made use of this probabilisticmodel of the relationships between prosody, theacoustic signal and information structure (Veilleux,1996) but only insofar as information structure couldbe captured using syntax and related features.
Fromthe examples above, it is clear that prosody, evenprosodic phrase structure, isnot constrained by syn-tax alone.
What remains to be investigated and in-corporated are other factors that constrain prosodyand are related to the concept he speaker intendsto convey.One such feature of the information structure isthe placement of focussed constituents in an utter-ance.
The literature presents a variety of definitionsof semantic focus, some describing focus in terms ofsemantic intent (Rooth, 1994; Gussenhoven, 1994)and others more directly in relationship to given-hess (Schwarzschild, 1997).
Furthermore, some defi-nitions of focus overlap with theme (Prevost, 1996),while others do not.
In any case, focus is generallyagreed to be linked to pitch accent placement (e.g.
(Selkirk, 1984)) and probably to phrase break place-ment as well (Selkirk, 1997)).This focus/prosody relationship resents an op-portunity to generate synthetic speech that has amore appropriately assigned prosodic structure, re-flecting the underlying meaning to be conveyed.
Asthe mapping between prosody and focus is investi-gated more fully, results can be incorporated intothe computational model presented here by simplyrepresenting focus markings as labeled features inthe binary tree.Some promising work by Selkirk (Selkirk, 1997)describes the choice of prosodic phrase structure tobe the outcome of an ordering of competing factors,including focus, syntax and pragmatic onstraints.While some constraints may be violable (such as thealignment of major prosodic breaks with syntacticboundaries), the outcome isoptimal, i.e.
it conformsto the constraints of the highest ranked factor (e.g.the alignment of a major prosodic phrase boundarywith the right edge of a focussed constituent).Previous use of the acoustic/prosody/syntaxmodel has already established the function of syn-tactic edges to predict prosodic phrasing (note theuse of e.g.
dom..lft features in the tree given in Fig-ure 1).
Labeling the right edges of focussed con-stituents in training data and growing a binary deci-sion tree based on this additional feature, will gener-ate probability distributions as functions of the focusas well as syntactic and pragmatic structure.
If thesupposition about he relationship between focussedconstituents and prosodic boundaries i representedin data, such a feature should be selected as useful indecreasing the mis-classification f prosodic phrasebreak indices between two words in the automaticdesign of a binary tree.
Moreover, a ranking impliesan interaction of factors, each of which can be en-coded as binary tree features.
The order in whichthe features are selected in the tree structure, aswell as their co-occurance (or lack of) on a root-leafpath, can indicate potential areas of interaction orredundancy.
In this way, binary decision trees notonly generate conditional probabilities for synthesismodels, but also test hypotheses about the relativeuse of a feature in predicting a prosodic label.Work by Prevost explicitly addresses the relation-ship between theme-rheme and prosodic prominenceand phrase placement in cases of explicit contrast.This work significantly extends previous heuristicsconcerning newness and pitch accent placement.Again, training data labeled with theme-rheme no-tation, and devising a feature for the decision treegrowing algorithm to select, would incorporate thisrule in the estimation of probabilities of prosodicstructure.Another active and related area of research thataddresses the relationship between higher order lin-guistic structure and prosodic structure has beenexplored by (Terken and Hirshberg, 1994) and(Nakatani, 1993).
The latter work examines theplacement of accents, as constrained by the interac-tion of discourse, surface structure and lexical form.Pitch accent placement on pronouns as well as onexplicit forms in the subject position motivate the-ory that describes new and givenness in terms of ahierarchical discourse structure (Grosz and Sidner1986).
Again, the implications of this theoreticalframework can be extracted as features for generat-ing conditional probabilities of prosodic events, withreference to the theory.
One such feature could bean annotation ofdiscourse segmentation in the inputtext.
Using this annotation as a feature in the bi-nary tree would also serve to allow the tree to choselimits on how far back in the history list to lookfor an antecedent.
If the pronoun represents a new(re-introduced) item within this window, it may bemore likely to be accented.
Again, as a feature ina decision tree, this property would be a candidatefor selection to minimize mis-classification error andgenerate conditional probabilities that are functionsof the discourse nvironment.In summary, the formalism for incorporatingemerging linguistic theory in a joint model of theacoustic/prosody/concept rela ionships i  describedhere.
It makes use of binary decision trees to esti-mate model parameters, the conditional probabili-ties.
The binary decision trees themselves, make useof explicit linguistic infdrmation to partition datainto more homogeneous prosodic ontexts.
In doingso, the model remains general, and can accommo-date the results of our evolving understanding of theinteraction between factors that determine prosody.\?hile this model has been successful in both speechsynthesis and analysis applications, it has made useof syntactic and pragmatic information alone.
Ex-tension of this model to map prosodic structure toother higher order linguistic structures that morefully describe the meaning that an utterance is toconvey is straightforward.
As hypotheses are devel-oped in the ranking of competing constraints, in-cluding focus structure, and in the role of discoursehistory, they can be integrated into the model asfeatures in the binary decision tree.Re ferencesJoan Bachenko and Eileen Fitzpatrick.
1990.
AComputational Grammar of Discourse-NeutralProsodic Phrasing in English.
Computational Lin-guistics, Vol.
16, No.
3, pp.
155-170, 1990.T.
Boogaart and K. Silverman.
1992.
Evaluatingthe Overall Comprehensibility of Speech Synthe-sizers.
International Conference on Spoken Lan-guage Processing, pp.
1207-1210, Banff, October1992.James Gee and Francois Grosjean.
1983.
Perfor-mance Structures: A Psycholinguistic and Lin-gnistic Appraisal Cognitive Psychology, Vol.
15,pp.411-458, 1983.Barbara Grosz and Candance Sidner.
1986.
Atten-tion, Intentions, and the Structure of Discourse,"Computational Linguistics Vol.
12, no.
3, pp.175-204, 1986.Carl Gussenhoven.
1994.
Focus and Sentence Ac-cents in English Focus and Natural Language Pro-cessing, ed.
Peter Bosch and Rob van der Sandt,IBM Deutschland Informations ystems, Heidel-berg, 1994.Christine Nakatani 1993.
Discourse Structural Con-straints on Accent in Narrative May, 1993.Scott Prevost 1996 Modeling Contrast in the Gener-ation and Synthesis of Spoken Language Interna-tional Conference on Spoken Language Processing,Philadelphia, Pennsylvania, 1996.Patti.
J.
Price, Mari Ostendorf, Stefanie Shattuck-Hufhagel, and Cynthia Fong, 1991 The Use ofProsody in Syntactic Disambignation.
Journalof the Acoustical Society of America, Vol.
6, pp.2956-2970, 1991.Mari Ostendorf and Nanette M. Veilleux.
1993.
AHierarchical Stochastic Model for Automatic Pre-diction of Prosodic Boundary Location.
Compu-tational Linguistics December, 1993.Janet Pierrehumbert.
1980.
The Phonology andPhonetics of English Intonation Ph.D. Thesis,Massachusetts Institute of Technology, 1980.Ken Ross, Mari Ostendorf and Stefanie Shattuck-Hufnagel.
1992.
Factors Affecting Pitch AccentPlacement.
International Conference on SpokenLanguage Processing, pp.
365-368, Banff, October1992.Mats Rooth.
1994.
A Theory of Focus Interpreta-tion.
Handbook of Semantic Theory, Shalom Lap-pin, ed., Blackwell, 1994.Roger Schwarzschild 1996.
Givenness and OptimalFocusElisabeth O. Selkirk 1984.
Phonology and Syntax:The Relation between Sound and Structure MITPress, Cambridge, Massachusetts, 1984.Elisabeth O. Selkirk.
1997.
The Interactions of Con-straints on Prosodic Phrasing.
manuscript, 1997.Kim Silverman, 1993.
On Customizing Prosodyin Speech Synthesis: Names and Addresses as aCase in Point.
Proceedings of the ARPA Work-shop on Human Language Technology, pp.
317-322, Princeton, New Jersey, March 1993.Kim Silverman, Mary Beckman, John Pitrelli, MariOstendorf, Colin Wightman, Patti Price, JanetPierrehumbert, and Julia Hirschberg.
1992.
ToBhA Standard Scheme for Labeling Prosody.
Inter-national Conference on Spoken Language Process-ing, pp.
867-870, Banff, October 1992.C.
Sorin, D. Larreur and R. Llorca.
1987.
ARhythm-based Prosodic Parser for Text-to-SpeechSystems in French.
Proceedings of the Interna-tional Congress of Phonetic Sciences, Vol.
1, pp.125-128, Tallinn, 1987.Mark Steedman.
1991.
Surface Structure, Into-nation, and Focus.
The Institute for Researchin Cognitive Science, University of Pennsylvania,IRCS Report Number 91-31, September, 1991.Jacques Terken and Julia Hirschberg.
1994.
Deac-centuation of Words Representing Given Informa-tion: Effects of Persistence of Grammatical Func-tion and Surface Position.
Language and SpeechVol.
37, no.
2.J.
't Hart, R. Collier and A. Cohen, 1990.
A Percep-tual Study o\] Intonation.
Cambridge UniversityPress, 1990.Jan van Santen.
1993.
Perceptual Experimentsfor Diagnostic Testing of Text-to-Speech Systems.Computer Speech and Language 1993, 7, pp.
49-100.Michele Wang and Julia Hirschberg.
1992.
Auto-matic classification of Intonational Phrase Bound-aries.
Computer Speech and Language 6-2 April1992, pp.
175-196.Nanette M. Veilleux.
1994.
Computational Mod-els of the Prosody/Syntax Mapping for SpokenLanguage Systems Ph.
D. Thesis, Departmentof Electrical, Computer and Systems Engineering,Boston University, 1994.Nanette M. Veilleux.
1996.
Stochastic Models ofProsody for Automatic Spoken Language SystemsProceedings of the Acoustical Society of AmericaHonolulu, Hawaii, December, 1997Colin W. Wightman.
1991 Automatic Detection ofProsodic Constituents for Parsing Ph.D. Thesis,Department ofElectrical, Computer and SystemsEngineering, Boston University, 1991.10
