Answering Clinical Questions withKnowledge-Based and Statistical TechniquesDina Demner-Fushman?University of Maryland, College ParkJimmy Lin?University of Maryland, College ParkThe combination of recent developments in question-answering research and the availability ofunparalleled resources developed specifically for automatic semantic processing of text in themedical domain provides a unique opportunity to explore complex question answering in thedomain of clinical medicine.
This article presents a system designed to satisfy the informationneeds of physicians practicing evidence-based medicine.
We have developed a series of knowl-edge extractors, which employ a combination of knowledge-based and statistical techniques, forautomatically identifying clinically relevant aspects of MEDLINE abstracts.
These extractedelements serve as the input to an algorithm that scores the relevance of citations with respect tostructured representations of information needs, in accordance with the principles of evidence-based medicine.
Starting with an initial list of citations retrieved by PubMed, our systemcan bring relevant abstracts into higher ranking positions, and from these abstracts generateresponses that directly answer physicians?
questions.
We describe three separate evaluations: onefocused on the accuracy of the knowledge extractors, one conceptualized as a document rerankingtask, and finally, an evaluation of answers by two physicians.
Experiments on a collectionof real-world clinical questions show that our approach significantly outperforms the alreadycompetitive PubMed baseline.1.
IntroductionRecently, the focus of question-answering research has shifted away from simple fact-based questions that can be answered with relatively little linguistic knowledge to?harder?
questions that require fine-grained text analysis, reasoning capabilities, andthe ability to synthesize information from multiple sources.
General purpose reasoningon anything other than superficial lexical relations is exceedingly difficult because thereis a vast amount of world knowledge that must be encoded, either manually or auto-matically, to overcome the brittleness often associated with long chains of evidence.
Thissituation poses a serious bottleneck to ?advanced?
question-answering systems.
How-ever, the availability of existing knowledge sources and ontologies in certain domainsprovides exciting opportunities to experiment with knowledge-rich approaches.
Howmight one go about leveraging these resources effectively?
How might one integrate?
Department of Computer Science and Institute for Advanced Computer Studies.
E-mail:demner@umd.edu.?
College of Information Studies, Department of Computer Science, and Institute for Advanced ComputerStudies.
E-mail: jimmylin@umd.edu.Submission received: 4 July 2005; revised submission received: 7 January 2006; accepted for publication:12 April 2006.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 1statistical techniques to overcome the brittleness often associated with knowledge-based approaches?We explore these interesting research questions in the domain of medicine, fo-cusing on the information needs of physicians in clinical settings.
This domain iswell-suited for exploring the posed research questions for several reasons.
First,substantial understanding of the domain has already been codified in the UnifiedMedical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).
Sec-ond, software for utilizing this ontology already exists: MetaMap (Aronson 2001)identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extractsrelations between the concepts.
Both systems utilize and propagate semantic infor-mation from UMLS knowledge sources: the Metathesaurus, the Semantic Network,and the SPECIALIST lexicon.
The 2004 version of the UMLS Metathesaurus (usedin this work) contains information about over 1 million biomedical concepts and5 million concept names from more than 100 controlled vocabularies.
The Seman-tic Network provides a consistent categorization of all concepts represented in theUMLS Metathesaurus.
Third, the paradigm of evidence-based medicine (Sackett et al2000) provides a task-based model of the clinical information-seeking process.
ThePICO framework (Richardson et al 1995) for capturing well-formulated clinical queries(described in Section 2) can serve as the basis of a knowledge representation thatbridges the needs of clinicians and analytical capabilities of a system.
The conflu-ence of these many factors makes clinical question answering a very exciting area ofresearch.Furthermore, the need to answer questions related to patient care at the point ofservice has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005).
MEDLINE, the authoritativerepository of abstracts from the medical and biomedical primary literature maintainedby the National Library of Medicine, provides the clinically relevant sources for answer-ing physicians?
questions, and is commonly used in that capacity (Cogdill and Moore1997; De Groote and Dorsch 2003).
However, studies have shown that existing systemsfor searching MEDLINE (such as PubMed, the search service provided by the NationalLibrary of Medicine) are often inadequate and unable to supply clinically relevantanswers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley1996).
Furthermore, it is clear that traditional document retrieval technology appliedto MEDLINE abstracts is insufficient for satisfactory information access; research andexperience point to the need for systems that automatically analyze text and returnonly the relevant information, appropriately summarizing and fusing segments frommultiple texts.
Not only is clinical question answering interesting from a researchperspective, it also represents a potentially high-impact, real-world application of lan-guage processing and information retrieval technology?better information systems toprovide decision support for physicians have the potential to improve the quality ofhealth care.Our question-answering system supports the practice of evidence-based medi-cine (EBM), a widely accepted paradigm for medical practice that stresses the impor-tance of evidence from patient-centered clinical research in the health care process.EBM prescribes an approach to structuring clinical information needs and identi-fies elements (for example, the problem at hand and the interventions under con-sideration) that factor into the assessment of clinically relevant studies for medicalpractice.
The foundation of our question-answering strategy is built on knowledgeextractors that automatically identify these elements in MEDLINE abstracts.
Usingthese knowledge extractors, we have developed algorithms for scoring the relevance64Demner-Fushman and Lin Answering Clinical Questionsof MEDLINE citations in accordance with the principles of EBM.
Our scorer is em-ployed to rerank citations retrieved by the PubMed search engine, with the goal ofbringing as many topically relevant abstracts to higher ranking positions as possible.From this reranked list of citations, our system is then able to generate textual re-sponses that directly address physicians?
information needs.
We evaluated our systemwith a collection of real-world clinical questions and demonstrate that our combinedknowledge-based and statistical approach delivers significantly better document re-trieval and question-answering performance, compared to systems used by physicianstoday.This article is organized in the following manner: We start in the next section withan overview of evidence-based medicine and its basic principles.
Section 3 provides anoverview of MEDLINE, the bibliographic database used by our system, and PubMed,the public gateway for accessing this database.
Section 4 describes our system architec-ture and outlines our conception of clinical question answering as ?semantic unifica-tion?
between query frames and knowledge frames derived from MEDLINE citations.The knowledge extractors that underlie our approach are described in Section 5, alongwith intrinsic evaluations of each component.
In Section 6, we detail an algorithm forscoring the relevance of MEDLINE citations with respect to structured query represen-tations.
This scoring algorithm captures the principles of EBM and uses the results ofthe knowledge extractors as basic features.
To evaluate the performance of this citationscoring algorithm, we have gathered a corpus of real-world clinical questions.
Section 7presents results from a document reranking experiment where our EBM scores wereused to rerank citations retrieved by PubMed.
Section 8 provides additional details onattempts to optimize the performance of our EBM citation scoring algorithm.
Answergeneration, based on reranked results, is described in Section 9.
Answers from oursystem were manually assessed by two physicians; results are presented in Section 10.Related work is discussed in Section 11, followed by future work in Section 12.
Finally,we conclude in Section 13.2.
The Framework of Evidence-Based MedicineEvidence-based medicine (EBM) is a widely accepted paradigm for medical practicethat involves the explicit use of current best evidence, that is, high-quality patient-centered clinical research such as reports from randomized controlled trials, in mak-ing decisions about patient care.
Naturally, such evidence, as reported in the primarymedical literature, must be suitably integrated with the physician?s own expertise andpatient-specific factors.
It is argued by many that practicing medicine in this mannerleads to better patient outcomes and higher quality health care.
The goal of our work isto develop question-answering techniques that complement this paradigm of medicalpractice.EBM offers three orthogonal facets that, when taken together, provide a frameworkfor codifying the knowledge involved in answering clinical questions.
These threecomplementary facets are outlined below.The first facet describes the four main clinical tasks that physicians engage in(arranged roughly in order of prevalence):Therapy: Selecting treatments to offer a patient, taking into account effectiveness, risk,cost, and other relevant factors (includes Prevention?selecting actions to reducethe chance of a disease by identifying and modifying risk factors).65Computational Linguistics Volume 33, Number 1Diagnosis: This encompasses two primary types:Differential diagnosis: Identifying and ranking by likelihood potential diseasesbased on findings observed in a patient.Diagnostic test: Selecting and interpreting diagnostic tests for a patient, consid-ering their precision, accuracy, acceptability, cost, and safety.Etiology/Harm: Identifying factors that cause a disease or condition in a patient.Prognosis: Estimating a patient?s likely course over time and anticipating likelycomplications.These activities represent what Ingwersen (1999) calls ?work tasks.?
It is importantto note that they exist independently of information needs, namely, searching is notnecessarily implicated in any of these activities.
We are, however, interested in situationswhere questions arise during one of these clinical tasks?only then does the physicianengage in information-seeking behavior.
These activities translate into natural ?searchtasks.?
For therapy, the search task is usually therapy selection (for example, determiningwhich course of action is the best treatment for a disease) or prevention (for example,selecting preemptive measures with respect to a particular disease).
For diagnosis,there are two different possibilities: in differential diagnosis, a physician is consider-ing multiple hypotheses regarding what disease a patient has; in diagnostic methodsselection, the clinician is attempting to ascertain the relative utility of different tests.For etiology, cause determination is the search task, and for prognosis, patient outcomeprediction.Terms and the types of studies relevant to each of the four tasks have been exten-sively studied by the Hedges Project at the McMaster University (Haynes et al 1994;Wilczynski, McKibbon, and Haynes 2001).
The results of this research are implementedin the PubMed Clinical Queries tools, which can be used to retrieve task-specific cita-tions (more about this in the next section).The second facet is independent of the clinical task and pertains to the struc-ture of a well-built clinical question.
The following four components have been iden-tified as the key elements of a question related to patient care (Richardson et al1995): What is the primary problem or disease?
What are the characteristics ofthe patient (e.g., age, gender, or co-existing conditions)? What is the main intervention (e.g., a diagnostic test, medication, ortherapeutic procedure)? What is the main intervention compared to (e.g., no intervention, anotherdrug, another therapeutic procedure, or a placebo)? What is the desired effect of the intervention (e.g., cure a disease, relieveor eliminate symptoms, reduce side effects, or lower cost)?These four elements are often referenced with the mnemonic PICO, which standsfor Patient/Problem, Intervention, Comparison, and Outcome.Finally, the third facet serves as a tool for appraising the strength of evidencepresented in the study, that is, how much confidence should a physician have in theresults?
Several taxonomies for appraising the strength of evidence based on the typeand quality of the study have been developed.
We chose the Strength of Recommenda-tions Taxonomy (SORT) as the basis for determining the potential upper bound on the66Demner-Fushman and Lin Answering Clinical Questionsquality of evidence, due to its emphasis on the use of patient-oriented outcomes and itsattempt to unify other existing taxonomies (Ebell et al 2004).
There are three levels ofrecommendations according to SORT: A-level evidence is based on consistent, good-quality patientoutcome-oriented evidence presented in systematic reviews, randomizedcontrolled clinical trials, cohort studies, and meta-analyses. B-level evidence is inconsistent, limited-quality, patient-oriented evidencein the same types of studies. C-level evidence is based on disease-oriented evidence or studies lessrigorous than randomized controlled clinical trials, cohort studies,systematic reviews, and meta-analyses.A question-answering system designed to support the practice of evidence-basedmedicine must be sensitive to the multifaceted considerations that go into evaluatingan abstract?s relevance to a clinical information need.
It is exactly these three comple-mentary facets that we attempt to encode in a question-answering system for clinicaldecision support.3.
MEDLINE and PubMedMEDLINE is a large bibliographic database maintained by the U.S. National Libraryof Medicine (NLM).
This database is viewed by medical professionals, biomedicalresearchers, and many other users as the authoritative source of clinical evidence, andhence we have adopted it as the target corpus for our clinical question-answering sys-tem.
MEDLINE contains over 15 million references to articles from approximately 4,800journals in 30 languages, dating back to the 1960s.
In 2004, over 571,000 new citationswere added to the database, and it continues to grow at a steady pace.
The subjectscope of MEDLINE is biomedicine and health, broadly defined to encompass thoseareas of the life sciences, behavioral sciences, chemical sciences, and bioengineeringneeded by health professionals and others engaged in basic research and clinical care,public health, health policy development, or related educational activities.
MEDLINEalso covers life sciences vital to biomedical practitioners, researchers, and educators,including aspects of biology, environmental science, marine biology, plant and animalscience, as well as biophysics and chemistry.1Each MEDLINE citation includes basic information such as the title of the article,name of the authors, name of the publication, publication type, date of publication,language, and so on.
Of the entries added over the last decade or so, approximately76% have English abstracts written by the authors of the articles?these texts providethe source for answers extracted by our system.Additional metadata are associated with each MEDLINE citation.
The most impor-tant of these is the controlled vocabulary terms assigned by human indexers.
NLM?scontrolled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approx-imately 23,000 descriptors arranged in a hierarchical structure and more than 151,000Supplementary Concept Records (additional chemical substance names) within a1 http://www.nlm.nih.gov/pubs/factsheets/medline.html2 Commonly referred to as MeSH terms or MeSH headings, although technically the latter is redundant.67Computational Linguistics Volume 33, Number 1separate thesaurus.
Indexing is performed by approximately 100 indexers with at leastbachelor?s degrees in life sciences and formal training in indexing provided by NLM.Since mid-2002, the Library has been employing software that automatically suggestsMeSH headings based on content (Aronson et al 2004).
Nevertheless, the indexingprocess remains firmly human-centered.As a concrete example, an abstract titled ?Antipyretic efficacy of ibuprofen vs.acetaminophen?
might have the following MeSH headings associated with it:MH - Acetaminophen/*therapeutic useMH - ChildMH - Comparative StudyMH - Fever/*drug therapyMH - Ibuprofen/*therapeutic useTo represent different aspects of the topic described by a particular MeSH heading,up to three subheadings may be assigned, as indicated by the slash notation.
In thisexample, a trained user could interpret from the MeSH terms that the article is aboutdrug therapy for fever and the therapeutic use of ibuprofen and acetaminophen.
Anasterisk placed next to a MeSH heading indicates that the human indexer interprets theterm to be the main focus of the article.
Multiple MeSH terms can be notated in thismanner.MEDLINE is publicly accessible on the World Wide Web through PubMed, the Na-tional Library of Medicine?s gateway, or through third-party organizations that licenseMEDLINE from NLM.
PubMed is a sophisticated boolean search engine that allowsusers to query not only on abstract text, but also on metadata fields such as MeSH terms.In addition, PubMed provides a number of pre-defined ?search templates?
called Clin-ical Queries (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001) that allowusers to narrow the scope of retrieved articles.
These filters are implemented as fixedboolean query fragments (containing restrictions on MeSH terms, for example) that areappended to the original user query.
Our experiments involve the use of PubMed toretrieve an initial set of candidate citations for subsequent processing.4.
System ArchitectureWe view clinical question answering as ?semantic unification?
between informationneeds expressed in a PICO-based frame and corresponding structures automaticallyextracted from MEDLINE citations.
In accordance with the principles of EBM, thismatching process should be sensitive to the nature of the clinical task and the strengthof evidence of retrieved abstracts.As a concrete example, consider the following clinical question:In children with an acute febrile illness, what is the efficacy of single-medicationtherapy with acetaminophen or ibuprofen in reducing fever?The information need might be formally encoded in the following manner:Search Task: therapy selectionProblem/Population: acute febrile illness/in childrenIntervention: acetaminophenComparison: ibuprofenOutcome: reducing fever68Demner-Fushman and Lin Answering Clinical QuestionsThis query representation explicitly encodes the search task and the PICO structureof the clinical question.
After processing MEDLINE citations, automatically extractingPICO elements from the abstracts, and semantically matching these elements with thequery, a system might produce the following answer:Ibuprofen provided greater temperature decrement and longer duration of antipyresisthan acetaminophen when the two drugs were administered in approximately equaldoses.PMID: 1621668Strength of Evidence: grade APhysicians are usually most interested in outcome statements that assert a patient-oriented clinical finding?for example, the relative efficacy of two drugs.
Thus, out-comes can serve as the basis for good answers and an entry point into the full text.
Thesystem should automatically evaluate the strength of evidence of the citations supplyingthe answer, but the decision to adopt the recommendations as suggested ultimately restswith the physician.What is the best input to a clinical question-answering system?
Two possibilitiesinclude a natural language question or a structured PICO query frame.
We advocatethe latter.
With a frame-based query interface, the physician shoulders the burden oftranslating an information need into a frame-based representation, but this providesseveral advantages.
Most importantly, formal representations force physicians to ?thinkthrough?
their questions, ensuring that relevant elements are captured.
Poorly formu-lated queries have been identified by Ely et al (2005) as one of the obstacles to findinganswers to clinical questions.
Because well-formed questions should have concretelyinstantiated PICO slots, a frame representation clearly lets the physician see missingelements.
In addition, a structured query representation obviates the need for linguisticanalysis of a natural language question, where ambiguities may negatively impactoverall performance.
We discuss alternative interfaces in Section 12.Ideally, we would like to match structured representations derived from the ques-tion with those derived from MEDLINE citations (taking into consideration other EBM-relevant factors).
However, we do not have access to the computational resources nec-essary to apply knowledge extractors to the 15 million plus citations in the MEDLINEdatabase and directly index their results.
As an alternative, we rely on PubMed toretrieve an initial set of hits that we then postprocess in greater detail?this is thestandard pipeline architecture commonly employed in other question-answering sys-tems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001).The architecture of our system is shown in Figure 1.
The query formulator is respon-sible for converting a clinical question (in the form of a query frame) into a PubMedsearch query.
Presently, these queries are already encoded in our test collection (seeSection 6).
PubMed returns an initial list of MEDLINE citations, which is then analyzedby our knowledge extractors (see Section 5).
The input to the semantic matcher, whichimplements our EBM citation scoring algorithm, is the query frame and annotatedMEDLINE citations.
The module outputs a ranked list of citations that have been scoredin accordance with the principles of EBM (see Section 6).
Finally, the answer generatortakes these citations and extracts appropriate answers (see Section 9).In summary, our conception of clinical question answering as semantic framematching suggests the need for a number of capabilities, which correspond to thebold outlined boxes in Figure 1: knowledge extraction, semantic matching for scoring69Computational Linguistics Volume 33, Number 1Figure 1Architecture of our clinical question-answering system.citations, and answer generation.
We have realized all three capabilities in an imple-mented clinical question-answering system and conducted three separate evaluationsto assess the effectiveness of our developed capabilities.
We do not tackle the queryformulator, although see discussion in Section 12.
Overall, results indicate that ourimplemented system significantly outperforms the PubMed baseline.5.
Knowledge Extraction for Evidence-Based MedicineThe automatic extraction of PICO elements from MEDLINE citations represents a keycapability integral to clinical question answering.
This section, which elaborates onpreliminary results reported in Demner-Fushman and Lin (2005), describes extractionalgorithms for population, problems, interventions, outcomes, and the strength of evi-dence.
For an example of a completely annotated abstract, see Figure 2.
Each individualPICO extractor takes as input the abstract text of a MEDLINE citation and identifies therelevant elements: Outcomes are complete sentences, while population, problems, andinterventions are short noun phrases.Our knowledge extractors rely extensively on MetaMap (Aronson 2001), a systemfor identifying segments of text that correspond to concepts in the UMLS Metathe-saurus.
Many of our algorithms operate at the level of coarser-grained semantictypes called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capturehigher-level generalizations about entities (e.g., CHEMICALS & DRUGS).
An additionalfeature we take advantage of (when present) is explicit section markers present in someabstracts.
These so-called structured abstracts were recommended by the Ad Hoc Work-ing Group for Critical Appraisal of the Medical Literature (1987) to help humans assessthe reliability and content of a publication and to facilitate the indexing and retrievalprocesses.
These abstracts loosely adhere to the introduction, methods, results, andconclusions format common in scientific writing, and delineate a study using explicitlymarked sections with variations of the above headings.
Although many core clinicaljournals require structured abstracts, there is a great deal of variation in the actualheadings.
Even when present, the headings are not organized in a manner focused onpatient care.
In addition, abstracts of much high-quality work remain unstructured.
Forthese reasons, explicit section markers are not entirely reliable indicators for the varioussemantic elements we seek to extract, but must be considered along with other sourcesof evidence.The extraction of each PICO element relies to a different extent on an annotatedcorpus of MEDLINE abstracts, created through an effort led by the first author atthe National Library of Medicine (Demner-Fushman et al 2006).
As will be describedherein, the population, problem, and the intervention extractors are based largely onrecognition of semantic types and a few manually constructed rules; the outcome extrac-70Demner-Fushman and Lin Answering Clinical Questionstor, in contrast, is implemented as an ensemble of classifiers trained using supervisedmachine learning techniques (Demner-Fushman et al 2006).
These two very differentapproaches can be attributed to differences in the nature of the frame elements: Whereasproblems and interventions can be directly mapped to UMLS concepts, and populationseasily mapped to patterns that include UMLS concepts, outcome statements follow nopredictable pattern.
The initial goal of the annotation effort was to identify outcomestatements in abstract text.
A physician, two registered nurses, and an engineeringresearcher manually identified sentences that describe outcomes in 633 MEDLINEabstracts; a post hoc analysis demonstrates good agreement (?
= 0.77).
The annotatedabstracts were retrieved using PubMed and attempted to model different user behaviorsranging from naive to expert (where advanced search features were employed).
With theexception of 50 citations retrieved to answer a question about childhood immunization,the rest of the results were retrieved by querying on a disease, for example, diabetes.
Ofthe 633 citations, 100 abstracts were also fully annotated with population, problems, andinterventions.
These 100 abstracts were set aside as a held-out test set.
Of the remainingcitations, 275 were used for training and rule derivation, as described in the followingsections.After much exploration, Demner-Fushman et al (2006) discovered that it was notpractical to annotate PICO entities at the phrase level due to significant unresolvabledisagreement and interannotator reliability issues.
Consider the following segment:This double-blind, placebo-controlled, randomized, 3-period, complete block, 6-weekcrossover study examined the efficacy of simvastatin in adult men and women (N =151) with stable type 2 DM, low density lipoprotein-cholesterol 100 mg/dL, HDL-C <40 mg/dL, and fasting triglyceride level > 150 and < 700 mg/dL.All annotators agreed that the sentence contained the problem, population, andintervention.
However, they could not agree on the exact phrasal boundaries of eachelement, and more importantly, general guidelines for ensuring consistent annotations.For example, should the whole clause starting with adult men and women be marked aspopulation, or should type 2 Diabetes Mellitus (type 2 DM) be marked up only as theproblem?
How should we indicate that the cholesterol levels description belongs to 151subjects of the study, and so forth?
This issue becomes important for evaluation becausethere is a mismatch between annotated ground truth and the output of our knowledgeextractors, as we will discuss.In what follows, we describe each of the individual PICO extractors and a series ofcomponent evaluations that assess their accuracy.
This section is organized such that thedescription of each extractor and its evaluation are paired together.
Results are reportedin terms of the percentage of correctly identified instances, percentage of instances forwhich the extractor had no answer, and percentage of incorrectly identified instances.The baselines and gold standards for each extractor vary, and will be described in-dividually.
The goal of these component evaluations is a general characterization ofperformance, as we focused the majority of our efforts on the two other evaluations.5.1 Population ExtractorThe PICO framework makes no distinction between the population and the problem,which is rooted in the concept of the population in clinical studies, as exemplified bytext such as POPULATION: Fifty-five postmenopausal women with a urodynamic diagnosisof genuine urinary stress incontinence.
Although this fragment simultaneously describes71Computational Linguistics Volume 33, Number 1the population (of which a particular patient can be viewed as a sample therefrom) andthe problem, we chose to separate the extraction of the two elements because they arenot always specified together in abstracts (issues with respect to exact boundaries men-tioned previously notwithstanding).
Furthermore, many clinical questions ask about aparticular problem without specifying a population.Population elements, which are typically noun phrases, are identified using a seriesof manually crafted rules that codify the following assumptions: The concept describing the population belongs to the semantic typeGROUP or any of its children.
In addition, certain nouns are often used todescribe study participants in medical texts; for example, an oftenobserved pattern is ?subjects?
or ?cases?
followed by a concept from thesemantic group DISORDER. The number of subjects that participated in the study often precedes orfollows a concept identified as a GROUP.
In the latter case, the number issometimes given in parentheses using a common pattern n = number,where ?n = ?
is a shorthand for the number of subjects, and numberprovides the actual number of study participants. The confidence that a clause with an identified number and GROUPcontains information about the population is inversely proportional to thedistance between the two entities. The confidence that a clause contains the population is influenced by theposition of the clause, with respect to headings in the case of structuredabstracts and with respect to the beginning of the abstract in the case ofunstructured abstracts.Given these assumptions, the population extractor searches for the followingpatterns: GROUP ([Nn]=[0?9]+)for example, in 5?6-year-old French children (n = 234), Subjects (n = 54) number* GROUPfor example, forty-nine infants number* DISORDER* GROUP?for example, 44 HIV-infected childrenThe confidence score assigned to a particular pattern match is a function of both itsposition in the abstract and its position in the clause from which it was extracted.
If anumber is followed by a measure, for example, year or percent, the number is discarded,and pattern matching continues.
After the entire abstract is processed in this manner,the match with the highest confidence value is retained as the population description.5.2 Evaluation of Population ExtractorNinety of the 100 fully annotated abstracts in our collection were agreed upon by theannotators as being clinical in nature, and were used as test data for our populationextractor.
Because these abstracts were not examined in the process of developing theextractor rules, they can be viewed as a blind held-out test set.
The output of our popu-72Demner-Fushman and Lin Answering Clinical QuestionsTable 1Evaluation of the population extractor.Correct (%) Unknown (%) Wrong (%)Baseline 53 ?
47Extractor 80 10 10lation extractor was judged to be correct if it occurred in a sentence that was annotatedas containing the population in the gold standard.
Note that this evaluation presents anupper bound on the performance of the population extractor, whose outputs are nounphrases.
We adopted such a lenient evaluation setup because of the boundary issuespreviously discussed, and also to forestall potential difficulties with scoring partiallyoverlapping string matches.For comparison, our baseline simply returned the first three sentences of the ab-stract.
We considered the baseline correct if any one of the sentences were annotatedas containing the population in the gold standard (an even more lenient criterion).This baseline was motivated by the observation that the aim and methods sections ofstructured abstracts are likely to contain the population information?for structured ab-stracts, explicit headings provide structural cues; for unstructured abstracts, positionalinformation serves as a surrogate.The performance of the population extractor is shown in Table 1.
A manual erroranalysis revealed three sources of error: First, not all population descriptions containa number explicitly, for example, The medical charts of all patients who were treated withetanercept for back or neck pain at a single private medical clinic in 2003.
Second, not all studypopulations are population groups, as for example in All primary care trusts in England.Finally, tagging and chunking errors propagate to the semantic type assignment leveland affect the quality of MetaMap output.
For example, consider the following sentence:We have compared the LD and recombination patterns defined by single-nucleotidepolymorphisms in ENCODE region ENm010, chromosome 7p15 2, in Korean, Japanese,and Chinese samples.Both Korean and Japanese were mistagged as nouns, which lead to the followingerroneous chunking:[We] [have] [compared] [the LD] [and] [recombination patterns] [defined] [bysingle-nucleotide polymorphisms] [in] [ENCODE] [region ENm010,] [chromosome7p15 2,] [in Korean,] [Japanese,] [and] [Chinese samples.
]This resulted in the tagging of Japanese as a population.
Errors of this type affectother extractors as well.
For example, lead was mistagged as a noun in the phraseEchocardiographic findings lead to the right diagnosis, which caused MetaMap to identifythe word as a PHARMACOLOGICAL SUBSTANCE (lead is sometimes used as a homeo-pathic preparation).5.3 Problem ExtractorThe problem extractor relies on the recognition of concepts belonging to the UMLSsemantic group DISORDER.
In short, it returns a ranked list of all such concepts withina given span of text.
We evaluate the performance of this simple heuristic on segments73Computational Linguistics Volume 33, Number 1Table 2Evaluation of the problem extractor.Correct (%) Unknown (%) Wrong (%)Abstract title 85 10 5Title + 1st two sentences 90 5 5Entire abstract 86 2 12of the abstract varying in length: abstract title only, abstract title and first two sentences,and entire abstract text.
Concepts in the title, in the introduction section of structuredabstracts, or in the first two sentences in unstructured abstracts, are given higher confi-dence values due to their discourse prominence.
Finally, the highest-scoring problemis designated as the primary problem in order to differentiate it from co-occurringconditions identified in the abstract.5.4 Evaluation of Problem ExtractorAlthough our problem extractor returns a list of clinical problems, we only evalu-ate performance on identification of the primary problem.
For some abstracts, MeSHheadings can be used as ground truth, because one of the human indexers?
tasks inassigning terms is to identify the main topic of the article (sometimes a disorder).
Forthis evaluation, we randomly selected 50 abstracts with disorders indexed as the maintopic from abstracts retrieved using PubMed on the five clinical questions describedin Sneiderman et al (2005).We applied our problem extractor on different segments of the abstract: the titleonly, the title and first two sentences, and the entire abstract.
These results are shown inTable 2.
Here, a problem was considered correctly identified only if it shared the sameconcept ID as the ground truth problem (from the MeSH heading).
The performance ofour best variant (abstract title and first two sentences) approaches the upper bound onMetaMap performance?which is limited by human agreement on the identification ofsemantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003).Although problem extraction largely depends on disease coverage in UMLS andMetaMap performance, the error rate could be further reduced by more sophisticatedrecognition of implicitly stated problems.
For example, with respect to a question aboutimmunization in children, an abstract about the measles-mumps-rubella vaccinationnever mentioned the disease without the word vaccination; hence, no concept of thetype DISEASE OR SYNDROME was identified.5.5 Intervention ExtractorThe intervention extractor identifies both the intervention and comparison elements ina PICO frame; processing of these two frame elements can be collapsed because theybelong to the same semantic group.
In many abstracts, it is unclear which intervention isthe primary one and which are the comparisons, and hence our extractor simply returnsa list of all interventions under study.For interventions, we are primarily interested in entities that may participate in theUMLS Semantic Network relations associated with each clinical task.
Restrictions onthe semantic types allowed in these relations prescribe the set of possible clinical in-terventions.
For therapy these relations include treats, prevents, and carries out; diagnoses74Demner-Fushman and Lin Answering Clinical QuestionsTable 3Evaluation of the intervention extractor.Correct (%) Unknown (%) Wrong (%)Baseline 60 ?
40Extractor 80 ?
20for diagnosis; causes and result of for etiology; and prevents for prognosis.
At present, theidentification of nine semantic types, for example, DIAGNOSTIC PROCEDURE, CLINICALDRUG, and HEALTH CARE ACTIVITY, serves as the foundation for our interventionextraction algorithm.Candidate scores are further adjusted to reflect a few different factors.
In structuredabstracts, concepts of the relevant semantic type are given additional weight if theyappear in the title, aims, and methods sections.
In unstructured abstracts, conceptstowards the beginning of the abstract text are favored.
Finally, the intervention extractortakes into account the presence of certain cue phrases that describe the aim and/ormethods of the study, such as This study examines or This paper describes.5.6 Evaluation of Intervention ExtractorThe intervention extractor was evaluated in the same manner as the population extrac-tor and compared to the same baseline.
To iterate, 90 held-out clinical abstracts thatcontained human-annotated interventions served as ground truth.
The output of ourintervention extractor was judged to be correct if it occurred in a sentence that wasannotated as containing the intervention in the gold standard.
As with the evaluationof the population extractor, this represents an upper bound on performance.
Results areshown in Table 3.Some of the errors were caused by ambiguity of terms.
For example, in the clauseserum levels of anti-HBsAg and presence of autoantibodies (ANA, ENA) were evaluated,serum is recognized as a TISSUE, levels as INTELLECTUAL PRODUCT, and autoantibodiesand ANA as IMMUNOLOGIC FACTORS.
In this case, however, autoantibodies shouldbe considered a LABORATORY OR TEST RESULT.3 In other cases, extraction errorswere caused by summary sentences that were very similar to intervention statements,for example, This study compared the effects of 52 weeks?
treatment with pioglitazone, athiazolidinedione that reduces insulin resistance, and glibenclamide, on insulin sensitivity,glycaemic control, and lipids in patients with Type 2 diabetes.
For this particular abstract,the correct interventions are contained in the sentence Patients with Type 2 diabeteswere randomized to receive either pioglitazone (initially 30 mg QD, n = 91) or micronizedglibenclamide (initially 1.75 mg QD, n = 109) as monotherapy.5.7 Outcome ExtractorWe approached outcome extraction as a classification problem at the sentence level, thatis, the outcome extractor assigns a probability of being an outcome to each sentencein an abstract.
Our preliminary work has led to a strategy based on an ensemble ofclassifiers, which includes a rule-based classifier, a unigram ?bag of words?
classifier,3 MetaMap does provide alternative mappings, but the current extractor only considers the best candidate.75Computational Linguistics Volume 33, Number 1an n-gram classifier, a position classifier, an abstract length classifier, and a semanticclassifier.
With the exception of the rule-based classifier, all classifiers were trained onthe 275 citations from the annotated collection of abstracts described previously.Knowledge for the rule-based classifier was hand-coded, prior to the annotationeffort, by a registered nurse with 20 years of clinical experience.
This classifier estimatesthe likelihood that a sentence states an outcome based on cue phrases such as signif-icantly greater, well tolerated, and adverse events.
The likelihood of a sentence being anoutcome as indicated by cue phrases is the ratio of the cumulative score for recognizedphrases to the maximum possible score.
For example, the sentence The dropout rate due toadverse events was 12.4% in the moxonidine and 9.8% in the nitrendipine group is segmentedinto eight phrases by MetaMap, which sets the maximum score to 8.
The two phrasesdropout rate and adverse events contribute one point each to the cumulative score, whichresults in a likelihood estimate of 0.25 for this sentence.The unigram ?bag of words?
classifier is a naive Bayes classifier implemented withthe API provided by the MALLET toolkit.4 This classifier outputs the probability of aclass assignment.The n-gram based classifier is also a naive Bayes classifier, but it operates on a differ-ent set of features.
We first identified the most informative unigrams and bigrams usingthe information gain measure (Yang and Pedersen 1997), and then selected only thepositive outcome predictors using odds ratio (Mladenic and Grobelnik 1999).
Disease-specific terms, such as rheumatoid arthritis, were then manually removed.
Finally, thelist of features was revised by the registered nurse who participated in the annotationeffort.
This classifier also outputs the probability of a class assignment.The position classifier returns the maximum likelihood estimate that a sentence isan outcome based on its position in the abstract (for structured abstracts, with respectto the results or conclusions sections; for unstructured abstracts, with respect to the endof the abstract).The abstract length classifier returns a smoothed (add one smoothing) probabilitythat an abstract of a given length (in the number of sentences) contains an outcomestatement.
For example, the probability that an abstract four sentences long contains anoutcome statement is 0.25, and the probability of finding an outcome in a ten sentence?long abstract is 0.92.
This feature turns out to be useful because the average length ofabstracts with and without outcome statements differs: 11.7 sentences for the former,7.95 sentences for the latter.The semantic classifier assigns to a sentence an ad hoc score based on the presenceof UMLS concepts belonging to semantic groups highly associated with outcomessuch as THERAPEUTIC PROCEDURE or PHARMACOLOGICAL SUBSTANCE.
The score isgiven a boost if the concept has already been identified as the primary problem or anintervention.The outputs of our basic classifiers are combined using a simple weighted linearinterpolation scheme:Soutcome = ?1Scues + ?2Sunigram + ?3Sn-gram + ?4Sposition + ?5Slength + ?6Ssemantic type (1)We attempted two approaches for assigning these weights.
The first method reliedon ad hoc weight selection based on intuition.
The second involved a more principledmethod using confidence values generated by the base classifiers and least squares lin-4 http://mallet.cs.umass.edu/76Demner-Fushman and Lin Answering Clinical Questionsear regression adapted for classification (Ting and Witten 1999), which can be describedby the following equation:LR(x) =N?k=1?kPk(X) (2)Pk is the probability that a sentence specifies an outcome, as determined by classifierk (for classifiers that do not return actual probabilities, we normalized the scores andtreated them as such).
To predict the class of a sentence, the probabilities generatedby n classifiers are combined using the coefficients (?0, ...,?n).
These values are de-termined in the training stage as follows: Probabilities predicted by base classifiersfor each sentence are represented in an N ?
M matrix A, where M is the number ofsentences in the training set, and N is the number of classifiers.
The gold standardclass assignments for each sentence is stored in a vector b, and weights are found bycomputing the vector ?
that minimizes ||A??
b||.
The solution can be found usingsingular value decomposition, as provided in the JAMA basic linear algebra packagereleased by NIST.55.8 Evaluation of Outcome ExtractorBecause outcome statements were annotated in each of the 633 citations in our collec-tion, it was possible to evaluate the outcome extractor on a broader set of abstracts.
Fromthose not used in training the outcome classifiers, 153 citations pertaining to therapywere selected.
Of these, 143 contained outcome statements and were used as the blindheld-out test set.
In addition, outcome statements in abstracts pertaining to diagnosis(57), prognosis (111), and etiology (37) were also used.The output of our outcome extractor is a ranked list of sentences sorted by con-fidence.
Based on the observation that human annotators typically mark two to threesentences in each abstract as outcomes, we evaluated the performance of our extractorat cutoffs of two and three sentences.
These results are shown in Table 4: The columnsmarked AH2 and AH3 show performance of the weighted linear interpolation approachwith ad hoc weight assignment at two- and three-sentence cutoffs, respectively; thecolumns marked LR2 and LR3 show performance of the least squares linear regressionmodel at the same cutoffs.
In the evaluation, our outcome extractor was consideredcorrect if the returned sentences intersected with sentences judged as outcomes byour human annotators.
Although this is a lenient criterion, it does roughly capturethe performance of our knowledge extractor.
Because outcome statements are typicallyfound in the conclusion of a structured abstract (or near the end of the abstract in thecase of unstructured abstracts), we compared our answer extractor to the baseline ofreturning either the final two or final three sentences in the abstract (B2 and B3 inTable 4).As can be seen, variants of our outcome extractor performed better than the baselineat the two-sentence cutoff, for the most part.
Bigger improvements, however, can beseen at the three-sentence cutoff level.
It is evident that the assignment of weights inour ad hoc model is primarily geared towards therapy questions, perhaps overly so.Better overall performance is obtained with the least squares linear regression model.5 http://math.nist.gov/javanumerics/jama/77Computational Linguistics Volume 33, Number 1Table 4Evaluation of the outcome extractor.
B = baseline, returns last sentences in abstract; AH = ad hocweight assignment; LR = least squares linear regression.
Statistically significant improvementover the baseline at the 1% level is indicated by  .2-sentence cutoff (%) 3-sentence cutoff (%)B2 AH2 LR2 B3 AH3 LR3Therapy 74 75 77 75 95 93Diagnosis 72 70 78 75 78 89Prognosis 73 76 79 85 87 89Etiology 64 68 74 78 83 88Table 5Examples of strength of evidence categories based on Publication Type and MeSH headings.Strength of Evidence Publication Type/MeSHLevel A(1) Meta-analysis, randomized controlled trials, cohort study,follow-up studyLevel B(2) Case-control study, case seriesLevel C(3) Case report, in vitro, animal and animal testing,alternatives studiesThe majority of errors made by the outcome extractor were related to inaccuratesentence boundary identification, chunking errors, and word sense ambiguity in theMetathesaurus.5.9 Determining the Strength of EvidenceThe strength of evidence is a classification scheme that helps physicians assess thequality of a particular citation for clinical purposes.
Metadata associated with mostMEDLINE citations (MeSH terms) are extensively used to determine the strength ofevidence and in our EBM citation scoring algorithm (Section 6).The potential highest level of the strength of evidence for a given citation can beidentified using the Publication Type (a metadata field) and MeSH terms pertainingto the type of the clinical study.
Table 5 shows our mapping from publication typeand MeSH headings to evidence grades based on principles defined in the Strengthof Recommendations Taxonomy (Ebell et al 2004).5.10 Sample OutputA complete example of our knowledge extractors working in unison is shown inFigure 2, which contains an abstract retrieved in response to the following question:?In children with an acute febrile illness, what is the efficacy of single-medicationtherapy with acetaminophen or ibuprofen in reducing fever??
(Kauffman, Sawyer, andScheinbaum 1992).
Febrile illness is the only concept mapped to DISORDER, and henceis identified as the primary problem.
37 otherwise healthy children aged 2 to 12 years iscorrectly identified as the population.
Acetaminophen, ibuprofen, and placebo are correctly78Demner-Fushman and Lin Answering Clinical QuestionsAntipyretic efficacy of ibuprofen vs acetaminophenKauffman RE, Sawyer LA, Scheinbaum MLAm J Dis Child.
1992 May;146(5):622-5OBJECTIVE?To compare the antipyretic efficacy of ibuprofen, placebo, andacetaminophen.
DESIGN?Double-dummy, double-blind, randomized, placebo-controlled trial.
SETTING?Emergency department and inpatient units of a large,metropolitan, university-based, children?s hospital in Michigan.
PARTICIPANTS???37???????????otherwise????????healthy?????????children?????aged??2???to???12?????
?yearsPopulation with??????acute,?????????????intercurrent,??????febrile???????illnessProblem.
INTERVENTIONS?Each child was randomly assigned to receivea single dose of??????????????
?acetaminophenIntervention (10 mg/kg),?????????
?ibuprofenIntervention (10 mg/kg) (7.5or 10 mg/kg), or???????
?placeboIntervention (10 mg/kg).
MEASUREMENTS/MAIN RESULTS?Oral temperature was measured before dosing, 30 minutes after dosing, andhourly thereafter for 8 hours after the dose.
Patients were monitored for ad-verse effects during the study and 24 hours after administration of the as-signed drug.???All?????three??????active???????????treatments??????????produced??????????significant???????????antipyresis???????????compared????with?????????placebo.Outcome??????????Ibuprofen??????????provided???????greater?????????????temperature??????????decrement?????and???????longer????????duration???of???????????antipyresis?????than???????????????acetaminophen??????when???the????two??????drugs?????were??????????????administered??in???????????????approximately??????equal?????
?doses.Outcome No adverse effects were observed in any treat-ment group.
CONCLUSION??????????Ibuprofen???is??a????????potent???????????antipyretic???????agent?????and??is???a????safe???????????alternative????for????the?????????selected???????febrile??????child?????who?????may????????benefit?????from????????????antipyretic???????????medication????but?????who??????either???????cannot????take???or?????does????not????????achieve???????????satisfactory????????????antipyresis????with???????????????
?acetaminophen.OutcomePublication Type: Clinical Trial, Randomized Controlled TrialPMID: 1621668Strength of Evidence: grade AFigure 2Sample output from our PICO extractors.extracted as the interventions under study.
The three outcome sentences are correctlyclassified; the short sentence concerning adverse effects was ranked lower than theother three sentences and hence below the cutoff.
The study design, from metadataassociated with the citation, allows our strength of evidence extractor to classify thisarticle as grade A.6.
Operationalizing Evidence-Based MedicineIn our view of clinical question answering, the knowledge extractors just described sup-ply the features on which semantic matching occurs.
This section describes an algorithmthat, when presented with a structured representation of an information need and aMEDLINE citation, automatically computes a topical relevance score in accordance withthe principles of EBM.In order to develop algorithms that operationalize the three facets of EBM, it isnecessary to possess a corpus of clinical questions on which to experiment.
Because nosuch test collection exists, we had to first manually create one.
Fortunately, collectionsof clinical questions (representing real-world information needs of physicians), are79Computational Linguistics Volume 33, Number 1Table 6Composition of our clinical questions collection.Therapy Diagnosis Prognosis Etiology TotalDevelopment 10 6 3 5 24Test 12 6 3 5 26available on-line.
From two sources, the Journal of Family Practice6 and the ParkhurstExchange,7 we gathered 50 clinical questions, which capture a realistic sampling of thescenarios that a clinical question-answering system would be confronted with.
Thesequestions were minimally modified from their original form as downloaded from theWorld Wide Web.
In a few cases, a single question actually consisted of several smallerquestions; such clusters were simplified by removing questions more peripheral to thecentral clinical problem.
All questions were manually classified into one of the fourclinical tasks; the distribution of the questions roughly follows the prevalence of eachtask type as observed in natural settings, noted by Ely et al (1999).
The final step inthe preparation process was manual translation of the natural language questions intoPICO query frames.Our collection was divided into a development set and a blind held-out test set forverification purposes.
The breakdown of these questions into the four clinical tasks andthe development/test split is shown in Table 6.
An example of each question type fromour development set is presented here, along with its query frame:Does quinine reduce leg cramps for young athletes?
(Therapy)search task: therapy selectionprimary problem: leg crampsco-occurring problems: muscle cramps, crampspopulation: young adultintervention: quinineHow often is coughing the presenting complaint in patients with gastroesophagealreflux disease?
(Diagnosis)search task: differential diagnosisprimary problem: gastroesophageal reflux diseaseco-occurring problems: coughWhat?s the prognosis of lupoid sclerosis?
(Prognosis)search task: patient outcome predictionprimary problem: lupus erythematosusco-occurring problems: multiple sclerosisWhat are the causes of hypomagnesemia?
(Etiology)search task: cause determinationprimary problem: hypomagnesemia6 http://www.jfponline.com/7 http://www.parkhurstexchange.com/qa/80Demner-Fushman and Lin Answering Clinical QuestionsAs discussed earlier, we do not believe that natural language text is the best inputfor a question-answering system.
Instead, a structured PICO-based representation cap-tures physicians?
information needs in a more perspicuous manner?primarily becauseclinicians are trained to analyze clinical situations with this framework.Mirroring the organization of our knowledge extractors, we broke up the P in PICOinto population, primary problem, and co-occurring problems in the query representa-tion.
The justification for this will become apparent when we present our algorithm forscoring MEDLINE citations, as each of these three facets must be treated differently.Note that many elements are specified only to the extent that they were explicit inthe original natural language question; for example, if the clinician does not specifya population, that element will be empty.
Finally, outcomes are not directly encoded inthe query representation because they are implicit most of the time; for example, in Doesquinine reduce leg cramps for young athletes?, the desired outcome, naturally, is to reducethe occurrence and severity of leg cramps.
Nevertheless, outcome identification is animportant component of the citation scoring algorithm, as we shall see later.What is the relevance of an abstract with respect to a particular clinical question?Evidence-based medicine outlines the need to consider three different facets (see Sec-tion 2), which we operationalize in the following manner:SEBM = SPICO + SSoE + Stask (3)The relevance of a particular citation, with respect to a structured query, includescontributions from matching PICO structures, the strength of evidence of the citation,and factors specifically associated with the search tasks (and indirectly, the clinicaltasks).
In what follows, we describe each of these contributions in detail.Viewed as a whole, each score component is a heuristic reflection of the factors thatenter into consideration when a physician examines a MEDLINE citation.
Although theassignment of numeric scores is based on intuition and may seem ad hoc in many cases,evaluation results in the next section demonstrate the effectiveness of our algorithm.This issue will be taken up further in Section 8.6.1 Scores Based on PICO ElementsThe score of an abstract based on extracted PICO elements, SPICO, is broken into individ-ual components according to the following formula:SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4)The first component in the equation, Sproblem, reflects a match between the primaryproblem in the query frame and the primary problem in the abstract (i.e., the highest-scoring problem identified by the problem extractor).
A score of 1 is given if the prob-lems match exactly based on their unique UMLS concept ID as provided by MetaMap.Matching based on concept IDs has the advantage that it abstracts away from termino-logical variation; in essence, MetaMap performs terminological normalization.
Failingan exact match of concept IDs, a partial string match is given a score of 0.5.
If the primaryproblem in the query has no overlap with the primary problem from the abstract, a scoreof ?1 is given.
Finally, if our problem extractor could not identify a problem (but thequery frame does contain a problem), a score of ?0.5 is given.Co-occurring problems must be taken into consideration in the differential diagnosisand cause determination search tasks because knowledge of the problems is typically81Computational Linguistics Volume 33, Number 1incomplete in these scenarios.
Therefore, physicians would normally be interested inany problems mentioned in the abstracts in addition to the primary problem specifiedin the query frame.
As an example, consider the question What is the differential diagnosisof chronic diarrhea in immunocompetent patients?
Although chronic diarrhea is the primaryproblem, citations that discuss additional related disorders should be favored over thosethat don?t.
In terms of actual scoring, disorders mentioned in the title receive threepoints, and disorders mentioned anywhere else receive one point (in addition to thematch score based on the primary problem, as discussed).Scores based on population and intervention, Spopulation and Sintervention respectively, mea-sure the overlap between query frame elements and corresponding elements extractedfrom abstracts.
A point is given to each matching intervention and matching population.For example, finding the population group children from a query frame in the abstractincrements the match score; the remaining words in the abstract population are ignored.Thus, if the query frame contains a population element and an intervention element, thescore for an abstract that contains the same UMLS concepts in the corresponding slotsis incremented by two.The outcome-based score, Soutcome, is simply the value assigned to the highest-scoringoutcome sentence (we employed the outcome extractor based on the linear regressionmodel for our experiments).
As outcomes are rarely explicitly specified in the originalquestion, we decided to omit them in the query representation.
Our citation scoringalgorithm simply considers the inherent quality of the outcome statements in an ab-stract, independent of the query.
This is justified because, given a match on the primaryproblem, all clinical outcomes are likely to be of interest to the physician.6.2 Scores Based on Strength of EvidenceThe relevance score component based on the strength of evidence is calculated in thefollowing manner:SSoE = Sjournal + Sstudy + Sdate (5)Citations published in core and high-impact journals such as Journal of the AmericanMedical Association (JAMA) get a score of 0.6 for Sjournal, and 0 otherwise.
In terms of thestudy type, Sstudy, clinical trials, such as randomized controlled trials, receive a score of0.5; observational studies, for example, case reports, 0.3; all non-clinical publications,?1.5; and 0 otherwise.
The study type is directly encoded in the Publication Type fieldof a MEDLINE citation.Finally, recency factors into the strength of evidence score according to the formula:Sdate = (yearpublication ?
yearcurrent )/100 (6)A mild penalty decreases the score of a citation proportionally to the time differencebetween the date of the search and the date of publication.6.3 Scores Based on Specific TasksThe final component of our EBM score is based on task-specific considerations, asreflected in manually assigned MeSH terms.
For search tasks falling into each clinicaltask, we gathered a list of terms that are positive and negative indicators of relevance.82Demner-Fushman and Lin Answering Clinical QuestionsThe task score, Stask, is given by:Stask =?t?MeSH?
(t) (7)The function ?
(t) maps a MeSH term to a positive score if the term is a positiveindicator for that particular task type, or a negative score if the term is a negative indi-cator for the clinical task.
Note that although our current system uses MeSH headingsassigned by human indexers, manually assigned terms can be replaced with automaticprocessing if needed (Aronson et al 2004).Below, we enumerate the relevant indicator terms by clinical task.
However, thereis a set of negative indicators common to all tasks; these were extracted from the setof genomics articles provided for the secondary task in the TREC 2004 genomics trackevaluation (Hersh, Bhupatiraju, and Corley 2004); examples include genetics and cellphysiology.
The positive and negative weights assigned to each term heuristically encodethe relative importance of different MeSH headings and are derived from the ClinicalQueries filters in PubMed, from the JAMA EBM tutorial series on critical appraisal ofmedical literature, from MeSH scope notes, and based on a physician?s understandingof the domain (the first author).Indicators for Therapy Tasks.
Positive indicators for therapy were derived from thePubMed?s Clinical Queries filters; examples include drug administration routes and anyof its children in the MeSH hierarchy.
A score of ?1 is given if the MeSH descriptoror qualifier is marked as the main theme of the article (indicated via the star notationby human indexers), and a score of ?0.5 otherwise.
If the question pertains to thesearch task of prevention, three additional headings are considered positive indicators:prevention and control, prevention measures, and prophylaxis.Indicators for Diagnosis Tasks.
Positive indicators for therapy are also used as negativeindicators for diagnosis because the relevant studies are usually disjoint; it is highlyunlikely that the same clinical trial will study both diagnostic methods and treatmentmethods.
The MeSH term diagnosis and any of its children are considered positiveindicators.
As with therapy questions, terms marked as the major theme get a score of?1.0, and ?0.5 otherwise.
This general assignment of indicator terms allows a systemto differentiate between questions such as Does a Short Symptom Checklist accuratelydiagnose ADHD?
and What is the most effective treatment for ADHD in children?, whichmight retrieve very similar sets of citations.Indicators for Prognosis Tasks.
Positive indicators for prognosis include the followingMeSH terms: survival analysis, disease-free survival, treatment outcome, health status, preva-lence, risk factors, disability evaluation, quality of life, and recovery of function.
For termsmarked as the major theme, a score of +2 is given; +1 otherwise.
There are no negativeindicators, other than those common to all tasks previously described.Indicators for Etiology Tasks.
Negative indicators for etiology include therapy-orientedMeSH terms; these terms are given a score of ?0.3.
Positive indicators for the diag-nosis task are weak positive indicators for etiology, and receive a positive score of+0.1.
The following MeSH terms are considered highly indicative of citations rele-vant to etiology: population at risk, risk factors, etiology, causality, and physiopathology.
If83Computational Linguistics Volume 33, Number 1one of these terms is marked as the major theme, a score of +2 is given; otherwise, ascore of +1 is given.7.
Evaluation of Citation ScoringThe previous section describes a relevance-scoring algorithm for MEDLINE citationsthat attempts to capture the principles of EBM.
In this section, we present an evaluationof this algorithm.Ideally, questions should be answered by directly comparing queries to knowl-edge structures derived from MEDLINE citations.
However, knowledge extraction onsuch large scales is impractical given our computational resources, so we opted foran IR-based pipeline approach.
Under this strategy, an existing search engine wouldbe employed to generate a candidate list of citations to be rescored, according to ouralgorithm.
PubMed is a logical choice for gathering this initial list of citations becauseit represents one of the most widely used tools employed by physicians and otherhealth professionals today.
The system supports boolean operators and sorts resultschronologically, most recent citations first.This two-stage retrieval process immediately suggests an evaluation methodologyfor our citation scoring algorithm?as a document reranking task.
Given an initial hitlist, can our algorithm automatically re-sort the results such that relevant documentsare brought to higher ranks?
Not only is such a task intuitive to understand, thisconceptualization also lends itself to an evaluation based on widely accepted practicesin information retrieval.For each question in our test collection, PubMed queries were manually crafted tofetch an initial set of hits.
These queries took advantage of existing advanced searchfeatures to simulate the types of results that would be currently available to a knowl-edgeable physician.
Specifically, widely accepted tools for narrowing down PubMedsearch results such as Clinical Queries were employed whenever appropriate.As a concrete example, consider the following question: What is the best treatment foranalgesic rebound headaches?
The search started with the initial terms ?analgesic reboundheadache?
with a ?narrow therapy filter.?
In PubMed, this query is:((?headache disorders?
[TIAB] NOT Medline[SB]) OR ?headache disorders?
[MeSHTerms] OR analgesic rebound headache[Text Word]) AND (randomized controlledtrial[Publication Type] OR (randomized[Title/Abstract] ANDcontrolled[Title/Abstract] AND trial[Title/Abstract])) AND hasabstract[text] ANDEnglish[Lang] AND ?humans?
[MeSH Terms]Note that PubMed automatically identifies concepts and attempts matching bothin abstract text and MeSH headings.
We always restrict searches to articles that haveabstracts, are published in English, and are assigned the MeSH term humans (as opposedto say, experiments on animals)?these are all strategies commonly used by clinicians.In this case, because none of the top 20 results were relevant, the query was ex-panded with the term side effects to emphasize the aspect of the problem requiring anintervention.
The final query for the question became:(((?analgesics?
[TIAB] NOT Medline[SB]) OR ?analgesics?
[MeSH Terms] OR?analgesics?
[Pharmacological Action] OR analgesic[Text Word]) AND((?headache?
[TIAB] NOT Medline[SB]) OR ?headache?
[MeSH Terms] ORheadaches[Text Word]) AND (?adverse effects?
[Subheading] OR side effects[TextWord])) AND hasabstract[text] AND English[Lang] AND ?humans?
[MeSH Terms]84Demner-Fushman and Lin Answering Clinical QuestionsThe first author, who is a medical doctor, performed the query formulation processmanually for every question in our collection; she verified that each hit list contained atleast some relevant documents and that the results were as good as could be reasonablyachieved.
The process of generating queries averaged about 40 minutes per question.The top 50 results for each query were retained for our experiments.
In total, 2,309citations were gathered because some queries returned fewer than 50 hits.
The processof generating a ?good?
PubMed query is not a trivial task, which we have side-steppedin this work by placing a human in the loop.
We return to this issue in Section 12.All abstracts gathered by this process were exhaustively examined for relevance bythe first author.
It is important to note that relevance assessment in the clinical domainrequires significant medical knowledge (in short, a medical degree).
After careful con-sideration, we decided to assess only topical relevance, with the understanding thatthe applicability of information from a specific citation in real-world settings dependson a variety of other factors (see Section 10 for further discussion).
Each citation wasassigned one of four labels: Contains answer: The citation directly contains information that answersthe question. Relevant: The citation does not directly answer the question, but providestopically relevant information. Partially relevant: The citation provides information that is marginallyrelevant. Not relevant: The citation does not provide any topically relevantinformation.Because all abstracts were judged, we did not have to worry about impartialityissues when comparing different systems.
In total, the relevance assessment processtook approximately 100 hours, or about an average of 2 hours per question.Our reranking experiment compared four different systems: The baseline PubMed results. A term-based reranker that computes term overlap between the naturallanguage question and the citation (i.e., counted words shared between thetwo strings).
Each term match was weighted by the outcome score of thesentence from which it came (see Section 5.7).
This simple algorithm favorsterm matches that occur in sentences recognized as outcome statements. A reranker based on the EBM scorer described in the previous section. A reranker that combines normalized scores from the term-based rerankerand the EBM-based reranker (weighted linear interpolation).Questions in the development set were used to debug the EBM-based reranker aswe implemented the scoring algorithm.
The development questions were also used totune the weight for combining scores from the term-based scorer and EBM-based scorer;by simply trying all possible values, we settled on a ?
of 0.8, that is, 80% weight to theEBM score, and 20% weight to the term-based score.
As we shall see later, it is unclearif evidence combination in this simple manner helps at all; for one, it is debatablewhich metric should be optimized.
The test questions were hidden during the system85Computational Linguistics Volume 33, Number 1development phase and served as a blind held-out test set for assessing the generalityof our algorithm.In our experiment, we collected the following metrics, all computed automaticallyusing our relevance judgments: Precision at ten retrieved documents (P10) measures the fraction ofrelevant documents in the top ten results. Mean Average Precision (MAP) is the average of precision values aftereach relevant document is retrieved (Baeza-Yates and Ribeiro-Neto 1999).It is the most widely accepted single-value metric in information retrieval,and is seen to balance the need for both precision and recall. Mean Reciprocal Rank (MRR) is a measure of how far down a hit list theuser must browse before encountering the first relevant result.
The score isequal to the reciprocal of the rank, that is, a relevant document at rank 1gets a score of 1, 1/2 at rank 2, 1/3 at rank 3, and so on.
Note that thismeasure only captures the appearance of the first relevant document.Furthermore, due to its discretization, MRR values are noisy on smallcollections. Total Document Reciprocal Rank (TDRR) is the sum of the reciprocalranks of all relevant documents.
For example, if relevant documents werefound at ranks 2 and 5, the TDRR would be 1/2 + 1/5 = 0.7.
TDRRprovides an advantage over MRR in that it captures the ranks of allrelevant documents?emphasizing their appearance at higher ranks.
Thedownside, however, is that TDRR does not have an intuitive interpretation.For our reranking experiment, we applied the Wilcoxon signed-rank test to deter-mine the statistical significance of the results.
This test is commonly used in informationretrieval research because it makes minimal assumptions about the underlying distrib-ution of differences.
For each evaluation metric, significance at the 1% level is indicatedby either  or , depending on the direction of change; significance at the 5% levelis indicated by  or , depending on the direction of change.
Differences that are notstatistically significant are marked with the symbol ?.We report results under two different scoring criteria.
Under the lenient condition,documents marked ?contains answer?
and ?relevant?
were given credit; these resultsare shown in Table 7 (for the development set) and Table 8 (for the blind held-out testset).
Across all questions, both the EBM-based reranker and combination reranker sig-nificantly outperform the PubMed baseline on all metrics.
In many cases, the differencesare particularly noteworthy?for example, our EBM citation scoring algorithm morethan doubles the baseline in terms of MAP and P10 on the test set.
There are enoughtherapy questions to achieve statistical significance in the task-specific results; however,due to the smaller number of questions for the other clinical tasks, those results arenot statistically significant.
Results also show that the simple term-based reranker out-performs the PubMed baseline, demonstrating the importance of recognizing outcomestatements in MEDLINE abstracts.Are the differences in performance between the term-based, EBM, and combinationrerankers statistically significant?
Results of Wilcoxon signed-rank tests are shown inTable 11.
Both the EBM and combination rerankers significantly outperform the term-based reranker (at the 1% level, on all metrics, on both development and test set), with86Demner-Fushman and Lin Answering Clinical QuestionsTable 7(Lenient, Development) Lenient results of reranking experiment on development questionsfor the baseline PubMed condition, term-based reranker, EBM-based reranker, and combinationreranker.Therapy Diagnosis Prognosis EtiologyPrecision at 10 (P10)PubMed 0.300 0.367 0.400 0.533Term 0.520 (+73%) 0.383 (+4.5%)?
0.433 (+8.3%)?
0.553 (+3.8%)?EBM 0.730 (+143%) 0.800 (+118%) 0.633 (+58%)?
0.553 (+3.7%)?Combo 0.750 (+150%) 0.783 (+114%) 0.633 (+58%)?
0.573 (+7.5%)?Mean Average Precision (MAP)PubMed 0.354 0.421 0.385 0.608Term 0.622 (+76%) 0.438 (+4.0%)?
0.464 (+21%)?
0.720 (+18%)?EBM 0.819 (+131%) 0.794 (+89%) 0.635 (+65%)?
0.649 (+6.7%)?Combo 0.813 (+130%) 0.759 (+81%) 0.644 (+67%)?
0.686 (+13%)?Mean Reciprocal Rank (MRR)PubMed 0.428 0.792 0.733 0.900Term 0.853 (+99%) 0.739 (?6.7%)?
0.833 (+14%)?
1.000 (+11%)?EBM 0.933 (+118%) 0.917 (+16%)?
0.667 (?9.1%)?
1.000 (+11%)?Combo 0.933 (+118%) 0.917 (+16%)?
1.000 (+36%)?
0.900 (+0.0%)?Total Document Reciprocal Rank (TDRR)PubMed 1.317 1.805 1.778 2.008Term 2.305 (+75%) 1.887 (+4.6%)?
1.923 (+8.2%)?
2.291 (+14%)?EBM 2.869 (+118%) 2.944 (+63%) 2.238 (+26%)?
2.104 (+4.8%)?Combo 2.833 (+115%) 2.870 (+59%) 2.487 (+40%)?
2.108 (+5.0%)?
(a) Breakdown by clinical taskP10 MAP MRR TDRRPubMed 0.378 0.428 0.656 1.640Term 0.482 (+28%)?
0.577 (+35%) 0.853 (+30%)?
2.150 (+31%)EBM 0.699 (+85%) 0.754 (+76%) 0.910 (+39%) 2.650 (+62%)Combo 0.707 (+87%) 0.752 (+76%) 0.931 (+42%) 2.648 (+61%)(b) Performance across all clinical tasks Significance at the 1% level, depending on direction of change. Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.the exception of MRR on the development set.
However, for all metrics, on both thedevelopment set and test set, there is no significant difference between the EBM andcombination reranker (which combines both term-based and EBM-based evidence).
Inthe parameter tuning process, we could not find a weight where performance across allmeasures was higher; in the end, we settled on what we felt was a reasonable weightthat improved P10 and MRR on the development set.Under the strict condition, only documents marked ?contains answer?
were givencredit; these results are shown in Table 9 (for the development set) and Table 10(for the blind held-out test set).
The same trend is observed?in fact, larger relativegains were achieved under the strict scoring criteria for our EBM and combination87Computational Linguistics Volume 33, Number 1Table 8(Lenient, Test) Lenient results of reranking experiment on blind held-out test questions forthe baseline PubMed condition, term-based reranker, EBM-based reranker, and combinationreranker.Therapy Diagnosis Prognosis EtiologyPrecision at 10 (P10)PubMed 0.350 0.150 0.200 0.320Term 0.575 (+64%) 0.383 (+156%)?
0.333 (+67%)?
0.460 (+43%)?EBM 0.783 (+124%) 0.583 (+289%) 0.467 (+133%)?
0.660 (+106%)?Combo 0.792 (+126%) 0.633 (+322%) 0.433 (+117%)?
0.660 (+106%)?Mean Average Precision (MAP)PubMed 0.421 0.279 0.235 0.364Term 0.563 (+34%) 0.489 (+76%)?
0.415 (+77%)?
0.480 (+32%)?EBM 0.765 (+82%) 0.637 (+129%) 0.722 (+207%)?
0.701 (+93%)?Combo 0.770 (+83%) 0.653 (+134%) 0.690 (+194%)?
0.687 (+89%)?Mean Reciprocal Rank (MRR)PubMed 0.579 0.443 0.456 0.540Term 0.660 (+14%)?
0.765 (+73%)?
0.611 (+34%)?
0.650 (+20%)?EBM 0.917 (+58%) 0.889 (+101%)?
1.000 (+119%)?
1.000 (+85%)?Combo 0.958 (+66%) 0.917 (+107%)?
1.000 (+119%)?
1.000 (+85%)?Total Document Reciprocal Rank (TDRR)PubMed 1.669 0.926 0.895 1.381Term 2.204 (+32%) 1.880 (+103%)?
1.390 (+55%)?
1.736 (+26%)?EBM 2.979 (+79%) 2.341 (+153%) 2.101 (+138%)?
2.671 (+93%)?Combo 3.025 (+81%) 2.380 (+157%) 2.048 (+129%)?
2.593 (+88%)?
(a) Breakdown by clinical taskP10 MAP MRR TDRRPubMed 0.281 0.356 0.526 1.353Term 0.481 (+71%) 0.513 (+44%) 0.677 (+29%)?
1.945 (+44%)EBM 0.677 (+141%) 0.718 (+102%) 0.936 (+78%) 2.671 (+98%)Combo 0.688 (+145%) 0.718 (+102%) 0.962 (+83%) 2.680 (+98%)(b) Performance across all clinical tasksSignificance at the 1% level, depending on direction of change.Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.rerankers.
Results of Wilcoxon signed-rank tests on the term-based, EBM, and com-bination rerankers are also shown in Table 11 for the strict scoring condition.
In mostcases, combining term scoring with EBM scoring does not help.
In almost all cases,the EBM and combination reranker perform significantly better than the term-basedreranker.How does better ranking of citations impact end-to-end question answering perfor-mance?
We shall return to this issue in Sections 9 and 10, which describe and evaluatethe answer generation module, respectively.
In the next section, we describe moredetailed experiments with our EBM citation scoring algorithm.88Demner-Fushman and Lin Answering Clinical QuestionsTable 9(Strict, Development) Strict results of reranking experiment on development questions for thebaseline PubMed condition, term-based reranker, EBM-based reranker, and combinationreranker.Therapy Diagnosis Prognosis EtiologyPrecision at 10 (P10)PubMed 0.130 0.133 0.100 0.253Term 0.230 (+77%)?
0.217 (+63%)?
0.233 (+133%)?
0.293 (+16%)?EBM 0.350 (+170%) 0.350 (+163%)?
0.267 (+167%)?
0.293 (+16%)?Combo 0.350 (+170%) 0.367 (+175%)?
0.300 (+200%)?
0.313 (+24%)?Mean Average Precision (MAP)PubMed 0.088 0.108 0.058 0.164Term 0.205 (+134%)?
0.142 (+32%)?
0.090 (+54%)?
0.246 (+50%)?EBM 0.314 (+260%)?
0.259 (+140%)?
0.105 (+79%)?
0.265 (+62%)?Combo 0.301 (+244%)?
0.248 (+130%)?
0.129 (+122%)?
0.273 (+67%)?Mean Reciprocal Rank (MRR)PubMed 0.350 0.453 0.394 0.367Term 0.409 (+17%)?
0.581 (+28%)?
0.528 (+34%)?
0.700 (+91%)?EBM 0.675 (+93%) 0.756 (+67%)?
0.444 (+13%)?
0.800 (+118%)?Combo 0.569 (+63%) 0.676 (+49%)?
0.833 (+111%)?
0.700 (+91%)?Total Document Reciprocal Rank (TDRR)PubMed 0.610 0.711 0.568 0.721Term 0.872 (+43%)?
1.022 (+44%)?
0.804 (+42%)?
1.224 (+70%)?EBM 1.434 (+135%) 1.601 (+125%)?
0.824 (+45%)?
1.298 (+80%)?Combo 1.282 (+110%) 1.502 (+111%)?
1.173 (+106%)?
1.241 (+72%)?
(a) Breakdown by clinical taskP10 MAP MRR TDRRPubMed 0.153 0.105 0.385 0.653Term 0.240 (+57%) 0.183 (+75%)?
0.527 (+37%) 0.974 (+49%)EBM 0.328 (+115%) 0.264 (+152%) 0.693 (+80%) 1.371 (+110%)Combo 0.340 (+123%) 0.260 (+148%) 0.656 (+71%) 1.315 (+101%)(b) Performance across all clinical tasks Significance at the 1% level, depending on direction of change. Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.8.
Optimizing Citation ScoringA potential, and certainly valid, criticism of our EBM citation scoring algorithm is itsad hoc nature.
Weights for various features were assigned based on intuition, reflectingour understanding of the domain and our knowledge about the principles of evidence-based medicine.
Parameters were fine-tuned during the system implementation processby actively working with the development set; however, this was not done in anysystematic fashion.
Nevertheless, results on the blind held-out test set confirm thegenerality of our citation scoring algorithm.89Computational Linguistics Volume 33, Number 1Table 10(Strict, Test) Strict results of reranking experiment on blind held-out test questions for thebaseline PubMed condition, term-based reranker, EBM-based reranker, and combinationreranker.Therapy Diagnosis Prognosis EtiologyPrecision at 10 (P10)PubMed 0.108 0.017 0.000 0.080Term 0.192 (+77%)?
0.133 (+700%)?
0.033 ?
0.140 (+75%)?EBM 0.233 (+115%)?
0.167 (+900%)?
0.100 ?
0.200 (+150%)?Combo 0.258 (+139%) 0.200 (+1100%)?
0.100 ?
0.220 (+175%)?Mean Average Precision (MAP)PubMed 0.061 0.024 0.015 0.050Term 0.082 (+36%)?
0.118 (+386%)?
0.086 (+464%)?
0.086 (+74%)?EBM 0.109 (+80%)?
0.091 (+276%)?
0.234 (+1442%)?
0.159 (+220%)?Combo 0.120 (+99%)?
0.107 (+339%)?
0.224 (+1372%)?
0.165 (+232%)?Mean Reciprocal Rank (MRR)PubMed 0.282 0.073 0.031 0.207Term 0.368 (+31%)?
0.429 (+488%)?
0.146 (+377%)?
0.314 (+52%)?EBM 0.397 (+41%)?
0.431 (+490%)?
0.465 (+1422%)?
0.500 (+142%)?Combo 0.556 (+97%) 0.422 (+479%)?
0.438 (+1331%)?
0.467 (+126%)?Total Document Reciprocal Rank (TDRR)PubMed 0.495 0.137 0.038 0.331Term 0.700 (+41%)?
0.759 (+454%)?
0.171 (+355%)?
0.596 (+80%)?EBM 0.807 (+63%)?
0.654 (+377%)?
0.513 (+1262%)?
0.946 (+186%)?Combo 0.969 (+96%) 0.698 (+409%)?
0.479 (+1172%)?
0.975 (+195%)?
(a) Breakdown by clinical taskP10 MAP MRR TDRRPubMed 0.069 0.045 0.190 0.328Term 0.150 (+117%) 0.092 (+105%) 0.346 (+82%) 0.632 (+93%)EBM 0.196 (+183%) 0.129 (+187%) 0.433 (+127%) 0.765 (+133%)Combo 0.219 (+217%) 0.138 (+207%) 0.494 (+160%) 0.851 (+159%)(b) Performance across all clinical tasks Significance at the 1% level, depending on direction of change. Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.In the development of various language technology applications, it is common forthe first materialization of a new capability to be rather ad hoc in its implementation.This is a reflection of an initial attempt to understand both the problem and solutionspaces.
Subsequent systems, with a better understanding of the possible technical ap-proaches and their limitations, are then able to implement a more principled solution.Because our clinical question-answering system is the first of its type that we are awareof, in terms of both depth and scope, it is inevitable that our algorithms suffer fromsome of these limitations.
Similarly, our collection of clinical questions is the first testcollection of its type that we are aware of.
Typically, construction of formal models isonly made possible by the existence of test collections.
We hope that our work sheds newinsight on question answering in the clinical domain and paves the way for future work.90Demner-Fushman and Lin Answering Clinical QuestionsTable 11Performance differences between various rerankers.P10 MAP MRR TDRRDevelopment SetEBM vs.
Term +45.0%  +30.8%  +6.7% ?
+23.3% Combo vs.
Term +46.7%  +30.4%  +9.1% ?
+23.2% Combo vs. EBM +1.2% ?
?0.3% ?
+2.3% ?
?0.1% ?Test SetEBM vs.
Term +40.8  +40.1%  +38.3%  +37.3% Combo vs.
Term +43.2  +40.0%  +42.1%  +37.8% Combo vs. EBM +1.7 ?
?0.1% ?
+2.7% ?
+0.3% ?
(a) Lenient ScoringP10 MAP MRR TDRRDevelopment SetEBM vs.
Term +36.4%  +43.8%  +31.3% ?
+40.7% Combo vs.
Term +41.6%  +41.9%  +24.5% ?
+35.0% Combo vs. EBM +3.8% ?
?1.3% ?
?5.2% ?
?4.1% ?Test SetEBM vs.
Term +30.8 ?
+40.4%  +24.9% ?
+20.9% ?Combo vs.
Term +46.2  +50.0%  +42.8%  +34.6% Combo vs. EBM +11.8  +6.8% ?
+14.3% ?
+11.3% ?
(b) Strict Scoring Significance at the 1% level, depending on direction of change. Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.In addition, there are some theoretical obstacles for developing a more formal (say,generative) model.
Most methods for training such models require independently andidentically distributed samples from the underlying distribution?which is certainly notthe case with our test collection.
Moreover, the event space of queries and documentsis extremely large or even infinite, depending on how it is defined.
Our training data,assumed to be samples from this underlying distribution, is extremely small comparedto the event space, and hence it is unlikely that popular methods (e.g., maximumlikelihood estimates) would yield an accurate characterization of the true distribution.Furthermore, many techniques for automatically setting parameters make useof maximum likelihood techniques?which do not maximize the correct objectivefunction.
Maximizing the likelihood of generating the training data does not meanthat the evaluation metric under consideration (e.g., mean average precision) is alsomaximized?this phenomenon is known as metric divergence.Nevertheless, it is important to better understand the effects of parameter settingsin our system.
This section describes a few experiments aimed at this goal.The EBM score of a MEDLINE citation is the sum of three separate components,each representing a facet of evidence-based medicine.
This structure naturally suggestsa modification to Equation (3) that weights each score component differently:SEBM = ?1SPICO + ?2SSoE + (1 ?
?1 ?
?2)Stask (8)91Computational Linguistics Volume 33, Number 1Figure 3The MAP performance surface for ?1 and ?2.Table 12Results of optimizing ?1 and ?2 on therapy questions.P10 MAP MRR TDRRDevelopment TestBaseline 0.730 0.819 0.933 2.869Optimized 0.760 (+4.1%)?
0.822 (+0.4%)?
0.933 (+0.0%)?
2.878 (+0.3%)?Test TestBaseline 0.783 0.765 0.917 2.979Optimized 0.783 (+0.0%)?
0.762 (?0.4%)?
0.917 (+0.0%)?
2.972 (?0.2%)?
?Difference not statistically significant.The parameters ?1 and ?2 can be derived from our development set.
For therapyquestions, we exhaustively searched through the entire parameter space, in incrementsof hundredths, and determined the optimal settings to be ?1 = 0.38, ?2 = 0.34 (whichwas found to slightly improve all metrics).
The performance surface for mean averageprecision is shown in Figure 3, which plots results for all possible parameter valueson the development set.
Numeric results are shown in Table 12.
It can be seen thatoptimizing the parameters in this fashion does not lead to a statistically significantincrease in any of the metrics.
Furthermore, these gains do not carry over to the blindheld-out test set.
We also tried optimizing the ?
?s on all questions in the developmentset.
These results are shown in Table 13.
Once again, differences are not statisticallysignificant.Why does parameter optimization not help?
We believe that there are two factorsat play here: On the one hand, parameter settings should be specific to the clinicaltask.
This explains why optimizing across all question types at the same time didnot improve performance.
On the other hand, there are too few questions of anyparticular type to represent an accurate sampling of all possible questions.
This is whyparameter tuning on therapy questions did not significantly alter performance.
Theseexperiments point to the need for larger test collections, which is an area for futurework.92Demner-Fushman and Lin Answering Clinical QuestionsTable 13Results of optimizing ?1 and ?2 on all questions.P10 MAP MRR TDRRDevelopment TestBaseline 0.699 0.754 0.910 2.650Optimized 0.707 (+1.2%)?
0.755 (+0.1%)?
0.918 (+0.9%)?
2.660 (+0.4%)?Test TestBaseline 0.677 0.718 0.936 2.671Optimized 0.669 (?1.1%)?
0.716 (?0.3%)?
0.936 (+0.0%)?
2.662 (?0.3%)?
?Difference not statistically significant.Table 14Results of assigning uniform weights to the EBM score component based on the clinical task.P10 MAP MRR TDRRDevelopment TestBaseline 0.699 0.754 0.910 2.650?
(t) = ?1 0.690 (?1.2%)?
0.738 (?2.1%)?
0.927 (+1.9%)?
2.646 (?0.2%)?Test TestBaseline 0.677 0.718 0.936 2.671?
(t) = ?1 0.627 (?7.4%) 0.681 (?5.2%)?
0.913 (?2.4%)?
2.519 (?5.7%)? Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.Another component of our EBM citation scoring algorithm that contains manyad hoc weights is Stask, defined in Equation (7) and repeated here:Stask =?t?MeSH?
(t) (9)The function ?
(t) maps a particular MeSH term to a weight that quantifies thedegree to which it is a positive or negative indicator for the particular clinical task.Because these weights were heuristically assigned, it would be worthwhile to examinethe impact they have on performance.
As a variant, we modified ?
(t) so that all MeSHterms were mapped to ?1; in other words, we did not encode granular levels of?goodness.?
These results are shown in Table 8.
Although performance dropped acrossall metrics, none of the differences were statistically significant except for P10 on thetest set.The series of experiments described herein help us better understand the effects ofparameter settings on abstract reranking performance.
As can be seen from the results,our algorithm is relatively invariant with respect to the choice of parameters, con-firming that our primary contribution is the EBM-based approach to clinical question93Computational Linguistics Volume 33, Number 1answering, and that our performance gains cannot be simply attributed to a fortunatechoice of parameters.9.
From Scoring Citations to Answering QuestionsThe aim of question-answering technology is to move from the ?hit list?
paradigm ofinformation retrieval, where users receive a list of potentially relevant documents thatthey must then browse through, to a mode of interaction where users directly receiveresponses that satisfy their information needs.
In our current architecture, fetching ahigher-quality ranked list is a step towards generating responsive answers.The most important characteristic of answers, as recommended by Ely et al (2005)in their study of real-world physicians, is that they focus on bottom-line clinicaladvice?information that physicians can directly act on.
Ideally, answers should in-tegrate information from multiple clinical studies, pointing out both similarities anddifferences.
The system should collate concurrences, that is, if multiple abstracts ar-rive at the same conclusion?it need not be repeated unless the physician wishes to?drill down?
; the system should reconcile contradictions, for example, if two abstractsdisagree on a particular treatment because they studied different patient populations.We have noted that many of these desiderata make complex question answering quitesimilar to multi-document summarization (Lin and Demner-Fushman 2005b), but thesefeatures are also beyond the capabilities of current summarization systems.It is clear that the type of answers desired by physicians require a level of semanticanalysis that is beyond the current state of the art, even with the aid of existing medicalontologies.
For example, even the seemingly straightforward task of identifying simi-larities and differences in outcome statements is rendered exceedingly complex by thetremendous amount of background medical knowledge that must be brought to bearin interpreting clinical results and subtle differences in study design, objectives, andresults; the closest analogous task in computational linguistics?redundancy detectionfor multi-document summarization?seems easy by comparison.
Furthermore, it isunclear if textual strings make ?good answers.?
Perhaps a graphical rendering of thesemantic predicates present in relevant abstracts might more effectively convey thedesired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004).
Per-haps some variation of multi-level bulleted lists, appropriately integrated with interfaceelements for expanding and hiding items, might provide physicians a better overviewof the information landscape; see, for example, Demner-Fushman and Lin (2006).Recognizing this complex set of issues, we decided to take a simple extractiveapproach to answer generation.
For each abstract in our reranked list of citations,our system produces an answer by combining the title of the abstract and the topthree outcome sentences (in the order they appeared in the abstract).
We employed theoutcome scores generated by the regression model.
No attempt was made to synthesizeinformation from multiple citations.
A formal evaluation of this simple approach toanswer generation is presented in the next section.10.
Evaluation of Clinical AnswersEvaluation of answers within a clinical setting involves a complex decision that mustnot only take into account topical relevance (i.e., ?Does the answer address the infor-mation need??
), but also situational relevance (e.g., Saracevic 1975, Barry and Schamber94Demner-Fushman and Lin Answering Clinical Questions1998).
The latter factor includes many issues such as the strength of evidence, recencyof results, and reputation of the journal.
Clinicians need to carefully consider all theseelements before acting on any information for the purposes of patient care.
Within theframework of evidence-based medicine, the physician is the final arbiter of how clinicalanswers are integrated into the broader activities of medical care, but this complicatesany attempt to evaluate answers generated by our system.In assessing answers produced by our system, we decided to focus only on theevaluation of topical relevance?assessors were only presented with answer strings,generated in the manner described in the previous section.
Metadata that would con-tribute to judgments about situational relevance, such as the strength of evidence,names of the authors and the journal, and so on, were purposefully suppressed.
Ourevaluation compared the top five answers generated from the original PubMed hit listand the top five answers generated from our reranked list of citations.
Answers wereprepared for all 24 questions in our development set.We recruited two medical doctors (one family practitioner, one surgeon) from theNational Library of Medicine to evaluate the textual answers.
Our instructions clearlystated that only topical relevance was to be assessed.
We asked the physicians to providethree-valued judgments: A plus (+) indicates that the response directly answers the question.Naturally, the physicians would need to follow up and examine the sourcecitation in more detail. A check (?)
indicates that the response provides clinically relevantinformation that may factor into decisions about patient treatment, andthat the source citation was worth examining in more detail. A minus (?)
indicates that the response does not provide usefulinformation in answering the clinical question, and that the source citationwas not worth examining.We purposely avoided short linguistic labels for the judgments so as to sidestepthe question of ?What exactly is an answer to a clinical question??
Informally, an-swers marked with a plus can be considered ?actionable?
clinical advice.
Answersmarked with a check provide relevant information that may influence the physician?sactions.We adopted a double-blind study design for the actual assessment process: Answersfrom both systems were presented in a randomized order without any indication ofwhich system the response came from (duplicates were suppressed).
A paper printout,containing each question followed by the blinded answers, was presented to eachassessor.
We then coded the relevance judgments in a plain text file manually.
Duringthis entire time, the key that maps answers to systems was kept in a separate file andhidden from everyone, including the authors.
All scores were computed automaticallywithout human intervention.Answer precision was calculated for two separate conditions: Under the strictcondition (Table 15), only ?plus?
judgments were considered good; under the lenientcondition (Table 16), both ?plus?
and ?check?
judgments were considered good.
As canbe seen, our EBM algorithm significantly outperforms the baseline under both the strictand lenient conditions, according to both assessors.
On average, the length of answersgenerated from the original PubMed list of citations was 90 words; answers generatedfrom the reranked list of citations averaged 87 words.
Answers from both sources95Computational Linguistics Volume 33, Number 1Table 15Strict answer precision (considering only ?plus?
judgments).Therapy Diagnosis Prognosis Etiology AllAssessor 1Baseline .160 .233 .333 .480 .267EBM .260 (+63%) .367 (+58%) .333 (+0%) .600 (+25%) .367 (+37%)Assessor 2Baseline .040 .233 .200 .400 .183EBM .200 (+400%) .300 (+29%) .266 (+33%) .560 (+40%) .308 (+68%)Table 16Lenient answer precision (considering both ?plus?
and ?check?
judgments).Therapy Diagnosis Prognosis Etiology AllAssessor 1Baseline .400 .300 .533 .520 .417EBM .640 (+60%) .567 (+89%) .400 (?25%) .640 (+23%) .592 (+42%)Assessor 2Baseline .240 .267 .333 .440 .300EBM .520 (+117%) .600 (+125%) .400 (+20%) .560 (+27%) .533 (+78%)were significantly shorter than the abstracts from which they were extracted (250 wordaverage for original PubMed results and 270 word average for reranked results).To give a feel for the types of responses that are generated by our system, considerthe following question:What is the best treatment for analgesic rebound headaches?The following is an example of a response that received a ?plus?
judgment:Medication overuse headache from antimigraine therapy: clinical features,pathogenesis and management: Because of easy availability and low expense, thegreatest problem appears to be associated with barbiturate-containing combinationanalgesics and over-the-counter caffeine-containing combination analgesics.
The bestmanagement advice is to raise awareness and strive for prevention.
Reduction inheadache risk factors should include behavioural modification approaches to headachecontrol earlier in the natural history of migraine.This answer was accepted by both physicians because it clearly states that specificanalgesics are most likely to cause the problem, and gives a direct guideline for preven-tive treatment.In contrast, the following response to the same question received a ?check?
:Does chronic daily headache arise de novo in association with regular use ofanalgesics?
Regular use of analgesics preceded the onset of daily headache in 5 patientsby a mean of 5.4 years (range, 2 to 10 years).
In 1 patient, the onset of daily headachepreceded regular use of analgesics by almost 30 years.
These findings suggest thatindividuals with primary headache, specifically migraine, are predisposed todeveloping chronic daily headache in association with regular use of analgesics.96Demner-Fushman and Lin Answering Clinical QuestionsAlthough this answer provides information about the risks and causes of theheadaches, neither prevention nor treatment is explicitly mentioned.
For these reasonsthis response was marked as potentially leading to an answer, but not as containing one.To summarize, we have presented a simple answer generation algorithm thatis capable of supplying clinically relevant responses to physicians.
Compared toPubMed, which does not take into account the principles of evidence-based medicine,our question-answering system represents a leap forward in information accesscapabilities.811.
Related Work and DiscussionClinical question answering is an emerging area of research that has only recently begunto receive serious attention.
As a result, there exist relatively few points of comparison toour own work, as the research space is sparsely populated.
In this section, however, wewill attempt to draw connections to other clinical information systems (although notnecessarily for question answering) and related domain-specific question-answeringsystems.
For an overview of systems designed to answer open-domain factoid ques-tions, the TREC QA track overview papers are a good place to start (Voorhees andTice 1999).
In addition, there has been much work on the application of linguistic andsemantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) fora brief overview.The idea that clinical information systems should be sensitive to the practice ofevidence-based medicine is not new.
Based on analyses of 4,000 MEDLINE citations,Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basicclinical tasks of therapy, diagnosis, prognosis, and etiology.
The goal was to auto-matically classify citations for task-specific retrieval, similar in spirit to the HedgesProject (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001).
Cimino andMendonc?a reported good performance for etiology, diagnosis, and in particular therapy,but not prognosis.
Although originally developed as a tool to assist in query formu-lation, Booth (2000) pointed out that PICO frames can be employed to structure IRresults for improving precision.
PICO-based querying in information retrieval is merelyan instance of faceted querying, which has been widely used by librarians since theintroduction of automated retrieval systems (e.g., Meadow et al 1989).
The work ofHearst (1996) demonstrates that faceted queries can be converted into simple filteringconstraints to boost precision.The feasibility of automatically identifying outcome statements in secondarysources has been demonstrated by Niu and Hirst (2004).
Their study also illustratesthe importance of semantic classes and relations.
However, extraction of outcome state-ments from secondary sources (meta-analyses, in this case) differs from extraction ofoutcomes from MEDLINE citations because secondary sources represent knowledgethat has already been distilled by humans (which may limit its scope).
Because sec-ondary sources are often more consistently organized, it is possible to depend oncertain surface cues for reliable extraction (which is not possible for MEDLINE ab-stracts in general).
Our study tackles outcome identification in primary medical sourcesand demonstrates that respectable performance is possible with a feature-combinationapproach.8 Although note that answer generation from the PubMed results also requires the use of the outcomeextractor.97Computational Linguistics Volume 33, Number 1The literature also contains work on sentence-level classification of MEDLINEabstracts for non-clinical purposes.
For example, McKnight and Srinivasan (2003) de-scribe a machine learning approach to automatically label sentences as belonging tointroduction, methods, results, or conclusion using structured abstracts as training data(see also Lin et al 2006).
Tbahriti et al (2006) have demonstrated that differentialweighting of automatically labeled sections can lead to improved retrieval performance.Note, however, that such labels are orthogonal to PICO frame elements, and henceare not directly relevant to knowledge extraction for clinical question answering.
In asimilar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculativestatements in MEDLINE abstracts, but once again, this work is not directly applicableto clinical question answering.In addition to question answering, multi-document summarization provides a com-plementary approach to addressing clinical information needs.
The PERSIVAL project,the most comprehensive study of such techniques applied on medical texts to date,leverages patient records to generate personalized summaries in response to physicians?queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005).
Althoughthe system incorporates both a user and a task model, it does not explicitly capturethe principles of evidence-based medicine.
Patient information is no doubt importantto answering clinical questions, and our work could certainly benefit from experiencesgained in the PERSIVAL project.The application of domain models and deep semantic knowledge to questionanswering has been explored by a variety of researchers (e.g., Jacquemart andZweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshopson question answering in restricted domains at ACL 2004 and AAAI 2005.
Our workcontributes to this ongoing discourse by demonstrating a specific application in thedomain of clinical medicine.Finally, the evaluation of answers to complex questions remains an open researchproblem.
Although it is clear that measures designed for open-domain factoid questionsare not appropriate, the community has not agreed on a methodology that will allowmeaningful comparisons of results from different systems.
In Sections 9 and 10, wehave discussed many of these issues.
Recently, there is a growing consensus that anevaluation methodology based on the notion of ?information nuggets?
may providean appropriate framework for assessing the quality of answers to complex questions.Nugget F-score has been employed as a metric in the TREC question-answering tracksince 2003, to evaluate so-called definition and ?other?
questions (Voorhees 2003).
Anumber of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcom-ings of the original nugget scoring model, although a number of these issues have beenrecently addressed (Lin and Demner-Fushman 2005a, 2006b).
However, adaptation ofthe nugget evaluation methodology to a domain as specific as clinical medicine is anendeavor that has yet to be undertaken.12.
Future WorkThe design and implementation of our current system leaves many open avenues forfuture exploration, one of which concerns our assumptions about the query interface.Previously, a user study (Lin et al 2003) has shown that people are reluctant to typefull natural language questions, even after being told that they were using a question-answering system and that typing complete questions would result in better perform-ance.
We have argued that a query interface based on structured PICO frames willyield better-formulated queries, although it is unclear whether physicians would invest98Demner-Fushman and Lin Answering Clinical Questionsthe upfront effort necessary to accomplish this.
Issuing extremely short queries appearsto be an ingrained habit of information seekers today, and the dominance of World WideWeb searches reinforce this behavior.
Given these trends, physicians may actually preferthe rapid back-and-forth interaction style that comes with short queries.
We believethat if systems can produce noticeably better results with richer queries, users willmake more of an effort to formulate them.
This, however, presents a chicken-and-eggproblem: One possible solution is to develop models that can automatically fill queryframes given a couple of keywords?this would serve to kick-start the query generationprocess.The astute reader will have noticed that the initial retrieval of abstracts in ourstudy was performed with high-quality manually crafted queries (that were part ofthe test collection).
Although this was intended to demonstrate the performance of ourEBM citation scoring algorithm with respect to a strong baseline, it also means that wehave omitted a component in the automatic question-answering process.
Translating aclinical question into a good PubMed query is not a trivial task?in our experiments,it required an experienced searcher approximately 40 minutes on average per question.However, it is important to note that query formulation in the clinical domain is nota problem limited to question-answering systems, but one that users of all retrievalsystems must contend with.Nevertheless, there are three potential solutions to this problem: First, althoughthere is an infinite variety of clinical questions, the number of query types is boundedand far smaller in number; see Huang, Lin, and Demner-Fushman (2006) for an analysis.In a query interface based on PICO frames, it is possible to identify a number of proto-typical query frames.
From these prototypes, one can generate query templates that ab-stract over the actual slot fillers?this is the idea behind Clinical Queries.
Although thismethod will probably not retrieve citations as high in quality as custom-crafted queries,there is reason to believe that as long as a reasonable set of citations is retrieved, our sys-tem will be able to extract relevant answers (given the high accuracy of our knowledgeextractors and citation scoring algorithm).
The second approach to tackling this problemis to bypass PubMed altogether and index MEDLINE with another search engine.Due to the rapidly changing nature of the entire MEDLINE database, experiments forpractical purposes would most likely be conducted on a static subset of the collection,for example, the ten-year portion created for the TREC 2004 genomics track (Hersh,Bhupatiraju, and Corley 2004).
Recent results from TREC have demonstrated that highperformance ad hoc retrieval is possible in the genomics domain (Hersh et al 2005),and it is not a stretch to imagine adopting these technologies for clinical tasks.
Usinga separate search engine would provide other benefits as well: Greater control overthe document retrieval process would allow one to examine the effects of differentindexing schemes, different query operators, and techniques such as query expansion;see, for example, Aronson, Rindflesch, and Browne (1994).
Finally, yet another way tosolve the document retrieval problem is to eliminate that stage completely.
Recall thatour two-stage architecture was a practical expediency, because we did not have accessto the computing resources necessary to pre-extract PICO elements from the entireMEDLINE database and directly index the results.
Given access to more resources,a system could index identified PICO elements and directly match queries against aknowledge store.Finally, answer generation remains an area that awaits further exploration, althoughwe would have to first define what a good answer should be.
We have empiricallyverified that an extractive approach based on outcome sentences is actually quitesatisfactory, but our algorithm does not currently integrate evidence from multiple99Computational Linguistics Volume 33, Number 1abstracts; although see Demner-Fushman and Lin (2006).
Furthermore, the current an-swer generator does not handle complex issues such as contradictory and inconsistentstatements.
To address these very difficult challenges, finer-grained semantic analysisof medical texts is required.13.
ConclusionOur experiments in clinical question answering provide some answers to the broaderresearch question regarding the role of knowledge-based and statistical techniques inadvanced question answering.
This work demonstrates that the two approaches arecomplementary and can be seamlessly integrated into algorithms that draw from thebest of both worlds.
Explicitly coded semantic knowledge, in the form of UMLS, andsoftware for leveraging this resource?for example, MetaMap?combine to simplifymany knowledge extraction tasks that would be far more difficult otherwise.
The re-spectable performance of our population, problem, and intervention extractors, all ofwhich use relatively simple rules, provides evidence that complex clinical problemscan be tackled by appropriate use of ontological knowledge.
Explicitly coded semanticknowledge is less helpful for outcome identification due to the large variety of possi-ble ?outcomes;?
nevertheless, knowledge-rich features can be combined with simple,statistically derived features to build a good outcome classifier.
Overall, this workdemonstrates that the application of a semantic domain model yields clinical questionanswering capabilities that significantly outperform presently available technology,especially when coupled with traditional statistical methods (classification, evidencecombination, etc.
).We have taken an important step in building a complete question-answering systemthat assists physicians in the patient care process.
Our work demonstrates that theprinciples of evidence-based medicine can be computationally captured and imple-mented in a system, and although we are still far from operational deployment, thesepositive results are certainly encouraging.
Information systems in support of the clinicaldecision-making process have the potential to improve the quality of health care, whichis a worthy goal indeed.AcknowledgmentsWe would like to thank Dr. CharlesSneiderman and Dr. Kin Wah Fung forthe evaluation of the answers.
For thiswork, D. D-F. was supported by anappointment to the National Library ofMedicine Research Participation Programadministered by the Oak Ridge Institutefor Science and Education through aninter-agency agreement between the U.S.Department of Energy and the NationalLibrary of Medicine.
For this work, J. L.was supported in part by a grant from theNational Library of Medicine, where hewas a visiting researcher during thesummer of 2005.
We would like to thankthe anonymous reviewers for their valuablecomments.
J. L. would like to thank Kiriand Esther for their kind support.ReferencesAd Hoc Working Group for CriticalAppraisal of the Medical Literature.
1987.A proposal for more informative abstractsof clinical articles.
Annals of InternalMedicine, 106:595?604.Aronson, Alan R. 2001.
Effective mappingof biomedical text to the UMLSMetathesaurus: The MetaMap program.In Proceeding of the 2001 Annual Symposiumof the American Medical InformaticsAssociation (AMIA 2001), pages 17?21,Portland, OR.Aronson, Alan R., James G. Mork,Clifford W. Gay, Susanne M. Humphrey,and Willie J. Rogers.
2004.
The NLMIndexing Initiative?s Medical TextIndexer.
In Proceedings of the 11thWorld Congress on Medical Informatics100Demner-Fushman and Lin Answering Clinical Questions(MEDINFO 2004), pages 268?272,San Francisco, CA.Aronson, Alan R., Thomas C. Rindflesch,and Allen C. Browne.
1994.
Exploiting alarge thesaurus for information retrieval.In Proceedings of RIAO 1994: IntelligentMultimedia Information Retrieval Systemsand Management, pages 197?216, New York.Baeza-Yates, Ricardo and BerthierRibeiro-Neto.
1999.
Modern InformationRetrieval.
ACM Press, New York.Barry, Carol and Linda Schamber.
1998.Users?
criteria for relevance evaluation:A cross-situational comparison.Information Processing and Management,34(2/3):219?236.Booth, Andrew.
2000.
Formulating thequestion.
In Andrew Booth and GrahamWalton, editors, Managing Knowledge inHealth Services.
Library AssociationPublishing, London, England.Chambliss, M. Lee and Jennifer Conley.
1996.Answering clinical questions.
The Journal ofFamily Practice, 43:140?144.Cogdill, Keith W. and Margaret E. Moore.1997.
First-year medical students?information needs and resource selection:Responses to a clinical scenario.
Bulletin ofthe Medical Library Association, 85(1):51?54.Covell, David G., Gwen C. Uman, andPhil R. Manning.
1985.
Information needsin office practice: Are they being met?Annals of Internal Medicine, 103(4):596?599.De Groote, Sandra L. and Josephine L.Dorsch.
2003.
Measuring use patternsof online journals and databases.Journal of the Medical Library Association,91(2):231?240.Demner-Fushman, Dina, Barbara Few,Susan E. Hauser, and George Thoma.
2006.Automatically identifying health outcomeinformation in MEDLINE records.
Journalof the American Medical InformaticsAssociation, 13(1):52?60.Demner-Fushman, Dina and Jimmy Lin.2005.
Knowledge extraction for clinicalquestion answering: Preliminary results.In Proceedings of the AAAI-05 Workshop onQuestion Answering in Restricted Domains,pages 1?10, Pittsburgh, PA.Demner-Fushman, Dina and Jimmy Lin.2006.
Answer extraction, semanticclustering, and extractive summarizationfor clinical question answering.
InProceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association forComputational Linguistics (COLING/ACL2006), pages 841?848, Sydney, Australia.Ebell, Mark H., Jay Siwek, Barry D. Weiss,Steven H. Woolf, Jeffrey Susman, BernardEwigman, and Marjorie Bowman.
2004.Strength of Recommendation Taxonomy(SORT): A patient-centered approach tograding evidence in the medical literature.The Journal of the American Board of FamilyPractice, 17(1):59?67.Elhadad, Noemie, Min-Yen Kan, JudithKlavans, and Kathleen McKeown.
2005.Customization in a unified framework forsummarizing medical literature.
Journal ofArtificial Intelligence in Medicine,33(2):179?198.Ely, John W., Jerome A. Osheroff, Mark H.Ebell, George R. Bergus, Barcey T. Levy,M.
Lee Chambliss, and Eric R. Evans.
1999.Analysis of questions asked by familydoctors regarding patient care.
BMJ,319:358?361.Ely, John W., Jerome A. Osheroff, M. LeeChambliss, Mark H. Ebell, and Marcy E.Rosenbaum.
2005.
Answering physicians?clinical questions: Obstacles and potentialsolutions.
Journal of the American MedicalInformatics Association, 12(2):217?224.Fiszman, Marcelo, Thomas C. Rindflesch,and Halil Kilicoglu.
2004.
Abstractionsummarization for managing thebiomedical research literature.
InProceedings of the HLT/NAACL 2004Workshop on Computational LexicalSemantics, pages 76?83, Boston, MA.Gorman, Paul N., Joan S. Ash, andLeslie W. Wykoff.
1994.
Can primary carephysicians?
questions be answered usingthe medical journal literature?
Bulletin ofthe Medical Library Association,82(2):140?146.Haynes, R. Brian, Nancy Wilczynski, K. AnnMcKibbon, Cynthia J. Walker, and John C.Sinclair.
1994.
Developing optimal searchstrategies for detecting clinically soundstudies in MEDLINE.
Journal of theAmerican Medical Informatics Association,1(6):447?458.Hearst, Marti A.
1996.
Improving full-textprecision on short queries using simpleconstraints.
In Proceedings of the FifthAnnual Symposium on Document Analysisand Information Retrieval (SDAIR 1996),pages 217?232, Las Vegas, NV.Hersh, William, Ravi Teja Bhupatiraju,and Sarah Corley.
2004.
Enhancingaccess to the bibliome: The TRECgenomics track.
In Proceedings of the11th World Congress on Medical Informatics(MEDINFO 2004), pages 773?777,San Francisco, CA.101Computational Linguistics Volume 33, Number 1Hersh, William, Aaron Cohen, Jianji Yang,Ravi Teja Bhupatiraju1, Phoebe Roberts,and Marti Hearst.
2005.
TREC 2005genomics track overview.
In Proceedingsof the Fourteenth Text REtrieval Conference(TREC 2005), Gaithersburg, MD.Hildebrandt, Wesley, Boris Katz, and JimmyLin.
2004.
Answering definition questionswith multiple knowledge sources.
InProceedings of the 2004 Human LanguageTechnology Conference and the NorthAmerican Chapter of the Associationfor Computational Linguistics AnnualMeeting (HLT/NAACL 2004), pages 49?56,Boston, MA.Hirschman, Lynette and Robert Gaizauskas.2001.
Natural language questionanswering: The view from here.
NaturalLanguage Engineering, 7(4):275?300.Huang, Xiaoli, Jimmy Lin, and DinaDemner-Fushman.
2006.
Evaluation ofPICO as a knowledge representation forclinical questions.
In Proceeding of the 2006Annual Symposium of the American MedicalInformatics Association (AMIA 2006),pages 359?363, Washington, D.C.Ingwersen, Peter.
1999.
Cognitiveinformation retrieval.
Annual Review ofInformation Science and Technology, 34:3?52.Jacquemart, Pierre and Pierre Zweigenbaum.2003.
Towards a medicalquestion-answering system: A feasibilitystudy.
In Robert Baud, Marius Fieschi,Pierre Le Beux, and Patrick Ruch, editors,The New Navigators: From Professionals toPatients, volume 95 of Actes MedicalInformatics Europe, Studies in HealthTechnology and Informatics.
IOS Press,Amsterdam, pages 463?468.Kauffman, Ralph E., L. A. Sawyer, andM.
L. Scheinbaum.
1992.
Antipyreticefficacy of ibuprofen vs acetaminophen.American Journal of Diseases of Children,146(5):622?625.Light, Marc, Xin Ying Qiu, and PadminiSrinivasan.
2004.
The language ofbioscience: Facts, speculations, andstatements in between.
In Proceedings of theBioLink 2004 Workshop at HLT/NAACL2004, pages 17?24, Boston, MA.Lin, Jimmy and Dina Demner-Fushman.2005a.
Automatically evaluating answersto definition questions.
In Proceedings of the2005 Human Language Technology Conferenceand Conference on Empirical Methods inNatural Language Processing (HLT/EMNLP2005), pages 931?938, Vancouver, Canada.Lin, Jimmy and Dina Demner-Fushman.2005b.
Evaluating summaries andanswers: Two sides of the same coin?
InProceedings of the ACL 2005 Workshop onIntrinsic and Extrinsic Evaluation Measuresfor MT and/or Summarization, pages 41?48,Ann Arbor, MI.Lin, Jimmy and Dina Demner-Fushman.2006a.
The role of knowledge inconceptual retrieval: A study in thedomain of clinical medicine.
In Proceedingsof the 29th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR 2006),pages 99?106, Seattle, WA.Lin, Jimmy and Dina Demner-Fushman.2006b.
Will pyramids built of nuggetstopple over?
In Proceedings of the 2006Human Language Technology Conferenceand North American Chapter of theAssociation for Computational LinguisticsAnnual Meeting (HLT/NAACL 2006),pages 383?390, New York.Lin, Jimmy, Damianos Karakos, DinaDemner-Fushman, and SanjeevKhudanpur.
2006.
Generative contentmodels for structural analysis of medicalabstracts.
In Proceedings of the HLT/NAACL 2006 Workshop on BiomedicalNatural Language Processing (BioNLP?06),pages 65?72, New York.Lin, Jimmy, Dennis Quan, Vineet Sinha,Karun Bakshi, David Huynh, Boris Katz,and David R. Karger.
2003.
What makes agood answer?
The role of context inquestion answering.
In Proceedings of theNinth IFIP TC13 International Conference onHuman-Computer Interaction (INTERACT2003), pages 25?32, Zu?rich, Switzerland.Lindberg, Donald A., Betsy L. Humphreys,and Alexa T. McCray.
1993.
The UnifiedMedical Language System.
Methods ofInformation in Medicine, 32(4):281?291.McCray, Alexa T., Anita Burgun, and OlivierBodenreider.
2001.
Aggregating UMLSsemantic types for reducing conceptualcomplexity.
In Proceedings of 10th WorldCongress on Medical Informatics (MEDINFO2001), pages 216?220, London, England.McKeown, Kathleen, Noemie Elhadad,and Vasileios Hatzivassiloglou.
2003.Leveraging a common representation forpersonalized search and summarizationin a medical digital library.
In Proceedingsof the 3rd ACM/IEEE Joint Conference onDigital Libraries (JCDL 2003), pages159?170, Houston, TX.McKnight, Larry and Padmini Srinivasan.2003.
Categorization of sentence typesin medical abstracts.
In Proceeding of the2003 Annual Symposium of the American102Demner-Fushman and Lin Answering Clinical QuestionsMedical Informatics Association (AMIA2003), pages 440?444, Washington, D.C.Meadow, Charles T., Barbara A. Cerny,Christine L. Borgman, and Donald O.Case.
1989.
Online access to knowledge:System design.
Journal of the AmericanSociety for Information Science, 40(2):86?98.Mendonc?a, Eneida A. and James J. Cimino.2001.
Building a knowledge base tosupport a digital library.
In Proceedingsof 10th World Congress on MedicalInformatics (MEDINFO 2001),pages 222?225, London, England.Mladenic, Dunja and Marko Grobelnik.
1999.Feature selection for unbalanced classdistribution and Na?
?ve Bayes.
InProceedings of the Sixteenth InternationalConference on Machine Learning (ICML1999), pages 258?267, Bled, Slovenia.Nenkova, Ani and Rebecca Passonneau.2004.
Evaluating content selection insummarization: The pyramid method.
InProceedings of the 2004 Human LanguageTechnology Conference and the NorthAmerican Chapter of the Association forComputational Linguistics Annual Meeting(HLT/NAACL 2004), pages 145?152,Boston, MA.Niu, Yun and Graeme Hirst.
2004.
Analysisof semantic classes in medical text forquestion answering.
In Proceedings of theACL 2004 Workshop on Question Answeringin Restricted Domains, pages 54?61,Barcelona, Spain.Pratt, Wanda and Meliha Yetisgen-Yildiz.2003.
A study of biomedical conceptidentification: MetaMap vs. people.In Proceeding of the 2003 AnnualSymposium of the American MedicalInformatics Association (AMIA 2003),pages 529?533, Washington, D.C.Richardson, W. Scott, Mark C. Wilson, JamesNishikawa, and Robert S. Hayward.
1995.The well-built clinical question: A key toevidence-based decisions.
American Collegeof Physicians Journal Club, 123(3):A12?A13.Rinaldi, Fabio, James Dowdall, GeroldSchneider, and Andreas Persidis.
2004.Answering questions in the genomicsdomain.
In Proceedings of the ACL 2004Workshop on Question Answering inRestricted Domains, pages 46?53,Barcelona, Spain.Rindflesch, Thomas C. and Marcelo Fiszman.2003.
The interaction of domainknowledge and linguistic structure innatural language processing: Interpretinghypernymic propositions in biomedicaltext.
Journal of Biomedical Informatics,36(6):462?477.Sackett, David L., Sharon E. Straus, W. ScottRichardson, William Rosenberg, andR.
Brian Haynes.
2000.
Evidence-BasedMedicine: How to Practice and Teach EBM,second edition.
Churchill Livingstone,Edinburgh, Scotland.Saracevic, Tefko.
1975.
Relevance: A reviewof and a framework for thinking on thenotion in information science.
Journal of theAmerican Society for Information Science,26(6):321?343.Sneiderman, Charles, DinaDemner-Fushman, Marcelo Fiszman, andThomas C. Rindflesch.
2005.
Semanticcharacteristics of MEDLINE citationsuseful for therapeutic decision-making.In Proceeding of the 2005 Annual Symposiumof the American Medical InformaticsAssociation (AMIA 2005), page 1117,Washington, D.C.Tbahriti, Imad, Christine Chichester,Fre?de?rique Lisacek, and Patrick Ruch.2006.
Using argumentation to retrievearticles with similar citations: An inquiryinto improving related articles search inthe MEDLINE digital library.
InternationalJournal of Medical Informatics, 75(6):488?495.Ting, Kai Ming and Ian H. Witten.
1999.Issues in stacked generalization.
Journal ofArtificial Intelligence Research, 10:271?289.Voorhees, Ellen M. 2003.
Overview of theTREC 2003 question answering track.
InProceedings of the Twelfth Text REtrievalConference (TREC 2003), pages 54?68,Gaithersburg, MD.Voorhees, Ellen M. and Dawn M. Tice.1999.
The TREC-8 question answeringtrack evaluation.
In Proceedings of theEighth Text REtrieval Conference (TREC-8),pages 83?106, Gaithersburg, MD.Wilczynski, Nancy, K. Ann McKibbon, andR.
Brian Haynes.
2001.
Enhancing retrievalof best evidence for health care frombibliographic databases: Calibrationof the hand search of the literature.
InProceedings of 10th World Congress onMedical Informatics (MEDINFO 2001),pages 390?393, London, England.Yang, Yiming and Jan O. Pedersen.
1997.A comparative study on feature selectionin text categorization.
In Proceedingsof the Fourteenth International Conferenceon Machine Learning (ICML 1997),pages 412?420, Nashville, TN.103
