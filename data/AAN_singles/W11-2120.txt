Proceedings of the 6th Workshop on Statistical Machine Translation, pages 166?170,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe UZH System Combination System for WMT 2011Rico SennrichInstitute of Computational LinguisticsUniversity of ZurichBinzmu?hlestr.
14CH-8050 Zu?richsennrich@cl.uzh.chAbstractThis paper describes the UZH system that wasused for the WMT 2011 system combinationshared task submission.
We participated inthe system combination task for the translationdirections DE?EN and EN?DE.
The systemuses Moses as a backbone, with the outputsof the 2?3 best individual systems being inte-grated through additional phrase tables.
Thesystem compares well to other system com-bination submissions, with no other submis-sion being significantly better.
A BLEU-basedcomparison to the individual systems, how-ever, indicates that it achieves no significantgains over the best individual system.1 IntroductionFor our submission to the WMT 2011 shared task,we built a system with the multi-engine MT ap-proach described in (Sennrich, 2011), which buildson the idea by (Chen et al, 2007).
A Moses SMTsystem (Koehn et al, 2007) is used as a backbone,trained on the WMT 2011 training data.
Translationhypotheses by other systems are integrated througha second phrase table.
In this second phrase ta-ble, the phrase translation probabilities and lexicalweights are computed based on the word and phrasefrequencies in both the translation hypotheses and aparallel training corpus.
On the evaluation data in(Sennrich, 2011), this system significantly outper-formed MEMT (Heafield and Lavie, 2010), whichwas among the best-performing system combinationtools at WMT 2010 (Callison-Burch et al, 2010).In this paper, we apply the same approach to a dif-ferent translation scenario, namely the WMT 2011shared task.
We fail to significantly outperform thebest individual system in terms of BLEU score.
Insection 2, we describe our system combination ap-proach.
In section 3, we present the results, and dis-cuss possible reasons why the system fails to showthe same performance gains as in the translation taskon which it was evaluated initially.2 System DescriptionWe participated in the system combination task DE?EN and EN?DE.
Since the combination is achievedby integrating translation hypotheses into an existingMoses system, which we will call the primary sys-tem, we first describe the methods and data used fortraining this primary system.
Then, we describe howthe translation hypotheses are selected out of the in-dividual system submissions and integrated into theMoses system.2.1 Primary SystemFor the training of the primary systems, we mostlyfollowed the baseline instructions for the transla-tion task1.
We use news-commentary and Europarlas parallel training data.
The language models area linear interpolation of the news-commentary, Eu-roparl and news corpora, optimized for minimalcross-entropy on the newstest2008 data sets in therespective target language.Additionally, we prune the primary phrase tableusing statistical significance tests, as described by(Johnson et al, 2007).
For the translation directionDE?EN, the German source text is reordered based1described at http://www.statmt.org/wmt11/baseline.html166on syntactic parsing with Pro3GresDE (Sennrich etal., 2009), and reordering rules similar to those de-scribed by (Collins et al, 2005).The Moses phrase table consists of five fea-tures: phrase translation probabilities in both trans-lation directions (p(t|s) and p(s|t)), lexical weights(lex(t|s) and lex(s|t)), and a constant phrasepenalty (Koehn et al, 2003).
The computation of thephrase translation probabilities and lexical weightsis based on the word, phrase and word/phrase pairfrequencies that are extracted from the parallel cor-pus.
We modified the Moses training scripts to col-lect and store these frequencies for later re-use.We did not submit the primary system outputs tothe Machine Translation shared task, since we didnot experiment with new techniques.
Instead, theprimary system forms the backbone of the systemcombination system.2.2 Integrating Secondary Phrase TablesTo combine the output of several systems, we train asecond phrase table on the translation hypotheses ofthese systems.
For this, we create a parallel corpusconsisting of n translation hypotheses and n copiesof the corresponding source text, both lowercasedand detokenized.2We compute the word alignment with MGIZA++(Gao and Vogel, 2008), based on the word alignmentmodel from the primary corpus that we have previ-ously saved to disk.After training a phrase table from the word-aligned corpus with Moses, the lexical weights andtranslation probabilities are rescored, using the suffi-cient statistics (i.e.
the word, phrase and word/phrasepair counts) of both the primary and the secondarycorpus.
This rescoring step has been shown tomarkedly improve performance in (Sennrich, 2011).We will discuss its effects in section 3.1.
The re-scored phrase table is integrated into the primaryMoses system as an alternative decoding path, andtuned for maximal BLEU score on newssyscomb-tune2011 with MERT.2For convenience and speed, we combined thetranslation hypotheses for newssyscombtune2011 andnewssyscombtest2011 into a single corpus.
In principle,we could train separate phrase tables for each data set, or evenfor arbitrarily low numbers of sentences, without significantloss in performance (see (Sennrich, 2011)).System BLEUPrimary 21.11Best individual 24.16Submission 24.44Vanilla scoring 24.42Table 1: DE?EN results.
Case-insensitive BLEU scores.2.3 Hypothesis SelectionFor the secondary phrase table, we chose to se-lect the n best individual systems according to theirBLEU score on the tuning set.
We determined theoptimal n empirically by trying different n, measur-ing each system?s BLEU score on the tuning set andselecting the highest-scoring one.
For the DE?ENtranslation task, n = 2 turned out to be optimal, forEN?DE, n = 3.Chen et al (2009) propose additional, tunable fea-tures in the phrase table to indicate the origin ofphrase translations.
For better comparability withthe results described in (Sennrich, 2011), we didnot add such features.
This means that there areno a priori weights that bias the phrase selectionfor or against certain systems, but that decodingis purely driven by the usual Moses features: twophrase tables ?
the primary one and the re-scored,secondary one ?
the language model, the primaryreordering model, and the corresponding parametersestablished through MERT.3 ResultsIn the manual evaluation, the system combinationsubmissions are only compared to each other, notto the individual systems.
According to the manualevaluation, no other system combination submissionoutperforms ours by a statistically significant mar-gin.
In a comparison to individual systems, however,BLEU scores indicate that our system fails to yield asignificant performance gain over the best individualsystem in this translation scenario.In tables 1 and 2, we present case-insensitiveBLEU scores (Papineni et al, 2002).
As statisti-cal significance test, we applied bootstrap resam-pling (Riezler and Maxwell, 2005).
Tables 1 and2 show the BLEU scores for the translation direc-tions DE?EN and EN?DE, respectively.
Systems in-cluded are the primary translation system described167System BLEUPrimary 14.99Best individual 17.44Submission 17.51Vanilla scoring 17.32Table 2: EN?DE results.
Case insensitive BLEU scores.in section 2.1, the best individual system (online-Bin both cases) and the submitted combination sys-tem.
In terms of BLEU score, we achieved no sta-tistically significant improvement over the best indi-vidual system.As contrastive systems, we trained systems with-out the rescoring step described in section 2.2; wefound no statistically significant difference from thesubmission system.
In this translation task, thestatistics from the parallel corpus seem to be inef-fective at improving decoding, contrary to our find-ings in (Sennrich, 2011), where rescoring the phrasetable improved BLEU scores by 0.7 points.
We willaddress possible reasons for this discrepancy in thefollowing section.3.1 InterpretationThe main characteristic that sets our approach apartfrom other system combination software such asMANY (Barrault, 2010) and MEMT (Heafield andLavie, 2010) is its reliance on word and phrase fre-quencies in a parallel corpus to guide decoding,whereas MANY and MEMT operate purely on thetarget side, without requiring/exploiting the sourcetext or parallel data.
We integrate the informationfrom a parallel corpus into the decoding process byextracting phrase translations from the translationhypotheses and scoring these phrase translations onthe basis of the frequencies from the parallel corpus.The properties of this re-scored phrase tableproved attractive for the translation task in (Sen-nrich, 2011), but less so for the WMT 2011 trans-lation task.
To explain why, let us look at p(t|s),i.e.
the probability of a target phrase given a sourcephrase, as an example.
It is computed as follows,cprim and csec being the phrase count in the primaryand secondary corpus, respectively.p(t|s) =cprim(s, t) + csec(s, t)cprim(s) + csec(s)(1)We can assume that csec(s) and csec(s, t) aremostly fixed, having values between 1 and the num-ber of translation hypotheses.3 If cprim(s) is high,the phrase translation probabilities in the secondaryphrase table will only be marginally different fromthose in the primary phrase table (e.g.
5001000 = 0.5 vs.500+21000+2 = 0.501), whereas the secondary corpus hasa stronger effect for phrases that are rare or unseen inthe primary corpus (e.g.
13 = 0.333 vs.1+23+2 = 0.6).Analogously, the same reasoning applies to p(s|t),lex(t|s) and lex(s|t).45In short: the more frequent the phrases and phrasepairs in the primary corpus, the less effect does thesecondary corpus have on the final feature values.This is a desirable behaviour if we can ?trust?
thephrase pairs extracted from the primary corpus.
In(Sennrich, 2011), the primary corpus consisted ofin-domain texts, whereas the translation hypothe-ses came from an out-of-domain SMT system and arule-based one.
There, it proved an effective strategyto only consider those translation hypotheses that ei-ther agreed with the data from the primary corpus, orfor which the primary corpus had insufficient data,i.e.
unknown or rare source words.
With a primarysystem achieving a BLEU score of 17.18 and twotranslation hypotheses, scoring 13.29 and 12.94, weobtained a BLEU score of 20.06 for the combinedsystem.In the WMT 2011 system combination task, thestatistics from the primary corpus failed to effec-tively improve translation quality.
We offer theseexplanations based on an analysis of the results.First, the 2?3 systems whose translation hypothe-ses we combine obtain higher scores than the pri-mary system.
This casts doubt on whether we shouldtrust the scores from the primary system more thanthe translation hypotheses.
And in fact, the resultsin table 1 and 2 show that the submission system3Strictly speaking, this is only true if we build separatephrase tables for each sentence that is translated, and if thereare no repeated phrases.
This slight simplification serves illus-trative purposes.4For long phrases, phrase counts are typically low.
Still, theprimary corpus plays an important role in the computation ofthe lexical weights, which are computed from word frequencies,and thus typically less sparse than phrase frequencies.5Rare target words may obtain a undesirably high probabil-ity, but are penalized in the language model.
We set the LMlog-probability of unknown words to -100.168(whose phrase table features take into account theprimary corpus) is not better than a contrastive com-bination system with vanilla scoring, i.e.
one thatis solely based on the secondary corpus.
We canalso show why the primary corpus does not improvedecoding by way of example.
The German phraseBei der Wahl [der Matratze] (English: In the choice[of a mattress]), is translated by the three systemsas in the selection, when choosing and in the elec-tion.
In this context, the last translation hypothesisis the least correct, but since the political domainis strongly represented in the training data, it is themost frequent one in the primary corpus, and the onebeing chosen by both the primary and the combinedsystem.Second, there seems to be a significant overlap intraining data between the systems that we combineand our primary system6.
We only saw few casesin which a system produced a translation againstwhich there was evidence in our primary corpus.One instance is the German word Kindergarten (En-glish: kindergarten; nursery), which is translatedas children?s garden by one system.
In the com-bined system, this translation is dispreferred.
(Chenet al, 2009) argue that a combination of dissimi-lar systems might yield better results.
Rule-basedsystems could fulfill this role; they are also an at-tractive choice given their high quality (as judged byhuman evaluators) in earlier evaluations (e.g.
WMT2009 (Callison-Burch et al, 2009)).
We did not pur-sue this idea, since we optimized for highest BLEUscore, both during MERT and for the selection of thesubmission system, a scoring method that has beenshown to undervalue rule-based systems (Callison-Burch et al, 2006).The failure to outperform the individual best sys-tem in this translation task does not invalidate ourapproach.
It merely highlights that different con-ditions call for different tools.
Our approach re-lies strongly on parallel training data, in contrastto system combination tools such as MANY (Bar-rault, 2010) and MEMT (Heafield and Lavie, 2010).In this setting, this brought no benefit.
However,when developing a SMT system for a specific do-main and when combining an in-domain primary6This is especially true for all shared task participants build-ing constrained systems.
The amount of overlap between theanonymous online systems is unknown.system with out-of-domain translation hypotheses,we expect that this strong dependence on the pri-mary SMT system becomes an advantage.
It allowsthe system to discriminate between source phrasesthat are well-documented in the primary trainingdata, which will give other systems?
hypotheses lit-tle effect, and those that occur rarely or not at all inthe primary data, for which other systems may stillproduce a useful translation.4 ConclusionWe described the UZH system combination submis-sion to the Workshop of Machine Translation 2011.It uses the Moses architecture and includes transla-tion hypotheses through a second phrase table.
Itscentral characteristic is the extraction of phrase pairsfrom translations hypotheses and the scoring thereofon the basis of another parallel corpus.
We findthat, in the WMT 2011 system combination sharedtask, this approach fails to result in a significant im-provement over the best individual system in termsof BLEU score.
However, we argue that it is wellsuited for other translation tasks, such as the one de-scribed in (Sennrich, 2011).AcknowledgmentsThis research was funded by the Swiss National Sci-ence Foundation under grant 105215 126999.ReferencesLo?
?c Barrault.
2010.
MANY: Open source MT sys-tem combination at WMT?10.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 277?281, Uppsala,Sweden, July.
Association for Computational Linguis-tics.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluating the role of BLEU in machine transla-tion research.
In Proceedings the Eleventh Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 249?256, Trento, Italy.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009 work-shop on statistical machine translation.
In Proceedingsof the Fourth Workshop on Statistical Machine Trans-lation, pages 1?28, Athens, Greece, March.
Associa-tion for Computational Linguistics.169Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.
Association forComputational Linguistics.
Revised August 2010.Yu Chen, Andreas Eisele, Christian Federmann, EvaHasler, Michael Jellinghaus, and Silke Theison.
2007.Multi-engine machine translation with an open-sourcedecoder for statistical machine translation.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, StatMT ?07, pages 193?196, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Yu Chen, Michael Jellinghaus, Andreas Eisele, Yi Zhang,Sabine Hunsicker, Silke Theison, Christian Feder-mann, and Hans Uszkoreit.
2009.
Combining multi-engine translations with Moses.
In Proceedings of theFourth Workshop on Statistical Machine Translation,StatMT ?09, pages 42?46, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In ACL ?05: Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 531?540, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57, Columbus, Ohio,June.
Association for Computational Linguistics.Kenneth Heafield and Alon Lavie.
2010.
CMU multi-engine machine translation for WMT 2010.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, WMT ?10,pages 301?306, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving translation quality by dis-carding most of the phrasetable.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 48?54, Morristown, NJ, USA.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of ACL 2007, pages 177?180, Prague, CzechRepublic, June.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In ACL ?02: Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, pages 311?318, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 57?64, Ann Arbor, Michigan, June.
Association for Com-putational Linguistics.Rico Sennrich, Gerold Schneider, Martin Volk, and Mar-tin Warin.
2009.
A New Hybrid Dependency Parserfor German.
In Proceedings of the German Society forComputational Linguistics and Language Technology2009 (GSCL 2009), Potsdam, Germany.Rico Sennrich.
2011.
Combining multi-engine ma-chine translation and online learning through dynamicphrase tables.
In 15th Annual Conference of the Eu-ropean Association for Machine Translation (EAMT2011), Leuven, Belgium.170
