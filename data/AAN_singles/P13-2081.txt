Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 456?461,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSentence Level Dialect Identification in ArabicHeba ElfardyDepartment of Computer ScienceColumbia Universityheba@cs.columbia.eduMona DiabDepartment of Computer ScienceThe George Washington Universitymtdiab@gwu.eduAbstractThis paper introduces a supervised ap-proach for performing sentence level di-alect identification between Modern Stan-dard Arabic and Egyptian Dialectal Ara-bic.
We use token level labels to de-rive sentence-level features.
These fea-tures are then used with other core andmeta features to train a generative clas-sifier that predicts the correct label foreach sentence in the given input text.
Thesystem achieves an accuracy of 85.5%on an Arabic online-commentary datasetoutperforming a previously proposed ap-proach achieving 80.9% and reflecting asignificant gain over a majority baseline of51.9% and two strong baseline systems of78.5% and 80.4%, respectively.1 IntroductionThe Arabic language exists in a state of Diglos-sia (Ferguson, 1959) where the standard form ofthe language, Modern Standard Arabic (MSA) andthe regional dialects (DA) live side-by-side andare closely related.
MSA is the language usedin education, scripted speech and official settingswhile DA is the native tongue of Arabic speak-ers.
Arabic dialects may be divided into fivemain groups: Egyptian (including Libyan and Su-danese), Levantine (including Lebanese, Syrian,Palestinian and Jordanian), Gulf, Iraqi and Mo-roccan (Maghrebi) (Habash, 2010).
Even thoughthese dialects did not originally exist in a writtenform, they are pervasively present in social me-dia text (normally mixed with MSA) nowadays.DA does not have a standard orthography leadingto many spelling variations and inconsistencies.Linguistic Code switching (LCS) between MSAand DA happens both intra-sententially and inter-sententially.
LCS in Arabic poses a serious chal-lenge for almost all NLP tasks since MSA and DAdiffer on all levels of linguistic representation.
Forexample, MSA trained tools perform very badlywhen applied directly to DA or to a code-switchedDA-MSA text.
Hence a need for a robust dialectidentification tool as a preprocessing step arisesboth on the word and sentence levels.In this paper, we focus on the problem of dialectidentification on the sentence level.
We proposea supervised approach for identifying whether agiven sentence is prevalently MSA or EgyptianDA (EDA).
The system uses the approach that waspresented in (Elfardy et al, 2013) to perform tokendialect identification.
The token level decisionsare then combined with other features to train agenerative classifier that tries to predict the classof the given sentence.
The presented system out-performs the approach presented by Zaidan andCallison-Burch (2011) on the same dataset using10-fold cross validation.2 Related WorkDialect Identification in Arabic is crucial for al-most all NLP tasks, yet most of the research inArabic NLP, with few exceptions, is targeted to-wards MSA.
Biadsy et al (2009) present a sys-tem that identifies dialectal words in speech andtheir dialect of origin through the acoustic signals.Salloum and Habash (2011) tackle the problem ofDA to English Machine Translation (MT) by piv-oting through MSA.
The authors present a systemthat applies transfer rules from DA to MSA thenuses state of the art MSA to English MT system.Habash et al (2012) present CODA, a Conven-tional Orthography for Dialectal Arabic that aimsto standardize the orthography of all the variantsof DA while Dasigi and Diab (2011) present anunsupervised clustering approach to identify or-thographic variants in DA.
Zaidan and Callison-Burch (2011) crawl a large dataset of MSA-DAnews?
commentaries.
The authors annotate partof the dataset for sentence-level dialectalness on456Amazon Mechanical Turk and try a language mod-eling (LM) approach to solve the problem.
InElfardy and Diab (2012a), we present a set ofguidelines for token-level identification of dialec-talness while in (Elfardy and Diab, 2012b), (El-fardy et al, 2013) we tackle the problem of token-level dialect-identification by casting it as a code-switching problem.3 Approach to Sentence-Level DialectIdentificationWe present a supervised system that uses a NaiveBayes classifier trained on gold labeled data withsentence level binary decisions of either beingMSA or DA.3.1 FeaturesThe proposed supervised system uses two kinds offeatures: (1) Core Features, and (2) Meta Features.3.1.1 Core Features:These features indicate how dialectal (or non di-alectal) a given sentence is.
They are furtherdivided into: (a) Token-based features and (b)Perplexity-based features.3.1.1.1 Token-based Features: We use theapproach that was presented in (Elfardy et al,2013) to decide upon the class of each word inthe given sentence.
The aforementioned approachrelies on language models (LM) and MSA andEDA Morphological Analyzer to decide whethereach word is (a) MSA, (b) EDA, (c) Both (MSA& EDA) or (d) OOV.
We use the token-level classlabels to estimate the percentage of EDA wordsand the percentage of OOVs for each sentence.These percentages are then used as features forthe proposed model.
The following variants of theunderlying token-level system are built to assessthe effect of varying the level of preprocessingon the underlying LM on the performance of theoverall sentence level dialect identification pro-cess: (1) Surface, (2) Tokenized, (3) CODAfied,and (4) Tokenized-CODA.
We use the followingsentence to show the different techniques:AJJ??QJ??
?
@Qk ?Y?
kdh HrAm wktyr ElynA1.
Surface LMs: No significant preprocessingis applied apart from the regular initial cleanup of the text which includes removal ofURLs, normalization of speech effects suchas reducing all redundant letters in a word to1We use Buckwalter transliteration schemehttp://www.qamus.org/transliteration.htma standardized form, eg.
the elongated formof the word QJ?
ktyr1 ?a lot?
which could berendered in the text as QJJJJJJJ?
kttttyyyyris reduced to Q J JJ  J  J ?
ktttyyyr (specifi-cally three repeated letters instead of an un-predictable number of repetitions, to main-tain the signal that there is a speech effectwhich could be a DA indicator).ex.
A 	JJ??
QJ??
?
@Qk ?Y?kdh HrAm wktyr ElynA2.
Orthography Normalized (CODAfied)LM: since DA is not originally a writtenform of Arabic, no standard orthographyexists for it.
Habash et al (2012) attemptto solve this problem by presenting CODA,a conventional orthography for writing DA.We use the implementation of CODA pre-sented in CODAfy (Eskander et al, 2013),to build an orthography-normalized LM.While CODA and its applied version usingCODAfy solve the spelling inconsistencyproblem in DA, special care must be takenwhen using it for our task since it removesvaluable dialectalness cues.
For example, theletter H (v in Buckwalter (BW) Translitera-tion) is converted into the letter H (t in BW)in a DA context.
CODA suggests that suchcases get mapped to the original MSA phono-logical variant which might make the dialectidentification problem more challenging.
Onthe other hand, CODA solves the sparsenessissue by mapping multiple spelling-variantsto the same orthographic form leading to amore robust LM.ex.
A 	JJ??
QJ??
?
@Qk ?Y?kdh HrAm wkvyr ElynA3.
Tokenized LM: D3 tokenization-scheme isapplied to all data using MADA (Habash etal., 2009) (an MSA Tokenizer) for the MSAcorpora, and MADA-ARZ (Habash et al,2013) (an EDA tokenizer) for the EDA cor-pora.
For building the tokenized LM, wemaintain clitics and lexemes.
Some cliticsare unique to MSA while others are unique toEDA so maintaining them in the LM is help-ful, eg.
the negation enclitic ?
$ is onlyused in EDA but it could be seen with anMSA/EDA homograph, maintaining the en-clitic in the LM facilitates the identification457of the sequence as being EDA.
5-grams areused for building the tokenized LMs (as op-posed to 3-grams for the surface LMs)ex.
A 	K ???
QJ?
?
?
@Qk ?Y?kdh HrAm w+ ktyr Ely +nA4.
Tokenized & Orthography NormalizedLMs: (Tokenized-CODA) The data is tok-enized as in (3) then orthography normaliza-tion is applied to the tokenized data.ex.
A 	K ???
QJ?
?
?
@Qk ?Y?kdh HrAm w+ kvyr Ely +nAIn addition to the underlying token-level system,we use the following token-level features:1.
Percentage of words in the sentence that isanalyzable by an MSA morphological ana-lyzer.2.
Percentage of words in the sentence that isanalyzable by an EDA morphological ana-lyzer.3.
Percentage of words in the sentence that ex-ists in a precompiled EDA lexicon.3.1.1.2 Perplexity-based Features: We runeach sentence through each of the MSA and EDALMs and record the perplexity for each of them.The perplexity of a language model on a given testsentence; S(w1, .., wn) is defined as:perplexity = (2)?
(1/N)?i log2(p(wi|hi)) (1)where N is the number of tokens in the sentenceand hi is the history of token wi.The perplexity conveys how confused the LM isabout the given sentence so the higher the perplex-ity value, the less probable that the given sentencematches the LM.23.1.2 Meta Features.These are the features that do not directly relateto the dialectalness of words in the given sentencebut rather estimate how informal the sentence isand include:?
The percentage of punctuation, numbers,special-characters and words written in Ro-man script.2We repeat this step for each of the preprocessing schemesexplained in section 3.1.1.1?
The percentage of words having word-lengthening effects.?
Number of words & average word-length.?
Whether the sentence has consecutive re-peated punctuation or not.
(Binary feature,yes/no)?
Whether the sentence has an exclamationmark or not.
(Binary feature, yes/no)?
Whether the sentence has emoticons or not.
(Binary feature, yes/no)3.2 Model TrainingWe use the WEKA toolkit (Hall et al, 2009) andthe derived features to train a Naive-Bayes classi-fier.
The classifier is trained and cross-validatedon the gold-training data for each of our differentconfigurations (Surface, CODAfied, Tokenized &Tokenized-CODA).We conduct two sets of experiments.
In the firstone, Experiment Set A, we split the data into atraining set and a held-out test set.
In the secondset, Experiment Set B, we use the whole datasetfor training without further splitting.
For both setsof experiments, we apply 10-fold cross validationon the training data.
While using a held-out test-set for evaluation (in the first set of experiments)is a better indicator of how well our approach per-forms on unseen data, only the results from thesecond set of experiments are directly comparableto those produced by Zaidan and Callison-Burch(2011).4 Experiments4.1 DataWe use the code-switched EDA-MSA portion ofthe crowd source annotated dataset by Zaidanand Callison-Burch (2011).
The dataset consistsof user commentaries on Egyptian news articles.Table 1 shows the statistics of the data.MSA Sent.EDA Sent.MSA Tok.EDA Tok.Train 12,160 11,274 300,181 292,109Test 1,352 1,253 32,048 32,648Table 1: Number of EDA and MSA sentences andtokens in the training and test datasets.
In Experi-ment Set A only the train-set is used to perform a10-fold cross-validation and the test-set is used forevaluation.
In experiment Set B, all data is used toperform the 10-fold cross-validation.458(a) Experiment Set A (Uses 90% of the dataset) (b) Experiment Set B (Uses the whole dataset)Figure 1: Learning curves for the different configurations (obtained by applying 10-fold cross validationon the training set.
)4.2 BaselinesWe use four baselines.
The first of which is amajority baseline (Maj-BL); that assigns all thesentences the label of the most frequent class ob-served in the training data.
The second baseline(Token-BL) assumes that the sentence is EDA ifmore than 45% of its tokens are dialectal otherwiseit assumes it is MSA.3 The third baseline (Ppl-BL)runs each sentence through MSA & EDA LMs andassigns the sentence the class of the LM yieldingthe lower perplexity value.
The last baseline (OZ-CCB-BL) is the result obtained by Zaidan andCallison-Burch (2011) which uses the same ap-proach of our third baseline, Ppl-BL.4 For Token-BL and Ppl-BL, the performance is calculatedfor all LM-sizes of the four different configura-tions: Surface, CODAfied, Tokenized, Tokenized-CODA and the best performing configuration onthe cross-validation set is used as the baseline sys-tem.4.3 Results & DiscussionFor each of the different configurations, we build alearning curve by varying the size of the LMs be-tween 2M, 4M, 8M, 16M and 28M tokens.
Figures1a and 1b show the learning curves of the differentconfigurations on the cross-validation set for ex-periments A & B respectively.
In Table 2 we notethat both CODA and Tokenized solve the data-sparseness issue hence they produce better results3We experimented with different thresholds (15%, 30%,45%, 60% and 75%) and the 45% threshold setting yieldedCondition Exp.
Set A Exp.
Set BMaj-BL 51.9 51.9Token-BL 79.1 78.5Ppl-BL 80.4 80.4OZ-CCB-BL N/A 80.9Surface 82.4 82.6CODA 82.7 82.8Tokenized 85.3 85.5Tokenized-CODA 84.9 84.9Table 2: Performance Accuracies of the differ-ent configurations of the 8M LM (best-performingLM size) using 10-fold cross validation against thedifferent baselines.than Surface experimental condition.
However, asmentioned earlier, CODA removes some dialectal-ness cues so the improvement resulting from usingCODA is much less than that from using tokeniza-tion.
Also when combining CODA with tokeniza-tion as in the condition Tokenized-CODA, the per-formance drops since in this case the sparsenessissue has been already resolved by tokenizationso adding CODA only removes dialectalness cues.For example QJ??
wktyr ?and a lot?
does not oc-cur frequently in the data so when performing thetokenization it becomes QJ ?
?
w+ ktyr whichon the contrary is frequent in the data.
Addingthe best performance4This baseline can only be compared to the results of thesecond set of experiments.459Condition Test SetMaj-BL 51.9Token-BL 77Ppl-BL 81.1Tokenized 83.3Table 3: Performance Accuracies of the best-performing configuration (Tokenized) on the held-out test set against the baselines Maj-BL, Token-BL and Ppl-BL.Orthography-Normalization converts it to QJ?
?w+ kvyr which is more MSA-like hence the con-fusability increases.All configurations outperform all baselines withthe Tokenized configuration producing the best re-sults.
The performance of all systems drop asthe size of the LM increases beyond 16M tokens.As indicated in (Elfardy et al, 2013) as the sizeof the MSA & EDA LMs increases, the sharedngrams increase leading to higher confusabilitybetween the classes of tokens in a given sentence.Table 3 presents the results on the held out datasetcompared against three of the baselines, Maj-BL,Token-BL and Ppl-BL.
We note that the Tokenizedcondition, the best performing condition, outper-forms all baselines with a significant margin.5 ConclusionWe presented a supervised approach for sentencelevel dialect identification in Arabic.
The ap-proach uses features from an underlying systemfor token-level identification of Egyptian Dialec-tal Arabic in addition to other core and meta fea-tures to decide whether a given sentence is MSA orEDA.
We studied the impact of two types of pre-processing techniques (Tokenization and Orthog-raphy Normalization) as well as varying the size ofthe LM on the performance of our approach.
Thepresented approach produced significantly betterresults than a previous approach in addition tobeating the majority baseline and two other strongbaselines.AcknowledgmentsThis work is supported by the Defense AdvancedResearch Projects Agency (DARPA) BOLT pro-gram under contract number HR0011-12-C-0014.ReferencesFadi Biadsy, Julia Hirschberg, and Nizar Habash.2009.
Spoken arabic dialect identification usingphonotactic modeling.
In Proceedings of the Work-shop on Computational Approaches to Semitic Lan-guages at the meeting of the European Associa-tion for Computational Linguistics (EACL), Athens,Greece.Pradeep Dasigi and Mona Diab.
2011.
Codact: To-wards identifying orthographic variants in dialec-tal arabic.
In Proceedings of the 5th InternationalJoint Conference on Natural Language Processing(ICJNLP), Chiangmai, Thailand.Heba Elfardy and Mona Diab.
2012a.
Simplifiedguidelines for the creation of large scale dialectalarabic annotations.
In Proceedings of the 8th In-ternational Conference on Language Resources andEvaluation (LREC), Istanbul, Turkey.Heba Elfardy and Mona Diab.
2012b.
Token levelidentification of linguistic code switching.
In Pro-ceedings of the 24th International Conference onComputational Linguistics (COLING),Mumbai, In-dia.Heba Elfardy, Mohamed Al-Badrashiny, and MonaDiab.
2013.
Code Switch Point Detection in Arabic.In Proceedings of the 18th International Conferenceon Application of Natural Language to InformationSystems (NLDB2013), MediaCity, UK, June.Ramy Eskander, Nizar Habash, Owen Rambow, andNadi Tomeh.
2013.
Processing Spontaneous Or-thography.
In Proceedings of the 2013 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies (NAACL-HLT), Atlanta, GA.Ferguson.
1959.
Diglossia.
Word 15.
325340.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.Mada+ tokan: A toolkit for arabic tokenization, dia-critization, morphological disambiguation, pos tag-ging, stemming and lemmatization.
In Proceedingsof the 2nd International Conference on Arabic Lan-guage Resources and Tools (MEDAR), Cairo, Egypt,pages 102?109.Nizar Habash, Mona Diab, and Owen Rabmow.
2012.Conventional orthography for dialectal arabic.
InProceedings of the Language Resources and Eval-uation Conference (LREC), Istanbul.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
MorphologicalAnalysis and Disambiguation for Dialectal Arabic.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), Atlanta, GA.Nizar Habash.
2010.
Introduction to arabic naturallanguage processing.
Advances in neural informa-tion processing systems.460Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Wael Salloum and Nizar Habash.
2011.
Dialectalto standard arabic paraphrasing to improve arabic-english statistical machine translation.
In Proceed-ings of the First Workshop on Algorithms and Re-sources for Modelling of Dialects and Language Va-rieties, pages 10?21.
Association for ComputationalLinguistics.Omar F Zaidan and Chris Callison-Burch.
2011.
Thearabic online commentary dataset: an annotateddataset of informal arabic with high dialectal con-tent.
In Proceedings of ACL, pages 37?41.461
