Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
430?438, Prague, June 2007. c?2007 Association for Computational LinguisticsSmooth Bilingual N-gram TranslationHolger Schwenk Marta R. Costa-jussa` and Jose?
A.R.
FonollosaLIMSI-CNRS, BP 13391403 Orsay cedex, FRANCEschwenk@lismi.frUPC - TALPBarcelona 08034, Spain{mruiz,adrian}@gps.tsc.upc.eduAbstractWe address the problem of smoothing trans-lation probabilities in a bilingual N-gram-based statistical machine translation system.It is proposed to project the bilingual tuplesonto a continuous space and to estimate thetranslation probabilities in this representa-tion.
A neural network is used to perform theprojection and the probability estimation.Smoothing probabilities is most importantfor tasks with a limited amount of trainingmaterial.
We consider here the BTEC taskof the 2006 IWSLT evaluation.
Improve-ments in all official automatic measures arereported when translating from Italian to En-glish.
Using a continuous space model forthe translation model and the target languagemodel, an improvement of 1.5 BLEU on thetest data is observed.1 IntroductionThe goal of statistical machine translation (SMT) isto produce a target sentence e from a source sen-tence f .
Among all possible target language sen-tences the one with the highest probability is chosen:e?
= arg maxePr(e|f) = arg maxePr(f |e) Pr(e)where Pr(f |e) is the translation model and Pr(e)is the target language model.
This approach isusually referred to as the noisy source-channel ap-proach in statistical machine translation (Brown etal., 1993).During the last few years, the use of contextin SMT systems has provided great improvementsin translation.
SMT has evolved from the origi-nal word-based approach to phrase-based translationsystems (Och et al, 1999; Koehn et al, 2003).
Aphrase is defined as a group of source words f?
thatshould be translated together into a group of targetwords e?.
The translation model in phrase-based sys-tems includes the phrase translation probabilities inboth directions, i.e.
P (e?|f?)
and P (f?
|e?
).The use of a maximum entropy approach simpli-fies the introduction of several additional models ex-plaining the translation process :e?
= arg max p(e|f)= arg maxe{exp(?i?ihi(e, f))} (1)The feature functions hi are the system models andthe ?i weights are typically optimized to maximizea scoring function on a development set (Och andNey, 2002).The phrase translation probabilities P (e?|f? )
andP (f?
|e?)
are usually obtained using relative frequencyestimates.
Statistical learning theory, however, tellsus that relative frequency estimates have severaldrawbacks, in particular high variance and low bias.Phrase tables may contain several millions of en-tries, most of which appear only once or twice,which means that we are confronted with a datasparseness problem.
Surprisingly, there seems to belittle work addressing the issue of smoothing of thephrase table probabilities.On the other hand, smoothing of relative fre-quency estimates was extensively investigated in the430area of language modeling.
A systematic compari-son can be for instance found in (Chen and Good-man, 1999).
Language models and phrase tableshave in common that the probabilities of rare eventsmay be overestimated.
However, in language mod-eling probability mass must be redistributed in orderto account for the unseen n-grams.
Generalizationto unseen events is less important in phrase-basedSMT systems since the system searches only for thebest segmentation and the best matching phrase pairamong the existing ones.We are only aware of one work that performs asystematic comparison of smoothing techniques inphrase-based machine translation systems (Foster etal., 2006).
Two types of phrase-table smoothingwere compared: black-box and glass-box methods.Black-methods do not look inside phrases but in-stead treat them as atomic objects.
By these means,all the methods developed for language modelingcan be used.
Glass-box methods decompose P (e?|f?
)into a set of lexical distributions P (e|f?
).
For in-stance, it was suggested to use IBM-1 probabili-ties (Och et al, 2004), or other lexical translationprobabilities (Koehn et al, 2003; Zens and Ney,2004).
Some form of glass-box smoothing is nowused in all state-of-the-art statistical machine trans-lation systems.Another approach related to phrase table smooth-ing is the so-called N-gram translation model(Marin?o et al, 2006).
In this model, bilingual tu-ples are used instead of the phrase pairs and n-gramprobabilities are considered rather than relative fre-quencies.
Therefore, smoothing is obtained us-ing the standard techniques developed for languagemodeling.
In addition, a context dependence of thephrases is introduced.
On the other hand, somerestrictions on the segmentation of the source sen-tence must be used.
N-gram-based translation mod-els were extensively compared to phrase-based sys-tems on several tasks and typically achieve compa-rable performance.In this paper we propose to investigate improvedsmoothing techniques in the framework of the N-gram translation model.
Despite the undeniable suc-cess of n-graam back-off models, these techniqueshave several drawbacks from a theoretical point ofview: the words are represented in a discrete space,the vocabulary.
This prevents ?true interpolation?
ofthe probabilities of unseen n-grams since a changein this word space can result in an arbitrary changeof the n-gram probability.
An alternative approachis based on a continuous representation of the words(Bengio et al, 2003).
The basic idea is to convertthe word indices to a continuous representation andto use a probability estimator operating in this space.Since the resulting distributions are smooth func-tions of the word representation, better generaliza-tion to unknown n-grams can be expected.
Prob-ability estimation and interpolation in a continuousspace is mathematically well understood and numer-ous powerful algorithms are available that can per-form meaningful interpolations even when only alimited amount of training material is available.
Thisapproach was successfully applied to language mod-eling in large vocabulary continuous speech recogni-tion (Schwenk, 2007) and to language modeling inphrase-based SMT systems (Schwenk et al, 2006).In this paper, we investigate whether this ap-proach is useful to smooth the probabilities involvedin the bilingual tuple translation model.
Reliable es-timation of unseen n-grams is very important in thistranslation model.
Most of the trigram tuples en-countered in the development or test data were neverseen in the training data.
N-gram hit rates are re-ported in the results section of this paper.
We reportexperimental results for the BTEC corpus as usedin the 2006 evaluations of the international work-shop on spoken language translation IWSLT (Paul,2006).
This task provides a very limited amountof resources in comparison to other tasks like thetranslation of journal texts (NIST evaluations) or ofparliament speeches (TC-STAR evaluations).
There-fore, new techniques must be deployed to take thebest advantage of the limited resources.
Among thelanguage pairs tested in this years evaluation, Ital-ian to English gave the best BLEU results in thisyear evaluation.
The better the translation quality is,the more it is challenging to outperform it withoutadding more data.
We show that a new smoothingtechnique for the translation model achieves a sig-nificant improvement in the BLEU score for a state-of-the-art statistical translation system.This paper is organized as follows.
In the nextsection we first describe the baseline statistical ma-chine translation systems.
Section 3 presents the ar-chitecture and training algorithms of the continuous431space translation model and section 4 summarizesthe experimental evaluation.
The paper concludeswith a discussion of future research directions.2 N-gram-based Translation ModelThe N -gram-based translation model has been de-rived from the finite-state perspective; more specif-ically, from the work of Casacuberta (2001).
How-ever, different from it, where the translation modelis implemented by using a finite-state transducer,the N -gram-based system implements a bilingualN -gram model.
It actually constitutes a languagemodel of bilingual units, referred to as tuples, whichapproximates the joint probability between sourceand target languages by using N -grams, such as de-scribed by the following equation:p(e, f) ?K?k=1p((e, f)k|(e, f)k?1, .
.
.
, (e, f)k?4)(2)where e refers to target, f to source and (e, f)k tothe kth tuple of a given bilingual sentence pair.Bilingual units (tuples) are extracted from anyword-to-word alignment according to the followingconstraints:?
a monotonic segmentation of each bilingualsentence pairs is produced,?
no word inside the tuple is aligned to wordsoutside the tuple, and?
no smaller tuples can be extracted without vio-lating the previous constraints.As a consequence of these constraints, only onesegmentation is possible for a given sentence pair.Two important issues regarding this translationmodel must be considered.
First, it often occurs thata large number of single-word translation probabil-ities are left out of the model.
This happens for allwords that are always embedded in tuples contain-ing two or more words, then no translation probabil-ity for an independent occurrence of these embed-ded words will exist.
To overcome this problem, thetuple trigram model is enhanced by incorporating1-gram translation probabilities for all the embed-ded words detected during the tuple extraction step.These 1-gram translation probabilities are computedfrom the intersection of both the source-to-target andthe target-to-source alignments.The second issue has to do with the fact that somewords linked to NULL end up producing tuples withNULL source sides.
Since no NULL is actually ex-pected to occur in translation inputs, this type of tu-ple is not allowed.
Any target word that is linked toNULL is attached either to the word that precedesor the word that follows it.
To determine this, an ap-proach based on the IBM1 probabilities was used, asdescribed in (Marin?o et al, 2006).2.1 Additional featuresThe following feature functions were used in the N-gram-based translation system:?
A target language model.
In the baseline sys-tem, this feature consists of a 4-gram back-offmodel of words, which is trained from the tar-get side of the bilingual corpus.?
A source-to-target lexicon model and atarget-to-source lexicon model.
These fea-ture, which are based on the lexical parametersof the IBM Model 1, provide a complementaryprobability for each tuple in the translation ta-ble.?
A word bonus function.
This feature intro-duces a bonus based on the number of targetwords contained in the partial-translation hy-pothesis.
It is used to compensate for the sys-tem?s preference for short output sentences.All these models are combined in the de-coder.
Additionally, the decoder allows for anon-monotonic search with the following distorsionmodel.?
A word distance-based distorsion model.P (tK1 ) = exp(?K?k=1dk)where dk is the distance between the first wordof the kth tuple (unit), and the last word+1 ofthe (k ?
1)th tuple.432Figure 1: Comparing regular and unfolded tuples.Distance are measured in words referring to the unitssource side.To reduce the computational cost we place lim-its on the search using two parameters: the distor-tion limit (the maximum distance measured in wordsthat a tuple is allowed to be reordered, m) and thereordering limit (the maximum number of reorder-ing jumps in a sentence, j).
Tuples need to be ex-tracted by an unfolding technique (Marin?o et al,2006).
This means that the tuples are broken intosmaller tuples, and these are sequenced in the orderof the target words.
In order not to lose the infor-mation on the correct order, the decoder performs anon-monotonic search.
Figure 1 shows an exampleof tuple unfolding compared to the monotonic ex-traction.
The unfolding technique produces a differ-ent bilingual n-gram language model with reorderedsource words.In order to combine the models in the decodersuitably, an optimization tool based on the Simplexalgorithm is used to compute log-linear weights foreach model.3 Continuous Space N-gram ModelsThe architecture of the neural network n-grammodel is shown in Figure 2.
A standardfully-connected multi-layer perceptron isused.
The inputs to the neural network arethe indices of the n?1 previous units (wordsor tuples) in the vocabulary hj=wj?n+1,.
.
.
, wj?2, wj?1 and the outputs are the poste-rior probabilities of all units of the vocabulary:projectionlayer hiddenlayeroutputlayerinputprojectionssharedLM probabilitiesfor all wordsprobability estimationNeural Networkdiscreterepresentation:indices in wordlistcontinuousrepresentation:P dimensional vectorsNwj?1 PHNP (wj =1|hj)wj?n+1wj?n+2P (wj =i|hj)P (wj =N|hj)cloiMVdjp1 =pN =pi =Figure 2: Architecture of the continuous space LM.hj denotes the context wj?n+1, .
.
.
, wj?1.
P is thesize of one projection and H ,N is the size of thehidden and output layer respectively.
When short-lists are used the size of the output layer is muchsmaller than the size of the vocabulary.P (wj = i|hj) ?i ?
[1,N ] (3)where N is the size of the vocabulary.
The inputuses the so-called 1-of-n coding, i.e., the ith unit ofthe vocabulary is coded by setting the ith element ofthe vector to 1 and all the other elements to 0.
Theith line of the N ?P dimensional projection matrixcorresponds to the continuous representation of theith unit.
Let us denote cl these projections, dj thehidden layer activities, oi the outputs, pi their soft-max normalization, and mjl, bj , vij and ki the hid-den and output layer weights and the correspondingbiases.
Using these notations, the neural networkperforms the following operations:dj = tanh(?lmjl cl + bj)(4)oi =?jvij dj + ki (5)pi = eoi /N?r=1eor (6)The value of the output neuron pi corresponds di-rectly to the probability P (wj = i|hj).433Training is performed with the standard back-propagation algorithm minimizing the following er-ror function:E =N?i=1ti log pi + ???
?jlm2jl +?ijv2ij??
(7)where ti denotes the desired output, i.e., the proba-bility should be 1.0 for the next unit in the trainingsentence and 0.0 for all the other ones.
The first partof this equation is the cross-entropy between the out-put and the target probability distributions, and thesecond part is a regularization term that aims to pre-vent the neural network from over-fitting the train-ing data (weight decay).
The parameter ?
has to bedetermined experimentally.
Training is done usinga re-sampling algorithm as described in (Schwenk,2007).It can be shown that the outputs of a neural net-work trained in this manner converge to the posteriorprobabilities.
Therefore, the neural network directlyminimizes the perplexity on the training data.
Notealso that the gradient is back-propagated through theprojection-layer, which means that the neural net-work learns the projection of the units onto the con-tinuous space that is best for the probability estima-tion task.In general, the complexity to calculate one prob-ability with this basic version of the neural networkn-gram model is dominated by the dimension of theoutput layer since the size of the vocabulary (10kto 64k) is usually much larger than the dimension ofthe hidden layer (200 to 500).
Therefore, in previousapplications of the continuous space n-gram model,the output was limited to the s most frequent units, sranging between 2k and 12k (Schwenk, 2007).
Thisis called a short-list.Sents WordsTrain (bitexts) 20k 155.4/166.3kDev 489 5.2kEval 500 6kTable 1: Available data in the supplied resources ofthe 2006 IWSLT evaluation.4 Experimental EvaluationIn this work we report results on the Basic Travel-ing Expression Corpus (BTEC) as used in the 2006evaluations of the international workshop on spokenlanguage translation (IWSLT).
This corpus consistsof typical sentences from phrase books for tourists inseveral languages (Takezawa et al, 2002).
We reportresults on the supplied development corpus of 489sentences and the official test set of the IWSLT?06evaluation.
The main measure is the BLEU score,using seven reference translations.
The scoring iscase insensitive and punctuations are ignored.
De-tails on the available data are summarized in Table 1.We concentrated first on the translation from Ital-ian to English.
All participants in the IWSLT evalua-tion achieved much better performances for this lan-guage pair than for the other considered translationdirections.
This makes it more difficult to achieveadditional improvements.A non-monotonic search was performed follow-ing a local reordering named in Section 2, settingm = 5 and j = 3.
Also we used histogram prun-ing in the decoder, i.e.
the maximum number of hy-potheses in a stack is limited to 50.4.1 Language-dependent preprocessingItalian contracted prepositions have been separatedinto preposition + article, such as ?alla??
?a la?,?degli??
?di gli?
or ?dallo??
?da lo?, among others.4.2 Model trainingThe training and development data for the bilingualback-off and neural network translation model werecreated as follows.
Given the alignment of the train-ing parallel corpus, we perform a unique segmenta-tion of each parallel sentence following the criterionof unfolded segmentation seen in Section 2.
Thissegmentation is used in a sequence as training textfor building the language model.
As an example,given the alignment and the unfold extraction of Fig-ure 1, we obtain the following training sentence:<s> how long#cua?nto does#NULL last#durathe#el flight#vuelo </s>The reference bilingual trigram back-off transla-tion model was trained on these bilingual tuples us-434ing the SRI LM toolkit (Stolcke, 2002).
Differentsmoothing techniques were tried, and best resultswere obtained using Good-Turing discounting.The neural network approach was trained on ex-actly the same data.
A context of two tuples wasused (trigram model).
The training corpus containsabout 21,500 different bilingual tuples.
We decidedto limit the output of the neural network to the 8kmost frequent tuples (short-list).
This covers about90% of the requested tuple n-grams in the trainingdata.Similar to previous applications, the neural net-work is not used alone but interpolation is performedto combine several n-gram models.
First of all, theneural network and the reference back-off model areinterpolated together - this always improved perfor-mance since both seem to be complementary.
Sec-ond, four neural networks with different sizes of thecontinuous representation were trained and interpo-lated together.
This usually achieves better general-ization behavior than training one larger neural net-work.
The interpolation coefficients were calculatedby optimizing perplexity on the development data,using an EM procedure.
The obtained values are0.33 for the back-off translation model and about0.16 for each neural network model respectively.This interpolation is used in all our experiments.
Forthe sake of simplicity we will still call this the con-tinuous space translation model.Each network was trained independently usingearly stopping on the development data.
Conver-gence was achieved after about 10 iterations throughthe training data (less than 20 minutes of processingon a standard Linux machine).
The other parametersare as follows:?
Context of two tuples (trigram)?
The dimension of the continuous representationof the tuples were c =120,140,150 and 200,?
The dimension of the hidden layer was set toP = 200,?
The initial learning rate was 0.005 with an ex-ponential decay,?
The weight decay coefficient was set to ?
=0.00005.N-gram models are usually evaluated using per-plexity on some development data.
In our case, i.e.using bilingual tuples as basic units (?words?
), it isless obvious if perplexity is a useful measure.
Nev-ertheless, we provide these numbers for complete-ness.
The perplexity on the development data of thetrigram back-off translation model is 227.0.
Thiscould be reduced to 170.4 using the neural network.It is also very informative to analyze the n-gramhit-rates of the back-off model on the developmentdata: 10% of the probability requests are actually atrue trigram, 40% a bigram and about 49% are fi-nally estimated using unigram probabilities.
Thismeans that only a limited amount of phrase con-text is used in the standard N-gram-based translationmodel.
This makes this an ideal candidate to ap-ply the continuous space model since probabilitiesare interpolated for all possible contexts and neverbacked-up to shorter contexts.4.3 Results and analysisThe incorporation of the neural translation modelis done using n-best list.
Each hypothesis is com-posed of a sequence of bilingual tuples and the cor-responding scores of all the feature functions.
Fig-ure 3 shows an example of such an n-best list.
Theneural trigram translation model is used to replacethe scores of the trigram back-off translation model.This is followed by a re-optimization of the coef-ficients of all feature functions, i.e.
maximizationof the BLEU score on the development data usingthe numerical optimization tool CONDOR (Berghenand Bersini, 2005).
An alternative would be to adda feature function and to combine both translationmodels under the log-linear model framework, us-ing maximum BLEU training.Another open question is whether it might bybetter to already use the continuous space transla-tion model during decoding.
The continuous spacemodel has a much higher complexity than a back-off n-gram.
However, this can be heavily optimizedwhen rescoring n-best lists, i.e.
by grouping to-gether all calls in the whole n-best list with the samecontext, resulting in only one forward pass throughthe neural network.
This is more difficult to per-form when the continuous space translation modelis used during decoding.
Therefore, this was not in-vestigated in this work.435spiacente#sorry tutto occupato#it ?s fullspiacente#i ?m sorry tutto occupato#it ?s fullspiacente#i ?m afraid tutto occupato#it ?s fullspiacente#sorry tutto#all occupato#busyspiacente#sorry tutto#all occupato#takenFigure 3: Example of sentences in the n-best list ofbilingual tuples.
The special character ?#?
is used toseparate the source and target sentence words.
Sev-eral words in one tuple a grouped together using ?
.
?In all our experiments 1000-best lists were used.In order to evaluate the quality of these n-best lists,an oracle trigram back-off translation model wasbuild on the development data.
Rescoring the n-best lists with this translation model resulted in anincrease of the BLEU score of about 10 points (seeTable 2).
While there is an decrease of about 6%for the position dependent word error rate (mWER),a smaller change in the position independent worderror rate was observed (mPER).
This suggests thatmost of the alternative translation hypothesis re-sult in word reorderings and not in many alternativeword choices.
This is one of the major drawbacksof phrase- and N-gram-based translation systems:only translations observed in the training data canbe used.
There is no generalization to new phrasepairs.Back-off Oracle NeuralBLEU 42.34 52.45 43.87mWER 41.6% 35.6% 40.3%mPER 31.5% 28.2% 30.7%Table 2: Comparison of different N-gram-translation models on the development data.When the 1000-best lists are rescored with theneural network translation model the BLEU scoreincreases by 1.5 points (42.34 to 43.87).
Similar im-provements were observed in the word error rates(see Table 2).
For comparison, a 4-gram back-offtranslation model was also built, but no change ofthe BLEU score was observed.
This suggests thatcareful smoothing is more important than increasingthe context when estimating the translation probabil-ities in an N-gram-based statistical machine transla-tion system.In previous work, we have investigated the use ofthe neural network approach to modeling the targetlanguage for the IWSLT task (Schwenk et al, 2006).We also applied this technique to this improved N-gram-based translation system.
In our implemen-tation, the neural network target 4-gram languagemodel gives an improvement of 1.3 points BLEUon the development data (42.34 to 43.66), in com-parison to 1.5 points for the neural translation model(see Table 3).Back-off neural neural neuralTM+LM TM LM TM+LMBLEU 42.34 43.87 43.66 44.83Table 3: Combination of a neural translation model(TM) and a neural language model (LM).
BLEUscores on the development data.The neural translation and target language modelwere also applied to the test data, using of course thesame feature function coefficients as for the devel-opment data.
The results are given in Table 4 for allthe official measures of the IWSLT evaluation.
Thenew smoothing method of the translation probabili-ties achieves improvement in all measures.
It givesalso an additional gain (again in all measures) whenused together with a neural target language model.Surprisingly, neural TM and neural LM improve-ments almost add up: when both techniques are usedtogether, the BLEU scores increases by 1.5 points(36.97 ?
38.50).
Remember that the reference N-gram-based translation system already uses a localreordering approach.Back-off neural neural neuralTM+LM TM LM TM+LMBLEU 36.97 37.21 38.04 38.50mWER 48.10 47.42 47.83 47.61mPER 38.21 38.07 37.26 37.12NIST 8.3 8.3 8.6 8.7Meteor 63.16 63.40 64.70 65.20Table 4: Test set scores for the combination of aneural translation model (TM) and a neural languagemodel (LM).4365 DiscussionPhrase-based approaches are the de-facto standardin statistical machine translation.
The phrases areextracted automatically from the word alignmentsof parallel texts, and the different possible transla-tions of a phrase are weighted using relative fre-quency.
This can be problematic when the data issparse.
However, there seems to be little work onpossible improvements of the relative frequency es-timates by some smoothing techniques.
It is todaycommon practice to use additional feature functionslike IBM-1 scores to obtain some kind of smoothing(Och et al, 2004; Koehn et al, 2003; Zens and Ney,2004), but better estimation of the phrase probabili-ties is usually not addressed.An alternative way to represent phrases is to de-fine bilingual tuples.
Smoothing, and context de-pendency, is obtained by using an n-gram model onthese tuples.
In this work, we have extended thisapproach by using a new smoothing technique thatoperates on a continuous representation of the tu-ples.
Our method is distinguished by two charac-teristics: better estimation of the numerous unseenn-grams, and a discriminative estimation of the tu-ple probabilities.
Results are provided on the BTECtask of the 2006 IWSLT evaluation for the translationdirection Italian to English.
This task provides verylimited amount of resources in comparison to othertasks.
Therefore, new techniques must be deployedto take the best advantage of the limited resources.We have chosen the Italian to English task because itis challenging to enhance a good quality translationtask (over 40 BLEU percentage).
Using the continu-ous space model for the translation and target lan-guage model, an improvement of 2.5 BLEU on thedevelopment data and 1.5 BLEU on the test data wasobserved.Despite these encouraging results, we believe thatadditional research on improved estimation of prob-abilities in N-gram- or phrase-based statistical ma-chine translation systems is needed.
In particu-lar, the problem of generalization to new trans-lations seems to be promising to us.
This couldbe addressed by the so-called factored phrase-basedmodel as implemented in the Moses decoder (Koehnet al, 2007).
In this approach words are decom-posed into several factors.
These factors are trans-lated and a target phrase is generated.
This modelcould be complemented by a factored continuoustuple N-gram.
Factored word language modelswere already successfully used in speech recogni-tion (Bilmes and Kirchhoff, 2003; Alexandrescu andKirchhoff, 2006) and an extension to machine trans-lation seems to be promising.The described smoothing method was explicitlydeveloped to tackle the data sparseness problem intasks like the BTEC corpus.
It is well known fromlanguage modeling that careful smoothing is less im-portant when large amounts of data are available.We plan to investigate whether this also holds forsmoothing of the probabilities in phrase- or tuple-based statistical machine translation systems.6 AcknowledgmentsThis work has been partially funded by the EuropeanUnion under the integrated project TC-STAR (IST-2002-FP6-506738), by the French Government un-der the project INSTAR (ANR JCJC06 143038) andthe the Spanish government under a FPU grant andthe project AVIVAVOZ (TEC2006-13964-C03).ReferencesA.
Alexandrescu and K. Kirchhoff.
2006.
Factored neu-ral language models.
In HLT-NAACL.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.A neural probabilistic language model.
Journal of Ma-chine Learning Research, 3(2):1137?1155.F.
Vanden Berghen and H. Bersini.
2005.
CON-DOR, a new parallel, constrained extension of pow-ell?s UOBYQA algorithm: Experimental results andcomparison with the DFO algorithm.
Journal of Com-putational and Applied Mathematics, 181:157?175.J.
A. Bilmes and K. Kirchhoff.
2003.
Factored languagemodels and generalized backoff.
In HLT-NAACL.P.
Brown, S. Della Pietra, V. J. Della Pietra, and R: Mer-cer.
1993.
The mathematics of statistical machinetranslation.
Computational Linguistics, 19(2):263?311.F.
Casacuberta, D. Llorens, C.
Mart?
?nez, S. Molau,F.
Nevado, H. Ney, M. Pastor, D.
Pico?, A. Sanchis,E.
Vidal, and J.M.
Vilar.
2001.
Speech-to-speechtranslation based on finite-state transducers.
Interna-tional Conference on Acoustic, Speech and Signal Pro-cessing, 1.437S.
F. Chen and J. T. Goodman.
1999.
An empirical studyof smoothing techniques for language modeling.
CSL,13(4):359?394.G.
Foster, R. Kuhn, and H. Johnson.
2006.
Phrasetablesmoothing for statistical machine translation.
InEMNLP06, pages 53?61.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrased-based machine translation.
In Human Lan-guage Technology Conference (HLT-NAACL), pages127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL, demonstration session.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M. R. Costa-jussa`.2006.
Bilingual n-gram statistical machine transla-tion.
Computational Linguistics, 32(4):527?549, De-cember.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In ACL, pages 295?302.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Improvedalignment models for statistical machine translation.In Joint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Copora,pages 20?28.F.-J.
Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgasbordof features for statistical machine translation.
In HLT-NAACL, pages 161?168.M.
Paul.
2006.
Overview of the IWSLT 2006 campaign.In IWSLT, pages 1?15.H.
Schwenk, M. R. Costa-jussa`, and J.
A. R. Fonollosa.2006.
Continuous space language models for the iwslt2006 task.
IWSLT, pages 166?173.H.
Schwenk.
2007.
Continuous space language models.Computer Speech and Language, 21:492?518.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In ICSLP, pages II: 901?904.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a borad-coverage bilin-gual corpus for speech translation of travel conversa-tions in the real world.
In LREC, pages 147?152.R.
Zens and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In HLT/NACL,pages 257?264.438
