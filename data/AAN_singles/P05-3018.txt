Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 69?72, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsWord Alignment and Cross-Lingual Resource Acquisition ?Carol Nichols and Rebecca HwaDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260{cln23,hwa}@cs.pitt.eduAbstractAnnotated corpora are valuable resourcesfor developing Natural Language Process-ing applications.
This work focuses onacquiring annotated data for multilingualprocessing applications.
We present anannotation environment that supports aweb-based user-interface for acquiring wordalignments between English and Chinese aswell as a visualization tool for researchersto explore the annotated data.1 IntroductionThe performance of many Natural Language Pro-cessing (NLP) applications can be improved throughsupervised machine learning techniques that trainsystems with annotated training examples.
For ex-ample, a part-of-speech (POS) tagger might be in-duced from words that have been annotated withthe correct POS tags.
A limitation to the super-vised approach is that the annotation is typicallyperformed manually.
This poses as a challenge inthree ways.
First, researchers must develop a com-prehensive annotation guideline for the annotatorsto follow.
Guideline development is difficult becauseresearchers must be specific enough so that differentannotators?
work will be comparable, but also gen-eral enough to allow the annotators to make theirown linguistic judgments.
Reported experiences ofprevious annotation projects suggest that guidelinedevelopment is both an art and a science and is itself?This work has been supported, in part, by CRA-W Distributed Mentor Program.
We thank Karina Iva-netich, David Chiang, and the NLP group at Pitt forhelpful feedbacks on the user interfaces; Wanwan Zhangand Ying-Ju Suen for testing the system; and the anony-mous reviewers for their comments on the paper.a time-consuming process (Litman and Pan, 2002;Marcus et al, 1993; Xia et al, 2000; Wiebe, 2002).Second, it is common for the annotators to makemistakes, so some form of consistency check is nec-essary.
Third, the entire process (guideline develop-ment, annotation, and error corrections) may haveto be repeated with new domains.This work focuses on the first two challenges: help-ing researchers to design better guidelines and to col-lect a large set of consistently labeled data from hu-man annotators.
Our annotation environment con-sists of two pieces of software: a user interface forthe annotators and a visualization tool for the re-searchers to examine the data.
The data-collectioninterface asks the users to make lexical and phrasalmappings (word alignments) between the two lan-guages.
Some studies suggest that supervised wordaligned data may improve machine translation per-formance (Callison-Burch et al, 2004).
The inter-face can also be configured to ask the annotatorsto correct projected annotated resources.
The ideaof projecting English annotation resources acrossword alignments has been explored in several studies(Yarowsky and Ngai, 2001; Hwa et al, 2005; Smithand Smith, 2004).
Currently, our annotation inter-face is configured for correcting projected POS tag-ging for Chinese.
The visualization tool aggregatesthe annotators?
work, takes various statistics, and vi-sually displays the aggregate information.
Our goalis to aid the researchers conducting the experimentto identify noise in the annotations as well as prob-lematic constructs for which the guidelines shouldprovide further clarifications.Our longer-term plan is to use this framework tosupport active learning (Cohn et al, 1996), a ma-chine learning approach that aims to reduce the num-ber of training examples needed by the system whenit is provided with more informative training exam-69ples.
We believe that through a combination of an in-tuitive annotation interface, a visualization tool thatchecks for style and quality consistency, and appro-priate active learning techniques, we can make su-pervised training more effective for developing mul-tilingual applications.2 Annotation InterfaceOne way to acquire annotations quickly is to appealto users across the Internet.
First, we are more likelyto find annotators with the necessary qualifications.Second, many more users can work simultaneouslythan would be feasible to physically host in a lab.Third, having many users annotate the same dataallows us to easily identify systematic problems aswell as spurious mistakes.
The OpenMind Initiative(Stork, 2001) has had success collecting informationthat could not be obtained from data mining toolsor with a local small group of annotators.Collecting data from users over the Internet in-troduces complications.
Since we cannot ascertainthe computer skills of the annotators, the interfacemust be easy to use.
Our interface is a JAVA ap-plet on a webpage so that it is platform indepen-dent.
An online tutorial is also provided (and re-quired for first-time users).
Another problem of so-liciting unknown users for data is the possibility ofreceiving garbage data created by users who do nothave sufficient knowledge or are maliciously enteringrandom input.
Our system minimizes this risk inseveral ways.
First, new users are required to workthrough the tutorial, which also serves as a shortguide to reduce stylistic differences between the an-notators.
Second, we require the same data to belabeled by multiple people to ensure reliability, andresearchers can use the visualization tool (see Section3) to compare the agreement rates between annota-tors.
Finally, our program is designed with a filter formalicious users.
After completing the tutorial, theuser is given a randomly selected sample sentence(for which we already have verified alignments) toannotate.
The user must obtain an F-measure agree-ment of 60% with the ?correct?
alignments in orderto be allowed to annotate sentences.1Because word alignment annotation is a useful re-source for both training and testing, quite a few in-terfaces have already been developed.
The earliest1The correct alignments were performed by twotrained annotators who had an average agreement rateof about 85%.
We chose 60% to be the figure of meritbecause this level is nearly impossible to obtain throughrandom guessing but is lenient enough to allow for the in-experience of first time users.
Automatic computer align-ments average around 50%.is the Blinker Project (Melamed, 1998); more re-cent systems have been released to support more lan-guages and visualization features (Ahrenberg et al,2003; Lambert and Castell, 2004).
2 Our interfacedoes share some similarities with these systems, butit is designed with additional features to support ourexperimental goals of guideline development, activelearning and resource projection.
Following the ex-perimental design proposed by Och and Ney (2000),we instruct the annotators to indicate their level ofconfidence by choosing sure or unsure for each align-ment they made.
This allows researchers to identifyareas where the translation may be unclear or diffi-cult.
We provide a text area for comments on eachsentence so that the annotator may explain any as-sumptions or problems.
A hidden timer records howlong each user spends on each sentence in order togauge the difficulty of the sentence; this informationwill be a useful measurement of the effectiveness ofdifferent active learning algorithms.
Finally, our in-terface supports cross projection annotation.
As aninitial study, we have focused on POS tagging, butthe framework can be extended for other types ofresources such as syntactic and semantic trees andcan be configured for languages other than Englishand Chinese.
When words are aligned, the knownand displayed English POS tag of the last Englishword involved in the alignment group is automati-cally projected onto all Chinese words involved, buta drop-down menu allows the user to correct this ifthe projection is erroneous.
A screenshot of the in-terface is provided in Figure 1a.3 Tools for ResearchersGood training examples for NLP learning systemsshould have a high level of consistency and accuracy.We have developed a set of tools for researchers tovisualize, compare, and analyze the work of the an-notators.
The main interface is a JAVA applet thatprovides a visual representation of all the alignmentssuperimposed onto each other in a grid.For the purposes of error detection, our systemprovides statistics for researchers to determine theagreement rates between the annotators.
The metricwe use is Cohen?s K (1960), which is computed for ev-ery sentence across all users?
alignments.
Cohen?s Kis a measure of agreement that takes the total prob-ability of agreement, subtracts the probability theagreement is due to chance, and divides by the max-imum agreement possible.
We use a variant of the2Rada Mihalcea maintains an alignment resourcerepository (http://www.cs.unt.edu/~rada/wa) thatcontains other downloadable interface packages that donot have companion papers.70(a) (b)Figure 1: (a) A screenshot of the word alignment user-interface.
(b) A screenshot of the visualization toolfor analyzing multiple annotators?
alignments.equation that allows for having three or more judges(Davies and Fleiss, 1982).
The measurement rangesfrom 0 (chance agreement) to 1 (perfect agreement).For any selected sentence, we also compute for eachannotator an average pair-wise Cohen?s K against allother users who aligned this sentence.3 This statisticmay be useful in several ways.
First, someone with aconsistently low score may not have enough knowl-edge to perform the task (or is malicious).
Second,if an annotator received an unusually low score fora particular sentence, it might indicate that the per-son made mistakes in that sentence.
Third, if there istoo much disagreement among all users, the sentencemight be a poor example to be included.In addition to catching individual annotation er-rors, it is also important to minimize stylistic incon-sistencies.
These are differences in the ways differentannotators (consistently) handle the same phenom-ena.
A common scenario is that some function wordsin one language do not have an equivalent counter-part in the other language.
Without a precise guide-line ruling, some annotators always leave the func-tion words unaligned while others always group thefunction words together with nearby content words.Our tool can be useful in developing and improvingstyle guides.
It highlights the potential areas thatneed further clarifications in the guidelines with anat-a-glance visual summary of where and how the an-notators differed in their work.
Each cell in the gridrepresents an alignment between one particular wordin the English sentence and one particular word inthe Chinese sentence.
A white cell means no one pro-posed an alignment between the words.
Each coloredcell has two components: an upper green portion in-3not shown in the screenshot here.dicating a sure alignment and a lower yellow portionindicating an unsure alignment.
The proportion ofthese components indicates the ratio of the numberof people who marked this alignment as sure to thosewho were unsure (thus, an all-green cell means thateveryone who aligned these words together is sure).Moreover, we use different saturation in the cells toindicate the percentage of people who aligned thetwo words together.
A cell with faint colors meansthat most people did not chose to align these wordstogether.
Furthermore, researchers can elect to viewthe annotation decisions of a particular user by click-ing on the radio buttons below.
Only the selecteduser?s annotation decisions would be highlighted byred outlines (i.e., only around the green portions ofthose cells that the person chose sure and around theyellow portions of this person?s unsure alignments).Figure 1b displays the result of three annotators?alignments of a sample sentence pair.
This sentenceseems reasonably easy to annotate.
Most of the col-ored cells have a high saturation, showing that theannotators agree on the words to be aligned.
Mostof the cells are only green, showing that the anno-tators are sure of their decisions.
Three out of thefour unsure alignments coincide with the other an-notators?
sure alignments, and even in those cases,more annotators are sure than unsure (the green ar-eas are 2/3 of the cells while the yellow areas are1/3).
The colored cells with low saturation indicatepotential outliers.
Comparing individual annotator?salignments against the composite, we find that oneannotator, rh, may be a potential outlier annota-tor since this person generated the most number oflightly saturated cells.
The person does not appearto be malicious since the three people?s overall agree-ments are high.
To determine whether the conflict71arises from stylistic differences or from careless mis-takes, researchers can click on the disputed cell (across will appear) to see the corresponding Englishand Chinese words in the text boxes in the top andleft margin.Different patterns in the visualization will indicatedifferent problems.
If the visualization patterns re-veal a great deal of disagreement and unsure align-ments overall, we might conclude that the sentencepair is a bad translation; if the disagreement is local-ized, this may indicate the presence of an idiom ora structure that does not translate word-for-word.Repeated occurrences of a pattern may suggest astylistic inconsistency that should be addressed inthe guidelines.
Ultimately, each area of wide dis-agreement will require further analysis in order todetermine which of these problems is occurring.4 Conclusion and Future WorkIn summary, we have presented an annotation envi-ronment for acquiring word alignments between En-glish and Chinese as well as Part-Of-Speech tags forChinese.
The system is in place and the annotationprocess is underway.4Once we have collected a medium-sized corpus, wewill begin exploring different active learning tech-niques.
Our goal is to find the best way to assignutility scores to the as-of-yet unlabeled sentences inorder to obtain the greatest improvement in wordalignment accuracy.
Potential information useful forthis task includes various measurements of the com-plexity of the sentence such as the rate of (auto-matic) alignments that are not one-to-one, the num-ber of low-frequency words, and the number of po-tential language divergences (for example, many En-glish verbs are nominalized in Chinese), and the co-occurrence of word pairs deemed to be unsure by theannotators in other contexts.
Furthermore, we be-lieve that the aggregate visualization tool will alsohelp us uncover additional characteristics of poten-tially informative training examples.ReferencesLars Ahrenberg, Magnus Merkel, and Michael Petterst-edt.
2003.
Interactive word alignment for language en-gineering.
In Proceedings from EACL 2003, Budapest.Christopher Callison-Burch, David Talbot, and Miles Os-borne.
2004.
Statistical machine translation with4The annotation interface isopen to public.
Please visithttp://flan.cs.pitt.edu/~hwa/align/align.htmlword- and sentence-aligned parallel corpora.
In Pro-ceedings of the Annual Meeting of the Association forComputational Linguistics, July.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Meas., 20:37?46.David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-dan.
1996.
Active learning with statistical models.Journal of Artificial Intelligence Research, 4:129?145.M.
Davies and J. Fleiss.
1982.
Measuring agreement formultinomial data.
Biometrics, 38:1047?1051.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Journal of Natural Language Engineering.
To appear.Patrik Lambert and Nuria Castell.
2004.
Alignmentof parallel corpora exploiting asymmetrically alignedphrases.
In Proc.
of the LREC 2004 Workshop on theAmazing Utility of Parallel and Comparable Corpora,May.Diane Litman and S. Pan.
2002.
Desiging and evaluatingan adaptive spoken dialogue system.
User Modelingand User-adapted Interaction, 12(2/3):111?137.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.I.
Dan Melamed.
1998.
Annotation style guide for theblinker project.
Technical Report IRCS 98-06, Univer-sity of Pennsylvania.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 440?447.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using english toparse korean.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.David G Stork.
2001.
Toward a computational theoryof data acquisition and truthing.
In Proceedings ofComputational Learning Theory (COLT 01).J.
Wiebe.
2002.
Instructions for annotating opinionsin newspaper articles.
Technical Report TR-02-101,University of Pittsburgh, Pittsburgh, PA.Fei Xia, Martha Palmer, Nianwen Xue, Mary EllenOcurowski, John Kovarik, Fu-Dong Chiou, ShizheHuang, Tony Kroch, and Mitch Marcus.
2000.
Devel-oping guidelines and ensuring consistency for chinesetext annotation.
In Proceedings of the Second Lan-guage Resources and Evaluation Conference, Athens,Greece, June.David Yarowsky and Grace Ngai.
2001.
Inducing multi-lingual pos taggers and np bracketers via robust pro-jection across aligned corpora.
In Proceedings of theSecond Meeting of the North American Association forComputational Linguistics, pages 200?207.72
