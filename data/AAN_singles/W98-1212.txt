IlII|l/A Bayes ian  Approach  to  Automat ing  Argumentat ionR ichard  McConachy ,  Kev in  B. Korb  & Ingr id  ZukermanDepartment of Computer Science, Monash UniversityClayton, Victoria 3168, AUSTRALIA{r icky ,  korb ,  ingr id}~cs  .monash.
edu.
auAbst rac tOur argumentation system NAG uses Bayesiannetworks in a user model and in a normativemodel to assemble and assess nice arguments,that is arguments which balance persuasive-ness with normative correctness.
Attentionalfocus is simulated in both models to selectrelevant subnetworks for Bayesian propaga-tion.
Bayesian propagation in the user modelis modified to represent some human cognitiveweaknesses.
The subnetworks are expanded inan iterative abductive process until argumen-tative goals are achieved in both models, whenthe argument is presented to the user.1 In t roduct ion"The St. Kilda Football Club will probably win thegrand final in 1997.
They are likely to do so be-cause they have won at least as many games asany other side during the regular season, they havescored more points than any other team this yearso far, and they have few unavailable players."
Howstrongly should St. Kilda's good form in the laterpart of the 1997 season be taken as a premiershipindicator?
Is it better to lose narrowly to a strongside or narrowly defeat a weak side?
Most argu-ments, like the St. Kilda example above, must beformed using information that is both incompleteand uncertain.This paper focuses upon the Bayesian underpin-nings of our argument generation-analysis system,NAG (Nice Argument Generator).
The goal of oursystem is to generate nice arguments.
A nice ar-gument  employs normatively strong inferences fromaccepted premises while also being persuasive for thetarget audience.The main modules of NAG are shown in Figure 1.The Generator (Section 3) uses semantic activationto quickly form the initial Argument Graph for anargument, or to quickly extend an already existingt ,Re-nin l LAromentGenerator I Agents Analyzer.
ArgumentGoal Analysis .~"~Propos i t ion  aa lys i s~/ /Goal _ I ArgumentProposition r StrategistArgument I User Argument/InquiryUSERFigure 1: System ArchitectureArgument Graph.
An Argument Graph is a net-work structure with nodes that represent proposi-tions, and connecting links that represent the infer-ences that connect these propositions.
An ArgumentGraph is fleshed out by consulting several sources ofinformation called Reasoning Agents and incorporat-ing the relevant inferences and propositions returnedby these sources into the Argument Graph.
This Ar-gument Graph is passed to the Strategist.The Strategist decides what NAG should do next:call the Generator to continue the argument buildingprocess; call the Analyzer (Section 4) to estimatehow nice the current Argument Graph is; or presentan argument based on the current Argument Graphto the user for inspection and response.The Strategist will pass the current ArgumentGraph to the Analyzer at least once before the argu-ment is presented to the user, and often more thanonce.
To estimate the persuasive power of an argu-ment represented by an Argument Graph, the An-alyzer consults a revisable user model that reflectsthe beliefs and cognitive abilities of the audience.The Analyzer uses a normative model to gauge thenormative strength of an argument.
Belief updatingin both the user and the normative model is doneby a constrained Bayesian propagation scheme (Sec-McConachy, Korb and Zukerman 91 A Bayesian Approach to Automating ArgumentationRichard McConachy, Kevin B. Korb and Ingrid Zukerman (1998) A Bayesian Approach to Automating Argumentation.
ID.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural LanguageLearning, ACL, pp 91-100.tion 4).
In the user model, Bayesian updating is ad-justed by multiplicative factors which model threehuman cognitive weaknesses (Section 4.1).If the Analyzer detects problems with the Argu-ment Graph it highlights the weaknesses for the Gen-erator to fix.
In this way a cycle of alternately cri-tiquing and extending the graph is continued untila successful Argument Graph is built, or NAG isconfronted with an event hat prevents it from con-tinuing, such as the Generator failing to find relevantnew evidence or the Strategist noticing that the al-lowed amount of time has run out.2 Re la ted  ResearchCharniak and Goldman (1993) describe a Bayesianplan recognition system that uses marker passing asa method of focusing attention on a manageable por-tion of the space of all possible plans.
This is similarto the way in which NAG uses spreading activationto focus on a small portion of the available data whengenerating arguments.The approach of "interpretation as abduction"used in (Hobbs et al, 1993) aims to recover thepremises and inferential links which lead to the con-clusion of some given argument.
This is similar toNAG's argument analysis.
There are two importantdifferences between NAG and the work by Hobbs etal.
: NAG is a system that reasons under uncertainty,and NAG performs both analysis and generation.
Agenerative system based on the work of Hobbs et alis described in (Thomason et al, 1996).
This sys-tem deals with what can be readily inferred, and sodeleted, during communication, but the generateddiscourse does not present an argument in support ofa proposition.
Two systems that can turn an exist-ing fully explicit argument into an enthymematic oneare described in (Horacek, 1994) and (Mehl, 1994).However, both of these systems require a completeargument as input, unlike NAG, which constructsits own.3 Argument  Generat ionThe Generator receives the following inputs: (1) aproposition to be argued for; (2) an initial argumentcontext; (3) two target ranges of degrees of belief tobe achieved (one each for the normative model andthe user model); and (4) a system attitude param-eter, which determines the extent o which the sys-tem will take advantage of the user's erroneous be-liefs.
1 An argument context is the set of propositions1The results in this paper are for a "normal" attitude,which allows a small departure from what is normativelycorrect.and concepts appearing in the discussion precedingthe argument or in the current Argument Graph.The degrees of belief to be achieved are expressedas ranges of probabilities, e.g., \[0.5, 0.6\], in order tobe able to represent a variety of goals, e.g., inducingindifference or assent.The Generator produces an Argument Graphwhich starts from admissible premises and ends inthe goal proposition.
Admissible premises are nor-matively acceptable propositions that are either al-ready believed by NAG and the user (sufficientlyfor the argument o work) or are assented to bythe user (e.g., sourced from an accepted referencework).
When constructing an argument, he Gener-ator relies on two collections of information: a nor-mative model composed of different types of Knowl-edge Bases (KBs) and a user model also composedof different ypes of KBs which represent the user'spresumed beliefs and inferences.
A single KB rep-resents information in one format, e.g., a semanticnetwork (SN), Bayesian network (BN), rule-basedsystem, or database.
During the argument gener-ation process, relevant material from several of theKBs may need to be combined into a common rep-resentation.
We have chosen BNs for this purposebecause of their ability to represent ormatively cor-rect reasoning under uncertainty, and because alter-ing the normal Bayesian propagation rules allows usto model some human cognitive phenomena.When assembling an Argument Graph, NAG de-velops two BNs: the BN forming one of the KBsin the user model, and the BN forming one of theKBs in the normative model.
As arguments are builtup, material obtained from other KBs may be con-vetted to BN form and added to the appropriateBN, e.g., material from a rule-based system in thenormative model may be added to the normativeBN (Section 3.4).
In order to reduce the amount ofinformation NAG must deal with, a focusing mech-anism is applied to highlight he portion of the com-plete BN in each model needed for the current argu-ment (Section 3.2).
Hence, both the user model andthe normative model will contain a single Bayesiansubnetwork that is under focus.
The structural in-tersection of these Bayesian subnetworks form theArgument Graph.
When analyzing the current Ar-gurnent Graph, propagation is performed twice, onceover the Bayesian subnetwork in the user model andonce over the Bayesian subnetwork in the norma-tive mode, each time using probabilistic informa-tion sourced from within the model being propa-gated (Section 4).
Thus, we obtain the strength ofthe same argument in both the user and normativemodels.rMcConachy, Korb and Zukerman 92 A Bayesian Approach to Automating ArgumentationIIIIIIIi!iIIIII/II/l/I///3.1 The Generation-Analysis AlgorithmAs previously described, part of NAG's input is aninitial argument context, contexto, which is definedby the propositions and concepts mentioned in thepreamble to the argument plus the argument's goal.Having established the initial context, NAG acti-vates the following algorithm:Generation-Analysis Algorithm1.
i +-- 0.2.
Clamp any items related to the current con-text, contexti, and perform spreading activa-tion.
This results in an Argument Graph con-taining: the clamped nodes, the activated nodes(those whose activation exceeds a threshold),plus the links connecting the nodes.
(Section 3.2)3.
Identify new subgoals in the current ArgumentGraph and tag them for further investigation.
(Section 3.3)4.
Pass each of the argument subgoals identified inthe previous tep to the Reasoning Agents, andadd the new information returned by the Rea-soning Agents to the current Argument Graph.
(Section 3.4)5.
Pass the current Argument Graph to the Ana-lyzer for evaluation.
(Section 4)6.
If the Analyzer eports that the current Argu-ment Graph is sufficiently nice, then presentan argument based on the current ArgumentGraph to the user, and wait for a response.
(Section 5)/A~ Higher level / / / \  /conceptsLower level / / I ~ 'rocks'concepts like / / ~ /'quartz phenocrysts' / / /~/ ~ - - - - - - - ' - ' - -~- Semantic \ / ?
/' Networko 2nd layerSemantic / \ ~ ~o/  1~ ~ ~Net work--------Tz-~.~/'~ / ~/ l~ ~ Z1st layer \[\] ~ j o/ / /  .
/o  / ~q~ /........._Bayesiano/~- -' '-  ~ NetworkProposition, e.g., \[Porphyry copper deposit exists at Mineral Park\]Figure 2: Semantic and Bayesian Networks7.
i+- i+ l .8. contexti e-- contexti_l + new nodes connectedto the goal during the cycle i-1.9.
Go to Step 2.3.2 Focusing the ArgumentBayesian network propagation (Pearl, 1988), op-timized or otherwise (see, for example, Li &D'Ambrosio, 1994), is an NP-hard problem in thegeneral case (Cooper, 1990).
NAG is designed tobe an interactive system, potentially drawing uponvery large knowledge bases, so complete propaga-tion over large BNs would be too slow.
In addition,NAG's user model is designed to model human cog-nitive abilities, and humans normally cannot absorband analyze all data relevant o a complex prob-lem.
To cope with both of these limits on complexitywe emulate the principal means available to humansfor applying limited cognitive capacity to problemsolving, namely attention (see, for example, Baars,1987).NAG uses two hierarchical SNs, one built on top ofthe user model BN and one built on top of the nor-mative model BN, to capture connections betweenthe items mentioned in the discourse.
Figure 2 illus-trates one such semantic-Bayesian 'pyramid'.
TheSN (upper levels of the pyramid) and the BN (baseof the pyramid) are used by NAG to simulate atten-tional focus in each model.
NAG takes the context inwhich theargnment occurs as providing an initial setof salient objects.
For example, if the user presentsan argument to NAG, the concepts occurring in thepropositions within the argument or in the precedingMcConachy, Korb and Zukerman 93 A Bayesian Approach to Automating Argumentationdiscussion will be marked as salient.
We use acti-vation with decay (Anderson, 1983), spreading fromthe salient Objects (which are clamped) to determinethe focus of attention.
All items in the semantic-Bayesian networks which achieve a threshold acti-vation level during the spreading activation processaxe brought into the span of attention.
The spread-ing activation process passes activation through thepyramidal semantic-Bayesian networks, each nodebeing activated to the degree implied by the activa-tion levels of its neighbors, the strength of associa-tion to those neighbors, and its immediately prioractivation level (vitiated by a time-decay factor).By these means we have a direct implementationof attention which we use to zero-in upon the moreuseful portions of the pyramidal semantic-Bayesiannetworks.
This iterative process ceases when an ac-tivation cycle fails to activate any new node.Determining the more useful portions of the pyra-midal semantic-Bayesian networks in this way allowsNAG to save processing time in two ways: NAG canrestrict itself to searching for information connectedwith only the most relevant propositions (the ones infocus, see Section 3.3), rather than all of the propo-sitions known to the system; and similarly, NAG cananalyze its arguments with respect o just the samerelevant propositions, saving time in the Bayesianpropagation procedure (Section 4).3.2.1 Focusing ExampleConsider the generation of an argument for theproposition "A porphyry copper deposit exists atMineral Park, Arizona," preceded by the preamble"A porphyry copper deposit derives its name from aporphyritic stock located at the center of the min-eral deposit.
A stock is a cylindrical igneous massworking its way up through the earth's crust.
Theword porphyritic describes the bimodal nature of theminerals within the stock: some of these mineralsaxe large and the rest are microscopic.
For example,porphyritic granite at Mineral Park usually containslarge crystals (phenocrysts) of feldspar and smallgrains of quartz."
(this example is adapted fromDuda et al, 1978).
Initially, the goal propositionand the preamble activate any propositions contain-ing one or more of the italicized concepts, i.e., nodesN1-N4, Ns-Ns, and NI~ in Figure 3 (shown in greyboxes).After clamping the nodes that correspond to thisdiscourse context and performing spreading acti-vation, additional nodes become activated in thesemantic-Bayesian networks.
All nodes whose ac-tivation level exceeds a threshold are retained andadded to the Argument Graph.
For this example,this yields an Argument Graph composed of twoargument fragments: (1) N1-Ns, Ns, Ns, Ns andNll-Nls; and (2) N4, N7 and Nlo (Figure 3).3.3 Choosing Argument  SubgoalsHaving used the semantic priming process to additems of likely interest o the current ArgumentGraph, NAG must now decide which of these newlyadded items should be set as argument subgoals re-quiring further inspection.
At present, all nodes con-nected to the goal in the current Argument Graphthat have not been previously passed to the Reason-ing Agents axe tagged as subgoals to be investigated(Section 3.4).We are currently looking at methods for reduc-ing the list of current subgoals.
One technique forachieving this is to choose a subset of the currentlist of subgoals that seems to offer good potentialfor adding new support o the overall argument goalwith minimal additional effort.
A node is deemedto have a high potential for support if changes in itsvalue would cause a large change in the final proba-bility of the goal node after propagation (Section 4),and the node has not previously been passed to theReasoning Agents (in which case NAG would havealready examined the potential contribution of thisnode).
This scheme of selecting only some of theavailable options for further inspection by the Rea-soning Agents has not yet been implemented.3.3.1 Subgoals - Example ContinuedSince none of the nodes in the current ArgumentGraph (Figure 3) have been passed to the Reason-ing Agents at this point, all of the nodes connectedto the goal in the Argument Graph are tagged assubgoals.
In this example, the nodes N1-Ns, Ns,Ns, Ns, Nu and N12 are tagged as subgoals, andtogether with the goal node, NlS, are passed to theReasoning Agents in order to obtain additional in-formation (Section 3.4).
The nodes that form thesecond fragment shown in Figure 3 - nodes N4, N7and N10 - are kept, but are not expanded upon atthis stage.3.4 Extending the Argument GraphThe initial Argument Graph consists of the subsetof the BNs which was activated by the attentionalmechanism.
The Generator then activates the Rea-soning Agents to collect information relevant to eachsubgoal in the current Argument Graph.
Duringthe process of adding this new information to theArgument Graph, the Generator must determine:(1) which of the newly returned inferences should beintegrated into the Argument Graph; (2) the struc-I/I1//////IMcConachy, Korb and Zukerman 94 A Bayesian Approach to Automating Argumentation ?i!1!1IIIIIIIIIIIIIIIIIIIIIIIII!I85 I Favo~groumN3'no're I I  vo?b'e system in regionaltarget area environmentN4~,0regionalenvironmentFigure 3: Initial Argument Graph for the Porphyry Copper Deposit Exampleture of the additions to the Argument Graph repre-senting the new inferences; and (3) the parametersof the new inferences and propositions.Which propositions and inferences to inte-grate.
New propositions returned by the ReasoningAgents are added to the current Argument Graph asnew nodes.
NAG decides whether to introduce newinferences returned by the Reasoning Agents intothe Argument Graph (or to replace existing infer-ences with new ones) by applying two simple rulesdesigned to ensure that each relationship betweenpropositions in the Argument Graph is representedonly once.
The two rules state that:1.
At most only one inference may directly connectany two propositions in the current Bayesiansubnetworks in each of the user model and thenormative model.2.
When selecting from multiple candidate infer-ences, preference is given to inferences ourcedfrom more expressive representations, whereexpressiveness means how much probabilisticinformation, including joint interactions withother factors, can be expressed by the represen-tation.For example, assume NAG's qualitative rule-basedsystem agent finds a rule stating "If D then E is pos-sible."
If the agent responsible for quantitative rule-based systems also finds a rule "If D then E withprob = x," which NAG translates into D ev~-~ceEwith P(EID ) = z (assuming independence fromother links incident upon node E), then which ofthese inference rules, if any, should be added into theArgument Graph?
The first rule above states thatat most one of these two inferences will be incor-porated into the current Argument Graph.
2 NAGselects which one of the two inferences it will incor-porate by applying the second rule.
NAG imple-ments the second rule via the following preferenceordering for expressiveness: BNs, quantitative rule-based systems, qualitative rule-based systems andfinally database lookups.
The expectation is thatthe representations higher in the list, e.g., BNs andquantitative rule-based systems, will usually providemore complete and accurate probabilistic informa-tion about an inference or proposition than represen-tations lower in the list, e.g., qualitative rule-basedsystems.
Sometimes this expectation proves false.For example, if both a quantitative rule-based sys-tem and a BN return a probability estimate for aparticular proposition being true, it is possible thatthe value returned by the rule-based system will bethe more accurate one, perhaps because the BN hasnot been fully propagated (and hence the probabilityfor the proposition ot updated to incorporate all ofthe available information).
However, initial testingsuggests that NAG can often do a good enough job,i.e., produce nice enough arguments, using our twosimple information lossy rules.Structural  form of  the new propos i t ions  andinferences.
The various Reasoning Agents return2NAG does not try to merge information gleaned frommore than one available source since it is unclear how todo so.McConachy, Korb and Zukerman 95 A Bayesian Approach to Automating Argumentationargument fragments which take the form of propo-sitions linked by inferences and semantic relations.After the two rules mentioned previously in  thissection have been applied to determine which ofthese fragments hould be incorporated in the Ar-gument Graph, the selected fragments are added tothe Bayesian subnetwork in the appropriate model,e.g., fragments ourced from KBs in the normativemodel will be added to the subnetwork in the nor-mative model.
Inferences in the fragment which aregoing to replace inferences already in the subnet-work (the new inference is from a more expressiveKB than the one currently in the subnetwork, seerule two described previously) are handled in themanner described below.Adding parameters for the propositions andinferences.
Incorporating new information aboutthe prior probability of a proposition into the Argu-ment Graph requires that the information returnedby a Reasoning Agent be copied to the node in theArgument Graph which corresponds to the KB datapoint from which the information was sourced.
Forexample, information about priors sourced from aKB in the user model is copied to the correspond-ing node in the Bayesian subnetwork belonging tothe user model.
Similarly, information about pos-sible instantiation values for a proposition are alsocopied to the appropriate proposition in the correctmodel, e.g., \[Fred is a smoker\] may be instantiatedas TRUE (1.0) or FALSE (0.0).
This works so longas the new values are filling gaps in the ArgumentGraph.
However, if the current Argument Graphalready contains a prior probability or instantiationvalue for the proposition under consideration, thenthat previous value will be retained and the new in-formation ignored.
"Adding information to the Argument Graphabout joint conditional probabilities associated withnew inferences i a harder task.
If a Reasoning Agentcan provide complete conditional probability infor-mation for a new inference which takes into accountother inferences that impinge upon the propositiontargeted by this inference, then this information isadded to the conditional probability matrix for thenode containing this proposition in the ArgumentGraph.
However, complete probabilistic nformationis frequently not available to NAG, in which case thenew information (often a simple conditional proba-bility) is assumed to be conditionally independentof the other inferences impinging upon the node inquestion.
Since assuming conditional independencewithout a good understanding of the domain is dan-gerous, NAG reports this assumption to its operatorinterface.
This is done so that a human operator can,Ng \[ Favourablelevel oferosionN121 Favourableregional, environment/N13\[ Porphyry I copper deposit at Mineral ParkFigure 4: Results of node N12 being sent to the Rea-soning Agentsthen or later, spot where NAG is most likely to have"gone wrong" should one of its arguments appearincorrect.
The operator can then edit NAG's KBsto remove the offending inference or to add extra in-formation about the joint conditional probabilities.3.4.1 Extending the Argument Graph -Example ContinuedThe information returned by the ReasoningAgents is incorporated into the Argument Graph.Some of this information will be included in thefinal Argument Graph presented to the user, e.g.,the newly found node N9 and the link connectingN9 --+ N12 shown in Figure 4 (nodes NI~ and N13and the link between them were already containedin the current Argument graph).
Other fragmentsmay also be created uring this process, e.g., \[pres-ence of igneous rocks\] ev~_~ce \[history of high temperatureand pressure conditions\], but are eventually excludedbecause of their irrelevance to the goal.Node N9 and the link N9 ---r N12 were returnedwhen the Reasoning Agents were passed node Nlzfor inspection (recall that node N12, like all of thenodes in the left hand fragment of Figure 3, was pre-viously set as a subgoal to be investigated further).In this example, node N12 is the only subgoal shownin Figure 3 about which NAG could find new infor-mation.4 Argument AnalysisThe process of computing the anticipated belief ina goal proposition as a result of presenting an argu-ment starts with the belief in the premises of theArgument Graph and ends with a new degree ofbelief in the goal proposition.
The Analyzer com-putes the new belief in a proposition by combiningthe previous belief in it with the result of applyingthe inferences which precede this proposition in theMcConachy, Korb and Zukerman 96 A Bayesian Approach to Automating ArgumentationIIIIIIIIIIIIIIiIIIiI!II!1IiIII!mArgument Graph.
This belief computation processis performed by applying propagation procedures tothe Bayesian subnetwork corresponding to the cur-rent Argument Graph in the user model and sepa-rately to the subnetwork corresponding to the cur-rent Argument Graph in the normative model.In propagating only over the subnetworks initiallyseeded by the focusing mechanism (Section 3.2) andextended with information returned by the Reason-ing Agents (Section 3.4), NAG is ignoring thoseparts of the complete BNs in the user and norma-tive models not deemed relevant o the current ar-gument.
Propagating over the subnetwork corre-sponding to the current Argument Graph in eachof the user model and the normative model is muchfaster than having to perform propagation over thecomplete BN in each model, but the trade off is aless accurate stimate of the final belief in the goalproposition.
Nonetheless, in a system designed to beinteractive, some such trade off is necessary in viewof the complexity of Bayesian propagation.After propagation, the Analyzer eturns two mea-sures of an argument's strength: normative strength,which is the effect of an argument on the belief in thegoal proposition in the normative model, and effec-tiveness, which is its effect on the user's belief in thegoal proposition.
Of course, the effectiveness of anargument presented to a user may be quite differentfrom its normative strength.
When determining anargument's anticipated effect upon a user, NAG cur-rently takes into account hree cognitive rrors thathumans frequently succumb to: belief bias, overcon-fidence and the failure to use base rate information(Section 4.1).After the Analyzer has evaluated the normativestrength and effectiveness of the Argument Graphit will return an assessment.
This assessment maypoint out flaws in the argument structure such as in-sufficient support for some propositions within theArgument Graph and reasoning loops.
At presentflaws are fixed in the order they are found.
The de-termination of a preferred ordering in which to ad-dress flaws in an argument is left for future research.In the case of an insufficiently supported premise,the target premise is set as a new argument subgoal.This new subgoal is then submitted to the Genera-tor with the currently active subnetworks as its con-text, in order to generate a new subargnment.
Afterintegrating the new subargument into the previoussubnetworks, the now enlarged Argument Graph isagain sent to the Analyzer for inspection.
Henceby allowing NAG to complete additional focusing-generation-analysis cycles, Argument Graphs thatare initially unsatisfactory can often be improved.
InMcConachy, Korb and Zukerman 97more extreme cases where additional cycles do notgather any new evidence, altering the parameters ofthe semantic priming procedure, .g., increasing theinitial clamping value or decreasing the rate of de-cay, may help NAG identify a new subgoal that canbe profitably examined by the Reasoning Agents.4.1 Modeling Human WeaknessesDuring belief updating in the user model, multiplica-tire factors are incorporated into the Bayesian up-date formulas (Neapolitan, 1990) to model the hu-man cognitive weaknesses of belief bias, overconfi-dence and the base rate fallacy (for a fuller descrip-tion see Korb et al, 1997).
In future we shall modeladditional prominent features of human inference.Bel ief  bias is the assessment of an inference asbeing stronger (weaker) than it is normatively be-cause it supports (undermines) an existing belief(Evans~ 1989).
To model this effect we employ afunction which given the user's prior degree of be-lief in a proposition provides a multiplicative factorused in any update to that belief.
Should the user'sprior belief be 0.5, then the multiplicative factor is1, so that the belief is updated normally.
An ex-treme prior disbelief on the other hand suppressesthe impact of supporting evidence via a low multi-plier, whereas a strong prior belief enhances that im-pact via a factor greater than 1.
As a result, NAGtends to assume that users will require more andbetter arguments to be persuaded to change theirstrongly held beliefs than should be necessary nor-matively.Due to overconfidence people tend to exagger-ate the probability of very likely events and the im-probability of very unlikely events (Lichtenstein etaL, 1982).
NAG uses this bias to select prior prob-abilities.for propositions in the user model when ithas not been given explicit information about theseprobabilities, but only frequency information alsoknown to the user.
NAG applies an S-curve to con-vert frequencies into user probabilities (Figure 5).For example, a very low frequency will be mappedinto an even smaller prior probability.
NAG doesnot use this S-curve directly when the base rate fal-lacy applies, since in that case both errors are dealtwith at once.The  base rate  fallacy is the tendency peoplehave to ignore objective prior probabilities basedon frequency data available to them and to replacethese probabilities with a uniform probability distri-bution.
A striking example of this is the cab problemdescribed in (Tversky and Kahneman, 1982).A cab was involved in a hit and run ac-cident at night.
Two cab companies, theA Bayesian Approach to Automating Argumentation1.0Belief0.50.
( .0 0'.5FrequencyFigure 5: Overconfidence CurveGreen and the Blue, operate in the city.You are given the following data:?
85% of the cabs in the city are Greenand 15% are Blue.
* A witness identified the cab as Blue.The court tested the reliability of thewitness under the same circumstancesthat existed on the night of the acci-dent and concluded that the witnesscorrectly identified each one of the twocolors 80% of the time and failed 20%of the time.By Bayes theorem the probability that the cab wasBlue is 0.41 based on the evidence given.
This is be-cause the low prior probability of Blue Cabs (0.15)dominates the computation, z Nevertheless, mostpeople presented with the story respond that theprobability that the offending cab was blue is 0.8, aresponse that would be correct if the prior probabil-ities were uniformly distributed among the availableoptions.
This is characteristic of most people's re-sponse to many situations involving uncertainty - -they flatten all prior probabilities.
In order to ac-commodate this tendency, NAG computes implicitprior probabilities in the user model using a func-tion that flattens the middle of the belief-frequencygraph, thereby reflecting this bias towards uniformpriors.Using the odds-likelihood ratio method (Neapoli-tan, 1990) with the additional multiplicative factorsas described above, we obtain the following updateformula:O(N. , .
, IN , ,N2 , .
.
.
,N . )
= O(N.. .
, )t lx HA i  x belbiasi x overconf~ x baseratei,i= lwhere the three biasing factors for each inference3The computation being (.8 x .15)/(.8 x .15+ .2 x .85)= 0.41.are displayed separately, the evidence nodes Ni areconditionally independent given the "causal" nodeNne~, and the likelihood ratio is computed as Ai =P(Ni\[Nnew)/P(N~\[-~N~e~).
The multiplicative fac-tors for each of belbiasl, overconfi and baseratea axeeither 1.0, if the particular cognitive weakness i notactive for this inference, or derived as above if theweakness i active.
4 The propagation rule for uncer-tain evidence and the more general formulae usedfor propagation i singly connected causal networksgiven in (Neapolitan, 1990) are similarly modifiedby means of the three multiplicative factors.4.1.1 Analyzing the  ExampleThe argument that can be built at this stage hastwo main branches: from nodes N1-Nz to node N13via Nn (Figure 3); and from node N9 to NI3 viaN9 --+ N12 -+ N13 (Figure 4).
According to the usermodel, the user is believed to attach great strengthto the inferences N1 --+ N5 ~ Ns and N3 ---> N6 --+Ns, so propagation i  the user model leads to ananticipated final belief in the goal node within thetarget range.
However, the anticipated final beliefin the goal node in the normative model falls shortof the desired normative range.NAG continues through Steps 7-9 of theGeneration-Analysis algorithm, putting the newlyadded nodes connected to the goal node in the Argu-ment Graph into the current context.
On the nextcycle through the algorithm (Step 2) a new roundof semantic priming is performed.
However, in thecurrent example this does not lead to any furthernodes being connected to the argument goal.
As aconsequence, during Step 3 only one new subgoal istagged, node Ng, which was connected to the goalduring the last cycle of the algorithm.
When nodeN9 is then passed to the Reasoning Agents in Step 4,the new link Nlo -~ N9 is found and added to theArgument Graph.
The resulting Argument Graphis returned to the Analyzer again (Step 5), whichdetermines that the anticipated belief in the goal isnow within the target ranges in both models.The subgraph corresponding to a complete (non-enthymematic) argument generated for this exam-ple appears in Figure 6.
Node N2 is omitted be-cause of its weak contribution to the goal, as ex-plained in following section.
The focusing and sub-goal setting parts of the system managed to find an4At present, he activation of a cognitive weakness orlack thereof is a tag that accompanies ach inference inthe user model BN.
This will support he implementa-tion of a user model which determines the activation ordeactivation of a cognitive weakness depending on fac-tors such as a user's expertise in a domain or his/herlevel of alertness.McConachy, Korb and Zukerman 98 A Bayesian Approach to Automating ArgumentationIIIIIIllIIllIII!1IIIIIIIIIIIiIIIiIIIIacceptable solution to this example problem withouteven having to fully investigate every proposition inthe  Argument Graph (nodes Na, Nr and Nlo werenever passed to the Reasoning Agents), let alneevery proposition in the complete BN in the usermodel and the complete BN in the normative model.Partial propagation over just the subnetworks cor-responding to the Argument Graph in each modelalso ensured that inferences like \[presence of igneousrocks\] ev~_~c?
\[history of high temperature and pressureconditions\], which turned out to be irrelevant for thisargument goal, were not propagated over.5 P resentat ionAfter a successful Argument Graph has been built,so that the current Argument Graph now gives ananticipated belief in the goal proposition within thedesired ranges in both the normative and user mod-els, then comes the problem of how to present heargument to the user.
NAG attempts to minimizethe size of the current Argument Graph by search-ing it for the subargument with the fewest nodeswhich still yields a degree of belief in the goal propo-sition that falls inside the target ranges in both thenormative model and the user model.
During thisprocess, it tries to generate an enthymematic argu-ment by iteratively deleting nodes and invoking theAnalyzer to determine whether the belief in the goalproposition i  the now smaller Argument Graph stillsuffices.
Frequently this process will be able to sim-plify the Argument Graph, since upon completion ofthe focusing-generation-analysis c cles, some of thepropositions in the Argument Graph may be sup-ported more strongly than is necessary for the argu-ment to work.NAG was designed to be a system which gener-ates arguments and receives user arguments.
Atpresent, the front-end of the system is not imple-mented.
However, a graphical interface is currentlybeing constructed which allows the user to build andreceive arguments in an annotated network form.Methods of rendering the system's output in Englishare being investigated.6 Conc lus ionNAG uses a series of focusing-generation-analysis c -cles to build two Bayesian etworks (one in the nor-mative model and another in the user model) thatcontain the information required to construct a niceargument.
Partial propagation, performed over thesubnetworks in focus (the current Argument Graph),is used to estimate the impact of the resultant argu-ment.
Modified Bayesian update rules model threehuman cognitive weaknesses.Any argumentation system must have access to agreat deal of domain specific data if it is to gener-ate and analyze arguments well.
NAG is no excep-tion, and consequently setting up a good domain,one with sufficient depth and richness to test NAGwell, is not trivial.
By allowing NAG to use existingknowledge sources where possible, via small Reason-ing Agents written to match the various knowledgesource types, we have endeavoured to at least par-tially mitigate this problem.NAG has been tested on five sample scenarioswhich generate BNs containing up to about 50nodes.
The use of spreading activation to simulateattention, and the simplifications NAG employs toreduce the time taken to extend and propagate be-liefs through the Bayesian subnetworks, lead to asignificant reduction in argument generation timescompared to trials run with the same BNs not usingthese techniques.
These speed-up methods eem tohave little effect on the resulting arguments.Larger BNs and KBs, which are currently beingbuilt, will enable us to test more conclusively theeffects of our modifications on the speed of the gen-eration process and the quality of the argumentsproduced.
These richer scenarios will also allow usto better test the effects of our modeling of humancognitive weaknesses.
We are currently planning avariety of tests to evaluate the performance of oursystem.
The graphical interface currently under con-struction and an English generator will be used totest the effect of arguments generated by NAG onusers' beliefs.
In addition, the English output willbe used to compare NAG's arguments with thosegenerated by people and to test how the order of pre-sentation of the points in an argument affects users'beliefs.AcknowledgmentsThis work was supported in part by Australian Re-search Council grant A49531227.ReferencesJ.R.
Anderson.
1983.
The Architecture of Cogni-tion.
Cambridge, Massachusetts: Harvard Univer-sity Press.B.
Baars.
1987.
A Cognitive Theory of Conscious-ness.
Cambridge University Press.E.
Charniak and R.P.
Goldman.
1993.
A Bayesianmodel of plan recognition.
Artificial Intelligence64(1):53-80.McConachy, Korb and Zukerman 99 A Bayesian Approach to Automating ArgumentationN~Q~efoartz nd I Idspar und ?Phenocrysts containsmall grainsFavourable I I Favourable groundmass phenocrysts Jporphyry level of texture erosionIntrusive \[ Favourable system in regional target area environmentcopper depositat Mineral Parktexturesmall grainsI Suggestive I texture ofigneous rocksN,o 1regional environmenti N~2Figure 6: Final Sub-argument for the Porphyry Copper Deposit ExampleG.F.
Cooper.
1990.
The Computational Complexityof Probabilistic Inference Using Belief Networks.Artificial Intelligence 42:393-405.R.O.
Duda, P.E.
Hart, P. Barrett, J.G.
Gaschnig,K.
Konolige, R. Reboh and J. Slocum.
1978.
De-velopment of the Prospector Consultation Systemfor Mineral Exploration.
Final Report for SRIProjects 5821 and 6415, SRI International.J.St.B.T.
Evans.
1989.
Bias in Human Reasoning:Causes and Consequences.
Hillsdale, NJ, USA.J.R.
Hobbs, M.E.
Stickel, D.E.
Appelt and P. Mar-tin.
1993.
Interpretation asabduction.
ArtificialIntelligence 63(1-2):69-142.H.
Horacek.
1994.
How to Avoid Explaining Obvi-ous Things (Without Omitting Central Informa-tion).
Proceedings of the Eleventh European Con-ference on Artificial Intelligence, 520-524, JohnWiley & Sons.D.
Kahneman, P. Slovic and A. Tversky, editors.1982.
Judgment under Uncertainty: Heuristicsand Biases.
Cambridge University: CambridgeUniversity Press.K.B.
Korb, R.S.
McConachy and I. Zukerman.
1997.A Cognitive Model of Argumentation.
I  Proceed-ings of the Nineteenth Annual Conference of theCognitive Science Society, pages 400-405, Stan-ford University.
Lawrence Erlbaum Associates.Z.
Li and B.
D'Ambrosio.
1994.
Efficient Inferencein Bayes Nets as a Combinatorial OptimizationProblem.
International Journal of ApproximateReasoning 11(1):55-81.S.
Lichtenstein, B. Fischhoff and L.D.
Phillips.
1982.Calibrations of probabilities: The state of the artto 1980.
In Kahneman et al, pages 306-334.S.
Mehl.
1994.
Forward Inferences in Text Genera-tion.
Proceedings of the Eleventh European Con-\]erence on Artificial Intelligence, 525-529, JohnWiley & Sons.R.E.
Neapolitan.
i990.
Probabilistie Reasoningin Expert Systems: Theory and Algorithms.
NewYork: John Wiley & Sons.J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKmlfmann Publishers.R.H.
Thomason, J.R. Hobbs and J.D.
Moore.
1996.Communicative Goals.
In Proceedings ofECAI-96Workshop - Gaps and Bridges: New Directions inPlanning and NLG, pages 7-12.A.
Tversky and D. Kahneman.
1982.
Judgments ofand by representativeness.
In Kahneman et al,pages 84-98.M.A.
Walker.
1996.
The Effect of Resource Limitsand Task Complexity on Collaborative Planningin Dialogue.
Artificial Intelligence 85(1-2):181-243.McConachy, Korb and Zukerman I00 A Bayesian Approach to Automating ArgumentationI1IIIlIIIIIII
