Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 318?327,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsUsing idiolects and sociolects to improve word predictionWessel StoopCentre for Language and Speech TechnologyRadboud University Nijmegenw.stoop@let.ru.nlAntal van den BoschCentre for Language StudiesRadboud University Nijmegena.vandenbosch@let.ru.nlAbstractIn this paper the word prediction systemSoothsayer1is described.
This system pre-dicts what a user is going to write as heis keying it in.
The main innovation ofSoothsayer is that it not only uses idi-olects, the language of one individual per-son, as its source of knowledge, but alsosociolects, the language of the social cir-cle around that person.
We use Twitterfor data collection and experimentation.The idiolect models are based on individ-ual Twitter feeds, the sociolect models arebased on the tweets of a particular personand the tweets of the people he often com-municates with.
The idea behind this isthat people who often communicate startto talk alike; therefore the language of thefriends of person x can be helpful in try-ing to predict what person x is going tosay.
This approach achieved the best re-sults.
For a number of users, more than50% of the keystrokes could have beensaved if they had used Soothsayer.1 IntroductionThe main aim of the study presented here is toshow that the concepts of idiolect and sociolect,the language of one person and his or her so-cial circle, can be used to improve word predic-tion, the task of predicting what a user is goingto type, as he is typing.
Word prediction technol-ogy reduces the number of keystrokes we have tomake, thus saving time and preventing mistakes.With the rise of smartphones word prediction hasbecome widely known and used.
Preceding this1The system is available as an interactive demoat http://soothsayer.cls.ru.nl/ and its sourcecode is publicly available at https://github.com/woseseltops/soothsayerpopularization, word prediction systems were al-ready developed up to three decades ago to as-sist people with speech and motor disabilities, likecerebral palsy or hemiplexia.
By using a deviceequipped with word prediction technology, theycan increase their communication rate consider-ably (Garay-Vitoria and Abascal, 2006).
Indeed,most studies before the year 2000, when mobilephones were not widely used yet, targeted the dis-abled user group - Copestake (1997) even reportsbuilding a system for one individual user.
Morerecent work targets a wider audience, but in thispaper we return to the idea of using an individual?sown language to train individualized models.The concept of an idiolect, the language ofa single person, is well-known, but rarely evermodelled or in some other way operationalized(Mollin, 2009; Barlow, 2010; Louwerse, 2004).Almost every claim in the field of linguistics con-cerns language as a whole; whether the subjectof investigation is a particular syntactic construc-tion, phonological variable, or some other linguis-tic phenomenon, the results are always supposedto hold for an entire language variety.
Accordingto Mollin (2009) idiolects are a ?neglected area incorpus linguistics?, and Barlow (2010) states thatthe term ?is distinguished by the fact that there isprobably no other linguistic term in which thereis such a large gap between the familiarity of theconcept and lack of empirical data on the phe-nomenon.?
This is remarkable, since ?idiolects arethe only kind of language we can collect data on?,as Haugen (1972) points out; a language varietyessentially is a collection of idiolects.Word prediction systems typically operate withan algorithm and a language model, as theoverview of related work in Section 2 will show.Language models are created from training mate-rial, typically a large collection of text.
Section 3introduces our algorithm step by step.
The result-ing best-performing algorithm is used in Section3184, in which we investigate which language modelshould be used together with this algorithm.
Westart with the notion of an individual?s idiolect inSection 4.1, and expand this by using the languageof the people this individual communicates with,in Section 4.2.
In Section 5 we offer our conclu-sions and formulate points for further research.2 Related workAn early solution for word prediction was to useword frequency lists (Swiffin et al., 1985).
Al-though it is possible to wait until a word unicitypoint has been reached (the point in a word wherethere there is no other word with the same pre-fix), more keystrokes may be saved if the predic-tion can be done before the unicity point.
Afterthis first data-driven improvement, numerous au-thors have shown that taking the contextof previ-ously entered words into account improves predic-tion accuracy further.
A simple approach to im-plementing context-sensitivity is applying the fre-quency list technique to word n-grams (Hunnicutt,1987); in a string of work other statistical languagemodeling approaches have been proposed (Lesheret al., 1999; Langlais et al., 2000; Garay-Vitoriaand Abascal, 2006; Tanaka-Ishii, 2007; Van denBosch and Bogers, 2008).The accuracy of a context-sensitive systemlargely depends on how often a similar context isavailable in the training material; the amount oftraining data will be an important factor for thesystem?s success.
A key publication by Lesheret al.
(1999) indeed shows that the accuracy ofa context-sensitive word prediction system is re-lated to how much training material is provided.On the other hand, once most of the frequent com-binations are covered, it takes more and moretraining material to improve the results a littlebit.
Van den Bosch (2011) demonstrates thatthe relation between the amount of training dataand word completion performance is roughly log-linear.
For instance, when going from 100 to1,000 words in the training material, roughly 6%more keystrokes could be saved (from 15% to 21%keystrokes saved), while the same is true for thestep from 1,000,000 to 10,000,000 words (from40% to 46%).A large portion of the work on word predictionincludes linguistic knowledge in some way, forexample by also training the system which PoS-tags are likely to follow each other, and usingthat to limit the pool of suggestions (Carlbergeret al., 1997; Fazly and Hirst, 2003; Copestake,1997; Matiasek et al., 2002; Garay-Vitoria andGonzalez-Abascal, 1997).
Interestingly, most au-thors conclude that including linguistic knowledgeimproves the results, but only slightly (Garay-Vitoria and Gonzalez-Abascal, 1997; Fazly andHirst, 2003).
Fazly and Hirst (2003) note thatadding explicit linguistic knowledge ?might not beconsidered worth the considerable extra cost thatit requires?.
In the current study we have not usedany explicit linguistic knowledge, thus making oursystem language-independent.There have also been more successful optimiza-tions of word completion systems.
One is to usetraining material from the same domain.
Ver-berne et al.
(2012) show that trying to predictWikipedia text, tweets, transcriptions of conver-sational speech and Frequently Asked Questionsall worked best when using texts from the sametype.
As a second optimization, building on theidea of cache language models (Goodman, 2001),Van den Bosch (2011) proposes to make a wordcompletion system learn about register and topicon the fly with a recency buffer.
This buffer storesthe n latest words; if a word the user is keying inmatches one of the words in the recency buffer,this word is suggested instead of what the systemwould actually have suggested.
The idea behindthis is that if the user is writing about, for exam-ple, traveling, words like ?go?, ?hotel?, and ?see?are likely to be in the buffer and thus could be sug-gested quickly.
In other words, the systems learnsabout the user and the topic on the fly.Although both approaches help to increase thenumber of keystrokes saved, they also have down-sides: for the system by Verberne et al.
(2012)training texts in the same genre are needed, whichmight not be available, whereas the system byVan den Bosch (2011) ignores context informa-tion that it should weigh more intelligently.
Forexample, while a context-sensitive text-predictionsystem will probably be able to predict to for thesentence they were going t..., the one with the re-cency buffer will predict they.3 System descriptionSoothsayer predicts the next word the user is go-ing to type, or the rest of the word in case theuser already starting typing something.
To dothis, it works with a set of independent word pre-319diction modules.
Modules can be either context-insensitive or context-sensitive, and use one lan-guage model.
We will work with two languagemodels, one based on a large collection of textssampling from many different authors, the ?gen-eral language model?, and one based on a set oftexts written by an individual, the ?idiolect?.
Wethus have four possible modules:Module Type Model1 Context-sensitive idiolect2 Context-sensitive general language model3 Context-insensitive idiolect4 Context-insensitive general language modelTable 1: Four possible modules: combinations oftype and language modelModules can be concatenated in such a way thata second module takes over once the first modulesno longer has predictions, a third module takesover once the second one no longer has predic-tions, etc.
In future work, interpolation of the pre-dictions of these modules should be investigated.3.1 Context-insensitive modulesContext-insensitive modules only use informationof the word the user is currently keying in.
In sen-tence 1, for example, only the c will be used forprediction.
(1) I ate too much cThis means that a prediction like communica-tion is fully possible, despite the context.
This alsomeans that at the beginning of each new word noprediction will be available, because the modulehas no material to work with.
Despite these lim-itations, context-insensitive modules can alreadysave a lot of keystrokes, because the first few let-ters of a word impose strong limitations on whatletters can possibly follow, and some words haveearly unicity points.
Predictions are done by go-ing through a frequency list, so the most frequent(and thus more likely to occur again) words areconsidered first.
Once a word is encountered thatmatches what has been keyed in so far, it is sug-gested.3.2 Context-sensitive modulesContext-sensitive modules make use of the wordsthat came before the current word to limit whatwords are predicted.
Soothsayer approaches wordprediction as a classification task, where the threewords in the left context of the word to be pre-dicted are the features, and the word following thiscontext is the class label to be predicted.
Thismeans that we have a separate class for everyword that could possibly be predicted.
Sooth-sayer uses the k-nearest neighbour classificationmethod, which is insensitive to the number ofclasses to be predicted.
k-nearest neighbour clas-sification (henceforth KNN) means that the class isdetermined on the basis of similar cases in a train-ing corpus.
How many cases are taken into con-sideration, k, can be determined beforehand.
Thesimilarity between a new instance and memorizedinstances is determined using a simularity func-tion.
A classic implementation of KNN suited forthe type of symbolic features we have, the IB1-algorithm (Aha et al., 1991), simply counts howmany features overlap.
However, the IB1 algo-rithm generally is too slow to be used in practi-cal applications, ours included.
We adopt IGTree2(Daelemans et al., 1997), an approximation of IB1that does not require a comparison of the completecontext.IGTree calculates which features contain mostinformation about the class labels using the In-formation Gain or Gain Ratio metrics, orders thefeatures from most informative to least informa-tive, and compresses the training set in a decisiontree.
Classification of a new context reduces tomaking a small number of decisions (a maximumof three, because we have three features), insteadof comparing a new context to thousands to mil-lions of contexts.
If we ask IGTree to classifyunseen input, the algorithm may not come to aunique decision.
Soothsayer asks the algorithm toreturn everything it considers a possibility at thedeepest node it reached while traversing the deci-sion tree.
Analogous to the manner in which thecontext-insensitive module generates frequency-ranked lists of possible completions, IGTree willproduce a frequency-ranked list of possible com-pletions it found at this deepest node in the tree.Soothsayer then accepts the single (or three) top-ranking suggestion(s).3.3 Evaluating the systemThere is an ongoing discussion in the literatureon what is the best way to evaluate word pre-2IGTree is implemented in the TiMBL software package,http://ilk.uvt.nl/timbl320diction systems.
A straightforward evaluationmight be to calculate the percentage of correctlypredicted words (the so-called hit-ratio), but asGaray-Vitoria and Abascal (2006) note, this is notenough: a system that has 100% of the words cor-rect, but only gives this prediction at the last letterof every word saves very few keystrokes.
A morenatural way might be to test with real humans,and measure how much time they save when usingthe system (Carlberger et al., 1997; Koester andLevine, 1998; Garay-Vitoria and Abascal, 2006).However, this is a costly and time-consuming task,as the participants will need a considerable amountof time to get used to the system.
Therefore,we will evaluate Soothsayer by simulating some-body keying in a text, and counting how manykeystrokes this virtual user does not have to dowhen using the system.However, even when using simulations, thereare multiple ways to evaluate the system.
One pos-sibility is to provide the user with a list of the nmost likely predictions (Lesher et al., 1999; Fazlyand Hirst, 2003).
This approach has the advantagethat it results in high percentages of keystrokessaved - in particular when n is set to a high value,because this means the system can do multipleguesses at once, while only one has to be correct.As Van den Bosch (2011) notes, however, this alsohas important downsides:[I]n many devices and circum-stances it is inefficient or impossible topresent [...] suggestion lists.
Inspectinga list of suggestions also poses a largercognitive load than checking a singlesuggestion, and furthermore it is unclearhow scrolling, browsing and selectingitems from this list should be counted interms of keystrokes.For this reason, we calculate the number ofkeystrokes that could have been saved when theuser was presented only one prediction at a time.Predictions can be accepted with the space key.Because this is sometimes problematic (for in-stance, if the user wanted to type sun, but Sooth-sayer predicts sunday, hitting space would lead tothe wrong word), a rejection is also calculated asa keystroke.
The number of keystrokes that canbe saved if the word prediction system works thisway will be called Classical Keystrokes Saved(CKS) in the remainder of this paper.
Please notethat CKS assumes that the user always accepts aprediction immediately when it is available, whichmight not always be the case in reality.On the other hand, current popular smart-phone applications suggest this approach mightbe too strict.
The popular smartphone applica-tion SwiftKey3always shows the user three pre-dictions, which seem to be (1) what the user haskeyed in so far, (2) the most likely prediction and(3) the second most likely prediction.
In case theuser has not yet started typing the next word, op-tion (1) is replaced by the third most likely pre-diction.
The percentage of keystrokes that canbe saved when two (and sometimes three) predic-tions were shown will be referred to as SwiftKeyKeystrokes Saved (SKKS).
This percentage willmostly be higher than CKS.3.4 Other considerationsContext-sensitive before context-insensitiveThe context-sensitive module learns about whichwords in the training texts typically follow eachother, and thus is potentially powerful when itcomes to the more frequent, fixed combinations ofwords and words that often occur in each other?scontext, but is not useful with words that arealso frequent, but were not used earlier in thiscontext.
The context-insensitive module, on theother hand, can predict any word, as long as ithas been used before, but knows nothing aboutfixed combinations.
In other words, the modulescomplement each other.
Based on the fact thatcontext-sensitive modules have been reported asscoring better than context-insensitive modulesin direct comparisons in controlled experiments(Lesher et al., 1999), we rank context-sensitivemodules before context-insensitive ones in allstudies reported here.
The context-insensitivemodule trained on idiolects precedes the finalcontext-insensitive module trained on a generallanguage corpus; this order reflects the order alsofound in the context-sensitive modules describedin more detail in the next section.Attenuation IGTree is fast in classification, butwith tens of millions of training instances it be-comes too slow for real-time use, where fast typ-ists may reach as many as twenty keystrokes persecond.
To alleviate this issue we use a (simpli-fied version of a) solution from the field of syntac-tic parsing called attenuation (Eisner, 1996).
All3http://www.swiftkey.net/321words in the training material that occur less of-ten than a particular threshold are replaced by adummy value.
Replacing all low-frequent wordsby one dummy value makes the IGTree consid-erably smaller and thus faster to traverse duringclassification.
In a pilot experiment an attenua-tion threshold of 3 turned out to be the most desir-able: it leads to the largest increase in speed (from28 classifications per second to 89) without anymeasurable decrease in prediction accuracy.
Forthis reason, an attenuation threshold of 3 was usedthroughout the study.Handling morphology Some aspects of mor-phology are inherently problematic for word com-pletion, in particular compounding, inflections,and suffixes.
For example, imagine a user has al-ready written sentence 2, and wants to write theword cookies:(2) I would really like the cIf in the training material the word cookie wasmore frequent, Soothsayer will suggest that in-stead of cookies.
Normally, when a prediction iswrong, the algorithm will find out because the userkeys in another letter (so the predicted word nolonger matches what the user is typing), but thattechnique will not work here.
For words that onlydiffer in their suffix, the point of difference is at theend of the word, when there is nothing left to pre-dict.
Even if the correct word is the second mostlikely prediction, this will not be suggested, be-cause Soothsayer has no reason to switch predic-tion.However, there is a clue Soothsayer could use:normally, when a prediction is right, the user willaccept it, instead of going on writing.
He mightnot accept it immediately (typing often goes fasterthan mentally processing predictions), but oncethe user has not accepted a prediction for morethan two or three keystrokes in a row, it gets moreand more likely the user keeps on typing becausethe prediction is wrong.
In that case, the secondmost likely prediction could be displayed, whichin many cases will be the word with the secondmost likely suffix.
We use this early predictionswitching method throughout our experiments.Recency As Church (2000) showed, the proba-bility that a word recurs twice in a short stretch oftext is far higher than its frequency in languagewould suggests, which is mainly related to theword?s topicality.
Whereas knowledge about top-ics could be covered by training and testing withinthe same coherent set of texts (e.g.
all writtenby a single person), the aforementioned recencybuffer by definition uses more recent text (that is,material from the same text), and might this waybe able to do more accurate predictions.
We im-plemented a buffer that remembers the n mostrecent words, and suggests the most recent onethat matches with the word that is currently be-ing keyed in.
Following Van den Bosch (2011) weset n to 300.
If no word matches, the next modulewill take over.
In our experiments we have testedthe insertion of the recency buffer module after thecontext-sensitive modules and before the context-insensitive modules.4 The model: idiolects and sociolects4.1 IdiolectsIn this experiment the power of idiolects will beinvestigated by training and testing an array ofsystems on one hundred different idiolects of in-dividuals.
For this, the micro-blogging serviceTwitter4is used.
Twitter is a micro-blogging ser-vice where each user can submit status updatesknown as tweets, which consist of 140 charac-ters or less.
Using the Twitter API, all tweets ofa manually created seed set of 40 Dutch Twitterusers were retrieved from January until June 2013.Retweets, messages these authors did not producethemselves, were excluded.
These seed users werethe starting point of an iterative expansion of theset of crawled Twitter uses by following mentionsof other users (indicated with the syntax ?@user-name?).
The goal of this expansion was to find asmuch active Twitter users as possible for the sys-tem to follow, and to capture the network of theseusers.
The set was extended with two users every30 minutes with the following procedure:?
From the set of users mentioned in tweets al-ready harvested and not yet tracked, the mostfrequently mentioned user is selected.
Thisensures that the new person communicateswith at least one of the persons the systemis already following.?
From the set of users mentioned by morethan a single person already being tracked,the most frequently mentioned user is se-lected.
This ensures the new person is well4http://www.twitter.com322connected to the people the system is alreadyfollowing.The system limits itself to Dutch tweets using aconservative Dutch word frequency list containinghighly frequent Dutch words that have no counter-part in other languages.Concerning the relation between number oftweets and Twitter users, many scholars have no-ticed that it follows a Pareto-distribution (Heil andPiskorski, 2009; Asur and Huberman, 2010; Ruiand Whinston, 2012).
That is, a small part of theTwitter users produce a large part of the tweets.This distribution means that using all or a ran-dom selection of Twitter users is not likely to leadto good results, because for most users not muchmaterial is available.
Therefore, only data fromthe 100 Twitter users for which the most mate-rial was harvested are used to build the idiolectmodels.
Twitter accounts run by something otherthan an individual person (such as a company)were excluded manually.
The number of wordsranged from 61,098 words for the least active userof the 100 users to 167,685 words for the most ac-tive user.
As a general language model, a randomselection of blogs, emails and Wikipedia articlesfrom the SoNaR corpus for written Dutch (Oost-dijk et al., 2013) was made.
These texts were cho-sen because they were believed to be neither veryformal nor very informal, and fall in the same new-media category as Twitter messages.
The generallanguage corpus consisted of 55,212,868 words.First, we compared the general language modelagainst each user?s idiolect, and tested on all 100Twitter feeds of individual users.
We then com-bined the two models (the general model acting asback-off for the idiolect model).
These three se-tups were tested with and without a recency buffermodule, resulting in six runs.
For each of theseruns, we tried to predict the 10% most recent ma-terial, and trained on the remaining 90% (for idi-olects).
Tables 2 and 3 list the results on these sixruns measured in CKS and SKKS, respectively.We observe that using the idiolect model leadsto more keystrokes saved than using the generalmodel.
We also see that using the general lan-guage model as a background model leads to morekeystrokes saved than using the idiolect modelalone.
Using the recency buffer leads to morekeystrokes saved, especially when it is used in ad-dition to the general mode,An ANOVA for repeated measures showed thatthere is a significant effect of the training ma-terial F (2, 198) = 109.495, p < .001 andwhether the recency buffer was used F (1, 99) =469.648, p < .001.
Contrast analyses revealedthat both the differences between the results of thegeneral model and the idiolect model F (1, 99) =41.902, p < .001 and the idiolect model andthe idiolect model with the background modelF (1, 99) = 232.140, p < .001 were significant.The high standard deviations indicate a lot ofvariation.
The substantial individual differencesare illustrated in Figure 1, where the users are or-dered from least to most material.
Contrary toexpectations, no correlation between amount oftraining material and the results could be detected(Pearson?s correlation, p = .763); apparently, theindividual factor is that much stronger, and Sooth-sayer performs much better for one than for theother.
Using the overall best-performing moduleset-up, the set-up with the idiolect model, backedup by the general language model, and the recencybuffer, the worst result is 21.8% CKS and 24.1%SKKS for user 90, and the best result is 51.3%CKS and 52.4% SKKS for user 97.Figure 1: The proportion of keystrokes saved forindividual Twitter users, ordered from by amountof tweets (from left to right: from least to most),when using the best-performing module set-upThe large amount of variation between individ-ual Twitter users cannot easily be explained, witha few exceptions (for example, people with pow-erful idiolect models sometimes often repeatedlong words like goedemorgen ?good morning?,dankjewel ?thank you?, and welterusten ?sleepwell?
), but no clear patterns emerged.
Trying topredict for which persons word prediction will gowell and for which persons it will not might be aninteresting topic for future research.
It is a ques-tion that is related to the field of computational323Training material Test material Without recency buffer With recency bufferMean St. dev.
Mean St. dev.General Twitter 14.4 5.1 23.2 5.2Idiolect Twitter 23.2 7.9 26.7 7.9Idiolect + general Twitter 26.4 6.2 29.7 6.4Table 2: Mean percentage of keystrokes saved (CKS) and standard deviations for all module set-ups.Training material Test material Without recency buffer With recency bufferMean St. dev.
Mean St. dev.General Twitter 16.2 6.1 26 5.4Idiolect Twitter 24.8 8.3 27.9 7.2Idiolect + general Twitter 28.2 6.3 32.1 6.3Table 3: Mean percentage of keystrokes saved (SKKS) and standard deviations for all module set-ups.stylometry and in particular automatic authorshipattribution, although authorship attribution is theexact opposite of the task described here (guessingthe author on the basis of text instead of guessingthe text on the basis of the author) (Bagavandasand Manimannan, 2008).4.2 Social networks and language inputThe findings by Lesher et al.
(1999) suggest thatmore material leads to more keystrokes saved; thismay also hold for idiolects.
This material, how-ever, might not be available, simply because not allpeople write or tweet that much.
For a particularuser x, what other sources of language do we havethat might be similar to the idiolect of x?
One ofthe more obvious answers might be the languageof the people x often communicates with.
The factthat people that are in some way related to eachother speak alike using a ?group language?
or a so-ciolect, is well established in sociolinguistics.This approach of including the language of thepeople from a particular person?s environment canalso be viewed from a different perspective: sofar, we have followed Mollin (2009) and Bar-low(2010) in using only the output of speakers.This makes sense (since what comes out must havebeen inside), but can never be the full story.
Thesociolect model that will be constructed here canbe seen as a feasible and rough approximation ofrecording everything a person reads or hears: byincluding the language of the socially related per-sons of person x, the system can have a rough ideaof the kind of input person x gets.On the basis of the data already collected forthe idiolect experiments, sociolects were createdby collecting all addressees mentioned with the@addressee syntax for each of the 100 Twitterusers used in the previous experiment.
For all ad-dressees that were mentioned three times or more,it was checked if this addressee was in the dataset(which was almost always the case).
If so, it waschecked whether this addressee also mentionedthe original Twitter user at least three times.
If thiswas also the case, the system assumed the usersspeak to each other often enough to have their lan-guage adjusted to each other, and the tweets ofthis addressee were added to the sociolect of theoriginal Twitter user.
We thus end up with 100sociolects built around the 100 most active Twit-ter users, all based on the tweets of a Twitter userand the tweets of the persons that this person com-municated with at least six times (three times aswriter, three times as reader).The results of Verberne et al.
(2012) would pre-dict that adding tweets in general would lead to in-creases in the number of keystrokes saved, as thisis using more texts from the same genre.
To besure that any improvements can be attributed tothe fact that this is the language from friends, acontrol model will be built.
While the sociolectmodel consists of the tweets of Twitter user x andthe tweets of the friends of twitter user x, the con-trol model consists of the tweets of Twitter user xand the tweets of random other Twitter users, andhas approximately the same number of words.For each of the 100 Twitter users, comparativeruns are performed with the model created on thebasis of the idiolect and the random Twitter usersversus the sociolect model.
The best performingmodule set-up from the previous experiments isused.
The results are compared to the simulationswith the idiolect model from the previous experi-324Training material Test material CKS SKKSMean St. dev.
Mean St. dev.Idiolect Twitter feed 29.6 6.4 32.1 6.3Control model Twitter feed 31.2 6.3 33.9 6Sociolect Twitter feed 33.9 7.1 36.2 7.1Table 4: Mean percentage of keystrokes saved when using an idiolect, a control model (consisting of anidiolect and random other Twitter feeds) and a sociolect.Twitter user Idiolect Idiolect+random feeds SociolectCKS SKKS CKS SKKS CKS SKKS24 31.2 36.3 34 36.4 31.6 34.349 27.2 29.1 26.2 29.7 24.6 27.271 27.5 30.2 34.2 35.8 30.8 32.9Table 5: Percentage of keystrokes saved for 3 atypical Twitter users, using the the idiolect, control andsociolect modelsment.
The results of the simulations are summa-rized in Table 4.
We observe that adding moretweets to the idiolects leads to more keystrokessaved, and that the most keystrokes can be savedwhen using the tweets of the people the owner ofthe idiolect communicates with often.An ANOVA for repeated measures showed thatthere is a significant effect for the training materialF (2, 198) = 69.466, p < .001.
Contrast analysesrevealed that both the differences between the re-sults of the idiolect model and the idiolect modeland random feeds F (1, 99) = 93.471, p < .001and the idiolect model and random feeds and thesociolect model F (1, 99) = 61.871, p < .001 aresignificant.Again, the high standard deviations indicate no-table variation among the individual results.
Ta-ble 5 lists the deviating individual scores for threeindividual Twitter users.
In these results we seean increase when random tweets are added, but adecrease when the tweets from their conversationpartners are used.
For user 24 and 49, the percent-age of keystrokes saved when using the sociolectmodel is even lower than the idiolect model alone.Using the best-performing module set-up ingeneral, the set-up with the sociolect model,backed up by the general language model, andthe recency buffer, the worst result is 21.3% CKSand 22% SKKS for user 90, and the best result is56.2% CKS and 58.1% SKKS for user 38.5 ConclusionIn this paper we presented the word prediction sys-tem Soothsayer.
Testing the system we found thatword prediction and idiolects are an effective com-bination; our results show that word predictionis best done with a combination of an idiolect-based context-sensitive system, backed up by acontext-sensitive module equipped with a generallanguage model.
A recency buffer is a useful thirdmodule in the sequence.
Our average best scoreswith these three modules are 29.7% keystrokessaved according to the strict (one-best) CKS met-ric, and 32.1% keystrokes saved according to theSwiftkey-inspired SKKS metric.The fact that people speak like the peoplearound them can also be useful to word prediction.When we approximate a sociolect by expanding auser?s Twitter corpus by tweets from people thisperson communicates with, and retrain our firstcontext-sensitive module with this data, averagescores improve to 33.9% CKS and 36.2% SKKS.What works well for one speaker, might notnecessarily work for another, however.
Whilewe find significant advantages of idiolect-basedand sociolect-based training, the variance amongour 100 test users is substantial, and in individualcases idiolect-based training is not the best option.For other users the positive gains are substantiallyhigher than the mean; the best result for a singleuser is 56.2% CKS and 58.1% SKKS.In future research we aim to investigate methodsthat could predetermine which model and moduleorder will work best for a user.
Another set of openresearch questions concern the fact that we havenot tested many of the system?s settings.
Whatwould be the effects of predicting more words atthe same time?325ReferencesD.
W. Aha, D. Kibler, and M. K. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37?66.S.
Asur and B.
A. Huberman.
2010.
Predicting the fu-ture with social media.
In Proceedings of the 2010IEEE/WIC/ACM International Conference on WebIntelligence and Intelligent Agent Technology - Vol-ume 01, WI-IAT ?10, pages 492?499, Washington,DC, USA.
IEEE Computer Society.M.
Bagavandas and G. Manimannan.
2008.
Styleconsistency and authorship attribution: A statisticalinvestigation.
Journal of Quantitative Linguistics,15(1):100?110.M.
Barlow.
2010.
Individual usage: a corpus-basedstudy of idiolects.
In LAUD Conference, Landau.http://auckland.academia.edu/MichaelBarlow.A.
Carlberger, J. Carlberger, T. Magnuson, S. Hun-nicutt, S. E. Palazuelos Cagigas, and S. AguileraNavarro.
1997.
Profet, a new generation of wordprediction: An evaluation study.
In ACL Workshopon Natural Language Processing for Communica-tion Aids, pages 23 ?28.K.
W. Church.
2000.
Empirical estimates of adapta-tion: The chance of two noriegas is closer to p=2than p2.
In Proceedings of the 18th Conference onComputational Linguistics, volume 1, pages 180?186.A.
Copestake.
1997.
Augmented and alternative nlptechniques for augmented and alternative nlp tech-niques for augmentative and alternative communica-tion.
In Proceedings of the ACL workshop on Nat-ural Language Processing for Communication Aids,pages 37?42.W.
Daelemans, A. van den Bosch, and A. Weijters.1997.
Igtree: using trees for compression and clas-siffication in lazy learning algorithms.
Artificial In-telligence Review, 11:407?423.J.
Eisner.
1996.
An empirical comparison of probabil-ity models for dependency grammar.
In TechnicalReport IRCS-96-11, Institute for Research in Cogni-tive Science.
University of Pennsylvania.A.
Fazly and G. Hirst.
2003.
Testing the efficacy ofpart-of-speech information in word completion.
InProceedings of the 2003 EACL Workshop on Lan-guage Modeling for Text Entry Methods, pages 9?16.N.
Garay-Vitoria and J. Abascal.
2006.
Text predic-tion systems: a survey.
Univers.
Access Inf.
Soc.,4(3):188?203.N.
Garay-Vitoria and J. Gonzalez-Abascal.
1997.
In-telligent word-prediction to enhance text input rate.In Proceedings of the 2nd International Conferenceon Intelligent User Interfaces, pages 241?244.J.
Goodman.
2001.
A bit of progress in language mod-eling.
Computer Speech & Language, 15(4):403?434.E.
Haugen.
1972.
From idiolect to language.
InE.
Scherabon Firchow, K. Grimstad, N. Hasselmo,and W. A. ONeil, editors, Studies by Einar Hau-gen.
Presented on the Occasion of his 65th Birthday,pages 415?421.
Mouton, The Hague/Paris.B.
Heil and M. Piskorski.
2009.
New twitter re-search: Men follow men and nobody tweets.http://blogs.hbr.org/cs/2009/06/new\_twitter\_research\_men\_follo.html.S.
Hunnicutt.
1987.
Input and output alternatives inword prediction.
STL-QPSR, 28(2-3):015?029.H.
H. Koester and S. P. Levine.
1998.
Model sim-ulations of user performance with word prediction.Augmentative Alternative Commun, pages 25?35.P.
Langlais, G. Foster, and G. Lapalme.
2000.Transtype: a computer-aided translation typingsystem.
In Proceedings of the 2000 NAACL-ANLP Workshop on Embedded machine transla-tion systems-Volume 5, pages 46?51.
Association forComputational Linguistics.G.
W. Lesher, B. J. Moulton, and D. J. Higginbotham.1999.
Effects of ngram order and training text sizeon word prediction.
In Proceedings of the AnnualConference of the RESNA.M.
M. Louwerse.
2004.
Semantic variation in id-iolect and sociolect: Corpus linguistic evidencefrom literary texts.
Computers and the Humanities,38(2):207?221.J.
Matiasek, M. Baroni, and H. Trost.
2002.
FASTY:A multi-lingual approach to text prediction.
In Com-puters Helping People With Special Needs, pages165?176.
Springer Verlag, Berlin, Germany.S.
Mollin.
2009.
I entirely understand is a blairism:The methodology of identifying idiolectal colloca-tions.
Journal of Corpus Linguistics, 14 (3):367?392.N.
Oostdijk, M. Reynaert, V. Hoste, and I. Schuurman.2013.
The construction of a 500-million-word refer-ence corpus of contemporary written Dutch.
In Es-sential Speech and Language Technology for Dutch,pages 219?247.
Springer.H.
Rui and A. Whinston.
2012.
Information or at-tention?
an empirical study of user contribution ontwitter.
Information Systems and e-Business Man-agement, 10(3):309?324.A.
L. Swiffin, J.
A. Pickering, J. L. Arnott, and A. F.Newell.
1985.
PAL: An effort efficient portablecommunication aid and keyboard emulator.
In Pro-ceedings of the 8th annual coonference on Rehabili-tation Technology, RESNA, pages 197?199.326K.
Tanaka-Ishii.
2007.
Word-based Predictive TextEntry using Adaptive Language Models.
NaturalLanguage Engineering, 13(1):51?74.A.
Van den Bosch and T. Bogers.
2008.
Efficientcontext-sensitive word completion for mobile de-vices.
In MobileHCI 2008: Proceedings of the 10thInternational Conference on Human-Computer In-teraction with Mobile Devices and Services, IOP-MMI special track, pages 465?470.A.
Van den Bosch.
2011.
Effects of context and re-cency in scaled word completion.
ComputationalLinguistics in the Netherlands Journal, 1:79?94.S.
Verberne, A.
Van den Bosch, H. Strik, and L. Boves.2012.
The effect of domain and text type on text pre-diction quality.
In Proceedings of the 13th Confer-ence of the European Chapter of the Association forComputational Linguistics, Avignon, France, pages561?569, New Brunswick, NJ.
ACL.327
