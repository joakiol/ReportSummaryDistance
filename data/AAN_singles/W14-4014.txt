Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 122?131,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsTransformation and Decomposition for Efficiently Implementing andImproving Dependency-to-String Model In MosesLiangyou Li?, Jun Xie?, Andy Way?and Qun Liu??
?CNGL Centre for Global Intelligent Content, School of ComputingDublin City University, Dublin 9, Ireland?Key Laboratory of Intelligent Information Processing, Institute of Computing TechnologyChinese Academy of Sciences, Beijing, China{liangyouli,away,qliu}@computing.dcu.iejunxie@ict.ac.cnAbstractDependency structure provides grammat-ical relations between words, which haveshown to be effective in Statistical Ma-chine Translation (SMT).
In this paper, wepresent an open source module in Moseswhich implements a dependency-to-stringmodel.
We propose a method to trans-form the input dependency tree into a cor-responding constituent tree for reusing thetree-based decoder in Moses.
In our ex-periments, this method achieves compara-ble results with the standard model.
Fur-thermore, we enrich this model via thedecomposition of dependency structure,including extracting rules from the sub-structures of the dependency tree duringtraining and creating a pseudo-forest in-stead of the tree per se as the input dur-ing decoding.
Large-scale experimentson Chinese?English and German?Englishtasks show that the decomposition ap-proach improves the baseline dependency-to-string model significantly.
Our sys-tem achieves comparable results with thestate-of-the-art hierarchical phrase-basedmodel (HPB).
Finally, when resorting tophrasal rules, the dependency-to-stringmodel performs significantly better thanMoses HPB.1 IntroductionDependency structure models relations betweenwords in a sentence.
Such relations indicatethe syntactic function of one word to anotherword.
As dependency structure directly encodessemantic information and has the best inter-lingualphrasal cohesion properties (Fox, 2002), it is be-lieved to be helpful to translation.In recent years, dependency structure has beenwidely used in SMT.
For example, Shen et al.
(2010) present a string-to-dependency model byusing the dependency fragments of the neighbour-ing words on the target side, which makes it easierto integrate a dependency language model.
How-ever such string-to-tree systems run slowly in cu-bic time (Huang et al., 2006).Another example is the treelet approach(Menezes and Quirk, 2005; Quirk et al., 2005),which uses dependency structure on the sourceside.
Xiong et al.
(2007) extend the treelet ap-proach to allow dependency fragments with gaps.As the treelet is defined as an arbitrary connectedsub-graph, typically both substitution and inser-tion operations are adopted for decoding.
How-ever, as translation rules based on the treeletsdo not encode enough reordering information di-rectly, another heuristic or separate reorderingmodel is usually needed to decide the best targetposition of the inserted words.Different from these works, Xie et al.
(2011)present a dependency-to-string (Dep2Str) model,which extracts head-dependent (HD) rules fromword-aligned source dependency trees and targetstrings.
As this model specifies reordering infor-mation in the HD rules, during translation only thesubstitution operation is needed, because wordsare reordered simultaneously with the rule beingapplied.
Meng et al.
(2013) and Xie et al.
(2014)extend the model by augmenting HD rules with thehelp of either constituent tree or fixed/float struc-ture (Shen et al., 2010).
Augmented rules are cre-ated by the combination of two or more nodes in122the HD fragment, and are capable of capturingtranslations of non-syntactic phrases.
However,the decoder needs to be changed correspondinglyto handle these rules.Attracted by the simplicity of the Dep2Strmodel, in this paper we describe an easy way tointegrate the model into the popular translationframework Moses (Koehn et al., 2007).
In or-der to share the same decoder with the conven-tional syntax-based model, we present an algo-rithm which transforms a dependency tree into acorresponding constituent tree which encodes de-pendency information in its non-leaf nodes and iscompatible with the Dep2Str model.
In addition,we present a method to decompose a dependencystructure (HD fragment) into smaller parts whichenrich translation rules and also allow us to cre-ate a pseudo-forest as the input.
?Pseudo?
meansthe forest is not obtained by combining severaltrees from a parser, but rather that it is createdbased on the decomposition of an HD fragment.Large-scale experiments on Chinese?English andGerman?English tasks show that the transforma-tion and decomposition are effective for transla-tion.In the remainder of the paper, we first describethe Dep2Str model (Section 2).
Then we describehow to transform a dependency tree into a con-stituent tree which is compatible with the Dep2Strmodel (Section 3).
The idea of decomposition in-cluding extracting sub-structural rules and creat-ing a pseudo-forest is presented in Section 4.
Thenexperiments are conducted to compare translationresults of our approach with the state-of-the-artHPB model (Section 5).
We conclude in Section 6and present avenues for future work.2 Dependency-to-String ModelIn the Dep2Str model (Xie et al., 2011), the HDfragment is the basic unit.
As shown in Figure1, in a dependency tree, each non-leaf node is thehead of some other nodes (dependents), so an HDfragment is composed of a head node and all of itsdependents.1In this model, there are two kinds of rules fortranslation.
One is the head rule which specifiesthe translation of a source word:Juxing???
holds1In this paper, HD fragment of a node means the HD frag-ment with this node as the head.
Leaf nodes have no HDfragments.Boliweiya????/NNJuxing??/VVZongtong??/NNYu?/CCGuohui??/NNXuanju?
?/NNFigure 1: Example of a dependency tree, withhead-dependent fragments being indicated by dot-ted lines.The other one is the HD rule which consists ofthree parts: the HD fragment s of the sourceside (maybe containing variables), a target stringt (maybe containing variables) and a one-to-onemapping ?
from variables in s to variables in t, asin:s = (Boliweiya????)Juxing??
(x1:Xuanju??
)t = Bolivia holds x1?
= {x1:Xuanju???
x1}where the underlined element denotes the leafnode.
Variables in the Dep2Str model are con-strained either by words (like x1:??)
or Part-of-Speech (POS) tags (like x1:NN).Given a source sentence with a dependency tree,a target string and the word alignment between thesource and target sentences, this model first an-notates each node N with two annotations: headspan and dependency span.2These two spansspecify the corresponding target position of a node(by the head span) or sub-tree (by the depen-dency span).
After annotation, acceptable HDfragments3are utilized to induce lexicalized HD2Some definitions: Closure clos(S) of set S is the small-est superset of S in which the elements (integers) are contin-uous.
Let H be the set of indexes of target words aligned tonode N .
Head span hsp(N) of node N is clos(H).
Headspan hsp(N) is consistent if it does not overlap with headspan of any other node.
Dependency span dsp(N) of nodeN is the union of all consistent head spans in the subtreerooted at N .3A head-dependent fragment is acceptable if the headspan of the head node is consistent and none of the depen-dency spans of its dependents is empty.
We could see thatin an acceptable fragment, the head span of the head nodeand dependency spans of dependents are not overlapped witheach other.123Boliweiya Juxing  XuanjuR ule:  ( ????)
??
( x1 :??)
Boliv ia hold s  x1XuanjuR ule:  ( x1: N N )  ??
x1 elec tionsGuohuiR ule:  ??
p ar liam entZongtong Yu      GuohuiR ule:    ( ??)
( ?)
x1:??
p r es id ential and  x1Boliweiya????/NNJuxing??/VVZongtong??/NNYu?/CCGuohui??/NNXuanju??/NNZongtong??/NNYu?/CCGuohui?
?/NNBoliv ia hold s elec tionsBoliv ia hold sZongtong??/NNYu?/CCGuohui??/NNXuanju?
?/NNBoliv ia hold s  p r es id ential and  p ar liam ent elec tionsBoliv ia hold s  p r es id ential andGuohui?
?/NN elec tions( a)( b )( c )( d )( e)Figure 2: Example of a derivation.
Underlined el-ements indicate leaf nodes.rules (the head node and leaf node are representedby words, while the internal nodes are replaced byvariables constrained by word) and unlexicalizedHD rules (nodes are replaced by variables con-strained by POS tags).In HD rules, an internal node denotes the wholesub-tree and is always a substitution site.
The headnode and leaf nodes can be represented by eitherwords or variables.
The target side correspondingto an HD fragment and the mapping between vari-ables are determined by the head span of the headnode and the dependency spans of the dependents.A translation can be obtained by applying rulesto the input dependency tree.
Figure 2 shows aderivation for translating a Chinese sentence intoan English string.
The derivation proceeds fromtop to bottom.
Variables in the higher-level HDrules are substituted by the translations of lowerHD rules recursively.The final translation is obtained by finding thebest derivation d?from all possible derivationsD which convert the source dependency structureinto a target string, as in Equation (1):d?= argmaxd?Dp(d) ?
argmaxd?D?i?i(d)?i(1)where ?i(d) is the ith feature defined in the deriva-tion d, and ?iis the weight of the feature.3 Transformation of Dependency TreesIn this section, we introduce an algorithm to trans-form a dependency tree into a corresponding con-stituent tree, where words of the source sentenceare leaf nodes and internal nodes are labelled withhead words or POS tags which are constrained bydependency information.
Such a transformationmakes it possible to use the traditional tree-baseddecoder to translate a dependency tree, so we caneasily integrate the Dep2Str model into the popu-lar framework Moses.In a tree-based system, the CYK algorithm(Kasami, 1965; Younger, 1967; Cocke andSchwartz, 1970) is usually employed to translatethe input sentence with a tree structure.
Each timea continuous sequence of words (a phrase) in thesource sentence is translated.
Larger phrases canbe translated by combining translations of smallerphrases.In a constituent tree, the source words are leafnodes and all non-leaf nodes covering a phrase arelabelled with categories which are usually vari-ables defined in the tree-based model.
For trans-lating a phrase covered by a non-leaf node, the de-coder for the constituent tree can easily find ap-plied rules by directly matching variables in theserules to tree nodes.
However, in a dependency tree,each internal node represents a word of the sourcesentence.
Variables covering a phrase cannot berecognized directly.
Therefore, to share the samedecoder with the constituent tree, the dependencytree needs to be transformed into a constituent-style tree.As we described in Section 2, each variable inthe Dep2Str model represents a word (for the headand leaf node) or a sequence of continuous words(for the internal node).
Thus it is intuitive to usethese variables to label non-leaf nodes of the pro-duced constituent tree.
Furthermore, in order topreserve the dependency information of each HDfragment, the created constituent node needs to beconstrained by the dependency information in theHD fragment.Our transformation algorithm is shown in Al-gorithm 1, which proceeds recursively from top tobottom on each HD fragment.
There are a maxi-mum of three types of nodes in an HD fragment:head node, leaf nodes, and internal nodes.
The124Algorithm 1 Algorithm for transforming a depen-dency tree to constituent tree.
Dnode means nodein dependency tree.
Cnode means node in con-stituent tree.function CNODE(label, span)create a new Cnode CNCN.label?
labelCN.span?
spanend functionfunction TRANSFNODE(Dnode H)pos?
POS of Hconstrain pos .
with H0, like: NN:H0CNODE(label,H.position)for each dependent N of H dopos?
POS of Nword?
word of Nconstrain pos .
with Li or Ri, like: NN:R1constrain word .
with Li or Riif N is leaf thenCNODE(pos,N.position)elseCNODE(word,H.span)CNODE(pos,H.span)TRANSFNODE(N )end ifend forend functionleaf nodes and internal nodes are dependents ofthe head node.
For the leaf node and head node,we create constituent nodes that just cover oneword.
For an internal node N , we create con-stituent nodes that cover all the words in the sub-tree rooted at N .
In Algorithm 1, N.positionmeans the position of the word represented by thenode N .
N.span denotes indexes of words cov-ered by the sub-tree rooted at node N .Taking the dependency tree in Figure 1 as anexample, its transformation result for integrationwith Moses is shown in Figure 3.
In the Dep2Strmodel, leaf nodes can be replaced by a vari-able constrained by its POS tag, so for leaf node?Zongtong??
?
in HD fragment ?Zongtong(??)Yu(?)Guohui??
?,we create a constituent node ?NN:L2?, where?NN?
is the POS tag and ?L2?
denotes that the leafnode is the second left dependent of the head node.For the internal node ?Guohui???
in the HD fragment?Guohui(??)Xuanju??
?, we create two constituent nodesBoliweiya????Juxing??Zongtong??Yu?Guohui??Xuanju?
?N N : L 1 V V : H 0 N N : L 2 C C : L 1 N N : H 0N N : H 0N N : L 1N N : R 1SGuohui??
: L 1Xuanju??
: R 1Figure 3: The corresponding constituent tree af-ter transforming the dependency tree in Figure 1.Note in our implementation, we do not distinguishthe leaf node and internal node of a dependencytree in the produced constituent tree and inducedrules.which cover all words in the dependency sub-treerooted at this node, with one of them labelled bythe word itself.
Both nodes are constrained by de-pendency information ?L1?.
After such a transfor-mation is conducted on each HD fragment recur-sively, we obtain a constituent tree.This transformation makes our implementationof the Dep2Str model easier, because we can usethe tree-to-string decoder in Moses.
All we needto do is to write a new rule extractor which extractshead rules and HD rules (see Section 2) from theword-aligned source dependency trees and targetstrings, and represents these rules in the format de-fined in Moses.4Note that while this conversion is performedon an input dependency tree during decoding, thetraining part, including extracting rules and cal-culating translation probabilities, does not change,so the model is still a dependency-to-string model.4Taking the rule in Section 2 as an example, its represen-tation in Moses is:s =Boliweiya????Juxing??Xuanju[??
:R1][X] [H1]t = Bolivia holdsXuanju[??
:R1][X] [X]?
= {2 ?
2}where ?H1?
denotes the position of the head word is 1, ?R1?indicates the first right dependent of the head word, ?X?
is thegeneral label for the target side and ?
is the set of alignments(the index-correspondences between s and t).
The format hasbeen described in detail at http://www.statmt.org/moses/?n=Moses.SyntaxTutorial.125In addition, our transformation is different fromother works which transform a dependency treeinto a constituent tree (Collins et al., 1999; Xia andPalmer, 2001).
In this paper, the produced con-stituent tree still preserves dependency relationsbetween words, and the phrasal structure is di-rectly derived from the dependency structure with-out refinement.
Accordingly, the constituent treemay not be a linguistically well-formed syntacticstructure.
However, it is not a problem for ourmodel, because in this paper what matters is thedependency structure which has already been en-coded into the (ill-formed) constituent tree.4 Decomposition of DependencyStructureThe Dep2Str model treats a whole HD fragmentas the basic unit, which may result in a sparse-data problem.
For example, an HD fragment witha verb as head typically consists of more than fournodes (Xie et al., 2011).
Thus in this section, in-spired by the treelet approach, we describe a de-composition method to make use of smaller frag-ments.In an HD fragment of a dependency tree, thehead determines the semantic category, whilethe dependent gives the semantic specification(Zwicky, 1985; Hudson, 1990).
Accordingly, itis reasonable to assume that in an HD fragment,dependents could be removed or new dependentscould be attached as needed.
Thus, in this paper,we assume that a large HD fragment is formed byattaching dependents to a small HD fragment.
Forsimplicity and reuse of the decoder, such an at-tachment is carried out in one step.
This meansthat an HD fragment is decomposed into twosmaller parts in a possible decomposition.
Thisdecomposition can be formulated as Equation (2):Li?
?
?L1HR1?
?
?Rj= Lm?
?
?L1HR1?
?
?Rn+ Li?
?
?Lm+1HRn+1?
?
?Rjsubject toi ?
0, j ?
0i ?
m ?
0, j ?
n ?
0i+ j > m+ n > 0(2)whereH denotes the head node, Lidenotes the ithleft dependent and Rjdenotes the jth right depen-dent.
Figure 4 shows an example.s m ar t/ JJv er y/ R BS he/ P R Ps m ar t/ JJis / V BZS he/ P R Ps m ar t/ JJis / V BZ v er y/ R B+Figure 4: An example of decomposition on a head-dependent fragment.Algorithm 2 Algorithm for the decomposition ofan HD fragment into two sub-fragments.
Index ofnodes in a fragment starts from 0.function DECOMP(HD fragment frag)fset ?
{}len?
number of nodes in fraghidx?
the index of head node in fragfor s = 0 to hidx dofor e = hidx to len?
1 doif 0 < e?
s < len?
1 thencreate sub-fragment corecore?
nodes from s to eadd core to fsetcreate sub-fragment shellinitialize shell with head nodeshell?
nodes not in coreadd shell to fsetend ifend forend forend functionSuch a decomposition of an HD fragment en-ables us to create translation rules extracted fromsub-structures and create a pseudo-forest fromthe input dependency tree to make better use ofsmaller rules.4.1 Sub-structural RulesIn the Dep2Str model, rules are extracted onan entire HD fragment.
In this paper, whenthe decomposition is considered, we also extractsub-structural rules by taking each possible sub-fragment as a new HD fragment.
The algorithmfor recognizing the sub-fragments is shown in Al-gorithm 2.In Algorithm 2, we find all possible decom-126positions of an HD fragment.
Each decom-position produces two sub-fragments: core andshell.
Both core and shell include the head node.core contains the dependents surrounding the headnode, with the remaining dependents belonging toshell.
Taking Figure 4 as an example, the bottom-right part is core, while the bottom-left part isshell.
Each core and shell could be seen as anew HD fragment.
Then HD rules are extracted asdefined in the Dep2Str model.Note that different from the augmented HDrules, where Meng et al.
(2013) annotate rules withcombined variables and Xie et al.
(2014) createspecial rules from HD rules at runtime by com-bining several nodes, our sub-structural rules arestandard HD rules, which are extracted from theconnected sub-structures of a larger HD fragmentand can be used directly in the model.4.2 Pseudo-ForestAlthough sub-structural rules are effective in ourexperiments (see Section 5), we still do not usethem to their best advantage, because we only en-rich smaller rules in our model.
During decod-ing, for a large input HD fragment, the model isstill more likely to resort to glue rules.
However,the idea of decomposition allows us to create apseudo-forest directly from the dependency tree toalleviate this problem to some extent.As described above, an HD fragment can beseen as being created by combining two smallerfragments.
This means, for an HD fragment in theinput dependency tree, we can translate one of itssub-fragments first, then obtain the whole trans-lation by combining with translations of anothersub-fragment.
From Algorithm 2, we know thatthe sub-fragment core covers a continuous phraseof the source sentence.
Accordingly, we can trans-late this fragment first and then build the wholetranslation by translating another sub-fragmentshell.
Figure 5 gives an example of translatingan HD fragment by combining the translations ofits sub-fragments.Instead of taking the dependency tree as the in-put and looking for all rules for translating sub-fragments of a whole HD, we directly encode thedecomposition into the input dependency tree withthe result being a pseudo-forest.
Based on thetransformation algorithm in Section 3, the pseudo-forest can also be represented in the constituent-tree style, as shown in Figure 6.Yu  GuohuiR ule:  ( ?)
??
and  p ar lim entZongtong??/NNYu?/CCGuohui?
?/NNZongtongR ule:  ( ??)
x 1 : N N p r es id ential x1p r es id ential and  p ar liam entZongtong??
and  p ar liam entGuohui?
?/NN( a)( b )( c )Figure 5: An example of translating a large HDfragment with the help of translations of its de-composed fragments.SN N : L 1 V V : H 0N N : L 2 C C : L 1 N N : H 0N N : H 0N N : R 1Xuanju??
: R 1N N : L 1Guohui??
: L 1Boliweiya????Juxing??Zongtong??Yu?Guohui??Xuanju?
?N N : L 1N N : H 0V V : H 0V V : H 0Figure 6: An example of a pseudo-forest for thedependency tree in Figure 1.
It is represented us-ing the constituent-tree style described in Section3.
Edges drawn in the same type of line are ownedby the same sub-tree.
Solid lines are shared edges.In the pseudo-forest, we actually only create aforest structure for each HD fragment.
For ex-ample, based on Figure 5, we create a constituentnode labelled with ?NN:H0?
that covers the sub-fragment ?Yu(?)Guohui???.
In so doing, a new node la-belled with ?NN:L1?
is also created, which coversthe Node ?Zongtong??
?, because it is now the first leftdependent in the sub-fragment ?Zongtong(??)Guohui??
?.Compared to the forest-based model (Mi et al.,2008), such a pseudo-forest cannot efficiently re-duce the influence of parsing errors, but it is easilyavailable and compatible with the Dep2Str Model.127corpus sentences words(ch) words(en)train 1,501,652 38,388,118 44,901,788dev 878 22,655 26,905MT04 1,597 43,719 52,705MT05 1,082 29,880 35,326Table 1: Chinese?English corpus.
For the Englishdev and test sets, words counts are averaged across4 references.corpus sentences words(de) words(en)train 2,037,209 52,671,991 55,023,999dev 3,003 72,661 74,753test12 3,003 72,603 72,988test13 3,000 63,412 64,810Table 2: German?English corpus.
In the dev andtest sets, there is only one English reference foreach German sentence.5 ExperimentsWe conduct large-scale experiments to exam-ine our methods on the Chinese?English andGerman?English translation tasks.5.1 DataThe Chinese?English training corpus is fromthe LDC data, including LDC2002E18,LDC2003E07, LDC2003E14, LDC2004T07,the Hansards portion of LDC2004T08 andLDC2005T06.
We take NIST 2002 as the de-velopment set to tune weights, and NIST 2004(MT04) and NIST 2005 (MT05) as the test data toevaluate the systems.
Table 1 provides a summaryof the Chinese?English corpus.The German?English training corpus is fromWMT 2014, including Europarl V7 and NewsCommentary.
News-test 2011 is taken as the de-velopment set, while News-test 2012 (test12) andNews-test 2013 (test13) are our test sets.
Table 2provides a summary of the German?English cor-pus.5.2 BaselineFor both language pairs, we filter sentence pairslonger than 80 words and keep the length ratioless than or equal to 3.
English sentences are to-kenized with scripts in Moses.
Word alignment isperformed by GIZA++ (Och and Ney, 2003) withthe heuristic function grow-diag-final-and (Koehnet al., 2003).
We use SRILM (Stolcke, 2002) toSystems MT05XJ 33.91D2S 33.79Table 3: BLEU score [%] of the Dep2Str modelbefore (XJ) and after (D2S) dependency tree be-ing transformed.
Systems are trained on a selected1.2M Chinese?English corpus.train a 5-gram language model on the Xinhua por-tion of the English Gigaword corpus 5th editionwith modified Kneser-Ney discounting (Chen andGoodman, 1996).
Minimum Error Rate Train-ing (Och, 2003) is used to tune weights.
Case-insensitive BLEU (Papineni et al., 2002) is used toevaluate the translation results.
Bootstrap resam-pling (Koehn, 2004) is also performed to computestatistical significance with 1000 iterations.We implement the baseline Dep2Str modelin Moses with methods described in this paper,which is denoted as D2S.
The first experiment wedo is to sanity check our implementation.
Thuswe take a separate system (denoted as XJ) forcomparison which implements the Dep2Str modelbased on (Xie et al., 2011).
As shown in Table3, using the transformation of dependency trees,the Dep2Str model implemented in Moses (D2S)is comparable with the standard implementation(XJ).In the rest of this section, we describe exper-iments which compare our system with MosesHPB (default setting), and test whether our de-composition approach improves performance overthe baseline D2S.As described in Section 2, the Dep2Str modelonly extracts phrase rules for translating a sourceword (head rule).
This model could be enhancedby including phrase rules that cover more than onesource word.
Thus we also conduct experimentswhere phrase pairs5are added into our system.
Weset the length limit for phrase 7.5.3 Chinese?EnglishIn the Chinese?English translation task, the Stan-ford Chinese word segmenter (Chang et al., 2008)is used to segment Chinese sentences into words.The Stanford dependency parser (Chang et al.,2009) parses a Chinese sentence into the projec-tive dependency tree.5In this paper, the use of phrasal rules is similar to that ofthe HPB model, so they can be handled by Moses directly.128Systems MT04 MT05Moses HPB 35.56 33.99D2S 33.93 32.56+pseudo-forest 34.28 34.10+sub-structural rules 34.78 33.63+pseudo-forest 35.46 34.13+phrase 36.76* 34.67*Table 4: BLEU score [%] of our method andMoses HPB on the Chinese?English task.
We usebold font to indicate that the result of our methodis significantly better than D2S at p ?
0.01 level,and * to indicate the result is significantly betterthan Moses HPB at p ?
0.01 level.Table 4 shows the translation results.
We findthat the decomposition approach proposed in thispaper, including sub-structural rules and pseudo-forest, improves the baseline system D2S sig-nificantly (absolute improvement of +1.53/+1.57(4.5%/4.8%, relative)).
As a result, our sys-tem achieves comparable (-0.1/+0.14) results withMoses HPB.
After including phrasal rules, oursystem performs significantly better (absolute im-provement of +1.2/+0.68 (3.4%/2.0%, relative))than Moses HPB on both test sets.65.4 German?EnglishWe tokenize German sentences with scripts inMoses and use mate-tools7to perform morpho-logical analysis and parse the sentence (Bohnet,2010).
Then the MaltParser8converts the parseresult into the projective dependency tree (Nivreand Nilsson, 2005).Experimental results in Table 5 show that incor-porating sub-structural rules improves the base-line D2S system significantly (absolute improve-ment of +0.47/+0.63, (2.3%/2.8%, relative)), andachieves a slightly better (+0.08) result on test12than Moses HPB.
However, in the German?English task, the pseudo-forest produces a neg-ative effect on the baseline system (-0.07/-0.45),despite the fact that our system combining bothmethods together is still better (+0.2/+0.11) thanthe baseline D2S.
In the end, by resorting to6In our preliminary experiments, phrasal rules are alsoable to significantly improve our system on their own on bothChinese?English and German?English tasks, but the best per-formance is achieved by combining them with sub-structuralrules and/or pseudo-forest.7http://code.google.com/p/mate-tools/8http://www.maltparser.org/Systems test12 test13Moses HPB 20.44 22.77D2S 20.05 22.13+pseudo-forest 19.98 21.68+sub-structural rules 20.52 22.76+phrase 20.91* 23.46*+pseudo-forest 20.25 22.24+phrase 20.75* 23.20*Table 5: BLEU score [%] of our method andMoses HPB on German?English task.
We usebold font to indicate that the result of our methodis significantly better than baseline D2S at p ?0.01 level, and * to indicate the result is signifi-cantly better than Moses HPB at p ?
0.01 level.Systems# RulesCE task DE taskMoses HPB 388M 684MD2S 27M 41M+sub-structural rules 116M 121M+phrase 215M 274MTable 6: The number of rules in different sys-tems On the Chinese?English (CE) and German?English (DE) corpus.
Note that pseudo-forest (notlisted) does not influence the number of rules.phrasal rules, our system achieves the best perfor-mance overall which is significantly better (abso-lute improvement of +0.47/+0.59 (2.3%/2.6%, rel-ative)) than Moses HPB.5.5 DiscussionBesides long-distance reordering (Xie et al.,2011), another attraction of the Dep2Str model isits simplicity.
It can perform fast translation withfewer rules than HPB.
Table 6 shows the numberof rules in each system.
It is easy to see that all ofour systems use fewer rules than HPB.
However,the number of rules is not proportional to transla-tion quality, as shown in Tables 4 and 5.Experiments on the Chinese?English corpusshow that it is feasible to translate the dependencytree via transformation for the Dep2Str model de-scribed in Section 2.
Such a transformation causesthe model to be easily integrated into Moses with-out making changes to the decoder, while at thesame time producing comparable results with thestandard implementation (shown in Table 3).The decomposition approach proposed in this129paper also shows a positive effect on the base-line Dep2Str system.
Especially, sub-structuralrules significantly improve the Dep2Str model onboth Chinese?English and German?English tasks.However, experiments show that the pseudo-forestsignificantly improves the D2S system on theChinese?English data, while it causes translationquality to decline on the German?English data.Since using the pseudo-forest in our system isaimed at translating larger HD fragments via split-ting it into pieces, we hypothesize that when trans-lating German sentences, the pseudo-forest ap-proach more likely results in much worse rules be-ing applied.
This is probably due to the shorterMean Dependency Distance (MDD) and freerword order of German sentences(Eppler, 2013).6 ConclusionIn this paper, we present an open source mod-ule which integrates a dependency-to-string modelinto Moses.This module transforms an input depen-dency tree into a corresponding constituent treeduring decoding which makes Moses performdependency-based translation without necessitat-ing any changes to the decoder.
Experiments onChinese?English show that the performance if oursystem is comparable with that of the standarddependency-based decoder.Furthermore, we enhance the model by de-composing head-dependent fragments into smallerpieces.
This decomposition enriches the Dep2Strmodel with more rules during training and allowsus to create a pseudo-forest as input instead ofa dependency tree during decoding.
Large-scaleexperiments on Chinese?English and German?English tasks show that this decomposition cansignificantly improve the baseline dependency-to-string model on both language pairs.
Onthe German?English task, sub-structural rules aremore useful than the pseudo-forest input.
In theend, by resorting to phrasal rules, our systemperforms significantly better than the hierarchicalphrase-based model in Moses.Our implementation of the dependency-to-string model with methods described in this pa-per is available at http://computing.dcu.ie/?liangyouli/dep2str.zip.
In the fu-ture, we would like to conduct more experimentson other language pairs to examine this model,as well as reducing the restrictions on decompo-sition.AcknowledgmentsThis research has received funding from the Peo-ple Programme (Marie Curie Actions) of the Eu-ropean Union?s Seventh Framework ProgrammeFP7/2007-2013/ under REA grant agreement no.317471.
This research is also supported by theScience Foundation Ireland (Grant 12/CE/I2267)as part of the Centre for Next Generation Local-isation at Dublin City University.
The authors ofthis paper also thank the reviewers for helping toimprove this paper.ReferencesBernd Bohnet.
2010.
Very High Accuracy and FastDependency Parsing is Not a Contradiction.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, pages 89?97, Beijing,China.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese Word Seg-mentation for Machine Translation Performance.
InProceedings of the Third Workshop on StatisticalMachine Translation, pages 224?232, Columbus,Ohio.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminative Re-ordering with Chinese Grammatical Relations Fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation, pages51?59, Boulder, Colorado.Stanley F. Chen and Joshua Goodman.
1996.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
In Proceedings of the 34th AnnualMeeting on Association for Computational Linguis-tics, pages 310?318, Santa Cruz, California.John Cocke and Jacob T. Schwartz.
1970.
Program-ming Languages and Their Compilers: PreliminaryNotes.
Technical report, Courant Institute of Math-ematical Sciences, New York University, New York,NY.Michael Collins, Lance Ramshaw, Jan Haji?c, andChristoph Tillmann.
1999.
A Statistical Parser forCzech.
In Proceedings of the 37th Annual Meetingof the Association for Computational Linguistics onComputational Linguistics, pages 505?512, CollegePark, Maryland.Eva M. Duran Eppler.
2013.
Dependency Distanceand Bilingual Language Use: Evidence from Ger-man/English and Chinese/English Data.
In Proceed-ings of the Second International Conference on De-pendency Linguistics (DepLing 2013), pages 78?87,Prague, August.130Heidi J.
Fox.
2002.
Phrasal Cohesion and Statis-tical Machine Translation.
In Proceedings of theACL-02 Conference on Empirical Methods in Nat-ural Language Processing - Volume 10, pages 304?3111, Philadelphia.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.A Syntax-directed Translator with Extended Do-main of Locality.
In Proceedings of the Workshopon Computationally Hard Problems and Joint Infer-ence in Speech and Language Processing, pages 1?8, New York City, New York.Richard Hudson.
1990.
English Word Grammar.Blackwell, Oxford, UK.Tadao Kasami.
1965.
An Efficient Recognition andSyntax-Analysis Algorithm for Context-Free Lan-guages.
Technical report, Air Force Cambridge Re-search Lab, Bedford, MA.Philipp Koehn.
2004.
Statistical Significance Testsfor Machine Translation Evaluation.
In Proceedingsof EMNLP 2004, pages 388?395, Barcelona, Spain,July.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the 45th Annual Meeting of theACL on Interactive Poster and Demonstration Ses-sions, pages 177?180, Prague, Czech Republic.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, pages 48?54, Edmonton, Canada.Arul Menezes and Chris Quirk.
2005.
DependencyTreelet Translation: The Convergence of Statisticaland Example-Based Machine-translation?
In Pro-ceedings of the Workshop on Example-based Ma-chine Translation at MT Summit X, September.Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u,and Qun Liu.
2013.
Translation with Source Con-stituency and Dependency Trees.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1066?1076, Seattle,Washington, USA, October.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-Based Translation.
In Proceedings of ACL-08: HLT,pages 192?199, June.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-Projective Dependency Parsing.
In Proceedings ofthe 43rd Annual Meeting on Association for Com-putational Linguistics, pages 99?106, Ann Arbor,Michigan.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting on Association for Com-putational Linguistics - Volume 1, pages 160?167,Sapporo, Japan.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency Treelet Translation: Syntactically In-formed Phrasal SMT.
In Proceedings of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 271?279, AnnArbor, Michigan, June.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-Dependency Statistical Machine Transla-tion.
Computational Linguistics, 36(4):649?671,December.Andreas Stolcke.
2002.
SRILM-an Extensible Lan-guage Modeling Toolkit.
In Proceedings Interna-tional Conference on Spoken Language Processing,pages 257?286, November.Fei Xia and Martha Palmer.
2001.
Converting De-pendency Structures to Phrase Structures.
In Pro-ceedings of the First International Conference onHuman Language Technology Research, pages 1?5,San Diego.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A NovelDependency-to-string Model for Statistical MachineTranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 216?226, Edinburgh, United Kingdom.Jun Xie, Jinan Xu, and Qun Liu.
2014.
AugmentDependency-to-String Translation with Fixed andFloating Structures.
In Proceedings of the 25th In-ternational Conference on Computational Linguis-tics, pages 2217?2226, Dublin, Ireland.Deyi Xiong, Qun Liu, and Shouxun Lin.
2007.
A De-pendency Treelet String Correspondence Model forStatistical Machine Translation.
In Proceedings ofthe Second Workshop on Statistical Machine Trans-lation, pages 40?47, Prague, June.Daniel H. Younger.
1967.
Recognition and Parsing ofContext-Free Languages in Time n3.
Informationand Control, 10(2):189?208.Arnold M. Zwicky.
1985.
Heads.
Journal of Linguis-tics, 21:1?29, 3.131
