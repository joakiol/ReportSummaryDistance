Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929?937,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPHypernym Discovery Based on Distributional Similarityand Hierarchical StructuresIchiro Yamada?, Kentaro Torisawa?, Jun?ichi Kazama?, Kow Kuroda?,Masaki Murata?, Stijn De Saeger?, Francis Bond?
and Asuka Sumida?
?National Institute of Information and Communications Technology3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp?Japan Advanced Institute of Science and Technology1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPANa-sumida@jaist.ac.jpAbstractThis paper presents a new method of devel-oping a large-scale hyponymy relation data-base by combining Wikipedia and other Webdocuments.
We attach new words to the hy-ponymy database extracted from Wikipediaby using distributional similarity calculatedfrom documents on the Web.
For a given tar-get word, our algorithm first finds k similarwords from the Wikipedia database.
Then,the hypernyms of these k similar words areassigned scores by considering the distribu-tional similarities and hierarchical distancesin the Wikipedia database.
Finally, new hy-ponymy relations are output according to thescores.
In this paper, we tested two distribu-tional similarities.
One is based on raw verb-noun dependencies (which we call ?RVD?
),and the other is based on a large-scale clus-tering of verb-noun dependencies (called?CVD?).
Our method achieved an attachmentaccuracy of 91.0% for the top 10,000 rela-tions, and an attachment accuracy of 74.5%for the top 100,000 relations when usingCVD.
This was a far better outcome com-pared to the other baseline approaches.
Ex-cluding the region that had very high scores,CVD was found to be more effective thanRVD.
We also confirmed that most relationsextracted by our method cannot be extractedmerely by applying the well-known lexico-syntactic patterns to Web documents.1 IntroductionLarge-scale taxonomies such as WordNet (Fell-baum 1998) play an important role in informa-tion extraction and question answering.
However,extremely high costs are borne to manually en-large and maintain such taxonomies.
Thus, appli-cations using these taxonomies tend to face thedrawback of data sparseness.
This paper presentsa new method for discovering a large set of hy-ponymy relations.
Here, a word1 X is regarded asa hypernym of a word Y if Y is a kind of X or Yis an instance of X.
We are able to generatelarge-scale hyponymy relations by attaching newwords to the hyponymy database extracted fromWikipedia (referred to as ?Wikipedia relationdatabase?)
by using distributional similarity cal-culated from Web documents.
Relations ex-tracted from Wikipedia are relatively clean.
Onthe other hand, reliable distributional similaritycan be calculated using a large number of docu-ments on the Web.
In this paper, we combine theadvantages of these two resources.Using distributional similarity, our algorithmfirst computes k similar words for a target word.Then, each k similar word assigns a score to itsancestors in the hierarchical structures of theWikipedia relation database.
The hypernym thathas the highest score for the target word is se-lected as the hypernym of the target word.
Figure1 is an overview of the proposed approach.In the experiment, we extracted hypernyms forapproximately 670,000 target words that are notincluded in the Wikipedia relation database butare found on the Web.
We tested two distribu-tional similarities: one based on raw verb-noundependencies (RVD) and the other based on alarge-scale clustering of verb-noun dependencies(CVD).
The experimental results showed that theproposed methods were more effective than theother baseline approaches.
In addition, we con-firmed that most of the relations extracted by ourmethod could not be extracted using the lexico-syntactic pattern-based method.In the remainder of this paper, we first intro-1 In this paper, we use the term ?word?
for both ?asingle-word word?
and ?a multi-word word.
?929duce some related works in Section 2.
Section 3describes the Wikipedia relation database.
Sec-tion 4 describes the distributional similarity cal-culated by the two methods.
In Section 5, wedescribe a method to discover an appropriatehypernym for each target word.
The experimen-tal results are presented in Section 6 before con-cluding the paper in Section 7.2 Related WorksMost previous researchers have relied on lex-ico-syntactic patterns for hyponymy acquisition.Lexico-syntactic patterns were first used byHearst (1992).
The patterns used by her included?NP0 such as NP1,?
in which NP0 is a hypernymof NP1.
Using these patterns as seeds, Hearst dis-covered new patterns by which to semi-automatically extract hyponymy relations.
Pantelet al (2004a) proposed a method to automatical-ly discover the patterns using a minimal edit dis-tance.
Ando et al (2003) applied predefined lex-ico-syntactic patterns to Japanese news articles.Snow et al (2005) generalized these lexico-syntactic pattern-based methods by using depen-dency path features for machine learning.
Then,they extended the framework such that this me-thod was capable of making use of heterogenousevidence (Snow et al 2006).
These pattern-basedmethods require the co-occurrences of a targetword and the hypernym in a document.
It shouldbe noted that the requirement of such co-occurrences actually poses a problem when weextract a large set of hyponymy relations sincethey are not frequently observed (Shinzato et al2004, Pantel et al 2004b).Clustering-based methods have been proposedas another approach.
Caraballo (1999), Pantel etal.
(2004b), and Shinzato et al (2004) proposed amethod to find a common hypernym for wordclasses, which are automatically constructed us-ing some measures of word similarities or hierar-chical structures in HTML documents.
Etzioni etal.
(2005) used both a pattern-based approachand a clustering-based approach.
The requiredamount of co-occurrences is significantly re-duced due to class-based generalizationprocesses.
Note that these clustering-based me-thods obtain the same hypernym for all the wordsin a particular class.
This causes a problem forselecting an appropriate hypernym for each wordin the case when the granularity or the construc-tion of the classes is incorrect.
Figure 2 showsthe drawbacks of the existing approaches.Ponzetto et al (2007) and Sumida et al (2008)proposed a method for acquiring hyponymy rela-tions from Wikipedia.
This Wikipedia-based ap-proach can extract a large volume of hyponymyrelations with high accuracy.
However, it is alsotrue that this approach does not account for manywords that usually appear in Web documents;this could be because of the unbalanced topics inWikipedia or merely because of the incompletecoverage of articles on Wikipedia.
Our methodcan target words that frequently appear on theWeb but are not included in the Wikipedia rela-tion database, thus making the results of the Wi-kipedia-based approach richer and more ba-lanced.
Our approach uses distributional similari-Figure 1: Overview of the proposed approach.hypernym :Target word:  Selected from the Web: wordk similar wordsNo direct co-occurrences ofhypernym and hyponym incorpora are needed.Selected from hypernyms in theWikipedia relation database.A hypernym is selected foreach word independently.Wikipedia relation databaseWikipedia-based approach(Ponzetto et al 2007 andSumida et al 2008)Hyponymy relations areextracted using the layoutinformation of Wikipedia.WikipediaFigure 2: Drawbacks in existing approaches for hypo-nymy acquisition.Pattern-based method(Hearst 1992, Pantel et al2004a, Ando et al 2003,Snow et al 2005, Snow et al2006, and Etzioni et al 2005)Clustering-based method(Caraballo 1999, Pantel et al2004b, Shinzato et al 2004,and Etzioni et al 2005)DocumentsCorpus/documentsCo-occurrencesin a pattern areneededhypernym such as word hypernym ..?
wordwordwordwordwordWord ClasswordThe same hypernymis selected for allwords in a class.930ty, which is computed based on the noun-verbdependency profiles on the Web.
The use of dis-tributional similarity resembles the clustering-based approach; however, our method can selecta hypernym for each word independently, and itdoes not suffer from class granularity mismatchor the low quality of classes.
In addition, our ap-proach exploits the hierarchical structures of theWikipedia hypernym relations.3 Wikipedia Relation DatabaseOur Wikipedia relation database is based on theextraction method of Sumida et al (2008).
Theyproposed a method of automatically acquiringhyponymy relations by focusing on the hierar-chical layout of articles on Wikipedia.
By way ofan example, Figure 3 shows part of the sourcecode clipped from the article titled ?Penguin.
?An article has hierarchical structures composedof titles, sections, itemizations, etc.
The entirearticle is divided into sections titled ?Anatomy,?
?Mating habits,?
?Systematics and evolution,?
?Penguins in popular culture,?
and so on.
Thesection ?Systematics and evolution?
has a sub-section ?Systematics,?
which is further dividedinto ?Aptenodytes,?
?Eudyptes,?
and so on.Some of these section-subsection relations can beregarded as valid hyponymy relations.
In thisarticle, relations such as the one between ?Apte-nodytes?
and ?Emperor Penguin?
and that be-tween ?Book?
and ?Penguins of the World?
arevalid hyponymy relations.First, Sumida et al (2008) extracted hypony-my relation candidates from hierarchical struc-tures on Wikipedia.
Then, they selected properhyponymy relations using a support vector ma-chine classifier.
They used several kinds of fea-tures for the hyponymy relation candidate, suchas a POS tag for each word, the appearance ofmorphemes of each word, the distance betweentwo words in the hierarchical structures of Wiki-pedia, and the last character of each word.
As aresult of their experiments, approximately 2.4million hyponymy relations in Japanese wereextracted, with a precision rate of 90.1%.Compared to the traditional taxonomies, theseextracted hyponymy relations have the followingcharacteristics (Fellbaum 1998, Bond et al 2008).
(a) The database includes a more extensivevocabulary.
(b)  The database includes a large number ofnamed entities.Popular Japanese taxonomies GoiTaikei (Ike-hara et al 1997) and Bunrui-Goi-Hyo (1996)contain approximately 300,000 words and96,000 words, respectively.
In contrast, the ex-tracted hyponymy relations contain approximate-ly 1.2 million hyponyms and are undoubtedlymuch larger than the existing taxonomies.Another difference is that since Wikipedia coversa large number of named entities, the extractedhyponymy relations also contain a large numberof named entities.Note that the extracted relations have a hierar-chical structure because one hypernym of a cer-tain word may also be the hyponym of anotherhypernym.
However, we observed that the depthof the hierarchy, on an average, is extremelyshallow.
To make the hierarchy appropriate forour method, we extended these into a deeper hie-rarchical structure.
The extracted relations in-clude many compound nouns as hypernyms, andwe decomposed a compound noun into a se-quence of nouns using a morphological analyzer.Since Japanese is a head-final language, the suf-fix of a noun sequence becomes the hypernym ofthe original compound noun if the suffix formsanother valid compound noun.
We extracted suf-fixes of compound nouns and manually checkedwhether they were valid compound nouns; then,we constructed a hierarchy of compound nouns.The hierarchy can be extended such that it in-cludes the hyponyms of the original hypernymand the resulting hierarchy constitutes a hierar-chical taxonomy.
We use this hierarchical tax-onomy as a target for expansion.22  Note that this modification was performed as part ofanother project of ours aimed at constructing a large-scaleand clean hypernym knowledge base by human annotation.We do not think this cost is directly relevant to the methodproposed here.Figure 3: A part of source code clipped from thearticle ?Penguin?
in Wikipedia.
'''Penguins''' are a group of[[Aquatic animal|aquatic]],[[flightless bird]]s.== Anatomy ==== Mating habits ====Systematics and evolution=====Systematics===* Aptenodytes**[[Emperor Penguin]]** [[King Penguin]]* Eudyptes== Penguins in popular culture ==== Book ==* Penguins* Penguins of the World== Notes ==* Penguinone* the [[Penguin missile]][[Category:Penguins]][[Category:Birds]]9314 Distributional SimilarityThe distributional hypothesis states that wordsthat occur in similar contexts tend to be semanti-cally similar (Harris 1985).
In this section, wefirst introduce distributional similarity based onraw verb-noun dependencies (RVD).
To avoidthe sparseness problem of the co-occurrence ofverb-noun dependencies, we also use distribu-tional similarity based on a large-scale clusteringof verb-noun dependencies (CVD).In the experiment mentioned in the followingsection, we used the TSUBAKI corpus (Shinzatoet al 2008) to calculate distributional similarity.This corpus provides a collection of 100 millionJapanese Web pages containing 6 ?
109sentences.4.1 Distributional Similarity Based on RVDWhen calculating the distributional similaritybased on RVD, we use the triple <v, rel, n>,where v is a verb, n is a noun phrase, and relstands for the relation between v and n. In Japa-nese, a relation rel is represented by postposi-tions attached to n and the phrase composed of nand rel modifies v. Each triple is divided into twoparts.
The first is <v, rel> and the second is n.Then, we consider the conditional probability ofoccurrence of the pair <v, rel>: P(<v, rel>|n).P(<v, rel>|n) can be regarded as the distributionof the grammatical contexts of the noun phrase n.The distributional similarity can be defined asthe distance between these distributions.
Thereare several kinds of functions for evaluating thedistance between two distributions (Lee 1999).Our method uses the Jensen-Shannon divergence.The Jensen-Shannon divergence between twoprobability distributions, )|( 1nP ?
and )|( 2nP ?
,can be calculated as follows:)),2)|()|(||)|(()2)|()|(||)|(((21))|(||)|((21221121nPnPnPDnPnPnPDnPnPDKLKLJS?+??+?+??=?
?where DKL indicates the Kullback-Leibler diver-gence and is defined as follows:.
)|()|(log)|())|(||)|((21121 ?
???=??
nP nPnPnPnPDKLFinally, the distributional similarity betweentwo words, n1 and n2, is defined as follows:)).|(||)|((1),( 2121 nPnPDnnsim JS ??
?=This similarity assumes a value from 0 to 1.
Iftwo words are similar, the value will be close to1; if two words have entirely different meanings,the value will be 0.In the experiment, we used 1,000,000 nounphrases and 100,000 pairs of verbs and postposi-tions to calculate the probability P(<v, rel>|n)from the dependency relations extracted from theabove-mentioned Web corpus (Shinzato et al2008).
The probabilities are computed using thefollowing equation by modifying for the fre-quency using the log function:?>?<+><+><=><DrelvnrelvfnrelvfnrelvP,1),,(log(1)),,(log()|,(,0),,(if >>< nrelvfwhere f(<v, rel, n>) is the frequency of a triple<v, rel, n> and D is the set defined as { <v, rel > |f(<v, rel, n>) > 0 }.
In the case of f(<v, rel, n>) =0, P(<v, rel>|n) is set to 0.Instead of using the observed frequency di-rectly as in the usual maximum likelihood esti-mation, we modified it as above.
Although thismight seems strange, this kind of modification iscommon in information retrieval as a termweighing method (Manning et al 1999) and  it isalso applied in some studies to yield better wordsimilarities (Terada et al 2006, Kazama et al2009).
We also adopted this idea in this study.4.2 Distributional Similarity Based on CVDRooth et al (1999) and Torisawa (2001) showedthat EM-based clustering using verb-noun de-pendencies can produce semantically clean nounclusters.
We exploit these EM-based clusteringresults as the smoothed contexts for noun n. InTorisawa?s model (2001), the probability of oc-currence of the triple <v, rel, n> is defined asfollows:,)()|()|,(),,(?
?
><=><Aadef aPanParelvPnrelvPwhere a denotes a hidden class of <v,rel> and n.In this equation, the probabilities P(<v,rel>|a),P(n|a), and P(a) cannot be calculated directlybecause class a is not observed in a given corpus.The EM-based clustering method estimates theseprobabilities using a given corpus.
In the E-step,932the probability P(a|<v,rel>) is calculated.
In theM-step, the probabilities P(<v,rel>|a), P(n|a),and P(a) are updated to arrive at the maximumlikelihood using the results of the E-step.
Fromthe results of estimation of this EM-based clus-tering method, we can obtain the probabilitiesP(<v,rel>|a), P(n|a), and P(a) for each <v, rel>, n,and a.
Then, P(a|n) is calculated by the followingequation:.
)()|()()|()|( ?
?= Aa aPanPaPanPnaPP(a|n) can be used to find the class of n. Forexample, the class that has the maximum P(a|n)can be regarded as the class to which n belongs.Noun phrases that occur with similar pairs<v,rel> tend to be classified in the same class.Kazama et al (2008) proposed the paralleliza-tion of this EM-based clustering with the aim ofenabling large-scale clustering and using the re-sulting clusters in named entity recognition.
Ka-zama et al (2009) reported the calculation ofdistributional similarity using the clustering re-sults.
The distributional similarity was calculatedby the Jensen-Shannon divergence, which wasused in this paper.
Similar to the case in Kazamaet al, we performed word clustering using1,000,000 noun phrases and 2,000 classes.
Notethat the frequencies of dependencies were mod-ified with the log function, as in RVD, describedin the previous section.5 Discovering an Appropriate Hyper-nym for a Target wordIn the Wikipedia relation database, there areabout 95,000 hypernyms and about 1.2 millionhyponyms.
In both RVD and CVD, the wordsused were selected according to the number (thenumber of kinds, not the frequency) of <v, rel >sthat n has dependencies in the data.
As a result, 1million words were selected.
The number ofcommon words that are also included in the Wi-kipedia relation database are as follows:Hypernyms     28,015 (common hypernyms)Hyponyms   175,022 (common hyponyms)These common hypernyms become candidatesfor hypernyms for a target word.
On the otherhand, the common hyponyms are used as cluesfor identifying appropriate hypernyms.In our task, the potential target words areabout 810,000 in number and are not included inthe Wikipedia relation database.
These includesome strange words or word phrases that are ex-tracted due to the failure of morphological analy-sis.
We exclude these words using simple rules.Consequently, the number of target words for ourprocess is reduced to about 670,000.In the following section, we outline the scor-ing method that uses k similar words to discoveran appropriate hypernym for a target word.
Wealso explain several baseline approaches that usedistributional similarity.5.1 Scoring with k similar WordsIn this approach, we first calculate the similari-ties between the common hyponyms and a targetword and select the k most similar common hy-ponyms.
Here, we use a similarity threshold val-ue Smin to avoid the effect of words having lowersimilarities.
If the similarity is less than the thre-shold value, the word is excluded from the set ofk similar words.
Next, each k similar word votesa score to its ancestors in the hierarchical struc-tures of the Wikipedia relation database.
Thescore used to vote for a hypernym nhyper is as fol-lows:,),()()()(1),(????
?=trghyperhypohypohypernksimilarnDescnhypotrgnnrhypernnsimdnscorewhere ntrg is the target word, Desc(nhyper) is thedescendant of the hypernym nhyper, ksimilar(ntrg)is the k similar word of ntrg,1),( ?hypohyper nnrd is apenalty that depends on the differences in thedepth of hierarchy, d is a parameter for the penal-ty value and has a value between 0 and 1, andr(ntrg, nhypo) is the difference in the depth of hie-rarchy between ntrg and nhypo.
sim(ntrg,nhypo) is adistributional similarity between ntrg and nhypo.As a result of scoring, each hypernym has ascore for the target word.
The hypernym that hasthe highest score for the target word is selectedas its hypernym.
The hyponymy relations thusproduced are ranked according to the scores.Figure 4 shows an example of the scoringprocess.
In this example, we use CitroenAX as thetarget word whose hypernym will be identified.First, the k similar words are extracted from thecommon hyponyms in the Wikipedia relation:Opel Astra, TVR Tuscan, Mitsubishi Minica, andRenault Lutecia are extracted.
Next, each k simi-lar word votes a score to its ancestors.
The wordsOpel Astra, TVR Tuscan, and Renault Luteciavote to their parent car and the word Mitsubishi933Minica votes to its parent mini-vehicle and itsgrandparent car with a small penalty.
Finally, thehypernym car, which has the highest score, isselected as the hypernym of the target word Ci-troenAX.5.2 Baseline ApproachesUsing distributional similarity, we can also de-velop the following baseline approaches to dis-cover hyponymy relations.Selecting the hypernym of the most similar hy-ponym (baseline approach 1)We use the heuristics that similar words tend tohave the same hypernym.
In this approach, wefirst calculate the similarities between the com-mon hyponyms and the target word.
The com-mon hyponym most similar to the target word isextracted.
Then, the parent of the extractedcommon hyponym is regarded as the hypernymof the target word.
This approach outputs severalhypernyms when the most similar hyponym hasseveral hypernyms.
This approach can be consi-dered to be the same as the scoring method usingk similar words when k = 1.
We use the distribu-tional similarity between the target word and themost similar hyponym in the Wikipedia relationdatabase as the score for the appropriateness ofthe resulting hyponymy.Selecting the most similar hypernym (baselineapproach 2)The distributional similarity between the com-mon hypernym and the target word is calculated.Then, the hypernym that has the highest distribu-tional similarity is regarded as the hypernym ofthe target word.
The similarity is used as thescore of the appropriateness of the produced hy-ponymy.Scoring based on the average similarity of thehypernym?s children (baseline approach 3)This approach uses the probabilistic distributionsof the hypernym?s children.
We define the prob-ability )|( hyperchild nP ?
characterized by the childrenof the hypernym nhyper, as follows:,)()()|()|()()(????
?=?hyperhypohyperhyponChnhyponChnhypohypohyperchild nPnPnPnPwhere Ch(nhyper) is a set of all children of nhyper.Then, distributional similarities between a com-mon hypernym nhyper and the target word nhypo arecalculated.
The hypernym that has the highestdistributional similarity is selected as the hyper-nym of the word.
This distributional similarity isused as the score of the appropriateness of theproduced hyponymy.If a hypernym has only a few children, the re-liability of the probabilistic distribution ofhypernym defined here will be low because theWikipedia relation database includes some incor-rect relations.
For this reason, we use the hyper-nym only if the number of children it has is morethan a threshold value.6 ExperimentsWe evaluated our proposed methods by using itin experiments to discover hypernyms from theWikipedia relation database for the target wordsextracted from about 670,000 noun phrases.6.1 Parameter Estimation by PreliminaryExperimentsIn the proposed methods, there are several para-meters.
We performed parameter optimization byrandomly selecting 694 words as developmentdata in our preliminary experiments.
The hyper-nyms of these words were determined manually.We adjusted the parameters so that each methodachieved the best performance for this develop-ment data.The parameters in the scoring method with ksimilar words were adjusted as follows3:(RVD)Number of similar words:         k = 100.Similarity threshold:           Smin = 0.05.Penalty value for ancestors:    d = 0.6.3 We tested the parameter values k = {100, 200, 300, 400,500, 600, 700, 800, 900, 1000}, Smin={0, 0.05, 0.1, 0.15, 0.2,0.25, 0.3, 0.35, 0.4} and d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75,0.8, 0.85, 0.9, 0.95, 1.0}.Figure 4: Overview of the scoring process.carCitroenAXminivehiclehybridvehicleOpelAstraRenaultLuteciaMitsubishiMinicak similar wordsEach k-similar wordvotes the score to itsancestors in the Wikipediarelation database.Target word selectedfrom the Web text (ntrg).TVRTuscan: common hypernym(nhyper): k similar word &common hyponym(nhypo)x d1x d0x d0934(CVD)Number of similar words:         k = 200.Similarity threshold:                Smin = 0.3.Penalty value for ancestors:    d = 0.6.The parameter in baseline approach 3 was ad-justed as follows:Threshold for the number of children: 20.6.2 Evaluation of the Experimental Resultson the Basis of Score RankingUsing the adjusted parameters, we conductedexperiments to extract the hypernym of each tar-get word with the help of the scoring methodbased on k similar words.
In these experiments,two kinds of distributional similarity mentionedin Section 4 were exploited individually.
Thewords that were used in the development datawere excluded.We also conducted a comparative experimentin which the parameter value for the penalty ofthe hierarchal difference, d, was set to 0 to clari-fy the ability of using hierarchal structures in thek similar words method.
This means each k simi-lar word votes only to their parent.We then judged the quality of each acquiredhypernym.
The evaluation data sets were sam-pled from the top 1,000, 10,000, 100,000, and670,000 results that were ranked according to thescore of each method.
Then, against 200 samplesthat were randomly sampled from each set, oneof the authors judged whether the hypernym ex-tracted by each method for the target word wascorrect or not.
In this evaluation, if the sentence?The target word is a kind of the hypernym?
or?The target word is an instance of the hypernym?was consistent, the extracted hyponymy wasjudged as correct.
It should be noted that the out-puts of the compared methods are combined andshuffled to enable fair comparison.
In addition,baseline approach 1 extracted several hypernymsfor the target word.
In this case, we judged thehypernym as correct when the case where one ofthe hypernyms was correct.The precision of each result is shown in Table1.
The results of the k similar words method arefar better than those of the other baseline me-thods.
In particular, the k similar words methodwith CVD outperformed the methods of the ksimilar words where the parameter value d wasset to 0 and the method using RVD except for thetop 1,000 results.
This means that the use of hie-rarchal structures and the clustering process forcalculating distributional similarity are effectivefor this task.
We confirmed the significant differ-ences of the proposed method (CVD) as com-pared with all the baseline approaches at the 1%significant level by the Fisher?s exact test (Hays1988).The precision of baseline approach 2 that se-lected the most similar hypernym was the worstamong all the methods.
There were words thatwere similar to the target word among the hyper-nyms extracted incorrectly.
For example, theword semento-kojo (cement factory) was ex-tracted for the hypernym of the word kuriningu-kojo (dry cleaning plant).
It is difficult to judgewhether the word is a hypernym or just a similarword by using only the similarity measure.As for the results of baseline approach 1 usingthe most similar hyponym and baseline approach3 using the similarity of the set of hypernym?schildren, the noise on the Wikipedia relation da-tabase decreased the precision.
Moreover, over-specified hypernyms were extracted incorrectlyby these methods.
In contrast, the method ofscoring based on the use of k similar words wasrobust against noise because it uses the votingapproach for the similarities.
Further, this me-thod can extract hypernyms that are not over-specific because it uses all descendants for scor-ing.Table 2 shows some examples of relations ex-tracted by the k similar words method usingCVD.Table 1:  Precision of each approach based on the score ranking.
CVD represents the method that uses the dis-tributional similarity based on large-scale of clustering of verb-noun dependencies.
RVD represents theone based on raw verb-noun dependencies.k-similar words(CVD)k-similar words(RVD)k-similar words(CVD, d = 0)Baselineapproach 1(CVD)Baselineapproach 2(CVD)Baselineapproach 3(CVD)1,000 0.940 1.000 0.850 0.730 0.290 0.63010,000 0.910 0.875 0.875 0.555 0.300 0.445100,000 0.745 0.710 0.730 0.500 0.280 0.435670,000 0.520 0.500 0.470 0.345 0.115 0.1709356.3 Investigation of the Extracted RelationOverlap with a Conventional MethodWe randomly sampled 300 hyponymy rela-tions that were extracted correctly using the ksimilar words method exploiting CVD and inves-tigated whether or not these relations can be ex-tracted by the conventional method based on thelexico-syntactic pattern.
The possible hyponymyrelations were extracted using the pattern-basedmethod (Ando et al 2003) from the TSUBAKIcorpus (Shinzato et al 2008).
From a comparisonof these relations, we found only 57 commonhyponymy relations.
That is, the remaining 243hyponymy relations were not included in thepossible hyponymy relations.
This result indi-cates that our method can acquire the hyponymyrelations that cannot be extracted by the conven-tional pattern-based method.6.4 DiscussionsWe investigated the reason for the errors gener-ated by the method of scoring using k similarwords exploiting CVD.
We conducted experi-ments on hypernym extraction targeting 694words in the development data mentioned in Sec-tion 6.1.
Among these, 286 relations were ex-tracted incorrectly.
In these relations, there weresome frequent hypernyms.
For example, theword sakuhin (work) appeared 28 times and hon(book) appeared 20 times.
As shown in Table 2,hon (book) was also extracted for the target wordmeru-seminah (mail seminar).
It is really diffi-cult even for a human to identify whether thetitle is that of the book or the event.
If we canidentify these difficult hypernyms in advance, wecan improve precision by excluding them fromthe target hypernyms.
This will be one of the top-ics for future study.7 ConclusionIn this paper, we proposed a method for disco-vering hyponymy relations between nouns byfusing the Wikipedia relation database and wordsfrom the Web.
We demonstrated that the methodusing k similar words has high accuracy.
Theexperimental results showed the effectiveness ofusing hierarchal structures and the clusteringprocess for calculating distributional similarityfor this task.
The experimental results showedthat our method could achieve 91.0% attachmentaccuracy for the top 10,000 hyponymy relationsand 74.5% attachment accuracy for the top100,000 relations when using the clustering-based similarity.
We confirmed that most rela-tions extracted by the proposed method could notbe handled by the lexico-syntactic pattern-basedmethod.
Future work will be to filter out difficulthypernyms for hyponymy extraction process toachieve higher precision.ReferencesM.
Ando, S. Sekine and S. Ishizaki.
2003.
AutomaticExtraction of Hyponyms from Newspaper UsingLexicosyntactic Patterns.
IPSJ SIG Notes, 2003-NL-157, pp.
77?82 (in Japanese).F.
Bond, H. Isahara, K. Kanzaki and K. Uchimoto.2008.
Boot-strapping a WordNet Using MultipleExisting WordNets.
In the 6th International Confe-rence on Language Resources and Evaluation(LREC), Marrakech.Bunruigoihyo.
1996.
The National Language Re-search Institute (in Japanese).S.
A. Caraballo.
1999.
Automatic Construction of aHypernym-labeled Noun Hierarchy from Text.
InProceedings of the Conference of the Associationfor Computational Linguistics (ACL).O.
Etzioni, M. Cafarella, D. Downey, A. Popescu, T.Shaked, S. Soderland, D. Weld and A. Yates.
2005.Unsupervised Named-Entity Extraction from theWeb: An Experimental Study.
Artificial Intelli-gence, 165(1):91?134.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalTable2:  Hypernym discovery results by the k-similarwords based approach (CVD).
The underline indi-cates the hypernyms which are extracted incorrectly.Score Target word Extracted hypernym58.6 INDIVI burando(fashion label)54.3 kureome (Cleome) hana (flower)34.4 UOKR  gemu (game)21.7 Okido (Okido) machi (town)20.5 Sumatofotsu(Smart fortwo)kuruma(car)15.6 Fukagawameshi(Fukagawa rice)ryori (dish)8.9 John Barry sakkyokuka(composer)8.5 JVM sofuto-wea(software)6.6 metangasu(methane gas)genso(chemical element)5.4 me-ru semina(mail seminar)Hon (book)3.9 gurometto(grommet)shohin(merchandise)3.1 supuringubakku(spring back)gensho(phenomenon)936Database.
Cambridge, MA: MIT Press.Z.
Harris.
1985.
Distributional Structure.
In Katz, J.
J.(ed.)
The Philosophy of Linguistics, Oxford Uni-versity Press, pp.
26?47.W.
L. Hays.
1988.
Statistics: Analyzing QualitativeData, Rinehart and Winston, Inc., Ch.
18, pp.
769?783.M.
Hearst.
1992.
Automatic Acquisition of Hypo-nyms from Large Text Corpora.
In Proceedings ofthe 14th Conference on Computational Linguistics(COLING), pp.
539?545.S.
Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi.
1997.Goi-Taikei A Japanese Lexicon, Iwanami Shoten.J.
Kazama and K. Torisawa.
2008.
Inducing Gazet-teers for Named Entity Recognition by Large-scaleClustering of Dependency Relations.
In Proceed-ings of ACL-08: HLT, pp.
407?415.J.
Kazama, Stijn De Saeger, K. Torisawa and M. Mu-rata.
2009.
Generating a Large-scale Analogy ListUsing a Probabilistic Clustering Based on Noun-Verb Dependency Profiles.
In 15th Annual Meetingof the Association for Natural LanguageProcessing, C1?3 (in Japanese).L.
Lee.
1999.
Measures of Distributional Similarity.In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics, pp.
25?32.C.
D. Manning and H. Schutze.
1999.
Foundations ofStatistical Natural Language Processing.
The MITPress.P.
Pantel, D. Ravichandran and E. Hovy.
2004a.
To-wards Terascale Knowledge Acquisition.
In Pro-ceedings of the 20th International Conference onComputational Linguistics.P.
Pantel and D. Ravichandran.
2004b.
AutomaticallyLabeling Semantic Classes.
In Proceedings of theHuman Language Technology and North AmericanChapter of the Association for Computational Lin-guistics Conference.S.
P. Ponzetto, and M. Strube.
2007.
Deriving a LargeScale Taxonomy from Wikipedia.
In Proceedingsof the 22nd National Conference on Artificial Intel-ligence, pp.
1440?1445.M.
Rooth, S. Riezler, D. Presher, G. Carroll and F.Beil.
1999.
Inducing a Semantically AnnotatedLexicon via EM-based Clustering.
In Proceedingsof the 37th annual meeting of the Association forComputational Linguistics, pp.
104?111.K.
Shinzato and K. Torisawa.
2004.
Acquiring Hypo-nymy Relations from Web Documents.
In Proceed-ings of HLT-NAACL, pp.
73?80.K.
Shinzato, D. Kawahara, C. Hashimoto and S. Ku-rohashi.
2008.
A Large-Scale Web Data Collectionas A Natural Language Processing Infrastructure.In the 6th International Conference on LanguageResources and Evaluation (LREC).R.
Snow, D. Jurafsky and A. Y. Ng.
2005.
LearningSyntactic Patterns for Automatic Hypernym Dis-covery.
NIPS 2005.R.
Snow, D. Jurafsky, A. Y. Ng.
2006.
Semantic Tax-onomy Induction from Heterogenous Evidence.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pp.
801?808.A.
Sumida, N. Yoshinaga and K. Torisawa.
2008.Boosting Precision and Recall of Hyponymy Rela-tion Acquisition from Hierarchical Layouts in Wi-kipedia.
In the 6th International Conference onLanguage Resources and Evaluation (LREC).A.
Terada, M. Yoshida, H. Nakagawa.
2006.
A Toolfor Constructing a Synonym Dictionary using con-text Information.
In proceedings of IPSJ SIG Tech-nical Reports, vol.2006 No.124, pp.
87-94.
(In Jap-anese).K.
Torisawa.
2001.
An Unsupervised Method for Ca-nonicalization of Japanese Postpositions.
In Pro-ceedings of the 6th Natural Language ProcessingPacific Rim Symposium (NLPRS), pp.
211?218.K.
Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-ma, M. Murata, D. Noguchi and A. Sumida.
2008.TORISHIKI-KAI, An Autogenerated Web SearchDirectory.
In Proceedings of the second interna-tional symposium on universal communication, pp.179?186, 2008.937
