Proceedings of NAACL-HLT 2013, pages 1152?1162,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Participant-based Approach for Event Summarization UsingTwitter StreamsChao Shen1, Fei Liu2, Fuliang Weng2, Tao Li11School of Computing and Information Sciences, Florida International UniversityMiami, Florida 33199, USA2Research and Technology Center, Robert Bosch LLCPalo Alto, California 94304, USA{cshen001, taoli}@cs.fiu.edu{fei.liu, fuliang.weng}@us.bosch.comAbstractTwitter offers an unprecedented advantage onlive reporting of the events happening aroundthe world.
However, summarizing the Twit-ter event has been a challenging task that wasnot fully explored in the past.
In this paper,we propose a participant-based event summa-rization approach that ?zooms-in?
the Twit-ter event streams to the participant level, de-tects the important sub-events associated witheach participant using a novel mixture modelthat combines the ?burstiness?
and ?cohesive-ness?
properties of the event tweets, and gen-erates the event summaries progressively.
Weevaluate the proposed approach on differentevent types.
Results show that the participant-based approach can effectively capture thesub-events that have otherwise been shadowedby the long-tail of other dominant sub-events,yielding summaries with considerably bettercoverage than the state-of-the-art.1 IntroductionTwitter has increasingly become a critical source ofinformation.
People report the events they are ex-periencing or publish comments on a wide varietyof events happening around the world, ranging fromthe unexpected natural disasters, regional riots, tomany scheduled events, such as sports games, po-litical debates, local festivals, and even academicconferences.
The Twitter data streams thus covera broad range of events and broadcast these in-formation in a live manner.
Event summarizationin this paper aims to generate a representative andconcise textual description of the scheduled eventsthat are being lively reported on Twitter, providingpeople with an alternative means of observing theworld beyond the traditional journalism.
Specifi-cally, we investigate scheduled events of differenttypes, including six of the NBA (National Basket-ball Association) sports games and a representativeconference event, namely the Apple CEO?s keynotespeech in the Apple Worldwide Developers Confer-ence (WWDC 2012)1.
All these events have excitedgreat discussion among the Twitter community.Summarizing the Twitter event is a challengingtask that has yet been fully explored in the past.Most previous summarization studies focus on thewell-formatted news documents, as driven by theannual DUC2 and TAC3 evaluations.
In contrast,the Twitter messages (a.k.a., tweets) are very shortand noisy, containing nonstandard terms such as ab-breviations, acronyms, emoticons, etc.
(Liu et al2011b; Liu et al 2012; Eisenstein, 2013).
Thenoisy contents also cause great difficulties to the tra-ditional NLP tools such as NER and dependencyparser (Ritter et al 2011; Foster et al 2011), lim-iting the possibility of applying finer-grained eventanalysis tools.
In nature, the event tweets are closelyassociated with the timeline and are drastically dif-ferent from a static collection of news documents.The tweets converge into text streams that pulsealong the timeline and cluster around the importantmoments or sub-events.
These ?sub-events?
are ofcrucial importance since they represent a surge of in-terest from the Twitter audience and the correspond-1https://developer.apple.com/wwdc/2http://duc.nist.gov/3http://www.nist.gov/tac/1152Figure 1: Example Twitter event stream (upper) and par-ticipant stream (lower).
Event stream contains tweetsrelated to an NBA basketball game (Spurs vs Thunder)scheduled on May 31, 2012; participant stream containstweets corresponding to the player Russell Westbrook inteam Thunder.
X-axis denotes the timeline and y-axisrepresents the number of tweets per 10-second interval.ing key information must be reflected in the eventsummary.
As such, event summarization researchhas been focusing on developing accurate sub-eventdetection systems and generating text descriptionsthat can best summarize the sub-events in a progres-sive manner (Chakrabarti and Punera, 2011; Nicholset al 2012; Zubiaga et al 2012).In Figure 1, we show an example Twitter eventstream and one of its ?participant?
streams.
Theevent stream contains all the tweets related to anNBA basketball game Spurs vs Thunder; whilethe participant stream contains only tweets corre-sponding to the player Russell Westbrook in thisgame.
Previous research on event summarizationfocuses on identifying the important moments fromthe coarse-level event stream.
This may yield sev-eral side effects: first, the spike patterns are notclearly identifiable from the overall event stream,though they are more clearly seen if we ?zoom-in?
tothe participant level; second, it is arguable whetherthe important sub-events can be accurately detectedbased solely on the tweet volume change; third, apopular participant or sub-event can elicit huge vol-ume of tweets which dominant the event discussionand shield less prominent sub-events.
For example,in the NBA games, discussions about the key players(e.g., ?LeBron James?, ?Kobe Bryant?)
can heavilyshadow other important participants or sub-events,resulting in an event summary with repetitive de-scriptions about the dominant players.In this work, we propose a novel participant-based event summarization approach, which dynam-ically identifies the participants from data streams,then ?zooms-in?
the event stream to participantlevel, detects the important sub-events related toeach participant using a novel time-content mixturemodel, and generates the event summary progres-sively by concatenating the descriptions of the im-portant sub-events.
Results show that the mixturemodel-based sub-event detection approach can effi-ciently incorporate the ?burstiness?
and ?cohesive-ness?
of the participant streams, and the participant-based event summarization can effectively capturethe sub-events that have otherwise been shadowedby the long-tail of other dominant sub-events, yield-ing summaries with considerably better coveragethan the state-of-the-art approach.2 Related WorkMining Twitter for event information has receivedincreasing attention in recent years.
Many researchstudies focus on identifying the trending events fromTwitter and providing a concise and dynamic visual-ization of the information.
The identified events areoften represented using a set of keywords.
(Petro-vic et al 2010) proposed an algorithm based onlocality-sensitive hashing for detecting new eventsfrom a stream of Twitter posts.
(O?Connor et al2010; Becker et al 2011b; Becker et al 2011a;Weng et al 2011) proposed demo systems to dis-play the event-related themes and popular tweets,allowing the users to navigate through their topicof interest.
(Zhao et al 2011) described an effortto perform data collection and event recognition de-spite various limits to the free access of Twitter data.
(Diao et al 2012) integrated both temporal infor-mation and users?
personal interests for bursty topicdetection from the microblogs.
(Ritter et al 2012)described an open-domain event-extraction and cat-egorization system, which extracts an open-domaincalendar of significant events from Twitter.With the identified events of interest, there is anever-increasing demand for event summarization,which distills the huge volume of Twitter discus-sions into a concise and representative textual de-scription of the events.
Many studies start withthe text summarization approaches that have beenshown to perform well on the news documents and1153develop adaptations to fit these methods to a col-lection of event tweets.
(Sharifi et al 2010b) pro-posed a graph-based phrase reinforcement algorithmto build a one-sentence summary from a collectionof topic tweets.
(Sharifi et al 2010a; Inouye andKalita, 2011) presented a hybrid TF-IDF approachto extract one- or multiple-sentence summary foreach topic.
(Liu et al 2011a) proposed to usethe concept-based ILP framework for summarizingthe Twitter trending topics, using both tweets andthe webpages linked from the tweets as input textsources.
(Harabagiu and Hickl, 2011) introduced agenerative framework that incorporates event struc-ture and user behavior information in summarizingmultiple microblog posts related to the same topic.Regarding summarizing the data streams, (Mar-cus et al 2011) introduced a ?TwitInfo?
system tovisually summarize and track the events on Twit-ter.
They proposed an automatic peak detection andlabeling algorithm for the social streams.
(Taka-mura et al 2011) proposed a summarization modelbased on the facility location problem, which gener-ates summary for a stream of short documents alongthe timeline.
(Chakrabarti and Punera, 2011) pro-posed an event summarization algorithm based onlearning an underlying hidden state representationof the event via hidden Markov models.
(Louis andNewman, 2012) presented a method for summariz-ing a collection of tweets related to a business.
Theproposed procedure aggregates tweets into subtopicclusters which are then ranked and summarizedby a few representative tweets from each cluster.
(Nichols et al 2012; Zubiaga et al 2012) focusedon real-time event summarization, which detects thesub-events by identifying those moments where thetweet volume has increases sharply, then uses var-ious weighting schemes to perform tweet selectionand finally generates the event summary.Our work is different from the above researchstudies in three folds: first, we propose to ?zoom-in?
the Twitter event streams to the participantlevel, which allows us to clearly identify the im-portant sub-events associated with each participantand generate a balanced event summary with com-prehensive coverage of all the important sub-events;second, we propose a novel time-content mixturemodel approach for sub-event detection, which ef-fectively leverages the ?burstiness?
and ?cohesive-ness?
of the event tweets and accurately detectsthe participant-level sub-events.
Third, we evalu-ate the participant-based event summarization sys-tem on different event types and demonstrate that theproposed approach outperforms the state-of-the-artmethod by a considerable margin.3 Participant-based Event SummarizationWe propose a novel participant-centered event sum-marization approach that consists of three key com-ponents: (1) ?Participant Detection?
dynamicallyidentifies the event participants and divides theentire event stream into a number of participantstreams (Section 3.1); (2) ?Sub-event Detection?
in-troduces a novel time-content mixture model ap-proach to identify the important sub-events associ-ated with each participant; these ?participant-levelsub-events?
are then merged along the timeline toform a set of ?global sub-events?4, which captureall the important moments in the event stream (Sec-tion 3.2); (3) ?Summary Tweet Extraction?
extractsthe representative tweets from the global sub-eventsand forms a comprehensive coverage of the eventprogress (Section 3.3).3.1 Participant DetectionWe define event participants as the entities that playa significant role in shaping the event progress.
?Par-ticipant?
is a general concept to denote the eventparticipating persons, organizations, product lines,etc., each of which can be captured by a set ofcorrelated proper nouns.
For example, the NBAplayer ?LeBron Raymone James?
can be representedby {LeBron James, LeBron, LBJ, King James, L.James}, where each proper noun represents a uniquemention of the participant.
In this work, we automat-ically identify the proper nouns from tweet streams,filter out the infrequent ones using a threshold ?,and cluster them into individual event participants.This process allows us to dynamically identify thekey participating entities and provide a full-coveragefor these participants in the event summary.4We use ?participant sub-events?
and ?global sub-events?respectively to represent the important moments happened onthe participant-level and on the entire event-level.
A ?globalsub-event?
may consist of one or more ?participant sub-events?.For example., the ?steal?
action in the basketball game typicallyinvolves both the defensive and offensive players, and can begenerated by merging the two participant-level sub-events.1154We formulate the participant detection in a hier-archical agglomerative clustering framework.
TheCMU TweetNLP tool (Gimpel et al 2011) was usedfor proper noun tagging.
The proper nouns (a.k.a.,mentions) are grouped into clusters in a bottom-upfashion.
Two mentions are considered similar if theyshare (1) lexical resemblance, and (2) contextualsimilarity.
For example, in the following two tweets?Gotta respect Anthony Davis, still rocking the uni-brow?, ?Anthony gotta do something about that uni-brow?, the two mentions Anthony Davis and An-thony are referring to the same participant and theyshare both character overlap (?anthony?)
and con-text words (?unibrow?, ?gotta?).
We use sim(ci, cj)to represent the similarity between two mentions ciand cj , defined as:sim(ci, cj) = lex sim(ci, cj)?cont sim(ci, cj)where the lexical similarity (lex sim(?))
is definedas a binary function representing whether a mentionci is an abbreviation, acronym, or part of anothermention cj , or if the character edit distance betweenthe two mentions is less than a threshold ?5:lex sim(ci, cj)=??
?1 ci(cj) is part of cj(ci)1 EditDist(ci, cj) < ?0 OtherwiseWe define the context similarity (cont sim(?))
oftwo mentions as the cosine similarity between theircontext vectors ~vi and ~vj .
Note that on the tweetstream, two temporally distant tweets can be verydifferent even though they are lexically similar, e.g.,two slam dunk shots performed by the same playerat different time points are different.
We there-fore restrain the context to a segment of the tweetstream |Sk| and then take the weighted average ofthe segment-based similarity as the final contextsimilarity.
To build the context vector, we use termfrequency (TF) as the term weight and remove all thestopwords.
We use |D| to represent the total tweetsin the event stream.cont sim|Sk|(ci, cj) = cos(~vi, ~vj)cont sim(ci, cj) =?k|Sk||D|?
cont sim|Sk|(ci, cj)5?
was empirically set as 0.2?min{|ci|, |cj |}t wWz?
|D|?
?
?
?
'K BFigure 2: Plate notation of the mixture model.Similarity between two clusters of mentions are de-fined as the maximum possible similarity between apair of mentions, each from one cluster:sim(Ci, Cj) = maxci?Ci,cj?Cjsim(ci, cj)We perform bottom-up agglomerative clustering onthe mentions until a stopping threshold ?
has beenreached for sim(Ci, Cj).
The clustering approachnaturally groups the frequent proper nouns into par-ticipants.
The participant streams are then formedby gathering the tweets that contain one or morementions in the participant cluster.3.2 Mixture Model-based Sub-event DetectionA sub-event corresponds to a topic that emergesfrom the data stream, being intensively discussedduring a short period, and then gradually fades away.The tweets corresponding to a sub-event thus de-mand not only ?temporal burstiness?
but also a cer-tain degree of ?lexical cohesiveness?.
To incorporateboth the time and content aspects of the sub-events,we propose a mixture model approach for sub-eventdetection.
Figure 2 shows the plate notation.In the proposed model, each tweet d in the datastream D is generated from a topic z, weighted bypiz .
Each topic is characterized by both its contentand time aspects.
The content aspect is captured bya multinomial distribution over the words, param-eterized by ?
; while the time aspect is character-ized by a Gaussian distribution, parameterized by ?and ?, with ?
represents the average time point thatthe sub-event emerges and ?
determines the durationof the sub-event.
These distributions bear similari-ties with the previous work (Hofmann, 1999; Allan,2002; Haghighi and Vanderwende, 2009).
In addi-tion, there are often background or ?noise?
topicsthat are being constantly discussed over the entire1155event evolvement process and do not present the de-sired ?burstiness?
property.
We use a uniform dis-tribution U(tb, te) to model the time aspect of these?background?
topics, with tb and te being the eventbeginning and end time points.
The content aspectof a background topic is modeled by similar multi-nomial distribution, parameterized by ??.
We use themaximum likelihood parameter estimation.
The datalikelihood can be represented as:L(D) =?d?D?z{pizpz(td)?w?dpz(w)}where pz(td) models the timestamp of tweet d underthe topic z; pz(w) corresponds to the word distribu-tion in topic z.
They are defined as:pz(td) ={N(td;?z, ?z) if z is a sub-event topicU(tb, te) if z is background topicpz(w) ={p(w; ?z) if z is a sub-event topicp(w; ?
?z) if z is background topicwhere both p(w; ?z) and p(w; ?
?z) are multinomialdistributions over the words.
Initially, we assumethere are K sub-event topics and B background top-ics and use the EM algorithm for model fitting.
TheEM equations are listed below:E-step:p(zd = j) ???
?pijN(d;?j , ?j)?w?dp(w; ?j) if j <= KpijU(tb, te)?w?dp(w; ?
?j) elseM-step:pij ?
?dp(zd = j)p(w; ?j) ?
?dp(zd = j)?
c(w, d)p(w; ?
?j) ?
?dp(zd = j)?
c(w, d)?j =?d p(zd = j)?
td?Kj=1?d p(zd = j)?2j =?d p(zd = j)?
(td ?
?j)2?Kj=1?d p(zd = j)To process the data stream D, we divide the datainto 10-second bins and process each bin at a time.The peak time of a sub-event was determined asthe bin that has the most tweets related to this sub-event.
During EM initialization, the number of sub-event topics K was empirically decided by scanningthrough the data stream and examine tweets in ev-ery 3-minute stream segment.
If there was a spike6,we add a new sub-event to the model and use thetweets in this segment to initialize the value of ?,?, and ?.
Initially, we use a fixed number of back-ground topics with B = 4.
A topic re-adjustmentwas performed after the EM process.
We merge twosub-events in a data stream if they (1) locate closelyin the timeline, with peaks times within a 2-minutewindow; and (2) share similar word distributions:among the top-10 words with highest probability inthe word distributions, there are over 5 words over-lap.
We also convert the sub-event topics to back-ground topics if their ?
values are greater than athreshold ?7.
We then re-run the EM to obtain theupdated parameters.
The topic re-adjustment pro-cess continues until the number of sub-events andbackground topics do not change further.We obtain the ?participant sub-events?
by ap-plying this sub-event detection approach to each ofthe participant streams.
The ?global sub-events?are obtained by merging the participant sub-eventsalong the timeline.
We merge two participant sub-events into a global sub-event if (1) their peaks arewithin a 2-minute window, and (2) the Jaccard simi-larity (Lee, 1999) between their associated tweets isgreater than a threshold (set to 0.1 empirically).
Thetweets associated with each global sub-event are theones with p(z|d) greater than a threshold ?, where zis one of the participant sub-events and ?
was set to0.7 empirically.
After the sub-event detection pro-cess, we obtain a set of global sub-events and theirassociated event tweets.83.3 Summary Tweet ExtractionWe extract a representative tweet from each of theglobal sub-events and concatenate them to form aninformative event summary.
Note that our goal inthis work is to identify all the important moments6We use the algorithm described in (Marcus et al 2011) asa baseline and ad hoc spike detection algorithm.7?
was set to 5 minutes in our experiments.8We empirically set some threshold values in the topic re-adjustment and sub-event merging process.
In future, we wouldlike to explore more principled way of parameter selection.1156Event Date Duration #TweetsLakers vs Okc 05/19/2012 3h10m 218,313N Celtics vs 76ers 05/23/2012 3h30m 245,734B Celtics vs Heat 05/30/2012 3h30m 345,335A Spurs vs Okc 05/31/2012 3h 254,670Heat vs Okc (1) 06/12/2012 3h30m 331,498Heat vs Okc (2) 06/21/2012 3h30m 332,223Apple?s WWDC?12 Conf.
06/11/2012 3h30m 163,775Table 1: Statistics of the data set, including six NBA bas-ketball games and the WWDC 2012 conference event.for event summarization, but not on proposing newmethods for tweet selection.
We thus use the HybridTF-IDF approach (Sharifi et al 2010a; Liu et al2011a) to extract the representative sentences froma collection of tweets.
In this approach, each tweetwas considered as a sentence.
The sentences wereranked according to the average TF-IDF score of theconsisting words; top weighted sentences were it-eratively extracted, while excluding those that havehigh cosine similarity with the existing summarysentences.
(Inouye and Kalita, 2011) showed theHybrid TF-IDF approach performs constantly betterthan the phrase reinforcement algorithm and othertraditional summarization systems.4 Data CorpusWe evaluate the proposed event summarization ap-proach on six NBA basketball games and a repre-sentative conference event, namely the Apple CEO?skeynote speech in the Apple Worldwide Develop-ers Conference (WWDC 2012)9.
We use the het-erogeneous event types to verify that the proposedapproach can robustly and efficiently produce sum-maries on different event streams.
The tweet streamscorresponding to these events are collected usingthe Twitter Streaming API10 with pre-defined key-word set.
For NBA games, we use the team names,first name and last name of the players and headcoaches as keywords for retrieving the event tweets;for the WWDC conference, the keyword set containsabout 20 terms related to the Apple event, such as?wwdc?, ?apple?, ?mac?, etc.
We crawl the tweetsin real-time when these scheduled events are takingplace; nevertheless, certain non-event tweets couldbe mis-included due to the broad coverage of theused keywords.
During preprocessing, we filter out9https://developer.apple.com/wwdc/10https://dev.twitter.com/docs/streaming-apisTime Action (Sub-event) Score9:22 Chris Bosh misses 10-foot two point shot 7-29:22 Serge Ibaka defensive rebound 7-29:11 Kevin Durant makes 15-foot two point shot 9-28:55 Serge Ibaka shooting foul (Shane Battier draws 9-2the foul)8:55 Shane Battier misses free throw 1 of 2 9-28:55 Miami offensive team rebound 9-28:55 Shane Battier makes free throw 2 of 2 9-3Table 2: An example clip of the play-by-play live cov-erage of an NBA game (Heat vs Okc).
?Time?
corre-sponds to the minutes left in the current quarter of thegame; ?Score?
shows the score between the two teams.the tweets containing URLs, non-English tweets,and retweets since they are less likely containingnew information regarding the event progress.
Ta-ble 1 shows statistics of the event tweets after thefiltering process.
In total, there are over 1.8 milliontweets used in the event summarization experiments.We use the play-by-play live coverage collectedfrom the ESPN11 and MacRumors12 websites as ref-erence, which provide detailed descriptions of theNBA and WWDC events as they unfold.
Table 2shows an example clip of the play-by-play descrip-tions of an NBA game.
Ideally, each item in the livecoverage descriptions may correspond to a sub-eventin the tweet streams, but in reality, not all actionswould attract enough attention from the Twitter au-dience.
We use a human annotator to manually filterout the actions that did not lead to any spike in thecorresponding participant stream.
The rest items areprojected to the participant and event streams as thegoldstandard sub-events.
The projection was man-ually performed since the ?game clock?
associatedwith the goldstandard (first column in Table 2) doesnot align well with the ?wall clock?
due to the gamerules such as timeout and halftime rest.
To evalu-ate the participant detection performance, we ask theannotator to manually group the proper noun men-tions into clusters, each cluster corresponds to a par-ticipant.
The mentions that do not correspond to anyparticipant are discarded.
The goldstandard eventsummaries are generated by manually selecting onerepresentative tweet from each of the groundtruthglobal sub-events.
We choose not to use the play-by-play descriptions as reference summaries sincetheir vocabulary is rather limited and do not overlapwith the tweet language.11http://espn.go.com/nba/scoreboard12http://www.macrumorslive.com/archive/wwdc12/1157Example Participants - NBA gamewestbrook, russell westbrookstephen jackson, steven jackson, jacksonjames, james harden, hardenibaka, serge ibakaoklahoma city thunder, oklahomagregg popovich, greg popovich, popovichkevin durant, kd, durantthunder, okc, #okc, okc thunder, #thunderExample Participants - WWDC Conferencemacbooks, mbp, macbook pro, macbook air,...google maps, google, apple mapswwdc, apple wwdc, #wwdcos, mountain, os x mountain, os xiphone 4s, iphone 3gs, iphoneTable 3: Example participants automatically detectedfrom the NBA game Spurs vs Okc (2012-5-31) and theWWDC?12 conference.5 Experimental ResultsWe evaluate the participant-based event summariza-tion in a cascaded fashion and present results foreach of the three components, including the par-ticipant detection (Section 5.1), sub-event detection(Section 5.2), and quantitative and qualitative evalu-ation of example event summaries (Section 5.3).5.1 Participant Detection ResultsIn Table 3, we show example participants that wereautomatically detected by the proposed hierarchicalagglomerative clustering approach.
We note that theclusters include various mentions of the same eventparticipant, e.g., ?gregg popovich?, ?greg popovich?,and ?popovich?
are both referring to the head coachof the team Spurs; ?macbooks?, ?macbook pro?,?mbp?
are referring to a line of products from Apple.Quantitatively, we evaluate the participant detectionresults on both participant- and mention-level.
As-sume the system-detected and the goldstandard par-ticipant clusters are Ts and Tg respectively.
We de-fine a correct participant as a system detected par-ticipant with more than half of its associated men-tions are included in a goldstandard participant (re-ferred to as the hit participant).
As a result, wecan define the participant-level precision and recallas below:participant-prec = #correct-participants/|Ts|participant-recall = #hit-participants/|Tg|Note that a correct participant may include incor-rect mentions, and that more than one correct par-Figure 3: Participant detection performance.
The upperfigures represent the participant-level precision and re-call scores; while the lower figures represent the mention-level precision and recall.
X-axis corresponds to the sixNBA games and the WWDC conference.ticipants may correspond to the same hit participant,both of which are undesired.
In the latter case, weuse representative participant to refer to the cor-rect participant which contains the most mentionsin the hit participant.
In this way, we build a 1-to-1 mapping from the detected participants to thegroundtruth participants.
Next, we define correctmentions as the union of the overlapping mentionsbetween all pairs of representative and hit partici-pants.
Then we calculate the mention-level precisionand recall as the number of correct mentions dividedby the total mentions in the system or goldstandardparticipant clusters.Figure 3 shows the participant- and mention-levelprecision and recall scores.
We experimented withdifferent similarity measures for the agglomerativeclustering approach13.
The ?global context?
meansthat the context vectors are created from the entiredata stream; this may not perform well since dif-ferent participants can share similar global context.E.g., the terms ?shot?, ?dunk?, ?rebound?
can ap-pear in the context of any NBA players and are not13The stopping threshold ?
was set to 0.15, local contextlength is 3 minutes, and frequency threshold ?
was set to 200.1158Participant-level Sub-event Detection Global Sub-event DetectionEvent#P #SSpike MM#SSpike Participant + Spike Participant + MMR P F R P F R P F R P F R P FLakers vs Okc 9 65 0.75 0.31 0.44 0.71 0.39 0.50 48 0.67 0.38 0.48 0.94 0.19 0.32 0.88 0.40 0.55Celtics vs 76ers 10 88 0.52 0.39 0.45 0.53 0.43 0.47 60 0.65 0.51 0.57 0.72 0.18 0.29 0.78 0.39 0.52Celtics vs Heat 14 152 0.53 0.29 0.37 0.50 0.38 0.43 67 0.57 0.41 0.48 0.97 0.21 0.35 0.91 0.28 0.43Spurs vs Okc 12 98 0.78 0.46 0.58 0.84 0.57 0.68 81 0.41 0.42 0.41 0.88 0.35 0.50 0.91 0.54 0.68Heat vs Okc (1) 15 123 0.75 0.27 0.40 0.72 0.35 0.47 85 0.41 0.47 0.44 0.94 0.20 0.33 0.96 0.34 0.50Heat vs okc (2) 13 153 0.74 0.36 0.48 0.76 0.43 0.55 92 0.41 0.33 0.37 0.88 0.21 0.34 0.87 0.38 0.53WWDC?12 10 56 0.64 0.14 0.23 0.59 0.33 0.42 43 0.53 0.26 0.35 0.77 0.14 0.24 0.70 0.31 0.43Average 12 105 0.67 0.32 0.42 0.66 0.41 0.50 68 0.52 0.40 0.44 0.87 0.21 0.34 0.86 0.38 0.52Table 4: Sub-event detection results on both participant and the event streams.
?Spike?
corresponds to the spikedetection algorithm proposed in (Marcus et al 2011); ?MM?
represents our proposed time-content mixture modelapproach.
?#P?
and ?#S?
list the number of participants and sub-events in each event stream.discriminative enough.
We found that adding thelexical similarity measure greatly boosted the clus-tering performance, especially on the mention-level,and that combining the lexical similarity with the lo-cal context is even more helpful for some events.We notice that two events (celtics vs 76ers andceltics vs heat) yield relatively low precision on bothparticipant- and mention-level.
Taking a close lookat the data, we found that these two events acciden-tally co-occurred with other popular events, namelythe TV program ?American Idol?
finale and the NBADraft.
The keyword based data crawler thus includesmany noisy tweets in the event streams, leading tosome false participants being detected.5.2 Sub-event Detection ResultsWe compare our proposed time-content mixturemodel (noted as ?MM?)
against the spike detectionalgorithm proposed in (Marcus et al 2011) (notedas ?Spike?)
.
The spike algorithm is based on thetweet volume change.
It uses 10 seconds as a timeunit, calculates the tweet arrival rate in each unit,and identifies the rates that are significantly higherthan the mean tweet rate.
For these rate spikes, thealgorithm finds the local maximum of tweet rate andidentify a window surrounding the local maximum.We tune the parameter of the ?Spike?
approach (set?
= 4) so that it yields similar recall values as themixture model approach.
We then apply the ?MM?and ?Spike?
approaches to both the participant andevent streams and evaluate the sub-event detectionperformance.
Results are shown in Table 4.
A sys-tem detected sub-event is considered to match thegoldstandard sub-event if its peak time is within a2-minute window of the goldstandard.We first apply the ?Spike?
and ?MM?
approach tothe participant streams.
The participant streams onwhich we cannot detect any meaningful sub-eventshave been excluded, the resulting number of partic-ipants are listed in Table 4 and denoted as ?#P?.In general, we found the ?MM?
approach can per-form better since it inherently incorporates both the?burstiness?
and ?lexical cohesiveness?
of the eventtweets, while the ?Spike?
approach relies solely onthe ?burstiness?
property.
Note that although we di-vide the entire event stream into participant streams,some key participants still own huge amount of dis-cussion and the spike patterns are not always clearlyidentifiable.
The time-content mixture model gainsadvantages in these cases.We apply three settings to detect global sub-events on the data streams.
?Spike?
directly ap-plies the spike algorithm on the entire event stream;the ?Participant + Spike?
and ?Participant + MM?approaches first perform sub-event detection on theparticipant streams and then merge the detected sub-events along the timeline to generate global sub-events.
Note that there are fewer goldstandardsub-events (?#S?)
on the global streams since eachglobal sub-event may correspond to one or multipleparticipant-level sub-events.
Because of the averag-ing effect, spike patterns on the entire event streamis less obvious than those on the participant streams.As a result, few spikes have been detected on theevent stream using the ?Spike?
algorithm, whichleads to low recall as compared to other participant-based approaches.
It also indicates that, by dividingthe entire event stream into participant streams, wehave a better chance of identifying the sub-eventsthat have otherwise been shadowed by the domi-nant sub-events or participants.
The two participant-based methods yield similar recall but ?Participant1159+ Spike?
yields slightly worse precision, since it isvery sensitive to the spikes on the participant-level,leading to the rise of false alarms.
The ?Participant +MM?
approach is much better in precision, which isconsistent to our findings on the participant streams.5.3 Summarization ResultsSummarization evaluation has been a longstandingissue in the literature (Nenkova and Mckeown, 2011;Liu and Liu, 2010).
There are even less studies fo-cusing on evaluating the event summaries generatedfrom data streams.
Since the summary annotationtakes quite some effort, we sample a 10-minute seg-ment from each of the seven event streams and aska human annotator to select representative tweetsfor each segment.
We then compare the systemsummaries against the manual summaries using theROUGE-1 (Lin, 2004) metric.
The quantitative re-sults and qualitative analysis are presented in Table 5and Table 6 respectively.
Note that the ROUGEscores are based solely on the n-gram overlap be-tween the system and reference summaries, whichmay not be the most appropriate measure for eval-uating the Twitter event summaries.
However, wedo notice that the accurate sub-event detection per-formance can successfully translate into a gain ofthe ROUGE scores.
Qualitatively, the participant-based event summarization approach focus more onextracting tweets associated with the targeted partic-ipants, which could lead to better text coherence.6 Conclusion and Future WorkIn this work, we made an initial attempt to gen-erate event summaries using Twitter data streams.We proposed a participant-based event summariza-tion approach which ?zooms-in?
the Twitter eventstreams to the participant level, detects the impor-tant sub-events associated with each participant us-ing a novel mixture model that incorporates both the?burstiness?
and ?cohesiveness?
of tweets, and gen-erates the event summaries progressively.
Resultsshow that the proposed approach can effectively cap-ture the sub-events that have otherwise been shad-owed by the long-tail of other dominant sub-events,yielding summaries with considerably better cover-age.
Without loss of generality, we report resultson the entire event streams, though the proposed ap-proach can well be applied in an online fashion.Event Method R(%) P(%) F(%)NBA AverageSpike 14.73 23.24 16.87Participant + Spike 54.60 14.65 22.40Participant + MM 54.36 23.06 31.53WWDC Conf.Spike 26.58 39.62 31.82Participant + Spike 49.37 25.16 33.33Participant + MM 42.77 31.73 36.07Table 5: ROUGE-1 scores of summarizationMethod SummaryManualGood drive for durantPretty shot by DuncanGood 3 point tony parkerNice move westbrookGood shot WestbrookSpikeGame 3.
Spurs vs. OKCOkc and spurs game.Participant+ SpikeOKLAHOMA CITY THUNDER vs san antoniospurs!!
YAI hope okc win the series.
Ill hate too see the heatplay San Antoniowe aint in San Antonio anymore.NBA: SA 0 OKC 8, 9:11 1st.#TeamOkcSan antonio spurs for 21 consecutive win?
#nbaSomebody Should Stop Tim Duncan.Pass the damn ball WestbrookGood 3 pointer tony parker!Participant+ MMTim Duncan shot is so preciseTim Duncan is gettin startedGood 3 pointer tony parker!Sefalosa guarding tony parker.
Good fucking movecoach brooksWestbrook = 2 Fast 2 FuriousNiggas steady letting Tim Duncan shootWestbrook mid range shot is automaticTable 6: Example summaries for an event segment.
Par-ticipants are marked using italicized text.There are many challenges left in this line of re-search.
Having a standardized evaluation metric forevent summaries is one of them.
In the current work,we employed ROUGE-1 for summary evaluation,since it has been shown to correlate well with the hu-man judgements on noisy text genres (Liu and Liu,2010).
We would like to explore other evaluationmetrics (e.g., ROUGE-2, -SU4, Pyramid (Nenkovaet al 2007)) and the human evaluation in future.We will also explore better ways of integrating thesub-event detection and summarization approaches.AcknowledgmentsPart of this work was done during the first author?sinternship in Bosch Research and Technology Cen-ter.
The work is also partially supported by NSFgrants DMS-0915110 and HRD-0833093.1160ReferencesJames Allan.
2002.
Topic detection and tracking: Event-based information organization.
Kluwer AcademicPublishers Norwell, MA, USA.Hila Becker, Feiyang Chen, Dan Iter, Mor Naaman, andLuis Gravano.
2011a.
Automatic identification andpresentation of twitter content for planned events.
InProceedings of the Fifth International AAAI Confer-ence on Weblogs and Social Media (ICWSM), pages655?656.Hila Becker, Mor Naaman, and Luis Gravano.
2011b.Beyond trending topics: Real-world event identifica-tion on twitter.
In Proceedings of the Fifth Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM), pages 438?441.Deepayan Chakrabarti and Kunal Punera.
2011.
Eventsummarization using tweets.
In Proceedings of theFifth International AAAI Conference on Weblogs andSocial Media (ICWSM), pages 66?73.Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.2012.
Finding bursty topics from microblogs.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 536?544.Jacob Eisenstein.
2013.
What to do about bad languageon the internet.
In Proceedings of the 2013 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (NAACL/HLT).Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef van Genabith.
2011.
#hard-toparse: POS tagging and parsing the twitterverse.
InProceedings of the AAAI Workshop on Analyzing Mi-crotext, pages 20?25.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging for twit-ter: Annotation, features, and experiments.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (ACL-HLT), pages 42?47.Aria Haghighi and Lucy Vanderwende.
2009.
Explor-ing content models for multi-Document summariza-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics (NAACL), pages 362?370.Sanda Harabagiu and Andrew Hickl.
2011.
Relevancemodeling for microblog summarization.
In Proceed-ings of the Fifth International AAAI Conference on We-blogs and Social Media (ICWSM), pages 514?517.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proceedings of Uncertainty in ArtificialIntelligence (UAI).David Inouye and Jugal K. Kalita.
2011.
Compar-ing twitter summarization algorithms for multiple postsummaries.
In Proceedings of 2011 IEEE Third Inter-national Conference on Social Computing, pages 290?306.Lillian Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages25?32.Chin-Yew Lin.
2004.
ROUGE: A package for automaticevaluation of summaries.
In Workshop on Text Sum-marization Branches Out.Feifan Liu and Yang Liu.
2010.
Exploring correlationbetween ROUGE and human evaluation on meetingsummaries.
IEEE Transactions on Audio, Speech, andLanguage Processing, 18(1):187?196.Fei Liu, Yang Liu, and Fuliang Weng.
2011a.
Why is?SXSW?
trending?
Exploring multiple text sourcesfor twitter topic summarization.
In Proceedings of theACL Workshop on Language in Social Media (LSM),pages 66?75.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011b.
Insertion, deletion, or substitution?
Normal-izing text messages without pre-categorization nor su-pervision.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 71?76.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
A broad-coverage normalization system for social media lan-guage.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 1035?1044.Annie Louis and Todd Newman.
2012.
Summarizationof business-related tweets: A concept-based approach.In Proceedings of the 24th International Conferenceon Computational Linguistics (COLING).Adam Marcus, Michael S. Bernstein, Osama Badar,David R. Karger, Samuel Madden, and Robert C.Miller.
2011.
Twitinfo: Aggregating and visualizingmicroblogs for event exploration.
In Proceedings ofthe SIGCHI Conference on Human Factors in Com-puting Systems, pages 227?236.Ani Nenkova and Kathleen Mckeown.
2011.
Automaticsummarization.
Foundations and Trends in Informa-tion Retrieval, 5(2?3):103?233.Ani Nenkova, Rebecca Passonneau, and Kathleen Mcke-own.
2007.
The pyramid method: Incorporating hu-man content selection variation in summarization eval-uation.
ACM Transactions on Speech and LanguageProcessing, 4(2).1161Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.2012.
Summarizing sporting events using twitter.
InProceedings of the 2012 ACM Interntional Conferenceon Intelligent User Interfaces (IUI), pages 189?198.Brendan O?Connor, Michel Krieger, and David Ahn.2010.
TweetMotif: Exploratory search and topic sum-marization for twitter.
In Proceedings of the FourthInternational AAAI Conference on Weblogs and SocialMedia (ICWSM), pages 384?385.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applicationto twitter.
In Proceedings of the 2010 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics (NAACL), pages181?189.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An exper-imental study.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1524?1534.Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.2012.
Open domain event extraction from twitter.
InProceedings of the 18th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 1104?1112.Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.Kalita.
2010a.
Experiments in microblog summariza-tion.
In Proceedings of the 2010 IEEE Second Interna-tional Conference on Social Computing, pages 49?56.Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.Kalita.
2010b.
Summarizing microblogs automati-cally.
In Proceedings of the 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL), pages 685?688.Hiroya Takamura, Hikaru Yokono, and Manabu Oku-mura.
2011.
Summarizing a document stream.
InProceedings of the 33rd European Conference on Ad-vances in Information Retrieval (ECIR), pages 177?188.Jui-Yu Weng, Cheng-Lun Yang, Bo-Nian Chen, Yen-KaiWang, and Shou-De Lin.
2011.
Imass: An intelli-gent microblog analysis and summarization system.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT), pages 133?138.Siqi Zhao, Lin Zhong, Jehan Wickramasuriya, and VenuVasudevan.
2011.
Human as real-time sensors of so-cial and physical events: A case study of twitter andsports games.
Technical Report TR0620-2011, RiceUniversity and Motorola Labs.Arkaitz Zubiaga, Damiano Spina, Enrique Amigo?, andJulio Gonzalo.
2012.
Towards real-time summariza-tion of scheduled events from twitter streams.
In Pro-ceedings of the 23rd ACM Conference on Hypertextand Social Media, pages 319?320.1162
