Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20?27,Dublin, Ireland, August 23-29 2014.Integrating Dictionaries into an Unsupervised Model forMyanmar Word SegmentationYe Kyaw ThuNICTKeihanna Science CityKyoto, Japanyekyawthu@nict.go.jpAndrew FinchNICTKeihanna Science CityKyoto, Japanandrew.finch@nict.go.jpEiichiro SumitaNICTKeihanna Science CityKyoto, Japaneiichiro.sumita@nict.go.jpYoshinori SagisakaGITI/Speech Science Research Lab.Waseda UniverityTokyo, Japanysagisaka@gmail.comAbstractThis paper addresses the problem of word segmentation for low resource languages, with themain focus being on Myanmar language.
In our proposed method, we focus on exploiting lim-ited amounts of dictionary resource, in an attempt to improve the segmentation quality of anunsupervised word segmenter.
Three models are proposed.
In the first, a set of dictionaries(separate dictionaries for different classes of words) are directly introduced into the generativemodel.
In the second, a language model was built from the dictionaries, and the n-gram modelwas inserted into the generative model.
This model was expected to model words that did notoccur in the training data.
The third model was a combination of the previous two models.
Weevaluated our approach on a corpus of manually annotated data.
Our results show that the pro-posed methods are able to improve over a fully unsupervised baseline system.
The best of oursystems improved the F-score from 0.48 to 0.66.
In addition to segmenting the data, one pro-posed method is also able to partially label the segmented corpus with POS tags.
We found thatthese labels were approximately 66% accurate.1 IntroductionIn many natural language processing applications, for example machine translation, parsing and tagging,it is essential to have text that is segmented into sequences of tokens (these tokens usually represent?words?).
In many languages, including the Myanmar language (alternatively called the Burmese lan-guage), Japanese, and Chinese, words are not necessarily delimited by white space in running text.
How-ever, in some low-resource languages (Myanmar being one) broad-coverage word segmentation tools arescarce, and there are two common approaches to dealing with this issue.
The first is to apply unsuper-vised word segmentation tools to a body of monolingual text in order to induce a segmentation.
Thesecond is to use a dictionary of words in the language together with a set of heuristics to identify wordboundaries in text.Myanmar language can be accurately segmented into a sequence of syllables using finite state au-tomata (examples being (Berment, 2004; Thu et al., 2013a)).
However, words composed of single ormultiple syllables are not usually separated by white space.
Although spaces are sometimes used forseparating phrases for easier reading, it is not strictly necessary, and these spaces are rarely used in shortsentences.
There are no clear rules for using spaces in Myanmar language, and thus spaces may (ormay not) be inserted between words, phrases, and even between a root word and its affixes.
Myanmarlanguage is a resource-poor language and large corpora, lexical resources, and grammatical dictionariesare not yet widely available.
For this reason, using corpus-based machine learning techniques to developword segmentation tools is a challenging task.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/202 Related WorkIn this section, we will briefly introduce some proposed word segmentation methods with an emphasison the schemes that have been applied to Myanmar.Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chi-nese and Japanese languages.
These methods can be roughly classified into dictionary-based (Sorn-lertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993;Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991).
Indictionary-based methods, only words that are stored in the dictionary can be identified and the perfor-mance depends to a large degree upon the coverage of the dictionary.
New words appear constantly andthus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem.On the other hand, although statistical approaches can identify unknown words by utilizing probabilisticor cost-based scoring mechanisms, they also suffer from some drawbacks.
The main issues are: theyrequire large amounts of data; the processing time required; and the difficulty in incorporating linguisticknowledge effectively into the segmentation process (Teahan et al., 2000).
For low-resource languagessuch as Myanmar, there is no freely available corpus and dictionary based or rule based methods arebeing used as a temporary solution.If we only focus on Myanmar language word segmentation, as far as the authors are aware there havebeen only two published methodologies, and one study.
Both of the proposed methodologies operateaccording using a process of syllable breaking followed by Maximum Matching; the differences in theapproaches come from the manner in which the segmentation boundary decision is made.
In (Thet et al.,2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008)utilize a word list extracted from a monolingual Myanmar corpus.In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches includingcharacter segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised andsemi-supervised word segmentation, were investigated.
They reported that the highest quality machinetranslation was attained either without word segmentation using simply sequences of syllables, or bya process of Maximum Matching with a monolingual dictionary.
In this study the effectiveness of ap-proaches unsupervised word segmentation using latticelm (with 3-gram to 7-gram language models) andsupervised word segmentation using KyTea was evaluated, however, none of the approaches was able tomatch the performance of the simpler syllable/Maximum Matching techniques.In (Pei et al., 2013) an unsupervised Bayesian word segmentation scheme was augmented by using adictionary of words.
These words were obtained from segmenting the data using another unsupervisedword segmenter.
The probability distribution over these words was calculated from occurrence counts,and this distribution was interpolated into the base measure.3 Methodology3.1 Baseline Non-parametric Bayesian Segmentation ModelThe baseline system, and the model that forms the basis for all of the models is a non-parametric Bayesianunsupervised word segmenter similar to that proposed in (Goldwater et al., 2009).
The major differencesbeing the sampling strategy and the base measure.
The principles behind this segmenter are describedbelow.Intuitively, the model has two basic components: a model for generating an outcome that has alreadybeen generated at least once before, and a second model that assigns a probability to an outcome thathas not yet been produced.
Ideally, to encourage the re-use of model parameters, the probability ofgenerating a novel segment should be considerably lower then the probability of generating a previouslyobserved segment.
This is a characteristic of the Dirichlet process model we use and furthermore, themodel has a preference to generate new segments early on in the process, but is much less likely to do solater on.
In this way, as the cache becomes more and more reliable and complete, so the model prefers touse it rather than generate novel segments.
The probability distribution over these segments (including aninfinite number of unseen segments) can be learned directly from unlabeled data by Bayesian inferenceof the hidden segmentation of the corpus.The underlying stochastic process for the generation of a corpus composed of segments skis usuallywritten in the following from:21G|?,G0?
DP (?,G0)sk|G ?
G (1)G is a discrete probability distribution over the all segments according to a Dirichlet process priorwith base measure G0and concentration parameter ?.
The concentration parameter ?
> 0 controls thevariance of G; intuitively, the larger ?
is, the more similar G0will be to G.3.1.1 The Base MeasureFor the base measure G0that controls the generation of novel sequence-pairs, we use a spelling modelthat assigns probability to new segments according to the following distribution:G0(s) = p(|s|)p(s||s|)=?|s||s|!e?
?|V |?|s|(2)where |s| is the number of tokens in the segment; |V | and is the token set size; and ?
is the expectedlength of the segments.According to this model, the segment length is chosen from a Poisson distribution, and then the el-ements of the segment itself is generated given the length.
Note that this model is able to assign aprobability to arbitrary sequences of tokens drawn from the set of tokens V (in this paper V is the setof all Myanmar syllables).
The motivation for using a base measure of this form, is to overcome issueswith overfitting when training the model; other base measures are possible for example the enhancementproposed in Section 3.4.3.1.2 The Generative ModelThe generative model is given in Equation 3 below.
The equation assignes a probability to the kthsegment skin a derivation of the corpus, given all of the other segments in the history so far s?k.
Here?k is read as: ?up to but not including k?.p(sk|s?k) =N(sk) + ?G0(sk)N + ?
(3)In this equation, N is the total number of segments generated so far, N(sk) is the number of times thesegment skhas occurred in the history.
G0and ?
are the base measure and concentration parameter asbefore.3.1.3 Bayesian InferenceWe used a blocked version of a Gibbs sampler for training.
In (Goldwater et al., 2006) they reportissues with mixing in the sampler that were overcome using annealing.
In (Mochihashi et al., 2009)this issue was overcome by using a blocked sampler together with a dynamic programming approach.Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm(Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009).
We extendtheir approach to handle the joint segmentation and alignment of character sequences.
We refer the readerto (Mochihashi et al., 2009) for a complete description of the FFBS process.
In essence the process usesa forward variable at each node in the segmentation graph to store the probability of reaching the nodefrom the source node of the graph.
These forward variables are calculated efficiently in a single forwardpass through the graph, from source node to sink node (forward filtering).
During backward sampling, asingle path through the segmentation graph is sampled in accordance with its probability.
This samplingprocess uses the forward variables calculated in the forward filtering step.In each iteration of the training process, each entry in the training corpus was sampled without re-placement; its segmentation was removed and the models were updated to reflect this.
Then a newsegmentation for the sequence was chosen using the FFBS process, and the models were updated with22the counts from this new segmentation.
The two hyperparameters, the Dirichlet concentration parameter?, and the Poisson rate parameter ?
were set by slice sampling using vague priors (a Gamma prior in thecase of ?
and the Jeffreys prior was used for ?).
The token set size V used in the base measure was setto the number of types in the training corpus, and V = 3363.3.2 Dictionary Augmented ModelThe dictionary augmented model is in essence the same model as proposed by (Thu et al., 2013b), buta different dictionary was used.
Their method integrates dictionary-based word segmentation (similar tothe maximum matching approaches used successfully in (Thet et al., 2008; Htay and Murthy, 2008; Thuet al., 2013a) ) into a fully unsupervised Bayesian word segmentation scheme.Dictionary-based word segmentation has the advantage of being able to exploit human knowledgeabout the sequences of characters in the language that are used to form words.
This approach is simpleand has proven to be a very effective technique in previous studies.
Problems arise due to the coverageof the dictionary.
The dictionary may not be able to cover the running text well, for example in the caseof low-resource languages the dictionary might be small, or in the case of named entities, even though acomprehensive dictionary of common words may exist, it is likely to fall far short of covering all of thewords that can occur in the language.Unsupervised word segmentation techniques, have high coverage.
They are able to learn how tosegment by discovering patterns in the text that recur.
The weakness of these approaches is that theyhave no explicit knowledge of how words are formed in the language, and the sequences they discoverfrom text may simply be sequences in text that frequently occur and may bear no relationship to actualwords in the language.
As such these units, although they are useful in the context of the generativemodel used to discover them, may not be appropriate for use in an application that might benefit fromthese segments being words in the language.
We believe that machine translation is one such application.This method gives the unsupervised method a means of exploiting a dictionary of words in its trainingprocess, by allowing the integrated method to use the dictionary to segment text when appropriate, andotherwise use its unsupervised models to handle the segmentation.
To do this a separate dictionarygeneration process is integrated into the generative model of the unsupervised segmenter to create asemi-supervised segmenter that segments using a single unified generative model.3.3 Dictionary Set Augmented ModelIn this model, the a set of subsets of the dictionary were extracted based on the part-of-speech labelscontained in the dictionary (Lwin, 1993).
This set of subsets was not a partition of the original dictionarysince some of the types in the dictionary were ambiguous causing some overlap of the subsets.
In theprevious model, during the generative process a decision was made, with a certain probability learnedfrom the data, as to whether the segment would be generated from the unsupervised sub-model or thedictionary sub-model.
In this model, the decision to generate from the dictionary model is refined into anumber of decisions to generate from a number of subsets of the dictionary, each with its own probability.These probabilities were re-estimated from the sampled segmentation of the corpus at the end of eachiteration of the training (in a similar manner to the dictionary augmented model).
A diagram showingthe generative process is shown in Figure 1.3.4 Language Model Augmented ModelIn (Theeramunkong and Usanavasin, 2001) dictionary approaches were deliberately avoided in orderto address issues with unknown words.
Instead a decision tree model for segmentation was proposed.Our approach although different in character (since a generative model is used), shares the insight thatknowledge of how words are constructed is key to segmentation when dictionary information is absent.In this model we used the dictionary resource, but in a more indirect manner.
We use a language modelto capture the notion of exactly what constitutes a segment.
To do this words in the dictionary were firstsegmented into syllables.
Then, a language model was trained on this segmented dictionary.
This modelwill assign high probabilities to the words it has been trained on, and therefore in some sense is ableto capture the spirit of the dictionary-based methods described previously.
However, it will also havelearned something about the way Myanmar words are composed from syllables, and can be expected toassign a higher probability to unknown words that resemble the words it has been trained on, than tosequences of syllables that are not consistent with its training data.23DP ModelP(unsupervised)P(D1)P(D2)P(Dn)D1 ModelD2 ModelDn ModelFigure 1: Generative process with multiple dictionaries competing to generate the data alongside anunsupervised Dirichlet process model.This model can be naturally introduced directly into the Dirichlet process model as a component ofthe base measure.
Equation 2 decomposes into two terms:1.
A Poisson probability mass function:?|s||s|!e??2.
A uniform distribution over the vocabulary:1|V ||s|.The first term above models the choice of length for the segment.
The second term models the choiceof syllables that comprise the segment is in essence a unigram language model that provides little infor-mation about segments are constructed, and serves simply to discourage the formation of long segmentsthat would lead to overfitting.
We directly replace the part of the base measure with our more informativelanguage model built from the dictionary.4 Experiments4.1 OverviewIn the experimental section we aim to analyze two aspects of the performance of the proposed segmen-tation approaches.
Firstly their segmentation quality and secondly for those approaches that are capableof partially labeling the corpus, the accuracy of the labeling.4.2 CorporaFor all our experiments we used a 160K-sentence subset of the Basic Travel Expression (BTEC) corpus(Kikui et al., 2003), for the Myanmar language.
This corpus was segmented using an accurate rule-basedapproach into individual syllables, and this segmentation was used as the base segmentation in all ourexperiments.In addition a test corpus was made by sampling 150-sentences randomly from the corpus.
This corpuswas then segmented by hand and the segments were annotated manually using the set of POS tags thatour system was capable of annotating, together with an ?UNK?
tag to annotate segments that fell out ofthis scope.
The test sentences were included in the data used for the training of the Bayesian models.These sentences were treated in the same manner the rest of the data in that they were initially syllablesegmented.
At the end of the training, the test sentences together with the segmentation assigned by thetraining, were extracted from the corpus, and their segmentation/labeling was evaluated with referenceto the human annotation.4.3 Segmentation PerformanceWe used the Edit Distance of the Word Separator (EDWS) to evaluate the segmentation performance ofthe models.
This technique yields precision, recall and F-score statistics from the edit operations requiredto transform one segmented sequence into another by means of the insertion, deletion and substitution ofsegmentation boundaries.
In this measure the substitution operator corresponds to an identity substitutionand therefore indicates a correct segment boundary.
Insertions correspond to segmentation boundaries in24Method Precision Recall F-score Sub Ins DelUnsupervised 82.27 33.82 0.48 348 681 75Maximum Matching 78.39 99.42 0.88 1023 6 282Dictionary 89.67 57.34 0.70 590 439 68Dictionary Set 89.46 51.99 0.66 535 494 63Language Model (LM) 88.15 31.10 0.46 320 709 43Dictionary Set + LM 91.27 49.76 0.64 512 517 49Table 1: Segmentation Performance.the segmented output that do not correspond to any segmentation boundary in the reference.
Deletionscorrespond to segmentation boundaries in the reference that do not correspond to any segmentationboundary in the segmented output.
Precision recall and F-score are calculated as follows using theChinese Word Segmentation Evaluation Tookit (Joy, 2004):precision =#substitutions#segment boundaries in outputrecall =#substitutions#segment boundaries in referenceF-score =2 ?
precision ?
recallprecision + recallTable 1 shows the segmentation performance all of the systems.
In terms of precision we see an im-provement when the additional models are added to the baseline unsupervised model.
The maximummatching strategy has the lowest precision but the highest recall, this is due to the over-generation ofsegmentation boundaries in regions where no dictionary matches are possible.
In these regions the ref-erence segmentation boundaries are always annotated (since the model defaults to syllable segmentationin these regions), but at the expense of precision.
This is reflected in the relatively low numbers of inser-tions (6), and the relatively high number of deletions (282).
As expected the dictionary set approach gavesimilar performance to the Dictionary approach.
The language model approach produced a respectablelevel of precision, but a low value for recall.
When integrated into the dictionary-based models however,it was able to increase precision.4.4 Labeling AccuracyWe evaluated the accuracy of the labeling on two methods: the first method used only a set of dictio-naries (as described in Section 3.3).
This method was able to label with an accuracy of 64.53%.
Thesecond method consisted of the same technique, but with the addition of the language model trained onthe syllable segmented dictionaries (as described in Section 3.4).
We found that the dictionary-basedlanguage model was able to improved the labeling accuracy 1.2% to 65.73%.5 Examples and AnalysisFigure 2 shows an example an unsupervised segmentation with a typical error.
Frequent words are oftenattached to neighboring words to form erroneous compound segments.
In this example, taken from realoutput of the segmenter, the word ?De?
(this) has been attached to the word ?SarOuk?
(book), and similarthe words in the phrase ?KaBeMharLe?
(where is it) have all been segmented as a single segment (whichoccurs frequently in the corpus).In Figure 3, a typical segmentation from the maximum matcher is shown.
In this example the word?BarThar?
(language) occurs in the dictionary but the word ?JaPan?
(Japan) does not.
The maximummatcher defaults to segmenting the word for Japan into its component syllables, whereas the unsuper-vised segmenter with dictionary has attempted an unsupervised segmentation on this part of the string.The word ?Japan?
occurs sufficiently frequently in the BTEC corpus that the segmenter has been able to25this book ?"?$?u?
( DeSarOuk where is it.
???(?-$?/?
KaBeMharLeFigure 2: An unsupervised segmentation.learn the word during training and has thereby managed to successfully segment the word in the output.The word for language was segmented by means of the embedded dictionary model.Figure 4 shows an example of the partial labeling produced from the model that used a set of dictio-naries in combination with a dictionary-based language model in the base measure.
Due to the smallamount of resources available, substantial parts of the sequence are unable to be labeled (and are anno-tated with the ?U?
tag in the figure, indicating that they were segmented by the unsupervised componentof the model).
The remainder of the words are annotated with POS tags corresponding to the dictionarythey were generated from.Japan ?"?
?% Ja Pan language ?'?'
BarTharN/A ?"
Ja N/A ?
?% Pan language ?'?'
BarTharMaximum MatchingUnsupervised with dictionaryFigure 3: A segmentation from maximum matching.I ?"?$e?
'$/PRO KyunDawmanager ??$e??
*'/NS ManNayJarphonecall ?u?$-?iue?0/U PhoneGoKhawdoing  e??
'?2/U NayDarBar.
?/PFigure 4: A partially labeled segmentation.6 ConclusionIn this paper we have proposed and investigated the effectiveness of several methods intended to exploitlimited quantities of dictionary resources available for low resource languages.
Our results show thatby integrating a dictionary directly into an unsupervised word segmenter we were able to improve bothprecision and recall.
We found that attempting to model word formation using a language model onits own was ineffective compared with the approaches that directly used a dictionary.
However, thislanguage model proved useful when used in conjunction with the direct dictionary-based models, whereit served to assist the modeling of words that were not in the dictionary.
In future work we intend todevelop the dictionary set approach by extending it to introduce basic knowledge of the morphologicalstructure of the language directly into the model.ReferencesVincent Berment.
2004.
Sylla and gmsword: applications to myanmar languages computerization.
In BurmaStudies Conference.26Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson.
2006.
Contextual dependencies in unsupervised wordsegmentation.
In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics andthe 44th annual meeting of the Association for Computational Linguistics, pages 673?680, Morristown, NJ,USA.
Association for Computational Linguistics.Sharon Goldwater, Thomas L Griffiths, and Mark Johnson.
2009.
A bayesian framework for word segmentation:Exploring the effects of context.
Cognition, 112(1):21?54.Hla Hla Htay and Kavi Narayana Murthy.
2008.
Myanmar word segmentation using syllable level longest match-ing.
In IJCNLP, pages 41?48.Joy, 2004.
Chinese Word Segmentation Evaluation Toolkit.Chang Jyun-Shen, Chi-Dah Chen, and Shun-Der Chen.
1991.
Chinese word segmentation through constraintsatisfaction and statistical optimization.
In Proceedings of ROC Computational Linguistics Conference, pages147?165.G.
Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.
2003.
Creating corpora for speech-to-speech translation.
InProceedings of EUROSPEECH-03, pages 381?384.San Lwin.
1993.
Myanmar - English Dictionary.
Department of the Myanmar Language Commission, Ministryof Education, Union of Myanmar.Sun Maosong, Shen Dayang, and Benjamin K. Tsou.
1998.
Chinese word segmentation without using lexicon andhand-crafted training data.
In Proceedings of the 17th international conference on Computational linguistics -Volume 2, COLING ?98, pages 1265?1271, Stroudsburg, PA, USA.
Association for Computational Linguistics.Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009.
Bayesian unsupervised word segmentation withnested pitman-yor language modeling.
In ACL-IJCNLP ?09: Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume 1, pages 100?108, Morristown, NJ, USA.
Association for Computational Linguistics.Papageorgiou and Constantine P. 1994.
Japanese word segmentation by hidden markov model.
In Proceedings ofthe workshop on Human Language Technology, HLT ?94, pages 283?288, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Wenzhe Pei, Dongxu Han, and Baobao Chang.
2013.
A refined hdp-based model for unsupervised chinese wordsegmentation.
In Maosong Sun, Min Zhang, Dekang Lin, and Haifeng Wang, editors, Chinese ComputationalLinguistics and Natural Language Processing Based on Naturally Annotated Big Data, volume 8202 of LectureNotes in Computer Science, pages 44?51.
Springer Berlin Heidelberg.Steven L Scott.
2002.
Bayesian methods for hidden markov models : Recursive computing in the 21st century.Journal of the American Statistical Association, 97(457):337?351.Virach Sornlertlamvanich.
1993.
Word segmentation for thai in machine translation system.
Machine Translation,National Electronics and Computer Technology Center, Bangkok, pages 50?56.A Srithirath and P. Seresangtakul.
2013.
A hybrid approach to lao word segmentation using longest syllable levelmatching with named entities recognition.
In Electrical Engineering/Electronics, Computer, Telecommunica-tions and Information Technology (ECTI-CON), 2013 10th International Conference on, pages 1?5, May.W.
J. Teahan, Rodger McNab, Yingying Wen, and Ian H. Witten.
2000.
A compression-based algorithm forchinese word segmentation.
Comput.
Linguist., 26(3):375?393, September.Thanaruk Theeramunkong and Sasiporn Usanavasin.
2001.
Non-dictionary-based thai word segmentation us-ing decision trees.
In In Proceedings of the First International Conference on Human Language TechnologyResearch.Tun Thura Thet, Jin-Cheon Na, and Wunna Ko Ko.
2008.
Word segmentation for the myanmar language.
J.Information Science, 34(5):688?704.Ye Kyaw Thu, Andrew Finch, Yoshinori Sagisaka, and Eiichiro Sumita.
2013a.
A study of myanmar wordsegmentation schemes for statistical machine translation.
Proceeding of the 11th International Conference onComputer Applications, pages 167?179.Ye Kyaw Thu, Andrew Finch, Eiichiro Sumita, and Yoshinori Sagisaka.
2013b.
Unsupervised and semi-supervisedmyanmar word segmentation approaches for statistical machine translation.Zinmin Wu and Gwyneth Tseng.
1993.
Chinese text segmentation for text retrieval: Achievements and problems.Journal of the American Society for Information Science, 44(5):532?542.27
