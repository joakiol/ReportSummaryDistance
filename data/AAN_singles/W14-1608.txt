Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68?77,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsLooking for Hyponyms in Vector SpaceMarek ReiSwiftKey95 Southwark Bridge RdLondon, UKmarek@swiftkey.netTed BriscoeComputer LaboratoryUniversity of CambridgeCambridge, UKted.briscoe@cl.cam.ac.ukAbstractThe task of detecting and generating hy-ponyms is at the core of semantic under-standing of language, and has numerouspractical applications.
We investigate howneural network embeddings perform onthis task, compared to dependency-basedvector space models, and evaluate a rangeof similarity measures on hyponym gener-ation.
A new asymmetric similarity mea-sure and a combination approach are de-scribed, both of which significantly im-prove precision.
We release three newdatasets of lexical vector representationstrained on the BNC and our evaluationdataset for hyponym generation.1 IntroductionHyponymy is a relation between two word senses,indicating that the meaning of one word is alsocontained in the other.
It can be thought of as atype-of relation; for example car, ship and trainare all hyponyms of vehicle.
We denote a hy-ponymy relation between words a and b as (a?
b),showing that a is a hyponym of b, and b is a hyper-nym of a. Hyponymy relations are closely relatedto the concept of entailment, and this notation isconsistent with indicating the direction of infer-ence ?
if a is true, b must be true as well.Automatic detection and generation of hy-ponyms has many practical applications in nearlyall natural language processing tasks.
Informationretrieval, information extraction and question an-swering can be improved by performing appropri-ate query expansions.
For example, a user search-ing for arthritis treatment is most likely also inter-ested in results containing the hyponyms of treat-ment, such as arthritis therapy, arthritis medica-tion, and arthritis rehabilitation.
Summarisationsystems can increase coherence and reduce repe-tition by correctly handling hyponymous words inthe input text.
Entailment and inference systemscan improve sentence-level entailment resolutionby detecting the presence and direction of word-level hyponymy relations.
Distributionally simi-lar words have been used for smoothing languagemodels and word co-occurrence probabilities (Da-gan et al., 1999; Weeds and Weir, 2005), and hy-ponyms can be more suitable for this application.We distinguish between three different tasksrelated to hyponyms.
Given a directional wordpair, the goal of hyponym detection is to deter-mine whether one word is a hyponym of the other(Zhitomirsky-Geffet and Dagan, 2009; Kotlermanet al., 2010; Baroni and Lenci, 2011).
In con-trast, hyponym acquisition is the task of extract-ing all possible hyponym relations from a giventext (Hearst, 1992; Caraballo, 1999; Pantel andRavichandran, 2004; Snow et al., 2005).
Such sys-tems often make use of heuristic rules and patternsfor extracting relations from surface text, and pop-ulate a database with hyponymous word pairs.
Fi-nally, the task of hyponym generation is to re-turn a list of all possible hyponyms, given onlya single word as input.
This is most relevant topractical applications, as many systems require aset of appropriate substitutes for a specific term.Automated ontology creation (Biemann, 2005) isa related field that also makes use of distributionalsimilarity measures.
However, it is mostly focusedon building prototype-based ontologies throughclustering (Ushioda, 1996; Bisson et al., 2000;Wagner, 2000; Paa?
et al., 2004; Cimiano andStaab, 2005), and is not directly applicable to hy-ponym generation.While most work has been done on hyponymdetection (and the related task of lexical substitu-tion), barely any evaluation has been done for hy-ponym generation.
We have found that systems forhyponym detection often perform poorly on hy-ponym generation, as the latter requires returningresults from a much less restricted candidate set,68and therefore a task-specific evaluation is required.In this paper we focus on hyponym generationand approach it by scoring a very large candidateset of potential hyponyms.
Distributional similar-ity methods are especially interesting for this task,as they can be easily applied to different domains,genres and languages without requiring annotatedtraining data or manual pattern construction.
Weperform a systematic comparison of different vec-tor space models and similarity measures, in orderto better understand the properties of a successfulmethod for hyponym generation.The main contributions of this paper are:1.
Systematic evaluation of different vectorspace models and similarity measures on thetask of hyponym generation.2.
Proposal of new properties for modelling thedirectional hyponymy relation.3.
Release of three lexical vector datasets,trained using neural network, window-based,and dependency-based features.2 Vector space modelsIn order to use similarity measures for hyponymdetection, every word needs to be mapped to apoint in vector space.
The method of choosingappropriate features for these vectors is crucial toachieving the optimal performance.
We comparefive different approaches:Window: As a simple baseline, we created vec-tors by counting word co-occurrences in a fixedcontext window.
Every word that occurs within awindow of three words before or after is countedas a feature for the target word.
Pointwise mutualinformation is then used for weighting.CW: Collobert and Weston (2008) constructeda neural network language model that is trained topredict the next word in the sequence, and simul-taneously learns vector representations for eachword.
The vectors for context words are concate-nated and used as input for the neural network,which uses a sample of possible outputs for gra-dient calculation to speed up the training process.Turian et al.
(2010) recreated their experimentsand made the vectors available online.1HLBL: Mnih and Hinton (2007) created wordrepresentations using the hierarchical log-bilinear1http://metaoptimize.com/projects/wordreprs/model ?
a neural network that takes the concate-nated vectors of context words as input, and istrained to predict the vector representation of thenext word, which is then transformed into a prob-ability distribution over possible words.
To speedup training and testing, they use a hierarchical datastructure for filtering down the list of candidates.Both CW and HLBL vectors were trained using37M words from RCV1.Word2vec: We created word representationsusing the word2vec2toolkit.
The tool is basedon a feedforward neural network language model,with modifications to make representation learn-ing more efficient (Mikolov et al., 2013a).
Wemake use of the skip-gram model, which takeseach word in a sequence as an input to a log-linearclassifier with a continuous projection layer, andpredicts words within a certain range before andafter the input word.
The window size was set to5 and vectors were trained with both 100 and 500dimensions.Dependencies: Finally, we created vector rep-resentations for words by using dependency rela-tions from a parser as features.
Every incomingand outgoing dependency relation is counted as afeature, together with the connected term.
For ex-ample, given the dependency relation (play, dobj,guitar), the tuple (>dobj, guitar) is extracted as afeature for play, and (<dobj, play) as a feature forguitar.
We use only features that occur more thanonce in the dataset, and weight them using point-wise mutual information to construct feature vec-tors for every term.
Features with negative weightswere retained, as they proved to be beneficial forsome similarity measures.The window-based, dependency-based andword2vec vector sets were all trained on 112Mwords from the British National Corpus, with pre-processing steps for lowercasing and lemmatis-ing.
Any numbers were grouped and substitutedby more generic tokens.
For constructing thedependency-based vector representations, we usedthe parsed version of the BNC created by Ander-sen et al.
(2008) with the RASP toolkit (Briscoeet al., 2006).
When saved as plain text, the 500-dimensional word2vec vectors and dependency-based vectors are comparable in size (602MB and549MB), whereas the window-based vectors aretwice as large (1,004MB).
We make these vector2https://code.google.com/p/word2vec/69sets publically available for download.3Recently, Mikolov et al.
(2013b) published in-teresting results about linguistic regularities invector space models.
They proposed that the rela-tionship between two words can be characterisedby their vector offset, for example, we could findthe vector for word ?queen?
by performing the op-eration ?king - man + woman?
on correspondingvectors.
They also applied this approach to hy-ponym relations such as (shirt ?
clothing) and(bowl ?
dish).
We evaluate how well this methodapplies to hyponym generation with each of thevector space models mentioned above.
Using thetraining data, we learn a vector for the hyponymyrelation by averaging over all the offset vectorsfor hyponym-hypernym pairs.
This vector is thenadded to the hypernym during query time, andthe result is compared to hyponym candidates us-ing cosine similarity.
For sparse high-dimensionalvector space models it was not feasible to use thefull offset vector during experiments, therefore weretain only the top 1,000 highest-weighted fea-tures.3 Similarity measuresWe compare the performance of a range of simi-larity measures, both directional and symmetrical,on the task of hyponym generation.Cosine similarity is defined as the angle be-tween two feature vectors and has become a stan-dard measure of similarity between weighted vec-tors in information retrieval (IR).Lin similarity, created by Lin (1998), uses theratio of shared feature weights compared to all fea-ture weights.
It measures the weighted proportionof features that are shared by both words.DiceGen2 is one possible method for generalis-ing the Dice measure to real-valued weights (Cur-ran, 2003; Grefenstette, 1994).
The dot product ofthe weight vectors is normalised by the total sumof all weights.
The same formula can also be con-sidered as a possible generalisation for the Jaccardmeasure.WeedsPrec and WeedsRec were proposed byWeeds et al.
(2004) who suggested using precisionand recall as directional measures of word simi-larity.
In this framework, the features are treatedsimilarly to retrieved documents in information re-trieval ?
the vector of the broader term b is used asthe gold standard, and the vector of the narrower3http://www.marekrei.com/projects/vectorsets/term a is in the role of retrieval results.
Precisionis then calculated by comparing the intersection(items correctly returned) to the values of the nar-rower term only (all items returned).
In contrast,WeedsRec quantifies how well the features of thebreader term are covered by the narrower term.Balprec is a measure created by Szpektor andDagan (2008).
They proposed combining Weed-sPrec together with the Lin measure by takingtheir geometric average.
This aims to balance theWeedsPrec score, as the Lin measure will penalisecases where one vector contains very few features.ClarkeDE, proposed by Clarke (2009), is anasymmetric degree of entailment measure, basedon the concept of distributional generality (Weedset al., 2004).
It quantifies the weighted coverage ofthe features of the narrower term a by the featuresof the broader term b.BalAPInc, a measure described by Kotlermanet al.
(2010), combines the APInc score with Linsimilarity by taking their geometric average.
TheAPInc measure finds the proportion of shared fea-tures relative to the features for the narrower term,but this can lead to unreliable results when thenumber of features is very small.
The motivationbehind combining these measures is that the sym-metric Lin measure will decrease the final scorefor such word pairs, thereby balancing the results.4 Properties of a directional measureFinding similar words in a vector space, givena symmetric similarity measure, is a relativelystraightforward task.
However finding hyponymsis arguably more difficult, as the relation is asym-metric, and looking at the distance or angle be-tween the two words may not be enough.Kotlerman et al.
(2010) investigate the relatedproblem of detecting directional lexical entail-ment, and they propose three desirable propertiesthat a directional distributional similarity measureshould capture:1.
The relevance of the shared features to thenarrower term.2.
The relevance of the shared features to thebroader term.3.
That relevance is less reliable if the num-ber of features of either the narrower or thebroader term is small.70Given a term pair (a ?
b) we refer to a as thenarrower term and b as the broader term.
The fea-tures of a that are also found in b (have non-zeroweights for both a and b) are referred to as sharedfeatures.They show that existing measures which cor-respond to these criteria perform better and con-struct the BalAPInc measure based on the princi-ples.
However, it is interesting to note that theseproperties do not explicitly specify any directionalaspects of the measure, and symmetric similarityscores can also fulfil the requirements.Based on investigating hyponym distributionsin our training data, we suggest two additions tothis list of desired properties, one of which specif-ically targets the asymmetric properties of the de-sired similarity measures:4.
The shared features are more important tothe directional score calculation, compared tonon-shared features.5.
Highly weighted features of the broader termare more important to the score calculation,compared to features of the narrower term.Most existing directional similarity scores mea-sure how many features of the narrower term arepresent for the broader term.
If a entails b, thenit is assumed that the possible contexts of a are asubset of contexts for b, but b occurs in a widerrange of contexts compared to a.
This intuition isused by directional measures such as ClarkeDE,WeedsPrec and BalAPInc.
In contrast, we foundthat many features of the narrower term are oftenhighly specific to that term and do not generaliseeven to hypernyms.
Since these features have avery high weight for the narrower term, their ab-sence with the broader term will have a big nega-tive impact on the similarity score.We hypothesise that many terms have certainindividual features that are common to them butnot to other related words.
Since most weightingschemes reward high relative co-occurrence, thesefeatures are also likely to receive high weights.Therefore, we suggest that features which are notfound for both terms should have a decreased im-pact on the score calculation, as many of them arenot expected to be shared between hyponyms andhypernyms.
However, removing them completelyis also not advisable, as they allow the measureto estimate the overall relative importance of theshared features to the specific term.We also propose that among the shared features,those ranked higher for the broader term are moreimportant to the directional measure.
In the hy-ponymy relation (a ?
b), the term b is more gen-eral and covers a wider range of semantic con-cepts.
This also means it is more likely to beused in contexts that apply to different hyponymsof b.
For example, some of the high-ranking fea-tures for food are blandly-flavoured, high-calorieand uneaten.
These are properties that co-occuroften with the term food, but can also be appliedto most hyponyms of food.
Therefore, we hypoth-esise that the presence of these features for the nar-rower term is a good indication of a hyponymy re-lation.
This is somewhat in contrast to most previ-ous work, where the weights of the narrower termhave been used as the main guideline for similaritycalculation.5 Weighted cosineWe now aim to construct a similarity measure thatfollows all five of the properties mentioned above.Cosine similarity is one of the symmetric similar-ity measures which corresponds to the first threedesired properties, and our experiments showedthat it performs remarkably well at the task of hy-ponym generation.
Therefore, we decided to mod-ify cosine similarity to also reflect the final twoproperties and produce a more appropriate asym-metric score.The standard feature vectors for each word con-tain weights indicating how important this featureis to the word.
We specify additional weights thatmeasure how important the feature is to that spe-cific directional relation between the two terms.Weighted cosine similarity, shown in Table 1, canthen be used to calculate a modified similarityscore.
Fadenotes the set of weighted features forword a, wa(f) is the weight of feature f for worda, and z(f) is the additional weight for feature f ,given the directional word pair (a, b).Based on the new desired properties we wantto downweight the importance of features that arenot present for both terms.
For this, we choosethe simple solution of scaling them with a smallconstant C ?
[0, 1].
Next, we also want to assignhigher z(f) values to the shared features that havehigh weights for the broader term b.
We use therelative rank of feature f in Fb, rb(f), as the indi-cator of its importance and scale this value to therange from C to 1.
This results in the importance71WeightedCosine(Fa, Fb) =?f?Fa?Fb(z(f)?wa(f))?(z(f)?wb(f))??f?Fa(z(f)?wa(f))2??
?f?Fb(z(f)?wb(f))2z(f) ={(1?rb(f)|Fb|+1)?
(1?
C) + C if f ?
Fa?
FbC otherwiseTable 1: Weighted cosine similarity measurefunction decreasing linearly as the rank numberincreases, but the weights for the shared featuresalways remain higher compared to the non-sharedfeatures.
Tied feature values are handled by as-signing them the average rank value.
Adding 1to the denominator of the relative rank calculationavoids exceptions with empty vectors, and also en-sures that the value will always be strictly greaterthan C. While the basic function is still the sym-metric cosine, the z(f) values will be different de-pending on the order of the arguments.The parameter C controls the relative impor-tance of the ?unimportant?
features to the direc-tional relation.
Setting it to 0 will ignore thesefeatures completely, while setting it to 1 will resultin the traditional cosine measure.
Experiments onthe development data showed that the exact valueof this parameter is not very important, as long asit is not too close to the extreme values of 0 or 1.We use the value C = 0.5 for reporting our results,meaning that the non-shared features are half asimportant, compared to the shared features.6 DatasetAs WordNet (Miller, 1995) contains numerousmanually annotated hyponymy relations, we canuse it to construct suitable datasets for evaluat-ing hyponym generation.
While WordNet termsare annotated with only the closest hyponyms, weare considering all indirect/inherited hyponymsto be relevant ?
for example, given relations(genomics ?
genetics) and (genetics ?
biology),then genomics is also regarded as a hyponym ofbiology.
WordNet relations are defined betweensynsets, but we refrain from the task of word sensedisambiguation and count word a as a valid hy-ponym for word b if it is valid for any sense of b.Synonymy can be thought of as a symmetric is-a relation, and most real-world applications wouldrequire synonyms to also be returned, togetherwith hyponyms.
Therefore, in our dataset we con-sider synonyms as hyponyms in both directions.We also performed experiments without synonymsand found that this had limited effect on the re-sults ?
while the accuracy of all similarity mea-sures slightly decreased (due to fewer numbers ofcorrect answers), the relative ranking remained thesame.
As shown in the next section, the number ofsynonyms is typically small compared to the num-ber of all inherited hyponyms.To construct the dataset, we first found allsingle-word nouns in WordNet that are containedat least 10 times in the British National Corpus(BNC).
Next, we retained only words that haveat least 10 hyponyms, such that they occur 10 ormore times in the BNC.
This selection processaims to discard WordNet hypernyms that are veryrare in practical use, and would not have enoughexamples for learning informative vector represen-tations.
The final dataset contains the remainingterms, together with all of their hyponyms, includ-ing the rare/unseen hyponyms.
As expected, somegeneral terms, such as group or location, have alarge number of inherited hyponyms.
On average,each hypernym in the dataset has 233 hyponyms,but the distribution is roughly exponential, and themedian is only 36.In order to better facilitate future experimentswith supervised methods, such as described by Ba-roni et al.
(2012), we randomly separated the datainto training (1230 hypernyms), validation (922),and test (922) sets, and we make these datasetspublically available online.47 ExperimentsWe evaluate how well different vector space mod-els and similarity measures perform on the task ofhyponym generation.
Given a single word as in-put, the system needs to return a ranked list ofwords with correct hyponyms at the top.
As thelist of candidates for scoring we use all words inthe BNC that occur at least 10 times (a total of86,496 words).
All the experiments are performedusing tokenised and lemmatised words.As the main evaluation measure, we report4http://www.marekrei.com/projects/hypgen/72Cosine Cosine+offsetMAP P@1 P@5 MAP P@1 P@5Window 2.18 19.76 12.20 2.19 19.76 12.25CW-100 0.66 3.80 3.21 0.59 3.91 2.89HLBL-100 1.01 10.31 6.04 1.01 10.31 6.06Word2vec-100 1.78 15.96 10.12 1.50 12.38 8.71Word2vec-500 2.06 19.76 11.92 1.77 17.05 10.71Dependencies 2.73 25.41 14.90 2.73 25.52 14.92Table 2: Experiments using different vector space models for hyponym generation on the test set.
Wereport results using regular cosine similarity and the vector offset method described in Section 2.Mean Average Precision (MAP), which averagesprecision values at various recall points in the re-turned list.
It combines both precision and recall,as well as the quality of the ranking, into a sin-gle measure, and is therefore well-suited for com-paring different methods.
The reported MAP val-ues are very low ?
this is due to many rare Word-Net hyponyms not occurring in the candidate set,for which all systems are automatically penalised.However, this allows us to evaluate recall, makingthe results comparable between different systemsand background datasets.
We also report precisionat top-1 and top-5 returned hyponyms.As a baseline we report the results of a tra-ditional hyponym acquisition system.
For this,we implemented the pattern-based matching pro-cess described by Hearst (1992), and also used bySnow et al.
(2005).
These patterns look for ex-plicit examples of hyponym relations mentionedin the text, for example:X such as {Y1, Y2, ... , (and|or)} Ynwhere X will be extracted as the hypernym, and Y1to Ynas hyponyms.
We ran the patterns over theBNC and extracted 21,704 hyponym pairs, whichwere then ranked according to the number of timesthey were found.7.1 Evaluation of vector spacesTable 2 contains experiments with different vectorspace models.
We report here results using cosine,as it is an established measure and a competitivebaseline.
For our task, the HLBL vectors performbetter than CW vectors, even though they weretrained on the same data.
Both of them are out-performed by word2vec-100 vectors, which havethe same dimensionality but are trained on muchmore text.
Increasing the dimensionality withword2vec-500 gives a further improvement.
In-terestingly, the simple window-based vectors per-form just as well as the ones trained with neuralnetworks.
However, the advantage of word2vec-500 is that the representations are more compactand require only about half the space.
Finally,the dependency-based vectors outperform all othervector types, giving 2.73% MAP and 25.41% pre-cision at the top-ranked result.
While the othermodels are built by using neighbouring words ascontext, this model looks at dependency relations,thereby taking both semantic and syntactic rolesinto account.
The results indicate that word2vecand window-based models are more suitable whenthe general topic of words needs to be captured,whereas dependency-based vectors are preferredwhen the task requires both topical and functionalsimilarity between words.
Our experiments alsoincluded the evaluation of other similarity mea-sures on different vector space models, and we wefound these results to be representative.Contrary to previous work, the vector offsetmethod, described in Section 2, did not pro-vide substantial improvements on the hyponymgeneration task.
For the neural network-basedvectors this approach generally decreased perfor-mance, compared to using direct cosine similar-ity.
There are some marginal improvements forwindow and dependency-based models.
Unfortu-nately, the original work did not include baselineperformance using cosine similarity, without ap-plying vector modifications.
It is possible that thismethod does not generalise to all word relationsequally well.
As part of future work, it is worthexploring if a hypernym-specific strategy of se-lecting training examples could improve the per-formance.73Validation TestMAP P@1 P@5 MAP P@1 P@5Pattern-based 0.53 7.06 4.58 0.51 8.14 4.45Cosine 2.48 21.06 12.96 2.73 25.41 14.90Lin 1.87 16.50 10.75 2.01 21.17 12.23DiceGen2 2.27 18.57 12.62 2.44 21.82 14.55WeedsPrec 0.13 0.00 0.09 0.12 0.11 0.04WeedsRec 0.72 0.33 2.45 0.69 0.54 2.41BalPrec 1.78 15.31 10.55 1.88 17.48 11.34ClarkeDE 0.23 0.00 0.02 0.24 0.00 0.09BalAPInc 1.64 14.22 9.12 1.68 15.85 9.66WeightedCosine 2.59 21.39 13.59 2.85 25.84 15.46Combined 3.27 23.02 16.09 3.51 27.69 18.02Table 3: Evaluation of different vector similarity measures on the validation and test set of hyponymgeneration.
We report Mean Average Precision (MAP), precision at rank 1 (P@1), and precision at rank5 (P@5).7.2 Evaluation of similarity measuresTable 3 contains experiments with different sim-ilarity measures, using the dependency-basedmodel, and Table 4 contains sample output fromthe best system.
The results show that the pattern-based baseline does rather poorly on this task.MAP is low due to the system having very lim-ited recall, but higher precision at top ranks wouldhave been expected.
Analysis showed that thissystem was unable to find any hyponyms for morethan half (513/922) of the hypernyms in the vali-dation set, leading to such poor recall that it alsoaffects Precision@1.
While the pattern-based sys-tem did extract a relatively large number of hy-ponyms from the corpus (21,704 pairs), these arelargely concentrated on a small number of hyper-nyms (e.g., area, company, material, country) thatare more likely to be mentioned in matching con-texts.Cosine, DiceGen2 and Lin ?
all symmetricsimilarity measures ?
perform relatively well onthis task, whereas established directional measuresperform unexpectedly poorly.
This can perhaps beexplained by considering the distribution of hy-ponyms.
Given a word, the most likely candi-dates for a high cosine similarity are synonyms,antonyms, hypernyms and hyponyms of that word?
these are words that are likely to be used in simi-lar topics, contexts, and syntactic roles.
By def-inition, there are an equal number of hyponymand hypernym relations in WordNet, but this ra-tio changes rapidly as we remove lower-frequencywords.
Figure 1 shows the number of relations ex-tracted from WordNet, as we restrict the minimumfrequency of the main word.
It can be seen that thenumber of hyponyms increases much faster com-pared to the other three relations.
This also appliesto real-world data ?
when averaging over word in-stances found in the BNC, hyponyms cover 85% ofthese relations.
Therefore, the high performanceof cosine can be explained by distributionally sim-ilar words having a relatively high likelihood ofbeing hyponyms.0 10 20 30 40 50 60 70 80 90 100020406080100hyponyms hypernymssynonyms antonymsmin freqavgrelated wordsFigure 1: Average number of different relationsper word in WordNet, as we restrict the minimumword frequency.One possible reason for the poor performanceof directional measures is that most of them quan-tify how well the features of the narrower term areincluded in the broader term.
In contrast, we foundthat for hyponym generation it is more importantto measure how well the features of the broaderterm are included in the narrower term.
This74scientist researcher, biologist, psychologist, economist, observer, physicist, sociologistsport football, golf, club, tennis, athletics, rugby, cricket, game, recreation, entertainmenttreatment therapy, medication, patient, procedure, surgery, remedy, regimen, medicineTable 4: Examples of top results using the combined system.
WordNet hyponyms are marked in bold.is supported by WeedsRec outperforming Weed-sPrec, although the opposite was intended by theirdesign.Another explanation for the low performanceis that these directional measures are often devel-oped in an artificial context.
For example, Kotler-man et al.
(2010) evaluated lexical entailment de-tection on a dataset where the symmetric Lin sim-ilarity measure was used to select word pairs formanual annotation.
This creates a different task,as correct terms that do not have a high symmetricsimilarity will be excluded from evaluation.
TheBalAPInc measure performed best in that setting,but does not do as well for hyponym generation,where candidates are filtered only based on mini-mum frequency.The weighted cosine measure, proposed in Sec-tion 5, outperformed all other similarity measureson both hyponym generation datasets.
The im-provement over cosine is relatively small; how-ever, it is consistent and the improvement in MAPis statistically significant on both datasets (p <0.05), using the Approximate Randomisation Test(Noreen, 1989; Cohen, 1995) with 106iterations.This further supports the properties of a directionalsimilarity measure described in Section 4.Finally, we created a new system by combiningtogether two separate approaches: the weightedcosine measure using the dependency-based vec-tor space, and the normal cosine similarity usingword2vec-500 vectors.
We found that the formeris good at modelling the grammatical roles and di-rectional containment, whereas the latter can pro-vide useful information about the topic and seman-tics of the word.
Turney (2012) also demonstratedthe importance of both topical (domain) and func-tional vector space models when working with se-mantic relations.
We combined these approachesby calculating both scores for each word pair andtaking their geometric average, or 0 if it could notbe calculated.
This final system gives considerableimprovements across all evaluation metrics, and issignificantly (p < 0.05) better compared to cosineor weighted cosine methods individually.
Table 4contains some example output from this system.8 ConclusionHyponym generation has a wide range of pos-sible applications in NLP, such as query expan-sion, entailment detection, and language modelsmoothing.
Pattern-based hyponym acquisitioncan be used to find relevant hyponyms, but theseapproaches rely on both words being mentionedtogether in a specific context, leading to very lowrecall.
Vector similarity methods are interestingfor this task, as they can be easily applied to differ-ent domains and languages without any supervisedlearning or manual pattern construction.
We cre-ated a dataset for evaluating hyponym generationsystems and experimented with a range of vectorspace models and similarity measures.Our results show that choosing an appropriatevector space model is equally important to using asuitable similarity measure.
We achieved the high-est performance using dependency-based vectorrepresentations, which outperformed neural net-work and window-based models.
Symmetric sim-ilarity measures, especially cosine similarity, per-formed surprisingly well on this task.
This canbe attributed to an unbalanced distribution of hy-ponyms, compared to other high-similarity words.The choice of vector space can be highly depen-dent on the specific task, and we have made avail-able our vector datasets created from the samesource using three different methods.We proposed two new properties for detectinghyponyms, and used them to construct a new di-rectional similarity measure.
This weighted co-sine measure significantly outperformed all others,showing that a theoretically-motivated directionalmeasure is still the most accurate method for mod-elling hyponymy relations.
Finally, we combinedtogether two different methods, achieving furthersubstantial improvements on all evaluation met-rics.References?istein E. Andersen, Julien Nioche, Edward J. Briscoe,and John Carroll.
2008.
The BNC parsed withRASP4UIMA.
In Proceedings of the Sixth Interna-75tional Language Resources and Evaluation Confer-ence (LREC08), Marrakech, Morocco.Marco Baroni and Alessandro Lenci.
2011.
Howwe BLESSed distributional semantic evaluation.
InProceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,Edinburgh.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,and Chung-chieh Shan.
2012.
Entailment above theword level in distributional semantics.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 23?32.Chris Biemann.
2005.
Ontology learning from text: Asurvey of methods.
LDV Forum, 20(2002):75?93.Gilles Bisson, Claire N?edellec, and Dolores Ca?namero.2000.
Designing clustering methods for ontologybuilding-The Mo?K workbench.
In ECAI OntologyLearning Workshop.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the COLING/ACL 2006 Interactive Pre-sentation Sessions, number July, pages 77?80, Syd-ney, Australia.
Association for Computational Lin-guistics.Sharon A. Caraballo.
1999.
Automatic construction ofa hypernym-labeled noun hierarchy from text.
Pro-ceedings of the 37th annual meeting of the Asso-ciation for Computational Linguistics on Computa-tional Linguistics, pages 120?126.Philipp Cimiano and Steffen Staab.
2005.
Learningconcept hierarchies from text with a guided hierar-chical clustering algorithm.
In ICML-Workshop onLearning and Extending Lexical Ontologies by usingMachine Learning Methods.Daoud Clarke.
2009.
Context-theoretic semantics fornatural language: an overview.
In Proceedings ofthe Workshop on Geometrical Models of NaturalLanguage Semantics, number March, pages 112?119.
Association for Computational Linguistics.Paul R Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
The MIT Press, Cambridge, MA.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
Proceed-ings of the 25th international conference on Ma-chine learning.James R. Curran.
2003.
From distributional to seman-tic similarity.
Ph.D. thesis, University of Edinburgh.Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.1999.
Similarity-based models of word cooccur-rence probabilities.
Machine Learning, 31:1?31.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer Academic Pub-lishers, Norwell, MA, USA.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedingsof the 14th conference on Computational linguistics(COLING ?92), number July, page 539, Morristown,NJ, USA.
Association for Computational Linguis-tics.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(04):359?389.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th inter-national conference on Computational linguistics-Volume 2, pages 768?774.
Association for Compu-tational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient Estimation of Word Repre-sentations in Vector Space.
ICLR Workshop, pages1?12.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic Regularities in Continuous SpaceWord Representations.
(June):746?751.George A. Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.Proceedings of the 24th international conference onMachine learning - ICML ?07, pages 641?648.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley,New York.Gerhard Paa?, J?org Kindermann, and Edda Leopold.2004.
Learning prototype ontologies by hierachicallatent semantic analysis.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In Proceedingsof HLT/NAACL.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Advances in Neural Information Pro-cessing Systems.Idan Szpektor and Ido Dagan.
2008.
Learning en-tailment rules for unary templates.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics (COLING ?08), pages 849?856,Morristown, NJ, USA.
Association for Computa-tional Linguistics.76Joseph Turian, Lev Ratinov, and Y Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics.Peter D. Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.Akira Ushioda.
1996.
Hierarchical clustering of wordsand application to NLP tasks.
In Fourth Workshopon Very Large Corpora, pages 28?41.Andreas Wagner.
2000.
Enriching a lexical semanticnet with selectional preferences by means of statisti-cal corpus analysis.
In ECAI Workshop on OntologyLearning.Julie Weeds and David Weir.
2005.
Co-occurrenceretrieval: A flexible framework for lexical distribu-tional similarity.
Computational Linguistics.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
Proceedings of the 20th internationalconference on Computational Linguistics - COLING?04.Maayan Zhitomirsky-Geffet and Ido Dagan.
2009.Bootstrapping Distributional Feature Vector Quality.Computational Linguistics, 35(3):435?461, Septem-ber.77
