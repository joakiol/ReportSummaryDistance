Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697?704Manchester, August 2008Exploiting Constituent Dependencies for Tree Kernel-based SemanticRelation ExtractionLonghua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide QianJiangsu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and Technology, Soochow University1 Shizi Street, Suzhou, China 215006{qianlonghua,gdzhou,kongfang,qmzhu,pdqian}@suda.edu.cnAbstractThis paper proposes a new approach todynamically determine the tree span fortree kernel-based semantic relation ex-traction.
It exploits constituent dependen-cies to keep the nodes and their headchildren along the path connecting thetwo entities, while removing the noisy in-formation from the syntactic parse tree,eventually leading to a dynamic syntacticparse tree.
This paper also explores entityfeatures and their combined features in aunified parse and semantic tree, which in-tegrates both structured syntactic parseinformation and entity-related semanticinformation.
Evaluation on the ACERDC 2004 corpus shows that our dy-namic syntactic parse tree outperforms allprevious tree spans, and the compositekernel combining this tree kernel with alinear state-of-the-art feature-based ker-nel, achieves the so far best performance.1 IntroductionInformation extraction is one of the key tasks innatural language processing.
It attempts to iden-tify relevant information from a large amount ofnatural language text documents.
Of three sub-tasks defined by the ACE program1, this paperfocuses exclusively on Relation Detection andCharacterization (RDC) task, which detects andclassifies semantic relationships between prede-fined types of entities in the ACE corpus.
For?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.1 http://www.ldc.upenn.edu/Projects/ACE/example, the sentence ?Microsoft Corp. is basedin Redmond, WA?
conveys the relation ?GPE-AFF.Based?
between ?Microsoft Corp.?
[ORG]and ?Redmond?
[GPE].
Due to limited accuracyin state-of-the-art syntactic and semantic parsing,reliably extracting semantic relationships be-tween named entities in natural language docu-ments is still a difficult, unresolved problem.In the literature, feature-based methods havedominated the research in semantic relation ex-traction.
Featured-based methods achieve prom-ising performance and competitive efficiency bytransforming a relation example into a set of syn-tactic and semantic features, such as lexicalknowledge, entity-related information, syntacticparse trees and deep semantic information.
How-ever, detailed research (Zhou et al, 2005) showsthat it?s difficult to extract new effective featuresto further improve the extraction accuracy.Therefore, researchers turn to kernel-basedmethods, which avoids the burden of feature en-gineering through computing the similarity oftwo discrete objects (e.g.
parse trees) directly.From prior work (Zelenko et al, 2003; Culottaand Sorensen, 2004; Bunescu and Mooney, 2005)to current research (Zhang et al, 2006; Zhou etal., 2007), kernel methods have been showingmore and more potential in relation extraction.The key problem for kernel methods on rela-tion extraction is how to represent and capturethe structured syntactic information inherent inrelation instances.
While kernel methods usingthe dependency tree (Culotta and Sorensen, 2004)and the shortest dependency path (Bunescu andMooney, 2005) suffer from low recall perform-ance, convolution tree kernels (Zhang et al, 2006;Zhou et al, 2007) over syntactic parse treesachieve comparable or even better performancethan feature-based methods.However, there still exist two problems re-garding currently widely used tree spans.
Zhanget al (2006) discover that the Shortest Path-697enclosed Tree (SPT) achieves the best perform-ance.
Zhou et al (2007) further extend it to Con-text-Sensitive Shortest Path-enclosed Tree (CS-SPT), which dynamically includes necessarypredicate-linked path information.
One problemwith both SPT and CS-SPT is that they may stillcontain unnecessary information.
The other prob-lem is that a considerable number of useful con-text-sensitive information is also missing fromSPT/CS-SPT, although CS-SPT includes somecontextual information relating to predicate-linked path.This paper proposes a new approach to dy-namically determine the tree span for relationextraction by exploiting constituent dependenciesto remove the noisy information, as well as keepthe necessary information in the parse tree.
Ourmotivation is to integrate dependency informa-tion, which has been proven very useful to rela-tion extraction, with the structured syntactic in-formation to construct a concise and effectivetree span specifically targeted for relation extrac-tion.
Moreover, we also explore interesting com-bined entity features for relation extraction via aunified parse and semantic tree.The other sections in this paper are organizedas follows.
Previous work is first reviewed inSection 2.
Then, Section 3 proposes a dynamicsyntactic parse tree while the entity-related se-mantic tree is described in Section 4.
Evaluationon the ACE RDC corpus is given in Section 5.Finally, we conclude our work in Section 6.2 Related WorkDue to space limitation, here we only reviewkernel-based methods used in relation extraction.For those interested in feature-based methods,please refer to Zhou et al (2005) for more details.Zelenko et al (2003) described a kernel be-tween shallow parse trees to extract semanticrelations, where a relation instance is trans-formed into the least common sub-tree connect-ing the two entity nodes.
The kernel matches thenodes of two corresponding sub-trees from rootsto leaf nodes recursively layer by layer in a top-down manner.
Their method shows successfulresults on two simple extraction tasks.
Culottaand Sorensen (2004) proposed a slightly general-ized version of this kernel between dependencytrees, in which a successful match of two relationinstances requires the nodes to be at the samelayer and in the identical path starting from theroots to the current nodes.
These strong con-straints make their kernel yield high precision butvery low recall on the ACE RDC 2003 corpus.Bunescu and Mooney (2005) develop a shortestpath dependency tree kernel, which simplycounts the number of common word classes ateach node in the shortest paths between two enti-ties in dependency trees.
Similar to Culotta andSorensen (2004), this method also suffers fromhigh precision but low recall.Zhang et al (2006) describe a convolution treekernel (CTK, Collins and Duffy, 2001) to inves-tigate various structured information for relationextraction and find that the Shortest Path-enclosed Tree (SPT) achieves the F-measure of67.7 on the 7 relation types of the ACE RDC2004 corpus.
One problem with SPT is that itloses the contextual information outside SPT,which is usually critical for relation extraction.Zhou et al (2007) point out that both SPT andthe convolution tree kernel are context-free.
Theyexpand SPT to CS-SPT by dynamically includ-ing necessary predicate-linked path informationand extending the standard CTK to context-sensitive CTK, obtaining the F-measure of 73.2on the 7 relation types of the ACE RDC 2004corpus.
However, the CS-SPT only recovers partof contextual information and may contain noisyinformation as much as SPT.In order to fully utilize the advantages of fea-ture-based methods and kernel-based methods,researchers turn to composite kernel methods.Zhao and Grishman (2005) define several fea-ture-based composite kernels to capture diverselinguistic knowledge and achieve the F-measureof 70.4 on the 7 relation types in the ACE RDC2004 corpus.
Zhang et al (2006) design a com-posite kernel consisting of an entity linear kerneland a standard CTK, obtaining the F-measure of72.1 on the 7 relation types in the ACE RDC2004 corpus.
Zhou et al (2007) describe a com-posite kernel to integrate a context-sensitiveCTK and a state-of-the-art linear kernel.
Itachieves the so far best F-measure of 75.8 on the7 relation types in the ACE RDC 2004 corpus.In this paper, we will further study how to dy-namically determine a concise and effective treespan for a relation instance by exploiting con-stituent dependencies inherent in the parse treederivation.
We also attempt to fully capture boththe structured syntactic parse information andentity-related semantic information, especiallycombined entity features, via a unified parse andsemantic tree.
Finally, we validate the effective-ness of a composite kernel for relation extraction,which combines a tree kernel and a linear kernel.6983 Dynamic Syntactic Parse TreeThis section discusses how to generate dynamicsyntactic parse tree by employing constituentdependencies to overcome the problems existingin currently used tree spans.3.1 Constituent Dependencies in Parse TreeZhang et al (2006) explore five kinds of treespans and find that the Shortest Path-enclosedTree (SPT) achieves the best performance.
Zhouet al (2007) further propose Context-SensitiveSPT (CS-SPT), which can dynamically deter-mine the tree span by extending the necessarypredicate-linked path information outside SPT.However, the key problem of how to representthe structured syntactic parse tree is still partiallyresolved.
As we indicate as follows, current treespans suffer from two problems:(1) Both SPT and CS-SPT still contain unnec-essary information.
For example, in the sentence?
?bought one of town?s two meat-packingplants?, the condensed information ?one ofplants?
is sufficient to determine ?DISC?
rela-tionship between the entities ?one?
[FAC] and?plants?
[FAC], while SPT/CS-SPT include theredundant underlined part.
Therefore more un-necessary information can be safely removedfrom SPT/CS-SPT.
(2) CS-SPT only captures part of context-sensitive information relating to predicate-linkedstructure (Zhou et al, 2007) and still loses muchcontext-sensitive information.
Let?s take thesame example sentence ?
?bought one of town?stwo meat-packing plants?, where indeed there isno relationship between the entities ?one?
[FAC]and ?town?
[GPE].
Nevertheless, the informationcontained in SPT/CS-SPT (?one of town?)
mayeasily lead to their relationship being misclassi-fied as ?DISC?, which is beyond our expectation.Therefore the underlined part outside SPT/CS-SPT should be recovered so as to differentiate itfrom positive instances.Since dependency plays a key role in manyNLP problems such as syntactic parsing, seman-tic role labeling as well as semantic relation ex-traction, our motivation is to exploit dependencyknowledge to distinguish the necessary evidencefrom the unnecessary information in the struc-tured syntactic parse tree.On one hand, lexical or word-word depend-ency indicates the relationship among wordsoccurring in the same sentence, e.g.
predicate-argument dependency means that arguments aredependent on their target predicates, modifier-head dependency means that modifiers are de-pendent on their head words.
This dependencyrelationship offers a very condensed representa-tion of the information needed to assess the rela-tionship in the forms of the dependency tree (Cu-lotta and Sorensen, 2004) or the shortest depend-ency path (Bunescu and Mooney, 2005) that in-cludes both entities.On the other hand, when the parse tree corre-sponding to the sentence is derived using deriva-tion rules from the bottom to the top, the word-word dependencies extend upward, making aunique head child containing the head word forevery non-terminal constituent.
As indicated asfollows, each CFG rule has the form:P?
Ln?L1H R1?RmHere, P is the parent node, H is the head child ofthe rule, Ln?L1 and R1?Rm are left and rightmodifiers of H respectively, and both n and mmay be zero.
In other words, the parent node Pdepends on the head child H, this is what we callconstituent dependency.
Vice versa, we can alsodetermine the head child of a constituent in termsof constituent dependency.
Our hypothesis stipu-lates that the contribution of the parse tree to es-tablishing a relationship is almost exclusivelyconcentrated in the path connecting the two enti-ties, as well as the head children of constituentnodes along this path.3.2 Generation of Dynamic Syntactic ParseTreeStarting from the Minimum Complete Tree(MCT, the complete sub-tree rooted by the near-est common ancestor of the two entities underconsideration) as the representation of each rela-tion instance, along the path connecting two enti-ties, the head child of every node is found ac-cording to various constituent dependencies.Then the path nodes and their head children arekept while any other nodes are removed from thetree.
Eventually we arrive at a tree called Dy-namic Syntactic Parse Tree (DSPT), which isdynamically determined by constituent depend-encies and only contains necessary informationas expected.There exist a considerable number of constitu-ent dependencies in CFG as described by Collins(2003).
However, since our task is to extract therelationship between two named entities, our fo-cus is on how to condense Noun-Phrases (NPs)and other useful constituents for relation extrac-tion.
Therefore constituent dependencies can beclassified according to constituent types of theCFG rules:699(1) Modification within base-NPs: base-NPsmean that they do not directly dominate an NPthemselves, unless the dominated NP is a posses-sive NP.
The noun phrase right above the entityheadword, whose mention type is nominal orname, can be categorized into this type.
In thiscase, the entity headword is also the headword ofthe noun phrase, thus all the constituents beforethe headword are dependent on the headword,and may be removed from the parse tree, whilethe headword and the constituents right after theheadword remain unchanged.
For example, in thesentence ?
?bought one of town?s two meat-packing plants?
as illustrated in Figure 1(a), theconstituents before the headword  ?plants?
canbe removed from the parse tree.
In this way theparse tree ?one of plants?
could capture the?DISC?
relationship more concisely and pre-cisely.
Another interesting example is shown inFigure 1(b), where the base-NP of the secondentity ?town?
is a possessive NP and there is norelationship between the entities ?one?
and?town?
defined in the ACE corpus.
For both SPTand CS-SPT, this example would be condensedto ?one of town?
and therefore easily misclassi-fied as the ?DISC?
relationship between the twoentities.
In the contrast, our DSPT can avoid thisproblem by keeping the constituent ??s?
and theheadword ?plants?.
(2) Modification to NPs: except base-NPs,other modification to NPs can be classified intothis type.
Usually these NPs are recursive, mean-ing that they contain another NP as their child.The CFG rules corresponding to these modifica-tions may have the following forms:NP?
NP SBAR [relative clause]NP?
NP VP [reduced relative]NP?
NP PP [PP attachment]Here, the NPs in bold mean that the path con-necting the two entities passes through them.
Forevery right hand side, the NP in bold is modifiedby the constituent following them.
That is, thelatter is dependent on the former, and may bereduced to a single NP.
In Figure 1(c) we show asentence ?one of about 500 people nominatedfor ?
?, where there exists a ?DISC?
relationshipbetween the entities ?one?
and ?people?.
Sincethe reduced relative ?nominated for ??
modifiesand is therefore dependent on the ?people?, theycan be removed from the parse tree, that is, theright side (?NP VP?)
can be reduced to the lefthand side, which is exactly a single NP.
(a) Removal of constituents before the headword in base-NP(b) Keeping of constituents after the headword in base-NPNNoneINofDTtheNNtownPOS'sE-FACNNplantstwoCD NNoneINofNNtownPOS'sE-FACNNplantsmeat-packingJJNNonePPINofNPDTtheNNtownPOS'sNNplantstwoCD NNoneINofNNplantsmeat-packingJJ NNoneINofRBaboutQPCD500NNSpeople...nominatedVBNforINVPPP...E2-PERNNoneINofNNSpeopleNNpropertyPRPheVPVBZ INinNPPPstateNNStheNPJJrentalSownsDT NNpropertyPRPheVPVBZownsgovernors from connecticutNNS INNPE-GPENNP,,southNPE-GPENNPdakotaNNP,,andCCmontanaNNPgovernors fromNNS INmontanaNNP(c) Reduction of modification to NP(d) Removal of arguments to verb(e) Reduction of conjuncts for NP coordinationE-GPENPPPE1-FACNPE2-FACNPE1-FACNPNPNPNPE2-FAC E1-PERNPNPPPNPNPNPE1-PERPPNPE2-PERNPSBARE2-PERSNPNPE1-FACPPNPNPE1-PERNPE2-GPENPE1-PERPPNPNPNPE2-GPENPE1-FAC E2-PERNPNPSBARNPNPE1-FACPPNPNPE2-GPENPNPE1-PERPPNPNPE2-GPEFigure 1.
Removal and reduction of constituents using dependencies700(3) Arguments/adjuncts to verbs: this typeincludes the CFG rules in which the left side in-cludes S, SBAR or VP.
An argument representsthe subject or object of a verb, while an adjunctindicates the location, date/time or way of theaction corresponding to the verb.
They dependon the verb and can be removed if they are notincluded in the path connecting the two entities.However, when the parent tag is S or SBAR, andits child VP is not included in the path, this VPshould be recovered to indicate the predicateverb.
Figure 1(d) shows a sentence ??
maintainrental property he owns in the state?, where the?ART.User-or-Owner?
relation holds betweenthe entities ?property?
and ?he?.
While PP can beremoved from the rule  (?VP?
VBZ PP?
), theVP should be kept in the rule (?S?
NP VP?
).Consequently, the tree span looks more conciseand precise for relation extraction.
(4) Coordination conjunctions: In coordina-tion constructions, several peer conjuncts may bereduced into a single constituent.
Although thefirst conjunct is always considered as the head-word (Collins, 2003), actually all the conjunctsplay an equal role in relation extraction.
As illus-trated in Figure 1(e), the NP coordination in thesentence (?governors from connecticut, southdakota, and montana?)
can be reduced to a singleNP (?governors from montana?)
by keeping theconjunct in the path while removing the otherconjuncts.
(5) Modification to other constituents: ex-cept for the above four types, other CFG rulesfall into this type, such as modification to PP,ADVP and PRN etc.
These cases are similar toarguments/adjuncts to verbs, but less frequentthan them, so we will not detail this scenario.In fact, SPT (Zhang et al, 2006) can be ar-rived at by carrying out part of the above re-moval operations using a single rule (i.e.
all theconstituents outside the linking path should beremoved) and CS-CSPT (Zhou et al, 2007) fur-ther recovers part of necessary context-sensitiveinformation outside SPT, this justifies that SPTperforms well, while CS-SPT outperforms SPT.4 Entity-related Semantic TreeEntity semantic features, such as entity headword,entity type and subtype etc., impose a strongconstraint on relation types in terms of relationdefinition by the ACE RDC task.
Experiments byZhang et al (2006) show that linear kernel usingonly entity features contributes much when com-bined with the convolution parse tree kernel.Qian et al (2007) further indicates that amongthese entity features, entity type, subtype, andmention type, as well as the base form of predi-cate verb, contribute most while the contributionof other features, such as entity class, headwordand GPE role, can be ignored.In order to effectively capture entity-relatedsemantic features, and their combined features aswell, especially bi-gram or tri-gram features, webuild an Entity-related Semantic Tree (EST) inthree ways as illustrated in Figure 2.
In the ex-ample sentence ?they ?re here?, which is ex-cerpted from the ACE RDC 2004 corpus, thereexists a relationship ?Physical.Located?
betweenthe entities ?they?
[PER] and ?here?[GPE.Population-Center].
The features are en-coded as ?TP?, ?ST?, ?MT?
and ?PVB?, whichdenote type, subtype, mention-type of the twoentities, and the base form of predicate verb ifexisting (nearest to the 2nd entity along the pathconnecting the two entities) respectively.
Forexample, the tag ?TP1?
represents the type of the1st entity, and the tag ?ST2?
represents the sub-type of the 2nd entity.
The three entity-relatedsemantic tree setups are depicted as follows:TP2TP1(a) Bag Of Features(BOF)ENTST2ST1 MT2MT1 PVB(c) Entity-Paired Tree(EPT)ENTE1 E2(b) Feature Paired Tree(FPT)ENTTP ST MTST1TP1 MT1 TP2 ST2 MT2PVBTP1 TP2 ST1 ST2 MT1 MT2PVBPER null PRO GPE Pop.
PRO bePER null PRO GPE Pop.
PRObePER GPE null Pop.
PRO PRObeFigure 2.
Different setups for entity-related se-mantic tree (EST)(a) Bag of Features (BOF, e.g.
Fig.
2(a)): allfeature nodes uniformly hang under the root node,so the tree kernel simply counts the number ofcommon features between two relation instances.This tree setup is similar to linear entity kernelexplored by Zhang et al (2006).
(b) Feature-Paired Tree (FPT, e.g.
Fig.
2(b)):the features of two entities are grouped into dif-ferent types according to their feature names, e.g.?TP1?
and ?TP2?
are grouped to ?TP?.
This treesetup is aimed to capture the additional similarity701of the single feature combined from differententities, i.e., the first and the second entities.
(c) Entity-Paired Tree (EPT, e.g.
Fig.
2(c)): allthe features relating to an entity are grouped tonodes ?E1?
or ?E2?, thus this tree kernel can fur-ther explore the equivalence of combined entityfeatures only relating to one of the entities be-tween two relation instances.In fact, the BOF only captures the individualentity features, while the FPT/EPT can addition-ally capture the bi-gram/tri-gram features respec-tively.Rather than constructing a composite kernel,we incorporate the EST into the DSPT to pro-duce a Unified Parse and Semantic Tree (UPST)to investigate the contribution of the EST to rela-tion extraction.
The entity features can be at-tached under the top node, the entity nodes, ordirectly combined with the entity nodes as inFigure 1.
However, detailed evaluation (Qian etal., 2007) indicates that the UPST achieves thebest performance when the feature nodes are at-tached under the top node.
Hence, we also attachthree kinds of entity-related semantic trees (i.e.BOF, FPT and EPT) under the top node of theDSPT right after its original children.
Thereafter,we employ the standard CTK (Collins and Duffy,2001) to compute the similarity between twoUPSTs, since this CTK and its variations aresuccessfully applied in syntactic parsing, seman-tic role labeling (Moschitti, 2004) and relationextraction (Zhang et al, 2006; Zhou et al, 2007)as well.5 ExperimentationThis section will evaluate the effectiveness of theDSPT and the contribution of entity-related se-mantic information through experiments.5.1 Experimental SettingFor evaluation, we use the ACE RDC 2004 cor-pus as the benchmark data.
This data set contains451 documents and 5702 relation instances.
Itdefines 7 entity types, 7 major relation types and23 subtypes.
For comparison with previous work,evaluation is done on 347 (nwire/bnews) docu-ments and 4307 relation instances using 5-foldcross-validation.
Here, the corpus is parsed usingCharniak?s parser (Charniak, 2001) and relationinstances are generated by iterating over all pairsof entity mentions occurring in the same sentencewith given ?true?
mentions and coreferential in-formation.
In our experimentations, SVMlight(Joachims, 1998) with the tree kernel function(Moschitti, 2004) 2  is selected as our classifier.For efficiency, we apply the one vs. othersstrategy, which builds K classifiers so as toseparate one class from all others.
Forcomparison purposes, the training parameters C(SVM) and ?
(tree kernel) are also set to 2.4 and0.4 respectively.5.2 Experimental ResultsTable 1 evaluates the contributions of differentkinds of constituent dependencies to extractionperformance on the 7 relation types of the ACERDC 2004 corpus using the convolution parsetree kernel as depicted in Figure 1.
The MCTwith only entity-type information is first used asthe baseline, and various constituent dependen-cies are then applied sequentially to dynamicallyreshaping the tree in two different modes:--[M1] Respective:  every constituent depend-ency is individually applied on MCT.--[M2] Accumulative: every constituent de-pendency is incrementally applied on the previ-ously derived tree span, which begins with theMCT and eventually gives rise to a DynamicSyntactic Parse Tree (DSPT).Dependency types P(%) R(%) FMCT (baseline) 75.1 53.8 62.7Modification withinbase-NPs76.5(59.8)59.8(59.8)67.1(67.1)Modification to NPs77.0(76.2)63.2(56.9)69.4(65.1)Arguments/adjuncts to verb77.1(76.1)63.9(57.5)69.9(65.5)Coordination conjunctions77.3(77.3)65.2(55.1)70.8(63.8)Other modifications77.4(75.0)65.4(53.7)70.9(62.6)Table 1.
Contribution of constituent dependen-cies in respective mode (inside parentheses) andaccumulative mode (outside parentheses)The table shows that the final DSPT achievesthe best performance of 77.4%/65.4%/70.9 inprecision/recall/F-measure respectively after ap-plying all the dependencies, with the increase ofF-measure by 8.2 units compared to the baselineMCT.
This indicates that reshaping the tree byexploiting constituent dependencies may signifi-cantly improve extraction accuracy largely due tothe increase in recall.
It further suggests that con-stituent dependencies knowledge is very effec-2 http://ai-nlp.info.uniroma2.it/moschitti/702tive and can be fully utilized in tree kernel-basedrelation extraction.
This table also shows that:(1) Both modification within base-NPs andmodification to NPs contribute much to perform-ance improvement, acquiring the increase of F-measure by 4.4/2.4 units in mode M1 and 4.4/2.3units in mode M2 respectively.
This indicates thelocal characteristic of semantic relations, whichcan be effectively captured by NPs near the twoinvolved entities in the DSPT.
(2) All the other three dependencies show mi-nor contribution to performance enhancement,they improve the F-measure only by 2.8/0.9/-0.1units in mode M1 and 0.5/0.9/0.1 units in modeM2.
This may be due to the reason that these de-pendencies only remove the nodes far from thetwo entities.We compare in Table 2 the performance ofUnified Parse and Semantic Trees with differentkinds of Entity Semantic Tree setups using stan-dard convolution tree kernel, while the SPT andDSPT with only entity-type information arelisted for reference.
It shows that:(1) All the three unified parse and semantictree kernels significantly outperform the DSPTkernel, obtaining an average increase of ~4 unitsin F-measure.
This means that they can effec-tively capture both the structured syntactic in-formation and the entity-related semantic fea-tures.
(2) The Unified Parse and Semantic Tree withFeature-Paired Tree achieves the best perform-ance of 80.1/70.7/75.1 in P/R/F respectively,with an increase of F-measure by 0.4/0.3 unitsover BOF and EPT respectively.
This suggeststhat additional bi-gram entity features capturedby FPT are more useful than tri-gram entity fea-tures captured by EPT.Tree setups P(%) R(%) FSPT 76.3 59.8 67.1DSPT 77.4 65.4 70.9UPST (BOF) 80.4 69.7 74.7UPST (FPT) 80.1 70.7 75.1UPST (EPT) 79.9 70.2 74.8Table 2.
Performance of Unified Parse andSemantic Trees (UPSTs) on the 7 relation typesof the ACE RDC 2004 corpusIn Table 3 we summarize the improvements ofdifferent tree setups over SPT.
It shows that in asimilar setting, our DSPT outperforms SPT by3.8 units in F-measure, while CS-SPT outper-forms SPT by 1.3 units in F-measure.
This sug-gests that the DSPT performs best among thesetree spans.
It also shows that the Unified Parseand Semantic Tree with Feature-Paired Tree per-form significantly better than the other two treesetups (i.e., CS-SPT and DSPT) by 6.7/4.2 unitsin F-measure respectively.
This implies that theentity-related semantic information is very usefuland contributes much when they are incorporatedinto the parse tree for relation extraction.Tree setups P(%) R(%) FCS-SPT over SPT3 1.5   1.1 1.3DSPT over SPT 1.1   5.6 3.8UPST (FPT) over SPT 3.8 10.9 8.0Table 3.
Improvements of different tree setupsover SPT on the ACE RDC 2004 corpusFinally, Table 4 compares our system withother state-of-the-art kernel-based systems on the7 relation types of the ACE RDC 2004 corpus.
Itshows that our UPST outperforms all previoustree setups using one single kernel, and even bet-ter than two previous composite kernels (Zhanget al, 2006; Zhao and Grishman, 2005).
Fur-thermore, when the UPST (FPT) kernel is com-bined with a linear state-of-the-state feature-based kernel (Zhou et al, 2005) into a compositeone via polynomial interpolation in a settingsimilar to Zhou et al (2007) (i.e.
polynomial de-gree d=2 and coefficient ?=0.3), we get the so farbest performance of 77.1 in F-measure for 7 rela-tion types on the ACE RDC 2004 data set.Systems P(%) R(%) FOurs:composite kernel83.0 72.0 77.1Zhou et al, (2007):composite kernel82.2 70.2 75.8Zhang et al, (2006):composite kernel76.1 68.4 72.1Zhao and Grishman, (2005):4composite kernel69.2 70.5 70.4Ours:CTK with UPST80.1 70.7 75.1Zhou et al, (2007): context-sensitive CTK with CS-SPT81.1 66.7 73.2Zhang et al, (2006):CTK with SPT74.1 62.4 67.7Table 4.
Comparison of different systems onthe ACE RDC 2004 corpus3  We arrive at these values by subtracting P/R/F(79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F(81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest-enclosed Path Tree according to Table 2 (Zhou et al, 2007)4 There might be some typing errors for the performancereported in Zhao and Grishman (2005) since P, R and F donot match.7036 ConclusionThis paper further explores the potential of struc-tured syntactic information for tree kernel-basedrelation extraction, and proposes a new approachto dynamically determine the tree span (DSPT)for relation instances by exploiting constituentdependencies.
We also investigate different waysof how entity-related semantic features and theircombined features can be effectively captured ina Unified Parse and Semantic Tree (UPST).Evaluation on the ACE RDC 2004 corpus showsthat our DSPT is appropriate for structured repre-sentation of relation instances.
We also find that,in addition to individual entity features, com-bined entity features (especially bi-gram) con-tribute much when they are combined with aDPST into a UPST.
And the composite kernel,combining the UPST kernel and a linear state-of-the-art kernel, yields the so far best performance.For the future work, we will focus on improv-ing performance of complex structured parsetrees, where the path connecting the two entitiesinvolved in a relationship is too long for currentkernel methods to take effect.
Our preliminaryexperiment of applying certain discourse theoryexhibits certain positive results.AcknowledgementsThis research is supported by Project 60673041under the National Natural Science Foundationof China, Project 2006AA01Z147 under the?863?
National High-Tech Research and Devel-opment of China, and the National ResearchFoundation for the Doctoral Program of HigherEducation of China under Grant No.20060285008.
We would also like to thank theexcellent and insightful comments from the threeanonymous reviewers.ReferencesBunescu, Razvan C. and Raymond J. Mooney.
2005.A Shortest Path Dependency Kernel for RelationExtraction.
In Proceedings of the Human LanguageTechnology Conference and Conference on Em-pirical Methods in Natural Language Processing(EMNLP-2005), pages 724-731.
Vancover, B.C.Charniak, Eugene.
2001.
Intermediate-head Parsingfor Language Models.
In Proceedings of the 39thAnnual Meeting of the Association of Computa-tional Linguistics (ACL-2001), pages 116-123.Collins, Michael.
2003.
Head-Driven Statistics Mod-els for Natural Language Parsing.
Computationallinguistics, 29(4): 589-617.Collins, Michael and Nigel Duffy.
2001.
ConvolutionKernels for Natural Language.
In Proceedings ofNeural Information Processing Systems (NIPS-2001), pages 625-632.
Cambridge, MA.Culotta, Aron and Jeffrey Sorensen.
2004.
Depend-ency tree kernels for relation extraction.
In Pro-ceedings of the 42nd Annual Meeting of the Asso-ciation of Computational Linguistics (ACL-2004),pages 423-439.
Barcelona, Spain.Joachims, Thorsten.
1998.
Text Categorization withSupport Vector Machine: learning with many rele-vant features.
In Proceedings of the 10th EuropeanConference on Machine Learning (ECML-1998),pages 137-142.
Chemnitz, Germany.Moschitti, Alessandro.
2004.
A Study on ConvolutionKernels for Shallow Semantic Parsing.
In Proceed-ings of the 42nd Annual Meeting of the Associationof Computational Linguistics (ACL-2004).
Barce-lona, Spain.Qian, Longhua, Guodong Zhou, Qiaoming Zhu andPeide Qian.
2007.
Relation Extraction using Con-volution Tree Kernel Expanded with Entity Fea-tures.
In Proceedings of the 21st Pacific AsianConference on Language, Information and Compu-tation (PACLIC-21), pages 415-421.
Seoul, Korea.Zelenko, Dmitry, Chinatsu Aone and Anthony Rich-ardella.
2003.
Kernel Methods for Relation Extrac-tion.
Journal of Machine Learning Research,3(2003): 1083-1106.Zhang, Min, Jie Zhang, Jian Su and Guodong Zhou.2006.
A Composite Kernel to Extract Relations be-tween Entities with both Flat and Structured Fea-tures.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th Annual Meeting of the Association of Compu-tational Linguistics (COLING/ACL-2006), pages825-832.
Sydney, Australia.Zhao, Shubin and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of the 43rd Annual Meet-ing of the Association of Computational Linguistics(ACL-2005), pages 419-426.
Ann Arbor, USA.Zhou, Guodong, Jian Su, Jie Zhang and Min Zhang.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of the 43rd Annual Meet-ing of the Association of Computational Linguistics(ACL-2005), pages 427-434.
Ann Arbor, USA.Zhou, Guodong, Min Zhang, Donghong Ji andQiaoming Zhu.
2007.
Tree Kernel-based RelationExtraction with Context-Sensitive Structured ParseTree Information.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational NaturalLanguage Learning (EMNLP/CoNLL-2007), pages728-736.
Prague, Czech.704
