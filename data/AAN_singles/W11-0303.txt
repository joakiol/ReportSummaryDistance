Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 19?28,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsPunctuation: Making a Point in Unsupervised Dependency ParsingValentin I. SpitkovskyComputer Science DepartmentStanford University and Google Inc.valentin@cs.stanford.eduHiyan AlshawiGoogle Inc.Mountain View, CA, 94043, USAhiyan@google.comDaniel JurafskyDepartments of Linguistics and Computer ScienceStanford University, Stanford, CA, 94305, USAjurafsky@stanford.eduAbstractWe show how punctuation can be used to im-prove unsupervised dependency parsing.
Ourlinguistic analysis confirms the strong connec-tion between English punctuation and phraseboundaries in the Penn Treebank.
However,approaches that naively include punctuationmarks in the grammar (as if they were words)do not perform well with Klein and Manning?sDependency Model with Valence (DMV).
In-stead, we split a sentence at punctuation andimpose parsing restrictions over its fragments.Our grammar inducer is trained on the WallStreet Journal (WSJ) and achieves 59.5% ac-curacy out-of-domain (Brown sentences with100 or fewer words), more than 6% higherthan the previous best results.
Further evalu-ation, using the 2006/7 CoNLL sets, revealsthat punctuation aids grammar induction in17 of 18 languages, for an overall averagenet gain of 1.3%.
Some of this improvementis from training, but more than half is fromparsing with induced constraints, in inference.Punctuation-aware decoding works with exist-ing (even already-trained) parsing models andalways increased accuracy in our experiments.1 IntroductionUnsupervised dependency parsing is a type of gram-mar induction ?
a central problem in computationallinguistics.
It aims to uncover hidden relations be-tween head words and their dependents in free-formtext.
Despite decades of significant research efforts,the task still poses a challenge, as sentence structureis underdetermined by only raw, unannotated words.Structure can be clearer in formatted text, whichtypically includes proper capitalization and punctua-tion (Gravano et al, 2009).
Raw word streams, suchas utterances transcribed by speech recognizers, areoften difficult even for humans (Kim and Woodland,2002).
Therefore, one would expect grammar induc-ers to exploit any available linguistic meta-data.
Andyet in unsupervised dependency parsing, sentence-internal punctuation has long been ignored (Carrolland Charniak, 1992; Paskin, 2001; Klein and Man-ning, 2004; Blunsom and Cohn, 2010, inter alia).HTML is another kind of meta-data that is ordi-narily stripped out in pre-processing.
However, re-cently Spitkovsky et al (2010b) demonstrated thatweb markup can successfully guide hierarchicalsyntactic structure discovery, observing, for exam-ple, that anchors often match linguistic constituents:..., whereas McCain is secure on the topic, Obama<a>[VP worries about winning the pro-Israel vote]</a>.We propose exploring punctuation?s potential toaid grammar induction.
Consider a motivating ex-ample (all of our examples are from WSJ), in whichall (six) marks align with constituent boundaries:[SBAR Although it probably has reduced the level ofexpenditures for some purchasers], [NP utilization man-agement] ?
[PP like most other cost containment strate-gies] ?
[VP doesn?t appear to have altered the long-termrate of increase in health-care costs], [NP the Institute ofMedicine], [NP an affiliate of the National Academy ofSciences], [VP concluded after a two-year study].This link between punctuation and constituentboundaries suggests that we could approximateparsing by treating inter-punctuation fragments in-dependently.
In training, our algorithm first parseseach fragment separately, then parses the sequenceof the resulting head words.
In inference, we use abetter approximation that allows heads of fragmentsto be attached by arbitrary external words, e.g.
:The Soviets complicated the issue by offering to[VP include light tanks], [SBAR which are as light as ... ].19Count POS Sequence Frac Cum1 3,492 NNP 2.8%2 2,716 CD CD 2.2 5.03 2,519 NNP NNP 2.0 7.14 2,512 RB 2.0 9.15 1,495 CD 1.2 10.36 1,025 NN 0.8 11.17 1,023 NNP NNP NNP 0.8 11.98 916 IN NN 0.7 12.79 795 VBZ NNP NNP 0.6 13.310 748 CC 0.6 13.911 730 CD DT NN 0.6 14.512 705 PRP VBD 0.6 15.113 652 JJ NN 0.5 15.614 648 DT NN 0.5 16.115 627 IN DT NN 0.5 16.6WSJ +103,148 more with Count ?
621 83.4%Table 1: Top 15 fragments of POS tag sequences in WSJ.Count Non-Terminal Frac Cum1 40,223 S 32.5%2 33,607 NP 27.2 59.73 16,413 VP 13.3 72.94 12,441 PP 10.1 83.05 8,350 SBAR 6.7 89.76 4,085 ADVP 3.3 93.07 3,080 QP 2.5 95.58 2,480 SINV 2.0 97.59 1,257 ADJP 1.0 98.510 369 PRN 0.3 98.8WSJ +1,446 more with Count ?
356 1.2%Table 2: Top 99% of the lowest dominating non-terminalsderiving complete inter-punctuation fragments in WSJ.2 Definitions, Analyses and ConstraintsPunctuation and syntax are related (Nunberg, 1990;Briscoe, 1994; Jones, 1994; Doran, 1998, inter alia).But are there simple enough connections betweenthe two to aid in grammar induction?
This sectionexplores the regularities.
Our study of punctuationin WSJ (Marcus et al, 1993) parallels Spitkovskyet al?s (2010b, ?5) analysis of markup from a web-log, since their proposed constraints turn out to beuseful.
Throughout, we define an inter-punctuationfragment as a maximal (non-empty) consecutive se-quence of words that does not cross punctuationboundaries and is shorter than its source sentence.2.1 A Linguistic AnalysisOut of 51,558 sentences, most ?
37,076 (71.9%) ?contain sentence-internal punctuation.
These punc-tuated sentences contain 123,751 fragments, nearlyall ?
111,774 (90.3%) ?
of them multi-token.Common part-of-speech (POS) sequences compris-ing fragments are diverse (note also their flat distri-bution ?
see Table 1).
The plurality of fragmentsare dominated by a clause, but most are dominatedby one of several kinds of phrases (see Table 2).As expected, punctuation does not occur at all con-stituent boundaries: Of the top 15 productions thatyield fragments, five do not match the exact brack-eting of their lowest dominating non-terminal (seeranks 6, 11, 12, 14 and 15 in Table 3, left).
Four ofthem miss a left-adjacent clause, e.g., S?
S NP VP:[S [S It?s an overwhelming job], [NP she] [VP says.
]]This production is flagged because the fragmentNP VP is not a constituent ?
it is two; still, 49.4%of all fragments do align with whole constituents.Inter-punctuation fragments correspond morestrongly to dependencies (see Table 3, right).
Onlyone production (rank 14) shows a daughter outsideher mother?s fragment.
Some number of such pro-ductions is inevitable and expected, since fragmentsmust coalesce (i.e., the root of at least one fragment?
in every sentence with sentence-internal punc-tuation ?
must be attached by some word from adifferent, external fragment).
We find it noteworthythat in 14 of the 15 most common cases, a word inan inter-punctuation fragment derives precisely therest of that fragment, attaching none of the other,external words.
This is true for 39.2% of all frag-ments, and if we include fragments whose heads at-tach other fragments?
heads, agreement increases to74.0% (see strict and loose constraints in ?2.2, next).2.2 Five Parsing ConstraintsSpitkovsky et al (2010b, ?5.3) showed how to ex-press similar correspondences with markup as pars-ing constraints.
They proposed four constraints butemployed only the strictest three, omitting imple-mentation details.
We revisit their constraints, speci-fying precise logical formulations that we use in ourcode, and introduce a fifth (most relaxed) constraint.Let [x, y] be a fragment (or markup) spanning po-sitions x through y (inclusive, with 1 ?
x < y ?
l), ina sentence of length l. And let [i, j]h be a sealed spanheaded by h (1 ?
i ?
h ?
j ?
l), i.e., the word at po-sition h dominates precisely i .
.
.
j (but none other):i h j20Count Constituent Production Frac Cum1 7,115 PP?
IN NP 5.7%2 5,950 S?
NP VP 4.8 10.63 3,450 NP?
NP PP 2.8 13.34 2,799 SBAR?
WHNP S 2.3 15.65 2,695 NP?
NNP 2.2 17.86 2,615 S?
S NP VP 2.1 19.97 2,480 SBAR?
IN S 2.0 21.98 2,392 NP?
NNP NNP 1.9 23.89 2,354 ADVP?
RB 1.9 25.710 2,334 QP?
CD CD 1.9 27.611 2,213 S?
PP NP VP 1.8 29.412 1,441 S?
S CC S 1.2 30.613 1,317 NP?
NP NP 1.1 31.614 1,314 S?
SBAR NP VP 1.1 32.715 1,172 SINV?
S VP NP NP 0.9 33.6WSJ +82,110 more with Count ?
976 66.4%Count Head-Outward Spawn Frac Cum1 11,928 IN 9.6%2 8,852 NN 7.2 16.83 7,802 NNP 6.3 23.14 4,750 CD 3.8 26.95 3,914 VBD 3.2 30.16 3,672 VBZ 3.0 33.17 3,436 RB 2.8 35.88 2,691 VBG 2.2 38.09 2,304 VBP 1.9 39.910 2,251 NNS 1.8 41.711 1,955 WDT 1.6 43.312 1,409 MD 1.1 44.413 1,377 VBN 1.1 45.514 1,204 IN VBD 1.0 46.515 927 JJ 0.7 47.3WSJ +65,279 more with Count ?
846 52.8%Table 3: Top 15 productions yielding punctuation-induced fragments in WSJ, viewed as constituents (left) and as de-pendencies (right).
For constituents, we recursively expanded any internal nodes that did not align with the associatedfragmentation (underlined).
For dependencies we dropped all daughters that fell entirely in the same region as theirmother (i.e., both inside a fragment, both to its left or both to its right), keeping only crossing attachments (just one).Define inside(h, x, y) as true iff x ?
h ?
y; and letcross(i, j, x, y) be true iff (i < x ?
j ?
x ?
j < y) ?
(i > x ?
i ?
y ?
j > y).
The three tightest constraintsimpose conditions which, when satisfied, disallowsealing [i, j]h in the presence of an annotation [x, y]:strict ?
requires [x, y] itself to be sealed in theparse tree, voiding all seals that straddle exactly oneof {x, y} or protrude beyond [x, y] if their head is in-side.
This constraint holds for 39.2% of fragments.By contrast, only 35.6% of HTML annotations, suchas anchor texts and italics, agree with it (Spitkovskyet al, 2010b).
This necessarily fails in every sen-tence with internal punctuation (since there, somefragment must take charge and attach another), whencross(i, j, x, y) ?
(inside(h, x, y) ?
(i < x ?
j > y)).... the British daily newspaper, The Financial Times .x = i h = j = yloose ?
if h ?
[x, y], requires that everything inx .
.
.
y fall under h, with only h allowed external at-tachments.
This holds for 74.0% of fragments ?87.5% of markup, failing when cross(i, j, x, y).... arrests followed a ?
Snake Day ?
at Utrecht ...i x h = j = ysprawl ?
still requires that h derive x .
.
.
y butlifts restrictions on external attachments.
Holdingfor 92.9% of fragments (95.1% of markup), it failswhen cross(i, j, x, y) ?
?inside(h, x, y).Maryland Club also distributes tea , which ...x = i h y jThese three strictest constraints lend themselves to astraight-forward implementation as an O(l5) chart-based decoder.
Ordinarily, the probability of [i, j]his computed by multiplying the probability of the as-sociated unsealed span by two stopping probabilities?
that of the word at h on the left (adjacent if i = h;non-adjacent if i < h) and on the right (adjacent ifh = j; non-adjacent if h < j).
To impose a con-straint, we ran through all of the annotations [x, y]associated with a sentence and zeroed out this prob-ability if any of them satisfied disallowed conditions.There are faster ?
e.g., O(l4), and even O(l3) ?recognizers for split head automaton grammars (Eis-ner and Satta, 1999).
Perhaps a more practical, butstill clear, approach would be to generate n-best listsusing a more efficient unconstrained algorithm, thenapply the constraints as a post-filtering step.Relaxed constraints disallow joining adjacentsubtrees, e.g., preventing the seal [i, j]h from merg-ing below the unsealed span [j +1, J ]H , on the left:i h j j + 1 H J21tear ?
prevents x .
.
.
y from being torn apart byexternal heads from opposite sides.
It holds for94.7% of fragments (97.9% of markup), and is vi-olated when (x ?
j ?
y > j ?
h < x), in this case.... they ?were not consulted about the [Ridley decision]in advance and were surprised at the action taken .thread ?
requires only that no path from the rootto a leaf enter [x, y] twice.
This holds for 95.0% ofall fragments (98.5% of markup); it is violated when(x ?
j ?
y > j ?
h < x) ?
(H ?
y), again, in thiscase.
Example that satisfies thread but violates tear:The ... changes ?all make a lot of sense to me,?
he added.The case when [i, j]h is to the right is entirely sym-metric, and these constraints could be incorporatedin a more sophisticated decoder (since i and J donot appear in the formulae, above).
We implementedthem by zeroing out the probability of the word at Hattaching that at h (to its left), in case of a violation.Note that all five constraints are nested.
In partic-ular, this means that it does not make sense to com-bine them, for a given annotation [x, y], since the re-sult would just match the strictest one.
Our markupnumber for tear is lower (97.9 versus 98.9%) thanSpitkovsky et al?s (2010b), because theirs allowedcases where markup was neither torn nor threaded.Common structures that violate thread (and, con-sequently, all five of the constraints) include, e.g.,?seamless?
quotations and even ordinary lists:Her recent report classifies the stock as a ?hold.
?The company said its directors, management andsubsidiaries will remain long-term investors and ...2.3 Comparison with MarkupMost punctuation-induced constraints are less ac-curate than the corresponding markup-induced con-straints (e.g., sprawl: 92.9 vs. 95.1%; loose: 74.0vs.
87.5%; but not strict: 39.2 vs. 35.6%).
However,markup is rare: Spitkovsky et al (2010b, ?5.1) ob-served that only 10% of the sentences in their blogwere annotated; in contrast, over 70% of the sen-tences in WSJ are fragmented by punctuation.Fragments are more than 40% likely to be dom-inated by a clause; for markup, this number is be-low 10% ?
nearly 75% of it covered by nounphrases.
Further, inter-punctuation fragments arespread more evenly under noun, verb, prepositional,adverbial and adjectival phrases (approximately27:13:10:3:1 versus 75:13:2:1:1) than markup.13 The Model, Methods and MetricsWe model grammar via Klein and Manning?s (2004)Dependency Model with Valence (DMV), whichordinarily strips out punctuation.
Since this stepalready requires identification of marks, our tech-niques are just as ?unsupervised.?
We would havepreferred to test punctuation in their original set-up,but this approach wasn?t optimal, for several rea-sons.
First, Klein and Manning (2004) trained withshort sentences (up to only ten words, on WSJ10),whereas most punctuation appears in longer sen-tences.
And second, although we could augmentthe training data (say, to WSJ45), Spitkovsky etal.
(2010a) showed that classic EM struggles withlonger sentences.
For this reason, we use ViterbiEM and the scaffolding suggested by Spitkovsky etal.
(2010a) ?
also the setting in which Spitkovsky etal.
(2010b) tested their markup-induced constraints.3.1 A Basic SystemOur system is based on Laplace-smoothed ViterbiEM, following Spitkovsky et al?s (2010a) two-stagescaffolding: the first stage trains with just the sen-tences up to length 15; the second stage then retrainson nearly all sentences ?
those with up to 45 words.InitializationKlein and Manning?s (2004) ?ad-hoc harmonic?
ini-tializer does not work very well for longer sentences,particularly with Viterbi training (Spitkovsky et al,2010a, Figure 3).
Instead, we use an improved ini-tializer that approximates the attachment probabilitybetween two words as an average, over all sentences,of their normalized aggregate weighted distances.Our weighting function is w(d) = 1+1/ lg(1+d).2TerminationSpitkovsky et al (2010a) iterated until successivechanges in overall (best parse) per-token cross-entropy dropped below 2?20 bits.
Since smoothingcan (and does, at times) increase the objective, wefound it more efficient to terminate early, after ten1Markup and fragments are as likely to be in verb phrases.2Integer d ?
1 is a distance between two tokens; lg is log2.22steps of suboptimal models.
We used the lowest-perplexity (not necessarily the last) model found, asmeasured by the cross-entropy of the training data.Constrained TrainingTraining with punctuation replaces ordinary Viterbiparse trees, at every iteration of EM, with the out-put of a constrained decoder.
In all experimentsother than #2 (?5) we train with the loose constraint.Spitkovsky et al (2010b) found this setting to bebest for markup-induced constraints.
We apply it toconstraints induced by inter-punctuation fragments.Constrained InferenceSpitkovsky et al (2010b) recommended using thesprawl constraint in inference.
Once again, we fol-low their advice in all experiments except #2 (?5).3.2 Data Sets and ScoringWe trained on the Penn English Treebank?s WallStreet Journal portion (Marcus et al, 1993).
To eval-uate, we automatically converted its labeled con-stituents into unlabeled dependencies, using deter-ministic ?head-percolation?
rules (Collins, 1999),discarding punctuation, any empty nodes, etc., as isstandard practice (Paskin, 2001; Klein and Manning,2004).
We also evaluated against the parsed portionof the Brown corpus (Francis and Kuc?era, 1979),used as a blind, out-of-domain evaluation set,3 sim-ilarly derived from labeled constituent parse trees.We report directed accuracies ?
fractions of cor-rectly guessed arcs, including the root, in unlabeledreference dependency parse trees, as is also standardpractice (Paskin, 2001; Klein and Manning, 2004).One of our baseline systems (?3.3) produces depen-dency trees containing punctuation.
In this case wedo not score the heads assigned to punctuation anduse forgiving scoring for regular words: creditingcorrect heads separated from their children by punc-tuation alone (from the point of view of the child,looking up to the nearest non-punctuation ancestor).3.3 Baseline SystemsOur primary baseline is the basic system withoutconstraints (standard training).
It ignores punctu-ation, as is standard, scoring 52.0% against WSJ45.A secondary (punctuation as words) baseline in-3Note that WSJ{15, 45} overlap with Section 23 ?
trainingon the test set is standard practice in unsupervised learning.corporates punctuation into the grammar as if it werewords, as in supervised dependency parsing (Nivreet al, 2007b; Lin, 1998; Sleator and Temperley,1993, inter alia).
It is worse, scoring only 41.0%.4,54 Experiment #1: Default ConstraintsOur first experiment compares ?punctuation as con-straints?
to the baseline systems.
We use default set-tings, as recommended by Spitkovsky et al (2010b):loose in training; and sprawl in inference.
Evalua-tion is on Section 23 of WSJ (all sentence lengths).To facilitate comparison with prior work, we also re-port accuracies against shorter sentences, with up toten non-punctuation tokens (WSJ10 ?
see Table 4).We find that both constrained regimes improveperformance.
Constrained decoding alone increasesthe accuracy of a standardly-trained system from52.0% to 54.0%.
And constrained training yields55.6% ?
57.4% in combination with inference.4We were careful to use exactly the same data sets in bothcases, not counting punctuation towards sentence lengths.
Andwe used forgiving scoring (?3.2) when evaluating these trees.5To get this particular number we forced punctuation to betacked on, as a layer below the tree of words, to fairly comparesystems (using the same initializer).
Since improved initializa-tion strategies ?
both ours and Klein and Manning?s (2004)?ad-hoc harmonic?
initializer ?
rely on distances between to-kens, they could be unfairly biased towards one approach or theother, if punctuation counted towards length.
We also trainedsimilar baselines without restrictions, allowing punctuation toappear anywhere in the tree (still with forgiving scoring ?
see?3.2), using the uninformed uniform initializer (Spitkovsky etal., 2010a).
Disallowing punctuation as a parent of a real wordmade things worse, suggesting that not all marks belong nearthe leaves (sentence stops, semicolons, colons, etc.
make moresense as roots and heads).
We tried the weighted initializer alsowithout restrictions and repeated all experiments without scaf-folding, on WSJ15 and WSJ45 alone, but treating punctuationas words never came within even 5% of (comparable) standardtraining.
Punctuation, as words, reliably disrupted learning.WSJ?
WSJ10Supervised DMV 69.8 83.6w/Constrained Inference 73.0 84.3Punctuation as Words 41.7 54.8Standard Training 52.0 63.2w/Constrained Inference 54.0 63.6Constrained Training 55.6 67.0w/Constrained Inference 57.4 67.5Table 4: Directed accuracies on Section 23 of WSJ?
andWSJ10 for the supervised DMV, our baseline systems andthe punctuation runs (all using the weighted initializer).23These are multi-point increases, but they could dis-appear in a more accurate state-of-the-art system.To test this hypothesis, we applied constrained de-coding to a supervised system.
We found that this(ideal) instantiation of the DMV benefits as much ormore than the unsupervised systems: accuracy in-creases from 69.8% to 73.0%.
Punctuation seemsto capture the kinds of, perhaps long-distance, regu-larities that are not accessible to the model, possiblybecause of its unrealistic independence assumptions.5 Experiment #2: Optimal SettingsSpitkovsky et al (2010b) recommended trainingwith loose and decoding with sprawl based on theirexperiments with markup.
But are these the rightsettings for punctuation?
Inter-punctuation frag-ments are quite different from markup ?
they aremore prevalent but less accurate.
Furthermore, weintroduced a new constraint, thread, that Spitkovskyet al (2010b) had not considered (along with tear).We next re-examined the choices of constraints.Our full factorial analysis was similar, but signifi-cantly smaller, than Spitkovsky et al?s (2010b): weexcluded their larger-scale news and web data setsthat are not publicly available.
Nevertheless, westill tried every meaningful combination of settings,testing both thread and tear (instead of strict, sinceit can?t work with sentences containing sentence-internal punctuation), in both training and inference.We did not find better settings than loose for train-ing, and sprawl for decoding, among our options.A full analysis is omitted due to space constraints.Our first observation is that constrained inference,using punctuation, is helpful and robust.
It boostedaccuracy (on WSJ45) by approximately 1.5%, onaverage, with all settings.
Indeed, sprawl was con-sistently (but only slightly, at 1.6%, on average) bet-ter than the rest.
Second, constrained training hurtmore often than it helped.
It degraded accuracy in allbut one case, loose, where it gained approximately0.4%, on average.
Both improvements are statisti-cally significant: p ?
0.036 for training with loose;and p ?
5.6?
10?12 for decoding with sprawl.6 More Advanced MethodsSo far, punctuation has improved grammar inductionin a toy setting.
But would it help a modern system?Our next two experiments employ a slightly morecomplicated set-up, compared with the one used upuntil now (?3.1).
The key difference is that this sys-tem is lexicalized, as is standard among the more ac-curate grammar inducers (Blunsom and Cohn, 2010;Gillenwater et al, 2010; Headden et al, 2009).LexicalizationWe lexicalize only in the second (full data) stage, us-ing the method of Headden et al (2009).
For wordsseen at least 100 times in the training corpus, weaugment their gold POS tag with the lexical item.The first (data poor) stage remains entirely unlexi-calized, with gold POS tags for word classes, as inthe earlier systems (Klein and Manning, 2004).SmoothingWe do not use smoothing in the second stage exceptat the end, for the final lexicalized model.
Stage onestill applies ?add-one?
smoothing at every iteration.7 Experiment #3: State-of-the-ArtThe purpose of these experiments is to compare thepunctuation-enhanced DMV with other, recent state-of-the-art systems.
We find that, lexicalized (?6), ourapproach performs better, by a wide margin; withoutlexicalization (?3.1), it was already better for longer,but not for shorter, sentences (see Tables 5 and 4).We trained a variant of our system without goldpart-of-speech tags, using the unsupervised wordclusters (Clark, 2000) computed by Finkel and Man-ning (2009).6 Accuracy decreased slightly, to 58.2%on Section 23 of WSJ (down only 0.2%).
This resultimproves over substantial performance degradationspreviously observed for unsupervised dependencyparsing with induced word categories (Klein andManning, 2004; Headden et al, 2008, inter alia).6Available from http://nlp.stanford.edu/software/stanford-postagger-2008-09-28.tar.gz:models/egw.bnc.200Brown WSJ?
WSJ10(Headden et al, 2009) ?
?
68.8(Spitkovsky et al, 2010b) 53.3 50.4 69.3(Gillenwater et al, 2010) ?
53.3 64.3(Blunsom and Cohn, 2010) ?
55.7 67.7Constrained Training 58.4 58.0 69.3w/Constrained Inference 59.5 58.4 69.5Table 5: Accuracies on the out-of-domain Brown100 setand Section 23 of WSJ?
and WSJ10, for the lexicalizedpunctuation run and other recent state-of-the-art systems.24Unlexicalized, Unpunctuated Lexicalized ... and PunctuatedCoNLL Year Initialization @15 Training @15 Retraining @45 Retraining @45 Net& Language 1. w/Inference 2. w/Inference 3. w/Inference 3?.
w/Inference GainArabic 2006 23.3 23.6 (+0.3) 32.8 33.1 (+0.4) 31.5 31.6 (+0.1) 32.1 32.6 (+0.5) +1.1?7 25.6 26.4 (+0.8) 33.7 34.2 (+0.5) 32.7 33.6 (+0.9) 34.9 35.3 (+0.4) +2.6Basque ?7 19.3 20.8 (+1.5) 29.9 30.9 (+1.0) 29.3 30.1 (+0.8) 29.3 29.9 (+0.6) +0.6Bulgarian ?6 23.7 24.7 (+1.0) 39.3 40.7 (+1.4) 38.8 39.9 (+1.1) 39.9 40.5 (+0.6) +1.6Catalan ?7 33.2 34.1 (+0.8) 54.8 55.5 (+0.7) 54.3 55.1 (+0.8) 54.3 55.2 (+0.9) +0.9Czech ?6 18.6 19.6 (+1.0) 34.6 35.8 (+1.2) 34.8 35.7 (+0.9) 37.0 37.8 (+0.8) +3.0?7 17.6 18.4 (+0.8) 33.5 35.4 (+1.9) 33.4 34.4 (+1.0) 35.2 36.2 (+1.0) +2.7Danish ?6 22.9 24.0 (+1.1) 35.6 36.7 (+1.2) 36.9 37.8 (+0.9) 36.5 37.1 (+0.6) +0.2Dutch ?6 15.8 16.5 (+0.7) 11.2 12.5 (+1.3) 11.0 11.9 (+1.0) 13.7 14.0 (+0.3) +3.0English ?7 25.0 25.4 (+0.5) 47.2 49.5 (+2.3) 47.5 48.8 (+1.3) 49.3 50.3 (+0.9) +2.8German ?6 19.2 19.6 (+0.4) 27.4 28.0 (+0.7) 27.0 27.8 (+0.8) 28.2 28.6 (+0.4) +1.6Greek ?7 18.5 18.8 (+0.3) 20.7 21.4 (+0.7) 20.5 21.0 (+0.5) 20.9 21.2 (+0.3) +0.7Hungarian ?7 17.4 17.7 (+0.3) 6.7 7.2 (+0.5) 6.6 7.0 (+0.4) 7.8 8.0 (+0.2) +1.4Italian ?7 25.0 26.3 (+1.2) 29.6 29.9 (+0.3) 29.7 29.7 (+0.1) 28.3 28.8 (+0.5) -0.8Japanese ?6 30.0 30.0 (+0.0) 27.3 27.3 (+0.0) 27.4 27.4 (+0.0) 27.5 27.5 (+0.0) +0.1Portuguese ?6 27.3 27.5 (+0.2) 32.8 33.7 (+0.9) 32.7 33.4 (+0.7) 33.3 33.5 (+0.3) +0.8Slovenian ?6 21.8 21.9 (+0.2) 28.3 30.4 (+2.1) 28.4 30.4 (+2.0) 29.8 31.2 (+1.4) +2.8Spanish ?6 25.3 26.2 (+0.9) 31.7 32.4 (+0.7) 31.6 32.3 (+0.8) 31.9 32.3 (+0.5) +0.8Swedish ?6 31.0 31.5 (+0.6) 44.1 45.2 (+1.1) 45.6 46.1 (+0.5) 46.1 46.4 (+0.3) +0.8Turkish ?6 22.3 22.9 (+0.6) 39.1 39.5 (+0.4) 39.9 39.9 (+0.1) 40.6 40.9 (+0.3) +1.0?7 22.7 23.3 (+0.6) 41.7 42.3 (+0.6) 41.9 42.1 (+0.2) 41.6 42.0 (+0.4) +0.1Average: 23.4 24.0 (+0.7) 31.9 32.9 (+1.0) 31.9 32.6 (+0.7) 32.6 33.2 (+0.5) +1.3Table 6: Multi-lingual evaluation for CoNLL sets, measured at all three stages of training, with and without constraints.8 Experiment #4: Multi-Lingual TestingThis final batch of experiments probes the general-ization of our approach (?6) across languages.
Thedata are from 2006/7 CoNLL shared tasks (Buch-holz and Marsi, 2006; Nivre et al, 2007a), wherepunctuation was identified by the organizers, whoalso furnished disjoint train/test splits.
We testedagainst all sentences in their evaluation sets.7,8The gains are not English-specific (see Table 6).Every language improves with constrained decod-ing (more so without constrained training); and allbut Italian benefit in combination.
Averaged acrossall eighteen languages, the net change in accuracy is1.3%.
After standard training, constrained decodingalone delivers a 0.7% gain, on average, never caus-ing harm in any of our experiments.
These gains arestatistically significant: p ?
1.59 ?
10?5 for con-strained training; and p ?
4.27?10?7 for inference.7With the exception of Arabic ?07, from which we discardedone sentence with 145 tokens.
We down-weighed languagesappearing in both years by 50% in our analyses, and excludedChinese entirely, since it had already been cut up at punctuation.8Note that punctuation was treated differently in the twoyears: in ?06, it was always at the leaves of the dependencytrees; in ?07, it matched original annotations of the source tree-banks.
For both, we used punctuation-insensitive scoring (?3.2).We did not detect synergy between the two im-provements.
However, note that without constrainedtraining, ?full?
data sets do not help, on average, de-spite having more data and lexicalization.
Further-more, after constrained training, we detected no ev-idence of benefits to additional retraining: not withthe relaxed sprawl constraint, nor unconstrained.9 Related WorkPunctuation has been used to improve parsing sincerule-based systems (Jones, 1994).
Statistical parsersreap dramatic gains from punctuation (Engel et al,2002; Roark, 2001; Charniak, 2000; Johnson, 1998;Collins, 1997, inter alia).
And it is even known tohelp in unsupervised constituent parsing (Seginer,2007).
But for dependency grammar induction, untilnow, punctuation remained unexploited.Parsing Techniques Most-Similar to ConstraintsA ?divide-and-rule?
strategy that relies on punctua-tion has been used in supervised constituent parsingof long Chinese sentences (Li et al, 2005).
For En-glish, there has been interest in balanced punctua-tion (Briscoe, 1994), more recently using rule-basedfilters (White and Rajkumar, 2008) in a combinatorycategorial grammar (CCG).
Our focus is specifically25on unsupervised learning of dependency grammarsand is similar, in spirit, to Eisner and Smith?s (2005)?vine grammar?
formalism.
An important differenceis that instead of imposing static limits on alloweddependency lengths, our restrictions are dynamic ?they disallow some long (and some short) arcs thatwould have otherwise crossed nearby punctuation.Incorporating partial bracketings into grammarinduction is an idea tracing back to Pereira and Sch-abes (1992).
It inspired Spitkovsky et al (2010b) tomine parsing constraints from the web.
In that samevein, we prospected a more abundant and naturallanguage-resource ?
punctuation, using constraint-based techniques they developed for web markup.Modern Unsupervised Dependency ParsingState-of-the-art in unsupervised dependency pars-ing (Blunsom and Cohn, 2010) uses tree substitu-tion grammars.
These are powerful models, capa-ble of learning large dependency fragments.
To helpprevent overfitting, a non-parametric Bayesian prior,defined by a hierarchical Pitman-Yor process (Pit-man and Yor, 1997), is trusted to nudge training to-wards fewer and smaller grammatical productions.We pursued a complementary strategy: usingKlein and Manning?s (2004) much simpler Depen-dency Model with Valence (DMV), but persistentlysteering training away from certain constructions, asguided by punctuation, to help prevent underfitting.Various Other Uses of Punctuation in NLPPunctuation is hard to predict,9 partly because itcan signal long-range dependences (Lu and Ng,2010).
It often provides valuable cues to NLP taskssuch as part-of-speech tagging and named-entityrecognition (Hillard et al, 2006), information ex-traction (Favre et al, 2008) and machine transla-tion (Lee et al, 2006; Matusov et al, 2006).
Otherapplications have included Japanese sentence anal-ysis (Ohyama et al, 1986), genre detection (Sta-matatos et al, 2000), bilingual sentence align-ment (Yeh, 2003), semantic role labeling (Pradhan etal., 2005), Chinese creation-title recognition (Chenand Chen, 2005) and word segmentation (Li andSun, 2009), plus, recently, automatic vandalism de-9Punctuation has high semantic entropy (Melamed, 1997);for an analysis of the many roles played in the WSJ by thecomma ?
the most frequent and unpredictable punctuationmark in that data set ?
see Beeferman et al (1998, Table 2).tection in Wikipedia (Wang and McKeown, 2010).10 Conclusions and Future WorkPunctuation improves dependency grammar induc-tion.
Many unsupervised (and supervised) parserscould be easily modified to use sprawl-constraineddecoding in inference.
It applies to pre-trained mod-els and, so far, helped every data set and language.Tightly interwoven into the fabric of writing sys-tems, punctuation frames most unannotated plain-text.
We showed that rules for converting markupinto accurate parsing constraints are still optimal forinter-punctuation fragments.
Punctuation marks aremore ubiquitous and natural than web markup: whatlittle punctuation-induced constraints lack in preci-sion, they more than make up in recall ?
perhapsboth types of constraints would work better yet intandem.
For language acquisition, a natural ques-tion is whether prosody could similarly aid grammarinduction from speech (Kahn et al, 2005).Our results underscore the power of simple mod-els and algorithms, combined with common-senseconstraints.
They reinforce insights from joint mod-eling in supervised learning, where simplified, in-dependent models, Viterbi decoding and expressiveconstraints excel at sequence labeling tasks (Rothand Yih, 2005).
Such evidence is particularly wel-come in unsupervised settings (Punyakanok et al,2005), where it is crucial that systems scale grace-fully to volumes of data, on top of the usual desider-ata ?
ease of implementation, extension, under-standing and debugging.
Future work could exploresoftening constraints (Hayes and Mouradian, 1980;Chang et al, 2007), perhaps using features (Eisnerand Smith, 2005; Berg-Kirkpatrick et al, 2010) orby learning to associate different settings with var-ious marks: Simply adding a hidden tag for ?ordi-nary?
versus ?divide?
types of punctuation (Li et al,2005) may already usefully extend our model.AcknowledgmentsPartially funded by the Air Force Research Laboratory (AFRL),under prime contract no.
FA8750-09-C-0181, and by NSF, viaaward #IIS-0811974.
We thank Omri Abend, Slav Petrov andanonymous reviewers for many helpful suggestions, and we areespecially grateful to Jenny R. Finkel for shaming us into usingpunctuation, to Christopher D. Manning for reminding us to ex-plore ?punctuation as words?
baselines, and to Noah A. Smithfor encouraging us to test against languages other than English.26ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1998.CYBERPUNC: A lightweight punctuation annotationsystem for speech.
In ICASSP.T.
Berg-Kirkpatrick, A.
Bouchard-Co?te?, J. DeNero, andD.
Klein.
2010.
Painless unsupervised learning withfeatures.
In NAACL-HLT.P.
Blunsom and T. Cohn.
2010.
Unsupervised inductionof tree substitution grammars for dependency parsing.In EMNLP.E.
J. Briscoe.
1994.
Parsing (with) punctuation, etc.Technical report, Xerox European Research Labora-tory.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In CoNLL.G.
Carroll and E. Charniak.
1992.
Two experiments onlearning probabilistic dependency grammars from cor-pora.
Technical report, Brown University.M.-W. Chang, L. Ratinov, and D. Roth.
2007.
Guidingsemi-supervision with constraint-driven learning.
InACL.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In NAACL.C.
Chen and H.-H. Chen.
2005.
Integrating punctuationrules and na?
?ve Bayesian model for Chinese creationtitle recognition.
In IJCNLP.A.
Clark.
2000.
Inducing syntactic categories by contextdistribution clustering.
In CoNLL-LLL.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In ACL.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.C.
D. Doran.
1998.
Incorporating Punctuation intothe Sentence Grammar: A Lexicalized Tree Adjoin-ing Grammar Perspective.
Ph.D. thesis, University ofPennsylvania.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilexi-cal context-free grammars and head-automaton gram-mars.
In ACL.J.
Eisner and N. A. Smith.
2005.
Parsing with soft andhard constraints on dependency length.
In IWPT.D.
Engel, E. Charniak, and M. Johnson.
2002.
Parsingand disfluency placement.
In EMNLP.B.
Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-Tu?r, and M. Ostendorf.
2008.
Punctuating speech forinformation extraction.
In ICASSP.J.
R. Finkel and C. D. Manning.
2009.
Joint parsing andnamed entity recognition.
In NAACL-HLT.W.
N. Francis and H. Kuc?era, 1979.
Manual of Informa-tion to Accompany a Standard Corpus of Present-DayEdited American English, for use with Digital Com-puters.
Department of Linguistics, Brown University.J.
Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, andB.
Taskar.
2010.
Posterior sparsity in unsuperviseddependency parsing.
Technical report, University ofPennsylvania.A.
Gravano, M. Jansche, and M. Bacchiani.
2009.Restoring punctuation and capitalization in transcribedspeech.
In ICASSP.P.
J. Hayes and G. V. Mouradian.
1980.
Flexible parsing.In ACL.W.
P. Headden, III, D. McClosky, and E. Charniak.2008.
Evaluating unsupervised part-of-speech taggingfor grammar induction.
In COLING.W.
P. Headden, III, M. Johnson, and D. McClosky.2009.
Improving unsupervised dependency parsingwith richer contexts and smoothing.
In NAACL-HLT.D.
Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-Tu?r, M. Harper, M. Ostendorf, and W. Wang.
2006.Impact of automatic comma prediction on POS/nametagging of speech.
In IEEE/ACL: SLT.M.
Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24.B.
E. M. Jones.
1994.
Exploring the role of punctuationin parsing natural text.
COLING.J.
G. Kahn, M. Lease, E. Charniak, M. Johnson, andM.
Ostendorf.
2005.
Effective use of prosody in pars-ing conversational speech.
In HLT-EMNLP.J.-H. Kim and P. C. Woodland.
2002.
Implementation ofautomatic capitalisation generation systems for speechinput.
In ICASSP.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In ACL.Y.-S. Lee, S. Roukos, Y. Al-Onaizan, and K. Papineni.2006.
IBM spoken language translation system.
InTC-STAR: Speech-to-Speech Translation.Z.
Li and M. Sun.
2009.
Punctuation as implicit annota-tions for Chinese word segmentation.
ComputationalLinguistics, 35.X.
Li, C. Zong, and R. Hu.
2005.
A hierarchical parsingapproach with punctuation processing for long Chi-nese sentences.
In IJCNLP.D.
Lin.
1998.
Dependency-based evaluation of MINI-PAR.
In Evaluation of Parsing Systems.W.
Lu and H. T. Ng.
2010.
Better punctuation predictionwith dynamic conditional random fields.
In EMNLP.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics, 19.E.
Matusov, A. Mauser, and H. Ney.
2006.
Automaticsentence segmentation and punctuation prediction forspoken language translation.
In IWSLT.I.
D. Melamed.
1997.
Measuring semantic entropy.
InACL-SIGLEX: Tagging Text with Lexical Semantics.27J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007a.
The CoNLL2007 shared task on dependency parsing.
In EMNLP-CoNLL.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryig?it,S.
Ku?bler, S. Marinov, and E. Marsi.
2007b.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13.G.
Nunberg.
1990.
The Linguistics of Punctuation.CSLI Publications.Y.
Ohyama, T. Fukushima, T. Shutoh, and M. Shutoh.1986.
A sentence analysis method for a Japanese bookreading machine for the blind.
In ACL.M.
A. Paskin.
2001.
Grammatical bigrams.
In NIPS.F.
Pereira and Y. Schabes.
1992.
Inside-outside reesti-mation from partially bracketed corpora.
In ACL.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-tor.
Annals of Probability, 25.S.
Pradhan, K. Hacioglu, W. Ward, J. H. Martin, andD.
Jurafsky.
2005.
Semantic role chunking combin-ing complementary syntactic views.
In CoNLL.V.
Punyakanok, D. Roth, W.-t. Yih, and D. Zimak.
2005.Learning and inference over constrained output.
In IJ-CAI.B.
E. Roark.
2001.
Robust Probabilistic Predictive Syn-tactic Processing: Motivations, Models, and Applica-tions.
Ph.D. thesis, Brown University.D.
Roth and W.-t. Yih.
2005.
Integer linear programminginference for conditional random fields.
In ICML.Y.
Seginer.
2007.
Fast unsupervised incremental parsing.In ACL.D.
D. Sleator and D. Temperley.
1993.
Parsing Englishwith a link grammar.
In IWPT.V.
I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-ning.
2010a.
Viterbi training improves unsuperviseddependency parsing.
In CoNLL.V.
I. Spitkovsky, D. Jurafsky, and H. Alshawi.
2010b.Profiting from mark-up: Hyper-text annotations forguided parsing.
In ACL.E.
Stamatatos, N. Fakotakis, and G. Kokkinakis.
2000.Text genre detection using common word frequencies.In COLING.W.
Y. Wang and K. R. McKeown.
2010.
?Got you!?
: Au-tomatic vandalism detection in Wikipedia with web-based shallow syntactic-semantic modeling.
In COL-ING.M.
White and R. Rajkumar.
2008.
A more precise analy-sis of punctuation for broad-coverage surface realiza-tion with CCG.
In GEAF.K.
C. Yeh.
2003.
Bilingual sentence alignment based onpunctuation marks.
In ROCLING: Student.28
