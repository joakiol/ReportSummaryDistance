Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 169?179,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsInter-annotator Agreement for Dependency Annotation of LearnerLanguageMarwa RaghebIndiana UniversityBloomington, IN USAmragheb@indiana.eduMarkus DickinsonIndiana UniversityBloomington, IN USAmd7@indiana.eduAbstractThis paper reports on a study of inter-annotator agreement (IAA) for a dependencyannotation scheme designed for learner En-glish.
Reliably-annotated learner corpora area necessary step for the development of POStagging and parsing of learner language.
Inour study, three annotators marked severallayers of annotation over different levels oflearner texts, and they were able to obtaingenerally high agreement, especially after dis-cussing the disagreements among themselves,without researcher intervention, illustratingthe feasibility of the scheme.
We pinpointsome of the problems in obtaining full agree-ment, including annotation scheme vaguenessfor certain learner innovations, interface de-sign issues, and difficult syntactic construc-tions.
In the process, we also develop ways tocalculate agreements for sets of dependencies.1 IntroductionLearner corpora have been essential for develop-ing error correction systems and intelligent tutor-ing systems (e.g., Nagata et al 2011; Rozovskayaand Roth, 2010).
So far, error annotation has beenthe main focus, to the exclusion of corpora and an-notation for more basic NLP development, despitethe need for parse information for error detection(Tetreault et al 2010), learner proficiency identifi-cation (Hawkins and Buttery, 2010), and acquisitionresearch (Ragheb and Dickinson, 2011).
Indeed,there is very little work on POS tagging (Thoue?sny,2009; van Rooy and Scha?fer, 2002; de Haan, 2000)or parsing (Rehbein et al 2012; Krivanek and Meur-ers, 2011; Ott and Ziai, 2010) learner language, and,not coincidentally, there is a lack of annotated dataand standards for these tasks.
One issue is in know-ing how to handle innovative learner forms: somemap to a target form before annotating syntax (e.g.,Hirschmann et al 2010), while others propose di-rectly annotating the text (e.g., Ragheb and Dick-inson, 2011).
We follow this latter strand and fur-ther our work towards a syntactically-annotated cor-pus of learner English by: a) presenting an annota-tion scheme for dependencies, integrated with otherannotation layers, and b) testing the inter-annotatoragreement for this scheme.
Despite concerns that di-rect annotation of the linguistic properties of learn-ers may not be feasible (e.g., Rose?n and Smedt,2010), we find that annotators have generally strongagreement, especially after adjudication, and thereasons for disagreement often have as much to dowith the complexities of syntax or interface issues asthey do with learner innovations.Probing grammatical annotation can lead to ad-vancements in research on POS tagging and syntac-tic parsing of learner language, for it shows what canbe annotated reliably and what needs additional di-agnostics.
We specifically report on inter-annotatoragreement (IAA) for the annotation scheme de-scribed in section 2, focusing on dependency an-notation.
There are numerous studies investigatinginter-annotator agreement between coders for differ-ent types of grammatical annotation schemes, focus-ing on part-of-speech, syntactic, or semantic anno-tation (e.g., Passonneau et al 2006; Babarczy et al2006; Civit et al 2003).
For learner language, a169number of error annotation projects include mea-sures of interannotator agreement, (see, e.g., Boyd,2012; Lee et al 2012; Rozovskaya and Roth, 2010;Tetreault and Chodorow, 2008; Bonaventura et al2000), but as far as we are aware, there have been nostudies on IAA for grammatical annotation.We have conducted an IAA study to investigatethe quality and robustness of our annotation scheme,as reported in section 3.
In section 4, we report quan-titative results and a qualitative analysis of this studyto tease apart disagreements due to inherent ambigu-ity or text difficulty from those due to the annotationscheme and/or the guidelines.
The study has alreadyreaped benefits by helping us to revise our annota-tion scheme and guidelines, and the insights gainedhere should be applicable for future development ofother annotation schemes and to parsing studies.On a final note, our dependency annotation allowsfor multiple heads for each token in the corpus, vi-olating the so-called single-head constraint (Ku?bleret al 2009).
In the process of evaluating these de-pendencies (see section 4.1), we also make some mi-nor contributions towards comparing sets of depen-dencies, moving beyond just F-measure (e.g., Ceret al 2010) to account for partial agreements.2 Annotation schemeWe present a sketch of the annotation scheme here,outlining the layers and the general motivation.
Ourgeneral perspective is to annotate as closely as pos-sible to what the learner wrote, marking grammat-ical properties even if the meaning of the sentenceor clause is unclear within the particular grammat-ical analysis.
For example, in the learner sentence(1), the verb admit clearly occurs in the form ofan active verb, and is annotated as such, regard-less of the (passive) meaning of the sentence (cf.was admitted).
In this case, basing the annotationon syntactic evidence makes for a more straightfor-ward task.
Moreover, adhering to a syntactic anal-ysis helps outline the grammatical properties of alearner?s interlanguage and can thus assist in auto-matic tasks such as native language identification(e.g., Tetreault et al 2012), and proficiency level de-termination (Yannakoudakis et al 2011).
(1) When I admit to Korea University, I decide...Another part of the motivation for shying awayfrom marking target forms and annotating the syn-tactic properties of those (cf., e.g., Rehbein et al2012) is that, for general essays from learners ofmany levels, the grammatical evidence can be un-derstood even when the intended meaning is not.Consider (2): in the context of the learner?s es-say, the sentence probably means that this personguards their personal belongings very well becauseof prevalent theft in the city they are talking about.
(2) Now I take very hard my personal stuffs.Annotating the syntax of a target form here couldobscure the grammatical properties of the learner?sproduction (e.g., pluralizing a mass noun).
Encour-aging annotators to focus on the syntactic propertiesand not intended meanings makes identifying the de-pendency relations in a sentence like this one easy.Another aspect of our annotation scheme is thatwe do not directly annotate errors (except for lexi-cal violations; see section 2.1).
Annotators had ac-cess to an extensive manual detailing the annotationscheme, which will be made public soon.1 A briefoutline of the guidelines is in section 3.3.2.1 Initial annotation layersUsing ideas developed for annotating learner lan-guage (Ragheb and Dickinson, 2012, 2011; D?
?az-Negrillo et al 2010; Dickinson and Ragheb, 2009),we annotate several layers before targeting depen-dencies: 1) lemmas (i.e., normalized forms), 2) mor-phological part-of-speech (POS), 3) distributionalPOS, and 4) lexical violations.The idea for lemma annotation is to normalize aword to its dictionary form.
In (3), for example, themisspelled excersice is normalized to the correctly-spelled exercise for the lemma annotation.
We spec-ify that only ?reasonable?
orthographic or phoneticchanges are allowed; thus, for prison, it is lemma-annotated as prison, not person.
In this case, thelemma annotation does not affect the rest of the an-notation, as prison and person are both nouns, butfor no, the entire analysis changes based on whetherwe annotate the lemma as no or not.
Marking nomakes the final tree more difficult, but fits with theprinciple of staying true to the form the learner has1See: http://cl.indiana.edu/?salle170presented.
As we will see in section 4.3, determiningthe lemma can pose challenges for building trees.
(3) After to start , I want to tell that this excer-sice is very important in the life , no only asa prison .We annotate two POS layers, one capturing mor-phological evidence and one for distributional.
Formost words, the layers include the same informa-tion, but mismatches arise with non-canonical struc-tures.
For instance, in (3) the verb (to) start has amorphological POS of base form verb (VV0), butit appears in a context where some other verb formwould better be licensed, e.g., a gerund.
Since wedo not want to overstate claims, we allow for un-derspecified POS tags and annotate the distributionalPOS simply as verb (VV).
The use of two POS lay-ers captures the mismatch between morphology anddistribution without referencing a unified POS.Finally, annotators can mark lexical violationswhen nothing else appears to capture a non-standardform.
Specifically, lexical violations are for syntac-tically ungrammatical forms where the specific wordchoice seems to cause the ungrammaticality.
In (4),for example, about should be marked as a lexical vi-olation.
Lexical violations were intended as a last re-sort, but as we will see in section 4.3, there was con-fusion about when to use lexical violations and whento use other annotations, e.g., POS mismatches.
(4) ...
I agree about me that my country ?s helpand cooperation influenced .
.
.2.2 DependenciesWhile the initial annotation layers are used to buildthe syntactic annotation, the real focus of the anno-tation concerns dependencies.
Using a set of 45 de-pendencies,2 we mark two types of annotations here:1) dependency relations rooted in the lemma and themorphological POS tag, and 2) subcategorization in-formation, reflecting not necessarily what is in thetree, but what is required.
Justification for a mor-phological, or morphosyntactic, layer of dependen-cies, along with a layer of subcategorization, is givenin Ragheb and Dickinson (2012).
Essentially, thesetwo layers allow one to capture issues involving ar-gument structure (e.g., missing argument), without2We use a label set adapted from Sagae et al(2010).having to make the kind of strong claims a layer ofdistributional dependencies would require.
In (5),for example, wondered subcategorizes for a finitecomplement (COMP), but finds a non-finite comple-ment (XCOMP), as the tree is based on the morpho-logical forms (e.g., to).
(5) I wondered what success to be .An example tree is shown in figure 1, where wecan see a number of properties of our trees: a) weannotate many ?raised?
subjects, such as I being thesubject (SUBJ) of both would and like, thereby al-lowing for multiple heads for a single token; b) weignore semantic anomalies, such as the fact that lifeis the subject of be (successful); and c) dependenciescan be selected for, but not realized, as in the case ofcareer subcategorizing for a determiner (DET).3 Inter-annotator agreement study3.1 Selection of annotation textsFrom a learner corpus of written essays we have col-lected from students entering Indiana University, wechose a topic (What Are Your Plans for Life?)
andrandomly selected six essays, based on both learnerproficiency (beginner, intermediate, advanced) andthe native language of the speaker (L1).3 From eachessay, we selected the first paragraph and put the sixparagraphs into two texts; each text contained, inorder, one beginner, one intermediate, and one ad-vanced paragraph.
Text 1 contained 19 sentences(333 tokens), and Text 2 contained 22 sentences(271 tokens).
Annotators were asked to annotateonly these excerpts, but had access to the entire es-says, if they wanted to view them.While the total number of tokens is only 604, thedepth of the annotation is quite significant, in thatthere are at least seven decisions to be made for ev-ery token: lemma, lexical violation, morphologicalPOS, distributional POS, subcategorization, attach-ment, and dependency label, in addition to possi-ble extra dependencies for a given word, i.e., a fewthousand decisions.
It is hard to quantify the ef-fort, as some layers are automatically pre-annotated(see section 3.5) and some are used sparingly (lexi-cal violations), but we estimate around 2000 new orchanged annotations from each annotator.3Korean, Spanish, Chinese, Arabic, Japanese, Hungarian.171ROOT I would like my life to be successful in career ...<ROOT> <SUBJ,VC> <SUBJ,OBJ,XCOMP> <DET> <VC> <SUBJ,PRED> <POBJ> <DET> ...SUBJSUBJROOT VCDETOBJSUBJXCOMPVC PRED JCT POBJFigure 1: Morphosyntactic dependency tree with subcategorization information3.2 AnnotatorsThis study involved three annotators, who were un-dergraduate students at Indiana.
They were nativespeakers of English and majors in Linguistics (2 ju-niors, 1 senior).
Two had had a syntax course beforethe semester, and one was taking it concurrently.We trained them over the course of an academicsemester (fall 2012), by means of weekly meetingsto discuss relevant readings, familiarize them withthe scheme, and give feedback about their annota-tion.
The IAA study took place Nov. 9?Dec.
15.Annotators were taking course credit for partici-pating in this project.
This being the case, they wereencouraged to learn from the experience, and partof their training was to make notes of challengingcases and their decision-making process.
This hasprovided significant depth in qualitatively analyzingthe IAA outcomes (section 4.3).3.3 GuidelinesAt the start of the study, the annotators were givena set of guidelines (around 100 pages) to referenceas they made decisions.
These guidelines outlinethe general principles of the scheme (e.g., give thelearner the benefit of the doubt), an overview of theannotation layers, and annotation examples for eachlayer.
The guidelines refer to the label sets usedfor POS (Sampson, 1995) and dependencies (Sagaeet al 2010), but emphasize the properties of ourscheme.
Although the guidelines discuss generalsyntactic treatment (e.g., ?attach high?
in the case ofattachment ambiguities), a considerable focus is onhandling learner innovations, across different layers.While we cannot list every example of how learnersinnovate, we include instructions and examples thatshould generalize to other non-native constructions(e.g., when to underspecify a label).
Examples ofText 1 Text 2Time Avg.
Min.
Max.
Time Avg.
Min.
Max.A 224 11.8 3 25 151 6.9 2 21B 280 14.7 4 30 170* 8.5 3 20C 480 25.3 8 60 385 17.5 10 45Table 1: Annotation time, in minutes, for phase 1 (*timesfor two sentences were not reported and are omitted)how to treat difficult syntactic constructions are alsoillustrated (e.g., coordination).3.4 Annotation taskVia oral and written instructions, the annotatorswere asked to independently annotate the two textsand take notes on difficult issues, in addition tomarking how long they spent on each sentence.Times are reported in table 1 for the first phase, asdescribed next.
Longer sentences take more time(cf.
Text 1 vs.
Text 2), and annotator times vary,but, given the times of nearly 30?60 minutes per sen-tence at the start of the semester, these times seemedreasonable for the depth of annotation required.The annotation task proceeded in phases.
Phase1: Text 1 was annotated over the course of oneweek, and Text 2 over the next week.
Phase 2: Af-ter an hour-long meeting with annotators coveringgeneral annotation points that seemed to be prob-lematic (e.g., lemma definitions), they were givenanother week to individually go over their annota-tions and make modifications.
At the meeting, noth-ing about the scheme or guidelines was added, andno specific examples from the data being annotatedwere used (only ones from earlier in the semester).Phase 3: Each annotator received a document point-ing out pairwise disagreements between annotators,in a simple textual format like (6).
Each annota-172tor was asked to use this document and make anychanges where they thought that their analysis wasnot the best one, given the other two.
This processtook approximately a week.
Phase 4: The annota-tors met (for three hours) and discussed remainingdifferences, to see whether they could reach a con-sensus.
Each annotator fixed their own file based onthe results of this discussion.
At each point, we tooka snapshot of the data, but at no point did we providefeedback to the annotators on their decisions.
(6) Sentence 2, word 1: relation ... JCT NJCT3.5 Annotation interfaceThe annotation is done via the Brat rapid annotationtool (Stenetorp et al 2012).4 This online interface,shown in figure 2, allows an annotator to drag anarrow between words to create a dependency.
An-notators were given automatically-derived POS tagsfrom TnT (Brants, 2000), trained on the SUSANNEcorpus (Sampson, 1995), but created the dependen-cies from scratch.5 Subcategorizations, lemmas, andlexical violations are annotated within one of thePOS layers; lemmas are noted by the blue shading,and the presence of other layers is noted by asterisks,an interface point discussed in section 4.2.3.
Anno-tators liked the tool, but complained of its slowness.4 Evaluation4.1 Methods of comparisonFor lemma and POS annotation, we can calculatebasic agreement statistics, as there is one annotationfor each token.
But our primary focus is on subcat-egorization and dependency annotation, where therecan be multiple elements (or none) for a given token.For subcategorization, we treat elements as mem-bers of a set, as annotators were told that order wasunimportant (e.g., <SUBJ,OBJ> = <OBJ,SUBJ>);we discuss metrics for this in section 4.1.1.
For de-pendencies, we adapt standard parse evaluation (seeKu?bler et al 2009, ch.
6).
In brief, unlabeled at-tachment agreement (UAA) measures the numberof attachments annotators agree upon for each token,disregarding the label, whereas labeled attachment4http://brat.nlplab.org5Annotators need to provide the dependency annotationssince we lacked an appropriate L2 parser.
It is a goal of thisproject to provide annotated data for parser development.agreement (LAA) requires both the attachment andlabeling to be the same to count as an agreement.Label only agreement (LOA) ignores the head atoken attaches to and only compares labels.All three metrics (UAA, LAA, LOA) require cal-culations for sets of dependencies, described in sec-tions 4.1.1 and 4.1.2.
In figure 3, for instance, oneannotator (accidentally) drew a JCT arrow in thewrong direction, resulting in two heads for is.
Foris, the annotator?s set of dependencies is {(0,ROOT),(1,JCT)}, compared to another?s of {(0,ROOT)}.
Wethus treat dependencies as sets of (head, label) pairs.4.1.1 MetricsFor sets, we use two different calculations.
First isMASI (Measuring Agreement on Set-valued Items,Passonneau et al 2006), which assigns each com-parison between sets a value between 0 and 1, as-signing partial credit for partial set matches and al-lowing one to treat agreement on a per-token basis.We use a simplified form of MASI as follows: 1 =identical sets, 23 = one set is a subset of the other,13= the intersection of the sets is non-null, and so arethe set differences, & 0 = disjoint sets.6The second method is a global comparisonmethod (GCM), which counts all the elements ineach annotator?s sets in the whole file and countsup the total number of agreements.
In the followingsubcategorization example over three tokens, thereare two agreements, compared to four total elementsused by A1 (GCMA1 = 24 ) and compared to threeelements used by A2 (GCMA2 = 23 ).
These metricsare essentially precision and recall, depending uponwhich annotator is seen as the ?gold?
(Ku?bler et al2009, ch.
6).
For MASI scores, we have 0, 1, and 13 ,respectively, giving 113/3, or 0.44.?
A1: {SUBJ}, A2: {}?
A1: {SUBJ}, A2: {SUBJ}?
A1: {SUBJ,PRED}, A2: {SUBJ,OBJ}Since every word is annotated, the methods as-sign similar numbers for dependencies.
Subcatego-rization gives different results, due to empty sets.
Ifannotator 1 and annotator 2 both mark an empty set,6Since our sets tend to be small (rarely bigger than two), wedo not expect much change with a full MASI calculation.173Figure 2: Example of the annotation interfaceroot In my opinion , My Age is Very YoungJCTDETPOBJPUNCTDET SUBJROOTJCTPREDFigure 3: A mistaken arrow (JCT) leading to two dependencies for is ((0,ROOT),(1,JCT))we count full agreement for MASI, i.e., a score of 1;for GCM, nothing gets added to the totals.We could, of course, report various coefficientscommonly used in IAA studies, such as kappa oralpha (see Artstein and Poesio, 2008), but, giventhe large number of classes and lack of predominantclasses, chance agreement seems very small.4.1.2 Dependency-specific issuesAs a minor point: for dependencies, we calcu-late agreements for matches in only attachment orlabeling.
Consider (7), where there is one matchonly in attachment ((24,OBJ)-(24,JCT)), counting to-wards UAA, and one only in labeling ((24,SUBJ)-(22,SUBJ)) for LOA.
Importantly, we have to ensurethat (24,SUBJ) and (24,JCT) are not linked.
(7) A1: {(24,SUBJ), (24,OBJ)}A2: {(22,SUBJ), (24,JCT)}In general, we prioritize identical attachment overlabeling, if a dependency could match in either.We wrote a short script to align attachment/labelmatches between two sets, but omit details here, dueto space.
We generally do not have large sets of de-pendencies to compare, but these technical decisionsshould allow for any situation in the future.4.2 Results4.2.1 Bird?s-eye viewTable 2 presents an overview of pairwise agree-ments between annotators for all 604 tokens.
Of thefour phases of annotation, we report two: the filesthey annotated (and revised) independently (phase2) and the final files after discussion of problematiccases (phase 4).
Annotators reported feeling rushedduring phase 1, so phase 2 numbers likely betterindicate the ability to independently annotate, andphase 4 can help to investigate the reasons for lin-gering disagreements.
The numbers for subcatego-rization and dependency (UAA, LAA) agreementsare the MASI agreement rates.A few observations are evident from these fig-ures.
First, for both POSm (morphology) and POSd(distribution), the high agreement rates reflect thefact that annotators made very few changes to theautomatic pre-annotation, partly because such lay-ers were not heavily emphasized.
Lemmas werealso pre-annotated, as identical to the surface form,but more changes were made here (decapitaliza-tion, affix-stripping, etc.).
Comparing phases 2 and4 shows an improvement in agreement, althoughagreement seems like it could be higher, given thesimplicity of lemma information.
We discuss lem-mas, and associated lexical violations, more in sec-174Annotators lemma POSm POSd Subcat.
UAA LAAP2 P4 P2 P4 P2 P4 P2 P4 P2 P4 P2 P4A, B 93.4 96.9 99.0 98.7 99.2 98.7 85.5 94.0 86.6 97.0 80.0 95.2B, C 94.4 97.7 99.0 99.5 98.7 99.3 86.1 95.7 86.7 97.1 80.3 96.0C, A 92.4 96.9 99.7 99.7 98.5 99.3 86.1 96.6 86.9 97.7 82.4 96.7Table 2: Overview of agreement rates before & after discussion (phases 2 & 4)tion 4.3.Dependency-related annotations had no pre-annotation.
While the starting value of agreementrates for these last three layers is not as high as forlemma and POS annotation, agreement rates around80?85% still seem moderately high.
More importantis how much the agreement rates improved after dis-cussion, achieving approximately 95% agreement.This was without any direct intervention from the re-searchers regarding how to annotate disagreements.We examine dependencies in section 4.2.2 and sub-categorization in 4.2.3, breaking results down bytext to see differences in difficulty.4.2.2 DependenciesWe report MASI agreement rates for dependen-cies in tables 3 and 4 for Text 1 and Text 2, re-spectively.7 Comparing the starting agreement val-ues (e.g., 73.6% vs. 87.8% LAA for annotators Aand B), it is clear that text difficulty had an enor-mous impact on annotator agreement.
The clear dif-ference in tokens per sentence (17.5 in Text 1 vs.12.3 in Text 2; see section 3.1) contributed to thedifferences.
The reported difficulty from annotatorsreferred to more non-native properties present in thetext, and, to a smaller extent, the presence of morecomplex syntactic structures.
Though we take upsome of these issues up again in section 4.3, an in-depth analysis of how text difficulty affects the an-notation task is beyond the scope of this paper, andwe leave it for future investigation.Looking at the agreement rates for Text 1 in ta-ble 3, we can see that the initial rates of agree-ment for UAA and LOA are moderately high, indi-cating that annotator training and guideline descrip-tions were working moderately well.
However, they7We only report MASI scores for dependencies, since theGCM scores are nearly the same.
For example, for raters A &B, the GCM value for phase 4 is 96.15% with respect to eitherannotator vs. 96.10% for MASI.Ann.
UAA LAA LOAP2 P4 P2 P4 P2 P4A, B 81.8 96.1 73.6 93.4 80.3 95.5B, C 80.9 96.2 73.4 94.4 79.3 97.1A,C 83.6 97.6 79.7 96.7 81.8 97.9Table 3: MASI percentages for dependencies, Text 1Ann.
UAA LAA LOAP2 P4 P2 P4 P2 P4A, B 92.6 98.1 87.8 97.4 89.3 97.8B, C 93.8 98.3 88.7 97.9 90.2 98.6A, C 90.9 97.9 85.7 96.8 87.6 97.9Table 4: MASI percentages for dependencies, Text 2are only 73% for LAA.
Note, though, that this maybe more related to issues of fatigue and hurry thanof understanding of the guidelines: the numbers im-prove considerably by phase 4.
The labeled attach-ment rates, for example, increase between 17 and 21percent, to reach values around 95%.For Text 2 in table 4, we notice again the higherphase 2 rates and the similar improvement in phase4, with LAA around 97%.
Encouragingly, despitethe initially lower agreements for Text 1, annotatorswere able to achieve nearly the same level of agree-ment as for the ?easier?
text.
This illustrates thatannotators can learn the scheme, even for difficultsentences, though there may be a tradeoff betweenspeed and accuracy.4.2.3 SubcategorizationFor subcategorization, we present both MASI andGCM percentage rates, as they give different em-phases.
Results are again broken down by text, intables 5 and 6.
As with dependencies, we see solidimprovement from phase 2 to phase 4, and we see175generally higher agreement for Text 2.Ann.
MASI GCM1 GCM2P2 P4 P2 P4 P2 P4A,B 84.3 92.4 81.9 90.8 72.8 88.1B,C 83.6 93.8 74.4 91.6 73.6 90.2A,C 84.9 96.1 83.0 96.4 73.1 92.2Table 5: Agreement rates for subcategorization, Text 1Ann.
MASI GCM1 GCM2P2 P4 P2 P4 P2 P4A,B 87.1 95.9 88.9 96.0 77.2 94.1B,C 89.3 98.0 88.3 98.0 82.0 96.8A,C 87.6 97.2 91.2 97.3 73.7 94.2Table 6: Agreement rates for subcategorization, Text 2The GCM numbers are much lower because of theway empty subcategorization values are handled?being counted towards agreement for MASI andnot for GCM (see section 4.1.1).
A further issue,though, is that one annotator often simply left outsubcategorization annotation for a token.
In table 6,for example, annotators A and C have vastly differ-ent GCM values for phase 2 (91.2% vs. 73.7%), dueto annotator C annotating many more subcategoriza-tion labels.
This is discussed more in section 4.3.2.4.3 Qualitative differencesWe highlight some of the important issues that standout when we take a closer look at the nature of thedisagreements in the final phase.4.3.1 Text-related issuesAs pointed out earlier regarding the differencesbetween Text 1 and Text 2 (section 4.2.2), some dis-agreements are likely due to the nature of the textitself, both because of its non-native properties andbecause of the syntactic complexity.
Starting withunique learner innovations leading to non-uniformtreatment, several cases stemmed from not agreeingon the lemma, when a word looks non-English ordoes not fit the context.
An example is cares in (8):although the guidelines should lead the annotators tochoose care as the lemma, staying true to the learnerform, one annotator chose to accommodate the con-text and changed the lemma to case.
This relyingtoo heavily on intended meaning and not enough onsyntactic evidence?as the scheme is designed for?was a consistent problem.
(8) My majors are bankruptcy , corporate reor-ganizations .
.
.
and arquisisiton cares .For (8), the trees do not change because the dif-ferent lemmas are of the same syntactic category,but more problematic are cases where the trees differbased on different readings.
In the learner sentence(9), the non-agreement between this and cause led toa disagreement of this being a COORD of and vs. thisbeing an APPOS (appositive) of factors.
The anno-tator reported that the choice for this latter analysiscame from treating this as these, again contrary toguidelines but consistent with one meaning.
(9) Sometimes animals are subjected to changedenvironmental factors during their develop-mental process and this cause FA .Another great source of disagreement stems fromthe syntactic complexity of some of the structures,even if native-like, though this can be intertwinedwith non-native properties, as in (10).
Although an-notators eventually agreed on the annotation here,there was initial disagreement on the coordinationstructure of this sentence, questioning whether to becoordinates with pursuing or only with to earn, orwhether pursuing coordinates only with to earn (theanalysis they finally chose).
(10) My most important goals are pursuing theprofession to be a top marketing managerand then to earn a lot of money to buy abeautiful house and a good car .4.3.2 Task-related issuesAnnotator disagreements stemmed not only fromthe text, but from other factors as well, such as as-pects of the scheme that needed more clarification,some interface issues, and the fact that the guidelinesthough extensive, are still not comprehensive.A few parts of the annotation scheme were con-fusing to annotators and likely need refinement.
Forexample, if the form of a word was incorrect, wesaw a lot of lexical violation annotation, even if it176was only an issue of grammatical marking and POS(e.g., did/VVD instead of done/VVN), as opposedto a truly different word choice.
We are currentlytightening the annotation scheme and adding clarifi-cations about lexical violations in our guidelines.As another example, verb raising was often notmarked (cf.
figure 1), in spite of the scheme andguidelines requiring it.
In their comments, annota-tors mentioned that it seemed ?redundant?
to themand that it caused arcs to cross, which they found?unappealing.?
One annotator commented that theydid not have enough syntactic background to seewhy marking multiple subjects was necessary.
Weare thus considering a simpler treatment.
Anotheroption in the future is to hire annotators with morebackground in syntax.The interface may be partly to blame for some dis-agreements, including subcategorizations which an-notators often left unmarked (section 4.2.3) or onlypartly marked (e.g., leaving off a SUBJect for a verbwhich has been raised).
There are a few reasons forthis.
First, marking subcategorization likely neededmore emphasis in the training period, seeing as howit relates to complicated linguistic notions like dis-tinguishing arguments and adjuncts.
Secondly, theinterface is an issue, as the subcategorization field isnot directly visible, compared to the arcs drawn fordependencies; in figure 2, for instance, subcatego-rization can only be seen in the asterisks, which needto be clicked on to be seen and changed.
Relatedly,because it is not always necessary, subcategorizationmay seem more optional and thus forgettable.By the nature of being an in-progress project, theguidelines were necessarily not comprehensive.
Asone example, the TRANS(ition) label was only gen-erally defined, leading to disagreements.
As another,a slash could indicate coordination (actor/actress),and annotators differed on its POS labeling, as eitherCC (coordinating conjunction), or a PUNCT (punc-tuation).
The different POS labels then led to vastlydifferent dependency graphs.
In spite of a lengthysection on how to handle coordination in the guide-lines, it seems that an additional case needs to beadded to the guidelines to cover when punctuation isused as a conjunction.5 Conclusion and outlookDeveloping reliable annotation schemes for learnerlanguage is an important step towards better POStagging and parsing of learner corpora.
We have de-scribed an inter-annotator agreement study that hashelped shed light on several issues, such as the re-liability of our annotation scheme, and has helpedidentify room for improvement.
This study showsthat it is possible to apply a multi-layered depen-dency annotation scheme to learner text with consid-erably good agreement rates between three trainedannotators.
In the future, we will of course beapplying the (revised) annotation scheme to largerdata sets, but we hope other grammatical annota-tion schemes can learn from our experience.
In theshorter term, we are constructing a gold standard ofthe text files used here, to test annotation accuracyand whether any (or all) annotators had consistentdifficulties.
Another next step is to gather a largerpool of data and focus more on analyzing the ef-fects of L1 and learner proficiency level on anno-tation.
Finally, given that syntactic representationscan assist in automating tasks such as developmen-tal profiling of learners (e.g., Vyatkina, 2013), gram-matical error detection (Tetreault et al 2010), iden-tification of native language (e.g., Tetreault et al2012), and proficiency level determination (Dickin-son et al 2012)?all of which impact NLP-basededucational tools?one can explore the effect of spe-cific syntactic decisions on such tasks, as a way toprovide feedback on the annotation scheme.AcknowledgmentsWe would like to thank the three annotators for theirhelp with this experiment.
We also thank the IU CLdiscussion group, as well as the three anonymousreviewers, for their feedback and comments.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Compu-tational Linguistics, 34(4):555?596.Anna Babarczy, John Carroll, and Geoffrey Samp-son.
2006.
Definitional, personal, and mechanicalconstraints on part of speech annotation perfor-mance.
Natural Language Engineering, 12:77?90.177Patrizia Bonaventura, Peter Howarth, and WolfgangMenzel.
2000.
Phonetic annotation of a non-native speech corpus.
In Proceedings Interna-tional Workshop on Integrating Speech Technol-ogy in the (Language) Learning and Assistive In-terface, InStil, pages 10?17.Adriane Amelia Boyd.
2012.
Detecting and Diag-nosing Grammatical Errors for Beginning Learn-ers of German: From Learner Corpus Annotationto Constraint Satisfaction Problems.
Ph.D. thesis,Ohio State University.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the Sixth AppliedNatural Language Processing Conference (ANLP2000), pages 224?231.
Seattle, WA.Daniel Cer, Marie-Catherine de Marneffe, DanielJurafsky, and Christopher D. Manning.
2010.Parsing to Stanford dependencies: Trade-offs be-tween speed and accuracy.
In Proceedings ofLREC-10.
Malta.M.
Civit, A. Ageno, B. Navarro, N.
Buf?
?, and M.
A.Mart??.
2003.
Qualitative and quantitative analy-sis of annotators?
agreement in the developmentof Cast3LB.
In Proceedings of 2nd Workshop onTreebanks and Linguistics Theories (TLT-2003),pages 33?45.Pieter de Haan.
2000.
Tagging non-native En-glish with the TOSCA-ICLE tagger.
In ChristianMair and Markus Hundt, editors, Corpus Linguis-tics and Linguistic Theory, pages 69?79.
Rodopi,Amsterdam.Ana D?
?az-Negrillo, Detmar Meurers, SalvadorValera, and Holger Wunsch.
2010.
Towards in-terlanguage POS annotation for effective learnercorpora in SLA and FLT.
Language Forum, 36(1?2):139?154.
Special Issue on New Trends in Lan-guage Teaching.Markus Dickinson, Sandra Ku?bler, and AnthonyMeyer.
2012.
Predicting learner levels for onlineexercises of Hebrew.
In Proceedings of the Sev-enth Workshop on Building Educational Applica-tions Using NLP, pages 95?104.
Association forComputational Linguistics, Montre?al, Canada.Markus Dickinson and Marwa Ragheb.
2009.
De-pendency annotation for learner corpora.
In Pro-ceedings of the Eighth Workshop on Treebanksand Linguistic Theories (TLT-8), pages 59?70.Milan, Italy.John A. Hawkins and Paula Buttery.
2010.
Criterialfeatures in learner corpora: Theory and illustra-tions.
English Profile Journal, 1(1):1?23.Hagen Hirschmann, Anke Lu?deling, Ines Rehbein,Marc Reznicek, and Amir Zeldes.
2010.
Syntacticoveruse and underuse: A study of a parsed learnercorpus and its target hypothesis.
Talk given atthe Ninth Workshop on Treebanks and LinguisticTheory.Julia Krivanek and Detmar Meurers.
2011.
Compar-ing rule-based and data-driven dependency pars-ing of learner language.
In Proceedings of the Int.Conference on Dependency Linguistics (Depling2011).
Barcelona.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan & ClaypoolPublishers.Sun-Hee Lee, Markus Dickinson, and Ross Israel.2012.
Developing learner corpus annotation forKorean particle errors.
In Proceedings of theSixth Linguistic Annotation Workshop, LAW VI?12, pages 129?133.
Association for Computa-tional Linguistics, Stroudsburg, PA, USA.Ryo Nagata, Edward Whittaker, and Vera Shein-man.
2011.
Creating a manually error-tagged andshallow-parsed learner corpus.
In Proceedingsof the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 1210?1219.
Portland, OR.Niels Ott and Ramon Ziai.
2010.
Evaluating de-pendency parsing performance on German learnerlanguage.
In Proceedings of TLT-9, volume 9,pages 175?186.Rebecca Passonneau, Nizar Habash, and OwenRambow.
2006.
Inter-annotator agreement on amultilingual semantic annotation task.
In Pro-ceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC),pages 1951?1956.Marwa Ragheb and Markus Dickinson.
2011.Avoiding the comparative fallacy in the annota-tion of learner corpora.
In Selected Proceedings ofthe 2010 Second Language Research Forum: Re-178considering SLA Research, Dimensions, and Di-rections, pages 114?124.
Cascadilla ProceedingsProject, Somerville, MA.Marwa Ragheb and Markus Dickinson.
2012.
Defin-ing syntax for learner language annotation.
InProceedings of the 24th International Conferenceon Computational Linguistics (Coling 2012),Poster Session.
Mumbai, India.Ines Rehbein, Hagen Hirschmann, Anke Lu?deling,and Marc Reznicek.
2012.
Better tags give bettertrees - or do they?
Linguistic Issues in LanguageTechnology (LiLT), 7(10).Victoria Rose?n and Koenraad De Smedt.
2010.
Syn-tactic annotation of learner corpora.
In Hilde Jo-hansen, Anne Golden, Jon Erik Hagen, and Ann-Kristin Helland, editors, Systematisk, variert, menikke tilfeldig.
Antologi om norsk som andrespra?k ianledning Kari Tenfjords 60-a?rsdag [Systematic,varied, but not arbitrary.
Anthology about Norwe-gian as a second language on the occasion of KariTenfjord?s 60th birthday], pages 120?132.
Novusforlag, Oslo.Alla Rozovskaya and Dan Roth.
2010.
AnnotatingESL errors: Challenges and rewards.
In Proceed-ings of the NAACL HLT 2010 Fifth Workshop onInnovative Use of NLP for Building EducationalApplications, pages 28?36.
Los Angeles, Califor-nia.Kenji Sagae, Eric Davis, Alon Lavie, andBrian MacWhinney an Shuly Wintner.
2010.Morphosyntactic annotation of childes tran-scripts.
Journal of Child Language, 37(3):705?729.Geoffrey Sampson.
1995.
English for the Computer:The SUSANNE Corpus and Analytic Scheme.Clarendon Press, Oxford.Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,Tomoko Ohta, Sophia Ananiadou, and Jun?ichiTsujii.
2012. brat: a web-based tool for nlp-assisted text annotation.
In Proceedings of theDemonstrations at the 13th Conference of theEuropean Chapter of the Association for Com-putational Linguistics, pages 102?107.
Avignon,France.Joel Tetreault, Daniel Blanchard, Aoife Cahill, andMartin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in na-tive language identification.
In Proceedings ofCOLING 2012, pages 2585?2602.
Mumbai, In-dia.Joel Tetreault and Martin Chodorow.
2008.
Na-tive judgments of non-native usage: experimentsin preposition error detection.
In Proceedingsof the Workshop on Human Judgements in Com-putational Linguistics, HumanJudge ?08, pages24?32.
Association for Computational Linguis-tics, Stroudsburg, PA, USA.Joel Tetreault, Jennifer Foster, and MartinChodorow.
2010.
Using parse features forpreposition selection and error detection.
InProceedings of the ACL 2010 Conference ShortPapers, pages 353?358.
Uppsala, Sweden.Sylvie Thoue?sny.
2009.
Increasing the reliability ofa part-of-speech tagging tool for use with learnerlanguage.
Presentation given at the AutomaticAnalysis of Learner Language (AALL?09) work-shop on automatic analysis of learner language:from a better understanding of annotation needsto the development and standardization of anno-tation schemes.Bertus van Rooy and Lande Scha?fer.
2002.
The ef-fect of learner errors on POS tag errors during au-tomatic POS tagging.
Southern African Linguis-tics and Applied Language Studies, 20:325?335.Nina Vyatkina.
2013.
Specific syntactic complex-ity: Developmental profiling of individuals basedon an annotated learner corpus.
The Modern Lan-guage Journal, 97(S1):1?20.Helen Yannakoudakis, Ted Briscoe, and Ben Med-lock.
2011.
A new dataset and method for au-tomatically grading ESOL texts.
In Proceedingsof the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 180?189.
Portland, OR.179
