Proceedings of the Workshop on Linguistic Distances, pages 25?34,Sydney, July 2006. c?2006 Association for Computational LinguisticsComparison of Similarity Models for the Relation Discovery TaskBen HacheySchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LWbhachey@inf.ed.ac.ukAbstractWe present results on the relation discov-ery task, which addresses some of theshortcomings of supervised relation ex-traction by applying minimally supervisedmethods.
We describe a detailed experi-mental design that compares various con-figurations of conceptual representationsand similarity measures across six differ-ent subsets of the ACE relation extractiondata.
Previous work on relation discoveryused a semantic space based on a term-by-document matrix.
We find that represen-tations based on term co-occurrence per-form significantly better.
We also observefurther improvements when reducing thedimensionality of the term co-occurrencematrix using probabilistic topic models,though these are not significant.1 IntroductionThis paper describes work that aims to improveupon previous approaches to identifying relation-ships between named objects in text (e.g., people,organisations, locations).
Figure 1 contains sev-eral example sentences from the ACE 2005 cor-pus that contain relations and Figure 2 summarisesthe relations occurring in these sentences.
So, forexample, sentence 1 contains an employment rela-tion between Lebron James and Nike, sentence 2contains a sports-affiliation relation between StigToefting and Bolton and sentence 4 contains abusiness relation between Martha Stewart (she)and the board of directors (of Martha Stewart Liv-ing Omnimedia).Possible applications include identifying com-panies taking part in mergers/acquisitions from1 As for that $90 million shoe contract with Nike,it may be a good deal for James.2 Toefting transferred to Bolton in February 2002from German club Hamburg.3 Toyoda founded the automaker in 1937 ... .4 In a statement, she says she?s stepping aside inthe best interest of the company, but she willstay on the board of directors.Figure 1: Example sentences from ACE 2005.Sent Entity1 Entity2 Relation1 Lebron James Nike Employ2 Stig Toefting Bolton Sports-Aff2 Stig Toefting Hamburg Sports-Aff3 Kiichiro Toyoda Toyota Corp Founder4 Martha Stewart board BusinessFigure 2: Example entity pairs and relation types.business newswire, which could be inserted into acorporate intelligence database.
In the biomedicaldomain, we may want to identify relationships be-tween genes and proteins from biomedical publi-cations, e.g.
Hirschman et al (2004), to help scien-tists keep up-to-date on the literature.
Or, we maywant to identify disease and treatment relations inpublications and textbooks, which can be used tohelp formalise medical knowledge and assist gen-eral practitioners in diagnosis, treatment and prog-nosis (Rosario and Hearst, 2004).Another application scenario involves buildingnetworks of relationships from text collections thatindicate the important entities in a domain andcan be used to visualise interactions.
The net-works could provide an alternative to searchingwhen interacting with a document collection.
Thiscould prove beneficial, for example, in investiga-tive journalism.
It might also be used for socialscience research using techniques from social net-work analysis (Marsden and Lin, 1982).
In previ-25ous work, relations have been used for automatictext summarisation as a conceptual representationof sentence content in a sentence extraction frame-work (Filatova and Hatzivassiloglou, 2004).In the next section, we motivate and introducethe relation discovery task, which addresses someof the shortcomings of conventional approaches torelation extraction (i.e.
supervised learning or ruleengineering) by applying minimally supervisedmethods.1 A critical part of the relation discov-ery task is grouping entity pairs by their relationtype.
This is a clustering task and requires a ro-bust conceptual representation of relation seman-tics and a measure of similarity between relations.In previous work (Hasegawa et al, 2004; Chen etal., 2005), the conceptual representation has beenlimited to term-by-document (TxD) models of re-lation semantics.
The current work introduces aterm co-occurrence (TxT) representation for therelation discovery task and shows that it performssignificantly better than the TxD representation.We also explore dimensionality reduction tech-niques, which show a further improvement.Section 3 presents a parameterisation of similar-ity models for relation discovery.
For the purposesof the current work, this consists of the semanticrepresentation for terms (i.e.
how a term?s contextis modelled), dimensionality reduction technique(e.g.
singular value decomposition, latent Dirich-let alocation), and the measure used to computesimilarity.We also build on the evaluation paradigm forrelation discovery with a detailed, controlled ex-perimental setup.
Section 4 describes the experi-ment design, which compares the various systemconfigurations across six different subsets of therelation extraction data from the automatic con-tent extraction (ACE) evaluation.
Finally, Section5 presents results and statistical analysis.2 The Relation Discovery TaskConventionally, relation extraction is consideredto be part of information extraction and has beenapproached through supervised learning or ruleengineering (e.g., Blaschke and Valencia (2002),Bunescu and Mooney (2005)).
However, tradi-tional approaches have several shortcomings.
First1The relation discovery task is minimally supervised inthe sense that it relies on having certain resources such asnamed entity recognition.
The focus of the current paper isthe unsupervised task of clustering relations.and foremost, they are generally based on pre-defined templates of what types of relations ex-ist in the data and thus only capture informationwhose importance was anticipated by the templatedesigners.
This poses reliability problems whenpredicting new data in the same domain as thetraining data will be from a certain epoch in thepast.
Due to language change and topical varia-tion, as time passes, it is likely that the new datawill deviate more and more from the trained mod-els.
Additionally, there are cost problems asso-ciated with the conventional supervised approachwhen updating templates or transferring to a newdomain, both of which require substantial effort inre-engineering rules or re-annotating training data.The goal of the relation discovery task is toidentify the existence of associations between en-tities, to identify the kinds of relations that oc-cur in a corpus and to annotate particular associ-ations with relation types.
These goals correspondto the three main steps in a generalised algorithm(Hasegawa et al, 2004):1.
Identify co-occurring pairs of named entities2.
Group entity pairs using the textual context3.
Label each cluster of entity pairsThe first step is the relation identification task.In the current work, this is assumed to have beendone already.
We use the gold standard relationsin the ACE data in order to isolate the performanceof the second step.
The second step is a clusteringtask and as such it is necessary to compute simi-larity between the co-occurring pairs of named en-tities (relations).
In order to do this, a model of re-lation similarity is required, which is the focus ofthe current work.We also assume that it is possible to perform thethird step.2 The evaluation we present here looksjust at the quality of the clustering and does notattempt to assess the labelling task.3 Modelling Relation SimilarityThe possible space of models for relation similar-ity can be explored in a principled manner by pa-rameterisation.
In this section, we discuss several2Previous approaches select labels from the collection ofcontext words for a relation cluster (Hasegawa et al, 2004;Zhang et al, 2005).
Chen et al (2005) use discriminativecategory matching to make sure that selected labels are alsoable to differentiate between clusters.26parameters including the term context representa-tion, whether or not we apply dimensionality re-duction, and what similarity measure we use.3.1 Term ContextRepresenting texts in such a way that they can becompared is a familiar problem from the fieldsof information retrieval (IR), text mining (TM),textual data analysis (TDA) and natural languageprocessing (NLP) (Lebart and Rajman, 2000).The traditional model for IR and TM is basedon a term-by-document (TxD) vector representa-tion.
Previous approaches to relation discovery(Hasegawa et al, 2004; Chen et al, 2005) havebeen limited to TxD representations, using tf*idfweighting and the cosine similarity measure.
Ininformation retrieval, the weighted term represen-tation works well as the comparison is generallybetween pieces of text with large context vectors.In the relation discovery task, though, the termcontexts (as we will define them in Section 4) canbe very small, often consisting of only one or twowords.
This means that a term-based similaritymatrix between entity pairs is very sparse, whichmay pose problems for performing reliable clus-tering.An alternative method widely used in NLPand cognitive science is to represent a term con-text by its neighbouring words as opposed to thedocuments in which it occurs.
This term co-occurrence (TxT) model is based on the intu-ition that two words are semantically similar ifthey appear in a similar set of contexts (see e.g.Pado and Lapata (2003)).
The current work ex-plores such a term co-occurrence (TxT) represen-tation based on the hypothesis that it will providea more robust representation of relation contextsand help overcome the sparsity problems asso-ciated with weighted term representations in therelation discovery task.
This is compared to abaseline term-by-document (TxD) representationwhich is a re-implementation of the approach usedby Hasegawa et al (2004) and Chen et al (2005).3.2 Dimensionality ReductionDimensionality reduction techniques for docu-ment and corpus modelling aim to reduce descrip-tion length and model a type of semantic similar-ity that is more linguistic in nature (e.g., see Lan-dauer et al?s (1998) discussion of LSA and syn-onym tests).
In the current work, we explore sin-gular value decomposition (Berry et al, 1994), atechnique from linear algebra that has been ap-plied to a number of tasks from NLP and cogni-tive modelling.
We also explore latent Dirichletallocation, a probabilistic technique analogous tosingular value decomposition whose contributionto NLP has not been as thoroughly explored.Singular value decomposition (SVD) has beenused extensively for the analysis of lexical seman-tics under the name of latent semantic analysis(Landauer et al, 1998).
Here, a rectangular matrixis decomposed into the product of three matrices(Xw?p = Ww?nSn?n(Pp?n)T ) with n ?latent se-mantic?
dimensions.
The resulting decompositioncan be viewed as a rotation of the n-dimensionalaxes such that the first axis runs along the directionof largest variation among the documents (Man-ning and Schu?tze, 1999).
W and P representterms and documents in the new space.
And S isa diagonal matrix of singular values in decreasingorder.Taking the product Ww?kSk?k(Pp?k)T overthe first D columns gives the best least square ap-proximation of the original matrix X by a matrixof rank D, i.e.
a reduction of the original matrix toD dimensions.
SVD can equally be applied to theword co-occurrence matrices obtained in the TxTrepresentation presented in Section 2, in whichcase we can think of the original matrix as being aterm ?
co-occurring term feature matrix.While SVD has proved successful and has beenadapted for tasks such as word sense discrimi-nation (Schu?tze, 1998), its behaviour is not easyto interpret.
Probabilistic LSA (pLSA) is a gen-erative probabilistic version of LSA (Hofmann,2001).
This models each word in a document asa sample from a mixture model, but does not pro-vide a probabilistic model at the document level.Latent Dirichlet Allocation (LDA) addresses thisby representing documents as random mixturesover latent topics (Blei et al, 2003).
Besides hav-ing a clear probabilistic interpretation, an addi-tional advantage of these models is that they haveintuitive graphical representations.Figure 3 contains a graphical representationof the LDA model as applied to TxT wordco-occurrence matrices in standard plate nota-tion.
This models the word features f in theco-occurrence context (size N ) of each word w(where w ?
W and |W| = W ) with a mixture oftopics z.
In its generative mode, the LDA modelsamples a topic from the word-specific multino-27?
?
?z?dNT WfFigure 3: Graphical representation of LDA.mial distribution ?.
Then, each context feature isgenerated by sampling from a topic-specific multi-nomial distribution ?z .3 In a manner analogous tothe SVD model, we use the distribution over top-ics for a word w to represent its semantics and weuse the average topic distribution over all contextwords to represent the conceptual content of an en-tity pair context.3.3 Measuring SimilarityCosine (Cos) is commonly used in the literature tocompute similarities between tf*idf vectors:Cos(p, q) =?i piqi??p2?
?q2In the current work, we use cosine over termand SVD representations of entity pair context.However, it is not clear which similarity measureshould be used for the probabilistic topic models.Dagan et al (1997) find that the symmetric infor-mation radius measure performs best on a pseudo-word sense disambiguation task, while Lee (1999)find that the asymmetric skew divergence ?
a gen-eralisation of Kullback-Leibler divergence ?
per-forms best for improving probability estimates forunseen word co-occurrences.In the current work, we compare KL divergencewith two methods for deriving a symmetric mea-3The hyperparameters ?
and ?
are Dirichlet priors on themultinomial distributions for word features (?
?
Dir(?
))and topics (?
?
Dir(?)).
The choice of the Dirichlet isexplained by its conjugacy to the multinomial distribution,meaning that if the parameter (e.g.
?, ?)
for a multinomialdistribution is endowed with a Dirichlet prior then the poste-rior will also be a Dirichlet.
Intuitively, it is a distribution overdistributions used to encode prior knowledge about the pa-rameters (?
and ?)
of the multinomial distributions for wordfeatures and topics.
Practically, it allows efficient estimationof the joint distribution over word features and topics P (~f, ~z)by integrating out ?
and ?.sure.
The KL divergence of two probability dis-tributions (p and q) over the same event space isdefined as:KL(p||q) =?ipi logpiqiIn information-theoretic terms, KL divergence isthe average number of bits wasted by encodingevents from a distribution p with a code based ondistribution q.
The symmetric measures are de-fined as:Sym(p, q) =12[KL(p||q) + KL(q||p)]JS(p, q) =12[KL(p||p + q2)+ KL(q||p + q2)]The first is termed symmetrised KL divergence(Sym) and the second is termed Jensen-Shannon(JS) divergence.
We explore KL divergence aswell as the symmetric measures as it is not knownin advance whether a domain is symmetric or not.Technically, the divergence measures are dis-similarity measures as they calculate the differ-ence between two distributions.
However, theycan be converted to increasing measures of simi-larity through various transformations.
We treatedthis as a parameter to be tuned during develop-ment and considered two approaches.
The first isfrom Dagan et al (1997).
For KL divergence, thisfunction is defined as Sim(p, q) = 10?
?KL(p||q),where ?
is a free parameter, which is tuned on thedevelopment set (as described in Section 4.2).
Thesame procedure is applied for symmetric KL di-vergence and JS divergence.
The second approachis from Lee (1999).
Here similarity for KL is de-fined as Sim(p, q) = C ?KL(p||q), where C isa free parameter to be tuned.4 Experimental Setup4.1 MaterialsFollowing Chen et al (2005), we derive our rela-tion discovery data from the automatic content ex-traction (ACE) 2004 and 2005 materials for eval-uation of information extraction.4 This is prefer-able to using the New York Times data used byHasegawa et al (2004) as it has gold standard an-notation, which can be used for unbiased evalua-tion.The relation clustering data is based on the goldstandard relations in the information extraction4http://www.nist.gov/speech/tests/ace/28data.
We only consider data from newswire orbroadcast news sources.
We constructed six datasubsets from the ACE corpus based on four of theACE entities: persons (PER), organisations (ORG),geographical/social/political entities (GPE) and fa-cilities (FAC).
The six data subsets were chosenduring development based on a lower limit of 50for the data subset size (i.e.
the number of entitypairs in the domain), ensuring that there is a rea-sonable amount of data.
We also set a lower limitof 3 for the number of classes (relation types) in adata subset, ensuring that the clustering task is nottoo simple.The entity pair instances for clustering werechosen based on several criteria.
First, we do notuse ACE?s discourse relations, which are relationsin which the entity referred to is not an official en-tity according to world knowledge.
Second, weonly use pairs with one or more non-stop wordsin the intervening context, that is the context be-tween the two entity heads.5 Finally, we only keeprelation classes with 3 or more members.
Table4.1 contains the full list of relation types from thesubsets of ACE that we used.
(Refer to Table 4.2for definition of the relation type abbreviations.
)We use the Infomap tool6 for singular valuedecomposition of TxT matrices and compute theconceptual content of an entity pair context as theaverage over the reduced D-dimensional represen-tation of the co-occurrence vector of the terms inthe relation context.
For LDA, we use Steyversand Griffiths?
Topic Modeling Toolbox7).
The in-put is produced by a version of Infomap whichwas modified to output the TxT matrix.
Again, wecompute the conceptual content of an entity pairas the average over the topic vectors for the con-text words.
As documents are explicitly modelledin the LDA model, we input a matrix with raw fre-quencies.
In the TxD, unreduced TxT and SVDmodels we use tf*idf term weighting.We use the same preprocessing when prepar-ing the text for building the SVD and probabilistictopic models as we use for processing the interven-ing context of entity pairs.
This consisted of Mx-Terminator (Reynar and Ratnaparkhi., 1997) forsentence boundary detection, the Penn Treebank5Following results reported by Chen et al (2005), whotried unsuccessfully to incorporate words from the surround-ing context to represent a relation?s semantics, we use onlyintervening words.6http://infomap.stanford.edu/7http://psiexp.ss.uci.edu/research/programs_data/toolbox.htmsed script8 for tokenisation, and the Infomap stopword list.
We also use an implementation of thePorter algorithm (Porter, 1980) for stemming.94.2 Model SelectionWe used the ACE 2004 relation data to performmodel selection.
Firstly, dimensionality (D) needsto be optimised for SVD and LDA.
SVD wasfound to perform best with the number of dimen-sions set to 10.
For LDA, dimensionality inter-acts with the divergence-to-similarity conversionso they were tuned jointly.
The optimal con-figuration varies by the divergence measure withD = 50 and C = 14 for KL divergence, D = 200and C = 4 for symmetrised KL, and D = 150and C = 2 for JS divergence.
For all divergencemeasures, Lee?s (1999) method outperformed Da-gan et al?s (1997) method.
Also for all divergencemeasures, the model hyper-parameter ?
was foundto be optimal at 0.0001.
The ?
hyper-parameterwas always set to 50/T following Griffiths andSteyvers (2004).Clustering is performed with the CLUTO soft-ware10 and the technique used is identical acrossmodels.
Agglomerative clustering is used forcomparability with the original relation discoverywork of Hasegawa et al (2004).
This choice wasmotivated because as it is not known in advancehow many clusters there should be in a new do-main.One way to view the clustering problem is asan optimisation process where an optimal cluster-ing is chosen with respect to a criterion functionover the entire solution.
The criterion functionused here was chosen based on performance onthe development data.
We compared a number ofcriterion functions including single link, completelink, group average, I1, I2, E1 and H1.
I1 is acriterion function that maximises sum of pairwisesimilarities between relation instances assigned toeach cluster, I2 is an internal criterion functionthat maximises the similarity between each rela-tion instance and the centroid of the cluster it is as-signed to, E1 is an external criterion function thatminimises the similarity between the centroid vec-tor of each cluster and the centroid vector of the8http://www.cis.upenn.edu/?treebank/tokenizer.sed9http://www.ldc.usb.ve/?vdaniel/porter.pm10http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview29ORG-GPE ORG-ORG PER-FAC PER-GPE PER-ORG PER-PERbasedin 54 subsidiary 36 located 127 located 222 staff 121 business 81subsidiary 27 emporgothr 14 owner 14 resident 79 executive 100 family 20located 15 partner 8 near 4 executive 42 member 44 persocothr 16gpeaffothr 3 member 6 staff 30 emporgothr 27 perorgothr 9employgen 7 employgen 9 near 7located 4 ethnic 5executive 3ideology 3member 3Total 99 Total 64 Total 145 Total 380 Total 305 Total 147Table 1: Relation distributions for entity pair domains.Type Subtype AbbrAGENT-ARTIFACT User-or-Owner ownerEMPLOY/MEMBER Employ-Executive executiveEmploy-Staff staffEmploy-Undet?d employgenMember-of-Group memberOther artothrPartner partnerSubsidiary subsidiaryGPE AFFILIATION Based-In basedinCitizen-or-Resdent residentOther gpeaffothrPER/ORG AFFIL?N Ethnic ethnicIdeology ideologyOther perorgothrPERSONAL-SOC?L Business businessFamily familyOther persocothrPHYSICAL Located locatedNear nearTable 2: Overview of ACE relations with abbrevi-ations used here.entire collection, and H1 is a combined criterionfunction that consists of the ration of I1 over E1.The I2, H1 and H2 criterion functions outper-formed single link, complete link and group aver-age on the development data.
We use I2, whichperformed as well as H1 and H2 and is superiorin terms of computational complexity (Zhao andKarypis, 2004).5 Experiment5.1 MethodThis section describes experimental setup, whichuses relation extraction data from ACE 2005 to an-swer four questions concerning the effectivenessof similarity models based on term co-occurrenceand dimensionality reduction for the relation dis-covery task:1.
Do term co-occurrence models provide a bet-ter representation of relation semantics thanstandard term-by-document vector space?2.
Do textual dimensionality reduction tech-niques provide any further improvements?3.
How do probabilistic topic models performwith respect to SVD on the relation discoverytask?4.
Does one similarity measure (for probabilitydistributions) outperform the others on the re-lation discovery task?System configurations are compared acrosssix different data subsets (entity type pairs, i.e.,organisation-geopolitical entity, organisation-organisation, person-facility, person-geopoliticalentity, person-organisation, person-person)and evaluated following suggestions byDems?ar (2006) for statistical comparison ofclassifiers over multiple data sets.The dependent variable is the clustering perfor-mance as measured by the F-score.
F-score ac-counts for both the amount of predictions madethat are true (Precision) and the amount of trueclasses that are predicted (Recall).
We use theCLUTO implementation of this measure for eval-uating hierarchical clustering.
Based on (Larsenand Aone, 1999), this is a balanced F-score(F = 2RPR+P ) that computes the maximum per-classscore over all possible alignments of gold stan-dard classes with nodes in the hierarchical tree.The average F-score for the entire hierarchical treeis a micro-average over the class-specific scoresweighted according to the relative size of the class.5.2 ResultsTable 3 contains F-score performance on the testset (ACE 2005).
The columns contain results fromthe different system configurations.
The columnlabels in the top row indicate the different repre-sentations of relation similarity.
The column la-bels in the second row indicate the dimensional-30Sem Space TxD TxT TxT TxT TxT TxTDim Red?n None None SVD LDA LDA LDASimilarity Cos Cos Cos KL Sym JSORG-GPE 0.644 0.673 0.645 0.680 0.670 0.673ORG-ORG 0.879 0.922 0.879 0.904 0.900 0.904PER-FAC 0.811 0.827 0.831 0.832 0.826 0.820PER-GPE 0.595 0.637 0.627 0.664 0.642 0.670PER-ORG 0.520 0.551 0.532 0.569 0.552 0.569PER-PER 0.534 0.572 0.593 0.633 0.553 0.618Micro Ave 0.627 0.661 0.652 0.683 0.658 0.681Macro Ave 0.664 0.697 0.684 0.714 0.689 0.709RankAve 5.917 3.083 4.250 1.500 4.000 2.250Table 3: F-score performance on the test data (ACE 2005) using agglomerative clustering with the I2criterion function.ity reduction technique used.
The column labelsin the third row indicated the similarity measureused, i.e.
cosine (Cos) and KL (KL), symmetrisedKL (Sym) and JS (JS) divergence.
The rows con-tain results for the different data subsets.
Whilewe do not use them for analysis of statistical sig-nificance, we include micro and macro averagesover the data subsets.11 We also include the aver-age ranks, which show that the LDA system usingKL divergence performed best.Initial inspection of the table shows that all sys-tems that use the term co-occurrence semanticspace outperform the baseline system that uses theterm-by-document semantic space.
To test for sta-tistical significance, we use non-parametric testsproposed by Dems?ar (2006) for comparing clas-sifiers across multiple data sets.
The use of non-parametric tests is safer here as they do not as-sume normality and outliers have less effect.
Thefirst test we perform is a Friedman test (Friedman,1940), a multiple comparisons technique whichis the non-parametric equivalent of the repeated-measures ANOVA.
The null hypothesis is that allmodels perform the same and observed differencesare random.
With a Friedman statistic (?2F ) of21.238, we reject the null hypothesis at p < 0.01.The first question we wanted to address iswhether term co-occurrence models outperformthe term-by-document representation of relationsemantics.
To address this question, we continuewith post-hoc analysis.
The objective here is to11Averages over data sets are unreliable where it is notclear whether the domains are commensurable (Webb, 2000).We present averages in our results but avoid drawing conclu-sions based on them.compare several conditions to a control (i.e., com-pare the term co-occurrence systems to the term-by-document baseline) so we use a Bonferroni-Dunn test.
At a significance level of p < 0.05,the critical difference for the Bonferroni-Dunn testfor comparing 6 systems across 6 data sets is2.782.
We conclude that the unreduced term co-occurrence system and the LDA systems with KLand JS divergence all perform significantly betterthan baseline, while the SVD system and the LDAsystem with symmetrised KL divergence do not.The second question asks whether SVD andLDA dimensionality reduction techniques provideany further improvement.
We observe that the sys-tems using KL and JS divergence both outperformthe unreduced term co-occurrence system, thoughthe difference is not significant.The third question asks how the probabilistictopic models perform with respect to the SVDmodels.
Here, Holm-correct Wilcoxon signed-ranks tests show that the KL divergence systemperforms significantly better than SVD while thesymmetrised KL divergence and JS divergencesystems do not.The final question is whether one of the diver-gence measures (KL, symmetrised KL or JS) out-performs the others.
With a statistic of ?2F =9.336, we reject the null hypothesis that all sys-tems are the same at p < 0.01.
Post-hoc analysiswith Holm-corrected Wilcoxon signed-ranks testsshow that the KL divergence system and the JSdivergence system both perform significantly bet-ter than the symmetrised KL system at p < 0.05,while there is no significant difference between theKL and JS systems.316 DiscussionAn interesting aspect of using the ACE corpus isthe wealth of linguistic knowledge encoded.
Withrespect to named entities, this includes class infor-mation describing the kind of reference the entitymakes to something in the world (i.e., specific ref-erential, generic referential, under-specified ref-erential) and it includes mention type informa-tion (i.e., names, quantified nominal construc-tions, pronouns).
It also includes information de-scribing the lexical condition of a relation (i.e.,possessive, preposition, pre-modifier, formulaic, ,verbal).
Based on a mapping between gold stan-dard and predicted clusters, we assigned each casea value of 1 or 0 to indicate whether it is a corrector incorrect classification.
We then carried out de-tailed statistical analysis12 to test for effects of theentity and relation information described above oneach system in each domain.Overall, the effects were fairly small and do notgeneralise across domains or systems very well.However, there were some observable tendencies.With respect to entity class, relations with specificreferential entities tend to correlate positively withcorrect classifications while under-specified refer-ential entities tend to correlate negatively with cor-rect classifications.
With respect to entity men-tion type, relations entities that consist of namestend to correlate positively with correct classifica-tions while pronouns tend to correlate negativelywith correct classifications.
Though, this is onlyreliably observed in the PER-GPE domain.
Fi-nally, with respect to lexical condition, we observethat possessive conditioned relations tend to cor-relate negatively, especially in the PER-GPE andPER-ORG domains with the PER-PER domain alsoshowing some effect.
Pre-modifier conditioned re-lations also tend to correlate negatively in the PER-GPE domain.
The effect with verbally conditionedrelations is mixed.
This is probably due to thefact that verbal relations tend to have more wordsoccurring between the entity pair, which providesmore context but can also be misleading when thekey terms describing the relation do not occur be-tween the entity pair (e.g., the first sentence in Fig-ure 1).It is also informative to look at overall proper-ties of the entity pair domains and compare this12For this analysis, we used the Phi coefficient, which isa measure of relatedness for binomial variables that is inter-preted like correlation.Domain Score TTR EntrpyORG-GPE 0.680 0.893 1.554ORG-ORG 0.904 0.720 1.642PER-FAC 0.832 0.933 0.636PER-GPE 0.664 0.933 1.671PER-ORG 0.569 0.973 2.001PER-PER 0.633 0.867 2.179Table 4: System score, type-to-token ratio (TTR)and relation type entropy (Entrpy) for entity pairdomains.to the system performance.
Table 6 contains, foreach domain, the F-score of the LDA+KL system,the type-to-token ratio, and the entropy of the re-lation type distribution for each domain.
Type-to-token ratio (TTR) is the number of words dividedby the number of word instances and indicateshow much repetition there is in word use.
SinceTTR can vary depending on the size of the text,we compute it on a random sample of 75 tokensfrom each domain.
Entropy can be interpreted asa measure of the uniformity of a distribution.
Lowentropy indicates a more spiked distribution whilehigh entropy indicates a more uniform distribu-tion.
Though there is not enough data to make areliable conclusion, it seems that the system doespoorly on domains that have both a high type-to-token ratio and a high entropy (uniform relationtype distribution), while it performs very well ondomains that have low TTR or low entropy.7 Conclusions and Future WorkThis paper presented work on the relation dis-covery task.
We tested several systems for theclustering subtask that use different models of theconceptual/semantic similarity of relations.
Thesemodels included a baseline system based on aterm-by-document representation of term context,which is equivalent to the representation used inprevious work by Hasegawa et al (Hasegawa etal., 2004) and Chen et al (Chen et al, 2005).
Wehypothesised that this representation suffers froma sparsity problem and showed that models thatuse a term co-occurrence representation performsignificantly better.Furthermore, we investigated the use of singularvalue decomposition and latent Dirichlet aloca-tion for dimensionality reduction.
It has been sug-gested that representations using these techniquesare able to model a similarity that is less reliant on32specific word forms and therefore more semanticin nature.
Our experiments showed an improve-ment over a term co-occurrence baseline when us-ing LDA with KL and JS divergence, though itwas not significant.
We also found that LDA withKL divergence performs significantly better thanSVD.Comparing the different divergence measuresfor LDA, we found that KL and JS perform sig-nificantly better than symmetrised KL divergence.Interestingly, the performance of the asymmetricKL divergence and the symmetric JS divergenceis very close, which makes it difficult to con-clude whether the relation discovery domain is asymmetric domain or an asymmetric domain likeLee?s (1999) task of improving probability esti-mates for unseen word co-occurrences.A shortcoming of all the models we will de-scribe here is that they are derived from the basicbag-of-words models and as such do not accountfor word order or other notions of syntax.
Relatedwork on relation discovery by Zhang et al (2005)addresses this shortcoming by using tree kernels tocompute similarity between entity pairs.
In futurework we will extend our experiment to explore theuse of syntactic and semantic features followingthe frame work of Pado and Lapata (2003).
Weare also planning to look at non-parametric ver-sions of LDA that address the model order selec-tion problem and perform an extrinsic evaluationof the relation discovery task.AcknowledgementsThis work was supported by Scottish EnterpriseEdinburgh-Stanford Link grant R37588 as part ofthe EASIE project.
I would like to thank ClaireGrover, Mirella Lapata, Gabriel Murray and Se-bastian Riedell for very useful comments and dis-cussion on this work.
I would also like to thankthe anonymous reviewers for their comments.ReferencesMichael W. Berry, Susan T. Dumais, and Gavin W.O?Brien.
1994.
Using linear algebra for intelligentinformation retrieval.
SIAM Review, 37(4):573?595.Christian Blaschke and Alfonso Valencia.
2002.
Theframe-based module of the suiseki information ex-traction system.
IEEE Intelligent Systems, 17:14?20.David Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3.Razvan C. Bunescu and Raymond J. Mooney.
2005.Subsequence kernels for relation extraction.
In Pro-ceedings of the 19th Conference on Neural Informa-tion Processing Systems, Vancouver, BC, Canada.Jinxiu Chen, Donghong Ji, Chew Lim Tan, andZhengyu Niu.
2005.
Automatic relation extractionwith model order selection and discriminative labelidentification.
In Proceedings of the 2nd Interna-tional Joint Conference on Natural Language Pro-cessing.Ido Dagan, Lillian Lee, and Fernando Pereira.
1997.Similarity-based methods for word sense disam-biguation.
In Proceedings of the 35th Annual Meet-ing of the Association for Computational Linguis-tics, Madrid, Spain.Janez Dems?ar.
2006.
Statistical comparisons of clas-sifiers over multiple data sets.
Journal of MachineLearning Research, 7:1?30, Jan.Elena Filatova and Vasileios Hatzivassiloglou.
2004.Event-based extractive summarization.
In Proceed-ings of the ACL-2004 Text Summarization BranchesOut Workshop, Barcelona, Spain.Milton Friedman.
1940.
A comparison of alternativetests of significance for the problem of m rankings.The Annals of Mathematical Statistics, 11:86?92.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-man.
2004.
Discovering relations among namedentities from large corpora.
In Proceedings of the42nd Annual Meeting of Association of Computa-tional Linguistics.Lynette Hirschman, Alexander Yeh, ChristianBlaschke, and Alfonso Valencia.
2004.
Overviewof BioCreAtIvE: Critical assessment of informationextraction for biology.
In Proceedings of CriticalAssessment of Information Extraction Systems inBiology Workshop (BioCreAtIvE), Granada, Spain.Thomas Hofmann.
2001.
Unsupervised learningby probabilistic latent semantic analysis.
MachineLearning, 42:177?196.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic anal-ysis.
Discourse Processes, 25:259?284.Buornar Larsen and Chinatsu Aone.
1999.
Fast and ef-fective text mining using linear-time document clus-tering.
In Proceedings of the 5th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, San Diego, CA, USA.33Ludovic Lebart and Martin Rajman.
2000.
Comput-ing similarity.
In Robert Dale, Hermann Moisl, andHarold Somers, editors, Handbook of Natural Lan-guage Processing, pages 477?505.
Marcel Dekker,New York.Lillian Lee.
1999.
Measures of distributional similar-ity.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics, Col-lege Park, MD, USA.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Peter V. Marsden and Nan Lin, editors.
1982.
So-cial Structure and Network Analysis.
Sage, BeverlyHills.Sebastian Pado and Mirella Lapata.
2003.
Construct-ing semantic space models from parsed corpora.
InProceedings of the 41st Annual Meeting of the As-sociation for Computational Linguistics, Sapporo,Japan.Martin F. Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14(3):130?137.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proceedings of the 5th Conference onApplied Natural Language Processing, Washington,D.C., USA.Barbara Rosario and Marti Hearst.
2004.
Classifyingsemantic relations in bioscience text.
In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics, Barcelona, Spain.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):91?124.Geoffrey I. Webb.
2000.
Multiboosting: A tech-nique for combining boosting and wagging.
Ma-chine Learning, 40(2):159?196.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,and Chew Lim Tan.
2005.
Discovering relationsfrom a large raw corpus using tree similarity-basedclustering.
In Proceedings of the 2nd InternationalJoint Conference on Natural Language Processing.Ying Zhao and George Karypis.
2004.
Empirical andtheoretical comparisons of selected criterion func-tions for document clustering.
Machine Learning,55:311?331.34
