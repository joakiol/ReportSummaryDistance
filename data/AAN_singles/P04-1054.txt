Dependency Tree Kernels for Relation ExtractionAron CulottaUniversity of MassachusettsAmherst, MA 01002USAculotta@cs.umass.eduJeffrey SorensenIBM T.J. Watson Research CenterYorktown Heights, NY 10598USAsorenj@us.ibm.comAbstractWe extend previous work on tree kernels to estimatethe similarity between the dependency trees of sen-tences.
Using this kernel within a Support VectorMachine, we detect and classify relations betweenentities in the Automatic Content Extraction (ACE)corpus of news articles.
We examine the utility ofdifferent features such as Wordnet hypernyms, partsof speech, and entity types, and find that the depen-dency tree kernel achieves a 20% F1 improvementover a ?bag-of-words?
kernel.1 IntroductionThe ability to detect complex patterns in data is lim-ited by the complexity of the data?s representation.In the case of text, a more structured data source(e.g.
a relational database) allows richer queriesthan does an unstructured data source (e.g.
a col-lection of news articles).
For example, current websearch engines would not perform well on the query,?list all California-based CEOs who have social tieswith a United States Senator.?
Only a structuredrepresentation of the data can effectively providesuch a list.The goal of Information Extraction (IE) is to dis-cover relevant segments of information in a datastream that will be useful for structuring the data.In the case of text, this usually amounts to findingmentions of interesting entities and the relations thatjoin them, transforming a large corpus of unstruc-tured text into a relational database with entries suchas those in Table 1.IE is commonly viewed as a three stage process:first, an entity tagger detects all mentions of interest;second, coreference resolution resolves disparatementions of the same entity; third, a relation extrac-tor finds relations between these entities.
Entity tag-ging has been thoroughly addressed by many statis-tical machine learning techniques, obtaining greaterthan 90% F1 on many datasets (Tjong Kim Sangand De Meulder, 2003).
Coreference resolution isan active area of research not investigated here (Pa-Entity Type LocationApple Organization Cupertino, CAMicrosoft Organization Redmond, WATable 1: An example of extracted fieldssula et al, 2002; McCallum and Wellner, 2003).We describe a relation extraction technique basedon kernel methods.
Kernel methods are non-parametric density estimation techniques that com-pute a kernel function between data instances,where a kernel function can be thought of as a sim-ilarity measure.
Given a set of labeled instances,kernel methods determine the label of a novel in-stance by comparing it to the labeled training in-stances using this kernel function.
Nearest neighborclassification and support-vector machines (SVMs)are two popular examples of kernel methods (Fuku-naga, 1990; Cortes and Vapnik, 1995).An advantage of kernel methods is that they cansearch a feature space much larger than could berepresented by a feature extraction-based approach.This is possible because the kernel function can ex-plore an implicit feature space when calculating thesimilarity between two instances, as described in theSection 3.Working in such a large feature space can lead toover-fitting in many machine learning algorithms.To address this problem, we apply SVMs to the taskof relation extraction.
SVMs find a boundary be-tween instances of different classes such that thedistance between the boundary and the nearest in-stances is maximized.
This characteristic, in addi-tion to empirical validation, indicates that SVMs areparticularly robust to over-fitting.Here we are interested in detecting and classify-ing instances of relations, where a relation is somemeaningful connection between two entities (Table2).
We represent each relation instance as an aug-mented dependency tree.
A dependency tree repre-sents the grammatical dependencies in a sentence;we augment this tree with features for each nodeAT NEAR PART ROLE SOCIALBased-In Relative-location Part-of Affiliate, Founder Associate, GrandparentLocated Subsidiary Citizen-of, Management Parent, SiblingResidence Other Client, Member Spouse, Other-professionalOwner, Other, Staff Other-relative, Other-personalTable 2: Relation types and subtypes.(e.g.
part of speech) We choose this representationbecause we hypothesize that instances containingsimilar relations will share similar substructures intheir dependency trees.
The task of the kernel func-tion is to find these similarities.We define a tree kernel over dependency trees andincorporate this kernel within an SVM to extractrelations from newswire documents.
The tree ker-nel approach consistently outperforms the bag-of-words kernel, suggesting that this highly-structuredrepresentation of sentences is more informative fordetecting and distinguishing relations.2 Related WorkKernel methods (Vapnik, 1998; Cristianini andShawe-Taylor, 2000) have become increasinglypopular because of their ability to map arbitrary ob-jects to a Euclidian feature space.
Haussler (1999)describes a framework for calculating kernels overdiscrete structures such as strings and trees.
Stringkernels for text classification are explored in Lodhiet al (2000), and tree kernel variants are describedin (Zelenko et al, 2003; Collins and Duffy, 2002;Cumby and Roth, 2003).
Our algorithm is similarto that described by Zelenko et al (2003).
Ourcontributions are a richer sentence representation, amore general framework to allow feature weighting,as well as the use of composite kernels to reducekernel sparsity.Brin (1998) and Agichtein and Gravano (2000)apply pattern matching and wrapper techniques forrelation extraction, but these approaches do notscale well to fastly evolving corpora.
Miller et al(2000) propose an integrated statistical parsing tech-nique that augments parse trees with semantic la-bels denoting entity and relation types.
WhereasMiller et al (2000) use a generative model to pro-duce parse information as well as relation informa-tion, we hypothesize that a technique discrimina-tively trained to classify relations will achieve bet-ter performance.
Also, Roth and Yih (2002) learn aBayesian network to tag entities and their relationssimultaneously.
We experiment with a more chal-lenging set of relation types and a larger corpus.3 Kernel MethodsIn traditional machine learning, we are provideda set of training instances S = {x1 .
.
.
xN},where each instance xi is represented by some d-dimensional feature vector.
Much time is spent onthe task of feature engineering ?
searching for theoptimal feature set either manually by consultingdomain experts or automatically through feature in-duction and selection (Scott and Matwin, 1999).For example, in entity detection the original in-stance representation is generally a word vector cor-responding to a sentence.
Feature extraction andinduction may result in features such as part-of-speech, word n-grams, character n-grams, capital-ization, and conjunctions of these features.
In thecase of more structured objects, such as parse trees,features may include some description of the ob-ject?s structure, such as ?has an NP-VP subtree.
?Kernel methods can be particularly effective at re-ducing the feature engineering burden for structuredobjects.
By calculating the similarity between twoobjects, kernel methods can employ dynamic pro-gramming solutions to efficiently enumerate oversubstructures that would be too costly to explicitlyinclude as features.Formally, a kernel function K is a mappingK : X ?
X ?
[0,?]
from instance space Xto a similarity score K(x, y) =?i ?i(x)?i(y) =?
(x) ?
?(y).
Here, ?i(x) is some feature func-tion over the instance x.
The kernel function mustbe symmetric [K(x, y) = K(y, x)] and positive-semidefinite.
By positive-semidefinite, we requirethat the if x1, .
.
.
, xn ?
X, then the n ?
n matrixG defined by Gij = K(xi, xj) is positive semi-definite.
It has been shown that any function thattakes the dot product of feature vectors is a kernelfunction (Haussler, 1999).A simple kernel function takes the dot product ofthe vector representation of instances being com-pared.
For example, in document classification,each document can be represented by a binary vec-tor, where each element corresponds to the presenceor absence of a particular word in that document.Here, ?i(x) = 1 if word i occurs in document x.Thus, the kernel function K(x, y) returns the num-ber of words in common between x and y.
We referto this kernel as the ?bag-of-words?
kernel, since itignores word order.When instances are more structured, as in thecase of dependency trees, more complex kernelsbecome necessary.
Haussler (1999) describes con-volution kernels, which find the similarity betweentwo structures by summing the similarity of theirsubstructures.
As an example, consider a kernelover strings.
To determine the similarity betweentwo strings, string kernels (Lodhi et al, 2000) countthe number of common subsequences in the twostrings, and weight these matches by their length.Thus, ?i(x) is the number of times string x containsthe subsequence referenced by i.
These matches canbe found efficiently through a dynamic program,allowing string kernels to examine long-range fea-tures that would be computationally infeasible in afeature-based method.Given a training set S = {x1 .
.
.
xN}, kernelmethods compute the Gram matrix G such thatGij = K(xi, xj).
Given G, the classifier finds ahyperplane which separates instances of differentclasses.
To classify an unseen instance x, the classi-fier first projects x into the feature space defined bythe kernel function.
Classification then consists ofdetermining on which side of the separating hyper-plane x lies.A support vector machine (SVM) is a type ofclassifier that formulates the task of finding the sep-arating hyperplane as the solution to a quadratic pro-gramming problem (Cristianini and Shawe-Taylor,2000).
Support vector machines attempt to find ahyperplane that not only separates the classes butalso maximizes the margin between them.
The hopeis that this will lead to better generalization perfor-mance on unseen instances.4 Augmented Dependency TreesOur task is to detect and classify relations betweenentities in text.
We assume that entity tagging hasbeen performed; so to generate potential relationinstances, we iterate over all pairs of entities oc-curring in the same sentence.
For each entity pair,we create an augmented dependency tree (describedbelow) representing this instance.
Given a labeledtraining set of potential relations, we define a treekernel over dependency trees which we then use inan SVM to classify test instances.A dependency tree is a representation that de-notes grammatical relations between words in a sen-tence (Figure 1).
A set of rules maps a parse tree toa dependency tree.
For example, subjects are de-pendent on their verbs and adjectives are dependentTroopsTikritadvancedneartttt 01 23Figure 1: A dependency tree for the sentenceTroops advanced near Tikrit.Feature Exampleword troops, Tikritpart-of-speech (24 values) NN, NNPgeneral-pos (5 values) noun, verb, adjchunk-tag NP, VP, ADJPentity-type person, geo-political-entityentity-level name, nominal, pronounWordnet hypernyms social group, cityrelation-argument ARG A, ARG BTable 3: List of features assigned to each node inthe dependency tree.on the nouns they modify.
Note that for the pur-poses of this paper, we do not consider the link la-bels (e.g.
?object?, ?subject?
); instead we use onlythe dependency structure.
To generate the parse treeof each sentence, we use MXPOST, a maximum en-tropy statistical parser1; we then convert this parsetree to a dependency tree.
Note that the left-to-rightordering of the sentence is maintained in the depen-dency tree only among siblings (i.e.
the dependencytree does not specify an order to traverse the tree torecover the original sentence).For each pair of entities in a sentence, we findthe smallest common subtree in the dependency treethat includes both entities.
We choose to use thissubtree instead of the entire tree to reduce noiseand emphasize the local characteristics of relations.We then augment each node of the tree with a fea-ture vector (Table 3).
The relation-argument featurespecifies whether an entity is the first or second ar-gument in a relation.
This is required to learn asym-metric relations (e.g.
X OWNS Y).Formally, a relation instance is a dependency tree1http://www.cis.upenn.edu/?adwait/statnlp.htmlT with nodes {t0 .
.
.
tn}.
The features of node tiare given by ?
(ti) = {v1 .
.
.
vd}.
We refer to thejth child of node ti as ti[j], and we denote the setof all children of node ti as ti[c].
We reference asubset j of children of ti by ti[j] ?
ti[c].
Finally, werefer to the parent of node ti as ti.p.From the example in Figure 1, t0[1] = t2,t0[{0, 1}] = {t1, t2}, and t1.p = t0.5 Tree kernels for dependency treesWe now define a kernel function for dependencytrees.
The tree kernel is a function K(T1, T2) thatreturns a normalized, symmetric similarity score inthe range (0, 1) for two trees T1 and T2.
We de-fine a slightly more general version of the kerneldescribed by Zelenko et al (2003).We first define two functions over the features oftree nodes: a matching function m(ti, tj) ?
{0, 1}and a similarity function s(ti, tj) ?
(0,?].
Let thefeature vector ?
(ti) = {v1 .
.
.
vd} consist of twopossibly overlapping subsets ?m(ti) ?
?
(ti) and?s(ti) ?
?(ti).
We use ?m(ti) in the matchingfunction and ?s(ti) in the similarity function.
Wedefinem(ti, tj) ={1 if ?m(ti) = ?m(tj)0 otherwiseands(ti, tj) =?vq??s(ti)?vr?
?s(tj)C(vq, vr)where C(vq, vr) is some compatibility functionbetween two feature values.
For example, in thesimplest case whereC(vq, vr) ={1 if vq = vr0 otherwises(ti, tj) returns the number of feature values incommon between feature vectors ?s(ti) and ?s(tj).We can think of the distinction between functionsm(ti, tj) and s(ti, tj) as a way to discretize the sim-ilarity between two nodes.
If ?m(ti) 6= ?m(tj),then we declare the two nodes completely dissimi-lar.
However, if ?m(ti) = ?m(tj), then we proceedto compute the similarity s(ti, tj).
Thus, restrict-ing nodes by m(ti, tj) is a way to prune the searchspace of matching subtrees, as shown below.For two dependency trees T1, T2, with root nodesr1 and r2, we define the tree kernel K(T1, T2) asfollows:K(T1, T2) =????
?0 if m(r1, r2) = 0s(r1, r2)+Kc(r1[c], r2[c]) otherwisewhere Kc is a kernel function over children.
Leta and b be sequences of indices such that a is asequence a1 ?
a2 ?
.
.
.
?
an, and likewise for b.Let d(a) = an ?
a1 +1 and l(a) be the length of a.Then we have Kc(ti[c], tj [c]) =?a,b,l(a)=l(b)?d(a)?d(b)K(ti[a], tj [b])The constant 0 < ?
< 1 is a decay factor thatpenalizes matching subsequences that are spreadout within the child sequences.
See Zelenko et al(2003) for a proof that K is kernel function.Intuitively, whenever we find a pair of matchingnodes, we search for all matching subsequences ofthe children of each node.
A matching subsequenceof children is a sequence of children a and b suchthat m(ai, bi) = 1 (?i < n).
For each matchingpair of nodes (ai, bi) in a matching subsequence,we accumulate the result of the similarity functions(ai, bj) and then recursively search for matchingsubsequences of their children ai[c], bj [c].We implement two types of tree kernels.
Acontiguous kernel only matches children subse-quences that are uninterrupted by non-matchingnodes.
Therefore, d(a) = l(a).
A sparse tree ker-nel, by contrast, allows non-matching nodes withinmatching subsequences.Figure 2 shows two relation instances, whereeach node contains the original text plus the featuresused for the matching function, ?m(ti) = {general-pos, entity-type, relation-argument}.
(?NA?
de-notes the feature is not present for this node.)
Thecontiguous kernel matches the following substruc-tures: {t0[0], u0[0]}, {t0[2], u0[1]}, {t3[0], u2[0]}.Because the sparse kernel allows non-contiguousmatching sequences, it matches an additional sub-structure {t0[0, ?, 2], u0[0, ?, 1]}, where (?)
indi-cates an arbitrary number of non-matching nodes.Zelenko et al (2003) have shown the contiguouskernel to be computable in O(mn) and the sparsekernel in O(mn3), where m and n are the numberof children in trees T1 and T2 respectively.6 ExperimentsWe extract relations from the Automatic ContentExtraction (ACE) corpus provided by the NationalInstitute for Standards and Technology (NIST).
ThepersonnounNANAverbARG_Bgeo?political10troopsadvancednounTikritARG_ApersonnounforcesNANAverbmovedNANApreptowardARG_Bttt tt102 34geo?politicalnounBaghdadquicklyadverbNANAARG_AnearprepNANA23uuuuFigure 2: Two instances of the NEAR relation.data consists of about 800 annotated text documentsgathered from various newspapers and broadcasts.Five entities have been annotated (PERSON, ORGA-NIZATION, GEO-POLITICAL ENTITY, LOCATION,FACILITY), along with 24 types of relations (Table2).
As noted from the distribution of relationshiptypes in the training data (Figure 3), data imbalanceand sparsity are potential problems.In addition to the contiguous and sparse treekernels, we also implement a bag-of-words ker-nel, which treats the tree as a vector of featuresover nodes, disregarding any structural informa-tion.
We also create composite kernels by combin-ing the sparse and contiguous kernels with the bag-of-words kernel.
Joachims et al (2001) have shownthat given two kernels K1, K2, the composite ker-nel K12(xi, xj) = K1(xi, xj)+K2(xi, xj) is also akernel.
We find that this composite kernel improvesperformance when the Gram matrix G is sparse (i.e.our instances are far apart in the kernel space).The features used to represent each node areshown in Table 3.
After initial experimentation,the set of features we use in the matching func-tion is ?m(ti) = {general-pos, entity-type, relation-argument}, and the similarity function examines theFigure 3: Distribution over relation types in train-ing data.remaining features.In our experiments we tested the following fivekernels:K0 = sparse kernelK1 = contiguous kernelK2 = bag-of-words kernelK3 = K0 + K2K4 = K1 + K2We also experimented with the function C(vq, vr),the compatibility function between two feature val-ues.
For example, we can increase the importanceof two nodes having the same Wordnet hypernym2.If vq, vr are hypernym features, then we can defineC(vq, vr) ={?
if vq = vr0 otherwiseWhen ?
> 1, we increase the similarity ofnodes that share a hypernym.
We tested a num-ber of weighting schemes, but did not obtain a setof weights that produced consistent significant im-provements.
See Section 8 for alternate approachesto setting C.2http://www.cogsci.princeton.edu/?wn/Avg.
Prec.
Avg.
Rec.
Avg.
F1K1 69.6 25.3 36.8K2 47.0 10.0 14.2K3 68.9 24.3 35.5K4 70.3 26.3 38.0Table 4: Kernel performance comparison.Table 4 shows the results of each kernel withinan SVM.
(We augment the LibSVM3 implementa-tion to include our dependency tree kernel.)
Notethat, although training was done over all 24 rela-tion subtypes, we evaluate only over the 5 high-levelrelation types.
Thus, classifying a RESIDENCE re-lation as a LOCATED relation is deemed correct4.Note also that K0 is not included in Table 4 becauseof burdensome computational time.
Table 4 showsthat precision is adequate, but recall is low.
Thisis a result of the aforementioned class imbalance ?very few of the training examples are relations, sothe classifier is less likely to identify a testing in-stances as a relation.
Because we treat every pairof mentions in a sentence as a possible relation, ourtraining set contains fewer than 15% positive rela-tion instances.To remedy this, we retrain each SVMs for a bi-nary classification task.
Here, we detect, but do notclassify, relations.
This allows us to combine allpositive relation instances into one class, which pro-vides us more training samples to estimate the classboundary.
We then threshold our output to achievean optimal operating point.
As seen in Table 5, thismethod of relation detection outperforms that of themulti-class classifier.We then use these binary classifiers in a cascadingscheme as follows: First, we use the binary SVMto detect possible relations.
Then, we use the SVMtrained only on positive relation instances to classifyeach predicted relation.
These results are shown inTable 6.The first result of interest is that the sparse treekernel, K0, does not perform as well as the con-tiguous tree kernel, K1.
Suspecting that noise wasintroduced by the non-matching nodes allowed inthe sparse tree kernel, we performed the experi-ment with different values for the decay factor ?
={.9, .5, .1}, but obtained no improvement.The second result of interest is that all tree ker-nels outperform the bag-of-words kernel, K2, mostnoticeably in recall performance, implying that the3http://www.csie.ntu.edu.tw/?cjlin/libsvm/4This is to compensate for the small amount of training datafor many classes.Prec.
Rec.
F1K0 ?
?
?K0 (B) 83.4 45.5 58.8K1 91.4 37.1 52.8K1 (B) 84.7 49.3 62.3K2 92.7 10.6 19.0K2 (B) 72.5 40.2 51.7K3 91.3 35.1 50.8K3 (B) 80.1 49.9 61.5K4 91.8 37.5 53.3K4 (B) 81.2 51.8 63.2Table 5: Relation detection performance.
(B) de-notes binary classification.D C Avg.
Prec.
Avg.
Rec.
Avg.
F1K0 K0 66.0 29.0 40.1K1 K1 66.6 32.4 43.5K2 K2 62.5 27.7 38.1K3 K3 67.5 34.3 45.3K4 K4 67.1 35.0 45.8K1 K4 67.4 33.9 45.0K4 K1 65.3 32.5 43.3Table 6: Results on the cascading classification.
Dand C denote the kernel used for relation detectionand classification, respectively.structural information the tree kernel provides is ex-tremely useful for relation detection.Note that the average results reported here arerepresentative of the performance per relation, ex-cept for the NEAR relation, which had slightly lowerresults overall due to its infrequency in training.7 ConclusionsWe have shown that using a dependency tree ker-nel for relation extraction provides a vast improve-ment over a bag-of-words kernel.
While the de-pendency tree kernel appears to perform well at thetask of classifying relations, recall is still relativelylow.
Detecting relations is a difficult task for a ker-nel method because the set of all non-relation in-stances is extremely heterogeneous, and is thereforedifficult to characterize with a similarity metric.
Animproved system might use a different method todetect candidate relations and then use this kernelmethod to classify the relations.8 Future WorkThe most immediate extension is to automaticallylearn the feature compatibility function C(vq, vr).A first approach might use tf-idf to weight each fea-ture.
Another approach might be to calculate theinformation gain for each feature and use that asits weight.
A more complex system might learn aweight for each pair of features; however this seemscomputationally infeasible for large numbers of fea-tures.One could also perform latent semantic indexingto collapse feature values into similar ?categories??
for example, the words ?football?
and ?baseball?might fall into the same category.
Here, C(vq, vr)might return ?1 if vq = vr, and ?2 if vq and vr arein the same category, where ?1 > ?2 > 0.
Anymethod that provides a ?soft?
match between fea-ture values will sharpen the granularity of the kerneland enhance its modeling power.Further investigation is also needed to understandwhy the sparse kernel performs worse than the con-tiguous kernel.
These results contradict those givenin Zelenko et al (2003), where the sparse kernelachieves 2-3% better F1 performance than the con-tiguous kernel.
It is worthwhile to characterize rela-tion types that are better captured by the sparse ker-nel, and to determine when using the sparse kernelis worth the increased computational burden.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-textcollections.
In Proceedings of the Fifth ACM In-ternational Conference on Digital Libraries.Sergey Brin.
1998.
Extracting patterns and rela-tions from the world wide web.
In WebDB Work-shop at 6th International Conference on Extend-ing Database Technology, EDBT?98.M.
Collins and N. Duffy.
2002.
Convolution ker-nels for natural language.
In T. G. Dietterich,S.
Becker, and Z. Ghahramani, editors, Advancesin Neural Information Processing Systems 14,Cambridge, MA.
MIT Press.Corinna Cortes and Vladimir Vapnik.
1995.Support-vector networks.
Machine Learning,20(3):273?297.N.
Cristianini and J. Shawe-Taylor.
2000.
An intro-duction to support vector machines.
CambridgeUniversity Press.Chad M. Cumby and Dan Roth.
2003.
On kernelmethods for relational learning.
In Tom Fawcettand Nina Mishra, editors, Machine Learning,Proceedings of the Twentieth International Con-ference (ICML 2003), August 21-24, 2003, Wash-ington, DC, USA.
AAAI Press.K.
Fukunaga.
1990.
Introduction to Statistical Pat-tern Recognition.
Academic Press, second edi-tion.D.
Haussler.
1999.
Convolution kernels on dis-crete structures.
Technical Report UCS-CRL-99-10, University of California, Santa Cruz.Thorsten Joachims, Nello Cristianini, and JohnShawe-Taylor.
2001.
Composite kernels for hy-pertext categorisation.
In Carla Brodley and An-drea Danyluk, editors, Proceedings of ICML-01, 18th International Conference on MachineLearning, pages 250?257, Williams College, US.Morgan Kaufmann Publishers, San Francisco,US.Huma Lodhi, John Shawe-Taylor, Nello Cristian-ini, and Christopher J. C. H. Watkins.
2000.
Textclassification using string kernels.
In NIPS, pages563?569.A.
McCallum and B. Wellner.
2003.
Toward con-ditional models of identity uncertainty with ap-plication to proper noun coreference.
In IJCAIWorkshop on Information Integration on the Web.S.
Miller, H. Fox, L. Ramshaw, and R. Weischedel.2000.
A novel use of statistical parsing to ex-tract information from text.
In 6th Applied Nat-ural Language Processing Conference.H.
Pasula, B. Marthi, B. Milch, S. Russell, andI.
Shpitser.
2002.
Identity uncertainty and cita-tion matching.Dan Roth and Wen-tau Yih.
2002.
Probabilisticreasoning for entity and relation recognition.
In19th International Conference on ComputationalLinguistics.Sam Scott and Stan Matwin.
1999.
Feature engi-neering for text classification.
In Proceedings ofICML-99, 16th International Conference on Ma-chine Learning.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 sharedtask: Language-independent named entity recog-nition.
In Walter Daelemans and Miles Osborne,editors, Proceedings of CoNLL-2003, pages 142?147.
Edmonton, Canada.Vladimir Vapnik.
1998.
Statistical Learning The-ory.
Whiley, Chichester, GB.D.
Zelenko, C. Aone, and A. Richardella.
2003.Kernel methods for relation extraction.
Jour-nal of Machine Learning Research, pages 1083?1106.
