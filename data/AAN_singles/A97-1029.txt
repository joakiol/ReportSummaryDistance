Nymble:a High-Performance Learning Name-finderDaniel M. BikeiBBN Corporation70 Fawcett StreetCambr idge,  MA 02138db ike l@bbn.comScott MillerBBN Corporation70 Fawcett StreetCambr idge,  MA 02138szmi l le r@bbn.comRichard SchwartzBBN Corporat ion70 Fawcett  StreetCambr idge,  MA 02138schwar tz@bbn.comRalph WeischedelBBN Corporat ion70 Fawcett  StreetCambr idge,  MA 02138we isched@bbn.comAbstractThis paper presents a statistical, earnedapproach to finding names and other non-recursive ntities in text (as per the MUC-6definition of the NE task), using a variant ofthe standard hidden Markov model.
Wepresent our justification for the problem andour approach, a detailed discussion of themodel itself and finally the successful resultsof this new approach.1.
IntroductionIn the past decade, the speech recognition commu-nity has had huge successes in applying hiddenMarkov models, or HMM's to their problems.
Morerecently, the natural anguage processing communityhas effectively employed these models for part-of-speech tagging, as in the seminal (Church, 1988) andother, more recent efforts (Weischedel et al, 1993).We would now propose that HMM's have success-fully been applied to the problem of name-finding.We have built a named-entity (NE) recognitionsystem using a slightly-modified version of anHMM; we call our system "Nymble".
To ourknowledge, Nymble out-performs the best publishedresults of any other learning name-finder.
Further-more, it performs at or above the 90% accuracy level,often considered "near-human performance".The system arose from the NE task as specified inthe last Message Understanding Conference (MUC),where organization ames, person names, locationnames, times, dates, percentages and money amountswere to be delimited in text using SGML-markup.We will describe the various models employed, themethods for training these models and the method for"decoding" on test data (the term "decoding" borrowedfrom the speech recognition community, since onegoal of traversing an HMM is to recover the hiddenstate sequence).
To date, we have successfully gainedand used the model on both English and Spanish, thelatter for MET, the multi-lingual entity task.2.
Background2.1 Name-finding as an Information-theoretic ProblemThe basic premise of the approach is to considerthe raw text encountered when decoding as though ithad passed through anoisy channel, where it had beenoriginally marked with named entities.
~ The job ofthe generative model is to model the original processwhich generated the name-class-annotated words,before they went through the noisy channel.More formally, we must find the most likelysequence of name-classes (NC) given a sequence ofwords (W):Pr(NC I W) (2.1)In order to treat this as a generative model (where itgenerates the original, name-class-annotated words),we use Bayes' Rule:Pr(NC \[ W) -  Pr(W, NC) (2.2)Pr(W)and since the a priori probability of the wordsequence--the denominator--is constant for anygiven sentence, we can maxi-mize Equation 2,2 bymaximizing the numerator alone.1 See (Cover and Thomas, 1991), ch.
2, for an excellent overviewof the principles of information theory.1942.2 PreviousApproaches to Name-f indingPrevious approaches havetypically used manuallyconstructed finite statepatterns (Weischedel, 1995,Appelt et al, 1995).
For START-OF-SENTENCE.every new language and every ~new class of new informationto spot, one has to write anew set of rules to cover thenew language and to cover thenew class of information.
Afinite-state pattern ruleattempts to match against asequence of tokens (words), inmuch the same way as ageneral regular expressionmatcher.In addition to these finite-state pattern approaches, avariant of Brill rules has been applied to the problem,as outlined in (Aberdeen et al, 1995).2.3 Interest in Problem and PotentialApplicationsThe atomic elements of information extraction--indeed, of language as a whole---could be consideredthe who, where, when and how much in a sentence.A name-finder performs what is known as surface- orlightweight-parsing, delimiting sequences of tokensthat answer these important questions.
It can be usedas the first step in a chain of processors: a next levelof processing could relate two or more named entities,or perhaps even give semantics to that relationshipusing a verb.
In this way, further processing coulddiscover the "what" and "how" of a sentence or bodyof text.Furthermore, name-finding can be useful in itsown right: an Internet query system might use name-finding to construct more appropriately-formedqueries: "When was Bill Gates born?"
could yield thequery "BS.1\]_ Gat :es"+born .
Also, name-findingcan be directly employed for link analysis and otherinformation retrieval problems.3.
ModelWe will present the model twice, first in aconceptual and informal overview, then in a more-detailed, formal description of it as a type of HMM.The model bears resemblance to Scott Miller's novelwork in the Air Traffic Information System (ATIS)task, as documented in (Miller et al, 1994).PERSONORGANIZATION(five other n'ame-classes)',NOT-A-NAMEEND-OF-SENTENCEFigure 3.1 Pictorial representation fconceptual model.3.1 Conceptual ModelFigure 3.1 is a pictorial overview of our model.Informally, we have an ergodic HMM with onlyeight internal states (the name classes, including theNOT-A-NAME class), with two special states, theSTART- and END-OF-SENTENCE states.
Within eachof the name-class states, we use a statistical bigramlanguage model, with the usual one-word-per-stateemission.
This means that the number of states ineach of the name-class states is equal to thevocabulary size, IVI.The generation of words and name-classes proceedsin three steps:1.
Select a name-class NC, conditioning on theprevious name-class and the previous word.2.
Generate the first word inside that name-class,conditioning on the current and previous name-classes.3.
Generate all subsequent words inside the currentname-class, where each subsequent word isconditioned on its immediate predecessor.These three steps are repeated until the entire observedword sequence is generated.
Using the Viterbialgorithm, we efficiently search the entire space of allpossible name-class assignments, maximizing thenumerator of Equation 2.2, Pr(W, NC).Informally, the construction of the model in thismanner indicates that we view each type of "name" tobe its own language, with separate bigramprobabilities for generating its words.
While thenumber of word-states within each name-class i  equal195to IVI, this "interior" bigram language model isergodic, i.e., there is a probability associated withevery one of the IVI 2 transitions.
As a parameter-ized, trained model, if such a transition were neverobserved, the model "backs off" to a less-powerfulmodel, as described below, in ?3.3.3 on p. 4.3.2 Words and Word-FeaturesThroughout most of the model, we consider wordsto be ordered pairs (or two-element vectors),composed of word and word-feature, denoted (w, f ) .The word feature is a simple, deterministic computa-tion performed on each word as it is ~ to orfeature computation is an extremely small part of theimplementation, at roughly ten lines of code.
Also,most of the word features are used to distinguishtypes of  numbers, which are language-independent.
2The rationale for having such features is clear: inRoman languages, capitalization gives good evidenceof names.
33.3 Formal ModelThis section describes the model formally,discussing the transition probabilities to the word-states, which "generate" the words of each name-class.3 .3 .1  Top  Level ModelAs with most trained, probabilistic models, weWordFeamreitwoDi~itNumfourDigitNumcontainsDigitAndAlphacontainsDigitAndDashcontainsDigitAndSlashcontainsDigitAndCommacontainsDigitAndPeriodotherNumallCapscapPeriodfirstWordinitCaplowercaseotherExample Text901990A8956-6709-9611/9/8923,000.001.00456789BBNM.first word of sentenceSallycallIntuitionTwo-digit yearFour digit yearProduct codeDateDateMonetary amountMonetary amount, percentageOther numberOrganizationPerson name initialNo useful capitalization i formationCapitalized wordUncapitalized wordPunctuation marks, all other wordsTable 3.1 Word features, examples and intuition behind themlooked up in the vocabulary.
It produces one of thefourteen values in Table 3.1.These values are computed in the order listed, sothat in the case of  non-disjoint feature-classes, such ascontainsDigitAndAlpha andcontainsDigitAndDash, the former will takeprecedence.
The first eight features arise from theneed to distinguish and annotate monetary amounts,percentages, times and dates.
The rest of the featuresdistinguish types of  capitalization and all other words(such as punctuation marks, which are separatetokens).
In particular, the firstWord feature arisesfrom the fact that if a word is capitalized and is thefirst word of the sentence, we have no goodinformation as to why it is capitalized (but note thatallCaps and capPeriod are computed beforef i r  s tWord,  and therefore take precedence).The word feature is the one part of this modelwhich is language-dependent.
Fortunately, the word196have a most accurate, most powerful model, whichwill "back off" to a less-powerful model when there isinsufficient training, and ultimately back-off tounigram probabilities.In order to generate the first word, we must makea transition from one name-class to another, as wellas calculate the likelihood of that word.
Our intuitionwas that a word preceding the start of a name-class(such as "Mr.", "President" or other titles precedingthe PERSON name-class) and the word following aname-class would be strong indicators of thesubsequent and preceding name-classes, respectively.2 Non-english languages tend to use the comma nd period in thereverse way in which English does, i.e., the comma is a decimalpoint and the period separates groups of three digits in largenumbers.
However, the re-ordering ofthe precedence of the tworelevant word-features had little effect when decoding Spanish,so they were left as is.3 Although Spanish as many lower-case words in organizationnames.
See ?4.1 on p. 6 for more details.Accordingly, the probabilitiy for generating the firstword of a name-class i factored into two parts:Pr(NC I NC_I, w_,)- Pr((w,f):rs, I NC, NC_I).
(3.1)The top level model for generating all but the firstword in a name-class iPr((w,f) I (w,f)_,, NO).
(3.2)There is also a magical "+end+" word, so that theprobability may be computed for any current word tobe the final word of its name-class, i.e.,pr((+end+,other) l(w,f):,~,NC ).
(3.3)As one might imagine, it would be useless to havethe first factor in Equation 3.1 be conditioned off ofthe +end+ word, so the probability is conditioned onthe previous real word of the previous name-class,i.e., we computew_ = +end + ifPr(NC\[ NO_l, W_l ) 1 NC_, =START-OF-SENTENCE\[W I = last observed word otherwise(3.4)Note that the above probability is not conditioned onthe word-feature of w_ l, the intuition of which isthat in the cases where the previous word would helpthe model predict the next name-class, the wordfeature---capitalization in particular--is not impor-tant: "Mr." is a good indicator of the next wordbeginning the PERSON name-class, regardless ofcapitalization, especially since it is almost never seenas "mr.".3.3.2 Calculation of ProbabilitiesThe calculation of the above probabilities isstraightforward, using events/sample-size:Pr(NC I NC_, ' W_l)= c(NC, NC_,,w_,) (3.5)c(NC_I,W_ 1)pr((w,f):r., \] NC, NC_,)= c((w'f)e'*:NC'NC-O (3.6)c( NC, NC_, )c(lw, f>,(w,:>_,, ,vc) pr((w,f) I (w,f)_l, gC)= C((w'f)-l'gC) (3.7)where c0 represents he number of times the eventsoccurred in the training data (the count).3.3.3 Back-off Models and SmoothingIdeally, we would have sufficient raining (or atleast one observation of!)
every event whoseconditional probability we wish to calculate.
Also,ideally, we would have sufficient samples of thatupon which each conditional probability isconditioned, e.g., for Pr(NC I NC_,, w_,), we wouldlike to have seen sufficient numbers of NC_I, w_~.Unfortunately, there is rarely enough training data tocompute accurate probabilities when "decoding" onnew data.3.3.3.1 Unknown WordsThe vocabulary of the system is built as it trains.Necessarily, then, the system knows about all wordsfor which it stores bigram counts in order to computethe probabilities in Equations 3.1 - 3.3.
Thequestion arises how the system should deal withunknown words, since there are three ways in whichthey can appear in a bigram: as the current word, asthe previous word or as both.
A good answer is totrain a separate, unknown word-model off of held-outdata, to gather statistics of unknown words occurringin the midst of known words.Typically, one holds out 10-20% of one'straining for smoothing or unknown word-training.In order to overcome the limitations of a smallamount of training data--particularly in Spanish--wehold out 50% of our data to train the unknown word-model (the vocabulary is built up on the first 50%),save these counts in training data file, then hold outthe other 50% and concatentate hese bigram countswith the first unknown word-training file.
This way,we can gather likelihoods of an unknown wordappearing in the bigram using all available trainingdata.
This approach is perfectly valid, as we amtrying to estimate that which we have notlegitimately seen in training.
When decoding, ifeither word of the bigram is unknown, the model usedto estimate the probabilities of Equations 3.1-3 is theunknown word model, otherwise it is the model fromthe normal training.
The unknown word-model canbe viewed as a first level of back-off, therefore, sinceit is used as a backup model when an unknown wordis encountered, and is necessarily not as accurate asthe bigram model formed from the actual training.3.3.3.2 Further Back-off Models andSmoothingWhether a bigram contains an unknown word ornot, it is possible that either model may not haveseen this bigram, in which case the model backs offto a less-powerful, less-descriptive model.
Table 3.2shows a graphic illustration of the back-off scheme:197Name-class Bi~ramsPr(NC I NC_ I, w_,)Pr(NC I NC_,)Pr( NC)1number of name- classesFirst-word BiisramsPr((w,:>,,,, I NC, l,:C_,)Pr((w,f) l (+begin+, other), NC)Pr((w,f) \[ NC)Pr(w I NC).
Pr(f I NC)1 1IVl number of word featuresTable 3.2 Back-off strategyThe weight for each back-off model is computed on-the-fly, using the following formula:If computing Pr(XIY), assign weight of ;~ tothe direct computation (using one of theformulae of ?3.3.2) and a weight of (1 - ;t.) tothe back-off model, where( i ,/q, = I c(Y) J I + unique outcomes of Yc(Y)(3.8)where "old c(Y)" is the sample size of the model fromwhich we are backing off.
This is a rather simplemethod of smoothing, which tends to work wellwhen there are only three or four levels of back-off:This method also overcomes the problem when aback-off model has roughly the same amount oftraining as the current model, via the first factor ofEquation 3.8, which essentially ignores the back-offmodel and puts all the weight on the primary model,in such an equi-trained situation.As an example---disregarding the first factor--ifwe saw the bigram "come hither" once in training andwe saw "come here" three times, and nowhere lse didwe see the word "come" in the NOT-A-NAME class,when computingPr("hither" I "come", NOT-A-NAME),we would back off to the unigram probabilityPr("hither" I NOT-A-NAME)with a weight of ?, since the number of uniqueoutcomes for the word-state for "come" would be two,and the total number of times "come" had been thepreceding word in a bigram would be four (a4 Any more levels of back-off might require a more sophisticatedsmoothing technique, such as deleted interpolation.
No matterwhat smoothing technique is used, one must remember thatsmoothing is the art of estimating the probability of that which isunknown (i.e., not seen in training).Non-first-word Bi~ramsPr((w,S> I (w,S/_,, NC)Pr((w,S) I ::C)Pr(w l NC).
Pr(f l  NC)1 1IV\[ number of word features1/(1 + ?)
= ~ weight for the bigram probability, al 1 - ~ = 3 weight for the back-off model).3.4 Comparison with a traditional HMMUnlike a traditional HMM, the probability ofgenerating a particular word is 1 for each word-stateinside each of the name-class states.
An alternative---and more traditional--model would have a smallnumber of states within each name-class, eachhaving, perhaps, some semantic signficance, e.g.,three states in the PERSON name-class, representing afirst, middle and last name, where each of these threestates would have some probability associated withemitting any word from the vocabulary.
We chose touse a bigram language model because, while lesssemantically appealing, such n-gram language modelswork remarkably well in practice.
Also, as a firstresearch attempt, an n-gram model captures the mostgeneral significance of the words in each name-class,without presupposing any specifics of the structure ofnames, ~i la the PERSON name-class example, above.More important, either approach is mathematicallyvalid, as long as all transitions out of a given statesum to one.3.5 DecodingAll of this modeling would be for naught were itnot for the existence of an efficient algorithm forfinding the optimal state sequence, thereby "decoding"the original sequence of name-classes.
The number ofpossible state sequences for N states in an ergodicmodel for a sentence of m words is N m, but, usingdynamic programming and an appropriate merging ofmultiple theories when they converge on a particularstate--the Viterbi decoding algorithm--a sentence canbe "decoded" in time linear to the number of tokens inthe sentence, O(m) (Viterbi, 1967).
Since we are198interested in recovering the name-class state sequence,we pursue eight theories at every given step of thealgorithm.4.
Implementation and Results4.1 Development HistoryInitially, the word-feature was not in the model;instead the system relied on a third-level back-off part-of-speech tag, which in turn was computed by ourstochastic part-of-speech tagger.
The tags were takenat face value: there were not k-best tags; the systemtreated the part-of-speech tagger as a "black box".Although the part-of-speech tagger used capitalizationto help it determine proper-noun tags, this feature wasonly implicit in the model, and then only after twolevels of back-off!
Also, the capitalization of a wordwas submerged in the muddiness of part-of-speechtags, which can "smear" the capitalization probabilitymass over several tags.
Because it seemed thatcapitalization would be a good name-predictingfeature, and that it should appear earlier in the model,we eliminated the reliance on part-of-speechaltogether, and opted for the more direct, word-featuremodel described above, in ?3.
Originally, we had avery small number of features, indicating whether theword was a number, the first word of a sentence, alluppercase, inital-capitalized or lower-case.
We thenexpanded the feature set to its current state in order tocapture more subtleties related mostly to numbers;due to increased performance (although not entirelydramatic) on every test, we kept the enlarged featureset.Contrary to our expectations (which were based onour experience with English), Spanish containedmany examples of lower-case words in organizationand location names.
For example, departamento("Department") could often start an organizationname, and adjectival place-names, uch as coreana("Korean") could appear in locations and byconvention are not capitalized.4.2 Current ImplementationThe entire system is implemented in C++, atop a"home-brewed", general-purpose class library,providing a rapid code-compile-train-test cycle.
Infact, many NLP systems suffer from a lack ofsoftware and computer-science engineering effort: run-time efficiency is key to performing numerousexperiments, which, in turn, is key to improvingperformance.
A system may have excellentperformance on a given task, but if it takes long tocompile and/or run on test data, the rate ofimprovement of that system will be minisculecompared to that which can run very efficiently.
On aSpare20 or SGI Indy with an appropritae amount ofRAM, Nymble can compile in 10 minutes, train in 5minutes and run at 6MB/hr.
There were days inwhich we had as much as a 15% reduction in errorrate, to borrow the performance measure used by thespeech community, where error rate = 100% - F-measure.
(See ?4.3 for the definition of F-measure.
)4.3 Results of evaluationIn this section we report the results of evaluatingthe final version of the learning software.
We reportthe results for English and for Spanish and then theresults of a set of experiments to determine theimpact of the training set size on the algorithm'sperformance in both English and Spanish.For each language, we have a held-outdevelopment test set and a held-out, blind test set.We only report results on the blind test set for eachrespective language.4.3.1 F-measureThe scoring program measures both precision andrecall, terms borrowed from the information-retrievalcommunity, whereP = number of correct responses andnumber responsesR = number ofcorrect responses (4.1)number correct in keyPut informally, recall measures the number of "hits"vs. the number of possible correct answers asspecified in the key file, whereas precision measureshow many answers were correct ones compared to thenumber of answers delivered.
These two measures ofperformance combine to form one measure ofperformance, the F-measure, which is computed bythe weighted harmonic mean of precision and recall:F = (f12 + 1)RP (4.2)( 2R)+Pwhere f f  represents the relative weight of recall toprecision (and typically has the value 1).
To ourknowledge, our learned name-finding system hasachieved a higher F-measure than any other learnedsystem when compared to state-of-the-art manual(rule-based) systems on similar data.4.3.2 English and Spanish ResultsOur test set of English data for reporting results isthat of the MUC-6 test set, a collection of 30 WSJdocuments (we used a different test set duringdevelopment).
Our Spanish test set is that used forMET, comprised of articles from the news agencyAFP.
Table 4.1 illustrates Nymble's performance ascompared to the best reported scores for each category.199Case LanBual~eMixed EnglishUpper EnglishMixed SpanishBest ReportedScore \[ Nymble96 9389 9 193 9 0Table 4.1 F-measure Scores4.3.3 The Amount of Training DataRequiredWith any learning technique one of the importantquestions i  how much training data is required to getacceptable performance.
More generally how doesperformance vary as the training set size is increasedor decreased?
We ran a sequence of experiments inEnglish and in Spanish to try to answer this questionfor the final model that was implemented.For English, there were 450,000 words of trainingdata.
By that we mean that the text of the documentitself (including headlines but not including SGMLtags) was 450,000 words long.
Given this maximumsize of training available to us, we successfullydivided the training material in half until we wereusing only one eighth of the original training set sizeor a training set of 50,000 words for the smallestexperiment.
To give a sense of the size of 450,000words, that is roughly half the length of one editionof the Wall Street Journal.The results are shown in a histogram in Figure4.1 below.
The positive outcome of the experimentis that half as much training data would have givenalmost equivalent performance.
Had we used onlyone quarter of the data or approximately 100,000words, performance would have degraded slightly,only about 1-2 percent.
Reducing the training setsize to 50,000 words would have had a moresignificant decrease in the performance of the system;however, the performance is still impressive venwith such a small training set.100.90.80.70.60,50.40,30,20,100 ~.7~ i ~i o= o~c ?
- o~ "2 "EFigure 4.1: Impact of Various TrainingSet Sizes on Performance in English.
Thelearning algorithm performs remarkable well, nearlycomparable to handcrafted systems with as little as100,000 words of training data.On the other hand, the result also shows thatmerely annotating more data will not yield dramaticimprovement in the performance.
With increasedtraining data it would be possible to use even moredetailed models that require more data and couldachieve significantly improved overall systemperformance with those more detailed models.For Spanish we had only 223,000 words oftraining data.
We also measured the performance ofthe system with half the training data or slightlymore than 100,000 words of text.
Figure 4.2 showsthe results.
There is almost no change inperformance by using as little as 100,000 words oftraining data.Therefore the results in both languages werecomparable.
As little as 100,000 words of trainingdata produces performance nearly comparable tohandcrafted systems.2001009080706050403020100 K~tT~ .~ .~_ o~ .=- C.) t.- O.
~ _ ~ ~~Figure 4.2: Impact of Training Set Size onPerformance in Spanish5.
Further WorkWhile our initial results have been quite favorable,there is still much that can be done potentially toimprove performance and completely close the gapbetween learned and rule-based name-finding systems.We would like to incorporate the following into thecurrent model:?
lists of organizations, person names andlocations?
an aliasing algorithm, which dynamically updatesthe model (where e.g.
IBM is an alias ofInternational Business Machines)?
longer-distance information, to find names notcaptured by our bigram model6.
Conc lus ionsWe have shown that using a fairly simpleprobabilistic model, finding names and othernumerical entities as specified by the MUC tasks canbe performed with "near-human performance", oftenlikened to an F of 90 or above.
We have also shownthat such a system can be gained efficiently and that,given appropriately and consistently marked answerkeys, it can be trained on languages foreign to thetrainer of the system; for example, we do not speakSpanish, but trained Nymble on answer keys markedby native speakers.
None of the formalisms ortechniques presented in this paper is new; rather, theapproach to this task the model itself--is whereinlies the novelty.
Given the incredibly difficult natureof many NLP tasks, this example of a learned,stochastic approach to name-finding lends credence tothe argument that the NLP community ought o pushthese approaches, tofind the limit of phenomena thatmay be captured by probabilistic, finite-statemethods.7.
Re ferencesAberdeen, J., Burger, J., Day, D., Hirschman, L.,Robinson, P. and Vilain, M. (1995) InProceedings of the Sixth MessageUnderstanding Conference (MUC-6)MorganKaufmann Publishers, Inc., Columbia,Maryland, pp.
141-155.Appelt, D. E., Jerry R. Hobbs, Bear, J., Israel, D.,Kameyama, M., Kehler, A., Martin, D.,Myers, K. and Tyson, M. (1995) InProceedings of the Sixth MessageUnderstanding Conference (MUC-6)MorganKaufmann Publishers, Inc., Columbia,Maryland, pp.
237-248.Church, K. (1988) In Second Conference on AppliedNatural Language Processing, Austin, Texas.Cover, T. and Thomas, J.
A.
(1991) Elements ofInformation Theory, John Wiley & Sons, Inc.,New York.Miller, S., Bobrow, R., Schwartz, R. and Ingria, R.(1994) In Human Language TechnologyWorkshop, Morgan Kaufmann Publishers,Plainsboro, New Jersey, pp.
278-282.Viterbi, A. J.
(1967) IEEE Transactions onInformation Theory, IT-13(2), 260-269.Weischedel, R. (1995) In Proceedings of the SixthMessage Understanding Conference (MUC-6)Morgan Kaufmann Publishers, Inc.,Columbia, Maryland, pp.
55-69.Weischedel, R., Meteer, M., Schwartz, R.,Ramshaw, L. and Palmucci, J.
(1993)Computational Linguistics, 19(2), 359-382.8.
AcknowledqementsThe work reported here was supported in part bythe Defense Advanced Research Projects Agency; atechnical agent for part of the work was FortHuachucha under contract number DABT63-94-C-0062.
The views and conclusions contained in thisdocument are those of the authors and should not beinterpreted as necessarily representing the officialpolicies, either expressed or implied, of the DefenseAdvanced Research Projects Agency or the UnitedStates Government.We would also like to give special acknowledge-ment to Stuart Shieber, McKay Professor ofComputer Science at Harvard University, whoendorsed and helped foster the completion of this, thefirst phase of Nymble's development.201
