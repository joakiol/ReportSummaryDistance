Proceedings of the ACL 2010 Conference Short Papers, pages 147?150,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsFixed Length Word Suffix for FactoredStatistical Machine TranslationNarges Sharif RazavianSchool of Computer ScienceCarnegie Mellon UniversiyPittsburgh, USAnsharifr@cs.cmu.eduStephan VogelSchool of Computer ScienceCarnegie Mellon UniversiyPittsburgh, USAstephan.vogel@cs.cmu.eduAbstractFactored Statistical Machine Translation ex-tends the Phrase Based SMT model by al-lowing each word to be a vector of factors.Experiments have shown effectiveness ofmany factors, including the Part of Speechtags in improving the grammaticality of theoutput.
However, high quality part ofspeech taggers are not available in opendomain for many languages.
In this paperwe used fixed length word suffix as a newfactor in the Factored SMT, and were ableto achieve significant improvements in threeset of experiments: large NIST Arabic toEnglish system, medium WMT Spanish toEnglish system, and small TRANSTACEnglish to Iraqi system.1 IntroductionStatistical Machine Translation(SMT) is current-ly the state of the art solution to the machinetranslation.
Phrase based SMT is also among thetop performing approaches available as of today.This approach is a purely lexical approach, usingsurface forms of the words in the parallel corpusto generate the translations and estimate proba-bilities.
It is possible to incorporate syntacticalinformation into this framework through differ-ent ways.
Source side syntax based re-orderingas preprocessing step, dependency based reorder-ing models, cohesive decoding features areamong many available successful attempts forthe integration of syntax into the translationmodel.
Factored translation modeling is anotherway to achieve this goal.
These models alloweach word to be represented as a vector of factorsrather than a single surface form.
Factors canrepresent richer expression power on each word.Any factors such as word stems, gender, part ofspeech, tense, etc.
can be easily used in thisframework.Previous work in factored translation modelinghave reported consistent improvements from Partof Speech(POS) tags, morphology, gender, andcase factors (Koehn et.
a.
2007).
In another work,Birch et.
al.
2007 have achieved improvementusing Combinational Categorial Grammar (CCG)super-tag factors.
Creating the factors is done asa preprocessing step, and so far, most of the ex-periments have assumed existence of externaltools for the creation of these factors (i. e. Part ofspeech taggers, CCG parsers, etc.).
Unfortunatelyhigh quality language processing tools, especial-ly for the open domain, are not available for mostlanguages.While linguistically identifiable representations(i.e.
POS tags, CCG supertags, etc) have beenvery frequently used as factors in many applica-tions including MT, simpler representations havealso been effective in achieving the same resultin other application areas.
Grzymala-Busse andOld 1997, DINCER et.al.
2008, were able to usefixed length suffixes as features for training aPOS tagging.
In another work Saberi and Perrot1999 showed that reversing middle chunks of thewords while keeping the first and last part intact,does not decrease listeners?
recognition ability.This result is very relevant to Machine Transla-tion, suggesting that inaccurate context which isusually modeled with n-gram language models,can still be as effective as accurate surface forms.Another research (Rawlinson 1997) confirms thisfinding; this time in textual domain, observingthat randomization of letters in the middle ofwords has little or no effect on the ability ofskilled readers to understand the text.
These re-sults suggest that the inexpensive representation-al factors which do not need unavailable toolsmight also be worth investigating.These results encouraged us to introduce lan-guage independent simple factors for machinetranslation.
In this paper, following the work ofGrzymala-Busse et.
al.
we used fixed length suf-147fix as word factor, to lower the perplexity of thelanguage model, and have the factors roughlyfunction as part of speech tags, thus increasingthe grammaticality of the translation results.
Wewere able to obtain consistent, significant im-provements over our baseline in 3 different expe-riments, large NIST Arabic to English system,medium WMT Spanish to English system, andsmall TRANSTAC English to Iraqi system.The rest of this paper is as follows.
Section 2briefly reviews the Factored Translation Models.In section 3 we will introduce our model, andsection 4 will contain the experiments and theanalysis of the results, and finally, we will con-clude this paper in section 5.2 Factored Translation ModelStatistical Machine Translation uses the log li-near combination of a number of features, tocompute the highest probable hypothesis as thetranslation.e = argmaxe p(e|f) = argmaxe p exp ?i=1n ?i hi(e,f)In phrase based SMT, assuming the source andtarget phrase segmentation as {(fi,ei)}, the mostimportant features include: the Language Modelfeature hlm(e,f) = plm(e); the phrase translationfeature ht(e,f) defined as product of translationprobabilities, lexical probabilities and phrase pe-nalty; and the reordering probability, hd(e,f),usually defined as ?i=1n d(starti,endi-1) over thesource phrase reordering events.Factored Translation Model, recently intro-duced by (Koehn et.
al.
2007), allow words tohave a vector representation.
The model can thenextend the definition of each of the features froma uni-dimensional value to an arbitrary joint andconditional combination of features.
Phrasebased SMT is in fact a special case of FactoredSMT.The factored features are defined as an exten-sion of phrase translation features.
The function?
(fj,ej), which was defined for a phrase pair be-fore, can now be extended as a log linear combi-nation ?f ?f(fjf,ejf).
The model also allows for ageneration feature, defining the relationship be-tween final surface form and target factors.
Otherfeatures include additional language model fea-tures over individual factors, and factored reor-dering features.Figure 1 shows an example of a possible fac-tored model.Figure 1: An example of a Factored Translation andGeneration ModelIn this particular model, words on both sourceand target side are represented as a vector of fourfactors: surface form, lemma, part of speech(POS) and the morphology.
The target phrase isgenerated as follows: Source word lemma gene-rates target word lemma.
Source word's Part ofspeech and morphology together generate thetarget word's part of speech and morphology, andfrom its lemma, part of speech and morphologythe surface form of the target word is finally gen-erated.
This model has been able to result inhigher translation BLEU score as well as gram-matical coherency for English to German, Eng-lish to Spanish, English to Czech, English toChinese, Chinese to English and German to Eng-lish.3 Fixed Length Suffix Factors for Fac-tored Translation ModelingPart of speech tagging, constituent and depen-dency parsing, combinatory categorical grammarsuper tagging are used extensively in most appli-cations when syntactic representations areneeded.
However training these tools requiremedium size treebanks and tagged data, whichfor most languages will not be available for awhile.
On the other hand, many simple wordsfeatures, such as their character n-grams, have infact proven to be comparably as effective inmany applications.
(Keikha et.
al.
2008) did an experiment on textclassification on noisy data, and compared sever-al word representations.
They compared surfaceform, stemmed words, character n-grams, andsemantic relationships, and found that for noisyand open domain text, character-ngrams outper-form other representations when used for textclassification.
In another work (Dincer et al2009) showed that using fixed length word end-ing outperforms whole word representation fortraining a part of speech tagger for Turkish lan-guage.148Based on this result, we proposed a suffix fac-tored model for translation, which is shown inFigure 2.Figure 2: Suffix Factored model: Source word de-termines factor vectors (target word, target word suf-fix) and each factor will be associated with itslanguage model.Based on this model, the final probability ofthe translation hypothesis will be the log linearcombination of phrase probabilities, reorderingmodel probabilities, and each of the languagemodels?
probabilities.P(e|f) ~  plm-word(eword)* plm-suffix(esuffix)* ?i=1n  p(eword-j & esuffix-j|fj)* ?i=1n p(fj | eword-j & esuffix-j)Where plm-word is the n-gram language modelprobability over the word surface sequence, withthe language model built from the surface forms.Similarly, plm-suffix(esuffix) is the language modelprobability over suffix sequences.
p(eword-j &esuffix-j|fj) and p(fj | eword-j & esuffix-j) are translationprobabilities for each phrase pair i , used in bythe decoder.
This probability is estimated afterthe phrase extraction step which is based ongrow-diag heuristic at this stage.4 Experiments and ResultsWe used Moses implementation of the factoredmodel for training the feature weights, and SRItoolkit for building n-gram language models.
Thebaseline for all systems included the moses sys-tem with lexicalized re-ordering, SRI 5-gramlanguage models.4.1 Small System from Dialog Domain:English to IraqiThis system was TRANSTAC system, whichwas built on about 650K sentence pairs with theaverage sentence length of 5.9 words.
Afterchoosing length 3 for suffixes, we built a newparallel corpus, and SRI 5-gram language modelsfor each factor.
Vocabulary size for the surfaceform was 110K whereas the word suffixes hadabout 8K distinct words.
Table 1 shows the result(BLEU Score) of the system compared to thebaseline.System Tune onSet-July07Test onSet-June08Test onSet-Nov08Baseline 27.74 21.73 15.62Factored 28.83 22.84 16.41Improvement 1.09 1.11 0.79Table 1: BLEU score, English to Iraqi Transtac sys-tem, comparing Factored and Baseline systems.As you can see, this improvement is consistentover multiple unseen datasets.
Arabic cases andnumbers show up as the word suffix.
Also, verbnumbers usually appear partly as word suffix andin some cases as word prefix.
Defining a lan-guage model over the word endings increases theprobability of sequences which have this caseand number agreement, favoring correct agree-ments over the incorrect ones.4.2 Medium System on Travel Domain:Spanish to EnglishThis system is the WMT08 system, on a corpusof 1.2 million sentence pairs with average sen-tence length 27.9 words.
Like the previous expe-riment, we defined the 3 character suffix of thewords as the second factor, and built the lan-guage model and reordering model on the jointevent of (surface, suffix) pairs.
We built 5-gramlanguage models for each factor.
The system hadabout 97K distinct vocabulary in the surface lan-guage model, which was reduced to 8K using thesuffix corpus.
Having defined the baseline, thesystem results are as follows.System Tune-WMT06Test set-WMT08Baseline 33.34 32.53Factored 33.60 32.84Improvement 0.26 0.32Table 2: BLEU score, Spanish to English WMT sys-tem, comparing Factored and Baseline systems.Here, we see improvement with the suffix fac-tors compared to the baseline system.
Word end-ings in English language are major indicators ofword?s part of speech in the sentence.
In factWord Language ModelSuffix Language ModelLMWordWord ?Suffix ?Source Target149most common stemming algorithm, Porter?sStemmer, works by removing word?s suffix.Having a language model on these suffixes push-es the common patterns of these suffixes to thetop, making the more grammatically coherentsentences to achieve a better probability.4.3 Large NIST 2009 System: Arabic toEnglishWe used NIST2009 system as our baseline inthis experiment.
The corpus had about 3.8 Mil-lion sentence pairs, with average sentence lengthof 33.4 words.
The baseline defined the lexica-lized reordering model.
As before we defined 3character long word endings, and built 5-gramSRI language models for each factor.
The resultof this experiment is shown in table 3.System TuneonMT06Test onDev07NewsWireTest onDev07WeblogTestonMT08Baseline 43.06 48.87 37.84 41.70Factored 44.20 50.39 39.93 42.74Improvement1.14 1.52 2.09 1.04Table 3: BLEU score, Arabic to English NIST 2009system, comparing Factored and Baseline systems.This result confirms the positive effect of thesuffix factors even on large systems.
As men-tioned before we believe that this result is due tothe ability of the suffix to reduce the word into avery simple but rough grammatical representa-tion.
Defining language models for this factorforces the decoder to prefer sentences with moreprobable suffix sequences, which is believed toincrease the grammaticality of the result.
Futureerror analysis will show us more insight of theexact effect of this factor on the outcome.5 ConclusionIn this paper we introduced a simple yet veryeffective factor: fixed length word suffix, to usein Factored Translation Models.
This simple fac-tor has been shown to be effective as a roughreplacement for part of speech.
We tested ourfactors in three experiments in a small, English toIraqi system, a medium sized system of Spanishto English, and a large system, NIST09 Arabic toEnglish.
We observed consistent and significantimprovements over the baseline.
This result, ob-tained from the language independent and inex-pensive factor, shows promising newopportunities for all language pairs.ReferencesBirch, A., Osborne, M., and Koehn, P. CCG supertagsin factored statistical machine translation.
Proceed-ings of the Second Workshop on Statistical Ma-chine Translation, pages 9?16, Prague, CzechRepublic.
Association for Computational Linguis-tics, 2007.Dincer T., Karaoglan B. and Kisla T., A Suffix BasedPart-Of-Speech Tagger For Turkish, Fifth Interna-tional Conference on Information Technology:New Generations, 2008.Grzymala-Busse J.W., Old L.J.
A machine learningexperiment to determine part of speech from word-endings, Lecture Notes in Computer Science,Communications Session 6B Learning and Discov-ery Systems, 1997.Keikha M., Sharif Razavian N, Oroumchian F., andSeyed Razi H., Document Representation andQuality of Text: An Analysis, Chapter 12, Surveyof Text Mining II, Springer London, 2008.Koehn Ph., Hoang H., Factored Translation Models,Proceedings of 45th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), 2007.Rawlinson G. E., The significance of letter position inword recognition, PhD Thesis, Psychology De-partment, University of Nottingham, NottinghamUK, 1976.Saberi K and Perrot D R, Cognitive restoration ofreversed speech, Nature (London) 1999.150
