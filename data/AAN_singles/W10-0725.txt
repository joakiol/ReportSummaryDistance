Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 163?167,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCheap Facts and Counter-FactsRui WangComputational Linguistics DepartmentSaarland UniversityRoom 2.04, Building C 7.4Saarbruecken, 66123 Germanyrwang@coli.uni-sb.deChris Callison-BurchComputer Science DepartmentJohns Hopkins University3400 N. Charles Street (CSEB 226-B)Baltimore, MD 21218, USAccb@cs.jhu.com.eduAbstractThis paper describes our experiments of us-ing Amazon?s Mechanical Turk to generate(counter-)facts from texts for certain named-entities.
We give the human annotators a para-graph of text and a highlighted named-entity.They will write down several (counter-)factsabout this named-entity in that context.
Theanalysis of the results is performed by com-paring the acquired data with the recognizingtextual entailment (RTE) challenge dataset.1 MotivationThe task of RTE (Dagan et al, 2005) is to saywhether a person would reasonably infer some shortpassage of text, the Hypothesis (H), given a longerpassage, the Text (T).
However, collections of suchT-H pairs are rare to find and these resources are thekey to solving the problem.The datasets used in the RTE task were collectedby extracting paragraphs of news text and manu-ally constructing hypotheses.
For the data collectedfrom information extraction task, the H is usuallya statement about a relation between two named-entities (NEs), which is written by expertise.
Simi-larly, the H in question answering data is constructedusing both the question and the (in)correct answers.Therefore, the research questions we could ask are,1.
Are these hypotheses really those ones peopleinterested in?2.
Are hypotheses different if we construct themin other ways?3.
What would be a good negative hypothesescompared with the positive ones?In this paper, we address these issues by usingAmazon?s Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008).
Instead ofconstructing the hypotheses targeted to IE or QA, wejust ask the human annotators to come up with somefacts they consider as relevant to the given text.
Fornegative hypotheses, we change the instruction andask them to write counter-factual but still relevantstatements.
In order to narrow down the content ofthe generated hypotheses, we give a focused named-entity (NE) for each text to guide the annotators.2 Related WorkThe early related research was done by Cooper et al(1996), where they manually construct a textbook-style corpus aiming at different semantic phenom-ena involved in inference.
However, the dataset isnot large enough to train a robust machine-learning-based RTE system.
The recent research from theRTE community focused on acquiring large quan-tities of textual entailment pairs from news head-lines (Burger and Ferro, 2005) and negative exam-ples from sequential sentences with transitional dis-course connectives (Hickl et al, 2006).
Althoughthe quality of the data collected were quite good,most of the positive examples are similar to summa-rization and the negative examples are more like acomparison/contrast between two sentences insteadof a contradiction.
Those data are the real sen-tences used in news articles, but the way of obtain-ing them is not necessarily the (only) best way to163find entailment pairs.
In this paper, we investigatean alternative inexpensive way of collecting entail-ment/contradiction text pairs by crowdsourcing.In addition to the information given by the text,common knowledge is also allowed to be involvedin the inference procedure.
The Boeing-Princeton-ISI (BPI) textual entailment test suite1 is specificallydesigned to look at entailment problems requiringworld knowledge.
We will also allow this in the de-sign of our task.3 Design of the TaskThe basic idea of the task is to give the human an-notators a paragraph of text with one highlightednamed-entity and ask them to write some (counter-)facts about it.
In particular, we first preprocessan existing RTE corpus using a named-entity rec-ognizer to mark all the named-entities appearing inboth T and H. When we show the texts to Turkers,we highlight one named-entity and give them one ofthese two sets of instructions:Facts: Please write several facts about the high-lighted words according to the paragraph.
Youmay add additional common knowledge (e.g.Paris is in France), but please mainly use theinformation contained in the text.
But pleasedo not copy and paste!Counter-Facts: Please write several statements thatare contradictory to the text.
Make your state-ments about the highlighted words.
Please usethe information mainly in the text.
Avoid usingwords like not or never.Then there are three blank lines given for the annota-tors to fill in facts or counter-factual statements.
Foreach HIT, we gather facts or counter-facts for fivetexts, and for each text, we ask three annotators toperform the task.
We give Turkers one example as aguide along with the instructions.4 Experiments and ResultsThe texts we use in our experiments are the develop-ment set of the RTE-5 challenge (Bentivogli et al,1http://www.cs.utexas.edu/?pclark/bpi-test-suite/Total Average (per Text)Extracted NEsFacts 244 1.19Counter-Facts 121 1.11Generated HypothesesFacts 790 3.85Counter-Facts 203 1.86Table 1: The statistics of the (valid) data we collect.
TheTotal column presents the number of extracted NEs andgenerated hypotheses and the Average column shows theaverage numbers per text respectively.2009), and we preprocess the data using the Stan-ford named-entity recognizer (Finkel et al, 2005).In all, it contains 600 T-H pairs, and we use the textsto generate facts and counter-facts and hypotheses asreferences.
We put our task online through Crowd-Flower2, and on average, we pay one cent for each(counter-)fact to the Turkers.
CrowdFlower can helpwith finding trustful Turkers and the data were col-lected within a few hours.To get a sense of the quality of the data we collect,we mainly focus on analyzing the following threeaspects: 1) the statistics of the datasets themselves;2) the comparison between the data we collect andthe original RTE dataset; and 3) the comparison be-tween the facts and the counter-facts.Table 1 show some basic statistics of the data wecollect.
After excluding invalid and trivial ones3, weacquire 790 facts and 203 counter-facts.
In general,the counter-facts seem to be more difficult to obtainthan the facts, since both the total number and theaverage number of the counter-facts are less thanthose of the facts.
Notice that the NEs are not manysince they have to appear in both T and H.The comparison between our data and the originalRTE data is shown in Table 2.
The average length ofthe generated hypotheses is longer than the originalhypotheses, for both the facts and the counter-facts.Counter-facts seem to be more verbose, since addi-tional (contradictory) information is added.
For in-stance, example ID 425 in Table 4, Counter Fact 1can be viewed as the more informative but contra-dictory version of Fact 1 (and the original hypoth-2http://crowdflower.com/3Invalid data include empty string or single words; and thetrivial ones are those sentences directly copied from the texts.164esis).
The average bag-of-words similarity scoresare calculated by dividing the number of overlap-ping words of T and H by the total number of wordsin H. In the original RTE dataset, the entailed hy-potheses have a higher BoW score than the contra-dictory ones; while in our data, facts have a lowerscore than the counter-facts.
This might be causedby the greater variety of the facts than the counter-facts.
Fact 1 of example ID 425 in Table 4 is almostthe same as the original hypothesis, and Fact 2 ofexample ID 374 as well, though the latter has someslight differences which make the answer differentfrom the original one.
The NE position in the sen-tence is another aspect to look at.
We find that peo-ple tend to put the NEs at the beginning of the sen-tences more than other positions, while in the RTEdatasets, NEs appear in the middle more frequently.In order to get a feeling of the quality of thedata, we randomly sampled 50 generated facts andcounter-facts and manually compared them with theoriginal hypotheses.
Table 3 shows that generatedfacts are easier for the systems to recognize, and thecounter-facts have the same difficulty on average.Although it is subjective to evaluate the difficultyof the data by human reading, in general, we followthe criteria that1.
Abstraction is more difficult than extraction;2.
Inference is more difficult than the direct en-tailment;3.
The more sentences in T are involved, the moredifficult that T-H pair is.Therefore, we view the Counter Fact 1 in exampleID 16 in Table 4 is more difficult than the originalhypothesis, since it requires more inference than thedirect fact validation.
However, in example ID 374,Fact 1 is easier to be verified than the original hy-pothesis, and same as those facts in example ID 506.Similar hypotheses (e.g.
Fact 1 in example ID 425and the original hypothesis) are treated as being atthe same level of difficulty.After the quantitive analysis, let?s take a closerlook at the examples in Table 4.
The facts are usuallyconstructed by rephrasing some parts of the text (e.g.in ID 425, ?after a brief inspection?
is paraphrasedby ?investigated by?
in Fact 2) or making a shortValid Harder Easier SameFacts 76% 16% 24% 36%Counter-Facts 84% 36% 36% 12%Table 3: The comparison of the generated (counter-)factswith the original hypotheses.
The Valid column shows thepercentage of the valid (counter-)facts; and other columnspresent the distribution of harder, easier cases than theoriginal hypotheses or with the same difficulty.RTE-5 Our DataCounter-/Facts 300/300 178/178All ?YES?
50% 50%BoW Baseline 57.5% 58.4%Table 5: The results of baseline RTE systems on the datawe collected, compared with the original RTE-5 dataset.The Counter-/Facts row shows the number of the T-Hpairs contained in the dataset; and the other scores in per-centage are accuracy of the systems.summary (e.g.
Fact 1 in ID 374, ?George Stranahanspoke of Thompson?s death.?).
For counter-facts, re-moving the negation words or changing into anotheradjective is one common choice, e.g.
in ID 374,Counter Fact 1 removed ?n?t?
and Counter Fact 3changed ?never?
into ?fully?.
The antonyms canalso make the contradiction, as ?rotten?
to ?great?in Counter Fact 2 in ID 374.Example ID 506 in Table 4 is another interest-ing case.
There are many facts about Yemen, butno valid counter-facts are generated.
Furthermore,if we compare the generated facts with the originalhypothesis, we find that people tend to give straight-forward facts instead of abstracts4.At last, we show some preliminary results on test-ing a baseline RTE system on this dataset.
Forthe sake of comparison, we extract a subset of thedataset, which is balanced on entailment and con-tradiction text pairs, and compare the results withthe same system on the original RTE-5 dataset.
Thebaseline system uses a simple BoW-based similar-ity measurement between T and H (Bentivogli et al,2009) and the results are shown in Table 5.The results indicate that our data are slightly ?eas-ier?
than the original RTE-5 dataset, which is consis-tent with our human evaluation on the sampled data4But this might also be caused by the design of our task.165Ave.
Length Ave. BoWNE PositionHead Middle TailOriginal Entailment Hypotheses 7.6 0.76 46% 53% 1%Facts 9.8 0.68 68% 29% 3%Original Contradiction Hypotheses 7.5 0.72 44% 56% 0%Counter-Facts 12.3 0.75 59% 38% 3%Table 2: The comparison between the generated (counter-)facts and the original hypotheses from the RTE dataset.
TheAve.
Length column represents the average number of words in each hypothesis; The Ave.
BoW shows the averagebag-of-words similarity compared with the text.
The three columns on the right are all about the position of the NEappearing in the sentence, how likely it is at the head, middle, or tail of the sentence.
(Table 3).
However, it is still too early to draw con-clusions based on the simple baseline results.5 Conclusion and Future WorkIn this paper, we report our experience of usingMTurk to collect facts and counter-facts about thegiven NEs and texts.
We find that the generated hy-potheses are not entirely the same as the originalhypotheses in the RTE data.
One direct extensionwould be to use more than one NE at one time, but itmay also cause problems, if those NEs do not haveany relations in-between.
Another line of researchwould be to test this generated resources using somereal existing RTE systems and compare the resultswith the original RTE datasets, and also further ex-plore the potential application of this resource.AcknowledgmentsThe first author is supported by the PIRE scholar-ship program.
The second author is supported bythe EuroMatrixPlusProject (funded by the EuropeanCommission), by the DARPA GALE program underContract No.
HR0011-06-2-0001, and by the NSFunder grant IIS-0713448.
The views and findingsare the authors?
alone.ReferencesL.
Bentivogli, B. Magnini, I. Dagan, H.T.
Dang, andD.
Giampiccolo.
2009.
The fifth pascal recogniz-ing textual entailment challenge.
In Proceedings ofthe Text Analysis Conference (TAC 2009) Workshop,Gaithersburg, Maryland, USA, November.
NationalInstitute of Standards and Technology.John Burger and Lisa Ferro.
2005.
Generating an entail-ment corpus from news headlines.
In Proceedings ofthe ACL Workshop on Empirical Modeling of SemanticEquivalence and Entailment, pages 49?54, Ann Arbor,Michigan, USA.
Association for Computational Lin-guistics.Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox,Johan Van Genabith, Jan Jaspars, Hans Kamp, DavidMilward, Manfred Pinkal, Massimo Poesio, and StevePulman.
1996.
Using the framework.
Technical Re-port LRE 62-051 D-16, The FraCaS Consortium.I.
Dagan, O. Glickman, and B. Magnini.
2005.
Thepascal recognising textual entailment challenge.
InProceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43nd Annual Meeting of the As-sociation for Computational Linguistics (ACL 2005),pages 363?370.
Association for Computational Lin-guistics.Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recogniz-ing textual entailment with lcc?s groundhog system.
InProceedings of the Second PASCAL Challenges Work-shop.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast - but is it good?Evaluating non- expert annotations for natural lan-guage tasks.
In Proceedings of EMNLP.166ID: 16 Answer: ContradictionOriginal Text The father of an Oxnard teenager accused of gunning down a gay classmate who was romanti-cally attracted to him has been found dead, Ventura County authorities said today.
Bill McIner-ney, 45, was found shortly before 8 a.m. in the living room of his Silver Strand home by a friend,said James Baroni, Ventura County?s chief deputy medical examiner.
The friend was supposedto drive him to a court hearing in his son?s murder trial, Baroni said.
McInerney?s 15-year-oldson, Brandon, is accused of murder and a hate crime in the Feb. 12, 2008, shooting death ofclassmate Lawrence ?Larry?
King, 15.
The two boys had been sparring in the days before thekilling, allegedly because Larry had expressed a romantic interest in Brandon.Original Hypothesis Bill McInerney is accused of killing a gay teenager.NE 1: Bill McInerneyCounter Fact 1 Bill McInerney is still alive.ID: 374 Answer: ContradictionOriginal Text Other friends were not surprised at his death.
?I wasn?t surprised,?
said George Stranahan, aformer owner of the Woody Creek Tavern, a favourite haunt of Thompson.
?I never expectedHunter to die in a hospital bed with tubes coming out of him.?
Neighbours have said how hisbroken leg had prevented him from leaving his house as often as he had liked to.
One neighbourand long-standing friend, Mike Cleverly, said Thompson was clearly hobbled by the broken leg.
?Medically speaking, he?s had a rotten year.
?Original Hypothesis The Woody Creek Tavern is owned by George Stranahan.NE 1: George StranahanFact 1 George Stranahan spoke of Thompson?s death.Fact 2 George Stranahan once owned the Woody Creek Tavern.Counter Fact 1 George Stranahan was surprised by his friend?s death.Counter Fact 2 Medically, George Stranahan?s friend, Humter Thompson, had a great year.Counter Fact 3 George Stranahan fully expected Thompson to die in a hospital with tubes coming out of him.NE 2: Woody Creek TavernFact 1 Woody Creek Tavern was previously owned by George Stranahan.ID: 425 Answer: EntailmentOriginal Text Merseyside Police concluded after a brief inspection that the controversial blog Liverpool EvilCabal does not break criminal law.
However the council officers continue to search for theeditor.
The blog has been blocked on computers controlled by Liverpool Direct Ltd, a companyjointly owned by Liverpool City Council and British Telecom.
The council?s elected officialshave denied ordering the block and are currently investigating its origin.Original Hypothesis Liverpool Evil Cabal is the name of an online blog.NE 1: Liverpool Evil CabalFact 1 Liverpool Evil Cabal is a web blog.Fact 2 Liverpool Evil Cabal was a blog investigated by the Merseyside Police.Counter Fact 1 Liverpool Evil Cabal is a blog of Liverpool Direct Ltd.Counter Fact 2 Liverpool Evil Cabal is freed from the charges of law breaking.ID: 506 Answer: EntailmentOriginal Text At least 58 people are now dead as a result of the recent flooding in Yemen, and at least 20,000in the country have no access to shelter.
Five people are also reported missing.
The Yemenigovernment has pledged to send tents to help the homeless.
The flooding is caused by the recentheavy rain in Yemen, which came as a shock due to the fact that the country only receives severalcentimeters of rain per year.Original Hypothesis Heavy rain caused flooding in Yemen.NE 1: YemenFact 1 58 people are dead in Yemen because of flooding.Fact 2 5 people in Yemen are missing.Fact 3 At least 58 people are dead in Yemen because of flooding.Table 4: Examples of facts and counter-facts, compared with the original texts and hypotheses.
We ask the Turkers towrite several (counter-)facts about the highlighted NEs, and only part of the results are shown here.167
