Revision Learning and its Application to Part-of-Speech TaggingTetsuji Nakagawa?
and Taku Kudo and Yuji Matsumototetsu-na@plum.freemail.ne.jp,{taku-ku,matsu}@is.aist-nara.ac.jpGraduate School of Information ScienceNara Institute of Science and Technology8916?5 Takayama, Ikoma, Nara 630?0101, JapanAbstractThis paper presents a revision learn-ing method that achieves high per-formance with small computationalcost by combining a model with highgeneralization capacity and a modelwith small computational cost.
Thismethod uses a high capacity model torevise the output of a small cost model.We apply this method to English part-of-speech tagging and Japanese mor-phological analysis, and show that themethod performs well.1 IntroductionRecently, corpus-based approaches have beenwidely studied in many natural language pro-cessing tasks, such as part-of-speech (POS) tag-ging, syntactic analysis, text categorization andword sense disambiguation.
In corpus-basednatural language processing, one important is-sue is to decide which learning model to use.Various learning models have been studied suchas Hidden Markov models (HMMs) (Rabinerand Juang, 1993), decision trees (Breiman etal., 1984) and maximum entropy models (Bergeret al, 1996).
Recently, Support Vector Ma-chines (SVMs) (Vapnik, 1998; Cortes and Vap-nik, 1995) are getting to be used, which aresupervised machine learning algorithm for bi-nary classification.
SVMs have good generaliza-tion performance and can handle a large num-ber of features, and are applied to some tasks?
Presently with Oki Electric Industrysuccessfully (Joachims, 1998; Kudoh and Mat-sumoto, 2000).
However, their computationalcost is large and is a weakness of SVMs.
Ingeneral, a trade-off between capacity and com-putational cost of learning models exists.
Forexample, SVMs have relatively high generaliza-tion capacity, but have high computational cost.On the other hand, HMMs have lower compu-tational cost, but have lower capacity and dif-ficulty in handling data with a large number offeatures.
Learning models with higher capac-ity may not be of practical use because of theirprohibitive computational cost.
This problembecomes more serious when a large amount ofdata is used.To solve this problem, we propose a revisionlearning method which combines a model withhigh generalization capacity and a model withsmall computational cost to achieve high per-formance with small computational cost.
Thismethod is based on the idea that processing theentire target task using a model with higher ca-pacity is wasteful and costly, that is, if a largeportion of the task can be processed easily usinga model with small computational cost, it shouldbe processed by such a model, and only difficultportion should be processed by the model withhigher capacity.Revision learning can handle a general multi-class classification problem, which includes POStagging, text categorization and many othertasks in natural language processing.
We ap-ply this method to English POS tagging andJapanese morphological analysis.This paper is organized as follows: Section2 describes the general multi-class classificationComputational Linguistics (ACL), Philadelphia, July 2002, pp.
497-504.Proceedings of the 40th Annual Meeting of the Association forproblem and the one-versus-rest method whichis known as one of the solutions for the prob-lem.
Section 3 introduces revision learning, anddiscusses how to combine learning models.
Sec-tion 4 describes one way to conduct Japanesemorphological analysis with revision learning.Section 5 shows experimental results of EnglishPOS tagging and Japanese morphological anal-ysis with revision learning.
Section 6 discussesrelated works, and Section 7 gives conclusion.2 Multi-Class ClassificationProblems and the One-versus-RestMethodLet us consider the problem to decide the classof an example x among multiple classes.
Such aproblem is called multi-class classification prob-lem.
Many tasks in natural language processingsuch as POS tagging are regarded as a multi-class classification problem.
When we only havebinary (positive or negative) classification algo-rithm at hand, we have to reformulate a multi-class classification problem into a binary classi-fication problem.
We assume a binary classifierf(x) that returns positive or negative real valuefor the class of x, where the absolute value |f(x)|reflects the confidence of the classification.The one-versus-rest method is known as oneof such methods (Allwein et al, 2000).
For onetraining example of a multi-class problem, thismethod creates a positive training example forthe true class and negative training examplesfor the other classes.
As a result, positive andnegative examples for each class are generated.Suppose we have five candidate classes A, B, C,D and E , and the true class of x is B. Fig-ure 1 (left) shows the created training examples.Note that there are only two labels (positive andnegative) in contrast with the original problem.Then a binary classifier for each class is trainedusing the examples, and five classifiers are cre-ated for this problem.
Given a test example x?,all the classifiers classify the example whetherit belongs to a specific class or not.
Its classis decided by the classifier that gives the largestvalue of f(x?).
The algorithm is shown in Figure2 in a pseudo-code.x A : B : C : D : E :Training DataOXXXXAEBCDA : B :Training DataOX123Rank AEBCD45xxxxxx xx-X-O-X-X-X-X-OOXLabel : Positive: NegativeClass ClassFigure 1: One-versus-Rest Method (left) andRevision Learning (right)# Training Procedure of One-versus-Rest# This procedure is given training examples# {(xi, yi)}, and creates classifiers.# C = {c0, .
.
.
, ck?1}: the set of classes,# xi: the ith training example,# yi ?
C: the class of xi,# k: the number of classes,# l: the number of training examples,# fc(?
): the binary classifier for the class c# (see the text).procedure TrainOVR({(x0, y0), .
.
.
, (xl?1, yl?1)})begin# Create the training data with binary labelfor i := 0 to l ?
1beginfor j := 0 to k ?
1beginif cj 6= yi thenAdd xi to the training data for the class cj as anegative example.elseAdd xi to the training data for the class cj as apositive example.endend# Train the binary classifiersfor j := 0 to k ?
1Train the classifier fcj (?)
using the training data.end# Test Function of One-versus-Rest# This function is given a test example and# returns the predicted class of it.# C = {c0, .
.
.
, ck?1}: the set of classes,# x: the test example,# k: the number of classes,# fc(?
): binary classifier trained with the# algorithm above.function TestOVR(x)beginfor j := 0 to k ?
1confidencej := fcj (x)return cargmaxj confidencejendFigure 2: Algorithm of One-versus-RestHowever, this method has the problem of be-ing computationally costly in training, becausethe negative examples are created for all theclasses other than the true class, and the to-tal number of the training examples becomeslarge (which is equal to the number of originaltraining examples multiplied by the number ofclasses).
The computational cost in testing isalso large, because all the classifiers have to workon each test example.3 Revision LearningAs discussed in the previous section, the one-versus-rest method has the problem of compu-tational cost.
This problem become more se-rious when costly binary classifiers are used orwhen a large amount of data is used.
To copewith this problem, let us consider the task ofPOS tagging.
Most portions of POS tagging isnot so difficult and a simple POS-based HMMslearning 1 achieves more than 95% accuracy sim-ply using the POS context (Brants, 2000).
Thismeans that the low capacity model is enoughto do most portions of the task, and we neednot use a high accuracy but costly algorithm inevery portion of the task.
This is the base mo-tivation of the revision model we are proposinghere.Revision learning uses a binary classifier withhigher capacity to revise the errors made bythe stochastic model with lower capacity as fol-lows: During the training phase, a ranking isassigned to each class by the stochastic modelfor a training example, that is, the candidateclasses are sorted in descending order of its con-ditional probability given the example.
Then,the classes are checked in their ranking order tocreate binary classifiers as follows.
If the classis incorrect (i.e.
it is not equal to the true classfor the example), the example is added to thetraining data for that class as a negative exam-ple, and the next ranked class is checked.
Ifthe class is correct, the example is added to thetraining data for that class as a positive exam-1HMMs can be applied to either of unsupervised orsupervised learning.
In this paper, we use the latter case,i.e., visible Markov Models, where POS-tagged data isused for training.ple, and the remaining ranked classes are nottaken into consideration (Figure 1, right).
Us-ing these training data, binary classifiers are cre-ated.
Note that each classifier is a pure binaryclassifier regardless with the number of classesin the original problem.
The binary classifier istrained just for answering whether the outputfrom the stochastic model is correct or not.During the test phase, first the ranking ofthe candidate classes for a given example is as-signed by the stochastic model as in the training.Then the binary classifier classifies the exampleaccording to the ranking.
If the classifier an-swers the example as incorrect, the next high-est ranked class becomes the next candidate forchecking.
But if the example is classified as cor-rect, the class of the classifier is returned as theanswer for the example.
The algorithm is shownin Figure 3.The amount of training data generated in therevision learning can be much smaller than thatin one-versus-rest.
Since, in revision learning,negative examples are created only when thestochastic model fails to assign the highest prob-ability to the correct POS tag, whereas negativeexamples are created for all but one class in theone-versus-rest method.
Moreover, testing timeof the revision learning is shorter, because onlyone classifier is called as far as it answers as cor-rect, but all the classifiers are called in the one-versus-rest method.4 Morphological Analysis withRevision LearningWe introduced revision learning for multi-classclassification in the previous section.
How-ever, Japanese morphological analysis cannot beregarded as a simple multi-class classificationproblem, because words in a sentence are notseparated by spaces in Japanese and the mor-phological analyzer has to segment the sentenceinto words as well as to decide the POS tag ofthe words.
So in this section, we describe howto apply revision learning to Japanese morpho-logical analysis.For a given sentence, a lattice consisting of allpossible morphemes can be built using a mor-# Training Procedure of Revision Learning# This procedure is given training examples# {(xi, yi)}, and creates classifiers.# C = {c0, .
.
.
, ck?1}: the set of classes,# xi: the ith training example,# yi ?
C: the class of xi,# k: the number of classes,# l: the number of training examples,# ni: the ordered indexes of C# (see the following code),# fc(?
): the binary classifier for the class c# (see the text).procedure TrainRL({(x0, y0), .
.
.
, (xl?1, yl?1)})begin# Create the training data with binary labelfor i := 0 to l ?
1beginCall the stochastic model to obtain theordered indexes {n0, .
.
.
, nk?1}such that P (cn0 |xi) ?
?
?
?
?
P (cnk?1 |xi).for j := 0 to k ?
1beginif cnj 6= yi thenAdd xi to the training data for the class cnj as anegative example.elsebeginAdd xi to the training data for the class cnj as apositive example.breakendendend# Train the binary classifiersfor j := 0 to k ?
1Train the classifier fcj (?)
using the training data.end# Test Function of Revision Learning# This function is given a test example and# returns the predicted class of it.# C = {c0, .
.
.
, ck?1}: the set of classes,# x: the test example,# k: the number of classes,# ni: the ordered indexes of C# (see the following code),# fc(?
): binary classifier trained with the# algorithm above.function TestRL(x)beginCall the stochastic model to obtain theordered indexes {n0, .
.
.
, nk?1}such that P (cn0 |x) ?
?
?
?
?
P (cnk?1 |x).for j := 0 to k ?
1if fcnj (x) > 0 thenreturn cnjreturn undecidableendFigure 3: Algorithm of Revision Learningpheme dictionary as in Figure 4.
Morphologicalanalysis is conducted by choosing the most likelypath on it.
We adopt HMMs as the stochasticmodel and SVMs as the binary classifier.
Forany sub-paths from the beginning of the sen-tence (BOS) in the lattice, its generative prob-ability can be calculated using HMMs (Nagata,1999).
We first pick up the end node of thesentence as the current state node, and repeatthe following revision learning process backwarduntil the beginning of the sentence.
Rankingsare calculated by HMMs to all the nodes con-nected to the current state node, and the bestof these nodes is identified based on the SVMsclassifiers.
The selected node then becomes thecurrent state node in the next round.
This canbe seen as SVMs deciding whether two adjoiningnodes in the lattice are connected or not.In Japanese morphological analysis, for anygiven morpheme ?, we use the following featuresfor the SVMs:1. the POS tags, the lexical forms and the in-flection forms of the two morphemes pre-ceding ?;2.
the POS tags and the lexical forms of thetwo morphemes following ?;3.
the lexical form and the inflection form of?.The preceding morphemes are unknown becausethe processing is conducted from the end of thesentence, but HMMs can predict the most likelypreceding morphemes, and we use them as thefeatures for the SVMs.English POS tagging is regarded as a specialcase of morphological analysis where the seg-mentation is done in advance, and can be con-ducted in the same way.
In English POS tag-ging, given a word w, we use the following fea-tures for the SVMs:1. the POS tags and the lexical forms of thetwo words preceding w, which are given byHMMs;2. the POS tags and the lexical forms of thetwo words following w;3. the lexical form of w and the prefixes andsuffixes of up to four characters, the exis-BOS EOSkinou (yesterday)[noun]ki (tree)[noun]nou (brain)[noun]ki (come)[verb]no[particle]u[auxiliary]gakkou (school)[noun]sentence:ni (to)[particle]ni (resemble)[verb]it (went)[verb]ta[auxiliary]kinougakkouitkikinounverbnounverbnoun...
...Dictionary:Lattice:"kinougakkouniitta (I went to school yesterday)"Figure 4: Example of Lattice for Japanese Morphological Analysistence of numerals, capital letters and hy-phens in w.5 ExperimentsThis section gives experimental results of En-glish POS tagging and Japanese morphologicalanalysis with revision learning.5.1 Experiments of EnglishPart-of-Speech TaggingExperiments of English POS tagging with revi-sion learning (RL) are performed on the PennTreebank WSJ corpus.
The corpus is randomlyseparated into training data of 41,342 sentencesand test data of 11,771 sentences.
The dictio-nary for HMMs is constructed from all the wordsin the training data.T3 of ICOPOST release 0.9.0 (Schro?der,2001) is used as the stochastic model for rankingstage.
This is equivalent to POS-based secondorder HMMs.
SVMs with second order polyno-mial kernel are used as the binary classifier.The results are compared with TnT (Brants,2000) based on second order HMMs, and withPOS tagger using SVMs with one-versus-rest (1-v-r) (Nakagawa et al, 2001).The accuracies of those systems for knownwords, unknown words and all the words areshown in Table 1.
The accuracies for bothknown words and unknown words are improvedthrough revision learning.
However, revisionlearning could not surpass the one-versus-rest.The main difference in the accuracies stems fromthose for unknown words.
The reason for thatseems to be that the dictionary of HMMs forPOS tagging is obtained from the training data,as a result, virtually no unknown words exist inthe training data, and the HMMs never makemistakes for unknown words during the train-ing.
So no example of unknown words is avail-able in the training data for the SVM reviser.This is problematic: Though the HMMs handlesunknown words with an exceptional method,SVMs cannot learn about errors made by theunknown word processing in the HMMs.
Tocope with this problem, we force the HMMsto make mistakes by eliminating low frequentwords from the dictionary.
We eliminated thewords appearing only once in the training dataso as to make SVMs to learn about unknownwords.
The results are shown in Table 1 (row?cutoff-1?).
Such procedure improves the accu-racies for unknown words.One advantage of revision learning is its smallcomputational cost.
We compare the computa-tion time with the HMMs and the one-versus-rest.
We also use SVMs with linear kernel func-tion that has lower capacity but lower computa-tional cost compared to the second order poly-nomial kernel SVMs.
The experiments are per-formed on an Alpha 21164A 500MHz processor.Table 2 shows the total number of training ex-amples, training time, testing time and accu-racy for each of the five systems.
The trainingtime and the testing time of revision learningare considerably smaller than those of the one-versus-rest.
Using linear kernel, the accuracydecreases a little, but the computational cost ismuch lower than the second order polynomialkernel.Accuracy (Known Words / Unknown Words) Number of ErrorsT3 Original 96.59% (96.90% / 82.74%) 9720with RL 96.93% (97.23% / 83.55%) 8734with RL (cutoff-1) 96.98% (97.25% / 85.11%) 8588TnT 96.62% (96.90% / 84.19%) 9626SVMs 1-v-r 97.11% (97.34% / 86.80%) 8245Table 1: Result of English POS TaggingTotal Number of Training Time Testing Time AccuracyExamples for SVMs (hour) (second)T3 Original ?
0.004 89 96.59%with RL (polynomial kernel, cutoff-1) 1027840 16 2089 96.98%with RL (linear kernel, cutoff-1) 1027840 2 129 96.94%TnT ?
0.002 4 96.62%SVMs 1-v-r 999984?50 625 55239 97.11%Table 2: Computational Cost of English POS Tagging5.2 Experiments of JapaneseMorphological AnalysisWe use the RWCP corpus and some additionalspoken language data for the experiments ofJapanese morphological analysis.
The corpus israndomly separated into training data of 33,831sentences and test data of 3,758 sentences.
Asthe dictionary for HMMs, we use IPADIC ver-sion 2.4.4 with 366,878 morphemes (Matsumotoand Asahara, 2001) which is originally con-structed for the Japanese morphological ana-lyzer ChaSen (Matsumoto et al, 2001).A POS bigram model and ChaSen version2.2.8 based on variable length HMMs are used asthe stochastic models for the ranking stage, andSVMs with the second order polynomial kernelare used as the binary classifier.We use the following values to evaluateJapanese morphological analysis:recall = ?# of correct morphemes in system?s output?
?# of morphemes in test data?
,precision = ?# of correct morphemes in system?s output?
?# of morphemes in system?s output?
,F-measure = 2?
recall?
precisionrecall + precision .The results of the original systems and thosewith revision learning are shown in Table 3,which provides the recalls, precisions and F-measures for two cases, namely segmentation(i.e.
segmentation of the sentences into mor-phemes) and tagging (i.e.
segmentation andPOS tagging).
The one-versus-rest method isnot used because it is not applicable to mor-phological analysis of non-segmented languagesdirectly.When revision learning is used, all the mea-sures are improved for both POS bigram andChaSen.
Improvement is particularly clear forthe tagging task.The numbers of correct morphemes for eachPOS category tag in the output of ChaSen withand without revision learning are shown in Ta-ble 4.
Many particles are correctly revised byrevision learning.
The reason is that the POStags for particles are often affected by the fol-lowing words in Japanese, and SVMs can revisesuch particles because it uses the lexical forms ofthe following words as the features.
This is theadvantage of our method compared to simpleHMMs, because HMMs have difficulty in han-dling a lot of features such as the lexical formsof words.6 Related WorksOur proposal is to revise the outputs of astochastic model using binary classifiers.
Brillstudied transformation-based error-driven learn-ing (TBL) (Brill, 1995), which conducts POStagging by applying the transformation rules tothe POS tags of a given sentence, and has aresemblance to revision learning in that the sec-ond model revises the output of the first model.Word Segmentation Tagging Training TestingTime TimeRecall Precision F-measure Recall Precision F-measure (hour) (second)POS Original 98.06% 98.77% 98.42% 95.61% 96.30% 95.96% 0.02 8bigram with RL 99.06% 99.27% 99.16% 98.13% 98.33% 98.23% 11 184ChaSen Original 99.06% 99.20% 99.13% 97.67% 97.81% 97.74% 0.05 15with RL 99.22% 99.34% 99.28% 98.26% 98.37% 98.32% 6 573Table 3: Result of Morphological AnalysisPart-of-Speech # in Test Data Original with RL DifferenceNoun 41512 40355 40556 +201Prefix 817 781 784 +3Verb 8205 8076 8115 +39Adjective 678 632 655 +23Adverb 779 735 750 +15Adnominal 378 373 373 0Conjunction 258 243 243 0Particle 20298 19686 19942 +256Auxiliary 4419 4333 4336 +3Interjection 94 90 91 +1Symbol 15665 15647 15651 +4Others 1 1 1 0Filler 43 36 36 0Table 4: The Number of Correctly Tagged Morphemes for Each POS Category TagHowever, our method differs from TBL in twoways.
First, our revision learner simply answerswhether a given pattern is correct or not, andany types of binary classifiers are applicable.Second, in our model, the second learner is ap-plied to the output of the first learner only once.In contrast, rewriting rules are applied repeat-edly in the TBL.Recently, combinations of multiple learnershave been studied to achieve high performance(Alpaydm, 1998).
Such methodologies to com-bine multiple learners can be distinguished intotwo approaches: one is the multi-expert methodand the other is the multi-stage method.
In theformer, each learner is trained and answers inde-pendently, and the final decision is made basedon those answers.
In the latter, the multiplelearners are ordered in series, and each learner istrained and answers only if the previous learnerrejects the examples.
Revision learning belongsto the latter approach.
In POS tagging, somestudies using the multi-expert method were con-ducted (van Halteren et al, 2001; Ma`rquez etal., 1999), and Brill and Wu (1998) combinedmaximum entropy models, TBL, unigram andtrigram, and achieved higher accuracy than anyof the four learners (97.2% for WSJ corpus).Regarding the multi-stage methods, cascading(Alpaydin and Kaynak, 1998) is well known,and Even-Zohar and Roth (2001) proposed thesequential learning model and applied it to POStagging.
Their methods differ from revisionlearning in that each learner behaves in the sameway and more than one learner is used in theirmethods, but in revision learning the stochasticmodel assigns rankings to candidates and the bi-nary classifier selects the output.
Furthermore,mistakes made by a former learner are fatal intheir methods, but is not so in revision learn-ing because the binary classifier works to revisethem.The advantage of the multi-expert method isthat each learner can help each other even ifit has some weakness, and generalization er-rors can be decreased.
On the other hand,the computational cost becomes large becauseeach learner is trained using every training dataand answers for every test data.
In contrast,multi-stage methods can decrease the computa-tional cost, and seem to be effective when a largeamount of data is used or when a learner withhigh computational cost such as SVMs is used.7 ConclusionIn this paper, we proposed the revision learningmethod which combines a stochastic model anda binary classifier to achieve higher performancewith lower computational cost.
We applied it toEnglish POS tagging and Japanese morpholog-ical analysis, and showed improvement of accu-racy with small computational cost.Compared to the conventional one-versus-restmethod, revision learning has much lower com-putational cost with almost comparable accu-racy.
Furthermore, it can be applied not only toa simple multi-class classification task but alsoto a wider variety of problems such as Japanesemorphological analysis.AcknowledgmentsWe would like to thank Ingo Schro?der for makingICOPOST publicly available.ReferencesErin L. Allwein, Robert E. Schapire, and YoramSinger.
2000.
Reducing Multiclass to Binary: AUnifying Approach for Margin Classifiers.
In Pro-ceedings of 17th International Conference on Ma-chine Learning, pages 9?16.Ethem Alpaydin and Cenk Kaynak.
1998.
Cascad-ing Classifiers.
Kybernetika, 34(4):369?374.Ethem Alpaydm.
1998.
Techniques for CombiningMultiple Learners.
In Proceedings of Engineeringof Intelligent Systems ?98 Conference.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A Maximum EntropyApproach to Natural Language Processing.
Com-putational Linguistics, 22(1):39?71.Thorsten Brants.
2000.
TnT ?
A StatisticalPart-of-Speech Tagger.
In Proceedings of ANLP-NAACL 2000, pages 224?231.Leo Breiman, Jerome H. Friedman, Richard A. Ol-shen, and Charles J.
Stone.
1984.
Classificationand Regression Trees.
Wadsworth and Brooks.Eric Brill and Jun Wu.
1998.
Classifier Combi-nation for Improved Lexical Disambiguation.
InProceedings of the Thirty-Sixth Annual Meeting ofthe Association for Computational Linguistics andSeventeenth International Conference on Compu-tational Linguistics, pages 191?195.Eric Brill.
1995.
Transformation-Based Error-Driven Learning and Natural Language Process-ing: A Case Study in Part-of-Speech Tagging.Computational Linguistics, 21(4):543?565.Corinna Cortes and Vladimir Vapnik.
1995.
SupportVector Networks.
Machine Learning, 20:273?297.Yair Even-Zohar and Dan Roth.
2001.
A SequentialModel for Multi-Class Classification.
In Proceed-ings of the 2001 Conference on Empirical Methodsin Natural Language Processing, pages 10?19.Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning with ManyRelevant Features.
In Proceedings of the 10th Eu-ropean Conference on Machine Learning, pages137?142.Taku Kudoh and Yuji Matsumoto.
2000.
Use of Sup-port Vector Learning for Chunk Identification.
InProceedings of the Fourth Conference on Compu-tational Natural Language Learning, pages 142?144.Llui?
?s Ma`rquez, Horacio Rodr?
?guez, Josep Carmona,and Josep Montolio.
1999.
Improving POS Tag-ging Using Machine-Learning Techniques.
In Pro-ceedings of 1999 Joint SIGDAT Conference onEmpirical Methods in Natural Language Process-ing and Very Large Corpora, pages 53?62.Yuji Matsumoto and Masayuki Asahara.
2001.IPADIC User?s Manual version 2.2.4.
Nara In-stitute of Science and Technology.
(in Japanese).Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Yoshitaka Hirano, Hiroshi Matsuda, KazumaTakaoka, and Masayuki Asahara.
2001.
Mor-phological Analysis System ChaSen version 2.2.8Manual.
Nara Institute of Science and Technol-ogy.Masaaki Nagata.
1999.
Japanese Language Process-ing Based on Stochastic Models.
Kyoto University,Doctoral Thesis.
(in Japanese).Tetsuji Nakagawa, Taku Kudoh, and Yuji Mat-sumoto.
2001.
Unknown Word Guessing andPart-of-Speech Tagging Using Support Vector Ma-chines.
In Proceedings of 6th Natural LanguageProcessing Pacific Rim Symposium, pages 325?331.Lawrence R. Rabiner and Biing-Hwang Juang.1993.
Fundamentals of Speech Recognition.
PTRPrentice-Hall.Ingo Schro?der.
2001.
ICOPOST ?
Ingo?s CollectionOf POS Taggers.http://nats-www.informatik.uni-hamburg.de/~ingo/icopost/.Hans van Halteren, Jakub Zavrel, and Walter Daele-mans.
2001.
Improving Accuracy in Word-class Tagging through Combination of MachineLearning Systems.
Computational Linguistics,27(2):199?230.Vladimir Vapnik.
1998.
Statistical Learning Theory.Springer.
