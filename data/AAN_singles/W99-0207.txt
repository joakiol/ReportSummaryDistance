Corpus-Based Anaphora Resolution Towards AntecedentPreferenceMichae l  PAUL ,  Kazuh ide  YAMAMOTO and E i i ch i ro  SUMITAATR In terpret ing  Te lecommunicat ions  Research Laborator ies2-2 Hikar idai ,  Seika-cho, Soraku-gun,  Kyoto ,  619-0288, Japan{paul, yamamoto, sumita}@itl.atr.co.jpAbst rac tIn this paper we propose a corpus-basedapproach to anaphora resolution combin-ing a machine learning method and sta-tistical information.
First, a decision treetrained on an annotated corpus determinesthe coreference r lation of a given anaphorand antecedent candidates and is utilizedas a filter in order to reduce the num-ber of potential candidates.
In the sec-ond step, preference selection is achievedby taking into account he frequency infor-mation of coreferential nd non-referentialpairs tagged in the training corpus as wellas distance features within the current dis-course.
Preliminary experiments concern-ing the resolution of Japanese pronouns inspoken-language dialogs result in a successrate of 80.6%.1 In t roduct ionCoreference information is relevant for numerousNLP systems.
Our interest in anaphora resolu-tion is based on the demand for machine translationsystems to be able to translate (possibly omitted)anaphoric expressions in agreement with the mor-phosyntactic characteristics of the referred object inorder to prevent contextual misinterpretations.So far various approaches 1 to anaphora resolutionhave been proposed.
In this paper a machine learn-ing approach (decision tree) is combined with a pref-erence selection method based on the frequency in-formation of non-/coreferential pairs tagged in thecorpus as well as distance features within the cur-rent discourse.The advantage of machine learning approaches ithat they result in modular anaphora resolution sys-tems automatically trainable from a corpus with no1See section 4 for a more detailed comparison withrelated research.or only a minimal amount of human intervention.
Inthe case of decision trees, we do have to provide in-formation about possible antecedent indicators (syn-tactic, semantic, and pragmatic features) containedin the corpus, but the relevance of features for theresolution task is extracted automatically from thetraining data.Machine learning approaches using decision treesproposed so far have focused on preference selectioncriteria directly derived from the decision tree re-sults.
The work described in (Conolly et al, 1994)utilized a decision tree capable of judging which oneof two given anaphor-antecedent pairs is "better".Due to the lack of a strong assumption on "transi-tivity", however, this sorting algorithm is more likea greedy heuristic search as it may be unable to findthe "best" solution.The preference selection for a single antecedent in(Aone and Bennett, 1995) is based on the maximiza-tion of confidence values returned from a pruned de-cision tree for given anaphor-candidate pairs.
How-ever, decision trees are characterized by an indepen-dent learning of specific features, i.e., relations be-tween single attributes cannot be obtained automat-ically.
Accordingly, the use of dependency factorsfor preference selection during decision tree train-ing requires that the artificially created attributesexpressing these dependencies be defined.
However,this not only extends human intervention into theautomatic learning procedure (i.e., which dependen-cies are important?
), but can also result in somedrawbacks on the contextual adaptation of prefer-ence selection methods.The preference selection in our approach is basedon the combination of statistical frequency informa-tion and distance features in the discourse.
There-fore, our decision tree is not applied directly to thetask of preference selection, but aims at the elimina-tion of irrelevant candidates based on the knowledgeobtained from the training data.47The decision tree is trained on syntactic (lexi-cal word attributes), semantic, and primitive dis-course (distance, frequency) information and deter-mines the coreferential relation between an anaphorand antecedent Candidate in the given context.
Irrel-evant antecedent candidates are filtered out, achiev-ing a noise reduction for the preference selectionalgorithm.
A preference value is assigned to each "potential anaphor-candidate pair depending on theproportion of non-/coreferential occurrences of thepair in the training corpus (frequency ratio) and therelative position of both elements in the discourse(distance).
The candidate with the maximal pref-erence value is resolved as the antecedent of theanaphoric expression.2 Corpus-Based Anaphora ResolutionIn this section we introduce a new approach toanaphora resolution based on coreferential proper-ties automatically extracted from a training corpus.In the first step, the decison tree filter is trained onthe linguistic, discourse and coreference informationannotated in the training corpus which is describedin section 2.1.Data Comferenc?
Analysis Preference Selectioncoref filter IF (^*c) t preference.
.
.
.
.
I \] I I L .
.
.
.Figure 1: System outlineThe resolution system in Figure 1 applies thecoreference filter (cf.
section 2.2) to all anaphor-candidate pairs (Ai + C/#) found in the discourse his-tory.
The detection of anaphoric expressions i outof the scope of this paper and just reduced to tagsin our annotated corpus.
Antecedent candidates areidentified according to noun phrase part-of-speechtags.
The reduced set (Ai + C/~) forms the inputof the preference algorithm which selects the mostsalient candidate C~ as described in section 2.3.Preliminary experiments are conducted for thetask of pronominal anaphora resolution and the per-formance of our system is evaluated in section 3.2.1  Data  CorpusFor our experiments we use the ATR-ITL Speechand Language Database (Takezawa et al, 1998) con-sisting of 500 Japanese spoken-language dialogs an-notated with coreferential tags.
It includes nomi-nal, pronominal, and ellipsis annotations, wherebythe anaphoric expressions used in our experimentsare limited to those referring to nominal antecedents(nominal: 2160, pronominal: 526, ellipsis: 3843).Besides the anaphor type, we also include mor-phosyntactic nformation like stem form and inflec-tion attributes for each surface word as well as se-mantic codes for content words (Ohno and Haman-ishi, 1981) in this corpus.rl: ~ ~) ~)~'~ O ~- '~H"~-o  ">"7" 4 -t,'T-)l~"~'~b~ Ht~\[thank you very much\] \[City Hotel\]'~'hank you for calling City Hotel.
"\[hello\] \[l\]\[Himko Tanaka\]\[the name is\]"Hello, my name is Hiroko Tanaka.
"\]there\] \[hotel\] \[reservation\]\[wonld like to have\]"I would like to make a reservation at your hotel.
"\[yonr\] \[name\] \[spening\] \[can I have\]"Can you spell your name for me, please?c2: II~,% -7"4 - - .
.x .
- -~ , .x .
- -9"4" , -x - - -~o\[yes\] rr\] \[A\] \[N\] {A\] \[K\] \[A\] \[be\]"It's T A N A K A.
"\[yes\] \[tenth\] \[here\] \[arrival\] \[be\]"Okay, you will arrive here on the tenth, right?
"Figure 2: Example dialogIn the example dialog between the hotel reception(r) and a customer (c) listed in Figure 2 the propernoun (r l )"5,#-# ~Y-~P \[City Hotel\]" is tagged asthe antecedent of the pronoun (cl)"~-~5 ~9 \[there\]"as well as the noun (cl)"$ff-)l~ \[hotel\]".
An exam-ple for ellipsis is the ommitted subject (c2)"@\[it\]"referring to (r2)"Y~-x~P \[spelling\]".According to the tagging guidelines used for ourcorpus an anaphoric tag refers to the most recentantecedent found in the dialog.
However, this an-tecedent might also refer to a previous one, e.g.
(r3)"~- ?9 ~ \[here\]"-*(cl)" ~-?9 ~ \[there\]"--~(rl) " 5"~-~" ?
~)P \[City Hotel\]".
Thus, the transitive clo-sure between the anaphora nd the first mention ofthe antecedent in the discourse history defines theset of positive examples, e.g.
(~-~ ~9, 5,if-4 $~-)P),whereas the nominal candidates outside the transi-tive closure are considered negative xamples, e.g.
(~- ~5 C9, ~ qu), for coreferential relationships.Based on the corpus annotations we extractthe frequency information of coreferential naphor-antecedent pairs and non-referential pairs from thetraining data.
For each non-/coreferential pair theoccurrences of surface and stem form as well as se-mantic code combinations are counted.typeword-wordword-semTable 1: Frequency dataanaphor candidate f req T f req -  ratio~" Ca r9 mS o 11 -1~- '~ tB  0 0 -o.1=~ {shop} 33 33 0{demonstrat ives} {shop} 51 18 0.48In Table 1 some examples are given for pronounanaphora, whereas the expressions "{...}" denotesemantic lasses assigned to the respective words.48The values freq +, freq-  and ratio and their usageare described in more detailed in section 2.3.Moreover, each dialog is subdivided into utter-ances consisting of one or more clauses.
There-fore, distance features are available on the utterance,clause, candidate, and morpheme levels.
For exam-ple, the distance values of the pronoun (r3)"~- ?9\[here\]" and the antecedent (rl)" "Y if-4 ?
ff-)l~ \[CityHotel\]" in our sample dialog in Figure 2 are d~tte~=4,dclaus~=7, dcand=14, dmorph=40.2.2 Core ferenee  Analys isTo learn the coreference relations from our corpuswe have chosen a C4.52-1ike machine learning al-gorithm without pruning.
The training attributesconsist of lexical word attributes (surface word, stemform, part-of-speech, semantic ode, morphologicalattributes) applied to the anaphor, antecedent can-didate, and clause predicate.
In addition, featureslike attribute agreement, distance and frequency ra-tio are checked for each anaphor-candidate pair.
Thedecision tree result consists of only two classes de-termining the coreference r lation between the givenanaphor-candidate pair.During anaphora resolution the decision tree isused as a module determining the coreferential prop-erty of each anaphor-candidate pair.
For each de-tected anaphoric expression a candidate list 3 is cre-ated.
The decision tree filter is then successivelyapplied to all anaphor-candidate pairs.If the decision tree results in the non-referenceclass, the candidate is judged as irrelevant and elim-inated from the list of potential antecedents formingthe input of the preference selection algorithm.2.3 P re ference  Select ionThe primary order of candidates is given by theirword distance from the anaphoric expression.
Astraightforward preference strategy we could chooseis the selection of the most recent candidate (MRC)as the antecedent, i.e., the first element of the can-didate list.
The success rate of this baseline test,however, is quite low as shown in section 3.But, this result does not mean that the recencyfactor is not important at all for the determinationof saliency in this task.
One reason for the bad per-formance is the application of the baseline test to theunfiltered set of-candidates resulting in the frequentselection of non-referential ntecedents.
Addition-ally, long-range references to candidates introducedfirst in the dialog are quite frequent in our  data.2Cf.
(Quinlan, 1993)3A list of noun phrase candidates preceding theanaphor element in the current discourse.An examination of our corpus gives rise to sus-picion that similarities to references in our trainingdata might be useful for the identification of thoseantecedents.
Therefore, we propose a preference se-lection scheme based on the combination of distanceand frequency information.First, utilizing statistical information about thefrequency of coreferential naphor-antecedent pairs(freq +) and non-referential pairs ( f req-)  extractedfrom the training data, we define the ratio of a givenreference pair as follows4:I -6  : (freq + -- f req-  = O) ratio = \ ] req  + - \ ] req -  f req+ 4- \ ] req -  : otherwiseThe value of ratio is in the range of \[-1,-1-1\],whereby ratio = -1  in the case of exclusive non-referential relations and ratio -- +1 in the case ofexclusive coreferential relationships.
In order for ref-erential pairs occurring in the training corpus withratio = 0 to be preferred to those without frequencyinformation, we slightly decrease the ratio value ofthe latter ones by a factor 6.As mentioned above the distance plays a crucialrole in our selection method, too.
We define a pref-erence value pref  by normalizing the ratio value ac-cording to the distance dist given by the primaryorder of the candidates in the discourse.ratiopre f = distThe pref value is calculated for each candidate andthe precedence ordered list of candidates i resortedtowards the maximization of the preference factor.Similarly to the baseline test, the first element ofthe preferenced candidate list is chosen as the an-tecedent.
The precedence order between candidatesof the same confidence continues to remain so andthus a final decision is made in the case of a draw.The robustness of our approach is ensured by thedefinition of a backup strategy which ultimately se-lects one candidate occurring in the history in thecase that all antecedent candidates are rejected bythe decision tree filter.
For our experiments reportedin section 3 we adopted the selection of the dialog-initial candidate as the backup strategy.3 Eva luat ionFor the evaluation of the experimental results de-scribed in this section we use F-measure metrics cal-culated by the recall and precision of the system per-formance.
Let ~\]t denote the total number of tagged4In order to keep the formula simple the frequencytypes are omitted (cf.
Table 1)49anaphor-antecedent pairs contained in the test data,E l  the number of these pairs passing the decisiontree filter, and ~ the number of correctly selectedantecedents.During evaluation we distinguish three classes:whether the correct antecedent is the first element ofthe candidate list (f), is in the candidate list (i), oris filtered out by the decision tree (o).
The metricsF, recall (R) and precision (P) are defined as follows:Z =l f l2 x P x RP+RF= E, p= ~"\]c sEjtIn order to prove the feasibility of our approachwe compare the four preference selection methodslisted in Figure 3.tagged corpus~Figure 3: Preference selection experimentsFirst, the baseline test MRC selects the most re-cent candidate as the antecedent of an anaphoric ex-pression.
The necessity of the filter and preferenceselection components i  shown by comparing the de-cision tree filter scheme DT (i.e., select the first el-ement of the filtered candidate list) and preferencescheme PREF (i.e., resort the complete candidatelist) against our combined method DT+PREF (i.e.,resort the filtered candidate list).5-way cross-validation experiments are conductedfor pronominal anaphora resolution.
The selectedantecedents are checked against the annotated cor-rect antecedents according to their morphosyntacticand semantic attributes.3.1 Training SizeWe use varied numbers of training dialogs (50-400)for the training of the decision tree and the extrac-tion of the frequency information from the corpus.Open tests are conducted on 100 non-training dialogswhereas closed tests use the training data for evalua-tion.
The results of the different preference selectionmethods are shown in Figure 4.The baseline test MRC succeeds in resolving only43.9% of the most recent candidates correctly asthe antecedent.
The best F-measure rate for DT is65.0% and for PREF the best rate is 78.1% whereas90.0 \[ I - i I60.0 -- *', 11  .
.
.
.
, .
.~m~?
"* f ' "~ ~ "  DT+PREF (pteci$ion).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, , .
, "  .
: : - -  '~ , - -50.0  - / ~ r,e.Ev m'~i~on j  - -, /~  MRC DT (F-meast~).
.
.
.
.
f .
.
.
.
.
.
.
.
.
.
T .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.40.0100 200 300 400training size (dialog)Figure 4: Training size versus performancethe combination of both methods achieves a successrate of 80.6%.The PREF method seems to reach a plateau ataround 300 dialogs which is borne out by the closedtest reaching a maximum of 81.1%.
Comparing therecall rate of DT (61.2%) and DT+PREF (75.9%)with the PREF result, we might conclude that thedecision tree is not much of a help due to the side-effect of 11.8% of the correct antecedents being fil-tered out.However, in contrast o the PREF algorithm, theDT method improves continuously according to thetraining size implying a lack of training data for theidentification of potential candidates.
Despite thesparse data the filtering method proves to be veryeffective.
The average number of all candidates (his-tory) for a given anaphor in our open data is 39 can-didates which is reduced to 11 potential candidatesby the decision tree filter resulting in a reduction rateof 71.8% (closed test: 81%).
The number of trivialselection cases (only one candidate) increases from2.7% (history) to 11.4% (filter; closed test: 21%).On average, two candidates are skipped in the his-tory to select the correct antecedent.Moreover, the precision rates of DT (69.4%) andDT+PREF (86.0%) show that the utilization of thedecision tree filter in combination with the statisti-cal preference selection gains a relative improvementof 9% towards the preference and 16% towards thefilter method.Additionally, the system proves to be quite robust,because the decision tree filters out all candidatesin only 1% of the open test samples.
Selecting thecandidate first introduced in the dialog as a backupstrategy shows the best performance due to the fre-quent dialog initial references contained in our data.50l DTrecall 61.2precision l\] 69.4F-measure I 65.0(filtered-out) 11.8Table 2: Frequency and distance dependencyDT-no-dist60.168.764.112.5DT-no-freq53.664.558.516.9DT+PREF75.986.080.611.8DT+PREF-no-dist73.082.877.611.83.2 Feature  DependencyIn our approach frequency ratio and distance infor-mation plays a crucial role not only for the identi-fication of potential candidates during decision treefiltering, but also for the calculation of the prefer-ence value for each antecedent candidate.In the first case these features are used indepen-dently to characterize the training samples whereasthe preference selection method is based on the de-pendency between the frequency and distance valuesof the given anaphor-candidate pair in the contextof the respective discourse.
The relative importanceof each factor is shown in Table 2.First, we compare our decision tree filter DT tothose methods that do not use either frequency (DT-no-freq) or distance (DT-no-dist) information.
Fre-quency information does appear to be more relevantfor the identification of potential candidates thandistance features extracted from the training corpus.The recall performance of DT-no-freq decreases by7.6% whereas DT-no-dist is only 1.1% below the re-sult of the original DT filter 5.
Moreover, the numberof correct antecedents not passing the filter increasesby 5.1% (DT-no-freq) and 0.7% (DT-no-dist).However, the distance factor proves to be quiteimportant as a preference criterion.
Relying only onthe frequency ratio as the preference value, the re-call performance of DT+PREF-no-dist is only 73.0%,down 2.9% of the original DT+PREF method.The effectiveness of our approach is not only basedon the usage of single antecedent indicators ex-tracted from the corpus, but also on the combinationof these features for the selection of the most prefer-able candidate in the context of the given discourse.4 Re la ted  ResearchDue to the characteristics of the underlying dataused in these experiments a comparison involvingabsolute numbers to previous approaches gives usless evidence.
However, the difficulty of our taskcan be verified according to the baseline xperiment5So far we have considered the decision tree filter justas a black-box tool.
Further investigations ontree struc-tures, however, should give us more evidence about therelative importance of the respective f atures.results reported in (Mitkov, 1998).
Resolving pro-nouns in English technical manuals to the most re-cent candidate achieved a success rate of 62.5%,whereas in our experiments only 43.9% of the mostrecent candidates are resolved correctly as the an-tecedent (cf.
section 3).Whereas knowledge-based ystems like (Carbonelland Brown, 1988) and (Rich and LuperFoy, 1988)combining multiple resolution strategies are expen-sive in the cost of human effort at development timeand limited ability to scale to new domains, more re-cent knowledge-poor approaches like (Kennedy andBoguraev, 1996) and (Mitkov, 1998) address theproblem without sophisticated linguistic knowledge.Similarly to them we do not use any sentence parsingor structural analysis, but just rely on morphosyn-tactic and semantic word information.Moreover, clues are used about the grammaticaland pragmatic functions of expressions as in (Groszet al, 1995), (Strube, 1998), "or (Azzam et al,1998) as well as rule-based empirical approaches like(Nakaiwa and Shirai, 1996) or (Murata and Nagao,1997), to determine the most salient referent.
Thesekinds of manually defined scoring heuristics, how-ever, involve quite an amount of human interventionwhich is avoided in machine learning approaches.As briefly noted in section 1, the work describedin (Conolly et al, 1994) and (Aone and Bennett,1995) differs from our approach according to the us-age of the decision tree in the resolution task.
In(Conolly et al, 1994) a decision tree is trained ona small number of 15 features concerning anaphortype, grammatical function, recency, morphosyntac-tic agreement and subsuming concepts.
Given twoanaphor-candidate pairs the system judges whichis "better".
However, due to the lack of a strongassumption on "transitivity" this sorting algorithmmay be unable to find the "best" solution.Based on discourse markers extracted from lexical,syntactic, and semantic processing, the approach of(Aone and Bennett, 1995) uses 66 unary and bi-nary attributes (lexical, syntactic, semantic, posi-tion, matching category, topic) during decision treetraining.
The confidence values returned from thepruned decision tree are utilized as a saliency mea-sure for each anaphor-candidate pair in order to se-51lect a single antecedent.
However, we use depen-dency factors for preference selection which cannotbe learned automatically because of the indepen-dent learning of specific features during decision treetraining.
Therefore, our decision tree is not applieddirectly to the task of preference selection, but onlyused as a filter to reduce the number of potentialcandidates for preference selection.In addition to salience preference, a statisticallymodeled iexical preference is exploited in (Dagan etal., 1995) by comparing the conditional probabili-ties of co-occurrence patterns given the occurrenceof candidates.
Experiments, however, are carriedout on computer manual texts with mainly intra-sentential references.
This kind of data is also char-acterized by the avoidance of disambiguities andonly short discourse units, which prohibits almostany long-range references.
In contrast o this re-search, our results show that the distance factor inaddition to corpus-based frequency information isquite relevant for the selection of the most salientcandidate in our task.5 ConclusionIn this paper we proposed a corpus-based anaphoraresolution method combining an automatic learningalgorithm for coreferential relationships with statis-tical preference selection in the discourse context.We proved the applicability of our approach to pro-noun resolution achieving a resolution accuracy of86.0% (precision) and 75.9% (recall) for Japanesepronouns despite the limitation of sparse data.
Im-provements in these results can be expected by in-creasing the training data as well as utilizing moresophisticated linguistic knowledge (structural anal-ysis of utterances, etc.)
and discourse information(extra-sentential knowledge, etc.)
which should leadto a rise of the decision tree filter performance.Preliminary experiments with nominal referenceand ellipsis resolution showed promising results, too.We plan to incorporate this approach in multi-lingual machine translation which enables us to han-dle a variety of referential relations in order to im-prove the translation quality.AcknowledgementWe would like to thank Hitoshi Nishimura (ATR) forhis programming support and Hideki Tanaka (ATR)for helpful personal communications.ReferencesC.
Aone and S. Bennett.
1995.
Evaluating Auto-mated and Manual Acquisition of Anaphora Res-olution Strategies.
In Proc.
of the 33th ACL, p.122-129.S.
Azzam, K. Humphreys, and R. Gaizauskas.
1998.Evaluating a Focus-Based Approach to AnaphoraResolution.
In Proc.
of the 17th COLING, p. 74-78, Montreal, Canada.J.
Carbonell and R. Brown.
1988.
Anaphora Res-olution: A Multi-Strategy Approach.
In Proc.
ofthe 12th COLING, p. 96-101, Budapest, Hungary.D.
Conolly, J. Burger, and D. Day.
1994.
A MachineLearning Approach to Anaphoric Reference.
InProc.
of NEMLAP'94, p. 255-261, Manchester.I.
Dagan, J. Justeson, S. Lappin, H. Leass, andA.
Ribak.
1995.
Syntax and Lexical Statisticsin Anaphora Resolution.
Applied Artificial Intel-ligence, 9:633-644.B.
Grosz, A. Joshi, and S. Weinstein.
1995.
AFramework for Modeling the Local Coherence ofDiscourse.
Comp.
Linguistics, 21(2):203-225.C.
Kennedy and B. Boguraev.
1996.
Anaphorafor Everyone: Pronominal Anaphora Resolutionwithout a Parser.
In Proc.
of the 16th COLING,p.
113-118, Copenhagen, Denmark.R.
Mitkov.
1998.
Robust pronoun resolution withlimited knowledge.
In Proc.
of the 17th COLING,p.
869-875, Montreal, Canada.M.
Murata and M. Nagao.
1997.
An Estimate ofReferents of Pronouns in Japanese Sentences us-ing Examples and Surface Expressions.
Journalof Natural Language Processing, 4(1):87-110.H.
Nakaiwa and S. Shirai.
1996.
Anaphora Resolu-tion of Japanese Zero Pronouns with Deictic Ref-erence.
In Proc.
of the 16th COLING, p. 812-817,Copenhagen, Denmark.S.
Ohno and M. Hamanishi.
1981.
Ruigo-Shin-Jiten.
Kadokawa.J.
Quinlan.
i993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.E.
Rich and S. LuperFoy.
1988.
An Architecture forAnaphora Resolution.
In Proc.
of the 2nd Con-ference on Applied Natural Language Processing,p.
18-23, Austin, TX.M.
Strube.
1998.
Never Look Back: An Alternativeto Centering.
In Proc.
of the 17th COLING, p.1251-1257, Montreal, Canada.T.
Takezawa, T. Morimoto, and Y. Sagisaka.
1998.Speech and language database for speech transla-tion research in ATR.
In Proc.
of Oriental CO-COSDA Workshop, p. 148-155.52
