Translating Collocations for Use in Bilingual LexiconsFrank Smadja and Kathleen McKeownComputer Science DepartmentColumbia UniversityNew York, NY 10027(smadja/kathy) @cs.columbia.eduABSTRACTCollocations are notoriously difficult for non-native speakers totranslate, primarily because they are opaque and can not be translatedon a word by word basis.
We describe a program named Champollionwhich, given a pair of parallel corpora in two different languages,automatically produces translations of an input list of collocations.Our goal is to provide atool to compile bilingual lexical informationabove the word level in multiple languages and domains.
The algo-rithm we use is based on statistical methods and produces p wordtranslations of n word collocations in which n and p need not bethe same; the collocations can be either flexible or fixed compounds.For example, Champollion translates "to make a decision," "employ-ment equity," and "stock market," respectively into: "prendre unedecision," "tquit6 en mati~re d'emploi," and "bourse."
Testing andevaluation ofChampollion on one year's worth of the Hansards cor-pus yielded 300 collocations and their translations, evaluated at77%accuracy.
In this paper, we describe the statistical measures used,the algorithm, and the implementation of Champollion, presentingour results and evaluation.1.
IntroductionHieroglyphics remained undeciphered forcenturies until the discov-ery of the Rosetta Stone in the beginning of the 19th century inRosetta, Egypt.
The Rosetta Stone is a tablet of black basalt contain-ing parallel inscriptions inthree different writings; one in greek, andthe two others in two different forms of ancient Egyptian writings?
(demotic and hieroglyphics).
Jean-Francois Champollion, a linguistand egyptologist, made the assumption that these inscriptions wereparallel and managed after several years of research to decipher thehyerogliphic nscriptions.
He used his work on the Rosetta Stone asa basis from which to produce the first comprehensive hyeroglyphicsdictionary.In this paper, we describe a modem version of a similar approach:given a large corpus in two languages, our program, Champollion,produces translations of common word pairs and phrases which canform the basis for a bilingual lexicon.
Our focus is on the useof statistical methods for the translation ofmulti-word expressions,such as collocations, which cannot consistently be translated on aword by word basis.
Bilingual collocation dictionaries are currentlyunavailable even in languages such as French and English despitethe fact that collocations have been recognized as one of the mainobstacles to second language acquisition \[ 15\].We developed a program, Champollion, which translates colloca-tions using an aligned parallel bilingual corpus, or database cor-pus, as a reference.
It represents Champollion's knowledge of bothlanguages.
For a given source language collocation, Champollionuses statistical methods to incrementally construct the collocationtranslation, adding one word at a time.
Champollion first identifiesindividual words in the target language which are highly correlatedwith the source collocation.
Then, it identifies any pairs in this setof individual words which are highly correlated with the source col-location.
Similarly, triplets are produced by adding a word to a pairif it is highly correlated, and so forth until no higher combinationof words is found.
Champollion selects as the target collocationthe group of words with highest cardinality and correlation factor.Finally, it orders the words of the target collocation by examiningsamples in the corpus.
If word order is variable in the target collo-cation, Champollion labels it asflexible (as in to take steps to whichcan appear as: took steps to, steps were taken to, etc.
).To evaluate Champollion, we used a collocation compiler,Xtract\[12\], to automatically produce several lists of source (En-glish) collocations.
These source collocations contain both flexibleword pairs which can be separated by an arbitrary number of words,and fixed constituents, such as compound noun phrases.
We thenran Champolfion on separate corpora, each consisting of one year'sworth of data extracted from the Hansards Corpus.
We asked severalhumans who are conversant in both French and English to judge theresults.
Accuracy was rated at 77% for one test set and 61% for thesecond set.
In our discussion of results, we show how problems forthe second test set can be alleviated.In the following sections, we first describe the algorithm and st~/tisticsused in Champollion, we then present our evaluation and results, andfinally, we move to a discussion ofrelated work and our conclusions.2.
Champollion: Algorithm and StatisticsChampollion's algorithm relies on the following two assumption:?
If two groups of words are translations of one another, thenthe number of paired sentences in which they appear in thedatabase corpus is greater than expected by chance.
In otherwords, the two groups of words are correlated.?
If a set of words is correlated with the source collocation, itssubsets will also be correlated with the source collocation.The first assumption allows us to use a correlation measure as a basisfor producing translations, and the second assumption allows us toreduce our search from exponential time to constant time (on thesize of the corpus) using an iterative algorithm.
In this section, wefirst describe prerequisites necessary before running Champollion,we then describe the correlation statistics, and finally we describethe algorithm and its implementation.1522.1.
Preprocessing.There are two steps that must be carried out before running Cham-pollion.
The database corpus must be aligned sentence wise and alist of collocations to be translated must be provided in the sourcelanguage.Aligning the database corpus Champollion requires that the database corpus be aligned so that sentences that are translations of oneanother are co-indexed.
Most bilingual corpora are given as twoseparate (sets of) files.
The problem of identifying which sentencesin one language correspond to which sentences in the other is com-plicated by the fact that sentence order may be reversed or severalsentences may translate a single sentence.
Sentence alignment pro-grams (i.e., \[10\], \[2\], \[11\], \[1\], \[4\]) insert identifiers before eachsentence in the source and the target text so that translations aregiven the same identifier.
For Champollion, we used corpora thathad been aligned by Church's entence alignment program \[10\] asour input data.Providing Champolllon with a list of  source  collocations A listof source collocations can be compiled manually by experts, butit can also be compiled automatically by tools such as Xtract \[17\],\[12\].
Xtract produces a wide range of coUocations, including flexiblecollocations of the type "to make a decision," in which the wordscan be inflected, the word order might change and the number ofadditional words can vary.
Xtract also produces compounds, uchas "The Dow Jones average of 30 industrial stock," which are rigidcollocations.
We used Xtract to produce a list of input collocationsfor Champollion.2.2.
Statistics used: The Dice coefficient.There are several ways to measure the correlation of two events.In information retrieval, measures such as the cosine measure, theDice coefficient, and the Jaccard coefficient have been used \[21\], \[5\],while in computational linguistics mutual information of two eventsis most widely used (i.e., \[18\], \[19\]).
For this research we use theDice coefficient because it offers several advantages in our context.Let x and y be two basic events in our probability space, representingthe occurrence of a given word (or group of words) in the Englishand French corpora respectively.
Let f(x) represent the frequencyof occurrence of event x, i.e., the number of sentences containing x.Then p(x), the probability of event x, can be estimated by f(x) dividedby the total number of sentences.
Similarly, the joint probability ofx and y, p(x ^  y) is the number of sentences containing x in theirEnglish version and y in their French version ( f (z  ^ y)) divided bythe total number of sentences.
We can now define the Dice coefficientand the mutual information of of x and y as:Dice(z,  y) = A ?
$(z)+l(y)MU(x ,y )  !o J "ff~^Y) ~ = Y($(z)xl(y)) + BIn which A and B are constants related to the size of the corpus.We found the Dice Coefficient to be better suited than the more widelyused mutual information to our problem.
We are looking for a clearcut test that would decide when two events are correlated.
Both forIWe are thankful to Ken Church and the Bell Laboratories for providingus with a prealigned Hansards corpus.mutual information and the Dice coefficient this involves comparisonwith a threshold that has to be determined by experimentation.
Whileboth measures are similar in that they compare the joint probabilityof the two events (p(x ^  y)) with their independent probabilities,they have different asymptotic behaviors.
For example,?
when the two events are perfectly independent, p(x ^  y) =p(x) ?
p(y).?
when one event is fully determined by the other (y occurs whenand only when, x occurs), p(x ^  y) = p(x).In the first case, mutual information is equal to a constant and is thuseasily testable, whereas the Dice coefficient isequal to 2x~(~+) ~ ~))and is thus a function of the individual frequencies of x and y. Inthis case, the test is easier to decide when using mutual information.In case two, the results are reversed; mutual information is equalto: - l og( f  (x)) and thus grows with the inverse of the individualfrequency of x, whereas the Dice coefficient is equal to a constant.Not only is the test is easier to decide using the Dice Coefficient inthis case, but also note that low frequency events will have highermutual information than high frequency events, a counter-intuitiveresult.
Since we are looking for a way to identify correlated eventswe must be able to easily identify the coefficient when the two eventsare perfectly correlated as in case two.Another eason that mutual information is less appropriate for ourtask than the Dice Coefficient is that it is, by definition, symmetric,weighting equally one-one and zero-zero matches, while the DiceCoefficient gives more weight o one,one matches.
One-one matchesare cases where both source and target words (or word groups) ap-pear in corresponding sentences, while in zero-zero matches, neithersource nor target words (or word groups) appear.In short, we prefer the use of the Dice coefficient because it is a betterindicator of similarity.
We confirmed the performance of the Diceover mutual information experimentally aswell.
In our tests with asmall sample of collocations, the Dice Coefficient corrected errorsintroduced by mutual information and never contradicted mutualinformation when it was correct \[20\].2.3.
Description of the algorithm.For a given source collocation, ChampoUion produces the target col-location by first computing the set of single words that are highlycorrelated with the source collocation and then searching for anycombination of words in that set with a high correlation with thesource.
In order to avoid computing and testing every possible com-bination which would yield a search space equal to the powersetof the set of highly correlated individual words, ChampoUion itera-tively searches the set of combinations containing n words by addingone word from the original set to each combination of (n -1) wordthat has been identified as highly correlated to the source colloca-tion.
At each stage, Champollion throws out any combination witha low correlation, thereby avoiding examining any supersets of thatcombination i a later stage.
The algorithm can be described moreformally as follows:Notation: L1 and L2 are the two languages used, and the followingsymbols are used:?
S: source collocation in L1?
T: target collocation in L2153?
WS: list of L2 words correlated with S?
P(WS): powerset of WS?
n: number of elements of P(WS)?
CC: list of candidate arget L2 collocations?
P(i, WS): subset of P(WS) containing all the i-tuples?
CT: correlation threshold fixed by experimentation.Experiment !
OK X W OverallC1/DBI 70  11 19 77C2/DB1 58 11 I 31 61Table 2: Evaluation results for Champollion.Step 1: Initialization of the work space.
Collect all the wordsin L2 that are correlated with S, producing WS.
At this point, thesearch space is P(WS); i.e., T is an element of P(WS).
Champollionsearches this space in Step 2 in an iterative manner by looking atgroups of words of increasing cardinality.Step 2;: Main iteration.Vi in .\[1,2, 3..... n}1.
Construct P(i, WS).P(i, WS) is constructed by considering all the i-tuples fromP(WS) that are supersets ofelements of P(i-1, WS).
We defineP(0, WS) as null.2.
Compute correlation scores forall elementsofP(i, WS).
Elim-inate from P(i, WS) all elements whose scores are below CT.3.
If P(i, WS) is empty exit the iteration loop.4.
Add the element of P(i,WS) with highest score to CC.5.
Increment i and go back to beginning ofthe iteration loop item1.Step 3: Determination of the best translation.
Among all theelements of CC select as the target collocation T, the element withhighest correlation factor.
When two elements of CC have the samecorrelation factor then we select he one containing the largest num-ber of words.Step 4: Determination of word ordering.
Once the translation hasbeen selected,Champollion examines all the sentences containing theselected translation i order to determine the type of the collocation,i.e., if the collocation is flexible (i.e., word order is not fixed) or ifthe collocation is rigid.
This is done by looking at all the sentencescontaining the target collocation and determining if the words areused in the same order in the majority of the cases and at the samedistance from one another.
In cases when the collocation is rigid,then the word order is also produced.
Note that although this is doneas a post processing stage, it does not require rereading the corpussince the information needed has already been precomputed.Example output of Champollion isgiven in Table 1.
Flexible collo-cations are shown with a "..." indicating where additional, variablewords could appear.
These examples show cases where a two wordcollocation istranslated asone word (e.g., "health insurance"), a twoword collocation is translated as three words (e.g., "employmentequity"), and how words can be inverted in the translation (e.g.,"advance notice").3.
EvaluationWe are carrying out three tests with Champollion with two data basecorpora and three sets of source collocations.
The first data basecorpus (DB1) consist of 8 months of Hansards aligned ata takenfrom 1986 and the second ata base corpus consists of all of the1986 and 1987 transcripts of the Canadian Parliament.
The first setof source collocations (C1) are 300 collocations identified by Xtracton all data from 1986, the second set (C2) is a set of 300 collocationsidentified by Xtract on all data from 1987, and the third set of collo-cations (C3) consists of 300 collocations identified by Xtract on alldata from 1988.
We used DB1 with both C1 (experiment 1)and C2(experiment 2) and are currently using DB2 on C3 (experiment 3).Results from the third experiment were not yet available at time ofpublication.We asked three bilingual speakers to evaluate the results for the dif-ferent experiments and the results are shown in Table 2.
The secondcolumn gives the percentage of correct ranslations, the third col-umn gives the percentage of Xtract errors, the fourth column givesthe percentage of Champollion's errors, and the last column givesthe percentage of Champollion's correct ranslation if the input isfiltered of errors introduced by Xtract.
Averages of the three eval-uators' scores are shown, but we noted that scores of individualevaluators were within 1-2% of each other; thus, there was highagreement between judges.
The best results are obtained when thedata base corpus is also used as a training corpus for Xtract; ig-nonng Xtract errors the evaluation is as high as 77%.
The secondexperiment produces low results as many input collocations did notappear often enough in the database corpus.
We hope to show thatwe can compensate for this by increasing the corpus ize in the thirdexperiment.One class of Champollion's errors arises because it does not.trans-late closed class words such as prepositions.
Since the frequency ofprepositions is so high in comparison toopen class words, includingthem in the translations throws off the correlations measures.
Trans-lations that should have included prepositions were judged inaccurateby our evaluators and this accounted for approximately 5% of theerrors.
This is an obvious place to begin improving the accuracy ofChampollion.4.
Related Work.The recent availability oflarge amounts of bilingual data has attractedinterest in several areas, including sentence alignment \[10\], \[2\], \[11\],\[1\], \[4\], word alignment \[6\], alignment of groups of words \[3\], \[7\],and statistical translation \[8\].
Of these, aligning roups of wordsis most similar to the work reported here, although we considera greater variety of groups.
Note that additional research usingbilingual corpora is less related to ours, addressing, for example,word sense disambiguation in the source language by examiningdifferent translations in the target \[9\], \[8\].One line of research uses statistical techniques only for machinetranslation \[8\].
Brown et.
al.
use a stochastic language modelbased on the techniques u ed in speech recognition \[19\], combinedwith translation probabilities compiled on the aligned corpus in or-der to do sentence translation.
The project produces high quality154English French Equivalentadvance noticeadditional costapartheid ... South Africaaffirmative actioncollective agreementfree tradefreer tradehead officehealth insuranceemployment equitymake a decisionto take stepsto demonstrate supportprtvenu avancecoflts suppltmentairesapartheid ... afrique sudaction positiveconvention collectivelibre-tchangelibtralisation ... 6changessi~ge socialassurance-maladie6quit6 ... mati'ere ... emploiprendre ... dtcisionsprendre ... mesuresprouver .. adhtsionTable 1: Some Translations produced by Champollion.translations for shorter sentences ( ee Berger et.
al., this volume,for information on most recent results) using little linguistic and nosemantic information.
While they also align groups of words acrosslanguages in the process of translation, they are careful to point outthat such groups may or may not occur at constituent breaks in thesentence.
In contrast, our work aims at identifying syntactically andsemantically meaningful nits, which may either be constituents orflexible word pairs separated by intervening words, and provides thetranslation of these units for use in a variety of bilingual applications.Thus, the goals of our research are somewhat different.Kupiec \[3\] describes a technique for finding noun phrase corre-spondences in bilingual corpora.
First, (as for Champollion), thebilingual corpus must be aligned sentence-wise.
Then, each corpusis run through a part of speech tagger and noun phrase recognizerseparately.
Finally, noun phrases are mapped to each other usingan iterative reestimation algorithm.
In addition to the limitationsindicated in \[3\], itonly handles NPs, whereas collocations have beenshown to include parts of NPs, categories other than NPs (e.g., verbphrases), as well as flexible phrases that do not fall into a single cat-egory but involve words separated by an arbitrary number of otherwords, such as "to take .. steps," to demonstrate ... support," etc.In this work as in earlier work \[7\], we address this full range ofcollocations.5.
ConclusionWe have presented a method for translating collocations, imple-mented in Champollion.
The ability to compile a set of translationsfor a new domain automatically will ultimately increase the porta-bility of machine translation systems.
The output of our system isa bilingual lexicon that is directly applicable to machine translationsystems that use a transfer approach, since they rely on correspon-dences between words and phrases of the source and target languages.For interlingua systems, translating collocations can aid in augment-ing the interlingua; since such phrases cannot be translated compo-sitionally, they indicate where concepts representing such phrasesmust be added to the interlingua.Since Champollion makes few assumptions about its input, it can beused for many pairs of languages with little modification.
Cham-pollion can also be applied to many domains of applications sinceit incorporates no assumptions about he domain.
Thus, we can ob-tain domain specific bilingual collocation dictionaries by applyingChampollion to different domain specific orpora.
Since collocationsand idiomatic phrases are clearly domain dependent, the facility toquickly construct the phrases used in new domains is important.
Atool such as Champollion isuseful for many tasks including machine(aided) translation, lexicography, language generation, and multilin-gual information retrieval.6.
AcknowledgementsMany thanks to Vasilis Hatzivassiloglou for technical and editorialcomments.
We also thank Eric Siegel for his comments on a draft ofthis paper.
This research was partially supported by a joint grant fromthe Office of Naval Research and the Defense Advanced ResearchProjects Agency under contract N00014-89-J-1782 and by NationalFoundation Grant GER-90-2406.References1.
Chen, S., "Aligning Sentences in Bilingual Corpora Using Lex-ical Information", Proceedings of the 31st meeting of the A CL,Association for Computational Linguistics, 1993, p. 9-16.2.
Church, K., "Char_align: A Program for Aligning ParallelTexts at the Character Level", Proceedings of the 31st meetingof the ACL, Association for Computational Linguistics, 1993,p.
1-8.3.
Kupiec, J., "An Algorithm for Finding Noun Phrase Correspon-dences in Bilingual Corpora", Proceedings of the 31st meetingof the ACL, Association for Computational Linguistics, 1993,p.
17-22.4.
Simard, M., Foster, G., and Isabelle, P., "Using Cognates toAlign Sentences inBilingual Corpora", Proceedingsofthe 31stmeeting oftheA CL, Association for Computational Linguistics,1993, p. 17-22.5.
Frakes, W., Information Retrieval.
Data Structures and Algo-rithms, ed.
W. Frakes and R. Baeza-Yates, Prentice Hall, 1992.6.
Gale, W. and Church, K., "Identifying word correspondencesin parallel texts", Darpa Speech and Natural Language Work-shop, Defense Advanced Research Projects Agency, 1991.7.
Smadja, E, "How to Compile a Bilingual Collocational Lex-icon Automatically", Proceedings of the AAAI Workshop onStatistically-Based NLP Techniques, 1992.1558.
Brown, P., Pietra, S., Pietra, V, and Mercer, R., "Word-SenseDisambiguation Using Statistical Methods", Proceedings ofthe 29th meeting of the ACL, Association for ComputationalLinguistics, 1991, p. 169-184.9.
Dagan, I., Itai, A., and Schwall, U., "Two Languages are moreinformative than one", Proceedings of the 29th meeting of theACL, Association for Computational Linguistics, 1991, p. 130-137.10.
Gale, W. and Church, K., "A Program for Aligning Sentencesin Bilingual C~rpom.
", Proceedings of the 29th meeting ofthe A CL, Association for Computational Linguistics, 1991, p.177-184.11.
Brown, P., Lai, J. and Mercer, R., "Aligning Sentences inParallel Corpora", Proceedings of the 29th meeting of the A CL,Association for Computational Linguistics, 1991, p. 169-184.12.
Smadja, E, "Retrieving collocations from text: XTRACT",The Journal of Computational Linguistics, 1993.13.
Benson, M.,"CollocationsandIdioms",Dictionaries, Lexicog-raphy and Language Learning, ed.
R. Ilson, Pergamon I stituteof English, 1985.14.
Benson, M., Benson, E. and Ilson, R., The BBI CombinatoryDictionary of English: A Guide to Word Combinations, JohnBenjamins, 1986.15.
Leed, R. L. and Nakhimovsky, A. D., "Lexical Functions andLanguage Learning ", Slavic and East European Journal, Vol.23, No.
1, 1979.16.
Smadja, E, Retrieving Collocational Knowledge from TextualCorpora.
An Application: Language Generation., ComputerScience Department, Columbia University, 1991.17.
Smadja, E and McKeown, K., "Automatically Extracting andRepresenting Collocations for Language Generation", Pro-ceedings of the 28th annual meeting of the ACL, Associationfor Computational Linguistics, 1990.18.
Church, K. and Gale, W. and Hanks, P. and Hindle, D., "UsingStatistics in Lexical Analysis", LexicalAcquisition: Using on-line resources to build a lexicon, ed.
Ufi ~.,emik, LawrenceErlbaum, 1991.19.
Bahl, L. and Brown, P. and de Souza, P. and Mercer, R., "Max-imum Mutual Information of Hidden Markov Model Parame-ters", Proceedings of the IEEE Acoustics, Speech and SignalProcessing Society (ICASSP), The Institute of Electronics andCommunication E gineers of Japan and The Acoustical Soci-ety of Japan, 1986, p. 49.20.
Smadja, E and McKeown, K., "Champollion: An AutomaticTool for Developing Bilingual Lexicons," in preparation.21.
Salton, G. and McGiU, M. J., Introduction to Modem Informa-tion Retrieval, McGraw Hill, 1983.22.
Zipf, G. K., Human Behavior and the Principle of Least Effort,Addison-Wesley, 1949.23.
Church, K., "Stochastic Parts Program and Noun Phrase Parserfor Unrestricted Text", Proceedings of the Second Conferenceon Applied Natural Language Processing, 1988.24.
Halliday, M.A.K., "Lexis as a Linguistic Level", In memory ofJ.R.
Firth, Longmans Linguistics Library, 1966, p. 148-162.156
