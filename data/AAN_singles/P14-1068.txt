Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721?732,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Grounded Meaning Representations with AutoencodersCarina Silberer and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABc.silberer@ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we address the problem ofgrounding distributional representations oflexical meaning.
We introduce a newmodel which uses stacked autoencoders tolearn higher-level embeddings from tex-tual and visual input.
The two modali-ties are encoded as vectors of attributesand are obtained automatically from textand images, respectively.
We evaluate ourmodel on its ability to simulate similar-ity judgments and concept categorization.On both tasks, our approach outperformsbaselines and related models.1 IntroductionRecent years have seen a surge of interest in sin-gle word vector spaces (Turney and Pantel, 2010;Collobert et al, 2011; Mikolov et al, 2013) andtheir successful use in many natural language ap-plications.
Examples include information retrieval(Manning et al, 2008), search query expansions(Jones et al, 2006), document classification (Se-bastiani, 2002), and question answering (Yih et al,2013).
Vector spaces have been also popular incognitive science figuring prominently in simula-tions of human behavior involving semantic prim-ing, deep dyslexia, text comprehension, synonymselection, and similarity judgments (see Griffithset al, 2007).
In general, these models specifymechanisms for constructing semantic representa-tions from text corpora based on the distributionalhypothesis (Harris, 1970): words that appear insimilar linguistic contexts are likely to have relatedmeanings.Word meaning, however, is also tied to thephysical world.
Words are grounded in the exter-nal environment and relate to sensorimotor experi-ence (Regier, 1996; Landau et al, 1998; Barsalou,2008).
To account for this, new types of perceptu-ally grounded distributional models have emerged.These models learn the meaning of words basedon textual and perceptual input.
The latter is ap-proximated by feature norms elicited from humans(Andrews et al, 2009; Steyvers, 2010; Silbererand Lapata, 2012), visual information extractedautomatically from images, (Feng and Lapata,2010; Bruni et al, 2012a; Silberer et al, 2013)or a combination of both (Roller and Schulte imWalde, 2013).
Despite differences in formulation,most existing models conceptualize the problemof meaning representation as one of learning frommultiple views corresponding to different modali-ties.
These models still represent words as vectorsresulting from the combination of representationswith different statistical properties that do not nec-essarily have a natural correspondence (e.g., textand images).In this work, we introduce a model, illus-trated in Figure 1, which learns grounded mean-ing representations by mapping words and im-ages into a common embedding space.
Our modeluses stacked autoencoders (Bengio et al, 2007)to induce semantic representations integrating vi-sual and textual information.
The literature de-scribes several successful approaches to multi-modal learning using different variants of deepnetworks (Ngiam et al, 2011; Srivastava andSalakhutdinov, 2012) and data sources includingtext, images, audio, and video.
Unlike most pre-vious work, our model is defined at a finer levelof granularity ?
it computes meaning representa-tions for individual words and is unique in its useof attributes as a means of representing the textualand visual modalities.
We follow Silberer et al(2013) in arguing that an attribute-centric repre-sentation is expedient for several reasons.Firstly, attributes provide a natural way of ex-pressing salient properties of word meaning asdemonstrated in norming studies (e.g., McRaeet al, 2005) where humans often employ attributeswhen asked to describe a concept.
Secondly, from721a modeling perspective, attributes allow for eas-ier integration of different modalities, since theseare rendered in the same medium, namely, lan-guage.
Thirdly, attributes are well-suited to de-scribing visual phenomena (e.g., objects, scenes,actions).
They allow to generalize to new in-stances for which there are no training exam-ples available and to transcend category and taskboundaries whilst offering a generic description ofvisual data (Farhadi et al, 2009).Our model learns multimodal representationsfrom attributes which are automatically inferredfrom text and images.
We evaluate the embed-dings it produces on two tasks, namely word sim-ilarity and categorization.
In the first task, modelestimates of word similarity (e.g., gem?jewel aresimilar but glass?magician are not) are comparedagainst elicited similarity ratings.
We performeda large-scale evaluation on a new dataset consist-ing of human similarity judgments for 7,576 wordpairs.
Unlike previous efforts such as the widelyused WordSim353 collection (Finkelstein et al,2002), our dataset contains ratings for visual andtextual similarity, thus allowing to study the twomodalities (and their contribution to meaning rep-resentation) together and in isolation.
We alsoassess whether the learnt representations are ap-propriate for categorization, i.e., grouping a setof objects into meaningful semantic categories(e.g., peach and apple are members of FRUIT,whereas chair and table are FURNITURE).
On bothtasks, our model outperforms baselines and relatedmodels.2 Related WorkThe presented model has connections to severallines of work in NLP, computer vision research,and more generally multimodal learning.
We re-view related work in these areas below.Grounded Semantic Spaces Grounded seman-tic spaces are essentially distributional modelsaugmented with perceptual information.
A modelakin to Latent Semantic Analysis (Landauer andDumais, 1997) is proposed in Bruni et al (2012b)who concatenate two independently constructedtextual and visual spaces and subsequently projectthem onto a lower-dimensional space using Singu-lar Value Decomposition.Several other models have been extensions ofLatent Dirichlet Allocation (Blei et al, 2003)where topic distributions are learned from wordsand other perceptual units.
Feng and Lapata(2010) use visual words which they extract from acorpus of multimodal documents (i.e., BBC newsarticles and their associated images), whereas oth-ers (Steyvers, 2010; Andrews et al, 2009; Silbererand Lapata, 2012) use feature norms obtained inlongitudinal elicitation studies (see McRae et al(2005) for an example) as an approximation of thevisual environment.
More recently, topic mod-els which combine both feature norms and vi-sual words have also been introduced (Roller andSchulte im Walde, 2013).
Drawing inspirationfrom the successful application of attribute clas-sifiers in object recognition, Silberer et al (2013)show that automatically predicted visual attributesact as substitutes for feature norms without anycritical information loss.The visual and textual modalities on which ourmodel is trained are decoupled in that they are notderived from the same corpus (we would expectco-occurring images and text to correlate to someextent) but unified in their representation by natu-ral language attributes.
The use of stacked autoen-coders to extract a shared lexical meaning repre-sentation is new to our knowledge, although, aswe explain below related to a large body of workon deep learning.Multimodal Deep Learning Our work employsdeep learning (a.k.a deep networks) to project lin-guistic and visual information onto a unified rep-resentation that fuses the two modalities together.The goal of deep learning is to learn multiple lev-els of representations through a hierarchy of net-work architectures, where higher-level representa-tions are expected to help define higher-level con-cepts.A large body of work has focused on projectingwords and images into a common space using a va-riety of deep learning methods ranging from deepand restricted Boltzman machines (Srivastava andSalakhutdinov, 2012; Feng et al, 2013), to au-toencoders (Wu et al, 2013), and recursive neuralnetworks (Socher et al, 2013b).
Similar methodshave been employed to combine other modalitiessuch as speech and video (Ngiam et al, 2011) orimages (Huang and Kingsbury, 2013).
Althoughour model is conceptually similar to these studies(especially those applying stacked autoencoders),it differs considerably from them in at least twoaspects.
Firstly, most of these approaches aim tolearn a shared representation between modalities722so as to infer some missing modality from others(e.g., to infer text from images and vice versa); incontrast, we aim to learn an optimal representa-tion for each modality and their optimal combi-nation.
Secondly, our problem setting is differentfrom the former studies, which usually deal withclassification tasks and fine-tune the deep neuralnetworks using training data with explicit class la-bels; in contrast we fine-tune our autoencoders us-ing a semi-supervised criterion.
That is, we useindirect supervision in the form of object classifi-cation in addition to the objective of reconstruct-ing the attribute-centric input representation.3 Autoencoders for Grounded Semantics3.1 BackgroundOur model learns higher-level meaning represen-tations for single words from textual and visualinput in a joint fashion.
We first briefly reviewautoencoders in Section 3.1 with emphasis on as-pects relevant to our model which we then de-scribe in Section 3.2.Autoencoders An autoencoder is an unsuper-vised neural network which is trained to recon-struct a given input from its latent representation(Bengio, 2009).
It consists of an encoder f?whichmaps an input vector x(i)to a latent representa-tion y(i)= f?
(x(i)) = s(Wx(i)+ b), with s beinga non-linear activation function, such as a sig-moid function.
A decoder g?
?then aims to recon-struct input x(i)from y(i), i.e., ?x(i)= g??
(y(i)) =s(W?y(i)+ b?).
The training objective is the de-termination of parameters??
= {W,b} and???={W?,b?}
that minimize the average reconstructionerror over a set of input vectors {x(1), ...,x(n)}:??,??
?= argmin?,??1nn?i=1L(x(i),g??
( f?
(x(i)))), (1)where L is a loss function, such as cross-entropy.Parameters ?
and ?
?can be optimized by gradientdescent methods.Autoencoders are a means to learn representa-tions of some input by retaining useful features inthe encoding phase which help to reconstruct theinput, whilst discarding useless or noisy ones.
Tothis end, different strategies have been employedto guide parameter learning and constrain the hid-den representation.
Examples include imposinga bottleneck to produce an under-complete rep-resentation of the input, using sparse representa-tions, or denoising.Denoising Autoencoders The training criterionwith denoising autoencoders is the reconstructionof clean input x(i)given a corrupted version ?x(i)(Vincent et al, 2010).
The underlying idea is thatthe learned latent representation is good if the au-toencoder is capable of reconstructing the actualinput from its corruption.
The reconstruction errorfor an input x(i)with loss function L then is:L(x(i),g??
( f?
(?x(i)))) (2)One possible corruption process is masking noise,where the corrupted version ?x(i)results from ran-domly setting a fraction v of x(i)to 0.Stacked Autoencoders Several (denoising) au-toencoders can be used as building blocks to forma deep neural network (Bengio et al, 2007; Vin-cent et al, 2010).
For that purpose, the autoen-coders are pre-trained layer by layer, with the cur-rent layer being fed the latent representation of theprevious autoencoder as input.
Using this unsuper-vised pre-training procedure, initial parameters arefound which approximate a good solution.
Subse-quently, the original input layer and hidden repre-sentations of all the autoencoders are stacked andall network parameters are fine-tuned with back-propagation.To further optimize the parameters of the net-work, a supervised criterion can be imposed on topof the last hidden layer such as the minimizationof a prediction error on a supervised task (Bengio,2009).
Another approach is to unfold the stackedautoencoders and fine-tune them with respect tothe minimization of the global reconstruction error(Hinton and Salakhutdinov, 2006).
Alternatively,a semi-supervised criterion can be used (Ranzatoand Szummer, 2008; Socher et al, 2011) throughcombination of the unsupervised training criterion(global reconstruction) with a supervised criterion(prediction of some target given the latent repre-sentation).3.2 Semantic RepresentationsTo learn meaning representations of single wordsfrom textual and visual input, we employ stacked(denoising) autoencoders (SAEs).
Both inputmodalities are vector-based representations ofwords, or, more precisely, the objects they refer to(e.g., canary, trolley).
The vector dimensions cor-respond to textual and visual attributes, examplesof which are shown in Table 1.
We explain howthese representations are obtained in more detail723.........input xTEXTW(1)W(3).........IMAGESW(2)W(4)...bimodal coding ?yW(5?)W(5)...softmax?tW(6)......W(3?)......W(4?)...reconstruction?xW(1?)...W(2?
)Figure 1: Stacked autoencoder trained with semi-supervised objective.
Input to the model are single-word vector representations obtained from text and images.
Vector dimensions correspond to textual andvisual attributes, respectively (see Table 1).in Section 4.1.
We first train SAEs with two hid-den layers (codings) for each modality separately.Then, we join these two SAEs by feeding their re-spective second coding simultaneously to anotherautoencoder, whose hidden layer thus yields thefused meaning representation.
Finally, we stackall layers and unfold them in order to fine-tunethe SAE.
Figure 1 illustrates the model.Unimodal Autoencoders For both modalities,we use the hyperbolic tangent function as activa-tion function for encoder f?and decoder g?
?and anentropic loss function for L. The weights of eachautoencoder are tied, i.e., W?= WT.
We employdenoising autoencoders (DAEs) for pre-trainingthe textual modality.
Regarding the visual autoen-coder, we derive a new (?denoised?)
target vectorto be reconstructed for each input vector x(i), andtreat x(i)itself as corrupted input.
The unimodalautoencoder is thus trained to denoise a given in-put.
The target vector is derived as follows: eachobject o in our data is represented by multiple im-ages, and each image is in turn represented by avisual attribute vector x(i).
The target vector is thesum of x(i)and the centroid x(j)of the remainingattribute vectors representing object o.Bimodal Autoencoder The bimodal autoen-coder is fed with the concatenated final hiddencodings of the visual and textual modalities as in-put and maps these inputs to a joint hidden layer ?ywith B units.
We normalize both unimodal inputcodings to unit length.
Again, we use tied weightsfor the bimodal autoencoder.
We also encouragethe autoencoder to detect dependencies betweenthe two modalities while learning the mappingto the bimodal hidden layer.
We therefore applymasking noise to one modality with a masking fac-tor v (see Section 3.1), so that the corrupted modal-ity optimally has to rely on the other modality inorder to reconstruct its missing input features.Stacked Bimodal Autoencoder We finallybuild a stacked bimodal autoencoder (SAE) withall pre-trained layers and fine-tune them with re-spect to a semi-supervised criterion.
That is, weunfold the stacked autoencoder and furthermoreadd a softmax output layer on top of the bimodallayer ?y that outputs predictions?t with respect tothe inputs?
object labels (e.g., boat):?t(i)=exp(W(6)?y(i)+b(6))?Ok=1exp(W(6)k.?y(i)+b(6)k), (3)with weights W(6)?
RO?B, b(6)?
RO?1, where Ois the number of unique object labels.
The over-all objective to be minimized is therefore theweighted sum of the reconstruction error Lrandthe classification error Lc:L =1nn?i=1(?rLr(x(i), ?x(i))+?cLc(t(i),?t(i)))+?R (4)where ?rand ?care weighting parameters thatgive different importance to the partial objectives,724eats seeds has beak has claws has handlebar has wheels has wings is yellow made of woodcanary 0.05 0.24 0.15 0.00 ?0.10 0.19 0.34 0.00Visualtrolley 0.00 0.00 0.00 0.30 0.32 0.00 0.00 0.25bird:n breed:v cage:n chirp:v fly:v track:n ride:v run:v rail:n wheel:ncanary 0.16 0.19 0.39 0.13 0.13 0.00 0.00 0.00 0.00 ?0.05Textualtrolley ?0.40 0.00 0.00 0.00 0.00 0.14 0.16 0.33 0.17 0.20Table 1: Examples of attribute-based representations provided as input to our autoencoders.Lcand Lrare entropic loss functions, and R isa regularization term with R =?5j=12||W(j)||2+||W(6)||2.
Finally,?t(i)is the object label vector pre-dicted by the softmax layer for input vector x(i),and t(i)is the correct object label, represented as aO-dimensional one-hot vector1.The additional supervised criterion drives thelearning towards a representation capable of dis-criminating between different objects.
Further-more, the semi-supervised setting affords flexibil-ity, allowing to adapt the architecture to specifictasks.
For example, by setting the corruption pa-rameter v for the textual modality to one and ?rto zero, a standard object classification model forimages can be trained.
Setting v close to one for ei-ther modality enables the model to infer the other(missing) modality.
As our input consists of nat-ural language attributes, the model would infertextual attributes given visual attributes and viceversa.4 Experimental SetupIn this section we present our experimental setupfor assessing the performance of our model.
Wegive details on the tasks and datasets used for eval-uation, we explain how the textual and visual in-puts were constructed, how the SAE model wastrained, and describe the approaches used for com-parison with our own work.4.1 DataWe learn meaning representations for the nounscontained in McRae et al?s (2005) feature norms.These are 541 concrete animate and inanimate ob-jects (e.g., animals, clothing, vehicles, utensils,fruits, and vegetables).
The norms were elicitedby asking participants to list properties (e.g., barks,an animal, has legs) describing the nouns they werepresented with.1In a one-hot vector, the element corresponding to the ob-ject label is one and the others are zero.As shown in Figure 1, our model takes as in-put two (real-valued) vectors representing the vi-sual and textual modalities.
Vector dimensionscorrespond to textual and visual attributes, respec-tively.
Textual attributes were extracted by run-ning Strudel (Baroni et al, 2010) on a 2009 dumpof the English Wikipedia.2Strudel is a fullyautomatic method for extracting weighted word-attribute pairs (e.g., bat?species:n, bat?bite:v) froma lemmatized and POS-tagged corpus.
Weightsare log-likelihood ratio scores expressing howstrongly an attribute and a word are associated.
Weonly retained the ten highest scored attributes foreach target word.
This returned a total of 2,362dimensions for the textual vectors.
Associationscores were scaled to the [?1,1] range.To obtain visual vectors, we followed themethodology put forward in Silberer et al (2013).Specifically, we used an updated version of theirdataset to train SVM-based attribute classifiersthat predict visual attributes for images (Farhadiet al, 2009).
The dataset is a taxonomy of 636 vi-sual attributes (e.g., has wings, made of wood) andnearly 700K images from ImageNet (Deng et al,2009) describing more than 500 of McRae et al?s(2005) nouns.
The classifiers perform reason-ably well with an interpolated average precisionof 0.52.
We only considered attributes assignedto at least two nouns in the dataset, obtaining a414 dimensional vector for each noun.
Analo-gously to the textual representations, visual vec-tors were scaled to the [?1,1] range.We follow Silberer et al?s (2013) partition of thedataset into training, validation, and test set andacquire visual vectors for each of the sets.
We usethe visual vectors of the training and developmentset for training the autoencoders, and the vectorsfor the test set for evaluation.2The corpus is downloadable from http://wacky.sslmit.unibo.it/doku.php?id=corpora.7254.2 Model ArchitectureModel parameters were optimized on a subset ofthe word association norms collected by Nelsonet al (1998).3These were established by present-ing participants with a cue word (e.g., canary) andasking them to name an associate word in response(e.g., bird, sing, yellow).
For each cue, the normsprovide a set of associates and the frequencieswith which they were named.
The dataset con-tains a very large number of cue-associate pairs(63,619 in total) some of which luckily are cov-ered in McRae et al (2005).4During trainingwe used correlation analysis (Spearman?s ?)
tomonitor the degree of linear relationship betweenmodel cue-associate (cosine) similarities and hu-man probabilities.The best autoencoder on the word associationtask obtained a correlation coefficient of 0.33.This performance is superior to the results re-ported in Silberer et al (2013) (their correlationcoefficients range from 0.16 to 0.28).
This modelhas the following architecture: the textual autoen-coder (see Figure 1, left-hand side) consists of 700hidden units which are then mapped to the sec-ond hidden layer with 500 units (the corruptionparameter was set to v = 0.1); the visual autoen-coder (see Figure 1, right-hand side) has 170 and100 hidden units, in the first and second layer, re-spectively.
The 500 textual and 100 visual hiddenunits were fed to a bimodal autoencoder contain-ing 500 latent units, and masking noise was ap-plied to the textual modality with v = 0.2.
Theweighting parameters for the joint training objec-tive of the stacked autoencoder were set to ?r= 0.8and ?c= 1 (see Equation (4)).We used the model described above and themeaning representations obtained from the out-put of the bimodal latent layer for all the eval-uation tasks detailed below.
Some performancegains could be expected if parameter optimizationtook place separately for each task.
However, wewanted to avoid overfitting, and show that our pa-rameters are robust across tasks and datasets.4.3 Evaluation TasksWord Similarity We first evaluated how wellour model predicts word similarity ratings.
Al-though several relevant datasets exist, such as3http://w3.usf.edu/Freeassociation.4435 word pairs constitute the overlap between Nelson etal.
?s norms (1998) and McRae et al?s (2005) nouns.the widely used WordSim353 (Finkelstein et al,2002) or the more recent Rel-122 norms (Szum-lanski et al, 2013), they contain many abstractwords, (e.g., love?sex or arrest?detention) whichare not covered in McRae et al (2005).
This is fora good reason, as most abstract words do not havediscernible attributes, or at least attributes that par-ticipants would agree upon.
We thus created anew dataset consisting exclusively of McRae et al(2005) nouns which we hope will be useful for thedevelopment and evaluation of grounded semanticspace models.5Initially, we created all possible pairings overMcRae et al?s (2005) nouns and computed theirsemantic relatedness using Patwardhan and Peder-sen (2006)?s WordNet-based measure.
We optedfor this specific measure as it achieves high corre-lation with human ratings and has a high coverageon our nouns.
Next, for each word we randomlyselected 30 pairs under the assumption that theyare representative of the full variation of semanticsimilarity.
This resulted in 7,576 word pairs forwhich we obtained similarity ratings using Ama-zon Mechanical Turk (AMT).
Participants wereasked to rate a pair on two dimensions, visualand semantic similarity using a Likert scale of 1(highly dissimilar) to 5 (highly similar).
Each taskconsisted of 32 pairs covering examples of weakto very strong semantic relatedness.
Two con-trol pairs from Miller and Charles (1991) were in-cluded in each task to potentially help identify andeliminate data from participants who assigned ran-dom scores.
Examples of the stimuli and meanratings are shown in Table 2.The elicitation study comprised overall 255tasks, each task was completed by five volun-teers.
The similarity data was post-processed soas to identify and remove outliers.
We consid-ered an outlier to be any individual whose meanpairwise correlation fell outside two standard de-viations from the mean correlation.
11.5% ofthe annotations were detected as outliers and re-moved.
After outlier removal, we further ex-amined how well the participants agreed in theirsimilarity judgments.
We measured inter-subjectagreement as the average pairwise correlation co-efficient (Spearman?s ?)
between the ratings of allannotators for each task.
For semantic similarity,the mean correlation was 0.76 (Min =0.34, Max5Available from http://homepages.inf.ed.ac.uk/mlap/index.php?page=resources.726Word Pairs Semantic Visualfootball?pillow 1.0 1.2dagger?pencil 1.0 2.2motorcycle?wheel 2.4 1.8orange?pumpkin 2.5 3.0cherry?pineapple 3.6 1.2pickle?zucchini 3.6 4.0canary?owl 4.0 2.4jeans?sweater 4.5 2.2pan?pot 4.7 4.0hornet?wasp 4.8 4.8airplane?jet 5.0 5.0Table 2: Mean semantic and visual similarity rat-ings for the McRae et al (2005) nouns using ascale of 1 (highly dissimilar) to 5 (highly similar).=0.97, StD =0.11) and for visual similarity 0.63(Min =0.19, Max =0.90, SD =0.14).
These re-sults indicate that the participants found the taskrelatively straightforward and produced similarityratings with a reasonable level of consistency.
Forcomparison, Patwardhan and Pedersen?s (2006)measure achieved a coefficient of 0.56 on thedataset for semantic similarity and 0.48 for vi-sual similarity.
The correlation between the aver-age ratings of the AMT annotators and the Millerand Charles (1991) dataset was ?
= 0.91.
In ourexperiments (see Section 5), we correlate model-based cosine similarities with mean similarity rat-ings (again using Spearman?s ?
).Categorization The task of categorization(i.e., grouping objects into meaningful categories)is a classic problem in the field of cognitivescience, central to perception, learning, and theuse of language.
We evaluated model outputagainst a gold standard set of categories createdby Fountain and Lapata (2010).
The datasetcontains a classification, produced by humanparticipants, of McRae et al?s (2005) nouns into(possibly multiple) semantic categories (40 intotal).6To obtain a clustering of nouns, we used Chi-nese Whispers (Biemann, 2006), a randomizedgraph-clustering algorithm.
In the categorizationsetting, Chinese Whispers (CW) produces a hardclustering over a weighted graph whose nodes cor-6The dataset can be downloaded from http://homepages.inf.ed.ac.uk/s0897549/data/.respond to words and edges to cosine similarityscores between vectors representing their mean-ing.
CW is a non-parametric model, it induces thenumber of clusters (i.e., categories) from the dataas well as which nouns belong to these clusters.In our experiments, we initialized Chinese Whis-pers with different graphs resulting from differentvector-based representations of the McRae et al(2005) nouns.
We also transformed the datasetinto hard categorizations by assigning each nounto its most typical category as extrapolated fromhuman typicality ratings (for details see Foun-tain and Lapata, 2010).
CW can optionally ap-ply a minimum weight threshold which we opti-mized using the categorization dataset from Ba-roni et al (2010).
The latter contains a classifica-tion of 82 McRae et al (2005) nouns into 10 cate-gories.
These nouns were excluded from the goldstandard (Fountain and Lapata, 2010) in our finalevaluation.We evaluated the clusters produced by CW us-ing the F-score measure introduced in the Se-mEval 2007 task (Agirre and Soroa, 2007); it isthe harmonic mean of precision and recall definedas the number of correct members of a cluster di-vided by the number of items in the cluster andthe number of items in the gold-standard class, re-spectively.4.4 Comparison with Other ModelsThroughout our experiments we compare a bi-modal stacked autoencoder against unimodal au-toencoders based solely on textual and visual in-put (left- and right-hand sides in Figure 1, respec-tively).
We also compare our model against twoapproaches that differ in their fusion mechanisms.The first one is based on kernelized canonical cor-relation (kCCA, Hardoon et al, 2004) with a lin-ear kernel which was the best performing modelin Silberer et al (2013).
The second one emulatesBruni et al?s (2014) fusion mechanism.
Specifi-cally, we concatenate the textual and visual vec-tors and project them onto a lower dimensional la-tent space using SVD (Golub and Reinsch, 1970).All these models run on the same datasets/itemsand are given input identical to our model, namelyattribute-based textual and visual representations.We furthermore report results obtained withBruni et al?s (2014) bimodal distributional model,which employs SVD to integrate co-occurrence-based textual representations with visual repre-727Semantic VisualModels T V T+V T V T+VMcRae 0.71 0.49 0.68 0.58 0.52 0.62Attributes 0.58 0.61 0.68 0.46 0.56 0.58SAE 0.65 0.60 0.70 0.52 0.60 0.64SVD ?
?
0.67 ?
?
0.57kCCA ?
?
0.57 ?
?
0.55Bruni ?
?
0.52 ?
?
0.46RNN-640 0.41 ?
?
0.34 ?
?Table 3: Correlation of model predictions againstsimilarity ratings for McRae et al (2005) nounpairs (using Spearman?s ?
).sentations constructed from low-level image fea-tures.
In their model, the textual modality isrepresented by the 30K-dimensional vectors ex-tracted from UKWaC and WaCkypedia.7Thevisual modality is represented by bag-of-visual-words histograms built on the basis of clusteredSIFT features (Lowe, 2004).
We rebuilt theirmodel on the ESP image dataset (von Ahn andDabbish, 2004) using Bruni et al?s (2013) publiclyavailable system.Finally, we also compare to the word embed-dings obtained using Mikolov et al?s (2011) re-current neural network based language model.These were pre-trained on Broadcast news data(400M words) using the word2vec tool.8We re-port results with the 640-dimensional embeddingsas they performed best.5 ResultsTable 3 presents our results on the word simi-larity task.
We report correlation coefficients ofmodel predictions against similarity ratings.
As anindicator to how well automatically extracted at-tributes can approach the performance of clean hu-man generated attributes, we also report results ofa distributional model induced from McRae et al?s(2005) norms (see the row labeled McRae in thetable).
Each noun is represented as a vector withdimensions corresponding to attributes elicited byparticipants of the norming study.
Vector compo-nents are set to the (normalized) frequency withwhich participants generated the corresponding at-tribute.
We show results for three models, using allattributes except those classified as visual (T), only7We thank Elia Bruni for providing us with their data.8Available from http://www.rnnlm.org/.# Pair # Pair1 pliers?tongs 11 cello?violin2 cathedral?church 12 cottage?house3 cathedral?chapel 13 horse?pony4 pistol?revolver 14 gun?rifle5 chapel?church 15 cedar?oak6 airplane?helicopter 16 bull?ox7 dagger?sword 17 dress?gown8 pistol?rifle 18 bolts?screws9 cloak?robe 19 salmon?trout10 nylons?trousers 20 oven?stoveTable 4: Word pairs with highest semantic and vi-sual similarity according to SAE model.
Pairs areranked from highest to lowest similarity.visual attributes (V), and all available attributes(V+T).9As baselines, we also report the perfor-mance of a model based solely on textual attributes(which we obtain from Strudel), visual attributes(obtained from our classifiers), and their concate-nation (see row Attributes in Table 3, and columnsT, V, and T+V, respectively).
The automaticallyobtained textual and visual attribute vectors serveas input to SVD, kCCA, and our stacked autoen-coder (SAE).
The third row in the table presentsthree variants of our model trained on textual andvisual attributes only (T and V, respectively) andon both modalities jointly (T+V).Recall that participants were asked to provideratings on two dimensions, namely semantic andvisual similarity.
We would expect the textualmodality to be more dominant when modeling se-mantic similarity and conversely the perceptualmodality to be stronger with respect to visual sim-ilarity.
This is borne out in our unimodal SAEs.The textual SAE correlates better with seman-tic similarity judgments (?
= 0.65) than its vi-sual equivalent (?
= 0.60).
And the visual SAEcorrelates better with visual similarity judgments(?
= 0.60) compared to the textual SAE (?
= 0.52).Interestingly, the bimodal SAE is better than theunimodal variants on both types of similarity judg-ments, semantic and visual.
This suggests thatboth modalities contribute complementary infor-mation and that the SAE model is able to extracta shared representation which improves general-ization performance across tasks by learning them9Classification of attributes into categories is provided byMcRae et al (2005) in their dataset.728Models T V T+VMcRae 0.52 0.31 0.42Attributes 0.35 0.37 0.33SAE 0.36 0.35 0.43SVD ?
?
0.39kCCA ?
?
0.37Bruni ?
?
0.34RNN-640 0.32 ?
?Table 5: F-score results on concept categorization.jointly.
The bimodal autoencoder (SAE, T+V)outperforms all other bimodal models on both sim-ilarity tasks.
It yields a correlation coefficientof ?
= 0.70 on semantic similarity and ?
= 0.64 onvisual similarity.
Human agreement on the formertask is 0.76 and 0.63 on the latter.
Table 4 showsexamples of word pairs with highest semantic andvisual similarity according to the SAE model.We also observe that simply concatenatingtextual and visual attributes (Attributes, T+V)performs competitively with SVD and betterthan kCCA.
This indicates that the attribute-basedrepresentation is a powerful predictor on its own.Interestingly, both Bruni et al (2013) and Mikolovet al (2011) which do not make use of attributesare out-performed by all other attribute-based sys-tems (see columns T and T+V in Table 3).Our results on the categorization task are givenin Table 5.
In this task, simple concatenation of vi-sual and textual attributes does not yield improvedperformance over the individual modalities (seerow Attributes in Table 5).
In contrast, all bimodalmodels (SVD, kCCA, and SAE) are better thantheir unimodal equivalents and RNN-640.
TheSAE outperforms both kCCA and SVD by a largemargin delivering clustering performance similarto the McRae et al?s (2005) norms.
Table 6 showsexamples of clusters produced by Chinese Whis-pers when using vector representations providedby the SAE model.In sum, our experiments show that the bi-modal SAE model delivers superior performanceacross the board when compared against competi-tive baselines and related models.
It is interestingto note that the unimodal SAEs are in most casesbetter than the raw textual or visual attributes.This indicates that higher level embeddings maybe beneficial to NLP tasks in general, not only tothose requiring multimodal information.STICK-LIKE UTENSILS baton, ladle, peg, spatula,spoonRELIGIOUS BUILDINGS cathedral, chapel, churchWIND INSTRUMENTS clarinet, flute, saxophone, trom-bone, trumpet, tubaAXES axe, hatchet, machete, toma-hawkFURNITURE W/ LEGS bed, bench, chair, couch, desk,rocker, sofa, stool, tableFURNITURE W/O LEGS bookcase, bureau, cabinet,closet, cupboard, dishwasher,dresserLIGHTINGS candle, chandelier, lamp,lanternENTRY POINTS door, elevator, gateUNGULATES bison, buffalo, bull, calf, camel,cow, donkey, elephant, goat,horse, lamb, ox, pig, pony,sheepBIRDS crow, dove, eagle, falcon, hawk,ostrich, owl, penguin, pigeon,raven, stork, vulture, wood-peckerTable 6: Examples of clusters produced by CWusing the representations obtained from the SAEmodel.6 ConclusionsIn this paper, we presented a model that usesstacked autoencoders to learn grounded meaningrepresentations by simultaneously combining tex-tual and visual modalities.
The two modalities areencoded as vectors of natural language attributesand are obtained automatically from decoupledtext and image data.
To the best of our knowl-edge, our model is novel in its use of attribute-based input in a deep neural network.
Experimen-tal results in two tasks, namely simulation of wordsimilarity and word categorization, show that ourmodel outperforms competitive baselines and re-lated models trained on the same attribute-basedinput.
Our evaluation also reveals that the bimodalmodels are superior to their unimodal counterpartsand that higher-level unimodal representations arebetter than the raw input.
In the future, we wouldlike to apply our model to other tasks, such as im-age and text retrieval (Hodosh et al, 2013; Socheret al, 2013b), zero-shot learning (Socher et al,2013a), and word learning (Yu and Ballard, 2007).Acknowledgment We would like to thank Vit-torio Ferrari, Iain Murray and members of theILCC at the School of Informatics for their valu-able feedback.
We acknowledge the support ofEPSRC through project grant EP/I037415/1.729ReferencesAgirre, Eneko and Aitor Soroa.
2007.
SemEval-2007 Task 02: Evaluating Word Sense Induc-tion and Discrimination Systems.
In Proceed-ings of the Fourth International Workshop onSemantic Evaluations.
Prague, Czech Republic,pages 7?12.Andrews, M., G. Vigliocco, and D. Vinson.
2009.Integrating Experiential and Distributional Datato Learn Semantic Representations.
Psycholog-ical Review 116(3):463?498.Baroni, M., B. Murphy, E. Barbu, and M. Poe-sio.
2010.
Strudel: A Corpus-Based SemanticModel Based on Properties and Types.
Cogni-tive Science 34(2):222?254.Barsalou, Lawrence W. 2008.
Grounded Cogni-tion.
Annual Review of Psychology 59:617?845.Bengio, Y., P. Lamblin, D. Popovici, andH.
Larochelle.
2007.
Greedy Layer-Wise Train-ing of Deep Networks.
In Bernhard Sch?olkopf,John Platt, and Thomas Hoffman, editors, Ad-vances in Neural Information Processing Sys-tems 19.
MIT Press, pages 153?160.Bengio, Yoshua.
2009.
Learning Deep Architec-tures for AI.
Foundations and Trends in Ma-chine Learning 2(1):1?127.Biemann, Chris.
2006.
Chinese Whispers ?
an Ef-ficient Graph Clustering Algorithm and its Ap-plication to Natural Language Processing Prob-lems.
In Proceedings of TextGraphs: the 1stWorkshop on Graph Based Methods for Natu-ral Language Processing.
New York, NY, pages73?80.Blei, D. M., A. Y. Ng, and M. I. Jordan.
2003.Latent Dirichlet Allocation.
Journal of MachineLearning Research 3:993?1022.Bruni, E., G. Boleda, M. Baroni, and N. Tran.2012a.
Distributional Semantics in Technicolor.In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics.Jeju Island, Korea, pages 136?145.Bruni, E., U. Bordignon, A. Liska, J. Uijlings, andI.
Sergienya.
2013.
Vsem: An open library forvisual semantics representation.
In Proceedingsof the 51st Annual Meeting of the Associationfor Computational Linguistics: System Demon-strations.
Sofia, Bulgaria, pages 187?192.Bruni, E., N. Tran, and M. Baroni.
2014.
Multi-modal distributional semantics.
J. Artif.
Intell.Res.
(JAIR) 49:1?47.Bruni, E., J. Uijlings, M. Baroni, and N. Sebe.2012b.
Distributional Semantics with Eyes: Us-ing Image Analysis to Improve ComputationalRepresentations of Word Meaning.
In Proceed-ings of the 20th ACM International Conferenceon Multimedia.
Nara, Japan, pages 1219?1228.Collobert, R., J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
NaturalLanguage Processing (almost) from Scratch.Journal of Machine Learning Research12:2493?2537.Deng, J., W. Dong, R. Socher, L. Li, K. Li, andL.
Fei-Fei.
2009.
ImageNet: A Large-ScaleHierarchical Image Database.
In Proceedingsof the IEEE Computer Society Conference onComputer Vision and Pattern Recognition.
Mi-ami, Florida, pages 248?255.Farhadi, A., I. Endres, D. Hoiem, and D. Forsyth.2009.
Describing Objects by their Attributes.In Proceedings of the IEEE Computer Soci-ety Conference on Computer Vision and Pat-tern Recognition.
Miami Beach, Florida, pages1778?1785.Feng, Fangxiang, Ruifan Li, and Xiaojie Wang.2013.
Constructing Hierarchical Image-tags Bi-modal Representations for Word Tags Alter-native Choice.
In Proceedings of the ICML2013 Workshop on Challenges in Representa-tion Learning.
Atlanta, Georgia.Feng, Yansong and Mirella Lapata.
2010.
VisualInformation in Semantic Representation.
In Hu-man Language Technologies: The 2010 AnnualConference of the North American Chapter ofthe Association for Computational Linguistics.Los Angeles, California, pages 91?99.Finkelstein, L., E. Gabrilovich, Y. Matias,E.
Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.2002.
Placing Search in Context: The ConceptRevisited.
ACM Transactions on InformationSystems 20(1):116?131.Fountain, Trevor and Mirella Lapata.
2010.
Mean-ing Representation in Natural Language Cat-egorization.
In Proceedings of the 31st An-nual Conference of the Cognitive Science Soci-ety.
Amsterdam, The Netherlands, pages 1916?1921.730Golub, Gene and Christian Reinsch.
1970.
Sin-gular Value Decomposition and Least SquaresSolutions.
Numerische Mathematik 14(5):403?420.Griffiths, T. L., M. Steyvers, and J.
B. Tenenbaum.2007.
Topics in Semantic Representation.
Psy-chological Review 114(2):211?244.Hardoon, D. R., S. R. Szedmak, and J. R. Shawe-Taylor.
2004.
Canonical Correlation Analy-sis: An Overview with Application to LearningMethods.
Neural Computation 16(12):2639?2664.Harris, Zellig.
1970.
Distributional Structure.
InPapers in Structural and Transformational Lin-guistics, pages 775?794.Hinton, Geoffrey E. and Ruslan R. Salakhutdinov.2006.
Reducing the Dimensionality of Datawith Neural Networks.
Science 313(5786):504?507.Hodosh, Micah, Peter Young, and Julia Hocken-maier.
2013.
Framing Image Description as aRanking Task: Data, Models and EvaluationMetrics.
Journal of Artificial Intelligence Re-search 47:853?899.Huang, Jing and Brian Kingsbury.
2013.
Audio-visual Deep Learning for Noise Robust SpeechRecognition.
In Proceedings of the 38th Inter-national Conference on Acoustics, Speech, andSignal Processing.
Vancouver, Canada, pages7596?7599.Jones, R., B. Rey, O. Madani, and W. Greiner.2006.
Generating Query Substititions.
In Pro-ceedings of the 15th International Conferenceon the World-Wide Web.
Edinburgh, Scotland,pages 387?396.Landau, B., L. Smith, and S. Jones.
1998.
ObjectPerception and Object Naming in Early Devel-opment.
Trends in Cognitive Science 27:19?24.Landauer, Thomas and Susan T. Dumais.
1997.
ASolution to Plato?s Problem: the Latent Seman-tic Analysis Theory of Acquisition, Induction,and Representation of Knowledge.
Psychologi-cal Review 104(2):211?240.Lowe, D. 2004.
Distinctive Image Features fromScale-invariant Keypoints.
International Jour-nal of Computer Vision 60(2):91?110.Manning, C. D., P. Raghavan, and H. Sch?utze.2008.
Introduction to Information Retrieval.Cambridge University Press, New York, NY.McRae, K., G. S. Cree, M. S. Seidenberg, andC.
McNorgan.
2005.
Semantic Feature Pro-duction Norms for a Large Set of Living andNonliving Things.
Behavior Research Methods37(4):547?559.Mikolov, T., S. Kombrink, L. Burget, J.?Cernock?y,and S. Khudanpur.
2011.
Extensions of Recur-rent Neural Network Language Model.
In Pro-ceedings of the 2011 IEEE International Con-ference on Acoustics, Speech, and Signal Pro-cessing.
Prague, Czech Republic, pages 5528?5531.Mikolov, T., Wen-tau Yih, and G. Zweig.
2013.Linguistic Regularities in Continuous SpaceWord Representations.
In Proceedings of the2013 Conference of the North American Chap-ter of the Association for Computational Lin-guistics: Human Language Technologies.
At-lanta, Georgia, pages 746?751.Miller, George A. and Walter G. Charles.
1991.Contextual Correlates of Semantic Similarity.Language and Cognitive Processes 6(1).Nelson, D. L., C. L. McEvoy, and T. A.Schreiber.
1998.
The University of SouthFlorida Word Association, Rhyme, and WordFragment Norms.Ngiam, Jiquan, Aditya Khosla, Mingyu Kim,Juhan Nam, Honglak Lee, and Andrew Ng.2011.
Multimodal Deep Learning.
In Pro-ceedings of the 28th International Conferenceon Machine Learning.
Bellevue, Washington,pages 689?696.Patwardhan, Siddharth and Ted Pedersen.
2006.Using WordNet-based Context Vectors to Es-timate the Semantic Relatedness of Concepts.In Proceedings of the EACL 2006 Workshopon Making Sense of Sense: Bringing Compu-tational Linguistics and Psycholinguistics To-gether.
Trento, Italy, pages 1?8.Ranzato, Marc?Aurelio and Martin Szummer.2008.
Semi-supervised Learning of Com-pact Document Representations with Deep Net-works.
In Proceedings of the 25th InternationalConference on Machine Learning.
Helsinki,Finland, pages 792?799.Regier, Terry.
1996.
The Human Semantic Poten-tial.
MIT Press, Cambridge, Massachusetts.Roller, Stephen and Sabine Schulte im Walde.2013.
A Multimodal LDA Model integrating731Textual, Cognitive and Visual Modalities.
InProceedings of the 2013 Conference on Empir-ical Methods in Natural Language Processing.Seattle, Washington, pages 1146?1157.Sebastiani, Fabrizio.
2002.
Machine Learning inAutomated Text Categorization.
ACM Comput-ing Surveys 34:1?47.Silberer, C., V. Ferrari, and M. Lapata.
2013.
Mod-els of Semantic Representation with Visual At-tributes.
In Proceedings of the 51st AnnualMeeting of the Association for ComputationalLinguistics.
Sofia, Bulgaria, pages 572?582.Silberer, Carina and Mirella Lapata.
2012.Grounded Models of Semantic Representation.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Pro-cessing and Computational Natural LanguageLearning.
Jeju Island, Korea, pages 1423?1433.Socher, R., M. Ganjoo, C. D. Manning, and A. Y.Ng.
2013a.
Zero-shot learning through cross-modal transfer.
In Advances in Neural Informa-tion Processing Systems 26, pages 935?943.Socher, R., Quoc V. Le, C. D. Manning, and A. Y.Ng.
2013b.
Grounded Compositional Seman-tics for Finding and Describing Images withSentences.
In Proceedings of the NIPS DeepLearning Workshop.Socher, R., J. Pennington, E. H. Huang, A. Y. Ng,and C. D. Manning.
2011.
Semi-Supervised Re-cursive Autoencoders for Predicting SentimentDistributions.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural Lan-guage Processing.
Edinburgh, Scotland, pages151?161.Srivastava, Nitish and Ruslan Salakhutdinov.2012.
Multimodal Learning with Deep Boltz-mann Machines.
In Advances in Neural In-formation Processing Systems 25, pages 2231?2239.Steyvers, Mark.
2010.
Combining Feature Normsand Text Data with Topic Models.
Acta Psycho-logica 133(3):234?342.Szumlanski, S. R., F. Gomez, and V. K. Sims.2013.
A New Set of Norms for Semantic Re-latedness Measures.
In Proceedings of the 51stAnnual Meeting of the Association for Compu-tational Linguistics.
Sofia, Bulgaria, pages 890?895.Turney, Peter D. and Patrick Pantel.
2010.
FromFrequency to Meaning: Vector Space Modelsof Semantics.
Journal of Artificial IntelligenceResearch 37(1):141?188.Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio,and P. Manzagol.
2010.
Stacked Denoising Au-toencoders: Learning Useful Representations ina Deep Network with a Local Denoising Cri-terion.
Journal of Machine Learning Research11:3371?3408.von Ahn, Luis and Laura Dabbish.
2004.
LabelingImages with a Computer Game.
In Proceedingsof the SIGCHI Conference on Human Factorsin Computing Systems.
Vienna, Austria, pages319?326.Wu, Pengcheng, Steven C. H. Hoi, Hao Xia, PeilinZhao, Dayong Wang, and Chunyan Miao.
2013.Online Multimodal Deep Similarity Learningwith Application to Image Retrieval.
In Pro-ceedings of the 21st ACM International Con-ference on Multimedia.
Barcelona, Spain, pages153?162.Yih, Wen-tau, Ming-Wei Chang, ChristopherMeek, and Andrzej Pastusiak.
2013.
QuestionAnswering Using Enhanced Lexical SemanticModels.
In Proceedings of the 51st AnnualMeeting of the Association for ComputationalLinguistics.
Sofia, Bulgaria, pages 1744?1753.Yu, C. and D. H. Ballard.
2007.
A Unified Modelof Early Word Learning Integrating Statisticaland Social Cues.
Neurocomputing 70:2149?2165.732
