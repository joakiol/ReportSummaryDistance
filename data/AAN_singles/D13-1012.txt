Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 113?123,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsModeling Scientific Impact with Topical Influence RegressionJames Foulds Padhraic SmythDepartment of Computer ScienceUniversity of California, Irvine{jfoulds, smyth}@ics.uci.eduAbstractWhen reviewing scientific literature, it wouldbe useful to have automatic tools that iden-tify the most influential scientific articles aswell as how ideas propagate between articles.In this context, this paper introduces topicalinfluence, a quantitative measure of the ex-tent to which an article tends to spread itstopics to the articles that cite it.
Given thetext of the articles and their citation graph, weshow how to learn a probabilistic model to re-cover both the degree of topical influence ofeach article and the influence relationships be-tween articles.
Experimental results on cor-pora from two well-known computer scienceconferences are used to illustrate and validatethe proposed approach.1 IntroductionScientific articles are not created equal.
Some ar-ticles generate entire disciplines or sub-disciplinesof research, or revolutionize how we think abouta problem, while others contribute relatively little.When we are first introduced to a new area of scien-tific study, it would be useful to automatically findthe most important articles, and the relationships ofinfluence between articles.
Understanding the im-pact of scientific work is also crucial for hiring deci-sions, allocation of funding, university rankings andother tasks that involve the assessment of scientificmerit.
If scientific works stand on the shoulders ofgiants, we would like to be able to find the giants.The importance of a scientific work has previ-ously been measured chiefly through metrics derivedfrom citation counts, such as impact factors.
How-ever, citation counts are not the whole story.
Manycitations are made in passing, are relevant to onlyone section of an article, or make no impact on awork but are referenced out of ?politeness, policyor piety?
(Ziman, 1968).
In reality, scientific impacthas many dimensions.
Some articles are importantbecause they describe scientific discoveries that alterour understanding of the world, while some developessential tools and techniques which facilitate futureresearch.
Other articles are influential because theyintroduce the seeds of new ideas, which in turn in-spire many other articles.In this work we introduce topical influence, aquantitative metric for measuring the latter type ofscientific influence, defined in the context of an un-supervised generative model for scientific corpora.The model posits that articles ?coerce?
the articlesthat cite them into having similar topical content tothem.
Thus, articles with higher topical influencehave a larger effect on the topics of the articles thatcite them.
We model this influence mechanism viaa regression on the parameters of the Dirichlet priorover topics in an LDA-style topic model.
We showhow the models can be used to recover meaningfulinfluence scores, both for articles and for specific ci-tations.
By looking not just at the citation graph butalso taking into account the content of the articles,topical influence can provide a better picture of sci-entific impact than simple citation counts.2 BackgroundBibliometrics, the quantitative study of scientific lit-erature, has a long history.
One example of a widely-used bibliometric measure of interest is the impactfactor of a publication venue for a given year, de-fined to be the average number of times articles from113that venue, published in the previous two years, werecited in that year.
However, the quality of articlesin a given publication venue can vary wildly, and itis difficult to compare impact factors between dif-ferent disciplines of study.
The number of cita-tions an article receives is an indication of impor-tance, but this is confounded by the unknown func-tion of each citation.
Measures of importance suchas PageRank (Brin and Page, 1998) can be derivedrecursively from the citation graph.
Such graph-based measures do not in general make use of thetextual content of the articles, although it is possibleto apply them to graphs where the edges between ar-ticles are determined based on the similarity of theircontent instead of the citation graph (Lin, 2008).A variety of methods have previously been pro-posed for analyzing text and citation links together,such as modeling connections between words andcitations Cohn and Hofmann (2001), classifying ci-tation function (Teufel et al 2006), and jointlymodeling citation links and document content(Chang and Blei, 2009).
However, these methods donot directly measure article importance or influencerelationships between articles given their citations.More closely related to the present work,Dietz et al(2007) proposed the citation influencemodel (CIM).
Building on the latent Dirichlet allocation (LDA) framework, CIM assumes that eachword is drawn by first selecting either (a) the distri-bution over topics of a cited article (with probabilityproportional to the influence weight of that articleon the present article) or (b) a novel topic distribu-tion, and drawing a topic from the selected distribu-tion, then finally drawing the word from the chosentopic.1 In their approach, every word is assigned anextra latent variable, namely the cited article whosetopic distribution the topic was drawn from.
For themodel proposed in this paper, we do not need to in-troduce these additional latent variables, which leadsto a simpler latent representation and fewer variablesto sample during inference.
Dietz et al(2007) alsoassume that the citation graph is bipartite, consist-ing of one set of citing articles and one set of citedarticles?in contrast, our proposed models can han-dle arbitrary citation graphs in the form of directed1A somewhat similar model was also proposed byHe et al(2009)acyclic graphs (DAGs).
While both the CIM and ourapproach can identify the influence of specific cita-tions between articles, our model can also infer howinfluential each article is overall, and provides a flex-ible modeling framework which can handle differentassumptions about influence.Another related method is due toShaparenko and Joachims (2009), who proposea mixture modeling approach for the detection ofnovel text content.
Nallapati et al(2011) intro-duced TopicFlow, a PLSA-based model for the flowof topics in a document network.
In their model,citing articles ?vote?
on each cited article?s topicdistribution in retrospect, via a network flow model.Since this voting occurs in time-reversed order, itdoes not describe an influence mechanism and isnot a generative model that can simulate or predictnew documents.Finally, the document influence model ofGerrish and Blei (2010) can be viewed as orthogo-nal to this work, in that it models the impact of doc-uments on topics over time (specifically, how topicschange over time) rather than how articles influencethe specific articles that cite them.3 Topical Influence RegressionScientific research is seldom performed in a vacuum.New research builds on the research that came be-fore it.
Although there are many aspects by whichthe importance of a scientific article can be judged,in this work we are interested in the extent to which agiven article has or will have subsequent articles thatbuild upon it or are otherwise inspired by its ideas.We begin by defining topical influence, a quantita-tive measure for this type of influence.3.1 Topical InfluenceIt is not immediately obvious how one might quan-tify such a notion of ?idea-based?
influence.
How-ever, the mechanism used in the scientific commu-nity for giving credit to prior work is citation.
Thepresence of a citation from article b to article a there-fore indicates that article bmay have been influencedby the ideas in article a, to some unknown extent.We hypothesize that the extent of this influence man-ifests itself in the language of b.
Using latent Dirich-let alcation (LDA) topics as a concrete proxy for114the vague notion of ?ideas?, we define the topicalinfluence of a to be the extent to which article acoerces the documents which cite it to have simi-lar topic distributions to it.
Topical influence will bemade precise in the context of a generative model forscientific corpora, conditioned on the citation graph,called topical influence regression (TIR).The proposed model extends the LDA frameworkof Blei et al(2003).
In LDA, each word w(d)i ofeach document d is assigned to one of K latent top-ics, z(d)i .
Each topic ?
(k) is a discrete distributionover words.
Document d has a distribution over top-ics ?
(d), which can be viewed as a ?location in topicspace?
summarizing its thematic content.
The ?
(d)?shave a Dirichlet prior distribution with parameters?
= [?1, ?2, .
.
.
, ?K ]?.
Although the ?k?s are oftenset to be equal, representing a relatively uninforma-tive prior over the ?
?s, a unique ?
(d) for each doc-ument can also be used to encode prior informationsuch as the effect of other variables on the topicsof that document (Mimno and McCallum, 2008).
Inour case, we want to model the influence that a docu-ment has on the topic distributions of the documentsthat cite it.
A natural way to encode such influence,then, is to allow documents to affect the value of ?
(d)for each document d that cites them.Accordingly, we model each article d as havinga latent, non-negative ?topical influence?
value l(d).Let n(d) be number of words in article d, n(d)k be thenumber of words assigned to topic k, and let C(d) bethe set of articles that d cites.
We model ?
(d) as?
(d) =?c?C(d)l(c)z?
(c) + ?
, (1)where z?
(c) = 1n(c)[n(c)1 , .
.
.
, n(c)K ]?
is the normalizedhistogram of topic counts for document c, and ?
isa constant for smoothing.
Since the z?
(c)?s sum toone, the topical influence l(c) of article c can be in-terpreted as the number of words of precision thatit adds to the prior of the topic distributions of eachdocument that cites it.
As we increase l(c), the arti-cles that cite c become more likely to have similartopic proportions to it.
Thus, l(c) encodes the degreeto which article c influences the topics of each of thearticles that cite it.From another perspective, marginalizing out ?
(d),we can view the topic counts (in the standard LDAz(d)iw(d)in(d)?
(d) ?
(d)l(d)Articles that a citesz(a)iw(a)in(d)?
(a) ?
(a)l(a)Article az(d)iw(d)in(d)?
(d) ?
(d)l(d)Articles that cite a?K?
(k)Articles that d citesArticles that cite d?Figure 1: The graphical model for the portion of the TIRmodel connected to article a (the links from the z?s andl?s to the ?
(d)?s are deterministic).model) for document d as being drawn from a Polyaurn scheme with ?
(d)k (possibly fractional) balls ofeach color k ?
{1, .
.
.
,K} initially in the urn.
Foreach word, a ball is drawn randomly from the urnand the topic assignment is determined according toits color k. The ball is replaced in the urn, alongwith a new ball of color k. In our model, for eacharticle c cited by article d we place l(c) balls, withcolors distributed according to z?
(c), into article d?surn initially.
Thus, article d?s topic assignments aremore likely to be similar to those of the more influ-ential articles that it cites.
The total number of ballsthat d added to other articles?
urns,T (d) ,?b:d?C(b)l(d) = l(d)???
{b : d ?
C(b)}???
(2)measures the total impact (in a topical sense) of thearticle.
We refer to this as total topical influence.3.2 Generative Model for Topical InfluenceRegressionThe full assumed generative process for articles inthis model begins with a directed acyclic citationgraph G = {V,E}.
Intuitively, citation graphs aretypically DAGs because articles can normally onlycite articles that precede them in time.
We assumethat G is a DAG so that influence relationships are115consistent with some temporal ordering of the arti-cles, and so that the resulting model is a Bayesiannetwork.
Here, each vertex vi corresponds to an ar-ticle di, edge e = (v1, v2) ?
E IFF d1 is cited by d2,and vertices (articles) are numbered in a topologicalordering with respect to G. Such an ordering ex-ists because G is a DAG.
We model each article d?sword vector w(d) as being generated in topologicalsequence, similarly to LDA but with its prior overtopic distribution being Dirichlet(?
(d)), as given byEquation 1.
Note that each ?
(d) is a function of thetopics of the documents that it cites, parameterizedby their topical influence values.
We therefore callthis model topical influence regression (TIR).The TIR model provides us with topical influ-ence scores for each article, but it does not tell usabout topical influence relationships between spe-cific pairs of cited and citing articles.
To model suchrelationships, we can consider a hierarchical exten-sion to TIR, with edge-wise topical influences l(c,d)for each edge (c, d) of the citation graph, l(c,d) ?TruncGaussian(l(c), ?, l(c,d) ?
0).
In this case,?
(d) =?c?C(d)l(c,d)z?
(c) + ?
.
(3)This hierarchical setup allows us to continue to inferarticle-level topical influences, and provides a mech-anism for sharing statistical strength between influ-ences associated with one cited article.
We shall re-fer to the model with influences on just the nodes (ar-ticles) as TIR, and the hierarchical extension with in-fluences on the edges as TIRE.
The graphical modelfor TIR is given in Figure 1, and the generative pro-cess is detailed in the following pseudocode:?
For each topic k?
Sample the topic ?
(k) ?
Dirichlet(?)?
For each document d, in topological order?
Sample an influence weight,l(d) ?
Exponential(?)?
If using the TIRE model?
For each cited document c ?
C(d)?
Draw edge influence weight,l(c,d) ?TruncGauss(l(c), ?, l(c,d) ?
0)?
Assign a prior over topics via?
(d) =?c?C(d) l(c)z?
(c) + ?
(TIR), or?
(d) =?c?C(d) l(c,d)z?
(c) + ?
(TIRE)?
Sample a distribution over topics,?
(d) ?
Dirichlet(?(d))?
For each word i in document d?
Sample a topicz(d)i ?
Discrete(?(d))?
Sample a wordw(d)i ?
Discrete(?
(z(d)i ))3.3 Relationship to Dirichlet-MultinomialRegressionThe TIR model can be viewed as an adaption of theDirichlet-multinomial regression (DMR) frameworkof Mimno and McCallum (2008) to model topicalinfluence.
DMR also endows each document with itsown unique ?
(d), but with ?
(d)k = exp(x(d)?
?k) be-ing a function of the observed feature vector x(d) pa-rameterized by regression coefficients ?.
The DMRmodel can also be applied to text corpora with ci-tation information, by setting the feature vectors tobe binary indicators of the presence of a citation toeach article.
TIR differs in that the functional formof the regression is parameterized in a way that di-rectly models influence, and also differs in that theregression takes advantage of the content of the citedarticles via their topic assignments.Because an article?s prior over topic distributionsdepends on the topic assignments of the articlesthat it cites, TIR induces a network of dependenciesbetween the topic assignments of the documents.Specifically, if we collapse out ?, the dependenciesbetween the z?s of each document form a Bayesiannetwork whose graph is the citation graph.
In con-trast, DMR treats the documents as conditionally in-dependent given their citations, and does not exploittheir content in the regression.To illustrate this, Figure 2 shows an example ci-tation graph and the resulting Bayesian network.
Inthe figure, an edge in (a) from c to d correspondsto a citation of c by d. Conditioned on the topics,the dependence relationships between z nodes in (b)follow the same structure as the citation graph.4 InferenceWe perform inference using a Markov chainMonte Carlo technique.
We use a col-lapsed Gibbs sampling approach analogous toGriffiths and Steyvers (2004), integrating out ?
and116123465(a)(b)z(1)z(2)z(3)z(4)z(6)z(5)?Kw(1)w(2)w(3)w(4)w5)w(6)Figure 2: (a) An example citation network.
(b) Graphicalmodel for TIR on the example network, collapsing out?
but retaining topics ?.
Influence variables and hyper-parameters not shown for simplicity.?.
The update equation for the topic assignments isPr(z(d)i = k|z?
(d,i), .
.
.)?
(n(d)?
(d,i)k + ?
(d)k )n(w(d)i )?
(d,i)k + ?w(d)in?
(d,i)k +?w ?w??d?:d?C(d?)Polya(z(d?)|?(d?)
: z(d)i = k, z?
(d,i), l)(4)where the nk?s are the counts of the occurrencesof topic k over all of the entries determined by thesuperscript.
The ?
(d, i) superscript indicates ex-cluding the current assignment for z(d)i .
The up-date equation is similar to the update equations ofGriffiths and Steyvers, but with a different ?
foreach document d, and with multiplicative weightsfor each document that cites it.
These weightsPolya(z(d)|?
(d)) are the likelihood for a multivariatePolya (a.k.a.
Dirichlet-multinomial) distribution,Polya(z(d)|?
(d)) =?
(?k ?
(d)k )?
(n(d) +?k ?
(d)k )?k?
(n(d)k + ?
(d)k )?(?
(d)k ).In the case of TIR, in the collapsed model the fullconditional posterior for the topical influence valuesl is Pr(l|z, ?)
?
Pr(z|l)Pr(l|?).
Here, Pr(z|l) =?Dd=1 Polya(z(d)|lC(d) , zC(d)).
The topical influencevalues l can be sampled using Metropolis-Hastingsupdates, or slice sampling.
An alternative is to per-form stochastic EM, optimizing the likelihood orthe posterior probability of l, interleaved within theGibbs sampler, as in Mimno and McCallum (2008)and Wallach (2006).
In experiments on syntheticdata we found that maximum likelihood updates onl, obtained via gradient ascent, resulted in the lowestL1 error from the true l, so we use this strategy forthe experimental results in this paper.
The deriva-tive of the log-likelihood with respect to the topicalinfluence l(a) of article a isdPr(z|l)dl(a) =?d:a?C(d)(?(?k?c?C(d)l(c)z?
(c)k +K?)??(?k?c?C(d)l(c)z?
(c)k +K?
+ n(d)))+?d:a?C(d)K?k=1z?(a)k(?(?c?C(d)l(c)z?
(c)k + ?
+ n(d)k )??(?c?C(d)l(c)z?
(c)k + ?
)),where ?(.)
is the digamma function.
For TIRE,the likelihood decomposes across documents and wecan optimize the incoming edge weights for eachdocument separately.
We havedPr(z(d)|l)dl(a,d) =?(?k?c?C(d)l(c,d)z?
(c)k + K?)??(?k?c?C(d)l(c,d)z?
(c)k + K?
+ n(d))+K?k=1z?(a)k(?(?c?C(d)l(c,d)z?
(c)k + ?
+ n(d)k )??(?c?C(d)l(c,d)z?
(c)k + ?
)).117We optimize the node-level l?s in TIREvia the least squares estimate (LSE),l?
(a) = 1|{d:a?C(d)}|?d:a?C(d) l(a,d).
Althoughthe LSE for the mean of a truncated Gaussian isbiased, it is widely used as it is more robust than theMLE (A?Hearn, 2004).5 Experimental AnalysisIn this section we experimentally investigate theproperties of TIR and TIRE.
We consider two sci-entific corpora: a collection of 3286 of articlesfrom the Association for Computational Linguis-tics (ACL) conference2 (Radev et al 2009) pub-lished between 1987 and 2011, and a corpus of ar-ticles from the Neural Information Processing Sys-tems (NIPS) conference3 containing 1740 articlesfrom 1987 to 1999.
The corpora both containeda small number (53, and 14, respectively) of cita-tion graph loops due to insider knowledge of simul-taneous publications.
Some loops were removedby manual deletion of ?insider knowledge?
edges,and others were removed by deleting edges in theloop uniformly at random.
For computational ef-ficiency, we performed approximate Gibbs updateswhere we drop the multiplicative Polya likelihoodterms in Equation 4.
This corresponds to only trans-mitting influence information downward in the cita-tion DAG, but not transmitting ?reverse influence?information upwards.
Preliminary experiments onsynthetic data indicated that this did not significantlyimpact the ability of the model to recover the topicalinfluence weights.
As one might expect, LDA is al-ready capable of inferring topic distributions whichare good enough to perform the regression on, with-out fully exploiting the additional feedback from theregression.
This algorithm has a similar runningtime to the standard collapsed Gibbs sampler forLDA, as the regression step is not a bottleneck.In all experiments, we set the hyper-parametersto ?
= 0.1, ?
= 0.1 and the ?
parameter for thetruncated Gaussian in TIRE to be 1.
We interleavedregression steps every 10 Gibbs iterations.
For ex-ploratory data analysis experiments the models were2http://clair.eecs.umich.edu/aan/3http://www.arbylon.net/resources.html,published by Gregor Heinrich and based on an earlier collectiondue to Sam Roweis.024681012140 1 2 3 4 5Times Cited by Citing ArticleTopical InfluenceperCitationEdge(words)Figure 3: Topical influence per edge versus number oftimes cited by the citing article (NIPS).
Several articleshad zero in-text citations due to author or dataset errors.trained for 500 burn-in iterations, and the samplesfrom the final iterations were used for the analysis.5.1 Model Validation using MetadataIt is not immediately obvious how to best validate anunsupervised model of citation influence.
Groundtruth is not well-defined and human evaluation re-quires extensive knowledge of the individual papersin the corpora.
With this in mind, we explore howtopical influence scores relate to document meta-data, which serves as a proxy for ground truth.In many cases, if article c is repeatedly cited in thetext of article d it may indicate that d builds heavilyon c. We would therefore expect to see an associa-tion between repeated citations and edge-wise topi-cal influence l(c,d).
For each of the 106 papers in theNIPS corpus with at least three distinct references,we counted the number of repeated citations for themost influential and least influential references ac-cording to the TIRE model (Figure 3).
Overall, the?most influential?
references were cited 171 times inthe text of their citing articles, while the ?least influ-ential?
references were cited 128 times.
Of the 45articles where the counts were not tied, the most in-fluential references had the higher citation counts 33times.
A sign test rejects the null hypothesis that themedian difference in citation counts between leastand most influential references is zero at ?
= 0.05,with p-value ?
5?
10?4.118Self-citations, where at least one author is in com-mon between cited and citing articles, are also infor-mative (Figure 4).
Authors often build upon theirown work, so we would expect self-citations to havehigher edge-wise topical influence on average.
ForACL the mean topical influence for a self citationedge is 2.80 and for a non-self citation is 1.40.
ForNIPS the means are 5.05 (self) and 3.15 (non-self).A two-sample t-test finds these differences are bothsignificant at ?
= 0.05.5.2 Prediction ExperimentsWe also used a document prediction task to explorewhether the posited latent structure is predictivelyuseful.
We selected roughly 10% of the articles ineach corpus (170 and 330 documents for NIPS andACL, respectively) for testing, chosen among the ar-ticles that made at least one citation.
We held outa randomly selected set of 50% of their words andevaluated the log probability of the held out partialdocuments under each model.
This is equivalentto evaluating on a set of new documents with thesame set of references as the held out set.
Evaluationwas performed using annealed importance sampling(Neal, 2001), as in Wallach et al(2009) except weused multiple samples per likelihood computation.The TIR models were compared to LDA andan ?additive?
version of DMR with link function?
(d)k = x(d)?
?k + ?, where the ?s were con-strained to be positive and given an exponentialprior with mean one.
For DMR, binary feature vec-tors encoded the presence or absence of each pos-sible citation.
For each algorithm, we burned infor 250 iterations, then executed 1000 iterations,optimizing topical influence weights/DMR param-eters every 10th iteration.
Held-out log proba-bility scores were computed by performing AISwith every 100th sample, and averaging the re-sults to estimate the posterior predictive probabilityPr(held out article|training set, citations, model).It was found that all of the regression methods hadsuperior predictive performance to LDA on thesecorpora, demonstrating that topical influence haspredictive value (Table 1).
Although DMR per-formed slightly better than TIR predictively, TIRwas competitive despite the fact that it has a factor ofK less regression parameters.
Note that DMR doesnot provide an interpretable notion of influence.5.3 Exploring Topical InfluenceIn this section we explore the inferred topical influ-ence scores l(d), total topical influence scores T (d)and edgewise topical influence scores l(c,d) (recalltheir definitions in Equations 1, 2 and 3, respec-tively).
Table 2 shows the most influential articles inthe ACL corpus, according to citation counts, top-ical influence and total topical influence (the lattertwo inferred with the TIR model).
The most fre-quently cited paper within the ACL corpus, writtenby Papineni et al introduces BLEU, a technique forevaluating machine translation (MT) systems.4 Thispaper is of great importance to the computationallinguistics community because the method that itintroduces is widely used to validate MT systems.However, the BLEU article has a relatively low top-ical influence value of 0.58, consistent with the factthat most of the papers that cite it use the techniqueas part of their methodology but do not build uponits ideas.
We emphasize that topical influence mea-sures a specific dimension of scientific importance,namely the tendency of an article to influence theideas (as mediated by the topics) of citing articles;papers with low topical influence such as the BLEUarticle may be important for other reasons.Ranking papers by their influence weights l(d)(Table 2, middle) has the opposite difficulty to rank-ing by citation counts ?
the papers with the highesttopical influence were typically cited only once, bythe same authors.
This makes sense, given what themodel is designed to do.
The lone citing papers werecertainly topically influenced by these articles.A more useful metric, however, is the total top-ical influence T (d) (the bottom sub-table in Table2).
This is the total number of words of prior con-centration, summed over all of its citers, that thearticle has contributed, and is a measure of the to-tal corpus-wide topical influence of the paper.
Thismetric ranks the BLEU paper at 5th place, downfrom 1st place by citation count.
The ACL paperwith the highest total topical influence, by DavidChiang, won the ACL best paper award in 2005.The behavior of the different metrics is echoedin the NIPS corpus (Table 3).
The mostcited paper, ?Handwritten Digit Recognition,?
by4Citations within the corpora are of course only a small frac-tion of the total set of citations for many of these papers.1190510152025Non?Self Citations Self CitationsTopical InfluencePerCitationEdge(words)051015Non?Self Citations Self CitationsTopical InfluencePerCitationEdge(words)Figure 4: Topical influence for self and non-self citation edges.
Left: ACL.
Right: NIPS.ACL NIPSWins Losses Average Wins Losses AverageImprovement ImprovementTIR 297 33 65.7 150 20 38.2TIRE 276 54 63.0 148 22 38.7DMR 302 28 79.1 157 13 48.4Table 1: Wins, losses and average improvement for log probabilities of held-out articles, versus LDA.
Each ?Win?corresponds to the model assigning a higher log probability score for the test portion of a held-out document than LDAassigned to that document.Le Cun et al(1990), is an early successful applica-tion of neural networks.
The paper does not in-troduce novel models or algorithms, but rather, inthe authors?
words, ?show[s] that large back propa-gation (BP) networks can be applied to real imagerecognition problems.?
Thus, although it is has animportant role as a landmark neural network successstory, it does not score highly in terms of topical in-fluence.
This paper is ranked 13th according to totaltopical influence, with a score of 1.6.
The top two-ranked papers according to total topical influence,on Gaussian Process Regression and POMDPs re-spectively, were both seminal papers that spawnedlarge bodies of related work.
An interesting case isthe third-ranked paper in the NIPS corpus, by Wanget al on the theory of early stopping.
It is only ref-erenced three times, but has a very high topical in-fluence of 19.3 words.
All three citing papers arealso on the theory of early stopping, and one of thepapers, by Wang and Venkatesh, directly extends atheoretical result of this paper.
Although it is easyto see why this paper scores highly on topical in-fluence, in this case the metric has perhaps over-stated its importance.
A limitation of topical influ-ence is that it can potentially give more credit thanis due when an article is cited by a small number oftopically similar papers, due to overfitting.
This islikely to be an issue for any topic-based approachfor modeling scientific influence.
However, topicshelp to absorb lexical ambiguity and author-specificidiosyncracies, mitigating the problem relative toword-based approaches.Using the TIRE model, we can also look at in-fluence relationships between pairs of articles.
Ta-bles 4 and 5 show the most and least topically influ-ential references, and the most and least influencedciting papers, for three example articles from ACLand NIPS, respectively.
The model correctly assignshigher influence scores along the edges to and fromrelevant documents.
For the ACL papers, the BLEUalgorithm?s article is inferred to have zero topical in-fluence on Chiang?s paper, consistent with its role120Top 5 Articles by Citation Count140 BLEU: a Method for Automatic Evaluation of Machine Translation.
K. Papineni, S. Roukos, T. Ward, W. Zhu.105 Minimum Error Rate Training in Statistical Machine Translation.
F. Och.64 A Hierarchical Phrase-Based Model for Statistical Machine Translation.
D. Chiang.64 Accurate Unlexicalized Parsing.
D. Klein, C. Manning.59 Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.
D. Yarowsky.Top 5 articles by Topical Influence11.38 Refining Event Extraction through Cross-document Inference.
H. Ji, R. Grishman.11.37 Bayesian Learning of Non-compositional Phrases with Synchronous Parsing.
H. Zhang, C. Quirk, R. Moore, D. Gildea.10.48 A Plan Recognition Model for Clarification Subdialogues.
D. Litman, J. Allen.10.38 PCFGs with Syntactic and Prosodic Indicators of Speech Repairs.
J. Hale et al10.30 Referring as Requesting, P. CohenTop 5 Articles by Total Topical Influence111.46 (1.74 ?
64) A Hierarchical Phrase-Based Model for Statistical Machine Translation.
D. Chiang.101.12 (6.74 ?
15) Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.
D. Xiong, Q. Liu, S. Lin.98.56 (5.80 ?
17) A Logical Semantics for Feature Structures.
R. Kasper, W. Rounds.85.15 (2.18 ?
39) Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.
F. Och, H. Ney81.82 (0.58 ?
140) BLEU: a Method for Automatic Evaluation of Machine Translation, K. Papineni, S. Roukos, T. Ward, and W. Zhu.Table 2: Most influential articles in the ACL Conference corpus, according to citation counts (top), topical influencel(d) inferred by TIR (middle), and total topical influence T (d) inferred by TIR (bottom).
For total topical influence,the breakdown of T (d) = l(d)?
citation count is shown in parentheses.Top 5 Articles by Citation Count26 Handwritten Digit Recognition with a Back-Propagation Network.
Y.
Le Cun, et al19 Optimal Brain Damage.
Y.
Le Cun, J. Denker, S. Solla.17 A New Learning Algorithm for Blind Signal Separation.
S. Amari, A. Cichocki, H. Yang.17 Efficient Pattern Recognition Using a New Transformation Distance.
P. Simard, Y.
Le Cun, J. Denker.14 The Cascade-Correlation Learning Architecture.
S. Fahlman, C. Lebiere.Top 5 articles by Topical Influence29.7 Synchronization and Grammatical Inference in an Oscillating Elman Net.
B. Baird, T. Troyer, F. Eeckman.26.3 Learning the Solution to the Aperture Problem for Pattern Motion with a Hebb Rule.
M. Sereno.25.9 ALVINN: An Autonomous Land Vehicle in a Neural Network.
D. Pomerleau.25.1 Some Estimates of Necessary Number of Connections and Hidden Units for Feed-Forward Networks.
A. Kowalczyk.24.7 Complex- Cell Responses Derived from Center-Surround Inputs: The Surprising Power of Intradendritic Computation.B.
Mel, D. Ruderman, K. Archie.Top 5 Articles by Total Topical Influence84.7 (10.6 ?
8) Gaussian Processes for Regression.
C. Williams, C. Rasmussen.63.9 (7.1 ?
9) Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems.
T. Jaakkola, S. Singh, M. Jordan.57.9 (19.3 ?
3) Optimal Stopping and Effective Machine Complexity in Learning.
C. Wang, S. Venkatesh, J. Judd.54.7 (10.9 ?
5) Links Between Markov Models and Multilayer Perceptrons.
H. Bourlard, C. Wellekens.51.2 (3.7 ?
14) The Cascade-Correlation Learning Architecture.
S. Fahlman, C. Lebiere.Table 3: Most influential articles in the NIPS corpus, according to citation counts (top), topical influence l(d) inferredby TIR (middle), and total topical influence T (d) inferred by TIR (bottom).A Hierarchical Phrase-Based Model for Statistical Machine Translation.
D. Chiang.Most influential reference 1.48 Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.
F. Och and H. Ney.Least influential reference 0.00 BLEU: a Method for Automatic Evaluation of Machine Translation.
K. Papineni, S. Roukos, T. Ward, W. Zhu.Most influenced citer 2.54 Toward Smaller, Faster, and Better Hierarchical Phrase-based SMT.
M. Yang, J. Zheng.Least influenced citer 0.60 An Optimal-time Binarization Algorithm for Linear Context-Free Rewriting Systems with Fan-out Two.C.
Gmez-Rodrguez, G. Satta.Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.
D. Yarowsky.Most influential reference 2.52 Subject-dependent Co-occurrence and Word Sense Disambiguation.
J. Guthrie, L. Guthrie, Y. Wilks, H. Aidinejad.Least influential reference 0.53 Word-sense Disambiguation using Statistical Methods.
P. Brown, S. Della Pietra, V. Della Pietra, R. Mercer.Most influenced citer 1.81 Discriminating Image Senses by Clustering with Multimodal Features.
N. Loeff, C. Alm, D. Forsyth.Least influenced citer 0.00 Semi-supervised Convex Training for Dependency Parsing.
Q. Wang, D. Schuurmans, D. Lin.Accurate Unlexicalized Parsing.
D. Klein, C. Manning.Most influential reference 3.87 Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the Penn Treebank.D.
Klein and C. Manning.Least influential reference 0.81 Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars.
J. Eisner, G. Satta.Most influenced citer 1.67 Evaluating the Accuracy of an Unlexicalized Statistical Parser on the PARC DepBank.
T. Briscoe, J. Carroll.Least influenced citer 0.00 Finding Contradictions in Text.
M. de Marneffe, A. Rafferty, C. Manning.Table 4: Least and most influential references and citers, and the influence weights along these edges, inferred by theTIRE model for three example ACL articles.121Feudal Reinforcement Learning.
P. Dayan, G. HintonMost influential reference 5.47 Memory-based Reinforcement Learning: Efficient Computation with Prioritized Sweeping.
A. Moore, C. Atkeson.Least influential reference 0.00 A Delay-Line Based Motion Detection Chip.
T. Horiuchi, J. Lazzaro, A. Moore, C. Koch.Most influenced citer 3.36 The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces.
A. Moore.Least influenced citer 1.71 Multi-time Models for Temporally Abstract Planning.
D. Precup, R. Sutton.Optimal Brain Damage.
Y.
Le Cun, J. Denker , S. Solla.Most influential reference 2.82 Comparing Biases for Minimal Network Construction with Back-Propagation.
S. Hanson, L. Pratt.Least influential reference 0.15 Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment.
M. Mozer, P. Smolensky.Most influenced citer 3.08 Structural Risk Minimization for Character Recognition.
I. Guyon, V. Vapnik, B. Boser, L. Bottou, S. Solla.Least influenced citer 0.64 Structural and Behavioral Evolution of Recurrent Networks.
G. Saunders, P. Angeline, J. Pollack.An Input Output HMM Architecture.
Y. Bengio, P. Frasconi.Most influential reference 5.29 Credit Assignment through Time: Alternatives to Backpropagation.
Y. Bengio, P. Frasconi.Least influential reference 0.00 Induction of Multiscale Temporal Structure.
M. MozerMost influenced citer 2.66 Learning Fine Motion by Markov Mixtures of Experts.
M. Meila, M. Jordan.Least influenced citer 1.47 Recursive Estimation of Dynamic Modular RBF Networks.
V. Kadirkamanathan, M. Kadirkamanathan.Table 5: Least and most influential references and citers, and the influence weights along these edges, inferred by theTIRE model for three example NIPS articles.in the paper as an evaluation technique.
The papermost topically influenced by Chiang?s paper, writtenby Yang and Zheng, aims to improve upon the ideasin that paper.
In the NIPS corpus, the article by Ben-gio and Frasconi, on recurrent neural network archi-tectures, extends previous work by the same authors,which is correctly assigned the highest topical influ-ence.
A particularly interesting case is the paper byDayan and Hinton, which is heavily influenced bya paper by Moore, and in turn strongly influencesa later paper by Moore, thus illustrating the inter-play of scientific influence between authors alongthe citation graph.
These three papers were on re-inforcement learning, while the lowest scoring ref-erence and citer were on other subjects.6 Conclusions / DiscussionThis paper introduced the notion of topical influ-ence, a quantitative measure of scientific impactwhich arises from a latent variable model called top-ical influence regression.
The model builds upon theideas of Dirichlet-multinomial regression to encodeinfluence relationships between articles along the ci-tation graph.
By training TIR, we can recover topi-cal influence scores that give us insight into the im-pact of scientific articles.
The model was applied totwo scientific corpora, demonstrating the utility ofthe method both quantitatively and qualitatively.In future work, the proposed framework couldreadily be extended to model other aspects of sci-entific influence, such as the effects of authors andjournals on topical influence, and to exploit the con-text in which citations occur.
From an exploratoryanalysis perspective, it would be instructive to com-pare topical influence trajectories over time for dif-ferent papers.
This could be further facilitated by ex-plicitly modeling the dynamics of each article?s top-ical influence score.
The TIR framework could po-tentially also be applicable to other application do-mains such as modeling how interpersonal influenceaffects the spread of memes via social media.To complement TIR, it would be useful to alsohave systems for identifying articles which are im-portant for alternative reasons, such as providingmethodological tools and/or demonstrating impor-tant facts.
Ultimately a suite of such tools could feedinto a system such as Google Scholar or Citeseer.We envision that this line of work will also be usefulfor building visualization tools to help researchersexplore scientific corpora.AcknowledgmentsSupported by the Intelligence Advanced ResearchProjects Activity (IARPA) via Department of In-terior National Business Center contract numberD11PC20155.
The U.S. government is authorized toreproduce and distribute reprints for Governmentalpurposes notwithstanding any copyright annotationthereon.
Disclaimer: The views and conclusionscontained herein are those of the authors and shouldnot be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed orimplied, of IARPA, DoI/NBC, or the U.S. Govern-ment.122References[A?Hearn2004] B.
A?Hearn.
2004.
A restricted max-imum likelihood estimator for truncated height sam-ples.
Economics & Human Biology, 2(1):5?19.
[Blei et al003] D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.2003.
Latent Dirichlet alcation.
The Journal of Ma-chine Learning Research, 3:993?1022.
[Brin and Page1998] S. Brin and L. Page.
1998.
Theanatomy of a large-scale hypertextual web search en-gine.
Computer networks and ISDN systems, 30(1-7):107?117.
[Chang and Blei2009] J. Chang and D. Blei.
2009.
Rela-tional topic models for document networks.
In Artifi-cial Intelligence and Statistics, pages 81?88.
[Cohn and Hofmann2001] D. Cohn and T. Hofmann.2001.
The missing link-a probabilistic model of docu-ment content and hypertext connectivity.
In Advancesin Neural Information Processing Systems, pages 430?436.
[Dietz et al007] L. Dietz, S. Bickel, and T. Scheffer.2007.
Unsupervised prediction of citation influences.In Proceedings of the 24th International Conferenceon Machine Learning, pages 233?240.
[Gerrish and Blei2010] S. Gerrish and D.M.
Blei.
2010.A language-based approach to measuring scholarlyimpact.
In Proceedings of the 26th International Con-ference on Machine Learning, pages 375?382.
[Griffiths and Steyvers2004] T.L.
Griffiths andM.
Steyvers.
2004.
Finding scientific topics.Proceedings of the National Academy of Sciences ofthe United States of America, 101(Suppl 1):5228.
[He et al009] Q.
He, B. Chen, J. Pei, B. Qiu, P. Mitra,and L. Giles.
2009.
Detecting topic evolution in sci-entific literature: how can citations help?
In Proceed-ings of the 18th ACM Conference on Information andKnowledge Management, pages 957?966.
ACM.
[Le Cun et al990] B.B.
Le Cun, JS Denker, D. Hender-son, RE Howard, W. Hubbard, and LD Jackel.
1990.Handwritten digit recognition with a back-propagationnetwork.
In Advances in Neural Information Process-ing Systems, pages 396?404.
[Lin2008] J. Lin.
2008.
Pagerank without hyper-links: Reranking with pubmed related article networksfor biomedical text retrieval.
BMC bioinformatics,9(1):270.
[Mimno and McCallum2008] D. Mimno and A. McCal-lum.
2008.
Topic models conditioned on arbitraryfeatures with Dirichlet-multinomial regression.
In Un-certainty in Artificial Intelligence, pages 411?418.
[Nallapati et al011] R. Nallapati, D. McFarland, andC.
Manning.
2011.
Topicflow model: Unsupervisedlearning of topic-specific influences of hyperlinkeddocuments.
In International Conference on ArtificialIntelligence and Statistics, pages 543?551.
[Neal2001] R.M.
Neal.
2001.
Annealed importance sam-pling.
Statistics and Computing, 11(2):125?139.
[Radev et al009] D. R. Radev, P. Muthukrishnan, andV.
Qazvinian.
2009.
The ACL anthology network cor-pus.
In Proceedings, ACL Workshop on Natural Lan-guage Processing and Information Retrieval for Digi-tal Libraries, pages 54?61, Singapore.
[Shaparenko and Joachims2009] B. Shaparenko andT.
Joachims.
2009.
Identifying the original con-tribution of a document via language modeling.
InMachine Learning and Knowledge Discovery inDatabases, pages 350?365.
Springer.
[Teufel et al006] S. Teufel, A. Siddharthan, and D. Tid-har.
2006.
Automatic classification of citation func-tion.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 103?110.
Association for Computational Lin-guistics.
[Wallach et al009] H.M. Wallach, I. Murray,R.
Salakhutdinov, and D. Mimno.
2009.
Evalu-ation methods for topic models.
In Proceedings ofthe 26th Annual International Conference on MachineLearning, pages 1105?1112.
ACM.
[Wallach2006] H.M. Wallach.
2006.
Topic modeling:beyond bag-of-words.
In Proceedings of the 23rd In-ternational Conference on Machine Learning, pages977?984.
ACM.
[Ziman1968] J.M.
Ziman.
1968.
Public knowledge:an essay concerning the social dimension of science.Cambridge University Press.123
