Language Independent Text Correction using Finite State AutomataAhmed Hassan?
Sara NoemanIBM Cairo Technology Development CenterGiza, Egypthasanah,noemans,hanyh@eg.ibm.comHany HassanAbstractMany natural language applications, likemachine translation and information extrac-tion, are required to operate on text withspelling errors.
Those spelling mistakeshave to be corrected automatically to avoiddeteriorating the performance of such ap-plications.
In this work, we introduce anovel approach for automatic correction ofspelling mistakes by deploying finite stateautomata to propose candidates correctionswithin a specified edit distance from the mis-spelled word.
After choosing candidate cor-rections, a language model is used to assignscores the candidate corrections and choosebest correction in the given context.
Theproposed approach is language independentand requires only a dictionary and text datafor building a language model.
The ap-proach have been tested on both Arabic andEnglish text and achieved accuracy of 89%.1 IntroductionThe problem of detecting and correcting misspelledwords in text has received great attention due toits importance in several applications like text edit-ing systems, optical character recognition systems,and morphological analysis and tagging (Roche andSchabes, 1995).
Other applications, like machinetranslation and information extraction, operate ontext that might have spelling errors.
The automaticdetection, and correction of spelling erros should beof great help to those applications.The problem of detecting and correcting mis-spelled words in text is usually solved by checkingwhether a word already exists in the dictionary ornot.
If not, we try to extract words from the dictio-nary that are most similar to the word in question.
?Now with the University of Michigan Ann Arbor, has-sanam@umich.eduThose words are reported as candidate correctionsfor the misspelled word.Similarity between the misspelled word and dic-tionary words is measured by the Levenshtein editdistance (Levenshtein, 1966; Wagner and M.Fisher,1974).
The Levenshtein edit distance is usu-ally calculated using a dynamic programming tech-nique with quadratic time complexity (Wagner andM.Fisher, 1974).
Hence, it is not reasonable to com-pare the misspelled word to each word in the dictio-nary while trying to find candidate corrections.The proposed approach uses techniques from fi-nite state theory to detect misspelled words and togenerate a set of candidate corrections for each mis-spelled word.
It also uses a language model to selectthe best correction from the set of candidate correc-tions using the context of the misspelled word.
Us-ing techniques from finite state theory, and avoidingcalculating edit distances makes the approach veryfast and efficient.
The approach is completely lan-guage independent, and can be used with any lan-guage that has a dictionary and text data to buildinga language model.The rest of this paper will proceed as follows.Section 2 will present an overview of related work.Section 3 will discuss the different aspects of theproposed approach.
Section 4 presents a perfor-mance evaluation of the system.
Finally a conclu-sion is presented in section 5.2 Related WorkSeveral solutions were suggested to avoid comput-ing the Levenshtein edit distance while finding can-didate corrections.
Most of those solutions selecta number of dictionary words that are supposed tocontain the correction, and then measure the dis-tance between the misspelled word and all selectedwords.
The most popular of those methods are thesimilarity keys methods (Kukich, 1992; Zobel andDart, 1995; De Beuvron and Trigano, 1995).
In913those methods, the dictionary words are divided intoclasses according to some word features.
The inputword is compared to words in classes that have sim-ilar features only.In addition to the techniques discussed above,other techniques from finite state automata havebeen recently proposed.
(Oflazer, 1996) suggesteda method where all words in a dictionary are treatedas a regular language over an alphabet of letters.
Allthe words are represented by a finite state machineautomaton.
For each garbled input word, an exhaus-tive traversal of the dictionary automaton is initiatedusing a variant of Wagner-Fisher algorithm (Wag-ner and M.Fisher, 1974) to control the traversal ofthe dictionary.
In this approach Levenshtein dis-tance is calculated several times during the traversal.The method carefully traverses the dictionary suchthat the inspection of most of the dictionary statesis avoided.
(Schulz and Mihov, 2002) presents avariant of Oflazers?s approach where the dictionaryis also represented as deterministic finite state au-tomaton.
However, they avoid the computation ofLevenshtein distance during the traversal of the dic-tionary automaton.
In this technique, a finite stateacceptor is constructed for each input word.
Thisacceptor accepts all words that are within an edit dis-tance k from the input word.
The dictionary automa-ton and the Levenshtein-automaton are then tra-versed in parallel to extract candidate corrections forthe misspelled word.
The authors present an algo-rithm that can construct a deterministic Levenshtein-automaton for an arbitrary word of degrees 1, and2 which corresponds to 1 or 2 errors only.
Theysuggest another algorithm that can construct a non-deterministic Levenshtein-automaton for any otherdegree.
They report results using a Levenshtein-automaton of degree 1(i.e.
words having a singleinsertion, substitution, or deletion) only.The method we propose in this work also assumesthat the dictionary is represented as a determinis-tic finite state automaton.
However, we completelyavoid computing the Levenshtein-distance at anystep.
We also avoid reconstructing a Levenshtein-automaton for each input word.
The proposedmethod does not impose any constraints on thebound k, where k is the edit distance between theinput word and the candidate corrections.
The ap-proach can adopt several constraints on which char-Figure 1: An FSM representation of a word listacters can substitute certain other characters.
Thoseconstraints are obtained from a phonetic and spatialconfusion matrix of characters.The purpose of context-dependent error correc-tion is to rank a set of candidate corrections tak-ing the misspelled word context into account.
Anumber of approaches have been proposed to tacklethis problem that use insights from statistical ma-chine learning (Golding and Roth, 1999), lexicalsemantics (Hirst and Budanitsky, 2005), and webcrawls (Ringlstetter et al, 2007).3 Error Detection and Correction in TextUsing FSMsThe approach consists of three main phases: detect-ing misspelled words, generating candidate correc-tions for them, and ranking corrections.
A detaileddescription of each phase is given in the followingsubsections.3.1 Detecting Misspelled WordsThe most direct way for detecting misspelled wordsis to search the dictionary for each word, and reportwords not found in the dictionary.
However, we canmake use of the finite state automaton representationof the dictionary to make this step more efficient.In the proposed method, we build a finite state ma-chine (FSM) that contains a path for each word inthe input string.
This FSM is then composed withthe dictionary FSM.
The result of the compositionis merely the intersection of the words that exist inboth the input string and the dictionary.
If we calcu-lated the difference between the FSM containing allwords and this FSM, we get an FSM with a path for914each misspelled word.
Figure 1 illustrate an FSMthat contain all words in an input string.3.2 Generating Candidate CorrectionsThe task of generating candidate corrections for mis-spelled words can be divided into two sub tasks:Generating a list of words that have edit distanceless than or equal k to the input word, and select-ing a subset of those words that also exist in the dic-tionary.
To accomplish those tasks, we create a sin-gle transducer(Levenshtein-transducer) that is whencomposed with an FSM representing a word, gen-erates all words withing any edit distance k fromthe input word.
After composing the misspelledword with Levenshtein-transducer, we compose theresulting FSM with the dictionary FSM to filter outwords that do not exist in the dictionary.3.2.1 Levenshtein-transducers for primitiveedit distancesTo generate a finite state automaton that con-tain all words within some edit distance to the in-put word, we use a finite state transducer that al-lows editing its input according to the standardLevenshtein-distance primitive operations: substitu-tion, deletion, and insertion.A finite-state transducers (FST) is a a 6-tuple(Q,?1,?2, ?, i, F ), where Q is a set of states, ?1is the input alphabet, ?2 is the output alphabet, i isthe initial state, F ?
Q is a set of final states, and ?is a transition function (Hopcroft and Ullman, 1979;Roche and Shabes, 1997).
A finite state acceptor is aspecial case of an FST that has the same input/outputat each arc.Figure 2 illustrates the Levenshtein-transducer foredit distance 1 over a limited set of vocabulary (a,b,and c).
We can notice that we will stay in state zeroas long as the output is identical to the input.
Onthe other hand we can move from state zero, whichcorresponds to edit distance zero, to state one, whichcorresponds to edit distance one, with three differentways:?
input is mapped to a different output (input isconsumed and a different symbol is emitted)which corresponds to a substitution,?
input is mapped to an epsilon (input is con-sumed and no output emitted) which corre-sponds to a deletion, andFigure 2: A Levenshtein-transducer (edit distance 1)?
an epsilon is mapped to an output (output isemitted without consuming any input) whichcorresponds to an insertion.Once we reach state 1, the only possible transitionsare those that consume a symbol and emit the samesymbol again and hence allowing only one edit op-eration to take place.When we receive a new misspelled word, we rep-resent it with a finite state acceptor that has a singlepath representing the word, and then compose it withthe Levenshtein-transducer.
The result of the com-position is a new FSM that contains all words withedit distance 1 to the input word.3.2.2 Adding transpositionAnother non-primitive edit distance operation thatis frequently seen in misspelled words is transposi-tion.
Transposition is the operation of exchangingthe order of two consecutive symbols (ab ?
ba).Transposition is not a primitive operation becauseit can be represented by other primitive operations.However, this makes it a second degree operation.As transposition occurs frequently in misspelledwords, adding it to the Levenshtein-transducer as asingle editing operation would be of great help.915Figure 3: A Levenshtein-transducer for edit distance1 with transpositionTo add transposition, as a single editing opera-tion, to the Levenshtein-transducer we add arcs be-tween states zero and one that can map any symbolsequence xy to the symbol sequence yx, where x,and y are any two symbols in the vocabulary.
Fig-ure 3 shows the Levenshtein-transducer with degree1 with transposition over a limited vocabulary (a andb).3.2.3 Adding symbol confusion matricesAdding a symbol confusion matrix can help re-duce the number of candidate corrections.
The con-fusion matrix determines for each symbol a set ofsymbols that may have substituted it in the garbledword.
This matrix can be used to reduce the num-ber of candidate corrections if incorporated into theLevenshtein-transducer.
For any symbol x, we addan arc x : y between states zero, and one in the trans-ducer where y ?
Confusion Matrix(x) ratherthan for all symbols y in the vocabulary.The confusion matrix can help adopt the meth-ods to different applications.
For example, we canbuild a confusion matrix for use with optical char-acter recognition error correction that captures er-rors that usually occur with OCRs.
When used witha text editing system, we can use a confusion ma-trix that predicts the confused characters accordingto their phonetic similarity, and their spatial locationon the keyboard.Figure 4: A Levenshtein-transducer for edit distance2 with transposition3.2.4 Using degrees greater than oneTo create a Levenshtein-transducer that can gen-erate all words within edit distance two of the inputword, we create a new state (2) that maps to two editoperations, and repeat all arcs that moves from state0 to state 1 to move from state 1 to state 2.To allow the Levenshtein-transducer of degreetwo to produce words with edit distance 1 and 2 fromthe input word, we mark both state 1, and 2 as finalstates.
We may also favor corrections with loweredit distances by assigning costs to final states, suchthat final states with lower number of edit operationsget lower costs.
A Levenshtein-transducer of degree2 for the limited vocabulary (a and b) is shown infigure 4.3.3 Ranking CorrectionsTo select the best correction from a set of candidatecorrections, we use a language model to assign aprobability to a sequence of words containing thecorrected word.
To get that word sequence, we goback to the context where the misspelled word ap-peared, replace the misspelled word with the candi-date correction, and extract n ngrams containing thecandidate correction word in all possible positionsin the ngram.
We then assign a score to each ngramusing the language model, and assign a score to thecandidate correction that equals the average score ofall ngrams.
Before selecting the best scoring cor-rection, we penalize corrections that resulted fromhigher edit operations to favor corrections with theminimal number of editing operations.916Edit 1/with trans.
Edit 1/no trans.
Edit 2/with trans.
Edit 2 / no trans.word len.
av.
time av.
correcs.
av.
time av.
correcs.
av.
time av.
correcs.
av.
time av.
correcs.3 3.373273 18.769 2.983733 18.197 73.143538 532.637 69.709387 514.1744 3.280419 4.797 2.796275 4.715 67.864291 136.230 66.279842 131.6805 3.321769 1.858 2.637421 1.838 73.718353 33.434 68.695935 32.4616 3.590046 1.283 2.877242 1.277 75.465624 11.489 69.246055 11.2587 3.817453 1.139 2.785156 1.139 78.231015 6.373 72.2057 6.2778 4.073228 1.063 5.593761 1.062 77.096026 4.127 73.361455 4.0669 4.321661 1.036 3.124661 1.036 76.991945 3.122 73.058418 3.09110 4.739503 1.020 3.2084 1.020 75.427416 2.706 72.2143 2.68511 4.892105 1.007 3.405101 1.007 77.045616 2.287 71.293116 2.28112 5.052191 0.993 3.505089 0.993 78.616536 1.910 75.709801 1.90413 5.403557 0.936 3.568391 0.936 81.145124 1.575 78.732955 1.568Table 1: Results for EnglishEdit 1/with trans.
Edit 1/no trans.
Edit 2/with trans.
Edit 2 / no trans.word len.
av.
time av.
correcs.
av.
time av.
correcs.
av.
time av.
correcs.
av.
time av.
correcs.3 5.710543 31.702 4.308018 30.697 83.971263 891.579 75.539547 862.4954 6.033066 12.555 4.036479 12.196 80.481281 308.910 71.042372 296.7765 7.060306 6.265 4.360373 6.162 79.320644 104.661 69.71572 100.4286 9.08935 4.427 4.843784 4.359 79.878962 51.392 74.197127 48.9917 8.469497 3.348 5.419919 3.329 82.231107 24.663 70.681298 23.7818 10.078842 2.503 5.593761 2.492 85.32005 13.586 71.557569 13.2679 10.127946 2.140 6.027077 2.136 83.788916 8.733 76.199034 8.64510 11.04873 1.653 6.259901 1.653 92.671732 6.142 81.007893 6.08911 12.060286 1.130 7.327353 1.129 94.726469 4.103 77.464609 4.08412 13.093397 0.968 7.194902 0.967 95.35985 2.481 82.40306 2.46213 13.925067 0.924 7.740105 0.921 106.66238 1.123 78.966914 1.109Table 2: Results for Arabic4 Experimental Setup4.1 Time PerformanceThe proposed method was implemented in C++ ona 2GHz processor machine under Linux.
We used11,000 words of length 3,4,..., and 13, 1,000 wordfor each word length, that have a single error andcomputed correction candidates.
We report both theaverage correction time, and the average number ofcorrections for each word length.
The experimentwas run twice on different test data, one with con-sidering transposition as primitive operation, and theother without.
We also repeated the experiments foredit distance 2 errors, and also considered the twocases where transposition is considered as a primi-tive operation or not.
Table 1 shows the results foran English dictionary of size 225,400 entry, and Ta-ble 2 shows the results for an Arabic dictionary thathas 526,492. entries.4.2 Auto-correction accuracyTo measure the accuracy of the auto-correction pro-cess, we used a list of 556 words having commonspelling errors of both edit distances 1 and 2.
We puta threshold on the number of characters per word todecide whether it will be considered for edit distance1 or 2 errors.
When using a threshold of 7, the spellengine managed to correct 87% of the words.
Thispercentage raised to 89% when all words were con-sidered for edit distance 2 errors.
The small degra-dation in the performance occured because in 2% ofthe cases, the words were checked for edit distance 1errors although they had edit distance 2 errors.
Fig-ure 6 shows the effect of varying the characters limiton the correction accuracy.Figure 5 shows the effect of varying the weight as-signed to corrections with lower edit distances on theaccuracy.
As indicated in the figure, when we onlyconsider the language model weight, we get accura-cies as low as 79%.
As we favor corrections withlower edit distances the correction accuracy raises,but occasionally starts to decay again when empha-sis on the low edit distance is much larger than thaton the language model weights.Finally, we repeated the experiments but with us-917Figure 5: Effect of increasing lower edit distancefavoring factor on accuracyFigure 6: Effect of increasing Ed1/Ed2 char limitson accuracying a confusion matrix, as 3.2.3.
We found out thatthe average computation time dropped by 78% ( be-low 1 ms for edit distance 1 errors) at the price oflosing only 8% of the correction accuracy.5 ConclusionIn this work, we present a finite state automata basedspelling errors detection and correction method.The new method avoids calculating the edit dis-tances at all steps of the correction process.
It alsoavoids building a Levenshtein-automata for each in-put word.
The method is multilingual and may workfor any language for which we have an electronicdictionary, and a language model to assign probabil-ity to word sequences.
The preliminary experimen-tal results show that the new method achieves goodperformance for both correction time and accuracy.The experiments done in this paper can be extendedin several directions.
First, there is still much roomfor optimizing the code to make it faster especiallythe FST composition process.
Second, we can allowfurther editing operations like splitting and merging.ReferencesFrancois De Bertrand De Beuvron and Philippe Trigano.1995.
Hierarchically coded lexicon with variants.
In-ternational Journal of Pattern Recognition and Artifi-cial Intelligence, 9:145?165.Andrew Golding and Dan Roth.
1999.
A winnow-basedapproach to context-sensitive spelling correction.
Ma-chine learning, 34:107?130.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lexicalcohesion.
Natural Language Engineering, 11:87?111.J.E.
Hopcroft and J.D.
Ullman.
1979.
Introduction to au-tomata theory, languages, and computation.
Reading,Massachusetts: Addison-Wesley.Karen Kukich.
1992.
Techniques for automatically cor-recting words in text.
ACM Computing Surveys, pages377?439.V.I.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions, and reversals.
Soviet Physics- Doklady.Kemal Oflazer.
1996.
Error-tolerant finite-state recog-nition with applications to morphological analysisand spelling correction.
Computational Linguistics,22:73?89.Christoph Ringlstetter, Max Hadersbeck, Klaus U.Schulz, and Stoyan Mihov.
2007.
Text correctionusing domain dependent bigram models from webcrawls.
In Proceedings of the International Joint Con-ference on Artificial Intelligence (IJCAI-2007) Work-shop on Analytics for Noisy Unstructured Text Data.Emmanuel Roche and Yves Schabes.
1995.
Determinis-tic part-of-speech tagging with finite-state transducers.Computational Linguistics, 2:227253.Emmanuel Roche and Yves Shabes.
1997.
Finite-statelanguage processing.
Cambridge, MA, USA: MITPress.Klaus Schulz and Stoyan Mihov.
2002.
Fast string cor-rection with levenshtein-automata.
International Jour-nal of Document Analysis and Recognition (IJDAR),5:67?85.R.A.
Wagner and M.Fisher.
1974.
The string-to-stringcorrection problem.
Journal of the ACM.Justin Zobel and Philip Dart.
1995.
Finding approximatematches in large lexicons.
Software Practice and Ex-perience, 25:331?345.918
