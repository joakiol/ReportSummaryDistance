Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 8: Multi-Way Classificationof Semantic Relations Between Pairs of NominalsIris Hendrickx?
, Su Nam Kim?
, Zornitsa Kozareva?
, Preslav Nakov?
,Diarmuid O?
Se?aghdha?, Sebastian Pado??
, Marco Pennacchiotti?
?,Lorenza Romano?
?, Stan Szpakowicz?
?AbstractWe present a brief overview of the mainchallenges in the extraction of semanticrelations from English text, and discuss theshortcomings of previous data sets and sharedtasks.
This leads us to introduce a newtask, which will be part of SemEval-2010:multi-way classification of mutually exclusivesemantic relations between pairs of commonnominals.
The task is designed to comparedifferent approaches to the problem and toprovide a standard testbed for future research,which can benefit many applications inNatural Language Processing.1 IntroductionThe computational linguistics community has a con-siderable interest in robust knowledge extraction,both as an end in itself and as an intermediate stepin a variety of Natural Language Processing (NLP)applications.
Semantic relations between pairs ofwords are an interesting case of such semanticknowledge.
It can guide the recovery of useful factsabout the world, the interpretation of a sentence, oreven discourse processing.
For example, pears andbowl are connected in a CONTENT-CONTAINER re-lation in the sentence ?The bowl contained apples,?University of Antwerp, iris.hendrickx@ua.ac.be?University of Melbourne, snkim@csse.unimelb.edu.au?University of Alicante, zkozareva@dlsi.ua.es?National University of Singapore, nakov@comp.nus.edu.sg?University of Cambridge, do242@cl.cam.ac.uk?University of Stuttgart, pado@stanford.edu??Yahoo!
Inc., pennacc@yahoo-inc.com?
?Fondazione Bruno Kessler, romano@fbk.eu?
?University of Ottawa and Polish Academy of Sciences,szpak@site.uottawa.capears, and oranges.
?, while ginseng and taste are inan ENTITY-ORIGIN relation in ?The taste is not fromalcohol, but from the ginseng.
?.The automatic recognition of semantic relationscan have many applications, such as informationextraction (IE), document summarization, machinetranslation, or construction of thesauri and seman-tic networks.
It can also facilitate auxiliary taskssuch as word sense disambiguation, language mod-eling, paraphrasing or recognizing textual entail-ment.
For example, semantic network constructioncan benefit from detecting a FUNCTION relation be-tween airplane and transportation in ?the airplaneis used for transportation?
or a PART-WHOLE rela-tion in ?the car has an engine?.
Similarly, all do-mains that require deep understanding of text rela-tions can benefit from knowing the relations that de-scribe events like ACQUISITION between named en-tities in ?Yahoo has made a definitive agreement toacquire Flickr?.In this paper, we focus on the recognition of se-mantic relations between pairs of common nomi-nals.
We present a task which will be part of theSemEval-2010 evaluation exercise and for which weare developing a new benchmark data set.
This dataset and the associated task address three significantproblems encountered in previous work: (1) the def-inition of a suitable set of relations; (2) the incorpo-ration of context; (3) the desire for a realistic exper-imental design.
We outline these issues in Section2.
Section 3 describes the inventory of relations weadopted for the task.
The annotation process, thedesign of the task itself and the evaluation method-ology are presented in Sections 4-6.942 Semantic Relation Classification: Issues2.1 Defining the Relation InventoryA wide variety of relation classification schemes ex-ist in the literature, reflecting the needs and granular-ities of various applications.
Some researchers onlyinvestigate relations between named entities or in-ternal to noun-noun compounds, while others have amore general focus.
Some schemes are specific to adomain such as biomedical text.Rosario and Hearst (2001) classify noun com-pounds from the domain of medicine into 13 classesthat describe the semantic relation between the headnoun and the modifier.
Rosario et al (2002) classifynoun compounds using the MeSH hierarchy and amulti-level hierarchy of semantic relations, with 15classes at the top level.
Stephens et al (2001) pro-pose 17 very specific classes targeting relations be-tween genes.
Nastase and Szpakowicz (2003) ad-dress the problem of classifying noun-modifier rela-tions in general text.
They propose a two-level hier-archy, with 5 classes at the first level and 30 classesat the second one; other researchers (Kim and Bald-win, 2005; Nakov and Hearst, 2008; Nastase et al,2006; Turney, 2005; Turney and Littman, 2005)have used their class scheme and data set.
Moldovanet al (2004) propose a 35-class scheme to classifyrelations in various phrases; the same scheme hasbeen applied to noun compounds and other nounphrases (Girju et al, 2005).
Lapata (2002) presents abinary classification of relations in nominalizations.Pantel and Pennacchiotti (2006) concentrate on fiverelations in an IE-style setting.
In short, there is littleagreement on relation inventories.2.2 The Role of ContextA fundamental question in relation classification iswhether the relations between nominals should beconsidered out of context or in context.
When onelooks at real data, it becomes clear that context doesindeed play a role.
Consider, for example, the nouncompound wood shed : it may refer either to a shedmade of wood, or to a shed of any material used tostore wood.
This ambiguity is likely to be resolvedin particular contexts.
In fact, most NLP applica-tions will want to determine not all possible relationsbetween two words, but rather the relation betweentwo instances in a particular context.
While the in-tegration of context is common in the field of IE (cf.work in the context of ACE1), much of the exist-ing literature on relation extraction considers wordpairs out of context (thus, types rather than tokens).A notable exception is SemEval-2007 Task 4 Clas-sification of Semantic Relations between Nominals(Girju et al, 2007; Girju et al, 2008), the first to of-fer a standard benchmark data set for seven semanticrelations between common nouns in context.2.3 Style of ClassificationThe design of SemEval-2007 Task 4 had an im-portant limitation.
The data set avoided the chal-lenge of defining a single unified standard classifi-cation scheme by creating seven separate trainingand test sets, one for each semantic relation.
Thatmade the relation recognition task on each data seta simple binary (positive / negative) classificationtask.2 Clearly, this does not easily transfer to prac-tical NLP settings, where any relation can hold be-tween a pair of nominals which occur in a sentenceor a discourse.2.4 SummaryWhile there is a substantial amount of work on re-lation extraction, the lack of standardization makesit difficult to compare different approaches.
It isknown from other fields that the availability of stan-dard benchmark data sets can provide a boost to theadvancement of a field.
As a first step, SemEval-2007 Task 4 offered many useful insights into theperformance of different approaches to semantic re-lation classification; it has also motivated follow-up research (Davidov and Rappoport, 2008; Ka-trenko and Adriaans, 2008; Nakov and Hearst, 2008;O?
Se?aghdha and Copestake, 2008).Our objective is to build on the achievements ofSemEval-2007 Task 4 while addressing its short-comings.
In particular, we consider a larger set ofsemantic relations (9 instead of 7), we assume aproper multi-class classification setting, we emulatethe effect of an ?open?
relation inventory by meansof a tenth class OTHER, and we will release to theresearch community a data set with a considerably1http://www.itl.nist.gov/iad/mig/tests/ace/2Although it was not designed for a multi-class set-up, somesubsequent publications tried to use the data sets in that manner.95larger number of examples than SemEval-2007 Task4 or other comparable data sets.
The last point is cru-cial for ensuring the robustness of the performanceestimates for competing systems.3 Designing an Inventory of Semantic Re-lations Between NominalsWe begin by considering the first of the problemslisted above: defining of an inventory of semanticrelations.
Ideally, it should be exhaustive (should al-low the description of relations between any pair ofnominals) and mutually exclusive (each pair of nom-inals in context should map onto only one relation).The literature, however, suggests no such inventorythat could satisfy all needs.
In practice, one alwaysmust decide on a trade-off between these two prop-erties.
For example, the gene-gene relation inven-tory of Stephens et al (2001), with relations like Xphosphorylates Y, arguably allows no overlaps, butis too specific for applications to general text.On the other hand, schemes aimed at exhaus-tiveness tend to run into overlap issues, dueto such fundamental linguistic phenomena asmetaphor (Lakoff, 1987).
For example, in the sen-tence Dark clouds gather over Nepal., the relationbetween dark clouds and Nepal is literally a type ofENTITY-DESTINATION, but in fact it refers to theethnic unrest in Nepal.We seek a pragmatic compromise between thetwo extremes.
We have selected nine relations withsufficiently broad coverage to be of general andpractical interest.
We aim at avoiding ?real?
overlapto the extent that this is possible, but we include twosets of similar relations (ENTITY-ORIGIN/ENTITY-DESTINATION and CONTENT-CONTAINER/COM-PONENT-WHOLE/MEMBER-COLLECTION), whichcan help assess the models?
ability to make suchfine-grained distinctions.3As in Semeval-2007 Task 4, we give ordered two-word names to the relations, where each word de-scribes the role of the corresponding argument.
Thefull list of our nine relations follows4 (the definitionswe show here are intended to be indicative ratherthan complete):3COMPONENT-WHOLE and MEMBER-COLLECTION areproper subsets of PART-WHOLE, one of the relations inSemEval-2007 Task 4.4We have taken the first five from SemEval-2007 Task 4.Cause-Effect.
An event or object leads to an effect.Example: Smoking causes cancer.Instrument-Agency.
An agent uses an instrument.Example: laser printerProduct-Producer.
A producer causes a product toexist.
Example: The farmer grows apples.Content-Container.
An object is physically storedin a delineated area of space, the container.
Ex-ample: Earth is located in the Milky Way.Entity-Origin.
An entity is coming or is derivedfrom an origin (e.g., position or material).
Ex-ample: letters from foreign countriesEntity-Destination.
An entity is moving towards adestination.
Example: The boy went to bed.Component-Whole.
An object is a component of alarger whole.
Example: My apartment has alarge kitchen.Member-Collection.
A member forms a nonfunc-tional part of a collection.
Example: There aremany trees in the forest.Communication-Topic.
An act of communication,whether written or spoken, is about a topic.
Ex-ample: The lecture was about semantics.We add a tenth element to this set, the pseudo-relation OTHER.
It stands for any relation whichis not one of the nine explicitly annotated relations.This is motivated by modelling considerations.
Pre-sumably, the data for OTHER will be very nonho-mogeneous.
By including it, we force any model ofthe complete data set to correctly identify the deci-sion boundaries between the individual relations and?everything else?.
This encourages good generaliza-tion behaviour to larger, noisier data sets commonlyseen in real-world applications.3.1 Semantic Relations versus Semantic RolesThere are three main differences between our task(classification of semantic relations between nomi-nals) and the related task of automatic labeling ofsemantic roles (Gildea and Jurafsky, 2002).The first difference is to do with the linguisticphenomena described.
Lexical resources for theo-ries of semantic roles such as FrameNet (Fillmore et96al., 2003) and PropBank (Palmer et al, 2005) havebeen developed to describe the linguistic realizationpatterns of events and states.
Thus, they target pri-marily verbs (or event nominalizations) and their de-pendents, which are typically nouns.
In contrast,semantic relations may occur between all parts ofspeech, although we limit our attention to nominalsin this task.
Also, semantic role descriptions typi-cally relate an event to a set of multiple participantsand props, while semantic relations are in practice(although not necessarily) binary.The second major difference is the syntactic con-text.
Theories of semantic roles usually developedout of syntactic descriptions of verb valencies, andthus they focus on describing the linking patterns ofverbs and their direct dependents, phenomena likeraising and noninstantiations notwithstanding (Fill-more, 2002).
Semantic relations are not tied topredicate-argument structures.
They can also be es-tablished within noun phrases, noun compounds, orsentences more generally (cf.
the examples above).The third difference is that of the level of gen-eralization.
FrameNet currently contains more than825 different frames (event classes).
Since the se-mantic roles are designed to be interpreted at theframe level, there is a priori a very large numberof unrelated semantic roles.
There is a rudimen-tary frame hierarchy that defines mappings betweenroles of individual frames,5 but it is far from com-plete.
The situation is similar in PropBank.
Prop-Bank does use a small number of semantic roles, butthese are again to be interpreted at the level of in-dividual predicates, with little cross-predicate gen-eralization.
In contrast, all of the semantic relationinventories discussed in Section 1 contain fewer than50 types of semantic relations.
More generally, se-mantic relation inventories attempt to generalize re-lations across wide groups of verbs (Chklovski andPantel, 2004) and include relations that are not verb-centered (Nastase and Szpakowicz, 2003; Moldovanet al, 2004).
Using the same labels for similar se-mantic relations facilitates supervised learning.
Forexample, a model trained with examples of sell re-lations should be able to transfer what it has learnedto give relations.
This has the potential of adding5For example, it relates the BUYER role of the COM-MERCE SELL frame (verb sell ) to the RECIPIENT role of theGIVING frame (verb give).1.
People in Hawaii might be feeling<e1>aftershocks</e1> from that power-ful <e2>earthquake</e2> for weeks.2.
My new <e1>apartment</e1> has a<e2>large kitchen</e2>.Figure 1: Two example sentences with annotationcrucial robustness and coverage to analysis tools inNLP applications based on semantic relations.4 AnnotationThe next step in our study will be the actual annota-tion of relations between nominals.
For the purposeof annotation, we define a nominal as a noun or abase noun phrase.
A base noun phrase is a noun andits pre-modifiers (e.g., nouns, adjectives, determin-ers).
We do not include complex noun phrases (e.g.,noun phrases with attached prepositional phrases orrelative clauses).
For example, lawn is a noun, lawnmower is a base noun phrase, and the engine of thelawn mower is a complex noun phrase.We focus on heads that are common nouns.
Thisemphasis distinguishes our task from much work inIE, which focuses on named entities and on consid-erably more fine-grained relations than we do.
Forexample, Patwardhan and Riloff (2007) identify cat-egories like Terrorist organization as participants interror-related semantic relations, which consists pre-dominantly of named entities.
We feel that namedentities are a specific category of nominal expres-sions best dealt with using techniques which do notapply to common nouns; for example, they do notlend themselves well to semantic generalization.Figure 1 shows two examples of annotated sen-tences.
The XML tags <e1> and <e2> mark thetarget nominals.
Since all nine proper semantic re-lations in this task are asymmetric, the ordering ofthe two nominals must be taken into account.
Inexample 1, CAUSE-EFFECT(e1, e2) does not hold,although CAUSE-EFFECT(e2, e1) would.
In exam-ple 2, COMPONENT-WHOLE(e2, e1) holds.We are currently developing annotation guide-lines for each of the relations.
They will give a pre-cise definition for each relation and some prototypi-cal examples, similarly to SemEval-2007 Task 4.The annotation will take place in two rounds.
Inthe first round, we will do a coarse-grained search97for positive examples for each relation.
We willcollect data from the Web using a semi-automatic,pattern-based search procedure.
In order to ensurea wide variety of example sentences, we will useseveral dozen patterns per relation.
We will alsoensure that patterns retrieve both positive and nega-tive example sentences; the latter will help populatethe OTHER relation with realistic near-miss negativeexamples of the other relations.
The patterns willbe manually constructed following the approach ofHearst (1992) and Nakov and Hearst (2008).6The example collection for each relation R willbe passed to two independent annotators.
In order tomaintain exclusivity of relations, only examples thatare negative for all relations but R will be includedas positive and only examples that are negative forall nine relations will be included as OTHER.
Next,the annotators will compare their decisions and as-sess inter-annotator agreement.
Consensus will besought; if the annotators cannot agree on an exam-ple it will not be included in the data set, but it willbe recorded for future analysis.Finally, two other task organizers will look foroverlap across all relations.
They will discard anyexample marked as positive in two or more relations,as well as examples in OTHER marked as positive inany of the other classes.
The OTHER relation will,then, consist of examples that are negatives for allother relations and near-misses for any relation.Data sets.
The annotated data will be divided intoa training set, a development set and a test set.
Therewill be 1000 annotated examples for each of theten relations: 700 for training, 100 for developmentand 200 for testing.
All data will be released underthe Creative Commons Attribution 3.0 Unported Li-cense7.
The annotation guidelines will be includedin the distribution.5 The Classification TaskThe actual task that we will run at SemEval-2010will be a multi-way classification task.
Not all pairsof nominals in each sentence will be labeled, so thegold-standard boundaries of the nominals to be clas-sified will be provided as part of the test data.6Note that, unlike in Semeval 2007 Task 4, we will not re-lease the patterns to the participants.7http://creativecommons.org/licenses/by/3.0/In contrast with Semeval 2007 Task 4, in whichthe ordering of the entities was provided with eachexample, we aim at a more realistic scenario inwhich the ordering of the labels is not given.
Par-ticipants in the task will be asked to discover boththe relation and the order of the arguments.
Thus,the more challenging task is to identify the mostinformative ordering and relation between a pairof nominals.
The stipulation ?most informative?is necessary since with our current set of asym-metrical relations that includes OTHER, each pairof nominals that instantiates a relation in one di-rection (e.g., REL(e1, e2)), instantiates OTHER inthe inverse direction (OTHER (e2, e1)).
Thus, thecorrect answers for the two examples in Figure 1are CAUSE-EFFECT (earthquake, aftershocks) andCOMPONENT-WHOLE (large kitchen, apartment).Note that unlike in SemEval-2007 Task 4, we willnot provide manually annotated WordNet senses,thus making the task more realistic.
WordNet sensesdid, however, serve for disambiguation purposes inSemEval-2007 Task 4.
We will therefore have toassess the effect of this change on inter-annotatoragreement.6 Evaluation MethodologyThe official ranking of the participating systems willbe based on their macro-averaged F-scores for thenine proper relations.
We will also compute and re-port their accuracy over all ten relations, includingOTHER.
We will further analyze the results quan-titatively and qualitatively to gauge which relationsare most difficult to classify.Similarly to SemEval-2007 Task 4, in order toassess the effect of varying quantities of trainingdata, we will ask the teams to submit several sets ofguesses for the labels for the test data, using varyingfractions of the training data.
We may, for example,request test results when training on the first 50, 100,200, 400 and all 700 examples from each relation.We will provide a Perl-based automatic evalua-tion tool that the participants can use when train-ing/tuning/testing their systems.
We will use thesame tool for the official evaluation.7 ConclusionWe have introduced a new task, which will be part ofSemEval-2010: multi-way classification of semantic98relations between pairs of common nominals.
Thetask will compare different approaches to the prob-lem and provide a standard testbed for future re-search, which can benefit many NLP applications.The description we have presented here shouldbe considered preliminary.
We invite the in-terested reader to visit the official task web-site http://semeval2.fbk.eu/semeval2.php?location=tasks\#T11, where up-to-date information will be published; there is also adiscussion group and a mailing list.ReferencesTimothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the web for fine-grained semantic verbrelations.
In Proc.
EMNLP 2004, pages 33?40.Dmitry Davidov and Ari Rappoport.
2008.
Classifica-tion of semantic relationships between nominals usingpattern clusters.
In Proc.
ACL-08: HLT, pages 227?235.Charles J. Fillmore, Christopher R. Johnson, andMiriam R.L.
Petruck.
2003.
Background toFrameNet.
International Journal of Lexicography,16:235?250.Charles J. Fillmore.
2002.
FrameNet and the linking be-tween semantic and syntactic relations.
In Proc.
COL-ING 2002, pages 28?36.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-tohe.
2005.
On the semantics of noun compounds.Computer Speech and Language, 19:479?496.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 task 04: Classification of semantic re-lations between nominals.
In Proc.
4th Semantic Eval-uation Workshop (SemEval-2007).Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2008.Classification of semantic relations between nominals.Language Resources and Evaluation.
In print.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
COLING92, pages 539?545.Sophia Katrenko and Pieter Adriaans.
2008.
Semantictypes of some generic relation arguments: Detectionand evaluation.
In Proc.
ACL-08: HLT, Short Papers,pages 185?188.Su Nam Kim and Timothy Baldwin.
2005.
Automaticinterpretation of noun compounds using WordNet sim-ilarity.
In Proc.
IJCAI, pages 945?956.George Lakoff.
1987.
Women, fire, and dangerousthings.
University of Chicago Press, Chicago, IL.Maria Lapata.
2002.
The disambiguation of nominalisa-tions.
Computational Linguistics, 28:357?388.Dan Moldovan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the se-mantic classification of noun phrases.
In HLT-NAACL2004: Workshop on Computational Lexical Semantics,pages 60?67.Preslav Nakov and Marti A. Hearst.
2008.
Solving rela-tional similarity problems using the web as a corpus.In Proc.
ACL-08: HLT, pages 452?460.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Fifth Interna-tional Workshop on Computational Semantics (IWCS-5), pages 285?301.Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,and Stan Szpakowicz.
2006.
Learning noun-modifiersemantic relations with corpus-based and WordNet-based features.
In Proc.
AAAI, pages 781?787.Diarmuid O?
Se?aghdha and Ann Copestake.
2008.
Se-mantic classification with distributional kernels.
InProc.
COLING 2008, pages 649?656.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: An annotated corpus of seman-tic roles.
Computational Linguistics, 31(1):71?106.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically harvest-ing semantic relations.
In Proc.
COLING/ACL, pages113?120.Siddharth Patwardhan and Ellen Riloff.
2007.
Effectiveinformation extraction with semantic affinity patternsand relevant regions.
In Proc.
EMNLP-CoNLL), pages717?727.Barbara Rosario and Marti Hearst.
2001.
Classifying thesemantic relations in noun compounds via a domain-specific lexical hierarchy.
In Proc.
EMNLP 2001,pages 82?90.Barbara Rosario, Marti Hearst, and Charles Fillmore.2002.
The descent of hierarchy, and selection in re-lational semantics.
In Proc.
ACL-02, pages 247?254.Matthew Stephens, Mathew Palakal, SnehasisMukhopadhyay, Rajeev Raje, and Javed Mostafa.2001.
Detecting gene relations from Medline ab-stracts.
In Pacific Symposium on Biocomputing, pages483?495.Peter D. Turney and Michael L. Littman.
2005.
Corpus-based learning of analogies and semantic relations.Machine Learning, 60(1-3):251?278.Peter D. Turney.
2005.
Measuring semantic similarity bylatent relational analysis.
In Proc.
IJCAI, pages 1136?1141.99
