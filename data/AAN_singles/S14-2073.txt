Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 424?427,Dublin, Ireland, August 23-24, 2014.MindLab-UNAL: Comparing Metamap and T-mapper for MedicalConcept Extraction in SemEval 2014 Task 7Alejandro Riveros, Maria De-Arteaga,Fabio A. Gonz?alez and Sergio JimenezUniversidad Nacional de ColombiaCiudad UniversitariaBogot?a, Colombia[lariverosc,mdeg,fagonzalezo,sgjimenezv]@unal.edu.coHenning M?ullerUniv.
of Applied SciencesWestern Switzerland, HES-SOSierre, Switzerlandhenning.mueller@hevs.chAbstractThis paper describes our participation intask 7 of SemEval 2014, which focuseson analysis of clinical text.
The task isdivided into two parts: recognizing men-tions of concepts that belong to the UMLS(Unified Medical Language System) se-mantic group disorders, and mapping eachdisorder to a unique UMLS CUI (ConceptUnique Identifier), if possible.
For identi-fying and mapping disorders belonging tothe UMLS meta thesaurus, we explore twotools: Metamap and T-mapper.
Addition-ally, a Named Entity Recognition system,based on a maximum entropy model, wasimplemented to identify other disorders.1 IntroductionClinical texts are unstructured data that, when pro-cessed properly, can be of great value.
Extractingkey information from these documents can makemedical notes more suitable for automatic pro-cessing.
It can also help diagnose patients, struc-ture their medical histories and optimize otherclinical procedures and research.The task of identifying mentions to medicalconcepts in free text and mapping these mentionsto a knowledge base was recently proposed inShARe/CLEF eHealth Evaluation Lab 2013, at-tracting the attention of several research groupsworldwide (Pradhan et al., 2013).
The task 7 inSemEval 2014 (Pradhan et al., 2014) elaboratesin that previous effort focusing on the recognitionand normalization of named entity mentions be-longing to the UMLS semantic group disorders.The paper is organized as follows: in section 2we briefly present the data, section 3 contains theThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/description of the methods and tools used in oursystem.
Later, on sections 4 and 5 we provide thedetails of the three submitted runs and expose theofficial results.
Finally, sections 6 and 7 includediscussions on variations that could be done to im-prove performance and conclusions to be drawnfrom our participation in the task.2 Data DescriptionThe training data for SemEval 2014 Task 7 con-sists of the ShARe (Shared Annotation Resource)corpus, which contains clinical notes from MIMICII database (Multiparameter Intelligent Monito?ring in Intensive Care).
The data were manuallyannotated for disorder mentions, normalized to aUMLS Concept Unique Identifier when possible,and marked as CUI-less otherwise.Four types of reports where found in the cor-pus: 61 discharge summaries, 54 ECG reports, 42ECHO reports and 42 radiology reports, for a to-tal of 199 training documents, each containing se?veral disorder mentions.3 Methods Used3.1 Named-Entity RecognitionUsing the Java libraries Apache OpenNLP1andMaxent2, a maximum entropy model was im-plemented for Named Entity Recognition (NER).Two types of classifiers were built: the first oneusing the library?s default configuration, and a se?cond one including additional features.
The de-fault model includes the following attributes: tar-get word, two words of context at the left of thetarget word, two words of context at the right ofthe target word, type of token for target word (cap-italized word, number, hyphen, commas, etc.
), andtype of token for words in the context.1http://opennlp.apache.org2http://maxent.sourceforge.net/about.html424For the enhanced model, we included n-gramsat character level extracted from the target word,going from two to five characters.OpenNLP uses the BIO tagging scheme, whichmarks each token as either beginning a chunk,continuing it, or not in a chunk, therefore, thismodel cannot identify discontinuous terms.
Giventhis, we excluded discontinuous term annotationsfrom the training data, and trained the model withthe resulting corpus.During the experiments, we also consideredPOS (Part of Speech) tags obtained with theOpenNLP library, POS tags obtained with theStanford Java library and the number of charac-ters in each token.
However, we decided not toinclude any of these because accuracy decreasedwhen using them.3.2 Weirdness MeasureAccording to preliminary experiments, the cho-sen enhanced NER method exhibited low preci-sion, i.e.
a high number of false positives.
Todeal with this problem we calculated a measure forthe specificity of a candidate named entity with re-spect to a specialized corpus, this quantity is basedon the weirdness (Ahmad et al., 1999) of the can-didate words.
Having a general corpus Cgand aspecialized corpus Cs, where wgand wsrefer tothe number of occurrences of a word w in eachcorpus and tsand tgto the total count of words ineach corpus, the weirdness of a word is defined asfollows:Weirdness(w) =wsts/wgtgThose words that are common to any domainwill very likely have a low weirdness score, whilethose with a high weirdness score indicate w is notused in the general corpus as much as in the spe-cialized one, meaning it probably corresponds tospecialized vocabulary.Using around 1000 books from the GuttenbergProject as the general corpus, and the terms inUMLS as the specialized corpus, we applied theweirdness measure to those words that, accordingto the NER model, are disorders.
By keeping onlythose with high weirdness measures, we preventour system from tagging words that are not evenmedical vocabulary, thus reducing the amount offalse positives.3.3 MetamapFor identifying and mapping disorders includedin the UMLS meta thesaurus to its correspondingCUI, we explored two tools.
Both of them findcandidates in the document and give the possibleCUIs for each; in both cases, we selected the CUIthat belongs to the UMLS semantic group disor-ders, as specified in the task description.The first tool we explored is Metamap.
Forprocessing the documents, we use the followingMetamap features: allow concept gap and wordsense disambiguation.After processing a document, the results werefiltered, keeping only those tags that were mappedto a CUI that belongs to one of the followingUMLS semantic types: congenital abnormality,acquired abnormality, injury or poisoning, patho-logic function, disease or syndrome, mental orbehavioral dysfunction, cell or molecular dys-function, experimental model of disease, anato?mical abnormality, neoplastic process, and signsor symptoms.3.4 T-mapperAs an alternative to Metamap we experimentedwith T-mapper3, an annotation tool developed atMindLab4that works in languages different thanEnglish and with any knowledge source (i.e.
notonly UMLS).
The method implemented by T-mapper is inspired by the one in Metamap, withsome modifications.
The method works as fol-lows:1.
Indexing and vocabulary generation: an in-verted index and other data structures arebuilt to perform fast lookups over the dictio-nary and the vocabulary list in Cgand Cs.2.
Sentence detection and tokenization: the in-put text is divided into sentences and theneach sentence is divided into tokens using awhitespace as separator.3.
Spelling correction: to deal with noise andsimple morphological variations, each tokenthat does not match a word within the voca?bulary is replaced by the most frequent wordamong the most similar words found above athreshold of 0.75.
The similarity is computedusing a normalized score based on the Leven-sthein distance.3https://github.com/lariverosc/tmapper4http://mindlaboratory.org/4254.
Candidate generation and scoring: a subsetthat contains all the terms that match at leastone of the words in the sentence is gene?rated, the terms contained in this set arecalled candidates.
Once this subset is built,each of the candidate terms is scored usinga simplified version of Metamap?s scoringfunction (Aronson, 2001).
In comparison, T-mapper?s function uses only variation, cov-erage and cohesiveness as criteria, excludingcentrality, since it is language dependant.5.
Candidate selection and disambiguation: thescore computed in the previous step is usedto choose the candidates that will be used asmappings.
Ambiguity can occur because oftwo reasons: a tie in the scores or by over-lapping over the sentence tokens.
In the firstcase, the Lin?s measure (Lin, 1998) is usedas disambiguation criteria between the can-didates and the previous detected concepts.In the second case, the most concrete term ischosen according to the UMLS hierarchy.4 System SubmissionsThe team submitted three runs.
The run 0 wasintended as a baseline; run 1 used Metamap forUMLS concept mapping and run 2 did this usingT-mapper.
Both run 1 and run 2 used the enhancedfeatures for NER and applied the weirdness mea-sure.For run 0, the documents were processed withMetamap and those concepts mapped to a CUIbelonging to one of the desired UMLS seman-tic types were chosen.
Parallel to this, the do?cument was tagged using the default NER model.Finally, results were merged, preferring Metamapmapping outputs in the cases where a concept wasmapped by both tools (in an ideal scenario, allterms mapped by Metamap would have also beenmapped by the NER model).Run 1 differs from run 0 in two steps of the pro-cess: the NER model included the enhanced fea-tures described previously and its output was fil-tered, keeping only those concepts whose weird-ness measure exceeds 0.7.
For multiword conceptsthe weirdness of each word was aggregated.Finally, run 2 was equal to run 1, with the di?fference that T-mapper was used to map conceptsto the UMLS meta thesaurus.Rank Run Strict P Strict R Strict F1 best 0.843 0.786 0.81331 2 0.561 0.534 0.54732 1 0.578 0.515 0.54537 0 0.321 0.565 0.409Table 1: Official results for task A obtained by thebest system and our runs (ranked by exact acc.
)Rank Run Strict Accuracy1 best 0.74119 2 0.46121 0 0.43524 1 0.411Table 2: Official results for task B obtained by thebest system and our runs (ranked by exact acc.
)5 ResultsFor both task A and B, run 2 produced the bestperformance among our systems.
In Table 1 the re-sults of the three runs are presented, together withthe information of the system with the best perfor-mance among all participating teams (labeled asbest).
The position in the ranking is from a totalof 43 submitted systems.
Table 2 shows analogousresults for Task B, where 37 systems were submit-ted.Even though the official ranking is based on thestrict accuracy, which only considers a tag to becorrect if it matches exactly both the first and lastcharacters, a relaxed accuracy is also provided bythe organizers.
This second scoring measure con-siders a tag to be correct if it has an overlap withthe actual one.
Tables 3 and 4 show these results.In both tables 1 and 3, P stands for Precision, Rfor Recall, and F for F-score.
The ranking is basedon the F-score.6 DiscussionThe system that gave the best results for both taskswas the one based on T-mapper.
Certain featuresRank Run Relax P Relax R Relax F1 best 0.916 0.907 0.91135 2 0.769 0.677 0.72037 1 0.777 0.654 0.71040 0 0.439 0.725 0.547Table 3: Official results for task A obtained by thebest system and our runs (ranked by relaxed acc.
)426Rank Run Relaxed Accuracy1 best 0.92811 2 0.86319 0 0.79721 1 0.771Table 4: Official results for task B obtained by thebest system and our runs (ranked by relaxed acc.
)of this tool make this finding particularly inte?resting: it works for any language and ontology,and it is considerably faster than Metamap.
WhileMetamap took 581 minutes to tag 133 documents,T-mapper only required 96 minutes (133 is thenumber of documents in the test set).One aspect that might have damaged the per-formance of our system is the fact that, unlikemost of the teams, we did not use the develop-ment data for training.
However, there are stilla number of changes that could be made, whichwould very likely improve the accuracy of our sys-tem.
First, the tokenizer used for the NER modeland for T-mapper were too simple.
Separation wasdone based on blank spaces, therefore slashes, cer-tain punctuation marks and hyphens might not betreated properly.In addition to this, the spell checker used by T-mapper also needs to be improved.
Currently, itgives a ranked list of options for each word thatshould be replaced, and automatically chooses thefirst one in the ranking.
However, the best matchis often the second or third in the list.
Changingthe criteria used to choose the replacement, takinginto account word sense disambiguation, wouldenhance the accuracy of T-mapper.The weirdness measure is also something thatshould be reconsidered, since it would be inte?resting to use a metric that responds better to un-seen terms.
And in case this was still the chosenmeasure, other training corpora could work better,since an ontology might lack words that are cur-rently used in a medical context but do not havea CUI, and it also fails to give a notion of whichwords are more frequently used than others.
It isnot easy, however, to replace UMLS as corpus,since it is not easy to compete with its size andrichness.Finally, the OpenNLP NER system does notrecognize discontinuous terms.
Therefore, noCUI-less term with a gap can currently be iden-tified by the system.
For this reason, the NERmethod should be changed to one that allows thistype of mentions to be present in texts.For Task B, it is very interesting to see the di?fference between the strict and relaxed evaluationrankings.
We go from being in position 19 to beingin position 11.
This might be partially explainedby some of the flaws previously mentioned; in par-ticular, the weak tokenizer and the incapability toidentify CUI-less terms with gaps.7 ConclusionWe participated with three runs in the Semeval2014 task for analysis of clinical texts.
Eventhough the performance of our runs indicates theystill need to be enhanced in order to be com?petitive in this specific task, the performance ofthe run based on T-mapper compared to that of theones that use Metamap proves that T-mapper is aviable alternative for mapping concepts to clinicalterminologies.
Moreover, T-mapper should also beconsidered for cases in which Metamap cannot beused: languages other than English and terminolo-gies other than UMLS.ReferencesKhurshid Ahmad, Lee Gillam, Lena Tostevin, et al.1999.
University of surrey participation in trec8:Weirdness indexing for logical document extrapola-tion and retrieval (wilder).
In TREC.Alan R Aronson.
2001.
Effective mapping of biomed-ical text to the umls metathesaurus: the metamapprogram.
In Proceedings of the AMIA Symposium,page 17.
American Medical Informatics Associa-tion.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In ICML, volume 98, pages 296?304.Sameer Pradhan, Noemie Elhadad, Brett R. South,David Martinez, Lee Chistensen, Amy Vogel, HannaSuominen, Wendy W. Chapman, and GuerganaSavova.
2013.
Task 1: ShARe/CLEF eHealth eval-uation lab 2013.
In Online Working Notes of theCLEF 2013 Evaluation Labs and Workshop, Valen-cia, Spain, September.Sameer Pradhan, Noemie Elhadad, Wendy W. Chap-man, and Guergana Savova.
2014.
Semeval-2014task : Analysis of clinical text.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014), Dublin, Ireland, August.427
