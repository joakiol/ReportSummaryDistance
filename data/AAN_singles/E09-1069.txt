Proceedings of the 12th Conference of the European Chapter of the ACL, pages 603?611,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsDeterministic shift-reduce parsing for unification-based grammars byusing default unificationTakashi NinomiyaInformation Technology CenterUniversity of Tokyo, Japanninomi@r.dl.itc.u-tokyo.ac.jpTakuya MatsuzakiDepartment of Computer ScienceUniversity of Tokyo, Japanmatuzaki@is.s.u-tokyo.ac.jpNobuyuki ShimizuInformation Technology CenterUniversity of Tokyo, Japanshimizu@r.dl.itc.u-tokyo.ac.jpHiroshi NakagawaInformation Technology CenterUniversity of Tokyo, Japannakagawa@dl.itc.u-tokyo.ac.jpAbstractMany parsing techniques including pa-rameter estimation assume the use of apacked parse forest for efficient and ac-curate parsing.
However, they have sev-eral inherent problems deriving from therestriction of locality in the packed parseforest.
Deterministic parsing is one ofsolutions that can achieve simple and fastparsing without the mechanisms of thepacked parse forest by accurately choos-ing search paths.
We propose (i) deter-ministic shift-reduce parsing for unifica-tion-based grammars, and (ii) best-firstshift-reduce parsing with beam threshold-ing for unification-based grammars.
De-terministic parsing cannot simply be ap-plied to unification-based grammar pars-ing, which often fails because of its hardconstraints.
Therefore, it is developed byusing default unification, which almostalways succeeds in unification by over-writing inconsistent constraints in gram-mars.1 IntroductionOver the last few decades, probabilistic unifica-tion-based grammar parsing has been investi-gated intensively.
Previous studies (Abney,1997; Johnson et al, 1999; Kaplan et al, 2004;Malouf and van Noord, 2004; Miyao and Tsujii,2005; Riezler et al, 2000) defined a probabilisticmodel of unification-based grammars, includinghead-driven phrase structure grammar (HPSG),lexical functional grammar (LFG) and combina-tory categorial grammar (CCG), as a maximumentropy model (Berger et al, 1996).
Geman andJohnson (Geman and Johnson, 2002) and Miyaoand Tsujii (Miyao and Tsujii, 2002) proposed afeature forest, which is a dynamic programmingalgorithm for estimating the probabilities of allpossible parse candidates.
A feature forest canestimate the model parameters without unpack-ing the parse forest, i.e., the chart and its edges.Feature forests have been used successfullyfor probabilistic HPSG and CCG (Clark and Cur-ran, 2004b; Miyao and Tsujii, 2005), and itsparsing is empirically known to be fast and accu-rate, especially with supertagging (Clark andCurran, 2004a; Ninomiya et al, 2007; Ninomiyaet al, 2006).
Both estimation and parsing withthe packed parse forest, however, have severalinherent problems deriving from the restrictionof locality.
First, feature functions can be de-fined only for local structures, which limit theparser?s performance.
This is because parserssegment parse trees into constituents and factorequivalent constituents into a single constituent(edge) in a chart to avoid the same calculation.This also means that the semantic structures mustbe segmented.
This is a crucial problem whenwe think of designing semantic structures otherthan predicate argument structures, e.g., syn-chronous grammars for machine translation.
Thesize of the constituents will be exponential if thesemantic structures are not segmented.
Lastly,we need delayed evaluation for evaluating fea-ture functions.
The application of feature func-tions must be delayed until all the values in the603segmented constituents are instantiated.
This isbecause values in parse trees can propagate any-where throughout the parse tree by unification.For example, values may propagate from the rootnode to terminal nodes, and the final form of theterminal nodes is unknown until the parser fi-nishes constructing the whole parse tree.
Conse-quently, the design of grammars, semantic struc-tures, and feature functions becomes complex.To solve the problem of locality, several ap-proaches, such as reranking (Charniak and John-son, 2005), shift-reduce parsing (Yamada andMatsumoto, 2003), search optimization learning(Daum?
and Marcu, 2005) and sampling me-thods (Malouf and van Noord, 2004; Nakagawa,2007), were studied.In this paper, we investigate shift-reduce pars-ing approach for unification-based grammarswithout the mechanisms of the packed parse for-est.
Shift-reduce parsing for CFG and dependen-cy parsing have recently been studied (Nivre andScholz, 2004; Ratnaparkhi, 1997; Sagae and La-vie, 2005, 2006; Yamada and Matsumoto, 2003),through approaches based essentially on deter-ministic parsing.
These techniques, however,cannot simply be applied to unification-basedgrammar parsing because it can fail as a result ofits hard constraints in the grammar.
Therefore,in this study, we propose deterministic parsingfor unification-based grammars by using defaultunification, which almost always succeeds inunification by overwriting inconsistent con-straints in the grammars.
We further pursuebest-first shift-reduce parsing for unification-based grammars.Sections 2 and 3 explain unification-basedgrammars and default unification, respectively.Shift-reduce parsing for unification-based gram-mars is presented in Section 4.
Section 5 dis-cusses our experiments, and Section 6 concludesthe paper.2 Unification-based grammarsA unification-based grammar is defined as a pairconsisting of a set of lexical entries and a set ofphrase-structure rules.
The lexical entries ex-press word-specific characteristics, while thephrase-structure rules describe constructions ofconstituents in parse trees.
Both the phrase-structure rules and the lexical entries arerepresented by feature structures (Carpenter,1992), and constraints in the grammar are forcedby unification.
Among the phrase-structure rules,a binary rule is a partial function: ?
?
?
?
?
,where ?
is the set of all possible feature struc-tures.
The binary rule takes two partial parsetrees as daughters and returns a larger partialparse tree that consists of the daughters and theirmother.
A unary rule is a partial function:?
?
?, which corresponds to a unary branch.In the experiments, we used an HPSG (Pollardand Sag, 1994), which is one of the sophisticatedunification-based grammars in linguistics.
Gen-erally, an HPSG has a small number of phrase-structure rules and a large number of lexical en-tries.
Figure 1 shows an example of HPSG pars-ing of the sentence, ?Spring has come.?
The up-per part of the figure shows a partial parse treefor ?has come,?
which is obtained by unifyingeach of the lexical entries for ?has?
and ?come?with a daughter feature structure of the head-complement rule.
Larger partial parse trees areobtained by repeatedly applying phrase-structurerules to lexical/phrasal partial parse trees.
Final-ly, the parse result is output as a parse tree thatdominates the sentence.3 Default unificationDefault unification was originally investigated ina series of studies of lexical semantics, in orderto deal with default inheritance in a lexicon.
It isalso desirable, however, for robust processing,because (i) it almost always succeeds and (ii) afeature structure is relaxed such that the amountof information is maximized (Ninomiya et al,2002).
In our experiments, we tested a simpli-fied version of Copestake?s default unification.Before explaining it, we first explain Carpenter?sFigure 1: Example of HPSG parsing.HEAD  nounSUBJ  <>COMPS <>HEAD  verbHEAD  nounSUBJ  <      SUBJ  <>     >COMPS <>COMPS <>HEAD  verbSUBJ  <   >COMPS <   >HEAD  verbSUBJ  <   >COMPS <>head-compSpring has come11 122HEAD  verbSUBJ  <>COMPS <>HEAD  nounSUBJ  <>COMPS <>HEAD  verbSUBJ  <   >COMPS <>HEAD  verbSUBJ  <   >COMPS <   >HEAD  verbSUBJ  <   >COMPS <>subject-headhead-compSpring has come11 11 22604two definitions of default unification (Carpenter,1993).
(Credulous Default Unification)?
??
?
?
=  ??
?
?????
?
?
is maximal suchthat ?
?
?
?is defined ?
(Skeptical Default Unification)?
??
?
?
=  ?(?
???
?)?
is called a strict feature structure, whose in-formation must not be lost, and ?
is called a de-fault feature structure, whose information can belost but as little as possible so that ?
and ?
canbe unified.Credulous default unification is greedy, in thatit tries to maximize the amount of informationfrom the default feature structure, but it results ina set of feature structures.
Skeptical default un-ification simply generalizes the set of featurestructures resulting from credulous default unifi-cation.
Skeptical default unification thus leads toa unique result so that the default informationthat can be found in every result of credulousdefault unification remains.
The following is anexample of skeptical default unification:[F: ?]
??
?
?F: 1 ?G: 1H: ??
=  ??
?F: ?G: ?H: ??
, ?F: 1 ?G: 1H: ???
= ?F: ?G: ?H: ?
?.Copestake mentioned that the problem withCarpenter?s default unification is its time com-plexity (Copestake, 1993).
Carpenter?s defaultunification takes exponential time to find the op-timal answer, because it requires checking theunifiability of the power set of constraints in adefault feature structure.
Copestake thus pro-posed another definition of default unification, asfollows.
Let ??(?)
be a function that returns aset of path values in ?, and let ??(?)
be a func-tion that returns a set of path equations, i.e., in-formation about structure sharing in ?.
(Copestake?s default unification)?
??
?
?
=  ?
?
?
????
?
??(?
)and there is no ??
?
??(?
)such that ?
?
?
?is defined and?
?
?
?
?
?is not defined?,where ?
= ?
?
???(?
).Copestake?s default unification works effi-ciently because all path equations in the defaultfeature structure are unified with the strict fea-ture structures, and because the unifiability ofpath values is checked one by one for each nodein the result of unifying the path equations.
Theimplementation is almost the same as that ofnormal unification, but each node of a featurestructure has a set of values marked as ?strict?
or?default.?
When types are involved, however, itis not easy to find unifiable path values in thedefault feature structure.
Therefore, we imple-mented a more simply typed version of Corpes-take?s default unification.Figure 2 shows the algorithm by which weimplemented the simply typed version.
First,each node is marked as ?strict?
if it belongs to astrict feature structure and as ?default?
otherwise.The marked strict and default feature structuresprocedure forced_unification(p, q)queue := {?p, q?
};while( queue is not empty )?p, q?
:= shift(queue);p := deref(p); q := deref(q);if p ?
q?
(p) ?
?
(p) ?
?(q);?
(q) ?
ptr(p);forall f ?
feat(p)?
feat(q)if f ?
feat(p) ?
f ?
feat(q)queue := queue ?
??
(f, p), ?
(f, q)?
;if f ?
feat(p) ?
f ?
feat(q)?
(f, p) ?
?
(f, q);procedure mark(p, m)p := deref(p);if p has not been visited?
(p) := {??(p),m?
};forall f ?
feat(p)mark(?
(f, p), m);procedure collapse_defaults(p)p := deref(p);if p has not been visitedts := ?
; td := ?
;forall ?t, ???????
?
?
(p)ts := ts ?
t;forall ?t, ????????
?
?
(p)td := td ?
t;if ts is not definedreturn false;if ts ?
td is defined?
(p) := ts ?
td;else?
(p) := ts;forall f ?
feat(p)collapse_defaults(?
(f, p));procedure default_unification(p, q)mark(p, ??????
);mark(q, ???????
);forced_unification(p, q);collapse_defaults(p);?
(p) is (i) a single type, (ii) a pointer, or (iii) a set of pairs oftypes and markers in the feature structure node p.A marker indicates that the types in a feature structure nodeoriginally belong to the strict feature structures or the defaultfeature structures.A pointer indicates that the node has been unified with othernodes and it points the unified node.
A function deref tra-verses pointer nodes until it reaches to non-pointer node.?
(f, p) returns a feature structure node which is reached byfollowing a feature f from p.Figure 2: Algorithm for the simply typed ver-sion of Corpestake?s default unification.605are unified, whereas the types in the featurestructure nodes are not unified but merged as aset of types.
Then, all types marked as ?strict?are unified into one type for each node.
If thisfails, the default unification also returns unifica-tion failure as its result.
Finally, each node isassigned a single type, which is the result of typeunification for all types marked as both ?default?and ?strict?
if it succeeds or all types markedonly as ?strict?
otherwise.4 Shift-reduce parsing for unification-based grammarsNon-deterministic shift-reduce parsing for unifi-cation-based grammars has been studied by Bris-coe and Carroll (Briscoe and Carroll, 1993).Their algorithm works non-deterministically withthe mechanism of the packed parse forest, andhence it has the problem of locality in the packedparse forest.
This section explains our shift-reduce parsing algorithms, which are based ondeterministic shift-reduce CFG parsing (Sagaeand Lavie, 2005) and best-first shift-reduce CFGparsing (Sagae and Lavie, 2006).
Sagae?s parserselects the most probable shift/reduce actions andnon-terminal symbols without assuming explicitCFG rules.
Therefore, his parser can proceeddeterministically without failure.
However, inthe case of unification-based grammars, a deter-ministic parser can fail as a result of its hard con-straints in the grammar.
We propose two newshift-reduce parsing approaches for unification-based grammars: deterministic shift-reduce pars-ing and shift-reduce parsing by backtracking andbeam search.
The major difference between ouralgorithm and Sagae?s algorithm is that we usedefault unification.
First, we explain the deter-ministic shift-reduce parsing algorithm, and thenwe explain the shift-reduce parsing with back-tracking and beam search.4.1 Deterministic shift-reduce parsing forunification-based grammarsThe deterministic shift-reduce parsing algorithmfor unification-based grammars mainly compris-es two data structures: a stack S, and a queue W.Items in S are partial parse trees, including a lex-ical entry and a parse tree that dominates thewhole input sentence.
Items in W are words andPOSs in the input sentence.
The algorithm de-fines two types of parser actions, shift and reduce,as follows.?
Shift: A shift action removes the first item(a word and a POS) from W.  Then, onelexical entry is selected from among thecandidate lexical entries for the item.
Fi-nally, the selected lexical entry is put onthe top of the stack.Common features: Sw(i), Sp(i), Shw(i), Shp(i), Snw(i), Snp(i),Ssy(i), Shsy(i), Snsy(i), wi-1, wi,wi+1, pi-2, pi-1, pi, pi+1,pi+2, pi+3Binary reduce features: d, c, spl, syl, hwl, hpl, hll, spr, syr,hwr, hpr, hlrUnary reduce features: sy, hw, hp, hlSw(i) ?
head word of i-th item from the top of the stackSp(i) ?
head POS of i-th item from the top of the stackShw(i) ?
head word of the head daughter of i-th item from thetop of the stackShp(i) ?
head POS of the head daughter of i-th item from thetop of the stackSnw(i) ?
head word of the non-head daughter of i-th itemfrom the top of the stackSnp(i) ?
head POS of the non-head daughter of i-th item fromthe top of the stackSsy(i) ?
symbol of phrase category of the i-th item from thetop of the stackShsy(i) ?
symbol of phrase category of the head daughter ofthe i-th item from the top of the stackSnsy(i) ?
symbol of phrase category of the non-head daughterof the i-th item from the top of the stackd ?
distance between head words of daughtersc ?
whether a comma exists between daughters and/or insidedaughter phrasessp ?
the number of words dominated by the phrasesy ?
symbol of phrase categoryhw ?
head wordhp ?
head POShl ?
head lexical entryFigure 3: Feature templates.Shift Features[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)][Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)][Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)][Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2][pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1,wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi][pi-1, pi, pi+1] [pi, pi+1, pi+2] [pi-2, pi-1] [pi-1, pi] [pi,pi+1] [pi+1, pi+2]Binary Reduce Features[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)][Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)][Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)][Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2][pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d,c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c,sp, hw,hl] [c, sp, sy, hw] [d, c, hp, hl] [d, c, hp] [d, c, hl] [d,c, sy] [c, sp, hp, hl] [c, sp, hp] [c, sp, hl] [c, sp, sy]Unary Reduce Features[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)][Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)][Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)][Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2][pi-1] [pi] [pi+1] [pi+2] [pi+3] [hw, hp, hl] [hw, hp] [hw, hl][sy, hw] [hp, hl] [hp] [hl] [sy]Figure 4: Combinations of feature templates.606?
Binary Reduce: A binary reduce actionremoves two items from the top of thestack.
Then, partial parse trees are derivedby applying binary rules to the first re-moved item and the second removed itemas a right daughter and left daughter, re-spectively.
Among the candidate partialparse trees, one is selected and put on thetop of the stack.?
Unary Reduce: A unary reduce action re-moves one item from the top of the stack.Then, partial parse trees are derived byapplying unary rules to the removed item.Among the candidate partial parse trees,one is selected and put on the top of thestack.Parsing fails if there is no candidate for selec-tion (i.e., a dead end).
Parsing is considered suc-cessfully finished when W is empty and S hasonly one item which satisfies the sentential con-dition: the category is verb and the subcategori-zation frame is empty.
Parsing is considered anon-sentential success when W is empty and Shas only one item but it does not satisfy the sen-tential condition.In our experiments, we used a maximum en-tropy classifier to choose the parser?s action.Figure 3 lists the feature templates for the clas-sifier, and Figure 4 lists the combinations of fea-ture templates.
Many of these features were tak-en from those listed in (Ninomiya et al, 2007),(Miyao and Tsujii, 2005) and (Sagae and Lavie,2005), including global features defined over theinformation in the stack, which cannot be used inparsing with the packed parse forest.
The fea-tures for selecting shift actions are the same asthe features used in the supertagger (Ninomiya etal., 2007).
Our shift-reduce parsers can be re-garded as an extension of the supertagger.The deterministic parsing can fail because ofits grammar?s hard constraints.
So, we use de-fault unification, which almost always succeedsin unification.
We assume that a head daughter(or, an important daughter) is determined foreach binary rule in the unification-based gram-mar.
Default unification is used in the binaryrule application in the same way as used in Ni-nomiya?s offline robust parsing (Ninomiya et al,2002), in which a binary rule unified with thehead daughter is the strict feature structure andthe non-head daughter is the default featurestructure, i.e.,  (?
?
?)
??
?
?, where R is a bi-nary rule, H is a head daughter and NH is a non-head daughter.
In the experiments, we used thesimply typed version of Copestake?s default un-ification in the binary rule application1.
Notethat default unification was always used insteadof normal unification in both training and evalua-tion in the case of the parsers using default unifi-cation.
Although Copestake?s default unificationalmost always succeeds, the binary rule applica-tion can fail if the binary rule cannot be unifiedwith the head daughter, or inconsistency iscaused by path equations in the default featurestructures.
If the rule application fails for all thebinary rules, backtracking or beam search can beused for its recovery as explained in Section 4.2.In the experiments, we had no failure in the bi-nary rule application with default unification.4.2 Shift-reduce parsing by backtrackingand beam-searchAnother approach for recovering from the pars-ing failure is backtracking.
When parsing failsor ends with non-sentential success, the parser?sstate goes back to some old state (backtracking),and it chooses the second best action and triesparsing again.
The old state is selected so as tominimize the difference in the probabilities forselecting the best candidate and the second bestcandidate.
We define a maximum number ofbacktracking steps while parsing a sentence.Backtracking repeats until parsing finishes withsentential success or reaches the maximum num-ber of backtracking steps.
If parsing fails to finda parse tree, the best continuous partial parsetrees are output for evaluation.From the viewpoint of search algorithms, pars-ing with backtracking is a sort of depth-firstsearch algorithms.
Another possibility is to usethe best-first search algorithm.
The best-firstparser has a state priority queue, and each stateconsists of a tree stack and a word queue, whichare the same stack and queue explained in theshift-reduce parsing algorithm.
Parsing proceedsby applying shift-reduce actions to the best statein the state queue.
First, the best state is re-1 We also implemented Ninomiya?s default unification,which can weaken path equation constraints.
In the prelim-inary experiments, we tested binary rule application givenas (?
?
?)
??
??
with Copestake?s default unification,(?
?
?)
??
??
with Ninomiya?s default unification, and(?
?
??)
??
?
with Ninomiya?s default unification.
How-ever, there was no significant difference of F-score amongthese three methods.
So, in the main experiments, we onlytested (?
?
?)
??
??
with Copestake?s default unificationbecause this method is simple and stable.607moved from the state queue, and then shift-reduce actions are applied to the state.
The new-ly generated states as results of the shift-reduceactions are put on the queue.
This process re-peats until it generates a state satisfying the sen-tential condition.
We define the probability of aparsing state as the product of the probabilities ofselecting actions that have been taken to reachthe state.
We regard the state probability as theobjective function in the best-first search algo-rithm, i.e., the state with the highest probabilitiesis always chosen in the algorithm.
However, thebest-first algorithm with this objective functionsearches like the breadth-first search, and hence,parsing is very slow or cannot be processed in areasonable time.
So, we introduce beam thre-sholding to the best-first algorithm.
The searchspace is pruned by only adding a new state to thestate queue if its probability is greater than 1/b ofthe probability of the best state in the states thathas had the same number of shift-reduce actions.In what follows, we call this algorithm beamsearch parsing.In the experiments, we tested both backtrack-ing and beam search with/without default unifi-cation.
Note that, the beam search parsing forunification-based grammars is very slow com-pared to the shift-reduce CFG parsing with beamsearch.
This is because we have to copy parsetrees, which consist of a large feature structures,in every step of searching to keep many states onthe state queue.
In the case of backtracking, co-pying is not necessary.5 ExperimentsWe evaluated the speed and accuracy of parsingwith Enju 2.3?, an HPSG for English (Miyao andTsujii, 2005).
The lexicon for the grammar wasextracted from Sections 02-21 of the Penn Tree-bank (39,832 sentences).
The grammar consistedof 2,302 lexical entries for 11,187 words.
Twoprobabilistic classifiers for selecting shift-reduceactions were trained using the same portion ofthe treebank.
One is trained using normal unifi-cation, and the other is trained using default un-ification.We measured the accuracy of the predicate ar-gument relation output of the parser.
A predi-cate-argument relation is defined as a tuple?
?, ?
?, ?, ??
?, where ?
is the predicate type (e.g.,Section 23 (Gold POS)LP(%)LR(%)LF(%)Avg.Time(ms)# ofbacktrackAvg.
#ofstates# ofdeadend# of non-sententialsuccess# ofsententialsuccessPreviousstudies(Miyao and Tsujii, 2005) 87.26 86.50 86.88 604 - - - - -(Ninomiya et al, 2007) 89.78 89.28 89.53 234 - - - - -Oursdet 76.45 82.00 79.13 122 0 - 867 35 1514det+du 87.78 87.45 87.61 256 0 - 0 117 2299back40 81.93 85.31 83.59 519 18986 - 386 23 2007back10 + du 87.79 87.46 87.62 267 574 - 0 45 2371beam(7.4) 86.17 87.77 86.96 510 - 226 369 30 2017beam(20.1)+du 88.67 88.79 88.48 457 - 205 0 16 2400beam(403.4) 89.98 89.92 89.95 10246 - 2822 71 14 2331Section 23 (Auto POS)LP(%)LR(%)LF(%)Avg.Time(ms)# ofbacktrackAvg.
#ofstates# ofdeadend# of nonsententialsuccess# ofsententialsuccessPreviousstudies(Miyao and Tsujii, 2005) 84.96 84.25 84.60 674 - - - - -(Ninomiya et al, 2007) 87.28 87.05 87.17 260 - - - - -(Matsuzaki et al, 2007)  86.93 86.47 86.70 30 - - - - -(Sagae et al, 2007)  88.50 88.00 88.20 - - - - - -Oursdet 74.13 80.02 76.96 127 0 - 909 31 1476det+du 85.93 85.72 85.82 252 0 - 0 124 2292back40 78.71 82.86 80.73 568 21068 - 438 27 1951back10 + du 85.96 85.75 85.85 270 589 - 0 46 2370beam(7.4) 83.84 85.82 84.82 544 - 234 421 33 1962beam(20.1)+du 86.59 86.36 86.48 550 - 222 0 21 2395beam(403.4) 87.70 87.86 87.78 16822 - 4553 89 16 2311Table 1: Experimental results for Section 23.608adjective, intransitive verb), ??
is the head wordof the predicate, ?
is the argument label (MOD-ARG, ARG1, ?, ARG4), and ??
is the headword of the argument.
The labeled precision(LP) / labeled recall (LR) is the ratio of tuplescorrectly identified by the parser, and the labeledF-score (LF) is the harmonic mean of the LP andLR.
This evaluation scheme was the same oneused in previous evaluations of lexicalizedgrammars (Clark and Curran, 2004b; Hocken-maier, 2003; Miyao and Tsujii, 2005).
The expe-riments were conducted on an Intel Xeon 5160server with 3.0-GHz CPUs.
Section 22 of thePenn Treebank was used as the development set,and the performance was evaluated using sen-tences of ?
100 words in Section 23.
The LP,LR, and LF were evaluated for Section 23.Table 1 lists the results of parsing for Section23.
In the table, ?Avg.
time?
is the average pars-ing time for the tested sentences.
?# of backtrack?is the total number of backtracking steps that oc-curred during parsing.
?Avg.
# of states?
is theaverage number of states for the tested sentences.
?# of dead end?
is the number of sentences forwhich parsing failed.
?# of non-sentential suc-cess?
is the number of sentences for which pars-ing succeeded but did not generate a parse treesatisfying the sentential condition.
?det?
meansthe deterministic shift-reduce parsing proposedin this paper.
?back??
means shift-reduce pars-ing with backtracking at most ?
times for eachsentence.
?du?
indicates that default unificationwas used.
?beam??
means best-first shift-reduceparsing with beam threshold ?.
The upper halfof the table gives the results obtained using goldPOSs, while the lower half gives the results ob-tained using an automatic POS tagger.
The max-imum number of backtracking steps and thebeam threshold were determined by observingthe performance for the development set (Section22) such that the LF was maximized with a pars-ing time of less than 500 ms/sentence (except?beam(403.4)?).
The performance of?beam(403.4)?
was evaluated to see the limit ofthe performance of the beam-search parsing.Deterministic parsing without default unifica-tion achieved accuracy with an LF of around79.1% (Section 23, gold POS).
With backtrack-ing, the LF increased to 83.6%.
Figure 5 showsthe relation between LF and parsing time for thedevelopment set (Section 22, gold POS).
Asseen in the figure, the LF increased as the parsingtime increased.
The increase in LF for determi-nistic parsing without default unification, how-ever, seems to have saturated around 83.3%.Table 1 also shows that deterministic parsingwith default unification achieved higher accuracy,with an LF of around 87.6% (Section 23, goldPOS), without backtracking.
Default unificationis effective: it ran faster and achieved higher ac-curacy than deterministic parsing with normalunification.
The beam-search parsing withoutdefault unification achieved high accuracy, withan LF of around 87.0%, but is still worse thandeterministic parsing with default unification.However, with default unification, it achievedthe best performance, with an LF of around88.5%, in the settings of parsing time less than500ms/sentence for Section 22.For comparison with previous studies usingthe packed parse forest, the performances ofMiyao?s parser, Ninomiya?s parser, Matsuzaki?sparser and Sagae?s parser are also listed in Table1.
Miyao?s parser is based on a probabilisticmodel estimated only by a feature forest.
Nino-miya?s parser is a mixture of the feature forestFigure 5: The relation between LF and the average parsing time (Section 22, Gold POS).82.00%83.00%84.00%85.00%86.00%87.00%88.00%89.00%90.00%0 1 2 3 4 5 6 7 8LFAvg.
parsing time (s/sentence)backback+dubeambeam+du609and an HPSG supertagger.
Matsuzaki?s parseruses an HPSG supertagger and CFG filtering.Sagae?s parser is a hybrid parser with a shallowdependency parser.
Though parsing without thepacked parse forest is disadvantageous to theparsing with the packed parse forest in terms ofsearch space complexity, our model achievedhigher accuracy than Miyao?s parser.?beam(403.4)?
in Table 1 and ?beam?
in Fig-ure 5 show possibilities of beam-search parsing.?beam(403.4)?
was very slow, but the accuracywas higher than any other parsers except Sagae?sparser.Table 2 shows the behaviors of default unifi-cation for ?det+du.?
The table shows the 20most frequent path values that were overwrittenby default unification in Section 22.
In most ofthe cases, the overwritten path values were in theselection features, i.e., subcategorization frames(COMPS:, SUBJ:, SPR:, CONJ:) and modifieespecification (MOD:).
The column of ?Defaulttype?
indicates the default types which wereoverwritten by the strict types in the column of?Strict type,?
and the last column is the frequencyof overwriting.
?cons?
means a non-empty list,and ?nil?
means an empty list.
In most of thecases, modifiee and subcategorization frameswere changed from empty to non-empty and viceversa.
From the table, overwriting of head in-formation was also observed, e.g., ?noun?
waschanged to ?verb.
?6 Conclusion and Future WorkWe have presented shift-reduce parsing approachfor unification-based grammars, based on deter-ministic shift-reduce parsing.
First, we presenteddeterministic parsing for unification-basedgrammars.
Deterministic parsing was difficult inthe framework of unification-based grammarparsing, which often fails because of its hardconstraints.
We introduced default unification toavoid the parsing failure.
Our experimental re-sults have demonstrated the effectiveness of de-terministic parsing with default unification.
Theexperiments revealed that deterministic parsingwith default unification achieved high accuracy,with a labeled F-score (LF) of 87.6% for Section23 of the Penn Treebank with gold POSs.Second, we also presented the best-first parsingwith beam search for unification-based gram-mars.
The best-first parsing with beam searchachieved the best accuracy, with an LF of 87.0%,in the settings without default unification.
De-fault unification further increased LF from87.0% to 88.5%.
By widening the beam width,the best-first parsing achieved an LF of 90.0%.ReferencesAbney, Steven P. 1997.
Stochastic Attribute-ValueGrammars.
Computational Linguistics, 23(4), 597-618.Path StricttypeDefaulttypeFreqSYNSEM:LOCAL:CAT:HEAD:MOD: cons nil 434SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD:MOD: cons nil 237SYNSEM:LOCAL:CAT:VAL:SUBJ: nil cons 231SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: nil cons 125SYNSEM:LOCAL:CAT:HEAD: verb noun 110SYNSEM:LOCAL:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT:HEAD:MOD:cons nil 101SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT:HEAD:MOD:cons nil 96SYNSEM:LOCAL:CAT:HEAD:MOD: nil cons 92SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: verb noun 91SYNSEM:LOCAL:CAT:VAL:SUBJ: cons nil 79SYNSEM:LOCAL:CAT:HEAD: noun verbal 77SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: noun verbal 77SYNSEM:LOCAL:CAT:HEAD: nominal verb 75SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:HEAD:MOD: cons nil 74SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:HEAD:MOD: cons nil 69SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64SYNSEM:LOCAL:CAT:VAL:COMPS:hd:LOCAL:CAT:HEAD: nominal verb 63SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: cons nil 63?
?
?
?Total   10,598Table 2: Path values overwritten by default unification in Section 22.610Berger, Adam, Stephen Della Pietra, and Vincent Del-la Pietra.
1996.
A Maximum Entropy Approach toNatural Language Processing.
Computational Lin-guistics, 22(1), 39-71.Briscoe, Ted and John Carroll.
1993.
Generalizedprobabilistic LR-Parsing of natural language (cor-pora) with unification-based grammars.
Computa-tional Linguistics, 19(1), 25-59.Carpenter, Bob.
1992.
The Logic of Typed FeatureStructures: Cambridge University Press.Carpenter, Bob.
1993.
Skeptical and Credulous De-fault Unification with Applications to Templatesand Inheritance.
In Inheritance, Defaults, and theLexicon.
Cambridge: Cambridge University Press.Charniak, Eugene and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In proc.
of ACL'05, pp.
173-180.Clark, Stephen and James R. Curran.
2004a.
The im-portance of supertagging for wide-coverage CCGparsing.
In proc.
of COLING-04, pp.
282-288.Clark, Stephen and James R. Curran.
2004b.
Parsingthe WSJ using CCG and log-linear models.
In proc.of ACL'04, pp.
104-111.Copestake, Ann.
1993.
Defaults in Lexical Represen-tation.
In Inheritance, Defaults, and the Lexicon.Cambridge: Cambridge University Press.Daum?, Hal III and Daniel Marcu.
2005.
Learning asSearch Optimization: Approximate Large MarginMethods for Structured Prediction.
In proc.
ofICML 2005.Geman, Stuart and Mark Johnson.
2002.
Dynamicprogramming for parsing and estimation of sto-chastic unification-based grammars.
In proc.
ofACL'02, pp.
279-286.Hockenmaier, Julia.
2003.
Parsing with GenerativeModels of Predicate-Argument Structure.
In proc.of ACL'03, pp.
359-366.Johnson, Mark, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators for Sto-chastic ``Unification-Based'' Grammars.
In proc.
ofACL '99, pp.
535-541.Kaplan, R. M., S. Riezler, T. H. King, J. T. MaxwellIII, and A. Vasserman.
2004.
Speed and accuracyin shallow and deep stochastic parsing.
In proc.
ofHLT/NAACL'04.Malouf, Robert and Gertjan van Noord.
2004.
WideCoverage Parsing with Stochastic Attribute ValueGrammars.
In proc.
of IJCNLP-04 Workshop``Beyond Shallow Analyses''.Matsuzaki, Takuya, Yusuke Miyao, and Jun'ichi Tsu-jii.
2007.
Efficient HPSG Parsing with Supertag-ging and CFG-filtering.
In proc.
of IJCAI 2007, pp.1671-1676.Miyao, Yusuke and Jun'ichi Tsujii.
2002.
MaximumEntropy Estimation for Feature Forests.
In proc.
ofHLT 2002, pp.
292-297.Miyao, Yusuke and Jun'ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage HPSGparsing.
In proc.
of ACL'05, pp.
83-90.Nakagawa, Tetsuji.
2007.
Multilingual dependencyparsing using global features.
In proc.
of theCoNLL Shared Task Session of EMNLP-CoNLL2007, pp.
915-932.Ninomiya, Takashi, Takuya Matsuzaki, YusukeMiyao, and Jun'ichi Tsujii.
2007.
A log-linearmodel with an n-gram reference distribution for ac-curate HPSG parsing.
In proc.
of IWPT 2007, pp.60-68.Ninomiya, Takashi, Takuya Matsuzaki, YoshimasaTsuruoka, Yusuke Miyao, and Jun'ichi Tsujii.
2006.Extremely Lexicalized Models for Accurate andFast HPSG Parsing.
In proc.
of EMNLP 2006, pp.155-163.Ninomiya, Takashi, Yusuke Miyao, and Jun'ichi Tsu-jii.
2002.
Lenient Default Unification for RobustProcessing within Unification Based GrammarFormalisms.
In proc.
of COLING 2002, pp.
744-750.Nivre, Joakim and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In proc.
ofCOLING 2004, pp.
64-70.Pollard, Carl and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar: University of ChicagoPress.Ratnaparkhi, Adwait.
1997.
A linear observed timestatistical parser based on maximum entropy mod-els.
In proc.
of EMNLP'97.Riezler, Stefan, Detlef Prescher, Jonas Kuhn, andMark Johnson.
2000.
Lexicalized Stochastic Mod-eling of Constraint-Based Grammars using Log-Linear Measures and EM Training.
In proc.
ofACL'00, pp.
480-487.Sagae, Kenji and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In proc.
ofIWPT 2005.Sagae, Kenji and Alon Lavie.
2006.
A best-first prob-abilistic shift-reduce parser.
In proc.
of COL-ING/ACL on Main conference poster sessions, pp.691-698.Sagae, Kenji, Yusuke Miyao, and Jun'ichi Tsujii.2007.
HPSG parsing with shallow dependencyconstraints.
In proc.
of ACL 2007, pp.
624-631.Yamada, Hiroyasu and Yuji Matsumoto.
2003.
Statis-tical Dependency Analysis with Support VectorMachines.
In proc.
of IWPT-2003.611
