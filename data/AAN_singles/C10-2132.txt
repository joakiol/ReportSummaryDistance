Coling 2010: Poster Volume, pages 1149?1157,Beijing, August 2010Towards Automatic Building of Document KeywordsJoaquim SilvaCITI/DI/FCTUniversidade Nova de Lisboajfs@di.fct.unl.ptGabriel LopesCITI/DI/FCTUniversidade Nova de Lisboagpl@di.fct.unl.ptAbstractDocument keywords are associated todocuments as summarized versions of thedocuments?
content.
Considering that thenumber of documents is quickly growingevery day, the availability of these key-words is very important.
Although, usu-ally keywords are manually written.
Thismotivated us to work on an approach tochange this manual procedure for an auto-matic one.This paper presents a language indepen-dent approach that extracts the most rel-evant Multiword Expressions and singlewords from documents and propose themto describe the core content of each docu-ment.1 IntroductionKeywords provide efficient and sharp access todocuments concerning their main topics, that is,their core content.
Keywords are semantically rel-evant terms, usually being relevant noun-phrasesrather than long full phrases.
Full phrases suchas ?John F Kennedy?s speechwriter hails Obama?saddress?
can be extracted by summarization ap-proaches, but it wouldn?t be appropriate if usedas keywords since it doesn?t mean any maintopic/subtopic.
On the other hand, by using Local-Maxs algorithm (Silva and Lopes, 1999) it is pos-sible to extract Multiword Expressions (MWEs)from documents and, some of the most relevantones relatively to each document can be used asthat document?s descriptor, if properly selected.In this paper we will show that MWEs having2, 3 our 4 words, that is, (2-4)-gram MWEs, arethe most appropriate ones to fit the typical key-words?
semantic sharpness, as would be the caseof ?climate change?, ?American Red Cross?, ?so-cial and economic policy?, etc., rather than (5-7)-grams and larger MWEs addressing more specificmeanings, such as ?skills for lifelong learning pro-cess report?
or ?Assessment of the use of Mag-netic Resonans Tomography?.On the other hand, although MWEs extractedby LocalMaxs algorithm are usually relevant,some of them are semantically vague or simplynot relevant, such as ?general use?
or ?Annex I?,not having the semantic relevance and sharpnessrequired to form keywords.
Other MWEs such as?in case of?
or ?as soon as possible?
may be usefulfor lexicon enrichment to improve Natural Lan-guage Processing, but they are not relevant MWEsto be taken as keywords.During our investigation, we discovered that themedian of the words?
length in each MWE hasa strong influence in the MWE relevance.
Thus,combining this and other factors that influence rel-evance, a metric, Mk, is proposed to better evalu-ate the relevance of each MWE under the purposeof obtaining keywords, and consequently its rele-vance score in each document.Although most document keywords are mul-tiwords, there are some single words , thatis, 1-grams, whose strong and sharp meaningmake them good keywords, such as ?Agricul-ture?, ?salmonella?, among others.
Then, sincewe wanted to include single words in the set of themain keywords of each document, and becauseLocalMaxs algorithm does not extracts 1-grams,we had to select the most informative single words1149from documents using another metric, Sk, alsopresented in this paper.This paper proposes a statistical and language-independent approach to generate document de-scriptors based on the automatic extraction ofthe most informative MWEs and single words,in terms of document summarization, under thepurpose of keywords, taken from each document.Next section analyzes related work.
A brief expla-nation of the LocalMaxs algorithm is presented insection 3.
In section 4 we propose the metrics Mkand Sk and consider other measures.
Results arepresented in section 5 and conclusion are made inthe last section.2 Related WorkIn (Cigarra?n et al, 2005; Liu et al, 2009; Hulth,2004) authors propose extraction of noun phasesand keywords.
However, these are not language-independent approaches, since they use somelanguage-dependent tools such as stop-words re-moving, lemmatization, part-of-speech tagging orsyntactic pattern recognition.In (Delort et al, 2003), authors address the is-sue of Web document summarization by context.They consider the context of a Web document bythe textual content of all documents linking to it.According to the authors, the efficiency of thisapproach depends on the size of the content andcontext of the target document.
However, its ef-ficiency also depends on the existence of links tothe target documents.In (Aliguliyev, 2006) a generic summarizationmethod is proposed.
It extracts the most rel-evance sentences from the source document toform a summary.
The summary can contain themain contents of different topics.
This approachis based on clustering of sentences and, althoughresults are not shown, it does not use language-dependent tools.Other Information Extraction methods rely onpredefined linguistic rules and templates to iden-tify certain entities in text documents (Yangarberand Grishman, 2000; Jacquemin, 2001).
Again,these are not language-independent approaches,despite the good results that they give rise to.Some approaches address specific-domainproblems.
In (Alani et al, 2003), authors proposea method to extract artist information, such asname and date of birth from documents andthen generate his or her biography.
It workswith meta-data triples such as (subject-relation-object), using ontology-relation declarations andlexical information.
Clearly, this approach is notlanguage-independent.
In (Velardi et al, 2001),a method to extract a domain terminology fromavailable documents such as the Web pages isproposed.
The method is based on two measures:Domain Relevance and Domain Consensus thatgive the specificity of a terminological candi-date.
In (Mart?
?nez-Ferna?ndez et al, 2004) theNews specific-domain is addressed.
Again, thisapproach is not language-independent.A supervised approach (Ercan and Cicekli,2007) extracts keywords by using lexical chainsbuilt from the WordNet ontology (Miller, 1991), atool which is not available for every language.Rather than being dependent on specific lan-guages, structured data or domain, we try to findout more general and language-independent fea-tures from free text data.In (Silva and Lopes, 2009), a MWEs extractorand a metric, LeastRvar, extracts keywords fromdocuments.
However, single words are ignoredas possible keywords and their global results areoutperformed by our proposal.3 Using LocalMaxs Algorithm to ExtractKeyword CandidatesWe used the SCP f cohesion metric and the Lo-calMaxs algorithm to extract MWEs from docu-ment corpora.
Although details about these toolsare given in (Silva and Lopes, 1999; Silva et al,1999), here follows a brief description for paperself-containment.
Thus, LocalMaxs is based onthe idea that each n-gram1 has a kind of glue orcohesion sticking the words together within then-gram.
Different n-grams usually have differentcohesion values.
One can intuitively accept thatthere is a strong cohesion within the n-gram ?Gis-card d?Estaing?
i.e.
between the words ?Giscard?and ?d?Estaing?.
However, one cannot say thatthere is a strong cohesion within the 2-grams ?orgiven?
or within the ?of two?.
Thus, in order to1w1 .
.
.
wn or (w1 .
.
.
wn) are also used to denote an n-gram of length n.1150measure the cohesion value not only of 2-grams,but also for every n-gram of any size in the cor-pus, we used the SCP f(.)
metric:SCP f(w1 .
.
.
wn) =p(w1 .
.
.
wn)2Avp (1)Avp = 1n?
1n?1?i=1p(w1 .
.
.
wi) .
p(wi+1 .
.
.
wn)(2)where p(w1 .
.
.
wn) is the probability of the n-gram w1 .
.
.
wn in the corpus.
This way, any sizen-gram is transformed in a pseudo-bigram that re-flects the average cohesion between any two ad-jacent contiguous sub-n-gram of the original n-gram.
Now it is possible to compare cohesionsfrom n-grams of different sizes.3.1 LocalMaxs AlgorithmLocalMaxs is a language independent algorithmto filter out cohesive n-grams of text elements(words, tags or characters), requiring no thresholdarbitrarily assigned.Definition 1.
Let W =w1 .
.
.
wn be an n-gramand g(.)
a cohesion generic function.
And let:?n?1(W ) be the set of g(.)
values for all con-tiguous (n?1)-grams contained in the n-gram W ;?n+1(W ) be the set of g(.)
values for all con-tiguous (n+1)-grams which contain the n-gramW , and let len(W ) be the length (number of ele-ments) of n-gram W .
So, it is stated thatW is a Multi Element Unit (MEU) if and only if,for ?x ?
?n?1(W ),?y ?
?n+1(W )(len(W ) = 2 ?
g(W ) > y) ?
(len(W ) > 2 ?
g(W ) > x + y2 ) .Then, for n-grams with n ?
3, LocalMaxs algo-rithm elects every n-gram whose cohesion valueis greater than the average of two maxima: thegreatest cohesion value found in the contiguous(n?
1)-grams contained in the n-gram, and thegreatest cohesion found in the contiguous (n+1)-grams containing the n-gram.
Thus, in the presentapproach we used LocalMaxs as a MWEs extrac-tor ?
MWEs are MEUs where the elements arewords ?
and used SCP f(.)
cohesion measureas the g(.)
function referred in the algorithm defi-nition above.4 Selecting Keywords from MWEsNot every MWE extracted by LocalMaxs hasequal relevance or semantic sharpness.
SomeMWEs are vague in terms of semantic sharpness,such as ?important meeting?
or ?general use?
;other ones are very specific in terms of the topicthey point to, for example ?Assessment of the useof Magnetic Resonans Tomografy?
; some othersare (2-4)-gram strongly informative MWEs, fit-ting the semantic sharpness of typical keywordssuch as ?computer science?
or ?Food and Agri-culture Organization?, and will be privileged bythe metric we present in subsection 4.4.Some single words have adequate semanticsharpness to be included as keywords, such as?Algebra?
or ?Agriculture?, among others.
How-ever, most single words are not informativeenough for that purpose.As a consequence, we felt the need to workon adequate metrics to value and privilege thestrongly informative MWEs and single words inorder to find keywords in documents.4.1 The Tf-Idf MetricTf?Idf (Term frequency?Inverse document fre-quency) is a statistical metric often used in IRand text mining.
Usually, it is used to evaluatehow important a word is to a document in a cor-pus.
The importance increases proportionally tothe number of times a word/multiword appears inthe document but it is offset by its frequency inthe corpus.
Thus, this is one of the metrics withwhich we will try to privilege the most informa-tive MWEs and 1-grams in each document.Tf?Idf(W,dj) = p(W,dj) .
Idf(W,dj) (3)p(W,dj) =f(W,dj)Ndj(4)Idf(W,dj) = log?D??
{dj : W ?
dj}?
(5)where f(W,dj) if the frequency of word/multi-word W in document dj and Ndj stands for thenumber of words of dj ; ?D?
is the number of doc-uments of the corpus.
So, Tf?Idf(W,dj) willgive a measure of the importance of W , that is aMWE or a single word, within the particular doc-ument dj .
By the structure of term Idf we can see1151that it privileges MWEs and single words occur-ring in less documents, particularly those occur-ring in just one document.4.2 The LeastRvar MetricMost weakly relevant MWE and errors extractedby LocalMaxs begin or end with a so called stop-word, that is a highly frequent word appearing inmost documents.
However, stop-words may ex-ist in the middle of a relevant MWE, for example?United States of America?
or ?Life on Mars?
; butusually not in the leftmost or rightmost word ofthe MWEs.
By considering this, LeastRvar wasproposed in (Silva and Lopes, 2009):LeastRvar(MWEi) = least(Lrv,Rrv) (6)where Lrv = Rvar(leftmostw(MWEi)) ,Rrv = Rvar(rightmostw(MWEi))andRvar(W )= 1?D??di?D(p(W,di)?
p(W, .
)p(W, .))2.
(7)p(W, .)
means the average probability of the wordW considering all documents.
Rvar(.)
is ap-plied to the leftmost and the rightmost word of theMWE:p(W, .)
= 1?D??di?Dp(W,di).
(8)Rvar(W ) measures the variation of the proba-bility of the word W along all documents.
Ap-parently the usual formula of the variance (thesecond moment about the mean), would measurethat variation; however, it would wrongly bene-fit the very frequent words such as ?of?, ?the?
or?and?, among others.
This happens because theabsolute differences between the occurrence prob-abilities of any of those frequent words along alldocuments is high, regardless of the fact that theyusually occur in every document.
These differ-ences are captured and over-valued by the vari-ance since it measures the average value of thequantity (distance frommean)2, ignoring theorder of magnitude of the individual probabilities.Then, Rvar(.)
divides each individual distance,in the original formula of the variance, by the or-der of magnitude of these probabilities, that is, themean probability, given by p(W, .
); see equations7 and 8.Then, LeastRvar(MWEi) is given by theleast Rvar(.)
values considering the leftmostword and the rightmost word of MWEi.
Thisway, LeastRvar(.)
tends to privilege informativeMWEs and penalize those multiword expressionshaving semantically meaningless words in the be-gin or in the end of it.4.3 The LeastCv metricIn oder to try to obtain better results than thoseproduced by LeastRvar, we changed Rvar(.)
toan alternative to measure the relative variationof the probability of the leftmost and rightmostwords in MWEs.
Then we defined:LeastCv(MWEi) = least(Lcv,Rcv) (9)where Lcv = Cv(leftmostw(MWEi)) ,Rcv = Cv(rightmostw(MWEi)) ,Cv(W ) = ?
(W )/?
(W ) , (10)?
(W )=????
1?D?
?di?D(p(W,di) ?
p(W, .
))2 ,(11)and?
(W ) = p(W, .)
; (12)p(W,di) and p(W, .)
have the same meaning as inequation 7.
The reader may recognize Cv(.)
asthe coefficient of variation, which is given by theratio of the standard deviation ?
to the mean ?.Results in section 5 will show that LeastCv alsotends to privilege informative MWEs.4.4 Two New Metrics to Find KeywordsConsidering the results obtained for LeastRvarand LeastCv, as we will see in section 5, wewanted to develop a better metric to find MWEkeywords and another one for single word key-words.
They were built by combining some im-portant factors that we present next.The Median of the MWE Words?
Length:Since most of the semantically meaningless words1152are small and long words usually have sharpmeaning, we considered the median length of thewords in each MWE to help on selecting themost informative MWEs.
By comparison, medianlength showed better results than average length.For example, MWE ?Language Institute?
has anaverage word length of 8.5 characters, but thesemantically equivalent ?Institute of Languages?has a different average length of 6.66.
On the con-trary, the median length for both MWEs presentsmore close values: ((8 + 9)/2 = 8.5) for ?Lan-guage Institute?
and 9 for ?Institute of languages?
(the middle number after sorting the MWE wordslength: 2, 9 and 9).
Thus, because the medianvalues is more robust to outliers than the aver-age value, the length of the meaningless word?of?
was, say, ignored in the median calcula-tion.
In fact, those equivalent meaning MWEshave similar median length values (8.5 and 9),but not so similar average length values (8.5 and6.66).
Furthermore, the robustness of the medianlength enables more similar values when consider-ing MWEs in English and other equivalent MWEsin other languages where stop words are moreused; for example ?e?coles de conduite?
(drivingschools), ?produccio?n de batata?
(potato produc-tion), etc..How Many Words for a Keyword?
As thereader may check in documents having associ-ated keywords, we noticed that the main docu-ment keywords are usually (2-4)-grams.
So, wedefined a factor, Ckl(MWEi), to measure howsimilar is the pseudo number of words of MWEito the typical number of words of keywords.
Wedefine the pseudo number of words of a MWE:Pnw(MWEi) =NumChars(MWEi)Med(MWEi).
(13)NumChars(MWEi) stands for the numberof characters of MWEi and Med(MWEi) isthe median length of its words.
Pnw(MWEi)gives a value close to the number of mean-ingful words of MWEi.
For example,Pnw(?Institute of Languages?)
= 20/9 = 2.2(close to 2); Pnw(?European Council?)
=15/7.5 = 2, etc.. Now, Ckl(.)
is given by:Ckl(MWEi) =1|Pnw(MWEi) ?
T |+ 1,(14)where T is the typical number of words of the key-words.
Maximum value for CkLen(MWEi) is 1;it happens if Pnw(MWEi) equals to T .
As wewill see by the results in section 5, we tried two Tvalues: 2.5 and 3.5; and compared results.The Mk Metric for MWE Keywords: Webuilt Mk(.)
metric by improving LeastRvar(.
):Mk(M)=LeastRvar(M).Med(M).Ckl(M)(15)Thus, Mk(.)
privileges MWEs having not onlyinformative leftmost and rightmost words, butalso having long words and a pseudo number ofwords close to the number of words of typical key-words ?
for reasons of lack of space, we used Minstead of MWEi in equation 15 ?.The Sk Metric for Single Word Keywords:We built Sk(.)
from Rvar(.)
?
see equation 7 ?to measure how meaningful is each single word:Sk(Wi) = Rvar(Wi).Len(Wi) .
(16)Len(Wi) means the length of words Wi.
Thus,Sk(.)
privileges single words having, not only ahigh relative variation of their probabilities alongall documents, but also being long words.5 ResultsWe analyze the quality of the document descrip-tors after applying the LocalMaxs extractor fol-lowed by each of the six different metrics to threedifferent document corpora, each one for a differ-ent language: English, French and Spanish.
Met-rics applied to MWEs were Tf?Idf , LeastCv,LeastRvar, Mk [2.5] ?
that is T = 2.5 in equa-tion 14; and Mk [3.5].
Metrics applied to singlewords were Tf?Idf and Sk.5.1 The Document DescriptorWe decided to represent the core content of eachdocument by using its 15 most informative terms,in the sense of keywords: 11 MWEs and 4 singlewords.
An independent evaluation criteria were1153defined by Prof. Francisca Xavier from the Lin-guistics Department of Universidade Nova de Lis-boa.
It was considered that, for example, ?aimof mission?
and ?16 December 2003?
are wrongkeywords, as the first one is a too vague nounphrase and the second one, just a simple date.
Rel-evant MWEs such as ?nuclear weapons?
and ?fi-nancial crisis?
were evaluated as keywords.
How-ever, although some proposed multi-word expres-sions are not keywords, they are informative in thecontext of the descriptor and correspond to wellformed morphosyntactic tags, for example, ?56%of GDP?
or ?comfort zone?
: these near-miss caseswere classified as half-correct half-wrong terms;the same classification was given to single wordssuch as ?macro-economic?
?
see table 7 ?
which,although it?s not a noun, it?s an informative adjec-tive.Thus, for each document, the extracted MWEsare sorted according to each metric and the top11 MWEs are taken as the document?s MWEs de-scriptor.
The single words of the document arealso sorted according to one of the two appliedmetrics (Tf?Idf or Sk).
By ignoring the rest ofthe MWEs and single words, there is document in-formation which will be lost by these descriptors,but they must be taken as core content descriptors,not as complete/detailed reports of the documents.Although descriptors are composed by MWEs andsingle words, for better comparison of the metrics,tables separately show MWE descriptors or singleword descriptors.
Table 1 shows an example of adocument MWE descriptor resulting from the ap-plication of one of the metrics (Mk) to the docu-ment?s MWEs extracted by LocalMaxs algorithm:5.2 The Multi-Language Corpora TestWe used the EUR-Lex corpora, http://eur-lex.europa.eu/en/, containing European Union lawdocuments about several topics in several Euro-pean languages.
We took 60 documents writtenin each language, English, French and Spanish toform three different sub-corpora.
These are un-structured row text documents.To evaluate the approach?s performance, weused Precision and Recall concepts.
Precision wasgiven by the number of keywords in the set ofTable 1: Example of an English Document MWEDescriptor ?
Application of the Mk [2.5] Metric.enterprise profitscomfort zonemedium-sized enterprisesbrain draincold warBalance of Payment56% of GDPexcessive deficitlooking aheadexports and importsStability and Growth Pactthe 11 most scored MWEs proposed as descrip-tor, by the combination LocalMaxs?metric used,divided by 11.
Recall was given by the numberof keywords that are simultaneously in the doc-ument?s descriptor proposed and in the set madeof the 11 most informative keywords of the docu-ment, divided by 11.According to the criteria mentioned above, thisis the evaluation of the descriptor shown in ta-ble 1, considering Precision: 8 MWEs can be ac-cepted as keywords (1st, 3rd, 4th, 5th, 6th, 8th,10th and 11th); 2 near-miss MWEs (2nd and 7th);and 1 weak or wrong MWE (9th).
So, precisionis (8 + 2 ?
0.5)/11 = 0.818.
Concerning thedocument this descriptor represents, there are 3strong keywords that should be in the descriptor,but they weren?t: ?financial crisis?, ?structural re-forms?
and ?macroeconomic imbalances?.
Thus,Recall is 8/11 = 72.7 for this case.5.3 Results for Different Metrics andLanguagesBy table 2 we may see that for the same metric,Precision or Recall values are similar for English,French and Spanish.
So, this approach does notseem to privilege any of these languages, and webelieve that probably this happens for many otherlanguages, as no specific morphosyntactic infor-mation was used.
Even the difference betweenRecall values for Spanish and English producedby LeastRvar (0.61 and 0.63) would probablydecrease if the test corpora had more documents.Table 2 also shows that Tf?Idf presents the poor-1154Table 2: Precision and Recall Average Values forthe Document MWE Descriptors.Language Metric Precision RecallTf?Idf 0.51 0.35LeastCv 0.62 0.61English LeastRvar 0.65 0.63Mk [2.5] 0.76 0.72Mk [3.5] 0.74 0.68Tf?Idf 0.50 0.35LeastCv 0.62 0.60French LeastRvar 0.64 0.63Mk [2.5] 0.75 0.71Mk [3.5] 0.73 0.68Tf?Idf 0.51 0.34LeastCv 0.61 0.60Spanish LeastRvar 0.64 0.61Mk [2.5] 0.75 0.72Mk [3.5] 0.74 0.67est results.
In fact, due to its structure ?
seeequation 3 ?
we can see that MWEs that occurmany times in just one document are the most val-ued/privileged ones.
This explains why the de-scriptors made by this measure tend to include toospecific/local MWEs, regardless of some impor-tant ones.
Table 3 shows a document descriptorgenerated by the combination LocalMaxs?Tf?Idf : for example MWE ?new Members?
occursin just one document, 10 times; however, ?newMembers?
is not a keyword.
This is the descriptorof the same document from where other descrip-tors were generated by the combinations includingLeastRvar and Mk [2.5], and shown in tables 4and 1.For reasons of space limitation we don?t showdescriptors produced by LeastCv and MK [3.5]metrics.
However, table 2 shows that LeastCvwas outperformed by LeastRvar.
This table alsoshows that Mk [2.5] metric presents the highestPrecision (0.76, 0.75 and 0.75 for English, Frenchand Spanish).
The highest Recall values are alsoobtained for the same metric: 0.72, 0.71 and 0.72for the same languages.Tables 5 and 6 show examples of MWE de-scriptors of French and Spanish documents, by theapplication of Mk [2.5] as it produced the best re-Table 3: Example of an English Document MWEDescriptor ?
Application of the Tf?Idf Metric.in the new Member Statesin the new Membernew MembersSingle Marketincome convergencesome of the new Memberfinancial crisisstructural reformsnew and oldeuro areareap the full benefits of the Single MarketTable 4: Example of an English Document MWEDescriptor ?
Application of the LeastRvar Met-ric.five yearsCold Warold Membersenterprise profitsCentral BankExcessive Deficitmedium-sized enterprisescomfort zone56% of GDP1.5% of GDPbrain drainsults.Tables 7 and 8 show examples of single worddescriptors for the same document described in ta-ble 1.
As we could expect, Precision and Recallvalues for single word descriptors are lower thanthe values for MWEs descriptors, since singleswords are usually semantically less sharp thanmultiwords: see table 9.
Sk shows better perfor-mance than Tf?Idf , specially for Recall.6 ConclusionsKeywords are semantic tags associated to docu-ments, usually declared manually by users.
Thesetags form small document descriptors and enableapplications to access to the summarized docu-ments?
core content.
This paper proposes an ap-proach to automatically generate document de-1155Table 5: Example of a French Document MWEDescriptor ?
Application of the Mk [2.5] Metric.moto-fraises et motofaucheusesagrumeraies et oliveraieshommes TravailFumier liquidefamiliale occupe?eMieux le?gife?rerd?arbres fruitiersSuperficie irrigue?eMain-d?oeuvre non familialeactivite?s lucrativesAlignements d?arbresTable 6: Example of a Spanish Document MWEDescriptor ?
Application of the Mk [2.5] Metric.ingredientes de cosme?ticoscombinaciones de ingredientessometer a ensayoSustancias y Preparadostoxicidad agudairritacio?n ocularfototoxicidad agudaexplicaciones dadascorrosio?n cuta?neaanimales utilizadosSustancias y Preparados Qu?
?micosTable 7: Example of an English Document SingleWord Descriptor ?
Application of the Sk Metric.vulnerabilitiesgrowth-enhancingpost-enlargementmacro-economicTable 8: Example of an English Document Sin-gle Word Descriptor ?
Application of the Tf?IdfMetric.economicnewenlargementreformsTable 9: Precision and Recall Average Values forthe Document Single Word Descriptors.Language Metric Precision RecallEnglish Tf?Idf 0.52 0.36Sk 0.55 0.48French Tf?Idf 0.51 0.37Sk 0.54 0.47Spanish Tf?Idf 0.52 0.37Sk 0.56 0.48scriptors, as a language-independent and domain-independent alternative to related work from otherauthors.
This approach uses LocalMaxs algorithmto extract MWEs, and two new statistical metrics,Mk and Sk, to select the 15 most relevant MWEsand single words from each document in order toform document descriptors.Comparing the results produced by Mk withthe second best metric, LeastRvar, we may con-clude that the introduction of the median of thewords?
length of each MWE and the preferencefor (2-4)-grams, improve the quality of docu-ment descriptors by about 11% and 9% for Pre-cision and Recall, respectively.
Furthermore, bycomparison of Mk [2.5] and Mk [3.5] results weconclude that keywords are mostly (2-3)-grams,rather than (3-4)-grams or longer n-grams.Results also showed that Precision and Recallvalues are similar for the three languages tested(English, French and Spanish), which enable usto expect similar performance to other languages.Apart from the Precision and Recall values, doc-ument descriptors made by this approach does in-deed capture the core content of each document.We believe this may contribute to improve doc-ument summarization.
Future work will includetests in other languages and we will work to im-prove results, specially for single words.ReferencesAlani, Harith, Kim Sanghee, David E. Millard, Mark J.Weal, Paul H. Lewis, Wendy Hall and Nigel Shad-bolt.
2003.
Automatic Extraction of Knowledgefrom Web Documents.
In Proceedings of Workshopof Human Language Technology for the SemanticWeb and Web Services, 2nd International Seman-1156tic Web Conference.
October 20th, Sanibel Island,Florida, USA.Aliguliyev, Ramiz M. 2006.
A Novel Partitioning-Based Clustering Method and Generic DocumentSummarization.
In Proceedings of the 2006IEEE/Web Intelligence/Association for ComputingMachinery and the Intelligent Agent TechnologyInternational Conference (2006 Workshops)(WI-IATW?06).
December 18-22, Hong Kong, China.Cigarra?n, Joan.
M., Anselmo Peas, Julio Gonzalo andFelisa Verdejo.
2005.
Automatic Selection of NounPhrases as Document Descriptors in an FCA-BasedInformation Retrieval System.
B. Ganter and R.Godin (Eds.).
ICFCA 2005, Lecture Notes in Com-puter Science 3403, pp.
49-63.
Springer-Verlag.Ciravegna, Fabio, Alexeie Dingli, David Guthrie andYorick Wilks.
2003.
Mining Web Sites UsingUnsupervised Adaptive Information Extraction.
InProceedings of the 10th Conference of the EuropeanChapter of the Association for Computational Lin-guistics.
April, 12-17.
Budapest, Hungary.Delort, J.-Y., B. Bouchon-Meunier and M. Rifqi.2003.
Enhanced Web Document SummarizationUsing Hyperlinks.
In Proceedings of the fourteenthAssociation for Computing Machinery conferenceon Hypertext and hypermedia.
August 26-30, Not-tingham, UK.Ercan, Gonenc and Ilyas Cicekli.
2007.
Using lexi-cal chains for keyword extraction.
Information Pro-cessing and Management: an International Jour-nal archive.
Volume 43, Issue 6, November, Pages1705-1714, Pergamon Press, Inc.. ISSN 0306-4573.Hulth, Anette.
2004.
Enhancing linguistically ori-ented automatic keyword extraction.
Proceedings ofHuman Language Technology-North American As-sociation for Computational Linguistics 2004 con-ference.
Pag.17-20.
May 02-07.
Boston, Mas-sachusetts.
Publisher: Association for Computa-tional Linguistics, Morristown, NJ, USA.Jacquemin Christian.
2001.
Spotting and DiscoveringTerms through Natural Language Processing.
MITPress, ISBN 0262100851.Liu, Feifan, Deana Pennell, Fei Liu and Yang Liu.2009.
Unsupervised approaches for automatic key-word extraction using meeting transcripts.
Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguis-tics.
May 31-June 05.
Boulder, Colorado.Mart?
?nez-Ferna?ndez, J. L., A.
Garc?
?a-Serrano, P.Mart?
?nez, J. Villena.
2004.
Automatic KeywordExtraction for News Finder.
Lectures Notes in Ar-tificial Intelligence, Springer-Verlag, volume 3094,pages 99?119.Miller, George A.
1991.
The science of words.
Scien-tific American Library, New York.Silva, Joaquim and Gabriel Lopes.
1999.
A LocalMaxima Method and a Fair Dispersion Normaliza-tion for Extracting Multi-word Units.
In Proceed-ings of the 6th Meeting on the Mathematics of Lan-guage, pages 369-381.
23-25 July, University ofCentral Florida, Orlando.Silva, Joaquim and Gabriel Lopes.
2009.
A Docu-ment Descriptor Extractor Based on Relevant Ex-pressions.
14 Encontro Portugus para a Intelign-cia Artificial (Fourteenth Portuguese Conference onArtificial Intelligence).
October 12-15.
Univerity ofAveiro.
Lectures Notes in Artificial Intelligence,Springer-Verlag, volume 5816, pages 646-657.Silva, Joaquim, Gael Dias, Sylvie Guillore?
andGabriel Lopes.
1999.
Using LocalMaxs Al-gorithm for the Extraction of Contiguous andNon-contiguous Multi-word Lexical Units.
9thPortuguese Conference on Artificial Intelligence.September, vora,Portugal.
Lectures Notes in Arti-ficial Intelligence, Pedro Barahora and Jos Alferes(Eds.).
Springer-Verlag, volume 1695, pages 113-132.Yangarber, Roman and Ralph Grishman.
2000.
Ma-chine Learning of Extraction Patterns from Unan-otated Corpora: Position Statement.
Workshop onMachine Learning for Information Extraction.
Heldin conjunction with the 14th European Conferenceon Artificial Intelligence (ECAI).
21 August.
Berlin,Humboldt University.Velardi, Paula, Michele Missikoff, and Roberto Basili.2001.
Identification of relevant terms to supportthe construction of Domain Ontogies.
Associa-tion for Computational Linguistics-European Asso-ciation for Computational Linguistics Workshop onHuman Language Technologies.
July 6-7.
Toulouse,France.1157
