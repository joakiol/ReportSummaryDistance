Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 665?668,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsReranking the Berkeley and Brown Parsers?Mark JohnsonDepartment of ComputingMacquarie UniversitySydney, Australiamjohnson@science.mq.edu.auAhmet Engin UralCognitive and Linguistic SciencesBrown UniversityProvidence, RI, USAaeural@gmail.comAbstractThe Brown and the Berkeley parsers are twostate-of-the-art generative parsers.
Since bothparsers produce n-best lists, it is possible toapply reranking techniques to the output ofboth of these parsers, and to their union.
Wenote that the standard reranker feature set dis-tributed with the Brown parser does not dowell with the Berkeley parser, and propose anextended set that does better.
An ablation ex-periment shows that different parsers benefitfrom different reranker features.1 IntroductionSyntactic parsing is the task of identifying thephrases and clauses in natural language sentences.It has been intensively studied primarily because itis generally believed that identifying syntactic struc-ture is a first step towards semantic interpretation.This paper focuses on parsing the Wall Street Jour-nal (WSJ) section of the University of Pennsylva-nia treebank corpus (Marcus et al, 1993).
Thereare a large number of different approaches to thistask.
For simplicity we focus on two popular gener-ative statistical parsing models: Charniak?s ?Maxi-mum Entropy Inspired?
parser (Charniak and John-son, 2005) and Petrov?s ?split-merge?
parser (Petrovet al, 2006).
We follow conventional informal usageand refer to these as the ?Brown?
and the ?Berkeley?parsers respectively.?
We would like to thank Eugene Charniak and the othermembers of BLLIP for their helpful advice on this work.
Natu-rally all errors remain our own.Briefly, the Berkeley parser is a smoothed PCFGwhose non-terminals are refinements of the originaltreebank grammar obtained by an automatic split-merge procedure, while the Brown parser is effec-tively a smoothed PCFG whose non-terminals en-code a wide variety of manually chosen condition-ing information, such as heads, governors, etc.
TheBerkeley parser is usually viewed as unlexicalized(although the preterminals may be split so finelythat they may be viewed as identifying lexical clus-ters), while essentially every distribution used inthe Brown parser conditions on lexical information.Even from this cursory description it is clear thatthese parsers parsers extract generalizations from thetraining data in different ways.This paper applies reranking (Collins and Koo,2005) to the n-best output of both parsers individ-ually, as well as to an n-best list consisting of theunion of the outputs of both parsers.
We are inter-ested to see whether the same kinds of features im-prove the performance of both the Berkeley and theBrown parsers, or whether successful reranking re-quires features that are specially tuned to the parserit is applied to.
Finally, we are interested in theperformance of the reranker trained on the union n-best lists.
Combining the output of multiple parsersin other more complex ways has been previouslydemonstrated to improve overall accuracy, so it is in-teresting to see if the relatively simple method usedhere improves parsing accuracy as well.The approach of Zhang et al (2009) is closest tothe work described here.
They combine n-best listsproduced by the same parsers as we do, but use onlya relatively small set of features (the log probabil-665Trees Reranker featuresstandard extendedBerkeley 91.6 91.7Brown 91.8 91.6Combined 91.8 91.9Table 1: The f-scores on section 22 of rerankers trainedon folds 1?18 by minimizing a regularized ?MaxEnt?
ob-jective (negative log likelihood with a Gaussian regular-izer) using L-BFGS.
The weight of the regularizer wastuned to optimize f-score on folds 19?20.ity of the parses plus a constituent overlap feature),while we investigate models with millions of fea-tures here.
They report a higher f-score than we dowhen they replace the generative Brown parser withthe the self-trained discriminatively-reranked parserof McClosky et al (2006), but with inputs providedby the generative Berkeley and Brown n-best parsersthey report an f-score of 91.43 on section 23, whichis consistent with the results reported here.2 Experimental setupWe ran both parsers in 50-best mode, and con-structed 20-fold cross-validated training data as de-scribed in Collins and Koo (2005) and Charniakand Johnson (2005), i.e., the trees in sections 2?21of the WSJ treebank were divided into 20 equal-sized folds, and the parses for each fold were gen-erated by a parser trained on the trees in the otherfolds.
Then sections 22, 23 and 24 were parsed us-ing the standard ?out-of-the-box?
parser.
Follow-ing the suggestion in Collins and Koo (2005), in or-der to avoid over-training on section 23 all rerank-ing experiments reported here (except the final one)used folds 1?18 as training data, used folds 19?20as development data and used section 22 as test data.
(The averaged perceptron algorithm does not requiredevelopment data, so the experiments using that al-gorithm report averages over folds 19?20 and sec-tion 22).The Berkeley parser can be run in many modes;in order to produce the 20-fold training data we ranthe Berkeley trainer with 6 splits, and ran the re-sulting parsers in ?accurate?
mode.
It failed to pro-duce any parses for 12 sentences in sections 2?21and one sentence in section 24.
The Brown parserwas trained using the ?out-of-the-box?
settings, andproduced parses for all sentences.Using the reranker features distributed with theBrown reranker (Charniak and Johnson, 2005),which we call the ?standard?
set below, we ob-tained no overall improvement in f-score when ei-ther reranking the Berkeley parser n-best lists alone,or when the Berkeley parses were combined with theBrown parses.However, it is possible that these results reflectthe fact that the features used by the reranker werechosen because they improve the Brown parser, i.e.,they are the result of feature selection based onreranking the Brown parser?s n-best lists.
In orderto determine if this is the case, we developed an ?ex-tended?
feature set that incorporates a wider set offeatures, specifically including features that captureglobal properties of the tree that might be harder forthe Berkeley parser to learn.Our extended feature set consists of4,256,553 features, which are instances of162 feature classes, which in turn are groupedinto 20 feature ?super-classes?.
By contrast, thestandard feature set contains 1,333,950 features in90 feature classes, grouped into 14 super-classes.A brief description of the extended feature setsuper-classes follows:Parser: an indicator feature indicating which parsersgenerated this parse,RelLogP: the log probability of this parse according toeach parser,InterpLogCondP: an indicator feature based on thebinned log conditional probability according toeach parser,RightBranch: an indicator function of each node thatlies on the right-most branch of the parse tree,Heavy: an indicator function based on the size and lo-cation of each nonterminal (designed to identify thelocations of ?heavy?
phrases),LeftBranchLength: an indicator function of the binnedlength of each left-branching chain,RightBranchLength: an indicator function of thebinned length of each right-branching chain,Rule: an indicator function of parent and children cate-gories, optionally with head POS annotations,NNGram: and indicator function of parent and n-gramsequences of children categories, optionally head666ParserRelLogPInterpLogCondPRightBranchHeavyLeftBranchLengthRightBranchLengthRuleNNGramHeadsSynSemHeadsRBContextSubjVerbAgrCoParCoLenParWordWProjWEdgesNGramTreeHeadTree-0.3-0.2-0.10.00.1BerkeleyBrownCombinedFigure 1: The average change in f-score on folds 19?20 and section 22 caused by removing a feature superclassfrom the extended feature set and retraining.
Difference in scores less that 0.1% are probably not significant.
In thisexperiment all rerankers were trained using the averaged perceptron algorithm.
With the full extended feature set,rerankers trained with the averaged perceptron algorithm achieve f-scores of 91.2% on both the Berkeley and Brownparses, and 91.6% on the combined parses.annotated, inspired by the n-gram rule features de-scribed in Collins and Koo (2005),Heads: an indicator function of ?head-to-head?
depen-dencies,SynSemHeads: an indicator function of the pair of syn-tactic (i.e., functional) and semantic (i.e., lexical)heads of each non-terminal,RBContext: an indicator function of how much eachsubtree deviates from from right-branching,SubjVerbAgr: an indicator function of whether subject-verb agreement is violated,CoPar: an indicator function that fires when conjoinedphrases in a coordinate structure have approxi-mately parallel syntactic structure,CoLenPar: an indicator function that fires when con-joined phrases in a coordinate structure have ap-proximately the same length,Word: an indicator function that identifies words andtheir preterminals,WProj: an indicator function that identifies words andtheir phrasal projections up to their maximal pro-jection,WEdges: an indicator function that identifies the wordsand POS tags appearing at the edges of each nonter-minal,NGramTree: an indicator function of the subtree con-sisting of nodes connecting each pair of adjacentwords in the parse tree, andHeadTree: a tree fragment consisting of a head wordand its projection up to its maximal projection, plusall of the siblings of each node in this sequence (thisis like an auxiliary tree in a TAG).The InterpLogCondP features were designed tocapture non-linearities in the way that the Berke-ley and Brown parsers assign probabilities to trees.We deliberately added features that incorporated lin-guistic notions such as head, governor and maximalprojection, as the Berkeley parser does not explic-itly condition on such information (in contrast to theBrown parser, which does).In fact, as the reader can verify the differences inf-scores between rerankers containing the extendedfeatures and the standard features is minimal.
Inorder to better study the importance of the variousfeatures we conducted an ablation study, in whichwe trained rerankers which were missing one featuresuperclass from the 20 superclasses of the extendedfeature set.
In order to speed training time we usedthe averaged perceptron algorithm (Collins, 2002)(it converges an order of magnitude faster than the L-BFGS algorithm we used in the other experiments,but the f-score of the model estimated with the av-eraged perceptron is approximately 0.1% lower thanwhen using L-BFGS).
The results from this experi-ment are shown in Figure 1.
The averaged percep-tron algorithm does not rely on the development data667(folds 19?20), so the results we report are average f-scores on the development data and on section 22(we did this because the differences are small, soa larger evaluation set may be able to detect differ-ences more reliably).It is interesting that linguistically-informed fea-tures (such as Heads, SynSemHeads and HeadTree)seem to be much more important when rerankingthe combined n-best lists than when reranking theoutput of either parser alone.
This suggests that thelog probability scores from both parsers are inter-nally consistent, but need to be recalibrated whenthe parses are combined.
The log probability scoresfrom the parsers themselves (in the form of the In-terpLogCondP feature) are also supplying useful in-formation that the reranker features on their own arenot providing.
Finally, the WEdges feature, whichidentifies the words and POS at the left and rightboundaries of each nonterminal, also provides ex-tremely useful information, especially for rerankingthe Berkeley parser.3 ConclusionReranking is a straight-forward method for improv-ing the accuracy of n-best parsers.
While onemight have hoped that reranking the n-best outputof the Berkeley parser, or the union of the outputsof the Berkeley and Brown parsers, would dramat-ically improve overall f-score, this seems not to bethe case.
It?s possible that the features of currentrerankers have been implicitly designed to work wellwith parsers like the Brown parser, but a rerankerwith a dramatically enlarged feature set performsonly marginally better.
This result was confirmedby training a reranker with the extended features onthe union of the output of the Berkeley and Brownparsers on sections 2?21 and testing on section 23(i.e., the standard WSJ parsing evaluation), whichachieved an f-score of 91.49%; approximately 0.1%higher than a reranker with the standard feature settrained on the output of the Brown parser alone.AcknowledgmentsThis research was funded by NSF awards 0530118,0544127 and 0631667.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages173?180, Ann Arbor, Michigan, June.
Association forComputational Linguistics.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?70.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 152?159, NewYork City, USA, June.
Association for ComputationalLinguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia.
Association for Computational Linguistics.Hui Zhang, Min Zhang, Chew Lim Tan, and HaizhouLi.
2009.
K-best combination of syntactic parsers.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1552?1560, Singapore.
Association for ComputationalLinguistics.668
