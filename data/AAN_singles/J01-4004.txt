A Machine Learning Approach toCoreference Resolution of Noun PhrasesWee Meng Soon*DSO National LaboratoriesDanie l  Chung Yong Lim*DSO National LaboratoriesHwee Tou Ng tDSO National LaboratoriesIn this paper, we present a learning approach to coreference resolution of noun phrases in unre-stricted text.
The approach learns from a small, annotated corpus and the task includes resolvingnot just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.
It alsodoes not restrict the entity types of the noun phrases; that is, coreference is assigned whetherthey are of "organization," "person," or other types.
We evaluate our approach on common datasets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, in-dicating that on the general noun phrase coreference task, the learning approach olds promiseand achieves accuracy comparable to that of nonlearning approaches.
Our system is the firstlearning-based system that offers performance comparable to that of state-of-the-art nonlearningsystems on these data sets.1.
IntroductionCoreference r solution is the process of determining whether two expressions in nat-ural language refer to the same entity in the world.
It is an important subtask in nat-ural language processing systems.
In particular, information extraction (IE) systemslike those built in the DARPA Message Understanding Conferences (Chinchor 1998;Sundheim 1995) have revealed that coreference r solution is such a critical componentof IE systems that a separate coreference subtask has been defined and evaluated sinceMUC-6 (MUC-6 1995).In this paper, we focus on the task of determining coreference r lations as definedin MUC-6 (MUC-6 1995) and MUC-7 (MUC-7 1997).
Specifically, a coreference r lationdenotes an identity of reference and holds between two textual elements known asmarkables, which can be definite noun phrases, demonstrative noun phrases, propernames, appositives, ub-noun phrases that act as modifiers, pronouns, and so on.
Thus,our coreference task resolves general noun phrases and is not restricted to a certaintype of noun phrase such as pronouns.
Also, we do not place any restriction on thepossible candidate markables; that is, all markables, whether they are "organization,""person," or other entity types, are considered.
The ability to link coreferring nounphrases both within and across entences i  critical to discourse analysis and languageunderstanding in general.
* 20 Science Park Drive, Singapore 118230.
E-mail: sweemeng@dso.org.sgt 20 Science Park Drive, Singapore 118230.
E-maih nhweetou@dso.org.sg.
This author is also affiliatedwith the Department of Computer Science, School of Computing, National University of Singapore.Web address: http://www.comp.nus.edu.sg/~nght:~ 20 Science Park Drive, Singapore 118230.
E-maih lchungyo@dso.org.sg(~) 200l Association for Computational LinguisticsComputational Linguistics Volume 27, Number 4Free text :~ TokenizatiOnsentence & Ii ~ Morphological ~ISegmentation \[ ~ ProcessingPOS taggerNamed Entity ~ Nested \[iNoun Phrase :~Recognition ~ Extraction \[Semantic ClassDeterminationFigure 1System architecture of natural anguage processing pipeline.Noun PhraseIdentificationMarkables2.
A Machine Learning Approach to Coreference ResolutionWe adopt a corpus-based, machine learning approach to noun phrase coreferenceresolution.
This approach requires a relatively small corpus of training documents thathave been annotated with coreference chains of noun phrases.
All possible markablesin a training document are determined by a pipeline of language-processing modules,and training examples in the form of feature vectors are generated for appropriate pairsof markables.
These training examples are then given to a learning algorithm to builda classifier.
To determine the coreference chains in a new document, all markables aredetermined and potential pairs of coreferring markables are presented to the classifier,which decides whether the two markables actually corefer.
We give the details of thesesteps in the following subsections.2.1 Determination of MarkablesA prerequisite for coreference resolution is to obtain most, if not all, of the possi-ble markables in a raw input text.
To determine the markables, a pipeline of naturallanguage processing (NLP) modules is used, as shown in Figure 1.
They consist of to-kenization, sentence segmentation, morphological processing, part-of-speech tagging,noun phrase identification, named entity recognition, nested noun phrase extraction,and semantic lass determination.
As far as coreference resolution is concerned, thegoal of these NLP modules is to determine the boundary of the markables, and toprovide the necessary information about each markable for subsequent generation offeatures in the training examples.Our part-of-speech tagger is a standard statistical tagger based on the HiddenMarkov Model (HMM) (Church 1988).
Similarly, we built a statistical HMM-basednoun phrase identification module that determines the noun phrase boundaries olelybased on the part-of-speech tags assigned to the words in a sentence.
We also im-plemented a module that recognizes MUC-style named entities, that is, organization,person, location, date, time, money, and percent.
Our named entity recognition moduleuses the HMM approach of Bikel, Schwartz, and Weischedel (1999), which learns froma tagged corpus of named entities.
That is, our part-of-speech tagger, noun phraseidentification module, and named entity recognition module are all based on HMMsand learn from corpora tagged with parts of speech, noun phrases, and named entities,respectively.
Next, both the noun phrases determined by the noun phrase identifica-tion module and the named entities are merged in such a way that if the noun phraseoverlaps with a named entity, the noun phrase boundaries will be adjusted to subsumethe named entity.522Soon, Ng, and Lira Coreference ResolutionThe nested noun phrase extraction module subsequently accepts the noun phrasesand determines the nested phrases for each noun phrase.
The nested noun phrasesare divided into two groups:..Nested noun phrases from possessive noun phrases.
Consider twopossessive noun phrases marked by the noun phrase module, hislong-range strategy and Eastern's parent.
The nested noun phrase for thefirst phrase is the pronoun his, while for the second one, it is the propername Eastern.Nested noun phrases that are modifier nouns (or prenominals).
Forexample, the nested noun phrase for wage reductions is wage, and forUnion representatives, it is Union.Finally, the markables needed for coreference r solution are the union of the nounphrases, named entities, and nested noun phrases found.
For markables without anynamed entity type, semantic lass is determined by the semantic lass determinationmodule.
More details regarding this module are given in the description of the seman-tic class agreement feature.To achieve acceptable recall for coreference resolution, it is most critical that theeligible candidates for coreference be identified correctly in the first place.
In order totest our system's effectiveness in determining the markables, we attempted to matchthe markables generated by our system against those appearing in the coreferencechains annotated in 100 SGML documents, a subset of the training documents availablein MUC-6.
We found that our system is able to correctly identify about 85% of thenoun phrases appearing in coreference chains in the 100 annotated SGML documents.Most of the unmatched noun phrases are of the following types:1...Our system generated a head noun that is a subset of the noun phrase inthe annotated corpus.
For example, Saudi Arabia, the cartel's biggestproducer was annotated as a markable, but our system generated onlySaudi Arabia.Our system extracted a sequence of words that cannot be considered as amarkable.Our system extracted markables that appear to be correct but do notmatch what was annotated.
For example, our system identified selectivewage reductions, but wage reductions was annotated instead.2.2 Determinat ion of Feature VectorsTo build a learning-based coreference engine, we need to devise a set of features that isuseful in determining whether two markables corefer or not.
In addition, these featuresmust be generic enough to be used across different domains.
Since the MUC-6 andMUC-7 tasks define coreference guidelines for all types of noun phrases and differenttypes of noun phrases behave differently in terms of how they corefer, our featuresmust be able to handle this and give different coreference decisions based on differenttypes of noun phrases.
In general, there must be some features that indicate the type ofa noun phrase.
Altogether, we have five features that indicate whether the markablesare definite noun phrases, demonstrative noun phrases, pronouns, or proper names.There are many important knowledge sources useful for coreference.
We wantedto use those that are not too difficult to compute.
One important factor is the distance523Computational Linguistics Volume 27, Number 4between the two markables.
McEnery, Tanaka, and Botley (1997) have done a study onhow distance affects coreference, particularly for pronouns.
One of their conclusions isthat the antecedents of pronouns do exhibit clear quantitative patterns of distribution.The distance feature has different effects on different noun phrases.
For proper names,locality of the antecedents may not be so important.
We include the distance featureso that the learning algorithm can best decide the distribution for different classes ofnoun phrases.There are other features that are related to the gender, number, and semantic lassof the two markables.
Such knowledge sources are commonly used for the task ofdetermining coreference.Our feature vector consists of a total of 12 features described below, and is derivedbased on two extracted markables, i and j, where i is the potential antecedent and jis the anaphor.
Information eeded to derive the feature vectors is provided by thepipeline of language-processing modules prior to the coreference engine.1.
Distance Feature (DIST): Its possible values are 0,1, 2, 3 .
.
.
.
.
This featurecaptures the distance between i and j.
If i and j are in the same sentence,the value is 0; if they are one sentence apart, the value is 1; and so on.2.
i-Pronoun Feature (I_PRONOUN): Its possible values are true or false.
Ifi is a pronoun, return true; else return false.
Pronouns include reflexivepronouns (himself, herself), personal pronouns (he, him, you), andpossessive pronouns (hers, her).3. j-Pronoun Feature (J_PRONOUN): Its possible values are true or false.
Ifj is a pronoun (as described above), then return true; else return false.4.
String Match Feature (STR_MATCH): Its possible values are true orfalse.
If the string of i matches the string of j, return true; else returnfalse.
We first remove articles (a, an, the) and demonstrative pronouns(this, these, that, those) from the strings before performing the stringcomparison.
Therefore, the license matches this license, that computermatches computer.5.
Definite Noun Phrase Feature (DEF_NP): Its possible values are true orfalse.
In our definition, a definite noun phrase is a noun phrase thatstarts with the word the.
For example, the car is a definite noun phrase.
Ifj is a definite noun phrase, return true; else return false.6.
Demonstrative Noun Phrase Feature (DEM_NP): Its possible values aretrue or false.
A demonstrative noun phrase is one that starts with theword this, that, these, or those.
If j is a demonstrative noun phrase, thenreturn true; else return false.7.
Number Agreement Feature (NUMBER): Its possible values are true orfalse.
If i and j agree in number (i.e., they are both singular or bothplural), the value is true; otherwise false.
Pronouns such as they and themare plural, while it, him, and so on, are singular.
The morphological rootof a noun is used to determine whether it is singular or plural if thenoun is not a pronoun.8.
Semantic Class Agreement Feature (SEMCLASS): Its possible valuesare true, false, or unknown.
In our system, we defined the followingsemantic lasses: "female," male," "person," organization," "location,""date," "time," "money, .... percent," and "object."
These semantic lasses524Soon, Ng, and Lim Coreference Resolution.10.11.are arranged in a simple ISA hierarchy.
Each of the "female" and "male"semantic lasses is a subclass of the semantic lass "person," while eachof the semantic lasses "organization," "location," date," "time,""money," and "percent" is a subclass of the semantic lass "object."
Eachof these defined semantic lasses is then mapped to a WordNet synset(Miller 1990).
For example, "male" is mapped to the second sense of thenoun male in WordNet, "location" is mapped to the first sense of thenorm location, and so on.The semantic lass determination module assumes that the semanticclass for every markable extracted is the first sense of the head noun ofthe markable.
Since WordNet orders the senses of a noun by theirfrequency, this is equivalent to choosing the most frequent sense as thesemantic lass for each norm.
If the selected semantic lass of a markableis a subclass of one of our defined semantic lasses C, then the semanticclass of the markable is C; else its semantic lass is "unknown.
"The semantic lasses of markables i and j are in agreement if one isthe parent of the other (e.g., chairman with semantic lass "person" andMr.
Lim with semantic lass "male"), or they are the same (e.g., Mr. Limand he, both of semantic lass "male").
The value returned for such casesis true.
If the semantic lasses of i and j are not the same (e.g., IBM withsemantic lass "organization" and Mr. Lim with semantic lass "male"),return false.
If either semantic lass is "unknown," then the head nounstrings of both markables are compared.
If they are the same, return true;else return unknown.Gender Agreement Feature (GENDER): Its possible values are true,false, or unknown.
The gender of a markable is determined in severalways.
Designators and pronouns such as Mr., Mrs., she, and he, candetermine the gender.
For a markable that is a person's name, such asPeter H. Diller, the gender cannot be determined by the above method.
Inour system, the gender of such a markable can be determined ifmarkables are found later in the document hat refer to Peter H. Diller byusing the designator form of the name, such as Mr. Diller.
If thedesignator form of the name is not present, the system will look throughits database of common human first names to determine the gender ofthat markable.
The gender of a markable will be unknown for nounphrases uch as the president and chief executive officer.
The gender of othermarkables that are not "person" is determined by their semantic lasses.Unknown semantic lasses will have unknown gender while those thatare objects will have "neutral" gender.
If the gender of either markable ior j is unknown, then the gender agreement feature value is unknown;else if i and j agree in gender, then the feature value is true; otherwise itsvalue is false.Both-Proper-Names Feature (PROPER_NAME): Its possible values aretrue or false.
A proper name is determined based on capitalization.Prepositions appearing in the name such as of and and need not be inuppercase.
If i and j are both proper names, return true; else return false.Alias Feature (ALIAS): Its possible values are true or false.
If i is an aliasof j or vice versa, return true; else return false.
That is, this feature valueis true if i and j are named entities (person, date, organization, etc.)
thatrefer to the same entity.
The alias module works differently depending525Computational Linguistics Volume 27, Number 412.on the named entity type.
For i and j that are dates (e.g., 01-08 andJan.
8), by using string comparison, the day, month, and year values areextracted and compared.
If they match, then j is an alias of i.
For i and jthat are "person," such as Mr. Simpson and Bent Simpson, the last wordsof the noun phrases are compared to determine whether one is an aliasof the other.
For organization ames, the alias function also checks foracronym match such as IBM and International Business Machines Corp. Inthis case, the longer string is chosen to be the one that is converted intothe acronym form.
The first step is to remove all postmodif iers such asCorp.
and Ltd. Then, the acronym function considers each word in turn,and if the first letter is capitalized, it is used to form the acronym.
Twovariations of the acronyms are produced: one with a period after eachletter, and one without.Apposit ive Feature (APPOSITIVE): Its possible values are true or false.If j is in apposit ion to i, return true; else return false.
For example, themarkable the chairman of Microsoft Corp. is in apposit ion to Bill Gates in thesentence Bill Gates, the chairman of Microsoft Corp .
.
.
.
.
.
Our systemdetermines whether j is a possible apposit ive construct by first checkingfor the existence of verbs and proper punctuation.
Like the aboveexample, most apposit ives do not have any verb; and an apposit ive isseparated by a comma from the most immediate antecedent, i, to whichit refers.
Further, at least one of i and j must  be a proper name.
TheMUC-6 and MUC-7 coreference task definitions are slightly different.
InMUC-6, j needs to be a definite noun phrase to be an appositive, whileboth indefinite and definite noun phrases are acceptable in MUC-7.As an example, Table 1 shows the feature vector associated with the antecedent i,Frank Newman, and the anaphor j, vice chairman, in the following sentence:(1) Separately, Clinton transition officials said that Frank Newman, 50, vicechairman and chief financial officer of BankAmerica Corp., is expected tobe nominated as assistant Treasury secretary for domestic finance.Tab le  1Feature vector of the markable pair (i = Frank Newman, j = vice chairman).Feature  Va lue  CommentsDIST 0 i and j are in the same sentenceI_PRONOUN - i is not a pronounJ~RONOUN - j is not a pronounSTR_MATCH - i and j do not matchDEF_NP - j is not a definite noun phraseDEMaNP - j is not a demonstrative noun phraseNUMBER + i and j are both singularSEMCLASS 1 i and j are both persons (This feature has three values:false(0), true(l), unknown(2).
)GENDER 1 i and j are both males (This feature has three values:false(0), true(l), unknown(2).
)PROPER_NAME - Only i is a proper nameALIAS - j is not an alias of iAPPOSITIVE + j is in apposition to i526Soon, Ng, and Lira Coreference ResolutionBecause of capitalization, markables in the headlines of MUC-6 and MUC-7 doc-uments are always considered proper names even though some are not.
Our systemsolves this inaccuracy by first preprocessing a headline to correct the capitalizationbefore passing it into the pipeline of NLP modules.
Only those markables in the head-line that appear in the text body as proper names have their capitalization changedto match those found in the text body.
All other headline markables are changed tolowercase.2.3 Generating Training ExamplesConsider a coreference chain A1 - A2 - A3 - A4 found in an annotated training docu-ment.
Only pairs of noun phrases in the chain that are immediately adjacent (i.e., A1 -A2, A2 - A3, and A3 - A4) are used to generate the positive training examples.
Thefirst noun phrase in a pair is always considered the antecedent, while the second isthe anaphor.
On the other hand, negative training examples are extracted as follows.Between the two members of each antecedent-anaphor pair, there are other markablesextracted by our language-processing modules that either are not found in any coref-erence chain or appear in other chains.
Each of them is then paired with the anaphorto form a negative xample.
For example, if markables a, b, and B1 appear betweenA1 and A2, then the negative xamples are a - A2, b - A2, and B1 - A2.
Note that aand b do not appear in any coreference chain, while B1 appears in another coreferencechain.For an annotated noun phrase in a coreference chain in a training document, thesame noun phrase must be identified as a markable by our pipeline of language-processing modules before this noun phrase can be used to form a feature vectorfor use as a training example.
This is because the information ecessary to derivea feature vector, such as semantic lass and gender, is computed by the language-processing modules.
If an annotated noun phrase is not identified as a markable, itwill not contribute any training example.
To see more clearly how training examplesare generated, consider the following four sentences:?
Sentence 11.
(Eastern Air)a1 Proposes (Date For Talks on ((Pay)cl-CUt)dlPlan)hi2.
(Eastern Air)l Proposes (Date)2 For (Talks)3 on (Pay-Cut Plan)4?
Sentence 21.
(Eastern Airlines)a2 executives notified (union)el eaders that thecarrier wishes to discuss selective ((wage)c2 reductions)d2 on(Feb. 3)b2.2.
((Eastern Airlines)5 executives)6 notified ((union)7 leaders)8 that(the carrier)9 wishes to discuss (selective (wage)10 reductions)non (Feb.
3)12.1..Sentence 3((Union)e2 representatives who could be reached)f1 said (they)f2hadn't decided whether (they)f3 would respond.
((Union)13 representatives)14 who could be reached said (they)ishadn't decided whether (they)16 would respond.527Computational Linguistics Volume 27, Number 4?
Sentence 4..By proposing (a meeting date)b3, (Eastern)a3 moved one stepcloser toward reopening current high-cost contract agreementswith ((its)a4 unions)e3.By proposing (a meeting dateh7, (Eastern)18 moved (one step)19closer toward reopening (current high-cost contractagreements)20 with ((its)a1 UniOnS)a2.Each sentence is shown twice with different noun phrase boundaries.
Sentenceslabeled (1) are obtained irectly from part of the training document.
The letters in thesubscripts uniquely identify the coreference chains, while the numbers identify thenoun phrases.
Noun phrases in sentences labeled (2) are extracted by our language-processing modules and are also uniquely identified by numeric subscripts.Let's consider chain e, which is about the union.
There are three noun phrasesthat corefer, and our system managed to extract he boundaries that correspond toall of them: (union)7 matches with (union)el, (union)13 with (union)e2, and (its unions)22with (its unions)e3.
There are two positive training examples formed by ((union)13, (itsunions)22) and ((union)7, (union)13).
Noun phrases between (union)7 and (union)13 thatdo not corefer with (union)13 are used to form the negative xamples.
The negativeexamples are ((the carrier)9, (union)is), ((wage)lo, (union)13), ((selective wage reductions)11,(union)13), and ((Feb. 3)12, (union)13).
Negative examples can also be found similarlybetween ((union)13, (its unions)22).As another example, neither noun phrase in chain d, (Pay-Cut)all and (wage reduc-tions)a2, matches with any machine-extracted noun phrase boundaries.
In this case, nopositive or negative xample is formed for noun phrases in chain d.2.4 Building a ClassifierThe next step is to use a machine learning algorithm to learn a classifier based on thefeature vectors generated from the training documents.
The learning algorithm usedin our coreference engine is C5, which is an updated version of C4.5 (Quinlan 1993).C5 is a commonly used decision tree learning algorithm and thus it may be consideredas a baseline method against which other learning algorithms can be compared.2.5 Generating Coreference Chains for Test DocumentsBefore determining the coreference chains for a test document, all possible markablesneed to be extracted from the document.
Every markable is a possible anaphor, andevery markable before the anaphor in document order is a possible antecedent ofthe anaphor, except when the anaphor is nested.
If the anaphor is a child or nestedmarkable, then its possible antecedents must not be any markable with the same rootmarkable as the current anaphor.
However, the possible antecedents can be otherroot markables and their children that are before the anaphor in document order.
Forexample, consider the two root markables, Mr. Tom's daughter and His daughter's eyes,appearing in that order in a test document.
The possible antecedents of His cannot beHis daughter or His daughter's eyes, but can be Mr. Tom or Mr. Tom's daughter.The coreference r solution algorithm considers every markable j starting from thesecond markable in the document to be a potential candidate as an anaphor.
For eachj, the algorithm considers every markable i before j as a potential antecedent.
For eachpair i and j, a feature vector is generated and given to the decision tree classifier.
Acoreferring antecedent is found if the classifier eturns true.
The algorithm starts fromthe immediately preceding markable and proceeds backward in the reverse order of528Soon, Ng, and Lim Coreference Resolutionthe markables in the document until there is no remaining markable to test or anantecedent is found.As an example, consider the following text with markables already detected bythe NLP modules:(2) (Ms. Washington)73's candidacy is being championed by (severalpowerful awmakers)74 including ((her)76 boss)75, (Chairman JohnDingell)77 (D., (Mich.)78) of (the House Energy and CommerceCommittee)79.
(She)so currently is (a counsel)81 to (the committee)s2.
(Ms.Wash ington)s3  and (Mr. DingeU)s4 have been considered (allies)s5 of (the(securities)s7 exchanges)s6, while (banks)s8 and ((futures)90 exchanges)89have often fought with (themM.We will consider how the boldfaced chains are detected.
Table 2 shows the pairsof markables tested for coreference to form the chain for Ms. Washington-her-She-Ms.Washington.
When the system considers the anaphor, (her)76, all preceding phrases,except (her boss)75, are tested to see whether they corefer with it.
(her boss)75 is nottested because (her)76 is its nested noun phrase.
Finally, the decision tree determinesthat the noun phrase (Ms. Washington)73 corefers with (her)76.
In Table 2, we only showthe system considering the three anaphors (her)76, (She)so, and (Ms. Washington)s3, inthat order.Table 2Pairs of markables that are tested in forming the coreference hain Ms. Washington-her-She-Ms.Washington.
The feature vector format: DIST, SEMCLASS, NUMBER, GENDER,PROPER_NAME, ALIAS, ,_PRONOUN, DEF_NP, DEMANP, STR_MATCH, APPOSITIVE,Id~RONOUN.Antecedent  Anaphor  Feature Vector Corefers?
(several powerfullawmakers)74(Ms. Washington)73(the House Energyand CommerceCommittee)79(Mich.)TS(Chairman JohnDingell)77(her) 76(the committee)s2(a counsel)81(She)so(the House Energyand CommerceCommittee) 79(Mich.)7s(Chairman JohnDingell)77(her) 76(her boss)75(several powerfullawmakers)74(Ms. Washington)73(her)76 0,1,-,2,-,-,+ .
.
.
.
(her)76 0,1,+,1,-,-,+, , , ,(She)80 1,0,+,0,-,-,+ .
.
.
.
(She)80 2,0,+,0,-,-,+ .
.
.
.
(She)s0 3,1,+,0,-,-,+, , , ,(She)80(Ms. Washington)8s(Ms. Washington)s3(Ms. Washington)83(Ms. Washington)83Ms.
Washington)s3Ms.
Washington)s3Ms.
Washington)83Ms.
Washington)83Ms.
Washington)s3Ms.
Washington)833,1,+,1,-,-,+,,1,0,+,0,-, ,1,1,+,2, , , ,1,1,+,1, , , ,2,0,+,0,+,-, ,, , ' 1+?
, \]\] \] \], ,+\]3,0,+,0,+,-,4,1,+,0,+ .
.
.
.4,1,%1, , ,4,1,-,0,-, ,4,1,-,2, , ,, , r+\ [ Y Y ,4,1,+,1,+,+, - - , - - , - - ,+, -  ,NoYesNoNoNoYesNoNoNoNoNoNoNoNoNoYes529Computational Linguistics Volume 27, Number 4We use the same method to generate coreference chains for both MUC-6 and MUC-7, except for the following.
For MUC-7, because of slight changes in the coreferencetask definition, we include a filtering module to remove certain coreference chains.The task definition states that a coreference chain must contain at least one elementthat is a head noun or a name; that is, a chain containing only prenominal  modifiersis removed by the filtering module.3.
EvaluationIn order to evaluate the performance of our learning approach to coreference resolu-tion on common data sets, we util ized the annotated corpora and scoring programsfrom MUC-6 and MUC-7, which assembled a set of newswire documents annotatedwith coreference chains.
Although we did not participate in either MUC-6 or MUC-7,we were able to obtain the training and test corpora for both years from the MUC orga-nizers for research purposes.
1 To our knowledge, these are the only publicly availableannotated corpora for coreference resolution.For MUC-6, 30 dry-run documents annotated with coreference :information wereused as the training documents for our coreference engine.
There are also 30 annotatedtraining documents from MUC-7.
The total size of the 30 training documents is closeto 12,400 words for MUC-6 and 19,000 words for MUC-7.
There are altogether 20,910(48,872) training examples used for MUC-6 (MUC-7), of which only 6.5% (4.4%) arepositive examples in MUC-6 (MUC-7).
2After training a separate classifier for each year, we tested the performance of eachclassifier on its corresponding test corpus.
For MUC-6, the C5 pruning confidence is setat 20% and the min imum number  of instances per leaf node is set at 5.
For MUC-7, thepruning confidence is 60% and the min imum number  of instances is 2.
The parametersare determined by performing 10-fold cross-validation on the whole training set foreach MUC year.
The possible pruning confidence values that we tried are 10%, 20%,40%, 60%, 80%, and 100%, and for min imum instances, we tried 2, 5, 10, 15, and 20.Thus, a total of 30 (6 x 5) cross-validation runs were executed.One advantage of using a decision tree learning algorithm is that the resultingdecision tree classifier can be interpreted by humans.
The decision tree generated forMUC-6, shown in Figure 2, seems to encapsulate a reasonable rule of thumb thatmatches our intuitive linguistic notion of when two noun phrases can corefer.
It isalso interesting to note that only 8 out of the 12 available features in the trainingexamples are actually used in the final decision tree built.MUC-6 has a standard set of 30 test documents, which is used by all systems thatparticipated in the evaluation.
Similarly, MUC-7 has a test corpus of 20 documents.
Wecompared our system's MUC-6 and MUC-7 performance with that of the systems thattook part in MUC-6 and MUC-7, respectively.
When the coreference ngine is givennew test documents, its output is in the form of SGML files with the coreference chainsproperly annotated according to the guidelines.
3 We then used the scoring programs1 See http://www.itl.nist.gov/iad/894.02/related_projects/muc/index.html for tails on obtaining thecorpora.2 Our system runs on a Pentium III 550MHz PC.
It took less than 5 minutes to generate the trainingexamples from the training documents for MUC-6, and about 7 minutes for MUC-7.
The training timefor the C5 algorithm to generate a decision tree from all the training examples was less than 3 secondsfor both MUC years.3 The time taken to generate the coreference hains for the 30 MUC-6 test documents of close to 13,400words was less than 3 minutes, while it took less than 2 minutes for the 20 MUC-7 test documents ofabout 10,000 words.530Soon, Ng, and Lira Coreference ResolutionSTR_MATCH = +: +STR_MATCH = -": .
.
.
J _PRONOUN = -": .
.
.APPOSIT IVE  = +:  +: APPOSIT IVE  = - :: : .
.
.AL IAS  = +:  +: AL IAS  .
.
.
.J _PRONOUN = +:: .
.
.GENDER = O: -GENDER = 2:  -GENDER = I:: .
.
.1  PRONOUN = +: +I _PRONOUN = -": .
.
.D IST  > 0:  -D IST  <= 0:: .
.
.NUMBER = +:  +NUMBER .
.
.
.Figure 2The decision tree classifier learned for MUC-6.for the respective years to generate the recall and precision scores for our coreferenceengine.Our coreference engine achieves a recall of 58.6% and a precision of 67.3%, yieldinga balanced F-measure of 62.6% for MUC-6.
For MUC-7, the recall is 56.1%, the precisionis 65.5%, and the balanced F-measure is 60.4%.
4We plotted the scores of our coreferenceengine (square-shaped) against the official test scores of the other systems (cross-shaped) in Figure 3 and Figure 4.We also plotted the learning curves of our coreference ngine in Figure 5 andFigure 6, showing its accuracy averaged over three random trials when trained on1, 2, 3, 4, 5, 10, 15, 20, 25, and 30 training documents.
The learning curves indicatethat our coreference ngine achieves its peak performance with about 25 trainingdocuments, or about 11,000 to 17,000 words of training documents.
This number  oftraining documents would generate tens of thousands of training examples, sufficientfor the decision tree learning algorithm to learn a good classifier.
At higher numbersof training documents, our system seems to start overfitting the training data.
Forexample, on MUC-7 data, training on the full set of 30 training documents results ina more complex decision tree.Our system's cores are in the upper  region of the MUC-6 and MUC-7 systems.We performed a simple one-tailed, paired sample t-test at significance level p = 0.05 todetermine whether the difference between our system's F-measure score and each ofthe other systems' F-measure score on the test documents is statistically significant.
5We found that at the 95% significance level (p = 0.05), our system performed betterthan three MUC-6 systems, and as well as the rest of the MUC-6 systems.
Using the4 Note that MUC-6 did not use balanced F-measure as the official evaluation measure, but MUC-7 did.5 Though the McNemar test is shown to have low Type I error compared with the paired t-test(Dietterich 1998), we did not carry out this test in the context of coreference.
This is because anexample instance defines a coreference link between two noun phrases, and since this link is transitivein nature, it is unclear how the number of links misclassified by System A but not by System B andvice versa can be obtained to execute the McNemar test.531Computational Linguistics Volume 27, Number 4100c-OO,..9080706050403020100X~x x {x. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
X .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.X0 10 20 30 40 50 60 70 80 90 100RecallFigure 3Coreference scores of MUC-6 systems and our system.100t -OO0..9080706050403020100X%.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
X .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.XI0 10 20 30 40 50 60 70 80 90 100RecallFigure 4Coreference scores of MUC-7 systems and our system.same significance l vel, our system performed better than four MUC-7 systems, and aswell as the rest of the MUC-7 systems.
Our result is encouraging since it indicates thata learning approach using relatively shallow features can achieve scores comparableto those of systems built using nonlearning approaches.532Soon, Ng, and Lira Coreference Resolution6362616059585756-1,~ 5554,,' 53525150494847460I I5 10 15 20 25Number of MUC-6 Documents Trained OnFigure 5Learning curve of coreference r solution accuracy for MUC-6.63 I I I I I"3?
)ELI_6261605958575655545352515049484746Figure 6JJ80I I I I I I0 5 10 15 20 25 30Number of MUC-7 Documents Trained OnLearning curve of coreference r solution accuracy for MUC-7.It should be noted that the accuracy of our coreference resolution engine dependsto a large extent on the performance of the NLP modules that are executed beforethe coreference ngine.
Our current learning-based, HMM named entity recognitionmodule is trained on 318 documents (a disjoint set from both the MUC-6 and MUC-7533Computational Linguistics Volume 27, Number 4test documents) tagged with named entities, and its score on the MUC-6 named entitytask for the 30 formal test documents is only 88.9%, which is not considered very highby MUC-6 standards.
For example, our named entity recognizer could not identifythe two named entities USAir and Piedmont in the expression USAir and Piedmont butinstead treat them as one single named entity.
Our part-of-speech tagger achieves 96%accuracy, while the accuracy of noun phrase identification is above 90%.4.
The Contr ibut ion of the FeaturesOne factor that affects the performance of a machine learning approach is the set offeatures used.
It is interesting to find out how useful each of our 12 features is in theMUC-6 and MUC-7 coreference tasks.
One way to do this is to train and test using justone feature at a time.
Table 3 and Table 4 show the results of the experiment.
For bothMUC-6 and MUC-7, the 3 features that give nonzero recall and precision are ALIAS,STR_MATCH, and APPOSITIVE.
The 12 features can be divided into unary and binaryTable 3MUC-6 results of complete and baseline systems to study the contribution of the features.System ID Recall Prec F RemarksComplete systemsDSO 58.6 67.3 62.6 Our systemDSO_TRG 52.6 67.6 59.2 Our system using RESOLVE's methodof generating positive and negativeexamplesRESOLVE 44.2 50.7 47.2 The RESOLVE coreference system at theUniversity of MassachusettsBaseline systems using just one featureDIST 0.0 0.0 0.0 OnlySEMCLASS 0.0 0.0 0.0 OnlyNUMBER 0.0 0.0 0.0 OnlyGENDER 0.0 0.0 0.0 OnlyPROPER_NAME 0.0 0.0 0.0 OnlyALIAS 24.5 88.7 38.4 OnlyJ_PRONOUN 0.0 0.0 0.0 OnlyDEF_NP 0.0 0.0 0.0 OnlyDEM_NP 0.0 0.0 0.0 OnlySTR_MATCH 45.7 65.6 53.9 OnlyAPPOSITIVE 3.9 57.7 7.3 OnlyI_PRONOUN 0.0 0.0 0.0 only"distance" feature is used"semantic lass agreement""number agreement""gender agreement""both proper names""alias""j-pronoun""definite noun phrase""demonstrative noun phrase""string match""appositive""i-pronoun"Other baseline systemsALIAS_STR 51.5 66.4 58.0 Only the "alias" and "string match" fea-tures are usedALIAS_STR~PPOS 55.2 66.4 60.3 Only the "alias," "string match," and"appositive" features are usedONE_CHAIN 89.9 31.8 47.0 All markables form one chainONE_WRD 55.4 36.6 44.1 Markables corefer if there is at least onecommon wordHD_WRD 56.4 50.4 53.2 Markables corefer if their head words arethe same534Soon, Ng, and Lim Coreference ResolutionTable 4MUC-7 results of complete and baseline systems to study the contribution of the features.System ID Recall Prec F RemarksComplete systemsDSO 56.1 65.5 60.4 Our systemDSO_TRG 53.3 69.7 60.4 Our system using RESOLVE's methodof generating positive and negativeexamplesBaseline systems using just one featureDIST 0.0 0.0 0.0 Only "distance" feature is usedSEMCLASS 0.0 0.0 0.0 Only "semantic lass agreement"NUMBER 0.0 0.0 0.0 Only "number agreement"GENDER 0.0 0.0 0.0 Only "gender agreement"PROPER3X/AME 0.0 0.0 0.0 Only "both proper names"ALIAS 25.6 81.1  38.9 Only "alias"J_PRONOUN 0.0 0.0 0.0 Only "j-pronoun"DEF_NP 0.0 0.0 0.0 Only "definite noun phrase"DEM_NP 0.0 0.0 0.0 Only "demonstrative noun phrase"STRdV\[ATCH 43.8 71.4 54.3 Only "string match"APPOSITIVE 2.4 60.0 4.6 Only "appositive"I_PRONOUN 0.0 0.0 0.0 Only "i-pronoun"Other baseline systemsALIAS_STR 49.4 70.4 58.1 Only the "alias" and "string match" fea-tures are usedALIAS_STR_APPOS 51.6 69.9 59.4 Only the "alias," "string match," and"appositive" features are usedONE_CHAIN 87.5 30.5 45.2 All markables form one chainONE_WRD 55.9 38.7 45.7 Markables corefer if there is at least onecommon wordHD_WRD 55.2 55.6 55.4 Markables corefer if their head words arethe samefeatures.
The unary features are I_PRONOUN, J_PRONOUN, DEF_NP, and DEM_NP,while the rest are binary in nature.
All the unary features core an F-measure of 0.
Thebinary features with 0 F-measure are DIST, PROPERd'qAME, GENDER, SEMCLASS,and NUMBER.The ALIAS, APPOSITIVE, and STR_MATCH features give nonzero F-measure.All these features give rather high precision scores (> 80% for ALIAS, > 65% forSTR_MATCH, and > 57% for APPOSITIVE).
Since these features are highly informa-tive, we were curious to see how much they contribute to our MUC-6 and MUC-7results of 62.6% and 60.4%, respectively.
Systems ALIAS_STR and ALIAS_STR~a~PPOSin Table 3 and Table 4 show the results of the experiment.
In terms of absolute F-measure, the difference between using these three features and using all features is2.3% for MUC-6 and 1% for MUC-7; in other words, the other nine features contributejust 2.3% and 1% more for each of the MUC years.
These nine features will be thefirst ones to be considered for pruning away by the C5 algorithm.
For example, fourfeatures, namely, SEMCLASS, PROPER_NAME, DEF_NP, and DEM_NP, are not usedin the MUC-6 tree shown in Figure 2.
Figure 7 shows the distribution of the test casesover the five positive leaf nodes of the MUC-6 tree.
For example, about 66.3% of all535Computational Linguistics Volume 27, Number 4STR_MATCH = +: + 944 (66.3%)STR_MATCH = -": .
.
.
J _PRONOUN = -": .
.
.APPOSIT IVE  = +:  + I i i  (7.8%)STR_MATCH = -": .
.
.
J _PRONOUN = -": .
.
.APPOSIT IVE  = -": .
.
.AL IAS  = +:  + 163 (11.5%)STR_MATCH = -": .
.
.
J _PRONOUN = +:: .
.
.GENDER = I:: .
.
.
I _PRONOUN = +:  + 77 (5.4%)STR_MATCH = - -: .
.
.
J _PRONOUN = +:: .
.
.GENDER = I:: .
.
.
I _PRONOUN = -": .
.
.D IST  <= 0:: .
.
.NUMBER = +:  + 128 (9 .0%)Figure 7Distribution of test examples from the 30 MUC-6 test documents for positive leaf nodes of theMUC-6 tree.the test examples that are classified positive go to the "If STRiVIATCH" branch of thetree.Other baseline systems that are used are ONE_CHAIN, ONE_WRD, and HD_WRD(Cardie and Wagstaff 1999).
For ONE_CHAIN, all markables formed one chain.
InONE_WRD, markables corefer if there is at least one common word.
In HD_WRD,markables corefer if their head words are the same.
The purpose of ONE_CHAIN is todetermine the maximum recall our system is capable of.
The recall evel here indirectlymeasures how effective the noun phrase identification module is.
Both ONE_WRD andHD_WRD are less stringent variations of STR_MATCH.
The performance ofONE_WRDis the worst.
HD_WRD offers better ecall compared to STR_MATCH, but poorer pre-cision.
However, its F-measure is comparable to that of STRA4ATCH.The score of the coreference system at the University of Massachusetts (RESOLVE),which uses C4.5 for coreference r solution, is shown in Table 3.
RESOLVE is shownbecause among the MUC-6 systems, it is the only machine learning-based systemthat we can directly compare to.
The other MUC-6 systems were not based on alearning approach.
Also, none of the systems in MUC-7 adopted a learning approachto coreference r solution (Chinchor 1998).RESOLVE's score is not high compared to scores attained by the rest of the MUC-6 systems.
In particular, the system's recall is relatively low.
Our system's core ishigher than that of RESOLVE, and the difference is statistically significant.
The RE-SOLVE system is described in three papers: McCarthy and Lehnert (1995), Fisher etal.
(1995), and McCarthy (1996).
As explained in McCarthy (1996), the reason for thislow recall is that RESOLVE takes only the "relevant entities" and "relevant references"as input, where the relevant entities and relevant references are restricted to "person"536Soon, Ng, and Lim Coreference Resolutionand "organization."
In addition, because of limitations of the noun phrase detectionmodule, nested phrases are not extracted and therefore do not take part in coreference.Nested phrases can include prenominal modifiers, possessive pronouns, and so forth.Therefore, the number of candidate markables to be used for coreference is small.On the other hand, the markables extracted by our system include nested nounphrases, MUC-style named entity types (money, percent, date, etc.
), and other typesnot defined by MUC.
These markables will take part in coreference.
About 3,600 top-level markables are extracted from the 30 MUC-6 test documents by our system.
Asdetected by our NLP modules, only about 35% of these 3,600 phrases are "person"and "organization" entities and references.
Concentrating on just these types has thusaffected the overall recall of the RESOLVE system.RESOLVE's way of generating training examples also differs from our system's:instances are created for all possible pairings of "relevant entities" and "relevant ref-erences," instead of our system's method of stopping at the first coreferential nounphrase when traversing back from the anaphor under consideration.
We implementedRESOLVE's way of generating training examples, and the results (DSO-TRG) are re-ported in Table 3 and Table 4.
For MUC-7, there is no drop in F-measure; for MUC-6,the F-measure dropped slightly.RESOLVE makes use of 39 features, considerably more than our system's 12 fea-tures.
RESOLVE's feature set includes the two highly informative features, ALIAS andSTR_MATCH.
RESOLVE does not use the APPOSITIVE feature.5.
Error AnalysisIn order to determine the major classes of errors made by our system, we randomlychose five test documents from MUC-6 and determined the coreference links that wereeither missing (false negatives) or spurious (false positives) in these sample documents.Missing links result in recall errors; spurious links result in precision errors.Breakdowns of the number of spurious and missing links are shown in Table 5and Table 6, respectively.
The following two subsections describe the errors in moredetail.5.1 Errors Causing Spurious LinksThis section describes the five major types of errors summarized in Table 5 in moredetail.5.1.1 Prenominal Modifier String Match.
This class of errors occurs when some stringsof the prenominal modifiers of two markables match by surface string comparison andthus, by the C5 decision tree in Figure 2, the markables are treated as coreferring.
How-Table 5The types and frequencies oferrors that affect precision.Types of Errors Causing Spurious Links Frequency %Prenominal modifier string matchStrings match but noun phrases refer todifferent entitiesErrors in noun phrase identificationErrors in apposition determinationErrors in alias determination161142.1%28.9%10.5%13.2%5.3%537Computational Linguistics Volume 27, Number 4Table 6The types and frequencies oferrors that affect recall.Types of Errors Causing Missing Links Frequency %Inadequacy ofcurrent surface featuresErrors in noun phrase identificationErrors in semantic lass determinationErrors in part-of-speech assignmentErrors in apposition determinationErrors in tokenization38 63.3%7 , 11.7%7 11.7%5 8.3%2 3.3%1 1.7%ever, the entire markable actually does not corefer.
The nested noun phrase extractionmodule is responsible for obtaining the possible prenominal modifiers from a nounphrase.In (3), the noun phrase extraction module mistakenly extracted (vice)l and (vice)2,which are not prenominal modifiers.
Because of string match, (vice)l and (vice)2 incor-rectly corefer.
In (4), (undersecretary)2 was correctly extracted as a prenorninal modifier,but incorrectly corefers with (undersecretary)l by string match.
(3)(4)David J. Bronczek, (vice)l president and general manager of FederalExpress Canada Ltd., was named senior (vice)2 president, Europe, Africaand Mediterranean, at this air-express concern.Tarnoff, a former Carter administration fficial and president of theCouncil on Foreign Relations, is expected to be named (undersecretary)lfor political affairs .
.
.
.
Former Sen. Tim Wirth is expected to get a newlycreated (undersecretary)2 post for global affairs, which would includerefugees, drugs and environmental issues.5.1.2 Strings Match but Noun Phrases Refer to Different Entities.
This error occurswhen the surface strings of two markables match and thus, by the C5 decision treein Figure 2, they are treated as coreferring.
However, they actually refer to differententities and should not corefer.
In (5), (the committee)l actually refers to the entity theHouse Energy and Commerce Committee, and (the committee)2 refers to the Senate FinanceCommittee; therefore, they should not corefer.
In (6), the two instances of chief executiveofficer refer to two different persons, namely, Allan Laufgraben and Milton Petrie, and,again, should not corefer.(5)(6)Ms.
Washington's candidacy is being championed by several powerfullawmakers including her boss, Chairman John Dingell (D., Mich.) of theHouse Energy and Commerce Committee.
She currently is a counsel to(the committeeh .
.
.
.
Mr. Bentsen, who headed the Senate FinanceCommittee for the past six years, also is expected to nominate SamuelSessions, (the committee)2's chief tax counsel, to one of the top tax jobsat Treasury.Directors also approved the election of Allan Laufgraben, 54 years old,as president and (chief executive officerh and Peter A.
Left, 43, as chiefoperating officer.
Milton Petrie, 90-year-old chairman, president and(chief executive officer)2 since the company was founded in 1932, willcontinue as chairman.538Soon, Ng, and Lira Coreference Resolution5.1,3 Errors in Noun Phrase Identification.
This class of errors is caused by mistakesmade by the noun phrase identification module.
In (7), May and June are incorrectlygrouped together by the noun phrase identification module as one noun phrase, thatis, May, June.
This markable then incorrectly causes the APPOSITIVE feature to betrue, which results in classifying the pair as coreferential.
In fact, (thefirst week of July)2should not be in apposition to (May, Juneh.
However, we classified this error as anoun phrase identification error because it is the first module that causes the error.
In(8), the noun phrase module extracted Metaphor Inc. instead of Metaphor Inc. unit.
Thiscauses (it)2 to refer to Metaphor Inc. instead of Metaphor Inc.
unit.
(7)(8)The women's apparel specialty retailer said sales at stores open morethan one year, a key barometer of a retail concern's trength, declined2.5% in (May, June)l and (the first week of July)2.... International Business Machines Corp.'s (Metaphor InC.)l unit said(it)2 will shed 80 employees ...5.1.4 Errors in Apposition Determination.
This class of errors occurs when the anaphoris incorrectly treated as being in apposition to the antecedent and therefore causes thenoun phrases to corefer.
The precision scores obtained when using the APPOSITIVEfeature alone are shown in Table 3 and Table 4, which suggest that the module can beimproved further.
Examples where apposition determination is incorrect are shown in(9) and (10).
(9)(10)Clinton officials are said to be deciding between recently retired Rep.Matthew McHugh (D., (N.Y.)l) and (environmental activist)2 andtransition official Gus Speth for the director of the Agency forInternational Development.Metaphor, a software subsidiary that IBM purchased in 1991, also named(Chris Grejtak)l, (43 years old)2, currently a senior vice president,president and chief executive officer.5.1.5 Errors in Alias Determination.
This class of errors occurs when the anaphor isincorrectly treated as an alias of the antecedent, thus causing the noun phrase pair tocorefer.
In (11), the two phrases (House)l and (the House Energy and Commerce Committee)2corefer because the ALIAS feature is incorrectly determined to be true.
(11) Consuela Washington, a longtime (House)l staffer and an expert insecurities laws, is a leading candidate to be chairwoman of the Securitiesand Exchange Commission in the Clinton administration .
.
.
.
Ms.Washington's candidacy is being championed by several powerfullawmakers including her boss, Chairman John Dingell (D., Mich.) of (theHouse Energy and Commerce Committee)2.5.2 Errors Causing Missing LinksThis subsection describes the six major classes of errors summarized in Table 6 in moredetail.5.2.1 Inadequacy of Current Surface Features.
This class of errors is due to the in-adequacy of the current surface features because they do not have information about539Computational Linguistics Volume 27, Number 4other words (such as the connecting conjunctions, prepositions, or verbs) and otherknowledge sources that may provide important clues for coreference.
As a result, theset of shallow features we used is unable to correctly classify the noun phrases in theexamples below as coreferring.Example (12) illustrates why resolving (them)2 is difficult.
(allies)l, securities ex-changes, banks, and futures exchanges are all possible antecedents of (them)2, and thefeature set must include more information to be able to pick the correct one.
The con-junction and in (13) and was named in (16) are important cues to determine coreference.In addition, it may also be possible to capture noun phrases in predicate constructionslike (17), where (Mr. Gleason)l is the subject and (president)a is the object.(12)(13)(14)(15)(16)(17)Ms.
Washington and Mr. Dingell have been considered (allies)l of thesecurities exchanges, while banks and futures exchanges have oftenfought with (them)2.Separately, Clinton transition officials said that Frank Newman, 50, (vicechairmanh and (chief financial officer)2 of BankAmerica Corp., isexpected to be nominated as assistant Treasury secretary for domesticfinance.Separately, (Clinton transition officials)l said that Frank Newman, 50,vice chairman and chief financial officer of BankAmerica Corp., isexpected to be nominated as assistant Treasury secretary for domesticfinance .
.
.
.
As early as today, (the Clinton camp)2 is expected to namefive undersecretaries of tate and several assistant secretaries.
(Metro-Goldwyn-Mayer InC.)l said it named Larry Gleason president ofworld-wide theatrical distribution of (the movie studio)2's distributionunit.... (general managerh of Federal Express Canada Ltd., was named(senior vice president)a, Europe, Africa and Mediterranean ...(Mr. Gleasonh, 55 years old, was (president)2 of theatrical exhibition forParamount Communications Inc., in charge of the company's 1,300movie screens in 12 countries.5.2.2 Errors in Noun Phrase Identification.
This class of errors was described in Sec-tion 5.1.3.
The noun phrase identification module may extract noun phrases that donot match the phrases in the coreference chain, therefore causing missing links andrecall error.5.2.3 Errors in Semantic Class Determination.
These errors are caused by the wrongassigmnent of semantic lasses to words.
For example, (Metaphor)l should be assigned"organization" but it is assigned "unknown" in (18), and (second-quarter)2 should beassigned "date" instead of "unknown" in (19).
However, correcting these classes willstill not cause the noun phrases in the examples to corefer.
This is because the valuesof the SEMCLASS feature in the training examples are extremely noisy, a situationcaused largely by our semantic lass determination module.
In many of the negativetraining examples, although the noun phrases are assigned the same semantic lasses,these assignments do not seem to be correct.
Some examples are (four-year, NBC), (Theunion, Ford Motor Co.), and (base-wage, job-security).
A better algorithm for assigningsemantic lasses and a more refined semantic lass hierarchy are needed.540Soon, Ng, and Lira Coreference Resolution(18)(19)(Metaphorh, a software subsidiary that IBM purchased in 1991, alsonamed Chris Grejtak, ... Mr. Grejtak said in an interview that the staffreductions will affect most areas of (the company)2 related to its earlyproprietary software products.Business brief--Petrie Stores Corp.: Losses for (Fiscal 2nd Period)l, halfseen likely by retailer .
.
.
.
Petrie Stores Corp., Secaucus, N.J., said anuncertain economy and faltering sales probably will result in a(second-quarter)2 loss and perhaps a deficit for the first six months offiscal 1994.5.2.4 Errors in Part-of-Speech Assignment.
This class of errors is caused by the wrongassignment of part-of-speech tags to words.
In (20), (there)2 is not extracted becausethe part-of-speech tag assigned is "RB," which is an adverb and not a possible nounphrase.
(20) Jon W. Slangerup, who is 43 and has been director of customer service in(Canadah, succeeds Mr. Bronczek as vice president and general manager(there)2.5.2.5 Errors in Apposition Determination.
This class of errors was described in Sec-tion 5.1.4.5.2.6 Errors in Tokenization.
This class of errors is due to the incorrect okenizationof words.
In (21), (1-to-2)1 and (15-to-1)2 are not found because the tokenizer breaks1-to-2 into 1, -, to-2.15-to-1 is broken up similarly.
(21) Separately, MGM said it completed a previously announced financialrestructuring designed to clean up its balance sheet--removing $900million in bank debt from MGM's books and reducing its debt-to-equityratio to (1-to-2)1 from (15-to-1)2--with a view toward a future sale of thecompany.5.3 Comparing Errors Made by RESOLVEMcCarthy (1996) has also performed an analysis of errors while conducting an evalua-tion on the MUC-5 English Joint Venture (EJV) corpus.
A large number of the spuriouslinks are caused by what he terms "feature ambiguity," which means that feature val-ues are not computed perfectly.
As seen in Table 5, our string match feature accountsfor most of the spurious links.
Also, seven of the spurious links are caused by aliasand apposition determination.
As with RESOLVE, "feature ambiguity" is the mainsource of precision errors.For RESOLVE, a large number of the missing links are caused by "incompletesemantic knowledge" (32%) and "unused features" (40.5%).
For our system, the errorsdue to the inadequacy of surface features and semantic lass determination problemsaccount for about 75% of the missing links.
"Unused features" means that some ofthe features, or combinations of features, that are needed to classify pairs of phrasesas coreferential re not present in the decision trees (McCarthy 1996).
Similarly, theinadequacy of our system's urface features means that the current feature set maynot be enough and more information sources hould be added.Because a detailed error analysis of RESOLVE would require not only its MUC-6response file, but also the output of its various components, we cannot perform thesame error analysis that we did for our system on RESOLVE.541Computational Linguistics Volume 27, Number 46.
Related WorkThere is a long tradition of work on coreference r solution within computational lin-guistics, but most of it was not subject o empirical evaluation until recently.
Amongthe papers that have reported quantitative valuation results, most are not based onlearning from an annotated corpus (Baldwin 1997; Kameyama 1997; Lappin and Leass1994; Mitkov 1997).To our knowledge, the research efforts of Aone and Bennett (1995), Ge, Hale, andCharniak (1998), Kehler (1997), McCarthy and Lehnert (1995), Fisher et al (1995), andMcCarthy (1996) are the only ones that are based on learning from an annotated corpus.Ge, Hale, and Charniak (1998) used a statistical model for resolving pronouns, whereaswe used a decision tree learning algorithm and resolved general noun phrases, notjust pronouns.
Similarly, Kehler (1997) used maximum entropy modeling to assign aprobability distribution to alternative sets of coreference relationships among nounphrase entity templates, whereas we used decision tree learning.The work of Aone and Bennett (1995), McCarthy and Lehnert (1995), Fisher etal.
(1995), and McCarthy (1996) employed ecision tree learning.
The RESOLVE sys-tem is presented in McCarthy and Lehnert (1995), Fisher et al (1995), and McCarthy(1996).
McCarthy and Lehnert (1995) describe how RESOLVE was tested on the MUC-5 English Joint Ventures (EJV) corpus.
It used a total of 8 features, 3 of which werespecific to the EJV domain.
For example, the feature JV-CHILD-i determined whetheri referred to a joint venture formed as the result of a tie-up.
McCarthy (1996) describeshow the original RESOLVE for MUC-5 EJV was improved to include more features,8 of which were domain specific, and 30 of which were domain independent.
Fisheret al (1995) adapted RESOLVE to work in MUC-6.
The features used were slightlychanged for this domain.
Of the original 30 domain-independent features, 27 wereused.
The 8 domain-specific features were completely changed for the MUC-6 task.For example, JV-CHILD-i was changed to CHILD-/to decide whether i is a "unit" or a"subsidiary" of a certain parent company.
In contrast o RESOLVE, our system makesuse of a smaller set of 12 features and, as in Aone and Bennett's (1995) system, thefeatures used are generic and applicable across domains.
This makes our coreferenceengine a domain-independent module.Although Aone and Bennett's (1995) system also made use of decision tree learningfor coreference r solution, they dealt with Japanese texts, and their evaluation focusedonly on noun phrases denoting organizations, whereas our evaluation, which dealtwith English texts, encompassed noun phrases of all types, not just those denotingorganizations.
In addition, Aone and Bennett evaluated their system on noun phrasesthat had been correctly identified, whereas we evaluated our coreference resolutionengine as part of a total system that first has to identify all the candidate noun phrasesand has to deal with the inevitable noisy data when mistakes occur in noun phraseidentification and semantic lass determination.The contribution of our work lies in showing that a learning approach, whenevaluated on common coreference data sets, is able to achieve accuracy competitivewith that of state-of-the-art systems using nonlearning approaches.
It is also the firstmachine learning-based system to offer performance comparable to that of nonlearningapproaches.Finally, the work of Cardie and Wagstaff (1999) also falls under the machine learn-ing approach.
However, they used unsupervised learning and their method did notrequire any annotated training data.
Their clustering method achieved a balanced F-measure of only 53.6% on MUC-6 test data.
This is to be expected: supervised learningin general outperforms unsupervised learning since a supervised learning algorithm542Soon, Ng, and Lirn Coreference Resolutionhas access to a richer set of annotated ata to learn from.
Since our supervised learningapproach requires only a modest number of annotated training documents to achievegood performance (as can be seen from the learning curves), we argue that the betteraccuracy achieved more than justifies the annotation effort incurred.7.
ConclusionIn this paper, we presented a learning approach to coreference resolution of nounphrases in unrestricted text.
The approach learns from a small, annotated corpus andthe task includes resolving not just pronouns but general noun phrases.
We evalu-ated our approach on common data sets, namely, the MUC-6 and MUC-7 coreferencecorpora.
We obtained encouraging results, indicating that on the general noun phrasecoreference task, the learning approach achieves accuracy comparable to that of non-learning approaches.AcknowledgmentsThis paper is an expanded version of apreliminary paper that appeared in theProceedings ofthe 1999 Joint S1GDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora.We would like to thank the MUC organizerswho made available to us the MUC-6 andMUC-7 data sets, without which this workwould have been impossible.
We also thankBeth Sundheim for helpful comments on anearlier version of this paper, and Hai LeongChieu for his implementation f theHMM-based named entityrecognition module.ReferencesAone, Chinatsu and Scott William Bennett.1995.
Evaluating automated and manualacquisition of anaphora resolutionstrategies.
In Proceedings ofthe 33rd AnnualMeeting of the Association for ComputationalLinguistics, 122-129.Baldwin, Breck.
1997.
CogNIAC: Highprecision coreference with limitedknowledge and linguistic resources.
InProceedings ofthe ACL Workshop onOperational Factors in Practical, RobustAnaphora Resolution for Unrestricted Texts,pages 38-45.Bikel, Daniel M., Richard Schwartz, andRalph M. Weischedel.
1999.
An algorithmthat learns what's in a name.
MachineLearning, 34(1-3):211-231, February.Cardie, Claire and Kiri Wagstaff.
1999.Noun phrase coreference asclustering.
InProceedings ofthe 1999 Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora,pages 82-89.Chinchor, Nancy A.
1998.
Overview ofMUC-7/MET-2.
In Proceedings oftheSeventh Message Understanding Conference(MUC-7).
http://www.itl.nist.gov/iad/894.02/related_projects/muc/proceedings/muc_7_toc.html.Church, Kenneth.
1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Proceedings oftheSecond Conference on Applied NaturalLanguage Processing, pages 136-143.Dietterich, Thomas G. 1998.
Approximatestatistical tests for comparing supervisedclassification learning algorithms.
NeuralComputation, 10(7):1895-1924, October.Fisher, David, Stephen Soderland, JosephMcCarthy, Fangfang Feng, and WendyLehnert.
1995.
Description of the UMasssystem as used for MUC-6.
In Proceedingsof the Sixth Message UnderstandingConference (MUC-6), pages 127-140.Ge, Niyu, John Hale, and Eugene Charniak.1998.
A statistical approach to anaphoraresolution.
In Proceedings ofthe SixthWorkshop on Very Large Corpora, pages161-170.Kameyama, Megumi.
1997.
Recognizingreferential links: An informationextraction perspective.
In Proceedings oftheACL Workshop on Operational Factors inPractical, Robust Anaphora Resolution forUnrestricted Texts, pages 46-53.Kehler, Andrew.
1997.
Probabilisticcoreference in information extraction.
InProceedings ofthe Second Conference onEmpirical Methods in Natural LanguageProcessing, pages 163-173.Lappin, Shalom and Herbert J. Leass.
1994.An algorithm for pronominal anaphoraresolution.
Computational Linguistics,20(4):535-561, December.McCarthy, Joseph F. 1996.
A TrainableApproach to Coreference R solution forInformation Extraction.
Ph.D. thesis,543Computational Linguistics Volume 27, Number 4University of Massachusetts Amherst,Department of Computer Science,September.McCarthy, Joseph F. and Wendy Lehnert.1995.
Using decision trees for coreferenceresolution.
In Proceedings ofthe FourteenthInternational Joint Conference on ArtificialIntelligence, pages 1050-1055.McEnery, A., I. Tanaka, and S. Botley.
1997.Corpus annotation and referenceresolution.
In Proceedings ofthe ACLWorkshop on Operational Factors in Practical,Robust Anaphora Resolution for UnrestrictedTexts, pages 67-74.Miller, George A.
1990.
WordNet: Anon-line lexical database.
InternationalJournal of Lexicography, 3(4):235-312.Mitkov, Ruslan.
1997.
Factors in anaphoraresolution: They are not the only thingsthat matter.
A case study based on twodifferent approaches.
In Proceedings oftheACL Workshop on Operational Factors inPractical, Robust Anaphora Resolution forUnrestricted Texts, pages 14-21.MUC-6.
1995.
Coreference task definition(v2.3, 8 Sep 95).
In Proceedings ofthe SixthMessage Understanding Conference (MUC-6),pages 335-344.MUC-7.
1997.
Coreference task definition(v3.0, 13 Jul 97).
In Proceedings oftheSeventh Message Understanding Conference(MUC-7).Quinlan, John Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann, SanFrancisco, CA.Sundheim, Beth M. 1995.
Overview ofresults of the MUC-6 evaluation.
InProceedings ofthe Sixth MessageUnderstanding Conference (MUC-6), pages13-31.544
