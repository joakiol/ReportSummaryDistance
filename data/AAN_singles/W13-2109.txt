Proceedings of the 14th European Workshop on Natural Language Generation, pages 82?91,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsWhat and where: An empirical investigation of pointing gestures anddescriptions in multimodal referring actionsAlbert GattInstitute of LinguisticsUniversity of Maltaalbert.gatt@um.edu.mtPatrizia PaggioInstitute of LinguisticsUniversity of Maltapatrizia.paggio@um.edu.mtAbstractPointing gestures are pervasive in humanreferring actions, and are often combinedwith spoken descriptions.
Combining ges-ture and speech naturally to refer to objectsis an essential task in multimodal NLGsystems.
However, the way gesture andspeech should be combined in a referringact remains an open question.
In particu-lar, it is not clear whether, in planning apointing gesture in conjunction with a de-scription, an NLG system should seek tominimise the redundancy between them,e.g.
by letting the pointing gesture indi-cate locative information, with other, non-locative properties of a referent includedin the description.
This question has abearing on whether the gestural and spo-ken parts of referring acts are planned sep-arately or arise from a common underly-ing computational mechanism.
This paperinvestigates this question empirically, us-ing machine-learning techniques on a newcorpus of dialogues involving multimodalreferences to objects.
Our results indi-cate that human pointing strategies inter-act with descriptive strategies.
In partic-ular, pointing gestures are strongly asso-ciated with the use of locative features inreferring expressions.1 IntroductionReferring Expression Generation (REG) is consid-ered a core task in many NLG systems (Krahmerand van Deemter, 2012).
Typically, the REG task isdefined in terms of identification: a referent needsto be unambiguously identified in a discourse, en-abling the reader or listener to pick it out fromamong its potential distractors.
Most work in thisarea has focused on algorithms that select the con-tent for definite descriptions (Dale, 1989; Dale andReiter, 1995), or on the best form for a referringexpression given the discourse context, for exam-ple, whether it should be a full definite description,a reduced one, or a pronoun (McCoy and Strube,1999; Callaway and Lester, 2002; Krahmer andTheune, 2002).Less attention has been payed to the role ofgestures in referring actions and the way thesecan be coupled with discursive strategies for ref-erent identification.
This question becomes partic-ularly important in the context of multimodal sys-tems, for example, those involving embodied con-versational agents, where the ?naturalness?
of aninteraction hinges in part on the appropriate useof embodied actions, including referring actions.Multimodal strategies can also make communica-tion more efficient.
For example, Louwerse andBangerter (2010) found that the use of pointinggestures resulted in significantly faster resolutionof ambiguous referring expressions; crucially, thisresult was replicated when the pointing gesturewas artificially generated, rather than made by ahuman.Like human communicators, embodied agentsneed the ability to plan multimodal referring acts,combining both linguistic reference and pointing.An important question is whether these two com-ponents of a referring act should be planned in or-der to minimise redundancy between them or not.For example, given that a pointing gesture can ef-ficiently locate a target referent in a visual do-main, should an accompanying description avoidmentioning locative properties, thereby minimis-ing redundancy?
This question is the main focusof this paper.
However, it bears on a deeper is-sue, of relevance to the architecture of multimodalsystems (and the cognitive architectures whose be-haviours such systems seek to emulate): Shouldgestural and descriptive strategies be viewed asseparate (implying that a REG module can planits linguistic referring expressions more or less in-82dependently of whether a pointing gesture is alsoused) or should they be viewed as tightly coupled?If they are indeed coupled, are there any featuresof a linguistic description (for example, an object?slocation) which are excluded when a pointing ges-ture is used, or are linguistic features always re-dundant with pointing?The present paper addresses these questions in adata-driven fashion, using a multimodal corpus ofdialogues collected specifically to study referringactions at both the linguistic and gestural levels.We focus on pointing (that is, deictic) gestures di-rected at an intended referent (as opposed to, say,iconic gestures) and investigate the extent to whichpointing interacts with linguistic means for refer-ent identification.
Following an overview of pre-vious work on pointing and reference (Section 2)and a description of the corpus (Section 3), we de-scribe a number of machine-learning experimentsthat address the main empirical question (Section4), concluding with a discussion.2 Background: Pointing and describingThere is a growing consensus in the psycholin-guistic literature, especially following the workof McNeill (McNeill, 1985), that gesture andlanguage share a number of underlying mentalprocesses and are therefore coupled to a signif-icant degree.
This view is in part based on theobservation that gestures are temporally coupledwith speech and contribute meaningfully to theachievement of a communicative intention (Mc-Neill and Duncan, 2000).
For instance, in the ex-ample below, extracted from our corpus (see Sec-tion 3), a speaker identifies a landmark (composedof a collection of five circles) on a map through acombination of a pointing gesture and the mentionof the size and colour of the elements making upthe landmark.
(1) there?s a group of five large red ones [points]In this case, the pointing gesture further con-tributes to the communicative aim of identifyingthe cluster of five objects, in tandem with the vi-sual features mentioned in the description.
Mc-Neill?s proposal (McNeill and Duncan, 2000) isthat speech and gesture should be considered asthe joint outcome of the language production pro-cess, rather than as outcomes of separate pro-cesses.
Various models have been proposed whichare more or less congruent with this view.
Forexample, de Ruiter (2000) proposes that the twomodalities are planned together at early stages ofconceptualisation during speech production, whileKita and O?zyu?rek (2003) suggest that gestures areplanned by spatio-motoric processes which differfrom the planning of speech production, but inter-act with it at particular points.Recent computational work has also taken theseideas on board.
For example, Kopp et.
al.
(2008)describe a system for the concurrent planning andgeneration of gesture and speech, whose archi-tecture is inspired by Kita and O?zyu?rek (2003)and which makes use of ?multimodal concepts?
(inspired by McNeill?s ?growth points?)
combin-ing both propositional and visuo-spatial proper-ties.
This contrasts with earlier architectures, suchas that proposed by Andre?
and Rist (1996), wheregeneration of text and gesture is undertaken byseparate modules communicating with a centralplanner.The idea that the planning of language is tightlycoupled with that of gesture raises the possibilitythat the two modalities may overlap to differentdegrees.
Gesture may be completely redundantwith speech, or may encode aspects of the com-municative intention that are not included in thelinguistic message itself.
This raises an interestingquestion for multimodal REG: are there features ofobjects that tend to be mentioned in tandem witha pointing gesture; if so, which are they?
For ex-ample, the reference in (1) mentions the size andcolour of the landmark, but not its location, pos-sibly suggesting that the speaker relied on point-ing to convey the ?where?
of the target referent, asopposed to the ?what?, which is conveyed by thedescription.
This, however, is not the case in theexample below, where pointing is accompanied bya mention of the referent?s location.
(2) [...] the red ones directly to the left [...][points]There are at least two views on the relationshipbetween pointing and describing (de Ruiter et al2012).
On the one hand, the trade-off hypothesisholds that the decision to use a pointing gesture de-pends on the effort or ?cost?
involved (the furtheraway from the speaker and the smaller a referentis, the more costly it would be to point at it), com-pared to the effort involved in describing a referentlinguistically.On the other hand, pointing and (some aspectsof) describing might proceed hand in hand, so that83there is some degree of redundancy between thetwo modalities.
Under this view, pointing may bechosen not based on (low) cost assessment but aspart of a specifically multimodal cognitive strat-egy.Evidence for the trade-off hypothesis is reportedby Bangerter (2004), who found that, as pointingbecame easier in a task-oriented dialogue (becausethe distance between the speaker and the referentwas shorter), there was a decrease in verbal effort,as measured by the number of words produced, aswell as a decrease in the use of locative and visualfeatures such as colour.
Piwek (2007) also foundthat referring acts accompanied by pointing tendedto include descriptions containing fewer propertiesthan those which were not.
These results are com-patible with a view of the speaker/generator as es-sentially seeking to minimise effort in the commu-nicative act, adopting the easiest available strategythat will not compromise communicative success(Beun and Cremers, 1998).Similar results are reported by van der Sluis andKrahmer (2007), who model the trade-off hypoth-esis in a multimodal REG algorithm based on thegraph-based framework of Krahmer et.
al.
(2003).The algorithm chooses to use pointing gestures,with various degrees of precision, depending ontheir cost relative to that of features that can beused in a linguistic description.There is also evidence against the trade-offmodel.
Recent experimental work by de Ruiteret.
al.
(2012) showed that the tendency for speak-ers to point was unaffected by the difficulty of re-ferring to an object using linguistic features, al-though pointing did decrease with repeated refer-ence to the same entities.
Interestingly, the authorsobserved a correlation between the rate of pointingand the use of locative properties of objects.
Thiswould appear to favour a model in which the lin-guistically describable features of objects are dif-ferentiated: speakers may be using locative prop-erties and pointing together as part of a strategy toidentify the ?where?
of an object.
This is in linewith the observation by Louwerse and Bangerter(2010) that, in visual domains, using pointing ges-tures with locative expressions increases the speedwith which references are resolved.The evidence from de Ruiter et.
al would seemto contradict the assumptions underlying currentmultimodal REG models.
As we have seen, van derSluis and Krahmer (van der Sluis and Krahmer,2007) assume a trade-off between speech and ges-ture.
A similar assumption is made by Kranstedtand Wachsmuth (2005), who view pointing ges-turs as mainly concerned with the ?where?
of anobject.
Their algorithm, which underlies the plan-ning of multimodal references by a virtual agent,extends the Incremental Algorithm (Dale and Re-iter, 1995) as follows.
Given an object in a 3Dspace, the algorithm first considers the possibil-ity of producing an unambiguous pointing ges-ture; failing this, a pointing gesture covering theintended referent and some of its surrounding dis-tractors may be planned.
In the latter case, the al-gorithm then integrates other features of the ob-ject (e.g.
its colour), in an effort to exclude thedistractors that remain within the scope of the am-biguous point.
One of the claims underlying thismodel is that ?absolute?
location, which is coveredby pointing, is given first preference after pointingitself, with other features of a referent being con-sidered afterwards, in a preference order that willonly use relative location if all other options (suchas colour) are exhausted.In summary, the empirical evidence for therelationship between pointing and describing ismixed.
While the view that the planning of lan-guage in different modalities should be tightlycoupled has proven useful and productive, the pre-cise way in which the two interact in a referring actis still an open question, especially where the re-lationship between location and the other featuresof a target referent is concerned.
In the remain-der of this paper, we report on an empirical studythat used machine learning methods with a view toestablishing the relationship between descriptivefeatures and pointing in multimodal references.Our study is not committed to a specific architec-ture for multimodal reference planning; rather, ouraim is to establish whether pointing and describ-ing can partly overlap in the information that theyconvey about a referent.
Specifically, we are in-terested in whether the use of a description thatincludes spatial or locative information excludes apointing gesture.3 Corpus and dataThe data used in this study comes from the MREDI(Multimodal REference in DIalogue) corpus (vander Sluis et al 2008)2, a new collection of dia-2We intend to make this corpus publicly available in thenear future.84Matcher's mapwithoutitineraryDirector's mapwith itineraryDirector FollowerLow screensto hide maps(a) Experiment setupSTART1/51692/18174/ 151412 7/118/13101616161614 1414144/ 154/ 15 4/ 154/ 1512 121212101010 107/117/117/11 7/118/138/13 8/138/131/51/51/51/5999 917 1717172/182/18 2/18 2/183/63/63/63/63/6(b) Group circles map (numbers indicate the orderin which landmarks are visited along the itinerary)Figure 1: MREDI dialogue setupFeature Name Definition ExampleVisualS Size mention of the target size the group of small circlesSh Shape mention of the target shape the circles at the bottomC Colour mention of the target colour The blue square near the red squareDeictic/anaphoricI Identity Statement of identity betweenthe current and a previous or later targetthe red square,the same one we saw at number 5D Deixis Use of a deictic reference those squaresLocativeRP Relative position Position of the target landmark relativeto another object on the mapthe blue squarejust below the red squareAP Absolute position Target position based on absoluteframe of referenceThe blue circle down at the bottomFP Path references References to non-targets on thepath leading to the target.go east to the first tiny square,past the blue oneDIR Directions Direction-giving.
take a right, go acrossand straight downAction GZ Gaze Gaze at the shared map (boolean).Point Pointing Use of a pointing gesture (boolean).1Table 1: Features annotated in the dialogues.
All features have frequency values, except for the Actionfeatures, which are boolean.logues elicited using a task similar to the Map-Task (Anderson et al 1991), in which a directorand a follower talked about a map displayed on awall in front of them, approximately 1 metre away.Each also had a private copy of the map; the di-rector?s map had an itinerary on it, and her taskwas to communicate the itinerary to the follower,who marked it on his own private map.
Partici-pants were free to interact using speech and ges-ture, without touching the shared map or standingup.
They could see each other, but could not seeeach other?s private maps.
Figure 1(a) displays thebasic experimental setup.The maps consisted of shapes (squares or cir-cles), with a sequence of landmarks constitutingthe itinerary (initially known only to the director).The maps were designed to manipulate a numberof independent variables, in a balanced design:?
Cardinality The target destinations in theitineraries were either individual landmarks(in 2 of the maps) or sets of 5 landmarks withthe same attributes (e.g., all green squares);?
Visual Attributes: Targets on the itinerarydiffered from their distractors ?
the objectsin their immediate vicinity (the focus area)?
in colour, or in size, or in both colour andsize.
The focus area was defined as the set ofobjects immediately surrounding a target;?
Prior reference: Some of the targets werevisited twice in the itinerary;?
Shift of domain focus: Targets were locatednear to or far away from the previous target.Note that if two targets t1 and t2 were in thenear condition, then t1 is one of the distrac-tors of t2 and vice versa.Each participant dyad did all four maps (single-ton squares and circles; group squares and circles),85in a pseudo-random order, alternating in the di-rector/matcher role so that each was director fortwo of the maps.
Figure 1(b) displays the direc-tor?s map consisting of group circles.
Note that theitinerary is marked by numbering the target land-marks.
Landmarks with two numbers are visitedtwice (for example, the first landmark is marked1, but is also marked 5, meaning that it is the firstand the fifth landmark in the itinerary).
During theexperiment, the map was mounted on a wall andblown up to A0 size; this significantly reduced theimpression of visual clutter.Data was collected from 8 pairs of participants3.In the present study, we focus exclusively on thedirectors?
utterances.
These were transcribed andsplit up according to the landmark to which theycorresponded.
In case a landmark was describedover multiple turns in the dialogue, each turn wasannotated as a separate utterance.
Utterances wereannotated with the features displayed in Table 1.Broadly, features are divided into four types: (a)Deictic/Anaphoric, pertaining to the use of de-ictic demonstratives, and/or references to previ-ously identified entities; (ii) Visual, that is, cor-responding to a landmark?s perceptual properties;(iii) Locative, involving a description of the ob-ject?s location; and (iv) Action, pertaining to ges-ture and gaze.
All features are frequencies perutterance, except for Action features, which areboolean.Feature Frequency Mean SDS 510 0.23 0.48Sh 252 0.10 0.40C 603 0.30 0.50I 249 0.10 0.40D 375 0.17 0.43RP 529 0.13 0.40AP 293 0.13 0.40FP 989 0.40 0.70DIR 251 0.11 0.37GZ 836Point 370Table 2: Descriptive statistics for features in thecorpusThe corpus consists of a total of 2255 director?s3A number of other dialogues were recorded, but were notincluded in the corpus because participants focused on theirown private maps and never used pointing gestures, making itimpossible to study the conditions under which such gesturesare produced.utterances.
The frequency of each feature in thecorpus, as well as the per-utterance mean and stan-dard deviation (where relevant), are indicated inTable 2; note that, with the exception of Actionfeatures, all feature values are frequencies per ut-terance.Type No point (#) Point (#) TotalGroup 907 201 1108Singleton 978 169 1147Total 1885 370 2255Table 3: Frequency of occurrence of pointing ges-tures relative to different object types.As expected, linguistic features are much morefrequent than pointing gestures.
In fact only16.4% of the utterances in the corpus are accompa-nied by pointing gestures.
Previous studies, suchas that by Beun and Cremers (Beun and Cremers,1998) report a higher incidence of pointing (48%overall).
Note, however, that Beun and Cremersfocussed exclusively on first mention descriptions(which numbered 145 in all), while our corpus in-cludes subsequent mentions, as well as multipleconsecutive references to the same object dividedover several utterances (which are counted sepa-rately in our totals).Table 3 shows frequency figures for the pointinggestures in the corpus relative to the type of objectthey refer to (group vs. singleton): in accordancewith the trade-off theory, which predicts that largerobjects should be easier to point at, we see a sig-nificant difference (?2(1) = 4.769, p = 0.028)between the two types, with more pointing occur-ring with group objects (that is, in group maps).4 ExperimentsIn much of the work discussed in Section 2, thegeneration of pointing gestures is viewed as de-pendent on physical characteristics of the refer-ents, in other words on their being suitable forpointing.
This is especially true of work relatedto the trade-off hypothesis, in which the costs ofpointing gestures are calculated as a function ofthe referent object?s size and its distance from thespeaker.
In the present paper, by contrast, weare interested in investigating the relation betweenpointing and linguistic means of referent identi-fication.
More specifically, we address the ques-tion to what degree the different linguistic expres-sions used by the speaker to refer to objects in86the MREDI dialogues, can be used to predict theoccurrence of pointing gestures.
Note that thisquestion addresses the correlation between prop-erties in a description and the occurrence of point-ing, rather than the issue of how pointing and de-scribing should be planned.
Nevertheless, as wehave emphasised in Section 2, the question of co-occurrence of the two referential strategies doeshave a bearing on architectural issues.A first set of experiments were run in order totest the general trade-off hypothesis.
We testeda number of classifiers on the task of classifyingthe binary feature point, given all the linguisticfeatures in the corpus.
More specifically, the at-tributes used for the classification were MapConfl,DIR, RP, AP, FP, S, Sh, C, D, I, Point.
They areall explained and exemplified in Table 1 with theexception of MapConfl, which indicates whether aspecific case in the data comes from a group or asingleton map.
This feature was included because,as noted in the previous section, whether a targetlandmark was a singleton or a group made a dif-ference, presumably because groups are larger andmore visually salient.
Note further that one of theAction features, GZ (gaze), is ignored in the ex-periments because it is an almost univocal predic-tor of pointing.
Indeed, gazing is involved roughlyevery time Point has the value y (yes) (but not theother way round).The experiments were run using the Weka (Wit-ten and Frank, 2005) tool, which gives accessto many different algorithms, and 10-fold cross-validation was used throughout.
The results areshown in Table (4) in terms of Precision, Recalland F-measure for each of the classifiers.Classifier P R FBaseline 1 (ZeroR) 0.699 0.836 0.761Baseline 2 (OneR) 0.762 0.834 0.765SMO 0.699 0.836 0.761NaiveBayes 0.795 0.811 0.802Logistic 0.806 0.84 0.808J48 0.829 0.85 0.833Table 4: Predicting pointing gestures given all thelinguistic features in the corpus: classification re-sults.Two baselines were created to evaluate the re-sults.
The first one is provided by the ZeroR clas-sifier, which always chooses the most frequentclass, in this case n (no pointing gesture).
TheF-measure obtained by this method is somewhathigh at 0.761, because there are relatively fewpointing gestures in the data.
The second base-line, which provides a slightly more interesting re-sult against which to evaluate the other classifiers,is provided by OneR.
It achieves an F-measure of0.765 by predicting a pointing gesture if DIR >=2.5, in other words if there are at least 2.5 occur-rences of direction expressions in the utterance.Using this rule has the effect of predicting a fewof the pointing gestures, with an F-measure on they class (occurrence of pointing gestures) of 0.031.The other four sets of results were obtainedby running four different classification algorithmswith the same set of attributes.
Apart from SMO(an algorithm using support vector machines), allthe classifiers perform better than the baseline.The best results are produced by the decision treeclassifier J48, which obtains an overall F-measureof 0.833, and an F-measure of 0.421 on the y class.The confusion matrix generated by J48 on thisdata-set is shown in Table (5)a b ?
classified as1794 91 a = n247 123 b = yTable 5: Predicting pointing given all the linguisticfeatures in the corpus: confusion matrix.The model created by the decision tree classi-fier (J48) is quite complex (size=57 and no.
ofleaves=29).
The first branching, which corre-sponds to no AP (Absolute Position) and no C(Colour), assigns n to as many as 1571 instances(with 115 errors).
The tree is shown in Fig-ure (2).
The tree also shows that certain combina-tions of features are more likely to be associatedwith pointing gestures.
These are predominantlycombinations including occurrences of AP, or, inthe absence of absolute position, combinations in-cluding positive values for FP (Frequency of ref-erence on Path) and DIR (Direction).The maximum entropy model, built by the lo-gistic regression algorithm (Logistic), shows sim-ilar tendencies in that the attributes that are as-signed the highest weights are AP, C and DIR.These results confirm the general hypothesisthat there is a strong relationship between linguis-tic features used in a description and pointing ges-tures.
Indeed, it is possible to predict pointing ges-tures on the basis of the linguistic features used.87Figure 2: J48 decision treeClassifier P R F FeaturesExp1: J48 0.829 0.85 0.833 All featuresExp3: Logistic 0.806 0.84 0.808 Loc+D+IExp2: J48 0.835 0.851 0.806 MapConfl+Loc+D+IExp6: NaiveBayes 0.793 0.825 0.802 LocExp4: NaiveBayes 0.764 0.804 0.779 MapConfl+Visual+D+IExp5: J48 0.761 0.808 0.777 MapConfl+VisualExp8: NaiveBayes 0.761 0.808 0.777 VisualExp9: NaiveBayes 0.761 0.801 0.775 Visual+D+IBaseline 2: OneR 0.762 0.834 0.765 DirExp7: F48 0.699 0.836 0.761 MapConfl+D+IBaseline 1: ZeroR 0.699 0.836 0.761 Most freq classTable 6: Predicting pointing gestures with different feature combinations: classification results.In particular, the results suggest a difference be-tween features that express locative properties andthose having to do with the visual description ofthe same object (its colour, size and shape).
Morespecifically, it would seem that locative featuresare more useful to the classifiers than visual prop-erties.To test this second hypothesis, we ran a seriesof experiments where the task was still to predictpointing gestures, but different subsets of the lin-guistic features were tested one at the time.
Foreach feature combination, we run the classificationusing J48, Naive Bayes and the Logistic regressionalgorithm.
In Table (6), we show the best resultobtained for each feature combination.
The classi-fiers are ordered from the most accurate to the leastaccurate, and the combination of features used byeach of them is listed in the last column.
The bestresults and the two baselines from the previous setof experiments are included for the sake of com-parison.
Note that the term Loc is used to refer toall the locative attributes AP, DIR, RP, AP and FP,88while Visual refers to S, Sh and C.The best results are those obtained when thecomplete feature set is used in the training.
How-ever, the next best results are achieved by the clas-sifiers using the locative features, either alone ortogether with features concerning the map type,identity with a previously mentioned object anddeictic reference, with an F-measure in the range0.802?0.808.
If visual features are used instead,the F-measure is in the range 0.775?0.779.
Theworst results are obtained if neither location norvisual description are used.
Thus, although the dif-ferences between the best and the worst classifiersare not dramatic, in this data we see a tendency forthe locative features to be slightly better predictorsof pointing gestures than features corresponding tovisual descriptions.5 Discussion and conclusionsThe automatic classification experiments de-scribed above show that to a certain extent, thepointing gestures occurring in the MREDI corpuscan be predicted based on the linguistic expres-sions used by the speaker in conjunction withpointing.
More precisely, linguistic descriptionscan be used to predict about one third of the point-ing gestures that speakers have produced in thecorpus.
This is an interesting and novel result,which not only supports the general notion thatgestures and speech should be seen as tightly cou-pled, but also suggests that this coupling does notresult in a minimisation of redundancy betweenthe two modalities.
Rather, it appears that a num-ber of pointing gestures accompanied descriptionscontaining locative properties, something that con-tradicts the predictions of models based on thetrade-off hypothesis (Kranstedt and Wachsmuth,2005; van der Sluis and Krahmer, 2007).There are a number of limitations of the presentstudy, which we plan to address in future work.First, pointing gestures in our corpus were rela-tively scarce (16.4% of utterances were accompa-nied by pointing).
This in part explains the relativeaccuracy of our baselines: predicting the major-ity class (that is, no pointing) in every case willclearly yield reasonable results given that the sizeof the class is so large.
On the other hand, therelative scarcity of pointing may also indicate thatpointing is somewhat more costly than linguisticdescription, in cognitive and physical terms.
Infact, the difference we see in the number of point-ing gestures between singleton and group mapsalso seems to confirm this assumption: in thegroup maps, where objects are larger, and thusmore easily pointed at according to the trade-offmodel, there are in fact significantly more pointinggestures.
The incidence of pointing may also havebeen affected by the nature of the domains used:although the shared maps in the experiments werelarge and quite close to the interlocutors, the pres-ence of objects of the same shape may have addedto the general visual clutter of the maps, makingpointing less likely.Another aspect of the data that we have notinvestigated is the presence of individual strate-gies.
We know that speakers differ a lot in theiruse of gesturing as regards e.g.
frequency, typeof gesture and representation techniques.
Recentmodels of gesture production for embodied agentsare taking such differences into account (Neff etal., 2008; Bergmann and Kopp, 2009).
Similarly,some speakers might have a greater preference forpointing than others.
For example, Beun and Cre-mers (1998) note that certain speakers in their cor-pus explicitly stated that they had attempted to per-form the task in their dialogues without pointing,in spite of their having been told that they couldpoint.
Recent data-driven experiments on referen-tial descriptions by Dale and Viethen (Dale and Vi-ethen, 2010), In a domain quite similar to the oneused here, suggest that speakers do indeed clus-ter according to their preferred referential strat-egy.
Similar assumptions have informed REG al-gorithms trained on the TUNA Corpus, in the con-text of the Generation Challenges (Gatt and Belz,2010) (Bohnet, 2008; Di Fabbrizio et al 2008).In future work, we plan to address this questionin a multimodal context, where results by Piwek(2007) have already suggested that such individ-ual strategies may play an important role.The hypothesis that specific combinations ofpointing and linguistic descriptions (for example,an object?s colour or size) can be excluded, isclearly not borne out by the data.
There is, how-ever, a tendency for locative features to act asstronger predictors of pointing gestures.
Althoughthe trend is not very strong, it is an interestingone since it confirms the experimental results byde Ruiter et.
al.
reviewed earlier (de Ruiter et al2012).
This may suggest that a pointing gesturemay ultimately be planned within the same systemas locative features (i.e.
the decision of whether or89not to point is not dependent on the decision ofwhether or not to describe inherent, visual proper-ties of the object, but on whether the object?s lo-cation is to be indicated).
Another feature that isworth exploring further is deixis, specifically thedifference between proximal and distal deictic ex-pressions and their interaction with pointing ges-tures.
For example, Piwek et al(2007) found thatproximal deictic expressions tend to be associatedwith a more intensive attentional focusing mecha-nism, while Bangerter (2004) also observes an as-sociation between pointing and the use of deicticexpressions.From an NLG perspective, our results suggestthat decisions to generate a pointing gesture andthose to select visual attributes might take placeindependently (perhaps in parallel, perhaps in dif-ferent modules).
From a cognitive perspective, itsuggests two types of interaction between atten-tion/vision and language/gesture, related to the de-scription of the ?what?
of an object and its ?where?
(Landau and Jackendoff, 1993).Finally, our study focused on the relationshipbetween the two modalities involved in a referen-tial act, addressing the question of redundancy be-tween them.
We have not addressed the impact ofthe visual properties of a target referent in relationto its surrounding objects, on the choices speakersmake in these two modalities.
This is a priority forfuture work, given that the corpus was designed tobalance the presence or absence of various visualproperties of an object (see Section 3).
Taking thiseven further, it remains to be investigated, for ex-ample, whether there would be interesting differ-ences in the relationship betwene pointing and de-scribing between 2D scenes of the kind used here,and 3D environments of the sort used by Kranst-edt and Wachsmuth (2005).
Another priority isto take into account the interactive nature of thedialogues, with particular focus on the follower?sfeedback to the director, as an indicator of the suc-cess of referential expressions.
This is another as-pect of the dialogue situation that may have an im-pact on planning multimodal referential acts.AcknowledgementsSpecial thanks are due to Ielka van der Sluis,Adrian Bangerter and Paul Piwek, who were in-volved in every step of the design, collection andannotation of the MREDI corpus, and who alsocommented on preliminary drafts of this paper.ReferencesA.
Anderson, M. Bader, E. Bard, E. Boyle, G. M. Do-herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,J.
Miller, C. Sotillo, H. S. Thompson, and R. Wein-ert.
1991.
The HCRC Map Task corpus.
Languageand Speech, 34:351?366.E.
Andre?
and T. Rist.
1996.
Coping with temporalconstraints in multimedia presentation planning.
InProceedings of the 13th National Conference on Ar-tificial Intelligence (AAAI?96).A.
Bangerter.
2004.
Using pointing and describing toachieve joint focus of attention in dialogue.
Psycho-logical Science, 15(6):415?419.K.
Bergmann and S. Kopp.
2009.
GNetIc - usingbayesian decision networks for iconic gesture gen-eration.
In A. Nijholt and H. Vilhja?lmsson, editors,Proceedings of the 9th International Conference onIntelligent Virtual Agents (LNAI 5773), pages 76?89.Springer.R.J.
Beun and A. Cremers.
1998.
Object reference ina shared domain of conversation.
Pragmatics andCognition, 6(1-2):121?152.B.
Bohnet.
2008.
The fingerprint of human refer-ring expressions and their surface realization withgraph transducers.
In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion (INLG?08).C.
Callaway and J. C. Lester.
2002.
Pronominalizationin generated discourse and dialogue.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL?02).R.
Dale and E. Reiter.
1995.
Computational interpre-tation of the Gricean maxims in the generation of re-ferring expressions.
Cognitive Science, 19(8):233?263.R.
Dale and J. Viethen.
2010.
Attribute-centric re-ferring expression generation.
In E. Krahmer andM.
Theune, editors, Empirical Methods in Natu-ral Language Generation, volume 5790 of LNAI.Springer, Berlin and Heidelberg.R.
Dale.
1989.
Cooking up referring expressions.
InProceedings of the 27th annual meeting of the As-sociation for Computational Linguistics (ACL?89),pages 68?75.J.P.
de Ruiter, A. Bangerter, and P. Dings.
2012.
Theinterplay between gesture and speech in the produc-tion of referring expressions: Investigating the trade-off hypothesis.
Topics in Cognitive Science, 4:232?248.J.P.
de Ruiter.
2000.
The production of gesture andspeech.
In D. McNeill, editor, Language and Ges-ture, pages 284?311.
Cambridge University Press.90G.
Di Fabbrizio, A. J. Stent, and S. Bangalore.2008.
Trainable speaker-based referring expressiongeneration.
In Proceedings of the 12th Confer-ence on Computational Natural Language Learning(CONLL?08), pages 151?158.A.
Gatt and A. Belz.
2010.
Introducing shared taskevaluation to nlg: The TUNA shared task evaluationchallenges.
In E. Krahmer and M. Theune, editors,Empirical Methods in Natural Language Genera-tion.
Springer.S.
Kita and A. O?zyu?rek.
2003.
What does cross-linguistic variation in semantic coordination ofspeech and gesture reveal?
: Evidence for an inter-face representation of spatial thinking and speaking.Journal of Memory and Language, 48:16?32.S.
Kopp, K. Bergmann, and I. Wachsmuth.
2008.
Mul-timodal communication from multimodal thinking:Towards an integrated model of speech and gestureproduction.
International Journal of Semantic Com-puting, 2(1):115?136.E.
Krahmer and M. Theune.
2002.
Efficient context-sensitive generation of referring expressions.
InK.
van Deemter and R. Kibble, editors, InformationSharing: Reference and Presupposition in LanguageGeneration and Interpretation.
CSLI Publications,Stanford.E.
Krahmer and K. van Deemter.
2012.
Computationalgeneration of referring expressions: A survey.
Com-putational Linguistics, 38(1):173?218.E.
Krahmer, S. van Erk, and A. Verleg.
2003.
Graph-based generation of referring expressions.
Compu-tational Linguistics, 29(1):53?72.A.
Kranstedt and I. Wachsmuth.
2005.
Incrementalgeneration of multimodal deixis referring to objects.In Proceedings of the 10th European Workshop onNatural Language Generation (ENLG?05).B.
Landau and R. Jackendoff.
1993. what and where inspatial language and spatial cognition.
Behavioraland Brain Sciences, 16:217?238.M.
Louwerse and A. Bangerter.
2010.
Effects of am-biguous gestures and language on the time-course ofreference resolution.
Cognitive Science, 34:1517?1529.K.F.
McCoy and M. Strube.
1999.
Generatinganaphoric expressions: Pronoun or definite descrip-tion?
In Proceedings of the Workshop on the Rela-tion of Discourse/Dialogue Structure and Reference.D.
McNeill and S.D.
Duncan.
2000.
Growth points inthinking for speaking.
In D. McNeill, editor, Lan-guage and Gesture, pages 141?161.
Cambridge Uni-versity Press.D.
McNeill.
1985.
So you think gestures are nonver-bal?
Psychological Review, 92(3):350?371.M.
Neff, M. Kipp, I. Albrecht, and H.-P. Seidel.
2008.Gesture modeling and animation based on a proba-bilistic recreation of speaker style.
ACM Transac-tions on Graphics, 27(1):1?24.P.
Piwek, R-J.
Beun, and A. Cremers.
2007. proximaland distal in language and cognition: Evidence fromdeictic demonstratives in dutch.
Journal of Prag-matics, 40(4):694?718.P.
Piwek.
2007.
Modality choice for generation of re-ferring acts: Pointing vs describing.
In Proceedingsof the Workshop on Multimodal Output Generation(MOG?07)., pages 129?139.I.
van der Sluis and E. Krahmer.
2007.
Generatingmultimodal referring expressions.
Discourse Pro-cesses, 44(3):145?174.I.
van der Sluis, P. Piwek, A. Gatt, and A. Bangerter.2008.
Towards a balanced corpus of multimodal re-ferring expressions in dialogue.
In Proceedings ofthe Symposium on Multimodal Output Generation(MOG?08).I.H.
Witten and E. Frank.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
MorganKaufmann, San Francisco, second edition.91
