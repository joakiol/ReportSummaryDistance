Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 841?848Manchester, August 2008Modeling Latent-Dynamic in Shallow Parsing:A Latent Conditional Model with Improved InferenceXu Sun?
Louis-Philippe Morency?
Daisuke Okanohara?
Jun?ichi Tsujii??
?Department of Computer Science, The University of Tokyo, Hongo 7-3-1, Tokyo, Japan?USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, USA?School of Computer Science, The University of Manchester, 131 Princess St, Manchester, UK?
{sunxu, hillbig, tsujii}@is.s.u-tokyo.ac.jp ?morency@ict.usc.eduAbstractShallow parsing is one of many NLP tasksthat can be reduced to a sequence la-beling problem.
In this paper we showthat the latent-dynamics (i.e., hidden sub-structure of shallow phrases) constitutes aproblem in shallow parsing, and we showthat modeling this intermediate structureis useful.
By analyzing the automaticallylearned hidden states, we show how thelatent conditional model explicitly learnlatent-dynamics.
We propose in this paperthe Best Label Path (BLP) inference algo-rithm, which is able to produce the mostprobable label sequence on latent condi-tional models.
It outperforms two existinginference algorithms.
With the BLP infer-ence, the LDCRF model significantly out-performs CRF models on word features,and achieves comparable performance ofthe most successful shallow parsers on theCoNLL data when further using part-of-speech features.1 IntroductionShallow parsing identifies the non-recursive coresof various phrase types in text.
The paradigmaticshallow parsing problem is noun phrase chunking,in which the non-recursive cores of noun phrases,called base NPs, are identified.
As the represen-tative problem in shallow parsing, noun phrasechunking has received much attention, with the de-velopment of standard evaluation datasets and withc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.extensive comparisons among methods (McDon-ald 2005; Sha & Pereira 2003; Kudo & Matsumoto2001).Syntactic contexts often have a complex under-lying structure.
Chunk labels are usually far toogeneral to fully encapsulate the syntactic behaviorof word sequences.
In practice, and given the lim-ited data, the relationship between specific wordsand their syntactic contexts may be best modeledat a level finer than chunk tags but coarser thanlexical identities.
For example, in the noun phrase(NP) chunking task, suppose that there are two lex-ical sequences, ?He is her ??
and ?He gave her?
?.
The observed sequences, ?He is her?
and?He gave her?, would both be conventionally la-beled by ?BOB?, where B signifies the ?beginningNP?, and O the ?outside NP?.
However, this label-ing may be too general to encapsulate their respec-tive syntactic dynamics.
In actuality, they have dif-ferent latent-structures, crucial in labeling the nextword.
For ?He is her ?
?, the NP started by ?her?
isstill incomplete, so the label for ?
is likely to be I,which conveys the continuation of the phrase, e.g.,?
[He] is [her brother]?.
In contrast, for ?He gaveher ?
?, the phrase started by ?her?
is normally self-complete, and makes the next label more likely tobe B, e.g., ?
[He] gave [her] [flowers]?.In other words, latent-dynamics is an interme-diate representation between input features and la-bels, and explicitly modeling this can simplify theproblem.
In particular, in many real-world cases,when the part-of-speech tags are not available, themodeling on latent-dynamics would be particu-larly important.In this paper, we model latent-dynamics inshallow parsing by extending the Latent-DynamicConditional Random Fields (LDCRFs) (Morencyet al 2007), which offer advantages over previ-841y y y1 2 m h h h1 2 my y y1 2 mCRF LDCRFx x x1 2 mx x x1 2 mFigure 1: Comparison between CRF and LDCRF.In these graphical models, x represents the obser-vation sequence, y represents labels and h repre-sents hidden states assigned to labels.
Note thatonly gray circles are observed variables.
Also,only the links with the current observation areshown, but for both models, long range dependen-cies are possible.ous learning methods by explicitly modeling hid-den state variables (see Figure 1).
We expect LD-CRFs to be particularly useful in those cases with-out POS tags, though this paper is not limited tothis.The inference technique is one of the most im-portant components for a structured classificationmodel.
In conventional models like CRFs, the op-timal label path can be directly searched by usingdynamic programming.
However, for latent condi-tional models like LDCRFs, the inference is kindof tricky, because of hidden state variables.
In thispaper, we propose an exact inference algorithm,the Best Label Path inference, to efficiently pro-duce the optimal label sequence on LDCRFs.The following section describes the relatedwork.
We then review LDCRFs, and propose theBLP inference.
We further present a statisticalinterpretation on learned hidden states.
Finally,we show that LDCRF-BLP is particularly effectivewhen pure word features are used, and when POStags are added, as existing systems did, it achievescomparable results to the best reported systems.2 Related WorkThere is a wide range of related work on shallowparsing.
Shallow parsing is frequently reduced tosequence labeling problems, and a large part ofprevious work uses machine learning approaches.Some approaches rely on k-order generative proba-bilistic models of paired input sequences and labelsequences, such as HMMs (Freitag & McCallum2000; Kupiec 1992) or multilevel Markov mod-els (Bikel et al 1999).
The generative modelprovides well-understood training and inferencebut requires stringent conditional independence as-sumptions.To accommodate multiple overlapping featureson observations, some other approaches view thesequence labeling problem as a sequence of clas-sification problems, including support vector ma-chines (SVMs) (Kudo & Matsumoto 2001) and avariety of other classifiers (Punyakanok & Roth2001; Abney et al 1999; Ratnaparkhi 1996).Since these classifiers cannot trade off decisions atdifferent positions against each other (Lafferty etal.
2001), the best classifier based shallow parsersare forced to resort to heuristic combinations ofmultiple classifiers.A significant amount of recent work has shownthe power of CRFs for sequence labeling tasks.CRFs use an exponential distribution to model theentire sequence, allowing for non-local dependen-cies between states and observations (Lafferty etal.
2001).
Lafferty et al (2001) showed that CRFsoutperform classification models as well as HMMson synthetic data and on POS tagging tasks.
As forthe task of shallow parsing, CRFs also outperformmany other state-of-the-art models (Sha & Pereira2003; McDonald et al 2005).When the data has distinct sub-structures, mod-els that exploit hidden state variables are advanta-geous in learning (Matsuzaki et al 2005; Petrovet al 2007).
Sutton et al (2004) presented anextension to CRF called dynamic conditional ran-dom field (DCRF) model.
As stated by the authors,training a DCRF model with unobserved nodes(hidden variables) makes their approach difficultto optimize.
In the vision community, the LD-CRF model was recently proposed by Morency etal.
(2007), and shown to outperform CRFs, SVMs,and HMMs for visual sequence labeling.In this paper, we introduce the concept of latent-dynamics for shallow parsing, showing how hid-den states automatically learned by the modelpresent similar characteristics.
We will also pro-pose an improved inference technique, the BLP,for producing the most probable label sequence inLDCRFs.3 Latent-Dynamic Conditional RandomFieldsThe task is to learn a mapping between a sequenceof observations x = x1, x2, .
.
.
, xmand a sequenceof labels y = y1, y2, .
.
.
, ym.
Each yjis a class la-842bel for the j?th token of a word sequence and is amember of a set Y of possible class labels.
Foreach sequence, the model also assumes a vector ofhidden state variables h = {h1, h2, .
.
.
, hm}, whichare not observable in the training examples.Given the above definitions, we define a latentconditional model as follows:P(y|x,?)
=?hP(y|h, x,?)P(h|x,?
), (1)where ?
are the parameters of the model.
The LD-CRF model can seem as a natural extension of theCRF model, and the CRF model can seem as a spe-cial case of LDCRFs employing one hidden statefor each label.To keep training and inference efficient, we re-strict the model to have disjointed sets of hiddenstates associated with each class label.
Each hjisa member of a set Hyjof possible hidden states forthe class label yj.
We define H, the set of all pos-sible hidden states to be the union of all Hyjsets.Since sequences which have any hj< Hyjwill bydefinition have P(y|x,?)
= 0, we can express ourmodel as:P(y|x,?)
=?h?Hy1?...?HymP(h|x,?
), (2)where P(h|x,?)
is defined using the usual con-ditional random field formulation: P(h|x,?)
=exp ??f(h|x)/?
?hexp ?
?f(h|x), in which f(h|x) isthe feature vector.
Given a training set consistingof n labeled sequences (xi, yi) for i = 1 .
.
.
n, train-ing is performed by optimizing the objective func-tion to learn the parameter ??:L(?)
=n?i=1log P(yi|xi,?)
?
R(?).
(3)The first term of this equation is the conditionallog-likelihood of the training data.
The secondterm is the regularizer.4 BLP Inference on Latent ConditionalModelsFor testing, given a new test sequence x, we wantto estimate the most probable label sequence (BestLabel Path), y?, that maximizes our conditionalmodel:y?= argmaxyP(y|x,??).
(4)In the CRF model, y?can be simply searched byusing the Viterbi algorithm.
However, for latentconditional models like LDCRF, the Best LabelPath y?cannot directly be produced by the Viterbialgorithm because of the incorporation of hiddenstates.In this paper, we propose an exact inference al-gorithm, the Best Label Path inference (BLP), forproducing the most probable label sequence y?onLDCRF.
In the BLP schema, top-n hidden pathsHPn= {h1,h2.
.
.
hn} over hidden states are effi-ciently produced by using A?search (Hart et al,1968), and the corresponding probabilities of hid-den paths P(hi|x,?)
are gained.
Thereafter, basedon HPn, the estimated probabilities of various la-bel paths, P(y|x,?
), can be computed by summingthe probabilities of hidden paths, P(h|x,?
), con-cerning the association between hidden states andeach class label:P(y|x,?)
=?h: h?Hy1?...?Hym?h?HPnP(h|x,?).
(5)By using the A?search, HPncan be extended in-crementally in an efficient manner, until the algo-rithm finds that the Best Label Path is ready, andthen the search stops and ends the BLP inferencewith success.
The algorithm judges that y?is readywhen the following condition is achieved:P(y1|x,?)
?
P(y2|x,?)
+?h<Hy1?...?HymP(h|x,?
), (6)where y1is the most probable label sequence, andy2is the second ranked label sequence estimatedby using HPn.
It would be straightforward to provethat y?= y1, and further search is unnecessary, be-cause in this case, the unknown probability masscan not change the optimal label path.
The un-known probability mass can be computed by using?h<Hy1?...?HymP(h|x,?)
= 1 ??h?Hy1?...?HymP(h|x,?).
(7)The top-n hidden paths of HPnproduced by theA?-search are exact, and the BLP inference is ex-act.
To guarantee HPnis exact in our BLP in-ference, an admissible heuristic function shouldbe used in A?search (Hart et al, 1968).
We usea backward Viterbi algorithm (Viterbi, 1967) tocompute the heuristic function of the forward A?search:Heui(hj) = maxh?i=hj?h?i?HP|h|iP?(h?|x,??
), (8)843where h?i= hjrepresents a partial hidden pathstarted from the hidden state hj, and HP|h|irep-resents all possible partial hidden paths from theposition i to the ending position |h| .
Heui(hj) isan admissible heuristic function for the A?searchover hidden paths, therefore HPnis exact and BLPinference is exact.The BLP inference is efficient when the prob-ability distribution among the hidden paths is in-tensive.
By combining the forward A?with thebackward Viterbi algorithm, the time complexityof producing HPnis roughly a linear complexityconcerning its size.
In practice, on the CoNLL testdata containing 2,012 sentences, the BLP infer-ence finished in five minutes when using the fea-ture set based on both word and POS information(see Table 3).
The memory consumption is alsorelatively small, because it is an online style algo-rithm and it is not necessary to preserve HPn.In this paper, to make a comparison, we alsostudy the Best Hidden Path inference (BHP):yBHP= argmaxyP(hy|x,??
), (9)where hy?
Hy1?
.
.
.
?Hym.
In other words, theBest Hidden Path is the label sequence that is di-rectly projected from the most probable hiddenpath h?.In (Morency et al 2007), y?is estimated by us-ing the Best Point-wise Marginal Path (BMP).
Toestimate the label yjof token j, the marginal prob-abilities P(hj= a|x,?)
are computed for possiblehidden states a ?
H. Then, the marginal probabili-ties are summed and the optimal label is estimatedby using the marginal probabilities.The BLP produces y?while the BHP and theBMP perform an estimation on y?.
We will makean experimental comparison in Section 6.5 Analyzing Latent-DynamicsThe chunks in shallow parsing are represented withthe three labels shown in Table 1, and shallow pars-ing is treated as a sequence labeling task with thosethree labels.
A challenge for most shallow parsingapproaches is to determine the concepts learned bythe model.
In this section, we show how we cananalyze the latent-dynamics.5.1 Analyzing Latent-DynamicsIn this section, we show how to analyze the charac-teristics of the hidden states.
Our goal is to find thewords characterizing a specific hidden state, andB words beginning a chunkI words continuing a chunkO words being outside a chunkTable 1: Shallow parsing labels.then look at the selected words with their associ-ated POS tags to determine if the LDCRF modelhas learned meaningful latent-dynamics.In the experiments reported in this section, wedid not use the features on POS tags in order toisolate the model?s capability of learning latent dy-namics.
In other words, the model could simplylearn the dynamics of POS tags as the latent dy-namics if the model is given the information aboutPOS tags.
The features used in the experiments arelisted on the left side (Word Features) in Table 3.The main idea is to look at the marginal proba-bilities P(hj= a|x,?)
for each word j, and selectthe hidden state a?with the highest probability.
Bycounting how often a specific word selected a asthe optimal hidden state, i.e., ?
(w, a), we can cre-ate statistics about the relationship between hiddenstates and words.
We define relative frequency asthe number of times a specific word selected a hid-den state while normalized by the global frequencyof this word:RltFreq(w, hj) =Freq( ?
(w, hj) )Freq(w).
(10)5.2 Learned Latent-Dynamics from CoNLLIn this subsection, we show the latent-dynamicslearned automatically from the CoNLL dataset.The details of these experiments are presented inthe following section.The most frequent three words corresponding tothe individual hidden states of the labels, B and O,are shown in Table 2.
As shown, the automati-cally learned hidden states demonstrate prominentcharacteristics.
The extrinsic label B, which beginsa noun phrase, is automatically split into 4 sub-categories: wh-determiners (WDT, such as ?that?
)together with wh-pronouns (WP, such as ?who?
),the determiners (DT, such as ?any, an, a?
), the per-sonal pronouns (PRP, such as ?they, we, he?
), andthe singular proper nouns (NNP, such as ?Nasdaq,Florida?)
together with the plural nouns (NNS,such as ?cities?).
The results of B1 suggests thatthe wh-determiners represented by ?that?, and thewh-pronouns represented by ?who?, perform simi-844Labels HidStat Words POS RltFreqBThat WDT 0.85B1 who WP 0.49Who WP 0.33any DT 1.00B2 an DT 1.00a DT 0.98They PRP 1.00B3 we PRP 1.00he PRP 1.00Nasdaq NNP 1.00B4 Florida NNP 0.99cities NNS 0.99OBut CC 0.88O1 by IN 0.73or IN 0.674.6 CD 1.00O2 1 CD 1.0011 CD 0.62were VBD 0.94O3 rose VBD 0.93have VBP 0.92been VBN 0.97O4 be VB 0.94to TO 0.92Table 2: Latent-dynamics learned automatically bythe LDCRF model.
This table shows the top threewords and their gold-standard POS tags for eachhidden states.lar roles in modeling the dynamics in shallow pars-ing.
Further, the singular proper nouns and theplural nouns are grouped together, suggesting thatthey may perform similar roles.
Moreover, we cannotice that B2 and B3 are highly consistent.The label O is automatically split into the coordi-nating conjunctions (CC) together with the prepo-sitions (IN) indexed by O1, the cardinal numbers(CD) indexed by O2, the past tense verbs (VBD)together with the personal verbs (VBP) indexed byO3, and another sub-category, O4.
From the resultswe can find that gold-standard POS tags may notbe adequate in modeling latent-dynamics in shal-low parsing, as we can notice that three hiddenstates out of four (O1, O3 and O4) contains relat-ing but different gold-standard POS tags.6 ExperimentsFollowing previous studies on shallow parsing, ourexperiments are performed on the CoNLL 2000Word Features:{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}?
{hi, hi?1hi, hi?2hi?1hi}POS Features:{ti?1, ti, ti+1, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2,ti?2ti?1ti, ti?1titi+1, titi+1ti+2}?
{hi, hi?1hi, hi?2hi?1hi}Table 3: Feature templates used in the experi-ments.
wiis the current word; tiis current POStag; and hiis the current hidden state (for the caseof latent models) or the current label (for the caseof conventional models).data set (Sang & Buchholz 2000; Ramshow &Marcus 1995).
The training set consists of 8,936sentences, and the test set consists of 2,012 sen-tences.
The standard evaluation metrics for thistask are precision p (the fraction of output chunksmatching the reference chunks), recall r (the frac-tion of reference chunks returned), and the F-measure given by F = 2pr/(p + r).6.1 LDCRF for Shallow ParsingWe implemented LDCRFs in C++, and optimizedthe system to cope with large scale problems, inwhich the feature dimension is beyond millions.We employ similar predicate sets defined in Sha& Pereira (2003).
We follow them in using predi-cates that depend on words as well as POS tags inthe neighborhood of a given position, taking intoaccount only those 417,835 features which occurat least once in the training data.
The features arelisted in Table 3.As for numerical optimization (Malouf 2002;Wallach 2002), we performed gradient decent withthe Limited-Memory BFGS (L-BFGS) optimiza-tion technique (Nocedal & Wright 1999).
L-BFGSis a second-order Quasi-Newton method that nu-merically estimates the curvature from previousgradients and updates.
With no requirement onspecialized Hessian approximation, L-BFGS canhandle large-scale problems in an efficient manner.We implemented an L-BFGS optimizer in C++ bymodifying the OWLQN package (Andrew & Gao2007) developed by Galen Andrew.
In our exper-iments, storing 10 pairs of previous gradients forthe approximation of the function?s inverse Hes-sian worked well, making the amount of the ex-tra memory required modest.
Using more pre-vious gradients will probably decrease the num-845ber of iterations required to reach convergence,but would increase memory requirements signifi-cantly.
To make a comparison, we also employedthe Conjugate-Gradient (CG) optimization algo-rithm.
For details of CG, see Shewchuk (1994).Since the objective function of the LDCRFmodel is non-convex, it is suggested to use the ran-dom initialization of parameters for the training.To reduce overfitting, we employed an L2Gaus-sian weight prior (Chen & Rosenfeld 1999).
Dur-ing training and validation, we varied the numberof hidden states per label (from 2 to 6 states perlabel), and also varied the L2-regularization term(with values 10k, k from -3 to 3).
Our experimentssuggested that using 4 or 5 hidden states per labelfor the shallow parser is a viable compromise be-tween accuracy and efficiency.7 Results and Discussion7.1 Performance on Word FeaturesAs discussed in Section 4, it is preferred to notuse the features on POS tags in order to isolatethe model?s capability of learning latent dynam-ics.
In this sub-section, we use pure word fea-tures with their counts above 10 in the training datato perform experimental comparisons among dif-ferent inference algorithms on LDCRFs, includingBLP, BHP, and existing BMP.Since the CRF model is one of the success-ful models in sequential labeling tasks (Lafferty etal.
2001; Sha & Pereira 2003; McDonald et al2005), in this section, we also compare LDCRFswith CRFs.
We tried to make experimental resultsmore comparable between LDCRF and CRF mod-els, and have therefore employed the same fea-tures set, optimizer and fine-tuning strategy be-tween LDCRF and CRF models.The experimental results are shown in Table 4.In the table, Acc.
signifies ?label accuracy?, whichis useful for the significance test in the follow-ing sub-section.
As shown, LDCRF-BLP outper-forms LDCRF-BHP and LDCRF-BMP, suggestingthat BLP inference1is superior.
The superiorityof BLP is statistically significant, which will beshown in next sub-section.
On the other side, allthe LDCRF models outperform the CRF model.
Inparticular, the gap between LDCRF-BLP and CRFis 1.53 percent.1In practice, for efficiency, we approximated the BLP on afew sentences by limiting the number of search steps.Models: WF Acc.
Pre.
Rec.
F1LDCRF-BLP 97.01 90.33 88.91 89.61LDCRF-BHP 96.52 90.26 88.21 89.22LDCRF-BMP 97.26 89.83 89.06 89.44CRF 96.11 88.12 88.03 88.08Table 4: Experimental comparisons among differ-ent inference algorithms on LDCRFs, and the per-formance of CRFs using the same feature set onpure word features.
The BLP inference outper-forms the BHP and BMP inference.
LDCRFs out-perform CRFs.Models F1Gap Acc.
Gap Sig.BLP vs. BHP 0.39 0.49 1e-10BLP vs. CRF 1.53 0.90 5e-13Table 5: The significance tests.
LDCRF-BLP issignificantly more accurate than LDCRF-BHP andCRFs.7.2 Labeling Accuracy and Significance TestAs shown in Table 4, the accuracy rate for individ-ual labeling decisions is over-optimistic as a mea-sure for shallow parsing.
Nevertheless, since test-ing the significance of shallow parsers?
F-measuresis tricky, individual labeling accuracy provides amore convenient basis for statistical significancetests (Sha & Pereira 2003).
One such test is theMcNemar test on paired observations (Gillick &Cox 1989).
As shown in Table 5, for the LD-CRF model, the BLP inference schema is sta-tistically more accurate than the BHP inferenceschema.
Also, Evaluations show that the McNe-mar?s value on labeling disagreement between theLDCRF-BLP and CRF models is 5e-13, suggest-ing that LDCRF-BLP is significantly more accu-rate than CRFs.On the other hand, the accuracy rate of BMP in-ference is a special case.
Since the BMP inferenceis essentially an accuracy-first inference schema,the accuracy rate and the F-measure have a differ-ent relation in BMP.
As we can see, the individuallabeling accuracy achieved by the LDCRF-BMPmodel is as high as 97.26%, but its F-measure isstill lower than LDCRF-BLP.7.3 Convergence SpeedIt would be interesting to compare the convergencespeed between the objective loss function of LD-CRFs and CRFs.
We apply the L-BFGS optimiza-8461502002503003504004505000  50  100  150  200  250PenalizedLossPassesLDCRFCRFFigure 2: The value of the penalized loss based onthe number of iterations: LDCRFs vs. CRFs.1601802002202402602800  50  100  150  200  250PenalizedLossPassesL-BFGSCGFigure 3: Training the LDCRF model: L-BFGSvs.
CG.tion algorithm to optimize the loss function of LD-CRF and CRF models, making a comparison be-tween them.
We find that the iterations requiredfor the convergence of LDCRFs is less than forCRFs (see Figure 2).
Normally, the LDCRF modelarrives at the plateau of convergence in 120-150iterations, while CRFs require 210-240 iterations.When we replace the L-BFGS optimizer by the CGoptimization algorithm, we observed as well thatLDCRF converges faster on iteration numbers thanCRF does.On the contrary, however, the time cost of theLDCRF model in each iteration is higher than theCRF model, because of the incorporation of hid-den states.
The time cost of the LDCRF modelin each iteration is roughly a quadratic increaseconcerning the increase of the number of hiddenstates.
Therefore, though the LDCRF model re-quires less passes for the convergence, it is practi-cally slower than the CRF model.
Improving thescalability of the LDCRF model would be a inter-esting topic in the future.Furthermore, we make a comparison betweenModels: WF+POS Pre.
Rec.
F1LDCRF-BLP 94.65 94.03 94.34CRFN/A N/A 93.6(Vishwanathan et al 06)CRF94.57 94.00 94.29(McDonald et al 05)Voted perceptronN/A N/A 93.53(Collins 02)Generalized Winnow93.80 93.99 93.89(Zhang et al 02)SVM combination94.15 94.29 94.22(Kudo & Matsumoto 01)Memo.
classifier93.63 92.89 93.26(Sang 00)Table 6: Performance of the LDCRF-BLP model,and the comparison with CRFs and other success-ful approaches.
In this table, all the systems haveemployed POS features.the L-BFGS and the CG optimizer for LDCRFs.We observe that the L-BFGS optimizer is slightlyfaster than CG on LDCRFs (see Figure 3), whichechoes the comparison between the L-BFGS andthe CG optimizing technique on the CRF model(Sha & Pereira 2003).7.4 Comparisons to Other Systems with POSFeaturesPerformance of the LDCRF-BLP model and someof the best results reported previously are summa-rized in Table 6.
Our LDCRF model achievedcomparable performance to those best reportedsystems in terms of the F-measure.McDonald et al (2005) achieved an F-measureof 94.29% by using a CRF model.
By employing amulti-model combination approach, Kudo & Mat-sumoto (2001) also achieved a good performance.They use a combination of 8 kernel SVMs witha heuristic voting strategy.
An advantage of LD-CRFs over max-margin based approaches is thatLDCRFs can output N-best label sequences andtheir probabilities using efficient marginalizationoperations, which can be used for other compo-nents in an information extraction system.8 Conclusions and Future WorkIn this paper, we have shown that automatic model-ing on ?latent-dynamics?
can be useful in shallowparsing.
By analyzing the automatically learned847hidden states, we showed how LDCRFs can natu-rally learn latent-dynamics in shallow parsing.We proposed an improved inference algorithm,the BLP, for LDCRFs.
We performed experimentsusing the CoNLL data, and showed how the BLPinference outperforms existing inference engines.When further employing POS features as othersystems did, the performance of the LDCRF-BLPmodel is comparable to those best reported results.The LDCRF model demonstrates a significant ad-vantage over other models on pure word featuresin this paper.
We expect it to be particularly usefulin the real-world tasks without rich features.The latent conditional model handles latent-dynamics naturally, and can be easily extended toother labeling tasks.
Also, the BLP inference algo-rithm can be extended to other latent conditionalmodels for producing optimal label sequences.
Asa future work, we plan to further speed up the BLPalgorithm.AcknowledgmentsMany thanks to Yoshimasa Tsuruoka for helpfuldiscussions on the experiments and paper-writing.This research was partially supported by Grant-in-Aid for Specially Promoted Research 18002007(MEXT, Japan).
The work at the USC Institute forCreative Technology was sponsored by the U.S.Army Research, Development, and EngineeringCommand (RDECOM), and the content does notnecessarily reflect the position or the policy of theGovernment, and no official endorsement shouldbe inferred.ReferencesAbney, S. 1991.
Parsing by chunks.
In R. Berwick, S. Ab-ney, and C. Tenny, editors, Principle-based Parsing.
KluwerAcademic Publishers.Abney, S.; Schapire, R. E. and Singer, Y.
1999.
Boostingapplied to tagging and PP attachment.
In Proc.
EMNLP/VLC-99.Andrew, G. and Gao, J.
2007.
Scalable training of L1-regularized log-linear models.
In Proc.
ICML-07.Bikel, D. M.; Schwartz, R. L. and Weischedel, R. M. 1999.An algorithm that learns what?s in a name.
Machine Learning,34: 211-231.Chen, S. F. and Rosenfeld, R. 1999.
A Gaussian priorfor smooth-ing maximum entropy models.
Technical ReportCMU-CS-99-108, CMU.Collins, M. 2002.
Discriminative training methods for hid-den Markov models: Theory and experiments with perceptronalgorithms.
In Proc.
EMNLP-02.Freitag, D. and McCallum, A.
2000.
Information extrac-tion with HMM structures learned by stochastic optimization.In Proc.
AAAI-00.Gillick, L. and Cox, S. 1989.
Some statistical issues inthe comparison of speech recognition algorithms.
In Interna-tional Conference on Acoustics Speech and Signal Process-ing, v1, pages 532-535.Hart, P.E.
; Nilsson, N.J.; and Raphael, B.
1968.
A formalbasis for the heuristic determination of minimum cost path.IEEE Trans.
On System Science and Cybernetics, SSC-4(2):100-107.Kudo, T. and Matsumoto, Y.
2001.
Chunking with supportvector machines.
In Proc.
NAACL-01.Kupiec, J.
1992.
Robust part-of-speech tagging using ahidden Markov model.
Computer Speech and Language.6:225-242.Lafferty, J.; McCallum, A. and Pereira, F. 2001.
Condi-tional random fields: Probabilistic models for segmenting andlabeling sequence data.
In Proc.
ICML-01, pages 282-289.Malouf, R. 2002.
A comparison of algorithms for maxi-mum entropy parameter estimation.
In Proc.
CoNLL-02.Matsuzaki, T.; Miyao Y. and Tsujii, J.
2005.
ProbabilisticCFG with Latent Annotations.
In Proc.
ACL-05.McDonald, R.; Crammer, K. and Pereira, F. 2005.
FlexibleText Segmentation with Structured Multilabel Classification.In Proc.
HLT/EMNLP-05, pages 987- 994.Morency, L.P.; Quattoni, A. and Darrell, T. 2007.
Latent-Dynamic Discriminative Models for Continuous GestureRecognition.
In Proc.
CVPR-07, pages 1- 8.Nocedal, J. and Wright, S. J.
1999.
Numerical Optimiza-tion.
Springer.Petrov, S.; Pauls, A.; and Klein, D. 2007.
Discriminativelog-linear grammars with latent variables.
In Proc.
NIPS-07.Punyakanok, V. and Roth, D. 2001.
The use of classifiersin sequential inference.
In Proc.
NIPS-01, pages 995-1001.MIT Press.Ramshaw, L. A. and Marcus, M. P. 1995.
Text chunkingusing transformation-based learning.
In Proc.
Third Work-shop on Very Large Corpora.
In Proc.
ACL-95.Ratnaparkhi, A.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
EMNLP-96.Sang, E.F.T.K.
2000.
Noun Phrase Representation by Sys-tem Combination.
In Proc.
ANLP/NAACL-00.Sang, E.F.T.K and Buchholz, S. 2000.
Introduction to theCoNLL-2000 shared task: Chunking.
In Proc.
CoNLL-00,pages 127-132.Sha, F. and Pereira, F. 2003.
Shallow Parsing with Condi-tional Random Fields.
In Proc.
HLT/NAACL-03.Shewchuk, J. R. 1994.
An introduction to theconjugate gradient method without the agonizing pain.http://www.2.cs.cmu.edu/jrs/jrspapers.html/#cg.Sutton, C.; Rohanimanesh, K. and McCallum, A.
2004.Dynamic conditional random fields: Factorized probabilisticmodels for labeling and segmenting sequence data.
In Proc.ICML-04.Viterbi, A.J.
1967.
Error bounds for convolutional codesand an asymptotically optimum decoding algorithm.
IEEETransactions on Information Theory.
13(2):260-269.Vishwanathan, S.; Schraudolph, N. N.; Schmidt, M.W.
andMurphy, K. 2006.
Accelerated training of conditional randomfields with stochastic meta-descent.
In Proc.
ICML-06.Wallach, H. 2002.
Efficient training of conditional randomfields.
In Proc.
6th Annual CLUK Research Colloquium.Zhang, T.; Damerau, F. and Johnson, D. 2002.
Text chunk-ing based on a generalization of winnow.
Journal of MachineLearning Research, 2:615-637.848
