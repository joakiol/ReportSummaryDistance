Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1769?1779,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning Prototypical Event Structure from Photo AlbumsAntoine Bosselut?, Jianfu Chen?, David Warren?, Hannaneh Hajishirzi?and Yejin Choi?
?Computer Science & Engineering, University of Washington, Seattle, WA{antoineb, hannaneh, yejin}@cs.washington.edu?Department of Computer Science, Stony Brook University, Stony Brook, NY{jianchen, warren}@cs.stonybrook.eduAbstractActivities and events in our lives are struc-tural, be it a vacation, a camping trip, ora wedding.
While individual details vary,there are characteristic patterns that arespecific to each of these scenarios.
For ex-ample, a wedding typically consists of asequence of events such as walking downthe aisle, exchanging vows, and dancing.In this paper, we present a data-driven ap-proach to learning event knowledge from alarge collection of photo albums.
We for-mulate the task as constrained optimiza-tion to induce the prototypical temporalstructure of an event, integrating both vi-sual and textual cues.
Comprehensiveevaluation demonstrates that it is possibleto learn multimodal knowledge of eventstructure from noisy web content.1 IntroductionMany common scenarios in our lives, such as awedding or a camping trip, show characteristicstructural patterns.
As illustrated in Figure 1, thesepatterns can be sequential, such as in a wedding,where exchanging vows generally happens beforecutting the cake.
In other scenarios, there may be aset of composing events, but no prominent tempo-ral relations.
A camping trip, for example, mightinclude events such as hiking, which can happeneither before or after setting up a tent.This observation on the prototypical patterns ineveryday scenarios goes back to early artificial in-telligence research.
Scripts (Schank and Abelson,1975), an early formalism, were developed to en-code the necessary background knowledge to sup-port an inference engine for common sense rea-soning in limited domains.
However, early ap--Ring	?me.
-Exchanging	our	rings.
-Rings	and	promises.Kiss-Our	first	ever	kiss.
-You	may	kiss	the	bride.
-Sealed	with	a	kiss.Cut	the	cake-Cake	cu?g.
-The	cake	was	so	solid.-Dancing	excitement.
-First	dance.
-Ballroom	dancing.reading	vows	 presen?g	rings	 best	cake	everPhoto	albumsdown	the	aislePrototypical	Cap?ns:Exchange	rings	DanceLearned	Events:-Reading	our	vows.
-Our	vows.VowsTemporal	Knowledge:Dance?
?Kiss	 Cake	 Vows	 RingsFigure 1: We collect photo albums of common scenar-ios (e.g., weddings) and cluster their images and captions tolearn the hierarchical events that make up these scenarios.
Weuse constrained optimization to decode the temporal order ofthese events, and we extract the prototypical descriptions thatdefine them.proaches based on hand-coded symbolic represen-tations proved to be brittle and difficult to scale.An alternative direction in recent years hasbeen statistical knowledge induction, i.e., learn-ing script or common sense knowledge bottom-upfrom large-scale data.
While most prior work isbased on text (Pichotta and Mooney, 2014; Janset al, 2012; Chambers and Jurafsky, 2008; Cham-bers, 2013), recent work begins exploring the useof images as well (Bagherinezhad et al, 2016;Vedantam et al, 2015).In this paper, we present the first study for learn-ing knowledge about common life scenarios (e.g.,weddings, camping trips) from a large collectionof online photo albums with time-stamped imagesand their captions.
The resulting dataset includes34,818 time-stamped photo albums correspondingto 12 distinct event scenarios with 1.5 million im-ages and captions (see Table 1 for more details).We cast unsupervised learning of event struc-ture as a sequential multimodal clustering prob-1769lem, which requires solving two subproblems con-currently: identifying the boundaries of events andassigning identities to each of these events.
Weformulate this process as constrained optimiza-tion, where constraints encode the temporal eventpatterns that are induced directly from the data.The outcome is a statistically induced prototypi-cal structure of events characterized by their visualand textual representations.We evaluate the quality and utility of the learnedknowledge in three tasks: temporal event ordering,segmentation prediction, and multimodal summa-rization.
Our experimental results show the per-formance of our model in predicting the order ofphotos in albums, partitioning photo albums intoevent sequences, and summarizing albums.2 OverviewThe high-level goal of this work is unsupervisedinduction of the prototypical event structure ofcommon scenarios from multimodal data.
Weassume a two-level structure: high-level events,which we refer to as scenarios (e.g., wedding, fu-neral), are given, and low-level events (e.g., dance,kiss, vows), which we refer to as events, are to beautomatically induced.
In this section, we providethe overview of the paper (Section 2.1), and intro-duce our new dataset (Section 2.2).2.1 ApproachGiven a large collection of photo albums corre-sponding to a scenario, we want to learn three as-pects of event knowledge by (1) identifying eventscommon to the given scenario (Section 4.1), (2)learning temporal relations across events (Sec-tion 4.2), and (3) extracting prototypical captionsfor each event (Section 4.3).To induce the prototypical event structure, animportant subproblem we consider is individualphoto album analysis, where the task is (1) par-titioning each photo album into a sequence of seg-ments, and (2) assigning the event identity to eachsegment.
We present an inference model basedon Integer Linear Programming (ILP) in Section 3to perform both segmentation and event identi-fication simultaneously, in consideration of thelearned knowledge that we describe in Section 4.Finally, we evaluate the utility of the automat-ically induced knowledge in the context of threeconcrete tasks: temporal ordering of photos (Sec-tion 6.1), album segmentation (Section 6.2), andscenario # of albums # of imagesWEDDING 4689 192KMARATHON 3961 158KCOOKING 1168 36KFUNERAL 781 28KBARBECUE 735 22KBABY BIRTH 688 21KPARIS TRIP 4603 306KNEW YORK TRIP 4205 267KCAMPING 4063 159KTHANKSGIVING 5928 153KCHRISTMAS 3449 98KINDEPENDENCE DAY 548 22KTOTAL 34,818 1.5MTable 1: Dataset Statistics: the number of albums and im-ages compiled for each scenario.
The middle horizontal lineseparates the scenarios we predict have a well-defined tem-poral structure (top) from those we predict do not (bottom).photo album summarization (Section 6.3).2.2 DatasetFor this study, we have compiled a new corpus ofmultimodal photo albums across 12 distinct sce-narios.
It comprises of 34,818 albums containing1.5 million pairs of online photographs and theirtextual descriptions.
Table 1 shows the list of sce-narios and the corresponding data statistics.
Wechoose six scenarios (the top half of Table 1) thatwe expect have an inherent temporal event struc-ture that can be learned and six that we expect donot (the bottom half of Table 1).The dataset is collected using the Flickr API1,2.We use the scenario names and variations of them(e.g., Paris Trip, Paris Vacation) as queries forimages.
We then form albums from these im-ages by grouping images by user, sorting themby timestamp, and extracting groups that arewithin a contained time frame (e.g., 24 hoursfor a wedding, 5 days for a trip).
For all im-ages, we extract the first sentences of the cor-responding textual descriptions as captions andalso store their timestamps.
This data is publiclyavailable at https://www.cs.washington.edu/projects/nlp/protoevents.3 Inference Model for Multimodal EventSegmentation and IdentificationGiven a photo album, the goal of the inference is toassign events to photos and to segment albums byevent.
More formally, given a sequence ofM pho-tos P = {p1, .
.
.
, pM}, andN learned eventsE ={e1, .
.
.
, eN}, the task is to assign each photo to a1https://www.flickr.com/services/api/2https://pypi.python.org/pypi/flickrapi/1.4.51770PL(kiss?????dance)?=?.04??
vows?kiss?
toasts?cut?cake?
dancing?
aisle?ready?dancing?ready?
toasts?aisle?
vows?
kiss?Event?affinity?scores?Ac?Av?bc?:?????.31???????????????.09??????????????.82?
PL(dance?????toast)?=?.17?PG(vows?????toast)?=?.84?Learned??Events:?Photo?Album:?Event?Assignments?and?Segments:?bv?:?????.69???????????????.32??????????????.91????????
?
PG(vows??????dance)?=?.79??
?
?Event?Seg ?temporalFigure 2: The events learned in Section 4 are assigned to photos based on textual (Ac) and visual (Av) affinities, which encodehow well a photo represents an event (?event).
Segmentation scores (?seg) between adjacent photos encourage similar photosto be assigned the same event.
Local transition, PL, and global pairwise ordering, PG, probabilities encode the learned temporalknowledge between events.
?temporalencourages event assignments toward a learned temporal structure of the scenario.single event.
The event assignment can be viewedas a latent variable for each photo.
We formulatea constrained optimization (depicted in Figure 2)that maximizes the objective function, F , whichconsists of three scoring components: (a) event as-signment scores ?event(Section 3.1), (b) segmen-tation scores ?seg(Section 3.2), and (c) temporalknowledge scores ?temporal(Section 3.3):F = ?event+ ?seg+ ?temporal(1)Decision Variables.
The binary decision variableXi,kindicates that photo piis assigned to event ek.The binary decision variable Zi,j,k,lindicates thatphotos piand pjare assigned to events ekand el,respectively:Zi,j,k,l:= Xi,k?Xj,l(2)3.1 Event Assignment ScoresEvent assignment scores quantify the textual andvisual affinity between a photo piand an eventek.
Affinities are measures of representation sim-ilarity between photos and events.
These scorespush photos displaying a certain event to be as-signed to that event.
For now we assume the tex-tual affinity matrix Ac?
[0, 1]M?Nand the vi-sual affinity matrix Av?
[0, 1]M?Nare given.We describe how we obtain these affinity matri-ces in Section 4.1.
Event assignment scores aredefined as the weighted sum of both textual andvisual affinity:?event=M?i=1N?k=1(?ceAci,k+ ?veAvi,k)Xi,k(3)where Xi,kis a photo-event assignment decisionvariable, and ?ceand ?veare hyperparameters thatbalance the contribution of both affinities.3.2 Segmentation ScoresSegmentation scores quantify textual and visualsimilarities between adjacent photos.
These scoresencourage similar adjacent photos to be assignedto the same event.
We define a similarity score be-tween adjacent photos equal to the weighted sumof their textual (bc) and visual (bv) similarities:?seg=M?1?i=1N?k=1(?csbci+ ?vsbvi)Zi,i+1,k,k(4)where bc, bv?
[0, 1](M?1)?1are vectors of tex-tual and visual similarity scores between adjacentphotos whose ithelement corresponds to the sim-ilarity score between photos piand pi+1, Z is adecision variable defined by Equation 2, and ?csand ?vsare hyperparameters balancing the contri-bution of both types of similarity.
The similarityscores in the b vectors are computed using cosinesimilarity of the feature representations of adja-cent images in both the textual and visual modes.3.3 Temporal Knowledge ScoresTemporal knowledge scores quantify the compati-bilities across different event assignments in termsof their relative ordering.
For now, we assume twotypes of temporal knowledge matrices are given:L ?
[0, 1]N?Nwhich stores local transition prob-abilities for every pair of events, ekand el, andG ?
[0, 1]N?Nwhich stores global pairwise or-dering probabilities for every pair of events, ek1771and el.
We describe how we obtain these temporalknowledge matrices in Section 4.2.
The temporalknowledge score, defined below, encourages theinference model to assign events that are compati-ble with the learned temporal knowledge:?temporal= ?lpM?i=0N?k,l=1Lk,lZi,i+1,k,l(5)+ ?gpM?i=1M?j=iN?k,l=1Gk,lZi,j,k,lwhere Z is a decision variable defined by Equa-tion 2, and ?lpand ?gpare hyperparameters thatbalance the contribution of local and global tem-poral knowledge in the objective.3.4 ConstraintsWe include hard constraints that force each phototo be assigned to exactly one event:N?k=1Xi,k= 1 (6)The number of these constraints is linear in thenumber of photos in an album.
We also includehard constraints to ensure consistencies among bi-nary decision variables X and Z:12(Xi,k+Xj,l)?
Zi,j,k,l?
0 (7)which states that Zi,j,k,lcan be 1 only if both Xi,kand Xj,lare 1.
The number of constraints for seg-mentation scores and local transition probabilitiesis O(MN2) because they model interactions be-tween adjacent photos for all event pairs.
Thenumber of these constraints for global pairwiseordering probabilities is O(M2N2) because theymodel interactions between all pairs of photos inan album for all event pairs.4 Learned Event KnowledgeWe learn base events for each scenario by cluster-ing photos from training albums related to that sce-nario (Figure 3).
As described in Section 3, thesebase events and their temporal knowledge are in-corporated in a joint model for event induction inunseen albums.4.1 Learned Event RepresentationWe perform k-means clustering over captions tocreate a base event model.
We perform text-onlyclustering at first since visual cues are significantly-Arc	de	Triomphe	-At	the	Arc.Louvre	Arc	de	Triomphe-Louvre	Square.
-Pyramid	of	the	Louvre.Invalides-The	Hotel	des		Invalides.
-Cannons	at	les	Invalides.Notre	Dame-Notre	Dame	Cathedral.
-Front	of	Notre	Dame	.Eiffel-I	spy	the	Eiffel	Tower.
-Under	the	Eiffel.Local transition probabilities Global pairwise ordering probabilities PG(e5					e1)	=	.47			eiffel					arc	de	triomphe	PG(e2					e4)	=	.48			louvre						notre	dame	PG(e3					e2)	=	.68			invalides					louvrePL(e1					e5)	=	.24			arc	de	triomphe					eiffel	PL(e4					e3)	=	.01			notre	dame					invalides	PL(e2					e5)	=	.12			louvre					eiffel???
??????
??
?Event	Visual	Center,	eV	Event	Textual	Center,	eCFigure 3: Photos are clustered by their captions.
We cancompute the visual, evk, and caption, eck, centers for all theclusters, as well as the local transition, PL, and global pair-wise ordering, PG, probabilities between these events basedon the sequential patterns they exhibit in the training set.noisier.
Because not all photos have informativecaptions, it is expected that this base clusteringwill form meaningful clusters only over a subsetof the data.
For each scenario, the largest clus-ter corresponds to the ?miscellaneous?
cluster asthe captions in it tend to be relatively uninforma-tive about specific events.
This cluster is excludedwhen computing temporal knowledge probabili-ties (Section 4.2).The visual and textual representations of anevent are computed using the average of the vi-sual and textual features, respectively, of photosassigned to that event.
We compute each textualaffinity Aci,kin the event assignment scores (Equa-tion 3) as the cosine similarity between the textualfeatures of the caption for photo piand the textualrepresentation of event ek.
For textual features,we extract noun and verb unigrams using Turbo-Tagger (Martins et al, 2013) and weigh them bytheir discriminativeness relative to their scenario,P (S|w).
Given scenario S and word w, P (S|w)is defined as the number of albums for the scenariothe word occurs in divided by the total number ofalbums in that scenario.
The visual affinity Avi,kisthe similarity between the visual features of photopiand the visual representation of event ek.
Forvisual features, we use the convolutional featuresfrom the final layer activations of the 16-layer VG-GNet model (Simonyan and Zisserman, 2015).4.2 Temporal KnowledgeLocal transition probabilities.
These probabili-ties, denoted as PL, encode an expected sequenceof events using temporal patterns among adjacent1772Wedding Camping FuneralaisleWalking down the aisletentInside our tentserviceGraveside serviceBride walking down the aisle Setting up the tent The servicevowExchanging vowsfireBuilding the Firepay respectPaying RespectsReading the vows Getting the Fire going RespectReciting vows to each other Around the FiredanceFirst DancesunsetWatching the SunsetgoodbyeSaying GoodbyeEverybody Dancing Sunset from campDancing the Night Away Sunset on the first nightTable 2: Sample learned events and prototypical captionsphotos.
We model PLfor each pair of events as amultinomial distribution,PL(ek?
el) =C(ek?
el)?Nm=1C(ek?
em)(8)where C is the observed counts of that specificevent transition.
This is the likelihood that anevent ekis immediately followed by event el.Global pairwise ordering probabilities.
Theseprobabilities, denoted as PG, encode global struc-tural patterns about events.
We model PGfor eachpair of events as a binomial distribution by com-puting the likelihood that an event occurs beforeanother at any point in an album,PG(ek?
el) =C(ek?
el)C(ek?
el) + C(el?
ek)(9)where C(ek?
el) is the observed counts of ekoccurring anytime before elin all photo albums.These global probabilities model relations amongevents assigned to all photos in the album, notjust events assigned to photos that are adjacent toone another.
This distinction is important becausethese probabilities can encode global patterns be-tween events and are not limited to modeling a se-quential event chain.We use these learned temporal probabilities, PLand PG, in matrices L and G from ?temporal(Equation 5).
These matrices are used to index lo-cal transition probabilities and global pairwise or-dering probabilities for pairs of events when com-puting temporal knowledge scores in the inferencemodel (Section 3.3).4.3 Prototypical CaptionsAfter clustering the photos, the representative lan-guage of the captions in each cluster begins totell a story about each scenario.
The event namesare automatically extracted using the most com-mon content words among captions in the clus-ter.
For each cluster, we also compile prototypicalcaptions by extracting captions whose lemmatizedforms are frequently observed throughout multiplealbums in the scenario.
Sample events and theirprototypical captions from three scenarios are dis-played in Table 2.5 Experimental SetupData split.
For scenarios with more than 1000 al-bums, we use 100 albums for each of the develop-ment and test sets and use the rest for training.
Forscenarios with less than 1000 albums, we use 50albums for each of the development and test sets,and the rest for training.Implementation details.
We optimize our ob-jective function using integer linear programming(Roth and Yih, 2004) with the Gurobi solver (Inc.,2015).
For computational efficiency, temporallyclose sets of consecutive photos are treated as oneunit during the optimization.
We use these units toreduce the number of variables and constraints inthe model from a function of the number of photosto a function of the number of units.
We form theseunits heuristically by merging images agglomera-tively when their timestamps are within a certainrange of the closest image in a unit.
When merg-ing photos, the textual affinity of each unit for aparticular event is the maximum affinity for thatevent among photos in that unit.
The visual affin-ity of each unit is the average of all affinities forthat event among photos in that unit.
The textualand visual similarities of consecutive units are de-fined in terms of the similarities between the twophotos at the units?
boundary.
Temporal informa-tion for events not aligned well with a particu-lar unit should not influence the objective, so weinclude temporal scores only for unit-event pairswhich have both textual and visual event assign-ment scores greater than 0.05.Hyperparameters.
We tune the hyperparametersusing grid search on the development set.
In mod-els where the corresponding objective componentsare included, we set ?ce= 1, ?ve= 1, ?cs= .5,?vs= .15, ?lp= 1, and ?gp=4Q(where Q is the1773Model Wedding Baby Birth Marathon Cooking Funeral Barbecue Indep.
Day Camping Thanksgiving Paris Trip NY Trip Christmask-MEANS 52.7 52.5 53.8 53.0 50.5 50.2 53.2 52.3 51.4 53.1 50.3 51.4NO TEMPORAL 58.6 66.3 62.6 56.5 50.8 51.7 58.0 52.6 54.3 51.7 49.1 50.9FULL MODEL 60.0 66.5 64.5 63.2 53.1 58.6 56.0 55.5 56.1 52.3 48.5 52.4Table 3: Temporal ordering pairwise photo results.
The metric reported is accuracy, the percentage of time the correct photois picked as coming first based on the event assigned to it.
Scenarios with an expected temporal structure are in the left half ofthe table.number of event units).
For k-means clustering,we use 10 random restarts and 40 cluster centersfor the WEDDING, CAMPING, PARIS TRIP, and NY TRIPscenarios.
For all other scenarios, we use 30 clus-ter centers.6 Experimental ResultsWe evaluate the performance of our model onthree tasks.
The first task evaluates the effectof learned temporal knowledge in predicting thecorrect order of photos in an unseen album (Sec-tion 6.1).
The second task evaluates the model?sability to segment albums into logical groupings(Section 6.2).
The third task evaluates the qual-ity of prototypical captions and their use in photoalbum summarization (Section 6.3).6.1 Temporal Ordering of PhotosWe evaluate the model?s ability to capture the tem-poral relationships between events in the scenario.Given two randomly selected photos piand pjfrom an album, the task is to predict which ofthe photos appears earlier in the album using theirevent assignments.
We compare the full modelthat assigns events to photos using ILP (Section 3)with two baselines: k-MEANS , which assigns eventsto photos using k-means clustering over captions(Section 4), and NO TEMPORAL: a variant of thefull model that does not use temporal knowledgescores (?temporalin Equation 1) for optimization.We run each method over a test photo album,in which the events ekand elare assigned tothe photos piand pj, respectively.
We then usethe learned global pairwise ordering probabilities(Section 4.2) to predict which photo appears ear-lier in the album.
We report the accuracy of eachmethod in predicting the order of photos comparedto the actual order of photos in the albums.
Weperform this experiment 50 times for each albumand average the number of correct choices acrossevery album and every trial.Results.
Table 3 reports the results of the fullmodel compared to the baselines.
The resultsshow that temporal knowledge generally helpsin predicting photo ordering.
We observe thatthe full model achieves higher scores for scenar-ios for which we expect would have a sequentialstructure (e.g., WEDDING, BABY BIRTH, MARATHON).Conversely, the full model achieves lower over-all scores in non-sequential scenarios (e.g., PARISTRIP, NEW YORK TRIP).
Qualitatively, we notice in-teresting temporal patterns such as the fact thatduring a marathon, the starting line occurs beforethe medal awards with 92.3% probability, or thatParisian tourists have a 24% chance (?10?
higherthan random chance) of visiting the Eiffel Towerimmediately after the Arc de Triomphe (a highlocal transition probability that correctly impliestheir real world proximity).6.2 Album SegmentationOur model partitions photos in albums into coher-ent events.
The album segmentation evaluationtests if the model recovers the same sequences ofphotos that a human would identify in a photo al-bum as events.Evaluation.
We had an impartial annotator labelwhere they thought events began and ended in 10candidate albums of greater than 100 photos forthree scenarios: WEDDING, FUNERAL, CAMPING.
Weevaluate how well our model can replicate theseboundaries with two metrics.
The first metric isthe F1score of recovering the same boundariesannotated by humans.
The second metric is d,the difference between the number of events seg-mented by the model compared to the annotatedalbums.
We report results for exact event bound-aries as well as relaxed boundaries where the startof an event can be r photos away from the startof an annotated event, where r is the relaxationcoefficient.
For reference, we note that albums inthe wedding scenario were dual annotated and theagreement between annotators is 56.9% for r = 0and 77.5% for r = 2.Results.
Table 4 shows comparison of the the fullILP model with same baselines we described be-fore, k-MEANS and NO TEMPORAL.
The table showsthat the full model generally outperforms the k-MEANS baseline for all three scenarios.In the WEDDING scenario, the F1score for the full1774Model rWEDDING FUNERAL CAMPINGF1d F1d F1dk-MEANS027.1 32.9 27.9 29.6 31.2 46.0NO TEMPORAL 32.0 -5.6 35.9 .9 22.0 -15.6FULL MODEL 37.8 1.3 32.2 4.2 27.5 -10.1k-MEANS240.8 32.9 38.3 29.6 46.2 46.0NO TEMPORAL 49.6 -5.6 57.6 .9 35.4 -15.8FULL MODEL 57.5 1.3 51.6 5.0 51.4 -10.1Table 4: Segmentation results for our full model.
F1scoreshow often our model recovers the same boundaries annotatedby humans.
d is the average difference between the numberof events identified by the model in an album and marked byannotators.
r is the relaxation coefficient.Feature Group Excluded P R F1dFULL MODEL 36.7 42.8 37.8 1.3- Visual Event Affinity 37.7 37.0 35.3 -1.7- Textual Segmentation 37.1 41.5 37.4 .8- Visual Segmentation 35.1 42.1 36.5 1.7- Local Ordering Probs.
36.9 40.3 36.9 .2- Global Ordering Probs.
40.5 25.0 29.5 -5.8Table 5: Ablation study of objective function componentsfor the wedding scenario.
P, R, and F1are the precision,recall and F-measure of recovering the same boundaries an-notated by humans.
d is the average difference between thenumber of events identified by our models and the annotators.model is consistently higher.
The k-MEANS base-line oversamples the number of events in albums,which is indicated by an average d significantlygreater than 0.
For the FUNERAL scenario, theNO TEMPORAL baseline outperforms the full model.We attribute this difference to the smaller data sub-set (see Table 1) making it harder to learn thetemporal relations in the scenario, which makesthe contributions of the local and global temporalprobabilities unexpected.
In the CAMPING scenario,the F1score for the k-MEANS baseline is higherthan that of the full model when r = 0.
At a high-level, CAMPING is a scenario we expect has less of aknown structure compared to other scenarios andmay be harder to segment into its events.Ablation Study.
Table 5 depicts the performanceof ablations of the full model for the wedding sce-nario.
Results show that removing any componentof the objective functions yields lower recall andF1scores than the full model for r = 0.
The ex-ception is removing local ordering probabilities,which yields a higher d. These observations sup-port the hypothesis that all of the components ofthe objective function contribute to segmenting thealbum into subsequences of photos depicting thesame event.
Particularly, we note the degradationwhen removing the global ordering probabilities,indicating that approaches which model only localevent transitions such as hidden Markov modelswould not be suitable for this task.6.3 Photo Album SummarizationThe final experiment evaluates how our learnedprototypical captions can improve downstreamtasks such as summarization and captioning.6.3.1 SummariesThe goal of a good summary is to select the mostsalient pictures of an album.
In our setting, agood summary should have a high coverage of theevents in an album and choose the photos that mostappropriately depict these events.
Given a photobudget b, we choose a subset of photos that aimsfor these goals.
To summarize a test album, werun our model over the entire album.
This willyield h unique events assigned to the photos in thealbum.
For each of these h events, we choose thephoto with the highest event assignment score forthat event (Equation 3) to be in the summary.
Ifh > b, we count the number of photos in the train-ing set assigned to each of the h events and choosethe photos corresponding to the b events with thelargest membership of photos in the training set.
Ifh < b, we complete the summary with b?
h pho-tos from the ?miscellaneous?
event that are spacedevenly throughout the album.
Finally, we replacethe caption of each selected photo with a prototyp-ical caption (Section 4.3) for the assigned event.Baseline.
We evaluate against two baselines.
Thefirst baseline, KTH, involves including a photo inthe summary every k = M/b photos.
The sec-ond baseline, k-MEANS, uses the events assigned tophotos from k-means clustering and then picks bphotos in the same manner as our main model.Evaluation.
We evaluate the summaries pro-duced by each method with a human evaluationusing Amazon Mechanical Turk (AMT).
We usealbums from the test set that contain more than 40photos for the wedding scenario.
For each album,at random, we present two summaries generatedby two algorithms.
AMT workers are instructedto choose the better summary considering both theimages and the captions.
For each comparison oftwo summaries for an album, we aggregate an-swers from three workers by majority voting.
Weset b = 7.
The number of assigned events in analbum, h, varies by album.Results.
As seen in Table 6, the summary fromthe full model is preferred 57.7% of the time com-pared to the KTH baseline.
The summaries gener-ated using the full model perform slightly betterthan the summaries from k-MEANS.
We attribute1775Figure 4: Example summaries from the wedding, Paris trip, and baby birth scenarios.
In cases where the album had less eventsthan b, the additionally chosen photos are outlined in red.
These photos do not have their caption replaced by a prototypicalcaptions and merely fill out the summary.Method Selection RatesFULL MODEL vs. KTH 57.7 42.3FULL MODEL vs. k-MEANS 53.8 47.2k-MEANS vs. KTH 53.8 47.2Table 6: Summarization results.
The selection rates indi-cate the percentage of time the corresponding method in theleft-most column was picked.the superior performance of the full model to thefact that it redistributes photos with noisy captionsthroughout the events, allowing for a larger sampleto estimate visual representations of events, yield-ing more accurate visual affinity measurements tochoose the summarization photos.
As can be seenfrom qualitative examples in Figure 4, the photoschosen and the captions assigned cover key eventsthat would occur during the scenario and describethem in a coherent way.
Additional examples areavailable at https://www.cs.washington.edu/projects/nlp/protoevents.6.3.2 Prototypical CaptionsWe also evaluate the quality of the prototypicalcaptions assigned to every photo in the summaries.For each album, we use the same sets of b pho-tos from the full model in the summarization taskand evaluate the quality of the prototypical cap-tions paired with that group of photos.Evaluation.
We evaluate the quality of captionsassigned to every photo by asking AMT work-ers to rate the captions on three different metrics:grammaticality, relevance to the scenario to whichthe image belongs, and relevance to its paired im-MethodScenario ImageGrammarRelevance RelevanceLSTM 4.90 2.85 3.74FULL MODEL 4.55 3.66 4.08RAW CAPTIONS 4.10 4.36 4.28Table 7: Captioning results.
We evaluate the caption qual-ity of the prototypical captions of the full model, those gen-erated by an LSTM trained on the raw captions, and originalcaptions.
Captions were evaluated on 3 metrics: grammaticalcorrectness, how relevant they were to the scenario, and howrelevant they were to their assigned image.age.
Five AMT workers rate each group of b pho-tos on a five point Likert scale for each metric.We compare the prototypical captions for everyphoto in the summary with captions generated byan LSTM model3trained on every photo-captionpair in the training set for a scenario.
We also com-pare with the original raw captions for each imagein the summary.
Because we chose photos withthe highest event assignment scores (Equation 3)to be in the summary, the raw captions for thisevaluation are cleaner and more descriptive thanmost captions in the dataset.Results.
Our model outperforms the LSTM-generated captions in the image relevance andgrammaticality scores, but did worse in scenario3We use a single-layer encoder-decoder LSTM.
The cellstate and the input embedding dimensions are 256.
Visual in-puts are the final layer convolutional features of the VGG-16model and are fine-tuned during training.
We use RMSpropto train the network with a base learning rate of .0001 and30% dropout.
We train the model for 45 epochs on a singleNVIDIA Titan X GPU with mini batch size 100.
To decode,we use beam search with beam size 5.1776relevance.
We attribute this result to LSTMcaptions having little caption variation becausethe model learns frequency statistics without anyknowledge of latent events.
Almost all LSTM cap-tions mention the words bride, wedding, or groom,yielding a very high scenario score for the caption,even if that caption is grammatically incorrect orirrelevant to the image.
As expected the raw cap-tions have high relevance to the original image,and they are grammatical, but can be less relevantto the corresponding scenario.7 Related WorkPrevious studies have explored unsupervised in-duction of salient content structure in newswiretexts (Barzilay and Lee, 2004), temporal graphrepresentations (Bramsen et al, 2006), and story-line extraction and event summarization (Xu et al,2013).
Another line of research finds the commonevent structure from children?s stories (McIntyreand Lapata, 2009), where the learned plot struc-ture is used to stochastically generate new stories(Goyal et al, 2010; Goyal et al, 2013).
Our worksimilarly aims to learn the typical temporal pat-terns and compositional elements that define com-mon scenarios, but with multimodal integration.Compared to studies that learn narrativeschemas from natural language (Pichotta andMooney, 2014; Jans et al, 2012; Chambers andJurafsky, 2009; Chambers, 2013; Cassidy et al,2014), or compile script knowledge from crowd-sourcing (Regneri et al, 2010), our work exploresa new source of knowledge that allows groundedevent learning with temporal dimensions, result-ing in a new dataset of scenario types that are notnaturally accessible from newswire or literature.While recent studies have explored videos andphoto streams as a source of discovering com-plex events and learning their sequential patterns(Kim and Xing, 2014; Kim and Xing, 2013; Tanget al, 2012; Tschiatschek et al, 2014), their fo-cus was mostly on the visual modality.
Zhanget al (2015) explored multimodal information ex-traction focusing specifically on identifying videoclips that referred to the same event in televisionnews.
This contrasts to the goal of our study thataims to learn the temporal structure by which com-mon scenarios unfold.Integrating language and vision has attracted in-creasing attention in recent years across diversetasks such as image captioning (Karpathy and Fei-Fei, 2015; Vinyals et al, 2015; Fang et al, 2015;Xu et al, 2015; Chen et al, 2015), cross modalsemantic modeling (Lazaridou et al, 2015), infor-mation extraction (Morency et al, 2011; Rosaset al, 2013; Zhang et al, 2015; Izadinia et al,2015), common-sense knowledge (Vedantam etal., 2015; Bagherinezhad et al, 2016), and visualstorytelling (Huang et al, 2016).
Our work is sim-ilar to both common sense knowledge learning andvisual story completion.
Our model learns com-monsense knowledge on the hierarchical and tem-poral event structure from scenario-specific multi-modal photo albums, which can be viewed as vi-sual stories about common life events.Recent work focused on photo album summa-rization using visual (Sadeghi et al, 2015) andmultimodal representations (Sinha et al, 2011).Our work identifies the nature of common eventsin scenarios and learns their timelines and charac-teristic forms.8 ConclusionWe introduce a novel exploration to learn script-like knowledge from photo albums.
We modelstochastic event structure to learn both the eventrepresentations (textual and visual) and the tempo-ral relations among those events.
Our event induc-tion method incorporates learned knowledge aboutevents, partitions photo albums into segments, andassigns events to those segments.
We show thesignificance of our model in learning and usinglearned knowledge for photo ordering, album seg-mentation, and summarization.
Finally, we pro-vide a dataset depicting 12 scenarios with ?1.5 Mimages for future research.
Future directions couldinclude exploring nuances in the type of tempo-ral knowledge that can be learned across differentscenarios.AcknowledgementsWe thank the anonymous reviewers for many in-sightful comments and members of UW NLP forfeedback and support.
The work is in part sup-ported by NSF grants IIS-1408287, IIS-1524371,IIS-1616112, DARPA under the CwC programthrough the ARO (W911NF-15-1-0543), the AllenInstitute for AI (66-9175), the Allen DistinguishedInvestigator Award, and gifts by Google and Face-book.1777ReferencesHessam Bagherinezhad, Hannaneh Hajishirzi, YejinChoi, and Ali Farhadi.
2016.
Are elephants biggerthan butterflies?
reasoning about sizes of objects.
InProceedings of the Conference in Artificial Intelli-gence (AAAI).Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In NAACL-HLT.Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,and Regina Barzilay.
2006.
Inducing temporalgraphs.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, pages 189?198.
Association for ComputationalLinguistics.Taylor Cassidy, Bill McDowell, Nathanael Chambers,and Steven Bethard.
2014.
An annotation frame-work for dense event ordering.
In ACL.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In ACL.Nathanael Chambers and Dan Jurafsky.
2009.
Un-supervised learning of narrative schemas and theirparticipants.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2-Volume2, pages 602?610.
Association for ComputationalLinguistics.Nathanael Chambers.
2013.
Event schema inductionwith a probabilistic entity-driven model.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing.Jianfu Chen, Polina Kuznetsova, David S Warren, andYejin Choi.
2015.
D?eja image-captions: A corpusof expressive descriptions in repetition.
In NAACL-HLT.Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K.Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao,Xiaodong He, Margaret Mitchell, John C. Platt,C.
Lawrence Zitnick, and Geoffrey Zweig.
2015.From captions to visual concepts and back.
In TheIEEE Conference on Computer Vision and PatternRecognition (CVPR), June.Amit Goyal, Ellen Riloff, and Hal Daum?e III.
2010.Automatically producing plot unit representationsfor narrative text.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 77?86.
Association for Computa-tional Linguistics.Amit Goyal, Ellen Riloff, et al 2013.
A computationalmodel for plot units.
Computational Intelligence,29(3):466?488.Ting-Hao Huang, Francis Ferraro, NasrinMostafazadeh, Ishan Misra, Aishwarya Agrawal,Jacob Devlin, Ross Girshick, Xiaodong He, Push-meet Kohli, Dhruv Batra, C. Lawrence Zitnick,Devi Parikh, Lucy Vanderwende, Michel Galley,and Margaret Mitchell.
2016.
Visual storytelling.In NAACL.Gurobi Optimization Inc. 2015.
Gurobi optimizer ref-erence manual.Hamid Izadinia, Fereshteh Sadeghi, Santosh K Div-vala, Hannaneh Hajishirzi, Yejin Choi, and AliFarhadi.
2015.
Segment-phrase table for seman-tic segmentation, visual entailment and paraphras-ing.
In Proceedings of the IEEE International Con-ference on Computer Vision, pages 10?18.Bram Jans, Steven Bethard, Ivan Vuli, and Marie-Francine Moens.
2012.
Skip n-grams and rankingfunctions for predicting script events.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 336?344.Andrej Karpathy and Li Fei-Fei.
2015.
Deep visual-semantic alignments for generating image descrip-tions.
In The IEEE Conference on Computer Visionand Pattern Recognition (CVPR), June.Gunhee Kim and Eric P Xing.
2013.
Jointly align-ing and segmenting multiple web photo streams forthe inference of collective photo storylines.
In Com-puter Vision and Pattern Recognition (CVPR), 2013IEEE Conference on, pages 620?627.
IEEE.Gunhee Kim and Eric Xing.
2014.
Reconstructing sto-ryline graphs for image recommendation from webcommunity photos.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition, pages 3882?3889.Angeliki Lazaridou, Nghia The Pham, and Marco Ba-roni.
2015.
Combining language and vision with amultimodal skip-gram model.
In Proceedings of the2015 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 153?163, Den-ver, Colorado, May?June.
Association for Compu-tational Linguistics.Andr?e FT Martins, Miguel B Almeida, and Noah ASmith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In ACL.Neil McIntyre and Mirella Lapata.
2009.
Learning totell tales: A data-driven approach to story genera-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 1-Volume 1, pages217?225.
Association for Computational Linguis-tics.Louis-Philippe Morency, Rada Mihalcea, and PayalDoshi.
2011.
Towards multimodal sentiment analy-sis: Harvesting opinions from the web.
In Proceed-ings of the 13th international conference on multi-modal interfaces, pages 169?176.
ACM.1778Karl Pichotta and Raymond J Mooney.
2014.
Statis-tical script learning with multi-argument events.
InEACL, volume 14, pages 220?229.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning script knowledge with webexperiments.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 979?988.
Association for Computa-tional Linguistics.Ver?onica P?erez Rosas, Rada Mihalcea, and Louis-Philippe Morency.
2013.
Multimodal sentimentanalysis of spanish online videos.
IEEE IntelligentSystems, (3):38?45.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
Technical report, DTIC Document.Fereshteh Sadeghi, J Rafael Tena, Ali Farhadi, andLeonid Sigal.
2015.
Learning to select and ordervacation photographs.
In Applications of ComputerVision (WACV), 2015 IEEE Winter Conference on,pages 510?517.
IEEE.Roger C Schank and Robert P Abelson.
1975.
Scripts,plans, and knowledge.
Yale University.K.
Simonyan and A. Zisserman.
2015.
Very deep con-volutional networks for large-scale image recogni-tion.
In Proceedings of the International Conferenceon Learning Representations (ICLR).Pinaki Sinha, Sharad Mehrotra, and Ramesh Jain.2011.
Summarization of personal photologs usingmultidimensional content and context.
In Proceed-ings of the 1st ACM International Conference onMultimedia Retrieval, page 4.
ACM.Kevin Tang, Li Fei-Fei, and Daphne Koller.
2012.Learning latent temporal structure for complex eventdetection.
In Computer Vision and Pattern Recog-nition (CVPR), 2012 IEEE Conference on, pages1250?1257.
IEEE.Sebastian Tschiatschek, Rishabh K Iyer, Haochen Wei,and Jeff A Bilmes.
2014.
Learning mixtures of sub-modular functions for image collection summariza-tion.
In Advances in Neural Information ProcessingSystems, pages 1413?1421.R.
Vedantam, X. Lin, T. Batra, C. L. Zitnick, andD.
Parikh.
2015.
Learning common sense throughvisual abstraction.
In Proceedings of the Interna-tional Conference in Computer Vision (ICCV).Oriol Vinyals, Alexander Toshev, Samy Bengio, andDumitru Erhan.
2015.
Show and tell: A neural im-age caption generator.
In The IEEE Conference onComputer Vision and Pattern Recognition (CVPR),June.Shize Xu, Shanshan Wang, and Yan Zhang.
2013.Summarizing complex events: a cross-modal so-lution of storylines extraction and reconstruction.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages1281?1291.Kelvin Xu, Jimmy Ba, Ryan Kiros, KyunghyunCho, Aaron C. Courville, Ruslan Salakhutdinov,Richard S. Zemel, and Yoshua Bengio.
2015.Show, attend and tell: Neural image caption gen-eration with visual attention.
In Proceedings of The32nd International Conference on Machine Learn-ing, pages 2048?2057.Tongtao Zhang, Hongzhi Li, Heng Ji, and Shih-FuChang.
2015.
Cross-document event coreferenceresolution based on cross-media features.
In Proc.Conference on Empirical Methods in Natural Lan-guage Processing.1779
