Towards an implementable dependency grammarTimo JK rv inen  and Pas i  Tapana inenResearch Unit for Multil ingual Language TechnologyP.O.
Box 4, FIN-00014 University of Helsinki, FinlandIiI!iIIIIIIi| !i'AbstractSyntactic models should be descriptively ade-quate and parsable.
A syntactic description isautonomous in the sense that it has certain ex-plicit formal properties.
Such a description re-lates to the semantic interpretation of the sen-tences, and to the surface text.
As the formal-ism is implemented in a broad-coverage syntac-tic parser, we concentrate on issues that mustbe resolved by any practical system that usessuch models.
The correspondence between thestructure and linear order is discussed.1 IntroductionThe aim of this paper is to define a dependencygrammar framework which is both linguisticallymotivated and computationally parsable.A linguistically adequate grammar is the pri-mary target because if we fail to define a de-scriptive grammar, its application is less use-ful for any linguistically motivated purposes.
Infact, our understanding of the potential bene-fits of the linguistic means can increase only ifour practical solutions tand on an adequate de-scriptive basis.Traditionally, grammatical models have beenconstructed by linguists without any consider-ation for computational pplication, and later,by computationally oriented scientists who havefirst taken a parsable mathematical model andthen forced the linguistic description into themodel which has usually been too weak to de-scribe what a linguist would desire.Our approach is somewhere between thesetwo extremes.
While we define the grammarstrictly in linguistic terms, we simultaneouslytest it in the parsing framework.
What is excep-tional here is that the parsing framework is notrestricted by an arbitrary mathematical modelsuch as a context-free phrase structure gram-mar.
This leads us to a situation where theparsing problem is extremely hard in the gen-eral, theoretical case, but fortunately parsablein practise.
Our result shows that, while in gen-eral we have an NP-hard parsing problem, thereis a specific solution for the given grammar thatcan be run quickly.
Currently, the speed of theparsing system I is several hundred words persecond.In short, the grammar should be empiricallymotivated.
We have all the reason to believethat if a linguistic analysis rests on a solid de-scriptive basis, analysis tools based on the the-ory would be more useful for practical purposes.We are studying the possibilities of using com-putational implementation as a developing andtesting environment for a grammatical formal-ism.
We refer to the computational implemen-tation of a grammar as a parsing grammar.1.1 AdequacyA primary requirement for a parsing rammar isthat it is descriptively adequate.
Extreme dis-tortion results if the mathematical propertiesof the chosen model restrict the data.
How-ever, this concern is not often voiced in the dis-cussion.
For example, McCawley (1982, p. 92)notes that such a basic assumption concerninglinguistic structures that "strings are more basicthan trees and that trees are available only as aside product of derivations that operate in termsof strings" was attributable to the historical ac-cident that early transformational grammariansknew some automata theory but no graph the-Ory.
"One reason for computationally oriented syn-tacticians to favour restricted formalisms i thatthey are easier to implement.
Those who beganl Demo:  http://www.conezor.fi/analysers.htmlIiIII!iIIII!iIIIIto use dependency models in the 1960's largelyignored descriptive adequacy in order to developmodels which were mathematically simple and,as a consequence, for which effective parsing al-gorithms could be presented.
These inadequa-cies had to be remedied from the beginning,which resulted in ad hoc theories or engineeringsolutions 2 without any motivation in the theory.There have been some serious efforts to re-solve these problems.
Hudson (1989), for exam-ple, has attempted to construct a parser thatwould reflecs the claims of the theory (WordGrammar) as closely as possible.
However, itseems that even linguistically ambitious depen-dency theories, such as Hudson's Word Gram-mar, contain some assumptions which are at-tributable to certain mathematical properties ofan established formalism rather than imposedby the linguistic data 3.
These kinds of unwar-ranted assumptions tend to focus the discus-sion on phenomena which are rather marginal,if a complete description of a language is con-cerned.
No wonder that comprehensive d scrip-tions, such as Quirk et al (1985), have usuallybeen non-formal.1.2 The  European s t ruc tura l i s tt rad i t ionWe argue for a syntactic description that isbased on dependency rather than constituency,and we fully agree with Haji~ovA (1993, p. 1)that "making use of the presystemic insights ofclassical European linguistics, it is then possi-ble that constituents may be dispensed with asbasic elements of (the characterization of) thesentence structure."
However, we disagree withthe notion of "presystemic" if it is used to implythat earlier work is obsolete.
From a descriptivepoint of view, it is crucial to look at the datathat was covered by earlier non-formal gram-marians.?
As far as syntactic theory is concerned, thereis no need to reinvent the wheel.
Our de-scription has its basis in the so-called "classi-cal model" based on the work of the Frenchlinguist Lucien Tesni~re.
His structural modelshould be capable of describing any occurring2See discussion of an earlier engineering art in apply-ing a dependency grammar in Kettunen (1994).3For instance, the notion of adjacency was redefinedin WG, but was still unsuitable for "free ~ word orderlanguages.2natural language.
His main work, (1959) ad-dresses a large amount of material from typo-logically different languages.
It is indicative ofTesni~re's empirical orientation that there areexamples from some 60 languages, though hismethod was not empirical in the sense that hewould have used external data inductively.
AsHeringer (1996) points out, Tesni~re used datamerely as an expository device.
However, inorder to achieve formal rigour he developed amodel of syntactic description, which obviouslystems from the non-formal tradition developedsince antiquity but without compromising thedescriptive needs.
We give a brief historicaloverview of the formal properties inherent inTesni~re's theory in Section 5 before we proceedto the implementational issues in Section 6.1.3 The  surface syntact i c  approachWe aim at a theoretical framework where wehave a dependency theory that is both descrip-tively adequate and formally explicit.
The lat-ter is required by the broad-coverage parsinggrammar for English that we have implemented.We maintain the parallelism between the syn-tactic structure and the semantic structure inour design of the syntactic description: when achoice between alternative syntactic construc-tions in a specific context should be made, thesemantically motivated alternative is selected 4.Although semantics determines what kind ofstructure a certain sentence should have., fromthe practical point, of view, we have a completelydifferent problem: how to resolve the syntacticstructure in a given context.
Sometimes, thelatter problem leads us back to redefine the syn-tactic structure so that it can be detected inthe sentence s .
Note, however, that this redef-inition is now made on a linguistic basis.
Inorder to achieve parsability, the surface descrip-41n such sentence as "I asked John to go home", thenoun before the infinitive clause is analysed as the (se-mantic) subject of the infinitive rather than as a com-plement of the governing verb.SFor instance, detecting the distinct roles of the to-infinitive clause in the functional roles of the purposeor reason is usually difficult (e.g.
Quirk et al (1985,p.
564): "Whg did he do itf; purpose: "To relieve hisanger" and reason: "Because he was angry").
In suchsentence as UA man came to the party to have a goodtime", the interpretation of the infinitive clause dependson the interaction of the contextual and lexical semanticsrather than a structural distinction.iIiIIIII1I!iIiillm !tion should not contain elements which can notbe selected by using contextual information.
Itis important that the redefinition should not bemade because an arbitrary mathematical modeldenies e.g.
crossing dependencies between thesyntactic elements.2 Const i tuency  vs. dependencyA central idea in American structuralism wasto develop rigorous mechanical procedures,i.e.
"discovery procedures", which were assumedto decrease the grammarians' own, subjectiveassessment in the induction of the grammars.This practice was culminated in Harris (1960,p.
5), who claimed that "the main researchof descriptive linguistics, and the only rela-tion which will be accepted as relevant in thepresent survey, is the distribution or arrange-ment within the flow of speech of some parts orfeatures relative to others.
"The crucial descriptive problem for a distri-butional grammar (i.e.
phrase-structure gram-mar) is the existence of non-contiguous ele-ments.
The descriptive praxis of some earlier ICtheoricians allows discontiguous constituents.For example, already Wells (1947) discussed theproblem at length and defined a restriction fordiscontiguous constituents 6.
Wells' restrictionimplies that a discontiguous sequence can bea constituent only if it appears as a contigu-ous sequence in another context.
This meansthat Wells' characterisation f a constituent de-fines an element which is broadly equivalent tothe notion of bunch in Tesni~re's (1959} the-ory.
Consequently, these two types of grammarsare capable of describing the equivalent syntac-tic phenomena and share the assumption thata syntactic structure is compatible with its se-mantic interpretation.
However, the extendedconstituent grammar thus no longer provides arigorous distributional basis for a description,and its formal properties are unknown.We can conclude our argument by statingthat the reason to reject constitutional gram-mars is that the formal properties for descrip-6Wells (1947): "A discontinuous sequence is a con-st ituent i l  in some environment the corresponding contin-uous sequence occurs as a const i tuent in a construct ionsemantical ly harmonious with the construct ions in whichthe given discontinuous sequence occurs."
Further, Wellsnotes that "The phrase semantical ly harmonious is leftundefined, and will merely be elucidated by examples.
"3tively adequate constitutional grammars are notknown.
In the remaining sections, we show thata descriptively adequate dependency model canbe constructed so that it is formally explicit andparsable.3 Para l le l i sm between the  syntact i cand  semant ic  s t ruc turesObviously, distributional descriptions that donot contribute to their semantic analysis canbe given to linguistic strings.
Nevertheless,the minimal descriptive requirement should bethat a syntactic description is compatible withthe semantic structure.
The question whicharises is that if the correspondence b tweensyntactic and semantic structures exists, whyshould these linguistic levels be separated.
Forexample, Sgall (1992, p. 278) has questionedthe necessity of the syntactic level altogether.His main argument for dispensing with thewhole surface syntactic level is that there areno strictly synonymous syntactic onstructions,and he therefore suggests that the surface wordorder belongs more properly to the level of mor-phemics.
This issue is rather complicated.
Weagree that surface word order does not belongto syntactic structure, but for different reasons.In contradistinction to Sgall's claim, Mel'~uk(1987, p. 33) has provided some evidence wherethe morphological marker appears either?
in thehead or the dependent element in different lan-guages, as in the Russian "kniga professor+a"(professor's book) and its Hungarian equivalent'~professzor kSnyv?e'.
Consequently, Mel'~uk(1987, p. 108) distinguishes the morphologicaldependency as a distinct type of dependency.Thus morphology does not determine the syn-tactic dependency, as Tesni~re (1959, Ch.
15)also argues.For Tesni~re (1959, Ch.
20:17) meaning(Fr.
sens) and structure are, in principle, inde-pendent.
This is backed by the intuition thatone recognises the existence of the linguisticstructures which are semantically absurd, as il-lustrated by the structural similarity betweenthe nonsensical sentence "Le silence vertebralindispose la voie licite" and the meaningful sen-tence "Le signal vert indique la voie libre".The independence of syntactic and seman-tic levels is crucial for understanding Tesni~re'sthesis that the syntactic structure follows fromIIIIIIiIIIIIIIlIIthe semantic structure, but not vice versa.
Thismeans that whenever there is a syntactic rela-tion, there is a semantic relation (e.g.
comple-mentation or determination) going in the op-posite direction.
In this view, the syntactichead requires emantic omplementation fromits dependents.
Only because the syntactic andsemantic structures belong to different levelsis there no interdependency or mutual depen-dency, though the issue is sometimes raised inthe literature.There is no full correspondence b tween thesyntactic and semantic structures because somesemantic relations are not marked in the func-tional structure.
In Tesni~re (1959, p. 85), forexample, there are anaphoric relations, seman-tic relations without correspondent syntactic re-lations.4 Sur face representat ion  andsyntact i c  s t ruc ture4.1 The nucleus as a syntact i c  primitiveThe dependency syntactic models are inher-ently more "word oriented" than constituent-structure models, which use abstract phrase cat-egories.
The notion of word, understood as anorthographic unit in languages similar to En-glish, is not the correct choice as a syntacticprimitive.
However, many dependency theo-ries assume that the orthographic words directlycorrespond r to syntactic primitives (nodes inthe trees).
Although the correspondence couldbe very close in languages like English, there arelanguages where the word-like units are muchlonger (i.e.
incorporating languages).TesniSre observed that because the syntacticconnexion implies a parallel semantic onnex-ion, each node has to contain a syntactic and asemantic entre.
The node element, or nucleus,is the genuine syntactic primitive.
There is noone-to-one correspondence b tween uclei andorthographic words, but the nucleus consists ofone or more, possibly discontiguous, words orparts of words.
The segmentation belongs tothe linearisation, which obeys language-specificrules.
Tesni~re (1959, Ch 23:17) argued thatthe notion word, a linear unit in a speech-chain,does not belong to syntactic description at all.A word is nothing but a segment in the speechchain (1959, Ch 10:3).7See Kunze (1975, p. 491) and Hudson (1991).The basic element in syntactic description isthe nucleus.
It corresponds to a node in a de-pendency tree.
When the sentence is repre-sented as a dependency tree, the main node con-tains the whole verb chain.There are at least two reasons why the con-cept of the nucleus is needed.
In the first place,there are no cross-linguistically valid criteriato determine the head in, say, a prepositionalphrase.
One may decide, arbitrarily, that ei-ther the preposition or the noun is the head ofthe construction.
Second, because the nucleusis also the basic semantic unit, it is the minimalunit in a lexicographical description.4.2  L inear i sa t ionTesni~re makes a distinction between the linearorder, which is a one-dimensional property ofthe physical manifestations of the language, andthe structural order, which is two-dimensional.According to his conception, constructing thestructural description is converting the linearorder into the structural order.
Restricting him-self to syntactic description, Tesni~re does notformalise this conversion though he gives twomain principles: (1) usually dependents eitherimmediately follow or precede their heads (pro-jectivity) and when they do not, (2) additionaldevices uch as morphological greement can in-dicate the connexion.Although Tesni~re:s distinction between thelinear and structural order corresponds to someextent with the distinction between the linearprecedence (LP) and the immediate dominance,there is a crucial difference in emphasis with re-spect to those modern syntactic theories, suchas GPSG, that have distinct ID and LP compo-nents.
Tesni~re xcludes word order phenom-ena from his structural syntax and thereforedoes not formalise the LP component at all.Tesni~re's solution is adequate, considering thatin many languages word order is considerablyfree.
This kind of "free" word order means thatthe alternations in the word order do not neces-sarily change the meaning of the sentence, andtherefore the structural description implies sev-eral linear sequences of the words.
This doesnot mean that there are no restrictions in thelinear word order but these restrictions do notemerge in the structural analysis.In fact, Tesni~re assumes that a restrictionthat is later formalised as an adjacency princi-v!!!!
'iple characterizes the neutral word order when hesays that there are no syntactic reasons for vio-lating adjacency in any language, but the prin-ciple can be violated, as he says, for stylisticreasons or to save the metric structure in poet-ics.
If we replace the stylistic reasons with themore broader notion which comprises the dis-course functions, his analysis seems quite con-sistent with our view.
Rather than seeing thatthere are syntactic restrictions concerning wordorder, one should think that some languages dueto their rich morphology have more freedom inusing word order to express different discoursefunctions.
Thus, linearisation rules are not for-mal restrictions, but language-specific and func-tional.There is no need for constituents.
Tesni~re'stheory has two mechanisms to refer to linearisa-tion.
First, there are static functional categorieswith dynamic potential to change the initial cat-egory.
Thus, it is plausible to separately definethe combinatorial nd linearisation properties ofeach category.
Second, the categories are hierar-chical so that, for instance, a verb in a sentencegoverns a noun, an adverb or an adjective.
Thelexical properties, inherent to each lexical ele-ment, determine what the governing elementsare and what elements are governed.There are no simple rules or principles forlinearisation.
Consider, for example, the treat-ment of adjectives in English.
The basic rule isthat attributive adjectives precede their heads.However, there are notable exceptions, includ-ing the postmodified adjectives s, which followtheir heads, and some lexical exceptions 9, whichusually or always are postmodifying.5 Histor ical  formulat ionsIn this section, the early formalisations ofthe dependency grammar and their relation toTesni~re's theory are discussed.
The depen-dency notion was a target of extensive formalstudies already in the first half of the 1960's l?.8Example: "It is a phenomenon consistent with ..."9Example: "president el.ect"l?A considerable number of the earlier studies werelisted by Marcus (1967, p. 263), who also claimed that"Tesni~re was one ol the first who used (dependency)graphs in syntax.
His ideas n,ere repeated, eveloped andprecised by Y. Lecer\] ~ P. l"hm (1960), L. Hirschbe~ andL Lynch, particularly by studying syntactic projectivityand linguistic subtrees.
"5.1 Gai fman 's  formulat ionThe classical studies of the formal propertiesof dependency grammar are Gaifman (1965)and Hays (1964) 11, which demonstrate that de-pendency grammar of the given type is weaklyequivalent o the class of context-free phrasestructure grammars.
The formalisation of de-pendency grammars is given in Gaifman (1965,p.
305): For each category X, there will be a fi-nite number of rules of the type X (YI, Y2"'" Y!
*Fi+t-.
"Yn), which means that YI"" "Y~ can de-pend on X in this given order, where X is tooccupy the position of , .Hays, referring to Gaifman's formulationabove, too strongly claims that "\[d\]ependencytheory is weakly omnipotent to IC theory.
Theproof is due to Gaifman, and is too lengthy topresent here.
The consequence of Gaifman'stheorem is that the class of sets of utterances \[...\]is Chomsky's class of context-free languages.
"This claim was later taken as granted to ap-ply to any dependency grammar, and the first,often cited, attestation of this apparently falseclaim appeared in Robinson (1970).
She pre-sented four axioms of the theory and claimedthey were advocated by Tesni~re and formalisedby Hays and Gaifman.Thus, the over-all result of the Gaifman-Hays proof was that there is a weak equiv-alence of dependency theory and context-freephrase-structure grammars.
This weak equiva-lence means only that both grammars charac-S lTesni~re is not mentioned in these papers.
Gaif-man's paper describes the results "... obtained whilethe author was a consultant for the RAND Corporationin the summer of 1960.
~ Whereas phrase-structure sys-tems were defined by referring to Chomsky's SyntacticStructures, the corresponding definition for the depen-dency systems reads as follows: "By dependency s stemwe mean a system, containing a finite number of rules, bywhich dependency analysis for certain language isdone,as described incertain RAND publications (Hays: Febru-ary 1960; Hays and Ziehe, April 1960).
~Speaking of thedependency theory, Hays (1960) refers to the Soviet workon machine translation using the dependency theory ofKulagina et al In Hays (1964), the only linguistic refer-ence is to the 1961 edition of Hjelmslev's Prolegomena:"Some of Hjelmslev's empirical principles are closely re-lated to the insight behind dependency theory, but em-pirical dependency in his sense cannot be identified withabstract dependency in the sense of the present paper,since he explicitly differentiates dependencies 1ram otherkinds at relations, whereas the present heorlt intends tobe complete, i.e.
to account lor all relations among unitsot utterances.
"IIIII!IIIIIIIIIterize the same sets of strings.
Unfortunately,this formulation had little to do with TesniSre'sdependency theory, but as this result met the re-quirements of a characterisation theory, interestin the formal properties of dependency grammardiminished considerably.5.2 Linguistic hypothesesTesni~re's Hypothesis, as Marcus (1967) calls it,assumes that each element has exactly one head.Marcus also formulates a stronger hypothesis,the Projectivity hypothesis, which connects thelinear order of the elements of a sentence to thestructural order of the sentence.
The hypoth-esis is applied in the following formulation: letX "- a la  2 .
.
.a i  .
.
.
an  be a sentence, where ai andaj are terms in the sentence.
If the term ai issubordinate to the term aj, and there is an in-dex k which holds min(i, j )  < k < max(i, j),then the term ak is subordinate to the term a j .This is the formal definition of projectivity,also known as adjacency or planarity.
The intu-itive content of adjacency is that modifiers areplaced adjacent to their heads.
The intuitivecontent behind this comes from Behaghel's FirstLaw 12 (Siewierska, 1988, p. 143).The adjacency principle is applicable only ifthe linear order of strings is concerned.
How-ever, the target of Tesni~re's yntax is struc-tural description and, in fact, Tesnibre discusseslinear order, a property attributable to strings,only to exclude linearisation from his concep-tion of syntax.
This means that a formalisa-tion which characterises sets of strings can noteven be a partial formalisation of Tesni~re's the-ory because his syntax is not concerned withstrings, but structures.
Recently, Neuhaus andBrSker (1997) have studied some formal prop-erties of dependency grammar, observing thatGaifman's conception is not compatible itherwith Tesnibre's original formulation or with the"current" variants of DG.There are several equivalent formalisationsfor this intuition.
In effect they say that in asyntactic tree, where words are printed in lin-ear order, the arcs between the words must notcross.
For example, in our work, as the arc be-tween the node "what" and the node "do" inl~ "The most important law is that what belongs to*gether mentally (semantically) is placed close togethersyntactically.""
---- main:LIKEWOULD DO~VHAT YOU ME TOFigure 1: Non-projective dependency treeFigure I violates the principle, the constructionis non-projective.5.3 Formal properties of  aTesn i~re- type DGOur current work argues for a dependencygrammar that is conformant with the originalformulation in Tesni~re (1959) and contains thefollowing axioms:?
The primitive element of syntactic descrip-tion is a nucleus.?
Syntactic structure consists of connexionsbetween uclei.?
Connexion (Tesnikre, 1959, Ch.
1:11) is abinary functional relation between a supe-rior term (regent.)
and inferior term (depen-dent).?
Each nucleus is a node in the syntactic treeand it has exactly one regent (Tesnibre,1959, Ch.
3:1).?
A regent, which has zero or more depen-dents, represents the whole subtree.?
The uppermost regent is the central nodeof the sentence.These axioms define a structure graph whichis acyclic and directed, i.e.
the result is a tree.These strong empirical claims restrict the the-ory.
For example, multiple dependencies and allkinds of cyclic dependencies, including mutualdependency, are excluded.
In addition, therecan be no isolated nodes.However, it is not required that the structurebe projective, a property usually required inmany formalised dependency theories that doII6!
'iSI iii | iIiilnot take into account he empirical fact thatnon-projective constructions occur in naturallanguages.6 The  Funct iona l  DependencyGrammarOur parsing system, called the Functional De-pendency Grammar (FDG), contains the follow-ing parts:?
the lexicon,?
the CG-2 morphological disambiguation(Voutilainen, 1995; Tapanainen, 1996), and?
the Functional Dependency Grammar (Ta-panainen and J~rvinen, 1997; J~irvinen andTapanainen, 1997).6.1 On the formal ism and outputIt has been necessary to develop an expressiveformalism to represent the linguistic rules thatbuild up the dependency structure.
The de-scriptive formalism developed by Tapanainencan be used to write effective recognition gram-mars and has been used to write a comprehen-sive parsing grammar of English.When doing fully automatic parsing it isnecessary to address word-order phenomena.Therefore, it is necessary that the grammar for-realism be capable of referring simultaneouslyboth to syntactic order and linear order.
Obvi-ously, this feature is an extension of Tesni~re'stheory, which does not formalise linearisation.Our solution, to preserve the linear order whilepresenting the structural order requires thatfunctional information is no longer coded to thecanonical order of the dependents Is.In the FDG output, the functional informa-tion is represented explicitly using arcs with la-bels of syntactic functions.
Currently, some 30syntactic functions are applied.To obtain a closer correspondence with thesemantic structure, the nucleus format corre-sponding to Tesni~re's stemmas is applied.
ItlSCompare this solution with the Prague approach,which uses horizontal ordering as a formal device to ex-press the topic-focus articulation at their tectogrammat-ical level.
The mapping from the tectogrammatical levelto the linear order requires eparate rules, called 8hallowrules (Petkevi~, 1987).
Before such a description exists,one can not make predictions concerning the complexityof the grammar.WAS RUNNINGDOG IN HOUSETHE THEFigure 2: "The dog was running in the house"~ff l l \ [ l  i l l  .
"DID RUNDOG IN HOUSEaep-THE THEFigure 3: "Did the dog run in the house"is useful for many practical purposes.
Con-sider, for example, collecting arguments for agiven verb "RUN".
Having the analysis uch asthose illustrated in Figure 2, it is easy to ex-cerpt all sentences where the governing node isverbal having a main element hat has "run"as the base form, e.g.
ran, "was running" (Fig-ure 2), "did run" {Figure 3).
The contractionform "won't run" obtains the same analysis (thesame tree although the word nuclei can containextra information which makes the distinction)as a contraction of the words "will not run".As the example shows, orthographic words weresegmented whenever required by the syntacticanalysis.This solution did not exist prior the FDGand generally is not possible in a monostrataldependency description, which takes the (or-thographic) words as primitives.
The problemis that the non-contiguous elements in a verb-chain are assigned into a single node while thesubject in between belongs to its own node.For historical reasons, the representation con-tains a lexico-functional level closely similar tothe syntactic analysis of the earlier English Con-straint Grammar (ENGCG) (Karlsson et al ,1995) parsing system.
The current FDG for-malism overcomes several shortcomings 14of theearlier approaches: (1) the FDG does not relyon the detection of clause boundaries, (2) pars-ing is no longer sequential, (3) ambiguity is rep-resented at the clause level rather than wordlevel, (4) due to explicit representation of de-pendency structure, there is no need to refer tophrase-like units.
Because the FDG rule formal-ism is more expressive, linguistic generalisationcan be formalised in a more transparent way,which makes the rules more readable.7 Descr ip t ive  so lu t ions7.1 Coord inat ionWe now tackle the problem of how coordinationcan be represented in the framework of depen-dency model.
For example, Hudson (1991) hasargued that coordination is a phenomenon thatrequires resorting to a phrase-structure model.Coordination should not be seen as a directedfunctional relation, but instead as a special con-nexion between two functionally equal elements.The coordination connexions are called junc-tions in Tesnibre (1959, Chs.
134-150).
Tesni~reconsidered junctions primarily as a mechanismto pack multiple sentences economically intoone.
Unfortunately, his solution, which repre-sents all coordinative connexions in stemmas,is not adequate, because due to cyclic arcs theresult is no longer a tree.Our solution is to pay due respect o the for-mal properties of the dependency model, whichrequires that each element should have one andonly one head} 5 This means that coordinatedelements are chained (Figure 4) using a specificarc for coordination (labeled as cc).
The coordi-nators are mostly redundant markers (Tesnibre,1959, Ch.
39:5) 16, especially, they do not have~4Listed in Voutilainen (1994).lSThe treatment of coordination and gapping in Ka-hane (1997) resembles ours in simple cases.
However,this model maintains projectivity, and consequently,both multiple heads and extended nuclei, which are es-sentially phrase-level units, are used in complex cases,making the model broadly similar to Hudson (1991).~The redundancy is shown in the existence of asyn-detic coordination.
As syntactic markers, coordinatorsare not completely void of semantic ontent, which is8LOVEJOHN MARYBILL AND AND JOANFigure 4: Coordinated elementsany (governing) role in the syntactic structureas they do in many word-based forms of depen-dency theory (e.g, Kunze (1975) and Mel'/:uk(1987)).Unlike the other arcs in the tree, the arcmarking coordination does not imply a depen-dency relation but rather a functional equiva-lence.
If we assume that the coordinated el-ements have exactly the same syntactic func-tions, the information available is similar to thatprovided in Tesnibre:s representation.
If needed,we can simply print all the possible combina-tions of the coordinated elements: "Bill lovesMary", "John loves Mary ~, etc.7.2 Gapp ingIt is claimed that gapping is even a more se-rious problem for dependency theories, a phe-nomenon which requires the presence of non-terminal nodes.
The treatment of gapping,where the main verb of a clause is missing, fol-lows from the treatment of simple coordination.In simple coordination, the coordinator hasan auxiliary role without any specific functionin the syntactic tree.
In gapping, only the coor-dinator is present while the verb is missing.
Onecan think that as the coordinator epresents allmissing elements in the clause, it inherits allproperties of the missing (verbal) elements (Fig-ure 6).
This solution is also computationallyeffective because we do not need to postulateempty nodes in the actual parsing system.From a descriptive point of view there is noproblem if we think that the coordinator ob-tains syntactic properties from the nucleus thatdemonstrated !
)5' the existence of contrasting set of co-ordinators; 'and', 'or', 'but' etc.i:i>=!I Ii I<John>"John" N SG @SUBJ subj:>2<gave>"give" V PAST ~,+FV #2 main:>0<the>"the" DET ART SG/PL ~.
'DN> det:>4<lecture>"lecture" N SG ~OBJ #4 obj:>2<on>"on" PREP @ADVL #5 tmp:>2<Tuesday>"Tuesday" N SG ~<P pcomp:>5<and>"and" CC ~CC #7 cc:>2<Bill>"Bill" N SG @SUBJ subj:>7<on>"on" PREP ~ADVL #9 tmp:>7<Wednesday>"Wednesday" N SG ~<P pcomp:>9<.>Figure 5: Text-based representationit is connected to.
Thus, in a sentence with ver-bal ellipsis, e.g.
in the sentence "Jack paintedthe kitchen white and the living room blue", thecoordinator obtains the subcategorisation prop-erties of a verb.
A corresponding graph is seenin Figure 6.Due to 'flatness' of dependency model, thereis no problem to describe gapping where a sub-ject rather than complements are involved, asthe Figure 5 shows.
Note that gapping providesclear evidence that the syntactic element is anucleus rather than a word.
For example, inthe sentence "Jack has been lazy and Jill an-gry', the elliptic element is the verbal nucleushas been.8 Conc lus ionThis paper argues for a descriptively adequatesyntactic theory that is based on dependencyrather than constituency.
Tesni~re's theoryseems to provide a useful descriptive frameworkfor syntactic phenomena occurring in variousnatural languages.
We apply the theory anddevelop the representation to meet he require-ments of computerised parsing description.
Si-9multaneously, we explicate the formal proper-ties of Tesni~re;s theory that are used in con-structing a practical parsing system.A solution to the main obstacle to the utilisa-tion of the theory, the linearisation of the syn-tactic structure, is presented.
As a case study,we reformulate the theory for the descriptionof coordination and gapping, which are difficultproblems for any comprehensive syntactic the-ory.AcknowledgmentsWe thank Fred Karlsson, Atro Voutilainen andthree Coling-ACL '98 workshop referees for usefulcomments on earlier draft of this paper.ReferencesHaim Gaifman.
1965.
Dependency s stems andphrase-structure systems.
Information andControl, 8:304-337.Eva Haji~:ov~i.
1993.
Issues of Sentence Struc-tune and Discourse Patterns, volume 2 ofTheoretical and Computational Linguistics.Institute of Theoretical and ComputationalLinguistics, Charles University, Prague.Zellig S. Harris.
1960.
Structural Linguistics.Phoenix Books.
The University of ChicagoPress, Chicago & London, first Phoenix edi-tion.
Formerly entitled: Methods in Struc-tural Linguistics, 1951.David G. Hays.
1960.
Grouping and depen-dency theories.
Technical Report RM-2646,The RAND Corporation, September.David G. Hays.
1964.
Dependency theory: Aformalism and some observations.
Language,40:511-525.Hans Jiirgen Heringer.
1996.
Empiric und Intu-ition bei Tesni~re.
In Gertrud Greciano andHelmut Schumacher, editors, Lucien Tesni~ne- Syntaze structurale t operations mentales,volume 330 of Linguistische Arbeiten, pages15-31.
Niemeyer.Richard Hudson.
1989.
Towards a computer-testable word grammar of English.
UCLworking papers in Linguistics, 1:321-338.Richard Hudson.
1991.
English Ilion/ Gram-mar.
Basil Blackwell, Cambridge, MA.Timo J~rvinen and Pasi Tapanainen.
1997.
Adependency parser for English.
Technical Re-port TR-1, Department of General Linguis-tics, University of Helsinki, Finland, March.Sylvain Kahane.
1997.
Bubble trees and syn-PAINTED7JACK KITCHEN WHITE ANDTHE LIVING_ROOM BLUETHEFigure 6: Jack painted the kitchen white and the living room blue.tactic representations.
In Becker and Krieger,editors, Proceedings 5th Meeting of Mathe-matics of language, Saarbriicken, DFKI.Fred Karlsson, Atro Voutilainen, Juha Heikkil~i,and Arto Anttila, editors.
1995.
ConstraintGrammar: a language-independent systemfor parsing unrestricted text, volume 4 ofNatural Language Processing.
_Mouton deGruyter, Berlin and New York.Kimmo Kettunen.
1994.
Evaluating FUNDPL,a dependency parsing formalism for Finnish.In Research in Humanities Computing, vol-ume 2, pages 47-63.
Clarendon Press, Oxford.Jiirgen Kunze.
1975.
Abh6ngigkeitsgrammatik.Akadenaie-Verlag, Berlin.Solomon Marcus.
1967.
Introductionmathdmatique h la linguistique structurale.Dunod, Paris.James D. McCawley.
1982.
Parentheticals anddiscontinuous constituent s ructure.
Linguis-tic Inquiry, 13(1):91-106.Igor A. Mel'~.uk.
1987.
Dependency Syntax:Theory and Practice.
State University of NewYork Press, Albany.Peter Neuhaus and Norbert BrSker.
1997.
Thecomplexity of recognition of linguisticallyadequate dependency grammars.
In ACL-EACL'97 Proceedings, pages pp.
337-343,Madrid, Spain, July.
Association for Compu-tational Linguistics.Vladimir Petkevi~:.
1987.
A new dependencybased specification.
Theoretical Linguistics,14:143-172.Randolph Quirk, Sidney Greenbaum, Geoffrey10Leech, and Jan Sx~rtvik.
1985.
A Compre-hensive Grammar of the English Language.Longman, Harcourt.Jane J. Robinson.
1970.
Dependency struc-tures and transformational rules.
Language,46:259-285.Petr Sgall.
1992.
Underlying structure of sen-tences and its relations to semantics.
WienerSlawistischer Almanach, Sonderband 30:349-368.Anna Siewierska.
1988.
Word Order Rules.Croom Hehn, London.Pasi Tapanainen and Timo J~irvinen.
1997.
Anon-projective dependency parser.
In Pro-ceedings of the 5th Conference on AppliedNatural Language Processing, Washington,D.C, pages 64-71, Washington, D.C.,April.Association for Computational Linguistics.Pasi Tapanainen.
1996.
The constraint gram-mar parser CG-2.
Publications 27, Depart-ment of General Linguistics, University ofHelsinki, Finland.Lucien Tesni~re.
1959.
Elgments de syntaxestructurale.
Editions Klincksieck, Paris.Atro Voutilainen.
1994.
Designing a ParsingGrammar.
Publications of Department ofGeneral Linguistics, University of Helsinki,No.
22, Helsinki.Atro Voutilainen.
1995.
Morphological disam-biguation.
In Karlsson et al (1995), chap-ter 6, pages 165-284.Rulon S. Wells.
1947.
Immediate constituents.Language.
Reprinted in Martin Joos: Read-ings In Linguistics I, 1957, pp.
186-207.I-ii
