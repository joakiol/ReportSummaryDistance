Bilingual Lexicon Extraction from Comparable Corpora as MetasearchAmir Hazem and Emmanuel MorinUniversite?
de Nantes,LINA - UMR CNRS 62412 rue de la Houssinie`re,BP 92208 44322 Nantes Cedex 03amir.hazem@univ-nantes.fremmanuel.morin@univ-nantes.frSebastian Pen?a Saldarriaga1100 rue Notre-Dame Ouest,Montre?al, Que?bec,Canada H3C 1K3spena@synchromedia.caAbstractIn this article we present a novel way of look-ing at the problem of automatic acquisitionof pairs of translationally equivalent wordsfrom comparable corpora.
We first presentthe standard and extended approaches tradi-tionally dedicated to this task.
We then re-interpret the extended method, and motivate anovel model to reformulate this approach in-spired by the metasearch engines in informa-tion retrieval.
The empirical results show thatperformances of our model are always betterthan the baseline obtained with the extendedapproach and also competitive with the stan-dard approach.1 IntroductionBilingual lexicon extraction from comparable cor-pora has received considerable attention since the1990s (Rapp, 1995; Fung, 1998; Fung and Lo,1998; Peters and Picchi, 1998; Rapp, 1999; Chiaoand Zweigenbaum, 2002a; De?jean et al, 2002;Gaussier et al, 2004; Morin et al, 2007; Larocheand Langlais, 2010, among others).
This attentionhas been motivated by the scarcity of parallel cor-pora, especially for countries with only one officiallanguage and for language pairs not involving En-glish.
Furthermore, as a parallel corpus is com-prised of a pair of texts (a source text and a translatedtext), the vocabulary appearing in the translated textis highly influenced by the source text, especially intechnical domains.
Consequently, comparable cor-pora are considered by human translators to be moretrustworthy than parallel corpora (Bowker and Pear-son, 2002).
Comparable corpora are clearly of usein the enrichment of bilingual dictionaries and the-sauri (Chiao and Zweigenbaum, 2002b; De?jean etal., 2002), and in the improvement of cross-languageinformation retrieval (Peters and Picchi, 1998).According to (Fung, 1998), bilingual lexiconextraction from comparable corpora can be ap-proached as a problem of information retrieval (IR).In this representation, the query would be the wordto be translated, and the documents to be foundwould be the candidate translations of this word.
Inthe same way that as documents found, the candi-date translations are ranked according to their rele-vance (i.e.
a document that best matches the query).More precisely, in the standard approach dedicatedto bilingual lexicon extraction from comparable cor-pora, a word to be translated is represented by avector context composed of the words that appearin its lexical context.
The candidate translationsfor a word are obtained by comparing the translatedsource context vector with the target context vectorsthrough a general bilingual dictionary.
Using thisapproach, good results on single word terms (SWTs)can be obtained from large corpora of several millionwords, with an accuracy of about 80% for the top 10-20 proposed candidates (Fung and McKeown, 1997;Rapp, 1999).
Cao and Li (2002) have achieved 91%accuracy for the top three candidates using the Webas a comparable corpus.
Results drop to 60% forSWTs using specialized small size language cor-pora (Chiao and Zweigenbaum, 2002a; De?jean andGaussier, 2002; Morin et al, 2007).In order to avoid the insufficient coverage of thebilingual dictionary required for the translation ofsource context vectors, an extended approach has35Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 35?43,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticsbeen proposed (De?jean et al, 2002; Daille andMorin, 2005).
This approach can be seen as a queryreformulation process in IR for which similar wordsare substituted for the word to be translated.
Thesesimilar words share the same lexical environmentsas the word to be translated without appearing withit.
With the extended approach, (De?jean et al, 2002)obtained for single French-English words 43% and51% precision out of the ten and twenty first candi-dates applied to a medical corpus of 100 000 words(respectively 44% and 57% with the standard ap-proach) and 79% and 84% precision on the ten andtwenty first candidates applied to a social sciencecorpus of 8 million words (respectively 35% and42% with the standard approach).
Within this con-text, we want to show how metasearch engines canbe used for bilingual lexicon extraction from spe-cialized comparable corpora.
In particular, we willfocus on the use of different strategies to take fulladvantage of similar words.The remainder of this paper is organized as fol-lows.
Section 2 presents the standard and extendedapproaches based on lexical context vectors dedi-cated to word alignment from comparable corpora.Section 3 describes our metasearch approach thatcan be viewed as the combination of different searchengines.
Section 4 describes the different linguisticresources used in our experiments and evaluates thecontribution of the metasearch approach on the qual-ity of bilingual terminology extraction through dif-ferent experiments.
Finally, Section 5 presents ourconclusions.2 Related WorkIn this section, we first describe the standard ap-proach dedicated to word alignment from compara-ble corpora.
We then present an extension of thisapproach.2.1 Standard ApproachThe main work in bilingual lexicon extraction fromcomparable corpora is based on lexical context anal-ysis and relies on the simple observation that a wordand its translation tend to appear in the same lexi-cal contexts.
The basis of this observation consistsin the identification of first-order affinities for eachsource and target language: First-order affinities de-scribe what other words are likely to be found inthe immediate vicinity of a given word (Grefenstette,1994a, p. 279).
These affinities can be representedby context vectors, and each vector element repre-sents a word which occurs within the window ofthe word to be translated (for instance a seven-wordwindow approximates syntactical dependencies).The implementation of this approach can be car-ried out by applying the following four steps (Rapp,1995; Fung and McKeown, 1997):Context characterizationAll the lexical units in the context of each lexicalunit i are collected, and their frequency in a windowof n words around i extracted.
For each lexical uniti of the source and the target languages, we obtain acontext vector i where each entry, ij , of the vector isgiven by a function of the co-occurrences of units jand i.
Usually, association measures such as the mu-tual information (Fano, 1961) or the log-likelihood(Dunning, 1993) are used to define vector entries.Vector transferThe lexical units of the context vector i are trans-lated using a bilingual dictionary.
Whenever thebilingual dictionary provides several translations fora lexical unit, all the entries are considered butweighted according to their frequency in the targetlanguage.
Lexical units with no entry in the dictio-nary are discarded.Target language vector matchingA similarity measure, sim(i, t), is used to scoreeach lexical unit, t, in the target language with re-spect to the translated context vector, i.
Usual mea-sures of vector similarity include the cosine similar-ity (Salton and Lesk, 1968) or the weighted jaccardindex (WJ) (Grefenstette, 1994b) for instance.Candidate translationThe candidate translations of a lexical unit are thetarget lexical units ranked following the similarityscore.2.2 Extended ApproachThe main shortcoming of the standard approach isthat its performance greatly relies on the coverage ofthe bilingual dictionary.
When the context vectors36mot Identify ksimilar vectorsSource language Target languagemot wordmot wordmot wordmot wordmot word......wordwordwordMatch vectorsin target language ...Bilingual dictionaryFigure 1: Illustration of the extended approach.are well translated, the translation retrieval rate inthe target language improves.Although, the coverage of the bilingual dictionarycan be extended by using specialized dictionariesor multilingual thesauri (Chiao and Zweigenbaum,2003; De?jean et al, 2002), translation of contextvectors remains the core of the approach.In order to be less dependent on the coverageof the bilingual dictionary, De?jean and Gaussier(2002) have proposed an extension to the standardapproach.
The basic intuition of this approach isthat words sharing the same meaning will sharethe same environments.
The approach is basedon the identification of second-order affinities inthe source language: Second-order affinities showwhich words share the same environments.
Wordssharing second-order affinities need never appeartogether themselves, but their environments are sim-ilar (Grefenstette, 1994a, p. 280).Generally speaking, a bilingual dictionary is abridge between two languages established by its en-tries.
The extended approach is based on this ob-servation and avoids explicit translation of vectorsas shown in Figure 1.
The implementation of thisextended approach can be carried out in four stepswhere the first and last steps are identical to the stan-dard approach (De?jean and Gaussier, 2002; Dailleand Morin, 2005):Reformulation in the target languageFor a lexical unit i to be translated, we identifythe k-nearest lexical units (k nlu), among the dic-tionary entries corresponding to words in the sourcelanguage, according to sim(i, s).
Each nlu is trans-lated via the bilingual dictionary, and the vector inthe target language, s, corresponding to the transla-tion is selected.
If the bilingual dictionary providesseveral translations for a given unit, s is given bythe union of the vectors corresponding to the trans-lations.
It is worth noting that the context vectors arenot translated directly, thus reducing the influence ofthe dictionary.Vector matching against reformulationsThe similarity measure, sim(s, t), is used to scoreeach lexical unit, t, in the target language with re-spect to the k nlu.
The final score assigned to eachunit, t, in the target language is given by:sim(i, t) = ?s?kNLUsim(i, s)?
sim(s, t) (1)An alternate scoring function has been proposedby Daille and Morin (2005).
The authors computedthe centroid vector of the k nlu, then scored targetunits with respect to the centroid.3 The Metasearch Approach3.1 MotivationsThe approach proposed by De?jean and Gaussier(2002) implicitly introduces the problem of select-ing a good k. Generally, the best choice of k dependson the data.
Although several heuristic techniques,like cross-validation, can be used to select a goodvalue of k, it is usually defined empirically.The application of the extended approach (EA) toour data showed that the method is unstable withrespect to k. In fact, for values of k over 20, theprecision drops significantly.
Furthermore, we can-not ensure result stability within particular ranges of37values.
Therefore, the value of k should be carefullytuned.Starting from the intuition that each nearest lexi-cal unit (nlu) contributes to the characterization of alexical unit to be translated, our proposition aims atproviding an algorithm that gives a better precisionwhile ensuring higher stability with respect to thenumber of nlu.
Pushing the analogy of IR style ap-proaches (Fung and Lo, 1998) a step further, we pro-pose a novel way of looking at the problem of wordtranslation from comparable corpora that is concep-tually simple: a metasearch problem.In information retrieval, metasearch is the prob-lem of combining different ranked lists, returnedby multiple search engines in response to a givenquery, in such a way as to optimize the performanceof the combined ranking (Aslam and Montague,2001).
Since the k nlu result in k distinct rankings,metasearch provides an appropriate framework forexploiting information conveyed by the rankings.In our model, we consider each list of a given nluas a response of a search engine independently fromthe others.
After collecting all the lists of the se-lected nlu?s, we combine them to obtain the finalsimilarity score.
It is worth noting that all the listsare normalized to maximize in such a way the con-tribution of each nlu.
A good candidate is the onethat obtains the highest similarity score which is cal-culated with respect to the selected k. If a given can-didate has a high frequency in the corpus, it may besimilar not only to the selected nearest lexical units(k), but also to other lexical units of the dictionary.
Ifthe candidate is close to the selected nlu?s and alsoclose to other lexical units, we consider it as a po-tential noise (the more neighbours a candidate has,the more it?s likely to be considered as noise).
Wethus weight the similarity score of a candidate bytaking into account this information.
We comparethe distribution of the candidate with the k nlu andalso with all its neighbours.
This leads us to sup-pose that a good candidate should be closer to theselected nlu?s than the rest of its neighbours, if it?snot the case there is more chances for this candidateto be a wrong translation.3.2 Proposed ApproachIn the following we will describe our extensionto the method proposed by De?jean and Gaussier(2002).
The notational conventions adopted are re-viewed in Table 1.
Elaborations of definitions willbe given when the notation is introduced.
In all ourexperiments both terms and lexical units are singlewords.Symbol Definitionl a list of a given lexical unit.k the number of selected nearest lex-ical units (lists).freq(w, k) the number of lists (k) in which aterm appears.n all the neighbours of a given term.u all the lexical units of the dictio-nary.wl a term of a given list l.s(wl) the score of the term w in the list l.maxl the maximum score of a given listl.maxAll the maximum score of all the lists.snorm(wl) the normalized score of term w inthe list l.s(w) the final score of a term w.?w the regulation parameter of theterm w.Table 1: Notational conventions.The first step of our method is to collect eachlist of each nlu.
The size of the list has its impor-tance because it determines how many candidatesare close to a given nlu.
We noticed from our ex-periments that, if we choose lists with small sizes,we should lose information and if we choose listswith large sizes, we could keep more informationthan necessary and this should be a potential noise,so we consider that a good size of each list shouldbe between 100 and 200 terms according to our ex-periments.After collecting the lists, the second step is to nor-malize the scores.
Let us consider the equation 2 :snorm(wl) = s(wl)?
maxlmaxAll (2)We justify this by a rationale derived from twoobservations.
First, scores in different rankings arecompatible since they are based on the same simi-larity measure (i.e., on the same scale).
The secondobservations follows from the first: if max (l)38max (m), then the system is more confident aboutthe scores of the list l than m.Using scores as fusion criteria, we compute thesimilarity score of a candidate by summing its scoresfrom each list of the selected nlu?s :s(w) = ?w ?
?kl=1 snorm(wl)?nl=1 snorm(wl)(3)the weight ?
is given by :?w = freq(w, k)?
(u?
(k ?
freq(w, k)))(u?
freq(w, n)) (4)The aim of this parameter is to give more con-fidence to a term that occurs more often with theselected nearest neighbours (k) than the rest of itsneighbours.
We can not affirm that the best candi-date is the one that follows this idea, but we can nev-ertheless suppose that candidates that appear with ahigh number of lexical units are less confident andhave higher chances to be wrong candidates (we canconsider those candidates as noise).
So, ?
allows usto regulate the similarity score, it is used as a confi-dent weight or a regulation parameter.
We will referto this model as the multiple source (MS) model.
Wealso use our model without using ?
and refer to it by(LC), this allows us to show the impact of ?
in ourresults.4 Experiments and Results4.1 Linguistic ResourcesWe have selected the documents from the Elsevierwebsite1 in order to obtain a French-English spe-cialized comparable corpus.
The documents weretaken from the medical domain within the sub-domain of ?breast cancer?.
We have automaticallyselected the documents published between 2001 and2008 where the title or the keywords contain theterm ?cancer du sein?
in French and ?breast can-cer?
in English.
We thus collected 130 documentsin French and 118 in English and about 530,000words for each language.
The documents compris-ing the French/English specialized comparable cor-pus have been normalized through the following lin-guistic pre-processing steps: tokenisation, part-of-1www.elsevier.comspeech tagging, and lemmatisation.
Next, the func-tion words were removed and the words occurringless than twice (i.e.
hapax) in the French and theEnglish parts were discarded.
Finally, the compara-ble corpus comprised about 7,400 distinct words inFrench and 8,200 in English.The French-English bilingual dictionary requiredfor the translation phase was composed of dictionar-ies that are freely available on the Web.
It contains,after linguistic pre-processing steps, 22,300 Frenchsingle words belonging to the general language withan average of 1.6 translations per entry.In bilingual terminology extraction from special-ized comparable corpora, the terminology refer-ence list required to evaluate the performance ofthe alignment programs are often composed of 100single-word terms (SWTs) (180 SWTs in (De?jeanand Gaussier, 2002), 95 SWTs in (Chiao andZweigenbaum, 2002a), and 100 SWTs in (Dailleand Morin, 2005)).
To build our reference list,we selected 400 French/English SWTs from theUMLS2 meta-thesaurus and the Grand dictionnaireterminologique3.
We kept only the French/Englishpair of SWTs which occur more than five times ineach part of the comparable corpus.
As a result offiltering, 122 French/English SWTs were extracted.4.2 Experimental SetupThree major parameters need to be set to the ex-tended approach, namely the similarity measure, theassociation measure defining the entry vectors andthe size of the window used to build the context vec-tors.
Laroche and Langlais (2010) carried out a com-plete study about the influence of these parameterson the quality of bilingual alignment.As similarity measure, we chose to use theweighted jaccard index:sim(i, j) =?t min (it, jt)?t max (it, jt)(5)The entries of the context vectors were deter-mined by the log-likelihood (Dunning, 1993), andwe used a seven-word window since it approximatessyntactic dependencies.
Other combinations of pa-rameters were assessed but the previous parametersturned out to give the best performance.2http://www.nlm.nih.gov/research/umls3http://www.granddictionnaire.com/394.3 ResultsTo evaluate the performance of our method, we useas a baseline, the extended approach (EA) proposedby De?jean and Gaussier (2002).
We compare thisbaseline to the two metasearch strategies definedin Section 3: the metasearch model without theregulation parameter ?
(LC); and the one which isweighted by theta (MS).
We also provide results ob-tained with the standard approach (SA).We first investigate the stability of the metasearchstrategies with respect to the number of nlu consid-ered.
Figure 2 show the precision at Top 20 as afunction of k.1 10 20 30 40 50 60010203040506070Number of NLUPrecisionattop20LCMSEASAFigure 2: Precision at top 20 as a function of the numberof nlu.In order to evaluate the contribution of the param-eter ?, we chose to evaluate the metasearch methodstarting from k = 4, this explains why the precisionis extremely low for low values of k. We furtherconsidered that less than four occurrences of a termin the whole lexical units lists can be considered asnoise.
On the other side, we started from k = 1 forthe extended approach since it makes no use of theparameter ?.
Figure 2 shows that extended approachreaches its best performance at k = 7 with a preci-sion of 40.98%.
Then, after k = 15 the precisionstarts steadily decreasing as the value of k increases.The metasearch strategy based only on similarityscores shows better results than the baseline.
Forevery value of k ?
10, the LC model outperform theextended approach.
The best precision (48.36%) isobtained at k = 14, and the curve corresponding tothe LC model remains above the baseline regardlessof the increasing value of the parameter k. The curvecorresponding to the MS model is always above the(EA) for every value of k ?
10.
The MS modelconsistently improves the precision, and achieves itsbest performance (60.65%) at k = 21.We can notice from Figure 2 that the LC and MSmodels outperform the baseline (EA).
More impor-tantly, these models exhibit a better stability of theprecision with respect to the k-nearest lexical units.Although the performance decrease as the value of kincreases, it does not decrease as fast as in the base-line approach.For the sake of comparability, we also provideresults obtained with the standard approach (SA)(56.55%) represented by a straight line as it is notdependent on k. As we can see, the metasearchapproach (MS) outperforms the standard approachfor values of k bertween 20 and 30 and for greatervalues of k the precision remains more or less al-most the same as the standard approach (SA).
Thus,the metasearch model (MS) can be considered as acompetitive approach regarding to its results as it isshown in the figure 2.Finally, Figure 3 shows the contribution of eachnlu taken independently from the others.
This con-firms our intuition that each nlu contribute to thecharacterization of a lexical unit to be translated, andsupports our idea that their combination can improvethe performances.1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20510152025lexical unitsPrecisionattop20Figure 3: Precision at top 20 for each of the 20 nlu.
Theprecision is computed by taking the each nlu indepen-dently from the others.Figure 3 shows the top 20 of each nlu.
Notice40that the nlu are ordered from the most similar to thelexical unit to be translated to the less similar, andthat each one of the nearest lexical units containsinformation that it is worth taking into account.Although each nlu can only translate few terms,by using the metasearch idea we are able to improvethe retrieval of translation equivalents.
The mainidea of the metasearch paradigm is to take into ac-count the information conveyed by all the k nlu, us-ing either similarity scores, their behaviour with allthe neighbours, in order to improve the performanceof the alignment process.Although significant improvements can be ob-tained with the metasearch models (comparativelyto the EA and SA approach), especially concerningprecision stability with respect to the k nlu, we be-lieve that we need to address the estimation of k be-forehand.
Rather than fixing the same k for all theunits to be translated, there is the possibility to adaptan optimal value of k to each lexical unit, accordingto some criteria which have to be determined.Approachs Top 5 Top 10 Top 15 Top 20SA 37.70 45.08 52.45 56.55EA 21.31 31.14 36.88 40.98MS 40.98 54.91 56.55 60.65Table 2: Precision(%) at top 5, 10, 15, 20 for SA, EA andMS.Finally, we present in table 2 a comparison be-tween SA, EA and MS for the top 5, 10, 15 and 20.By choosing the best configuration of each method,we can note that our method outperforms the othersin each top.
In addition, for the top 10 our preci-sion is very close to the precision of the standard ap-proach (SA) at the top 20. we consider these resultsas encouraging for future work.4.4 DiscussionOur experiments show that the parameter k remainsthe core of both EA and MS approaches.
A goodselection of the nearest lexical units of a term guar-antee to find the good translation.
It is important tosay that EA and MS which are based on the k nlu?sdepends on the coverage of the terms to be trans-lated.
Indeed, these approaches face three cases :firstly, if the frequency of the word to be translatedis high and the frequency of the good translation inthe target language is low, this means that the nearestlexical units of the candidate word and its translationare unbalanced.
This leads us to face a lot of noisebecause of the high frequency of the source wordthat is over-represented by its nlu?s comparing to thetarget word which is under-represented.
Secondly,we consider the inverse situation, which is: low fre-quency of the source word and high frequency ofthe target translation, here as well, we have both thesource and the target words that are unbalanced re-garding to the selected nearest lexical units.
Thethird case, represents more or less the same distri-bution of the frequencies of source candidate andtarget good translation.
This can be considered asthe most appropriate case to find the good transla-tion by applying the approaches based on the nlu?s(EA or MS).
Our experiments show that our methodworks well in all the cases by using the parameter ?which regulate the similarity score by taken into ac-count the distribution of the candidate according toboth : selected nlu?s and all its neighbours.
In re-sume, words to be translated as represented in caseone and two give more difficulties to be translatedbecause of their unbalanced distribution which leadsto an unbalanced nlu?s.
Future works should con-firm the possibility to adapt an optimal value of kto each candidate to be translated, according to itsdistribution with respect to its neighbours.5 ConclusionWe have presented a novel way of looking at theproblem of bilingual lexical extraction from compa-rable corpora based on the idea of metasearch en-gines.
We believe that our model is simple andsound.
Regarding the empirical results of our propo-sition, performances of the multiple source modelon our dataset was better than the baseline proposedby De?jean and Gaussier (2002), and also outper-forms the standard approach for a certain range ofk.
We believe that the most significant result is thata new approach to finding single word translationshas been shown to be competitive.
We hope thatthis new paradigm can lead to insights that wouldbe unclear in other models.
Preliminary tests in thisperspective show that using an appropriate value ofk for each word can improve the performance of thelexical extraction process.
Dealing with this prob-41lem is an interesting line for future research.6 AcknowledgmentsThe research leading to these results has receivedfunding from the French National Research Agencyunder grant ANR-08-CORD-013.ReferencesJaved A. Aslam and Mark Montague.
2001.
Models forMetasearch.
In SIGIR ?01, proceedings of the 24thAnnual SIGIR Conference, pages 276?284.Lynne Bowker and Jennifer Pearson.
2002.
Workingwith Specialized Language: A Practical Guide to Us-ing Corpora.
Routledge, London/New York.Yunbo Cao and Hang Li.
2002.
Base Noun Phrase Trans-lation Using Web Data and the EM Algorithm.
InProceedings of the 19th International Conference onComputational Linguistics (COLING?02), pages 127?133, Tapei, Taiwan.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002a.Looking for candidate translational equivalents in spe-cialized, comparable corpora.
In Proceedings of the19th International Conference on Computational Lin-guistics (COLING?02), pages 1208?1212, Tapei, Tai-wan.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002b.Looking for French-English Translations in Compara-ble Medical Corpora.
Journal of the American Societyfor Information Science, 8:150?154.Yun-Chuang Chiao and Pierre Zweigenbaum.
2003.
TheEffect of a General Lexicon in Corpus-Based Identifi-cation of French-English Medical Word Translations.In Robert Baud, Marius Fieschi, Pierre Le Beux, andPatrick Ruch, editors, The New Navigators: from Pro-fessionals to Patients, Actes Medical Informatics Eu-rope, volume 95 of Studies in Health Technology andInformatics, pages 397?402, Amsterdam.
IOS Press.Be?atrice Daille and Emmanuel Morin.
2005.
French-English Terminology Extraction from ComparableCorpora.
In Proceedings of the 2nd InternationalJoint Conference on Natural Language Processing (IJ-CLNP?05), pages 707?718, Jeju Island, Korea.Herve?
De?jean and E?ric Gaussier.
2002.
Une nouvelle ap-proche a` l?extraction de lexiques bilingues a` partir decorpus comparables.
Lexicometrica, Alignement lexi-cal dans les corpus multilingues, pages 1?22.Herve?
De?jean, Fatia Sadat, and E?ric Gaussier.
2002.An approach based on multilingual thesauri and modelcombination for bilingual lexicon extraction.
In Pro-ceedings of the 19th International Conference onComputational Linguistics (COLING?02), pages 218?224, Tapei, Taiwan.Ted Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence.
Computational Linguis-tics, 19(1):61?74.Robert M. Fano.
1961.
Transmission of Information:A Statistical Theory of Communications.
MIT Press,Cambridge, MA, USA.Pascale Fung and Yuen Yee Lo.
1998.
An ir approach fortranslating new words from nonparallel, comparabletexts.
In Proceedings of the 17th international con-ference on Computational linguistics (COLING?98),pages 414?420.Pascale Fung and Kathleen McKeown.
1997.
Find-ing Terminology Translations from Non-parallel Cor-pora.
In Proceedings of the 5th Annual Workshop onVery Large Corpora (VLC?97), pages 192?202, HongKong.Pascale Fung.
1998.
A Statistical View on Bilingual Lex-icon Extraction: From ParallelCorpora to Non-parallelCorpora.
In David Farwell, Laurie Gerber, and EduardHovy, editors, Proceedings of the 3rd Conference ofthe Association for Machine Translation in the Ameri-cas (AMTA?98), pages 1?16, Langhorne, PA, USA.E?ric Gaussier, Jean-Michel Renders, Irena Matveeva,Cyril Goutte, and Herve?
De?jean.
2004.
A Geomet-ric View on Bilingual Lexicon Extraction from Com-parable Corpora.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics (ACL?04), pages 526?533, Barcelona, Spain.Gregory Grefenstette.
1994a.
Corpus-Derived First, Sec-ond and Third-Order Word Affinities.
In Proceedingsof the 6th Congress of the European Association forLexicography (EURALEX?94), pages 279?290, Ams-terdam, The Netherlands.Gregory Grefenstette.
1994b.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publisher,Boston, MA, USA.Audrey Laroche and Philippe Langlais.
2010.
Re-visiting Context-based Projection Methods for Term-Translation Spotting in Comparable Corpora.
InProceedings of the 23rd International Conference onComputational Linguistics (COLING?10), pages 617?625, Beijing, China.Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, andKyo Kageura.
2007.
Bilingual Terminology Mining ?Using Brain, not brawn comparable corpora.
In Pro-ceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL?07), pages 664?671, Prague, Czech Republic.Carol Peters and Eugenio Picchi.
1998.
Cross-languageinformation retrieval: A system for comparable cor-pus querying.
In Gregory Grefenstette, editor, Cross-language information retrieval, chapter 7, pages 81?90.
Kluwer Academic Publishers.42Reinhard Rapp.
1995.
Identify Word Translations inNon-Parallel Texts.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguis-tics (ACL?95), pages 320?322, Boston, MA, USA.Reinhard Rapp.
1999.
Automatic Identification of WordTranslations from Unrelated English and German Cor-pora.
In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics (ACL?99),pages 519?526, College Park, MD, USA.Gerard Salton and Michael E. Lesk.
1968.
Computerevaluation of indexing and text processing.
Jour-nal of the Association for Computational Machinery,15(1):8?36.43
