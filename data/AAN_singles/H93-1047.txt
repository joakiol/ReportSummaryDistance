Automatic Grammar Induction and Parsing Free Text:A Transformation-Based ApproachEr ic  Br i l l  *Depar tment  o f  Computer  and In fo rmat ion  Sc ienceUn ivers i ty  o f  Pennsy lvan iabri l l  @ unag i .
c i s .upenn.eduABSTRACTIn this paper we describe a new technique for parsing free text: atransformational grammar I is automatically earned that is capableof accurately parsing text into binary-branching syntactic trees withnonterminals unlabelled.
The algorithm works by beginning in avery naive state of knowledge about phrase structure.
By repeatedlycomparing the results of bracketing in the current state to properbracketing provided in the training corpus, the system learns a set ofsimple structural transformations that can be applied to reduce rror.After describing the algorithm, we present results and compare theseresults to other ecent results in automatic grammar induction.1.
INTRODUCTIONThere has been a great deal of interest of late in the automaticinduction of natural anguage grammar.
Given the difficultyinherent in manually building a robust parser, along with theavailability of large amounts of training material, automaticgrammar induction seems like a path worth pursuing.
Anumber of systems have been built which can be trained au-tomatically to bracket ext into syntactic onstituents.
In \[ 10\]mutual information statistics are extracted from a corpus oftext and this information is then used to parse new text.
\[13\]defines a function to score the quality of parse trees, and thenuses simulated annealing to heuristically explore the entirespace of possible parses for a given sentence.
In \[3\], distri-butional analysis techniques are applied to a large corpus tolearn a context-free grammar.The most promising results to date have been based on theinside-outside algorithm (i-o algorithm), which can be usedto train stochastic ontext-free grammars.
The i-o algorithmis an extension of the finite-state based Hidden Markov Model(by \[1\]), which has been applied successfully in many areas,including speech recognition and part of speech tagging.
Anumber of recent papers have explored the potential of usingthe i-o algorithm to automatically earn a grammar \[9, 15, 12,6, 7, 14\].Below, we describe a new technique for grammar induction.
2*The author would like to thank Mark Liberman, Meiting Lu, DavidMagerman, Mitch Marcus, Rich Pito, Giorgio Satta, Yves Schabes and TomVeatch.
This work was supported by DARPA and AFOSR jointly under grantNo.
AFOSR-90-0066, and by A'RO grant No.
DAAL 03-89-C0031 PRI.INot in the traditional sense of the term.2A similar method has been applied effectively inpart of speech tagging;The algorithm works by beginning in a very naive state ofknowledge about phrase structure.
By repeatedly comparingthe results of parsing in the current state to the proper phrasestructure for each sentence in the training corpus, the systemlearns a set of ordered transformations which can be appliedto reduce parsing error.
We believe this technique has ad-vantages over other methods of phrase structure induction.Some of the advantages include: the system is very simple,it requires only a very small set of transformations, learningproceeds quickly and achieves ahigh degree of accuracy, andonly a very small training corpUs is necessary.
In addition,since some tokens in a sentence are not even considered inparsing, the method could prove to be considerably more re-sistant o noise than a CFG-based approach.
After describingthe algorithm, we present results and compare these results toother ecent results in automatic phrase structure induction.2.
THE ALGORITHMThe learning algorithm is trained on a small corpus of partiallybracketed text which is also annotated with part of speech in-formation.
All of the experiments presented below were doneusing the Penn Treebank annotated corpus\[11\].
The learnerbegins in a naive initial state, knowing very little about thephrase structure of the target corpus.
In particular, all that isinitially known is that English tends to be right branching andthat final punctuation is final punctuation.
Transformationsare then learned automatically which transform the outputof the naive parser into output which better resembles thephrase structure found in the training corpus.
Once a set oftransformations has been learned, the system is capable oftaking sentences tagged with parts of speech and returning abinary-branching structure with nonterminals unlabelled 3.2.1.
The Initial State Of  The ParserInitially, the parser operates by assigning a right-linear struc-ture to all sentences.
The only exception is that final punctu-ation is attached high.
So, the sentence "The dog and old catate ."
would be incorrectly bracketed as:( ( The ( dog ( and ( old ( cat.ate ) ) ) ) ) .
)see \[5, 4\].3This is the same output given:by systems described in \[10, 3, 12, 14\]237The parser in its initial state will obviously not bracket sen-tences with great accuracy.
In some experiments below, webegin with an even more naive initial state of knowledge:sentences are parsed by assigning them a random binary-branching structure with final punctuation always attachedhigh.2.2.
Structural TransformationsThe next stage involves learning a set of transformations thatcan be applied to the output of the naive parser to make thesesentences better conform to the proper structure specified inthe training corpus.
The list of possible transformation typesis prespecified.
Transformations involve making a simplechange triggered by a simple environment.
In the currentimplementation, there are twelve allowable transformationtypes:?
(1-8) (Addldelete) a (leftlvight) parenthesis to the( left I right ) of part of speech tag X.?
(9-12) (Add\[delete) a (left\[right) parenthesis betweentags X and Y.To carry out a transformation by adding or deleting a paren-thesis, a number of additional simple changes must ake placeto preserve balanced parentheses and binary branching.
Togive an example, to delete a left paren in a particular envi-ronment, the following operations take place (assuming, ofcourse, that there is a left paren to delete):1.
Delete the left paren.2.
Delete the right paren that matches the just deleted paren.3.
Add a left paren to the left of the constituent immediatelyto the left of the deleted left paren.4.
Add a right paren to the right ofthe constituent immedi-ately to the right of the deleted paren.5.
If there is no constituent immediately tothe right, or noneimmediately to the left, then the transformation fails toapply.Structurally, the transformation can be seen as follows.
If wewish to delete a left paren to the right of constituent X 4, whereX appears in a subtree of the form:XAYY Z4To the fight of the rightmost terminal dominated by X if X is anonterminal.carrying out these operations will transform this subtree intoS:ZX YYGiven the sentence6:The dog barked.this would initially be bracketed by the naive parser as:( ( The ( dog barked ) ).
)If the transformation delete a left paren to the right of adeterminer is applied, the structure would be transformed tothe correct bracketing:( ( ( The dog ) barked ).
)To add a right parenthesis to the right of YY, YY must onceagain be in a subtree of the form:XAYY ZIf it is, the following steps are carried out to add the rightparen:1.
Add the right paren.2.
Delete the left paren that now matches the newly addedparen.3.
Find the right paren that used to match the just deletedparen and delete it.4.
Add a left paren to match the added right paren.5The twelve transformations can be decomposedinto two structural trans-formations, that shown here and its converse, along with six triggeringenvironments.6Input sentences are also labelled with parts of speech.238This results in the same structural change as deleting a leftparen to the right of X in this particular structure.Applying the transformation add a right paren to the right ofa noun to the bracketing:( ( The ( dog barked ) ).
)will once again result in the correct bracketing:( ( ( The dog ) barked ) .
)2.3.
Learning TransformationsLearning proceeds as follows.
Sentences in the training setare first parsed using the naive parser which assigns right lin-ear structure to all sentences, attaching final punctuation high.Next, for each possible instantiation of the twelve transforma-tion templates, that particular transformation is applied to thenaively parsed sentences.
The resulting structures are thenscored using some measure of success which compares theseparses to the correct structural descriptions for the sentencesprovided in the training corpus.
The transformation whichresults in the best scoring structures then becomes the firsttransformation of the ordered set of transformations that areto be learned.
That transformation is applied to the right-linear structures, and then learning proceeds on the corpus ofimproved sentence bracketings.
The following procedure iscarried out repeatedly on the training corpus until no moretransformations can be found whose application reduces theerror in parsing the training corpus:1.
The best transformation is found for the structures outputby the parser in its current state.
72.
The transformation is applied to the output resulting frombracketing the corpus using the parser in its current state.3.
This transformation is added to the end of the orderedlist of transformations.4.
Go to 1.After a set of transformations has been learned, it can be usedto effectively parse fresh text.
To parse fresh text, the text isfirst naively parsed and then every transformation is applied,in order, to the naively parsed text.One nice feature of this method is that different measures ofbracketing success can be used: learning can proceed in such7The state of the parser is defined as naive initial-state knowledge plus alltransformations that currently have been learned.a way as to try to optimize any specified measure of success.The measure we have chosen for our experiments i  the samemeasure described in \[12\], which is one of the measures thatarose out of a parser evaluation workshop \[2\].
The measureis the percentage of constituents ( trings of words betweenmatching parentheses) from sentences output by our systemwhich do not cross any constituents in the Penn Treebankstructural description of the sentence.
For example, if oursystem outputs:( ( ( The big ) ( dog ate ) ) .
)and the Penn Treebank bracketing for this sentence was:( ( ( The big dog ) ate ) .
)then the constituent the big would be judged correct whereasthe constituent dog ate would not.Below are the first seven transformations found from onerun of training on the Wall Street Journal corpus, which wasinitially bracketed using the right-linear initial-state parser.1.
Delete a left paren to the left of a singular noun.2.
Delete a left paren to the left of a plural noun.3.
Delete a left paren between two proper nouns.4.
Delet a left paren to the right of a determiner.5.
Add a right paren to the left of a comma.6.
Add a right paren to the left of a period.7.
Delete a right paren to the left of a plural noun.The first four transformations all extract noun phrases from theright linear initial structure.
The sentence "The cat meowed" would initially be bracketed as: 8( ( The ( cat meowed ) ).
)Applying the first transformation to this bracketing wouldresult in:( ( ( The cat ) meowed ).
)8These xamples are not actual sentences in the corpus.
We have chosensimple sentences for clarity.239Applying the'fifth transformation tothe bracketing:( ( We ( ran (,  ( and ( they walked ) ) ) ) ).
)would result inimprove performance in the test corpus.
One way around thisovertraining would be to set a threshold: specify a minimumlevel of improvement that must result for a transformation tobe learned.
Another possibility is to use additional trainingmaterial to prune the set of learned transformations.
( ( ( We ran ) ( ,  ( and ( they walked ) ) ) ).
)3.
RESULTSIn the first experiment we ran, training and testing weredone on the Texas Instruments Air Travel Information Sys-tem (ATIS) corpus\[8\].
9 In table 1, we compare results weobtained to results cited in \[ 12\] using the inside-outside algo-rithm on the same corpus.
Accuracy is measured in terms ofthe percentage of noncrossing constituents in the test corpus,as described above.
Our system was tested by using the train-ing set to learn a set of transformations, and then applyingthese transformations to the test set and scoring the resultingoutput.
In this experiment, 64 transformations were learned(compared with 4096 context-free rules and probabilities usedin the i-o experiment).
It is significant that we obtained com-parable performance using a training corpus only 21% as largeas that used to train the inside-outside algorithm.Method # of Training AccuracyCorpus SentencesInside-Outside 700 90.36%Transformation-Learner 150 91.12%Table 1: Comparing two learning methods on the ATIS cor-pus.After applying all learned transformations to the test corpus,60% of the sentences had no crossing constituents, 74% hadfewer than two crossing constituents, and 85% had fewer thanthree.
The mean sentence length of the test corpus was 11.3.In figure 1, we have graphed percentage correct as a func-tion of the number of transformations that have been appliedto the test corpus.
As the transformation number increases,overtraining sometimes occurs.
In the current implementa-tion of the learner, a transformation is added to the list if itresults in any positive net change in the training set.
To-ward the end of the learning procedure, transformations arefound that only affect a very small percentage of training sen-tences.
Since small counts are less reliable than large counts,we cannot reliably assume that these transformations will also9In all experiments described in this paper, esults are calculated on atest corpus which was not used in any way in either training the learningalgorithm or in developing the system.Cb09.o13..tOo0 10 20 30 40 50 60RuleNumberFigure 1: Results From the ATIS Corpus, Starting With Right-Linear StructureWe next ran an experiment to determine what performancecould be achieved if we dropped the initial right-linear as-sumption.
Using the same training and test sets as above,sentences were initially assigned a random binary-branchingstructure, with final punctuation always attached high.
Sincethere was less regular structure in this case than in the right-linear case, many more transformations were found, 147 trans-formations in total.
When these transformations were appliedto the test set, a bracketing accuracy of 87.13% resulted.The ATIS corpus is structurally fairly regular.
To determinehow well our algorithm performs on a more complex corpus,we ran experiments on the Wall Street Journal.
Results fromthis experiment can be found in table 2.1?
Accuracy is againmeasured as the percentage ofconstituents in the test set whichdo not cross any Penn Treebank constituents.
1~ As a pointof comparison, in \[14\] an experiment was done using the i-o algorithm on a corpus of WSJ sentences of length 1-15.Training was carried out on 1,095 sentences, and an accuracyof 90.2% was obtained in bracketing a test set.l?For sentences of length 2-15, the initial right-linear parser achieves 69%accuracy.
For sentences of length 2-20, 63% accuracy is achieved and forsentences of length 2-25, accuracy is 59%.11 In all of our experiments carried out on the Wall Street Journal, the testset was a randomly selected set of 500 sentences.240Sent.Length2-152-152-152-202-25# Training # of %Corpus Sents Transformations Accuracy250 83 88.1500 163 89.31000 221 91.6250 145 86.2250 160 83.8punctuation attached high.
In this experiment, 325 transfor-mations were found using a 250-sentence training corpus, andthe accuracy resulting from applying these transformations toa test set was 84.72%.Finally, in figure 2 we show the sentence l ngth distributionin the Wall Street Journal corpus.Table 2: WSJ SentencesIn the corpus used for the experiments of sentence l ngth 2-15, the mean sentence l ngth was 10.80.
In the corpus usedfor the experiment of sentence l ngth 2-25, the mean lengthwas 16.82.
As would be expected, performance degradessomewhat as sentence l ngth increases.
In table 3, we showthe percentage of sentences in the test corpus which have nocrossing constituents, and the percentage that have only a verysmall number of crossing constituents 12.Sent.
# Training % ofLength Corpus Sents 0-errorsents2-15 500 53.72-15 1000 62.42-25 250 29.2% of % of<l-error <2-errorsents sents72.3 84.677.2 87.844.9 59.900o00O0o 8000i i i i ,0 20 40 60 80 1 O0Sentence LengthTable 3: WSJ SentencesIn table 4, we show the standard eviation measured fromthree different randomly chosen training sets of each samplesize and randomly chosen test sets of 500 sentences each, aswell as the accuracy as a function of training corpus ize.Sent.
# Training % Std.Length Corpus Sents Correct Dev.2-20 0 63.0 0.692-20 10 75.8 2.952-20 50 82.1 1.942-20 100 84.7 0.562-20 250 86.2 0.462-20 750 87.3 0.61Table 4: More WSJ ResultsWe also ran an experiment on WSJ sentences of length 2-15starting with random binary-branching structures with final12For sentences of  length 2-15, the initial r ight l inear parser parses 17%of sentences with no crossing errors, 35% with one or fewer errors and 50%with two or fewer.
For sentences of  length 2-25, 7% of sentences are parsedwith no crossing errors, 16% with one or fewer, and 24% with two or fewer.Figure 2: The Distribution of Sentence Lengths in the WSJCorpus.While the numbers presented above allow us to comparethe transformation learner with systems trained and testedon Comparable corpora, these results are all based upon theassumption that the test data is tagged fairly reliably (manu-ally tagged text was used in all of these experiments, aswellin the experiments of \[12, 14\].)
When parsing free text, wecannot assume that the text will be tagged with the accuracyof a human annotator.
Instead, an automatic tagger wouldhave to be used to first tag the text before parsing.
To ad-dress this issue, we ran one experiment where we randomlyinduced a 5% tagging error rate beyond the error rate of thehuman annotator.
Errors were induced in such a way as topreserve the unigram part of speech tag probability distribu-tion in the corpus.
The experiment was run for sentences oflength 2-15, with a training set of 1000 sentences and a testset of 500 sentences.
The resulting bracketing accuracy was90.1%, compared to 91.6% accuracy when using an unadul-terated corpus.
Accuracy only degraded by a small amountwhen using the corpus with adulterated part of speech tags,suggesting that high parsing accuracy rates could be achievedif tagging of the input was done automatically b  a tagger.2414.
CONCLUSIONSIn this paper, we have described a new approach for learninga gran~nar to automatically parse free text.
The method canbe used to obtain good parsing accuracy with a very smalltraining set.
Instead of learning a traditional grammar, an or-dered set of structural transformations is learned which can beapplied to the output of a very naive parser to obtain binary-branching trees with unlabelled nonterminals.
Experimentshave shown that these parses conform with high accuracy tothe structural descriptions pecified in a manually annotatedcorpus.
Unlike other recent attempts at automatic grammarinduction which rely heavily on statistics both in training andin the resulting rammar, our learner is only very weakly sta-tistical.
For training, only integers are needed and the onlymathematical operations carried out are integer addition andinteger comparison.
The resulting grammar is completelysymbolic.
Unlike learners based on the inside-outside algo-rithm which attempt to find a grammar to maximize the prob-ability of the training corpus in hopes that this grammar willmatch the grammar that provides the most accurate structuraldescriptions, the transformation-based learner can readily useany desired success measure in learning.We have already begun the next step in this project: auto-matically labelling the nonterminal nodes.
The parser willfirst use the "transformational grammar" to output a parsetree without nonterminal labels, and then a separate algo-rithm will be applied to that tree to label the nonterminals.The nonterminal-node labelling algorithm makes use of ideassuggested in \[3\], where nonterminals are labelled as a func-tion of the labels of their daughters.
In addition, we plan toexperiment with other types of transformations.
Currently,each transformation i the learned list is only applied oncein each appropriate environment.
For a transformation to beapplied more than once in one environment, it must appearin the transformation list more than once.
One possible ex-tension to the set of transformation types would be to allowfor transformations of the form: add/delete a paren as manytimes as is possible in a particular environment.
We also planto experiment with other scoring functions and control strate-gies for finding transformations and to use this system as apostprocessor to other grammar induction systems, learningtransformations to improve their performance.
We hope thesefuture paths will lead to a trainable and very accurate parserof free text.References1.
Baker, J.
(1979) Trainable grammars for speech recognition.
InJared J. Wolf and Dennis H. Klatt, eds.
Speech communicationpapers presentedat the 97th Meeting of the Acoustical Societyof Ameriea, MIT.2.
Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman,R., Harrison, E, Hindle, D., Ingria, R., Jelinek, E, Klavans, J.,Liberman, M., Marcus, M., Roukos, S., Santorini, B. and Strza-lkowski, T. (1991) A Procedure for Quantitatively Comparingthe Syntactic Coverage of English Grammars.
Proceedings ofthe DARPA Workshop on Speech and Natural Language.3.
Brill, E. and Marcus, M. (1992) Automatically acquiring phrasestructure using distributional analysis.
Proceedings of the 5thDARPA Workshop on Speech and Natural Language.
Hard-man, N.Y.4.
Brill, E. and Marcus, M. (1992) Tagging an Unfamiliar TextWith Minimal Human Supervision.
American Association forArtificial Intelligence (AAAI) Fall Symposium on Probabilis-tic Approaches to Natural Language, Cambridge, Ma.
AAAITechnical Report.5.
Brill, E. (1992) A Simple Rule-Based Part of Speech Tagger.Proceedings of the Third Conference on Applied Computa-tional Linguistics (ACL).
Trento, Italy.6.
Briscoe, T and Waegner, N. (1992) Robust Stochastic ParsingUsing the Inside-Outside Algorithm.
In Workshop notes fromthe AAAI Statistically-Based NLP Techniques Workshop.7.
Carroll, G. and Chamiak, E. (1992) Learning Probabilistic De-pendency Grammars from Labelled Text.
In: Working Notesof the AAAI Fall Symposium on Probabilistic Approaches toNatural Language.
Cambridge, Ma.8.
Hemphill, C., Godfrey, J. and Doddington, G. (1990).
TheATIS spoken language systems pilot corpus.
In 1990 DARPASpeech and Natural Language Workshop.9.
Lari, K. and Young, S. (1990) The estimation of stochas-tic context-free grammars using the inside-outside algorithm.Computer Speech and Language.10.
Magerman, D. and Marcus, M. (1990) Parsing a natural an-guage using mutual information statistics, Proceedings, EighthNational Conference on Artificial Intelligence (AAA190).11.
Marcus, M., Santorini, B., and Marcinkiewicz, M. (1993)Building a large annotated corpus of English: the Penn Tree-bank.
To appear in Computational Linguistics.12.
Pereira, E and Schabes, Y.
(1992) Inside-outside r estimationfrom partially bracketed corpora.
Proceedings ofthe 20th Meet-ing of the Association for Computational Linguistics.
Newark,De.13.
Sampson, G. (1986) A stochastic approach to parsing.
In Pro-ceedings of COL1NG 1986, Bonn.14.
Schabes, Y., Roth, M. and Osborne, R. (1993) Parsing theWall Street Journal with the Inside-Outside algorithm.
1993European ACL.15.
Sharman, R., Jelinek, E and Mercer, R. (1990) Generating agrammar for statistical training.
Proceedings ofthe 1990 DarpaSpeech and Natural Language Workshop.242
