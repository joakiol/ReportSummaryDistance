Proceedings of the ACL 2014 Student Research Workshop, pages 71?77,Baltimore, Maryland USA, June 22-27 2014.c?2014 Association for Computational LinguisticsDisambiguating prepositional phrase attachment siteswith sense information captured in contextualized distributional dataClayton GreenbergDepartment of Computational Linguistics and PhoneticsUniversit?at des Saarlandescgreenbe@alumni.princeton.eduAbstractThis work presents a supervised preposi-tional phrase (PP) attachment disambigua-tion system that uses contextualized distri-butional information as the distance met-ric for a nearest-neighbor classifier.
Con-textualized word vectors constructed fromthe GigaWord Corpus provide a methodfor implicit Word Sense Disambiguation(WSD), whose reliability helps this systemoutperform baselines and achieve compa-rable results to those of systems with fullWSD modules.
This suggests that targetedWSD methods are preferable to ignoringsense information and also to implement-ing WSD as an independent module in apipeline.1 IntroductionArriving at meaning from a linguistic expression ishardly a trivial process, but a ?simple?
four-wordexpression shows some of the kinds of knowledgeand interactions involved:(1) a. eat [seeds [in plants]]b.
[eat seeds] [in plants](a) and (b) illustrate two possible interpretationsfor the expression.
In (a), the seeds are part oflarger organic units, and in (b), the eating takesplace in refineries.
Choosing (a) or (b) helps thesystem construct accurate relationships betweenthe events and participants mentioned, which is es-sential for many natural language processing tasksincluding machine translation, information extrac-tion, and textual inference.These two groupings represent an example ofthe widely-studied phenomenon of prepositionalphrase (PP) attachment ambiguity.
We definethe governor of a PP as the word or phrase thatthe PP modifies.
Ambiguity arises from multi-ple candidates for the governor.
Strings such asin (1) can be represented by quadruples of theform (V,N1, P,N2), where V is a transitive verb,N1is the head noun of an object of V , P is apreposition, and N2is the head noun of the objectof P .
Then, (a) and (b) reflect the two possiblechoices of governor for the PP: V (adverbial PP)andN1(adjectival PP).
Therefore, disambiguationfor such quadruples is a binary classification ofthe PP as adjectival or adverbial, or equivalently,noun-attach or verb-attach.In our example, classifying the sense ofthe word plant as either organic unit orrefinery is key to choosing the correct struc-ture.
These senses have significantly different re-spective relationships to eat and seeds.
In partic-ular, we often eat most except, or only, the seedsfrom an organic unit, but we have no such intu-itions about refineries.
The training data must beanalyzed carefully in order to prevent unwantedmixing of senses, since that causes noise in pre-dictions about word relationships.Given that V ?N2andN1?N2relationships arevery important for PP-attachment disambiguation,it is not surprising that leading PP-attachment dis-ambiguation systems include a Word Sense Dis-ambiguation (WSD) module.
The challenging as-pect of this is that it introduces a subtask that inthe general case has lower accuracy levels than theentire system.
Hence, its place and form withinthe system deserves to be examined closely.
Sincea representation of the predicted sense is not partof the attachment decision, it does not need to beexplicitly present within the procedure.
In thispaper, we investigate the importance of properword sense decisions for PP-attachment disam-biguation, and describe a highly-accurate systemthat encodes sense information in contextualizeddistributional data.
Its high performance showsthe benefit of representing and handling sense in-formation in a targeted fashion for the task.712 Background and related workSense information provides an illuminatingthrough line for many previous PP-attachmentdisambiguation systems.
We begin by describinga very popular dataset for the problem and itssubsequent development, and then trace throughthe two main approaches to sense informationrepresentation and the results obtained using thisdataset.2.1 The corpusA standard corpus for the binary classificationproblem described above was developed by Ratna-parkhi, Reynar and Roukos (1994).
They system-atically extracted (V,N1, P,N2) quadruples fromthe Penn Treebank Wall Street Journal (WSJ) cor-pus and used the manually-generated constituencyparses to obtain attachment decisions for eachof the extracted PPs.
The final dataset con-tained 27,937 quadruples.
These were divided into20,801 training quadruples, 4,039 developmentquadruples, and 3,097 test quadruples.
Their max-imum entropy model achieved 81.6% accuracy onthis dataset and their decision tree achieved 77.7%.Accuracy on this corpus is defined to be the num-ber of quadruples for which the classifier assignedthe same attachment site as the site indicated inthat sentence?s parse tree, divided by the totalnumber of quadruples.
Although some parse treesin the corpus are known to have errors, the accu-racy figures do not take this into account.Also, Ratnaparkhi et al (1994) conducted hu-man experiments with a subset of their corpus.They found that humans, when given just thequadruple, were accurate 88.2% of the time.When given the entire sentence for context, ac-curacy improved to 93.2%.
The perhaps un-derwhelming human performance is partially dueto misclassifications by the Treebank assemblerswho made these determinations by hand, and alsounclear cases, which we discuss in the next sec-tion.Collins and Brooks (1995) introduced modifica-tions to the Ratnaparkhi et al (1994) dataset meantto combat data sparsity and used the modified ver-sion to train their backed-off model.
They re-placed four digit numbers with YEAR, other num-bers with NUM.
Verbs and prepositions were con-verted to all lowercase.
In nouns, all words thatstarted with an uppercase letter followed by a low-ercase letter were replaced with NAME.
Then, allstrings NAME-NAME were replaced with NAME.Finally all verbs were automatically lemmatized.They did not release statistics on how these mod-ifications affected performance, so it is unclearhow to allocate the performance increase betweenthe backed-off model and the modifications to thedataset.
The paper also provided some baselines:they achieve 59.0% accuracy on the Ratnaparkhi etal.
(1994) corpus by assigning noun-attach to ev-ery quadruple, and 72.2% accuracy by assigninga default classification determined for each prepo-sition.
They show, and many subsequent papersconfirm, that the preposition is the most predictivedimension in the quadruple.Abney, Schapire, and Singer (1999) used thedataset from Collins and Brooks (1995) with aboosting algorithm and achieved 85.4% accuracy.Their algorithm also was able to order the spe-cific data points by how much weight they wereassigned by the learning algorithm.
The highestdata points tended to be those that contained er-rors.
Thus, they were able to improve the qualityof the dataset in a systematic way.2.2 The WordNet approachWordNet (Fellbaum, 1998) can be quite a power-ful aid to PP-attachment disambiguation becauseit provides a way to systematically quantify se-mantic relatedness.
The drawback is, though, thatsince WordNet semantic relations are between ex-plicit word senses (SynSets), the words in thequadruples must be associated with these explicitword senses.
The systems described below outlinethe different ways to make those associations.Brill and Resnik (1994) trained atransformation-based learning algorithm on12,766 quadruples from WSJ, with modificationssimilar to those by Collins and Brooks (1995).As a particularly human-interpretable feature,the rules used word sense hierarchies.
Namely, aWordNet rule applied to the named node and all ofits hyponyms.
For example, a rule involving boatwould apply to instances of kayak.
Importantly,each noun in the corpus inherited hypernyms fromall of its senses.
Therefore, they did not performexplicit WSD.
Their accuracy was 81.8%.The neural network by Nadh and Huyck (2012)also used WordNet word sense hierarchies.
Onlythe first (intended to be the most frequent) sense ofthe word was used in computations.
Hence, theyexplicitly perform WSD using a baseline method.72On a training corpus of 4,810 quadruples and atest corpus of 3,000 quadruples from WSJ, theyachieve 84.6% accuracy.
This shows the suc-cess of performing baseline WSD as part of a PP-attachment disambiguation system, although thedifferent dataset makes comparison less direct.At the other extreme, Stetina and Nagao (1997)developed a customized, explicit WSD algorithmas part of their decision tree system.
For each am-biguous word in each quadruple, this algorithmselected a most semantically similar quadruple inthe training data using unambiguous or previouslydisambiguated terms.
Then, the word was as-signed the WordNet sense that was semanticallyclosest to the sense of the corresponding wordin the other quadruple.
Their distance metricwas L1/D1+ L2/D2, where Liis the distancefrom word sense i to the common ancestor, andDiis the depth of the tree (distance to root) atword sense i.
Such a metric captures the notionthat more fine grained distinctions exist deeperin the WordNet graph, so the same absolute dis-tance between nodes matters less at greater depths.Stetina and Nagao (1997) trained on a versionof the Ratnaparkhi et al (1994) dataset that con-tained modifications similar to those by Collinsand Brooks (1995) and excluded forms not presentin WordNet.
The system achieved 88.1% accuracyon the entire test set and 90.8% accuracy on thesubset of the test set in which all four of the wordsin the quadruple were present in WordNet.Finally, Greenberg (2013) implemented a de-cision tree that reimplemented the WSD modulefrom Stetina and Nagao (1997), and also usedWordNet morphosemantic (teleological) links,WordNet evocations, and a list of phrasal verbsas features.
The morphosemantic links and evo-cations brought more semantic relatedness infor-mation after the cost of explicit WSD had al-ready been incurred.
The system achieved 89.0%on a similarly modified Ratnaparkhi et al (1994)dataset.2.3 The distributional approachAs an alternative to the WordNet approach, thedistributional tradition allows for implicit sensehandling given that contexts from all senses ofthe word are represented together in the vec-tor.
Without modification, the senses are repre-sented according to their relative frequencies inthe data.
Pantel and Lin (2000) created a col-location database that, for a given word, trackedthe words that appeared in specific syntactic rela-tions to it, such as subject (for verbs), adjective-modifier (for nouns), etc.
Then, they used thecollocation database to construct a corpus-basedthesaurus that evaluated semantic relatedness be-tween quadruples.
With a mix of unsupervisedlearning algorithms, they achieved 84.3% accu-racy.
They also argued that rules involving bothV and N1should be excluded because they causeover-fitting.Zhao and Lin (2004) implemented a nearestneighbor system that used various vector similar-ity metrics to calculate distances between quadru-ples.
The vectors were generated from the AC-QUAINT corpus with both syntactic relation andproximity-based (bag of words) models.
Theyfound that the cosine of pointwise mutual informa-tion metric on a syntactic model performed withthe greatest accuracy (86.5%, k = 33).
They useda version of the Ratnaparkhi et al (1994) datasetthat had all words lemmatized and all digits re-placed by @.Using the Web as a large unsupervised corpus,Nakov and Hearst (2005) created a PP-attachmentdisambiguation system that exploits n-grams, de-rived surface features, and paraphrases to predictclassifications.
The system searched for six spe-cific disambiguating paraphrases such as openedthe door (with a key), which suggests verb-attach,and eat: spaghetti with sauce, which suggestsnoun-attach.
Paraphrases and n-gram modelsrepresent the aim to gather context beyond thequadruple as a disambiguation method.
Their fi-nal system had 85.0% precision and 91.8% recallon the Ratnaparkhi et al (1994) dataset.
Whenassigning unassigned quadruples to verb-attach, ithad 83.6% accuracy and 100% recall.
Their sys-tem continued the trend that the most common er-ror is classifying a noun-attach quadruple as verb-attach.
This is because the majority of difficultcases are verb-attach, so all of the difficult casesget assigned verb-attach as a default.3 Linguistic analysisIn this section, we will discuss some difficul-ties with and observations about the task of PP-attachment disambiguation.
The analyses andconclusions drawn here set the linguistic founda-tion for the structure of the system described in thenext section.733.1 Lexically-specified prepositionsHindle and Rooth (1993) provided many linguis-tic insights for the PP-attachment disambiguationproblem, including the tendency to be verb-attachif N1is a pronoun, and that idiomatic expres-sions (e.g.
give way to mending) and light verbconstructions (e.g.
make cuts to Social Security)are particularly troublesome for humans to clas-sify.
The defining feature of such constructions isa semantically-vacuous preposition.
For example,in (2), we have semantically similar verbs appear-ing with different prepositions and yet the mean-ings of these sentences are still similar.
(2) a.
She was blamed for the crime.b.
She was accused of the crime.c.
She was charged with the crime.Further, when we nominalize charged we can getcharges of murder, but charged of murder is usu-ally unacceptable.
Also, (3) gives an analogousthree-way preposition variation following nouns.
(3) a.
They proposed a ban on tea.b.
They proposed a request for tea.c.
They proposed an alternative to tea.We argue that in these cases, a preceding wordcompletely determines the preposition selectedand that no further meaning is conveyed.
In fact,we might say that the prepositions in this caseserve analogously to morphological case mark-ing in languages more heavily inflected than En-glish.
Freidin (1992) makes a proposal along theselines.
The prescriptive rules that dictate ?correct?and ?incorrect?
prepositions associated with cer-tain verbs, nouns, and adjectives, as well as our ro-bust ability to understand these sentences with theprepositions omitted, strongly suggest that this se-lection is idiosyncratic and cannot be derived fromdeeper principles.The extreme case is phrasal verbs, for which it isproblematic to posit the existence of a PP becausethe object can occur before or after the ?preposi-tion.?
As shown in (4d), this is not acceptable forstandard prepositions.
(4) a.
He ran up the bill.b.
He ran the bill up.c.
He ran up the hill.d.
* He ran the hill up.For these, we say that there is one lexical entryfor the transitive verb plus the particle (prepositionwithout an object), as in to run up, and an optionaloperation reverses the order of the object of thephrasal verb and its particle.Usual paraphrase tests, such as those describedin Nakov and Hearst (2005), often do not leadto consistent conclusions about the proper attach-ment site for these lexically-specified preposi-tions.
Further, two separate governors do not ap-pear to be plausible.
Therefore, these construc-tions probably do not belong as data points inthe PP-attachment task.
However, if they mustconform to the task, the most reasonable attach-ment decision is likely to be the word that deter-mined the preposition.
Therefore, the PPs in (2)are verb-attach and those in (3) are noun-attach.This treatment of lexically-specified prepositionsaccounts for light verb constructions because theN1in those constructions dictates the preposition.3.2 The special case of ofPPs with the preposition of attach to nouns withvery few exceptions.
In fact, 99.1% of thequadruples with of in our training set are noun-attach.
The other 0.9% were misclassificationsand quadruples with verbs that lexically specifyof, such as accuse.
The behavior of of -PPs hasbeen widely studied.
We take the acceptability of(5a) and not (5b) as evidence that of -PPs introduceargument-like descriptions of their governors.
(5) a. a game of cards with incalculableoddsb.
* a game with incalculable odds ofcardsThe extremely high proportion of noun-attachments within of -PPs leads some to excludeof -PPs altogether from attachment disambigua-tion corpora.
In our data, excluding this most com-monly used English preposition shifts the mostfrequent attachment decision from noun-attach toverb-attach.
This is unfortunate for systems aim-ing to mimic human processing, since Late Clo-sure (Frazier, 1979) suggests a preference fornoun-attach as the default or elsewhere case.4 MethodsOur PP attachment disambiguation system is mostclosely related to Zhao and Lin (2004).
We ex-perimented with several similarity measures on a74slightly preprocessed version of the Ratnaparkhi etal.
(1994) dataset.4.1 Training dataBecause humans only perform 0.1% better thanStetina and Nagao?s (1997) system when giventhe quadruples but not the full sentences (althoughtechnically on different datasets), we found it im-portant to locate the full sentences in the PennTreebank.
So, we carefully searched for thequadruples in the raw version of the corpus.
Weensured that the corpus would be searched sequen-tially, i.e.
search for the current quadruple wouldbegin on the previous matched sentence and thenproceed forward.
By inspection, we could tellthat the sentences were roughly in order, so thischoice increased performance and accuracy.
How-ever, we had to adapt the program to be flexible sothat some truncated tokens in the quadruples, suchas incorrectly segmented contractions, would bematched to their counterparts.Next, we created some modified versions ofthe training corpus.
We explored the effectof excluding quadruples with lexically-specifiedprepositions (usually tagged PP-CLR in WSJ),removing sentences in which there was no ac-tual V,N1, P,N2string found, manually remov-ing encountered misclassifications, and reimple-menting data sparsity modifications from Collinsand Brooks (1995) and Stetina and Nagao (1997).In particular, we used the WordNet lemmatizer inNLTK to lemmatize the verbs in the corpus (Bird,Loper, and Klein 2009).
However, for direct com-parison with Zhao and Lin (2004), we decided touse in our final experiment a version of the cor-pus with all words lemmatized and all numbersreplaced by @, but no other modifications.4.2 Knowledge baseIn order to compute quadruple similarity mea-sures that take context information into account,we adopted the vector space model implementedby Dinu and Thater (2012).
This model constructsdistributional word vectors from the GigaWordcorpus.
We used a ?filtered?
model, meaning thatthe context for each occurrence is composed ofwords that are linked to that occurrence in a de-pendency parse.
Therefore, the model is similarto a bag of words model, but does contain somesyntactic weighting.
To contextualize a vector, themodel weights the components of the uncontextu-alized vector with the components of the contextvector, using the formulav(w, c) =?w??W?
(c, w?)f(w,w?
)~ew?where w is the target word, c is the context, Wis the set of words, ?
is the cosine similarity ofc and w?, f is a co-occurrence function, and ~ew?is a basis vector.
Positive pmi-weighting was alsoapplied to the vectors.4.3 ImplementationWe adopted the four-step classification procedurefrom Zhao and Lin (2004).
At each step for eachtest quadruple, the training examples are sortedby a different vector composition method, a set ofbest examples is considered, and if these examplescast equal votes for noun-attach and verb-attach,the algorithm moves to the next step.
Otherwise,the class with the greatest number of votes is as-signed to the test quadruple.1.
Consider only the training examples forwhich all four words are equal to those in thetest quadruple.2.
Consider the k highest (k experimentally de-termined) scoring examples, with the samepreposition as the test quadruple, using thecomposition functionsim(q1, q2) = vn1+ vn2+ n1n2where v, n1, and n2are the vector similaritiesof the V , N1, and N2pairs.3.
Same as (2), except using the functionsim(q1, q2) = v + n1+ n24.
Assign default class for the preposition (lastresort), or noun-attach if there is no defaultclass.4.4 Similarity measuresWe implemented four similarity measures.
(1)abs: absolute word similarity, which gives 1 if thetokens are identical, 0 otherwise.
(2) noctxt: co-sine similarity using uncontextualized word vec-tors.
(3) ctxtquad: cosine similarity using wordvectors contextualized by the quadruple words.
(4)ctxtsent: cosine similarity using word vectors con-textualized by words from the full sentence.755 ExperimentationWe set the k values by using five-fold cross-validation on the training quadruples.
Then, forintermediate numerical checks, we tested the sys-tems on the development quadruples.
The figuresin the next section are the result of a single run ofthe final trained systems on the test quadruples.6 ResultsTable 1 presents results from our binary classifierusing the different similarity measures.
Table 2compares our best binary classifier accuracy (us-ing ctxtquad) to other systems.
Table 3 shows thenumber, percentage, and accuracy of decisions bystep in the classification procedure for the ctxtquadrun.Similarity measure k value Accuracyabs 3 80.2%noctxt 11 86.6%ctxtquad10 88.4%ctxtsent8 81.9%Table 1: Similarity measure performance compar-ison.Method Sense handling AccuracyBR1994 All senses equal 81.8%PL2000 Global frequency 84.3%ZL2004 Global frequency 86.5%SN1997 Full WSD 88.1%Our system Context weighting 88.4%G2013 Full WSD 89.0%Table 2: Leading PP-attachment disambiguationsystems.Step Coverage Coverage % Accuracy1 244 7.88% 91.8%2 2849 91.99% 88.1%3 0 0.00% N/A4 4 0.13% 100.0%Table 3: Coverage and accuracy for classificationprocedure steps, using ctxtquad.7 DiscussionThe results above show that contextualizingthe word vectors, which is meant to implic-itly represent sense information, can statistically-significantly boost performance on PP-attachmentdisambiguation by 1.8% (?2= 4.31, p < 0.04) onan already quite accurate system.
We can see thatusing the full sentence as context, while helpfulfor human judgment, is not effective in this sys-tem because there are not enough examples in theknowledge base for reliable statistics.
It seems asthough too much context obscures generalizationsotherwise captured by the system.Nominal increases in accuracy aside, this sys-tem uses only a knowledge base that is not spe-cific to the task of PP-attachment disambiguation.We obtained highly accurate results without utiliz-ing task-specific resources, such as sense invento-ries, or performing labor-intensive modificationsto training data.
Since systems with full WSDmodules would likely require both of these, thisimplicit handling of sense information seems moreelegant.8 ConclusionThis paper describes a PP-attachment disambigua-tion system that owes its high performance to cap-turing sense information in contextualized distri-butional data.
We see that this implicit handling ispreferable to having no sense handling and also tohaving a full WSD module as part of a pipeline.In future work, we would like to investigatehow to systematically extract contexts beyond thequadruple, such as sentences or full documents,while maintaining the information captured in lesscontextualized vectors.
Perhaps there are certainparticularly informative positions whose wordswould positively affect the vectors.
Given thatwords tend to maintain the same sense within adocument, it is a particularly well-suited contextto consider.
However, care must be taken to min-imize unwanted sense mixing, combat data spar-sity, and restrict the number of similarity compar-isons for efficiency.AcknowledgmentsWe owe sincerest thanks to Prof. Dr. ManfredPinkal and Dr. Stefan Thater for initial directionand providing the vector space model used in oursystem.
Also, we thank Google for travel and con-ference support.76ReferencesSteven Abney, Robert E. Schapire and Yoram Singer.1999.
Boosting Applied to Tagging and PP-attachment.
In Proceedings of the Joint SIG-DAT Conference on Empirical Methods in Natu-ral Language Processing and Very Large Corpora,EMNLP-VLC, College Park, MD.
pp.
38?45.Steven Bird, Edward Loper and Ewan Klein.2009.
Natural Language Processing with Python.O?Reilly Media, Inc.Eric Brill and Philip Resnik.
1994.
A rule-basedapproach to prepositional phrase attachment disam-biguation.
In 5th International Conference on Com-putational Linguistics (COLING94), Kyoto, Japan.Michael Collins and James Brooks.
1995.
Prepo-sitional Attachment through a Backed-off Model.In David Yarovsky and Kenneth Church (ed.
), Pro-ceedings of the Third Workshop on Very Large Cor-pora, Somerset, New Jersey, Association for Com-putational Linguistics.
pp.
27?38.Georgiana Dinu and Stefan Thater.
2012.
Saarland:vector-based models of semantic textual similarity.In First Joint Conference on Lexical and Computa-tional Semantics (*SEM), Montr?eal.
pp.
603?607.Christiane Fellbaum (ed.)
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Lyn Frazier.
1979.
On Comprehending Sentences:Syntactic Parsing Techniques.
Unpublished doctoraldissertation, University of Connecticut.Robert Freidin.
1992.
Foundations of generative syn-tax.
MIT Press.Clayton Greenberg.
2013.
Disambiguating preposi-tional phrase attachment sites with graded seman-tic data or, how to rule out elephants in pajamas.Unpublished undergraduate thesis, Princeton Uni-versity.Donald Hindle and Mats Rooth.
1993.
Structural Am-biguity and Lexical Relations.
In Meeting of theAssociation for Computational Linguistics.
pp.
229?236.Kailash Nadh and Christian Huyck.
2012.
A neuro-computational approach to prepositional phrase at-tachment ambiguity resolution.
Neural Computa-tion, 24(7): pp.
1906?1925.Preslav Nakov and Marti Hearst.
2005.
Using the Webas an Implicit Training Set: Application to Struc-tural Ambiguity Resolution.
In Proceedings of HLT-NAACL.Patrick Pantel and Dekang Lin.
2000.
An unsuper-vised approach to prepositional phrase attachmentusing contextually similar words.
In Proceedingsof the 38th Annual Meeting of the Association forComputational Linguistics.
pp.
101?108.Adwait Ratnaparkhi, Jeff Reynar and Salim Roukos.1994.
A Maximum Entropy Model for PrepositionalPhrase Attachment.
In Proceedings of the ARPAHu-man Language Technology Workshop, Plainsboro,NJ.
pp.
250?255.Jiri Stetina and Makoto Nagao.
1997.
Corpus BasedPP Attachment Ambiguity Resolution with a Se-mantic Dictionary.
In Proceedings of the Fifth Work-shop on Very Large Corpora, Beijing and HongKong.
pp.
66?80.Shaojun Zhao Dekang Lin.
2004.
Corpus Based PPAttachment Ambiguity Resolution with a SemanticDictionary.
In Proceedings of the First InternationalJoint Conference on Natural Language Processing,Sanya, China.77
