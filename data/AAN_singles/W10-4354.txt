Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 281?288,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsF2 ?
New Technique for Recognition of User Emotional States inSpoken Dialogue SystemsRam?n L?pez-C?zarDept.
of Languages andComputer Systems, CTIC-UGR, University of Granada,Spainrlopezc@ugr.esJan SilovskyInstitute of InformationTechnology and Electronics,Technical University ofLiberec, Czech Republicjan.silovsky@tul.czDavid GriolDept.
of Computer ScienceCarlos III University ofMadrid, Spaindgriol@inf.uc3m.esAbstractIn this paper we propose a new technique toenhance emotion recognition by combiningin different ways what we call emotion pre-dictions.
The technique is called F2 as thecombination is based on a double fusionprocess.
The input to the first fusion phase isthe output of a number of classifiers whichdeal with different types of information re-garding each sentence uttered by the user.The output of this process is the input to thesecond fusion stage, which provides as out-put the most likely emotional category.
Ex-periments have been carried out using a pre-viously-developed spoken dialogue systemdesigned for the fast food domain.
Resultsobtained considering three and two emo-tional categories show that our techniqueoutperforms the standard single fusion tech-nique by 2.25% and 3.35% absolute, respec-tively.1 IntroductionAutomatic recognition of user emotional statesis a very challenging task that has attracted theattention of the research community for severaldecades.
The goal is to design methods tomake computers interact more naturally withhuman beings.
This is a very complex task dueto a variety of reasons.
One is the absence of agenerally agreed definition of emotion and ofqualitatively different types of emotion.
An-other is that we still have an incomplete under-standing of how humans process emotions, aseven people have difficulty in distinguishingbetween them.
Thus, in many cases a givenemotion is perceived differently by differentpeople.Studies in emotion recognition made by theresearch community have been applied to en-hance the quality or efficiency of several ser-vices provided by computers.
For example,these have been applied to spoken dialoguesystems (SDSs) used in automated call-centres,where the goal is to detect problems in the in-teraction and, if appropriate, transfer the callautomatically to a human operator.The remainder of the paper is organised asfollows.
Section  2 addresses related work onthe application of emotion recognition toSDSs.
Section  3 focuses on the proposed tech-nique, describing the classifiers and fusionmethods employed in the current implementa-tion.
Section  4 discusses our speech databaseand its emotional annotation.
Section  5 pre-sents the experiments, comparing results ob-tained using the standard single fusion tech-nique with the proposed double fusion.
Finally,Section  6 presents the conclusions and outlinespossibilities for future work.2 Related workMany studies can be found in the literatureaddressing potential improvements to SDSs byrecognising user emotional states.
A diversityof speech databases, features used for trainingand recognition, number of emotional catego-ries, and recognition methods have been pro-posed.
For example, Batliner et al (2003) em-ployed three different databases to detect trou-bles in communication.
One was collectedfrom a single experienced actor who was toldto express anger because of system malfunc-tions.
Other was collected from naive speakerswho were asked to read neutral and emotionalsentences.
The third database was collectedusing a WOZ scenario designed to deliberatelyprovoke user reactions to system malfunctions.The study focused on detecting two emotioncategories: emotional (e.g.
anger) and neutral,employing classifiers that dealt with prosodic,linguistic, and discourse information.281Liscombe et al (2005) made experimentswith a corpus of 5,690 dialogues collected withthe ?How May I Help You?
system, and con-sidered seven emotional categories: posi-tive/neutral, somewhat frustrated, very frus-trated, somewhat angry, very angry, somewhatother negative, and very other negative.
Theyemployed standard lexical, prosodic and con-textual features.Devillers and Vidrascu (2006) employedhuman-to-human dialogues on a financial task,and considered four emotional categories: an-ger, fear, relief and sadness.
Emotion classifi-cation was carried out considering linguisticinformation and paralinguistic cues.Ai et al (2006) used a database collectedfrom 100 dialogues between 20 students and aspoken dialogue tutor, and for classificationemployed lexical items, prosody, user gender,beginning and ending time of turns, user turnsin the dialogue, and system/user performancefeatures.
Four emotional categories were con-sidered: uncertain, certain, mixed and neutral.Morrison et al (2007) compared two emo-tional speech data sources The former was col-lected from a call-centre in which customerstalked directly to a customer service represen-tative.
The second database was collected from12 non-professional actors and actresses whosimulated six emotional categories: anger, dis-gust, fear, happiness, sadness and surprise.3 The proposed techniqueThe technique that we propose in this paper toenhance emotion recognition in SDSs consid-ers that a set of classifiers ?
= {C1, C2, ?, Cm}receive as input feature vectors f related toeach sentence uttered by the user.
As a result,each classifier generates one emotion predic-tion, which is a vector of pairs (hi, pi), i = 1?S,where hi is an emotional category (e.g.
Angry),pi is the probability of the utterance belongingto hi in accordance with the classifier, and S isthe number of emotional categories considered,which forms the set E = {e1, e2, ?, eS}.The emotion predictions generated by theclassifiers make up the input to the first fusionstage, which we call Fusion-0.
This stage em-ploys n fusion methods called F0i, i = 1?n, togenerate other predictions: vectors of pairs(h0j,k , p0j,k), j = 1?n, k = 1?S, where h0j,k is anemotional category, and p0j,k is the probabilityof the utterance belonging to h0j,k in accordancewith the fusion method F0j.The second fusion stage, called Fusion-1,receives the predictions provided by Fusion-0and generates the pair (h11,1 , p11,1), where h11,1is the emotional category with highest prob-ability, p11,1.
This emotional category is deter-mined employing a fusion method called F11,and represents the user?s emotional state de-duced by the technique.
The best combinationof fusion methods to be used in Fusion-0 (F01,F02,...,F0j, 1 ?
j ?
n) and the best fusion methodto be used in Fusion-1 (F11) must be experi-mentally determined.3.1 ClassifiersIn the current implementation our techniqueemploys four classifiers, which deal with pros-ody, acoustics, lexical items and dialogue actsregarding each utterance.3.1.1 Prosodic classifierThe input to our prosodic classifier is an n-dimensional feature vector obtained fromglobal statistics of pitch and energy, and fea-tures derived from the duration ofvoiced/unvoiced segments in each utterance.After carrying out experiments to find the ap-propriate feature set for the classifier, we de-cided to use the following 11 features: pitchmean, minimum and maximum, pitch deriva-tives mean, mean and variance of absolute val-ues of pitch derivatives, energy maximum,mean of absolute value of energy derivatives,correlation of pitch and energy derivatives,average length of voiced segments, and dura-tion of longest monotonous segment.The classifier employs gender-dependentGaussian Mixture Models (GMMs) to repre-sent emotional categories.
The likelihood forthe n-dimensional feature vector (x), given anemotional category ?, is defined as:( ) ( )?==Qlll xPwxP1?i.e., a weighted linear combination of Q uni-modal Gaussian densities Pl(x).
The densityfunction Pl(x) is defined as:( ) ( ) ( ) ( )??????
?????
?= ?
llllnl xxxP ??
?121expdet21where the ?l?s are mean vectors and the ?l?scovariance matrices.
The emotional category282deduced by the classifier, h, is decided accord-ing to the following expression:( )SSxPh ?maxarg=  (1)where ?S represents the models for the emo-tional categories considered, and the max func-tion is computed employing the EM (Expecta-tion-Maximization) algorithm.
To compute theprobabilities pi for the emotion prediction ofthe classifier we use the following expression:?==Skkiip1/ ??
(2)where ?i is the log-likelihood of hi, S is thenumber of emotional categories considered,and the ?k?s are the log-likelihoods of theseemotional categories.3.1.2 Acoustic classifierProsodic features are nowadays among themost popular features for emotion recognition(Dellaert et al 1996; Luengo et al 2005).However, several authors have evaluated otherfeatures.
For example, Nwe et al (2003) em-ployed several short-term spectral features andobserved that Logarithmic Frequency PowerCoefficients (LFPCs) provide better perform-ance than Mel-Frequency Cepstral Coefficient(MFCCs) or Linear Prediction Cepstral Coeffi-cients (LPCCs).
Experiments carried out withour speech database (which will be discussedin Section  4) have confirmed this observation.However, we have also noted that when weused the first and second derivatives, the bestresults were obtained for MFCCs.
Hence, wedecided to use 39-feature MFCCs (13 MFCCs,delta and delta-delta) for classification.The emotion patterns of the input utterancesare modelled by gender-dependent GMMs, aswith the prosodic classifier, but each input ut-terance is represented employing a sequence offeature vectors x = {x1,?,xT} instead of one n-dimensional vector.
We assume mutual inde-pendence of the feature vectors in x, and com-pute the log-likelihood for an emotional cate-gory ?
as follows:( ) ( )?==TttxPxP1log ?
?The emotional category deduced by the classi-fier, h, is decided employing Eq.
(1), whereasEq.
(2) is used to compute the probabilities forthe prediction, i.e.
for the vector of pairs (hi,pi).3.1.3 Lexical classifierA number of previous studies on emotion rec-ognition take into account information aboutthe kinds of word uttered by the users, assum-ing that there is a relationship between wordsand emotion categories.
For example, swearwords and insults can be considered as convey-ing a negative emotion (Lee and Narayanan,2005).
Analysis of our dialogue corpus (whichwill be discussed in Section  4) has shown thatusers did not utter swear words or insults dur-ing the interaction with the Saplen system.Nevertheless, there were particular moments inthe interaction at which their emotional statechanged from Neutral to Tired or Angry.
Thesemoments correspond to dialogue states wherethe system had problems in recognising thesentences uttered by the users.The reasons for these problems are basicallytwo.
On the one hand, most users spoke withstrong southern Spanish accents, characterisedby the deletion of the final s of plural words,and an exchange of the phonemes s and c inmany words.
On the other hand, there arewords in the system?s vocabulary that are verysimilar acoustically.Hence, our goal has been to automaticallyfind these words by means of a study of thespeech recognition results, and deduce theemotional category for each input utterancefrom the emotional information associatedwith the words in the recognition result.
To dothis we have followed the study of Lee andNarayanan (2005), which employs the infor-mation-theoretic concept of emotional sali-ence.
The emotional salience of a word for agiven emotional category can be defined as themutual information between the word and theemotional category.
Let W be a sentence(speech recognition result) comprised of a se-quence of n words: W = w1 w2 ?wn, and E aset of emotional categories, E = {e1, e2, ?
,eS}.The mutual information between the word wiand an emotional category ej is defined as fol-lows:( ))(|log),(_jijji ePwePewnInformatiomutual =283where P(ej | wi) is the posterior probability thata sentence containing the word wi implies theemotional category ej, and P(ej) represents theprior probability of the emotional category.Taking into account the previous definitions,we have defined the emotional salience of theword wi for an emotional category ej as fol-lows:),(_)|(),(jiijjiewnInformatiomutualwePewsalience?=After the salient words for each emotionalcategory have been identified employing atraining corpus, we can carry out emotion rec-ognition at the sentence level, considering thateach word in the sentence is independent of therest.
The goal is to map the sentence W to anyof the emotional categories in E. To do this, wecompute an activation value ak for each emo-tional category as follows:?=+=nmkmkmk wwIa1where k = 1?S, n is the number of words inW, Im represents an indicator that has the value1 if wk is a salient word for the emotional cate-gory (i.e.
salience(wi,ej) ?
0) and the value 0otherwise; wmk is the connection weight be-tween the word and the emotional category,and wk represents bias.
We define the connec-tion weight wmk as:),(_ kmmk ewnInformatiomutualw =whereas the bias is computed as:)(log kk ePw = .
Finally, the emotional cate-gory deduced by the classifier, h, is the onewith highest activation value ak:)max(arg kkah =To compute the probabilities pi?s for the emo-tion prediction, we use the following expres-sion:?==Sjjii aap1/where ai represents the activation value of hi,and the aj?s are the activation values of the Semotional categories considered.3.1.4 Dialogue acts classifierA dialogue act can be defined as the functionperformed by an utterance within the contextof a dialogue, for example, greeting, closing,suggestion, rejection, repeat, rephrase, confir-mation, specification, disambiguation, or help(Batliner et al 2003; Lee and Narayanan,2005; Liscombe et al 2005).Our dialogue acts classifier is inspired bythe study of Liscombe et al (2005), where thesequential structure of each dialogue is mod-elled by a sequence of dialogue acts.
A differ-ence is that they assigned one or more labelsrelated to dialogue acts to each user utterance,and did not assign labels to system prompts,whereas we assign just one label to each sys-tem prompt and none to user utterances.
Thisdecision is made from the examination of ourdialogue corpus.
We have observed that usersgot tired or angry if the system generated thesame prompt repeatedly (i.e.
repeated the samedialogue act) to try to get a particular dataitem.
For example, if it had difficulty in obtain-ing a telephone number then it employed sev-eral dialogue turns to obtain the number andconfirm it, which annoyed the users, especiallyif they had employed other turns previously tocorrect misunderstandings.
Hence, our dia-logue act classifier aims to predict these nega-tive emotional states by detecting successiverepetitions of the same system?s prompt types(e.g.
prompts to get the telephone number).In accordance with our approach, the emo-tional category of a user?s dialogue turn, En, isthat which maximises the posterior probabilitygiven a sequence of the most recent systemprompts:),...,,|(maxarg 13)12*( ???
?= nnLnkkn DADADAEPEwhere the prompt sequence is represented by asequence of dialogue acts (DAi?s) and L is thelength of the sequence, i.e.
the number of sys-tem?s dialogue turns in the sequence.
Note thatif L = 1 then the decision about En dependsonly on the previous system prompt.
In otherwords, the emotional category obtained is thatwith the greatest probability given just the pre-vious system turn in the dialogue.
The prob-ability of the considered emotional categories284given a sequence of dialogue acts is obtainedby employing a training dialogue corpus.By means of this equation, we decide themost likely emotional category for the inpututterance, selecting the category with the high-est probability given the sequence of dialogueacts of length L. This probability is used tocreate the pair (hi, pi) to be included in theemotion prediction.3.2 Fusion methodsIn the current implementation our techniqueemploys the three fusion methods discussed inthis section.
When used in Fusion-0, thesemethods are employed to combine the predic-tions provided by the classifiers.
When used inFusion-1, they are used to combine the predic-tions generated by Fusion-0.3.2.1 Average of probabilities (AP)This method combines the predictions by aver-aging their probabilities.
To do this we con-sider that each input utterance is representedby feature vectors x1,?,xm from feature spacesX1,?,Xm, where m is the number of classifiers.We also assume that each input utterance be-longs to one of S emotional categories hi, i =1?S.
In each of the m feature spaces a classi-fier can be created that approximates the poste-rior probability P(hi | xk) as follows:)()|()( kkikikki xxhPxf ?+=where )( kki x?
is the error made by classifierk.
We estimate P(hi | xk) by )( kki xf  and as-suming a zero-mean error for )( kki x?
, we av-erage all the )( kki xf ?s to obtain a less error-sensitive estimation.
In this way we obtain thefollowing mean combination rule to decide themost likely emotional category:?==mkkkimi xfmxxhP11 )(1),...,|(3.2.2 Multiplication of probabilities (MP)Assuming that the feature spaces X1,?,Xm aredifferent and independent, the probabilities canbe written as follows:)|(...)|()|()|,...,(211imiiimhxPhxPhxPhxxP??
?=Using Bayes rule we can obtain the followingequation, which we use to decide the mostlikely emotional category for each input utter-ance (represented as feature vectors x1,?,xm):?
???????
?= ??
'1''''11)(/)|()(/)|(),...,|(imikkikmikimihPxhPhPxhPxxhP3.2.3 Unweighted vote (UV)This method combines the emotion predictionsby counting the number of classifiers (if usedin Fusion-0) or fusion methods (if used in Fu-sion-1) that consider an emotional category hias the most likely for the input utterance.
If weconsider three emotional categories X, Y and Z,hi is decided as follows:???????????????=?
?
?
??
?
???
?
?
?= = = == = === = ==mjmjmjmjjjijmjmjmjjjmjjjmjmjmjjjmjjjiYZandXZifZZYandXYifYZXandYXifXh1 1 1 11 1 111 1 11where m is the number of classifiers or fusionmethods employed (e.g., in our experiments, X= Neutral, Y = Tired and Z = Angry).
Theprobability pi for hi to be included in the emo-tion prediction is computed as follows:?==31/),,|(jjii VhVhZYXhPwhere Vhi is the number of votes for hi, and theVhj?s are the number of votes for the 3 emo-tional categories.
If we consider two emotionalcategories X and Y, the most likely emotionalcategory hi and its probability pi are analo-gously computed (e.g., in our experiments, X =Non-negative and Y = Negative).4 Emotional speech databaseOur emotional speech database has been con-structed from a corpus of 440 telephone-baseddialogues between students of the Universityof Granada and the Saplen system, which was285previously developed in our lab for the fastfood domain (L?pez-C?zar et al 1997; L?pez-C?zar and Callejas, 2006).
Each dialogue wasstored in a log file in text format that includeseach system prompt (e.g.
Would you like todrink anything?
), the type of prompt (e.g.
Any-FoodOrDrinkToOrder?
), the name of the voicesamples file (utterance) that stores the user re-sponse to the prompt, and the speech recogni-tion result for the utterance.
The dialogue cor-pus contains 7,923 utterances, 50.3% of whichwere recorded by male users and the remainingby female users.The utterances have been annotated by 4 la-bellers (2 male and 2 female).
The order of theutterances has been randomly chosen to avoidinfluencing the labellers by the situation in thedialogues, thus minimising the effect of dis-course context.
The labellers have initially as-signed one label to each utterance, either<NEUTRAL>, <TIRED> or <ANGRY> accordingto the perceived emotional state of the user.One of these labels has been finally assigned toeach utterance according to the majority opin-ion of the labellers, so that 81% of the utter-ances are annotated as ?Neutral?, 9.5% as?Tired?
and 9.4% as ?Angry?.
This shows thatthe database is clearly unbalanced in terms ofemotional categories.To measure the amount of agreement be-tween the labellers we employed the Kappastatistic (K), which is computed as follows(Cohen, 1960):)(1)()(EPEPAPK ?
?=where P(A) is the proportion of times that thelabellers agree, and P(E) is the proportion oftimes we would expect the labellers to agree bychance.
We obtained that K = 0.48 and K =0.45 for male and female labellers, respec-tively, which according to Landis and Koch(1977) represents moderate agreement.5 ExperimentsThe main goal of the experiments has been totest the proposed technique using our emo-tional speech database, and employing:?
Three emotional categories (Neutral, An-gry and Tired) on the one hand, and twoemotional categories (Non-negative andNegative) on the other.
The experimentsemploying the former category set will becalled 3-emotion experiments, whereasthose employing the latter category will becalled 2-emotion experiments.?
The four classifiers described in Section3.1, and the three fusion methods dis-cussed in Section  3.2.In the 3-emotion experiments we consider thatan input utterance is correctly classified if theemotional category deduced by the techniquematches the label assigned to the utterance.
Inthe 2-emotion experiments, the utterance isconsidered to be correctly classified if eitherthe deduced emotional category is Non-negative and the label is Neutral, or the cate-gory is Negative and the label is Tired or An-gry.To carry out training and testing we haveused a script that takes as its input a set of la-belled dialogues in a corpus, and processeseach dialogue by locating within it, from thebeginning to the end, each prompt of theSaplen system, the voice samples file that con-tains the user?s response to the prompt, and theresult provided by the system?s speech recog-niser (sentence in text format).
The type ofeach prompt is used to create a sequence ofdialogue acts of length L, which is the input tothe dialogue acts classifier.
The voice samplesfile is the input to the prosodic and acousticclassifiers, and the speech recognition result isthe input to the lexical classifier.
This proce-dure is repeated for all the dialogues in the cor-pus.Experimental results have been obtained us-ing 5-fold cross-validation, with each partitioncontaining the utterances corresponding to 88different dialogues in the corpus.5.1 Performance of Fusion-0Table 1 sets out the average results obtainedfor Fusion-0 considering several combinationsof the classifiers and employing the three fu-sion methods.
As can be observed, MP is thebest fusion method, with average classificationrates of 89.08% and 87.43% for the 2 and 3emotion experiments, respectively.
The bestclassification rates (92.23% and 90.67%) areobtained by employing the four classifiers,which  means that the four types of informa-tion considered (acoustic, prosodic, lexical andrelated to dialogue acts) are really useful toenhance classification rates.286FusionMethodClassifiers 2 emot.
3 emot.Aco, Pro 84.15 82.46Lex, Pro 85.04 82.71DA, Pro 90.49 87.48Aco, Lex, Pro 89.20 86.17Aco, DA, Pro 90.24 88.56DA, Lex, Pro 90.02 88.02Aco, DA, Lex, Pro 90.49 88.32APAverage 88.66 86.25Aco, Pro 84.15 82.86Lex, Pro 85.16 83.71DA, Pro 91.49 89.78Aco, Lex, Pro 89.17 87.91Aco, DA, Pro 91.33 89.23DA, Lex, Pro 90.06 87.82Aco, DA, Lex, Pro 92.23 90.67MPAverage 89.08 87.43Aco, Pro 88.64 85.19Lex, Pro 86.40 83.01DA, Pro 88.20 84.92Aco, Lex, Pro 88.76 85.54Aco, DA, Pro 88.91 85.89DA, Lex, Pro 88.47 85.61Aco, DA, Lex, Pro 89.04 87.56UVAverage 88.35 85.39Table 1: Performance of Fusion-0 (results in %).5.2 Performance of Fusion-1Table 2 shows the average results obtainedwhen Fusion-1 is used to combine the predic-tions of Fusion-0.
The three fusion methods aretested in Fusion-1, with Fusion-0 employingfour combinations of these methods: AP,MP;AP,UV; MP,UV; and AP,MP,UV.
In all casesFusion-0 uses the four classifiers as this is theconfiguration that provides the highest classifi-cation accuracy according to the previous sec-tion.Comparison of both tables shows that Fu-sion-1 clearly outperforms Fusion-0.
The bestresults are attained for MP, which means thatthis method is preferable when the data containsmall errors (emotion predictions generated byFusion-0 with accuracy rates around 90%).To find the reasons for these enhancementswe have analysed the confusion matrix of Fu-sion-1 using MP.
The study reveals that for the2-emotion experiments this fusion stage worksvery well in predicting the Non-negative cate-gory, very slightly enhancing the classificationrate of Fusion-0 (96.58% vs. 95.93%), whereasthe classification rate of the Negative categoryis the same as that obtained by Fusion-0(88.91%).
Overall, the best performance ofFusion-1 employing MP (94.48%) outdoes thatof Fusion-0 employing AP (90.49%) and MP(92.23%).Regarding the 3-emotion experiments, ouranalysis shows that using MP, Fusion-1slightly lowers the classification rate of theNeutral category obtained by Fusion-0(97.79% vs. 97.9%), but slightly raises the rateof the Tired category (93.62% vs. 93.26%),and the Angry category (77.49% vs. 76.81%).Overall, the performance of Fusion-1 employ-ing MP (94.02%) outdoes that of Fusion-0 em-ploying AP (88.32%) and MP (90.67%).Fusion methodsused in Fusion-0Fusion methodused in Fusion-1(2 emotions)Fusion methodused in Fusion-1(3 emotions)AP MP UV AP MP UVAP,MP 93.68 94.48 93.53 91.77 94.02 90.96AP,UV 93.20 93.23 93.20 91.65 93.13 90.10MP,UV 93.34 94.38 93.20 91.27 93.98 89.48AP,MP,UV 93.23 94.36 93.17 91.57 93.97 89.06Average 93.40 94.11 93.28 91.57 93.78 89.90Table 2: Performance of Fusion-1 (results in %).6 Conclusions and future workOur experimental results show that the pro-posed technique is useful to improve the classi-fication rates of the standard fusion technique,which employs just one fusion stage.
Compar-ing results in Table 1 and Table 2 we can ob-serve that for the 2-emotion experiments, Fu-sion-1 enhances Fusion-0 by 2.25% absolute(from 92.23% to 94.48%), while for the 3-emotion experiments, the improvement is3.35% absolute (from 90.67% to 94.02%).These improvements are obtained by employ-ing AP and MP in Fusion-0 to combine theemotion predictions of the four classifiers, andusing MP in Fusion-1 to combine the outputsof Fusion-0.The reason for these improvements is thatthe double fusion process (Fusion-0 and Fu-sion-1) allows us to benefit from the advan-287tages of using different methods to combineinformation.
According to our results, the bestmethods are AP and MP.
The former allowsgaining maximally from the independent datarepresentation available, which are the input toFusion-0 (in our study, prosody, acoustics,speech recognition errors, and dialogue acts).The latter provides better results when the datacontain small errors, which occurs when thepredictions provided by Fusion-0 are the inputto Fusion-1.Future work will include testing the tech-nique employing information sources not con-sidered in this study.
The sources we havedealt with in the experiments (prosodic, acous-tic, lexical, and dialogue acts) are those mostcommonly employed in previous studies.However, there are also studies that suggestusing other information sources, such as speak-ing style, subject and problem identification,and non-verbal cues.Another future work is to test the techniqueemploying other methods for classification andinformation fusion.
For example, it is knownthat people are usually confused when they tryto determine the emotional state of a speaker,given that the difference between some emo-tions is not always clear.
Hence, it would beinteresting to investigate the performance ofthe technique employing classification algo-rithms that deal with this vague boundary, suchas fuzzy inference methods, and using boostingmethods for improving the accuracy of theclassifiers.Finally, in terms of application of the tech-nique to improve the system-user interaction,we will evaluate different dialogue manage-ment strategies to enable the system?s adapta-tion to negative emotional states of users (Uni-versity students).
For example, a dialoguemanagement strategy could be as follows: i) ifthe emotional state is Tired begin the followingprompt apologising, and transfer the call to ahuman operator if this state is recognised twiceconsecutively, and ii) if the emotional state isAngry apologise and transfer the call to a hu-man operator immediately.AcknowledgmentsThis research has been funded by Spanish pro-ject HADA TIN2007-64718, and the CzechGrant Agency project no.
102/08/0707.ReferencesAi, H., Litman, D. J., Forbes-Riley, K., Rotaru, M.,Tetreault, J., Purandare, A.
2006.
Using systemand user performance features to improve emo-tion detection in spoken tutoring systems.
Proc.of Interspeech, pp.
797-800.Batliner, A., Fischer, K., Huber, R., Spilker, J.,N?th, E. 2003.
How to find trouble in communi-cation.
Speech Communication, vol.
40, pp.
117-143.Cohen, J.
1960.
A coefficient of agreement fornominal scales.
Educat.
Psychol.
Measurement,vol.
20, pp.
37-46.Dellaert, F., Polzin, T., Waibel, A.
1996.
Recogniz-ing emotion in speech.
Proc.
of ICSLP, pp.1970-1973.Devillers, L., Vidrascu, L. 2006.
Real-life emotionsdetection with lexical and paralinguistic cues onhuman-human call center dialogs.
Proc.
of Inter-speech, pp.
801-804.Landis, J. R., Koch, G. G. 1977.
The measurementof observer agreement for categorical data.
Bio-metrics, vol.
33, pp.
159-174.Lee, C. M., Narayanan, S. S. 2005.
Toward detect-ing emotions in spoken dialogs.
IEEE Transac-tions on Speech and Audio Processing, vol.13(2), pp.
293-303.Liscombe, J., Riccardi, G., Hakkani-T?r, D. 2005.Using context to improve emotion detection inspoken dialogue systems.
Proc.
of Interspeech,pp.
1845-1848.L?pez-C?zar, R., Garc?a, P., D?az, J., Rubio, A. J.1997.
A voice activated dialog system for fast-food restaurant applications.
Proc.
of Eu-rospeech, pp.
1783-1786.L?pez-C?zar, R., Callejas, Z.
2006.
CombiningLanguage Models in the Input Interface of aSpoken Dialogue System.
Computer Speech andLanguage, 20, pp.
420-440.Luengo, I., Navas, E., Hern?ez, I, Sanchez, J.
2005.Automatic emotion recognition using prosodicparameters.
Proc.
of Interspeech, pp.493-496.Morrison, D., Wang, R., De Silva, L. C. 2007.
En-semble methods for spoken emotion recognitionin call-centres.
Speech Communication, vol.49(2) pp.
98-112.Nwe, T. L., Foo, S. V., De Silva, L. C. 2003.Speech emotion recognition using hiddenMarkov models.
Speech Communication, vol.41(4), pp.
603-623.288
