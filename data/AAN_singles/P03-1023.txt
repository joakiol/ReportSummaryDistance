Coreference Resolution Using Competition Learning ApproachXiaofeng Yang*+ Guodong Zhou*  Jian Su* Chew Lim Tan +*Institute for Infocomm Research,21 Heng Mui Keng Terrace,Singapore 119613+Department of Computer Science,National University of Singapore,Singapore 117543*{xiaofengy,zhougd,sujian}@i2r.a-star.edu.sg+(yangxiao,tancl)@comp.nus.edu.sgAbstractIn this paper we propose a competitionlearning approach to coreference resolu-tion.
Traditionally, supervised machinelearning approaches adopt the single-candidate model.
Nevertheless the prefer-ence relationship between the antecedentcandidates cannot be determined accu-rately in this model.
By contrast, our ap-proach adopts a twin-candidate learningmodel.
Such a model can present thecompetition criterion for antecedent can-didates reliably, and ensure that the mostpreferred candidate is selected.
Further-more, our approach applies a candidatefilter to reduce the computational cost anddata noises during training and resolution.The experimental results on MUC-6 andMUC-7 data set show that our approachcan outperform those based on the single-candidate model.1 IntroductionCoreference resolution is the process of linkingtogether multiple expressions of a given entity.
Thekey to solve this problem is to determine the ante-cedent for each referring expression in a document.In coreference resolution, it is common that twoor more candidates compete to be the antecedent ofan anaphor (Mitkov, 1999).
Whether a candidate iscoreferential to an anaphor is often determined bythe competition among all the candidates.
So far,various algorithms have been proposed to deter-mine the preference relationship between two can-didates.
Mitkov?s knowledge-poor pronounresolution method (Mitkov, 1998), for example,uses the scores from a set of antecedent indicatorsto rank the candidates.
And centering algorithms(Brennan et al, 1987; Strube, 1998; Tetreault,2001), sort the antecedent candidates based on theranking of the forward-looking or backward-looking centers.In recent years, supervised machine learningapproaches have been widely used in coreferenceresolution (Aone and Bennett, 1995; McCarthy,1996; Soon et al, 2001; Ng and Cardie, 2002a),and have achieved significant success.
Normally,these approaches adopt a single-candidate model inwhich the classifier judges whether an antecedentcandidate is coreferential to an anaphor with a con-fidence value.
The confidence values are generallyused as the competition criterion for the antecedentcandidates.
For example, the ?Best-First?
selectionalgorithms (Aone and Bennett, 1995; Ng andCardie, 2002a) link the anaphor to the candidatewith the maximal confidence value (above 0.5).One problem of the single-candidate model,however, is that it only takes into account the rela-tionships between an anaphor and one individualcandidate at a time, and overlooks the preferencerelationship between candidates.
Consequently, theconfidence values cannot accurately represent thetrue competition criterion for the candidates.In this paper, we present a competition learningapproach to coreference resolution.
Motivated bythe research work by Connolly et al (1997), ourapproach adopts a twin-candidate model to directlylearn the competition criterion for the antecedentcandidates.
In such a model, a classifier is trainedbased on the instances formed by an anaphor and apair of its antecedent candidates.
The classifier isthen used to determine the preference between anytwo candidates of an anaphor encountered in a newdocument.
The candidate that wins the most com-parisons is selected as the antecedent.
In order toreduce the computational cost and data noises, ourapproach also employs a candidate filter to elimi-nate the invalid or irrelevant candidates.The layout of this paper is as follows.
Section 2briefly describes the single-candidate model andanalyzes its limitation.
Section 3 proposes in de-tails the twin-candidate model and Section 4 pre-sents our coreference resolution approach based onthis model.
Section 5 reports and discusses the ex-perimental results.
Section 6 describes related re-search work.
Finally, conclusion is given inSection 7.2 The Single-Candidate ModelThe main idea of the single-candidate model forcoreference resolution is to recast the resolution asa binary classification problem.During training, a set of training instances isgenerated for each anaphor in an annotated text.An instance is formed by the anaphor and one ofits antecedent candidates.
It is labeled as positiveor negative based on whether or not the candidateis tagged in the same coreferential chain of theanaphor.After training, a classifier is ready to resolve theNPs1 encountered in a new document.
For each NPunder consideration, every one of its antecedentcandidates is paired with it to form a test instance.The classifier returns a number between 0 and 1that indicates the likelihood that the candidate iscoreferential to the NP.The returned confidence value is commonlyused as the competition criterion to rank the candi-date.
Normally, the candidates with confidencesless than a selection threshold (e.g.
0.5) are dis-carded.
Then some algorithms are applied tochoose one of the remaining candidates, if any, asthe antecedent.
For example, ?Closest-First?
(Soonet al, 2001) selects the candidate closest to theanaphor, while ?Best-First?
(Aone and Bennett,1995; Ng and Cardie, 2002a) selects the candidatewith the maximal confidence value.One limitation of this model, however, is that itonly considers the relationships between a NP en-countered and one of its candidates at a time dur-ing its training and testing procedures.
Theconfidence value reflects the probability that thecandidate is coreferential to the NP in the overall1 In this paper a NP corresponds to a Markable in MUCcoreference resolution tasks.distribution 2 , but not the conditional probabilitywhen the candidate is concurrent with other com-petitors.
Consequently, the confidence values areunreliable to represent the true competition crite-rion for the candidates.To illustrate this problem, just suppose a dataset where an instance could be described with fourexclusive features: F1, F2, F3 and F4.
The rankingof candidates obeys the following rule:CSF1 >> CSF2 >> CSF3 >> CSF4Here CSFi ( 41 ??
i ) is the set of antecedent can-didates with the feature Fi on.
The mark of ?>>?denotes the preference relationship, that is, thecandidates in CSF1 is preferred to those in CSF2, andto those in CSF3 and CSF4.Let CF2 and CF3 denote the class value of a leafnode ?F2 = 1?
and ?F3 = 1?, respectively.
It is pos-sible that CF2 < CF3, if the anaphors whose candi-dates all belong to CSF3 or CSF4 take the majority inthe training data set.
In this case, a candidate inCSF3 would be assigned a larger confidence valuethan a candidate in CSF2.
This nevertheless contra-dicts the ranking rules.
If during resolution, thecandidates of an anaphor all come from CSF2 orCSF3, the anaphor may be wrongly linked to a can-didate in CSF3 rather than in CSF2.3 The Twin-Candidate ModelDifferent from the single-candidate model, thetwin-candidate model aims to learn the competitioncriterion for candidates.
In this section, we willintroduce the structure of the model in details.3.1 Training Instances CreationConsider an anaphor ana and its candidate set can-didate_set, {C1, C2, ?, Ck}, where Cj is closer toana than Ci if j > i.
Suppose positive_set is the setof candidates that occur in the coreferential chainof ana, and negative_set is the set of candidates notin the chain, that is, negative_set = candidate_set- positive_set.
The set of training instances basedon ana, inst_set, is defined as follows:2 Suppose we use C4.5 algorithm and the class value takes thesmoothed ration,21++tp , where p is the number of positiveinstances and t is the total number of instances contained inthe corresponding leaf node.}
_  C  , _Cj,i |{} _  C  ,_ C j,i |{_ji),,(ji),,(setpositvesetnegativeinstsetnegativesetpositveinstsetinstanaCjCianaCjCi??>?
?>=UFrom the above definition, an instance isformed by an anaphor, one positive candidate andone negative candidate.
For each instance,)ana,cj,ci(inst , the candidate at the first position, Ci,is closer to the anaphor than the candidate at thesecond position, Cj.A training instance )ana,cj,ci(inst is labeled aspositive if Ci ?
positive-set and Cj ?
negative-set;or negative if Ci ?
negative-set and Cj ?
positive-set.See the following example:Any design to link China's accession to the WTOwith the missile tests1 was doomed to failure.
?If some countries2 try to block China TO acces-sion, that will not be popular and will fail to win thesupport of other countries3?
she said.Although no governments4 have suggested formalsanctions5 on China over the missile tests6, the UnitedStates has called them7 ?provocative and reckless?
andother countries said they could threaten Asian stability.In the above text segment, the antecedent can-didate set of the pronoun ?them7?
consists of sixcandidates highlighted in Italics.
Among the can-didates, Candidate 1 and 6 are in the coreferentialchain of ?them7?, while Candidate 2, 3, 4, 5 are not.Thus, eight instances are formed for ?them7?
:(2,1,7)  (3,1,7)  (4,1,7)  (5,1,7)(6,5,7)  (6,4,7)  (6,3,7)  (6,2,7)Here the instances in the first line are negative,while those in the second line are all positive.3.2 Features DefinitionA feature vector is specified for each training ortesting instance.
Similar to those in the single-candidate model, the features may describe thelexical, syntactic, semantic and positional relation-ships of an anaphor and any one of its candidates.Besides, the feature set may also contain inter-candidate features characterizing the relationshipsbetween the pair of candidates, e.g.
the distancebetween the candidates in the number distances orparagraphs.3.3 Classifier GenerationBased on the feature vectors generated for eachanaphor encountered in the training data set, aclassifier can be trained using a certain machinelearning algorithm, such as C4.5, RIPPER, etc.Given the feature vector of a test instance)ana,cj,ci(inst  (i > j), the classifier returns the posi-tive class indicating that Ci is preferred to Cj as theantecedent of ana; or negative indicating that Cj ispreferred.3.4 Antecedent IdentificationLet CR( )ana,cj,ci(inst ) denote the classification re-sult for an instance )ana,cj,ci(inst .
The antecedent ofan anaphor is identified using the algorithm shownin Figure 1.Algorithm ANTE-SELInput: ana: the anaphor under considerationcandidate_set: the set of antecedent can-didates of ana, {C1, C2,?,Ck}for i = 1 to K doScore[ i ] = 0;for  i = K downto 2 dofor j = i ?
1 downto 1 doif  CR( )ana,cj,ci(inst ) = = positive thenScore[ i ]++;elseScore[ j ] ++;endifSelectedIdx= ][maxarg_iScoresetcandidateCii ?return CselectedIdx;Figure 1:The antecedent identification algorithmAlgorithm ANTE-SEL takes as input an ana-phor and its candidate set candidate_set, and re-turns one candidate as its antecedent.
In thealgorithm, each candidate is compared against anyother candidate.
The classifier acts as a judge dur-ing each comparison.
The score of each candidateincreases by one every time when it wins.
In thisway, the final score of a candidate records the totaltimes it wins.
The candidate with the maximalscore is singled out as the antecedent.If two or more candidates have the same maxi-mal score, the one closest to the anaphor would beselected.3.5 Single-Candidate Model: A Special Caseof Twin-Candidate Model?While the realization and the structure of the twin-candidate model are significantly different fromthe single-candidate model, the single-candidatemodel in fact can be regarded as a special case ofthe twin-candidate model.To illustrate this, just consider a virtual ?blank?candidate C0 such that we could convert an in-stance )ana,ci(inst in the single-candidate model toan instance )ana,c,ci( 0inst in the twin-candidatemodel.
Let )ana,c,ci( 0inst have the same class labelas )ana,ci(inst , that is, )ana,c,ci( 0inst is positive if Ci isthe antecedent of ana; or negative if not.Apparently, the classifier trained on the in-stance set { )ana,ci(inst }, T1, is equivalent to thattrained on { )ana,c,ci( 0inst }, T2.
T1 and T2 wouldassign the same class label for the test instances)ana,ci(inst  and )ana,c,ci( 0inst , respectively.
That is tosay, determining whether Ci is coreferential to anaby T1 in the single-candidate model equals todetermining whether Ci is better than C0 w.r.t anaby T2 in the twin-candidate model.
Here we couldtake C0 as a ?standard candidate?.While the classification in the single-candidatemodel can find its interpretation in the twin-candidate model, it is not true vice versa.
Conse-quently, we can safely draw the conclusion that thetwin-candidate model is more powerful than thesingle-candidate model in characterizing the rela-tionships among an anaphor and its candidates.4 The Competition Learning ApproachOur competition learning approach adopts thetwin-candidate model introduced in the Section 3.The main process of the approach is as follows:1.
The raw input documents are preprocessed toobtain most, if not all, of the possible NPs.2.
During training, for each anaphoric NP, wecreate a set of candidates, and then generatethe training instances as described in Section 3.3.
Based on the training instances, we make useof the C5.0 learning algorithm (Quinlan, 1993)to train a classifier.4.
During resolution, for each NP encountered,we also construct a candidate set.
If the set isempty, we left this NP unresolved; otherwisewe apply the antecedent identification algo-rithm to choose the antecedent and then linkthe NP to it.4.1 PreprocessingTo determine the boundary of the noun phrases, apipeline of Nature Language Processing compo-nents are applied to an input raw text:z Tokenization and sentence segmentationz Named entity recognitionz Part-of-speech taggingz Noun phrase chunkingAmong them, named entity recognition, part-of-speech tagging and text chunking apply the sameHidden Markov Model (HMM) based engine witherror-driven learning capability (Zhou and Su,2000 & 2002).
The named entity recognitioncomponent recognizes various types of MUC-stylenamed entities, i.e., organization, location, person,date, time, money and percentage.4.2 Features SelectionFor our study, in this paper we only select thosefeatures that can be obtained with low annotationcost and high reliability.
All features are listed inTable 1 together with their respective possible val-ues.4.3 Candidates FilteringFor a NP under consideration, all of its precedingNPs could be the antecedent candidates.
Neverthe-less, since in the twin-candidate model the numberof instances for a given anaphor is about the squareof the number of its antecedent candidates, thecomputational cost would be prohibitively large ifwe include all the NPs in the candidate set.
More-over, many of the preceding NPs are irrelevant oreven invalid with regard to the anaphor.
These datanoises may hamper the training of a good-performanced classifier, and also damage the accu-racy of the antecedent selection: too many com-parisons are made between incorrect candidates.Therefore, in order to reduce the computationalcost and data noises, an effective candidate filter-ing strategy must be applied in our approach.During training, we create the candidate set foreach anaphor with the following filtering algorithm:1.
If the anaphor is a pronoun,(a) Add to the initial candidate set al the pre-ceding NPs in the current and the previoustwo sentences.
(b) Remove from the candidate set those thatdisagree in number, gender, and person.
(c) If the candidate set is empty, add the NPs inan earlier sentence and go to 1(b).2.
If the anaphor is a non-pronoun,(a) Add all the non-pronominal antecedents tothe initial candidate set.
(b) For each candidate added in 2(a), add thenon-pronouns in the current, the previousand the next sentences into the candidate set.During resolution, we filter the candidates foreach encountered pronoun in the same way as dur-ing training.
That is, we only consider the NPs inthe current and the preceding 2 sentences.
Such acontext window is reasonable as the distance be-tween a pronominal anaphor and its antecedent isgenerally short.
In the MUC-6 data set, for exam-ple, the immediate antecedents of 95% pronominalanaphors can be found within the above distance.Comparatively, candidate filtering for non-pronouns during resolution is complicated.
A po-tential problem is that for each non-pronoun underconsideration, the twin-candidate model alwayschooses a candidate as the antecedent, even thoughall of the candidates are ?low-qualified?, that is,unlikely to be coreferential to the non-pronoun un-der consideration.In fact, the twin-candidate model in itself canidentify the qualification of a candidate.
We cancompare every candidate with a virtual ?standardcandidate?, C0.
Only those better than C0 aredeemed qualified and allowed to enter the ?roundrobin?, whereas the losers are eliminated.
As wehave discussed in Section 3.5, the classifier on thepairs of a candidate and C0 is just a single-candidate classifier.
Thus, we can safely adopt thesingle-candidate classifier as our candidate filter.The candidate filtering algorithm during resolu-tion is as follows:Features describing the candidate:1.2.3.4.5.6.7.8.9.10ante_DefNp_1(2)ante_IndefNP_1(2)ante_Pron_1(2)ante_ProperNP_1(2)ante_M_ProperNP_1(2)ante_ProperNP_APPOS_1(2)ante_Appositive_1(2)ante_NearestNP_1(2)ante_Embeded_1(2)ante_Title_1(2)1 if Ci (Cj) is a definite NP; else 01 if Ci (Cj) is an indefinite NP; else 01 if Ci (Cj) is a pronoun; else 01 if Ci (Cj) is a proper NP; else 01 if Ci (Cj) is a mentioned proper NP; else 01 if Ci (Cj) is a proper NP modified by an appositive; else 01 if Ci (Cj) is in a apposition structure; else 01 if Ci (Cj) is the nearest candidate to the anaphor; else 01 if Ci (Cj) is in an embedded NP; else 01 if Ci (Cj) is in a title; else 0Features describing the anaphor:11.12.13.14.15.16.ana_DefNPana_IndefNPana_Pronana_ProperNPana_PronTypeana_FlexiblePron1 if ana is a definite NP; else 01 if ana is an indefinite NP; else 01 if ana is a pronoun; else 01 if ana is a proper NP; else 01 if ana is a third person pronoun; 2 if a single neuter pro-noun; 3 if a plural neuter pronoun; 4 if other types1 if ana is a flexible pronoun; else 0Features describing the candidate and the anaphor:17.18.18.20.21.ante_ana_StringMatch_1(2)ante_ana_GenderAgree_1(2)ante_ana_NumAgree_1(2)ante_ana_Appositive_1(2)ante_ana_Alias_1(2)1 if Ci (Cj) and ana match in string; else 01 if Ci (Cj) and ana agree in gender; else 0 if disagree; -1 ifunknown1 if Ci (Cj) and ana agree in number; 0 if disagree; -1 if un-known1 if Ci (Cj) and ana are in an appositive structure; else 01 if Ci (Cj) and ana are in an alias of the other; else 0Features describing the two candidates22.23.inter_SDistanceinter_PdistanceDistance between Ci and Cj in sentencesDistance between Ci and Cj in paragraphsTable 1:  Feature set for coreference resolution (Feature 22, 23 and features involving Cj are notused in the single-candidate model)1.
If the current NP is a pronoun, construct thecandidate set in the same way as during training.2.
If the current NP is a non-pronoun,(a) Add all the preceding non-pronouns to the ini-tial candidate set.
(b) Calculate the confidence value for each candi-date using the single-candidate classifier.
(c) Remove the candidates with confidence valueless than 0.5.5 Evaluation and DiscussionOur coreference resolution approach is evaluatedon the standard MUC-6 (1995) and MUC-7 (1998)data set.
For MUC-6, 30 ?dry-run?
documents an-notated with coreference information could be usedas training data.
There are also 30 annotated train-ing documents from MUC-7.
For testing, we util-ize the 30 standard test documents from MUC-6and the 20 standard test documents from MUC-7.5.1 Baseline SystemsIn the experiment we compared our approach withthe following research works:1.
Strube?s S-list algorithm for pronoun resolu-tion (Stube, 1998).2.
Ng and Cardie?s machine learning approach tocoreference resolution (Ng and Cardie, 2002a).3.
Connolly et al?s machine learning approach toanaphora resolution (Connolly et al, 1997).Among them, S-List, a version of centeringalgorithm, uses well-defined heuristic rules to rankthe antecedent candidates; Ng and Cardie?s ap-proach employs the standard single-candidatemodel and ?Best-First?
rule to select the antece-dent; Connolly et al?s approach also adopts thetwin-candidate model, but their approach lacks ofcandidate filtering strategy and uses greedy linearsearch to select the antecedent (See ?Relatedwork?
for details).We constructed three baseline systems based onthe above three approaches, respectively.
For com-parison, in the baseline system 2 and 3, we usedthe similar feature set as in our system (see table 1).5.2 Results and DiscussionTable 2 and 3 show the performance of differentapproaches in the pronoun and non-pronoun reso-lution, respectively.
In these tables we focus on theabilities of different approaches in resolving ananaphor to its antecedent correctly.
The recallmeasures the number of correctly resolved ana-phors over the total anaphors in the MUC test dataset, and the precision measures the number of cor-rect anaphors over the total resolved anaphors.
TheF-measure F=2*RP/(R+P) is the harmonic mean ofprecision and recall.The experimental result demonstrates that ourcompetition learning approach achieves a betterperformance than the baseline approaches in re-solving pronominal anaphors.
As shown in Table 2,our approach outperforms Ng and Cardie?s single-candidate based approach by 3.7 and 5.4 in F-measure for MUC-6 and MUC-7, respectively.Besides, compared with Strube?s S-list algorithm,our approach also achieves gains in the F-measureby 3.2 (MUC-6), and 1.6 (MUC-7).
In particular,our approach obtains significant improvement(21.1 for MUC-6, and 13.1 for MUC-7) over Con-nolly et al?s twin-candidate based approach.MUC-6 MUC-7R P F R P FStrube (1998)  76.1 74.3 75.1 62.9 60.3 61.6Ng and Cardie (2002a) 75.4 73.8 74.6 58.9 56.8 57.8Connolly et al (1997) 57.2 57.2 57.2 50.1 50.1 50.1Our approach 79.3 77.5 78.3 64.4 62.1 63.2Table 2:  Results for the pronoun resolutionMUC-6 MUC-7R P F R P FNg and Cardie (2002a) 51.0 89.9 65.0 39.1 86.4 53.8Connolly et al (1997) 52.2 52.2 52.2 43.7 43.7 43.7Our approach  51.3 90.4 65.4 39.7 87.6 54.6Table 3:  Results for the non-pronoun resolutionMUC-6 MUC-7R P F R P FNg and Cardie (2002a) 62.2 78.8 69.4 48.4 74.6 58.7Our approach 64.0 80.5 71.3 50.1 75.4 60.2Table 4: Results for the coreference resolutionCompared with the gains in pronoun resolution,the improvement in non-pronoun resolution isslight.
As shown in Table 3, our approach resolvesnon-pronominal anaphors with the recall of 51.3(39.7) and the precision of 90.4 (87.6) for MUC-6(MUC-7).
In contrast to Ng and Cardie?s approach,the performance of our approach improves only 0.3(0.6) in recall and 0.5 (1.2) in precision.
The rea-son may be that in non-pronoun resolution, thecoreference of an anaphor and its candidate is usu-ally determined only by some strongly indicativefeatures such as alias, apposition, string-matching,etc (this explains why we obtain a high precisionbut a low recall in non-pronoun resolution).
There-fore, most of the positive candidates are coreferen-tial to the anaphors even though they are not the?best?.
As a result, we can only see comparativelyslight difference between the performances of thetwo approaches.Although Connolly et al?s approach also adoptsthe twin-candidate model, it achieves a poor per-formance for both pronoun resolution and non-pronoun resolution.
The main reason is the absenceof candidate filtering strategy in their approach(this is why the recall equals to the precision in thetables).
Without candidate filtering, the recall mayrise as the correct antecedents would not be elimi-nated wrongly.
Nevertheless, the precision dropslargely due to the numerous invalid NPs in thecandidate set.
As a result, a significantly low F-measure is obtained in their approach.Table 4 summarizes the overall performance ofdifferent approaches to coreference resolution.
Dif-ferent from Table 2 and 3, here we focus onwhether a coreferential chain could be correctlyidentified.
For this purpose, we obtain the recall,the precision and the F-measure using the standardMUC scoring program (Vilain et al 1995) for thecoreference resolution task.
Here the recall meansthe correct resolved chains over the wholecoreferential chains in the data set, and precisionmeans the correct resolved chains over the wholeresolved chains.In line with the previous experiments, we seereasonable improvement in the performance of thecoreference resolution: compared with the baselineapproach based on the single-candidate model, theF-measure of approach increases from 69.4 to 71.3for MUC-6, and from 58.7 to 60.2 for MUC-7.6 Related WorkA similar twin-candidate model was adopted in theanaphoric resolution system by Connolly et al(1997).
The differences between our approach andtheirs are:(1) In Connolly et al?s approach, all the precedingNPs of an anaphor are taken as the antecedentcandidates, whereas in our approach we usecandidate filters to eliminate invalid or irrele-vant candidates.
(2) The antecedent identification in Connolly etal.
?s approach is to apply the classifier tosuccessive pairs of candidates, each timeretaining the better candidate.
However, due tothe lack of strong assumption of transitivity,the selection procedure is in fact a greedysearch.
By contrast, our approach evaluates acandidate according to the times it wins overthe other competitors.
Comparatively thisalgorithm could lead to a better solution.
(3) Our approach makes use of more indicativefeatures, such as Appositive, Name Alias,String-matching, etc.
These features are effec-tive especially for non-pronoun resolution.7 ConclusionIn this paper we have proposed a competitionlearning approach to coreference resolution.
Westarted with the introduction of the single-candidate model adopted by most supervised ma-chine learning approaches.
We argued that the con-fidence values returned by the single-candidateclassifier are not reliable to be used as ranking cri-terion for antecedent candidates.
Alternatively, wepresented a twin-candidate model that learns thecompetition criterion for antecedent candidatesdirectly.
We introduced how to adopt the twin-candidate model in our competition learning ap-proach to resolve the coreference problem.
Particu-larly, we proposed a candidate filtering algorithmthat can effectively reduce the computational costand data noises.The experimental results have proved the effec-tiveness of our approach.
Compared with the base-line approach using the single-candidate model, theF-measure increases by 1.9 and 1.5 for MUC-6 andMUC-7 data set, respectively.
The gains in thepronoun resolution contribute most to the overallimprovement of coreference resolution.Currently, we employ the single-candidate clas-sifier to filter the candidate set during resolution.While the filter guarantees the qualification of thecandidates, it removes too many positive candi-dates, and thus the recall suffers.
In our futurework, we intend to adopt a looser filter togetherwith an anaphoricity determination module (Beanand Riloff, 1999; Ng and Cardie, 2002b).
Only ifan encountered NP is determined as an anaphor,we will select an antecedent from the candidate setgenerated by the looser filter.
Furthermore, wewould like to incorporate more syntactic featuresinto our feature set, such as grammatical role orsyntactic parallelism.
These features may be help-ful to improve the performance of pronoun resolu-tion.ReferencesChinatsu Aone and Scott W.Bennett.
1995.
Evaluatingautomated and manual acquisition of anaphora reso-lution strategies.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Lin-guistics, Pages 122-129.D.Bean and E.Riloff.
1999.
Corpus-Based identificationof non-anaphoric noun phrases.
In Proceedings of the37th Annual Meeting of the Association for Computa-tional Linguistics, Pages 373-380.Brennan, S, E., M. W. Friedman and C. J. Pollard.
1987.A Centering approach to pronouns.
In Proceedings ofthe 25th Annual Meeting of The Association for Com-putational Linguistics, Page 155-162.Dennis Connolly, John D. Burger and David S. Day.1997.
A machine learning approach to anaphoric ref-erence.
New Methods in Language Processing, Page133-144.Joseph F. McCarthy.
1996.
A trainable approach tocoreference resolution for Information Extraction.Ph.D.
thesis.
University of Massachusetts.Ruslan Mitkov.
1998.
Robust pronoun resolution withlimited knowledge.
In Proceedings of the 17th Int.Conference on Computational Linguistics (COLING-ACL'98), Page 869-875.Ruslan Mitkov.
1999.
Anaphora resolution: The state ofthe art.
Technical report.
University of Wolverhamp-ton, Wolverhampton.MUC-6.
1995.
Proceedings of the Sixth Message Un-derstanding Conference (MUC-6).
Morgan Kauf-mann, San Francisco, CA.MUC-7.
1998.
Proceedings of the Seventh MessageUnderstanding Conference (MUC-7).
Morgan Kauf-mann, San Francisco, CA.Vincent Ng and Claire Cardie.
2002a.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40rd Annual Meeting of the As-sociation for Computational Linguistics, Pages 104-111.Vincent Ng and Claire Cardie.
2002b.
Identifying ana-phoric and non-anaphoric noun phrases to improvecoreference resolution.
In Proceedings of 19th Inter-national Conference on Computational Linguistics(COLING-2002).J R. Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann, San Mateo, CA.Wee Meng Soon, Hwee Tou Ng and Daniel ChungYong Lim.
2001.
A machine learning approach tocoreference resolution of noun phrases.
Computa-tional Linguistics, 27(4), Page 521-544.Michael Strube.
Never look back: An alternative toCentering.
1998.
In Proceedings of the 17th Int.
Con-ference on Computational Linguistics and 36th An-nual Meeting of ACL, Page 1251-1257Joel R. Tetreault.
2001.
A Corpus-Based evaluation ofCentering and pronoun resolution.
ComputationalLinguistics, 27(4), Page 507-520.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In Proceedings of the Sixth Messageunderstanding Conference (MUC-6), Pages 42-52.GD Zhou and J. Su, 2000.
Error-driven HMM-basedchunk tagger with context-dependent lexicon.
InProceedings of the Joint Conference on EmpiricalMethods on Natural Language Processing and VeryLarge Corpus (EMNLP/ VLC'2000).GD Zhou and J. Su.
2002.
Named Entity recognitionusing a HMM-based chunk tagger.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics, P473-478.
