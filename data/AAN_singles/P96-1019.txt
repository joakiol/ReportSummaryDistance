An Iterative Algor i thm to Build Chinese Language ModelsXiaoq iang  LuoCenter  for Languageand  Speech  Process ingThe  Johns  Hopk ins  Un ivers i ty3400 N. Char les  St.Ba l t imore ,  MD21218,  USAx iao@j  hu.
eduSa l im RoukosIBM T. J. Watson  Research  CenterYork town Heights ,  NY  10598, USAroukos?wat  son.
ibm.
comAbst ract?
?We present an iterative procedure to builda Chinese language model (LM).
We seg-ment Chinese text into words based on aword-based Chinese language model.
How-ever, the construction of a Chinese LM it-self requires word boundaries.
To get outof the chicken-and-egg problem, we proposean iterative procedure that alternates twooperations: segmenting text into words andbuilding an LM.
Starting with an initialsegmented corpus and an LM based uponit, we use a Viterbi-liek algorithm to seg-ment another set of data.
Then, we buildan LM based on the second set and use theresulting LM to segment again the first cor-pus.
The alternating procedure provides aself-organized way for the segmenter to de-tect automatically unseen words and cor-rect segmentation errors.
Our prelimi-nary experiment shows that the alternat-ing procedure not only improves the accu-racy of our segmentation, but discovers un-seen words surprisingly well.
The resultingword-based LM has a perplexity of 188 fora general Chinese corpus.1 In t roduct ionIn statistical speech recognition(Bahl et al, 1983),it is necessary to build a language model(LM) for as-signing probabilities to hypothesized sentences.
TheLM is usually built by collecting statistics of wordsover a large set of text data.
While doing so isstraightforward for English, it is not trivial to collectstatistics for Chinese words since word boundariesare not marked in written Chinese text.
Chineseis a morphosyllabic language (DeFrancis, 1984) inthat almost all Chinese characters represent a singlesyllable and most Chinese characters are also mor-phemes.
Since a word can be multi-syllabic, it is gen-erally non-trivial to segment a Chinese sentence intowords(Wu and Tseng, 1993).
Since segmentation isa fundamental problem in Chinese information pro-cessing, there is a large literature to deal with theproblem.
Recent work includes (Sproat et al, 1994)and (Wang et al, 1992).
In this paper, we adopt astatistical approach to segment Chinese text basedon an LM because of its autonomous nature and itscapability to handle unseen words.As far as speech recognition is concerned, what isneeded is a model to assign a probability to a stringof characters.
One may argue that we could bypassthe segmentation problem by building a character-based LM.
However, we have a strong belief that aword-based LM would be better than a character-based 1 one.
In addition to speech recognition, theuse of word based models would have value in infor-mation retrieval and other language processing ap-plications.If word boundaries are given, all established tech-niques can be exploited to construct an LM (Jelineket al, 1992) just as is done for English.
Therefore,segmentation is a key issue in building the ChineseLM.
In this paper, we propose a segmentation al-gorithm based on an LM.
Since building an LM it-self needs word boundaries, this is a chicken-and-eggproblem.
To get out of this, we propose an iterativeprocedure that alternates between the segmentationof Chinese text and the construction of the LM.
Ourpreliminary experiments show that the iterative pro-cedure is able to improve the segmentation accuracyand more importantly, it can detect unseen wordsautomatically.In section 2, the Viterbi-like segmentation algo-rithm based on a LM is described.
Then in sec-tion section:iter-proc we discuss the alternating pro-cedure of segmentation and building Chinese LMs.We test the segmentation algorithm and the alter-nating procedure and the results are reported in sec-I A character-based trigram model has a perplexity of46 per character or 462 per word (a Chinese word hasan average length of 2 characters), while a word-basedtrigram model has a perplexity 188 on the same set ofdata.
While the comparison would be fairer using a 5-gram character model, that the word model would havea lower perplexity as long as the coverage is high.139tion 4.
Finally, the work is summarized in section 5.2 segmentat ion  based  on  LMIn this section, we assume there is a word-based Chi-nese LM at our disposal so that we are able to com-pute the probability of a sentence (with word bound-aries).
We use a Viterbi-like segmentation algorithmbased on the LM to segment texts.Denote a sentence S by C1C~.. "C,,-1Cn, whereeach Ci (1 < i < n } is a Chinese character.
To seg-ment a sentence into words is to group these char-acters into words, i.e.S = C :C2 .
.
.C , - :C ,  (1)= (c:...c,,,)(c,,,+:...c,,,) (2)?
.
.
(3)= w:w2.
.
.w , ,  (4)where xk is the index of the last character in k ~hword wk, i,e wk = Cxk_ l+: ' "Cxk(k  = 1,2,- .
- ,m),and of course, z0 = 0, z,~ = n.Note that a segmentation of the sentence S canbe uniquely represented by an integer sequencez : , .
-  -, zrn, so we will denote a segmentation by itscorresponding integer sequence thereafter.
LetG(S) = {(=: .
.
.
: <_ <_... _< _< (5)be the set of all possible segmentations of sentenceS.
Suppose a word-based LM is given, then for asegmentation g(S) -" ( z : .
.
.
xm)  e G(S), we canassign a score to g(S) byL(g(S)) = logPg(w: ' "Wm) (6)m= ~--~logPa(wi\[hi) (7)/=1where w i = C=~_,+:.
.
.C~( j  = 1,2, - .
.
,m) ,  and hiis understood as the history words w: .
.
.w i - t .
Inthis paper the trigram model(Jelinek et al, 1992) isused and therefore hi = wi-2wi- :Among all possible segmentations, wepick the oneg* with the highest score as our result.
That is,g* = arg g~Ga~S) L(g(S)) (8)= arg max logPg(wl .
.
.wm) (9)gea(S)Note the score depends on segmentation g and thisis emphasized by the subscript in (9).
The optimalsegmentation g* can be obtained by dynamic pro-gramming.
With a slight abuse of notation, let L(k)be the max accumulated score for the first k charac-ters.
L(k) is defined for k = 1, 2 , .
.
.
,  n with L(1) = 0and L(g*) = L(n).
Given {L(i) : 1 < i < k - l} ,L(k) can be computed recursively as follows:L(k)--  max \[L(i)-t- logP(Ci+:...C~\]hi)\] (10) :<i_<k-:where hi is the history words ended with the i thcharacter Ci.
At the end of the recursion, we needto trace back to find the segmentation points.
There-fore, it's necessary to record the segmentation pointsin (10).Let p(k) be the index of the last character in thepreceding word.
ThenV(k) = arg :<sm.<~x :\[L(i ) + log P(C i+: .
.
.
Ck \]hi)\] (11)that is, Cp(k)+: "" ?
Ck comprises the last word of theoptimal segmentation up to the k 'h character.A typical example of a six-character sentence isshown in table 1.
Since p(6) = 4, we know the lastword in the optimal segmentation is C5C6.
Sincep(4) = 3, the second last word is C4.
So on and soforth.
The optimal segmentation for this sentence is(61) (C2C3) (C4) (65C6)  ?Table 1: A segmentation examplechars I C: C2 C3 C4 C5 C6k I 1 2 3 4 5 6p(k) 0 1 1 3 3 4The searches in (10) and (11) are in general time-consuming.
Since long words are very rare in Chi-nese(94% words are with three or less characters(Wu and Tseng, 1993)), it won't hurt at all to limitthe search space in (10) and (11) by putting an up-per bound(say, 10) to the length of the exploringword, i.e, impose the constraint i >_ ma?l ,  k - d in(10) and (11), where d is the upper bound of Chineseword length.
This will speed the dynamic program-ming significantly for long sentences.It is worth of pointing out that the algorithm in(10) and (11) could pick an unseen word(i.e, a wordnot included in the vocabulary on which the LM isbuilt on) in the optimal segmentation provided LMassigns proper probabilities to unseen words.
This isthe beauty of the algorithm that it is able to handleunseen words automatically.3 I te ra t ive  procedure  to  bu i ld  LMIn the previous section, we assumed there exists aChinese word LM at our disposal.
However, this isnot true in reality.
In this section, we discuss an it-erative procedure that builds LM and automaticallyappends the unseen words to the current vocabulary.The procedure first splits the data into two parts,set T1 and T2.
We start from an initial segmenta-tion of the set T1.
This can be done, for instance,by a simple greedy algorithm described in (Sproatet al, 1994).
With the segmented T1, we constructa LMi on it.
Then we segment the set T2 by usingthe LMi and the algorithm described in section 2.At the same time, we keep a counter for each unseenword in optimal segmentations and increment hecounter whenever its associated word appears in an140optimal segmentation.
This gives us a measure totell whether an unseen word is an accidental charac-ter string or a real word not included in our vocab-ulary.
The higher a counter is, the more likely it isa word.
After segmenting the set T2, we add to ourvocabulary all unseen words with its counter greaterthan a threshold e. Then we use the augmentedvocabulary and construct another LMi+I using thesegmented T2.
The pattern is clear now: LMi+I isused to segment the set T1 again and the vocabularyis further augmented.To be more precise, the procedure can be writtenin pseudo code as follows.S tep  0: Initially segment he set T1.Construct an LM LMo with an initial vocabu-lary V0.set i=1.S tep  1: Let j=i  mod 2;For each sentence S in the set Tj, do1.1 segment it using LMi-1.1.2 for each unseen word in the optimal seg-mentation, increment its counter by thenumber of times it appears in the optimalsegmentation.S tep  2: Let A=the set of unseen words withcounter greater than e.set Vi = ~-1  U A.Construct another LMi using the segmented setand the vocabulary ~.S tep  3: i - - i+l and goto step 1.Unseen words, most of which are proper nouns,pose a serious problem to Chinese text segmenta-tion.
In (Sproat et al, 1994) a class based model wasproposed to identify personal names.
In (Wang etal., 1992), a title driven method was used to identifypersonal names.
The iterative procedure proposedhere provides a self-organized way to detect unseenwords, including proper nouns.
The advantage isthat it needs little human intervention.
The proce-dure provides a chance for us to correct segmentingerrors.4 Exper iments  and  Eva luat ion4.1 Segmentat ion  AccuracyOur first attempt is to see how accurate the segmen-tation algorithm proposed in section 2 is.
To thisend, we split the whole data set ~ into two parts, halffor building LMs and half reserved for testing.
Thetrigram model used in this experiment is the stan-dard deleted interpolation model described in (Je-linek et al, 1992) with a vocabulary of 20K words.Since we lack an objective criterion to measurethe accuracy of a segmentation system, we ask three~The corpus has about 5 million characters and iscoarsely pre-segmented.native speakers to segment manually 100 sentencespicked randomly from the test set and comparethem with segmentations by machine.
The result issummed in table 2, where ORG stands for the orig-inal segmentation, P1, P2 and P3 for three humansubjects, and TRI and UNI stand for the segmen-tations generated by trigram LM and unigram LMrespectively.
The number eported here is the arith-metic average of recall and precision, as was used inn_~ (Sproat et al, 1994), i.e., 1/2(~-~ + n2), where ncis the number of common words in both segmenta-tions, nl and n2 are the number of words in each ofthe segmentations.Table 2: Segmentation AccuracyORG P1 P2ORGP1 85.9P2 79.1 90.9P3 87.4 85.7 82.2P3 TRI94.285.380.185.6UNI91.287.482.285.7We can make a few remarks about the resultin table 2.
First of all, it is interesting to notethat the agreement of segmentations among humansubjects is roughly at the same level of that be-tween human subjects and machine.
This confirmswhat reported in (Sproat et al, 1994).
The majordisagreement for human subjects comes from com-pound words, phrases and suffices.
Since we don'tgive any specific instructions to human subjects,one of them tends to group consistently phrasesas words because he was implicitly using seman-tics as his segmentation criterion.
For example, hesegments thesentence 3 dao4 j ia l  l i2  ch i l  dun4fan4(see table 3) as two words dao4 j?a l  l?2(gohome) and ch i l  dun4 :fem4(have a meal) becausethe two "words" are clearly two semantic units.
Theother two subjects and machine segment it as dao4/ j i a l  l i 2 /  ch i l /  dtm4 / fern4.Chinese has very limited morphology (Spencer,1991) in that most grammatical concepts are con-veyed by separate words and not by morphologicalprocesses.
The limited morphology includes someending morphemes to represent tenses of verbs, andthis is another source of disagreement.
For exam-ple, for the partial sentence zuo4 were2 le, wherele  functions as labeling the verb zuo4 wa.u2 as "per-fect" tense, some subjects tend to segment it as twowords zuo4 ~an2/ le  while the other treat it as onesingle word.Second, the agreement ofeach of the subjects witheither the original, trigram, or unigram segmenta-tion is quite high (see columns 2, 6, and 7 in Table 2)and appears to be specific to the subject.3Here we use Pin Yin followed by its tone to representa character.141Third, it seems puzzling that the trigram LMagrees with the original segmentation better than aunigram model, but gives a worse result when com-pared with manual segmentations.
However, sincethe LMs are trained using the presegmented data,the trigram model tends to keep the original segmen-tation because it takes the preceding two words intoaccount while the unigram model is less restrictedto deviate from the original segmentation.
In otherwords, if trained with "cleanly" segmented data, atrigram model is more likely to produce a better seg-mentation since it tends to preserve the nature oftraining data.4.2 Exper iment  of the i terat ive procedureIn addition to the 5 million characters of segmentedtext, we had unsegmented data from various ourcesreaching about 13 million characters.
We appliedour iterative algorithm to that corpus.Table 4 shows the figure of merit of the resultingsegmentation f the 100 sentence test set describedearlier.
After one iteration, the agreement withthe original segmentation decreased by 3 percentagepoints, while the agreement with the human segmen-tation increased by less than one percentage point.We ran our computation i tensive procedure for oneiteration only.
The results indicate that the impacton segmentation accuracy would be small.
However,the new unsegmented corpus is a good source of au-tomatically discovered words.
A 20 examples pickedrandomly from about 1500 unseen words are shownin Table 5.
16 of them are reasonably good wordsand are listed with their translated meanings.
Theproblematic words are marked with "?
".4.3 Perp lex i ty  of  the language modelAfter each segmentation, an interpolated trigrammodel is built, and an independent test set with2.5 million characters i segmented and then usedto measure the quality of the model.
We got a per-plexity 188 for a vocabulary of 80K words, and thealternating procedure has little impact on the per-plexity.
This can be explained by the fact that thechange of segmentation is very little ( which is re-flected in table reftab:accuracy-iter ) and the addi-tion of unseen words(1.5K) to the vocabulary is alsotoo little to affect the overall perplexity.
The meritof the alternating procedure is probably its abilityto detect unseen words.5 Conc lus ionIn this paper, we present an iterative procedureto build Chinese language model(LM).
We segmentChinese text into words based on a word-based Chi-nese language model.
However, the construction ofa Chinese LM itself requires word boundaries.
Toget out of the chicken-egg problem, we propose aniterative procedure that alternates two operations:segmenting text into words and building an LM.Starting with an initial segmented corpus and anLM based upon it, we use Viterbi-like algorithm tosegment another set of data.
Then we build an LMbased on the second set and use the LM to seg-ment again the first corpus.
The alternating proce-dure provides a self-organized way for the segmenterto detect automatically unseen words and correctsegmentation errors.
Our preliminary experimentshows that the alternating procedure not only im-proves the accuracy of our segmentation, but dis-covers unseen words surprisingly well.
We get a per-plexity 188 for a general Chinese corpus with 2.5million characters 46 AcknowledgmentThe first author would like to thank various mem-bers of the Human Language technologies Depart-ment at the IBM T.J Watson center for their en-couragement and helpful advice.
Special thanks goto Dr. Martin Franz for providing continuous helpin using the IBM language model tools.
The authorswould also thank the comments and insight of twoanonymous reviewers which help improve the finaldraft.Re ferencesRichard Sproat, Chilin Shih, William Gale andNancy Chang.
1994.
A stochastic finite-stateword segmentation algorithm for Chinese.
In Pro-ceedings of A GL 'Y~ , pages 66-73Zimin Wu and Gwyneth Tseng 1993.
Chinese TextSegmentation for Text Retrieval: Achievementsand Problems Journal of the American Societyfor Information Science, 44(9):532-542.John DeFrancis.
1984.
The Chinese Language.
Uni-versity of Hawaii Press, Honolulu.Frederick Jelinek, Robert L. Mercer and SalimRoukos.
1992.
Principles of Lexical LanguageModeling for Speech recognition.
In Advances inSpeech Signal Processing, pages 651-699, editedby S. Furui and M. M. Sondhi.
Marcel Dekker Inc.,1992L.R Bahl, Fred Jelinek and R.L.
Mercer.
1983.A Maximum Likelihood Approach to Continu-ous Speech Recognition.
In IEEE Transactionson Pattern Analysis and Machine Intelligence,1983,5(2):179-190Liang-Jyh Wang, Wei-Chuan Li, and Chao-HuangChang.
1992.
Recognizing unregistered names formandarin word identification.
In Proceedings ofCOLING-92, pages 1239-1243.
COLING4Unfortunately, we could not find a report of Chineseperplexity for comparison i the published literature con-cerning Mandarin speech recognition142Andrew Spencer.
1992.
Morphological theory :an introduction to word structure in generativegrammar pages 38-39.
Oxford, UK ; Cambridge,Mass., USA.
Basil Blackwell, 1991.Table 3: Segmentation of phrasesChinese \[ dao4 j ial li2 chil dun4 fan4Meaning I go home eat a mealTable 4: Segmentation of accuracy after one itera-tion ~ TR0 TR1 .920 .890 .863 .877 .817 .832.850 .849Table 5: Examples of unseen wordsPinYinkui2 er2he2 shi4 lu4 yinl  dai4shou2 d~o3ren4 zhong4ji4 j ian3zi4 hai4shuangl  bao3ji4 donglzi3 j iaolxiaol long2 shi21i4 bo4 h~i3du4 shan lshang l  ban4liu6 ha, J4sa4 he4 le4ku~i4 xun4cheng4 j ing3hu~ng2 du2ba3 lian2he2 dao3Meaninglast name of former  US vice presidentcassette of audio tape(abbr)pretect  ( the)  islandfirst name or p~rt of a phrase(abbr)  discipline monitor ing?double guarantee(abbr)  Eastern He Bei provincepurple gluepersonal name??
(abbr)  commerc ia l  orientedsix (types of) harmst r,xnslat ed no,  mefast newstrain copyellow poison?a (biological) jargon143
