Proceedings of the 43rd Annual Meeting of the ACL, pages 581?588,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic Role Labeling Using Different Syntactic Views?Sameer Pradhan, Wayne Ward,Kadri Hacioglu, James H. MartinCenter for Spoken Language Research,University of Colorado,Boulder, CO 80303{spradhan,whw,hacioglu,martin}@cslr.colorado.eduDaniel JurafskyDepartment of Linguistics,Stanford University,Stanford, CA 94305jurafsky@stanford.eduAbstractSemantic role labeling is the process ofannotating the predicate-argument struc-ture in text with semantic labels.
In thispaper we present a state-of-the-art base-line semantic role labeling system basedon Support Vector Machine classifiers.We show improvements on this systemby: i) adding new features including fea-tures extracted from dependency parses,ii) performing feature selection and cali-bration and iii) combining parses obtainedfrom semantic parsers trained using dif-ferent syntactic views.
Error analysis ofthe baseline system showed that approx-imately half of the argument identifica-tion errors resulted from parse errors inwhich there was no syntactic constituentthat aligned with the correct argument.
Inorder to address this problem, we com-bined semantic parses from a Minipar syn-tactic parse and from a chunked syntac-tic representation with our original base-line system which was based on Charniakparses.
All of the reported techniques re-sulted in performance improvements.1 IntroductionSemantic Role Labeling is the process of annotat-ing the predicate-argument structure in text with se-?This research was partially supported by the ARDAAQUAINT program via contract OCG4423B and by the NSFvia grants IS-9978025 and ITR/HCI 0086132mantic labels (Gildea and Jurafsky, 2000; Gildeaand Jurafsky, 2002; Gildea and Palmer, 2002; Sur-deanu et al, 2003; Hacioglu and Ward, 2003; Chenand Rambow, 2003; Gildea and Hockenmaier, 2003;Pradhan et al, 2004; Hacioglu, 2004).
The architec-ture underlying all of these systems introduces twodistinct sub-problems: the identification of syntacticconstituents that are semantic roles for a given pred-icate, and the labeling of the those constituents withthe correct semantic role.A detailed error analysis of our baseline systemindicates that the identification problem poses a sig-nificant bottleneck to improving overall system per-formance.
The baseline system?s accuracy on thetask of labeling nodes known to represent semanticarguments is 90%.
On the other hand, the system?sperformance on the identification task is quite a bitlower, achieving only 80% recall with 86% preci-sion.
There are two sources of these identificationerrors: i) failures by the system to identify all andonly those constituents that correspond to semanticroles, when those constituents are present in the syn-tactic analysis, and ii) failures by the syntactic ana-lyzer to provide the constituents that align with cor-rect arguments.
The work we present here is tailoredto address these two sources of error in the identifi-cation problem.The remainder of this paper is organized as fol-lows.
We first describe a baseline system based onthe best published techniques.
We then report ontwo sets of experiments using techniques that im-prove performance on the problem of finding argu-ments when they are present in the syntactic analy-sis.
In the first set of experiments we explore new581features, including features extracted from a parserthat provides a different syntactic view ?
a Combi-natory Categorial Grammar (CCG) parser (Hocken-maier and Steedman, 2002).
In the second set ofexperiments, we explore approaches to identify opti-mal subsets of features for each argument class, andto calibrate the classifier probabilities.We then report on experiments that address theproblem of arguments missing from a given syn-tactic analysis.
We investigate ways to combinehypotheses generated from semantic role taggerstrained using different syntactic views ?
one trainedusing the Charniak parser (Charniak, 2000), anotheron a rule-based dependency parser ?
Minipar (Lin,1998), and a third based on a flat, shallow syntacticchunk representation (Hacioglu, 2004a).
We showthat these three views complement each other to im-prove performance.2 Baseline SystemFor our experiments, we use Feb 2004 release ofPropBank1 (Kingsbury and Palmer, 2002; Palmeret al, 2005), a corpus in which predicate argumentrelations are marked for verbs in the Wall StreetJournal (WSJ) part of the Penn TreeBank (Marcuset al, 1994).
PropBank was constructed by as-signing semantic arguments to constituents of hand-corrected TreeBank parses.
Arguments of a verbare labeled ARG0 to ARG5, where ARG0 is thePROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.In addition to these CORE ARGUMENTS, additionalADJUNCTIVE ARGUMENTS, referred to as ARGMsare also marked.
Some examples are ARGM-LOC,for locatives; ARGM-TMP, for temporals; ARGM-MNR, for manner, etc.
Figure 1 shows a syntax treealong with the argument labels for an example ex-tracted from PropBank.
We use Sections 02-21 fortraining, Section 00 for development and Section 23for testing.We formulate the semantic labeling problem asa multi-class classification problem using SupportVector Machine (SVM) classifier (Hacioglu et al,2003; Pradhan et al, 2003; Pradhan et al, 2004)TinySVM2 along with YamCha3 (Kudo and Mat-1http://www.cis.upenn.edu/?ace/2http://chasen.org/?taku/software/TinySVM/3http://chasen.org/?taku/software/yamcha/Shhhh((((NPhhhh((((The acquisitionARG1VP```VBDwasNULLVPXXXVBNcompletedpredicatePP```in SeptemberARGM?TMP[ARG1 The acquisition] was [predicate completed] [ARGM?TMP in September].Figure 1: Syntax tree for a sentence illustrating thePropBank tags.sumoto, 2000; Kudo and Matsumoto, 2001) are usedto implement the system.
Using what is known asthe ONE VS ALL classification strategy, n binaryclassifiers are trained, where n is number of seman-tic classes including a NULL class.The baseline feature set is a combination of fea-tures introduced by Gildea and Jurafsky (2002) andones proposed in Pradhan et al, (2004), Surdeanu etal., (2003) and the syntactic-frame feature proposedin (Xue and Palmer, 2004).
Table 1 lists the featuresused.PREDICATE LEMMAPATH: Path from the constituent to the predicate in the parse tree.POSITION: Whether the constituent is before or after the predicate.VOICEPREDICATE SUB-CATEGORIZATIONPREDICATE CLUSTERHEAD WORD: Head word of the constituent.HEAD WORD POS: POS of the head wordNAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features.PARTIAL PATH: Path from the constituent to the lowest common ancestorof the predicate and the constituent.VERB SENSE INFORMATION: Oracle verb sense information from PropBankHEAD WORD OF PP: Head of PP replaced by head word of NP inside it,and PP replaced by PP-prepositionFIRST AND LAST WORD/POS IN CONSTITUENTORDINAL CONSTITUENT POSITIONCONSTITUENT TREE DISTANCECONSTITUENT RELATIVE FEATURES: Nine features representingthe phrase type, head word and head word part of speech of theparent, and left and right siblings of the constituent.TEMPORAL CUE WORDSDYNAMIC CLASS CONTEXTSYNTACTIC FRAMECONTENT WORD FEATURES: Content word, its POS and named entitiesin the content wordTable 1: Features used in the Baseline systemAs described in (Pradhan et al, 2004), we post-process the n-best hypotheses using a trigram lan-guage model of the argument sequence.We analyze the performance on three tasks:?
Argument Identification ?
This is the pro-cess of identifying the parsed constituents inthe sentence that represent semantic argumentsof a given predicate.582?
Argument Classification ?
Given constituentsknown to represent arguments of a predicate,assign the appropriate argument labels to them.?
Argument Identification and Classification ?A combination of the above two tasks.ALL ARGs Task P R F1 A(%) (%) (%)HAND Id.
96.2 95.8 96.0Classification - - - 93.0Id.
+ Classification 89.9 89.0 89.4AUTOMATIC Id.
86.8 80.0 83.3Classification - - - 90.1Id.
+ Classification 80.9 76.8 78.8Table 2: Baseline system performance on all tasksusing hand-corrected parses and automatic parses onPropBank data.Table 2 shows the performance of the system us-ing the hand corrected, TreeBank parses (HAND)and using parses produced by a Charniak parser(AUTOMATIC).
Precision (P), Recall (R) and F1scores are given for the identification and combinedtasks, and Classification Accuracy (A) for the clas-sification task.Classification performance using Charniak parsesis about 3% absolute worse than when using Tree-Bank parses.
On the other hand, argument identifi-cation performance using Charniak parses is about12.7% absolute worse.
Half of these errors ?
about7% are due to missing constituents, and the otherhalf ?
about 6% are due to mis-classifications.Motivated by this severe degradation in argumentidentification performance for automatic parses, weexamined a number of techniques for improvingargument identification.
We made a number ofchanges to the system which resulted in improvedperformance.
The changes fell into three categories:i) new features, ii) feature selection and calibration,and iii) combining parses from different syntacticrepresentations.3 Additional Features3.1 CCG Parse FeaturesWhile the Path feature has been identified to be veryimportant for the argument identification task, it isone of the most sparse features and may be diffi-cult to train or generalize (Pradhan et al, 2004; Xueand Palmer, 2004).
A dependency grammar shouldgenerate shorter paths from the predicate to depen-dent words in the sentence, and could be a morerobust complement to the phrase structure grammarpaths extracted from the Charniak parse tree.
Gildeaand Hockenmaier (2003) report that using featuresextracted from a Combinatory Categorial Grammar(CCG) representation improves semantic labelingperformance on core arguments.
We evaluated fea-tures from a CCG parser combined with our baselinefeature set.
We used three features that were intro-duced by Gildea and Hockenmaier (2003):?
Phrase type ?
This is the category of the max-imal projection between the two words ?
thepredicate and the dependent word.?
Categorial Path ?
This is a feature formed byconcatenating the following three values: i) cat-egory to which the dependent word belongs, ii)the direction of dependence and iii) the slot inthe category filled by the dependent word.?
Tree Path ?
This is the categorial analogue ofthe path feature in the Charniak parse basedsystem, which traces the path from the depen-dent word to the predicate through the binaryCCG tree.Parallel to the hand-corrected TreeBank parses,we also had access to correct CCG parses derivedfrom the TreeBank (Hockenmaier and Steedman,2002a).
We performed two sets of experiments.One using the correct CCG parses, and the other us-ing parses obtained using StatCCG4 parser (Hocken-maier and Steedman, 2002).
We incorporated thesefeatures in the systems based on hand-correctedTreeBank parses and Charniak parses respectively.For each constituent in the Charniak parse tree, ifthere was a dependency between the head word ofthe constituent and the predicate, then the corre-sponding CCG features for those words were addedto the features for that constituent.
Table 3 shows theperformance of the system when these features wereadded.
The corresponding baseline performancesare mentioned in parentheses.3.2 Other FeaturesWe added several other features to the system.
Po-sition of the clause node (S, SBAR) seems to be4Many thanks to Julia Hockenmaier for providing us withthe CCG bank as well as the StatCCG parser.583ALL ARGs Task P R F1(%) (%)HAND Id.
97.5 (96.2) 96.1 (95.8) 96.8 (96.0)Id.
+ Class.
91.8 (89.9) 90.5 (89.0) 91.2 (89.4)AUTOMATIC Id.
87.1 (86.8) 80.7 (80.0) 83.8 (83.3)Id.
+ Class.
81.5 (80.9) 77.2 (76.8) 79.3 (78.8)Table 3: Performance improvement upon addingCCG features to the Baseline system.an important feature in argument identification (Ha-cioglu et al, 2004) therefore we experimented withfour clause-based path feature variations.
We addedthe predicate context to capture predicate sense vari-ations.
For some adjunctive arguments, punctuationplays an important role, so we added some punctu-ation features.
All the new features are shown inTable 4CLAUSE-BASED PATH VARIATIONS:I.
Replacing all the nodes in a path other than clause nodes with an ?
*?.For example, the path NP?S?VP?SBAR?NP?VP?VBDbecomes NP?S?*S?*?*?VBDII.
Retaining only the clause nodes in the path, which for the aboveexample would produce NP?S?S?VBD,III.
Adding a binary feature that indicates whether the constituentis in the same clause as the predicate,IV.
collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.PATH N-GRAMS: This feature decomposes a path into a series of trigrams.For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc.
Weused the first ten trigrams as ten features.
Shorter paths were paddedwith nulls.SINGLE CHARACTER PHRASE TAGS: Each phrase category is clusteredto a category defined by the first character of the phrase label.PREDICATE CONTEXT: Two words and two word POS around thepredicate and including the predicate were added as ten new features.PUNCTUATION: Punctuation before and after the constituent wereadded as two new features.FEATURE CONTEXT: Features for argument bearing constituentswere added as features to the constituent being classified.Table 4: Other Features4 Feature Selection and CalibrationIn the baseline system, we used the same set of fea-tures for all the n binary ONE VS ALL classifiers.Error analysis showed that some features specifi-cally suited for one argument class, for example,core arguments, tend to hurt performance on someadjunctive arguments.
Therefore, we thought thatselecting subsets of features for each argument classmight improve performance.
To achieve this, weperformed a simple feature selection procedure.
Foreach argument, we started with the set of features in-troduced by (Gildea and Jurafsky, 2002).
We prunedthis set by training classifiers after leaving out onefeature at a time and checking its performance ona development set.
We used the ?2 significancewhile making pruning decisions.
Following that, weadded each of the other features one at a time to thepruned baseline set of features and selected ones thatshowed significantly improved performance.
Sincethe feature selection experiments were computation-ally intensive, we performed them using 10k trainingexamples.SVMs output distances not probabilities.
Thesedistances may not be comparable across classifiers,especially if different features are used to train eachbinary classifier.
In the baseline system, we used thealgorithm described by Platt (Platt, 2000) to convertthe SVM scores into probabilities by fitting to a sig-moid.
When all classifiers used the same set of fea-tures, fitting all scores to a single sigmoid was foundto give the best performance.
Since different fea-ture sets are now used by the classifiers, we traineda separate sigmoid for each classifier.Raw Scores ProbabilitiesAfter lattice-rescoringUncalibrated Calibrated(%) (%) (%)Same Feat.
same sigmoid 74.7 74.7 75.4Selected Feat.
diff.
sigmoids 75.4 75.1 76.2Table 5: Performance improvement on selecting fea-tures per argument and calibrating the probabilitieson 10k training data.Foster and Stine (2004) show that the pool-adjacent-violators (PAV) algorithm (Barlow et al,1972) provides a better method for converting rawclassifier scores to probabilities when Platt?s algo-rithm fails.
The probabilities resulting from eitherconversions may not be properly calibrated.
So, webinned the probabilities and trained a warping func-tion to calibrate them.
For each argument classifier,we used both the methods for converting raw SVMscores into probabilities and calibrated them usinga development set.
Then, we visually inspectedthe calibrated plots for each classifier and chose themethod that showed better calibration as the calibra-tion procedure for that classifier.
Plots of the pre-dicted probabilities versus true probabilities for theARGM-TMP VS ALL classifier, before and after cal-ibration are shown in Figure 2.
The performance im-provement over a classifier that is trained using allthe features for all the classes is shown in Table 5.Table 6 shows the performance of the system af-ter adding the CCG features, additional features ex-5840 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Predicted ProbabilityTrueProbabilityBefore Calibration0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Predicted ProbabilityTrueProbabilityAfter CalibrationFigure 2: Plots showing true probabilities versus predicted probabilities before and after calibration on thetest set for ARGM-TMP.tracted from the Charniak parse tree, and performingfeature selection and calibration.
Numbers in paren-theses are the corresponding baseline performances.TASK P R F1 A(%) (%) (%)Id.
86.9 (86.8) 84.2 (80.0) 85.5 (83.3)Class.
- - - 92.0 (90.1)Id.
+ Class.
82.1 (80.9) 77.9 (76.8) 79.9 (78.8)Table 6: Best system performance on all tasks usingautomatically generated syntactic parses.5 Alternative Syntactic ViewsAdding new features can improve performancewhen the syntactic representation being used forclassification contains the correct constituents.
Ad-ditional features can?t recover from the situationwhere the parse tree being used for classificationdoesn?t contain the correct constituent representingan argument.
Such parse errors account for about7% absolute of the errors (or, about half of 12.7%)for the Charniak parse based system.
To addressthese errors, we added two additional parse repre-sentations: i) Minipar dependency parser, and ii)chunking parser (Hacioglu et al, 2004).
The hope isthat these parsers will produce different errors thanthe Charniak parser since they represent differentsyntactic views.
The Charniak parser is trained onthe Penn TreeBank corpus.
Minipar is a rule baseddependency parser.
The chunking parser is trainedon PropBank and produces a flat syntactic represen-tation that is very different from the full parse treeproduced by Charniak.
A combination of the threedifferent parses could produce better results than anysingle one.5.1 Minipar-based Semantic LabelerMinipar (Lin, 1998; Lin and Pantel, 2001) is a rule-based dependency parser.
It outputs dependenciesbetween a word called head and another called mod-ifier.
Each word can modify at most one word.
Thedependency relationships form a dependency tree.The set of words under each node in Minipar?sdependency tree form a contiguous segment in theoriginal sentence and correspond to the constituentin a constituent tree.
We formulate the semantic la-beling problem in the same way as in a constituentstructure parse, except we classify the nodes thatrepresent head words of constituents.
A similar for-mulation using dependency trees derived from Tree-Bank was reported in Hacioglu (Hacioglu, 2004).In that experiment, the dependency trees were de-rived from hand-corrected TreeBank trees usinghead word rules.
Here, an SVM is trained to as-sign PropBank argument labels to nodes in Minipardependency trees using the following features:Table 8 shows the performance of the Minipar-based semantic parser.Minipar performance on the PropBank corpus issubstantially worse than the Charniak based system.This is understandable from the fact that Miniparis not designed to produce constituents that wouldexactly match the constituent segmentation used inTreeBank.
In the test set, about 37% of the argu-585PREDICATE LEMMAHEAD WORD: The word representing the node in the dependency tree.HEAD WORD POS: Part of speech of the head word.POS PATH: This is the path from the predicate to the head word throughthe dependency tree connecting the part of speech of each node in the tree.DEPENDENCY PATH: Each word that is connected to the headword has a particular dependency relationship to the word.
Theseare represented as labels on the arc between the words.
Thisfeature is the dependencies along the path that connects two words.VOICEPOSITIONTable 7: Features used in the Baseline system usingMinipar parses.Task P R F1(%) (%)Id.
73.5 43.8 54.6Id.
+ Classification 66.2 36.7 47.2Table 8: Baseline system performance on all tasksusing Minipar parses.ments do not have corresponding constituents thatmatch its boundaries.
In experiments reported byHacioglu (Hacioglu, 2004), a mismatch of about8% was introduced in the transformation from hand-corrected constituent trees to dependency trees.
Us-ing an errorful automatically generated tree, a stillhigher mismatch would be expected.
In case ofthe CCG parses, as reported by Gildea and Hock-enmaier (2003), the mismatch was about 23%.
Amore realistic way to score the performance is toscore tags assigned to head words of constituents,rather than considering the exact boundaries of theconstituents as reported by Gildea and Hocken-maier (2003).
The results for this system are shownin Table 9.Task P R F1(%) (%)CHARNIAK Id.
92.2 87.5 89.8Id.
+ Classification 85.9 81.6 83.7MINIPAR Id.
83.3 61.1 70.5Id.
+ Classification 72.9 53.5 61.7Table 9: Head-word based performance using Char-niak and Minipar parses.5.2 Chunk-based Semantic LabelerHacioglu has previously described a chunk based se-mantic labeling method (Hacioglu et al, 2004).
Thissystem uses SVM classifiers to first chunk input textinto flat chunks or base phrases, each labeled witha syntactic tag.
A second SVM is trained to assignsemantic labels to the chunks.
The system is trainedon the PropBank training data.WORDSPREDICATE LEMMASPART OF SPEECH TAGSBP POSITIONS: The position of a token in a BP using the IOB2representation (e.g.
B-NP, I-NP, O, etc.
)CLAUSE TAGS: The tags that mark token positions in a sentencewith respect to clauses.NAMED ENTITIES: The IOB tags of named entities.TOKEN POSITION: The position of the phrase with respect tothe predicate.
It has three values as ?before?, ?after?
and ?-?
(forthe predicate)PATH: It defines a flat path between the token and the predicateCLAUSE BRACKET PATTERNSCLAUSE POSITION: A binary feature that identifies whether thetoken is inside or outside the clause containing the predicateHEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.DISTANCE: Distance of the token from the predicate as a numberof base phrases, and the distance as the number of VP chunks.LENGTH: the number of words in a token.PREDICATE POS TAG: the part of speech category of the predicatePREDICATE FREQUENCY: Frequent or rare using a threshold of 3.PREDICATE BP CONTEXT: The chain of BPs centered at the predicatewithin a window of size -2/+2.PREDICATE POS CONTEXT: POS tags of words immediately precedingand following the predicate.PREDICATE ARGUMENT FRAMES: Left and right core argument patternsaround the predicate.NUMBER OF PREDICATES: This is the number of predicates inthe sentence.Table 10: Features used by chunk based classifier.Table 10 lists the features used by this classifier.For each token (base phrase) to be tagged, a set offeatures is created from a fixed size context that sur-rounds each token.
In addition to the above features,it also uses previous semantic tags that have alreadybeen assigned to the tokens contained in the linguis-tic context.
A 5-token sliding window is used for thecontext.P R F1(%) (%)Id.
and Classification 72.6 66.9 69.6Table 11: Semantic chunker performance on thecombined task of Id.
and classification.SVMs were trained for begin (B) and inside (I)classes of all arguments and outside (O) class for atotal of 78 one-vs-all classifiers.
Again, TinySVM5along with YamCha6 (Kudo and Matsumoto, 2000;Kudo and Matsumoto, 2001) are used as the SVMtraining and test software.Table 11 presents the system performances on thePropBank test set for the chunk-based system.5http://chasen.org/?taku/software/TinySVM/6http://chasen.org/?taku/software/yamcha/5866 Combining Semantic LabelersWe combined the semantic parses as follows: i)scores for arguments were converted to calibratedprobabilities, and arguments with scores below athreshold value were deleted.
Separate thresholdswere used for each parser.
ii) For the remaining ar-guments, the more probable ones among overlap-ping ones were selected.
In the chunked system,an argument could consist of a sequence of chunks.The probability assigned to the begin tag of an ar-gument was used as the probability of the sequenceof chunks forming an argument.
Table 12 showsthe performance improvement after the combina-tion.
Again, numbers in parentheses are respectivebaseline performances.TASK P R F1(%) (%)Id.
85.9 (86.8) 88.3 (80.0) 87.1 (83.3)Id.
+ Class.
81.3 (80.9) 80.7 (76.8) 81.0 (78.8)Table 12: Constituent-based best system perfor-mance on argument identification and argumentidentification and classification tasks after combin-ing all three semantic parses.The main contribution of combining both theMinipar based and the Charniak-based parsers wassignificantly improved performance on ARG1 in ad-dition to slight improvements to some other argu-ments.
Table 13 shows the effect on selected argu-ments on sentences that were altered during the thecombination of Charniak-based and Chunk-basedparses.Number of Propositions 107Percentage of perfect props before combination 0.00Percentage of perfect props after combination 45.95Before AfterP R F1 P R F1(%) (%) (%) (%)Overall 94.8 53.4 68.3 80.9 73.8 77.2ARG0 96.0 85.7 90.5 92.5 89.2 90.9ARG1 71.4 13.5 22.7 59.4 59.4 59.4ARG2 100.0 20.0 33.3 50.0 20.0 28.5ARGM-DIS 100.0 40.0 57.1 100.0 100.0 100.0Table 13: Performance improvement on parseschanged during pair-wise Charniak and Chunk com-bination.A marked increase in number of propositions forwhich all the arguments were identified correctlyfrom 0% to about 46% can be seen.
Relatively fewpredicates, 107 out of 4500, were affected by thiscombination.To give an idea of what the potential improve-ments of the combinations could be, we performedan oracle experiment for a combined system thattags head words instead of exact constituents as wedid in case of Minipar-based and Charniak-based se-mantic parser earlier.
In case of chunks, first word inprepositional base phrases was selected as the headword, and for all other chunks, the last word was se-lected to be the head word.
If the correct argumentwas found present in either the Charniak, Minipar orChunk hypotheses then that was selected.
The re-sults for this are shown in Table 14.
It can be seenthat the head word based performance almost ap-proaches the constituent based performance reportedon the hand-corrected parses in Table 3 and thereseems to be considerable scope for improvement.Task P R F1(%) (%)C Id.
92.2 87.5 89.8Id.
+ Classification 85.9 81.6 83.7C+M Id.
98.4 90.6 94.3Id.
+ Classification 93.1 86.0 89.4C+CH Id.
98.9 88.8 93.6Id.
+ Classification 92.5 83.3 87.7C+M+CH Id.
99.2 92.5 95.7Id.
+ Classification 94.6 88.4 91.5Table 14: Performance improvement on head wordbased scoring after oracle combination.
Charniak(C), Minipar (M) and Chunker (CH).Table 15 shows the performance improvement inthe actual system for pairwise combination of theparsers and one using all three.Task P R F1(%) (%)C Id.
92.2 87.5 89.8Id.
+ Classification 85.9 81.6 83.7C+M Id.
91.7 89.9 90.8Id.
+ Classification 85.0 83.9 84.5C+CH Id.
91.5 91.1 91.3Id.
+ Classification 84.9 84.3 84.7C+M+CH Id.
91.5 91.9 91.7Id.
+ Classification 85.1 85.5 85.2Table 15: Performance improvement on head wordbased scoring after combination.
Charniak (C),Minipar (M) and Chunker (CH).5877 ConclusionsWe described a state-of-the-art baseline semanticrole labeling system based on Support Vector Ma-chine classifiers.
Experiments were conducted toevaluate three types of improvements to the sys-tem: i) adding new features including features ex-tracted from a Combinatory Categorial Grammarparse, ii) performing feature selection and calibra-tion and iii) combining parses obtained from seman-tic parsers trained using different syntactic views.We combined semantic parses from a Minipar syn-tactic parse and from a chunked syntactic repre-sentation with our original baseline system whichwas based on Charniak parses.
The belief was thatsemantic parses based on different syntactic viewswould make different errors and that the combina-tion would be complimentary.
A simple combina-tion of these representations did lead to improvedperformance.8 AcknowledgementsThis research was partially supported by the ARDAAQUAINT program via contract OCG4423B andby the NSF via grants IS-9978025 and ITR/HCI0086132.
Computer time was provided by NSFARI Grant #CDA-9601817, NSF MRI Grant #CNS-0420873, NASA AIST grant #NAG2-1646, DOESciDAC grant #DE-FG02-04ER63870, NSF spon-sorship of the National Center for Atmospheric Re-search, and a grant from the IBM Shared UniversityResearch (SUR) program.We would like to thank Ralph Weischedel andScott Miller of BBN Inc. for letting us use theirnamed entity tagger ?
IdentiFinder; Martha Palmerfor providing us with the PropBank data; Dan Gildeaand Julia Hockenmaier for providing the gold stan-dard CCG parser information, and all the anony-mous reviewers for their helpful comments.ReferencesR.
E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk.
1972.
Statis-tical Inference under Order Restrictions.
Wiley, New York.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings ofNAACL, pages 132?139, Seattle, Washington.John Chen and Owen Rambow.
2003.
Use of deep linguistics features forthe recognition and labeling of semantic arguments.
In Proceedings of theEMNLP, Sapporo, Japan.Dean P. Foster and Robert A. Stine.
2004.
Variable selection in data mining:building a predictive model for bankruptcy.
Journal of American StatisticalAssociation, 99, pages 303?313.Dan Gildea and Julia Hockenmaier.
2003.
Identifying semantic roles using com-binatory categorial grammar.
In Proceedings of the EMNLP, Sapporo, Japan.Daniel Gildea and Daniel Jurafsky.
2000.
Automatic labeling of semantic roles.In Proceedings of ACL, pages 512?520, Hong Kong, October.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The necessity of syntactic parsing forpredicate argument recognition.
In Proceedings of ACL, Philadelphia, PA.Kadri Hacioglu.
2004.
Semantic role labeling using dependency trees.
In Pro-ceedings of COLING, Geneva, Switzerland.Kadri Hacioglu and Wayne Ward.
2003.
Target word detection and semantic rolechunking using support vector machines.
In Proceedings of HLT/NAACL,Edmonton, Canada.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.2003.
Shallow semantic parsing using support vector machines.
TechnicalReport TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,Colorado.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Juraf-sky.
2004.
Semantic role labeling by tagging syntactic chunks.
In Proceed-ings of CoNLL-2004, Shared Task ?
Semantic Role Labeling.Kadri Hacioglu.
2004a.
A lightweight semantic chunking model based on tag-ging.
In Proceedings of HLT/NAACL, Boston, MA.Julia Hockenmaier and Mark Steedman.
2002.
Generative models for statisticalparsing with combinatory grammars.
In Proceedings of the ACL, pages 335?342.Julia Hockenmaier and Mark Steedman.
2002a.
Acquiring compact lexicalizedgrammars from a cleaner treebank.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation (LREC-2002), Las Pal-mas, Canary Islands, Spain.Paul Kingsbury and Martha Palmer.
2002.
From Treebank to PropBank.
InProceedings of LREC, Las Palmas, Canary Islands, Spain.Taku Kudo and Yuji Matsumoto.
2000.
Use of support vector learning for chunkidentification.
In Proceedings of CoNLL and LLL, pages 142?144.Taku Kudo and Yuji Matsumoto.
2001.
Chunking with support vector machines.In Proceedings of the NAACL.Dekang Lin and Patrick Pantel.
2001.
Discovery of inference rules for questionanswering.
Natural Language Engineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation of MINIPAR.
In In Workshopon the Evaluation of Parsing Systems, Granada, Spain.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, AnnBies, Mark Ferguson, Karen Katz, and Britta Schasberger.
1994.
The PennTreebank: Annotating predicate argument structure.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.
The proposition bank:An annotated corpus of semantic roles.
To appear Computational Linguistics.John Platt.
2000.
Probabilities for support vector machines.
In A. Smola,P.
Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in LargeMargin Classifiers.
MIT press, Cambridge, MA.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.2003.
Semantic role parsing: Adding semantic structure to unstructured text.In Proceedings of ICDM, Melbourne, Florida.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.2004.
Shallow semantic parsing using support vector machines.
In Proceed-ings of HLT/NAACL, Boston, MA.Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth.
2003.
Us-ing predicate-argument structures for information extraction.
In Proceedingsof ACL, Sapporo, Japan.Nianwen Xue and Martha Palmer.
2004.
Calibrating features for semantic rolelabeling.
In Proceedings of EMNLP, Barcelona, Spain.588
