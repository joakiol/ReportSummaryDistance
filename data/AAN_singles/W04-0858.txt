Word Sense Disambiguation by Web Miningfor Word Co-occurrence ProbabilitiesPeter D. TURNEYInstitute for Information TechnologyNational Research Council of CanadaOttawa, Ontario, Canada, K1A 0R6peter.turney@nrc-cnrc.gc.caAbstractThis paper describes the National Research Coun-cil (NRC) Word Sense Disambiguation (WSD) sys-tem, as applied to the English Lexical Sample (ELS)task in Senseval-3.
The NRC system approach-es WSD as a classical supervised machine learn-ing problem, using familiar tools such as the Wekamachine learning software and Brill?s rule-basedpart-of-speech tagger.
Head words are represent-ed as feature vectors with several hundred features.Approximately half of the features are syntactic andthe other half are semantic.
The main novelty in thesystem is the method for generating the semanticfeatures, based on word co-occurrence probabilities.The probabilities are estimated using the WaterlooMultiText System with a corpus of about one ter-abyte of unlabeled text, collected by a web crawler.1 IntroductionThe Senseval-3 English Lexical Sample (ELS) taskrequires disambiguating 57 words, with an averageof roughly 140 training examples and 70 testingexamples of each word.
Each example is about aparagraph of text, in which the word that is to be dis-ambiguated is marked as the head word.
The aver-age head word has around six senses.
The trainingexamples are manually classified according to theintended sense of the head word, inferred from thesurrounding context.
The task is to use the trainingdata and any other relevant information to automat-ically assign classes to the testing examples.This paper presents the National Research Coun-cil (NRC) Word Sense Disambiguation (WSD)system, which generated our four entries forthe Senseval-3 ELS task (NRC-Fine, NRC-Fine2,NRC-Coarse, and NRC-Coarse2).
Our approach tothe ELS task is to treat it as a classical supervisedmachine learning problem.
Each example is repre-sented as a feature vector with several hundred fea-tures.
Each of the 57 ambiguous words is represent-ed with a different set of features.
Typically, aroundhalf of the features are syntactic and the other halfare semantic.
After the raw examples are convertedto feature vectors, the Weka machine learning soft-ware is used to induce a model of the training dataand predict the classes of the testing examples (Wit-ten and Frank, 1999).The syntactic features are based on part-of-speech tags, assigned by a rule-based tagger (Brill,1994).
The main innovation of the NRC WSD sys-tem is the method for generating the semantic fea-tures, which are derived from word co-occurrenceprobabilities.
We estimated these probabilitiesusing the Waterloo MultiText System with a corpusof about one terabyte of unlabeled text, collected bya web crawler (Clarke et al, 1995; Clarke and Cor-mack, 2000; Terra and Clarke, 2003).In Section 2, we describe the NRC WSD system.Our experimental results are presented in Section 3and we conclude in Section 4.2 System DescriptionThis section presents various aspects of the systemin roughly the order in which they are executed.
Thefollowing definitions will simplify the description.Head Word: One of the 57 words that are to bedisambiguated.Example: One or more contiguous sentences, illus-trating the usage of a head word.Context: The non-head words in an example.Feature: A property of a head word in a context.For instance, the feature tag hp1 NNP is the prop-erty of having (or not having) a proper noun (NNPis the part-of-speech tag for a proper noun) immedi-ately following the head word (hp1 represents thelocation head plus one).Feature Value: Features have values, whichdepend on the specific example.
For instance,tag hp1 NNP is a binary feature that has the value1 (true: the following word is a proper noun) or 0(false: the following word is not a proper noun).Feature Vector: Each example is represented bya vector.
Features are the dimensions of the vectorspace and a vector of feature values specifies a pointin the feature space.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systems2.1 PreprocessingThe NRC WSD system first assigns part-of-speechtags to the words in a given example (Brill, 1994),and then extracts a nine-word window of taggedtext, centered on the head word (i.e., four wordsbefore and after the head word).
Any remainingwords in the example are ignored (usually most ofthe example is ignored).
The window is not allowedto cross sentence boundaries.
If the head wordappears near the beginning or end of the sentence,where the window may overlap with adjacent sen-tences, special null characters fill the positions ofany missing words in the window.In rare cases, a head word appears more than oncein an example.
In such cases, the system selectsa single window, giving preference to the earliestoccurring window with the least nulls.
Thus eachexample is converted into one nine-word window oftagged text.
Windows from the training examplesfor a given head word are then used to build the fea-ture set for that head word.2.2 Syntactic FeaturesEach head word has a unique set of feature names,describing how the feature values are calculated.Feature Names: Every syntactic feature has a nameof the form matchtype position model.
There arethree matchtypes, ptag, tag, and word, in orderof increasingly strict matching.
A ptag match isa partial tag match, which counts similar part-of-speech tags, such as NN (singular noun), NNS (plu-ral noun), NNP (singular proper noun), and NNPS(plural proper noun), as equivalent.
A tag matchrequires exact matching in the part-of-speech tagsfor the word and the model.
A wordmatch requiresthat the word and the model are exactly the same,letter-for-letter, including upper and lower case.There are five positions, hm2 (head minus two),hm1 (head minus one), hd0 (head), hp1 (head plusone), and hp2 (head plus two).
Thus syntactic fea-tures use only a five-word sub-window of the nine-word window.The syntactic feature names for a head wordare generated by all of the possible legal combina-tions of matchtype, position, and model.
For ptagnames, the model can be any partial tag.
For tagnames, the model can be any tag.
For word names,the model names are not predetermined; they areextracted from the training windows for the givenhead word.
For instance, if a training window con-tains the head word followed by ?of?, then one ofthe features will be word hp1 of.For word names, the model names are notallowed to be words that are tagged as nouns, verbs,or adjectives.
These words are reserved for use inbuilding the semantic features.Feature Values: The syntactic features are allbinary-valued.
Given a feature with a name of theform matchtype position model, the feature valuefor a given window depends on whether there is amatch of matchtype between the word in the posi-tion position and the model model.
For instance,the value of tag hp1 NNP depends on whetherthe given window has a word in the position hp1(head plus one) with a tag (part-of-speech tag) thatmatches NNP (proper noun).
Similarly, the featureword hp1 of has the value 1 (true) if the givenwindow contains the head word followed by ?of?
;otherwise, it has the value 0 (false).2.3 Semantic FeaturesEach head word has a unique set of feature names,describing how the feature values are calculated.Feature Names: Most of the semantic features havenames of the form position model.
The positionnames can be pre (preceding) or fol (following).They refer to the nearest noun, verb, or adjectivethat precedes or follows the head word in the nine-word window.The model names are extracted from the trainingwindows for the head word.
For instance, if a train-ing window contains the word ?compelling?, andthis word is the nearest noun, verb, or adjective thatprecedes the head word, then one of the features willbe pre compelling.A few of the semantic features have a differentform of name, avg position sense.
In names of thisform, position can be pre (preceding) or fol (fol-lowing), and sense can be any of the possible senses(i.e., classes, labels) of the head word.Feature Values: The semantic features are allreal-valued.
For feature names of the form posi-tion model, the feature value depends on the seman-tic similarity between the word in position positionand the model word model.The semantic similarity between two words isestimated by their Pointwise Mutual Information,        , using Information Retrieval (Turney,2001; Terra and Clarke, 2003):             We estimate the probabilities in this equation byissuing queries to the Waterloo MultiText System(Clarke et al, 1995; Clarke and Cormack, 2000;Terra and Clarke, 2003).
Laplace smoothing isapplied to the PMI estimates, to avoid division byzero.weka.classifiers.meta.Bagging-W weka.classifiers.meta.MultiClassClassifier-W weka.classifiers.meta.Vote-B weka.classifiers.functions.supportVector.SMO-B weka.classifiers.meta.LogitBoost -W weka.classifiers.trees.DecisionStump-B weka.classifiers.meta.LogitBoost -W weka.classifiers.functions.SimpleLinearRegression-B weka.classifiers.trees.adtree.ADTree-B weka.classifiers.rules.JRipTable 1: Weka (version 3.4) commands for processing the feature vectors.        has a value of zero when the twowords are statistically independent.
A high posi-tive value indicates that the two words tend to co-occur, and hence are likely to be semantically relat-ed.
A negative value indicates that the presence ofone of the words suggests the absence of the other.Past work demonstrates that PMI is a good estima-tor of semantic similarity (Turney, 2001; Terra andClarke, 2003) and that features based on PMI can beuseful for supervised learning (Turney, 2003).The Waterloo MultiText System allows us to setthe neighbourhood size for co-occurrence (i.e., themeaning of    ).
In preliminary experimentswith the ELS data from Senseval-2, we got goodresults with a neighbourhood size of 20 words.For instance, if  is the noun, verb, or adjec-tive that precedes the head word and is nearest tothe head word in a given window, then the valueof pre compelling is      .
Ifthere is no preceding noun, verb, or adjective withinthe window, the value is set to zero.In names of the form avg position sense, thefeature value is the average of the feature values ofthe corresponding features.
For instance, the val-ue of avg pre argument 1 10 02 is the aver-age of the values of all of the pre model features,such that model was extracted from a training win-dow in which the head word was labeled with thesense argument 1 10 02.The idea here is that, if a testing example shouldbe labeled, say, argument 1 10 02, and   is anoun, verb, or adjective that is close to the headword in the testing example, then        should be relatively high when  is extract-ed from a training window with the same sense,argument 1 10 02, but relatively low when is extracted from a training window with a differentsense.
Thus avg position argument 1 10 02is likely to be relatively high, compared to otheravg position sense features.All semantic features with names of the formposition model are normalized by converting themto percentiles.
The percentiles are calculated sepa-rately for each feature vector; that is, each featurevector is normalized internally, with respect to itsown values, not externally, with respect to the oth-er feature vectors.
The pre features are normalizedindependently from the fol features.
The semanticfeatures with names of the form avg position senseare calculated after the other features are normal-ized, so they do not need any further normalization.Preliminary experiments with the ELS data fromSenseval-2 supported the merit of percentile nor-malization, which was also found useful in anotherapplication where features based on PMI were usedfor supervised learning (Turney, 2003).2.4 Weka ConfigurationTable 1 shows the commands that were used to exe-cute Weka (Witten and Frank, 1999).
The defaultparameters were used for all of the classifiers.
Fivebase classifiers (-B) were combined by voting.
Mul-tiple classes were handled by treating them as mul-tiple two-class problems, using a 1-against-all strat-egy.
Finally, the variance of the system was reducedwith bagging.We designed the Weka configuration by evalu-ating many different Weka base classifiers on theSenseval-2 ELS data, until we had identified fivegood base classifiers.
We then experimented withcombining the base classifiers, using a variety ofmeta-learning algorithms.
The resulting system issomewhat similar to the JHU system, which hadthe best ELS scores in Senseval-2 (Yarowsky et al,2001).
The JHU system combined four base clas-sifiers using a form of voting, called ThresholdedModel Voting (Yarowsky et al, 2001).2.5 PostprocessingThe output of Weka includes an estimate of theprobability for each prediction.
When the headword is frequently labeled U (unassignable) in thetraining examples, we ignore U examples duringtraining, and then, after running Weka, relabel thelowest probability testing examples as U.3 ResultsA total of 26 teams entered 47 systems (bothsupervised and unsupervised) in the Senseval-3ELS task.
Table 2 compares the fine-grained andSystem Fine-Grained Recall Coarse-Grained RecallBest Senseval-3 System 72.9% 79.5%NRC-Fine 69.4% 75.9%NRC-Fine2 69.1% 75.6%NRC-Coarse NA 75.8%NRC-Coarse2 NA 75.7%Median Senseval-3 System 65.1% 73.7%Most Frequent Sense 55.2% 64.5%Table 2: Comparison of NRC-Fine with other Senseval-3 ELS systems.coarse-grained scores of our four entries with otherSenseval-3 systems.With NRC-Fine and NRC-Coarse, each seman-tic feature was scored by calculating its PMI withthe head word, and then low scoring semantic fea-tures were dropped.
With NRC-Fine2 and NRC-Coarse2, the threshold for dropping features waschanged, so that many more features were retained.The Senseval-3 results suggest that it is better todrop more features.NRC-Coarse and NRC-Coarse2 were designed tomaximize the coarse score, by training them withdata in which the senses were relabeled by theircoarse sense equivalence classes.
The fine scoresfor these two systems are meaningless and should beignored.
The Senseval-3 results indicate that thereis no advantage to relabeling.The NRC systems scored roughly midwaybetween the best and median systems.
This per-formance supports the hypothesis that corpus-basedsemantic features can be useful for WSD.
In futurework, we plan to design a system that combinescorpus-based semantic features with the most effec-tive elements of the other Senseval-3 systems.For reasons of computational efficiency, we chosea relatively narrow window of nine-words aroundthe head word.
We intend to investigate whether alarger window would bring the system performanceup to the level of the best Senseval-3 system.4 ConclusionThis paper has sketched the NRC WSD system forthe ELS task in Senseval-3.
Due to space limita-tions, many details were omitted, but it is likely thattheir impact on the performance is relatively small.The system design is relatively straightforwardand classical.
The most innovative aspect of the sys-tem is the set of semantic features, which are purelycorpus-based; no lexicon was used.AcknowledgementsWe are very grateful to Egidio Terra, Charlie Clarke,and the School of Computer Science of the Univer-sity of Waterloo, for giving us a copy of the Water-loo MultiText System.
Thanks to Diana Inkpen,Joel Martin, and Mario Jarmasz for helpful discus-sions.
Thanks to the organizers of Senseval for theirservice to the WSD research community.
Thanks toEric Brill and the developers of Weka, for makingtheir software available.ReferencesEric Brill.
1994.
Some advances in transformation-based part of speech tagging.
In Proceedings ofthe 12th National Conference on Artificial Intel-ligence (AAAI-94), pages 722?727.Charles L.A. Clarke and Gordon V. Cormack.
2000.Shortest substring retrieval and ranking.
ACMTransactions on Information Systems (TOIS),18(1):44?78.Charles L.A. Clarke, G.V.
Cormack, and F.J.Burkowski.
1995.
An algebra for structured textsearch and a framework for its implementation.The Computer Journal, 38(1):43?56.Egidio L. Terra and Charles L.A. Clarke.
2003.Frequency estimates for statistical word similari-ty measures.
In Proceedings of the Human Lan-guage Technology and North American Chapterof Association of Computational Linguistics Con-ference 2003 (HLT/NAACL 2003), pages 244?251.Peter D. Turney.
2001.
Mining the Web for syn-onyms: PMI-IR versus LSA on TOEFL.
In Pro-ceedings of the Twelfth European Conferenceon Machine Learning (ECML-2001), pages 491?502.Peter D. Turney.
2003.
Coherent keyphrase extrac-tion via Web mining.
In Proceedings of the Eigh-teenth International Joint Conference on Artifi-cial Intelligence (IJCAI-03), pages 434?439.Ian H. Witten and Eibe Frank.
1999.
Data Min-ing: Practical Machine Learning Tools andTechniques with Java Implementations.
MorganKaufmann, San Mateo, CA.D.
Yarowsky, S. Cucerzan, R. Florian, C. Schafer,and R. Wicentowski.
2001.
The Johns HopkinsSENSEVAL2 system descriptions.
In Proceed-ings of SENSEVAL2, pages 163?166.
