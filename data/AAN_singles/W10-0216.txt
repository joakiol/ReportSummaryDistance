Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 131?139,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsSentiment Classification using Automatically Extracted Subgraph FeaturesShilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose?
and Eric NybergLanguage Technologies InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213{shilpaa, emayfiel, cprose, ehn}@cs.cmu.eduAbstractIn this work, we propose a novel representa-tion of text based on patterns derived from lin-guistic annotation graphs.
We use a subgraphmining algorithm to automatically derive fea-tures as frequent subgraphs from the annota-tion graph.
This process generates a very largenumber of features, many of which are highlycorrelated.
We propose a genetic program-ming based approach to feature constructionwhich creates a fixed number of strong classi-fication predictors from these subgraphs.
Weevaluate the benefit gained from evolved struc-tured features, when used in addition to thebag-of-words features, for a sentiment classi-fication task.1 IntroductionIn recent years, the topic of sentiment analysis hasbeen one of the more popular directions in the fieldof language technologies.
Recent work in super-vised sentiment analysis has focused on innovativeapproaches to feature creation, with the greatest im-provements in performance with features that in-sightfully capture the essence of the linguistic con-structions used to express sentiment, e.g.
(Wilson etal., 2004), (Joshi and Rose?, 2009)In this spirit, we present a novel approach thatleverages subgraphs automatically extracted fromlinguistic annotation graphs using efficient subgraphmining algorithms (Yan and Han, 2002).
The diffi-culty with automatically deriving complex featurescomes with the increased feature space size.
Manyof these features are highly correlated and do notprovide any new information to the model.
For ex-ample, a feature of type unigram POS (e.g.
?cam-era NN?)
doesn?t provide any additional informa-tion beyond the unigram feature (e.g.
?camera?
),for words that are often used with the same part ofspeech.
However, alongside several redundant fea-tures, there are also features that provide new infor-mation.
It is these features that we aim to capture.In this work, we propose an evolutionary ap-proach that constructs complex features from sub-graphs extracted from an annotation graph.
A con-stant number of these features are added to the un-igram feature space, adding much of the represen-tational benefits without the computational cost of adrastic increase in feature space size.In the remainder of the paper, we review priorwork on features commonly used for sentiment anal-ysis.
We then describe the annotation graph rep-resentation proposed by Arora and Nyberg (2009).Following this, we describe the frequent subgraphmining algorithm proposed in Yan and Han (2002),and used in this work to extract frequent subgraphsfrom the annotation graphs.
We then introduce ournovel feature evolution approach, and discuss ourexperimental setup and results.
Subgraph featurescombined with the feature evolution approach givespromising results, with an improvement in perfor-mance over the baseline.2 Related WorkSome of the recent work in sentiment analysis hasshown that structured features (features that capturesyntactic patterns in text), such as n-grams, depen-dency relations, etc., improve performance beyond131the bag of words approach.
Arora et al (2009) showthat deep syntactic scope features constructed fromtransitive closure of dependency relations give sig-nificant improvement for identifying types of claimsin product reviews.
Gamon (2004) found that usingdeep linguistic features derived from phrase struc-ture trees and part of speech annotations yields sig-nificant improvements on the task of predicting sat-isfaction ratings in customer feedback data.
Wilsonet al (2004) use syntactic clues derived from depen-dency parse tree as features for predicting the inten-sity of opinion phrases1.Structured features that capture linguistic patternsare often hand crafted by domain experts (Wilsonet al, 2005) after careful examination of the data.Thus, they do not always generalize well acrossdatasets and domains.
This also requires a signif-icant amount of time and resources.
By automati-cally deriving structured features, we might be ableto learn new annotations faster.Matsumoto et al (2005) propose an approach thatuses frequent sub-sequence and sub-tree mining ap-proaches (Asai et al, 2002; Pei et al, 2004) to derivestructured features such as word sub-sequences anddependency sub-trees.
They show that these featuresoutperform bag-of-words features for a sentimentclassification task and achieve the best performanceto date on a commonly-used movie review dataset.Their approach presents an automatic procedure forderiving features that capture long distance depen-dencies without much expert intervention.However, their approach is limited to sequencesor tree annotations.
Often, features that combineseveral annotations capture interesting characteris-tics of text.
For example, Wilson et al (2004), Ga-mon (2004) and Joshi and Rose?
(2009) show thata combination of dependency relations and part ofspeech annotations boosts performance.
The anno-tation graph representation proposed by Arora andNyberg (2009) is a formalism for representing sev-eral linguistic annotations together on text.
With anannotation graph representation, instances are rep-resented as graphs from which frequent subgraphpatterns may be extracted and used as features forlearning new annotations.1Although, in this work we are classifying sentences and notphrases, similar clues may be used for sentiment classificationin sentences as wellIn this work, we use an efficient frequent sub-graph mining algorithm (gSpan) (Yan and Han,2002) to extract frequent subgraphs from a linguis-tic annotation graph (Arora and Nyberg, 2009).
Anannotation graph is a general representation for ar-bitrary linguistic annotations.
The annotation graphand subgraph mining algorithm provide us a quickway to test several alternative linguistic representa-tions of text.
In the next section, we present a formaldefinition of the annotation graph and a motivatingexample for subgraph features.3 Annotation Graph Representation andFeature SubgraphsArora and Nyberg (2009) define the annotationgraph as a quadruple: G = (N,E,?, ?
), whereN is the set of nodes, E is the set of edges, s.t.E ?
N ?
N , and ?
= ?N ?
?E is the set of la-bels for nodes and edges.
?
: N ?
E ?
?
is thelabeling function for nodes and edges.
Examples ofnode labels (?N ) are tokens (unigrams) and annota-tions such as part of speech, polarity etc.
Examplesof edge labels (?E) are leftOf, dependency type etc.The leftOf relation is defined between two adjacentnodes.
The dependency type relation is defined be-tween a head word and its modifier.Annotations may be represented in an annotationgraph in several ways.
For example, a dependencytriple annotation ?good amod movie?, may be repre-sented as a d amod relation between the head word?movie?
and its modifier ?good?, or as a node d amodwith edges ParentOfGov and ParentOfDep to thehead and the modifier words.
An example of an an-notation graph is shown in Figure 1.The instance in Figure 1 describes a movie reviewcomment, ?interesting, but not compelling.?.
Thewords ?interesting?
and ?compelling?
both have pos-itive prior polarity, however, the phrase expressesnegative sentiment towards the movie.
Heuristics forspecial handling of negation have been proposed inthe literature.
For example, Pang et al (2002) ap-pend every word following a negation, until a punc-tuation, with a ?NOT?
.
Applying a similar techniqueto our example gives us two sentiment bearing fea-tures, one positive (?interesting?)
and one negative(?NOT-compelling?
), and the model may not be assure about the predicted label, since there is both132positive and negative sentiment present.In Figure 2, we show three discriminating sub-graph features derived from the annotation graph inFigure 1.
These subgraph features capture the nega-tive sentiment in our example phrase.
The first fea-ture in 2(a) captures the pattern using dependencyrelations between words.
A different review com-ment may use the same linguistic construction butwith a different pair of words, for example ?a prettygood, but not excellent story.?
This is the same lin-guistic pattern but with different words the modelmay not have seen before, and hence may not clas-sify this instance correctly.
This suggests that thefeature in 2(a) may be too specific.In order to mine general features that capture therhetorical structure of language, we may add priorpolarity annotations to the annotation graph, us-ing a lexicon such as Wilson et al (2005).
Fig-ure 2(b) shows the subgraph in 2(a) with polar-ity annotations.
If we want to generalize the pat-tern in 2(a) to any positive words, we may use thefeature subgraph in Figure 2(c) with X wild cardson words that are polar or negating.
This featuresubgraph captures the negative sentiment in bothphrases ?interesting, but not compelling.?
and ?apretty good, but not excellent story.?.
Similar gener-alization using wild cards on words may be appliedwith other annotations such as part of speech anno-tations as well.
By choosing where to put the wildcard, we can get features similar to, but more pow-erful than, the dependency back-off features in Joshiand Rose?
(2009).U_interesting U_, U_but U_not U_compelling U_.D_conj-butD_negL_POSITIVE L_POSITIVEpolQ polQposQP_VBNposQP_,posQP_CCposQP_RBposQP_JJposQP_.Figure 1: Annotation graph for sentence ?interesting, but notcompelling.?
.
Prefixes: ?U?
for unigrams (tokens), ?L?
for po-larity, ?D?
for dependency relation and ?P?
for part of speech.Edges with no label encode the ?leftOf?
relation between words.4 Subgraph Mining AlgorithmsIn the previous section, we demonstrated that sub-graphs from an annotation graph can be used to iden-U_interesting U_not U_compellingD_conj-butD_neg(a)U_interesting U_not U_compellingD_conj-butD_negL_POSITIVE L_POSITIVEpolQ polQ(b)X X XD_conj-butD_negL_POSITIVE L_POSITIVEpolQ polQ(c)Figure 2: Subgraph features from the annotation graph inFigure 1tify the rhetorical structure used to express senti-ment.
The subgraph patterns that represent generallinguistic structure will be more frequent than sur-face level patterns.
Hence, we use a frequent sub-graph mining algorithm to find frequent subgraphpatterns, from which we construct features to use inthe supervised learning algorithm.The goal in frequent subgraph mining is to findfrequent subgraphs in a collection of graphs.
Agraph G?
is a subgraph of another graph G if thereexists a subgraph isomorphism2 from G?
to G, de-noted by G?
?
G.Earlier approaches in frequent subgraph mining(Inokuchi et al, 2000; Kuramochi and Karypis,2001) used a two-step approach of first generatingthe candidate subgraphs and then testing their fre-quency in the graph database.
The second step in-volves a subgraph isomorphism test, which is NP-complete.
Although efficient isomorphism testingalgorithms have been developed making it practicalto use, with lots of candidate subgraphs to test, it can2http://en.wikipedia.org/wiki/Subgraph_isomorphism_problem133still be very expensive for real applications.gSpan (Yan and Han, 2002) uses an alternativepattern growth based approach to frequent subgraphmining, which extends graphs from a single sub-graph directly, without candidate generation.
Foreach discovered subgraph G, new edges are addedrecursively until all frequent supergraphs of G havebeen discovered.
gSpan uses a depth first search tree(DFS) and restricts edge extension to only verticeson the rightmost path.
However, there can be multi-ple DFS trees for a graph.
gSpan introduces a set ofrules to select one of them as representative.
Eachgraph is represented by its unique canonical DFScode, and the codes for two graphs are equivalent ifthe graphs are isomorphic.
This reduces the compu-tational cost of the subgraph mining algorithm sub-stantially, making gSpan orders of magnitude fasterthan other subgraph mining algorithms.
With sev-eral implementations available 3, gSpan has beencommonly used for mining frequent subgraph pat-terns (Kudo et al, 2004; Deshpande et al, 2005).
Inthis work, we use gSpan to mine frequent subgraphsfrom the annotation graph.5 Feature Construction using GeneticProgrammingA challenge to overcome when adding expressive-ness to the feature space for any text classificationproblem is the rapid increase in the feature spacesize.
Among this large set of new features, mostare not predictive or are very weak predictors, andonly a few carry novel information that improvesclassification performance.
Because of this, addingmore complex features often gives no improvementor even worsens performance as the feature space?ssignal is drowned out by noise.Riloff et al (2006) propose a feature subsump-tion approach to address this issue.
They define ahierarchy for features based on the information theyrepresent.
A complex feature is only added if itsdiscriminative power is a delta above the discrimi-native power of all its simpler forms.
In this work,we use a Genetic Programming (Koza, 1992) basedapproach which evaluates interactions between fea-3http://www.cs.ucsb.edu/?xyan/software/gSpan.htm, http://www.kyb.mpg.de/bs/people/nowozin/gboost/tures and evolves complex features from them.
Theadvantage of the genetic programing based approachover feature subsumption is that it allows us to eval-uate a feature using multiple criteria.
We show thatthis approach performs better than feature subsump-tion.A lot of work has considered this genetic pro-gramming problem (Smith and Bull, 2005).
Themost similar approaches to ours are taken by Kraw-iec (2002) and Otero et al (2002), both of which usegenetic programming to build tree feature represen-tations.
None of this work was applied to a languageprocessing task, though there has been some sim-ilar work to ours in that community, most notably(Hirsch et al, 2007), which built search queries fortopic classification of documents.
Our prior work(Mayfield and Rose?, 2010) introduced a new featureconstruction method and was effective when usingunigram features; here we extend our approach tofeature spaces which are even larger and thus moreproblematic.The Genetic Programming (GP) paradigm is mostadvantageous when applied to problems where thereis not a correct answer to a problem, but insteadthere is a gradient of partial solutions which incre-mentally improve in quality.
Potential solutions arerepresented as trees consisting of functions (non-leafnodes in the tree, which perform an action giventheir child nodes as input) and terminals (leaf nodesin the tree, often variables or constants in an equa-tion).
The tree (an individual) can then be inter-preted as a program to be executed, and the outputof that program can be measured for fitness (a mea-surement of the program?s quality).
High-fitness in-dividuals are selected for reproduction into a newgeneration of candidate individuals through a breed-ing process, where parts of each parent are combinedto form a new individual.We apply this design to a language processingtask at the stage of feature construction - given manyweakly predictive features, we would like to com-bine them in a way which produces a better feature.For our functions we use boolean statements ANDand XOR, while our terminals are selected randomlyfrom the set of all unigrams and our new, extractedsubgraph features.
Each leaf?s value, when appliedto a single sentence, is equal to 1 if that subgraph ispresent in the sentence, and 0 if the subgraph is not134present.The tree in Figure 3 is a simplified example of ourevolved features.
It combines three features, a uni-gram feature ?too?
(centre node) and two subgraphfeatures: 1) the subgraph in the leftmost node oc-curs in collocations containing ?more than?
(e.g.,?nothing more than?
or ?little more than?
), 2) thesubgraph in the rightmost node occurs in negativephrases such as ?opportunism at its most glaring?
(JJS is a superlative adjective and PRP$ is a pos-sessive pronoun).
A single feature combining theseweak indicators can be more predictive than any partalone.!"#$!
"#$%&'(($%&)(*+$,&-*+-&'./0$%&1'2$3"4&3#35$3"4&664$,&-(22$Figure 3: A tree constructed using subgraph features and GP(Simplified for illustrative purposes)In the rest of this section, we first describe thefeature construction process using genetic program-ming.
We then discuss how fitness of an individualis measured for our classification task.5.1 Feature Construction ProcessWe divide our data into two sets, training and test.We again divide our training data in half, and trainour GP features on only one half of this data4 This isto avoid overfitting the final SVM model to the GPfeatures.
In a single GP run, we produce one featureto match each class value.
For a sentiment classifica-tion task, a feature is evolved to be predictive of thepositive instances, and another feature is evolved tobe predictive of the negative documents.
We repeatthis procedure a total of 15 times (using differentseeds for random selection of features), producinga total of 30 new features to be added to the featurespace.4For genetic programming we used the ECJ toolkit(http://cs.gmu.edu/?eclab/projects/ecj/).5.2 Defining FitnessOur definition of fitness is based on the conceptsof precision and recall, borrowed from informa-tion retrieval.
We define our set of documentsas being comprised of a set of positive documentsP0, P1, P2, ...Pu and a set of negative documentsN0, N1, N2, ...Nv.
For a given individual I and doc-ument D, we define hit(I,D) to equal 1 if the state-ment I is true of that document and 0 otherwise.
Pre-cision and recall of an individual feature for predict-ing positive documents5 is then defined as follows:Prec(I) =u?i=0hit(I, Pi)u?i=0hit(I, Pi) +v?i=0hit(I,Ni)(1)Rec(I) =u?i=0hit(I, Pi)u(2)We then weight these values to give significantlymore importance to precision, using the F?
measure,which gives the harmonic mean between precisionand recall:F?
(I) =(1 + ?2)?
(Prec(I)?Rec(I))(?2 ?
Prec(I)) +Rec(I) (3)In addition to this fitness function, we add twopenalties to the equation.
The first penalty applies toprevent trees from becoming overly complex.
Oneoption to ensure that features remain moderatelysimple is to simply have a maximum depth beyondwhich trees cannot grow.
Following the work ofOtero et al (2002), we penalize trees based on thenumber of nodes they contain.
This discouragesbloat, i.e.
sections of trees which do not contribute tooverall accuracy.
This penalty, known as parsimonypressure, is labeled PP in our fitness function.The second penalty is based on the correlation be-tween the feature being constructed, and the sub-graphs and unigrams which appear as nodes withinthat individual.
Without this penalty, a feature may5Negative precision and recall are defined identically, withobvious adjustments to test for negative documents instead ofpositive.135often be redundant, taking much more complexityto represent the same information that is capturedwith a simple unigram.
We measure correlation us-ing Pearson?s product moment, defined for two vec-tors X , Y as:?x,y =E[(X ?
?X)(Y ?
?Y )]?X?Y(4)This results in a value from 1 (for perfect align-ment) to -1 (for inverse alignment).
We assign apenalty for any correlation past a cutoff.
This func-tion is labeled CC (correlation constraint) in our fit-ness function.Our fitness function therefore is:Fitness = F 18+ PP + CC (5)6 Experiments and ResultsWe evaluate our approach on a sentiment classifi-cation task, where the goal is to classify a moviereview sentence as expressing positive or negativesentiment towards the movie.6.1 Data and Experimental SetupData: The dataset consists of snippets from Rot-ten Tomatoes (Pang and Lee, 2005) 6.
It consistsof 10662 snippets/sentences total with equal num-ber positive and negative sentences (5331 each).This dataset was created and used by Pang and Lee(2005) to train a classifier for identifying positivesentences in a full length review.
We use the first8000 (4000 positive, 4000 negative) sentences astraining data and evaluate on remaining 2662 (1331positive, 1331 negative) sentences.
We added partof speech and dependency triple annotations to thisdata using the Stanford parser (Klein and Manning,2003).Annotation Graph: For the annotation graph rep-resentation, we used Unigrams (U), Part of Speech(P) and Dependency Relation Type (D) as labels forthe nodes, and ParentOfGov and ParentOfDep as la-bels for the edges.
For a dependency triple such as?amod good movie?, five nodes are added to the an-notation graph as shown in Figure 4(a).
ParentOf-Gov and ParentOfDep edges are added from the6http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gzD_amodU_goodP_JJP_NNU_movieParentofGovParentofGovParentofDepParentofDep(a)D_amodU_goodP_NNParentofGovParentofDep(b)D_amodXP_JJP_NNXposQParentofGovParentofDepposQ(c)Figure 4: Annotation graph and a feature subgraph fordependency triple annotation ?amod good camera?.
(c)shows an alternative representation with wild cardsdependency relation node D amod to the unigramnodes U good and U movie.
These edges are alsoadded for the part of speech nodes that correspondto the two unigrams in the dependency relation, asshown in Figure 4(a).
This allows the algorithm tofind general patterns, based on a dependency rela-tion between two part of speech nodes, two unigramnodes or a combination of the two.
For example,a subgraph in Figure 4(b) captures a general pat-tern where good modifies a noun.
This feature ex-ists in ?amod good movie?, ?amod good camera?and other similar dependency triples.
This feature issimilar to the the dependency back-off features pro-posed in Joshi and Rose?
(2009).The extra edges are an alternative to putting wildcards on words, as proposed in section 3.
On theother hand, putting a wild card on every word inthe annotation graph for our example (Figure 4(c)),will only give features based on dependency rela-tions between part of speech annotations.
Thus, thewild card based approach is more restrictive than136adding more edges.
However, with lots of edges, thecomplexity of the subgraph mining algorithm andthe number of subgraph features increases tremen-dously.Classifier: For our experiments we use SupportVector Machines (SVM) with a linear kernel.
Weuse the SVM-light7 implementation of SVM withdefault settings.Parameters: The gSpan algorithm requires settingthe minimum support threshold (minsup) for thesubgraph patterns to extract.
Support for a subgraphis the number of graphs in the dataset that containthe subgraph.
We experimented with several valuesfor minimum support and minsup = 2 gave us thebest performance.For Genetic Programming, we used the same pa-rameter settings as described in Mayfield and Rose?
(2010), which were tuned on a different dataset8than one used in this work, but it is from the samemovie review domain.
We also consider one alter-ation to these settings.
As we are introducing manynew and highly correlated features to our featurespace through subgraphs, we believe that a stricterconstraint must be placed on correlation betweenfeatures.
To accomplish this, we can set our correla-tion penalty cutoff to 0.3, lower than the 0.5 cutoffused in prior work.
Results for both settings are re-ported.Baselines: To the best of our knowledge, there isno supervised machine learning result published onthis dataset.
We compare our results with the fol-lowing baselines:?
Unigram-only Baseline: In sentiment analysis,unigram-only features have been a strong base-line (Pang et al, 2002; Pang and Lee, 2004).We only use unigrams that occur in at leasttwo sentences of the training data same as Mat-sumoto et al (2005).
We also filter out stopwords using a small stop word list9.?
?2 Baseline: For our training data, after filter-ing infrequent unigrams and stop words, we get7http://svmlight.joachims.org/8Full movie review data by Pang et al (2002)9http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html(with one modification: removed ?will?, added ?this?
)8424 features.
Adding subgraph features in-creases the total number of features to 44, 161,a factor of 5 increase in size.
Feature selec-tion can be used to reduce this size by select-ing the most discriminative features.
?2 featureselection (Manning et al, 2008) is commonlyused in the literature.
We compare two methodsof feature selection with ?2, one which rejectsfeatures if their ?2 score is not significant at the0.05 level, and one that reduces the number offeatures to match the size of our feature spacewith GP.?
Feature Subsumption (FS): Following the ideain Riloff et al (2006), a complex featureC is discarded if IG(S) ?
IG(C) ?
?,where IG is Information Gain and S isa simple feature that representationally sub-sumes C, i.e.
the text spans that match Sare a superset of the text spans that matchC.
In our work, complex features are sub-graph features and simple features are uni-gram features contained in them.
For example,(D amod) Edge ParentOfDep (U bad) isa complex feature for which U bad is a sim-ple feature.
We tried same values for ?
?
{0.002, 0.001, 0.0005}, as suggested in Riloffet al (2006).
Since all values gave us samenumber of features, we only report a single re-sult for feature subsumption.?
Correlation (Corr): As mentioned earlier,some of the subgraph features are highly corre-lated with unigram features and do not providenew knowledge.
A correlation based filter forsubgraph features can be used to discard a com-plex feature C if its absolute correlation with itssimpler feature (unigram feature) is more thana certain threshold.
We use the same thresholdas used in the GP criterion, but as a hard filterinstead of a penalty.6.2 Results and DiscussionIn Table 1, we present our results.
As can beseen, subgraph features when added to the unigrams,without any feature selection, decrease the perfor-mance.
?2 feature selection with fixed feature spacesize provides a very small gain over unigrams.
Allother feature selection approaches perform worse137Settings #Features Acc.
?Uni 8424 75.66 -Uni + Sub 44161 75.28 -0.38Uni + Sub, ?2 sig.
3407 74.68 -0.98Uni + Sub, ?2 size 8454 75.77 +0.11Uni + Sub, (FS) 18234 75.47 -0.19Uni + Sub, (Corr) 18980 75.24 -0.42Uni + GP (U) ?
8454 76.18 +0.52Uni + GP (U+S) ?
8454 76.48 +0.82Uni + GP (U+S) ?
8454 76.93 +1.27Table 1: Experimental results for feature spaces with un-igrams, with and without subgraph features.
Feature se-lection with 1) fixed significance level (?2 sig.
), 2) fixedfeature space size (?2 size), 3) Feature Subsumption (FS)and 4) Correlation based feature filtering (Corr)).
GP fea-tures for unigrams only {GP(U)}, or both unigrams andsubgraph features {GP(U+S)}.
Both the settings fromMayfield and Rose?
(2010) (?)
and more stringent correla-tion constraint (?)
are reported.
#Features is the num-ber of features in the training data.
Acc is the accuracyand ?
is the difference from unigram only baseline.
Bestperforming feature configuration is highlighted in bold.than the unigram-only approach.
With GP, we ob-serve a marginally significant gain (p < 0.1) in per-formance over unigrams, calculated using one-wayANOVA.
Benefit from GP is more when subgraphfeatures are used in addition to the unigram features,for constructing more complex pattern features.
Ad-ditionally, our performance is improved when weconstrain the correlation more severely than in previ-ously published research, supporting our hypothesisthat this is a helpful way to respond to the problemof redundancy in subgraph features.A problem that we see with ?2 feature selection isthat several top ranked features may be highly cor-related.
For example, the top 5 features based on ?2score are shown in Table 2; it is immediately obvi-ous that the features are highly redundant.With GP based feature construction, we can con-sider this relationship between features, and con-struct new features as a combination of selected un-igram and subgraph features.
With the correlationcriterion in the evolution process, we are able tobuild combined features that provide new informa-tion compared to unigrams.The results we present are for the best perform-(D advmod) Edge ParentOfDep (U too)U tooU badU movie(D amod) Edge ParentOfDep (U bad)Table 2: Top features based on ?2 scoreing parameter configuration that we tested, after aseries of experiments.
We realize that this places usin danger of overfitting to the particulars of this dataset, however, the data set is large enough to partiallymitigate this concern.7 Conclusion and Future WorkWe have shown that there is additional informationto be gained from text beyond words, and demon-strated two methods for increasing this information -a subgraph mining approach that finds common syn-tactic patterns that capture sentiment-bearing rhetor-ical structure in text, and a feature constructiontechnique that uses genetic programming to com-bine these more complex features without the redun-dancy, increasing the size of the feature space onlyby a fixed amount.
The increase in performance thatwe see is small but consistent.In the future, we would like to extend this work toother datasets and other problems within the field ofsentiment analysis.
With the availability of severaloff-the-shelf linguistic annotators, we may add morelinguistic annotations to the annotation graph andricher subgraph features may be discovered.
Thereis also additional refinement that can be performedon our genetic programming fitness function, whichis expected to improve the quality of our features.AcknowledgmentsThis work was funded in part by the DARPA Ma-chine Reading program under contract FA8750-09-C-0172, and in part by NSF grant DRL-0835426.We would like to thank Dr. Xifeng Yan and MarisaThoma for the gSpan code.ReferencesShilpa Arora, Mahesh Joshi and Carolyn P. Rose?.
2009.Identifying Types of Claims in Online Customer Re-138views.
Proceedings of the HLT/NAACL.Shilpa Arora and Eric Nyberg.
2009.
Interactive Anno-tation Learning with Indirect Feature Voting.
Proceed-ings of the HLT/NAACL (Student Research Work-shop).Tatsuya Asai, Kenji Abe, Shinji Kawasoe, HiroshiSakamoto and Setsuo Arikawa.
2002.
Efficient sub-structure discovery from large semi-structured data.Proceedings of SIAM Int.
Conf.
on Data Mining(SDM).Mukund Deshpande , Michihiro Kuramochi , Nikil Waleand George Karypis.
2005.
Frequent Substructure-Based Approaches for Classifying Chemical Com-pounds.
IEEE Transactions on Knowledge and DataEngineering.Michael Gamon.
2004.
Sentiment classification on cus-tomer feedback data: noisy data, large feature vec-tors, and the role of linguistic analysis, Proceedingsof COLING.Laurence Hirsch, Robin Hirsch and Masoud Saeedi.2007.
Evolving Lucene Search Queries for Text Clas-sification.
Proceedings of the Genetic and Evolution-ary Computation Conference.Mahesh Joshi and Carolyn P. Rose?.
2009.
GeneralizingDependency Features for Opinion Mining.
Proceed-ings of the ACL-IJCNLP Conference (Short Papers).Akihiro Inokuchi, Takashi Washio and Hiroshi Motoda.2000.
An Apriori-based Algorithm for Mining Fre-quent Substructures from Graph Data.
Proceedingsof PKDD.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
Proceedings of the main con-ference of the ACL.John Koza.
1992.
Genetic Programming: On the Pro-gramming of Computers by Means of Natural Selec-tion.
MIT Press.Krzysztof Krawiec.
2002.
Genetic programming-basedconstruction of features for machine learning andknowledge discovery tasks.
Genetic Programming andEvolvable Machines.Taku Kudo, Eisaku Maeda and Yuji Matsumoto.
2004.An Application of Boosting to Graph Classification.Proceedings of NIPS.Michihiro Kuramochi and George Karypis.
2002.
Fre-quent Subgraph Discovery.
Proceedings of ICDM.Christopher D. Manning, Prabhakar Raghavan and Hin-rich Schtze.
2008.
Introduction to Information Re-trieval.
Proceedings of PAKDD.Shotaro Matsumoto, Hiroya Takamura and Manabu Oku-mura.
2005.
Sentiment Classification Using WordSub-sequences and Dependency Sub-trees.
Proceed-ings of PAKDD.Elijah Mayfield and Carolyn Penstein-Rose?.
2010.Using Feature Construction to Avoid Large FeatureSpaces in Text Classification.
Proceedings of the Ge-netic and Evolutionary Computation Conference.Fernando Otero, Monique Silva, Alex Freitas and JulioNievola.
2002.
Genetic Programming for AttributeConstruction in Data Mining.
Proceedings of the Ge-netic and Evolutionary Computation Conference.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classication using Ma-chine Learning Techniques.
Proceedings of EMNLP.Bo Pang and Lillian Lee.
2004.
A Sentimental Educa-tion: Sentiment Analysis Using Subjectivity Summa-rization Based on Minimum Cuts.
Proceedings of themain conference of ACL.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
Proceedings of the main con-ference of ACL.Jian Pei, Jiawei Han, Behzad Mortazavi-asl, JianyongWang, Helen Pinto, Qiming Chen, Umeshwar Dayaland Mei-chun Hsu.
2004.
Mining Sequential Pat-terns by Pattern-Growth: The PrefixSpan Approach.Proceedings of IEEE Transactions on Knowledge andData Engineering.Ellen Riloff, Siddharth Patwardhan and Janyce Wiebe.2006.
Feature Subsumption for Opinion Analysis.Proceedings of the EMNLP.Matthew Smith and Larry Bull.
2005.
Genetic Program-ming with a Genetic Algorithm for Feature Construc-tion and Selection.
Genetic Programming and Evolv-able Machines.Theresa Wilson, Janyce Wiebe and Rebecca Hwa.
2004.Just How Mad Are You?
Finding Strong and WeakOpinion Clauses.
Proceedings of AAAI.Theresa Wilson, Janyce Wiebe and Paul Hoff-mann.
2005.
Recognizing Contextual Polarityin Phrase-Level Sentiment Analysis.
Proceedings ofHLT/EMNLP.Xifeng Yan and Jiawei Han.
2002. gSpan: Graph-Based Substructure Pattern Mining.
UIUC Techni-cal Report, UIUCDCS-R-2002-2296 (shorter versionin ICDM?02).139
