Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336?344,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsPerson Identification from Text and Speech Genre SamplesJade Goldstein-StewartU.S.
Department of Defensejadeg@acm.orgRansom WinderThe MITRE CorporationHanover, MD, USArwinder@mitre.orgRoberta Evans SabinLoyola UniversityBaltimore, MD, USAres@loyola.eduAbstractIn this paper, we describe experiments con-ducted on identifying a person using a novelunique correlated corpus of text and audiosamples of the person?s communication in sixgenres.
The text samples include essays,emails, blogs, and chat.
Audio samples werecollected from individual interviews and groupdiscussions and then transcribed to text.
Foreach genre, samples were collected for six top-ics.
We show that we can identify the com-municant with an accuracy of 71% for six foldcross validation using an average of 22,000words per individual across the six genres.For person identification in a particular genre(train on five genres, test on one), an averageaccuracy of 82% is achieved.
For identifica-tion from topics (train on five topics, test onone), an average accuracy of 94% is achieved.We also report results on identifying a per-son?s communication in a genre using text ge-nres only as well as audio genres only.1 IntroductionCan one identify a person from samples ofhis/her communication?
What common patternsof communication can be used to identifypeople?
Are such patterns consistent across va-rying genres?People tend to be interested in subjects andtopics that they discuss with friends, family, col-leagues and acquaintances.
They can communi-cate with these people textually via email, textmessages and chat rooms.
They can also com-municate via verbal conversations.
Other formsof communication could include blogs or evenformal writings such as essays or scientific ar-ticles.
People communicating in these different?genres?
may have different stylistic patterns andwe are interested in whether or not we couldidentify people from their communications indifferent genres.The attempt to identify authorship of writtentext has a long history that predates electroniccomputing.
The idea that features such as aver-age word length and average sentence lengthcould allow an author to be identified dates toMendenhall (1887).
Mosteller and Wallace(1964) used function words in a groundbreakingstudy that identified authors of The FederalistPapers.
Since then many attempts at authorshipattribution have used function words and otherfeatures, such as word class frequencies andmeasures derived from syntactic analysis, oftencombined using multivariable statistical tech-niques.Recently, McCarthy (2006) was able to diffe-rentiate three authors?
works, and Hill and Prov-ost (2003), using a feature of co-citations,showed that they could successfully identifyscientific articles by the same person, achieving85% accuracy when the person has authored over100 papers.
Levitan and Argamon (2006) andMcCombe (2002) further investigated authorshipidentification of The Federalist Papers (threeauthors).The genre of the text may affect the authorshipidentification task.
The attempt to characterizegenres dates to Biber (1988) who selected 67linguistic features and analyzed samples of 23spoken and written genres.
He determined sixfactors that could be used to identify written text.Since his study, new ?cybergenres?
haveevolved, including email, blogs, chat, and textmessaging.
Efforts have been made to character-ize the linguistic features of these genres (Baron,2003; Crystal, 2001; Herring, 2001; Shepherdand Watters, 1999; Yates, 1996).
The task iscomplicated by the great diversity that can beexhibited within even a single genre.
Email canbe business-related, personal, or spam; the style336can be tremendously affected by demographicfactors, including gender and age of the sender.The context of communication influences lan-guage style (Thomson and Murachver, 2001;Coupland, et al, 1988).
Some people use ab-breviations to ease the efficiency of communica-tion in informal genres ?
items that one wouldnot find in a formal essay.
Informal writing mayalso contain emoticons (e.g., ?:-)?
or ???)
toconvey mood.Successes have been achieved in categorizingweb page decriptions (Calvo, et al, 2004) andgenre determination (Goldstein-Stewart, et al,2007; Santini 2007).
Genders of authors havebeen successfully identified within the BritishNational Corpus (Koppel, et al, 2002).
Inauthorship identification, recent research has fo-cused on identifying authors within a particulargenre: email collections, news stories, scientificpapers, listserv forums, and computer programs(de Vel, et al, 2001; Krsul and Spafford, 1997;Madigan, et al, 2005; McCombe, 2002).
In theKDD Cup 2003 Competitive Task, systems at-tempted to identify successfully scientific articlesauthored by the same person.
The best system(Hill and Provost, 2003) was able to identifysuccessfully scientific articles by the same per-son 45% of the time; for authors with over 100papers, 85% accuracy was achieved.Are there common features of communicationof an individual across and within genres?
Un-doubtedly, the lack of corpora has been an impe-diment to answering this question, as gatheringpersonal communication samples faces consider-able privacy and accessibility hurdles.
To ourknowledge, all previous studies have focused onindividual communications in one or possiblytwo genres.To analyze, compare, and contrast the com-munication of individuals across and within dif-ferent modalities, we collected a corpus consist-ing of communication samples of 21 people insix genres on six topics.
We believe this corpusis the first attempt to create such a correlatedcorpus.From this corpus, we are able to perform expe-riments on person identification.
Specifically,this means recognizing which individual of a setof people composed a document or spoke an ut-terance which was transcribed.
We believe usingtext and transcribed speech in this manner is anovel research area.
In particular, the followingtypes of experiments can be performed:- Identification of person in a novel genre(using five genres as training)- Identification of person in a novel topic(using five topics as training)- Identification of person in written genres,after training on the two spoken genres- Identification of person in spoken genres,after training on the written genres- Identification of person in written genres,after training on the other written genresIn this paper, we discuss the formation andstatistics of this corpus and report results foridentifying individual people using techniquesthat utilize several different feature sets.2 Corpus CollectionOur interest was in the research question: can aperson be identified from their writing and audiosamples?
Since we hypothesize that peoplecommunicate about items of interest to themacross various genres, we decided to test thistheory.
Email and chat were chosen as textualgenres (Table 1), since text messages, althoughvery common, were not easy to collect.
We alsocollected blogs and essays as samples of textualgenres.
For audio genres, to simulateconversational speech as much as possible, wecollected data from interviews and discussiongroups that consisted of sets of subjectsparticipating in the study.
Genres labeled ?peergive and take?
allowed subjects to interact.Such a collection of genres allows us toexamine both conversational and non-conversational genres, both written and spokenmodalities, and both formal and informal writingwith the aim of contrasting and comparingcomputer-mediated and non-computer-mediatedgenres as well as informal and formal genres.GenreCom-puter-me-diatedPeerGiveandTakeModeConversa-tionalAu-dienceEmail yes no text yes ad-dresseeEssay No no text no unspecInter-viewNo no speech yes inter-viewerBlog yes yes text no worldChat yes yes text yes groupDis-cussionNo yes speech yes groupTable 1.
GenresIn order to ensure that the students could pro-duce enough data, we chose six topics that werecontroversial and politically and/or socially rele-337vant for college students from among whom thesubjects would be drawn.
These six topics werechosen from a pilot study consisting of twelvetopics, in which we analyzed the amount of in-formation that people tended to ?volunteer?
onthe topics as well as their thoughts about beingable to write/speak on such a topic.
The six top-ics are listed in Table 2.Topic QuestionChurch Do you feel the Catholic Churchneeds to change its ways to adapt tolife in the 21st Century?Gay Marriage While some states have legalized gaymarriage, others are still opposed toit.
Do you think either side is right orwrong?Privacy Rights Recently, school officials prevented aschool shooting because one of theshooters posted a myspace bulletin.Do you think this was an invasion ofprivacy?Legalization ofMarijuanaThe city of Denver has decided tolegalize small amounts of marijuanafor persons over 21.
How do you feelabout this?War in Iraq The controversial war in Iraq hasmade news headlines almost everyday since it began.
How do you feelabout the war?GenderDiscriminationDo you feel that gender discrimina-tion is still an issue in the present-dayUnited States?Table 2.
TopicsThe corpus was created in three phases(Goldstein-Stewart, 2008).
In Phase I, emails,essays and interviews were collected.
In PhaseII, blogs and chat and discussion groups werecreated and samples collected.
For blogs, sub-jects blogged over a period of time and couldread and/or comment on other subjects?
blogs intheir own blog.
A graduate research assistantacted as interviewer and discussion and chatgroup moderator.Of the 24 subjects who completed Phase I, 7decided not to continue into Phase II.
Sevenadditional students were recruited for Phase II.In Phase III, these replacement students werethen asked to provide samples for the Phase Igenres.
Four students fully complied, resultingin a corpus with a full set of samples for 21subjects, 11 women and 10 men.All audio recordings, interviews and discus-sions, were transcribed.
Interviewer/moderatorcomments were removed and, for each discus-sion, four individual files, one for each partici-pant?s contribution, were produced.Our data is somewhat homogeneous: it sam-ples only undergraduate university students andwas collected in controlled settings.
But we be-lieve that controlling the topics, genres, and de-mographics of subjects allows the elimination ofmany variables that effect communicative styleand aids the identification of common features.3 Corpus Statistics3.1 Word CountThe mean word counts for the 21 students pergenre and per topic are shown in Figures 1 and 2,respectively.
Figure 1 shows that the studentsproduced more content in the directly interactivegenres ?
interview and discussion (the spokengenres) as well as chat (a written genre).Figure 1.
Mean word counts for gender and genreFigure 2.
Mean word counts for gender and topic338The email genre had the lowest mean wordcount, perhaps indicating that it is a genre in-tended for succinct messaging.3.2 Word Usage By IndividualsWe performed an analysis of the word usage ofindividuals.
Among the top 20 most frequentlyoccurring words, the most frequent word used byall males was ?the?.
For the 11 females, six mostfrequently used ?the?, four used ?I?, and oneused ?like?.
Among abbreviations, 13 individu-als used ?lol?.
Abbreviations were mainly usedin chat.
Other abbreviations were used to vary-ing degrees such as the abbreviation ?u?.
Emoti-cons were used by five participants.4 Classification4.1 FeaturesFrequencies of words in word categories weredetermined using Linguistic Inquiry and WordCount (LIWC).
LIWC2001 analyzes text andproduces 88 output variables, among them wordcount and average words per sentence.
All oth-ers are percentages, including percentage ofwords that are parts of speech or belong to givendictionaries (Pennebaker, et al, 2001).
Defaultdictionaries contain categories of words that in-dicate basic emotional and cognitive dimensionsand were used here.
LIWC was designed forboth text and speech and has categories, suchnegations, numbers, social words, and emotion.Refer to LIWC (www.liwc.net) for a full descrip-tion of categories.
Here the 88 LIWC featuresare denoted feature set L.From the original 24 participants?
documentsand the new 7 participants?
documents fromPhase II, we aggregated all samples from all ge-nres and computed the top 100 words for malesand for females, including stop words.
Sixwords differed between males and females.
Ofthese top words, the 64 words with counts thatvaried by 10% or more between male and femaleusage were selected.
Excluded from this listwere 5 words that appeared frequently but werehighly topic-specific: ?catholic?, ?church?, ?ma-rijuana?, ?marriage?, and ?school.
?Most of these words appeared on a large stopword list (www.webconfs.com/stop-words.php).Non-stop word terms included the word ?feel?,which was used more frequently by females thanmales, as well as the terms ?yea?
and ?lot?
(usedmore commonly by women) and ?uh?
(usedmore commonly by men).
Some stop wordswere used more by males (?some?, ?any?
), oth-ers by females (?I?, ?and?).
Since this set mainlyconsists of stop words, we refer to it as the func-tional word features or set F.The third feature set (T) consisted of the fivetopic specific words excluded from F.The fourth feature set (S) consisted of the stopword list of 659 words mentioned above.The fifth feature set (I) we consider informalfeatures.
It contains nine common words not inset S: ?feel?, ?lot?, ?uh?, ?women?, ?people?,?men?, ?gonna?, ?yea?
and ?yeah?.
This set alocontains the abbreviations and emotional expres-sions ?lol?, ?ur?, ?tru?, ?wat?, and ?haha?.
Someof the expressions could be characteristic of par-ticular individuals.
For example the term ?wat?was consistently used by one individual in theinformal chat genre.Another feature set (E) was built around theemoticons that appeared in the corpus.
Theseincluded ?
:)?, ?
:(?, ?
:-(?, ?
;)?, ?
:-/?, and ?>:o)?.For our results, we use eight feature set com-binations: 1.
All 88 LIWC features (denoted L);2.
LIWC and functional word features, (L+F); 3.LIWC plus all functional word features and thetopic words (L+F+T); 4.
LIWC plus all function-al word features and emoticons (L+F+E); 5.LIWC plus all stop word features (L+S); 6.LIWC plus all stop word and informal features(L+S+I); 7.
LIWC supplemented by informal,topic, and stop word features, (L+S+I+T).
Notethat, when combined, sets S and I cover set F.4.2 ClassifiersClassification of all samples was performed us-ing four classifiers of the Weka workbench, ver-sion 3.5 (Witten and Frank, 2005).
All wereused with default settings except the RandomForest classifier (Breiman, 2001), which used100 trees.
We collected classification results forNa?ve-Bayes, J48 (decision tree), SMO (supportvector machine) (Cortes and Vapnik, 1995; Platt,1998) and RF (Random Forests) methods.5 Person Identification Results5.1 Cross Validation Across GenresTo identify a person as the author of a text, sixfold cross validation was used.
All 756 sampleswere divided into 126 ?documents,?
each con-sisting of all six samples of a person?s expressionin a single genre, regardless of topic.
There is abaseline of approximately 5% accuracy if ran-domly guessing the person.
Table 3 shows the339accuracy results of classification using combina-tions of the feature sets and classifiers.The results show that SMO is by far the bestclassifier of the four and, thus, we used only thisclassifier on subsequent experiments.
L+S per-formed better alone than when adding the infor-mal features ?
a surprising result.Table 4 shows a comparison of results usingfeature sets L+F and L+F+T.
The five topicwords appear to grant a benefit in the best trainedcase (SMO).Table 5 shows a comparison of results usingfeature sets L+F and L+F+E, and this shows thatthe inclusion of the individual emoticon featuresdoes provide a benefit, which is interesting con-sidering that these are relatively few and are typ-ically concentrated in the chat documents.Feature SMO RF100 J48 NBL 52 30 15 17L+F 60 44 21 25L+S 71 42 19 33L+S+I 71 39 17 33L+S+I+T 71 40 17 33Table 3.
Person identification accuracy (%) using sixfold cross validationFeature SMO RF100 J48 NBL+F 60 44 21 25L+F+T 67 40 21 25Table 4.
Accuracy (%) using six fold cross validationwith and without topic word features (T)Feature SMO RF100 J48 NBL+F 60 44 21 25L+F+E 65 41 21 25Table 5.
Accuracy (%) using six fold cross validationwith and without emoticon features (E)5.2 Predict Communicant in One GenreGiven Information on Other GenresThe next set of experiments we performed was toidentify a person based on knowledge of the per-son?s communication in other genres.
We firsttrain on five genres, and we then test on one ?
a?hold out?
or test genre.Again, as in six fold cross validation, a total of126 ?documents?
were used: for each genre, 21samples were constructed, each the concatena-tion of all text produced by an individual in thatgenre, across all topics.
Table 6 shows the re-sults of this experiment.
The result of 100% forL+F, L+F+T, and L+F+E in email was surpris-ing, especially since the word counts for emailwere the lowest.
The lack of difference in L+Fand L+F+E results is not surprising since theemoticon features appear only in chat docu-ments, with one exception of a single emoticonin a blog document (?:-/?
), which did not appearin any chat documents.
So there was no emoti-con feature that appeared across different genres.SMO HOLD OUT (TEST GENRE)Features A B C D E S IL 60 76 52 43 76 81 29L+F 75 81 57 48 100 90 71L+F+T 76 86 62 52 100 86 71L+F+E 75 81 57 48 100 90 71L+S 82 81 67 67 86 90 100L+S+I 79 86 52 57 86 90 100L+S+I+T 81 86 52 67 90 90 100Table 6.
Person identification accuracy (%) trainingwith SMO on 5 genres and testing on 1.
A=Averageover all genres, B=Blog, C=Chat, D=Discussion,E=Email, S=Essay, I=InterviewTrain Test L+F L+F+TCDSI Email 67 95BDSI Email 71 52BCSI Email 76 100BCDI Email 57 90BCDS Email 57 81Table 7.
Accuracy (%) using SMO for predictingemail author after training on 4 other genres.
B=Blog,C=Chat, D=Discussion, S=Essay, I=InterviewWe attempted to determine which genres weremost influential in identifying email authorship,by reducing the number of genres in its trainingset.
Results are reported in Table 7.
The differ-ence between the two sets, which differ only infive topic specific word features, is more markedhere.
The lack of these features causes accuracyto drop far more rapidly as the training set is re-duced.
It also appears that the chat genre is im-portant when identifying the email genre whentopical features are included.
This is probablynot just due to the volume of data since discus-sion groups also have a great deal of data.
Weneed to investigate further the reason for such ahigh performance on the email genre.The results in Table 6 are also interesting forthe case of L+S (which has more stop words thanL+F).
With this feature set, classification for theinterview genre improved significantly, whilethat of email decreased.
This may indicate thatthe set of stop words may be very genre specific?
a hypothesis we will test in future work.
If thisin indeed the case, perhaps certain different sets340of stop words may be important for identifyingcertain genres, genders and individual author-ship.
Previous results indicate that the usage ofcertain stop words as features assists with identi-fying gender (Sabin, et al, 2008).Table 6 also shows that, using the informalwords (feature set I) decreased performance intwo genres: chat (the genre in which the abbrevi-ations are mostly used) and discussion.
We planto run further experiments to investigate this.The sections that follow will typically show theresults achieved with L+F and L+S features.Train\Test B C D E S IBlog 100 14 14 76 57 5Chat 24 100 29 38 19 10Discussion 10 5 100 5 10 29Email 43 10 5 100 48 0Essay 67 5 5 33 100 5Interview 5 5 5 5 5 100Table 8.
Accuracy (%) using SMO for predicting per-son between genres after training on one genre usingL+F featuresTable 8 displays the accuracies when the L+Ffeature set of single genre is used for training amodel tested on one genre.
This generally sug-gests the contribution of each genre when all areused in training.
When the training and testingsets are the same, 100% accuracy is achieved.Examining this chart, the highest accuracies areachieved when training and test sets are textual.Excluding models trained and tested on the samegenre, the average accuracy for training and test-ing within written genres is 36% while the aver-age accuracy for training and testing within spo-ken genres is 17%.
Even lower are average ac-curacies of the models trained on spoken andtested on textual genres (9%) and the modelstrained on textual and tested on spoken genres(6%).
This indicates that the accuracies that fea-ture the same mode (textual or spoken) in train-ing and testing tend to be higher.Of particular interest here is further examina-tion of the surprising results of testing on emailwith the L+F feature set.
Of these tests, a modeltrained on blogs achieved the highest score, per-haps due to a greater stylistic similarity to emailthan the other genres.
This is also the highestscore in the chart apart from cases where trainand test genres were the same.
Training on chatand essay genres shows some improvement overthe baseline, but models trained with the twospoken genres do not rise above baseline accura-cy when tested on the textual email genre.5.3 Predict Communicant in One TopicGiven Information on Five TopicsThis set of experiments was designed to deter-mine if there was no training data provided for acertain topic, yet there were samples of commu-nication for an individual across genres for othertopics, could an author be determined?SMO HOLD OUT (TEST TOPIC)Features Avg Ch Gay Iraq Mar Pri SexL+F 87 81 95 86 95 100 67L+F+T 65 76 71 86 29 62 67L+F+E 87 81 95 86 95 95 67L+S 94 95 95 81 100 100 95Table 9.
Person identification accuracy (%) trainingwith SMO on 5 topics and testing on 1.
Avg = Aver-age over all topics: Ch=Catholic Church, Gay=GayMarriage, Iraq=Iraq War, Mar=Marijuana Legaliza-tion, Pri=Privacy Rights, Sex=Sex DiscriminationAgain a total of 126 ?documents?
were used:for each topic, 21 samples were constructed,each the concatenation of all text produced by anindividual on that topic, across all genres.
Onetopic was withheld and 105 documents (on theother 5 topics) were used for training.
Table 9shows that overall the L+S feature set performedbetter than either the L+F or L+F+T sets.
Themost noticeable differences are the drops in theaccuracy when the five topic words are added,particularly on the topics of marijuana and priva-cy rights.
For L+F+T, if ?marijuana?
is withheldfrom the topic word features when the marijuanatopic is the test set, the accuracy rises to 90%.Similarly, if ?school?
is withheld from the topicword features when the privacy rights topic is thetest set, the accuracy rises to 100%.
This indi-cates the topic words are detrimental to deter-mining the communicant, and this appears to besupported by the lack of an accuracy drop in thetesting on the Iraq and sexual discrimination top-ics, both of which featured the fewest uses of thefive topic words.
That the results rise when us-ing the L+S features shows that more featuresthat are independent of the topic tend to help dis-tinguish the person (as only the Iraq set expe-rienced a small drop using these features in train-ing and testing, while the others either increasedor remained the same).
The similarity here of theresults using L+F features when compared toL+F+E is likely due to the small number of emo-ticons observed in the corpus (16 total exam-ples).3415.4 Predict Communicant in a Speech Ge-nre Given Information on the OtherOne interesting experiment used one speech ge-nre for training, and the other speech genre fortesting.
The results (Table 10) show that the ad-ditional stop words (S compared to F) make apositive difference in both sets.
We hypothesizethat the increased performance of training withdiscussion data and testing on interview data isdue to the larger amount of training data availa-ble in discussions.
We will test this in futurework.Train Test L+F L+SInter Disc 5 19Disc Inter 29 48Table 10.
Person identification accuracy (%) trainingand testing SMO on spoken genres5.5 Predict Authorship in a Textual GenreGiven Information on Speech GenresTrain Test L+F L+SDisc+Inter Blog 19 24Disc+Inter Chat 5 14Disc+Inter Email 5 10Disc+Inter Essay 10 29Table 11.
Person identification accuracy (%) trainingSMO on spoken genres and testing on textual genresTable 11 shows the results of training on speechdata only and predicting the author of the textgenre.
Again, the speech genres alone do not dowell at determining the individual author of thetext genre.
The best score was 29% for essays.5.6 Predict Authorship in a Textual GenreGiven Information on Other TextualGenresTable 12 shows the results of training on textdata only and predicting authorship for one of thefour text genres.
Recognizing the authors in chatis the most difficult, which is not surprising sincethe blogs, essays and emails are more similar toeach other than the chat genre, which uses ab-breviations and more informal language as wellas being immediately interactive.Train Test L+F L+SC+E+S Blog 76 86B+E+S Chat 10 19B+C+S Email 90 81B+C+E Essay 90 86Table 12.
Person identification accuracy (%) train-ing and testing SMO on textual genres5.7 Predict Communicant in a Speech Ge-nre Given Information on Textual Ge-nresTraining on text and classifying speech-basedsamples by author showed poor results.
Similarto the results for speech genres, using the textgenres alone to determine the individual in thespeech genre results in a maximum score of 29%for the interview genre (Table 13).Train Test L+F L+SB+C+E+S Discussion 14 23B+C+E+S Interview 14 29Table 13.
Person identification accuracy (%) trainingSMO on textual genres and testing on speech genres5.8 Error AnalysisResults for different training and test sets varyconsiderably.
A key factor in determining whichsets can successfully be used to train other setsseems to be the mode, that is, whether or not aset is textual or spoken, as the lowest accuraciestend to be found between genres of differentmodes.
This suggests that how people write andhow they speak may be somewhat distinct.Typically, more data samples in the trainingtends to increase the accuracy of the tests, butmore features does not guarantee the same result.An examination of the feature sets revealed fur-ther explanations for this apart from any inherentdifficulties in recognizing authors between sets.For many tests, there is a tendency for the sameperson to be chosen for classification, indicatinga bias to that person in the training data.
This istypically caused by features that have mostly, butnot all, zero values in training samples, but havemany non-zero values in testing.
The moststriking examples of this are described in 5.3,where the removal of certain topic-relatedfeatures was found to dramatically increase theaccruacy.
Targetted removal of other featuresthat have the same biasing effect could increaseaccuracy.While Weka normalizes the incoming featuresfor SMO, it was also discovered that a simpleinitial normalization of the feature sets bydividing by the maximum or standardization bysubtracting the mean and dividing by thestandard deviation of the feature sets couldincrease the accuracy across the different tests.6 ConclusionIn this paper, we have described a novel uniquecorpus consisting of samples of communication342of 21 individuals in six genres across six topicsas well as experiments conducted to identify aperson?s samples within the corpus.
We haveshown that we can identify individuals with rea-sonably high accuracy for several cases: (1)when we have samples of their communicationacross genres (71%), (2) when we have samplesof their communication in specific genres otherthan the one being tested (81%), and (3) whenthey are communicating on a new topic (94%).For predicting a person?s communication inone text genre using other text genres only, wewere able to achieve a good accuracy for allgenres (above 76%) except chat.
We believe thisis because chat, due to its ?real-timecommunication?
nature is quite different fromthe other text genres of emails, essays and blogs.Identifying a person in one speech genre aftertraining with the other speech genre had loweraccuracies (less than 48%).
Since these resultsdiffered significantly, we hypothesize this is dueto the amount of data available for training ?
ahypothesis we plan to test in the future.Future plans also include further investigationof some of the suprising results mentioned in thispaper as well investigation of stop word listsparticular to communicative genres.
We alsoplan to investigate if it is easier to identify thoseparticipants who have produced more data(higher total word count) as well as perform asystematic study the effects of the number ofwords gathered on person identificaton.
?n addition, we plan to investigate the efficacyof using other features besides those available inLIWC, stopwords and emoticons in personidentification.
These include spelling errors,readability measures, complexity measures,suffixes, and content analysis measures.ReferencesNaomi S. Baron.
2003.
Why email looks like speech.In J. Aitchison and D. M. Lewis, editors, New Me-dia Language.
Routledge, London, UK.Douglas Biber.
1988.
Variation across speech andwriting.
Cambridge University Press, Cambridge,UK.Leo Breiman.
2001.
Random forests.
Technical Re-port for Version 3, University of California, Berke-ley, CA.Rafael A. Calvo, Jae-Moon Lee, and Xiaobo Li.
2004.Managing content with automatic document classi-fication.
Journal of Digital Information,  5(2).Corinna Cortes and Vladimir Vapnik.
1995.
Supportvector networks.
Machine Learning, 20(3):273-297.Nikolas Coupland, Justine Coupland, Howard Giles,and Karen L. Henwood.
1988.
Accommodating theelderly: Invoking and extending a theory, Lan-guage in Society, 17(1):1-41.David Crystal.
2001.
Language and the Internet.Cambridge University Press, Cambridge, UK.Olivier de Vel, Alison Anderson, Malcolm Corney,George Mohay.
2001.
Mining e-mail content forauthor identification forensics, In SIGMOD: Spe-cial Section on Data Mining for Intrusion Detec-tion and Threat Analysis.Jade Goldstein-Stewart, Gary Ciany, and Jaime Car-bonell.
2007.
Genre identification and goal-focusedsummarization, In Proceedings of the ACM 16thConference on Information and Knowledge Man-agement (CIKM) 2007, pages 889-892.Jade Goldstein-Stewart, Kerri A. Goodwin, RobertaE.
Sabin, and Ransom K. Winder.
2008.
Creatingand using a correlated corpora to glean communic-ative commonalities.
In LREC2008 Proceedings,Marrakech, Morocco.Susan Herring.
2001.
Gender and power in onlinecommunication.
Center for Social Informatics,Working Paper, WP-01-05.Susan Herring.
1996.
Two variants of an electronicmessage schema.
In Susan Herring, editor, Com-puter-Mediated Communication: Linguistic, Socialand Cross-Cultural Perspectives.
John Benjamins,Amsterdam, pages 81-106.Shawndra Hill and Foster Provost.
2003.
The myth ofthe double-blind review?
Author identification us-ing only citations.
SIGKDD Explorations.5(2):179-184.Moshe Koppel, Shlomo Argamon, and Anat RachelShimoni.
2002.
Automatically categorizing writtentexts by author gender.
Literary and LinguisticComputation.
17(4):401-412.Ivan Krsul and Eugene H. Spafford.
1997.
Author-ship analysis: Identifying the author of a program.Computers and Security 16(3):233-257.Shlomo Levitan and Shlomo Argamon.
2006.
Fixingthe federalist: correcting results and evaluating edi-tions for automated attribution.
In Digital Humani-ties, pages 323-328, Paris.LIWC, Linguistic Inquiry and Word Count.http://www.liwc.net/David Madigan, Alexander Genkin, David Lewis,Shlomo Argamon, Dmitriy Fradkin, and Li Ye.2005.
Author identification on the large scale.Proc.
of the Meeting of the Classification Societyof North America.343Philip M. McCarthy, Gwyneth A. Lewis, David F.Dufty, and Danielle S. McNamara.
2006.
Analyz-ing writing styles with Coh-Metrix, In Proceedingsof AI Research Society International Conference(FLAIRS), pages 764-769.Niamh McCombe.
2002.
Methods of author identifi-cation, Final Year Project, Trinity College, Ireland.Thomas C. Mendenhall.
1887.
The characteristiccurves of composition.
Science, 9(214):237-249.Frederick Mosteller and David L. Wallace.
1964.Inference and Disputed Authorship: The Federal-ist.
Addison-Wesley, Boston.James W. Pennebaker, Martha E. Francis, and RogerJ.
Booth.
2001.
Linguistic Inquiry and Word Count(LIWC): LIWC2001.
Lawrence Erlbaum Asso-ciates, Mahwah, NJ.John C. Platt.
1998.
Using sparseness and analytic QPto speed training of support vector machines.
In M.S.
Kearns, S. A. Solla, and D. A. Cohn, editors,Advances in Neural Information Processing Sys-tems 11.
MIT Press, Cambridge, Mass.Roberta E. Sabin, Kerri A. Goodwin, Jade Goldstein-Stewart, and Joseph A. Pereira.
2008.
Gender dif-ferences across correlated corpora: preliminary re-sults.
FLAIRS Conference 2008, Florida, pages207-212.Marina Santini.
2007.
Automatic Identification ofGenre in Web Pages.
Ph.D., thesis,  University ofBrighton, Brighton, UK.Michael Shepherd and Carolyn Watters.
1999.
Thefunctionality attribute of cybergenres.
In Proceed-ings of the 32nd Hawaii International Conf.
onSystem Sciences (HICSS1999), Maui, HI.Rob Thomson and Tamar Murachver.
2001.
Predict-ing gender from electronic discourse.
British Jour-nal of Social Psychology.
40(2):193-208.Ian Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques(Second Edition).
Morgan Kaufmann, San Francis-co, CA.Simeon J. Yates.
1996.
Oral and written linguisticaspects of computer conferencing: a corpus basedstudy.
In Susan Herring, editor, Computer-mediated Communication: Linguistic, Social, andCross-Cultural Perspectives.
John Benjamins,Amsterdam, pages 29-46.344
