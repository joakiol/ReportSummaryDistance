SENSEVAL-3 TASKWord-Sense Disambiguation of WordNet GlossesKenneth C. LitkowskiCL Research9208 Gue RoadDamascus, MD 20872ken@clres.comAbstractThe SENSEVAL-3 task to perform word-sensedisambiguation of WordNet glosses wasdesigned to encourage development of technologyto make use of standard lexical resources.
Thetask was based on the availability of sense-disambiguated hand-tagged glosses created in theeXtended WordNet project.
The hand-taggedglosses provided a ?gold standard?
for judgingthe performance of automated disambiguationsystems.
Seven teams participated in the task,with a total of 10 runs.
Scoring these runs as an?all-words?
task, along with considerablediscussions among participants, provided moreinsights than just the underlying technology.
Thetask identified several issues about the nature ofthe WordNet sense inventory and the underlyinguse of wordnet design principles, particularly thesignificance of WordNet-style relations.IntroductionIn SENSEVAL-2, performance in the lexical sampletask dropped considerably (Kilgarriff, 2001).Kilgarriff suggested that using WordNet (Fellbaum,1998) for SENSEVAL has drawbacks.
WordNet wasnot designed to serve as a lexical resource, but itspublic availability and reasonable comprehensivenesshave been dominant factors in its selection as thelexical resource of choice for Senseval and for manyapplications.
These factors have led to furtherfunding by U.S. government agencies and manyimprovements are currently underway.
Among theseimprovements is a planned hand-tagging of theWordNet glosses with their WordNet senses.
At thesame time, sense-tagging of the glosses is beingperformed in the Extended WordNet (XWN) projectunder development at the University of Texas atDallas (Mihalcea and Moldovan, 2001)1.
The XWNproject also parses the WordNet glosses into a part ofspeech tree and transforms them into a logicalpredicate form.More generally, sense disambiguation ofdefinitions in any lexical resource is an importantobjective in the language engineering community.The first significant disambiguation of dictionarydefinitions and creation of a hierarchy took place 25years ago in the groundbreaking work of Amsler(1980).
However, while substantial research has beenperformed on machine-readable dictionaries sincethat time, technology has not yet been developed tomake systematic use of these resources.
ThisSENSEVAL task was designed to encourage thelexical research community to take up the challengeof disambiguating dictionary definitions.XWN is used as a core knowledge base forapplications such as question answering, informationretrieval, information extraction, summarization,natural language generation, inferences, and otherknowledge intensive applications.
The glosses containa part of the world knowledge since they define themost common concepts of the English language.
Inthe XWN project, many open-class words inWordNet glosses have been hand-tagged and providea ?gold standard?
against which disambiguationsystems can be judged.
The SENSEVAL-3 task is toreplicate the hand-tagged results.The Extended WordNet (XWN) project hasdisambiguated the content words (nouns, verbs,adjectives, and adverbs) of all glosses, combininghuman annotation and automated methods usingWordNet 1.7.1.
A ?quality?
attribute was given toeach lemma.
XWN used two automatic systems todisambiguate the content words.
When the twosystems agreed, the lemma was given a ?silver?1http://www.hlt.utdallas.edu/Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsquality.
Otherwise, a lemma was given a ?normal?quality (even when there was only one sense inWordNet).
In a complex process described in moredetail below, certain glosses or lemmas were selectedfor hand annotation.
Lemmas which were hand-tagged were given a ?gold?
tag.The WordNet 1.7.1 data were next converted touse WordNet 2.0 glosses.
Word senses have beenassigned to 630,599 open class words, with 15,179(less than 2.5 percent) of the open-class words inthese glosses assigned manually.
Many glosses havemore than one word given a ?gold?
assignment.
Theresultant test set provided to participants consists of9,257 glosses, containing 15,179 ?gold?
taggedcontent words and a total of 42,491 content words,distributed as follows:Table 1.
Gloss Test SetPOS Glosses Golds WordsAdjective 94 263 370Adverb 1684 1826 3719Noun 6706 10985 35539Verb 773 2105 2863Total 9257 15179 42491The disambiguations (and hence the answer key) areavailable at the XWN web site.
Participants wereencouraged to investigate the XWN data as well asthe methods followed by the XWN team.
However,participants were expected to develop their ownsystems, for comparison with the XWN manualannotations.1 The Senseval-3 TaskParticipants were provided with all glosses fromWordNet in which at least one open-class word wasgiven a "Gold" quality assignment.
These glosseswere provided in an XML file, each with its WordNetsynset number, its part of speech, and the gloss itself.Glosses frequently include sample uses.
The samplesuses were not parsed in the XWN project and werenot to be included in the submissions.The task was configured as essentially identicalto the SENSEVAL-2 and SENSEVAL-3 "all-words"tasks, except without any context and with the glossnot constituting a complete sentence.
Unlike theall-words task, individual tokens to be disambiguatedwere not identified, so that participants were requiredto perform their own tokenization and identificationof multiword units.
The number of words in a glossis quite small, but a few glosses do contain the sameword more than once.
Participants were encouragedto consider a synset's placement within WordNet (itshypernyms, hyponyms, and other relations) to assistin disambiguation.
The XWN data contains part ofspeech tags for each word in the glosses, as well asparses and logical forms, which participants wereallowed to use.
Most of the glosses in the test sethave hand-tagged words as well as words tagged bythe automatic XWN systems.
The senses assigned toother open-class words have a tag of ?silver?
or?normal?.
In submitting test runs, participants did notknow which of the words had been assigned a ?gold?quality, but were only scored for the ?gold?
qualitywords.2No training data was available for this task sincethe number of items in the test set was so small.Participants were encouraged to become familiarwith the XWN dataset and to make use of it in waysthat would not compromise their performance of thetask.2 SubmissionsSeven teams participated in the task with one teamsubmitting two runs and one team submitting threeruns.
A submission contained an identifier (the partof speech of the gloss and its synset number) and aWordNet sense for each content word or phraseidentified by the system.
The answer key containspart of speech/synset identifier, the XWN qualityassignment, the lemma and the word form from theXWN data, and the WordNet sense.
The scoringprogram (a Perl script) stored the answers in threehashes according to quality (?gold?, ?silver?, and?normal?)
and then also stored the system?s answersin a hash.
The program then proceeded through the?gold?
answers and determined if a system?s answersincluded a match for that answer, equaling either the(lemma, sense) or (word form, sense).
No systemsubmitted more than one sense for each of its wordforms.
An exact match received a score of 1.0.
If a2The answer key contains all assignments, so it ispossible that runs can be analyzed with these othersense assignments with a voting system.
However,such an analysis has not yet been performed.system returned either the lemma or the word form,but had assigned an incorrect sense, the item wascounted as attempted.Precision was computed as the number correctdivided by the number attempted.
Recall wascomputed as the number correct divided by the totalnumber of ?gold?
items.
The percent attempted wascomputed as the number attempted divided by thetotal number of ?gold?
items.
Results for all runs areshown in Table 2.Table 2.
System Performance (All Items)Run Prec Rec Att01 (UPolit?cnica de Valencia) 0.534 0.405 76.002 (CL Research) 0.449 0.345 76.803 (LanguageComputerCorp) 0.701 0.504 71.904a (TALP Research Center) 0.686 0.683 99.504b (TALP Research Center) 0.574 0.558 97.205 (IRIT-ERSS) 0.388 0.385 99.106a (Uni-Roma1-DI) 0.777 0.311 40.006b (Uni-Roma1-DI) 0.668 0.667 99.906c (Uni-Roma1-DI) 0.716 0.362 50.507 (Indian Inst Technology) 0.343 0.301 87.8Systems 04a and 06b used the part of speech tagsavailable in the XWN files, while the other runs didnot.3 DiscussionDuring discussions on the SENSEVAL-3 mailing listand in interchanges assessing the scoring of thesystems, several issues of some importance arose.Most of these concerned the nature of the XWNannotation process and the ?correctness?
of the?gold?
quality assignments.Since glosses (or definitions) are only ?sentence?fragments, parsing them poses some inherentdifficulties.
In theory, a proper lexicographically-based definition is one that contains a genus term(hypernym or superordinate) and differentiae.
Agloss?
hypernym is somewhat easily identified as thehead of the first phrase, particularly in noun and verbdefinitions.
Since most WordNet synsets have ahypernym, a heuristic for disambiguating the head ofthe first phrase would be to use the hypernym as theproper disambiguated sense.
And, indeed, theinstructions for the task encouraged participants tomake use of WordNet relations in theirdisambiguation.However, the XWN annotators were not giventhis heuristic, but rather were presented with the setof WordNet senses without awareness of theWordNet relations.
As a result, many glosses had?gold?
assignments that seemed incorrect whenconsidering WordNet?s own hierarchy.
For example,naught is defined as ?complete failure?
; in WordNet,its hypernym failure is sense 1 (?an act that fails?
),but the XWN annotators tagged it with sense 2 (?anevent that does not accomplish its intendedpurpose?
).To investigate the use of WordNet relationsheuristics, we considered a set of 313 glossescontaining 867 ?gold?
assignments which team 06submitted as highly reliant on these relations.
Asshown in Table 3 (scored on 8944 glosses with14312 ?gold?
assignments), precision scores changedmost for 03 (0.020), 06b (0.017), and 04a (0.016);these runs had correspondingly much lower scoresfor the 313 glosses in this set (results not shown).These differences do not appear to be significant.
Amore complete assessment of the significance ofWordNet relations in disambiguation would requirea more complete identification of glosses wheresystems relied on such information.Table 3.
System Performance (Reduced Set)Run Prec Rec Att01 (UPolit?cnica de Valencia) 0.538 0.407 75.602 (CL Research) 0.446 0.342 76.603 (LanguageComputerCorp) 0.721 0.516 71.604a (TALP Research Center) 0.702 0.698 99.504b (TALP Research Center) 0.585 0.568 97.205 (IRIT-ERSS) 0.395 0.391 99.106a (Uni-Roma1-DI) 0.826 0.323 39.106b (Uni-Roma1-DI) 0.685 0.684 99.906c (Uni-Roma1-DI) 0.753 0.375 49.707 (Indian Inst Technology) 0.346 0.302 87.2Further discussion with members of the XWNproject about the annotation process revealed somefactors that should be taken into account whenassessing the various systems?
performances.
Firstly,the annotations of the 9257 glosses with ?gold?assignments were annotated using three differentmethods.
The first group of 1032 glosses were fullyhand-tagged by two graduate students, with 80percent agreement and with the project leaderchoosing a sense when there was disagreement.For the remaining glosses in WordNet, twoautomated disambiguation programs were run.
Whenboth programs agreed on a sense, they were given a?silver?
quality.
In those glosses for which all but oneor two words had been assigned a ?silver?
quality,the one or two words were hand-tagged by a graduatestudent, without any interannotator check or review.There are 4077 noun glosses in this second set.A third set, the remaining 4738 among the testset, were glosses for which all the words but one hadbeen assigned a ?silver?
quality.
The single word wasthen hand-tagged by a graduate student, and in somecases by the project leader (particularly when a wordhad been mistagged by the Brill tagger).To assess the effect of these three different stylesof annotation, we ran the scoring program, restrictingthe items scored to those in each of the threeannotation sets.
The scores were changed much moresignificantly for the various teams for the differentsets.
For the first set, precision was downapproximately 0.07 for three runs, with much lowerchanges for the other runs.
For the second set,precision was up approximately 0.075 for two runs,down approximately 0.08 for two runs, and relativelyunchanged for the remaining runs.
For the third set,there was relatively little changes in the precision forall runs (with a maximum change of 0.03).4 ConclusionsThe underlying guidance for this SENSEVAL-3 taskthat, in the absence of significant context,participants make use of WordNet relations fordisambiguating glosses has led to some significantinsights about the use and importance of wordnets.These insights emerge from the tension between thereliance on WordNet relations and the imprecision ofthe tagging process.Many investigators, including several of theparticipants in this task, are attempting to exploit thekinds of relations between lexical entries that areembodied in WordNet.
The use of wordnets in NLPapplications has become an important basic constructand increasingly valuable.
However, the constructionof wordnets is expensive and time-consuming, andwithout any significant prospects for commercialsupport.
While some dictionary publishers areincreasingly incorporating wordnet principles intotheir lexical resources, this process is slow.
Atpresent, the publicly available WordNet remains thewordnet of choice.The annotation process followed by the XWNproject, with the taggings used in this task, has againindicated difficulties with the WordNet senseinventory.
The fact remains that WordNet has nothad the benefit of sufficient lexicographic resourcesin the construction of its glosses and in theacquisition of other lexicographic information in itsentries.
The WordNet project continues its efforts toadd information, but with limited resources.With the diverse set of approaches represented bythe participants in this task, it is possible to envisionsets of steps that might be employed to improve thedetails of the WordNet sense inventory.
One stepwould include continued hand-tagging of WordNetglosses without consideration of WordNet relations.Another step would be the use of automateddisambiguation routines to act as checks onconsistency.
Such systems would include those thatrely on WordNet relations as well as those that donot, acting as checks on one another.ReferencesAmsler, Robert A.
1980.
The Structure of the Merriam-Webster Pocket Dictionary.
Ph.D.
Thesis., Austin:University of Texas.Fellbaum, Christiane (ed.).
1998.
WordNet: AnElectronic Lexical Database.
The MIT Press:Cambridge, MA.Kilgarriff, Adam.
2001.
English Lexical Sample TaskDescription.
In Proceedings of SENSEVAL-2: SecondInternational Workshop on Evaluating Word SenseDisambiguation Systems, Toulouse, France.Mihalcea, Rada and Dan Moldovan.
2001. eXtendedWordNet: Progress Report.
In Proceedings of NAACLWorkshop on WordNet and Other Lexical Resources,Pittsburgh, PA.
