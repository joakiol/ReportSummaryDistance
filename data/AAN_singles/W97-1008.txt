What makes a word: Learning base units in Japanese for speechrecognitionLaura Mayfield TomokiyoLanguage Technology InstituteCarnegie Mellon University4910 Forbes AvenuePittsburgh, PA 15213, USAlaura@as, cmu.
eduKlaus RiesUniversit?t KarlsruheFakult/it ffir InformatikInteractive Systems Laboratories76128 Karlsruhe, Germanykries@ira, uka.
deAbstractWe describe an automatic process for learn-ing word units in Japanese.
Since theJapanese orthography has no spaces de-limiting words, the first step in building aJapanese speech recognition system is todefine the units that will be recognized.Our method applies a compound-findingalgorithm, previously used to find word se-quences in English, to learning syllable se-quences in Japanese.
We report that wewere able not only to extract meaningfulunits, eliminating the need for possibly in-consistent manual segmentation, but alsoto decrease perplexity using this automaticprocedure, which relies on a statistical, notsyntactic, measure of relevance.
Our al-gorithm also uncovers the kinds of envi-ronments that help the recognizer predictphonological alternations, which are oftenhidden by morphologically-motivated tok-enization.1 IntroductionWhat defines a word when there are no spaces ina written language?
Words, as they are known inEnglish and other western languages, are the basicunits of recognition i  most CSR systems, but whena language is written as a string of characters withno white space, how does one go about specifying thefundamental units that must be recognized?
Map-ping onto English-style words is one solution, butan artificial one, and may hide natural character-istics of Japanese that can be important in recog-nition.
Recognizing phonemes, or short phonemeclusters, is another option, but recognition accuracycan improve when we have longer phoneme stringsto work with; acoustic onfusability decreases and along word is a more useful predictor of subsequentwords than a single syllable.
Automatic segmentingtools eliminate an often inconsistent manual segmen-tation step, but are generally based on morpholog-ical analysis, which can produce units smaller thanare desirable for speech recognition.
Certainly, thereexist words as can be looked up in a dictionary, butwhen a language is as heavily inflected as Japaneseis, that only solves part of the problem.
In this pa-per we describe an automatic process for learningbase units in Japanese and discuss its usefulness forspeech recognition.2 The problem with JapaneseThe Japanese language is written without spaces inbetween words.
This means that before one caneven start designing a recognition or translation sys-tem for Japanese the units that will be recognized,or translated, must be defined.
Many sequences ofphonemes, particularly those representing nouns, areclearly independent and can be designated as free-standing units.
Japanese has a rich and fusionalinflectional system, though, and delimiting where averb ending ends and another begins, for example,is seldom straightforward.Japanese has typically been segmented in vari-ations on four ways for the purposes of recogni-tion and parsing, although since many papers onJapanese recognition do not specify what units theyare using, or how they arrived at the definition of a"word" in Japanese, it is hard to compare systems.?
Phrase/Bunsetsu level: (Early ASURA (Mori-moto et al , 1993), QJP (Kameda, 1996))-advantages: long enough for accuraterecognition, captures common patterns- disadvantages: requires dictionary entryfor each possible phrase; vocabulary explo-sion?
"Word" level: (JANUS (Schultz and Koll,Mayfield Tomokiyo ~ Ries 60 Learning base units in JapaneseLaura Mayfield Tomokiyo and Klaus Ries (1997) What makes a word: Learning base units in Japanese forspeech recognit ion.
In T.M.
Ellison (ed.)
CoNLL97: Computational Natural Language Learning, ACL pp 60-69.
(~) 1997 Association for Computational Linguistics1997))- advantages: units long enough not to causeconfusion, but short enough to capture gen-eralizations- disadvantages: not natural for Japanese;easy to be inconsistent; may hide qualitiesof Japanese that could help in recognition?
Morpheme level: (Verbmobil (Yoshimoto andNanz, 1996))- advantages: mid-length units that are nat-ural to Japanese- disadvantages: a lot of room for incon-sistency; "morpheme" can be interpretedbroadly and if segmented in the strictestsense units can be single phonemes?
Phoneme cluster level: (NEC demi-syllable(Shinoda and Watanabe, 1996)), JANUSKSST 1- advantages: only need a short dictionary- disadvantages: high confusability, al-though confusability seems less of a prob-lem for Japanese than some other lan-guagesThe bunsetsu is a unit used to segment Japanesewhich generally consists of a content component onthe left side and a function component on the rightside.
Bunsetsu boundaries seem to be natural pointsfor pausing and repetition, and most elementaryschools include bunsetsu segmentation as a formal-ized part of grammar education.
John-ga ("John-NOM"), hon-o ("book-ACC"), and yonda ("gave")are all examples of bunsetsu.Bunsetsu can be quite long in terms of bothphonemes and morphemes, however, and quiteunique.
For example, saseteitadakitaindesuga wouldbe considered a single bunsetsu.
This phrase con-tains a causative form of the verb "to do", sase-,a gerunditive suffix -re-, the root of a formal verbmeaning to receive -itadaki-, a desidirative suffix -tai-, a complementizer -n-, a copula -desu-, and asoftener -ga.3 Our approachOur approach, described in detail in (Ries et al,1996), uses a statistical tool that automatically findsimportant sequences.
This tool was originally de-veloped to help mitigate the bias introduced by a1Korean Spontaneous Scheduling Task; SST de-scribed more fully in Section 4.1word-based orthography by explicitly modeling im-portant multi-word units.
The target of the toolwas languages for which the word seemed alreadya useful level of abstraction from which to expand,and experiments were first performed on English andGerman for the scheduling task.
One important mo-tivation for this work was the desire to capture lex-icalized expressions that exhibit, in natural speech,markedly different pronunciation from what con-catenating the constituent words would predict.
Ex-amples of such expressions are don't-know (dunno),i-would-have (ida), you-all (yaw).The objective of the phrase-finding procedure is tofind a pair of frequently co-occuring basic units forwhich joining all occurrences in the corpus is a use-ful operation.
Until very recently most implementa-tions of this idea have made use of measures of co-occurrence that have been useful in other domains,and the pair is chosen by maximizing that criterion.In contrast we assume that we want to model thecorpus with a statistical language model and searchfor those sequences that increase the modeling powerof the model by the largest amount.
Our measure-ments are based on information theoretic principlesand the usage of m-gram models of language, a com-mon practice in the speech community.
The modeldescribed here will therefore implicitly consider thewords surrounding the phrase candidates and use in-formation about the context o determine the good-ness of a sequence, which is in contrast o traditionalmeasures.
(Ries et al, 1996) has compared a variety of mea-sure as reported in the literature and has found theseto be not competitive with the new technique if usedin statistical language models.
In a very vague state-ment we want to add that this corresponds to theexperience in eyeballing these sequences.
The mea-sures that were compared against in this earlier workhave been:?
mutual information (Magerman and Marcus, )?
frequency?
iterative marking frequency (Pales et al, 1995)?
backward bigram: p(wl\[w2)?
backward perplexity: p(wl, w~).
log(p(wl \[w2))?
Suhotin's measure (Suhotin, 1973)3.1 Stat is t ica l  an6uage mode l ing  andspeech recogmt ionStatistical models of language are, to our knowledge,the type of language model used in all modern speechMayfield Tomokiyo 8J Ries 61 Learning base units in Japaneserecognition engines, especially in research systemsbut also in most commercial large vocabulary sys-tems that can recognize naturally fluent spoken lan-guage.
In principle the speech recognition problemis to find the most likely word sequence W given theacoustic A.argmaxwP(W \[A)Using Bayes theorem and the knowledge that p(A)does not change the maximization we arrive atargm xwp(AIW), v(w)p(AIW ) is commonly referred to as the acousticmodel, p(W) is the language model and the argmaxoperator is realized by specialized search procedures.This paper for the most part ignores the search prob-lem.
The acoustic model is in part influenced by thesequences since we can change the entries in the pro-nunciation dictionary that encode the phoneme se-quences the speech system uses to generate its mod-els.
During this generation process most modern sys-tems make only partial use of neighboring words andthe construction process is up to date also unable tomodel contractions, especially at word boundaries.It is therefore of great advantage to have a basicunit in the decoder that allows for manual or au-tomatic dictionary modification that captures thesephenomena.
This has recently been reported to bea very promising modeling idea on several differentspeech recognition tasks in English.
The underlyingassumption is that sequences of units that have ahigh stickiness are by conventional usage very likelyto show idiosyncratic pronuncations much like singlewords do: They are for the most part lexicalized.The statistical language modeling problem for thesequence of words W = Wl , .
.
.
,wn where wn is aspecial end of sentence symbol can then be rephrasedasnp(W) = r Ip (w~lwl , .
.
.
,  w,_x)i=1We will for most applications probably never be ableto find enough data to estimate p as presented above.An often practiced shortcut is therefore to assumethat each word is only dependent on the last m - 1words and that this distribution is the same in allpositions of the string.
These models are called m-gram models and have proved to be very effectivein a large number of applications, even though theyare a naive model of language.Information theoretic measures (Cover andThomas, 1991) are frequently used to describe thepower of language models.
(Cover and Thomas,1991) shows in chapter 4.2 that the entropy rate of arandom process converges, under additional assump-tions, to the entropy of the random source.
Thishas been taken as the justification for using an ap-proximation of a notational difference of the entropyrate,dubbed perplexity, as a measure of the strengthof the language model.
Given a bigram model p anda test text w l , .
.
.
,  w,~ the perplexity PP is definedasPP = 2- ~ ~=1 logP(w,lw,_~)where we make usage of a special "start-of-sentence"symbol as w0.
In the sequel we happily ignore thisfor notational convenience.Since we will be changing the basic unit duringthe sequence finding procedure it is useful to nor-malize the perplexity onto one standard corpus.
Saythe standard test corpus has length n and the newtest corpus has length n' we define for the test cor-pus ppret = pp-~.
ppr~l is therefore a notationalvariant of the probability of the test text given themodel which is independent of the used sequencesof words and is the only meaningful measure in thiscontext.The calculation of the model p itself from em-pirical data involves a number of estimation prob-lems.
We are using the well understood and empir-ically tested backoff method, as recently describede.g.
by (Kneser and Ney, 1995).3.2 A lgor i thm descr ip t ionThe idea of the algorithm is to search for sequencesthat reduce the relative perplexity of the corpus inan optimal way.
For example, if we were workingwith a bigram model and came across the sequencecredit card bill, not only would we have to chooseamong words like "report," "history" and "check" aspossible successors for "credit," but the word "card"itself has many senses and "game," "shop" and "ta-ble" might all be more likely followers of "card" than"bill," if no other context is known.
By creating anew word, credit_card, we eliminate one choice anddecrease the surprise of seeing the next word.Since the new word is now treated exactly likeother word instances in the corpus, it can in turn bethe first or second half of a future joining operation,leading to multi-word compounds.The sequence-finding algorithm iterates over allword pairs in a training corpus, and in each iterationchooses the pair (recall that one or both elements ofthis pair can themselves be sequences) that reducesthe bigram perplexity the most.
This can be doneby just calculating the number of times all possi-ble word triples appeared and going over this table(except for those entries that have a count of zero)Mayfield Tomokiyo 8J Ries 62 Learning base units in Japaneseonce.
This is iterated until no possible compoundreduces perplexity.
This technique is obviously justan approximation of an algorithm that considers allword sequences at once and would allow the statisti-cal model to produce the components of a sequenceseparately.
The clustering is therefore a bottom upprocedure and during the training of our models weare making a variation of the Viterbi assumption injoining the sequences in the corpus blindly.For the corpora we worked with, this techniquewas sufficiently fast with the efficient implementa-tion described in (Ries et al, 1996), which makesfurther use of estimation tools from pattern recogni-tion such as the leaving one out technique.Inspired by (Lauer, 1995), we have very recentlyextended this technique so that the algorithm hasthe option of, instead of replacing a sequence of twounits by a new symbol, replacing it by either theleft or right component of that sequence.
The ideais that the resulting model could capture head in-formation.
We have tested this approach on someof our English corpora; the resulting sequences lookunpromising, however, and the new option was sel-dom used by the algorithm.3.3 App l i ca t ion  to  JapaneseRealizing that the phrase-finding procedure we usedon English and German was producing units thatwere both statistically important and semanticallymeaningful, we decided to apply the same techniquesto Japanese.
We needed units that were long enoughfor recognition and wanted to generalize on inflectedforms that are used over and over again with differ-ent stems, as well as longer sequences that are fre-quently repeated in the domain.
Other motivationsfor such a process include:?
language model estimation?
preserving important cross-morphological pho-netic environments?
inconsistency of human transcribers?
search sub-optimality due to poorly chosenunitsThe approach described in Section 3.2 is a bottom-up approach to sequence finding, and the segmen-tation of Japanese is more intuitively viewed as atop-down problem in which an input string is bro-ken down to some level of granularity.
In apply-ing the algorithm in (Ries et al, 1996) to Japanese,we reversed the problem, first breaking the corpusdown to the smallest possible stand-alone units inJapanese, and then building up again, constructingphrases.We chose the mora as our fundamental unit.
Amora is a suprasegmental unit similar to a syllable,with the important distinctions that a mora doesnot need to contain a vowel (syllabic /n /  and thefirst of double consonants are considered indepen-dent morae) and a mora-based segmentation wouldtreat long vowels as two morae.
The word gakkoo(school) would be two syllables, but four morae: ga-k-ko-o.
Each kana of the Japanese syllabary repre-sents one mora.
In some cases kana can be combinedand remain a single mora; kyo, as in Tokyo, is an ex-ample.There is some argument as to whether it is naturalto break multi-phoneme (CV) kana down further,to the phoneme level; specifically, some analyses ofJapanese verb inflections consider the root to includethe first phoneme of the alternating kana, as shownin Table 1.kana phoneme examplestem intl.
stem intl.hashi ra hashir a hashiranaihashi ri hashir i hashirimasuhashi ru hashir u hashiruhashi re hashir e hashirebahashi ro hashir o hashirooTable 1: kana-based vs. phoneme-based analyses ofverb stems and inflectionsThe nasal consonant kana is considered an inde-pendent unit.The problem of segmentation is not unique toJapanese; there are other languages without spacesin the written language, and verb conjugations andother inflective forms are issues in almost any lan-guage.
Words as defined by orthography can bemore a curse than a blessing, as having such conve-nient units of abstraction at our disposal can blindus to more natural representations.
(Ito and Kohda, 1996) describes an approach sim-ilar to ours.
Our work is different because of thephrase finding criterion we use, which is to maxi-mize the predictive power of the m-gram model di-rectly.
The recent (Ries et al, 1996) showed thata variation of that measure, coined bigram perplex-ity, outperforms classical measures often used to findphrases.
For Chinese (Law and Chan, 1995), a sim-ilar measure was combined with a tagging schemesince the basic dictionary already consisted of 80,000words.
The algorithm presented in (Ries et al,1996) is comparatively attractive computationally,and avoids problems with initialization as it worksMayfield Tomokiyo 8J Ries 63 Learning base units in Japanesein pure bottom up fashion.
Ries did not find specificimprovements from using word classes in the tasksunder consideration.Masataki (Masataki and Sagisaka, 1996) describeswork on word grouping at ATR, although what theydescribe is critically different in that they are group-ing previously defined words into sequences, notdefining new words from scratch.
Nobesawa presentsa method for segmenting strings in (Nobesawa et al, 1996) which uses a mutual information criterionto identify meaningful strings.
They evaluate thecorrectness of the segmentation by cross-referencingwith a dictionary, however, and seem to depend toa certain extent on grammar conventions.
More-over, a breaking-down approach is less suitable forspeech recognition applications than a building-upone because the risk of producing out-of-vocabularystrings is higher.
Teller and Batchelder (Teller andBatchelder, 1994) describe another segmentation al-gorithm which uses extensively knowledge about thetype of a character (hiragana/katakana/kanji, etc).This work, though, as well as Nobesawa's, is de-signed for processing Japanese text, and not speech.Our process is similar to noun compounding pro-cedures, such as described in (Lauer, 1995), but doesnot use a mutual information criterion.
The algo-rithm was originally developed to find sequences ofwords in English, initially in order to reduce lan-guage model perplexity, then to predict sequencesthat would be contracted in fast speech, again inEnglish.
The work described in this paper is an ap-plication of this algorithm to learning of word unitsin Japanese.4 Eva luat ionSince the phrase-finding algorithm described in 3.2is designed to maximize bigram perplexity, the eval-uations described here measure this criterion.4.1 TaskThe Spontaneous Scheduling Task (SST) databasesare a collection of dialogues in which two speak-ers are trying to schedule a time to meet together.Speakers are given a calendar and asked to finda two-hour slot given the constraints marked ontheir respective calendars.
Dialogues have been col-lected for English (ESST), German (GSST), Spanish(SSST), Korean (KSST) and Japanese (JSST).4.2 Test  corporaSix language models were created for the schedul-ing task JSST (Schultz and Koll, 1997).
The modelswere drawn from six different segmentations of theMayfield Tomokiyo ~ Ries 64same corpus, as described below.
Segments (alsoreferred to as "chunks") were found using the com-pounding algorithm described in Section 3.2.1.
Corpus C1 comprised only romanized mora syl-lables.
A romanization tool was run over theoriginal kanji transcriptions; the romanized textwas then split into kana (morae).2.
Corpus C2 was the result of running C1 throughthe sequencer.3.
Corpus C3 comprised chunks that were learnedbefore romanization.
The chunked kanji textwas then run through the same romanizationtool.4.
Corpus C4 was a hand-edited version of C3,where some word classes (like day of the week -if only "tuesday" existed in the corpus the restof the days were added by hand) were fleshedout and superfluous chunks removed.5.
Corpus C5 was the hand-segmented text usedin the current JSST system, with the errorfulsegmentations described in 56.
Corpus C6 was C5 + chunks from C4Only experiments involving romanized corporawere used.
The choice of using romanized text overkana text was primarily based on the requirementsof our language modeling toolkit; we used a one=to-one mapping between kana and roman characters.Equipped with a list of chunks (between 800 and900 were identified in these corpora), one can alwaysreproduce kanji representations.
Breaking down akanji-based corpus, though, would require a dictio-nary entry for each individual kanji, of which thereare over 2500 that occur in our database.
Not onlyis this difficult to do, given the 3-12 possible read-ings for each kanji, we would be left after the chunk-ing process with singleton kanji for which it is of-ten impossible to determine the correct reading outof context.
One experiment combining chunks ex-tracted from a kanji corpus with chunks from a kanacorpus was performed, but the results were not en-couraging.
Kanji are an extremely informative formof representation, and we will continue to look forways to incorporate them in future work.
However,experiments do show that even without them phrase-building can produce significant results.4.3 Perp lex i ty  resu l tsThe relative perplexities reported below are all nor-malized with respect o corpus C1.
The result be-low clearly indicates that we can do at least as goodLearning base units in Japaneseor even better than human segmentations u ing au-tomatically derived segmentations from the easilydefinable mora level.
We also want to point outthat the sequence trigram is better than a four-gramwhich indicates that the sequences play a critical rolein the calculation of the model.Our measure of success o far is relative perplexity,and for speech recognition the ultimate measure is ofcourse the accuracy of the recognition results.
Theseresults however are in our judgement much betterthan our results on English or German and we arehopeful that we can integrate this into our JANUS -Japanese speech system.ppret corpus vocabmora size sizeC1 6.1 38963 189C1 4-gram 4.7 39995 189C2 4.5 16070 1058kanji chunksC3 4.7 19400 1118hand-editC4 4.6 19135 977"words"C5 6.3 25951 2357C6 6.0 25575 3286The dictionary size is the base dictionary size,without the chunks included.
The mora dictionaryhas only 189 word types because it comprises onlythe legal syllables in Japanese, plus the letters of thealphabet, human and non-human oise, and someother symbols.
The word dictionary, used in model-ing C5 and C6, had 2357 word types.To make the results as strong as possible we useda pseudo closed vocabulary for C5 and C6.
Thismeans that we included all word types that occur inthe training and test set in the vocabulary.
The dic-tionary size is therefore xactly the number of wordtypes found in both training and test sets and in-cludes the number of sequences added to the model.This favors C5 and C6 strongly, since words thatare not in the dictionary cannot be predicted by thelanguage model at all nor can a speech recognitionsystem detect them.
However this setup at leastguarantees that the models built for C5 and C6 pre-dict all words on the test set as C1-4 do.
For largertasks we assume that the unknown word problem inJapanese will be very pronounced.A speech system can obviously recognize onlywords that are in its dictionary.
Therefore, everyunknown word causes at least one word error, typ-ically even more since the recognizer tries to fit inanother word with a pronounciation that does notfit in well.
This may lead to wrong predictions ofthe language model and to wrong segmentations ofthe acoustic signal into base units.
C1-C4 have aclosed vocabulary that can in principle recognize allpossible sentences and these segmentations do notsuffer from this problem.In English, this would be equivalent o havingbeen able to build phoneme based language modelsthat are better than word models, even if we choosethe vocabulary such that we have just covered thetraining and test sets.
In some pilot experiments weactually ran the sequence finding procedure on anEnglish phoneme corpus and a letter corpus with-out word boundaries and found that the algorithmtends to discover short words and syllables; however,the resulting models are not nearly as strong as wordmodels.5 Emergence  o f  un i t sOne of the exciting things about this study wasthe emergence of units that are contracted in fastand casual speech.
A problem with morphologicalbreakdowns of Japanese, which are good for the pur-poses of speech recognition because they are consis-tent and publicly available tokenizers can be used,is that multi-morph units are often abbreviated incasual speech (as in "don't know" ~ "dunno" inEnglish) and segmenting purely along morphologicalboundaries hides the environment necessary to cap-ture these phenomena of spontaneous speech.
Wefound that the chunking process actually appearedto be extracting these sequences.5.1 Reduc ib le  sequences  capturedFollowing is an example comparing the chunking tothe original (termed word-based here) segmentationin JSST.
The task, again, is appointment scheduling.Numbered sentences are glossed in Table 2; (1) and(6) correspond to (A); (2,7) to (B); (3,8) to (C), etc.
(1) gozenchuu ni shi te itadake reba(2) getsuyoobi ni shi te itadakere ba to omoi masu(3) ukagawa shi te itadakere ba(4) renraku shi nakere ba to omot te(5) sorosoro kime nake re ba nara naiSentences 1-5 are shown as segmented by humantranscribers.
Sentences 6-10 are the same three sen-tences, segmented by our automated process.
(6) (gozenchuu) ni (shiteitada) (kereba)(7) (getsuyoobi) ni (shiteitada) (kereba) (toomoimasu)(8) (ukagawa) (shiteitada) (kereba)Mayfield Tomokiyo 8J Ries 65 Learning base units in Japanese(A) .gozenchuu-ni $ shite itadakereba $m the morning do if I could receive the favor ofIf you would be so kind as to make it in the morning .
.
.-(B) getsuyoobi-ni $ shire itadakereba-to $on monday do if I could receive the favor of-COMPIf you would be so kind as to make it on monday .
.
.
(C) ukagawashite $ itadakereba $cause to humbly go if I could receive the favor ofIf you would allow me to go ...omoimasu $\[I\] think(D) renraku shinakereba to $ omottecontact if \[I\] don't COMP thinkingI've been meaning to get in touch \[with you/him...\](E) sorosoro $ kimenakereba naranai $soon if \[I\] don't decide it won't do\[I\] have to decide soon .
.
.
(F) nan tte-yuu-ka $what COMP-say-QUEwhat to say .
.
.
(G) sono-hi-wa $ gogo-wa $ muri-desu $that-day-TOP afternoon-TOP impossible-COPto-yuu-ka $ sanji-made $COMP-say-QUE until-threekaigi-ga $meeting-SUBJhaitte-iru-node $ sanji-ikoo-nara $ daijoubu-desu-kedo $in-is-because three-after-if okay-COP-SOFTENERThat afternoon is impossible - that is to say, there's a meeting until three,so if it's after three it would be okay(H) ash hayaku-to $morning early-andearly morning and evening are open(J) sanji made $3:00 until\[There\] is a meeting until 3:00yuugata.nara $ aite fi)masu kedo $evening-if open Is SOFTENERkaigi ga$ haitte orimasu $meeting SUBJ in isTable 2: Glosses of sentences (1) through (17).
Space boundaries vary to illustrate the specific issues beingdiscussed at the point in the text where the sentences occur; dollar signs indicate bunsetsu boundaries.
(9) (renraku) shi na (kereba) (toomo) (tte)(10) (sorosoro) (kime) na (kereba) (nara) (nai)There are two issues of importance here.
First,the hand-segmenting, while it can be tailored to thetask, is inconsistent; he sequence "...ni-shi-te-i-ta-da-ke-re-ba" (If I could humbly receive the favor ofdoing...) is segmented at one mora boundary in (1)and at another in (2).
Sentences (4) and (7) showthe same sequences as segmented by the chunker;the segmentation is consistent.
The same is true for"...na-ke-re-ba in (4) and (5) as compared to (9)and (10).The second important issue is the composition ofthe sequences.
The sequence "kereba" in (6-10),while used here in a formal context, is one that isoften reduced to "kya" or "kerya" in casual speech.The knowledge that "kereba" can be a word is veryvaluable for the speech recognizer.
Once it has ac-cess to this information, it can train its expectedpronunciations of the.
sequence "kereba" to include"kya" pronunciations as they occur in the spokencorpus.
Without the knowledge that these threemorae can form one semantic unit, the recognizercannot abstract the information that when combinedin certain contexts they can be reduced in this spe-cial way.Although the (kereba) in (6) and (7) is attachedto a verb, itadaku, that is very formal and would notbe abbreviated in this way, let us consider sentences(D) and (E), here segmented into bunsetsu phrases:(11) renraku shinakereba to omotte(12) renraku shinakya to omotte(13) sorosoro kimenakereba n ranai(14) sorosoro kimenakya naranaiSentence (D) is shown in (11) in full form andin (12) in contracted form; sentence (E) is shownin (13) in full form and in (14) in contracted form.Selection of the chunk (kereba) provides the environ-ment necessary for modeling the contraction "kya"with some verbs and adjectives in informal speech.May\]ield Tomokiyo ~ Ries 66 Learning base units in JapaneseBasing a tokenizer on syntactic factors can hide pre-cisely such environments?A second example of a frequently contracted se-quence in Japanese is to yuu or tte yuu which be-comes something close to "chuu" or "tyuu" in fastand sloppy speech.
(15) naN tte yuu ka(16) sono hi wa gogo wa muri desu, to yuu ka, sanji madekaigi ga haitte, iru node sanji ikoo nara daijoubudesu l~edoThe to yuu sequence is recognized as a single se-quence in some tokenization methods and not in oth-ers, so the idea of treating it as a single word is notnovel, but in order for the variant "chuu" to be con-sidered during recognition, it is important hat oursystem recognize this environment?There are cases in which the combination to yuuwill not collapse to "chuu:"(17) asa hayaku to yuugata nara aitemasu kedoIn the scheduling domain, the word yuugata(evening) is common enough for it to be identifiedas a word on its own, and the utterance is correctlysegmented as (to) (yuugata).
In a different domain,however, the extraction of (toyuu) might take prece-dence over other segmentation, which would indeedbe incorrect.Yet another type of contraction common in casualspeech is blending of the participial suffix te and thebeginning of the auxiliary oru, as in (J).The -te form of the verb, also often referred to asthe participial (Shibatani, 1987) or gerundive (Mat-sumoto, 1990) form, is constructed by adding thesuffix te to the verb stem plus the renyoo inflection?This renyoo (conjunctive) form of the verb is alsoused with the past-tense suffix ta and provisionalsuffix tara.In the majority of the literature, the -te formseems to be analyzed either as a single unit inde-pendent of the auxiliary verb ( iru/oku/aru/morauetc.)
(Sells, 1990) or broken down into its morpholog-ical constituents (Yoshimoto and Nanz, 1996).
Anexception is (Sumita and Iida, 1995)?
With certainauxiliary verbs, though, the e in te is dropped andthe suffix-initial t is affixed to the initial vowel of theauxiliary, as in hait-torimasu, shi-tokimasu.
Thisphenomenon is very pronounced in some dialects andonly slight in others?Our method does identify several units that havethe -te appended irectly onto the auxiliary verb,creating a very useful phonetic environment for us..\[00m m mEn0hhnls h .+m.n -B--.IConJ~nesaFigure 1: Vocabulary growth rates for English,Spanish, German and Korean for the SpontaneousScheduling Task (SST).5.2 Long enough for  speech recogn i t ionIn speech recognition systems, short recognitionunits are to be avoided because they are confusible -it is much harder to distinguish between "bee" and"key" than "BMW" and "key lime pie."
This is onereason that we did not want to use a morphologi-cal breakdown of input sentences.
Segmented in thestrictest sense (Teller and Batchelder, 1994), the sen-tence "\[I\] was studying" could be written as:benkyoo shi te i mashi tastudy do PART PROG POLITE PASTSingle-phoneme units l i ke / i /and  sy l lab ic /n /areso small that they are easy to misrecognize.
Even/ te /and/ ta /are  shorter than would normally be de-sired, although Japanese phonemes appear to be lessconfusible than their English and German counter-parts (Schultz and Koll, 1997)?
Units such as (shite)and (imashita), as produced by our algorithm, arelong enough to be distinguishable from other words,yet short enough to generalize?
Since the basic unitfrom which we were building was the mora, endingup with units that were too short was a concern.We found that the average unit length in mora wascomparable to that of the hand-segmented system,however?It is also important, though, to control the vo-cabulary size if a reasonable search space is desired?Early experiments with recognizing at the bunsetsulevel in Korean indicated that vocabulary did ex-plode, since most full bunsetsu were used only once.The vocabulary growth actually did level off even-tually, but the initial growth was unacceptable, andwe switched to a syllable-based system in the end.Figure 5.2 shows vocabulary growth rates in Janusfor different languages in the scheduling domain.Mayfield Tomokiyo ~ Ries 67 Learning base units in Japanese5.3 Undesired effectsSince our algorithm evaluates all sequences withthe same components identically, some compound-ing that is clearly wrong occurs.5.3.1 Component  sharingFor example, the chunk (kuno} was identifiedby the system.
This was because the phrasesdaigaku-no "university-GEN" and boku-no "I/me-GEN" were both very common- the algorithm ab-stracted incorrectly that (kuno) was a meaningfulunit before it found the word daigaku, which it even-tually did identify.5.3.2 Incomplete sequencesAlthough the point where a stem should endand an inflection begin can be ambiguous, moststems have definite starting points, and this algo-rithm can miss them.
For example, mooshiwake-gozaimasen "I'm very sorry" occurs many times inthe database, but our algorithm only extracted part:(shiwakegozaimaseN}.
Because of the way our stop-ping criterion is defined, we can infer from the factthat the full phrase was not extracted that by form-ing this compound we would actually have increasedthe difficulty of the corpus; more analysis is neededto understand exactly why.6 Conc lus ionThe results reported here show that we can get sim-ilar entropies in our language model by using an au-tomatic process to segment the data.
This meansthat we do not have to rely on human segmenters,which can be inconsistent and time consuming.
Wecan also tailor the segmentation style to the task;the inflected forms and general word choice in ca-sual and formal speech are very different, and ourmethod allows us to target those which are mostrelevant.
This is in itself a significant result.Additionally, we found that our method finds se-quences which are likely to undergo contractions andreductions in casual speech.
This has implicationsnot only for Japanese, but also for speech recognitionin general.
If our algorithm is finding a natural baseunit in Japanese, we should be able to use a similarapproach to find units more natural than the wordin other languages.7 AcknowlegdementsThis research was performed at the University ofKarlsruhe and at Carnegie Mellon University, Pitts-burgh.
The authors were supported by project Verb-Mobil through the German BMBF.
We gratefully ac-knowledge their support.
The researchers also wouldlike to thank Professor Kurematsu of the Universityof Electro-communications in Japan for providingthe environment for this research as well as valuableadvice.
The views and conclusions contained in thisdocument are those of the authors and should notbe interpreted as necessarily representing the officialpolicies, either expressed or implied, of any organi-zation mentioned above.Re ferencesThomas M. Cover and Joy A. Thomas Elementsof Information Theory.
Wiley, 1991.
Series inTelecommunications.Sabine Deligne and Frederic Bimbot.
LanguageModeling by Variable Length Sequences: The-oretical Formulation and Evaluation of Multi-gram.
In ICASSP 1995, Vol.
1, pp.
169-172.Akinori Ito and Masaki Kohda.
Language Modelingby String Pattern N-gram for Japanese SpeechRecogniton.
In ICSLP, 1996.Masayuki Kameda.
A Portable & Quick JapaneseParser: QJP.
In COLING, Copenhagen, 1996.Reinhard Kneser and Hermann Ney.
ImprovedBacking-off for M-gram Language Modeling.In ICASSP 1995, Vol.
1, pp.
181-184.Mark Lauer.
Corpus Statistics Meet the Noun Com-pound: Some Empirical Results.
In ACL,1995.Hubert Hin-Cheung Law and Chorkin Chan.
Er-godic Multigram HMM Integrating Word Seg-mentation and Class Tagging for Chinese Lan-guage Modeling.
In ICASSP 1996, Vol.1, pp.196-199.David M. Magerman and Mitchell P. Marcus.Distituent Parsing and Grammar Induction.pages 122a-122e.Sven Martin, Joerg Liebermann, and Hermann Ney.Algorithms for Bigram and Trigram Cluster-ing.
In Eurospeech, 1995.Hirokazu Masataki and Yoshinori Sagisaka.Variable-order N-gram Generationi byWord-class Splitting and Consecutive WordGrouping.
In ICASSP 1996, Vol.
1, pp.188-191.Yo Matsumoto.
Constraints on the 'Intransitivizing'Resultatitive -re aru construction i Japanese.Mayfield Tomokiyo ~ Ries 68 Learning base units in JapaneseIn Japanese~Korean Linguistics, pp.
269-283,SLA, Stanford, 1990Michael K McCandless and James R Glass.
Empiri-cal Acquisition of Language Models for SpeechRecognition.
In ICSLP, Yokohama, Japan,1994.Tsuyoshi Morimoto et al ATR's Speech TranslationSystem: ASURA.
In Eurospeech, 1993.Shiho Nobesawa et al Segmenting Sentences intoLinkky Strings using D-bigram statistics.
InCOLING, Copenhagen, 1996.Klaus Ries, Finn Dag Bu?, and Alex Waibel ClassPhrase Models for Language Modeling.
In IC-SLP, 1996.Klaus Ries, Finn Dag Bu?, and Ye-Yi Wang.
Im-proved Language Modeling by UnsupervisedAcquisition of Structure.
In ICASSP 1995,Vol.
1, pp.
193-196.Tanja Schultz and Detlef Koll.
Spontaneously Spo-ken Japanese Speech Recognition with Janus-3To appear in EUROSPEECH, 1997.Peter Sells.
VP in Japanese: Evidence from-te Com-plements.
In Japanese/Korean Linguistics, pp.319-333, SLA, Stanford, 1990.Masayoshi Shibatani.
Japanese.
In The World's Ma-jor Languages, pp.
855-880, Bernard Comrie,ed., Oxford University Press, 1987.Koichi Shinoda and Takao Watanabe.
SpeakerAdaptation with Autonomous Model Com-plexity Control by MDL Principle.
In ICASSP1996, Vol.
2, pp.
717-720.Bernhard Suhm and Alex Waibel.
Towards BetterLanguage Models for Spontaneous Speech.
InICSLP, Yokohama, Japan, 1994.B.
V. Suhotin.
Methode de dechiffrage, outil derecherche " n linguistique.
TA Informationes,2:3-43, 1973.Eiichiro Sumita and Hitoshi Iida.
HeterogeneousComputing for Example-based Translation ofSpoken Language.
In Proceedings of the sixthinternational conference on theoretical andmethodological issues in Machine Translation,Leuven, Belgium, 1995.Virginia Teller and Eleanor Olds Batchelder.
AProbabilistic Algorithm for Segmenting Non-Kanji Japanese Strings.
In AAAI pp.
742-747,Seattle, 1994.Kei Yoshimoto and Christine Nanz.
A Study inTransfer Japanese-English.
Verbmobil report101 2/96.Mayfield Tomokiyo ~ Ries 69 Learning base units in Japanese
