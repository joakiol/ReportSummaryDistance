Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 1?10,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsAn on-line system for remote treatment of aphasiaAnna Pompili, Alberto Abad,Isabel TrancosoL2F - Spoken Language Systems LabINESC-ID/IST, Lisbon, Portugal{anna,alberto,imt}@l2f.inesc-id.ptJose?
Fonseca, Isabel P. Martins,Gabriela Leal, Luisa FarrajotaLEL - Language Research LaboratoryLisbon Faculty of Medicine, Portugaljfonseca@fm.ul.ptAbstractAphasia treatment for the recovery of lostcommunication functionalities is possiblethrough frequent and intense speech therapysessions.
In this sense, speech and languagetechnology may provide important support inimproving the recovery process.
The aim ofthe project Vithea (Virtual Therapist for Apha-sia Treatment) is to develop an on-line sys-tem designed to behave as a virtual thera-pist, guiding the patient in performing train-ing exercises in a simple and intuitive fashion.In this paper, the fundamental components ofthe Vithea system are presented, with particu-lar emphasis on the speech recognition mod-ule.
Furthermore, we report encouraging au-tomatic word naming recognition results usingdata collected from speech therapy sessions.1 IntroductionAphasia is a communication disorder that can af-fect various aspects of language, including hearingcomprehension, speech production, and reading andwriting fluency.
It is caused by damage to one ormore of the language areas of the brain.
Many timesthe cause of the brain injury is a cerebral vascularaccident (CVA), but other causes can be brain tu-mors, brain infections and severe head injury dueto an accident.
Unfortunately, in the last decadesthe number of individuals that suffer CVAs has dra-matically increased, with an estimated 600.000 newcases each year in the EU.
Typically, a third ofthese cases present language deficiencies (Pedersenet al, 1995).
This kind of language disorder in-volves countless professional, family and economicproblems, both from the point of view of the individ-ual and the society.
In this context, two remarkableconsiderations have led to the development of thePortuguese national project Vithea (Virtual Thera-pist for Aphasia treatment).First are the enormous benefits that speech andlanguage technology (SLT) may bring to the dailylives of people with physical impairment.
Informa-tion access and environment control are two areaswhere SLT has been beneficially applied, but SLTalso has great potential for diagnosis, assessmentand treatment of several speech disorders (Hawleyet al, 2005).
For instance, a method for speech in-telligibility assessment using both automatic speechrecognition and prosodic analysis is proposed in(Maier et al, 2009).
This method is applied to thestudy of patients that have suffered a laryngotomyand to children with cleft lip and palate.
(Castillo-Guerra and Lovey, 2003) presents a method fordysarthria assessment using features extracted frompathological speech signals.
In (Yin et al, 2009), theauthors describe an approach to pronunciation veri-fication for a speech therapy application.The second reason for undertaking the Vitheaproject is that several aphasia studies have demon-strated the positive effect of speech therapy activi-ties for the improvement of social communicationabilities.
These have focused on specific linguisticimpairments at the phonemic, semantic or syntac-tic levels (Basso, 1992).
In fact, it is believed moreand more that the intensity of speech therapy pos-itively influences language recovery in aphasic pa-tients (Bhogal et al, 2003).These compelling reasons have motivated the de-1velopment of an on-line system for the treatmentof aphasic patients incorporating recent advances inspeech and language technology in Portuguese.
Thesystem will act as a ?virtual therapist?, simulating anordinary speech therapy session, where by means ofthe use of automatic speech recognition (ASR) tech-nology, the virtual therapist will be able to recognizewhat was said by the patient and to validate if it wascorrect or not.
As a result of this novel and special-ized stimulation method for the treatment of aphasia,patients will have access to word naming exercisesfrom their homes at any time, which will certainlycause an increase in the number of training hours,and consequently it has the potential to bring signif-icant improvements to the rehabilitation process.In section 2 we provide a brief description of dif-ferent aphasia syndromes, provide an overview ofthe most commonly adopted therapies for aphasia,and describe the therapeutic focus of our system.Section 3 is devoted to an in depth description ofthe functionalities that make up the system, whilesection 4 aims at detailing its architecture.
Finally,section 5 describes the automatic speech recognitionmodule and discusses the results achieved within theautomatic naming recognition task.2 About the aphasia disorder2.1 Classification of aphasiaIt is possible to distinguish two different types ofaphasia on the basis of the fluency of the speech pro-duced: fluent and non-fluent aphasia.
The speechof someone with fluent aphasia has normal articula-tion and rhythm, but is deficient in meaning.
Typi-cally, there are word-finding problems that most af-fect nouns and picturable action words.
Non-fluentaphasic speech is slow and labored, with short ut-terance length.
The flow of speech is more or lessimpaired at the levels of speech initiation, the find-ing and sequencing of articulatory movements, andthe production of grammatical sequences.
Speech ischoppy, interrupted, and awkwardly articulated.Difficulty of recalling words or names is the mostcommon language disorder presented by aphasic in-dividuals (whether fluent or non-fluent).
In fact, itcan be the only residual defect after rehabilitation ofaphasia (Wilshire and Coslett, 2000).2.2 Common therapeutic approachesThere are several therapeutic approaches for thetreatment of the various syndromes of aphasia.
Of-ten these methods are focused on treating a specificdisorder caused from aphasia.
The most commonlyused techniques are output focused, such as the stim-ulation method and the Melodical Intonation Ther-apy (MIT) (Albert et al, 1994).
Other methods arelinguistic-oriented learning approaches, such as thelexical-semantic therapy or the mapping techniquefor the treatment of agrammatism.
Still, severalnon-verbal methods for the treatment of some se-vere cases of non-fluent aphasia, such as the visualanalog communication, iconic communication, vi-sual action and drawing therapies, are currently used(Sarno, 1981; Albert, 1998).Although there is an extensive list of treatmentsspecifically designed to recover from particular dis-orders caused by aphasia, one class of rehabilita-tion therapy especially important aims to improvethe recovery from word retrieval problems, given thewidespread difficulty of recalling words or names.Naming ability problems are typically treated withsemantic exercises like Naming Objects or Namingcommon actions (Adlam et al, 2006).
The approachtypically followed is to subject the patient to a set ofexercises comprising a set of stimuli in a variety oftasks.
The stimuli are chosen based on their seman-tic content.
The patient is asked to name the subjectthat has been shown.2.3 Therapeutic focus of the Vithea systemThe focus of the Vithea system is on the recoveryof word naming ability for aphasic patients.
So far,experiments have only been made with fluent apha-sia patients, but even for this type of aphasia, majordifferences may be found.
Particularly, patients withTranscortical sensorial aphasia, Conduction aphasiaand Anomic aphasia (Goodglass, 1993) have beenincluded in our studies.Although the system has been specifically de-signed for aphasia treatment, it may be easilyadapted to the treatment or diagnosis of other dis-orders in speech production.
In fact, two of thepatients that have participated in our experimen-tal study were diagnosed with acquired apraxia ofspeech (AOS), which typically results from a stroke,2Figure 1: Comprehensive overview of the Vithea system.tumor, or other known neurological illness or injury,and is characterized by inconsistent articulatory er-rors, groping oral movements to locate the correctarticulatory position, and increasing errors with in-creasing word and phrase length.3 The Vithea SystemThe overall flow of the system can be described asfollows: when a therapy session starts, the virtualtherapist will show to the patient, one at a time, aseries of visual or auditory stimuli.
The patient isrequired to respond verbally to these stimuli by nam-ing the contents of the object or action that is repre-sented.
The utterance produced is recorded, encodedand sent via network to the server side.
Here, a webapplication server receives the audio file via a servletthat serves as an interface to the ASR system, whichtakes as input the audio file encoding the patient?sanswer and generates a textual representation of it.This result is then compared with a set of predeter-mined textual answers (for that given question, ofcourse) in order to verify the correctness of the pa-tient?s input.
Finally, feedback is sent back to thepatient.
Figure 1 shows a comprehensive view ofthis process.The system comprises two specific modules, dedi-cated respectively to the patients for carrying out thetherapy sessions and to the clinicians for the admin-istration of the functionalities related to them.
Thetwo modules adhere to different requirements thathave been defined for the particular class of user forwhich they have been developed.
Nonetheless theyshare the set of training exercises, that are built bythe clinicians and performed by the patients.3.1 Speech therapy exercisesFollowing the common therapeutic approach fortreatment of word finding difficulties, a training ex-ercise is composed of several semantic stimuli items.The stimuli may be of several different types: text,audio, image and video.
Like in ordinary speechtherapy sessions, the patient is asked to respond tothe stimuli verbally, describing the imaging he/shesees or completing a popular saying (which was pre-sented verbally or in text).Exercise categoriesThe set of therapeutic exercises integrated inVithea has been designed by the Language ResearchLaboratory of the Department of Clinical Neuro-science of the Lisbon Faculty of Medicine (LEL).LEL has provided a rich battery of exercises that canbe classified into two macro-categories according tothe main modality of the stimulus, namely:A) Image or video: Naming object picture, Namingof verbs with action pictures, and Naming verbsgiven pictures of objects.B) Text or speech: Responsive Naming, CompleteSayings, Part-whole Associations, What nameis given to.
.
.
, Generic Designation, Naming byfunction, Phonological Evocation, and Seman-tics Evocation.Exercises can be also classified according toThemes, in order to immerse the individual in a prag-matic, familiar environment: a) Home b) Animalsc) Tools d) Food e) Furniture f) Professions g) Ap-pliances h) Transportation i) Alive/Not Alive j) Ma-nipulable/Not Manipulable k) Clothing l) Random.Evaluation exercisesIn addition to the set of training exercises, whichare meant to be used on a daily basis by the apha-sic patient, the Vithea system also supports a dif-ferent class of exercises: Evaluation Exercises.
Un-like training exercises, evaluation exercises are usedby human therapists to periodically assess the pa-tient?s progress and his/her current degree of apha-sia via an objective metric denoted as Aphasia Quo-tient (AQ).
Evaluation exercises are chosen from a3subset of the previously mentioned classes of ther-apeutic exercises, namely: Naming object picture,Naming of verbs with action pictures, and Namingverbs given pictures of objects.3.2 Patient ModuleThe patient module is meant to be used by aphasicindividuals to perform the therapeutic exercises.Visual design considerationsMost of the users for whom this module is in-tended have had a CVA.
Because of this, they mayhave some forms of physical disabilities such as re-duced arm mobility, and therefore they may experi-ence problems using a mouse.
Acknowledging thiseventuality, particular attention has been given to thedesign of the graphical user interface (GUI) for thismodule, making it simple to use both at the presen-tation level and in terms of functionality provided.Driven by the principle of accessibility, we designedthe layout in an easy to use and understand fashion,such that the interaction should be predictable andunmistakable.Moreover, even though aphasia is increasing inthe youngest age groups, it still remains a predomi-nant disorder among elderly people.
This age groupis prone to suffer from visual impairments.
Thus,we carefully considered the graphic elements cho-sen, using big icons for representing our interfaceelements.
Figure 2 illustrates some screenshots ofthe Patient Module on the top.Exercise protocolOnce logged into the system, the virtual therapistguides the patient in carrying out the training ses-sions, providing a list of possible exercises to be per-formed.
When the patient choses to start a trainingexercise, the system will present target stimuli oneat a time in a random way.
After the evaluation ofthe patient?s answer by the system, the patient canlisten again to his/her previous answer, record againan utterance (up to a number of times chosen beforestarting the exercise) or pass to the next exercise.Patient trackingBesides permitting training sessions, the patientmodule has the responsibility of storing statisticaland historical data related to user sessions.
User ut-terances and information about each user access tothe system are stored in a relational database.
Par-ticularly, start and end time of the whole trainingsession, of a training exercise, and of each stimulusare collected.
On the one hand, we log every accessin order to evaluate the impact and effectiveness ofthe program by seeing the frequency with which itis used.
On the other hand, we record the total timeneeded to accomplish a single stimulus or to end awhole exercise in order to estimate user performanceimprovements.3.3 Clinician ModuleThe clinician module is specifically designed to al-low clinicians to manage patient data, to regulatethe creation of new stimuli and the alteration of theexisting ones, and to monitor user performance interms of frequency of access to the system and userprogress.
The module is composed by three sub-modules: User, Exercise, Statistic.User sub-moduleThis module allows the management of a knowl-edge base of patients.
Besides basic informationrelated to the user personal profile, the databasealso stores for each individual his/her type of apha-sia, his/her aphasia severity (7-level subjective scale)and AQ information.Exercise sub-moduleThis module allows the clinician to create, update,preview and delete stimuli from an exercise.
An ex-ercise is composed of a varying number of stimuli.In addition to the canonical valid answer, the systemaccepts for each stimulus an extended word list com-prising three extra valid answers.
This list allows thesystem to consider the most frequent synonyms anddiminutives.Since the stimuli are associated with a wide as-sortment of multimedia files, besides the manage-ment of the set of stimuli, the sub-module also pro-vides a rich Web based interface to manage thedatabase of multimedia resources used within thestimuli.
Figure 2c shows a screenshot listing somemultimedia files.
From this list, it is possible to se-lect a desired file in order to edit or delete it.In this context, a preview feature has also beenprovided.
The system is capable of handling a widerange of multimedia encoding: audio (accepted file4Figure 2: Vithea system screenshots: a) Interface with preview of the stimuli constituting an exercise in the patientsmodule (top-left), b) interface for performing a specific stimulus in the patients module (top-right), c) interface forthe management of multimedia resources in the clinician module (bottom-left) and d) interface for the creation of newstimulus in the clinician module (bottom-right).types: wav, mp3), video (accepted file types: wmv,avi, mov, mp4, mpe, mpeg, mpg, swf), and images(accepted file types: jpe, jpeg, jpg, png, gif, bmp, tif,tiff).Given the diversity of the various file types ac-cepted by the system, a conversion to a unique filetype was needed, in order to show them all with onlyone external tool.
Audio files are therefore convertedto mp3 file format, while video files are converted toflv file format.Finally, a custom functionality has been designedto create new stimuli in an intuitive fashion similarin style to a WYSIWYG editor.
Figure 2d illustratesthe stimuli editor, showing how to insert a multime-dia resource.Statistics sub-moduleThis module allows the clinician both to monitorstatistical information related to user-system inter-actions and to access the utterances produced by thepatient during the therapeutic sessions.
The statisti-cal information comprises data related to the user?sprogress and to the frequency with which users ac-cess the system.
On the one hand, we provide allthe attempts recorded by the patients in order toallow a re-evaluation by clinicians.
This data canbe used to identify possible weaknesses or errorsfrom the recognition engine.
On the other hand, wethought that monitoring the utilization of the appli-cation from the users could be an important piece offeedback about the system?s feasibility.
This is moti-vated by common concerns about the fact that someusers abandon their therapeutic sessions when theyare not able to see quick results in terms of improve-ments.4 Architectural OverviewConsidering the aforementioned requirements andfeatures that will make up the system, LearningManagement Systems (LMSs) software applicationswere initially considered.
LMSs automate the ad-5ministration of training events, manage the log-in ofregistered users, manage course catalog, record datafrom learners and provide reports to the manage-ment (Aydin and Tirkes, 2010).
Thus, an in-depthevaluation of the currently widespread solutions wascarried out (Pompili, 2011).
Concretely, eight differ-ent LMSs (Atutor, Chamilo, Claroline, eFront, Ilias,Moodle, Olat, Sakai) were studied in detail.
Unfor-tunately, the outcome of this study revealed impor-tant drawbacks.The main problem noticed is that LMSs are typi-cally feature-rich tools that try to be of general pur-pose use, sometimes resulting in the loss of theirusefulness to the average user.
Often the initial userreaction to the interface of these tools is confusion:the most disorienting challenge is figuring out whereto get the information needed.
As previously men-tioned, patients who have had a CVA may experi-ence physical deficiencies, thus the Vithea systemneeds an easy to use and understandable interface.We dedicated some effort trying to personalize LMSsolutions, but most of them do not allow easy sim-plification of the presentation layout.Moreover, while there were several differencesbetween the functionalities that the evaluated LMSsprovided in terms of training exercises, they all pre-sented various limitations in their implementation.Eventually, we had to acknowledge that it wouldhave been extremely complex to customize the eval-uated frameworks to meet the Vithea project require-ments without introducing major structural changesto the code.Besides, the average user for whom the Vitheasystem is intended is not necessarily accustomedwith computers and even less with these tools, whichin most cases are developed for environments suchas universities or huge organizations.
This meansthat our users may lack the technical skills neces-sary to work with an LMS, and the extra effort ofunderstanding the system would result in a loss ofmotivation.Therefore, considering the conclusions from thisstudy, we have opted to build a modular, portableapplication which will totally adhere to our require-ments.
With these purposes in mind, the system hasbeen designed as a multi-tier web application, beingaccessible everywhere from a web browser.
The im-plementation of the whole system has been achievedby integrating different technologies of a heteroge-neous nature.
In fact, the presentation tier exploitsAdobe R?Flash R?technology in order to support richmultimedia interaction.
The middle tier comprisesthe integration of our own speech recognition sys-tem, AUDIMUS, and some of the most advancedopen source frameworks for the development of webapplications, Apache Tiles, Apache Struts 2, Hiber-nate and Spring.
In the data tier, the persistenceof the application data is delegated to the relationaldatabase MySQL.
This is where the system main-tains information related to patient clinical data, ut-terances produced during therapeutic sessions, train-ing exercises, stimuli and statistical data related bothto the frequency with which the system is used, andto the patient progress.4.1 Speech-related components of the systemAudio RecorderIn order to record the patient?s utterances, theVithea system takes advantage of opportunities of-fered by Adobe R?Flash R?technology.
This allowseasy integration in most browsers without any re-quired extra plugin, while avoiding the need for se-curity certificates to attest to the reliability of anexternal component running in the client machinewithin the browser.
This choice was mainly moti-vated from the particular kind of users who will usethe system, allowing them to enjoy the advantagesof the virtual therapist without the frustration of ad-ditional configuration.
A customized component hasbeen developed following the aforementioned prin-ciples of usability in terms of designing the user in-terface.
Keeping simplicity and understandabilityas our main guidelines, we used a reduced set oflarge symbols and we tried to keep the number ofinteractions required to a bare minimum.
Therefore,recording and sending an utterance to the server re-quires only that the patient starts the recording whenready, and then stops it when finished.
Another ac-tion is required to play back the recorded audio.Automatic Speech Recognition EngineAUDIMUS is the Automatic Speech Recognitionengine integrated into the Vithea system.
The AU-DIMUS framework has been developed during thelast years of research at the Spoken Language Pro-cessing Lab of INESC-ID (L2F), it has been success-6fully used for the development of several ASR appli-cations such as the recognition of Broadcast News(BN) (Meinedo et al, 2010).
It represents an essen-tial building block, being the component in chargeof receiving the patient answers and validating thecorrectness of the utterances with respect to the ther-apeutic exercises.
In the following section, this spe-cific module of the Vithea architecture is assessedand described in more detail.5 The Vithea speech recognition module5.1 The AUDIMUS hybrid speech recognizerAUDIMUS is a hybrid recognizer that follows theconnectionist approach (Boulard and Morgan, 1993;Boulard and Morgan, 1994).
It combines the tem-poral modeling capacity of Hidden Markov Mod-els (HMMs) with the pattern discriminative classi-fication of multilayer perceptrons (MLP).
A Markovprocess is used to model the basic temporal natureof the speech signal, while an artificial neural net-work is used to estimate posterior phone probabili-ties given the acoustic data at each frame.
Each MLPis trained on distinct feature sets resulting from dif-ferent feature extraction processes, namely Percep-tual Linear Predictive (PLP), log-RelAtive SpecTrAlPLP (RASTA-PLP) and Modulation SpectroGram(MSG).The AUDIMUS decoder is based on the WeightedFinite State Transducer (WFST) approach to largevocabulary speech recognition (Mohri et al, 2002).The current version of AUDIMUS for the Euro-pean Portuguese language uses an acoustic modeltrained with 57 hours of downsampled BroadcastNews data and 58 hours of mixed fixed-telephoneand mobile-telephone data (Abad and Neto, 2008).5.2 Word Naming Recognition taskWe refer to word recognition as the task that per-forms the evaluation of the utterances spoken by thepatients, in a similar way to the role of the thera-pist in a rehabilitation session.
This task representsthe main challenge addressed by the virtual ther-apist system.
Its difficulty is related to the utter-ances produced by aphasic individuals that are fre-quently interleaved with disfluencies like hesitation,repetitions, and doubts.
In order to choose the bestapproach to accomplish this critical task, prelimi-nary evaluations were performed with two sub-setsof the Portuguese Speech Dat II corpus.
These con-sist of word spotting phrases using embedded key-words: the development set is composed of 3334utterances, while the evaluation set comprises 481utterances.
The number of keywords is 27.
Two dif-ferent approaches were compared: the first basedon large vocabulary continuous speech recognition(LVCSR), the second based on the acoustic match-ing of speech with keyword models in contrast toa background model.
Experimental results showedpromising performance indicators by the latter ap-proach, both in terms of Equal Error Rate (EER),False Alarm (FA) and False Rejection (FR).
Thus,on the basis of these outcomes, background model-ing based keyword spotting (KWS) was consideredmore appropriate for this task.Background modeling based KWSIn this work, an equally-likely 1-gram modelformed by the possible target keywords and a com-peting background model is used for word detec-tion.
While keyword models are described by theirsequence of phonetic units provided by an auto-matic grapheme-to-phoneme module, the problemof background modeling must be specifically ad-dressed.
The most common method consists ofbuilding a new phoneme classification network thatin addition to the conventional phoneme set, alsomodels the posterior probability of a backgroundunit representing ?general speech?.
This is usuallydone by using all the training speech as positive ex-amples for background modeling and requires re-training the acoustic networks.
Alternatively, theposterior probability of the background unit can beestimated based on the posterior probabilities of theother phones (Pinto et al, 2007).
We followed thesecond approach, estimating the posterior probabil-ity of a garbage unit as the mean probability of thetop-6 most likely outputs of the phonetic network ateach time frame.
In this way there is no need foracoustic network re-training.
Then, a likelihood-dependent decision threshold (determined with tele-phonic data for development) is used to prune thebest recognition hypotheses to a reduced set of sen-tences where the target keyword is searched for.75.3 Experiments with real dataCorpus of aphasic speechA reduced speech corpus composed of data col-lected during therapy sessions of eight different pa-tients has been used to assess the performance ofthe speech recognition module.
As explained above,two of them (patients 2 and 7) were diagnosed withAOS.
Each of the sessions consists of naming ex-ercises with 103 objects per patient.
Each object isshown with an interval of 15 seconds from the pre-vious.
The objects and the presentation order are thesame for all patients.
Word-level transcription andsegmentation were manually produced for the pa-tient excerpts in each session, totaling 996 segments.The complete evaluation corpus has a duration of ap-proximately 1 hour and 20 minutes.Evaluation criteriaA word naming exercise is considered to be com-pleted correctly whenever the targeted word is saidby the patient (independently of its position, amountof silence before the valid answer, etc...).
It isworth noticing that this is not necessarily the crite-rion followed in therapy tests by speech therapists.In fact, doubts, repetitions, corrections, approxima-tion strategies and other similar factors are usuallyconsidered unacceptable in word naming tests, sincetheir presence is an indicator of speech pathologies.However, for the sake of comparability between ahuman speech therapist evaluation and an automaticevaluation, we keep this simplified evaluation crite-rion.
In addition to the canonical valid answer toevery exercise, an extended word list containing themost frequent synonyms and diminutives has beendefined, for a total KWS vocabulary of 252 words.Only answers included in this list have been ac-cepted as correct in both manual and automatic eval-uation.ResultsWord naming scores are calculated for eachspeaker as the number of positive word detectionsdivided by the total number of exercises (leftmostplot of Figure 3).
The correlation between the hu-man evaluation assessed during ordinary therapeu-tic sessions and the automatic evaluation assessedwith the word recognition task has resulted in a Per-son?s coefficient of 0.9043.
This result is consideredquite promising in terms of global evaluation.
Asconcerning individual evaluations (rightmost plot ofFigure 3), it can be seen that the system shows re-markable performance variability in terms of falsealarms and misses depending on the specific patient.In this sense, the adaptation to the specific user pro-file may be interesting in terms of adjusting the sys-tem?s operation point to the type and level of apha-sia.
As a preliminary attempt to tackle the cus-tomization issue, the word detector has been indi-vidually calibrated for each speaker following a 5-fold cross-validation strategy with the correspond-ing patient exercises.
The calibration is optimized tothe minimum false alarm operation point for patientswith high false-alarm rates (2, 3, 4, 5 and 8) and tothe minimum miss rate for patients with a high num-ber of misses (1, 6 and 7).
Figure 4 shows results forthis customized detector.
In this case, the correlationbetween human and automatic evaluation is 0.9652and a more balanced performance (in terms of falsealarm and false rejection ratios) is observed for mostspeakers.1 2 3 4 5 6 7 800.10.20.30.40.50.60.70.80.91PatientAverage word naming scoreHumanAuto1 2 3 4 5 6 7 800.050.10.150.20.250.30.350.40.450.5PatientFalsealarm/false rejectionratesFalse alarmFalse rejectionFigure 3: On the left side, average word naming scores ofthe human and automatic evaluations.
On the right side,false alarm and false rejection rates.1 2 3 4 5 6 7 800.10.20.30.40.50.60.70.80.91PatientAverage word naming scoreHumanAuto1 2 3 4 5 6 7 800.050.10.150.20.250.30.350.40.450.5PatientFalsealarm/false rejectionratesFalse alarmFalse rejectionFigure 4: On the left side, average word naming scores ofthe human and automatic evaluations with the customizeddetector.
On the right side, false alarm and false rejectionrates of the customized detector.8Analysis of word detection errorsThe most common cause for false alarms is thepresence of many ?invented?
nonexistent wordswithout semantic meaning, which are very oftenphonetically very close to the target words.
Theseparaphasic errors were present in all types of fluentaphasia and AOS that we have observed, but not forall patients.
In many of these errors, the stressed syl-lable is often pronounced right, or at least its rhyme.As the typical stress pattern in Portuguese is in thepenultimate syllable, most often the last syllable isalso pronounced correctly (e.g.
borco / porco).
Inpatients that try to say the word by approximation,that is, by successive attempts to get closer to thetarget word, but using only existent words, the dif-ferences between the percentages of miss and falsealarms are not so remarkable.One characteristic of aphasic patients that some-times causes keywords to be missed (both when cor-rectly or incorrectly pronounced) is pauses in be-tween syllables.
This may justify the inclusion ofalternative pronunciations, in case such pronuncia-tions are considered acceptable by therapists.
Ad-ditionally, more sophisticated speech tools may alsobe integrated, such as tools for computing the good-ness of pronunciation (Witt, 1999).
This would al-low a different type of assessment of the pronuncia-tion errors, which may provide useful feedback forthe therapist and the patients.6 Conclusions and future work6.1 ConclusionsThis paper described how automatic speech recog-nition technology has contributed to build up a sys-tem that will act as a virtual therapist, being capa-ble of facilitating the recovery of people who have aparticular language disorder: aphasia.
Early experi-ments conducted to evaluate ASR performance withspeech from aphasic patients yielded quite promis-ing results.The virtual therapist has been designed follow-ing relevant accessibility principles tailored to theparticular category of users targeted by the system.Special attention has been devoted to the user in-terface design: web page layout and graphical ele-ments have been chosen keeping in mind the possi-bility that a user may experience reduced arm mobil-ity and the technology that has been integrated wasselected with the idea of minimizing possible diffi-culties in using the system.
A pedagogical approachhas been followed in planning the functionalities ofthe virtual therapist.
This has been mainly drivenby the fundamental idea of avoiding an extra featurerich tool which could have resulted in frustration forsome patients, who seek help for recovery and donot need to learn how to use complex software.Overall, since the system is a web application, itallows therapy sessions anywhere at anytime.
Thus,we expect that this will bring significant improve-ments to the quality of life of the patients allowingmore frequent, intense rehabilitation sessions andthus a faster recovery.6.2 Future workThe Vithea system has recently achieved the firstphase of a project which still entails several im-provements.
Even though, Naming objects and Nam-ing common actions are the most commonly usedexercises during the rehabilitation therapies, the sys-tem has been designed to allow a more comprehen-sive set of therapeutic exercises which will be im-plemented during the next refinement phase.
Also,at this stage, we plan to make available the currentversion of the system to real patients in order to re-ceive effective feedback on the system.In the subsequent improvement phase, we will in-tegrate the possibility of providing help, both seman-tic and phonological to the patient whenever the vir-tual therapist is asked for.
Hints could be given bothin the form of a written solution or as a speech syn-thesized production based on Text To Speech (TTS).Furthermore, we are considering the possibility ofincorporating an intelligent animated agent that to-gether with the exploitation of synthesized speech,will behave like a sensitive and effective clinician,providing positive encouragements to the user.AcknowledgementsThis work was funded by the FCT projectRIPD/ADA/109646/2009, and partially supportedby FCT (INESC-ID multiannual funding) throughthe PIDDAC Program funds.
The authors would liketo thank to Prof. Dr. M. T. Pazienza, A. Costa and thereviewers for their precious comments.9ReferencesA.
Abad and J. P. Neto.
2008. International Confer-ence on Computational Processing of Portuguese Lan-guage, Portugal.
Automatic classification and tran-scription of telephone speech in radio broadcast data.A.
L. R. Adlam, K. Patterson, T. T. Rogers, P. J. Nestor,C.
H. Salmond, J. Acosta-Cabronero and J. R. Hodges.2006.
Brain.
Semantic dementia and PrimaryProgressive Aphasia: two side of the same coin?,129:3066?3080.M.
L. Albert, R. Sparks and N. A.
Helm.
1994.
Neu-rology.
Report of the Therapeutics and TechnologyAssessment Subcommittee of the American Academyof Neurology.
Assesment: melodic intonation therapy,44:566?568.M.
L. Albert.
1998.
Arch Neurol-Chicago Treatment ofaphasia, 55:1417?1419.C.
C. Aydin and G. Tirkes.
2010.
Education Engineer-ing.
Open source learning management systems in e-learning and Moodle, 54:593?600.A.
Basso.
1992.
Aphasiology.
Prognostic factors inaphasia, 6(4):337?348.S.
K. Bhogal, R. Teasell and M. Speechley.
2003.Stroke.
Intensity of aphasia therapy, impact on recov-ery, 34:987?993.H.
Bourlard and N. Morgan.
1993.
IEEE Transactionson Neural Networks.
Continuous speech recognitionby connectionist statistical methods, 4(6):893?909.H.
Bourlard and N. Morgan.
1994.
Springer.
Connec-tionist speech recognition: a hybrid approach.D.
Caseiro, I. Trancoso, C. Viana and M. Barros.2003.
International Congress of Phonetic Sciences,Barcelona, Spain.
A Comparative Description of GtoPModules for Portuguese and Mirandese Using FiniteState Transducers.E.
Castillo-Guerra and D. F. Lovey.
2003.
25th AnnualConference IEEE Engineering in Medicine and Biol-ogy Society.
A Modern Approach to Dysarthria Clas-sification.H.
Goodglass.
1993.
Understanding aphasia: technicalreport.
Academy Press, University of Califo?rnia.
SanDiego.M.
S. Hawley, P. D. Green, P. Enderby, S. P. Cunninghamand R. K. Moore.
2005.
Interspeech.
Speech technol-ogy for e-inclusion of people with physical disabilitiesand disordered speech, 445?448.A.
Maier, T. Haderlein, U. Eysholdt, F. Rosanowski,A.
Batliner, M. Schuster and E. No?th.
2009.
SpeechCommunication.
PEAKS - A System for the AutomaticEvaluation of Voice and Speech Disorders, 51(5):425?437.H.
Meinedo and J. P. Neto.
2000. International Con-ference on Spoken Language Processing, Beijing,China.
Combination Of Acoustic Models In Contin-uous Speech Recognition Hybrid Systems, 2:931?934.H.
Meinedo, A. Abad, T. Pellegrini, I. Trancoso andJ.
P. Neto.
2010.
Fala 2010, Vigo, Spain.
The L2FBroadcast News Speech Recognition System.M.
Mohri, F. Pereira and M. Riley.
2002.
ComputerSpeech and Language.
Weighted Finite-State Trans-ducers in Speech Recognition, 16:69?88.P.
M. Pedersen, H. S. J?rgensen, H. Nakayama,H.
O. Raaschou and T. S. Olsen.
1995.
Ann Neu-rol Aphasia in acute stroke: incidence, determinants,and recovery, 38(4):659?666.J.
Pinto, A. Lovitt and H. Hermansky.
2007.
Inter-speech.
Exploiting Phoneme Similarities in HybridHMM-ANN Keyword Spotting, 1817?1820.A.
Pompili.
2011.
Thesis, Department of Computer Sci-ence, University of Rome.
Virtual therapist for apha-sia treatment.M.
T. Sarno.
1981.
Recovery and rehabilitation in apha-sia, 485?530.
Acquired Aphasia, Academic Press,New York.C.
E. Wilshire and H. B. Coslett.
2000.
Disorders ofword retrieval in aphasia theories and potential appli-cations, 82?107.
Aphasia and Language: Theory topractice, The Guilford Press, New York.S.
M. Witt.
1999.
Use of speech recognition in Computerassisted Language Learning.
PhD thesis, Departmentof Engineering, University of Cambridge.S.
-C. Yin, R. Rose, O. Saz and E. Lleida.
2009.
IEEE In-ternational Conference on Acoustics, Speech and Sig-nal Processing.
A study of pronunciation verificationin a speech therapy application, 4609?4612.10
