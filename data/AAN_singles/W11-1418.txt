Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 142?151,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsUsing Semantic Distance to Automatically SuggestTransfer Course EquivalenciesBeibei YangUniversity of Massachusetts LowellOne University AvenueLowell, MA 01854byang1@cs.uml.eduJesse M. HeinesUniversity of Massachusetts LowellOne University AvenueLowell, MA 01854heines@cs.uml.eduAbstractSemantic distance is the degree of closenessbetween two pieces of text determined by theirmeaning.
Semantic distance is typically mea-sured by analyzing a set of documents or a listof terms and assigning a metric based on thelikeness of their meaning or the concept theyrepresent.
Although related research providessome semantic-based algorithms, few applica-tions exist.
This work proposes a semantic-based approach for automatically identifyingpotential course equivalencies given their cat-alog descriptions.
The method developed byLi et al (2006) is extended in this paper to takea course description from one university as theinput and suggest equivalent courses offered atanother university.
Results are evaluated andfuture work is discussed.1 IntroductionHundreds of students transfer to University of Mas-sachusetts Lowell (UML) each year.
As part ofthat process, courses taken at students?
previous ed-ucational institutions must be evaluated by UMLfor transfer credit.
Course descriptions are usuallyshort paragraphs of less than 200 words.
To de-termine whether an incoming course can be trans-ferred, the undergraduate and graduate transfer coor-dinators from each department must manually com-pare its course description to the courses offeredat UML.
This process can be tedious and time-consuming.
Although the publicly available coursetransfer dictionary (Figure 1) for students transfer-ring to UML lists equivalent courses from hundredsof institutions, it is not always up to date and the dataset is sparse and non-uniformed.This work proposes an approach to automati-cally identify course equivalencies by analyzing thecourse descriptions and comparing their semanticdistance.
The course descriptions are first prunedand unrelated contexts are removed.
Given a coursefrom another university, the algorithm measuresword, sentence, and paragraph similarities to sug-gest a list of potentially equivalent courses offeredby UML.
This work has two goals: (1) to efficientlyand accurately suggest equivalent courses to reducethe workload of transfer coordinators, and (2) to ex-plore new applications using semantic distance tomove toward the Semantic Web, i.e., to turn exist-ing resources into knowledge structures. 	   		      !
	   "	#"	"$ % 	&"'( )$$ %** "+,	-.% %!% ,	-.$/  % $/ 012,)3"012	)3"Figure 1.
A subset of the transfer dictionary forstudents transferred from an external institution toUML.1422 Related ResearchSemantic distance measures have been used in appli-cations such as automatic annotation, keyword ex-traction, and social network extraction (Matsuo etal., 2007).
It is important to note that there are twokinds of semantic distance: semantic similarity andsemantic relatedness.
Semantic relatedness is moregeneric than semantic similarity in that it includes allclassical and non-classical semantic relations suchas holonymy1, meronymy2, and antonymy3, wheresemantic similarity is limited to relations such ashyponymy4 and hypernymy5 (Budanitsky and Hirst,2006).
The terms semantic distance, semantic relat-edness, and semantic similarity are sometimes usedinterchangeably by different authors in the literaturerelated to this topic.
The relative generality of thethree terms is illustrated in Figure 2.Semantic DistanceSemantic RelatednessSemantic SimilarityFigure 2.
The relations of semantic distance, seman-tic relatedness, and semantic similarity as describedby Budanitsky and Hirst (2006).Related work in semantic distance measurementcan be roughly divided into three categories: (1) lex-icographic resource based methods, (2) corpus basedmethods, and (3) hybrid methods.1A holonym is a word that names the whole of which a givenword is a part.
For example, ?hat?
is a holonymy for ?brim?
and?crown.
?2A meronym is a word that names a part of a larger whole.For example, ?brim?
and ?crown?
are meronyms of ?hat.
?3A antonym is a word that expresses a meaning opposed tothe meaning of another word.
For example, ?big?
is an antonymof ?small.
?4A hyponym is a word that is more specific than a givenword.
For example, ?nickel?
is a hyponym of ?coin.
?5A hypernym is a word that is more generic than a givenword.
For example, ?coin?
is a hypernym of ?nickel.
?Figure 3.
A fragment of WordNet?s taxonomy.Lexicographic resource based methods typicallycalculate semantic distance based on WordNet6.
Inrelated work (Rada et al, 1989; Wu and Palmer,1994; Leacock and Chodorow, 1998; Hirst andSt-Onge, 1998; Yang and Powers, 2005), lexico-graphic resource based methods use one or moreedge-counting (also known as shortest-path) tech-niques in the WordNet taxonomy (Figure 3).
In thistechnique, concept nodes are constructed in a hierar-chical network and the minimum number of hops be-tween any two nodes represents their semantic dis-tance (Collins and Quillian, 1969).
The measure byHirst and St-Onge (1998) is based on the fact that thetarget concepts are likely more distant if the targetpath consists of edges that belong to many differentrelations.
The approach by Leacock and Chodorow(1998) combines the shortest path with maximumdepth so that edges lower down in the is-a hierar-chy correspond to smaller semantic distances thanthe ones higher up.
Yang and Powers (2005) furthersuggest that it is necessary to consider relations suchas holonymy and meronymy.A corpus-based method typically calculates co-occurrence on one or more corpora to deduce seman-tic closeness (Sahami and Heilman, 2006; Cilibrasiand Vitanyi, 2007; Islam and Inkpen, 2006; Mihal-cea et al, 2006).
Using this technique, two words6http://wordnet.princeton.edu/143are likely to have a short semantic distance if theyco-occur within similar contexts (Lin, 1998).Hybrid methods (including distributional mea-sures) combine lexicographic resources with corpusstatistics (Jiang and Conrath, 1997; Mohammad andHirst, 2006; Li et al, 2003; Li et al, 2006).
Relatedwork shows that hybrid methods generally outper-form lexicographic resource based and corpus basedmethods (Budanitsky and Hirst, 2006; Curran, 2004;Mohammad and Hirst, 2006; Mohammad, 2008).Li et al (2006) proposed a hybrid method basedonWordNet and the Brown corpus to incorporate se-mantic similarity between words, semantic similar-ity between sentences, and word order similarity tomeasure overall sentence similarity.
The semanticsimilarity between words is derived from WordNetbased on path lengths and depths of lowest commonhypernyms.
The semantic similarity between twosentences is defined as the cosine coefficient of twovectors that are derived from building two seman-tic vectors and collecting the information content foreach term from the Brown corpus.
The word ordersimilarity is then determined by the normalized dif-ference in word order of each sentence.
Finally, theoverall sentence similarity is defined as the weightedsum of the semantic similarity between sentencesand the word order similarity.3 Proposed MethodThis work proposes a variant of the hybrid methodby Li et al (2006) to identify course equivalen-cies by measuring the semantic distance betweencourse descriptions.
Our approach has three mod-ules: (1) semantic distance between words, (2) se-mantic distance between sentences, and (3) seman-tic distance between paragraphs.
Their word ordersimilarity and overall sentence similarity modulesare found to decrease the accuracy (See Section 4).Therefore, these methods are not used in our ap-proach.
This work modifies the semantic similaritybetween words and the semantic similarity betweensentences modules developed by Li et al (2006) andadds semantic distance between paragraphs tailoredto the domain of identifying equivalent courses.
Ex-periments show that these modifications maximizedaccuracy.3.1 Semantic Distance Between WordsGiven a concept c1 of word w1, and a concept c2of word w2, the semantic distance between the twowords (SDBW) is a function of the path length be-tween the two concepts and the depth of their lowestcommon hypernym.The path length p from c1 to c2 is determinedby one of five cases.
This work adds holonymyand meronymy relations to the method by Li et al(2006) to measure the semantic relatedness:1. c1 and c2 are in the same synonym set (synset).2. c1 and c2 are not in the same synset, but thesynset of c1 and the synset of c2 contain one ormore common words.3.
c1 is either a holonym or a meronym of c2.4.
c1 is neither a holonym nor a meronym of c2,but the synset of c1 contains one or more wordsthat are either holonyms or meronyms of one ormore words in the synset that c2 belongs to.5.
c1 and c2 do not satisfy any of the previous fourcases.If c1 and c2 belong to case 1, p is 0.
If c1 and c2belong to cases 2, 3, or 4, p is 1.
In case 5, p is thenumber of links between the two words.
Therefore,the semantic distance of c1 and c2 is an exponentialdecaying function of p, where ?
is a constant (Li etal., 2006):f1(p) = e?p (?
?
[?1, 0]).
(1)Let h be the depth of the lowest common hyper-nym of c1 and c2 in the WordNet hierarchy.
f2 isa monotonically increasing function of h (Li et al,2006):f2(h) =e?h ?
e?
?he?h + e??h(?
?
[0, 1]).
(2)The values of ?
and ?
are given in Section 4.The semantic distance between concepts c1 and c2is defined as:fword(c1, c2) = f1(p) ?
f2(h), (3)where f1 and f2 are given by Equations 1 and 2.
Thevalues of both f1 and f2 are between 0 and 1 (Li etal., 2006).WordNet is based on concepts, not words.
Wordswith different meanings are considered different144?words?
and are marked with sense tags (Budanit-sky and Hirst, 2006).
Unfortunately, common cor-pora (as well as course descriptions) are not sense-tagged.
Therefore, a mapping between a word anda certain sense must be provided.
Such mappingis called word sense disambiguation (WSD), whichis the ability to identify the meaning of words incontext in a computational manner (Navigli, 2009).We consider two strategies to perform the WSD: (1)compare all senses of two words and select the max-imum score, and (2) apply the first sense heuris-tic (McCarthy et al, 2004).
We will show that theoverall performance of the two strategies is aboutthe same.To improve accuracy, the parts of speech7 (POS)of two words have to be the same before visiting theWordNet taxonomy to determine their semantic dis-tance.
Therefore, ?book?
as in ?read a book?
and?book?
as in ?book a ticket?
are considered differ-ent.
We do not distinguish the plural forms of POSfrom singular forms.
Therefore, POS such as ?NN?
(the singular form of a noun) and ?NNS?
(the pluralform of a noun) are considered the same.The SDBW module also considers the stemmedforms of words.
Without considering stemmedwords, two equivalent course titles such as ?net-working?
and ?data communication?
are misclassi-fied as semantically distant because ?networking?
inWordNet is solely defined as socializing with peo-ple, not as a computer network.
The stemmed word?network?
is semantically closer to ?data communi-cation.
?Algorithm 1 shows how to determine the semanticdistance between two words w1 and w2.The SDBW module uses WordNet as a lexicalknowledge base to determine the semantic close-ness between words.
The path lengths and depthsin the WordNet IS-A hierarchy may be used to mea-sure how strongly a word contributes to the meaningof a sentence.
However, this approach has a prob-lem.
Because WordNet is a manually created lex-ical resource, it does not cover all the words thatappear in a sentence, even though some of thesewords are commonly seen in literature.
Words notdefined in WordNet are misclassified as semanti-7We use the part-of-speech tags from the Penn Treebankproject: http://www.ling.upenn.edu/courses/Fall 2003/ling001/penn treebank pos.html.Algorithm 1 Semantic Distance Between Words1: If two words w1 and w2 have different POS,consider them semantically distant.
Return 0.2: If w1 and w2 have the same POS and look thesame but do not exist inWordNet, consider themsemantically close.
Return 1.3: Using either maximum scores or the first senseheuristic to perform WSD, measure the seman-tic distance between w1 and w2 using Equation3.4: Using the same WSD strategy as the previousstep, measure the semantic distance between thestemmed w1 and the stemmed w2 using Equa-tion 3.5: Return the larger of the two results in steps (3)and (4), i.e., the score of the pair that is seman-tically closer.cally distant when compared with any other words.This is a huge problem for identifying equivalentcourses.
For example, course names ?propositionallogic?
and ?logic?
are differentiated solely by theword ?propositional,?
which is not defined in Word-Net8.
The semantic distance measurement betweensentences therefore cannot be simplified to all pair-wise comparisons of words using WordNet.
A cor-pus must be introduced to assess the semantic relat-edness of words in sentences.3.2 Semantic Distance Between SentencesTo measure the semantic distance between sen-tences, Li et al (2006) join two sentences S1 andS2 into a unique word set S, with a length of n:S = S1 ?
S2 = {w1, w2, .
.
.
wn}.
(4)A semantic vector SV1 is computed for sentence S1and another semantic vector SV2 for sentence S2.Given the number of words in S1 as t, Li et al (2006)define the value of an entry of SV1 for sentence S1as:SV1i = s?1i ?
I(wi) ?
I(w1j), (5)where i ?
[1, n], j ?
[1, t], s?1i is an entry of thelexical semantic vector s?1 derived from S1, wi is aword in S, and w1j is semantically the closest to wi8WordNet 3.0 is used in our implementation and experi-ments.145in S1.
I(wi) is the information content (IC) of wi inthe Brown corpus and I(w1j) is the IC of w1j in thesame corpus.Our work redefines the semantic vector as:SV1i = s?1i?
(TFIDF (wi)+)?
(TFIDF (w1j)+).
(6)There are two major modifications in our ver-sion.
First, we replace the information content withthe Term Frequency?Inverse Document Frequency(TFIDF) weighting scheme, which is a bag-of-wordsmodel (Joachims, 1997).
In the TFIDF formula,each term i in document D is assigned weight mi:mi = tfi ?
idfi = tfi ?
logNdfi, (7)where tfi is the frequency of term i in D, idfi is theinverse document frequency of term i, N is the totalnumber of documents, and dfi is the number of doc-uments that contain i (Salton and Buckley, 1987).Our approach uses a smoothing factor  to add asmall mass9 to the TFIDF.Second, we compute TFIDF over our customcourse description corpus instead of the Brown cor-pus.
The course description corpus is built fromcrawling the course catalogs from two universities?websites.
These two modifications find inner rela-tions of words from the course description data do-main, rather than from the various domains providedby the Brown corpus.The semantic distance of S1 and S2 is the co-sine coefficient of their semantic vectors SV1 andSV2 (Li et al, 2006):fsent(S1, S2) =SV1 ?
SV2||SV1|| ?
||SV2||.
(8)Although Li et al (2006) do not remove stopwords10, it is found that the removal of stop wordsremarkably improves accuracy to identify equivalentcourses.
(See Section 4.
)While building and deriving the lexical semanticvectors s?1 for sentence S1 and s?2 for sentence S2,9In our experiments, =0.01.10Stop words (such as ?the?, ?a?, and ?of?)
are words thatappear in almost every document, and have no discriminationvalue for contexts of documents.
Porter et al?s English stopwords list (http://snowball.tartarus.org/algorithms/english/stop.txt) are adapted in this work.it is found that some words from the joint word listS (Equation 4) which are not stop words, but arevery generic, in turn rank as semantically the clos-est words to most other words.
These generic wordscannot be simply regarded as domain-specific stopwords in that a generic word in a pair of courses maynot be generic in another pair.
To discourage thesegeneric words, we introduce a ticketing algorithm aspart of the process to build a lexical semantic vec-tor.
Algorithm 2 shows the steps to build the lexicalsemantic vector11 s?1 for sentence S1.
Similarly, wefollow these steps to build s?2 for S2.Algorithm 2 Lexical Semantic Vector s?1 for S11: for all words wi ?
S do2: if wi ?
S1, set s?1i = 1 where s?1i ?
s?1.3: if wi /?
S1, the semantic distance between wiand each word w1j ?
S1 is calculated (Sec-tion 3.1).
Set s?1i to the highest score if thescore exceeds a preset threshold ?
(?
?
[0, 1]),otherwise s?1i = 0.4: Let ?
?
[1, n] be the maximum number oftimes a word w1j ?
S1 is chosen as semanti-cally the closest word of wi.
Let the seman-tic distance of wi and w1j be d, and f1j bethe number of times that w1j is chosen.
Iff1j > ?, set s?1i = d/?
to give a penalty tow1j .
This step is called ticketing.5: end for3.3 Semantic Distance Between ParagraphsAlthough Li et al (2006) claim that their approachis for measuring the semantic similarity of sentencesand short texts, test cases show that the accuracy oftheir approach is not satisfactory on course descrip-tions.
We introduce the semantic distance measurebetween paragraphs to address this problem.Given course descriptions P1 and P2, the firststep is to remove generic data and prerequisite in-formation.
Let P1 be a paragraph consisting of aset of n sentences, and P2 be a paragraph of m sen-tences, where n and m are positive integers.
For s1i(s1i ?
P1, i ?
[1, n]) and s2j (s2j ?
P2, j ?
[1,m]),the semantic distance between paragraphs P1 and P2is defined as a weighted mean:11In our experiments, we chose ?=0.2.146fpara(P1, P2) =?ni=1(maxmj=1 fsent(s1i, s2j)) ?
Ni?ni=1 Ni,(9)where Ni is the sum of the number of words insentences s1i (s1i ?
P1) and s2j (s2j ?
P2), andfsent(s1i, s2j) is the semantic distance between sen-tences s1i and s2j (Section 3.2).
Algorithm 3 sum-marizes these steps.
Optionally the deletion flag canbe enabled to speed up the computation.
Empiricalresults show that accuracy is about the same whetheror not the deletion flag is enabled.Algorithm 3 Semantic Distance for Paragraphs1: If deletion is enabled, given two course descrip-tions, select the one with fewer sentences as P1,and the other as P2.
If deletion is disabled, se-lect the first course description as P1, and theother as P2.2: for each sentence s1i ?
P1 do3: Calculate the semantic distance between sen-tences (Section 3.2) for s1i and each of thesentences in P2.4: Find the sentence pair ?s1i, s2j?
(s2j ?
P2)that scores the highest.
Save the highest scoreand the total number of words of s1i and s2j .If deletion is enabled, remove sentence s2jfrom P2.5: end for6: Collect the highest score and the number ofwords from each run.
Use their weighted mean(Equation 9) as the semantic distance betweenP1 and P2.We introduce ?
to denote how much we weighcourse titles over course descriptions.
Course titlesare compared using the semantic distance measure-ment discussed in Section 3.2.
Given title T1 and de-scription P1 of course C1, and title T2 and descrip-tion P2 of course C2, the semantic distance of thetwo courses is defined as:fcourse(C1, C2) = ?
?
fsent(T1, T2)+ (1 ?
?)
?
fpara(P1, P2).
(10)4 Implementation and ExperimentalResultsThe method proposed in this paper is fully imple-mented using Python and NLTK (Bird et al, 2009).The WordNet interface built into NLTK is used toretrieve lexical information for word similarities.
Inour experiments, the default parameters are: ?
=?0.2, ?
= 0.45 (Li et al, 2006), ?
= 2, and?
= 0.7.
The ?
and ?
values are found empiricallyto perform well.A course description corpus must be built for theexperiments.
The UMass Lowell (UML) coursetransfer dictionary lists courses that are equivalentto those from hundreds of other institutions (see Fig-ure 1, shown in Section 1).
We only used the transferdictionary as a test corpus rather than a training cor-pus to keep the algorithm simple and efficient.
Mid-dlesex Community College (MCC) is picked as anexternal institution in our experiments.
The transferdictionary lists over 1,400 MCC courses in differ-ent majors.
We remove the rejected courses, elec-tive courses, and those with missing fields fromthe transfer dictionary.
Referring to the equiva-lencies from the transfer dictionary, we crawl over1,500 web pages from the course catalogs of bothUML and MCC to retrieve over 200 interconnectedcourses that contain both course names and descrip-tions.
Two XML files are created, one for UMLand one for MCC courses.
Given an MCC course,the goal is to suggest the most similar UML course.A fragment of the MCC XML file is shown below.Each course entry has features such as course ID,course name, credits, description, and the ID of itsequivalent course at UML.
The UML XML file hasthe same layout except that the equivalence tag isremoved and the root tag is uml.<mcc><course><courseid>ART 113</courseid><coursename>Color and Design</coursename><credits>3</credits><description>Basic concepts of compositionand color theory.
Stresses the process andconceptual development of ideas in twodimensions and the development of a strongsensitivity to color.</description><equivalence>70.101</equivalence></course>...</mcc>147After the integrity check, the MCC XML file con-tains 108 courses and the UML XML file contains89 courses.
The reason there are more MCC coursesthan UML courses is that the transfer dictionary al-lows multiple courses from MCC to be transferredto the same UML course.To monitor the accuracy change over differentnumbers of documents, we randomly select equiva-lent courses to create two smaller data sets for UMLand MCC respectively in the XML format.
The ran-dom number of courses in each XML file is shownin Table 1.
These three pairs of XML data sets areused both as the corpora and as the test data sets.XML Datasets MCC Courses UML Courses TotalSmall 25 24 49Medium 55 50 105Large 108 89 197Table 1.
Number of courses in the data setsConsider the small data set as an illustration.
Eachof the 25 MCC courses is compared with all 24UML courses.
All words are converted to low-ercase and punctuation is removed.
We also re-move both general stop words12 (such as ?a?
and?of?)
and domain-specific stop words13 (such as?courses,?
?students,?
and ?reading?).
We do notremove words based on high or low occurrence be-cause that is found empirically to decrease accuracy.Using the algorithms discussed in Section 3, a scoreis computed for each comparison.
After comparingan MCC course to all UML courses, the 24 UMLcourses are sorted by score in descending order.
Thecourse equivalencies indicated by the transfer dic-tionary are used as the benchmark.
In each run wemark the rank of the real UML course that is equiv-alent to the given MCC course as indicated by thetransfer dictionary.
We consider the result of eachrun correct when the equivalent course indicated bythe transfer dictionary is in the top 3 of the sortedlist.
After doing this for all the 25 MCC courses, wecalculate the overall accuracy and the average ranksof the real equivalent courses.Empirical results show that accuracy drops whensome inseparable phrases naming atomic keywords12A list of English stop words in NLTK is used in our exper-iments.13A list of domain-specific stop words is created manually.
(such as ?local area networks,?
?data communica-tions,?
and ?I/O?)
are tokenized.
To address thisproblem, a list of 40 atomic keywords is constructedmanually.Our approach is compared against two baselines:TFIDF only (Equation 7), and the method by Li et al(2006).
Since the method by Li et al (2006) does notmeasure semantic distance between paragraphs, weconsider each course description as a sentence.
Fig-ure 4 shows that the accuracy of our approach out-performs the TFIDF and Li et al (2006) approachesover the three sets of documents from Table 1.
It isinteresting to note that while the accuracies of theTFIDF and Li et al (2006) approaches decrease asthe number of documents increases, the accuracy ofour approach increases when the number of docu-ments increases from 105 to 195.
This observation iscounter-intuitive and therefore requires further anal-ysis in future work.49 105 197Number of documents2030405060708090100AccuracyBest caseAccuracy ComparisonOur approachTFIDFLiFigure 4.
Accuracy of our approach compared to theTFIDF and Li et al (2006) approaches.For each of the three different approaches, wenote the average ranks of the real equivalent coursesindicated by the transfer dictionary.
Figure 5 showsthat our approach outperforms the TFIDF and Li etal.
(2006) approaches.
It also shows that the averagerank in our approach does not increase as fast as theother two.The word order similarity module in the Li et al(2006) approach tokenizes two sentences into a listof unique words.
Each of the two sentences is con-verted into a numbered list where each entry in thelist is the index of the corresponding word in thejoint set.
The word order similarity between these14849 105 197Number of documents05101520AveragerankBest caseAverage ranks of the real equivalent coursesOur approachTFIDFLiFigure 5.
Average ranks of the real equivalentcourses.two sentences is in turn the normalized differenceof their word orders.
We experiment with enablingand disabling word order similarity to compare ac-curacy (Figure 6) and speed.
Empirical results showthat disabling word order similarity increases the ac-curacy of our approach and the speed is over 20%faster.
Therefore, the word order similarity moduleby Li et al (2006) is removed from our approach.We then compare the two WSD strategies as de-scribed in Section 3.1: (1) always select the maxi-mum score on all senses of two words (Max), and(2) apply the first sense heuristic.
As Figure 7 andFigure 8 suggest, the accuracy of Max is higher thanthe first sense heuristic, but the average rank of thefirst sense heuristic is better than Max.
Therefore,the overall performance of the two strategies is aboutthe same.We also experiment with enabling and disablingticketing (Section 3.2).
Results show that both accu-racy and average ranks are improved when ticketingis enabled.5 Future RefinementsThis paper presents a novel application of seman-tic distance to suggesting potential equivalencies fora course transferred from an external university.
Itproposes a hybrid method that incorporates seman-tic distance measurement for words, sentences, andparagraphs.
We show that a composite weightingscheme based on a lexicographic resource and a bag-of-words model outperforms previous work to iden-49 105 197Number of documents707580859095100AccuracyBest caseAccuracy comparison when word order similarity is disabled or enabledWord order similarity disabledWord order similarity enabledFigure 6.
The accuracy of our approach when en-abling or disabling word order similarity.49 105 197Number of documents707580859095100AccuracyBest caseAccuracy comparison under two WSD strategiesMaxFirst Sense HeuristicFigure 7.
Accuracy comparison under two WSDstrategies.49 105 197Number of documents012345678AveragerankBest caseAverage ranks of the real equivalent courses under two WSD strategiesMaxFirst Sense HeuristicFigure 8.
Average ranks of the real equivalentcourses under two WSD strategies.149tify equivalent courses.
In practice, it is not com-mon for two sentences in the course description cor-pus to have the exact same word order.
Therefore,word order similarity is not very useful for identi-fying course equivalencies.
Empirical results sug-gest that WSD and POS are helpful to increase ac-curacy, and that it is necessary to remove general anddomain-specific stop words.
The ticketing algorithm(Algorithm 2) also improves accuracy.UML?s transfer dictionary is only used as a testcorpus in this paper.
Alternatively, a set of ex-amples might be constructed from the transfer dic-tionary to automatically learn equivalent propertieswithout compromising the time complexity.
Ana-lyzing transfer dictionaries from other universitiesmight help as well.Meta data such as course levels, textbooks, andprerequisites can also be used as indicators of courseequivalencies, but unfortunately these data are notavailable in the resources we used.
Obtaining thesedata would require a great deal of manual work,which runs counter to our goal of devising a simpleand straightforward algorithm for suggesting courseequivalencies with a reasonable time complexity.WordNet is selected as the lexical knowledgebase for determining the semantic closeness be-tween words, but empirical results indicate thatWordNet does not cover all the concepts that exist incourse descriptions.
To address this issue, a domain-specific ontology could be constructed.We plan to test our approach against other seman-tic distance measures in addition to the approach byLi et al (2006), such as the work by Mihalcea et al(2006) and Islam and Inkpen (2007).Other directions for future work include: (1) opti-mizing performance and the exploration of more el-egant WSD algorithms, (2) testing the sensitivity ofresults to values of ?
and ?, (3) testing courses froma larger number of universities, (4) proposing robustmethodologies that tolerate poorly formed texts, (5)adding more data to the course description corpus,and (6) making the course description corpus pub-licly available to the research community.AcknowledgmentsThe authors thank Dr. Karen M. Daniels for review-ing drafts of this paper.
We also appreciate corre-spondence with Dr. Yuhua Li at the early stage ofour work.
Last, but not least, we thank the reviewersfor their insightful comments that guided improve-ment of the contents of this paper.ReferencesSteven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia, Inc. Sebastopol, CA, USAAlexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based Measures of Lexical SemanticRelatedness.
Computational Linguistics, volume 32.Rudi L. Cilibrasi and Paul M. B. Vitanyi.
2007.
TheGoogle Similarity Distance.
IEEE Transactions onKnowledge and Data Engineering.
19(3):370 ?
383Allan M. Collins and M. Ross Quillian.
1969.
Re-trieval Time from Semantic Memory.
Journal of Ver-bal Learning and Verbal Behavior, volume 8.James R. Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. Thesis.
University of Edinburgh,Edinburgh, U.K.Graeme Hirst and David St-Onge.
1998.
Lexical chainsas representations of context for the detection and cor-rection of malapropisms.
In Christiane Fellbaum, ed-itor, WordNet: An Electronic Lexical Database.
TheMIT Press, Cambridge, MA, pages 305?332.Aminul Islam and Diana Inkpen.
2006.
Second Or-der Co-occurrence PMI for Determining the Seman-tic Similarity of Words.
Proceedings of the Interna-tional Conference on Language Resources and Evalu-ation (LREC 2006), Genoa, Italy, pages 1033?1038.Aminul Islam and Diana Inkpen.
2007.
Semantic Simi-larity of Short Texts.
Proceedings of the InternationalConference on Recent Advances in Natural LanguageProcessing (RANLP), Bulgaria, September 2007.Jay J. Jiang and David W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxonomy.Proceedings of International Conference on Researchin Computational Linguistics (ROCLING X), Taiwan,pages 19?33.Thorsten Joachims.
1997.
A Probabilistic Analysis ofthe Rocchio Algorithm with TFIDF for Text Catego-rization.
Proceedings of International Conference onMachine Learning (ICML).Claudia Leacock and Martin Chodorow.
1998.
UsingCorpus Statistics and WordNet Relations for SenseIdentification.
Computational Linguistics, 24(1):147?165.Yuhua Li, Zuhair A. Bandar, and David McLean.
2003.An Approach for Measuring Semantic Similarity be-tween Words Using Multiple Information Sources.150IEEE Transactions on Knowledge and Data Engineer-ing, volume 15, pages 871?882.
IEEE Computer So-ciety.Yuhua Li, David McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence Simi-larity Based on Semantic Nets and Corpus Statistics.IEEE Transactions on Knowledge and Data Engineer-ing volume 18.
IEEE Computer Society.
Los Alami-tos, CA, USA.Dekang Lin.
1998a.
Extracting Collocations from TextCorpora.
Workshop on Computational Terminology,Montreal, Canada.Yutaka Matsuo, Junichiro Mori, Masahiro Hamasaki,Takuichi Nishimura, Hideaki Takeda, Koiti Hasida,and Mitsuru Ishizuka.
2007.
POLYPHONET: AnAdvanced Social Network Extraction System from theWeb.
Web Semantics, volume 5(4).
Elsevier SciencePublishers B.V., Amsterdam, The Netherlands.Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and Knowledge-based Mea-sures of Text Semantic Similarity.
Proceedings of theAmerican Association for Artificial Intelligence (AAAI2006), Boston.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Using Automatically Acquired Pre-dominant Senses for Word Sense Disambiguation.
InProceedings of the ACL SENSEVAL-3 Workshop.
Bar-clona, Spain.
pp 151-154.Saif Mohammad.
2008.
Measuring Semantic DistanceUsing Distributional Profiles of Concepts.
Ph.D. The-sis.
University of Toronto, Toronto, Canada.Saif Mohammad and Graeme Hirst.
2006.
DeterminingWord Sense Dominance Using a Thesaurus.
In Pro-ceedings of the 11th conference of the European chap-ter of the Association for Computational Linguistics(EACL-2006), April 2006, Trento, Italy.Roberto Navigli.
2009.
Word Sense Disambiguation: ASurvey.
ACM Computing Surveys, 41(2):1?69.Roy Rada, Hafedh Mili, Ellen Bicknell, and Maria Blet-tner.
1989.
Development and Application of a Met-ric on Semantic Nets.
IEEE Transactions on Systems,Man, and Cybernetics, volume 19.Philip Resnik.
1995.
Using Information Content to Eval-uate Semantic Similarity in a Taxonomy.
In Proceed-ings of the 14th International Joint Conference on Ar-tificial Intelligence (IJCAI ?95), volume 1.
MorganKaufmann Publishers Inc., San Francisco, CA, USA.Mehran Sahami and Timothy D. Heilman.
2006.
A Web-based Kernel Function for Measuring the Similarity ofShort Text Snippets.
In Proceedings of the 15th Inter-national Conference on World Wide Web (WWW ?06).ACM.
New York, NY, USA.Gerard Salton and Chris Buckley.
1987.
Term WeightingApproaches in Automatic Text Retrieval.
Technicalreport.
Ithaca, NY, USA.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques, 2nd Edi-tion.
Morgan Kaufmann, San Francisco, CA, USA,pages 161?171.Zhibiao Wu and Martha Palmer.
1994.
Verb SemanticsAnd Lexical Selection.
In Proceedings of the 32nd an-nual meeting on Association for Computational Lin-guistics (ACL ?94).
Association for ComputationalLinguistics, Stroudsburg, PA, USA.Dongqiang Yang and David M. W. Powers.
2005.
Mea-suring semantic similarity in the taxonomy of Word-Net.
In Proceedings of the 28th Australasian confer-ence on Computer Science (ACSC ?05), volume 38.Australian Computer Society, Darlinghurst, Australia.151
