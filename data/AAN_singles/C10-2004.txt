Coling 2010: Poster Volume, pages 27?35,Beijing, August 2010Going Beyond Traditional QA Systems: Challenges and Keysin Opinion Question AnsweringAlexandra BalahurDept.
of Software and Computing SystemsUniversity of Alicanteabalahur@dlsi.ua.esEster BoldriniDept.
of Software and Computing SystemsUniversity of Alicanteeboldrini@dlsi.ua.esAndr?s MontoyoDept.
of Software and Computing SystemsUniversity of Alicantemontoyo@dlsi.ua.esPatricio Mart?nez-BarcoDept.
of Software and Computing SystemsUniversity of Alicantepatricio@dlsi.ua.esAbstractThe treatment of factual data has beenwidely studied in different areas of Nat-ural Language Processing (NLP).
How-ever, processing subjective informationstill poses important challenges.
Thispaper presents research aimed at assess-ing techniques that have been suggestedas appropriate in the context of subjec-tive - Opinion Question Answering(OQA).
We evaluate the performance ofan OQA with these new componentsand propose methods to optimally tacklethe issues encountered.
We assess theimpact of including additional resourcesand processes with the purpose of im-proving the system performance on twodistinct blog datasets.
The improve-ments obtained for the different combi-nation of tools are statistically signifi-cant.
We thus conclude that the pro-posed approach is adequate for the OQAtask, offering a good strategy to dealwith opinionated questions.1 IntroductionThe State of the Blogosphere 2009 survey pub-lished by Technorati 1 concludes that in the pastyears the blogosphere has gained a high influ-ence on a high variety of topics, ranging fromcooking and gardening, to economics, politicsand scientific achievements.
The development1 http://technorati.com/of the Social Web and the new communicationframeworks also influenced the way informa-tion is transmitted through communities.
Blogsare part of the so-called new textual genres.They have distinctive features when comparedto the traditional ones, such as newspaper ar-ticles.
Blog language contains formal and in-formal expressions, and other elements, as re-peated punctuation or emoticons (used to stressupon different text elements).
With the growthin the content of the blogosphere, the quantityof subjective data of the Web is increasing ex-ponentially (Cui et al, 2006).
As it is being up-dated in real-time, this data becomes a source oftimely information on many topics, exploitableby different applications.
In order to properlymanage the content of this subjective informa-tion, its processing must be automated.
TheNLP task, which deals with the classification ofopinionated content is called Sentiment Analy-sis (SA).
Research in this field aims at discover-ing appropriate mechanisms to properly re-trieve, extract and classify opinions expressed intext.
While techniques to retrieve objective in-formation have been widely studied, imple-mented and evaluated, opinion-related tasks stillrepresent an important challenge.
As a conse-quence, the aim of our research is to study, im-plement and evaluate appropriate methods forthe task of Question Answering (QA) in theopinion treatment framework.2 Motivation and ContributionResearch in opinion-related tasks gained impor-tance in the past years.
However, there are stillmany aspects that require analysis and im-27provement, especially for approaches that com-bine SA with other NLP tasks such as QA orautomatic summarization.
The TAC 2008 Opi-nion Pilot task and the subsequent research per-formed on the competition data have demon-strated that answering opinionated questionsand summarizing subjective information aresignificantly different from the equivalent tasksin the same context, but dealing with factualdata.
This finding was confirmed by the recentwork by (Kabadjov et al, 2009).
The first moti-vation of our work is the need to detect and ex-plore the challenges raised by opinion QA(OQA), as compared to factual QA.
To this aim,we analyze the improvements that can bebrought at the different steps of the OQAprocess: question treatment (identification ofexpected polarity ?
EPT, expected source ?
ESand expected target ?ET-), opinion retrieval (atthe level of one and three-sentences long snip-pets, using topic-related words or using paraph-rases), opinion analysis (using topic detectionand anaphora resolution).
This preliminary re-search is motivated by the conclusions drawn byprevious studies (Balahur et al, 2009).
Our pur-pose is to verify if the inclusion of new ele-ments and methods - source and target detection(using semantic role labeling (SRL)), topic de-tection (using Latent Semantic Analysis), pa-raphrasing and joint topic-sentiment analysis(classification of the opinion expressed only insentences related to the topic), followed by ana-phora resolution (using a system whose perfor-mance is not optimal), affects the results of thesystem and how.
Our contribution to this respectis the identification of the challenges related toOQA compared to traditional QA.
A furthercontribution consists in adding the appropriatemethods, tools and resources to resolve theidentified challenges.
With the purpose of test-ing the effect of each tool, resource and tech-nique, we carry out a separate and a globalevaluation.
An additional motivation of ourwork is the fact that although previous ap-proaches showed that opinion questions havelonger answers than factual ones, the researchdone in OQA so far has only considered a sen-tence-level approach.
Another contribution thispaper brings is the retrieval at 1 and 3-sentencelevel and the retrieval based on similarity toquery paraphrases enriched with topic-relatedwords).
We believe retrieving longer text couldcause additional problems such as redundancy,coreference and temporal expressions or theneed to apply contextual information.
Paraph-rasing, on the other hand, had account for lan-guage variability in a more robust manner;however, the paraphrase collections that areavailable at the moment are known to be noisy.The following sections are structured as fol-lows: Section 3 presents the related work in thefield and the competitions organized for systemstackling the OQA task.
In Section 4 we describethe corpora used for the experiments we carriedout and the set of questions asked over each ofthem.
Section 5 presents the experimental set-tings and the different system configurations weassessed.
Section 6 shows the results of theevaluations, discusses the improvements anddrops in performance using different configura-tions.
We finally conclude on our approaches inSection 7, proposing the lines for future work.3 Related WorkQA can be defined as the task in which given aset of questions and a collection of documents,an automatic NLP system is employed to re-trieve the answer to the queries in Natural Lan-guage (NL).
Research focused on building fac-toid QA systems has a long tradition; however,it is only recently that researchers have startedto focus on the development of OQA systems.
(Stoyanov et al, 2005) and (Pustejovsky andWiebe, 2006) studied the peculiarities of opi-nion questions.
(Cardie et al, 2003) employedopinion summarization to support a Multi-Perspective QA system, aiming at identifyingthe opinion-oriented answers for a given set ofquestions.
(Yu and Hatzivassiloglou, 2003) se-parated opinions from facts and summarizedthem as answer to opinion questions.
(Kim andHovy, 2005) identified opinion holders, whichare a key component in retrieving the correctanswers to opinion questions.
Due to the rea-lized importance of blog data, recent years havealso marked the beginning of NLP research fo-cused on the development of opinion QA sys-tems and the organization of international con-ferences encouraging the creation of effectiveQA systems both for fact and subjective texts.The TAC 20082 QA track proposed a collection2 http://www.nist.gov/tac/28of factoid and opinion queries called ?rigid list?
(factoid) and ?squishy list?
(opinion) respective-ly, to which the traditional QA systems had tobe adapted.
Some participating systems treatedopinionated questions as ?other?
and thus theydid not employ opinion specific methods.
How-ever, systems that performed better in the?squishy list?
questions than in the ?rigid list?implemented additional components to classifythe polarity of the question and of the extractedanswer snippet.
The Alyssa system (Shen et al2007) uses a Support Vector Machines (SVM)classifier trained on the MPQA corpus (Wiebeet al, 2005), English NTCIR3 data and rulesbased on the subjectivity lexicon (Wilson et al,2005).
(Varma et al, 2008) performed queryanalysis to detect the polarity of the questionusing defined rules.
Furthermore, they filteropinion from fact retrieved snippets using aclassifier based on Na?ve Bayes with unigramfeatures, assigning for each sentence a score thatis a linear combination between the opinion andthe polarity scores.
The PolyU (Venjie et al,2008) system determines the sentiment orienta-tion of the sentence using the Kullback-Leiblerdivergence measure with the two estimated lan-guage models for the positive versus negativecategories.
The QUANTA (Li et al, 2008) sys-tem performs opinion question sentiment analy-sis by detecting the opinion holder, the objectand the polarity of the opinion.
It uses a seman-tic labeler based on PropBank 4  and manuallydefined patterns.
Regarding the sentiment clas-sification, they extract and classify the opinionwords.
Finally, for the answer retrieval, theyscore the retrieved snippets depending on thepresence of topic and opinion words and onlychoose as answer the top ranking results.
Otherrelated work concerns opinion holder and targetdetection.
NTCIR 7 and 8 organized MOAT(the Multilingual Opinion Analysis Task), inwhich most participants employed machinelearning approaches using syntactic patternslearned on the MPQA corpus (Wiebe et al,2005).
Starting from the abovementioned re-search, our aim is to take a step forward topresent approaches and employ opinion specificmethods focused on improving the performanceof our OQA.
We perform the retrieval at 1 sen-3 http://research.nii.ac.jp/ntcir/4http://verbs.colorado.edu/~mpalmer/projects/ace.htmltence and 3 sentence-level and also determinethe expected source (ES) and the expected tar-get (ET) of the questions, which are fundamen-tal to properly retrieve the correct answer.
Thesetwo elements are selected employing semanticroles (SR).
The expected answer type (EAT) isdetermined using Machine Learning (ML) usingSupport Vector Machine (SVM), by taking intoaccount the interrogation formula, the subjectiv-ity of the verb and the presence of polaritywords in the target SR.
In the case of expectedopinionated answers, we also compute the ex-pected polarity type (EPT) ?
by applying opi-nion mining (OM) on the affirmative version ofthe question (e.g.
for the question ?Why dopeople prefer Starbucks to Dunkin Donuts?
?,the affirmative version is ?People prefer Star-bucks to Dunkin Donuts because X?).
Theseexperiments are presented in more detail inSection 5.4 CorporaIn order to carry out the present research fordetecting and solving the complexities of opi-nion QA, we employed two corpora of blogposts: EmotiBlog (Boldrini et al, 2009a) andthe TAC 2008 Opinion Pilot test collection (partof the Blog06 corpus).The TAC 2008 Opinion Pilot test collection iscomposed by documents with the answers to theopinion questions given on 25 targets.
EmotiB-log is a collection of blog posts in English ex-tracted form the Web.
As a consequence, itrepresents a genuine example of this textual ge-nre.
It consists in a monothematic corpus aboutthe Kyoto Protocol, annotated with the im-proved version of EmotiBlog (Boldrini et al,2009b).
It is well know that Opinion Mining(OM) is a very complex task due to the highvariability of the language employed.
Thus, ourobjective is to build an annotation model that isable to capture the whole range of phenomenaspecific to subjectivity expression.
Additionalcriteria employed when choosing the elementsto be annotated were effectiveness and noiseminimization.
Thus, from the first version of themodel, the elements which did not prove to bestatistically relevant have been eliminated.
Theelements that compose the improved version ofthe annotation model are presented in Table 1.29Elements DescriptionObj.
speech Confidence, comment, source, target.Subj.
speech Confidence, comment, level, emotion,phenomenon, polarity, source andtarget.Adjec-tives/AdverbsConfidence, comment, level, emotion,phenomenon, modifier/not, polarity,source and target.Verbs/ Names Confidence, comment, level, emotion,phenomenon, polarity, mode, sourceand target.Anaphora Confidence, comment, type, source andtarget.Capital letter/PunctuationConfidence, comment, level, emotion,phenomenon, polarity, source andtarget.Phenomenon Confidence, comment, type, colloca-tion, saying, slang, title, and rhetoric.Reader/AuthorInterpr.
(obj.
)Confidence, comment, level, emotion,phenomenon, polarity, source andtarget.Emotions Confidence, comment, accept, anger,anticipation, anxiety, appreciation, bad,bewilderment, comfort, compassion?Table 1: EmotiBlog improved structureThe first distinction consists in separating objec-tive and subjective speech.
Subsequently, a fin-er-grained annotation is employed for each ofthe two types of data.
Objective sentences areannotated with source and target (when neces-sary, also the level of confidence of the annota-tor and a comment).
Subjective elements can beannotated at a sentence level, but they also haveto be labeled at a word and/or phrase level.EmotiBlog also contains annotations of anapho-ra at a cross-document level (to interpret thestoryline of the posts) and the sentence type(simple sentence or title, but also saying or col-location).
Finally, the Reader and the Writerinterpretation have to be marked in objectivesentences.
These elements are employed tomark and interpret correctly an apparent objec-tive discourse, whose aim is to implicitly ex-press an opinion (e.g.
?The camera broke in twodays?).
The first is useful to extract what is theinterpretation of the reader (for example if thewriter says The result of their governing was anincrease of 3.4% in the unemployment rate in-stead of The result of their governing was a dis-aster for the unemployment rate) and the secondto understand the background of the reader (i.e..These criminals are not able to govern insteadof saying the x party is not able to govern).From this sentence, for example, the reader candeduce the political ideas of the writer.
Thequestions whose answers are annotated withEmotiBlog are the subset of opinion questions inEnglish presented in (Balahur et al, 2009).
Thecomplete list of questions is shown in Table 2.N Question2 What motivates people?s negative opinions on theKyoto Protocol?5 What are the reasons for the success of the KyotoProtocol?6 What arguments do people bring for their criticismof media as far as the Kyoto Protocol is concerned?7 Why do people criticize Richard Branson?11 What negative opinions do people have on HilaryBenn?12 Why do Americans praise Al Gore?s attitude towardsthe Kyoto protocol?15 What alternative environmental friendly resourcesdo people suggest to use instead of gas en the future?16 Is Arnold Schwarzenegger pro or against the reduc-tion of CO2 emissions?18 What improvements are proposed to the Kyoto Pro-tocol?19 What is Bush accused of as far as political measuresare concerned?20 What initiative of an international body is thought tobe a good continuation for the Kyoto Protocol?Table 2: Questions over the EmotiBlogcorpusThe main difference between the two corporaemployed is that Emotiblog is monothematic,containing only posts about the Kyoto Protocol,while the TAC 2008 corpus contains documentson a multitude of subjects.
Therefore, differenttechniques must be adjusted in order to treateach of them.5 Experiments5.1 Question AnalysisIn order to be able to extract the correct answerto opinion questions, different elements must beconsidered.
As stated in (Balahur et al, 2009)we need to determine both the expected answertype (EAT) of the question ?
as in the case offactoid ones - as well as new elements ?
such asexpected polarity type (EPT).
However, opi-nions are directional ?
i.e., they suppose the ex-istence of a source and a target to which theyare addressed.
Thus, we introduce two newelements in the question analysis ?
expectedsource (ES) and expected target (ET).
Thesetwo elements are selected by applying SR andchoosing the source as the agent in the sentenceand the direct object (patient) as the target of theopinion.
Of course, the source and target of the30opinions expressed can also be found in otherroles, but at this stage we only consider thesecases.
The expected answer type (EAT) (e.g.opinion or other) is determined using MachineLearning (ML) using Support Vector Machine(SVM), by taking into account the interrogationformula, the subjectivity of the verb and thepresence of polarity words in the target SR. Inthe case of expected opinionated answers, wealso compute the expected polarity type (EPT) ?by applying OM on the affirmative version ofthe question.
An example of such a transforma-tion is: given the question ?What are the rea-sons for the success of the Kyoto Protocol?
?,the affirmative version of the question is ?Thereasons for the success of the Kyoto Protocolare X?.5.2 Candidate Snippet RetrievalIn the answer retrieval stage, we employ fourstrategies:1.
Using the JIRS (JAVA Information Re-trieval System) IR engine (G?mez et al,2007) to find relevant snippets.
JIRS re-trieves passages (of the desired length),based on searching the question struc-tures (n-grams) instead of the keywords,and comparing them.2.
Using the ?Yahoo?
search engine to re-trieve the first 20 documents that aremost related to the query.
Subsequently,we apply LSA on the retrieved docu-ments and extract the words that aremost related to the topic.
Finally, weexpand the query using words that arevery similar to the topic and retrievesnippets that contain at least one ofthem and the ET.3.
Generating equivalent expressions forthe query, using the DIRT paraphrasecollection (Lin and Pantel, 2001) andretrieving candidate snippets of length 1and 3 (length refers to the number ofsentences retrieved) that are similar toeach of the new generated queries andcontain the ET.
Similarity is computedusing the cosine measure.
Examples ofalternative queries for ?People likeGeorge Clooney?
are ?People adoreGeorge Clooney?, ?People enjoyGeorge Clooney?, ?People preferGeorge Clooney?.4.
Enriching the equivalent expressions forthe query in 3. with the topic-relatedwords discovered in 2. using LSA.5.3 Polarity and topic-polarity classifica-tion of snippetsIn order to determine the correct answers fromthe collection of retrieved snippets, we mustfilter for the next processing stage only the can-didates that have the same polarity as the ques-tion EPT.
For polarity detection, we use a com-bined system employing SVM ML on unigramand bigram features trained on the NTCIRMOAT 7 data and an unsupervised lexicon-based system.
In order to compute the featuresfor each of the unigrams and bigrams, we com-pute the tf-idf scores.The unsupervised system uses the OpinionFinder lexicon to filter out subjective sentences?
that contain more than two subjective wordsor a subjective word and a valence shifter (ob-tained from the General Inquirer resource).
Sub-sequently, it accounts for the presence of opi-nionated words from four different lexicons ?MicroWordNet (Cerini et al, 2007), WordNetAffect (Strapparava and Valitutti, 2004) Emo-tion Triggers (Balahur and Montoyo, 2008) andGeneral Inquirer (Stone et al, 1966).
For thejoint topic-polarity analysis, we first employLSA to determine the words that are stronglyassociated to the topic, as described in Section5.2 (second list item).
Consequently, we com-pute the polarity of the sentences that contain atleast one topic word and the question target.5.4 Filtering using SRFinally, answers are filtered using the Semrolsystem for SR labeling described in (Moreda,2008).
Subsequently, we filter all snippets withthe required target and source as agent or pa-tient.
Semrol receives as input plain text withinformation about grammar, syntax, wordsenses, Named Entities and constituents of eachverb.
The system output is the given text, inwhich the semantic roles information of eachconstituent is marked.
Ambiguity is resolved31depending on the machine algorithm employed,which in this case is TIMBL5.6 Evaluation and DiscussionWe evaluate our approaches on both the Emo-tiBlog question collection, as well as on theTAC 2008 Opinion Pilot test set.
We comparethem against the performance of the system eva-luated in (Balahur et al, 2009) and the best(Copeck et al, 2008) and worst (Varma et al,2008) scoring systems (as far as F-measure isconcerned) in the TAC 2008 task.
For both theTAC 2008 and EmotiBlog sets of questions, weemploy the SR system in SA and determine theES, ET and EPT.
Subsequently, for each of thetwo corpora, we retrieve 1-phrase and 3-phrasesnippets.
The retrieval of the of the EmotiBlogcandidate snippets is done using query expan-sion with LSA and filtering according to the ET.Further on, we apply sentiment analysis (SA)using the approach described in Section 5.3 andselect only the snippets whose polarity is thesame as the determined question EPT.
The re-sults are presented in Table 3.QNo.No.ABaseline(Balahur et al,2009)1 phrase +ET+SA3 phrases+ET+SA@1@5@10@50@1@5@10@50@1@5@10@202 5 0 2 3 4 1 2 3 4 1 2 3 45 110 0 0 0 0 2 2 2 1 2 3 46 2 0 0 1 2 1 1 2 2 0 1 2 27 5 0 0 1 3 1 1 1 3 0 2 2 4112 1 1 1 1 0 0 0 0 0 0 0 1123 0 1 1 1 0 1 2 3 0 0 1 2151 0 0 1 1 0 0 1 1 1 1 1 1166 1 4 4 4 0 1 1 2 1 2 2 6181 0 0 0 0 0 0 0 0 0 0 0 019271 5 6 180 1 1 2 0 1 1 1204 0 0 0 0 0 0 1 1 0 0 1 2Table 3: Results for questions overEmotiBlog5http://ilk.uvt.nl/downloads/pub/papers/Timbl_6.2_Manual.pdf and http://ilk.uvt.nl/timbl/The retrieval of the TAC 2008 1-phrase and 3-phrase candidate snippets was done using JIRSand, in a second approach, using the cosine si-milarity measure between alternative queriesgenerated using paraphrases and candidatesnippets.
Subsequently, we performed differentevaluations, in order to assess the impact of us-ing different resources and tools.
Since the TAC2008 had a limit of the output of 7000 charac-ters, in order to compute a comparable F-measure, at the end of each processing chain,we only considered the snippets for the 1-phraseretrieval and for the 3-phases one until this limitwas reached.1.
In the first evaluation, we only apply thesentiment analysis tool and select the snip-pets that have the same polarity as the ques-tion EPT and the ET is found in the snippet.(i.e.
What motivates peoples negative opi-nions on the Kyoto Protocol?
The KyotoProtocol becomes deterrence to economicdevelopment and international cooperation/Secondly, in terms of administrative aspect,the Kyoto Protocol is difficult to implement.- same EPT and ET)We also detected cases of same polarity butno ET, e.g.
These attempts mean annual ex-penditures of $700 million in tax credits inorder to endorse technologies, $3 billion indeveloping research and $200 million insettling technology into developing coun-tries ?
EPT negative but not same ET.2.
In the second evaluation, we add the resultof the LSA process to filter out the snippetsfrom 1., containing the words related to thetopic starting from the retrieval performedby Yahoo, which extracts the first 20 docu-ments about the topic.3.
In the third evaluation, we filter the resultsin 2 by applying the Semrol system and set-ting the condition that the ET and ES are theagent or the patient of the snippet.4.
In the fourth evaluation setting, we replacedthe set of snippets retrieved using JIRS withthe ones obtained by generating alternativequeries using paraphrases (as explained inthe third method in section 5.2.).
We subse-quently filtered these results based on theirpolarity  (so that it corresponds to the EPT)and on the condition that the source and tar-get of the opinion (identified through SRLusing Semrol) correspond to the ES and ET.325.
In the fourth evaluation setting, we replacedthe set of snippets retrieved using JIRS withthe ones obtained by generating alternativequeries using paraphrases, enriched with thetopic words determined using LSA.
Wesubsequently filtered these results based ontheir polarity (so that it corresponds to theEPT) and on the condition that the sourceand target of the opinion (identified throughSRL using Semrol) correspond to the ESand ET.System F-measureBest TAC 0.534Worst TAC 0.101JIRS + SA+ET (1 phrase)  0.377JIRS + SA+ET (3 phrases)  0.431JIRS + SA+ET+LSA (1 phrase)  0.489JIRS + SA+ET+LSA (3 phrases)  0.505JIRS + SA+ET+LSA+SR (1phrase)0.
533JIRS + SA+ET+LSA+SR (3phrases)0.571PAR+SA+ET+SR(1 phrase) 0.345PAR+SA+ET+SR(2 phrase) 0.386PAR_LSA+SA+ET+SR (1 phra-se)0.453PAR_LSA+SA+ET+SR (3 phra-ses)0.434Table 4: Results for the TAC 2008 test setFrom the results obtained (Table 3 and Table 4),we can draw the following conclusions.
Firstly,the hypothesis that OQA requires the retrievalof longer snippets was confirmed by the im-proved results, both in the case of EmotiBlog, aswell as the TAC 2008 corpus.
Secondly, opi-nion questions require the use of joint topic-sentiment analysis.
As we can see from the re-sults, the use of topic-related words when com-puting of the affect influences the results in apositive manner and joint topic-sentiment anal-ysis is especially useful for the cases of ques-tions asked on a monothematic corpus.
Thirdly,another conclusion that we can draw is that tar-get and source detection are highly relevantsteps at the time of answer filtering, not onlyhelping the more accurate retrieval of answers,but also at placing at the top of the retrieval therelevant results (as more relevant information iscontained within these 7000 characters).
Theuse of paraphrases at the retrieval stage wasshown to produce a significant drop in results,which we explain by the noise introduced andthe fact that more non-relevant answer candi-dates were introduced among the results.
None-theless, as we can see from the overall relativelylow improvement in the results, much remainsto be done in order to appropriately tackleOQA.
As seen in the results, there are still ques-tions for which no answer is found (e.g.
18).This is due to the fact that the treatment of suchquestions requires the use of inference tech-niques that are presently unavailable (i.e.
defineterms such as ?improvement?, possibly as ?Xbetter than Y?, in which case opinion extractionfrom comparative sentences should be intro-duced in the model).The results obtained when using all the compo-nents for the 3-sentence long snippets signifi-cantly improve the results obtained by the bestsystem participating in the TAC 2008 OpinionPilot competition (determined using a paired t-test for statistical significance, with confidencelevel 5%).
Finally, from the analysis of the er-rors, we could see that even though some toolsare in theory useful and should produce higherimprovements ?
such as SR ?
their performancein reality does not produce drastically higherresults.
The idea to use paraphrases for queryexpansion also proved to decrease the systemperformance.
From preliminary results obtainedusing JavaRap6  for coreference resolution, wealso noticed that the performance of the OQAlowered, although theoretically it should haveimproved.7 Conclusions ad Future WorkIn this paper, we presented and evaluated differ-ent methods and techniques with the objectiveof improving the task of QA in the context ofopinion data.
From the evaluations performedusing different NLP resources and tools, weconcluded that joint topic-sentiment analysis, aswell as the target and source identification, arecrucial for the correct performance of this task.We have also demonstrated that by retrievinglonger answers, the results have improved.
Wetested, within a simple setting, the impact ofusing paraphrases in the context of opinionquestions and saw that their use lowered thesystem results.
Although such paraphrase col-6http://wing.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.htm33lections include a lot of noise and have beenshown to decrease system performance even inthe case of factual questions, we believe thatother types of paraphrasing methods should beinvestigated in the context of OQA.
We thusshowed that opinion QA requires the develop-ment of appropriate strategies at the differentstages of the task (recognition of subjectivequestions, detection of subjective content of thequestions, source and target identification, re-trieval and classification of the candidate an-swer data).
Due to the high level of complexityof subjective language, our future work will befocused on testing higher-performing tools forcoreference resolution, other (opinion) paraph-rases collections and paraphrasing methods andthe employment of external knowledge sourcesthat refine the semantics of queries.
We alsoplan to include other SA methods and extendthe semantic roles considered for ET and ES,with the purpose of checking if they improve ornot the performance of the QA system.AcknowledgementsThis paper has been partially supported by Mi-nisterio de Ciencia e Innovaci?n - Spanish Gov-ernment (grant no.
TIN2009-13391-C04-01),and Conselleria d'Educaci?n - Generalitat Va-lenciana (grant no.
PROMETEO/2009/119 andACOMP/2010/286).ReferencesBalahur, A. and Montoyo, A.
2008.
Applying aCulture Dependent Emotion Triggers Data-base for Text Valence and EmotionClassification.
In Proceedings of the AISB2008 Symposium on Affective Language inHuman and Machine, Aberdeen, Scotland.Balahur, A., Lloret, E., Ferr?ndez, O., Montoyo,A., Palomar, M., and Mu?oz, R. 2008.
TheDLSIUAES Team?s Participation in the TAC2008 Tracks.
In Proceedings of the TextAnalysis Conference 2008 Workshop.Balahur, A., Boldrini, E., Montoyo A. andMart?nez-Barco P. 2009.
Opinion and GenericQuestion Answering Systems: a PerformanceAnalysis.
In Proceedings of ACL.
Singapur.Boldrini, E., Balahur, A., Mart?nez-Barco, P.and  Montoyo.
A.
2009a.
EmotiBlog: an An-notation Scheme for Emotion Detection andAnalysis in Non-traditional Textual Genre.
InProceedings of DMIN 2009, Las Vegas.
Ne-vada.Boldrini, E., Balahur, A., Mart?nez-Barco, P.and Montoyo.
A.
2009b.
EmotiBlog: a fine-grained model for emotion detection in non-traditional textual genre.
In Proceedings ofWOMSA 2009.
Seville.Cardie, C., Wiebe, J., Wilson, T. and Litman, D.2003.
Combining Low-Level and SummaryRepresentations of Opinions for Multi-Perspective Question Answering.
AAAISpring Symposium on New Directions inQuestion Answering.Cerini, S., Compagnoni, V., Demontis, A.,Formentelli, M. and Gandini, C. 2007.
Mi-cro-WNOp: A gold standard for the evalua-tion of automatically compiledlexical re-sources for opinion mining.
In: A.Sanso(ed.
): Language resources and linguistictheory: Typology, Second Language Acqui-sition, English Linguistics.
Milano.
IT.Copeck, T.,  Kazantseva, A., Kennedy, A.,Kunadze, A., Inkpen, D. and Szpakowicz,S.
2008.
Update Summary Update.
In Pro-ceedings of the Text Analysis Conference(TAC) 2008.Cui, H., Mittal, V. and Datar, M. 2006.
Com-parative Experiments on Sentiment Classifi-cation for Online Product Review.
Proceed-ings, The Twenty-First National Conferenceon Artificial Intelligence and the EighteenthInnovative Applications of Artificial Intelli-gence Conference.
Boston, Massachusetts,USA.G?mez, J.M., Rosso, P. and Sanchis, E. 2007.JIRS Language-Independent Passage Re-trieval System: A Comparative Study.
5thInternational Conference on NaturalLanguage Proceeding (ICON 2007).Kabadjov, M., Balahur, A.
And Boldrini, E.2009.
Sentiment Intensity: Is It a GoodSummary Indicator?.
Proceedings of the 4thLanguage Technology Conference LTC, pp.380-384.
Poznan, Poland, 6-8.11.2009.Kim, S. M. and Hovy, E. 2005.
IdentifyingOpinion Holders for Question Answering inOpinion Texts.
Proceedings of theWorkshop on Question Answering inRestricted Domain at the Conference of theAmerican Association of ArtificialIntelligence (AAAI-05).
Pittsburgh, PA.34Li, F., Zheng, Z.,Yang T., Bu, F., Ge, R., Zhu,X., Zhang, X., and Huang, M. 2008.
THUQUANTA at TAC 2008.
QA and RTE track.In Proceedings of the Text AnalysisConference (TAC).Lin, D. and Pantel, P. 2001.
Discovery ofInference Rules for Question Answering.Natural Language Engineering 7(4):343-360.Moreda.
P. 2008.
Los Roles Sem?nticos en laTecnolog?a del Lengauje Humano: Anota-ci?n y Aplicaci?n.
Doctoral Thesis.
Univer-sity of Alicante.Pustejovsky, J. and Wiebe, J.
2006.
Introductionto Special Issue on Advances in QuestionAnswering.
Language Resources and Eval-uation (2005), (39).Shen, D., Wiegand, M., Merkel, A., Kazalski,S., Hunsicker, S., Leidner, J. L. andKlakow, D. 2007.
The Alyssa System atTREC QA 2007: Do We Need Blog06?
InProceedings of the Sixteenth Text RetrievalConference (TREC 2007), Gaithersburg,MD, USA.Strapparava, C. and Valitutti, A.
2004.
Word-Net-Affect: an affective extension of Word-Net.
In Proceedings of 4th International Con-ference on Language Resources and Evalua-tion (LREC 2004), pages 1083 ?
1086, Lis-bon.Stoyanov, V., Cardie, C., and Wiebe, J.
2005.Multiperspective question answering usingthe opqa corpus.
In Proceedings of theHuman Language Technology Conferenceand the Conference on Empirical Methodsin Natural Language Processing(HLT/EMNLP 2005).Varma, V., Pingali, P., Katragadda, S., Krishna,R., Ganesh, S., Sarvabhotla, K.
Garapati,H., Gopisetty, H., Reddy, K. andBharadwaj, R. 2008.
IIIT Hyderabad atTAC 2008.
In Proceedings of Text AnalysisConference (TAC).Wenjie, L., Ouyang, Y., Hu, Y. and Wei, F.2008.
PolyU at TAC 2008.
In Proceedingsof the Text Analysis Conference (TAC).Wiebe, J., Wilson, T., and Cardie, C. 2005.Annotating expressions of opinions andemotions in language.
Language Resourcesand Evaluation, volume 39, issue 2-3, pp.165-210.Wilson, T., J. Wiebe, and Hoffmann, P. 2005.Recognizing Contextual Polarity in Phrase-level sentiment Analysis.
In Proceedings ofthe Human Language TechnologiesConference/Conference on EmpiricalMethods in Natural Language Processing(HLT/ EMNLP).Yu, H. and Hatzivassiloglou, V. 2003.
TowardsAnswering Opinion Questions: SeparatingFacts from Opinions.
In Proceedings ofEMNLP-03.Wiebe, J., Wilson, T., and Cardie, C. (2005).Annotating expressions of opinions andemotions in language.
In LanguageResources and Evaluation.
Vol.
39.35
