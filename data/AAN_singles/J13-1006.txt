Data-Driven Parsing using ProbabilisticLinear Context-Free Rewriting SystemsLaura Kallmeyer?Heinrich-Heine-Universita?t Du?sseldorfWolfgang Maier?
?Heinrich-Heine-Universita?t Du?sseldorfThis paper presents the first efficient implementation of a weighted deductive CYK parser forProbabilistic Linear Context-Free Rewriting Systems (PLCFRSs).
LCFRS, an extension of CFG,can describe discontinuities in a straightforward way and is therefore a natural candidate to beused for data-driven parsing.
To speed up parsing, we use different context-summary estimatesof parse items, some of them allowing for A?
parsing.
We evaluate our parser with grammarsextracted from the German NeGra treebank.
Our experiments show that data-driven LCFRSparsing is feasible and yields output of competitive quality.1.
IntroductionRecently, the challenges that a rich morphology poses for data-driven parsing havereceived growing interest.
A direct effect of morphological richness is, for instance, datasparseness on a lexical level (Candito and Seddah 2010).
A rather indirect effect is thatmorphological richness often relaxes word order constraints.
The principal intuition isthat a rich morphology encodes information that otherwise has to be conveyed by aparticular word order.
If, for instance, the case of a nominal complement is not providedby morphology, it has to be provided by the position of the complement relative to othercomplements in the sentence.
Example (1) provides an example of case marking and freeword order in German.
In turn, in free word order languages, word order can encodeinformation structure (Hoffman 1995).
(1) a. derthekleinelittleJungenomboyschicktsendsseinerhisSchwesterdatsisterdentheBriefaccletterb.
Other possible word orders:(i) der kleine Jungenom schickt den Briefacc seiner Schwesterdat(ii) seiner Schwesterdat schickt der kleine Jungenom den Briefacc(iii) den Briefacc schickt der kleine Jungenom seiner Schwesterdat?
Institut fu?r Sprache und Information, Universita?tsstr.
1, D-40225 Du?sseldorf, Germany.E-mail: kallmeyer@phil.uni-duesseldorf.de.??
Institut fu?r Sprache und Information, Universita?tsstr.
1, D-40225 Du?sseldorf, Germany.E-mail: maierw@hhu.de.Submission received: September 29, 2011; revised submission received: May 20, 2012; accepted for publication:August 3, 2012.?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 1It is assumed that this relation between a rich morphology and free word order doesnot hold in both directions.
Although it is generally the case that languages with a richmorphology exhibit a high degree of freedom in word order, languages with a free wordorder do not necessarily have a rich morphology.
Two examples for languages with avery free word order are Turkish and Bulgarian.
The former has a very rich and thelatter a sparse morphology.
See Mu?ller (2002) for a survey of the linguistics literature onthis discussion.With a rather free word order, constituents and single parts of them can be displacedfreely within the sentence.
German, for instance, has a rich inflectional system andallows for a free word order, as we have already seen in Example (1): Arguments canbe scrambled, and topicalizations and extrapositions underlie few restrictions.
Conse-quently, discontinuous constituents occur frequently.
This is challenging for syntacticdescription in general (Uszkoreit 1986; Becker, Joshi, and Rambow 1991; Bunt 1996;Mu?ller 2004), and for treebank annotation in particular (Skut et al1997).In this paper, we address the problem of data-driven parsing of discontinuous constit-uents on the basis of German.
In this section, we inspect the type of data we have to dealwith, and we describe the way such data are annotated in treebanks.
We briefly discussdifferent parsing strategies for the data in question and motivate our own approach.1.1 Discontinuous ConstituentsConsider the sentences in Example (2) as examples for discontinuous constituents(taken from the German NeGra [Skut et al1997] and TIGER [Brants et al2002] tree-banks).
Example (2a) shows several instances of discontinuous VPs and Example (2b)shows a discontinuous NP.
The relevant constituent is printed in italics.
(2) a. Fronting:(i) Daru?berThereofmussmustnachgedachtthoughtwerden.be(NeGra)?One must think of that?
(ii) Ohne internationalen SchadenWithout international damageko?nnecouldsichitselfBonnBonnvon dem Denkmalfrom the monumentnichtnotdistanzieren,distance... (TIGER)?Bonn could not distance itself from the monument without internationaldamage.?
(iii) AuchAlsowu?rdenwoulddurch die Regelungthrough the regulationnuronly?sta?ndig?constantlyneuenewAltfa?lleold casesentstehen?.
(TIGER)emerge?
?Apart from that, the regulation would only constantly produce new old cases.?b.
Extraposed relative clauses:(i) .
.
.
ob.
.
.
whetheraufonderentheirGela?ndeterraindertheTyp von Abstellanlagetype of parking facilitygebautbuiltwerdengetko?nne,could,der ...which .
.
.(NeGra)?.
.
.
whether one could build on their premises the type of parking facility,which .
.
.
?88Kallmeyer and Maier PLCFRS ParsingExamples of other such languages are Bulgarian and Korean.
Both show discontin-uous constituents as well.
Example (3a) is a Bulgarian example of a PP extracted out ofan NP, taken from the BulTreebank (Osenova and Simov 2004), and Example (3b) is anexample of fronting in Korean, taken from the Penn Korean Treebank (Han, Han, andKo 2001).
(3) a. Na kyshtataOf house-DETtoihepopravirepairedpokriva.roof.
?It is the roof of the house he repairs.?b.
Gwon.han-u?lAuthority-OBJnu.gawhoka.ji.gohasiss.ji?not?
?Who has no authority?
?Discontinuous constituents are by no means limited to languages with freedomin word order.
They also occur in languages with a rather fixed word order suchas English, resulting from, for instance, long-distance movements.
Examples (4a) and(4b) are examples from the Penn Treebank for long extractions resulting in discontin-uous S categories and for discontinuous NPs arising from extraposed relative clauses,respectively (Marcus et al1994).
(4) a.
Long Extraction in English:(i) Those chains include Bloomingdale?s, which Campeau recently said itwill sell.
(ii) What should I do.b.
Extraposed nominal modifiers (relative clauses and PPs) in English:(i) They sow a row of male-fertile plants nearby, which then pollinate the male-sterile plants.
(ii) Prices fell marginally for fuel and electricity.1.2 Treebank Annotation and Data-Driven ParsingMost constituency treebanks rely on an annotation backbone based on Context-FreeGrammar (CFG).
Discontinuities cannot be modeled with CFG, because they require alarger domain of locality than the one offered by CFG.
Therefore, the annotation back-bone based on CFG is generally augmented with a separate mechanism that accountsfor the non-local dependencies.
In the Penn Treebank (PTB), for example, trace nodesand co-indexation markers are used in order to establish additional implicit edges in thetree beyond the overt phrase structure.
In Tu?Ba-D/Z (Telljohann et al2012), a GermanTreebank, non-local dependencies are expressed via an annotation of topological fields(Ho?hle 1986) and special edge labels.
In contrast, some other treebanks, among themNeGra and TIGER, give up the annotation backbone based on CFG and allow annota-tion with crossing branches (Skut et al1997).
In such an annotation, non-local depen-dencies can be expressed directly by grouping all dependent elements under a singlenode.
Note that both crossing branches and traces annotate long-distance dependenciesin a linguistically meaningful way.
A difference is, however, that crossing branchesare less theory-dependent because they do not make any assumptions about the basepositions of ?moved?
elements.Examples for the different approaches of annotating discontinuities are given inFigures 1 and 2.
Figure 1 shows the NeGra annotation of Example (2a-i) (left), and an89Computational Linguistics Volume 39, Number 1Figure 1A discontinuous constituent.
Original NeGra annotation (left) and a Tu?Ba-D/Z-style annotation(right).WhatWPshouldMDIPRPdoVB*T*-NONE-?.WHNP NP NPVPSBJSQSBARQ*T*WhatWPshouldMDIPRPdoVB?.WHNP NPVPSBJSQSBARQFigure 2A discontinuous wh-movement.
Original PTB annotation (left) and NeGra-style annotation(right).annotation of the same sentence in the style of the Tu?Ba-D/Z treebank (right).
Figure 2shows the PTB annotation of Example (4a-ii) (on the left, note that the directed edgefrom the trace to the WHNP element visualizes the co-indexation) together with aNeGra-style annotation of the same sentence (right).In the past, data-driven parsing has largely been dominated by ProbabilisticContext-Free Grammar (PCFG).
In order to extract a PCFG from a treebank, the treesneed to be interpretable as CFG derivations.
Consequently, most work has excludednon-local dependencies; either (in PTB-like treebanks) by discarding labeling conven-tions such as the co-indexation of the trace nodes in the PTB, or (in NeGra/TIGER-liketreebanks) by applying tree transformations, which resolve the crossing branches (e.g.,Ku?bler 2005; Boyd 2007).
Especially for the latter treebanks, such a transformation isproblematic, because it generally is non-reversible and implies information loss.Discontinuities are no minor phenomenon: Approximately 25% of all sentencesin NeGra and TIGER have crossing branches (Maier and Lichte 2011).
In the PennTreebank, this holds for approximately 20% of all sentences (Evang and Kallmeyer2011).
This shows that it is important to properly treat such structures.1.3 Extending the Domain of LocalityIn the literature, different methods have been explored that allow for the use of non-local information in data-driven parsing.
We distinguish two classes of approaches.The first class consists of approaches that aim at using formalisms which producetrees without crossing branches but provide a larger domain of locality than CFG?for instance, through complex labels (Hockenmaier 2003) or through the derivation90Kallmeyer and Maier PLCFRS ParsingCFG:A?LCFRS: ?A?
?
?1 ?2 ?3Figure 3Different domains of locality.mechanism (Chiang 2003).
The second class, to which we contribute in this paper,consists of approaches that aim at producing trees which contain non-local information.Some methods realize the reconstruction of non-local information in a post- or pre-processing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004;Cai, Chiang, and Goldberg 2011).
Other work uses formalisms that accommodate thedirect encoding of non-local information (Plaehn 2004; Levy 2005).
We pursue the latterapproach.Our work is motivated by the following recent developments.
Linear Context-FreeRewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been estab-lished as a candidate for modeling both discontinuous constituents and non-projectivedependency trees as they occur in treebanks (Maier and S?gaard 2008; Kuhlmann andSatta 2009; Maier and Lichte 2011).
LCFRSs are a natural extension of CFGs wherethe non-terminals can span tuples of possibly non-adjacent strings (see Figure 3).
Be-cause LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs,PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighteddeductive parsing (Nederhof 2003), and A?
parsing (Klein and Manning 2003a) canbe transferred to LCFRS.
Finally, as mentioned before, languages such as Germanhave recently attracted the interest of the parsing community (Ku?bler and Penn 2008;Seddah, Ku?bler, and Tsarfaty 2010).We bring together these developments by presenting a parser for ProbabilisticLCFRS (PLCFRS), continuing the promising work of Levy (2005).
Our parser pro-duces trees with crossing branches and thereby accounts for syntactic long-distancedependencies while not making any additional assumptions concerning the positionof hypothetical traces.
We have implemented a CYK parser and we present severalmethods for context summary estimation of parse items.
The estimates either act asfigures-of-merit in a best-first parsing context or as estimates for A?
parsing.
A test ona real-world-sized data set shows that our parser achieves competitive results.
To ourknowledge, our parser is the first for the entire class of PLCFRS that has successfullybeen used for data-driven parsing.1The paper is structured as follows.
Section 2 introduces probabilistic LCFRS.
Sec-tions 3 and 4 present the binarization algorithm, the parser, and the outside estimateswhich we use to speed up parsing.
In Section 5 we explain how to extract an LCFRS froma treebank and we present grammar refinement methods for these specific treebankgrammars.
Finally, Section 6 presents evaluation results and Section 7 compares ourwork to other approaches.1 Parts of the results presented in this paper have been presented earlier.
More precisely, in Kallmeyer andMaier (2010), we presented the general architecture of the parser and all outside estimates except the LNestimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012).
In Maier andKallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2.
Finally,Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics).91Computational Linguistics Volume 39, Number 12.
Probabilistic Linear Context-Free Rewriting Systems2.1 Definition of PLCFRSLCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a non-terminal can span not only a single string but a tuple of strings of size k ?
1. k is therebycalled its fan-out.
We will notate LCFRS with the syntax of Simple Range Concate-nation Grammars (SRCG) (Boullier 1998b), a formalism that is equivalent to LCFRS.A third formalism that is equivalent to LCFRS is Multiple Context-Free Grammar(MCFG) (Seki et al1991).Definition 1 (LCFRS)A Linear Context-Free Rewriting System (LCFRS) is a tuple ?N, T, V, P, S?
wherea) N is a finite set of non-terminals with a function dim: N ?
N thatdetermines the fan-out of each A ?
N;b) T and V are disjoint finite sets of terminals and variables;c) S ?
N is the start symbol with dim(S) = 1;d) P is a finite set of rulesA(?1, .
.
.
,?dim(A) ) ?
A1(X(1)1 , .
.
.
, X(1)dim(A1)) ?
?
?Am(X(m)1 , .
.
.
, X(m)dim(Am ))for m ?
0 where A, A1, .
.
.
, Am ?
N, X(i)j ?
V for 1 ?
i ?
m, 1 ?
j ?
dim(Ai)and ?i ?
(T ?
V)?
for 1 ?
i ?
dim(A).
For all r ?
P, it holds that everyvariable X occurring in r occurs exactly once in the left-hand side andexactly once in the right-hand side of r.A rewriting rule describes how the yield of the left-hand side non-terminal can becomputed from the yields of the right-hand side non-terminals.
The rules A(ab, cd) ?
?and A(aXb, cYd) ?
A(X, Y) from Figure 4 for instance specify that (1) ?ab, cd?
is in theyield of A and (2) one can compute a new tuple in the yield of A from an already existingone by wrapping a and b around the first component and c and d around the second.A CFG rule A ?
BC would be written A(XY) ?
B(X)C(Y) as an LCFRS rule.Definition 2 (Yield, language)Let G = ?N, T, V, P, S?
be an LCFRS.1.
For every A ?
N, we define the yield of A, yield(A) as follows:a) For every rule A(?)
?
?, ?
?
yield(A);A(ab, cd) ?
?A(aXb, cYd) ?
A(X, Y)S(XY) ?
A(X, Y)Figure 4Sample LCFRS for {anbncndn | n ?
1}.92Kallmeyer and Maier PLCFRS Parsingb) For every rule A(?1, .
.
.
,?dim(A) ) ?
A1(X(1)1 , .
.
.
, X(1)dim(A1)) ?
?
?Am(X(m)1 , .
.
.
, X(m)dim(Am )) and for all ?i ?
yield(Ai) (1 ?
i ?
m):?
f (?1), .
.
.
, f (?dim(A) )?
?
yield(A) where f is defined as follows:(i) f (t) = t for all t ?
T,(ii) f (X(i)j ) = ?i(j) for all 1 ?
i ?
m, 1 ?
j ?
dim(Ai) and(iii) f (xy) = f (x)f (y) for all x, y ?
(T ?
V)+.We call f the composition function of the rule.c) Nothing else is in yield(A).2.
The language of G is then L(G) = {w | ?w?
?
yield(S)}.As an example, consider again the LCFRS in Figure 4.
The last rule tells us that,given a pair in the yield of A, we can obtain an element in the yield of S by concate-nating the two components.
Consequently, the language generated by this grammar is{anbncndn |n ?
1}.The terms of grammar fan-out and rank and the properties of monotonicity and?-freeness will be referred to later and are therefore introduced in the following defini-tion.
They are taken from the LCFRS/MCFG terminology; the SRCG term for fan-out isarity and the property of being monotone is called ordered in the context of SRCG.Definition 3Let G = ?N, T, V, P, S?
be an LCFRS.1.
The fan-out of G is the maximal fan-out of all non-terminals in G.2.
Furthermore, the right-hand side length of a rewriting rule r ?
P is calledthe rank of r and the maximal rank of all rules in P is called the rank of G.3.
G is monotone if for every r ?
P and every right-hand side non-terminal Ain r and each pair X1, X2 of arguments of A in the right-hand side of r, X1precedes X2 in the right-hand side iff X1 precedes X2 in the left-hand side.4.
A rule r ?
P is called an ?-rule if one of the left-hand side components of ris ?.G is ?-free if it either contains no ?-rules or there is exactly one ?-ruleS(?)
?
?
and S does not appear in any of the right-hand sides of the rulesin the grammar.For every LCFRS there exists an equivalent LCFRS that is ?-free (Seki et al1991;Boullier 1998a) and monotone (Michaelis 2001; Kracht 2003; Kallmeyer 2010).The definition of a probabilistic LCFRS is a straightforward extension of the defini-tion of PCFG and thus it follows (Levy 2005; Kato, Seki, and Kasami 2006) that:Definition 4 (PLCFRS)A probabilistic LCFRS (PLCFRS) is a tuple ?N, T, V, P, S, p?
such that ?N, T, V, P, S?
is anLCFRS and p : P ?
[0..1] a function such that for all A ?
N:?A(x)??
?Pp(A(x) ?
?)
= 193Computational Linguistics Volume 39, Number 1PLCFRS with non-terminals {S, A, B}, terminals {a} and start symbol S:0.2 : S(X) ?
A(X) 0.8 : S(XY) ?
B(X, Y)0.7 : A(aX) ?
A(X) 0.3 : A(a) ?
?0.8 : B(aX, aY) ?
B(X, Y) 0.2 : B(a, a) ?
?Figure 5Sample PLCFRS.As an example, consider the PLCFRS in Figure 5.
This grammar simply generatesa+.
Words with an even number of as and nested dependencies are more probablethan words with a right-linear dependency structure.
For instance, the word aa receivesthe two analyses in Figure 6.
The analysis (a) displaying nested dependencies hasprobability 0.16 and (b) (right-linear dependencies) has probability 0.042.3.
Parsing PLCFRS3.1 BinarizationSimilarly to the transformation of a CFG into Chomsky normal form, an LCFRS can bebinarized, resulting in an LCFRS of rank 2.
As in the CFG case, in the transformation,we introduce a non-terminal for each right-hand side longer than 2 and split the ruleinto two rules, using this new intermediate non-terminal.
This is repeated until allright-hand sides are of length 2.
The transformation algorithm is inspired by Go?mez-Rodr?
?guez et al(2009) and it is also specified in Kallmeyer (2010).3.1.1 General Binarization.
In order to give the algorithm for this transformation, weneed the notion of a reduction of a vector ?
?
[(T ?
V)?
]i by a vector x ?
Vj where allvariables in x occur in ?.
A reduction is, roughly, obtained by keeping all variables in ?that are not in x.
This is defined as follows:Definition 5 (Reduction)Let ?N, T, V, P, S?
be an LCFRS, ?
?
[(T ?
V)?
]i and x ?
Vj for some i, j ?
IN.Let w = ?1$ .
.
.
$?i be the string obtained from concatenating the components of ?,separated by a new symbol $ /?
(V ?
T).Let w?
be the image of w under a homomorphism h defined as follows: h(a) = $ forall a ?
T, h(X) = $ for all X ?
{x1, .
.
.xj} and h(y) = y in all other cases.Let y1, .
.
.
ym ?
V+ such that w?
?
$?y1$+y2$+ .
.
.
$+ym$?.
Then the vector?y1, .
.
.
ym?
is the reduction of ?
by x.For instance, ?aX1, X2, bX3?
reduced with ?X2?
yields ?X1, X3?
and ?aX1X2bX3?
re-duced with ?X2?
yields ?X1, X3?
as well.SBa a(a)SAa Aa(b)Figure 6The two derivations of aa.94Kallmeyer and Maier PLCFRS Parsingfor all rules r = A(?)
?
A0( ?0) .
.
.Am( ?m) in P with m > 1 doremove r from PR := ?pick new non-terminals C1, .
.
.
, Cm?1add the rule A(?)
?
A0( ?0)C1( ?1) to R where ?1 is obtained by reducing ?
with ?0for all i, 1 ?
i ?
m ?
2 doadd the rule Ci(?i) ?
Ai(?i)Ci+1( ?i+1) to R where ?i+1 is obtained by reducing ?i with ?iend foradd the rule Cm?1( ?m?2) ?
Am?1( ?m?1)Am( ?m) to Rfor every rule r?
?
R doreplace right-hand side arguments of length > 1 with new variables (in both sides) andadd the result to Pend forend forFigure 7Algorithm for binarizing an LCFRS.The binarization algorithm is given in Figure 7.
As already mentioned, it proceedslike the CFG binarization algorithm in the sense that for right-hand sides longer than2, we introduce a new non-terminal that covers the right-hand side without the firstelement.
Figure 8 shows an example.
In this example, there is only one rule with a right-hand side longer than 2.
In a first step, we introduce the new non-terminals and rulesthat binarize the right-hand side.
This leads to the set R. In a second step, before addingthe rules from R to the grammar, whenever a right-hand side argument contains severalvariables, these are collapsed into a single new variable.The equivalence of the original LCFRS and the binarized grammar is rather straight-forward.
Note, however, that the fan-out of the LCFRS can increase.The binarization depicted in Figure 7 is deterministic in the sense that for every rulethat needs to be binarized, we choose unique new non-terminals.
Later, in Section 5.3.1,we will introduce additional factorization into the grammar rules that reduces the setof new non-terminals.3.1.2 Minimizing Fan-Out and Number of Variables.
In LCFRS, in contrast to CFG, the orderof the right-hand side elements of a rule does not matter for the result of a derivation.Original LCFRS:S(XYZUVW) ?
A(X, U)B(Y, V)C(Z, W)A(aX, aY) ?
A(X, Y) A(a, a) ?
?B(bX, bY) ?
B(X, Y) B(b, b) ?
?C(cX, cY) ?
C(X, Y) C(c, c) ?
?Rule with right-hand side of length > 2: S(XYZUVW) ?
A(X, U)B(Y, V)C(Z, W)For this rule, we obtainR = {S(XYZUVW) ?
A(X, U)C1(YZ, VW), C1(YZ, VW) ?
B(Y, V)C(Z, W)}Equivalent binarized LCFRS:S(XPUQ) ?
A(X, U)C1(P, Q)C1(YZ, VW) ?
B(Y, V)C(Z, W)A(aX, aY) ?
A(X, Y) A(a, a) ?
?B(bX, bY) ?
B(X, Y) B(b, b) ?
?C(cX, cY) ?
C(X, Y) C(c, c) ?
?Figure 8Sample binarization of an LCFRS.95Computational Linguistics Volume 39, Number 1Therefore, we can reorder the right-hand side of a rule before binarizing it.
In thefollowing, we present a binarization order that yields a minimal fan-out and a minimalvariable number per production and binarization step.
The algorithm is inspired byGo?mez-Rodr?
?guez et al(2009) and has first been published in this version in Kallmeyer(2010).
We assume that we are only considering partitions of right-hand sides where oneof the sets contains only a single non-terminal.For a given rule c = A0(x0) ?
A1(x1) .
.
.Ak(xk), we define the characteristic strings(c, Ai) of the Ai-reduction of c as follows: Concatenate the elements of x0, separated withnew additional symbols $ while replacing every component from xi with a $.
We thendefine the arity of the characteristic string, dim(s(c, Ai)), as the number of maximal sub-strings x ?
V+ in s(Ai).
Take, for example, a rule c = VP(X, YZU) ?
VP(X, Z)V(Y)N(U).Then s(c, VP) =$$Y$U, s(c, V) = X$$ZU.Figure 9 shows how in a first step, for a given rule r with right-hand side length > 2,we determine the optimal candidate for binarization based on the characteristic strings(r, B) of some right-hand side non-terminal B and on the fan-out of B: On all right-hand side predicates B we check for the maximal fan-out (given by dim(s(r, B))) and thenumber of variables (dim(s(r, B)) + dim(B)) we would obtain when binarizing with thispredicate.
This check provides the optimal candidate.
In a second step we then performthe same binarization as before, except that we use the optimal candidate now insteadof the first element of the right-hand side.3.2 The ParserWe can assume without loss of generality that our grammars are ?-free and monotone(the treebank grammars with which we are concerned all have these properties) and thatthey contain only binary and unary rules.
Furthermore, we assume POS tagging to bedone before parsing.
POS tags are non-terminals of fan-out 1.
Finally, according to ourgrammar extraction algorithm (see Section 5.1), a separation between two componentsalways means that there is actually a non-empty gap in between them.
Consequently,two different components in a right-hand side can never be adjacent in the samecomponent of the left-hand side.
The rules are then either of the form A(a) ?
?
with A aPOS tag and a ?
T or of the form A(x) ?
B(x) or A(?)
?
B(x)C(y) where ?
?
(V+)dim(A),x ?
Vdim(B), y ?
Vdim(C), that is, only the rules for POS tags contain terminals in their left-hand sides.cand = 0fan-out = number of variables in rvars = number of variables in rfor all i = 0 to m docand-fan-out = dim(s(r, Ai));if cand-fan-out < fan-out and dim(Ai) < fan-out thenfan-out = max({cand-fan-out, dim(Ai)});vars = cand-fan-out + dim(Ai);cand = i;else if cand-fan-out ?
fan-out, dim(Ai) ?
fan-out and cand-fan-out + dim(Ai) < vars thenfan-out = max({cand-fan-out, dim(Ai)});vars = cand-fan-out + dim(Ai);cand = iend ifend forFigure 9Optimized version of the binarization algorithm, determining binarization order.96Kallmeyer and Maier PLCFRS ParsingDuring parsing we have to link the terminals and variables in our LCFRS rulesto portions of the input string.
For this purpose we need the notions of ranges, rangevectors, and rule instantiations.
A range is a pair of indices that characterizes the spanof a component within the input.
A range vector characterizes a tuple in the yield of anon-terminal.
A rule instantiation specifies the computation of an element from the left-hand side yield from elements in the yields of the right-hand side non-terminals basedon the corresponding range vectors.Definition 6 (Range)Let w ?
T?
with w = w1 .
.
.wn where wi ?
T for 1 ?
i ?
n.1.
Pos(w) := {0, .
.
.
, n}.2.
We call a pair ?l, r?
?
Pos(w) ?
Pos(w) with l ?
r a range in w. Its yield?l, r?
(w) is the substring wl+1 .
.
.wr.3.
For two ranges ?1 = ?l1, r1?,?2 = ?l2, r2?, if r1 = l2, then the concatenationof ?1 and ?2 is ?1 ?
?2 = ?l1, r2?
; otherwise ?1 ?
?2 is undefined.4.
A ?
?
(Pos(w) ?
Pos(w))k is a k-dimensional range vector for w iff?
= ?
?l1, r1?, .
.
.
, ?lk, rk??
where ?li, ri?
is a range in w for 1 ?
i ?
k.We now define instantiations of rules with respect to a given input string.
Thisdefinition follows the definition of clause instantiations from Boullier (2000).
An in-stantiated rule is a rule in which variables are consistently replaced by ranges.
Becausewe need this definition only for parsing our specific grammars, we restrict ourselves to?-free rules containing only variables.Definition 7 (Rule instantiation)Let G = (N, T, V, P, S) be an ?-free monotone LCFRS.
For a given rule r = A(?)
?A1(x1) ?
?
?Am( xm) ?
P (0 < m) that does not contain any terminals,1.
an instantiation with respect to a string w = t1 .
.
.
tn consists of a functionf : V ?
{?i, j?
| 1 ?
i ?
j ?
|w|} such that for all x, y adjacent in one of theelements of ?, f (x) ?
f (y) must be defined; we then define f (xy) = f (x) ?
f (y),2. if f is an instantiation of r, then A( f (?))
?
A1( f (x1)) ?
?
?Am( f ( xm)) is aninstantiated rule where f (?x1, .
.
.
, xk?)
= ?
f (x1), .
.
.
, f (xk)?.We use a probabilistic version of the CYK parser from Seki et al(1991).
The algo-rithm is formulated using the framework of parsing as deduction (Pereira and Warren1983; Shieber, Schabes, and Pereira 1995; Sikkel 1997), extended with weights (Nederhof2003).
In this framework, a set of weighted items representing partial parsing results ischaracterized via a set of deduction rules, and certain items (the goal items) representsuccessful parses.During parsing, we have to match components in the rules we use with portions ofthe input string.
For a given input w, our items have the form [A, ?]
where A ?
N and ?is a range vector that characterizes the span of A.
Each item has a weight in that encodesthe Viterbi inside score of its best parse tree.
More precisely, we use the log probabilitylog(p) where p is the probability.The first rule (scan) tells us that the POS tags that we receive as inputs are given.Consequently, they are axioms; their probability is 1 and their weight therefore 0.
The97Computational Linguistics Volume 39, Number 1Scan: 0 : [A, ?
?i, i + 1??]
A is the POS tag of wi+1Unary:in : [B, ?
]in + log(p) : [A, ?]
p : A(?)
?
B(?)
?
PBinary:inB : [B, ?B], inC : [C, ?C]inB + inC + log(p) : [A, ?A]p : A( ?A ) ?
B( ?B )C( ?C )is an instantiated ruleGoal: [S, ?
?0, n??
]Figure 10Weighted CYK deduction system.second rule, unary, is applied whenever we have found the right-hand side of aninstantiation of a unary rule.
In our grammar, terminals only occur in rules with POStags and the grammar is ordered and ?-free.
Therefore, the components of the yield ofthe right-hand side non-terminal and of the left-hand side terminals are the same.
Therule binary applies an instantiated rule of rank 2.
If we already have the two elementsof the right-hand side, we can infer the left-hand side element.
In both cases, unaryand binary, the probability p of the new rule is multiplied with the probabilities of theantecedent items (which amounts to summing up the antecedent weights and log(p)).We perform weighted deductive parsing, based on the deduction system fromFigure 10.
We use a chart C and an agenda A, both initially empty, and we proceedas in Figure 11.
Because for all our deduction rules, the weight functions f that computethe weight of a consequent item from the weights of the antecedent items are monotonenon-increasing in each variable, the algorithm will always find the best parse withoutthe need of exhaustive parsing.
All new items that we deduce involve at least one ofthe agenda items as an antecedent item.
Therefore, whenever an item is the best in theagenda, we can be sure that we will never find an item with a better (i.e., higher) weight.Consequently, we can safely store this item in the chart and, if it is a goal item, we havefound the best parse.As an example consider the development of the agenda and the chart in Figure 12when parsing aa with the PLCFRS from Figure 5, transformed into a PLCFRS withpre-terminals and binarization (i.e., with a POS tag Ta and a new binarization non-terminal B?).
The new PLCFRS is given in Figure 13.In this example, we find a first analysis for the input (a goal item) when combiningan A with span ?
?0, 2??
into an S. This S has however a rather low probability and istherefore not on top of the agenda.
Later, when finding the better analysis, the weightadd SCAN results to Awhile A = ?remove best item x : I from Aadd x : I to Cif I goal itemthen stop and output trueelsefor all y : I?
deduced from x : I and items in C:if there is no z with z : I?
?
C ?
Athen add y : I?
to Aelse if z : I?
?
A for some zthen update weight of I?
in A to max(y, z)Figure 11Weighted deductive parsing.98Kallmeyer and Maier PLCFRS Parsingchart agenda0 : [Ta, ?0, 1?
], 0 : [Ta, ?1, 2?
]0 : [Ta, ?0, 1?]
0 : [Ta, ?1, 2?
],?0.5 : [A, ?0, 1?
]0 : [Ta, ?0, 1?
], 0 : [Ta, ?1, 2?]
?0.5 : [A, ?0, 1?
],?0.5 : [A, ?1, 2?
],?0.7 : [B, ?0, 1?, ?1, 2?
]0 : [Ta, ?0, 1?
], 0 : [Ta, ?1, 2?
], ?0.5 : [A, ?1, 2?
],?0.7 : [B, ?0, 1?, ?1, 2?
],?0.5 : [A, ?0, 1?]
?1.2 : [S, ?0, 1?
]0 : [Ta, ?0, 1?
], 0 : [Ta, ?1, 2?
], ?0.65 : [A, ?0, 2?
],?0.7 : [B, ?0, 1?, ?1, 2?
],0.5 : [A, ?0, 1?
],?0.5 : [A, ?1, 2?]
?1.2 : [S, ?0, 1?
],?1.2 : [S, ?1, 2?
]0 : [Ta, ?0, 1?
], 0 : [Ta, ?1, 2?
], ?0.7 : [B, ?0, 1?, ?1, 2?
],?1.2 : [S, ?0, 1?
],?0.5 : [A, ?0, 1?
],?0.5 : [A, ?1, 2?
], ?1.2 : [S, ?1, 2?
],?1.35 : [S, ?0, 2?
]?0.65 : [A, ?0, 2?
]0 : [Ta, ?0, 1?
], 0 : [Ta, ?1, 2?
], ?0.8 : [S, ?0, 2?
],?1.2 : [S, ?0, 1?
],?0.5 : [A, ?0, 1?
],?0.5 : [A, ?1, 2?
], ?1.2 : [S, ?1, 2?
]?0.65 : [A, ?0, 2?
],?0.7 : [B, ?0, 1?, ?1, 2?
]Figure 12Parsing of aa with the grammar from Figure 5.PLCFRS with non-terminals {S, A, B, B?, Ta}, terminals {a} and start symbol S:0.2 : S(X) ?
A(X) 0.8 : S(XY) ?
B(X, Y)0.7 : A(XY) ?
Ta(X)A(Y) 0.3 : A(X) ?
Ta(X)0.8 : B(ZX, Y) ?
Ta(Z)B?
(X, Y) 1 : B?
(X, UY) ?
B(X, Y)Ta(U)0.2 : B(X, Y) ?
Ta(X)Ta(Y) 1 : Ta(a) ?
?Figure 13Sample binarized PLCFRS (with pre-terminal Ta).of the S item in the agenda is updated and then the goal item is the top agenda item andtherefore parsing has been successful.Note that, so far, we have only presented the recognizer.
In order to extend it to aparser, we do the following: Whenever we generate a new item, we store it not only withits weight but also with backpointers to its antecedent items.
Furthermore, wheneverwe update the weight of an item in the agenda, we also update the backpointers.
Inorder to read off the best parse tree, we have to start from the goal item and follow thebackpointers.4.
Outside EstimatesSo far, the weights we use give us only the Viterbi inside score of an item.
In orderto speed up parsing, we add the estimate of the costs for completing the item into agoal item to its weight?that is, to the weight of each item in the agenda, we add anestimate of its Viterbi outside score2 (i.e., the logarithm of the estimate).
We use contextsummary estimates.
A context summary is an equivalence class of items for which wecan compute the actual outside scores.
Those scores are then used as estimates.
Thechallenge is to choose the estimate general enough to be efficiently computable andspecific enough to be helpful for discriminating items in the agenda.2 Note that just as Klein and Manning (2003a), we use the terms inside score and outside score todenote the Viterbi inside and outside scores.
They are not to be confused with the actual inside oroutside probability.99Computational Linguistics Volume 39, Number 1Admissibility and monotonicity are two important conditions on estimates.
Allour outside estimates are admissible (Klein and Manning 2003a), which means thatthey never underestimate the actual outside score of an item.
In other words, theyare too optimistic about the costs of completing the item into an S item spanning theentire input.
For the full SX estimate described in Section 4.1 and the SX estimate withspan and sentence length in Section 4.4, the monotonicity is guaranteed and we can dotrue A?
parsing as described by Klein and Manning.
Monotonicity means that for eachantecedent item of a rule it holds that its weight is greater than or equal to the weightof the consequent item.
The estimates from Sections 4.2 and 4.3 are not monotonic.
Thismeans that it can happen that we deduce an item I2 from an item I1 where the weight ofI2 is greater than the weight of I1.
The parser can therefore end up in a local maximumthat is not the global maximum we are searching for.
In other words, those estimates areonly figures of merit (FOM).All outside estimates are computed off-line for a certain maximal sentence lengthlenmax.4.1 Full SX EstimateThe full SX estimate is a PLCFRS adaption of the SX estimate of Klein and Manning(2003a) (hence the name).
For a given sentence length n, the estimate gives the maximalprobability of completing a category X with a span ?
into an S with span ?
?0, n?
?.For its computation, we need an estimate of the inside score of a category C with aspan ?, regardless of the actual terminals in our input.
This inside estimate is computedas shown in Figure 14.
Here, we do not need to consider the number of terminals outsidethe span of C (to the left or right or in the gaps), because they are not relevant for theinside score.
Therefore the items have the form [A, ?l1, .
.
.
, ldim(A)?
], where A is a non-terminal and li gives the length of its ith component.
It holds that?1?i?dim(A)li ?
lenmax ?
dim(A) + 1because our grammar extraction algorithm ensures that the different components inthe yield of a non-terminal are never adjacent.
There is always at least one terminal inbetween two different components that does not belong to the yield of the non-terminal.The first rule in Figure 14 tells us that POS tags always have a single componentof length 1; therefore this case has probability 1 (weight 0).
The rules unary and binaryare roughly like the ones in the CYK parser, except that they combine items with lengthinformation.
The rule unary for instance tells us that if the log of the probability ofbuilding [B,l] is greater or equal to in and if there is a rule that allows to deduce anPOS tags: 0 : [A, ?1?]
A a POS tag Unary:in : [B,l]in + log(p) : [A,l]p : A(?)
?
B(?)
?
PBinary:inB : [B,lB], inC : [C,lC]inB + inC + log(p) : [A,lA]where p : A( ?A) ?
B( ?B)C( ?C) ?
P and the following holds: we define B(i) as{1 ?
j ?
dim(B) | ?B( j) occurs in ?A(i)} and C(i) as {1 ?
j ?
dim(C) | ?C( j) occurs in ?A(i)}.Then for all i, 1 ?
i ?
dim(A):lA(i) = ?j?B(i)lB( j) +?j?C(i)lC( j).Figure 14Estimate of the Viterbi inside score.100Kallmeyer and Maier PLCFRS ParsingAxiom : 0 : [S, ?0, len, 0?]
1 ?
len ?
lenmax Unary:out : [A,l]out + log(p) : [B,l]p : A(?)
?
B(?)
?
PBinary-right:out : [X,lX]out + in(A,l?A) + log(p) : [B,lB]Binary-left:out : [X,lX]out + in(B,l?B) + log(p) : [A,lA]where, for both binary rules, there is an instantiated rule p : X(?)
?
A( ?A)B( ?B) such thatlX = lout(?
),lA = lout(?A),l?A = lin(?A),lB = lout(?B),l?B = lin(?B).Figure 15Full SX estimate first version (top?down).A item from [B,l] with probability p, then the log of the probability of [A,l] is greateror equal to in + log(p).
For each item, we record its maximal weight (i.e., its maximalprobability).
The rule binary is slightly more complicated because we have to computethe length vector of the left-hand side of the rule from the right-hand side length vectors.A straightforward extension of the CFG algorithm from Klein and Manning (2003a)for computing the SX estimate is given in Figure 15.
Here, the items have the form [A,l]where the vectorl tells us about the lengths of the string to the left of the first component,the first component, the string in between the first and second component, and so on.The algorithm proceeds top?down.
The outside estimate of completing an S withcomponent length len and no terminals to the left or to the right of the S component(item [S, ?0, len, 0?])
is 0.
If we expand with a unary rule (unary), then the outsideestimate of the right-hand side item is greater or equal to the outside estimate of theleft-hand side item plus the log of the probability of the rule.
In the case of binary rules,we have to further add the inside estimate of the other daughter.
For this, we need adifferent length vector (without the lengths of the parts in between the components).Therefore, for a given range vector ?
= ?
?l1, r1?, .
.
.
, ?lk, rk??
and a sentence length n,we distinguish between the inside length vector lin(?)
= ?r1 ?
l1, .
.
.
, rk ?
lk?
and theoutside length vector lout(?)
= ?l1, r1 ?
l1, l2 ?
r1, .
.
.
, lk ?
rk?1, rk ?
lk, n ?
rk?.This algorithm has two major problems: Because it proceeds top?down, in thebinary rules we must compute all splits of the antecedent X span into the spans ofA and B, which is very expensive.
Furthermore, for a category A with a certain numberof terminals in the components and the gaps, we compute the lower part of the outsideestimate several times, namely, for every combination of number of terminals to the leftand to the right (first and last element in the outside length vector).
In order to avoidthese problems, we now abstract away from the lengths of the part to the left and theright, modifying our items such as to allow a bottom?up strategy.The idea is to compute the weights of items representing the derivations from acertain lower C up to some A (C is a kind of ?gap?
in the yield of A) while summing upthe inside costs of off-spine nodes and the log of the probabilities of the correspondingrules.
We use items [A, C,?A,?C, shift] where A, C ?
N and ?A,?C are range vectors, bothwith a first component starting at position 0.
The integer shift ?
lenmax tells us how manypositions to the right the C span is shifted, compared to the starting position of the A.?A and ?C represent the spans of C and A while disregarding the number of terminalsto the left and the right (i.e., only the lengths of the components and of the gaps areencoded).
This means in particular that the length n of the sentence does not play arole here.
The right boundary of the last range in the vectors is limited to lenmax.
For101Computational Linguistics Volume 39, Number 1any i, 0 ?
i ?
lenmax, and any range vector ?, we define shift(?, i) as the range vector oneobtains from adding i to all range boundaries in ?
and shift(?,?i) as the range vectorone obtains from subtracting i from all boundaries in ?.The weight of [A, C,?A,?C, i] estimates the log of the probability of completing aC tree with yield ?C into an A tree with yield ?A such that, if the span of A starts atposition j, the span of C starts at position i + j.
Figure 16 gives the computation.
Thevalue of in(A,l) is the inside estimate of [A,l].The SX-estimate for some predicate C with span ?
where i is the left boundary of thefirst component of ?
and with sentence length n is then given by the maximal weight of[S, C, ?0, n?, shift(?,?i), i].4.2 SX with Left, Gaps, Right, LengthA problem of the previous estimate is that with a large number of non-terminals (fortreebank parsing, approximately 12,000 after binarization and markovization), the com-putation of the estimate requires too much space.
We therefore turn to simpler estimateswith only a single non-terminal per item.
We now estimate the outside score of a non-terminal A with a span of a length length (the sum of the lengths of all the componentsof the span), with left terminals to the left of the first component, right terminals to theright of the last component, and gaps terminals in between the components of the Aspan (i.e., filling the gaps).
Our items have the form [X, len, left, right, gaps] with X ?
N,len + left + right + gaps ?
lenmax, len ?
dim(X), gaps ?
dim(X) ?
1.Let us assume that, in the rule X(?)
?
A( ?A)B( ?B), when looking at the vector ?,we have leftA variables for A-components preceding the first variable of a B component,rightA variables for A-components following the last variable of a B component, andrightB variables for B-components following the last variable of an A component.
(In ourgrammars, the first left-hand side argument always starts with the first variable from A.
)Furthermore, we set gapsA = dim(A) ?
leftA ?
rightA and gapsB = dim(B) ?
rightB.Figure 17 gives the computation of the estimate.
It proceeds top?down, as thecomputation of the full SX estimate in Figure 15, except that now the items are simpler.POS tags: 0 : [C, C, ?0, 1?, ?0, 1?, 0] C a POS tagUnary:0 : [B, B,?B,?B, 0]log(p) : [A, B,?B,?B, 0]p : A(?)
?
B(?)
?
PBinary-right:0 : [A, A,?A,?A, 0], 0 : [B, B,?B,?B, 0]in(A, lin(?A)) + log(p) : [X, B,?X,?B, i]Binary-left:0 : [A, A,?A,?A, 0], 0 : [B, B,?B,?B, 0]in(B, lin(?B)) + log(p) : [X, A,?X,?A, i]where i is such that for shift(?B, i) = ?
?B p : X(?X ) ?
A(?A)B(?
?B) is an instantiated rule.Starting sub-trees with larger gaps:out : [B, C,?B,?C, i]0 : [B, B,?B,?B, 0]Transitive closure of sub-tree combination:out1 : [A, B,?A,?B, i], out2 : [B, C,?B,?C, j]out1 + out2 : [A, C,?A,?C, i + j]Figure 16Full SX estimate second version (bottom?up).102Kallmeyer and Maier PLCFRS ParsingAxiom : 0 : [S, len, 0, 0, 0] 1 ?
len ?
lenmaxUnary:out : [X, len, l, r, g]out + log(p) : [A, len, l, r, g] p : X(?)
?
A(?)
?
PBinary-right:out : [X, len, l, r, g]out + in(A, len ?
lenB) + log(p) : [B, lenB, lB, rB, gB]Binary-left:out : [X, len, l, r, g]out + in(B, len ?
lenA) + log(p) : [A, lenA, lA, rA, gA]where, for both binary rules, p : X(?)
?
A( ?A)B( ?B) ?
P.Further side conditions for Binary-right:a) len + l + r + g = lenB + lB + rB + gB, b) lB ?
l + leftA,c) if rightA > 0, then rB ?
r + rightA, else (rightA = 0), rB = r, d) gB ?
gapsA.Further side conditions for Binary-left:a) len + l + r + g = lenA + lA + rA + gA, b) lA = l,c) if rightB > 0, then rA ?
r + rightB, else (rightB = 0), rA = r d) gA ?
gapsB.Figure 17SX estimate depending on length, left, right, gaps.The value in(X, l) for a non-terminal X and a length l, 0 ?
l ?
lenmax is an estimateof the probability of an X category with a span of length l. Its computation is specifiedin Figure 18.The SX-estimate for a sentence length n and for some predicate C with a rangecharacterized by ?
= ?
?l1, r1?, .
.
.
, ?ldim(C), rdim(C)??
where len = ?dim(C)i=1 (ri ?
li) and r =n ?
rdim(C) is then given by the maximal weight of the item [C, len, l1, r, n ?
len ?
l1 ?
r].4.3 SX with LR, Gaps, LengthIn order to further decrease the space complexity of the computation of the outsideestimate, we can simplify the previous estimate by subsuming the two lengths left andright in a single length lr.
The items now have the form [X, len, lr, gaps] with X ?
N,len + lr + gaps ?
lenmax, len ?
dim(X), gaps ?
dim(X) ?
1.The computation is given in Figure 19.
Again, we define leftA, gapsA, rightA andgapsB, rightB for a rule X(?)
?
A( ?A)B( ?B) as before.
Furthermore, in both Binary-leftand Binary-right, we have limited lr in the consequent item to the lr of the antecedentplus the length of the sister (lenB, resp.
lenA).
This results in a further reduction of thenumber of items while having only little effect on the parsing results.The SX-estimate for a sentence length n and for some predicate C with a span?
= ?
?l1, r1?, .
.
.
, ?ldim(C), rdim(C)??
where len = ?dim(C)i=1 (ri ?
li) and r = n ?
rdim(C) is then themaximal weight of [C, len, l1 + r, n ?
len ?
l1 ?
r].POS tags: 0 : [A, 1] A a POS tag Unary:in : [B, l]in + log(p) : [A, l] p : A(?)
?
B(?)
?
PBinary:inB : [B, lB], inC : [C, lC]inB + inC + log(p) : [A, lB + lC]where either p : A( ?A) ?
B( ?B)C( ?C) ?
P or p : A( ?A) ?
C( ?C)B( ?B) ?
P.Figure 18Estimate of the inside score with total span length.103Computational Linguistics Volume 39, Number 1Axiom : 0 : [S, len, 0, 0] 1 ?
len ?
lenmaxUnary:out : [X, len, lr, g]out + log(p) : [A, len, lr, g] p : X(?)
?
A(?)
?
PBinary-right:out : [X, len, lr, g]out + in(A, len ?
lenB) + log(p) : [B, lenB, lrB, gB] p : X(?)
?
A( ?A )B( ?B ) ?
PBinary-left:out : [X, len, lr, g]out + in(B, len ?
lenA) + log(p) : [A, lenA, lrA, gA] p : X(?)
?
A( ?A )B( ?B ) ?
PFurther side conditions for Binary-right:a) len + lr + g = lenB + lrB + gB b) lr < lrB c) gB ?
gapsAFurther side conditions for Binary-left:a) len + lr + g = lenA + lrA + gA b) if rightB = 0 then lr = lrA, else lr < lrA c) gA ?
gapsBFigure 19SX estimate depending on length, LR, gaps.4.4 SX with Span and Sentence LengthWe will now present a further simplification of the last estimate that records only thespan length and the length of the entire sentence.
The items have the form [X, len, slen]with X ?
N, dim(X) ?
len ?
slen.
The computation is given in Figure 20.
This last esti-mate is actually monotonic and allows for true A?
parsing.The SX-estimate for a sentence length n and for some predicate C with a span?
= ?
?l1, r1?, .
.
.
, ?ldim(C), rdim(C)??
where len = ?dim(C)i=1 (ri ?
li) is then the maximal weightof [C, len, n].In order to prove that this estimate allows for monotonic weighted deductive pars-ing and therefore guarantees that the best parse will be found, let us have a look at theCYK deduction rules when being augmented with the estimate.
Only Unary and Binaryare relevant because Scan does not have antecedent items.
The two rules, augmentedwith the outside estimate, are shown in Figure 21.We have to show that for every rule, if this rule has an antecedent item with weightw and a consequent item with weight w?, then w ?
w?.Let us start with Unary.
To show: inB + outB ?
inB + log(p) + outA.
Because of theUnary rule for computing the outside estimate and because of the unary production,Axiom : 0 : [S, len, len] 1 ?
len ?
lenmaxUnary:out : [X, lX, slen]out + log(p) : [A, lX, slen]p : X(?)
?
A(?)
?
PBinary-right:out : [X, lX, slen]out + in(A, lX ?
lB) + log(p) : [B, lB, slen] p : X(?)
?
A( ?A )B( ?B ) ?
PBinary-left:out : [X, lX, slen]out + in(B, lX ?
lA) + log(p) : [A, lA, slen] p : X(?)
?
A( ?A )B( ?B ) ?
PFigure 20SX estimate depending on span and sentence length.104Kallmeyer and Maier PLCFRS ParsingUnary:inB + outB : [B, ?
]inB + log(p) + outA : [A, ?
]p : A(?)
?
B(?)
?
PBinary:inB + outB : [B, ?B], inC + outC : [C, ?C]inB + inC + log(p) + outA : [A, ?A]p : A( ?A ) ?
B( ?B )C( ?C )is an instantiated rule(Here, outA, outB, and outC are the respective outside estimates of [A, ?A], [B, ?B] and [C, ?C].
)Figure 21Parsing rules including outside estimate.we obtain that, given the outside estimate outA of [A, ?]
the outside estimate outB of theitem [B, ?]
is at least outA + log(p), namely, outB ?
log(p) + outA.Now let us consider the rule Binary.
We treat only the relation between the weightof the C antecedent item and the consequent.
The treatment of the antecedent B issymmetric.
To show: inC + outC ?
inB + inC + log(p) + outA.
Assume that lB is the lengthof the components of the B item and n is the sentence length.
Then, because of theBinary-right rule in the computation of the outside estimate and because of our in-stantiated rule p : A( ?A) ?
B( ?B)C( ?C), we have that the outside estimate outC of theC-item is at least outA + in(B, lB) + log(p).
Furthermore, in(B, lB) ?
inB.
ConsequentlyoutC ?
inB + log(p) + outA.4.5 Integration into the ParserBefore parsing, the outside estimates of all items up to a certain maximal sentence lengthlenmax are precomputed.
Then, when performing the weighted deductive parsing asexplained in Section 3.2, whenever a new item is stored in the agenda, we add its outsideestimate to its weight.Because the outside estimate is always greater than or equal to the actual outsidescore, given the input, the weight of an item in the agenda is always greater than orequal to the log of the actual product of the inside and outside score of the item.
In thissense, the outside estimates given earlier are admissible.Additionally, as already mentioned, note that the full SX estimate and the SX esti-mate with span and sentence length are monotonic and allow for A?
parsing.
The othertwo estimates, which are both not monotonic, act as FOMs in a best-first parsing context.Consequently, they contribute to speeding up parsing but they decrease the quality ofthe parsing output.
For further evaluation details see Section 6.5.
Grammars for Discontinuous Constituents5.1 Grammar ExtractionThe algorithm we use for extracting an LCFRS from a constituency treebank with cross-ing branches has originally been presented in Maier and S?gaard (2008).
It interpretsthe treebank trees as LCFRS derivation trees.
Consider for instance the tree in Figure 22.The S node has two daughters, a VMFIN node and a VP node.
This yields a ruleS ?
VP VMFIN.
The VP is discontinuous with two components that wrap around theyield of the VMFIN.
Consequently, the LCFRS rule is S(XYZ) ?
VP(X, Z) VMFIN(Y).The extraction of an LCFRS from treebanks with crossing branches is almost im-mediate, except for the fan-out of the non-terminal categories: In the treebank, we canhave the same non-terminal with different fan-outs, for instance a VP without a gap(fan-out 1), a VP with a single gap (fan-out 2), and so on.
In the corresponding LCFRS,105Computational Linguistics Volume 39, Number 1SVPVPPROAV VMFIN VVPP VAINFdaru?ber mu?
nachgedacht werdenabout it must thought be?It must be thought about it?Figure 22A sample tree from NeGra.we have to distinguish these different non-terminals by mapping them to differentpredicates.The algorithm first creates a so-called lexical clause P(a) ?
?
for each pre-terminalP dominating some terminal a.
Then for all other non-terminals A0 with the childrenA1 ?
?
?Am, a clause A0 ?
A1 ?
?
?Am is created.
The number of components of the A1 ?
?
?Amis the number of discontinuous parts in their yields.
The components of A0 are concate-nations of variables that describe how the discontinuous parts of the yield of A0 areobtained from the yields of its daughters.More precisely, the non-terminals in our LCFRS are all Ak where A is a non-terminallabel in the treebank and k is a possible fan-out for A.
For a given treebank tree ?V, E, r, l?where V is the set of nodes, E ?
V ?
V the set of immediate dominance edges, r ?
Vthe root node, and l : V ?
N ?
T the labeling function, the algorithm constructs thefollowing rules.
Let us assume that w1, .
.
.
, wn are the terminal labels of the leavesin ?V, E, r?
with a linear precedence relation wi ?
wj for 1 ?
i < j ?
n. We introduce avariable Xi for every wi, 1 ?
i ?
n. For every pair of nodes v1, v2 ?
V with ?v2, v2?
?
E, l(v2) ?
T, we addl(v1)(l(v2)) ?
?
to the rules of the grammar.
(We omit the fan-out subscripthere because pre-terminals are always of fan-out 1.
) For every node v ?
V with l(v) = A0 /?
T such that there are exactly mnodes v1, .
.
.
, vm ?
V (m ?
1) with ?v, vi?
?
E and l(vi) = Ai /?
T for all1 ?
i ?
m, we now create a ruleA0(x(0)1 , .
.
.
, x(0)dim(A0 ))?
A1(x(1)1 , .
.
.
, x(1)dim(A1 )) .
.
.Am(x(m)1 , .
.
.
, x(m)dim(Am ))where for the predicate Ai, 0 ?
i ?
m, the following must hold:1.
The concatenation of all arguments of Ai, x(i)1 .
.
.
x(i)dim(Ai )is theconcatenation of all X ?
{Xi | ?vi, v?i?
?
E?
with l(v?i ) = wi} such thatXi precedes Xj if i < j, and2.
a variable Xj with 1 ?
j < n is the right boundary of an argument ofAi if and only if Xj+1 /?
{Xi | ?vi, v?i?
?
E?
with l(v?i ) = wi}, that is, anargument boundary is introduced at each discontinuity.As a further step, in this new rule, all right-hand side arguments of length> 1 are replaced in both sides of the rule with a single new variable.Finally, all non-terminals A in the rule are equipped with an additionalsubscript dim(A), which gives us the final non-terminal in our LCFRS.106Kallmeyer and Maier PLCFRS ParsingPROAV(Daru?ber) ?
?VMFIN(mu?)
?
?VVPP(nachgedacht) ?
?VAINF(werden) ?
?S1(X1X2X3) ?
VP2(X1, X3)VMFIN(X2)VP2(X1, X2X3) ?
VP2(X1, X2)VAINF(X3)VP2(X1, X2) ?
PROAV(X1)VVPP(X2)Figure 23LCFRS rules extracted from the tree in Figure 22.For the tree in Figure 22, the algorithm produces for instance the rules in Figure 23.As standard for PCFG, the probabilities are computed using Maximum LikelihoodEstimation.5.2 Head-Outward BinarizationAs previously mentioned, in contrast to CFG the order of the right-hand side elementsof a rule does not matter for the result of an LCFRS derivation.
Therefore, we can reorderthe right-hand side of a rule before binarizing it.The following, treebank-specific reordering results in a head-outward binarizationwhere the head is the lowest subtree and it is extended by adding first all sisters to itsleft and then all sisters to its right.
It consists of reordering the right-hand side of therules extracted from the treebank such that first, all elements to the right of the head arelisted in reverse order, then all elements to the left of the head in their original order, andthen the head itself.
Figure 24 shows the effect this reordering and binarization has onthe form of the syntactic trees.
In addition to this, we also use a variant of this reorderingTree in NeGra format:SVPNN VMFIN NN AV VAINFdas mu?
man jetzt machenthat must one now do?One has to do that now?Rule extracted for the S node: S(XYZU) ?
VP(X, U)VMFIN(Y)NN(Z)Reordering for head-outward binarization: S(XYZU) ?
NN(Z)VP(X, U)VMFIN(Y)New rules resulting from binarizing this rule:S(XYZ) ?
Sbin1(X, Z)NN(Y) Sbin1(XY, Z) ?
VP(X, Z)VMFIN(Y)Rule extracted for the VP node: VP(X, YZ) ?
NN(X)AV(Y)VAINF(Z)New rules resulting from binarizing this rule:VP(X, Y) ?
NN(X)VPbin1(Y) VPbin1(XY) ?
AV(X)VAINF(Y)Tree after binarization:SSbin1VPVPbin1NN VMFIN NN AV VAINFFigure 24Sample head-outward binarization.107Computational Linguistics Volume 39, Number 1where we add first the sisters to the right and then the ones to the left.
This is what Kleinand Manning (2003b) do.
To mark the heads of phrases, we use the head rules that theStanford parser (Klein and Manning 2003c) uses for NeGra.In all binarizations, there exists the possibility of adding additional unary ruleswhen deriving the head.
This allows for a further factorization.
In the experiments,however, we do not insert unary rules, neither at the highest nor at the lowest newbinarization non-terminal, because this was neither beneficial for parsing times nor forthe parsing results.5.3 Incorporating Additional Context5.3.1 Markovization.
As already mentioned in Section 3.1, a binarization that introducesunique new non-terminals for every single rule that needs to be binarized producesa large amount of non-terminals and fails to capture certain generalizations.
For thisreason, we introduce markovization (Collins 1999; Klein and Manning 2003b).Markovization is achieved by introducing only a single new non-terminal for thenew rules introduced during binarization and adding vertical and horizontal contextfrom the original trees to each occurrence of this new non-terminal.
As vertical context,we add the first v labels on the path from the root node of the tree that we want tobinarize to the root of the entire treebank tree.
The vertical context is collected duringgrammar extraction and then taken into account during binarization of the rules.
Ashorizontal context, during binarization of a rule A(?)
?
A0( ?0) .
.
.Am( ?m), for the newnon-terminal that comprises the right-hand side elements Ai .
.
.Am (for some 1 ?
i ?m), we add the first h elements of Ai, Ai?1, .
.
.
, A0.Figure 25 shows an example of a markovization of the tree from Figure 24 with v = 1and h = 2.
Here, the superscript is the vertical context and the subscript the horizontalcontext of the new non-terminal X.
Note that in this example we have disregarded thefan-out of the context categories.
The VP, for instance, is actually a VP2 because it hasfan-out 2.
For the context symbols, one can either use the categories from the originaltreebank (without fan-out) or the ones from the LCFRS rules (with fan-out).
We chosethe latter approach because it delivered better parsing results.5.3.2 Further Category Splitting.
Grammar annotation (i.e., manual enhancement of an-notation information through category splitting) has previously been successfully usedin parsing German (Versley 2005).
In order to see if such modifications can have abeneficial effect in PLCFRS parsing as well, we perform different category splits on the(unbinarized) NeGra constituency data.We split the category S (?sentence?)
into SRC (?relative clause?)
and S (all othercategories S).
Relative clauses mostly occur in a very specific context, namely, as theSXSVP,NNVPXVPADV,NNNN VMFIN NN ADV VAINFFigure 25Sample markovization with v = 1, h = 2.108Kallmeyer and Maier PLCFRS ParsingTable 1NeGra: Properties of the data with crossing branches.training testnumber of sentences 16,502 1,833average sentence length 14.56 14.62average tree height 4.62 4.72average children per node 2.96 2.94sentences without gaps 12,481 (75.63%) 1,361 (74.25%)sentences with one gap 3,320 (20.12%) 387 (21.11%)sentences with ?
2 gaps 701 (4.25%) 85 (4.64%)maximum gap degree 6 5right part of an NP or a PP.
This splitting should therefore speed up parsing and increaseprecision.
Furthermore, we distinguish NPs by their case.
More precisely, to all nodeswith categories N, we append the grammatical function label to the category label.
Wefinally experiment with the combination of both splits.6.
Experiments6.1 DataOur data source is the NeGra treebank (Skut et al1997).
We create two different datasets for constituency parsing.
For the first one, we start out with the unmodified NeGratreebank and remove all sentences with a length of more than 30 words.
We pre-processthe treebank following common practice (Ku?bler and Penn 2008), attaching all nodeswhich are attached to the virtual root node to nodes within the tree such that, ideally,no new crossing edges are created.
In a second pass, we attach punctuation whichcomes in pairs (parentheses, quotation marks) to the same nodes.
For the second dataset we create a copy of the pre-processed first data set, in which we apply the usualtree transformations for NeGra PCFG parsing (i.e., moving nodes to higher positionsuntil all crossing branches are resolved).
The first 90% of both data sets are used as thetraining set and the remaining 10% as test set.
The first data set is called NeGraLCFRSand the second is called NeGraCFG.Table 1 lists some properties of the training and test (respectively, gold) parts ofNeGraLCFRS, namely, the total number of sentences, the average sentence length, theaverage tree height (the height of a tree being the length of the longest of all pathsfrom the terminals to the root node), and the average number of children per node(excluding terminals).
Furthermore, gap degrees (i.e., the number of gaps in the spansof non-terminal nodes) are listed (Maier and Lichte 2011).Our findings correspond to those of Maier and Lichte except for small differencesdue to the fact that, unlike us, they removed the punctuation from the trees.6.2 Parser ImplementationWe have implemented the CYK parser described in the previous section in a systemcalled rparse.
The implementation is realized in Java.33 rparse is available under the GNU General Public License 2.0 at http://www.phil.hhu.de/rparse.109Computational Linguistics Volume 39, Number 1Table 2NeGraLCFRS: PLCFRS parsing results for different binarizations.Head-driven KM L-to-R Optimal DeterministicLP 74.00 74.00 75.08 74.92 72.40LR 74.24 74.13 74.69 74.88 71.80LF1 74.12 74.07 74.88 74.90 72.10UP 77.09 77.20 77.95 77.77 75.67UR 77.34 77.33 77.54 77.73 75.04UF1 77.22 77.26 77.75 77.75 75.356.3 EvaluationFor the evaluation of the constituency parses, we use an EVALB-style metric.
For a treeover a string w, a single constituency is represented by a tuple ?A, ??
with A being anode label and ?
?
(Pos(w) ?
Pos(w))dim(A).
We compute precision, recall, and F1 basedon these tuples from gold and de-binarized parsed test data from which all categorysplits have been removed.
This metric is equivalent to the corresponding PCFG metricfor dim(A) = 1.
Despite the shortcomings of such a measure (Rehbein and van Genabith2007), it still allows to some extent a comparison to previous work in PCFG parsing (seealso Section 7).
Note that we provide the parser with gold POS tags in all experiments.6.4 Markovization and BinarizationWe use the markovization settings v = 1 and h = 2 for all further experiments.
Thesetting which has been reported to yield the best results for PCFG parsing of NeGra,v = 2 and h = 1 (Rafferty and Manning 2008), required a parsing time which was toohigh.4Table 2 contains the parsing results for NeGraLCFRS using five different binariza-tions: Head-driven and KM are the two head-outward binarizations that use a headchosen on linguistic grounds (described in Section 5.2); L-to-R is another variant inwhich we always choose the rightmost daughter of a node as its head.5 Optimal reordersthe left-hand side such that the fan-out of the binarized rules is optimized (described inSection 3.1.2).
Finally, we also try a deterministic binarization (Deterministic) in whichwe binarize strictly from left to right (i.e., we do not reorder the right-hand sides ofproductions, and choose unique binarization labels).The results of the head-driven binarizations and the optimal binarization lie closetogether; the results for the deterministic binarization are worse.
This indicates that thepresence or absence of markovization has more impact on parsing results than the actualbinarization order.
Furthermore, the non-optimal binarizations did not yield a binarizedgrammar of a higher fan-out than the optimal binarization: For all five binarizations,the fan-out was 7 (caused by a VP interrupted by punctuation).4 Older versions of rparse contained a bug that kept the priority queue from being updated correctly(i.e., during an update, the corresponding node in the priority queue was not moved to its top, andtherefore the best parse was not guaranteed to be found); however, higher parsing speeds were achieved.The current version of rparse implements the update operation correctly, using a Fibonacci queue toensure efficiency (Cormen et al2003).
Thanks to Andreas van Cranenburgh for pointing this out.5 The term head is not used in its proper linguistic sense here.110Kallmeyer and Maier PLCFRS Parsing010020030040050060070015  17  19  21  23  25  27  29items(in1000)sentence lengthHead-drivenKML-to-ROptimalDeterministic5010015020025030035040015  17  19  21  23  25  27  29items(in1000)sentence lengthBaselineNPSNP+S5010015020025030035040015  17  19  21  23  25  27  29items(in1000)sentence lengthOFFLRLNFigure 26NeGraLCFRS: Items for PLCFRS parsing (left-to-right): binarizations, baseline and category splits,and estimates.The different binarizations result in different numbers of items, and therefore allowfor different parsing speeds.
The respective leftmost graph in Figures 26 and 27 showa visual representation of the number of items produced by all binarizations, and thecorresponding parsing times.
Note that when choosing the head with head rules thenumber of items is almost not affected by the choice of adding first the children tothe left of the head and then to the right of the head or vice versa.
The optimal bina-rization produces the best results.
Therefore we will use it in all further experiments,in spite of its higher parsing time.6.5 Baseline Evaluation and Category SplitsTable 3 presents the constituency parsing results for NeGraLCFRS and NeGraCFG, bothwith and without the different category splits.
Recall that NeGraLCFRS has crossingbranches and consequently leads to a PLCFRS of fan-out > 1 whereas NeGraCFG doesnot contain crossing branches and consequently leads to a 1-PLCFRS?in other words,0.111010015  17  19  21  23  25  27  29time(sec.
)sentence lengthHead-drivenKML-to-ROptimalDeterministic0.111010015  17  19  21  23  25  27  29time(sec.
)sentence lengthBaselineNPSNP+S0.111010015  17  19  21  23  25  27  29time(sec.
)sentence lengthOFFLRLNFigure 27NeGraLCFRS: Parsing times for PLCFRS parsing (left-to-right): binarizations, baseline andcategory splits, and estimates (log scale).111Computational Linguistics Volume 39, Number 1Table 3NeGraLCFRS and NeGraCFG: baseline and category splits.w/ category splits w/ category splitsNeGraLCFRS NP S NP ?
S NeGraCFG NP S NP ?
SLP 74.92 75.21 75.81 75.93 76.32 76.79 77.39 77.58LR 74.88 74.95 75.65 75.57 76.36 77.23 77.35 77.99LF1 74.90 75.08 75.73 75.75 76.34 77.01 77.37 77.79UP 77.77 78.16 78.31 78.60 79.12 79.62 79.84 80.09UR 78.73 77.88 78.15 78.22 79.17 80.08 79.80 80.52UF1 77.75 78.02 78.23 78.41 79.14 79.85 79.82 80.30a PCFG.
We evaluate the parser output against the unmodified gold data; that is,before we evaluate the experiments with category splits, we replace all split labels inthe parser output with the corresponding original labels.We take a closer look at the properties of the trees in the parser output forNeGraLCFRS.
Twenty-nine sentences had no parse, therefore, the parser output has 1,804sentences.
The average tree height is 4.72, and the average number of children per node(excluding terminals) is 2.91.
These values are almost identical to the values for the golddata.
As for the gap degree, we get 1,401 sentences with no gaps (1,361 in the gold set),334 with gap degree 1 (387 in the gold set), and 69 with 2 or 3 gaps (85 in the gold set).Even though the difference is only small, one can see that fewer gaps are preferred.
Thisis not surprising, since constituents with many gaps are rare events and therefore endup with a probability which is too low.We see that the quality of the PLCFRS parser output on NeGraLCFRS (which containsmore information than the output of a PCFG parser) does not lag far behind the qualityof the PCFG parsing results on NeGraCFG.
With respect to the category splits, the resultsshow furthermore that category splitting is indeed beneficial for the quality of thePLCFRS parser output.
The gains in speed are particularly visible for sentences witha length greater than 20 words (cf.
the number of produced items and parsing times inFigures 26 and 27 [middle]).6.6 Evaluating Outside EstimatesWe compare the parser performance without estimates (OFF) with its performancewith the estimates described in Sections 4.3 (LR) and 4.4 (LN).Unfortunately, the full estimates seem to be only of theoretical interest because theywere too expensive to compute both in terms of time and space, given the restrictionsimposed by our hardware.
We could, however, compute the LN and the LR estimate.Unlike the LN estimate, which allows for true A?
parsing, the LR estimate lets thequality of the parsing results deteriorate: Compared with the baseline, labeled F1 dropsfrom 74.90 to 73.76 and unlabeled F1 drops from 77.91 to 76.89.
The respective rightmostgraphs in Figures 26 and 27 show the average number of items produced by theparser and the parsing times for different sentence lengths.
The results indicate that theestimates have the desired effect of preventing unnecessary items from being produced.This is reflected in a significantly lower parsing time.The different behavior of the LR and the LN estimate raises the question of thetrade-off between maintaining optimality and obtaining a higher parsing speed.
In112Kallmeyer and Maier PLCFRS Parsingother words, it raises the question of whether techniques such as pruning or coarse-to-fine parsing (Charniak et al2006) would probably be superior to A?
parsing.
A firstimplementation of a coarse-to-fine approach has been presented by van Cranenburgh(2012).
He generates a CFG from the treebank PLCFRS, based on the idea of Barthe?lemyet al(2001).
This grammar, which can be seen as a coarser version of the actual PLCFRS,is then used for pruning of the search space.
The problem that van Cranenburgh tacklesis specific to PLCFRS: His PCFG stage generalizes over the distinction of labels by theirfan-out.
The merit of his work is an enormous increase in efficiency: Sentences with alength of up to 40 words can now be parsed in a reasonable time.
For a comparison ofthe results of van Cranenburgh (2012) with our work, the same version of evaluationparameters would have to be used.
The applicability and effectiveness of other coarse-to-fine approaches (Charniak et al2006; Petrov and Klein 2007) on PLCFRS remain tobe seen.7.
Comparison to Other ApproachesComparing our results with results from the literature is a difficult endeavor, becausePLCFRS parsing of NeGra is an entirely new task that has no direct equivalent inprevious work.
In particular, it is a harder task than PCFG parsing.
What we canprovide in this section is a comparison of the performance of our parser on NeGraCFGto the performance of previously presented PCFG parsers on the same data set andan overview on previous work on parsing which aims at reconstructing crossingbranches.For the comparison of the performance of our parser on NeGraCFG, we have per-formed experiments with Helmut Schmid?s LoPar (Schmid 2000) and with the StanfordParser (Klein and Manning 2003c) on NeGraCFG.6 For the experiments both parserswere provided with gold POS tags.
Recall that our parser produced labeled precision,recall, and F1 of 76.32, 76.46, and 76.34, respectively.
The plain PCFG provided by LoPardelivers lower results (LP 72.86, LR 74.43, and LF1 73.63).
The Stanford Parser results(markovization setting v = 2, h = 1 [Rafferty and Manning 2008], otherwise defaultparameters) lie in the vicinity of the results of our parser (LP 74.27, LR 76.19, LF1 75.45).Although the results for LoPar are no surprise, given the similarity of the modelsimplemented by our parser and the Stanford parser, it remains to be investigated whythe lexicalization component of the Stanford parser does not lead to better results.
Inany case the comparison shows that on a data set without crossing branches, our parserobtains the results one would expect.
A further data set to which we can provide acomparison is the PaGe workshop experimental data (Ku?bler and Penn 2008).7 Table 4lists the results of some of the papers in Ku?bler and Penn (2008) on TIGER, namely,for Petrov and Klein (2008) (P&K), who use the Berkeley Parser (Petrov and Klein2007); Rafferty and Manning (2008) (R&M), who use the Stanford parser (see above);and Hall and Nivre (2008) (H&N), who use a dependency-based approach (see nextparagraph).
The comparison again shows that our system produces good results.
Againthe performance gap between the Stanford parser and our parser warrants furtherinvestigation.6 We have obtained the former parser from http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/LoPar.html and the latter (Version 2.0.1) from http://nlp.stanford.edu/software/lex-parser.shtml.7 Thanks to Sandra Ku?bler for providing us with the experimental data.113Computational Linguistics Volume 39, Number 1Table 4PaGe workshop data.here P&K R&M H&NLP 66.93 69.23 58.52 67.06LR 60.79 70.41 57.63 58.07LF1 63.71 69.81 58.07 65.18As for the work that aims to create crossing branches, Plaehn (2004) obtains 73.16Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeitonly on sentences with a length of up to 15 words.
On those sentences, we obtain 83.97.The crucial difference between DPSG rules and LCFRS rules is that the former explicitlyspecify the material that can occur in gaps whereas LCFRS does not.
Levy (2005), like us,proposes to use LCFRS but does not provide any evaluation results of his work.
Veryrecently, Evang and Kallmeyer (2011) followed up on our work.
They transform thePenn Treebank such that the trace nodes and co-indexations are converted into crossingbranches and parse them with the parser presented in this article, obtaining promisingresults.
Furthermore, van Cranenburgh, Scha, and Sangati (2011) and van Cranenburgh(2012) have also followed up on our work, introducing an integration of our approachwith Data-Oriented Parsing (DOP).
The former article introduces an LCFRS adaptionof Goodman?s PCFG-DOP (Goodman 2003).
For their evaluation, the authors use thesame data as we do in Maier (2010), and obtain an improvement of roughly 1.5 pointsF-measure.
They are also confronted with the same efficiency issues, however, andencounter a bottleneck in terms of parsing time.
In van Cranenburgh (2012), a coarse-to-fine approach is presented (see Section 6.6).
With this approach much faster parsingis made possible and sentences with a length of up to 40 words can be parsed.
The costof the speed, however, is that the results lie well below the baseline results for standardPLCFRS parsing.A comparison with non-projective dependency parsers (McDonald et al2005;Nivre et al2007) might be interesting as well, given that non-projectivity is thedependency-counterpart to discontinuity in constituency parsing.
A meaningful com-parison is difficult to do for the following reasons, however.
Firstly, dependency parsingdeals with relations between words, whereas in our case words are not considered inthe parsing task.
Our grammars take POS tags for a given and construct syntactic trees.Also, dependency conversion algorithms generally depend on the correct identificationof linguistic head words (Lin 1995).
We cannot rely on grammatical function labels, suchas, for example, Boyd and Meurers (2008).
Therefore we would have to use heuristics forthe dependency conversion of the parser output.
This would introduce additional noise.Secondly, the resources one obtains from our PLCFRS parser and from dependencyparsers (the probabilistic LCFRS and the trained dependency parser) are quite differentbecause the former contains non-lexicalized internal phrase structure identifying mean-ingful syntactic categories such as VP or NP while the latter is only concerned with rela-tions between lexical items.
A comparison would concentrate only on relations betweenlexical items and the rich phrase structure provided by a constituency parser wouldnot be taken into account.
To achieve some comparison, one could of course transformthe discontinuous constituency trees into dependency trees with dependencies betweenheads and with edge labels that encode enough of the syntactic structure to retrievethe original constituency tree (Hall and Nivre 2008).
The result could then be used for114Kallmeyer and Maier PLCFRS Parsinga dependency evaluation.
It is not clear what is to gain by this evaluation becausethe head-to-head dependencies one would obtain are not necessarily the predicate-argument dependencies one would aim at when doing direct dependency parsing(Rambow 2010).88.
ConclusionWe have presented the first efficient implementation of a weighted deductive CYKparser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), showingthat LCFRS indeed allows for data-driven parsing while modeling discontinuities ina straightforward way.
To speed up parsing, we have introduced different context-summary estimates of parse items, some acting as figures-of-merit, others allowing forA?
parsing.
We have implemented the parser and we have evaluated it with grammarsextracted from the German NeGra treebank.
Our experiments show that data-drivenLCFRS parsing is feasible and yields output of competitive quality.There are three main directions for future work on this subject. On the symbolic side, LCFRS seems to offer more power than necessary.By removing symbolic expressivity, a lower parsing complexity can beachieved.
One possibility is to disallow the use of so-called ill-nestedLCFRS rules.
These are rules where, roughly, the spans of two right-handside non-terminals interleave in a cross-serial way.
See the parsingalgorithm in Go?mez-Rodr?
?guez, Kuhlmann, and Satta (2010).Nevertheless, this seems to be too restrictive for linguistic modeling(Chen-Main and Joshi 2010; Maier and Lichte 2011).
Our goal for futurework is therefore to define reduced forms of ill-nested rules with which weget a lower parsing complexity.Another possibility is to reduce the fan-out of the extracted grammar.
Wehave pursued the question whether the fan-out of the trees in the treebankcan be reduced in a linguistically meaningful way in Maier, Kaeshammer,and Kallmeyer (2012). On the side of the probabilistic model, there are certain independenceassumptions made in our model that are too strong.
The main problem inrespect is that, due to the definition of LCFRS, we have to distinguishbetween occurrences of the same category with different fan-outs.
Forinstance, VP1 (no gaps), VP2 (one gap), and so on, are differentnon-terminals.
Consequently, the way they expand are consideredindependent from each other.
This is of course not true, however.Furthermore, some of these non-terminals are rather rare; we thereforehave a sparse data problem here.
This leads to the idea to separate thedevelopment of a category (independent from its fan-out) and the fan-outand position of gaps.
We plan to integrate this into our probabilistic modelin future work.8 A way to overcome this difference in the content of the dependency annotation would be to usean evaluation along the lines of Tsarfaty, Nivre, and Andersson (2011); this is not available yet forannotations with crossing branches, however.115Computational Linguistics Volume 39, Number 1 Last, it is clear that a more informative evaluation of the parser output isstill necessary, particularly with respect to its performance at the task offinding long distance dependencies and with respect to its behavior whennot provided with gold POS tags.AcknowledgmentsWe are particularly grateful to Giorgio Sattafor extensive discussions of the details of theprobabilistic treebank model presented inthis paper.
Furthermore, we owe a debt toKilian Evang who participated in theimplementation of the parser.
Thanks toAndreas van Cranenburgh for helpfulfeedback on the parser implementation.Finally, we are grateful to our threeanonymous reviewers for many valuableand helpful comments and suggestions.A part of the work on this paper was fundedby the German Research Foundation DFG(Deutsche Forschungsgemeinschaft) in theform of an Emmy Noether Grant and asubsequent DFG research project.ReferencesBarthe?lemy, Franc?ois, Pierre Boullier,Philippe Deschamp, and E?ric Villemontede la Clergerie.
2001.
Guided parsingof range concatenation languages.In Proceedings of the 39th Annual Meetingof the Association for ComputationalLinguistics, pages 42?49, Toulouse.Becker, Tilman, Aravind K. Joshi, andOwen Rambow.
1991.
Long-distancescrambling and tree-adjoining grammars.In Proceedings of the Fifth Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 21?26,Berlin.Boullier, Pierre.
1998a.
A generalization ofmildly context-sensitive formalisms.In Proceedings of the Fourth InternationalWorkshop on Tree Adjoining Grammars andRelated Formalisms (TAG+4), pages 17?20,Philadelphia, PA.Boullier, Pierre.
1998b.
A proposal for anatural language processing syntacticbackbone.
Technical Report 3342, INRIA,Roquencourt.Boullier, Pierre.
2000.
Range concatenationgrammars.
In Proceedings of the SixthInternational Workshop on ParsingTechnologies (IWPT2000), pages 53?64,Trento.Boyd, Adriane.
2007.
Discontinuity revisited:An improved conversion to context-freerepresentations.
In the LinguisticAnnotation Workshop at ACL 2007,pages 41?44, Prague.Boyd, Adriane and Detmar Meurers.2008.
Revisiting the impact of differentannotation schemes on PCFG parsing:A grammatical dependency evaluation.In Proceedings of the Workshop on ParsingGerman at ACL 2008, pages 24?32,Columbus, OH.Brants, Sabine, Stefanie Dipper, SilviaHansen, Wolfgang Lezius, and GeorgeSmith.
2002.
The TIGER Treebank.In Proceedings of the 1st Workshop onTreebanks and Linguistic Theories,pages 24?42, Sozopol.Bunt, Harry.
1996.
Formal tools fordescribing and processing discontinuousconstituency structure.
In Harry Bunt andArthur van Horck, editors, DiscontinuousConstituency, volume 6 of Natural LanguageProcessing.
Mouton de Gruyter, Berlin,pages 63?83.Cai, Shu, David Chiang, and Yoav Goldberg.2011.
Language-independent parsingwith empty elements.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies, pages 212?216, Portland, OR.Candito, Marie and Djame?
Seddah.
2010.Parsing word clusters.
In Proceedings of theFirst Workshop on Statistical Parsing ofMorphologically-Rich Languages at NAACLHLT 2010, pages 76?84, Los Angeles, CA.Caraballo, Sharon A. and Eugene Charniak.1998.
New figures of merit for best-firstprobabilistic chart parsing.
ComputationalLinguistics, 24(2):275?298.Charniak, Eugene, Mark Johnson, MichaElsner, Joseph Austerweil, David Ellis,Isaac Haxton, Catherine Hill, R. Shrivaths,Jeremy Moore, Michael Pozar, and TheresaVu.
2006.
Multilevel coarse-to-fine PCFGparsing.
In Proceedings of the HumanLanguage Technology Conference of theNAACL, Main Conference, pages 168?175,New York, NY.Chen-Main, Joan and Aravind Joshi.
2010.Unavoidable ill-nestedness in naturallanguage and the adequacy of treelocal-MCTAG induced dependencystructures.
In Proceedings of the Tenth116Kallmeyer and Maier PLCFRS ParsingInternational Workshop on Tree AdjoiningGrammar and Related Formalisms (TAG+10),pages 119?126, New Haven, CT.Chiang, David.
2003.
Statistical parsing withan automatically extracted tree adjoininggrammar.
In Rens Bod, Remko Scha, andKhalil Sima?an, editors, Data-OrientedParsing, CSLI Studies in ComputationalLinguistics.
CSLI Publications, Stanford,CA, pages 299?316.Collins, Michael.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.Cormen, Thomas H., Charles E. Leiserson,Ronald L. Rivest, and Clifford Stein.
2003.Introduction to Algorithms.
MIT Press,Cambridge, 2nd edition.Dienes, Pe?ter.
2003.
Statistical Parsing withNon-local Dependencies.
Ph.D. thesis,Saarland University.Evang, Kilian and Laura Kallmeyer.
2011.PLCFRS parsing of English discontinuousconstituents.
In Proceedings of the 12thInternational Conference on ParsingTechnologies, pages 104?116, Dublin.Go?mez-Rodr?
?guez, Carlos, MarcoKuhlmann, and Giorgio Satta.
2010.Efficient parsing of well-nested linearcontext-free rewriting systems.
In HumanLanguage Technologies: The 2010 AnnualConference of the North American Chapter ofthe Association for Computational Linguistics,pages 276?284, Los Angeles, CA.Go?mez-Rodr?
?guez, Carlos, MarcoKuhlmann, Giorgio Satta, and David Weir.2009.
Optimal reduction of rule length inlinear context-free rewriting systems.
InProceedings of Human Language Technologies:The 2009 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 539?547,Boulder, CO.Goodman, Joshua.
2003.
Efficient parsing ofDOP with PCFG-reductions.
In Rens Bod,Remko Scha, and Khalil Sima?an, editors,Data-Oriented Parsing, CSLI Studies inComputational Linguistics.
CSLIPublications, Stanford, CA, pages 125?146.Hall, Johan and Joakim Nivre.
2008.A dependency-driven parser forGerman dependency and constituencyrepresentations.
In Proceedings of theWorkshop on Parsing German at ACL 2008,pages 47?54, Columbus, OH.Han, Chung-hye, Na-Rae Han, andEon-Suk Ko.
2001.
Bracketing guidelinesfor Penn Korean TreeBank.
TechnicalReport 01-10, IRCS, University ofPennsylvania, Philadelphia, PA.Hockenmaier, Julia.
2003.
Data and modelsfor Statistical Parsing with CombinatoryCategorial Grammar.
Ph.D. thesis,University of Edinburgh.Hoffman, Beryl.
1995.
Integrating ?free?word order syntax and informationstructure.
In Seventh Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 245?251,Dublin.Ho?hle, Tilman.
1986.
Der Begriff?Mittelfeld?
?Anmerkungen u?ber dieTheorie der topologischen Felder.In Akten des Siebten InternationalenGermanistenkongresses 1985, Go?ttingen,Germany.Johnson, Mark.
2002.
A simplepattern-matching algorithm for recoveringempty nodes and their antecedents.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics,pages 136?143, Philadelphia, PA.Kallmeyer, Laura.
2010.
Parsing BeyondContext-Free Grammars.
Springer, Berlin.Kallmeyer, Laura and Wolfgang Maier.
2010.Data-driven parsing with probabilisticlinear context-free rewriting systems.In Proceedings of the 23rd InternationalConference on Computational Linguistics(COLING 2010), pages 537?545, Beijing.Kato, Yuki, Hiroyuki Seki, and TadaoKasami.
2006.
Stochastic multiplecontext-free grammar for RNApseudoknot modeling.
In Proceedingsof the Eighth International Workshopon Tree Adjoining Grammar and RelatedFormalisms (TAG+8), pages 57?64,Sydney.Klein, Dan and Christopher D. Manning.2003a.
A* Parsing: Fast exact viterbi parseselection.
In Proceedings of the 2003 HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 40?47,Edmonton.Klein, Dan and Christopher D. Manning.2003b.
Accurate unlexicalized parsing.In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics,pages 423?430, Sapporo.Klein, Dan and Christopher D. Manning.2003c.
Fast exact inference with a factoredmodel for natural language parsing.
InAdvances in Neural Information ProcessingSystems 15 (NIPS), pages 3?10, Vancouver.Kracht, Marcus.
2003.
The Mathematicsof Language.
Number 63 in Studies inGenerative Grammar.
Mouton de Gruyter,Berlin.117Computational Linguistics Volume 39, Number 1Ku?bler, Sandra.
2005.
How do treebankannotation schemes influence parsingresults?
Or how not to compare applesand oranges.
In Recent Advances in NaturalLanguage Processing 2005 (RANLP 2005),pages 293?300, Borovets.Ku?bler, Sandra and Gerald Penn, editors.2008.
Proceedings of the Workshop onParsing German at ACL 2008.
Associationfor Computational Linguistics,Columbus, OH.Kuhlmann, Marco and Giorgio Satta.2009.
Treebank grammar techniques fornon-projective dependency parsing.In Proceedings of the 12th Conferenceof the European Chapter of the Associationfor Computational Linguistics,pages 478?486, Athens.Levy, Roger.
2005.
Probabilistic Models ofWord Order and Syntactic Discontinuity.Ph.D.
thesis, Stanford University.Levy, Roger and Christopher D. Manning.2004.
Deep dependencies from context-freestatistical parsers: Correcting the surfacedependency approximation.
In Proceedingsof the 42nd Meeting of the Association forComputational Linguistics (ACL?04), MainVolume, pages 328?335, Barcelona.Lin, Dekang.
1995.
A dependency-basedmethod for evaluating broad-coverageparsers.
In Proceedings of the 14thInternational Joint Conference on ArtificialIntelligence (IJCAI 95), pages 1420?1427,Montreal.Maier, Wolfgang.
2010.
Direct parsing ofdiscontinuous constituents in German.In Proceedings of the First Workshop onStatistical Parsing of Morphologically-RichLanguages at NAACL HLT 2010,pages 58?66, Los Angeles, CA.Maier, Wolfgang, Miriam Kaeshammer,and Laura Kallmeyer.
2012.
Data-drivenPLCFRS parsing revisited: Restrictingthe fan-out to two.
In Proceedings of theEleventh International Conference on TreeAdjoining Grammars and Related Formalisms(TAG+11), pages 126?134, Paris.Maier, Wolfgang and Laura Kallmeyer.
2010.Discontinuity and non-projectivity: Usingmildly context-sensitive formalisms fordata-driven parsing.
In Proceedings of theTenth International Workshop on TreeAdjoining Grammars and RelatedFormalisms (TAG+10), New Haven, CT.Maier, Wolfgang and Timm Lichte.
2011.Characterizing discontinuity in constituenttreebanks.
In Formal Grammar.
14thInternational Conference, FG 2009.
Bordeaux,France, July 25-26, 2009.
Revised SelectedPapers, volume 5591 of Lecture Notes inArtificial Intelligence, pages 167?182,Springer-Verlag, Berlin/Heidelberg/New York.Maier, Wolfgang and Anders S?gaard.
2008.Treebanks and mild context-sensitivity.In Proceedings of the 13th Conference onFormal Grammar (FG-2008), pages 61?76,Hamburg.Marcus, Mitchell, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre,Ann Bies, Mark Ferguson, Karen Katz,and Britta Schasberger.
1994.
The PennTreebank: Annotating predicate argumentstructure.
In Proceedings of the HumanLanguage Technology Conference,pages 114?119.McDonald, Ryan, Fernando Pereira,Kiril Ribarov, and Jan Hajic?.
2005.Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings ofHuman Language Technology Conference andConference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP),pages 523?530, Vancouver.Michaelis, Jens.
2001.
On Formal Propertiesof Minimalist Grammars.
Ph.D. thesis,Universita?t Potsdam.Mu?ller, Gereon.
2002.
Free word order,morphological case, and sympathy theory.In Gisbert Fanselow and Caroline Fery,editors, Resolving Conflicts in Grammars:Optimality Theory in Syntax, Morphology,and Phonology.
Buske Verlag, Hamburg,pages 265?397.Mu?ller, Stefan.
2004.
Continuous ordiscontinuous constituents?
Research onLanguage & Computation, 2(2):209?257.Nederhof, Mark-Jan. 2003.
Weighteddeductive parsing and knuth?s algorithm.Computational Linguistics, 29(1):135?143.Nivre, Joakim, Johan Hall, Jens Nilsson,Atanas Chanev, Gu?lsen Eryigit,Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser:A language-independent system fordata-driven dependency parsing.
NaturalLanguage Engineering, 13(2):95?135.Osenova, Petya and Kiril Simov.
2004.BTB-TR05: BulTreebank Stylebook.Technical Report 05, BulTreeBankProject, Sofia, Bulgaria.Pereira, Fernando C. N. and David Warren.1983.
Parsing as deduction.
In Proceedingsof the 21st Annual Meeting of the Associationfor Computational Linguistics, pages 137?144,Cambridge, MA.Petrov, Slav and Dan Klein.
2007.
Improvedinference for unlexicalized parsing.118Kallmeyer and Maier PLCFRS ParsingIn Human Language Technologies 2007: TheConference of the North American Chapter ofthe Association for Computational Linguistics;Proceedings of the Main Conference,pages 404?411, Rochester, NY.Petrov, Slav and Dan Klein.
2008.
ParsingGerman with latent variable grammars.In Proceedings of the Workshop on ParsingGerman at ACL 2008, pages 24?32,Columbus, OH.Plaehn, Oliver.
2004.
Computing the mostprobable parse for a discontinuousphrase-structure grammar.
In Harry Bunt,John Carroll, and Giorgio Satta, editors,New Developments in Parsing Technology,volume 23 of Text, Speech And LanguageTechnology.
Kluwer, Dordrecht,pages 91?106.Rafferty, Anna and Christopher D. Manning.2008.
Parsing three German treebanks:Lexicalized and unlexicalized baselines.In Proceedings of the Workshop on ParsingGerman at ACL 2008, pages 40?46,Columbus, OH.Rambow, Owen.
2010.
The simpletruth about dependency and phrasestructure representations: An opinionpiece.
In Human Language Technologies:The 2010 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 337?340,Los Angeles, CA.Rehbein, Ines and Josef van Genabith.2007.
Evaluating evaluation measures.In Proceedings of the 16th NordicConference of Computational Linguistics,pages 372?379, Tartu.Schmid, Helmut.
2000.
LoPar: Design andimplementation.
Arbeitspapiere desSonderforschungsbereiches 340 149,IMS, University of Stuttgart, Stuttgart,Germany.Seddah, Djame, Sandra Ku?bler, and ReutTsarfaty, editors.
2010.
Proceedings of theFirst Workshop on Statistical Parsing ofMorphologically-Rich Languages atNAACL HLT 2010.
Association forComputational Linguistics,Los Angeles, CA.Seki, Hiroyuki, Takahashi Matsumura,Mamoru Fujii, and Tadao Kasami.
1991.On multiple context-free grammars.Theoretical Computer Science, 88(2):191?229.Shieber, Stuart M., Yves Schabes, andFernando C. N. Pereira.
1995.
Principlesand implementation of deductive parsing.Journal of Logic Programming, 24(1?2):3?36.Sikkel, Klaas.
1997.
Parsing Schemata.
Texts inTheoretical Computer Science.
Springer,Berlin, Heidelberg, New York.Skut, Wojciech, Brigitte Krenn, ThortenBrants, and Hans Uszkoreit.
1997.
Anannotation scheme for free word orderlanguages.
In Proceedings of the FifthConference on Applied Natural LanguageProcessing (ANLP), pages 88?95,Washington, DC.Telljohann, Heike, Erhard W. Hinrichs,Sandra Ku?bler, Heike Zinsmeister, andKathrin Beck.
2012.
Stylebook for theTu?bingen Treebank of Written German(Tu?Ba-D/Z).
Technical report, Seminar fu?rSprachwissenschaft, Universita?t Tu?bingen,Tu?bingen, Germany.
http://www.sfs.uni.tuebingen.de/resources/tuebadz-stylebook-1201.pdf.Tsarfaty, Reut, Joakim Nivre, and EvelinaAndersson.
2011.
Evaluating dependencyparsing: Robust and heuristics-freecross-annotation evaluation.
In Proceedingsof the 2011 Conference on Empirical Methodsin Natural Language Processing,pages 385?396, Edinburgh.Uszkoreit, Hans.
1986.
Linear precedencein discontinuous constituents: Complexfronting in German.
CSLI reportCSLI-86-47, Center for the Study ofLanguage and Information, StanfordUniversity, Stanford, CA.van Cranenburgh, Andreas.
2012.
Efficientparsing with linear context-free rewritingsystems.
In Proceedings of the 13thConference of the European Chapter of theAssociation for Computational Linguistics,pages 460?470, Avignon.van Cranenburgh, Andreas, Remko Scha,and Federico Sangati.
2011.
Discontinuousdata-oriented parsing: A mildlycontext-sensitive all-fragments grammar.In Proceedings of the Second Workshop onStatistical Parsing of Morphologically RichLanguages (SPMRL 2011), pages 34?44,Dublin.Versley, Yannick.
2005.
Parser evaluationacross text types.
In Proceedings of theFourth Workshop on Treebanks and LinguisticTheories, pages 209?220, Barcelona, Spain.Vijay-Shanker, K., David J. Weir, andAravind K. Joshi.
1987.
Characterizingstructural descriptions produced byvarious grammatical formalisms.
InProceedings of the 25th Annual Meeting of theAssociation for Computational Linguistics,pages 104?111, Stanford, CA.119
