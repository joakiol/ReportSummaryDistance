Tagging accurate ly -  Don't guess if you knowPasi TapanainenRank Xerox Research CentreGrenoble Laboratory6, chemin de Maupertuis38240 Meylan, FrancePasi.Tapanainen@xerox.frAtro Vouti lainenResearch Unit for Computational LinguisticsUniversity of HelsinkiP.O.
Box 400014 University of Helsinki, FinlandAtro.Voutilainen@helsinki.fiAbstractWe discuss combining knowledge-based (orrule-based) and statistical part-of-speechtaggers.
We use two mature taggers,ENGCG and Xerox Tagger, to indepen-dently tag the same text and combine theresults to produce a fully disambiguatedtext.
In a 27000 word test sample takenfrom a previously unseen corpus we achieve98.5 % accuracy.
This paper presents thedata in detail.
We describe the problemswe encountered in the course of combiningthe two taggers and discuss the problem ofevaluating taggers.1 IntroductionThis paper combines knowledge-based and statisti-cal methods for part-of-speech disambiguation, tak-ing advantage of the best features of both ap-proaches.
The resulting output is fully and accu-rately disambiguated.We demonstrate a system that accurately resolvesmost part-of-speech ambiguities by means of syntac-tic rules and employs a stochastic tagger to elimi-nate the remaining ambiguity.
The overall resultsare clearly superior to the reported results for state-of-the-art stochastic systems.The input to our part-of-speech disambiguatorconsists of lexically analysed sentences.
Many wordshave more than one analysis.
The task of the dis-ambiguator is to select the contextually appropriatealternative by discarding the improper ones.Some of the inappropriate alternatives can be dis-carded reliably by linguistic rules.
For example, wecan safely exclude a finite-verb reading if the previ-ous word is an unambiguous determiner.
The appli-cation of such rules does not always result in a fullydisambiguated output (e.g.
adjective-noun ambigui-ties may be left pending) but the amount of ambigu-ity is reduced with next to no errors.
Using a largecollection of linguistic rules, a lot of ambiguity canbe resolved, though some cases remain unresolved.47The rule system may also exploit the fact that cer-tain linguistically possible configurations have sucha low frequency in certain types of text that theycan be ignored.
A rule that assumes that a prepo-sition is followed by a noun phrase may be a usefulheuristic rule in a practical system, considering thatdangling prepositions occur relatively infrequently.Such heuristic rules can be applied to resolve some ofthe ambiguities that survive the more reliable gram-mar rules.A stochastic disambiguator selects the most likelytag for a word by consulting the neighbouring tagsor words, typically in a two or three word window.Because of the limited size of the window, the choicesmade by a stochastic disambiguator a e often quitenaive from the linguistic point of view.
For instance,the correct resolution of a preposition vs. subordi-nating conjunction ambiguity in a small window isoften impossible because both morphological cate-gories can have identical local contexts (for instance,both can be followed by a noun phrase).
Some of theerrors made by a stochastic system can be avoidedin a knowledge-based system because the rules canrefer to words and tags in the scope of the entiresentence.We use both types of disambiguators.
Theknowledge-based disambiguator does not resolve allambiguities but the choices it makes are nearly al-ways correct.
The statistical disambiguator resolvesall ambiguities but its decisions are not very reliable.We combine these two disambiguators; here thismeans that the text is analysed with both systems.Whenever there is a conflict between the systems,we trust the analysis proposed by the knowledge-based system.
Whenever the knowledge-based sys-tem leaves an ambiguity unresolved, we select thatalternative which is closest o the selection made bythe statistical system.The two systems we use are ENGCG (Karlssonet al, 1994) and the Xerox Tagger (Cutting et al,1992).
We discuss problems caused by the fact thatthese taggers use different ag sets, and present heresults obtained by applying the combined taggersto a previously unseen sample of text.2 The taggers in outline2.1 English Constraint Grammar ParserThe English Constraint Grammar Parser, ENGCG(Voutilainen et al, 1992; Karlsson el al., 1994), isbased on Constraint Grammar, aparsing frameworkproposed by Fred Karlsson (1990).
It was developed1989-1993 at the Research Unit for ComputationalLinguistics, University of Helsinki, by Atro Voutilai-nen, Juha Heikkil~i and Arto Anttila; later on, Ti-mo J?rvinen has extended the syntactic description,and Pasi Tapanainen has made a new fast imple-mentation of the CG parsing program.
ENGCG isprimarily designed for the analysis of standard writ-ten English of the British and American varieties.In the development and testing of the system, over100 million words of running text have been used.The ENGTWOL lexicon is based on the two-levelmodel (Koskenniemi, 1983).
The lexicon containsover 80,000 lexical entries, each of which representsall inflected and central derived forms of the lexemes.The lexicon also employs a collection of tags for partof speech, inflection, derivation and even syntacticcategory (e.g.
verb classification).Usually less than 5 % of all word-form tokens inrunning text are not recognised by the morphologicalanalyser.
Therefore the system employs a rule-basedheuristic module that provides all unknown wordswith one or more readings.
About 99.5 % of wordsnot recognised by the ENGTWOL analyser itself geta correct analysis from the heuristic module.
Themodule contains a list of prefixes and suffixes, andpossible analyses for matching words.
For instance,words beginning with un...  and ending in ...al aremarked as adjectives.The grammar for morphological disambiguation(Voutilainen, 1994) is based on 23 linguistic gen-eralisations about the form and function of essen-tially syntactic onstructions, e.g.
the form of thenoun phrase, prepositional phrase, and finite verbchain.
These generalisations are expressed as 1,100highly reliable 'grammar-based' and some 200 lessreliable add-on 'heuristic' constraints, usually in apartial and negative fashion.
Using the 1,100 bestconstraints results in a somewhat ambiguous out-put.
Usually there are about 1.04-1.07 morpholog-ical analyses per word.
Usually at least 997 wordsout of every thousand retain the contextually appro-priate morphological reading, i.e.
the recall usuallyis at least 99.7 %.
If the heuristic constraints arealso used, the ambiguity rate falls to 1.02-1.04 read-ings per word, with an overall recall of about 99.5 %.This accuracy compares very favourably with resultsreported in (de Marcken, 1990; Weisehedel et al,1993; Kempe, 1994) - for instance, to reach the recallof 99.3 %, the system by (Weischedel et al, 1993)has to leave as many as three readings per word inits output.2.2 Xerox TaggerThe Xerox Tagger 1, XT, (Cutting et al, 1992) isa statistical tagger made by Doug Cutting, JulianKupiec, Jan Pedersen and Penelope Sibun in XeroxPARC.
It was trained on the untagged Brown Cor-pus (Francis and Kubera, 1982).The lexicon is a word-list of 50,000 words with al-ternative tags.
Unknown words are analysed accord-ing to their suffixes.
The lexicon and suffix tables areimplemented astries.
For instance, for the word livethere are the following alternative analyses: J J  (ad-jective) and VB (uninflected verb).
Unknown wordsnot recognised by suffix tables get al tags from aspecific set (called open-class).The tagger itself is based on the Hidden MarkovModel (Baum, 1972) and word equivalence classes(Kupiec, 1989).
Although the tagger is trained withthe untagged Brown corpus, there are several waysto 'force' it to learn.?
The symbol biases represent a kind of lexicalprobabilities for given word equivalence classes.?
The transition biases can be used for saying thatit is likely or unlikely that a tag is followed bysome specific tag.
The biases serve as defaultvalues for the Hidden Markov Model before thetraining.?
Some rare readings may be removed from thelexicon to prevent the tagger from selectingthem.?
There are some training parameters, like thenumber of iterations (how many times the sameblock of text is used in training) and the size ofthe block of the text used for training.?
The choice of the training corpus affects the re-sult.The tagger is reported (Cutting el al., 1992) tohave a better than 96 % accuracy in the analysis ofparts of the Brown Corpus.
The accuracy is similarto other probabilistic taggers.3 Grammatical representations ofthe taggersA major difference between a knowledge-based anda probabilistic tagger is that the knowledge-basedtagger needs as much information as possible whilethe probabilistic tagger equires ome compact setof tags that does not make too many distinctionsbetween similar words.
The difference can be seenby comparing the Brown Corpus tag set (used byXT) with the ENGCG tag set.The ENGTWOL morphological nalyser employs139 tags.
Each word usually receives several tags(see Figure 1).
There are also 'auxiliary' tags forderivational nd syntactic information that do not1 We use version 1.48ENGCGhas V PRES SG3 VEINhave V PRES -SG3 VEINV INFV IMP VFINV SUBJUNCTIVE VFINwas V PAST SG1,3 VEINdo V PRES -SG3 VEINV INFV IMP VEINV SUBJUNCTIVE VEINdone PCP2cookcoolV PRES -SG3 VEINV INFV IMP VEINV SUBJUNCTIVE VEINN NOM SGV PRES -SG3 VFINV INFV IMP VFINV SUBJUNCTIVE VEINA ABScooled PCP2V PAST VEINcooling PCP1JXT\[hvzhvbedzdovbnvbnnvbJJnnrbvbnvbdvbgnnFigure 1: Some morphological mbiguities for verbs.increase morphological mbiguity but serve as addi-tional information for rules.
If these auxiliary tagsare ignored, the morphological nalyser producesabout 180 different ag combinations.The XT lexicon contains 94 tags for words; 15 ofthem are assigned unambiguously to only one word.There are 32 verb tags: 8 tags for have, 13 for be, 6for do and 5 tags for other verbs.
ENGCG does notmake a distinction in the tagset between words have,be, do and the other verbs.
To see the difference withENGCG, see Figure 1.The ENGCG description differs from the BrownCorpus tag set in the following respects.
ENGCG ismore distinctive in that a part of speech distinctionis spelled out (see Figure 2) in the description of?
determiner-pronoun homographs,?
preposition-conjunction h mographs,?
determiner-adverb-pronoun h mographs, and?
uninflected verb forms (see Figure 1), which arerepresented as ambiguous due to the subjunc-tive, imperative, infinitive and present tensereadings.On the other hand, ENGCG does not spell out part-of-speech ambiguity in the description of?
-ing and nonfinite -ed forms,~ Two most probable ENGCG tags (%)cs cs (70 %)PREP (28 %)DT DET DEM SG (48 %)PRON DEM SG (27 %)DTI DET SG/PL (68 %)PRON SG/PL (28 %)IN PI~EP (99 %)ADV (0.5 %)JJ A ABS (93 %)N NOMSG (3 %)NN N NOM SG (88 %)N NOM SG/PL (7 %)NP N NOM SG (80 %)NNOMPL(7%)VB VINF (84 %)V PRES -SG3 VEIN (12 %)* NEG-PART (100 %)Figure 2: Some mappings from the Brown Corpusto the ENGCG tagset.?
noun-adjective homographs when the coremeanings of the adjective and noun readings aresimilar,?
ambiguities due to proper nouns, commonnouns and abbreviations.4 Combin ing  the  taggersIn our approach we apply ENGCG and XT indepen-dently.
Combining the taggers means aligning theoutputs of the taggers and transforming the resultof one tagger to that of the other.Aligning the output is straightforward: we onlyneed to match the word forms in the output of thetaggers.
Some minor problems occur when tokeni-sation is done differently.
For instance, XT handleswords like aren't as a single token, when ENGCGdivides it to two tokens, are and not.
Also ENGCGrecognises ome multiple word phrases like in spiteof as one token, while XT handles it as three tokens.We do not need to map both Brown tags toENGCG and vice versa.
It is enough to transformENGCG tags to Brown tags and select he tag thatXT has produced, or transform the tag of XT intoENGCG tags.
We do the latter because the ENGCGtags contain more information.
This is likely to bedesirable in the design of potential applications.There are a couple of problems in mapping:?
Difference in distinctiveness.
Sometimes ENG-TWOL makes a distinction ot made by theBrown tagset; sometimes the Brown tagsetmakes a distinction ot made by ENGTWOL(see Figure 2).?
Sometimes tags are used in a different way.
A49case in point is the word as.
In a sample of 76instances of as from the tagged Brown corpus,73 are analysed as CS; two as QL and one asIN, while in the ENGCG description the sameinstances of as were analysed 15 times as CS,four times as ADV, and 57 times as PREP.In ENGCG, the tag CS represents subordinat-ing conjunctions.
In the following sentencesthe correct analysis for word as in ENGCG isPREP, not CS, which the Brown corpus sug-gests.The city purchasing department, hejury said, is lacking in experiencedclerical personnel as(CS) a result ofcity personnel policies.
- -  The pe-tition listed the mayor's occupationas(CS) attorney and his age as(CS) 71.It listed his wife's age as(CS) 74 andplace of birth as(CS) Opelika, Ala.The sentences are the three first sentenceswhere word as appears in Brown corpus.
In theBrown Corpus as appears over 7000 times and itis the fourteenth most common word.
BecauseXT is trained according to the Brown Corpus,this is likely to cause problems.XT is applied independently to the text, and thetagger's prediction is consulted in the analysis ofthose words where ENGCG is unable to make aunique prediction.
The system selects the ENGCGmorphological reading that most closely correspondsto the tag proposed by XT.The mapping scheme is the following.
For eachBrown Corpus tag, there is a decision list for possibleENGCG tags, the most probable one first.
We havecomputed the decision list from the part of BrownCorpus that is also manually tagged according to theENGCG grammatical representation.
The mappingcan be used in two different ways.?
Careful mode: An ambiguous reading in theoutput of ENGCG may be removed only whenit is not in the decision list.
In practise thisleaves quite much ambiguity.?
Unambiguous mode: Select the reading in theoutput of ENGCG that comes first in the deci-sion list 2.5 Per fo rmance  tes t5.1 Test  dataThe system was tested against 26,711 words ofnewspaper text from The Wall Street Journal, TheEconomist and Today, all taken from the 200-millionword Bank of English corpus by the COBUILD teamat the University of Birmingham, England (see also(J/irvinen, 1994)).
None of these texts have been2In some cases a word may still remain ambiguous.used in the development of the system or the de-scription, i.e.
no training effects are to be expected.5.2 Creat ion  of  benchmark  corpusBefore the test, a benchmark version of the testcorpus was created.
The texts were first analysedusing the preprocessor, the morphological nalyser,and the module for morphological heuristics.
Thisambiguous data was then manually disambiguatedby judges, each having a thorough understanding ofthe ENGCG grammatical representation.
The cor-pus was independently disambiguated by two judges.In the instructions to the experts, special empha-sis was given to the quality of the work (there wasno time pressure).
The two disambiguated versionsof the corpus were compared using the Unix sdiffprogram.
At this stage, slightly above 99 % of allanalyses agreed.
The differences were jointly exam-ined by the judges to see whether they were causedby inattention or by a genuine difference of opinionthat could not be resolved by consulting the docu-mentation that outlines the principles adopted forthis grammatical representation (for the most partdocumented in (Karlsson et al, 1994)).
It turnedout that almost all of these differences were due toinattention.
Only in the analysis of a few words itwas agreed that a multiple choice was appropriatebecause of different meaning-level interpretations ofthe utterance (these were actually headings wheresome of the grammatical information was omitted).Overall, these results agree with our previous expe-riences (Karlsson et al, 1994): if the analysis is doneby experts in the adopted grammatical representa-tion, with emphasis on the quality of the work, aconsensus of virtually 100 % is possible, at least atthe level of morphological analysis (for a less opti-mistic view, see (Church, 1992)).5.3 Morpho log ica l  analys isThe preprocessed text was submitted to the ENG-TWOL morphological analyser, which assigns to25,831 words of the total 26,711 (96.7 %) at leastone morphological analysis.
The remaining 880word-form tokens were analysed with the rule-basedheuristic module.
After the combined effect of thesemodules, there were 47,269 morphological nalyses,i.e.
1.77 morphological analyses for each word onan average.
At this stage, 23 words missed a con-textually appropriate analysis, i.e.
the error rate ofthe system after morphological analysis was about0.1%.5.4 Morpho log ica l  d i sambiguat ionThe morphologically analysed text was submittedto five disambiguators ( ee Figure 3).
The first one,D1, is the grammar-based ENGCG disambiguator.In the next step (D2) we have used also heuristicENGCG constraints.
The probabilistic information50is used in D3, where the ambiguities of D2 are re-solved by XT.
We also tested the usefulness of theheuristic component of ENGCG by omitting it inD4.
The last test, D5, is XT alone, i.e.
only proba-bilistic techniques are used here for resolving ENG-TWOL ambiguities.The ENGCG disambiguator performed somewhatless well than usually.
With heuristic constraints,the error rate was as high as 0.63 %, with 1.04 mor-phological readings per word on an average.
How-ever, most (57 %) of the total errors were made af-ter ENGCG analysis (i.e.
in the analysis of no morethan 3.6 % of all words).
In a way, this is not verysurprising because ENGCG is supposed to tackle allthe 'easy' cases and leave the structurally hardestcases pending.
But it is quite revealing that as muchas three fourths of the probabilistic tagger's errorsoccur in the analysis of the structurally 'easy' cases;obviously, many of the probabilistic system's deci-sions are structurally somewhat naive.
Overall, thehybrid (D3#) reached an accuracy of about 98.5 % -significantly better than the 95-97 % accuracy whichstate-of-the-art probabilistic taggers reach alone.The hybrid D3~ is like hybrid D3~, but we haveused careful mapping.
There some problematicambiguity (see Figure 2) is left pending.
For in-stance, ambiguities between preposition and infini-tive marker (word to), or between subordinator andpreposition (word as), are resolved as far as ENGCGdisambiguates them, the prediction of XT is not con-sulted.
Also, when XT proposes tags like J J  (adjec-tive), AP (post-determiner) or VB (verb base-form)very little further disambiguation is done.
This hy-brid does not contain any mapping errors, and onthe other hand, not all the XT errors either.The test without the heuristic component ofENGCG (D4) suggests that ambiguity should be re-solved as far as possible with rules.
An open ques-tion is, how far we can go using only linguistic infor-mation (e.g.
by writing more heuristic onstraints tobe applied after the more reliable ones, in this wayavoiding many linguistically naive errors).The last test gives further evidence for the use-fulness of a carefully designed linguistic rule compo-nent.
Without such a rule component, he decreasein accuracy is quite dramatic although a part of theerrors come from the mapping between tag sets 3.6 Conc lus ionIn this paper we have demonstrated how knowledge-based and statistical techniques can be combined toimprove the accuracy of a part of speech tagger.
Oursystem reaches a better than 98 % accuracy using arelatively fine-grained grammatical representation.Some concluding remarks are in order.3Even without the mapping errors, the reported 4 %error rate of XT is considerably higher than that of ourhybrid.?
Using linguistic information before a statisticalmodule provides a better result than using astatistical module alone.?
ENGCG leaves some 'hard' ambiguities unre-solved (about 3-7 % of all words).
This amountis characteristic of the ENGCG rule-formMism,tagset and disambiguation grammar.
It doesnot necessarily hold for other knowledge-basedsystems.?
Only about 20-25 % of errors made by the sta-tistical component occur in the analysis of these'hard' ambiguities.
That means, 75-80 % of theerrors made by the statistical tagger were re-solved correctly using linguistic rules.?
Certain kinds of ambiguity left pending byENGCG, e.g.
CS vs. PREP, are resolved ratherunreliably by XT.?
The overall result is better than other state-of-the-art part-of-speech disambiguators.
In our27000 word test sample from previously unseencorpus, 98.5 % of words received a correct anal-ysis.
In other words, the error rate is reducedat least by half.Although the result is better than provided byany other tagger that produces fully disambiguatedoutput, we believe that the result could still be im-proved.
Some possibilities:?
We could use partly disambiguated text(e.g.
the output of parsers D1, D2 or D3~)and disambiguate he result using a knowledge-based syntactic parser (see experiments in (Vou-tilainen and Tapanainen, 1993)).?
We could leave the text partly disambiguated,and use a syntactic parser that uses both lin-guistic knowledge and corpus-based heuristics(see (Tapanainen and J//rvinen, 1994)).?
Some ambiguities are very difficult to resolve ina small window that statistical taggers currentlyuse (e.g.
CS vs. PREP ambiguity when a nounphrase follows).
A better way to resolve themwould probably be to write (heuristic) rules.?
We could train the statistical tagger on the out-put of a knowledge-based tagger.
That is prob-lematic because generally statistical methodsseem to require some compact set of tags, whilea knowledge-based system needs more informa-tive tags.
The tag set of a knowledge-based sys-tem should be reduced down to some subset.That might prevent some mapping errors butthere is no quarantee that the statistical taggerwould work any better.?
We could try the components in a different or-der: using statistics before heuristical knowl-edge etc.
However, currently the heuristic om-ponent makes less errors than the statisticaltagger.51DO (Morphological nalysis)D1 (DO + ENGCG)D2 (D1 + ENGCG heuristics)D3~ (D2 + XT + C-mapping)D3Z (D2 + XT + mapping)D4 (D1 + XT + mapping)D5 (DO + XT + mapping)\[ Amb.
words37.6 %6.4 %3.6 %2.2 %0.0 %0.0 %0.7 %Figure 3: Performance of theReadings Readings /47269 1.7728815 1.0827681 1.0427358 1.0226744 1.0026794 1.0026977 1.01word\]Errors \[Error ate (%)23 0.09 %94 0.35 %169 0.63 %220 0.82 %391 1.46 %6.38 %taggers on a 26,711-word corpus.?
We could use a better statistical tagger.
Butthe accuracy of XT is almost he same as theaccuracy of any other statistical tagger.
Whatis more, the accuracy of the purely statisticaltaggers has not been greatly increased since thefirst of its kind, CLAWS1, (Marshall, 1983) waspublished over ten years ago.We believe that the best way to boost the accu-racy of a tagger is to employ even more linguisticknowledge.
The knowledge should, in addition, con-tain more syntactic information so that we could re-fer to real (syntactic) objects of the language, notjust a sequence of words or parts of speech.
Statisti-cal information should be used only when one doesnot know how to resolve the remaing ambiguity, andthere is a definite need to get fully unambiguous out-put.7 AcknowledgementsWe would like to thank Timo J?rvinen, Lauri Kart-tunen, Jussi Piitulainen and anonymous referees foruseful comments on earlier versions of this paper.ReferencesL.
E. Baum.
1972.
An inequality and associatedmaximization technique in statistical estimationfor probabilistic functions of a Markov process.Inequatics, 3:1-8, 1972.Kenneth W. Church.
1992.
Current Practice in Partof Speech Tagging and Suggestions for the Fu-ture.
In Simmons (ed.
), Sbornik praci: In Honorof Henry Ku~era.
Michigan Slavic Studies.Doug Cutting, Julian Kupiec, Jan Pedersen andPenelope Sibun.
1992.
A Practical Part-of-SpeechTagger.
In Proceedings of ANLP-92.W.
N. Francis and F. KuSera.
1982.
Frequency Anal-ysis of English Usage.
Houghton Mifflin.Timo J~rvinen.
1994.
Annotating 200 millionwords: the Bank of English project.
In proceed-ings of COLING-94, Vol.
1,565-568.
Kyoto.Fred Karlsson.
1990.
Constraint Grammar as aFramework for Parsing Running Text.
In Proceed-ings of COLING-90.
Helsinki.
Vol.
3, 168-173.Fred Karlsson, Atro Voutilainen, Juha Heikkil~ andArto Anttila (eds.).
1994.
Constraint Grammar:a Language-Independent System for Parsing Un-restricted Text.
Berlin: Mouton de Gruyter.Andr6 Kempe.
1994.
A Probabilistic Tagger andan Analysis of Tagging Errors.
Research Report,Institut fiir Maschinelle Sprachverarbeitung, Uni-versit~t Stuttgart.Kimmo Koskenniemi.
1983.
Two-level Morphology.A General Computational Model for Word-formProduction and Generation.
Publication No.
11,Department of General Linguistics, University ofHelsinki.Julian M. Kupiec.
1989.
Probabilistic models ofshort and long distance word dependencies in run-ning text.
In Proceedings of the 1989 DARPASpeech and Natural Language Workshop pp.
290-295.
Philadelphia.
Morgan Kaufman.Carl de Marcken.
1990.
Parsing the LOB Corpus.In Proceedings of the 28th Annual Meeting of theACL.
243-251.Ian Marshall.
1983.
Choice of grammatical word-class without global syntactic analysis: taggingwords in the LOB Corpus.
Computers in the Hu-manities 17.
139-150.Pasi Tapanainen and Timo J~rvinen.
1994.
Syn-tactic analysis of natural language using linguisticrules and corpus-based patterns.
In proceedings ofCOLING-94, Vol.
1,629-634.
Kyoto.Atro Voutilainen.
1994.
Morphological disambigua-tion.
In Karlsson et al.Atro Voutilainen, Juha Heikkil?
and Arto Anttila.1992.
Constraint Grammar of English.
A Per-formance- Oriented Introduction.
Publication No.21, Department ofGeneral Linguistics, Universityof Helsinki.Atro Voutilainen and Pasi Tapanainen.
1993.
Am-biguity resolution in a reductionistic parser.
Pro-ceedings of EACL'93.
Utrecht, Holland.
394-403.Ralph Weischedel, Marie Meteer, Richard Schwartz,Lance Ramshaw and Jeff Palmuzzi.
1993.
Cop-ing with ambiguity and unknown words throughprobabilistic models.
Computational Linguistics,Vol.
19, Number 2.52
