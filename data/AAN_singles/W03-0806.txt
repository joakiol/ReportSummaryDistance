Blueprint for a High Performance NLP InfrastructureJames R. CurranSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh.
EH8 9LWjamesc@cogsci.ed.ac.ukAbstractNatural Language Processing (NLP) system de-velopers face a number of new challenges.
In-terest is increasing for real-world systems thatuse NLP tools and techniques.
The quantity oftext now available for training and processingis increasing dramatically.
Also, the range oflanguages and tasks being researched contin-ues to grow rapidly.
Thus it is an ideal timeto consider the development of new experimen-tal frameworks.
We describe the requirements,initial design and exploratory implementationof a high performance NLP infrastructure.1 IntroductionPractical interest in NLP has grown dramatically in re-cent years.
Accuracy on fundamental tasks, such as partof speech (POS) tagging, named entity recognition, andbroad-coverage parsing continues to increase.
We cannow construct systems that address complex real-worldproblems such as information extraction and question an-swering.
At the same time, progress in speech recogni-tion and text-to-speech technology has made completespoken dialogue systems feasible.
Developing thesecomplex NLP systems involves composing many differentNLP tools.
Unfortunately, this is difficult because manyimplementations have not been designed as componentsand only recently has input/output standardisation beenconsidered.
Finally, these tools can be difficult to cus-tomise and tune for a particular task.NLP is experiencing an explosion in the quantity ofelectronic text available.
Some of this new data will bemanually annotated.
For example, 10 million words ofthe American National Corpus (Ide et al, 2002) will havemanually corrected POS tags, a tenfold increase over thePenn Treebank (Marcus et al, 1993), currently used fortraining POS taggers.
This will require more efficientlearning algorithms and implementations.However, the greatest increase is in the amount of rawtext available to be processed, e.g.
the English Giga-word Corpus (Linguistic Data Consortium, 2003).
Re-cent work (Banko and Brill, 2001; Curran and Moens,2002) has suggested that some tasks will benefit fromusing significantly more data.
Also, many potential ap-plications of NLP will involve processing very large textdatabases.
For instance, biomedical text-mining involvesextracting information from the vast body of biologicaland medical literature; and search engines may eventu-ally apply NLP techniques to the whole web.
Other po-tential applications must process text online or in real-time.
For example, Google currently answers 250 millionqueries per day, thus processing time must be minimised.Clearly, efficient NLP components will need to be devel-oped.
At the same time, state-of-the-art performance willbe needed for these systems to be of practical use.Finally, NLP is growing in terms of the number oftasks, methods and languages being researched.
Al-though many problems share algorithms and data struc-tures there is a tendency to reinvent the wheel.Software engineering research on Generative Program-ming (Czarnecki and Eisenecker, 2000) attempts to solvethese problems by focusing on the development of con-figurable elementary components and knowledge to com-bine these components into complete systems.
Our in-frastructure for NLP will provide high performance1 com-ponents inspired by Generative Programming principles.This paper reviews existing NLP systems and discussesthe requirements for an NLP infrastructure.
We then de-scribe our overall design and exploratory implementa-tion.
We conclude with a discussion of programming in-terfaces for the infrastructure including a script languageand GUI interfaces, and web services for distributed NLPsystem development.
We seek feedback on the overalldesign and implementation of our proposed infrastruc-ture and to promote discussion about software engineer-ing best practice in NLP.1We use high performance to refer to both state of the artperformance and high runtime efficiency.2 Existing SystemsThere are a number of generalised NLP systems in the lit-erature.
Many provide graphical user interfaces (GUI) formanual annotation (e.g.
General Architecture for TextEngineering (GATE) (Cunningham et al, 1997) and theAlembic Workbench (Day et al, 1997)) as well as NLPtools and resources that can be manipulated from theGUI.
For instance, GATE currently provides a POS tag-ger, named entity recogniser and gazetteer and ontologyeditors (Cunningham et al, 2002).
GATE goes beyondearlier systems by using a component-based infrastruc-ture (Cunningham, 2000) which the GUI is built on topof.
This allows components to be highly configurable andsimplifies the addition of new components to the system.A number of stand-alone tools have also been devel-oped.
For example, the suite of LT tools (Mikheev et al,1999; Grover et al, 2000) perform tokenization, taggingand chunking on XML marked-up text directly.
Thesetools also store their configuration state, e.g.
the trans-duction rules used in LT CHUNK, in XML configurationfiles.
This gives a greater flexibility but the tradeoff isthat these tools can run very slowly.
Other tools havebeen designed around particular techniques, such as fi-nite state machines (Karttunen et al, 1997; Mohri et al,1998).
However, the source code for these tools is notfreely available, so they cannot be extended.Efficiency has not been a focus for NLP research ingeneral.
However, it will be increasingly important astechniques become more complex and corpus sizes grow.An example of this is the estimation of maximum en-tropy models, from simple iterative estimation algorithmsused by Ratnaparkhi (1998) that converge very slowly,to complex techniques from the optimisation literaturethat converge much more rapidly (Malouf, 2002).
Otherattempts to address efficiency include the fast Transfor-mation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL sys-tems, and the translation of TBL rules into finite state ma-chines for very fast tagging (Roche and Schabes, 1997).The TNT POS tagger (Brants, 2000) has also been de-signed to train and run very quickly, tagging between30,000 and 60,000 words per second.The Weka package (Witten and Frank, 1999) providesa common framework for several existing machine learn-ing methods including decision trees and support vectormachines.
This library has been very popular because itallows researchers to experiment with different methodswithout having to modify code or reformat data.Finally, the Natural Language Toolkit (NLTK) is apackage of NLP components implemented in Python(Loper and Bird, 2002).
Python scripting is extremelysimple to learn, read and write, and so using the existingcomponents and designing new components is simple.3 Performance RequirementsAs discussed earlier, there are two main requirementsof the system that are covered by ?high performance?
:speed and state of the art accuracy.
Efficiency is requiredboth in training and processing.
Efficient training is re-quired because the amount of data available for train-ing will increase significantly.
Also, advanced methodsoften require many training iterations, for example ac-tive learning (Dagan and Engelson, 1995) and co-training(Blum and Mitchell, 1998).
Processing text needs to beextremely efficient since many new applications will re-quire very large quantities of text to be processed or manysmaller quantities of text to be processed very quickly.State of the art accuracy is also important, particularlyon complex systems since the error is accumulated fromeach component in the system.
There is a speed/accuracytradeoff that is rarely addressed in the literature.
For in-stance, reducing the beam search width used for taggingcan increase the speed without significantly reducing ac-curacy.
Finally, the most accurate systems are often verycomputationally intensive so a tradeoff may need to bemade here.
For example, the state of the art POS tag-ger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text sepa-rately.
Sophisticated modelling may also give improvedaccuracy at the cost of training and processing time.The space efficiency of the components is importantsince complex NLP systems will require many differentNLP components to be executing at the same time.
Also,language processors many eventually be implemented forrelatively low-specification devices such as PDAs.
Thismeans that special attention will need to be paid to thedata-structures used in the component implementation.The infrastructure should allow most data to be storedon disk (as a configuration option since we must tradeoffspeed for space).
Accuracy, speed and compactness arethe main execution goals.
These goals are achieved byimplementing the infrastructure in C/C++, and profilingand optimising the algorithms and data-structures used.4 Design RequirementsThe remaining requirements relate to the overall andcomponent level design of the system.
Following theGenerative Programming paradigm, the individual com-ponents of the system must be elementary and highlyconfigurable.
This ensures minimal redundancy betweencomponents and makes them easier to understand, im-plement, test and debug.
It also ensures components aremaximally composable and extensible.
This is particu-larly important in NLP because of the high redundancyacross tasks and approaches.Machine learning methods should be interchangeable:Transformation-based learning (TBL) (Brill, 1993) andMemory-based learning (MBL) (Daelemans et al, 2002)have been applied to many different problems, so a sin-gle interchangeable component should be used to repre-sent each method.
We will base these components on thedesign of Weka (Witten and Frank, 1999).Representations should be reusable: for example,named entity classification can be considered as a se-quence tagging task or a bag-of-words text classificationtask.
The same beam-search sequence tagging compo-nent should be able to be used for POS tagging, chunk-ing and named entity classification.
Feature extractioncomponents should be reusable since many NLP compo-nents share features, for instance, most sequence taggersuse the previously assigned tags.
We will use an object-oriented hierarchy of methods, representations and fea-tures to allow components to be easily interchanged.
Thishierarchy will be developed by analysing the range ofmethods, representations and features in the literature.High levels of configurability are also very impor-tant.
Firstly, without high levels of configurability, newsystems are not easy to construct by composing exist-ing components, so reinventing the wheel becomes in-evitable.
Secondly, different languages and tasks show avery wide variation in the methods, representations, andfeatures that are most successful.
For instance, a trulymultilingual tagger should be able to tag a sequence fromleft to right or right to left.
Finally, this flexibility willallow for research into new tasks and languages to be un-dertaken with minimal coding.Ease of use is a very important criteria for an infras-tructure and high quality documentation and examplesare necessary to make sense of the vast array of compo-nents in the system.
Preconfigured standard components(e.g.
an English POS tagger) will be supplied with theinfrastructure.
More importantly, a Python scripting lan-guage interface and a graphical user interface will be builton top of the infrastructure.
This will allow componentsto be configured and composed without expertise in C++.The user interface will generate code to produce stand-alone components in C++ or Python.
Since the Pythoncomponents will not need to be compiled, they can bedistributed immediately.One common difficulty with working on text is therange of file formats and encodings that text can bestored in.
The infrastructure will provide components toread/write files in many of these formats including HTMLfiles, text files of varying standard formats, email folders,Postscript, Portable Document Format, Rich Text Formatand Microsoft Word files.
The infrastructure will alsoread XML and SGML marked-up files, with and withoutDTDs and XML Schemas, and provide an XPath/XSLTquery interface to select particular subtrees for process-ing.
All of these reading/writing components will use ex-isting open source software.
It will also eventually pro-vide components to manipulate groups of files: such as it-erate through directories, crawl web pages, get files fromftp, extract files from zip and tar archives.
The systemwill provide full support to standard character sets (e.g.Unicode) and encodings (e.g.
UTF-8 and UTF-16).Finally, the infrastructure will provide standard imple-mentations, feature sets and configuration options whichmeans that if the configuration of the components is pub-lished, it will be possible for anyone to reproduce pub-lished results.
This is important because there are manysmall design decisions that can contribute to the accuracyof a system that are not typically reported in the literature.5 Components GroupsWhen completed the infrastructure will provide highlyconfigurable components grouped into these broad areas:file processing reading from directories, archives, com-pressed files, sockets, HTTP and newsgroups;text processing reading/writing marked-up corpora,HTML, emails, standard document formats and textfile formats used to represent annotated corpora.lexical processing tokenization, word segmentation andmorphological analysis;feature extraction extracting lexical and annotation fea-tures from the current context in sequences, bag ofwords from segments of textdata-structures and algorithms efficient lexical repre-sentations, lexicons, tagsets and statistics; Viterbi,beam-search and n-best sequence taggers, parsingalgorithms;machine learning methods statistical models: Na?
?veBayes, Maximum Entropy, Conditional RandomFields; and other methods: Decision Trees and Lists,TBL and MBL;resources APIs to WordNet (Fellbaum, 1998), Googleand other lexical resources such as gazetteers, on-tologies and machine readable dictionaries;existing tools integrating existing open source compo-nents and providing interfaces to existing tools thatare only distributed as executables.6 ImplementationThe infrastructure will be implemented in C/C++.
Tem-plates will be used heavily to provide generality withoutsignificantly impacting on efficiency.
However, becausetemplates are a static facility we will also provide dy-namic versions (using inheritance), which will be slowerbut accessible from scripting languages and user inter-faces.
To provide the required configurability in the staticversion of the code we will use policy templates (Alexan-drescu, 2001), and for the dynamic version we will useconfiguration classes.A key aspect of increasing the efficiency of the systemwill be using a common text and annotation representa-tion throughout the infrastructure.
This means that we donot need to save data to disk, and load it back into mem-ory between each step in the process, which will providea significant performance increase.
Further, we can usetechniques for making string matching and other text pro-cessing very fast such as making only one copy of eachlexical item or annotation in memory.
We can also loada lexicon into memory that is shared between all of thecomponents, reducing the memory use.The implementation has been inspired by experiencein extracting information from very large corpora (Cur-ran and Moens, 2002) and performing experiments onmaximum entropy sequence tagging (Curran and Clark,2003; Clark et al, 2003).
We have already implementeda POS tagger, chunker, CCG supertagger and named entityrecogniser using the infrastructure.
These tools currentlytrain in less than 10 minutes on the standard training ma-terials and tag faster than TNT, the fastest existing POStagger.
These tools use a highly optimised GIS imple-mentation and provide sophisticated Gaussian smoothing(Chen and Rosenfeld, 1999).
We expect even faster train-ing times when we move to conjugate gradient methods.The next step of the process will be to add different sta-tistical models and machine learning methods.
We firstplan to add a simple Na?
?ve Bayes model to the system.This will allow us to factor out the maximum entropyspecific parts of the system and produce a general com-ponent for statistical modelling.
We will then implementother machine learning methods and tasks.7 InterfacesAlthough C++ is extremely efficient, it is not suitable forrapidly gluing components together to form new tools.To overcome this problem we have implemented an in-terface to the infrastructure in the Python scripting lan-guage.
Python has a number of advantages over otheroptions, such as Java and Perl.
Python is very easy tolearn, read and write, and allows commands to be enteredinteractively into the interpreter, making it ideal for ex-perimentation.
It has already been used to implement aframework for teaching NLP (Loper and Bird, 2002).Using the Boost.Python C++ library (Abrahams,2003), it is possible to reflect most of the componentsdirectly into Python with a minimal amount of coding.The Boost.Python library also allows the C++ code to ac-cess new classes written in Python that are derived fromthe C++ classes.
This means that new and extended com-ponents can be written in Python (although they will beconsiderably slower).
The Python interface allows thecomponents to be dynamically composed, configured andextended in any operating system environment withoutthe need for a compiler.
Finally, since Python can pro-duce stand-alone executables directly, it will be possibleto create distributable code that does not require the entireinfrastructure or Python interpreter to be installed.The basic Python reflection has already been imple-mented and used for large scale experiments with POStagging, using pyMPI (a message passing interface libraryfor Python) to coordinate experiments across a cluster ofover 100 machines (Curran and Clark, 2003; Clark et al,2003).
An example of using the Python tagger interfaceis shown in Figure 1.On top of the Python interface we plan to implementa GUI interface for composing and configuring compo-nents.
This will be implemented in wxPython which isa platform independent GUI library that uses the nativewindowing environment under Windows, MacOS andmost versions of Unix.
The wxPython interface will gen-erate C++ and Python code that composes and config-ures the components.
Using the infrastructure, Pythonand wxPython it will be possible to generate new GUI ap-plications that use NLP technology.Because C++ compilers are now fairly standards com-pliant, and Python and wxPython are available for mostarchitectures, the infrastructure will be highly portable.Further, we eventually plan to implement interfaces toother languages (in particular Java using the Java NativeInterface (JNI) and Perl using the XS interface).8 Web servicesThe final interface we intend to implement is a collec-tion of web services for NLP.
A web service providesa remote procedure that can be called using XML basedencodings (XMLRPC or SOAP) of function names, argu-ments and results transmitted via internet protocols suchas HTTP.
Systems can automatically discover and com-municate with web services that provide the functionalitythey require by querying databases of standardised de-scriptions of services with WSDL and UDDI.
This stan-dardisation of remote procedures is very exciting from asoftware engineering viewpoint since it allows systemsto be totally distributed.
There have already been severalattempts to develop distributed NLP systems for dialoguesystems (Bayer et al, 2001) and speech recognition (Ha-cioglu and Pellom, 2003).
Web services will allow com-ponents developed by different researchers in different lo-cations to be composed to build larger systems.Because web services are of great commercial interestthey are already being supported strongly by many pro-gramming languages.
For instance, web services can beaccessed with very little code in Java, Python, Perl, C,C++ and Prolog.
This allows us to provide NLP servicesto many systems that we could not otherwise support us-ing a single interface definition.
Since the service argu-ments and results are primarily text and XML, the webservice interface will be reasonably efficient for small% pythonPython 2.2.1 (#1, Sep 30 2002, 20:13:03)[GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-110)] on linux2Type "help", "copyright", "credits" or "license" for more information.>>> import nlp.tagger>>> op = nlp.tagger.Options(?models/pos/options?
)>>> print opnklasses = 46...alpha = 1.65>>> tagger = nlp.tagger.Tagger(op)>>> tags = tagger.tag([?The?, ?cat?, ?sat?, ?on?, ?the?, ?mat?, ?.?
])>>> print tags[?DT?, ?NN?, ?VBD?, ?IN?, ?DT?, ?NN?, ?.?
]>>> tagger.tag(?infile?, ?outfile?
)>>>Figure 1: Calling the POS tagger interactively from the Python interpreterquantities of text (e.g.
a single document).
The secondadvantage they have is that there is no startup costs whentagger loads up, which means local copies of the webservice could be run to reduce tagging latency.
Finally,web services will allow developers of resources such asgazetteers to provide the most up to date resources eachtime their functionality is required.We are currently in the process of implementing a POStagging web service using the gSOAP library, which willtranslate our C infrastructure binding into web servicewrapper code and produce the necessary XML service de-scription files.9 ConclusionThe Generative Programming approach to NLP infras-tructure development will allow tools such as sentenceboundary detectors, POS taggers, chunkers and namedentity recognisers to be rapidly composed from many el-emental components.
For instance, implementing an ef-ficient version of the MXPOST POS tagger (Ratnaparkhi,1996) will simply involve composing and configuring theappropriate text file reading component, with the sequen-tial tagging component, the collection of feature extrac-tion components and the maximum entropy model com-ponent.The individual components will provide state of the artaccuracy and be highly optimised for both time and spaceefficiency.
A key design feature of this infrastructure isthat components share a common representation for textand annotations so there is no time spent reading/writingformatted data (e.g.
XML) between stages.To make the composition and configuration processeasier we have implemented a Python scripting inter-face, which means that anyone can construct efficient newtools, without the need for much programming experi-ence or a compiler.
The development of a graphical userinterface on top of the infrastructure will further ease thedevelopment cycle.AcknowledgementsWe would like to thank Stephen Clark, Tara Murphy, andthe anonymous reviewers for their comments on draftsof this paper.
This research is supported by a Common-wealth scholarship and a Sydney University Travellingscholarship.ReferencesDavid Abrahams.
2003.
Boost.Python C++ library.http://www.boost.ory (viewed 23/3/2003).Andrei Alexandrescu.
2001.
Modern C++ Design: GenericProgramming and Design Patterns Applied.
C++ In-DepthSeries.
Addison-Wesley, New York.Michele Banko and Eric Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Proceedingsof the 39th annual meeting of the Association for Computa-tional Linguistics, pages 26?33, Toulouse, France, 9?11 July.Samuel Bayer, Christine Doran, and Bryan George.
2001.
Dia-logue interaction with the DARPA Communicator Infrastruc-ture: The development of useful software.
In J. Allan, editor,Proceedings of HLT 2001, First International Conference onHuman Language Technology Research, pages 114?116, SanDiego, CA, USA.
Morgan Kaufmann.Avrim Blum and Tom Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proceedings of the11th Annual Conference on Computational Learning Theory,pages 92?100, Madisson, WI, USA.Thorsten Brants.
2000.
TnT - a statistical part-of-speech tagger.In Proceedings of the 6th Conference on Applied NaturalLanguage Processing, pages 224?231, Seattle, WA, USA, 29April ?
4 May.Eric Brill.
1993.
A Corpus-Based Appreach to LanguageLearning.
Ph.D. thesis, Department of Computer and Infor-mation Science, University of Pennsylvania.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaussian priorfor smoothing maximum entropy models.
Technical report,Carnegie Mellon University.Stephen Clark, James R. Curran, and Miles Osborne.
2003.Bootstrapping POS-taggers using unlabelled data.
In Pro-ceedings of the 7th Conference on Natural Language Learn-ing, Edmonton, Canada, 31 May ?
1 June.
(to appear).Hamish Cunningham, Yorick Wilks, and Robert J. Gaizauskas.1997.
GATE ?
a general architecture for text engineering.
InProceedings of the 16th International Conference on Com-putational Linguistics, pages 1057?1060, Copenhagen, Den-mark, 5?9 August.Hamish Cunningham, Diana Maynard, C. Ursu K. Bontcheva,V.
Tablan, and M. Dimitrov.
2002.
Developing languageprocessing components with GATE.
Technical report, Uni-versity of Sheffield, Sheffield, UK.Hamish Cunningham.
2000.
Software Architecture for Lan-guage Engineering.
Ph.D. thesis, University of Sheffield.James R. Curran and Stephen Clark.
2003.
Investigating GISand smoothing for maximum entropy taggers.
In Proceed-ings of the 11th Meeting of the European Chapter of the As-sociation for Computational Lingustics, pages 91?98, Bu-dapest, Hungary, 12?17 April.James R. Curran and Marc Moens.
2002.
Scaling contextspace.
In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, Philadelphia, PA,USA, 7?12 July.Krzysztof Czarnecki and Ulrich W. Eisenecker.
2000.
Gen-erative Programming: Methods, Tools, and Applications.Addison-Wesley.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-tal van den Bosch.
2002.
TiMBL: Tilburg Memory-BasedLearner reference guide.
Technical Report ILK 02-10, In-duction of Linguistic Knowledge.
Tilburg University.Ido Dagan and Sean P. Engelson.
1995.
Committee-based sam-pling for training probabilistic classifiers.
In Proceedings ofthe International Conference on Machine Learning, pages150?157, Tahoe City, CA, USA, 9?12 July.David Day, John Aberdeen, Lynette Hirschman, RobynKozierok, Patricia Robinson, and Marc Vilain.
1997.Mixed-initiative development of language processing sys-tems.
In Proceedings of the Fifth Conference on AppliedNatural Language Processing, pages 384?355, Washington,DC, USA, 31 March ?
3 April.Cristiane Fellbaum, editor.
1998.
Wordnet: an electronic lexi-cal database.
The MIT Press, Cambridge, MA USA.Claire Grover, Colin Matheson, Andrei Mikheev, and MarcMoens.
2000.
LT TTT - a flexible tokenisation tool.
In Pro-ceedings of Second International Language Resources andEvaluation Conference, pages 1147?1154, Athens, Greece,31 May ?
2 June.Kadri Hacioglu and Bryan Pellom.
2003.
A distributed archi-tecture for robust automatic speech recognition.
In Proceed-ings of Conference on Acoustics, Speech, and Signal Pro-cessing (ICASSP), Hong Kong, China, 6?10 April.Nancy Ide, Randi Reppen, and Keith Suderman.
2002.
Theamerican national corpus: More than the web can provide.In Proceedings of the Third Language Resources and Eval-uation Conference, pages 839?844, Las Palmas, Canary Is-lands, Spain.Lauri Karttunen, Tama?s Gaa?l, and Andre?
Kempe.
1997.
XeroxFinite-State Tool.
Technical report, Xerox Research CentreEurope Grenoble, Meylan, France.Linguistic Data Consortium.
2003.
English Gigaword Corpus.catalogue number LDC2003T05.Edward Loper and Steven Bird.
2002.
NLTK: The Natural Lan-guage Toolkit.
In Proceedings of the Workshop on EffectiveTools and Methodologies for Teaching NLP and Computa-tional Linguistics, pages 63?70, Philadelphia, PA, 7 July.Robert Malouf.
2002.
A comparison of algorithms for max-imum entropy parameter estimation.
In Proceedings of the6th Conference on Natural Language Learning, pages 49?55, Taipei, Taiwan, 31 August ?
1 September.Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz.1993.
Building a large annotated corpus of English: ThePenn Treebank.
Computational Linguistics, 19(2):313?330.Andrei Mikheev, Claire Grover, and Marc Moens.
1999.
Xmltools and architecture for named entity recognition.
Journalof Markup Languages: Theory and Practice 1, 3:89?113.Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley.1998.
A rational design for a weighted finite-state transducerlibrary.
Lecture Notes in Computer Science, 1436.Grace Ngai and Radu Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proceedings of the SecondMeeting of the North American Chapter of the Associationfor Computational Linguistics, pages 40?47, Pittsburgh, PA,USA, 2?7 June.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EMNLP Conference,pages 133?142, Philadelphia, PA, USA.Adwait Ratnaparkhi.
1998.
Maximum Entropy Models for Nat-ural Language Ambiguity Resolution.
Ph.D. thesis, Univer-sity of Pennsylvania.Emmanuel Roche and Yves Schabes.
1997.
Deterministicpart-of-speech tagging with finite-state transducers.
In Em-manuel Roche and Yves Schabes, editors, Finite-State Lan-guage Processing, chapter 7.
The MIT Press.Hans van Halteren, Jakub Zavrel, and Walter Daelemans.
2001.Improving accuracy in wordclass tagging through combina-tion of machine learning systems.
Computational Linguis-tics, 27(2):199?229.Ian H. Witten and Eibe Frank.
1999.
Data Mining: PracticalMachine Learning Tools and Techniques with Java Imple-mentations.
Morgan Kaufmann Publishers.
