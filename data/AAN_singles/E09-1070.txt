Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612?620,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsAnalysing Wikipedia and Gold-Standard Corpora for NER TrainingJoel Nothman and Tara Murphy and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{jnot4610,tm,james}@it.usyd.edu.auAbstractNamed entity recognition (NER) for En-glish typically involves one of three goldstandards: MUC, CoNLL, or BBN, all createdby costly manual annotation.
Recent workhas used Wikipedia to automatically cre-ate a massive corpus of named entity an-notated text.We present the first comprehensive cross-corpus evaluation of NER.
We identifythe causes of poor cross-corpus perfor-mance and demonstrate ways of makingthem more compatible.
Using our process,we develop a Wikipedia corpus which out-performs gold standard corpora on cross-corpus evaluation by up to 11%.1 IntroductionNamed Entity Recognition (NER), the task of iden-tifying and classifying the names of people, organ-isations and other entities within text, is centralto many NLP systems.
NER developed from in-formation extraction in the Message Understand-ing Conferences (MUC) of the 1990s.
By MUC 6and 7, NER had become a distinct task: taggingproper names, and temporal and numerical expres-sions (Chinchor, 1998).Statistical machine learning systems haveproven successful for NER.
These learn patternsassociated with individual entity classes, mak-ing use of many contextual, orthographic, linguis-tic and external knowledge features.
However,they rely heavily on large annotated training cor-pora.
This need for costly expert annotation hin-ders the creation of more task-adaptable, high-performance named entity recognisers.In acquiring new sources for annotated corpora,we require an analysis of training data as a variablein NER.
This paper compares the three main gold-standard corpora.
We found that tagging mod-els built on each corpus perform relatively poorlywhen tested on the others.
We therefore presentthree methods for analysing internal and inter-corpus inconsistencies.
Our analysis demonstratesthat seemingly minor variations in the text itself,starting right from tokenisation can have a hugeimpact on practical NER performance.We take this experience and apply it to a corpuscreated automatically using Wikipedia.
This cor-pus was created following the method of Nothmanet al (2008).
By training the C&C tagger (Curranand Clark, 2003) on the gold-standard corpora andour new Wikipedia-derived training data, we eval-uate the usefulness of the latter and explore thenature of the training corpus as a variable in NER.Our Wikipedia-derived corpora exceed the per-formance of non-corresponding training and testsets by up to 11% F -score, and can be engineeredto automatically produce models consistent withvarious NE-annotation schema.
We show that it ispossible to automatically create large, free, namedentity-annotated corpora for general or domainspecific tasks.2 NER and annotated corporaResearch into NER has rarely considered the im-pact of training corpora.
The CoNLL evalua-tions focused on machine learning methods (TjongKim Sang, 2002; Tjong Kim Sang and De Meul-der, 2003) while more recent work has often in-volved the use of external knowledge.
Since manytagging systems utilise gazetteers of known enti-ties, some research has focused on their automaticextraction from the web (Etzioni et al, 2005) orWikipedia (Toral et al, 2008), although Mikheevet al (1999) and others have shown that largerNE lists do not necessarily correspond to increasedNER performance.
Nadeau et al (2006) use suchlists in an unsupervised NE recogniser, outper-forming some entrants of the MUC Named EntityTask.
Unlike statistical approaches which learn612patterns associated with a particular type of entity,these unsupervised approaches are limited to iden-tifying common entities present in lists or thosecaught by hand-built rules.External knowledge has also been used to aug-ment supervised NER approaches.
Kazama andTorisawa (2007) improve their F -score by 3% byincluding a Wikipedia-based feature in their ma-chine learner.
Such approaches are limited by thegold-standard data already available.Less common is the automatic creation of train-ing data.
An et al (2003) extracted sentences con-taining listed entities from the web, and produceda 1.8 million word Korean corpus that gave sim-ilar results to manually-annotated training data.Richman and Schone (2008) used a method sim-ilar to Nothman et al (2008) in order to deriveNE-annotated corpora in languages other than En-glish.
They classify Wikipedia articles in foreignlanguages by transferring knowledge from EnglishWikipedia via inter-language links.
With theseclassifications they automatically annotate entirearticles for NER training, and suggest that their re-sults with a 340k-word Spanish corpus are compa-rable to 20k-40k words of gold-standard trainingdata when using MUC-style evaluation metrics.2.1 Gold-standard corporaWe evaluate our Wikipedia-derived corporaagainst three sets of manually-annotated data from(a) the MUC-7 Named Entity Task (MUC, 2001);(b) the English CoNLL-03 Shared Task (TjongKim Sang and De Meulder, 2003); (c) theBBN Pronoun Coreference and Entity TypeCorpus (Weischedel and Brunstein, 2005).
Weconsider only the generic newswire NER task,although domain-specific annotated corpora havebeen developed for applications such as bio-textmining (Kim et al, 2003).Stylistic and genre differences between thesource texts affect compatibility for NER evalua-tion e.g., the CoNLL corpus formats headlines inall-caps, and includes non-sentential data such astables of sports scores.Each corpus uses a different set of entity labels.MUC marks locations (LOC), organisations (ORG)and personal names (PER), in addition to numeri-cal and time information.
The CoNLL NER sharedtasks (Tjong Kim Sang, 2002; Tjong Kim Sangand De Meulder, 2003) mark PER, ORG and LOCentities, as well as a broad miscellaneous classCorpus # tagsNumber of tokensTRAIN DEV TESTMUC-7 3 83601 18655 60436CoNLL-03 4 203621 51362 46435BBN 54 901894 142218 129654Table 1: Gold-standard NE-annotated corpora(MISC; e.g.
events, artworks and nationalities).BBN annotates the entire Penn Treebank corpuswith 105 fine-grained tags (Brunstein, 2002): 54corresponding to CoNLL entities; 21 for numeri-cal and time data; and 30 for other classes.
Forour evaluation, BBN?s tags were reduced to theequivalent CoNLL tags, with extra tags in the BBNand MUC data removed.
Since no MISC tags aremarked in MUC, they need to be removed fromCoNLL, BBN and Wikipedia data for comparison.We transformed all three corpora into a com-mon format and annotated them with part-of-speech tags using the Penn Treebank-trainedC&C POS tagger.
We altered the default MUCtokenisation to attach periods to abbreviationswhen sentence-internal.
While standard training(TRAIN), development (DEV) and final test (TEST)set divisions were available for CoNLL and MUC,the BBN corpus was split at our discretion: sec-tions 03?21 for TRAIN, 00?02 for DEV and 22-24for TEST.
Corpus sizes are compared in Table 1.2.2 Evaluating NER performanceOne challenge for NER research is establishing anappropriate evaluation metric (Nadeau and Sekine,2007).
In particular, entities may be correctlydelimited but mis-classified, or entity boundariesmay be mismatched.MUC (Chinchor, 1998) awarded equal score formatching type, where an entity?s class is identi-fied with at least one boundary matching, and text,where an entity?s boundaries are precisely delim-ited, irrespective of the classification.
This equalweighting is unrealistic, as some boundary errorsare highly significant, while others are arbitrary.CoNLL awarded exact (type and text) phrasalmatches, ignoring boundary issues entirely andproviding a lower-bound measure of NER per-formance.
Manning (2006) argues that CoNLL-style evaluation is biased towards systems whichleave entities with ambiguous boundaries un-tagged, since boundary errors amount simultane-ously to false positives and false negatives.
In bothMUC and CoNLL, micro-averaged precision, recalland F1 score summarise the results.613Tsai et al (2006) compares a number of meth-ods for relaxing boundary requirements: matchingonly the left or right boundary, any tag overlap,per-token measures, or more semantically-drivenmatching.
ACE evaluations instead use a customiz-able evaluation metric with weights specified fordifferent types of error (NIST-ACE, 2008).3 Corpus and error analysis approachesTo evaluate the performance impact of a corpuswe may analyse (a) the annotations themselves; or(b) the model built on those annotations and itsperformance.
A corpus can be considered in isola-tion or by comparison with other corpora.
We usethree methods to explore intra- and inter-corpusconsistency in MUC, CoNLL, and BBN in Section 4.3.1 N-gram tag variationDickinson and Meurers (2003) present a clevermethod for finding inconsistencies within POS an-notated corpora, which we apply to NER corpora.Their approach finds all n-grams in a corpus whichappear multiple times, albeit with variant tags forsome sub-sequence, the nucleus (see e.g.
Table3).
To remove valid ambiguity, they suggest us-ing (a) a minimum n-gram length; (b) a minimummargin of invariant terms around the nucleus.For example, the BBN TRAIN corpus includeseight occurrences of the 6-gram the San FranciscoBay area ,.
Six instances of area are tagged as non-entities, but two instances are tagged as part of theLOC that precedes it.
The other five tokens in thisn-gram are consistently labelled.3.2 Entity type frequencyAn intuitive approach to finding discrepancies be-tween corpora is to compare the distribution of en-tities within each corpus.
To make this manage-able, instances need to be grouped by more thantheir class labels.
We used the following groups:POS sequences: Types of candidate entities mayoften be distinguished by their POS tags, e.g.nationalities are often JJ or NNPS.Wordtypes: Collins (2002) proposed wordtypeswhere all uppercase characters map to A, low-ercase to a, and digits to 0.
Adjacent charac-ters in the same orthographic class were col-lapsed.
However, we distinguish single frommultiple characters by duplication.
e.g.
USSNimitz (CVN-68) has wordtype AA Aaa (AA-00).Wordtype with functions: We also map contentwords to wordtypes only?function wordsare retained, e.g.
Bank of New England Corp.maps to Aaa of Aaa Aaa Aaa..No approach provides sufficient discriminationalone: wordtype patterns are able to distinguishwithin common POS tags and vice versa.
Eachmethod can be further simplified by merging re-peated tokens, NNP NNP becoming NNP.By calculating the distribution of entities overthese groupings, we can find anomalies betweencorpora.
For instance, 4% of MUC?s and 5.9%of BBN?s PER entities have wordtype Aaa A. Aaa,e.g.
David S. Black, while CoNLL has only 0.05% ofPERs like this.
Instead, CoNLL has many names ofform A. Aaa, e.g.
S. Waugh, while BBN and MUChave none.
We can therefore predict incompatibil-ities between systems trained on BBN and evalu-ated on CoNLL or vice-versa.3.3 Tag sequence confusionA confusion matrix between predicted and correctclasses is an effective method of error analysis.For phrasal sequence tagging, this can be appliedto either exact boundary matches or on a per-tokenbasis, ignoring entity bounds.
We instead compiletwo matrices: C/P comparing correct entity classesagainst predicted tag sequences; and P/C compar-ing predicted classes to correct tag sequences.C/P equates oversized boundaries to correctmatches, and tabulates cases of undersized bound-aries.
For example, if [ORG Johnson and Johnson] istagged [PER Johnson] and [PER Johnson], it is markedin matrix coordinates (ORG, PER O PER).
P/C em-phasises oversized boundaries: if gold-standardMr.
[PER Ross] is tagged PER, it is counted as con-fusion between PER and O PER.
To further dis-tinguish classes of error, the entity type groupingsfrom Section 3.2 are also used.This analysis is useful for both tagger evalua-tion and cross-corpus evaluation, e.g.
BBN versusCoNLL on a BBN test set.
This involves findingconfusion matrix entries where BBN and CoNLL?sperformance differs significantly, identifying com-mon errors related to difficult instances in the testcorpus as well as errors in the NER model.4 Comparing gold-standard corporaWe trained the C&C NER tagger (Curran and Clark,2003) to build separate models for each gold-standard corpus.
The C&C tagger utilises a number614TRAINWith MISC Without MISCCoNLL BBN MUC CoNLL BBNMUC ?
?
73.5 55.5 67.5CoNLL 81.2 62.3 65.9 82.1 62.4BBN 54.7 86.7 77.9 53.9 88.4Table 2: Gold standard F -scores (exact-match)of orthographic, contextual and in-document fea-tures, as well as gazetteers for personal names.
Ta-ble 2 shows that each training set performs muchbetter on corresponding (same corpus) test sets(italics) than on test sets from other sources, alsoidentified by (Ciaramita and Altun, 2005).
NERresearch typically deals with small improvements(?1% F -score).
The 12-32% mismatch betweentraining and test corpora suggests that an appropri-ate training corpus is a much greater concern.
Theexception is BBN on MUC, due to differing TESTand DEV subject matter.
Here we analyse the vari-ation within and between the gold standards.Table 3 lists some n-gram tag variations for BBNand CoNLL (TRAIN + DEV).
These include cases ofschematic variations (e.g.
the period in Co .)
andtagging errors.
Some n-grams have three variants,e.g.
the Standard & Poor ?s 500 which appears un-tagged, as the [ORG Standard & Poor] ?s 500, or the[ORG Standard & Poor ?s] 500.
MUC is too small forthis method.
CoNLL only provides only a few ex-amples, echoing BBN in the ambiguities of trailingperiods and leading determiners or modifiers.Wordtype distributions were also used to com-pare the three gold standards.
We investigated allwordtypes which occur with at least twice the fre-quency in one corpus as in another, if that word-type was sufficiently frequent.
Among the differ-ences recovered from this analysis are:?
CoNLL has an over-representation of uppercase wordsdue to all-caps headlines.?
Since BBN also annotates common nouns, some havebeen mistakenly labelled as proper-noun entities.?
BBN tags text like Munich-based as LOC; CoNLLtags it as MISC; MUC separates the hyphen as a token.?
CoNLL is biased to sports and has many event namesin the form of 1990 World Cup.?
BBN separates organisation names from their productsas in [ORG Commodore] [MISC 64].?
CoNLL has few references to abbreviated US states.?
CoNLL marks conjunctions of people (e.g.
Ruth andEdwin Brooks) as a single PER entity.?
CoNLL text has Co Ltd instead of Co. Ltd.We analysed the tag sequence confusion whentraining with each corpus and testing on BBN DEV.While full confusion matrices are too large for thispaper, Table 4 shows some examples where theFigure 1: Deriving training data from WikipediaNER models disagree.
MUC fails to correctly tagU.K.
and U.S.. U.K. only appears once in MUC, andU.S.
appears 22 times as ORG and 77 times as LOC.CoNLL has only three instances of Mr., so it oftenmis-labels Mr. as part of a PER entity.
The MUCmodel also has trouble recognising ORG namesending with corporate abbreviations, and may failto identify abbreviated US state names.Our analysis demonstrates that seemingly mi-nor orthographic variations in the text, tokenisa-tion and annotation schemes can have a huge im-pact on practical NER performance.5 From Wikipedia to NE-annotated textWikipedia is a collaborative, multilingual, onlineencyclopedia which includes over 2.3 million arti-cles in English alone.
Our baseline approach de-tailed in Nothman et al (2008) exploits the hyper-linking between articles to derive a NE corpus.Since ?74% of Wikipedia articles describe top-ics covering entity classes, many of Wikipedia?slinks correspond to entity annotations in gold-standard NE corpora.
We derive a NE-annotatedcorpus by the following steps:1.
Classify all articles into entity classes2.
Split Wikipedia articles into sentences3.
Label NEs according to link targets4.
Select sentences for inclusion in a corpus615N-gram Tag # Tag #Co .
- 52 ORG 111Smith Barney , Harris Upham & Co. - 1 ORG 9the Contra rebels MISC 1 ORG 2in the West is - 1 LOC 1that the Constitution MISC 2 - 1Chancellor of the Exchequer Nigel Lawson - 11 ORG 2the world ?s - 80 LOC 11993 BellSouth Classic - 1 MISC 1Atlanta Games LOC 1 MISC 1Justice Minister - 1 ORG 1GOLF - GERMAN OPEN - 2 LOC 1Table 3: Examples of n-gram tag variations in BBN (top) and CoNLL (bottom).
Nucleus is in bold.Tag sequenceGrouping# if trained onExampleCorrect Pred.
MUC CoNLL BBNLOC LOC A.A. 101 349 343 U.K.- PER PER Aa.
Aaa 9 242 0 Mr. Watson- LOC Aa.
16 109 0 Mr.ORG ORG Aaa Aaa.
118 214 218 Campeau Corp.LOC - Aaa.
20 0 3 Calif.Table 4: Tag sequence confusion on BBN DEV when training on gold-standard corpora (no MISC)In Figure 1, a sentence introducing Holden as anAustralian car maker based in Port Melbourne haslinks to separate articles about each entity.
Cuesin the linked article about Holden indicate that it isan organisation, and the article on Port Melbourneis likewise classified as a location.
The originalsentence can then be automatically annotated withthese facts.
We thus extract millions of sentencesfrom Wikipedia to form a new NER corpus.We classify each article in a bootstrapping pro-cess using its category head nouns, definitionalnouns from opening sentences, and title capital-isation.
Each article is classified as one of: un-known; a member of a NE category (LOC, ORG,PER, MISC, as per CoNLL); a disambiguation page(these list possible referent articles for a given ti-tle); or a non-entity (NON).
This classifier classi-fier achieves 89% F -score.A sentence is selected for our corpus when allof its capitalised words are linked to articles with aknown class.
Exceptions are made for common ti-tlecase words, e.g.
I, Mr., June, and sentence-initialwords.
We also infer additional links ?
variant ti-tles are collected for each Wikipedia topic and aremarked up in articles which link to them ?
whichNothman et al (2008) found increases coverage.Transforming links into annotations that con-form to a gold standard is far from trivial.
Linkboundaries need to be adjusted, e.g.
to remove ex-cess punctuation.
Adjectival forms of entities (e.g.American, Islamic) generally link to nominal arti-cles.
However, they are treated by CoNLL and ourN-gram Tag # Tag #of Batman ?s MISC 2 PER 5in the Netherlands - 58 LOC 4Chicago , Illinois - 8 LOC 3the American and LOC 1 MISC 2Table 5: N-gram variations in the Wiki baselineBBN mapping as MISC.
POS tagging the corpus andrelabelling entities ending with JJ as MISC solvesthis heuristically.
Although they are capitalised inEnglish, personal titles (e.g.
Prime Minister) are nottypically considered entities.
Initially we assumethat all links immediately preceding PER entitiesare titles and delete their entity classification.6 Improving Wikipedia performanceThe baseline system described above achievesonly 58.9% and 62.3% on the CoNLL andBBN TEST sets (exact-match scoring) with 3.5-million training tokens.
We apply methods pro-posed in Section 3 to to identify and minimiseWikipedia errors on the BBN DEV corpus.We begin by considering Wikipedia?s internalconsistency using n-gram tag variation (Table 5).The breadth of Wikipedia leads to greater genuineambiguity, e.g.
Batman (a character or a comicstrip).
It also shares gold-standard inconsistencieslike leading modifiers.
Variations in American andChicago, Illinois indicate errors in adjectival entitylabels and in correcting link boundaries.Some errors identified with tag sequence confu-sion are listed in Table 6.
These correspond to re-616Tag sequenceGrouping# if trained onExampleCorrect Pred.
BBN WikiLOC LOC Aaa.
103 14 Calif.LOC - LOC ORG Aaa , Aaa.
0 15 Norwalk , Conn.LOC LOC Aaa-aa 23 0 Texas-based- PER PER Aa.
Aaa 4 208 Mr. Yamamoto- PER PER Aaa Aaa 1 49 Judge Keenan- PER Aaa 7 58 PresidentMISC MISC A.
25 1 R.MISC LOC NNPS 0 39 SovietsTable 6: Tag sequence confusion on BBN DEV with training on BBN and the Wikipedia baselinesults of an entity type frequency analysis and mo-tivate many of our Wikipedia extensions presentedbelow.
In particular, personal titles are tagged asPER rather than unlabelled; plural nationalities aretagged LOC, not MISC; LOCs hyphenated to fol-lowing words are not identified; nor are abbrevi-ated US state names.
Using R. to abbreviate Re-publican in BBN is also a high-frequency error.6.1 Inference from disambiguation pagesOur baseline system infers extra links using a setof alternative titles identified for each article.
Weextract the alternatives from the article and redirecttitles, the text of all links to the article, and the firstand last word of the article title if it is labelled PER.Our extension is to extract additional inferred ti-tles fromWikipedia?s disambiguation pages.
Mostdisambiguation pages are structured as lists of ar-ticles that are often referred to by the titleD beingdisambiguated.
For each link with target A thatappears at the start of a list item on D?s page, Dand its redirect aliases are added to the list of al-ternative titles for A.Our new source of alternative titles includesacronyms and abbreviations (AMP links to AMPLimited and Ampere), and given or family names(Howard links to Howard Dean and John Howard).6.2 Personal titlesPersonal titles (e.g.
Brig.
Gen., Prime Minister-elect) are capitalised in English.
Titles are some-times linked in Wikipedia, but the target articles,e.g.
U.S. President, are in Wikipedia categories likePresidents of the United States, causing their incor-rect classification as PER.Our initial implementation assumed that linksimmediately preceding PER entity links are titles.While this feature improved performance, it onlycaptured one context for personal titles and failedto handle instances where the title was only aportion of the link text, such as Australian PrimeMinister-elect or Prime Minister of Australia.To handle titles more comprehensively, wecompiled a list of the terms most frequently linkedimmediately prior to PER links.
These were man-ually filtered, removing LOC or ORG mentions andcomplemented with abbreviated titles extractedfrom BBN, producing a list of 384 base title forms,11 prefixes (e.g.
Vice) and 3 suffixes (e.g.
-elect).Using these gazetteers, titles are stripped of erro-neous NE tags.6.3 Adjectival formsIn English, capitalisation is retained in adjectivalentity forms, such as American or Islamic.
Whilethese are not exactly entities, both CoNLL and BBNannotate them as MISC.
Our baseline approachPOS tagged the corpus and marked all adjectivalentities as MISC.
This missed instances where na-tionalities are used nominally, e.g.
five Italians.We extracted 339 frequent LOC and ORG ref-erences with POS tag JJ.
Words from this list(e.g.
Italian) are relabelled MISC, irrespective ofPOS tag or pluralisation (e.g.
Italian/JJ, Italian/NNP,Italian/NNPS).
This unfiltered list includes some er-rors from POS tagging, e.g.
First, Emmy; and otherswhere MISC is rarely the appropriate tag, e.g.
theDemocrats (an ORG).6.4 Miscellaneous changesEntity-word aliases Longest-string matching forinferred links often adds redundant words, e.g.both Australian and Australian people are redirects toAustralia.
We therefore exclude from inference ti-tles of form X Y where X is an alias of the samearticle and Y is lowercase.State abbreviations A gold standard may usestylistic forms which are rare in Wikipedia.
Forinstance, the Wall Street Journal (BBN) uses USstate abbreviations, while Wikipedia nearly al-ways refers to states in full.
We boosted perfor-mance by substituting a random selection of USstate names in Wikipedia with their abbreviations.617TRAINWith MISC No MISCCoN.
BBN MUC CoN.
BBNMUC ?
?
82.3 54.9 69.3CoNLL 85.9 61.9 69.9 86.9 60.2BBN 59.4 86.5 80.2 59.0 88.0WP0 ?
no inf.
62.8 69.7 69.7 64.7 70.0WP1 67.2 73.4 75.3 67.7 73.6WP2 69.0 74.0 76.6 69.4 75.1WP3 68.9 73.5 77.2 69.5 73.7WP4 ?
all inf.
66.2 72.3 75.6 67.3 73.3Table 7: Exact-match DEV F -scoresRemoving rare cases We explicitly removedsentences containing title abbreviations (e.g.
Mr.)appearing in non-PER entities such as movie titles.Compared to newswire, these forms as personaltitles are rare in Wikipedia, so their appearance inentities causes tagging errors.
We used a similarapproach to personal names including of, whichalso act as noise.Fixing tokenization Hyphenation is a problemin tokenisation: should London-based be one token,two, or three?
Both BBN and CoNLL treat it as onetoken, but BBN labels it a LOC and CoNLL a MISC.Our baseline had split hyphenated portions fromentities.
Fixing this to match the BBN approachimproved performance significantly.7 ExperimentsWe evaluated our annotation process by build-ing separate NER models learned from Wikipedia-derived and gold-standard data.
Our results aregiven as micro-averaged precision, recall and F -scores both in terms of MUC-style and CoNLL-style(exact-match) scoring.
We evaluated all experi-ments with and without the MISC category.Wikipedia?s articles are freely available fordownload.1 We have used data from the 2008May 22 dump of English Wikipedia which in-cludes 2.3 million articles.
Splitting this into sen-tences and tokenising produced 32 million sen-tences each containing an average of 24 tokens.Our experiments were performed with aWikipedia corpus of 3.5 million tokens.
Althoughwe had up to 294 million tokens available, wewere limited by the RAM required by the C&C tag-ger training software.8 ResultsTables 7 and 8 show F -scores on the MUC, CoNLL,and BBN development sets for CoNLL-style exact1http://download.wikimedia.org/TRAINWith MISC No MISCCoN.
BBN MUC CoN.
BBNMUC ?
?
89.0 68.2 79.2CoNLL 91.0 75.1 81.4 90.9 72.6BBN 72.7 91.1 87.6 71.8 91.5WP0 ?
no inf.
71.0 79.3 76.3 71.1 78.7WP1 74.9 82.3 81.4 73.1 81.0WP2 76.1 82.7 81.6 74.5 81.9WP3 76.3 82.2 81.9 74.7 80.7WP4 ?
all inf.
74.3 81.4 80.9 73.1 80.7Table 8: MUC-style DEV F -scoresTraining corpusDEV (MUC-style F )MUC CoNLL BBNCorresponding TRAIN 89.0 91.0 91.1TRAIN + WP2 90.6 91.7 91.2Table 9: Wikipedia as additional training dataTRAINWith MISC No MISCCoN.
BBN MUC CoN.
BBNMUC ?
?
73.5 55.5 67.5CoNLL 81.2 62.3 65.9 82.1 62.4BBN 54.7 86.7 77.9 53.9 88.4WP2 60.9 69.3 76.9 61.5 69.9Table 10: Exact-match TEST results for WP2TRAINWith MISC No MISCCoN.
BBN MUC CoN.
BBNMUC ?
?
81.0 68.5 77.6CoNLL 87.8 75.0 76.2 87.9 74.1BBN 69.3 91.1 83.6 68.5 91.9WP2 70.2 79.1 81.3 68.6 77.3Table 11: MUC-eval TEST results for WP2match and MUC-style evaluations (which are typi-cally a few percent higher).
The cross-corpus goldstandard experiments on the DEV sets are shownfirst in both tables.
As in Table 2, the performancedrops significantly when the training and test cor-pus are from different sources.
The correspondingTEST set scores are given in Tables 9 and 10.The second group of experiments in these ta-bles show the performance of Wikipedia corporawith increasing levels of link inference (describedin Section 6.1).
Links inferred upon match-ing article titles (WP1) and disambiguation ti-tles (WP2) consistently increase F -score by ?5%,while surnames for PER entities (WP3) and all linktexts (WP4) tend to introduce error.
A key re-sult of our work is that the performance of non-corresponding gold standards is often significantlyexceeded by our Wikipedia training data.Our third group of experiments combined ourWikipedia corpora with gold-standard data to im-prove performance beyond traditional train-testpairs.
Table 9 shows that this approach may lead618Token Corr.
Pred.
Count Why?.
ORG - 90 Inconsistencies in BBNHouse ORG LOC 56 Article White House is a LOC due to classification bootstrappingWall - LOC 33 Wall Street is ambiguously a location and a conceptGulf ORG LOC 29 Georgia Gulf is common in BBN, but Gulf indicates LOC, ORG - 26 A difficult NER ambiguity in e.g.
Robertson , Stephens & Co.?s ORG - 25 Unusually high frequency of ORGs ending ?s in BBNSenate ORG LOC 20 Classification bootstrapping identifies Senate as a house, i.e.
LOCS&P - MISC 20 Rare in Wikipedia, and inconsistently labelled in BBND.
MISC PER 14 BBN uses D. to abbreviate DemocratTable 12: Tokens in BBN DEV that our Wikipedia model frequently mis-taggedClassBy exact phrase By tokenP R F P R FLOC 66.7 87.9 75.9 64.4 89.8 75.0MISC 48.8 58.7 53.3 46.5 61.6 53.0ORG 76.9 56.5 65.1 88.9 68.1 77.1PER 67.3 91.4 77.5 70.5 93.6 80.5All 68.6 69.9 69.3 80.9 75.3 78.0Table 13: Category results for WP2 on BBN TESTto small F -score increases.Our per-class Wikipedia results are shown inTable 13.
LOC and PER entities are relatively easyto identify, although a low precision for PER sug-gests that many other entities have been markederroneously as people, unlike the high precisionand low recall of ORG.
As an ill-defined category,with uncertain mapping between BBN and CoNLLclasses, MISC precision is unsurprisingly low.
Wealso show results evaluating the correct labellingof each token, where Nothman et al (2008) hadreported results 13% higher than phrasal match-ing, reflecting a failure to correctly identify entityboundaries.
We have reduced this difference to9%.
A BBN-trained model gives only 5% differ-ence between phrasal and token F -score.Among common tagging errors, we identified:tags continuing over additional words as in NewYork-based Loews Corp. all being marked as a sin-gle ORG; nationalities marked as LOC rather thanMISC; White House a LOC rather than ORG, aswith many sports teams; single-word ORG entitiesmarked as PER; titles such as Dr. included in PERtags; mis-labelling un-tagged title-case terms andtagged lowercase terms in the gold-standard.The corpus analysis methods described inSection 3 show greater similarity between ourWikipedia-derived corpus and BBN after imple-menting our extensions.
There is nonethelessmuch scope for further analysis and improvement.Notably, the most commonly mis-tagged tokens inBBN (see Table 12) relate more often to individualentities and stylistic differences than to a general-isable class of errors.9 ConclusionWe have demonstrated the enormous variability inperformance between using NER models trainedand tested on the same corpus versus tested onother gold standards.
This variability arises fromnot only mismatched annotation schemes but alsostylistic conventions, tokenisation, and missingfrequent lexical items.
Therefore, NER corporamust be carefully matched to the target text for rea-sonable performance.
We demonstrate three ap-proaches for gauging corpus and annotation mis-match, and apply them to MUC, CoNLL and BBN,and our automatically-derived Wikipedia corpora.There is much room for improving the results ofour Wikipedia-based NE annotations.
In particu-lar, a more careful approach to link inference mayfurther reduce incorrect boundaries of tagged en-tities.
We plan to increase the largest training setthe C&C tagger can support so that we can fullyexploit the enormous Wikipedia corpus.However, we have shown that Wikipedia canbe used a source of free annotated data for train-ing NER systems.
Although such corpora needto be engineered specifically to a desired appli-cation, Wikipedia?s breadth may permit the pro-duction of large corpora even within specific do-mains.
Our results indicate that Wikipedia datacan perform better (up to 11% for CoNLL on MUC)than training data that is not matched to the eval-uation, and hence is widely applicable.
Trans-formingWikipedia into training data thus providesa free and high-yield alternative to the laboriousmanual annotation required for NER.AcknowledgmentsWe would like to thank the Language Technol-ogy Research Group and the anonymous review-ers for their feedback.
This project was sup-ported by Australian Research Council DiscoveryProject DP0665973 and Nothman was supportedby a University of Sydney Honours Scholarship.619ReferencesJoohui An, Seungwoo Lee, and Gary Geunbae Lee.2003.
Automatic acquisition of named entity taggedcorpus from world wide web.
In The CompanionVolume to the Proceedings of 41st Annual Meetingof the Association for Computational Linguistics,pages 165?168.Ada Brunstein.
2002.
Annotation guidelines for an-swer types.
LDC2005T33.Nancy Chinchor.
1998.
Overview of MUC-7.
In Proc.of the 7th Message Understanding Conference.Massimiliano Ciaramita and Yasemin Altun.
2005.Named-entity recognition in novel domains with ex-ternal lexical knowledge.
In Proceedings of theNIPS Workshop on Advances in Structured Learningfor Text and Speech Processing.Michael Collins.
2002.
Ranking algorithms fornamed-entity extraction: boosting and the voted per-ceptron.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages489?496, Morristown, NJ, USA.James R. Curran and Stephen Clark.
2003.
Languageindependent NER using a maximum entropy tagger.In Proceedings of the 7th Conference on NaturalLanguage Learning, pages 164?167.Markus Dickinson and W. Detmar Meurers.
2003.
De-tecting errors in part-of-speech annotation.
In Pro-ceedings of the 10th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 107?114, Budapest, Hungary.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:An experimental study.
Artificial Intelligence,165(1):91?134.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Ex-ploiting Wikipedia as external knowledge for namedentity recognition.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 698?707.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, andJun?ichi Tsujii.
2003.
GENIA corpus?a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19(suppl.
1):i180?i182.Christopher Manning.
2006.
Doing named entityrecognition?
Don?t optimize for F1.
In NLPersBlog, 25 August.
http://nlpers.blogspot.com.Andrei Mikheev, Marc Moens, and Claire Grover.1999.
Named entity recognition without gazetteers.In Proceedings of the 9th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 1?8, Bergen, Norway.2001.
Message Understanding Conference (MUC) 7.Linguistic Data Consortium, Philadelphia.David Nadeau and Satoshi Sekine.
2007.
A sur-vey of named entity recognition and classification.Lingvisticae Investigationes, 30:3?26.David Nadeau, Peter D. Turney, and Stan Matwin.2006.
Unsupervised named-entity recognition:Generating gazetteers and resolving ambiguity.
InProceedings of the 19th Canadian Conference onArtificial Intelligence, volume 4013 of LNCS, pages266?277.NIST-ACE.
2008.
Automatic content extraction 2008evaluation plan (ACE08).
NIST, April 7.Joel Nothman, James R. Curran, and Tara Murphy.2008.
Transforming Wikipedia into named entitytraining data.
In Proceedings of the Australian Lan-guage Technology Workshop, pages 124?132, Ho-bart.Alexander E. Richman and Patrick Schone.
2008.Mining wiki resources for multilingual named entityrecognition.
In 46th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 1?9, Columbus, Ohio.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
InProceedings of the 7th Conference on Natural Lan-guage Learning, pages 142?147.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 shared task: language-independentnamed entity recognition.
In Proceedings of the 6thConference on Natural Language Learning, pages1?4.Antonio Toral, Rafael Mun?oz, and Monica Monachini.2008.
Named entityWordNet.
In Proceedings of the6th International Language Resources and Evalua-tion Conference.Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-ChiChou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting-Yi Sung, and Wen-Lian Hsu.
2006.
Various criteriain the evaluation of biomedical named entity recog-nition.
BMC Bioinformatics, 7:96?100.RalphWeischedel and Ada Brunstein.
2005.
BBN Pro-noun Coreference and Entity Type Corpus.
Linguis-tic Data Consortium, Philadelphia.620
