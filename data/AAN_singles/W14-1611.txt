Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98?108,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsImproved Pattern Learning for Bootstrapped Entity ExtractionSonal Gupta Christopher D. ManningDepartment of Computer ScienceStanford University{sonal, manning}@cs.stanford.eduAbstractBootstrapped pattern learning for entityextraction usually starts with seed entitiesand iteratively learns patterns and entitiesfrom unlabeled text.
Patterns are scoredby their ability to extract more positive en-tities and less negative entities.
A prob-lem is that due to the lack of labeled data,unlabeled entities are either assumed to benegative or are ignored by the existing pat-tern scoring measures.
In this paper, weimprove pattern scoring by predicting thelabels of unlabeled entities.
We use var-ious unsupervised features based on con-trasting domain-specific and general text,and exploiting distributional similarity andedit distances to learned entities.
Oursystem outperforms existing pattern scor-ing algorithms for extracting drug-and-treatment entities from four medical fo-rums.1 IntroductionThis paper considers the problem of building ef-fective entity extractors for custom entity typesfrom specialized domain corpora.
We approachthe problem by learning rules bootstrapped us-ing seed sets of entities.
Though entity extrac-tion using machine learning is common in aca-demic research, rule-based systems dominate incommercial use (Chiticariu et al., 2013), mainlybecause rules are effective, interpretable, and areeasy to customize by non-experts to cope with er-rors.
They also have been shown to perform bet-ter than state-of-the-art machine learning methodson some specialized domains (Nallapati and Man-ning, 2008; Gupta and Manning, 2014a).
In ad-dition, building supervised machine learning sys-tems for a reasonably large domain-specific cor-pus would require hand-labeling sufficient data toSeed dictionaryforclass ?animals?
: {dog}Text:I own acatnamed Fluffy.
I run with my petdog.
I also nap with my pet cat.
I owna car.Pattern1: my pet XExtractions= positive: {dog}, unlabeled: {cat}Pattern2: owna XExtractions= positive: {dog}, unlabeled: {car}Figure 1: An example pattern learning system forthe class ?animals?
from the text.
Pattern 1 and 2are candidate patterns.
Text matched with the pat-terns is shown in italics and the extracted entitiesare shown in bold.train a model, which can be costly and time con-suming.
Bootstrapped machine-learned rules canmake extraction easier and more efficient on sucha corpus.In a bootstrapped rule-based entity learningsystem, seed dictionaries and/or patterns provideweak supervision to label data.
The system itera-tively learns new entities belonging to a specificclass from unlabeled text (Riloff, 1996; Collinsand Singer, 1999).
Rules are typically definedby creating patterns around the entities, suchas lexico-syntactic surface word patterns (Hearst,1992) and dependency tree patterns (Yangarberet al., 2000).
Patterns are scored by their abil-ity to extract more positive entities and less neg-ative entities.
Top ranked patterns are used toextract candidate entities from text.
High scor-ing candidate entities are added to the dictionariesand are used to generate more candidate patternsaround them.
In a supervised setting, the efficacyof patterns can be judged by their performanceon a fully labeled dataset (Califf and Mooney,1999; Ciravegna, 2001).
In a bootstrapped sys-tem, where the data is not fully labeled, existingsystems score patterns by either ignoring the un-98labeled entities or assuming them to be negative.However, these scoring schemes cannot differenti-ate between patterns that extract good versus badunlabeled entities.
The problem is similar to theclosed world assumption in distantly supervisedinformation extraction systems, when all proposi-tions missing from a knowledge base are consid-ered false (Ritter et al., 2013; Xu et al., 2013).Predicting labels of unlabeled entities can im-prove scoring patterns.
Consider the exampleshown in Figure 1.
Current pattern learning sys-tems would score both patterns equally.
However,features like distributional similarity can predict?cat?
to be closer to {dog} than ?car?, and a pat-tern learning system can use that information torank ?Pattern 1?
higher than ?Pattern 2?.In this paper, we work on bootstrapping en-tity extraction using seed sets of entities and anunlabeled text corpus.
We improve the scoringof patterns for an entity class by defining a pat-tern?s score by the number of positive entities itextracts and the ratio of number of positive entitiesto expected number of negative entities it extracts.Our main contribution is introducing the expectednumber of negative entities in pattern scoring ?
wepredict probabilities of unlabeled entities belong-ing to the negative class.
We estimate an unla-beled entity?s negative class probability by averag-ing probabilities from various unsupervised classpredictors, such as distributional similarity, stringedit distances from learned entities, and TF-IDFscores.
Our system performs significantly betterthan existing pattern scoring measures for extract-ing drug-and-treatment entities from four medi-cal forums on MedHelp1, a user health discussionwebsite.We release the code for the systems described inthis paper at http://nlp.stanford.edu/software/patternslearning.shtml.We also release a visualization tool, describedin Gupta and Manning (2014b), that visualizesand compares output of multiple pattern-basedentity extraction systems.
It can be downloaded athttp://nlp.stanford.edu/software/patternviz.shtml.2 Related WorkRule based learning has been a topic of interestfor many years.
Patwardhan (2010) gives a goodoverview of the research in the field.
Rule learn-1www.medhelp.orging systems differ in how they create rules, scorethem, and score the entities they extract.
Here, wemainly discuss the rule scoring part of the previousentity extraction research.The pioneering work by Hearst (1992) usedhand written rules to automatically generatemore rules that were manually evaluated toextract hypernym-hyponym pairs from text.Other supervised systems like SRV (Freitag,1998), SLIPPER (Cohen and Singer, 1999),(LP )2(Ciravegna, 2001), and RAPIER (Califfand Mooney, 1999) used a fully labeled corpus toeither create or score rules.Riloff (1996) used a set of seed entities tobootstrap learning of rules for entity extractionfrom unlabeled text.
She scored a rule by aweighted conditional probability measure esti-mated by counting the number of positive entitiesamong all the entities extracted by the rule.
Thelenand Riloff (2002) extended the above bootstrap-ping algorithm for multi-class learning.
Yangar-ber et al.
(2002) and Lin et al.
(2003) used a com-bination of accuracy and confidence of a patternfor multiclass entity learning, where the accuracymeasure ignored unlabeled entities and the con-fidence measure treated them as negative.
Guptaand Manning (2014a) used the ratio of scaled fre-quencies of positive entities among all extractedentities.
None of the above measures predict labelsof unlabeled entities to score patterns.
Our sys-tem outperforms them in our experiments.
Steven-son and Greenwood (2005) used Wordnet to assesspatterns, which is not feasible for domains thathave low coverage in Wordnet, such as medicaldata.More recently, open information extractionsystems have garnered attention.
They focuson extracting entities and relations from theweb.
KnowItAll?s entity extraction from theweb (Downey et al., 2004; Etzioni et al., 2005)used components such as list extractors, genericand domain specific pattern learning, and subclasslearning.
They learned domain-specific patternsusing a seed set and scored them by ignoring un-labeled entities.
One of our baselines is similarto their domain-specific pattern learning compo-nent.
Carlson et al.
(2010) learned multiple se-mantic types using coupled semi-supervised train-ing from web-scale data, which is not feasible forall datasets and entity learning tasks.
They as-sessed patterns by their precision, assuming unla-99beled entities to be negative; one of our baselinesis similar to their pattern assessment method.Other open information extraction systems likeReVerb (Fader et al., 2011) and OLLIE (Mausamet al., 2012) are mainly geared towards generic,domain-independent relation extractors for webdata.
We tested learning an entity extractor for agiven class using ReVerb.
We labeled the binaryand unary ReVerb extractions using the class seedentities and retrained its confidence function, withpoor results.
Poon and Domingos (2010) founda similar result for inducing a probabilistic ontol-ogy: an open information extraction system ex-tracted low accuracy relational triples on a smallcorpus.In this paper, we use features such as distribu-tional similarity and edit distances from learnedentities to score patterns.
Similar measures havebeen used before but for learning entities, label-ing semantic classes, or for reducing noise in seedsets (Pantel and Ravichandran, 2004; McIntoshand Curran, 2009).
Measures for improving en-tity learning can be used alongside ours since wefocus on scoring candidate patterns.3 ApproachWe use lexico-syntactic surface word patterns toextract entities from unlabeled text starting withseed dictionaries of entities for multiple classes.For ease of exposition, we present the approachbelow for learning entities for one class C. It caneasily be generalized to multiple classes.
We re-fer to entities belonging to C as positive and en-tities belonging to all other classes as negative.The bootstrapping process involves the followingsteps, iteratively performed until no more patternsor entities can be learned.1.
Labeling data and creating patterns: The textis labeled using the class dictionaries, start-ing with the seed dictionaries in the first iter-ation.
A phrase matching a dictionary phraseis labeled with the dictionary?s class.
Patternsare then created using the context around thepositively labeled entities to create candidatepatterns for C.2.
Scoring Patterns: Candidate patterns arescored using a pattern scoring measure andthe top ones are added to the list of learnedpatterns for C.3.
Learning entities: Learned patterns for theclass are applied to the text to extract candi-date entities.
An entity scorer ranks the can-didate entities and adds the top entities to C?sdictionary.The success of bootstrapped pattern learningmethods crucially depends on the effectiveness ofthe pattern scorer and the entity scorer.
Here wefocus on improving the pattern scoring measure(Step 2 above).3.1 Creating PatternsCandidate patterns are created using contexts ofwords or their lemmas in a window of two to fourwords before and after a positively labeled token.Context words that are labeled with one of theclasses are generalized with that class.
The tar-get term has a part-of-speech (POS) restriction,which is the POS tag of the labeled token.
Wecreate flexible patterns by ignoring the words {?a?,?an?, ?the?}
and quotation marks when matchingpatterns to the text.
Some examples of the patternsare shown in Table 4.3.2 Scoring PatternsJudging the efficacy of patterns without using afully labeled dataset can be challenging because oftwo types of failures: 1. penalizing good patternsthat extract good (that is, positive) unlabeled enti-ties, and 2. giving high scores to bad patterns thatextract bad (that is, negative) unlabeled entities.Existing systems that assume unlabeled entities asnegative are too conservative in scoring patternsand suffer from the first problem.
Systems thatignore unlabeled entities can suffer from both theproblems.
In this paper, we propose to estimatethe labels of unlabeled entities to more accuratelyscore the patterns.For a pattern r, sets Pr, Nr, and Urdenote thepositive, negative, and unlabeled entities extractedby r, respectively.
The pattern score, ps(r) is cal-culated asps(r) =|Pr||Nr|+?e?Ur(1?
score(e))log(|Pr|)where |.| denotes size of a set.
The functionscore(e) gives the probability of an entity e be-longing to C. If e is a common word, score(e) is0.
Otherwise, score(e) is calculated as the aver-age of five feature scores (explained below), each100of which give a score between 0 and 1.
The fea-ture scores are calculated using the seed dictio-naries, learned entities for all labels, Google N-grams2, and clustering of domain words using dis-tributional similarity.
The log |Pr| term, inspiredfrom (Riloff, 1996), gives higher scores to patternsthat extract more positive entities.
Candidate pat-terns are ranked by ps(r) and the top patterns areadded to the list of learned patterns.To calculate score(e), we use features that as-sess unlabeled entities to be either closer to pos-itive or negative entities in an unsupervised way.We motivate our choice of the five features belowwith the following insights.
If the dataset consistsof informally written text, many unlabeled enti-ties are spelling mistakes and morphological vari-ations of labeled entities.
We use two edit distancebased features to predict labels for these unlabeledentities.
Second, some unlabeled entities are sub-strings of multi-word dictionary phrases but do notnecessarily belong to the dictionary?s class.
Forexample, for learning drug names, the positive dic-tionary might contain ?asthma meds?, but ?asthma?is negative and might occur in a negative dictio-nary as ?asthma disease?.
To predict the labels ofentities that are a substring of dictionary phrases,we use SemOdd, which was used in Gupta andManning (2014a) to learn entities.
Third, for aspecialized domain, unlabeled entities that com-monly occur in generic text are more likely to benegative.
We use Google Ngrams (called GN) toget a fast, non-sparse estimate of the frequency ofentities over a broad range of domains.
The abovefeatures do not consider the context in which theentities occur in text.
We use the fifth feature, Dist-Sim, to exploit contextual information of the la-beled entities using distributional similarity.
Thefeatures are defined as:Edit distance from positive entities (EDP): Thisfeature gives a score of 1 if e has low editdistance to the positive entities.
It is com-puted as maxp?Pr1(editDist(p,e)|p|< 0.2),where 1(c) returns 1 if the condition c is trueand 0 otherwise, |p| is the length of p, andeditDist(p, e) is the Damerau-Levenshteinstring edit distance between p and e.Edit distance from negative entities (EDN): It issimilar to EDP and gives a score of 1 if e has2http://storage.googleapis.com/books/ngrams/books/datasetsv2.html.
Accessed Jan-uary 2008.high edit distance to the negative entities.
It iscomputed as 1 ?maxn?Nr1(editDist(n,e)|n|<0.2).Semantic odds ratio (SemOdd): First, we cal-culate the ratio of frequency of the entityterm in the positive entities to its frequency inthe negative entities with Laplace smoothing.The ratio is then normalized using a softmaxfunction.
The feature values for the unlabeledentities extracted by all the candidate patternsare then normalized using the min-max func-tion to scale the values between 0 and 1.3Google Ngram (GN): We calculate the ratio ofscaled frequency of e in the dataset to the fre-quency in Google Ngrams.
The scaling factoris to balance the two frequencies and is com-puted as the ratio of total number of phrasesin the dataset to the total of phrases in GoogleNgrams.
The feature values are normalizedin the same way as SemOdd.Distributional similarity score (DistSim): Wordsthat occur in similar contexts, such as?asthma?
and ?depression?, are clustered us-ing distributional similarity.
Unlabeled en-tities that get clustered with positive entitiesare given higher score than the ones clusteredwith negative entities.
To score the clusters,we learn a logistic regression classifier usingcluster ID as features, and use their weightsas scores for all the entities in those clusters.The dataset for logistic regression is createdby considering all positively labeled words aspositive and sampling negative and unlabeledwords as negative.
The scores for entities arenormalized in the same way as SemOdd andGN.Out of feature vocabulary entities for SemOdd,GN, and DistSim are given a score of 0.
Weuse a simple way of combining the feature val-ues: we give equal weights to all features andaverage their scores.
Features can be combinedusing a weighted average by manually tuning theweights on a development set; we leave it to thefuture work.
Another way of weighting the fea-tures is to learn the weights using machine learn-ing.
We experimented with learning weights for3We do min-max normalization on top of the softmaxnormalization because the maximum and minimum value bysoftmax might not be close to 1 and 0, respectively.
And,treating the out-of-feature-vocabulary entities same as theworst scored entities by the feature, that is giving them a scoreof 0, performed best on the development dataset.101the features by training a logistic regression clas-sifier.
We considered all positive words as positiveand randomly sampled negative and unlabeled en-tities as negative to predict score(e), but it per-formed worse compared to averaging the scoreson the development dataset.
Preliminary investi-gation suggests that since the classifier was trainedon a dataset heuristically labeled using the seeddictionaries, it was too noisy for the classifier tolearn accurate weights.
Presumably, the classifieralso suffered from the closed world assumption oftreating unlabeled examples as negative.3.3 Learning EntitiesWe apply the learned patterns to the text andextract candidate entities.
We discard commonwords, negative entities, and those containing non-alphanumeric characters from the set.
The rest arescored by averaging the scores of DistSim, Sem-Odd, EDO, and EDN features from Section 3.2and the following features.Pattern TF-IDF scoring (PTF): For an entity e, itis calculated as1log freqe?r?Rps(r), whereR is the set of learned patterns that extract eand freqeis the frequency of e in the cor-pus.
Entities that are extracted by many highweighted patterns get higher weight.
To mit-igate the effect of many commonly occurringentities also getting extracted by several pat-terns, we normalize the feature value with thelog of the entity?s frequency.
The values arenormalized in the same way as DistSim andSemOdd.Domain N-gram TF-IDF (DN): This featuregives higher scores to entities that are moreprevalent in the corpus compared to the gen-eral domain.
For example, to learn enti-ties about a specific disease from a disease-related corpus, the feature favors entities re-lated to the disease over generic medical en-tities.
It is calculated in the same way as GNexcept the frequency is computed in the n-grams of the generic domain text.Including GN in the phrase scoring features orincluding DN in the pattern scoring features didnot perform well on the development set in our pi-lot experiments.4 Experiments4.1 DatasetWe evaluate our system on extracting drug-and-treatment (DT) entities in sentences from four fo-rums on the MedHelp user health discussion web-site: 1.
Acne, 2.
Adult Type II Diabetes (calledDiabetes), 3.
Ear Nose & Throat (called ENT),and 4.
Asthma.
The forums have discussionthreads by users concerning health related prob-lems and treatments.
The number of sentencesin each forum are: 215,623 in ENT, 39,637 inAsthma, 63,355 in Diabetes, and 65,595 in Acne.We used Asthma as the development forum forfeature engineering and parameter tuning.
Simi-lar to Gupta and Manning (2014a), a DT entity isdefined as a pharmaceutical drug, or any treatmentor intervention mentioned that may help a symp-tom or a condition.
It includes surgeries, lifestylechanges, alternative treatments, home remedies,and components of daily care and management ofa disease, but does not include diagnostic tests anddevices.
More information is in the supplemen-tal material.
A few example sentences from thedataset are below.I plan to start cinnamon and holy basil - knownto lower glucose in many people.She gave me albuteral and symbicort (plussome hayfever meds and asked me to use thepeak flow meter.My sinus infections were treated electrically,with high voltage million volt electricity, whichsolved the problem, but the treatment is notFDA approved and generally unavailable, exceptunder experimental treatment protocols.In these sentences, ?cinanmon?, ?holy basil?, ?al-buteral?, ?symbicort?, ?meds?, ?high voltage mil-lion volt electricity?, and ?treatment?
are DT enti-ties.We used entities from the following classes asnegative: symptoms and conditions (SC), medi-cal specialists, body parts, and common tempo-ral nouns to remove dates and dosage informa-tion.
We used the DT and SC seed dictionariesfrom Gupta and Manning (2014a).4The lists of4The DT seed dictionary (36,091 phrases) and SC seeddictionary (97,211 phrases) were automatically constructedfrom various sources on the Internet and expanded usingthe OAC Consumer Health Vocabulary (http://www.consumerhealthvocab.org), which maps medical jar-gon to everyday phrases and their variants.
Both dictionariesare large because they contain many variants of entities.
Foreach system, the SC dictionary was further expanded by run-ning the system with the SC class as positive (considering DT102body parts and temporal nouns were obtained fromWordnet (Fellbaum, 1998).
The common wordslist was created using most common words on theweb and Twitter.5For evaluation, the first author hand labeled thelearned entities pooled from all systems.
A wordwas evaluated by querying the word and the fo-rum name on Google and manually inspecting theresults.
More details on the labeling guidelinesare in the Supplement section.
Inter annotatoragreement between the annotator and another re-searcher was computed on 200 randomly sampledlearned entities from each of the Asthma and ENTforum.
The agreement for the entities from theAsthma forum was 96% and from the ENT forumwas 92.46%.
The Cohen?s kappa scores were 0.91and 0.83, respectively.
Most disagreements wereon food items like ?yogurt?, which are hard to la-bel.
Note that we use the hand labeled entities onlyas a test set for evaluation.4.2 BaselinesAs in Section 3, the sets Pr,Nr, andUrare definedas the positive, negative, and unlabeled entities ex-tracted by a pattern r, respectively.
The set Arisdefined as union of all the three sets.
We com-pare our system with the following pattern scoringalgorithms.
Candidate entities are scored in thesame way as described in Section 3.3.
It is impor-tant to note that previous works also differ in howthey create patterns, apply patterns, and score en-tities.
Since we focus on only the pattern scoringaspect, we run experiments that differ in only thatcomponent.PNOdd: Defined as |Pr|/|Nr|, this measure ig-nores unlabeled entities and is similar to thedomain specific pattern learning componentof Etzioni et al.
(2005) since all patterns with|Pr| < 2 were discarded (more details in thenext section).PUNOdd: Defined as |Pr|/(|Ur| + |Nr|), thismeasure treats unlabeled entities as negativeentities.RlogF: Measure used by Riloff (1996) andThelen and Riloff (2002), and calculatedas Rrlog |Pr|, where Rrwas defined as|Pr|/|Ar| (labeled RlogF-PUN).
It assumedand other classes as negative) and adding the top 50 words ex-tracted by the top 300 patterns to the SC class dictionary.
Thishelps in adding corpus specific SC words to the dictionary.5We used top 10,000 words from Google N-grams and top5,000 words from Twitter (www.twitter.com), accessedfrom May 19 to 25, 2012.unlabeled entities as negative entities.
Wealso compare with a variant that ignores theunlabeled entities, that is by defining Rras|Pr|/(|Pr+ |Nr|) (labeled RlogF-PN).Yangarber02: This measure from Yangarber etal.
(2002) calculated two scores, accr=|Pr|/|Nr| and confr= (|Pr|/|Ar|) log |Pr|.Patterns with accrless than a threshold werediscarded and the rest were ranked usingconfr.
We empirically determined that athreshold of 0.8 performed best on the devel-opment forum.Lin03: A measure proposed in Lin et al.
(2003),it was similar to Yangarber02, except confrwas defined as log |Pr|(|Pr| ?
|Nr|)/|Ar|.In essence, it discards a pattern if it extractsmore negative entities than positive entities.SqrtRatioAll: This pattern scoring method wasused in Gupta and Manning (2014a) anddefined as?k?Pr?freqk/?j?Ar?freqj,where freqiis the number of times entityi is extracted by r. Sublinear scaling ofthe term-frequency prevents high frequencywords from overshadowing the contributionof low frequency words.4.3 Experimental SetupWe used the same experimental setup for our sys-tem and the baselines.
When matching phrasesfrom a seed dictionary to text, a phrase is la-beled with the dictionary?s class if the sequence ofphrase words or their lemmas match with the se-quence of words of a dictionary phrase.
Since ourcorpora are from online discussion forums, theyhave many spelling mistakes and morphologicalvariations of entities.
To deal with the variations,we do fuzzy matching of words ?
if two words areone edit distance away and are more than 6 char-acters long, then they are considered a match.We used Stanford TokensRegex (Chang andManning, 2014) to create and apply surface wordpatterns to text, and used the Stanford Part-of-Speech (POS) tagger (Toutanova et al., 2003) tofind POS tags of tokens and lemmatize them.When creating patterns, we discarded patternswhose left or right context was 1 or 2 stop words toavoid generating low precision patterns.6In eachiteration, we learned a maximum 20 patterns withps(r) ?
?rand maximum 10 words with score ?6Three or more stop words resulted in some good patternslike ?I am on X?.
Our stop words list consists of punctuationmarks and around 200 very common English words.1030.76 0.780.8 0.820.84 0.860.88 0.90.92 0.940.960.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecall (out of 221 correct entities)ASTHMAOurSystemRlogF-PUNYangarber02SqrtAllRatioLin03PUNOdd0.750.80.850.90.9510.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9PrecisionRecall (out of 645 correct entities)ENTOurSystemRlogF-PUNYangarber02SqrtAllRatioLin03PUNOdd0.750.80.850.90.9510.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecall (out of 624 correct entities)ACNEOurSystemRlogF-PUNYangarber02SqrtAllRatioLin03PUNOdd0.760.780.80.820.840.860.880.90.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9PrecisionRecall (out of 118 correct entities)DIABETESOurSystemRlogF-PUNYangarber02SqrtAllRatioLin03PUNOddFigure 2: Precision vs. Recall curves of our system and the baselines for the four forums.
Rlog-PN andPNOdd are not shown to improve clarity.0.2.
The initial value of ?rwas 1.0, which was re-duced to 0.8?
?rwhenever the system did not ex-tract any more patterns and words.
We discardedpatterns that extracted less than 2 positive entities.We selected these parameters by their performanceon the development forum.For calculating the DistSim feature used forscoring patterns and entities, we clustered all ofMedHelp?s forum data into 1000 clusters using theBrown clustering algorithm (Brown et al., 1992;Liang, 2005).7For calculating the Domain Ngramfeature for scoring entities, we used n-grams fromall user forums in MedHelp as the domain n-grams.We evaluate systems by their precision and re-call in each iteration.
Precision is defined as thefraction of correct entities among the entities ex-tracted.
We stopped learning entities for a sys-tem if the precision dropped below 75% to extractentities with reasonably high precision.
Recall isdefined as the fraction of correct entities amongthe total unique correct entities pooled from allsystems while maintaining the precision ?
75%.Note that true recall is very hard to compute sinceour dataset is unlabeled.
To compare the systems7The data consisted of around 4 million tokens.
Wordsthat occurred less than 50 times were discarded, which re-sulted in 50353 unique words.overall, we calculate the area under the precision-recall curves (AUC-PR).System Asthma ENT Diabetes AcneOurSystem 68.36 60.71 67.62 68.01PNOdd 51.62 50.31 05.91 58.45PUNOdd 42.42 30.44 36.11 58.38RlogF-PUN 56.13 54.11 48.70 57.04RlogF-PN 53.46 52.84 16.59 62.35SqrtRatioAll 41.49 40.44 35.47 46.46Yangarber02 53.76 48.46 41.45 59.85Lin03 54.58 47.98 56.15 60.79Table 1: Area under Precision-Recall curves of thesystems.4.4 ResultsFigure 2 plots the precision and recall of systems.8Table 1 shows AUC-PR scores for all systems.RlogF-PN and PNOdd have low value for Dia-betes because they learned generic patterns in ini-tial iteration, which led them to learn incorrect en-tities.
Overall our system performed significantlybetter than existing systems.
All systems extractmore entities for Acne and ENT because differentdrugs and treatments are more prevalent in theseforums.
Diabetes and Asthma have more inter-ventions and lifestyle changes that are harder to8We do not show plots of PNOdd and RlogF-PN to im-prove clarity.
They performed similarly to other baselines.104Feature Asthma ENT Diabetes AcneAll Features 68.36 60.71 67.62 68.01EDP 68.66 59.07 60.03 65.15EDN 59.39 59.21 16.75 65.96SemOdd 67.07 58.41 60.51 65.04GN 57.52 59.53 48.76 68.61DistSim 64.87 59.05 71.11 69.48Table 2: Individual feature effectiveness: Area un-der Precision-Recall curves when our system usesindividual features during pattern scoring.
Otherfeatures are still used for entity scoring.Feature Asthma ENT Diabetes AcneAll Features 68.36 60.71 67.62 68.01minusEDP 66.29 60.45 69.84 69.46minusEDN 67.19 60.39 69.89 67.57minusGN 65.53 60.33 66.07 67.28minusSemOdd 66.66 60.76 70.79 68.25minusDistSim 66.10 60.58 66.59 67.85Table 3: Feature ablation study: Area underPrecision-Recall curves when individual featuresare removed from our system during pattern scor-ing.
The feature is still used for entity scoring.extract.To compare the effectiveness of each feature inour system, Table 2 shows the AUC-PR valueswhen each feature was individually used for pat-tern scoring (other features were still used to learnentities).
EDP and DistSim were strong predictorsof labels of unlabeled entities because many goodunlabeled entities were spelling mistakes of DTentities and occurred in similar context as them.Table 3 shows the AUC-PR values when each fea-ture was removed from the set of features used toscore patterns (the feature was still used for learn-ing entities).
Removing GN and DistSim reducedthe AUC-PR scores for all forums.Table 4 shows some examples of patterns andthe entities they extracted along with their labelswhen the pattern was learned.
We learned the firstpattern because ?pinacillin?
has low edit distancefrom the positive entity ?penicillin?.
Similarly, wescored the second pattern higher than the base-line because ?desoidne?
is a typo of the positiveentity ?desonide?.
Note that the seed dictionariesare noisy ?
the entity ?metro?, part of the positiveentity ?metrogel?, was falsely considered a neg-ative entity because it was in the common webwords list.
Our system learned the third patternfor two reasons: ?inhaler?, ?inhalers?, and ?hfa?
oc-curred frequently as sub-phrases in the DT dictio-nary, and they were clustered with positive enti-Our System RlogF-PUNlow dose of X* mg of Xmg of X treat with XX 10 mg take DT and Xshe prescribe X be take XX 500 mg she prescribe Xbe take DT and X* put on Xent put I on X* stop take XDT ( like X:NN i be prescribe Xlike DT and X have be take Xthen prescribe X* tell I to take XTable 5: Top 10 (simplified) patterns learned byour system and RlogF-PUN from the ENT forum.An asterisk denotes that the pattern was neverlearned by the other system.
X is the target word.ties by distributional similarity.
Since RlogF-PUNdoes not distinguish between unlabeled and nega-tive entities, it is does not learn the pattern.
Table 5shows top 10 patterns learned for the ENT forumby our system and RlogF-PUN, the best perform-ing baseline for the forum.
Our system preferredto learn patterns with longer contexts, which areusually higher precision, first.5 Discussion and ConclusionOur system extracted entities with higher preci-sion and recall than other existing systems.
How-ever, learning entities from an informal text corpusthat is partially labeled from seed entities presentssome challenges.
Our system made mistakes pri-marily due to three reasons.
One, it sometimesextracted typos of negative entities that were noteasily predictable by the edit distance measures,such as ?knowwhere?.
Second, patterns that ex-tracted many good but some bad unlabeled en-tities got high scores because of the good unla-beled entities.
However, the bad unlabeled enti-ties extracted by the highly weighted patterns werescored high by the PTF feature during the entityscoring phase, leading to extraction of the bad en-tities.
Better features to predict negative entitiesand robust text normalization would help mitigateboth the problems.
Third, we used automaticallyconstructed seed dictionaries that were not datasetspecific, which led to incorrectly labeling of someentities (for example, ?metro?
as negative in Ta-ble 4).
Reducing noise in the dictionaries wouldincrease precision and recall.In this paper, the features are weighted equally105Forum Pattern Positive entities Negative Unlabeled OurSystemBaselineENT he give I more X antibiotics, steroid, antibiotic pinacillin 68NA(RlogF-PUN)Acne topical DT ( X prednisone, clindamycin, differin,benzoyl peroxide, tretinoin, metro-gelmetro desoidne 149231(RlogF-PN)Asthma i be put on X cortisone, prednisone, asmanex, ad-vair, augmentin, bypass, nebulizer,xolair, steroids, prilosecinhaler,inhalers,hfa8NA(RlogF-PUN)Table 4: Example patterns and the entities extracted by them, along with the rank at which the patternwas added to the list of learned patterns.
NA means that the system never learned the pattern.
Baselinerefers to the best performing baseline system on the forum.
The patterns have been simplified to showjust the sequence of lemmas.
X refers to the target entity; all of them in these examples had noun POSrestriction.
Terms that have already been identified as the positive class were generalized to their classDT.by taking the average of the feature scores.
Onearea of future work is to learn weights usingmore sophisticated techniques; in pilot experi-ments, learning a logistic regression classifier onheuristically labeled data did not work well for ei-ther pattern scoring or entity scoring.One limitation of our system and evaluation isthat we learned single word entities, since calcu-lating some features for multi-word phrases is notstraightforward.
For example, word clusters usingdistributional similarity were constructed for sin-gle words.
Our future work includes expandingthe features to evaluate multi-word phrases.
An-other avenue for future work is to use our pat-tern scoring method for learning other kinds ofrules, such as dependency patterns, and in differ-ent kinds of systems, such as hybrid entity learn-ing systems (Etzioni et al., 2005; Carlson et al.,2010).
In addition, we did not explicitly addressthe problem of semantic drift (Curran et al., 2007)in this paper.
In theory, learning better patternswould help lessen the problem; we plan to investi-gate this further.In conclusion, we show that predicting the la-bels of unlabeled entities in the pattern scorer of abootstrapped entity extraction system significantlyimproves precision and recall of learned entities.Our experiments demonstrate the importance ofhaving models that contrast domain-specific andgeneral domain text, and the usefulness of featuresthat allow spelling variations when dealing withinformal texts.
Our pattern scorer outperforms ex-isting pattern scoring methods for learning drug-and-treatment entities from four medical web fo-rums.AcknowledgmentsWe thank Diana MacLean for labeling the test datafor calculating the inter-annotator agreement.
Weare also grateful to Gabor Angeli, Angel Chang,Manolis Savva, and the anonymous reviewers fortheir useful feedback.
We thank MedHelp forsharing their anonymized data with us.ReferencesPeter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18:467?479.Mary Elaine Califf and Raymond J. Mooney.
1999.Relational learning of pattern-match rules for in-formation extraction.
In Proceedings of the 16thNational Conference on Artificial Intelligence andthe 11th Innovative Applications of Artificial Intelli-gence Conference, AAAI-IAAI ?99, pages 328?334.Andrew Carlson, Justin Betteridge, Richard C. Wang,Estevam R. Hruschka, Jr., and Tom M. Mitchell.2010.
Coupled semi-supervised learning for infor-mation extraction.
In Proceedings of the 3rd ACMInternational Conference on Web Search and DataMining, WSDM ?10, pages 101?110.Angel X. Chang and Christopher D. Manning.
2014.TokensRegex: Defining cascaded regular expres-sions over tokens.
Technical Report CSTR 2014-02,Department of Computer Science, Stanford Univer-sity.Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.2013.
Rule-based information extraction is dead!Long live rule-based information extraction sys-tems!
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?13, pages 827?832.106Fabio Ciravegna.
2001.
Adaptive information extrac-tion from text by rule induction and generalisation.In Proceedings of the 17th International Joint Con-ference on Artificial Intelligence, IJCAI?01, pages1251?1256.William W. Cohen and Yoram Singer.
1999.
A simple,fast, and effective rule learner.
In Proceedings of the16th National Conference on Artificial Intelligenceand the 11th Innovative Applications of Artificial In-telligence Conference, pages 335?342.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
In Pro-ceedings of the Joint SIGDAT Conference on Empir-ical Methods in Natural Language Processing andVery Large Corpora, pages 100?110.J.
R. Curran, T. Murphy, and B. Scholz.
2007.
Min-imising semantic drift with mutual exclusion boot-strapping.
Proceedings of the Conference of thePacific Association for Computational Linguistics,pages 172?180.D.
Downey, O. Etzioni, S. Soderland, and D. S. Weld.2004.
Learning Text Patterns for Web Informa-tion Extraction and Assessment.
In Proceedings ofAAAI 2004 Workshop on Adaptive Text Extractionand Mining, ATEM ?04.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:An experimental study.
Artificial Intelligence,165(1):91 ?
134.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 1535?1545.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Dayne Freitag.
1998.
Toward general-purpose learn-ing for information extraction.
In Proceedings of the36th Annual Meeting of the Association for Compu-tational Linguistics and 17th International Confer-ence on Computational Linguistics, COLING-ACL?98, pages 404?408.Sonal Gupta and Christopher D. Manning.
2014a.
In-duced lexico-syntactic patterns improve informationextraction from online medical forums.
Under Sub-mission.Sonal Gupta and Christopher D. Manning.
2014b.Spied: Stanford pattern-based information extrac-tion and diagnostics.
In Proceedings of the ACL2014 Workshop on Interactive Language Learning,Visualization, and Interfaces (ACL-ILLVI).Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th International Conference on Computationallinguistics, COLING ?92, pages 539?545.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, MIT EECS.Winston Lin, Roman Yangarber, and Ralph Grishman.2003.
Bootstrapped learning of semantic classesfrom positive and negative examples.
In Proceed-ings of the ICML 2003 Workshop on The Continuumfrom Labeled to Unlabeled Data in Machine Learn-ing and Data Mining.Mausam, Michael Schmitz, Stephen Soderland, RobertBart, and Oren Etzioni.
2012.
Open language learn-ing for information extraction.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 523?534.Tara McIntosh and James R. Curran.
2009.
Reducingsemantic drift with bagging and distributional sim-ilarity.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, ACL-IJCNLP ?09, pages396?404.Ramesh Nallapati and Christopher D. Manning.
2008.Legal docket-entry classification: Where machinelearning stumbles.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 438?446.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In Proceedingsof the Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technologies, HLT-NAACL ?04,pages 321?328.S.
Patwardhan.
2010.
Widening the Field of Viewof Information Extraction through Sentential EventRecognition.
Ph.D. thesis, University of Utah, May.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontology induction from text.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, ACL ?10, pages 296?305.Ellen Riloff.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceedingsof the 13th National Conference on Artificial Intelli-gence, AAAI?96, pages 1044?1049.Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-zioni.
2013.
Modeling missing data in distant su-pervision for information extraction.
Transactionsof the Association for Computational Linguistics,1:367?378.107Mark Stevenson and Mark A. Greenwood.
2005.
Asemantic approach to IE pattern induction.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 379?386.Michael Thelen and Ellen Riloff.
2002.
A bootstrap-ping method for learning semantic lexicons usingextraction pattern contexts.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?02, pages 214?221.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,HLT-NAACL ?03, pages 173?180.Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-ishman.
2013.
Filling knowledge base gaps for dis-tant supervision of relation extraction.
In Proceed-ings of the Association for Computational Linguis-tics (ACL), pages 665?670.Roman Yangarber, Ralph Grishman, and PasiTapanainen.
2000.
Automatic acquisition ofdomain knowledge for information extraction.
InProceedings of the 18th International Conferenceon Computational Linguistics, COLING ?00, pages940?946.Roman Yangarber, Winston Lin, and Ralph Grishman.2002.
Unsupervised learning of generalized names.In Proceedings of the 19th International Conferenceon Computational Linguistics, COLING ?02.108
