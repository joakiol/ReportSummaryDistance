Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 11?20,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsPredicate Argument Alignment using a Global Coherence ModelTravis Wolfe, Mark Dredze, and Benjamin Van DurmeJohns Hopkins UniversityBaltimore, MD, USAAbstractWe present a joint model for predicate argu-ment alignment.
We leverage multiple sourcesof semantic information, including temporalordering constraints between events.
Theseare combined in a max-margin frameworkto find a globally consistent view of entitiesand events across multiple documents, whichleads to improvements over a very strong localbaseline.1 IntroductionNatural language understanding (NLU) requiresanalysis beyond the sentence-level.
For example,an entity may be mentioned multiple times in a dis-course, participating in various events, where eachevent may itself be referenced elsewhere in thetext.
Traditionally the task of coreference resolutionhas been defined as finding those entity mentionswithin a single document that co-refer, while cross-document coreference resolution considers a widerdiscourse context across many documents, yet stillpertains strictly to entities.Predicate argument alignment, or entity-eventcross-document coreference resolution, enlarges theset of possible co-referent elements to include thementions of situations in which entities participate.This expanded definition drives practitioners to-wards a more complete model of NLU, where sys-tems must not only consider who is mentioned, butalso what happened.
However, despite the drive to-wards an expanded notion of discourse, models typ-ically are formulated with strong notions of local-independence: viewing a multi-document task asone limited to individual pairs of sentences.
Thiscreates a mis-match between the goals of such work?
considering entire documents ?
with the systems ?consider individual sentences.In this work, we consider a system that takes adocument level view in considering coreference forentities and predictions: the task of predicate ar-gument linking.
We treat this task as a global in-ference problem, leveraging multiple sources of se-mantic information identified at the document level.Global inference for this problem is mostly unex-plored, with the exception of Lee et al (2012) (dis-cussed in ?
8).
Especially novel here is the use ofdocument-level temporal constraints on events, rep-resenting a next step forward on the path to full un-derstanding.Our approach avoids the pitfalls of local infer-ence while still remaining fast and exact.
We usethe pairwise features of a very strong predicate argu-ment aligner (Wolfe et al, 2013) (competitive withthe state-of-the-art (Roth, 2014)), and add quadraticfactors that constrain local decisions based on globaldocument information.
These global factors leadto superior performance compared to the previousstate-of-the-art.
We release both our code and data.12 ModelConsider the two sentences from the document pairshown in Figure 1.
These sentences describe thesame event, although with different details.
Thesource sentence has four predicates and four ar-guments, while the target has three predicates andthree arguments.
In this case, one of the predicatesfrom each sentence aligns, as do three of the argu-ments.
We also show additional information poten-tially helpful to determining alignments: temporalrelations between the predicates.
The goal of predi-cate argument alignment is to assign these links in-dicating coreferent predicates and arguments acrossa document pair (Roth and Frank, 2012).Previous work by Wolfe et al (2013) formulated1https://github.com/hltcoe/parma211Figure 1: An example analysis and predicate argument alignment task between a source and target document.Predicates appear as hollow ovals, have blue mentions, and are aligned considering their arguments (dashed lines).Arguments, in black diamonds with green mentions, represent a document-level entity (coreference chain), and arealigned using their predicate structure and mention-level features.
The alignment choices appear in the middle in red.Temporal relation information is lifted into the global inference over alignments.this as a binary classification problem: given a pairof arguments or predicates, construct features andscore the pair, where scores above threshold indi-cate links.
A binary classification framework has ad-vantages: it?s fast since individual decisions can bemade quickly, but it comes at the cost of global in-formation across links.
The result may be links thatconflict in their interpretation of the document.
Fig-ure 1 makes clear that jointly considering all links atonce can aid individual decisions, for example, byincluding temporal ordering of predicates.The global nature of this task is similar to wordalignment for machine translation (MT).
Many sys-tems consider alignment links between words indi-vidually, selecting the best link for each word inde-pendently of the other words in the sentence.
Just aswith an independent linking strategy in predicate ar-gument alignment, this can lead to inconsistencies inthe output.
Lacoste-Julien et al (2006) introduced amodel that jointly resolved word alignments basedon the introduction of quadratic variables, factorsthat depend on two alignment decisions which char-acterize patterns that span word-word links.
Theirapproach achieved improved results even in the pres-ence of little training data.We present a global predicate argument alignmentmodel based on considering quadratic interactionsbetween alignment variables to captures patterns weexpect in coherent discourse.
We introduce factorswhich are comprised of a binary variable, multiplequadratic constraints on that variable, and featuresthat determine the cost associated with that vari-able in order to characterize the dependence betweenalignment decisions.While the mathematical framework we use is sim-ilar to Lacoste-Julien et al (2006), predicate argu-ment alignment greatly differs from word alignment;thus our joint factors are based on different sourcesof regularity.
Word alignment favors monotonic-ity in word order, but this effect is very weak inpredicate argument alignment: aligned items can bespread throughout a document, and are often nested,gapped, or shuffled.
Instead, we encode assump-tions about consistency of temporal relations be-tween coreferent events, coherence between predi-cates and arguments that appear in both documents,and fertility (to prevent over-alignment).
We alsonote that our setting has much less data than typicalword alignment tasks, as well as richer features thatutilize semantic resources.12Notation An alignment between an item indexedby i in the source document and j in the target docu-ment is represented by variable zij?
{0, 1}, wherezij= 1 indicates that items i and j are aligned.
Insome cases, we will explicitly indicate when the twoitems are predicates as zpij; an argument alignmentwill be zaij.
We represent all alignments for a docu-ment pair as matrix z.For clarity, we omit any variable representingobserved data when discussing feature functions;alignment variables are endowed with this informa-tion.
For each pair of items we use ?local?
fea-ture functions f(?)
and corresponding parametersw, which capture the similarity between two itemswithout the context of other alignments.sij= w ?
f(zij) (1)where sijis the score of linking items i and j.Using only local features, our system wouldgreedily select alignments.
To capture global as-pects we add joint factors that capture effects be-tween alignment variables.
Each joint factor ?
iscomprised of a constrained binary variable z?asso-ciated with features f(?)
that indicates when the fac-tor is active.
Together with parameters w these formadditional scores s?for the objective:s?= w ?
f(?)
(2)The full linear scoring function on alignmentssums over both local similarity and joint factors:?ijsijzij+????s?z?.
(3)Lastly, it is convenient to describe the local fea-ture functions and their corresponding alignmentvariable as factors with no constraints, and we willdo so when describing the full score function.3 Local FactorsLocal factors encode features based on the men-tion pair, which include a wide variety of simi-larity measures, e.g.
whether two headwords ap-pear as synonyms in WordNet, gender agreementbased on possessive pronouns.
We adopt the fea-tures of Wolfe et al (2013), a strong baseline systemwhich doesn?t use global inference.2These featuresare built on top of a variety of semantic resources(PPDB (Ganitkevitch et al, 2013), WordNet (Miller,1995), FrameNet (Baker et al, 1998)) and methodsfor comparing mentions (tree edit distance (Yao etal., 2013), string transducer (Andrews et al, 2012)).4 Joint FactorsOur goal is to develop joint factors that improve overthe feature rich local factors baseline by consideringglobal information.Fertility A common mistake when making inde-pendent classification decisions is to align manysource items to a single target item.
While each linklooks promising on its own, they clearly cannot allbe right.
Empirically, the training set reveals thatmany to one alignments are uncommon; thus manyto one predictions are likely errors.
We add a fertilityfactor for predicates and arguments, where fertilityis defined as the number of links to an item.
Higherfertilities are undesired and are thus penalized.
For-mally, for matrix z, the fertility of a row i or columnj is the sum of that row or column.
We discuss fer-tility in terms of rows below.We include two types of fertility factors.
First,factor ?fert1distinguishes between rows with at leastone link from those with none.
For row i, we add oneinstance of the linear factor ?fert1with constraintsz?fert1?
zij?j (4)The cost associated with z?fert1, which we will re-fer to as sfert1, will be incurred any time an item ismentioned in both documents.
For data sets withmany singletons, sfert1more strongly penalizes non-singleton rows, reflecting this pattern in the trainingdata.
We make sfert1parametric, where the featuresof the ?fert1factor allow us to learn different weightsfor predicates and arguments, as well as the size ofthe row, i.e.
number of items in the pairing.The second fertility factory ?fert2considers itemswith a fertility greater than one, penalizing items forhaving too many links.
Its binary variable has the2Some features inspect the apparent predicate argumentstructure, based on things like dependency parses, but the modelmay not inspect more than one of its own decisions (joint fac-tors) while scoring an alignment.13quadratic constraints:z?fert2?
zijzik?j < k (5)This factor penalizes rows that have fertility of atleast two, but does not distinguish beyond that.
Analternative would be to introduce a factor for everypair of variables in a row, each with one constraint.This would heavily penalize fertilities greater thantwo.
We found that the resulting quadratic programtook longer to solve and gave worse results.Since documents have been processed to identifyin-document coreference chains, we do not expectmultiple arguments from a source document to alignto a single target item.
For this reason, we expect?fert2for arguments to have a large negative weight.In contrast, since predicates do not form chains, wemay have multiple source predicates for one target.We note an important difference between ourfertility factor compared with Lacoste-Julien et al(2006).
We parameterize fertility for only two cases(1 and 2) whereas they consider fertility factors from2 to D. We do not parameterize fertilities higherthan two because they are not common in our datasetand come at a high computational cost.The features f(?)
for both ?fert1and ?fert2are anintercept feature (which always fires), indicator fea-tures for whether this row corresponds to an argu-ment or a predicate, and a discretized feature for howmany alignments are in this row.Predicate Argument Structure We expect struc-ture among links that involve a predicate and its as-sociated arguments.
Therefore, we add joint factorsthat consider a predicate and its associated align-ments: the predicate argument structure.
We deter-mine this structure from a dependency parse, thoughthe idea is general to any semantic binding, e.g.FrameNet or Propbank style parses.
Given a co-herent discourse, there are several expected types ofpatterns in the PAS; we add factors for these.Predicate-centric We begin with a predicate-centric factor, which views scores an alignment be-tween predicates based on their arguments, i.e.
thetwo predicates share the same arguments.
Ideally,two predicates can only align when their argumentsare coreferent.
However, in practice we may in-correctly resolve argument links, or there may beimplicit arguments that do not appear as syntacticdependencies of the predicate trigger.
Therefore,we settle for a weaker condition, that there shouldbe some overlap in the arguments of two coreferentpredicates.For every predicate alignment zpij, we add a factor?psawhose score spsais a penalty for having no ar-gument overlap; predicates share arguments (psa).To constrain the variable of ?psa, we add a quadraticconstraint that considers every possible pair of argu-ment alignments that might overlap:z?psa?
zpij(1?
maxk?args(pi)l?args(pj)zakl)(6)where args(pi) finds the indices of all argumentsgoverned by the predicate pi.Entity-centric We expect similar behavior fromarguments (entities).
If an entity appears in two doc-uments, it is likely that this entity will be mentionedin the context of a common predicate, i.e.
argumentsshare predicates (asp).
For a given argument align-ment zaijwe add quadratic constraints so that z?asprepresents a penalty for two arguments not sharing asingle predicate:z?asp?
zaij(1?
maxk?preds(ai)l?preds(aj)zpkl)(7)where preds(ai) finds the indices of all predicatesthat govern any mention of argument ai.The features f(?)
for both psa and asp are anintercept feature and a bucketed count of the size ofargs(pi)?
args(pj) or preds(ai)?preds(aj) respec-tively.Temporal Information Temporal ordering, incontrast to textual ordering, can indicate when pred-icates cannot align: we expect aligned predicatesin both documents to share the same temporal re-lations.
SemEval 2013 included a task on predict-ing temporal relations between events (UzZaman etal., 2013).
Many systems produced partial rela-tions of events in a document based on lexical as-pect and tense, as well as discourse connectives like?during?
or ?after?.
We obtain temporal relationswith CAEVO, a state-of-the-art sieve-based system(Chambers et al, 2014).14TimeML (Pustejovsky et al, 2003), the format forspecifying temporal relations, defines relations be-tween predicates (e.g.
immediately before and si-multaneous), each with an inverse (e.g.
immediatelyafter and simultaneous respectively).
We will referto a relation as R and its inverse as R?1.
Supposewe had paand pbin the source document, pxand pyin the target document, and paR1pb, pxR2py.
Giventhis configuration the following alignments conflictwith the in-doc relations:zaxzbyzayzbxIn-Doc Relations* * 1 1 R1= R21 1 * * R1= R?12where 1 means there is a link and * means there isa link or no link (wildcard).
The simplest examplethat fits this pattern is: ?a before b?, ?x before y?, ?acorefers with y?, and ?b corefers with x?
implies aconflict.We introduce a factor that penalizes these conflict-ing configurations.
In every instance where the pre-dicted temporal relation for a pair of predicate align-ments matches one of the conflict patterns above, weadd a factor using z?temp:z?temp?
zayzbxif paR1pb, pxR2py, R1= R2z?temp?
zaxzbyif paR1pb, pxR2py, R1= R?12(8)Thus s?tempis the cost of disagreeing with the in-doc temporal relations.
This is a general techniquefor incorporating relational information into coref-erence decisions.
It only requires specifying whentwo relations are incompatible, e.g.
spouseOf andsiblingOf are incompatible relations (in moststates).
We leave this for future work.Since CAEVO gives each relation prediction aprobability, we incorporate this into the feature byindicating the probability of a conflict not arising:f(?temp) = log(1?
p(R1)p(R2) + )(9) avoids large negative values since CAEVO proba-bilities are not perfectly calibrated.
We use  = 0.1,allowing feature values of at most ?2.3.Summary The objective is a linear function overbinary variables.
There is a local similarity scoredef train(alignments):w = init_weights()working_set = set()while True:xi = solve_ILP(w, working_set)c = most_violated_constraint(w, alignments)working_set.add(c)if hinge(c, w) < xi:breakdef most_violated_constraint(w, alignments):delta_features = vector()loss = 0for z in alignments:z_mv = make_ILP(z)for phi in factors:costs = dot(w, phi.features)z_mv.add_terms(costs, phi.vars)z_mv.add_constraints(phi.constraints)solve_ILP(z_mv)mu = (z.size + k) / (avg_z_size + k)delta_features += mu*(f(z) - f(z_mv))loss += mu*Delta(z, z_mv)return Constraint(delta_features, loss)def hinge(c, w):return max(0, c.loss - dot(w, c.delta_features))Figure 2: Learning algorithm (caching and ILP solvernot shown).
The sum in each constraint is performed oncewhen finding the constraint, and implicitly thereafter.coefficient on every alignment variable, and a jointfactor similarity score on every quadratic variable.These quadratic variables are constrained by prod-ucts of the original alignment variables.
Decodingan alignment requires solving this quadratically con-strained integer program; in practice is can be solvedquickly without relations.5 InferenceLearning We use the supervised structured SVMformulation of Joachims et al (2009).
As is commonin structure prediction we use margin rescaling and1 slack variable, with the structural SVM objective:minw||w||22+ C?s.t.
?
?
0?
+N?i=1w ?
f(zi) ?N?i=1w ?
f(z?i) + ?
(zi, z?i)?z?i?
Zi(10)where Ziis the set of all possible alignments thathave the same shape as zi.15The score function for an alignment uses threetypes of terms: weights, features, and alignmentvariables.
When we decode, we take the productof the weights and the features to get the costs forthe ILP (e.g.
s?= w ?
f(?)).
When we optimize ourSVM objective, we take the product of the alignmentvariables and the features to get modified featuresfor the SVM:f(z) =?ijzijf(zij) +????z?f(?)
(11)Since we cannot iterate over the exponentiallymany margin constraints, we solve for this optimiza-tion using the cutting-plane learning algorithm.
Thisalgorithm repeatedly asks the ?separation oracle?
forthe most violated SVM constraint, which finds thisconstraint by solving:arg maxz?1...z?N?iw ?
f(z?i) + ?
(zi, z?i) (12)subject to the constraints defined by the joint fac-tors.
When the separation oracle returns a constraintthat is not violated or is already in the working set,then we have a guarantee that we solved the originalSVM problem with exponentially many constraints.This is the most time-consuming aspect of learning,but since the problem decomposes over documentalignments, we cache solutions on a per documentalignment basis.
With caching, we only call the sep-aration oracle around 100-300 times.We implement the separation oracle using an ILPsolver, CPLEX,3due to complexity of the discreteoptimization problem: there are 2mnpossible align-ments for and m?n alignment grid.
In practice thisis solved very efficiently, taking less than a third ofa second per document alignment on average.
Wewould like ?
to be F1, but we need a decomposableloss to include it in a linear objective (Taskar et al,2003).
Instead, we use Hamming loss as a surrogate,as in Lacoste-Julien et al (2006).Our training data is heavily biased towards nega-tive examples, performing poorly on F1 since preci-sion and recall are unbalanced.
We use an asym-metric version of Hamming loss that incurs cFPcost for predicting an alignment for two unaligned3http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/items and cFNfor predicting no alignment for twoaligned items.
We fixed cFP= 1 and tuned cFN?
{1, 2, 3, 4} on dev data.
Additionally we found ituseful to tune the scale of the loss function across{12, 1, 2, 4}.
Previous work, such as Joachims et al(2009), use a hand-chosen constant for the scale ofthe Hamming loss, but we observe some sensitivityin this parameter and choose to optimize it.Decoding Following Wolfe et al (2013), we tunethe threshold for classification ?
on dev data to max-imize F1 (via linesearch).
For SVMs ?
is typicallyfixed at 0: this is not necessarily good practice whenyour training loss differs from test loss (Hammingvs F1).
In our case this extra parameter is worth al-locating a portion of training data to enable tuning.Tuning ?
addresses the same problem as using anasymmetric Hamming loss, but we found that do-ing both led to better results.4Since we are using aglobal scoring function rather than a set of classifi-cations, ?
is implemented as a test-time unary factoron every alignment.6 ExperimentsData We consider two datasets for evaluation.
Thefirst is a cross-document entity and event corefer-ence resolution dataset called the Extended EventCoref Bank (EECB) created by Lee et al (2012) andbased on a corpus from Bejan and Harabagiu (2010).The dataset contains clusters of news articles takenfrom Google News with annotations about corefer-ence over entities and events.
Following the proce-dure of Wolfe et al (2013), we select the first doc-ument in every cluster and pair it with every otherdocument in the cluster.The second dataset (RF) comes from Roth andFrank (2012).
The dataset contains pairs of newsarticles that describe the same news story, and areannotated for predicate links between the documentpairs.
Due to the lack of annotated arguments, wecan only report predicate linking performance andthe psa and asp factors do not apply.
Lastly, thesize of the RF data should be noted as it is muchsmaller than EECB: the test set has 60 documentpairs and the dev set has 10 document pairs.4Only tuning ?
performed almost as well as tuning ?
andthe Hamming loss, but not tuning ?
performed much worse thanonly tuning the Hamming loss at train time.16Both datasets are annotated with parses and in-document coreference labels provided by the toolsetof Napoles et al (2012)5and are available with ourcode release.
Due to the small data size, we use k-fold cross validation for both datasets.
We choosek = 10 for RF due to its very small size (morefolds give more training examples) and k = 5 onEECB to save computation time (amount of trainingdata in EECB is less of a concern).
Hyperparam-eters were chosen by hand using using cross vali-dation on the EECB dataset using F1 as the crite-ria (rather than Hamming).
Figures report averagesacross these folds.Systems Following Roth and Frank (2012) andWolfe et al (2013) we include a Lemma baselinefor identifying alignments which will align any twopredicates or arguments that have the same lemma-tized head word.6The Local baseline uses the samefeatures as Wolfe et al, but none of our joint fac-tors.
In addition to running our joint model with allfactors, we measure the efficacy of each individualfactor by evaluating each with the local features.For evaluation we use a generous version of F1that is defined for alignment labels composed ofsure, Gs, and possible links, Gpand the system?sproposed links H (following Cohn et al (2008),Roth and Frank (2012) and Wolfe et al (2013)).P =|H ?Gp||H|R =|H ?Gs||Gs|F =2PRP +RNote that the EECB data does not have a sure andpossible distinction, so Gs= Gp, resulting in stan-dard F1.
In addition to F1, we separately measurepredicate and argument F1 to demonstrate where ourmodel makes the largest improvements.We performed a one-sided paired-bootstrap testwhere the null hypothesis was that the joint modelwas no better than the Local baseline (described inKoehn (2004)).
Cases where p < 0.05 are bolded.5https://github.com/cnap/anno-pipeline6The lemma baseline is obviously sensitive to the lemma-tizer used.
We used the Stanford CoreNLP lemmatizer (Man-ning et al, 2014) and found it yielded slightly better results thanpreviously reported as the lemma baseline (Roth and Frank,2012), so we used it for all systems to ensure fairness and thatthe baseline is as strong as it could be.7 ResultsResults for EECB and RF are reported in Table 7.
Aspreviously reported, using just local factors (featureson pairs) improves over lemma baselines (Wolfe etal., 2013).
The joint factors make statistically sig-nificant gains over local factors in almost all experi-ments.
Fertility factors provide the largest improve-ments from any single constraint.
A fertility penaltyactually allows the pairwise weights to be more op-timistic in that they can predict more alignmentsfor reasonable pairs, allowing the fertility penalty toensure only the best is chosen.
This penalty alsoprevents the ?garbage collecting?
effect that arisesfor instances that have rare features (Brown et al,1993).Temporal constraints are relatively sparse, ap-pearing just 2.8 times on average.
Nevertheless,it was very helpful across all experiments, thoughonly statistically significantly on the RF dataset.This is one of the first results to demonstrate ben-efits of temporal relations affecting an downstreamtask.
Perhaps surprisingly, these improvements re-sult from a a temporal relation system that has rela-tively poor absolute performance.
Despite this, im-provements are possibly due to the orthogonal na-ture of temporal information; no other feature cap-tures this signal.
This suggests that future work ontemporal relation prediction may yield further im-provements and deserves more attention as a usefulfeature for semantic tasks in NLP.The predicate-centric factors improved perfor-mance significantly on both datasets.
For thepredicate-centric factor, when a predicate wasaligned there is a 72.3% chance that there wasat least one argument aligned as well, comparedto only 14.1% of case of non-aligned predicates.As mentioned before, the reason the former num-ber isn?t 100% is primarily due to implicit argu-ments and errors in argument identification.
Theargument-centric features helped almost as much asthe predicate-centric version, but the improvementswere not significant on the EECB dataset.
Run-ning the same diagnostic as the predicate-centricfeature reveals similar support: in 57.1% of the caseswhere an argument was aligned, at least one pred-icate it partook in was aligned too, compared to7.6% of cases for non-aligned arguments.
Both the17EECBF1 P R Arg F1 Arg P Arg R Pred F1 Pred P Pred RLemma 68.1 79.3 * 59.6 61.7 79.1 * 50.6 75.0 87.3 * 65.7Local 73.0 75.8 70.5 67.7 76.3 60.8 78.7 81.4 76.2+Fertility 77.1 * 83.9 * 71.3 66.6 80.9 * 56.6 82.8 * 87.4 * 78.7 *+Predicate-centric 74.1 * 80.7 * 68.6 67.4 81.6 * 57.3 79.7 * 85.0 * 75.1+Argument-centric 73.7 81.2 * 67.5 66.8 83.0 * 55.9 79.3 85.1 * 74.3+Temporal 73.7 78.2 * 69.7 67.9 80.6 * 58.7 79.0 82.1 76.1+All Factors 77.5 * 86.3 * 70.3 65.8 83.1 * 54.5 83.7 * 89.7 * 78.4 *RFPred F1 Pred P Pred RLemma 52.4 47.6 58.2 *Local 58.1 63.5 53.6+Fertility 60.0 57.4 62.4 *+Predicate-centric NA NA NA+Argument-centric NA NA NA+Temporal 59.0 57.4 60.6 *+All factors 59.4 56.9 62.2 *Figure 3: Cross validation results for EECB (above)(Lee et al, 2012) and RF (left) (Roth and Frank,2012).
Statistically significant improvements from Lo-cal marked * (p < 0.05 using a one-sided paired-bootstrap test) and best results are bolded.predicate- and argument-centric improve similarlyacross both predicates and arguments on EECB.While each of the joint factors all improve overthe baselines on RF, the full model with all the jointfactors does not perform as well as with some fac-tors excluded.
Specifically, the fertility model per-forms the best.
We attribute this small gap to lackof training data (RF only contains 64 training docu-ment pairs in our experiments), as this is not a prob-lem on the larger EECB dataset.Additionally, the joint models seem to trade pre-cision for recall on the RF dataset compared to theLocal baseline.
Note that both models are tuned tomaximize F1, so this tells you more about the shapeof the ROC curve as opposed to either models?
abil-ity to achieve either high precision or recall.
Sincewe don?t see this behavior on the EECB corpus, it ismore likely that this is a property of the data than themodel.8 Related WorkThe task of predicate argument linking was intro-duced by Roth and Frank (2012), who used a graphparameterized by a small number of semantic fea-tures to express similarities between predicates andused min-cuts to produce an alignment.
This wasfollowed by Wolfe et al (2013), who gave a locally-independent, feature-rich log-linear model that uti-lized many lexical semantic resources, similar to thesort employed in RTE challenges.Lee et al (2012) considered a similar problembut sought to produce clusters of entities and eventsrather than an alignment between two documentswith the goal of improving coreference resolution.They used features which consider previous eventand entity coreference decisions to make futurecoreference decisions in a greedy manner.
This dif-fers from our model which is built on non-greedyjoint inference, but much of the signal indicatingwhen two mentions corefer or are aligned is similar.In the context of in-document coreference reso-lution, Recasens et al (2013) sought to overcomethe problem of opaque mentions7by finding high-precision paraphrases of entities by pivoting offverbs mentioned in similar documents.
We addressthe issue of opaque mentions not by building a para-phrase table, but by jointly reasoning about entitiesthat participate in coreferent events (c.f.
?4); the ap-proaches are complementary.In this work we incorporate ordering informationof events.
Though we consider it an upstream task,there is a line of work trying to predict temporal rela-tions between events (Pustejovsky et al, 2003; Maniet al, 2006; Chambers et al, 2014).
Our results in-dicate this is a useful source of information, one ofthe first results to show an improvement from this7A lexically disparate description of an entity.18type of system (Glava?s and?Snajder, 2013).We utilize an ILP to improve upon a pipelinedsystem, similar to Roth and Yih (2004), but our workdiffers in that we do not use piecewise-trained clas-sifiers.
Our local similarity scores are calibrated ac-cording to a global objective by propagating the gra-dient back from the loss to every parameter in themodel.
When using piecewise training, local clas-sifiers must focus more on recall (in the spirit ofWeiss and Taskar (2010)) than they would for an or-dinary classification task with no global objective.Our method trains classifiers jointly with a globalconvex objective.
While our training procedure re-quires decoding an integer program, the parameterswe learn are globally optimal.9 ConclusionWe presented a max-margin quadratic cost modelfor predicate argument alignment, seeking to ex-ploit discourse level semantic features to improveon previous, locally independent approaches.
Ourmodel includes factors that consider fertility of pred-icates and arguments, the predicate argument struc-ture present in coherent discourses, and soft con-straints on predicate coreference determined by atemporal relation classifier.
We have shown that thismodel significantly improves upon prior work whichuses extensive lexical resources but without the ben-efit of joint inference.
Additionally, this is one of thefirst demonstrations of the benefits of temporal rela-tion identification.
Overall, this work demonstratesthe benefits of considering global document infor-mation as part of natural language understanding.Future work should extend the problem formu-lation of predicate argument alignment to considerincremental linking: starting with a pair of docu-ments, perform linking, and then continue to addin documents over time.
This problem formula-tion would capture the evolution of a breaking newsstory, which closely matches the type of data (newsarticles) considered in this work (EECB and RFdatasets).
This formulation ties into existing workon news summarization, topic detection and track-ing, an multi-document NLU.
This goes hand withwork on better intra-document relation predictionmethods, such as the temporal relation model usedin this work, to lead to better joint linking decisions.ReferencesNicholas Andrews, Jason Eisner, and Mark Dredze.2012.
Name phylogeny: A generative model of stringvariation.
In EMNLP-CoNLL, pages 344?355.
ACL.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics - Volume 1,ACL ?98, pages 86?90, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Cosmin Adrian Bejan and Sanda Harabagiu.
2010.
Un-supervised event coreference resolution with rich lin-guistic features.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, ACL ?10, pages 1412?1422, Stroudsburg, PA,USA.
Association for Computational Linguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, Meredith J. Goldsmith, Jan Hajic,Robert L. Mercer, and Surya Mohanty.
1993.
Butdictionaries are data too.
In Proceedings of theWorkshop on Human Language Technology, HLT ?93,pages 202?205, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Nathanael Chambers, Taylor Cassidy, Bill McDowell,and Steven Bethard.
2014.
Dense event ordering witha multi-pass architecture.
Transactions of the Associ-ation for Computational Linguistics, 2.Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.2008.
Constructing corpora for the development andevaluation of paraphrase systems.
Comput.
Linguist.,34(4):597?614, December.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of NAACL-HLT, pages 758?764, Atlanta, Georgia, June.
Association for Compu-tational Linguistics.Goran Glava?s and Jan?Snajder.
2013.
Recognizingidentical events with graph kernels.
In Proceedingsof the 51st Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers),pages 797?803, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Thorsten Joachims, Thomas Finley, and Chun-Nam JohnYu.
2009.
Cutting-plane training of structural svms.Mach.
Learn., 77(1):27?59, October.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Simon Lacoste-Julien, Benjamin Taskar, Dan Klein, andMichael I. Jordan.
2006.
Word alignment via19quadratic assignment.
In Robert C. Moore, Jeff A.Bilmes, Jennifer Chu-Carroll, and Mark Sanderson,editors, HLT-NAACL.
The Association for Computa-tional Linguistics.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, EMNLP-CoNLL ?12, pages 489?500, Stroudsburg, PA, USA.Association for Computational Linguistics.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine learn-ing of temporal relations.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the Association forComputational Linguistics, pages 753?760.
Associa-tion for Computational Linguistics.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 55?60.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38:39?41.Courtney Napoles, Matthew Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In AKBC-WEKEX Workshop at NAACL 2012, June.James Pustejovsky, Jos Castao, Robert Ingria, RoserSaur, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2003.
Timeml: Robust specification of eventand temporal expressions in text.
In in Fifth Interna-tional Workshop on Computational Semantics (IWCS-5).Marta Recasens, Matthew Can, and Daniel Jurafsky.2013.
Same referent, different words: Unsupervisedmining of opaque coreferent mentions.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 897?906,Atlanta, Georgia, June.
Association for ComputationalLinguistics.Michael Roth and Anette Frank.
2012.
Aligning pred-icate argument structures in monolingual comparabletexts: a new corpus for a new task.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics - Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation, SemEval ?12, pages 218?227,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Dan Roth and Wen-tau Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In In Proceedings of CoNLL-2004, pages 1?8.Michael Roth.
2014.
Inducing Implicit Arguments viaCross-document Alignment: A Framework and its Ap-plications.
Ph.D. thesis, Heidelberg University, June.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.Max-margin markov networks.
MIT Press.Naushad UzZaman, Hector Llorens, Leon Derczynski,James Allen, Marc Verhagen, and James Pustejovsky.2013.
Semeval-2013 task 1: Tempeval-3: Evaluat-ing time expressions, events, and temporal relations.In Second Joint Conference on Lexical and Compu-tational Semantics (*SEM), Volume 2: Proceedings ofthe Seventh International Workshop on Semantic Eval-uation (SemEval 2013), pages 1?9, Atlanta, Georgia,USA, June.
Association for Computational Linguis-tics.David Weiss and Benjamin Taskar.
2010.
Structured pre-diction cascades.
Journal of Machine Learning Re-search - Proceedings Track, 9:916?923.Travis Wolfe, Benjamin Van Durme, Mark Dredze,Nicholas Andrews, Charley Bellar, Chris Callison-Burch, Jay DeYoung, Justin Snyder, Jonathann Weese,Tan Xu, and Xuchen Yao.
2013.
Parma: A predicateargument aligner.
In Proceedings of the 51th AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers).
Association for Compu-tational Linguistics, July.Xuchen Yao, Benjamin Van Durme, Chris Callison-burch, and Peter Clark.
2013.
Answer extraction assequence tagging with tree edit distance.
In In NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL.20
