Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9?16,Prague, Czech Republic, June 2007 c?2007 Association for Computational LinguisticsUsing Classifier Features for Studying the Effect of Native Language on theChoice of Written Second Language WordsOren TsurInstitute of Computer ScienceThe Hebrew UniversityJerusalem, Israeloren@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceThe Hebrew UniversityJerusalem, Israelwww.cs.huji.ac.il/?arirAbstractWe apply machine learning techniques tostudy language transfer, a major topic inthe theory of Second Language Acquisition(SLA).
Using an SVM for the problem ofnative language classification, we show thata careful analysis of the effects of variousfeatures can lead to scientific insights.
Inparticular, we demonstrate that character bi-grams alone allow classification levels ofabout 66% for a 5-class task, even when con-tent and function word differences are ac-counted for.
This may show that native lan-guage has a strong effect on the word choiceof people writing in a second language.1 IntroductionWhile advances in NLP achieve improved results forNLP applications such as machine translation, ques-tion answering and document summarization, thereare other fields of research that can benefit from themethods used by the NLP community.
Second Lan-guage Acquisition (SLA), a major area in AppliedLinguistics and Cognitive Science, is one such field.In this paper we demonstrate how modern machinelearning tools can contribute to SLA theory.
In par-ticular, we address the major SLA topic of languagetransfer, the effect of native language on second lan-guage learners.
Using an SVM for the computa-tional problem of native language classification, westudy in detail the effects of various SVM features.Surprisingly, character bi-grams alone lead to a clas-sification accuracy of about 66% in a 5-class task,even when accounting for differences in content andfunction words.This result leads us to form a novel hypothesis onthe role of language transfer in SLA: that the choiceof words people make when writing in a second lan-guage is strongly influenced by the phonology oftheir native language.As far as we know, this is the first time that sucha hypothesis has beed formulated.
Moreover, this isthe first statistical learning-supported hypothesis inlanguage transfer.
Our results should be further sub-stantiated by additional psycholinguistic and com-putational experiments; nonetheless, we provide astrong starting point.The next section provides some essential back-ground.
In Section 3 we describe our experimen-tal setup and feature selection, and in Section 4 wedetail an array of variations of experiments for rul-ing out some possible types of bias that might haveaffected the results.
In Section 5 we discuss our hy-pothesis in the context of psycho-linguistic theory.We conclude with directions for future research.2 BackgroundOur hypothesis is tested within an algorithm ad-dressing the practical problem of determining thenative language of an anonymous writer writing in aforeign language.
The problem is applicable to dif-ferent fields, such as language instructing, tailorederror correction, security applications and psycho-linguistic research.As background, we start from the somewhat re-lated problem of authorship attribution.
The au-thorship attribution problem was addressed by lin-9guists and other literary experts trying to pinpointan anonymous author, such as that of The FederalistPapers (Holmes and Forsyth, 1995).
Traditionally,authorship experts analyzed topics, stylistic idiosyn-crasies and personal information about the possiblecandidates in order to determine an author.While authorship is usually addressed with deephuman inspection of the texts in question, it has al-ready been shown that automatic text analysis basedon various stylistic features can identify the genderof an anonymous author with accuracy above 80%(Argamon et al 2003).
Various papers (Diedrich etal, 2003; Koppel and Schler, 2003; Koppel et al2005a; Stamatatos et al 2004) report relative suc-cess in machine based authorship attribution tasksfor small sets of known candidates.Native language detection is a harder problemthan the authorship attribution problem, since wewish to characterize the writing style of a set ofwriters rather than the unique style of a singleperson.
There are several works presenting non-native speech recognition and dialect analysis sys-tems (Bouselmi et al 2005; Bouselmi et al 2006;Hansen et al 2004).
However, all those works arebased on acoustic signals, not on written texts.Koppel et al(2005a) report an accuracy of 80% inthe task of determining a writer?s native language.To the best of our knowledge, this is the only pub-lished work on automated classification of an au-thor?s native language (along with another versionof the paper by the same authors (Koppel et al2005b)).
Koppel et alused an SVM (Scho?lkopf andSmola, 2002) and a combination of features in theirsystem (such as errors analysis and POS-error co-occurrences, as described in section 2.2), but sur-prisingly, it appears that a very naive set of featuresachieves a relatively high accuracy.
The charac-ter bi-gram frequencies feature performs rather well,and definitely outperforms the intuitive contributionof frequent bigrams in this type of task.3 Experimental Setting3.1 The CorpusThe corpus that served for all of the experimentsdescribed in this paper is the International Corpusof Learner English (ICLE) (Granger et al 2002),which was also the one used by Koppel et al(2005a;2005b).
The corpus was compiled for the purpose ofstudying the English writing of non-native speakers.All contributors to the corpus are advanced Englishstudents and are roughly the same age.
The corpus iscombined from a number of sub-corpora, each con-taining one native language.
The corpus was assem-bled in ten years of international collaboration be-tween a number of universities and it contains morethan 2 million words of writing by students from 19different native language backgrounds.
We followedKoppel et al(2005a) and worked on 5 sub-corpora,each containing 238 randomly selected essays by na-tive speakers of the following languages: Bulgarian,Czech, French, Russian and Spanish.
Each of thetexts in the corpus was written by a different authorand is of length between 500 to 1,000 words.
Eachof the sub corpora contains about 180,000 (unique)types, for a total of 886,677 tokens.Essays in the corpus are of two types: argumen-tative essays and literature examination papers.
De-scriptive, narrative or technical subjects were not in-cluded in the corpus.
The literature examination es-says were restricted to no more than 25% of eachsub-corpus.
Each contributor was requested to fill alearner profile that was used to fine-proof the corpusas needed.In order to verify our results we used another con-trol corpus containing the Dutch and Italian sub-corpora contained in the ICLE instead of the Bul-garian and French ones.3.2 Document RepresentationIn the original experiment by Koppel et al(2005a)each document was represented by a numerical vec-tor of 1,035 dimensions.
Each vector entry rep-resented the frequency (relative to the document?slength) of a given feature.
The features were of 4types:?
400 function words?
200 most frequent letter n-grams?
250 rare POS bi-gram?
185 error typesWhile the first three types of attributes are relativelystraightforward, the fourth is more complex.
It rep-resents clusters of families of spelling errors as wellas co-occurrences of errors and POS tags.
Document10representation is described in detail in (Koppel et al2005a; Koppel et al 2005b).A multi-class SVM (Witten and Frank, 2005) wasemployed for learning and evaluating the classifica-tion model.
The experiment was run in a 10-foldcross validation manner in order to test the effec-tiveness of the model.3.3 Previous ResultsKoppel et al(2005a) report that when all featurestypes were used in tandem, an accuracy of 80.2%was achieved.
In the discussion section they an-alyze the frequency of a few function words, er-ror types, the co-occurrences of POS tags and er-rors, and the co-occurrences of POS tags and certainfunction words that seem to have significance in thesupport vectors learnt by the SVM.The goal of their research was to obtain the bestclassification, therefore the results obtained by us-ing only bi-grams of characters were not particularlynoted, although, surprisingly, representing each doc-ument by only using the relative frequency of thetop 200 characters bi-grams achieves an accuracy ofabout 66%.
We believe that this surprising fact ex-poses some fundamental phenomenon of human lan-guage behavior.
In the next section we describe a setof experiments designed to isolate the causes of thisphenomenon.4 Experimental Variations and ResultsIntuitively, we do not expect the most frequent char-acter n-grams to serve as good native language pre-dictors, expecting that these will only reflect themost frequent English words (and characters se-quences).
Accordingly, without language transfereffects, a naive baseline classifier based on an n-gram model is expected to achieve about 20% ac-curacy in a 5 native languages classification task.However, using classification based on the relativefrequency of top 200 bi-grams achieves about 66%1in all experiments, substantially higher than the ran-dom baseline.
These results are so surprising thatthey suggest that the characters bi-grams classifi-cation masks some other bias or noise in the cor-pus, or, conversely, that it mirrors other simple-to-1Koppel et aldid not report these results explicitly.
How-ever, they can be roughly estimated from their graph.Figure 1: Classification accuracy of the differentvariations of document representation.
b-g: bi-grams, f-w: function words, c-w: content words.explain phenomena such as shallow language trans-fer through the use of function words, or contentbias.
The following sub-sections describe differentvariations of the experiment, ruling out the effect ofthese different types of bias.4.1 Unigram BaselineWe first implemented a naive baseline classifier.
Werepresented each document by the normalized fre-quencies of the (de-capitalized) letters it contains2.These frequencies are simply a unigram model ofthe sub-corpora.
Using the multi-class SVM (Wit-ten and Frank, 2005) we obtained 46.78% accu-racy.
This accuracy is more than twice the ran-dom baseline accuracy.
This result is in accordancewith our bi-grams results.
Our discussion focuses onbi-grams rather than unigrams because the former?sresults are much higher and because bi-grams aremuch closer to the phonology of the language (foralphabetic scripts, of course).4.2 Bi-grams Based ClassificationChoosing the 200 most frequent character bi-gramsin the corpus, we used a vector of the same dimen-sion.
Each vector entry contained the normalizedfrequency of one of the bi-grams.
Using a multi-class SVM in a 10-fold cross validation manner we2White spaces were considered a letter.
However, sequencesof white spaces and tabs were collapsed to a single white space.All the experiments that make use of character frequencies wereperformed twice, including and excluding punctuation marks.Results for both experiments are similar, therefore all the num-bers reported in this paper are based on letters and punctuationmarks.11Bulg.
Czech French Russian Spanishdr 170 183 n/a 195 n/aam 117 135 142 140 152m 121 120 133 119 139iv 104 138 144 148 148y 161 181 196 183 166la 122 123 122 142 105Table 1: Some of the separating bi-grams found inthe feature selection process.
?
?
indicates a whitespace.
The numbers are the frequency ranking ofthe bi-grams in each sub-corpus (e.g., there are 103bi-grams more frequent than ?iv?
in the Bulgariancorpus).
n/a indicates that this bi-gram is not one ofthe 200 most frequent bi-grams of the sub-corpus.achieved 65.60% accuracy with standard deviationof 3.99.The bi-grams features in the 200 dimensional vec-tor are the 200 most frequent bi-grams in the wholecorpus, regardless of their frequency in each sub-corpus.
We note that the effect of misspelled wordson the 200 most frequent bi-grams is negligible.A more sophisticated feature selection could re-duce the dimension of the representation vectorwithout detracting from the results.
Careful fea-ture selection can also give a better intuition regard-ing the support vectors.
We performed feature se-lection in the following manner: we chose the top200 bi-grams of each sub-corpus, getting 245 uniquebi-grams in total.
We then chose all the bi-gramsthat were ranked significantly higher or significantlylower in one language than in at least one otherlanguage, assuming that those bi-grams have strongseparating power.
With the threshold of significanceset to 20 we obtained 84 separating bi-grams.
Table1 shows some of the separating bi-grams thus found.For example, ?la?
is a good separator between Rus-sian and Spanish (its rank in the Spanish corpus ismuch higher than that in the Russian corpus), butnot between other pairs.Using only those 84 bigrams we obtained clas-sification accuracy of 61.38%, a drop of only 4%compared to the results achieved with the 200 di-mensional vectors.
These results show that increas-ing the dimension of the representation vector usingadditional bi-grams contribute a marginal improve-ment while it does not introduce substantial noise.4.3 Using Tri-gram Frequencies as FeaturesRepeating the same experiment with the top 200 tri-grams, we obtained an accuracy of 59.67%, whichis 40% higher than the expected baseline and 15%higher than the uni-grams baseline.
These resultsshow that the texts in our corpus can be classifiedby only using naive n-gram models, while the op-timal n of the n-gram is a different question thatmight be addressed in a different work (and mightbe language-dependent).4.4 Function Words Based ClassificationFunction words are words that have a little lexicalmeaning but instead serve to express grammaticalrelations within a sentence or specify the attitude ofthe speaker (function words should not be confusedwith stopwords, although the lists of most frequentfunction words and the stopword list share a largesubset).
We used the same list of 460 function wordsused by Koppel et al(2005a).
A partial list includes:{a, afterward, although, because, cannot, do, enter,eventually, fifteenth, hither, hath, hence, lastly, oc-casionally, presumable, said, seldom, undoubtedly,was}.In this variation of the experiment, we representedeach document only by the relative frequencies ofthe function words it contained.
Using the sameexperimental setup as before, we achieved an ac-curacy of 66.7%.
These results are less surprisingthan the results obtained by the character n-gramsvectors, since we do expect native speakers of a cer-tain language to use, misuse or ignore certain func-tion words as a result from language transfer mech-anisms (Odlin, 1989).
For example, it is well knownthat native speakers of Russian tend to omit Englisharticles.4.5 Function Words BiasThe previous results suggest that the n-gram basedclassification is simply the result of the differentuses of function words by speakers of different na-tive languages.
In order to rule out the effect of thefunction words on the bi-gram-based classification,we removed all function words from the corpus, re-calculated the bi-gram frequencies and ran the ex-periment once again, this time achieving an accuracyof 62.92% in the 10-fold cross validation test.12These results, obtained on the function words-freecorpus, clearly show that n-gram based classificationis not a mere artifact masking the use of functionwords.4.6 Content BiasBi-gram frequencies could also reflect content biasrather than language use.
By content bias we meanthat the subject matter of the documents in the dif-ferent sub-corpora could exhibit internal sub-corpusuniformity and external sub-corpus disparity.
In or-der to rule this out, we employed a variation on theTerm Frequency ?
Inverted Document Frequency(tf-idf ) content analysis metric.The tf-idf measure is a statistical measure that isused in information retrieval tasks to evaluate howimportant a word/term is to a document in a collec-tion or corpus (Salton and Buckley, 1988).
Given acollection of documents D, the tf-idf weight of termt in a document d ?
D is computed as follows:tfidft = ft,d ?
log|D|ft,Dwhere ft,d is the frequency of term t in documentd, and ft,D is the number of documents in which tappears.
Therefore, the weight of term t ?
d is max-imal if it is a common term in d while the number ofdocuments it appears in is relatively low.We used the tf-idf weights in the information re-trieval sense in order to discover the dominant con-tent words of each sub-corpus.
We treated each sub-corpus (set of documents by writers who share anative language) as a single document and calcu-lated the tf-idf of each word.
In order to determinewhether there is a content bias or not, we set a domi-nance threshold, and removed all words such that thedifference between their tf-idf score in two differentsub-corpora is higher than the dominance threshold.Given a threshold t, the dominance Dw,t, of a tokenw is given by:Dw,t = maxi,j |tfidfw,i ?
tfidfw,j |where tfidfw,k is the tf-idf score of token w insub-corpus k. Changing the threshold in 0.0005 in-tervals, we removed from 1 to 340 unique contentwords (between 1,545 and 84,725 word tokens in to-tal).
However, the classification accuracy was essen-tially the same (see Figure 2), with a slight drop ofWord Bulg.
Czech Fr.
Rus.
Spa.europe 0 0.3 2.7 0.2 0.2european 0 0.3 3 0.1 0.5imagination 4.3 2 0.8 1 0.8television 0 3.6 1.9 3.1 0.3women 0.4 1.7 1.2 5.5 2.6Table 2: The tf-idf score of some of the most domi-nant words, multiplied by 1,000 for easier reading.Subcorpus content function uniquewords words stemsBulgarian 1543 94685 11325Czech 2784 110782 12834French 2059 67016 9474Russian 2730 112410 12338Spanish 2985 108052 12627Total 12101 492945 36474Table 3: Numbers of dominant content words (witha threshold of 0.0025) and function words that wereremoved from each sub-corpus.
The unique stemscolumn indicates the number of unique stems (types)that remained after removal of c-w and f-w.only 2% after removing 51 content words (by usinga threshold of 0.0015).We calculated the tf-idf weights after stop-wordsremoval and stemming (using a Porter stemmer(Porter, 1980)), trying to pinpoint dominant stems.The results were similar to the word?s tf-idf and nosignificantly dominant stem was found in either ofthe sub-corpora.A drop of only 3% in accuracy was noticed afterremoving both dominant content words and functionwords.
These results show that if a content bias ex-ists in the corpus it has only a minor effect on theSVM classification, and that the n-grams based clas-Figure 2: Classification accuracy as a function of thethreshold (removed content words).13Thresh.
0.004 0.003 0.0025 0.0015 0.0012 c-w 9 c-w 15 c-w 51 c-w 113 c-wBulg.
77 908 1543 3955 7426Czech 306 1829 2784 5139 8588French 665 1829 2059 3603 6205Russian 781 1886 2730 6302 9918Spanish 389 1418 2985 6548 10521Total 2218 7970 12101 25547 42658Table 4: Number of occurrences of content wordsthat were removed from each sub-corpus for someof the thresholds.
The numbers in the top row indi-cate the threshold and the number of unique contentwords that were found with this threshold.sification is not an artifact of a content bias.We ran the same experiment five more times, eachtime on 4 sub-corpora instead of 5, removing one(different) language each time.
The results in all 54-class experiments were essentially the same, andsimilar to those of the 5 language task (beyond thefact that the random baseline for the former is 25%rather than 20%).4.7 Suffix BiasBias might also be attributed to the use of suf-fixes.
There are numerous types of English suf-fixes, which, roughly speaking, may be categorizedas derivational or inflectional.
It is reasonable to ex-pect that just like a use of function words, use or mis-use of certain suffixes might occur due to languagetransfer.
Frequent use of a certain suffix or avoid-ance of the use of a certain suffix may influence thebi-grams statistics and thus the bi-grams classifica-tion may be only an artifact of the suffixes usage.Checking the use of the 50 most productive suf-fixes taken from a standard list (e.g.
ing, ed, less,able, most, en) we have found that only a small num-ber of suffixes are not equally used by speakers of all5 languages.
Most notable are the differences in theuse of ing between native French speakers and na-tive Czech speakers and the differences of use of lessbetween Bulgarian and Spanish speakers (Table 5).However, no real bias can be attributed to the use ofany of the suffixes because their relative aggregateeffect on the values in the support vector entries isvery small.Suffix Bulg.
Czech French Russian Spanishing 872 719 932 903 759less 47 36 39 45 32Table 5: Counts of two of the suffixes whose fre-quency of use differs the most between sub-corpora.4.8 Control CorpusFinally, we have also ran the experiment on a differ-ent corpus replacing the French and the Spanish sub-corpora by the Dutch and Italian ones, introducing anew Roman language and a new Germanic languageto the corpus.
We obtained 64.66% accuracy, essen-tially the same as in the original 5-language setting.The corpus was compiled from works of advancedEnglish students of the same level who write essaysof approximately the same length, on a set of ran-domly and roughly equally distributed topics.
Weexpected that these students will use roughly thesame n-grams distribution.
However, the results de-scribed above suggest that there exists some mecha-nism that influences the authors?
choice of words.
Inthe next section we present a computational psycho-linguistic framework that might explain our results.5 Statistical Learning and LanguageTransfer in SLA5.1 Statistical Learning by InfantsPsychologists, linguists, and cognitive science re-searchers try to understand the process of languagelearning by infants.
Many models for languagelearning and cognitive language modeling were sug-gested (Clark, 2003).Infants learn their first language by a combina-tion of speech streams, vocal cues and body ges-tures.
Infants as young as 8 months old have alimited grasp of their native tongue as they reactto familiar words.
In that age they already under-stand the meaning of single words, they learn to spotthese words in a speech stream, and very soon theylearn to combine different words into new sententialunits.
Parental speech stream analysis shows that itis impossible to separate between words by identi-fying sequences of silence between words (Saffran,2001).
Recent studies of infant language learningare in favor of the statistical framework (Saffran,2001; Saffran et al 1996).
Saffran (2002) exam-14ined 8 month-old to one year-old infants who werestimulated by speech sequences.
The infants showeda significant discrimination between word and non-word stimuli.
In a different experimental setup in-fants showed a significant discrimination betweenfrequent syllable n-grams and non frequent sylla-ble n-grams, heard as part of a gibberish speech se-quence generated by a computer according to var-ious statistical language models.
In a third experi-mental setup infants showed a significant discrimi-nation in favor of English-like gibberish speech se-quences upon non-English-like gibberish speech se-quences.
These findings along with the establishedfinding (Jusczyk, 1997) that infants prefer the soundof their native tongue suggest that humans learn ba-sic language units in a statistical manner and thatthey store some statistical parameters pertaining tothese units.
We should note that some researchersdoubt these conclusions (Yang, 2004).5.2 Language Transfer in SLAThe role of the first language in second language ac-quisition is under a continuous debate (Ellis, 1999).Language Transfer between L1 and L2 is the pro-cess in which a language learner of L2 whose na-tive language is L1, is influenced by L1 when usingL2 (actually, when building his/her inter-language).This influence might appear helpful when L2 is rel-atively close to L1, but it interferes with the learn-ing process due to over- and under-generalization orother problems.
Although there is clear evidencethat language learners use constructs of their firstlanguage when learning a foreign language (James,1980; Odlin, 1989), it is not clear that the majorityof learner errors can be attributed to the L1 transfer(Ellis, 1999).5.3 Sound Transfer HypothesisFor alphabetic scripts, character bi-grams reflect ba-sic sounds and sound sequences of the language3.We have shown that native language strongly corre-lates with character bi-grams when people write inEnglish as a second language.
After ruling out usageof function words, content bias, and morphology-related influences, the most plausible explanation is3Note that for English, they do not directly correspond tophonemes or syllables.
Nonetheless, they do reflect Englishphonology to some extent.that these are language transfer effects related to L1sounds.We hypothesize that there are language transfereffects related to L1 sounds and manifested by thewords that people choose to use when writing in asecond language.
(We say ?writing?
because we haveonly experimented with written texts; a more gen-eral hypothesis covering speaking and writing canbe formulated as well.
)Furthermore, since the acquisition and represen-tation of phonology is strongly influenced by statis-tical considerations (Section 5.1), we speculate thatthe general language transfer phenomenon might berelated to frequency.
This does not directly followfrom our findings, of course, but is an exciting direc-tion to investigate, and it is in accordance with thegrowing body of work on the effects of frequencyon language learning and the emergence of syntax(Ellis, 2002; Bybee, 2006).We note that there is one obvious and well-knownlexical transfer effect: the usage of cognates (wordsthat have similar form (sound) and meaning in twodifferent languages).
However, the languages weused in our experiments contain radically differingamounts of cognates of English words (just considerFrench vs. Bulgarian, for example), while the clas-sification results were about the same for all 5 lan-guages.
Hence, cognates might play a role, but theydo not constitute a single major explaining factor forour findings.We note that the hypothesis put forward in thepresent paper is the first that attributes a languagetransfer phenomenon to a cognitive representation(phonology) whose statistical nature has been seri-ously substantiated.6 ConclusionIn this paper we have demonstrated how modern ma-chine learning can aid other fields, here the impor-tant field of Second Language Acquisition (SLA).Our analysis of the features useful for a multi-classSVM in the task of native language classification hasresulted in the formulation of a hypothesis of poten-tial significance in the theory of language transferin SLA.
We hypothesize language transfer effects atthe level of basic sounds and short sound sequences,manifested by the words that people choose when15writing in a second language.
In other words, wehypothesize that use of L2 words is strongly influ-enced by L1 sounds and sound patterns.As noted above, further experiments (psycholog-ical and computational) must be conducted for vali-dating our hypothesis.
In particular, construction ofa wide-scale learners?
corpus with tight control overcontent bias is essential for reaching stronger con-clusions.Additional future work should address sound se-quences vs. the orthographic sequences that wereused in this work.
If our hypothesis is correct, thenusing spoken language corpora should produce evenstronger results, since (1) writing systems rarelyshow a 1-1 correspondence with how words are atthe phonological level; and (2) writing allows moreconscious thinking that speaking, thus potentially re-duces transfer effects.
Our eventual goal is creatinga unified model of statistical transfer mechanisms.ReferencesArgamon S., Koppel M. and Shimoni A.
2003.
Gender,Genre, and Writing Style in Formal Written Texts.
Text23(3).Bouselmi G., Fohr D., Illina, I., and Haton J.P.2005.
Fully Automated Non-Native Speech Recog-nition Using Confusion-Based Acoustic Model.
Eu-rospeech/Interspeech ?05.Bouselmi G., Fohr D., Illina I., and Haton J.P. 2006.Fully Automated Non-Native Speech Recognition Us-ing Confusion-Based Acoustic Model Integration andGraphemic Constraints.
IEEE International Confer-ence on Acoustics, Speech and Signal Processing,2006.Bybee J.
2006.
Frequency of Use and the Organizationof Language.
Oxford University Press.Clark, E. 2003.
First Language Acquisition.
CambridgeUniversity Press.Diederich J., Kindermann J., Leopold E. and Paass G.2004.
Authorship Attribution with Support Vector Ma-chines.
Applied Intelligence, 109?123.Ellis N. 2002.
Frequency Effects in Language Pro-cessing.
Studies in Second Language Acquisition,24(2):143?188.Ellis R. 1999.
Understanding Second Language Acqui-sition.
Oxford University Press.Granger S., Dagneaux E. and Meunier F. 2002.
Inter-national Corpus of Learner English.
Presses universi-taires de Louvain.Hansen J. H., Yapanel U., Huang, R. and Ikeno A.
2004.Dialect Analysis and Modeling for Automatic Classi-fication.
Interspeech-2004/ICSLP-2004: InternationalConference Spoken Language Processing.
Jeju Island,South Korea.Holmes D. and Forsyth R. 1995.
The Federalist Revis-ited: New Directions in Authorship Attribution.
Liter-ary and Linguistic Computing, pp.
111?127.James C. E. 1980.
Contrastive Analysis.
New York:Longman.Jusczyk P. W. 1997.
The Discovery of Spoken Language.MIT Press.Koppel M. and Schler J.
2003.
Exploiting Stylistic Id-iosyncrasies for Authorship Attribution.
In Proceed-ings of IJCAI ?03 Workshop on Computational Ap-proaches to Style Analysis and Synthesis.
Acapulco,Mexico.Koppel M., Schler J. and Zigdon K. 2005(a).
Determin-ing an Author?s Native Language by Mining a Text forErrors.
Proceedings of KDD ?05.
Chicago IL.Koppel M., Schler J. and Zigdon K. 2005(b).
Auto-matically Determining an Anonymous Author?s NativeLanguage.
In Intelligence and Security Informatics(pp.
209?217).
Berlin / Heidelberg: Springer.Odlin T. 1989.
Language Transfer: Cross-Linguistic In-fluence in Language Learning.
Cambridge UniversityPress.Porter F. M. 1980.
An Algorithm for Suffix Stripping.Program, 14(3):130?137.Saffran J. R. 2001.
Words in a Sea of Sounds: The Outputof Statistical Learning.
Cognition, 81, 149?169.Saffran J. R. 2002.
Constraints on Statistical LanguageLearning.
Journal of Memory and Language, 47, 172?196.Saffran J. R., Aslin R. N. and Newport E. N. 1996.
Sta-tistical Learning by 8-month Old Infants.
Science, is-sue 5294, 1926?1928.Salton G. and Buckley C. 1988.
Term Weighing Ap-proaches in Automatic Text Retrieval.
InformationProcessing and Management, 24(5):513?523.Scho?lkopf B,.
Smola A 2002.
Learning with Kernels.MIT Press.Stamatatos E,.
Fakotakis N. and Kokkinakis G. 2004.Computer-Based Authorship Attribution Without Lex-ical Measures.
Computers and the Humanities, 193?214.Witten I. H. and Frank E. 2005.
Data Mining: PracticalMachine Learning Tools and Techniques.
San Fran-cisco: Morgan Kaufmann.Yang C. 2004.
Universal Grammar, Statistics, or Both?.Trends in Cognitive Science 8(10):451?456, 2004.16
