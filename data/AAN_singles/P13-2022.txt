Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 120?125,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBenefactive/Malefactive Event and Writer Attitude AnnotationLingjia Deng ?, Yoonjung Choi ?, Janyce Wiebe ???
Intelligent System Program, University of Pittsburgh?
Department of Computer Science, University of Pittsburgh?lid29@pitt.edu, ?
{yjchoi,wiebe}@cs.pitt.eduAbstractThis paper presents an annotation schemefor events that negatively or positivelyaffect entities (benefactive/malefactiveevents) and for the attitude of the writertoward their agents and objects.
Work onopinion and sentiment tends to focus onexplicit expressions of opinions.
However,many attitudes are conveyed implicitly,and benefactive/malefactive events areimportant for inferring implicit attitudes.We describe an annotation scheme andgive the results of an inter-annotatoragreement study.
The annotated corpus isavailable online.1 IntroductionWork in NLP on opinion mining and sentimentanalysis tends to focus on explicit expressions ofopinions.
Consider, however, the following sen-tence from the MPQA corpus (Wiebe et al, 2005)discussed by (Wilson and Wiebe, 2005):(1) I think people are happy becauseChavez has fallen.The explicit sentiment expression, happy, is pos-itive.
Yet (according to the writer), the peopleare negative toward Chavez.
As noted by (Wil-son and Wiebe, 2005), the attitude toward Chavezis inferred from the explicit sentiment toward theevent.
An opinion-mining system that recognizesonly explicit sentiments would not be able to per-ceive the negative attitude toward Chavez con-veyed in (1).
Such inferences must be addressedfor NLP systems to be able to recognize the fullrange of opinions conveyed in language.The inferences arise from interactions be-tween sentiment expressions and events such asfallen, which negatively affect entities (malefac-tive events), and events such as help, which pos-itively affect entities (benefactive events).
Whilesome corpora have been annotated for explicitopinion expressions (for example, (Kessler etal., 2010; Wiebe et al, 2005)), there isn?t apreviously published corpus annotated for bene-factive/malefactive events.
While (Anand andReschke, 2010) conducted a related annotationstudy, their data are artificially constructed sen-tences incorporating event predicates from a fixedlist, and their annotations are of the writer?sattitude toward the events.
The scheme pre-sented here is the first scheme for annotating, innaturally-occurring text, benefactive/malefactiveevents themselves as well as the writer?s attitudetoward the agents and objects of those events.2 OverviewFor ease of communication, we use the termsgoodFor and badFor for benefactive and malefac-tive events, respectively, and use the abbreviationgfbf for an event that is one or the other.
There aremany varieties of gfbf events, including destruc-tion (as in kill Bill, which is bad for Bill), cre-ation (as in bake a cake, which is good for thecake), gain or loss (as in increasing costs, whichis good for the costs), and benefit or injury (as incomforted the child, which is good for the child)(Anand and Reschke, 2010).The scheme targets clear cases of gfbf events.The event must be representable as a triple of con-tiguous text spans, ?agent, gfbf, object?.
Theagent must be a noun phrase, or it may be implicit(as in the constituent will be destroyed).
The ob-ject must be a noun phrase.120Another component of the scheme is the influ-encer, a word whose effect is to either retain orreverse the polarity of a gfbf event.
For example:(2) Luckily Bill didn?t kill him.
(3) The reform prevented companiesfrom hurting patients.
(4) John helped Mary to save Bill.In (2) and (3), didn?t and prevented, respectively,reverse the polarity from badFor to goodFor (notkilling Bill is good for Bill; preventing companiesfrom hurting patients is good for the patients).
In(4), helped is an influencer which retains the polar-ity (i.e., helping Mary to save Bill is good for Bill).Examples (3) and (4) illustrate the case where aninfluencer introduces an additional agent (reformin (3) and John in (4)).The agent of an influencer must be a nounphrase or implicit.
The object must be another in-fluencer or a gfbf event.Note that, semantically, an influencer can beseen as good for or bad for its object.
A reverserinfluencer makes its object irrealis (i.e., not hap-pen).
Thus, it is bad for it.
In (3), for example,prevent is bad for the hurting event.
A retainer in-fluencer maintains its object, and thus is good forit.
In (4), for example, helped maintains the sav-ing event.
For this reason, influencers and gfbfevents are sometimes combined in the evaluationspresented below (see Section 4.2).Finally, the annotators are asked to mark thewriter?s attitude towards the agents of the influ-encers and gfbf events and the objects of the gfbfevents.
For example:(5) GOP Attack on Reform Is a FightAgainst Justice.
(6) Jettison any reference to end-of-lifecounselling.In (5), there are two badFor events: ?GOP, Attackon, Reform?
and ?GOP Attack on Reform,FightAgainst, Justice?.
The writer?s attitude towardboth agents is negative, and his or her attitudetoward both objects is positive.
In (6), thewriter conveys a negative attitude toward end-of-life counselling.
The coding manual instructs theannotators to consider whether an attitude of thewriter is communicated or revealed in the particu-lar sentence which contains the gfbf event.3 Annotation SchemeThere are four types of annotations: gfbf event,influencer, agent, and object.
For gfbf events, theagent, object, and polarity (goodFor or badFor) areidentified.
For influencers, the agent, object andeffect (reverse or retain) are identified.
For agentsand objects, the writer?s attitude is marked (posi-tive, negative, or none).
The annotator links agentsand objects to their gfbf and influencer annotationsvia explicit IDs.
When an agent is not mentionedexplicitly, the annotator should indicate that it isimplicit.
For any span the annotator is not certainabout, he or she can set the uncertain option to betrue.The annotation manual includes guidelines tohelp clarify which events should be annotated.Though it often is, the gfbf span need not be averb or verb phrase.
We saw an example above,namely (5).
Even though attack on and fightagainst are not verbs, we still mark them becausethey represent events that are bad for the object.Note that, Goyal et al (2012) present a method forautomatically generating a lexicon of what theycall patient polarity verbs.
Such verbs correspondto gfbf events, except that gfbf events are, concep-tually, events, not verbs, and gfbf spans are notlimited to verbs (as just noted).Recall from Section 2 that annotators shouldonly mark gfbf events that may be represented as atriple, ?agent,gfbf,object?.
The relationship shouldbe perceptible by looking only at the spans in thetriple.
If, for example, another argument of theverb is needed to perceive the relationship, the an-notators should not mark that event.
(7) His uncle left him a massive amountof debt.
(8) His uncle left him a treasure.There is no way to break these sentences intotriples that follow our rules.
?His uncle, left, him?doesn?t work because we cannot perceive the po-larity looking only at the triple; the polarity de-pends on what his uncle left him.
?His uncle, lefthim, a massive amount of debt?
isn?t correct: theevent is not bad for the debt, it is bad for him.
Fi-nally, ?His uncle, left him a massive amount ofdebt, Null?
isn?t correct, since no object is iden-tified.Note that him in (7) and (8) are both consid-ered benefactive semantic roles (Zu?n?iga and Kit-tila?, 2010).
In general, gfbf objects are not equiva-121lent to benefactive/malefactive semantic roles.
Forexample, in our scheme, (7) is a badFor event and(8) is a goodFor event, while him fills the benefac-tive semantic role in both.
Further, according to(Zu?n?iga and Kittila?, 2010), me is the filler of thebenefactive role in She baked a cake for me.
Yet,in our scheme, a cake is the object of the good-For event; me is not included in the annotations.The objects of gfbf events are what (Zu?n?iga andKittila?, 2010) refer to as the primary targets of theevents, whereas, they state, beneficiary semanticroles are typically optional arguments.
The reasonwe annotate only the primary objects (and agents)is that the clear cases of attitude implicatures mo-tivating this work (see Section 1) are inferencestoward agents and primary objects of gfbf events.Turning to influencers, there may be chains ofthem, where the ultimate polarity and agent mustbe determined compositionally.
For example, thestructure of Jack stopped Mary from trying to killBill is a reverser influencer (stopped) whose objectis a retainer influencer (trying) whose object is, inturn, a badFor event (kill).
The ultimate polarity ofthis event is goodFor and the ?highest level?
agentis Jack.
In our scheme, all such chains of lengthNare treated as N ?
1 influencers followed by a sin-gle gfbf event.
It will be up to an automatic systemto calculate the ultimate polarity and agent usingrules such as those presented in, e.g., (Moilanenand Pulman, 2007; Neviarouskaya et al, 2010).To save some effort, the annotators are notasked to mark retainer influencers which do not in-troduce new agents.
For example, for Jack stoppedtrying to kill Bill, there is no need to mark ?trying.
?Of course, all reverser influencers must be marked.4 Agreement StudyTo validate the reliability of the annotationscheme, we conducted an agreement study.
In thissection we introduce how we designed the agree-ment study, present the evaluation method andgive the agreement results.
Besides, we conducta second-step consensus study to further analyzethe disagreement.4.1 Data and Agreement Study DesignFor this study, we want to use data that is rich inopinions and implicatures.
Thus we used the cor-pus from (Conrad et al, 2012), which consists of134 documents from blogs and editorials about acontroversial topic, ?the Affordable Care Act?.To measure agreement on various aspects ofthe annotation scheme, two annotators, who areco-authors, participated in the agreement study;one of the two wasn?t involved in developing thescheme.
The new annotator first read the anno-tation manual and discussed it with the first an-notator.
Then, the annotators labelled 6 docu-ments and discussed their disagreements to recon-cile their differences.
For the formal agreementstudy, we randomly selected 15 documents, whichhave a total of 725 sentences.
These documents donot contain any examples in the manual, and theyare different from the documents discussed duringtraining.
The annotators then independently anno-tated the 15 selected documents.4.2 Agreement Study EvaluationWe annotate four types of items (gfbf event, influ-encer, agent, and object) and their correspondingattributes.
As noted above in Section 2, influencerscan also be viewed as gfbf events.
Also, the twomay be combined together in chains.
Thus, wemeasure agreement for gfbf and influencer spanstogether, treating them as one type.
Then wechoose the subset of gfbf and influencer annota-tions that both annotators identified, and measureagreement on the corresponding agents and ob-jects.Sometimes the annotations differ even thoughthe annotators recognize the same gfbf event.Consider the following sentence:(9) Obama helped reform curb costs.Suppose the annotations given by the annotatorswere:Ann 1.
?Obama, helped, curb?
?reform, curb, costs?Ann 2.
?Obama, helped, reform?The two annotators do agree on the ?Obama,helped, reform?
triple, the first one marking helpedas a retainer and the other marking it as a goodForevent.
To take such cases into consideration in ourevaluation of agreement, if two spans overlap andone is marked as gfbf and the other as influencer,we use the following rules to match up their agentsand objects:?
for a gfbf event, consider its agent and objectas annotated;122?
for an influencer, assign the agent of the in-fluencer?s object to be the influencer?s object,and consider its agent as annotated and thenewly-assigned object.
In (9), Ann 2?s anno-tations remain the same and Ann 1?s become?Obama, helped, reform?
and ?reform, curb,costs?.We use the same measurement for agreementfor all types of spans.
Suppose A is a set of an-notations of a particular type and B is the set ofannotations of the same type from the other anno-tator.
For any text span a ?
A and b ?
B, the spancoverage c measures the overlap between a and b.Two measures of c are adopted here.Binary: As in (Wilson and Wiebe, 2003), if twospans a and b overlap, the pair is counted as 1,otherwise 0.c1(a, b) = 1 if |a ?
b| > 0Numerical: (Johansson and Moschitti, 2013)propose, for the pairs that are counted as 1 by c1, ameasure of the percentage of overlapping tokens,c2(a, b) =|a ?
b||b|where |a| is the number of tokens in span a, and ?gives the tokens that two spans have in common.As (Breck et al, 2007) point out, c2 avoids theproblem of c1, namely that c1 does not penalize aspan covering the whole sentence, so it potentiallyinflates the results.Following (Wilson and Wiebe, 2003), treat-ing each set A and B in turn as the gold-standard, we calculate the average F-measure, de-noted agr(A,B).
agr(A,B) is calculated twice,once with c = c1 and once with c = c2.match(A,B) =?a?A,b?B,|a?b|>0c(a, b)agr(A||B) = match(A,B)|B|agr(A,B) = agr(A||B) + agr(B||A)2Now that we have the sets of annotations onwhich the annotators agree, we use ?
(Artsteinand Poesio, 2008) to measure agreement for theattributes.
We report two ?
values: one for thepolarities of the gfbf events, together with the ef-fects of the influencers, and one for the writer?sgfbf & agent objectinfluencerall anno- c1 0.70 0.92 1.00tations c2 0.69 0.87 0.97only c1 0.75 0.92 1.00certain c2 0.72 0.87 0.98consensus c1 0.85 0.93 0.99study c2 0.81 0.88 0.98Table 1: Span overlapping agreement agr(A,B)in agreement study and consensus study.polarity & effect attitudeall 0.97 0.89certain 0.97 0.89Table 2: ?
for attribute agreement.attitude toward the agents and objects.
Note that,as in Example (9), sometimes one annotator marksa span as gfbf and the other marks it as an influ-encer; in such cases we regard retain and goodforas the same attribute value and reverse and badforas the same value.
Table 1 gives the agr valuesand Table 2 gives the ?
values.4.3 Agreement Study ResultsRecall that the annotator could choose whether(s)he is certain about the annotation.
Thus, weevaluate two sets: all annotations and only thoseannotations that both annotators are certain about.The results are shown in the top four rows in Table1.The results for agents and objects in Table 1 areall quite good, indicating that, given a gfbf or in-fluencer, the annotators are able to correctly iden-tify the agent and object.Table 1 also shows that results are not signifi-cantly worse when measured using c2 rather thanc1.
This suggests that, in general, the annotatorshave good agreement concerning the boundariesof spans.Table 2 shows that the ?
values are high for bothsets of attributes.4.4 Consensus AnalysisFollowing (Medlock and Briscoe, 2007), we ex-amined what percentage of disagreement is due tonegligence on behalf of one or the other annota-tor (i.e., cases of clear gfbfs or influencers thatwere missed), though we conducted our consensus123study in a more independent manner than face-to-face discussion between the annotators.
For anno-tator Ann1, we highlighted sentences for whichonly Ann2 marked a gfbf event, and gave Ann1?sannotations back to him or her with the highlightsadded on top.
For Ann2 we did the same thing.The annotators reconsidered their highlighted sen-tences, making any changes they felt they should,without communicating with each other.
Therecould be more than one annotation in a highlightedsentence; the annotators were not told the specificnumber.After re-annotating the highlighted sentences,we calculate the agreement score for all the an-notations.
As shown in the last two rows in Table1, the agreement for gfbf and influencer annota-tions increases quite a bit.
Similar to the claimin (Medlock and Briscoe, 2007), it is reasonableto conclude that the actual agreement is approx-imately lower bounded by the initial values andupper bounded by the consensus values, though,compared to face-to-face consensus, we provide atighter upper bound.5 Corpus and ExamplesRecall from in Section 4.1 that we use the corpusfrom (Conrad et al, 2012), which consists of 134documents with a total of 8,069 sentences fromblogs and editorials about ?the Affordable CareAct?.
There are 1,762 gfbf and influencer annota-tions.
On average, more than 20 percent of the sen-tences contain a gfbf event or an influencer.
Out ofall gfbf and influencer annotations, 40 percent areannotated as goodFor or retain and 60 percent areannotated as badFor or reverse.
For agents and ob-jects, 52 percent are annotated as positive and 47percent as negative.
Only 1 percent are annotatedas none, showing that almost all the sentences (inthis corpus of editorials and blogs) which con-tain gfbf annotations are subjective.
The annotatedcorpus is available online1.To illustrate various aspects of the annotationscheme, in this section we give several examplesfrom the corpus.
In the examples below, wordsin square brackets are agents or objects, words initalics are influencers, and words in boldface aregfbf events.1.
And [it] will enable [Obama and theDemocrats] - who run Washington - to get1http://mpqa.cs.pitt.edu/back to creating [jobs].
(a) Creating is goodFor jobs; the agent isObama and the Democrats.
(b) The phrase to get back to is a retainer in-fluencer.
But, the agent span is also Obamaand the Democrats, as the same with thegoodFor, so we don?t have to give an anno-tation for it.
(c) The phrase enable is a retainer influencer.Since its agent span is different (namely, it),we do create an annotation for it.2.
[Repealing [the Affordable Care Act]] wouldhurt [families, businesses, and our econ-omy].
(a) Repealing is a badFor event since it de-prives the object, the Affordable Care Act, ofits existence.
In this case the agent is implicit.
(b) The agent of the badFor event hurt is thewhole phrase Repealing the Affordable CareAct.
Note that the agent span is in fact a nounphrase (even though it refers to an event).Thus, it doesn?t break the rule that all agentgfbf spans should be noun phrases.3.
It is a moral obligation to end this indefensi-ble neglect of [hard-working Americans].
(a) This example illustrates a gfbf that cen-ters on a noun (neglect) rather than on a verb.
(b) It also illustrates the case when two wordscan be seen as gfbf events: both end and ne-glect of can be seen as badFor events.
Fol-lowing our specification, they are annotatedas a chain ending in a single gfbf event: endis an influencer that reverses the polarity ofthe badFor event neglect of.6 ConclusionAttitude inferences arise from interactionsbetween sentiment expressions and benefac-tive/malefactive events.
Corpora have beenannotated in the past for explicit sentiment ex-pressions; this paper fills in a gap by presentingan annotation scheme for benefactive/malefactiveevents and the writer?s attitude toward the agentsand objects of those events.
We conducted anagreement study, the results of which are positive.Acknowledgement This work was supportedin part by DARPA-BAA-12-47 DEFT grant#12475008 and National Science Foundationgrant #IIS-0916046.
We would like to thank theanonymous reviewers for their helpful feedback.124ReferencesPranav Anand and Kevin Reschke.
2010.
Verb classesas evaluativity functor classes.
In InterdisciplinaryWorkshop on Verbs.
The Identification and Repre-sentation of Verb Features.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Comput.Linguist., 34(4):555?596, December.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Iden-tifying expressions of opinion in context.
In Pro-ceedings of the 20th international joint conferenceon Artifical intelligence, IJCAI?07, pages 2683?2688, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.2012.
Recognizing arguing subjectivity and argu-ment tags.
In Proceedings of the Workshop onExtra-Propositional Aspects of Meaning in Com-putational Linguistics, ExProM ?12, pages 80?88,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Amit Goyal, Ellen Riloff, and Hal Daum III.
2012.
Acomputational model for plot units.
ComputationalIntelligence, pages no?no.Richard Johansson and Alessandro Moschitti.
2013.Relational features in fine-grained opinion analysis.Computational Linguistics, 39(3).Jason S. Kessler, Miriam Eckert, Lyndsay Clark, andNicolas Nicolov.
2010.
The 2010 icwsm jdpa sen-timent corpus for the automotive domain.
In 4thInt?l AAAI Conference on Weblogs and Social MediaData Workshop Challenge (ICWSM-DWC 2010).Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics.Karo Moilanen and Stephen Pulman.
2007.
Senti-ment composition.
In Proceedings of RANLP 2007,Borovets, Bulgaria.Alena Neviarouskaya, Helmut Prendinger, and Mit-suru Ishizuka.
2010.
Recognition of affect, judg-ment, and appreciation in text.
In Proceedings of the23rd International Conference on ComputationalLinguistics, COLING ?10, pages 806?814, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2/3):164?210.Theresa Wilson and Janyce Wiebe.
2003.
Annotatingopinions in the world press.
In Proceedings of the4th ACL SIGdial Workshop on Discourse and Dia-logue (SIGdial-03), pages 13?22.Theresa Wilson and Janyce Wiebe.
2005.
Annotat-ing attributions and private states.
In Proceedings ofACL Workshop on Frontiers in Corpus AnnotationII: Pie in the Sky.F.
Zu?n?iga and S. Kittila?.
2010.
Introduction.
InF.
Zu?n?iga and S.
Kittila?, editors, Benefactives andmalefactives, Typological studies in language.
J.Benjamins Publishing Company.125
