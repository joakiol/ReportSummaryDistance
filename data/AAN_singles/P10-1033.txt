Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 316?324,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDiscriminative Pruning for Discriminative ITG AlignmentShujie Liu?, Chi-Ho Li?
and Ming Zhou?
?School of Computer Science and TechnologyHarbin Institute of Technology, Harbin, Chinashujieliu@mtlab.hit.edu.cn?Microsoft Research Asia, Beijing, China{chl, mingzhou}@microsoft.comAbstractWhile Inversion Transduction Grammar (ITG)has regained more and more attention in recentyears, it still suffers from the major obstacle ofspeed.
We propose a discriminative ITG prun-ing framework using Minimum Error RateTraining and various features from previouswork on ITG alignment.
Experiment resultsshow that it is superior to all existing heuristicsin ITG pruning.
On top of the pruning frame-work, we also propose a discriminative ITGalignment model using hierarchical phrasepairs, which improves both F-score and Bleuscore over the baseline alignment system ofGIZA++.1 IntroductionInversion transduction grammar (ITG) (Wu, 1997)is an adaptation of SCFG to bilingual parsing.
Itdoes synchronous parsing of two languages withphrasal and word-level alignment as by-product.For this reason ITG has gained more and moreattention recently in the word alignment commu-nity (Zhang and Gildea, 2005; Cherry and Lin,2006; Haghighi et al, 2009).A major obstacle in ITG alignment is speed.The original (unsupervised) ITG algorithm hascomplexity of O(n6).
When extended to super-vised/discriminative framework, ITG runs evenmore slowly.
Therefore all attempts to ITGalignment come with some pruning method.
Forexample, Haghighi et al (2009) do pruning basedon the probabilities of links from a simpleralignment model (viz.
HMM); Zhang and Gildea(2005) propose Tic-tac-toe pruning, which isbased on the Model 1 probabilities of word pairsinside and outside a pair of spans.As all the principles behind these techniqueshave certain contribution in making good pruningdecision, it is tempting to incorporate all thesefeatures in ITG pruning.
In this paper, we pro-pose a novel discriminative pruning frameworkfor discriminative ITG.
The pruning model usesno more training data than the discriminative ITGparser itself, and it uses a log-linear model to in-tegrate all features that help identify the correctspan pair (like Model 1 probability and HMMposterior).
On top of the discriminative pruningmethod, we also propose a discriminative ITGalignment system using hierarchical phrase pairs.In the following, some basic details on the ITGformalism and ITG parsing are first reviewed(Sections 2 and 3), followed by the definition ofpruning in ITG (Section 4).
The ?DiscriminativePruning for Discriminative ITG?
model (DPDI)and our discriminative ITG (DITG) parsers willbe elaborated in Sections 5 and 6 respectively.The merits of DPDI and DITG are illustratedwith the experiments described in Section 7.2 Basics of ITGThe simplest formulation of ITG contains threetypes of rules: terminal unary rules ?
?
?/?
,where ?
and ?
represent words (possibly a nullword, ?)
in the English and foreign languagerespectively, and the binary rules ?
?
?,?
and?
?
?,?
, which refer to that the componentEnglish and foreign phrases are combined in thesame and inverted order respectively.From the viewpoint of word alignment, theterminal unary rules provide the links of wordpairs, whereas the binary rules represent the reor-dering factor.
One of the merits of ITG is that itis less biased towards short-distance reordering.Such a formulation has two drawbacks.
First ofall, it imposes a 1-to-1 constraint in word align-ment.
That is, a word is not allowed to align tomore than one word.
This is a strong limitation asno idiom or multi-word expression is allowed toalign to a single word on the other side.
In factthere have been various attempts in relaxing the1-to-1 constraint.
Both ITG alignment316approaches with and without this constraint willbe elaborated in Section 6.Secondly, the simple ITG leads to redundancyif word alignment is the sole purpose of applyingITG.
For instance, there are two parses for threeconsecutive word pairs, viz.
[?/??
[?/??
?/??]
]  and [[?/??
?/??]
?/??]
.
The problem of re-dundancy is fixed by adopting ITG normal form.In fact, normal form is the very first key to speed-ing up ITG.
The ITG normal form grammar asused in this paper is described in Appendix A.3 Basics of ITG ParsingBased on the rules in normal form, ITG wordalignment is done in a similar way to chart pars-ing (Wu, 1997).
The base step applies all relevantterminal unary rules to establish the links of wordpairs.
The word pairs are then combined intospan pairs in all possible ways.
Larger and largerspan pairs are recursively built until the sentencepair is built.Figure 1(a) shows one possible derivation for atoy example sentence pair with three words ineach sentence.
Each node (rectangle) represents apair, marked with certain phrase category, of for-eign span (F-span) and English span (E-span)(the upper half of the rectangle) and the asso-ciated alignment hypothesis (the lower half).Each graph like Figure 1(a) shows only one deri-vation and also only one alignment hypothesis.The various derivations in ITG parsing can becompactly represented in hypergraph (Klein andManning, 2001) like Figure 1(b).
Each hypernode(rectangle) comprises both a span pair (upper half)and the list of possible alignment hypotheses(lower half) for that span pair.
The hyperedgesshow how larger span pairs are derived fromsmaller span pairs.
Note that a hypernode mayhave more than one alignment hypothesis, since ahypernode may be derived through more than onehyperedge (e.g.
the topmost hypernode in Figure1(b)).
Due to the use of normal form, the hypo-theses of a span pair are different from each other.4 Pruning in ITG ParsingThe ITG parsing framework has three levels ofpruning:1) To discard some unpromising span pairs;2) To discard some unpromising F-spansand/or E-spans;3) To discard some unpromising alignmenthypotheses for a particular span pair.The second type of pruning (used in Zhang et.al.
(2008)) is very radical as it implies discardingtoo many span pairs.
It is empirically found to behighly harmful to alignment performance andtherefore not adopted in this paper.The third type of pruning is equivalent to mi-nimizing the beam size of alignment hypothesesin each hypernode.
It is found to be well handledby the K-Best parsing method in Huang andChiang (2005).
That is, during the bottom-upconstruction of the span pair repertoire, each spanpair keeps only the best alignment hypothesis.Once the complete parse tree is built, the k-bestlist of the topmost span is obtained by minimallyexpanding the list of alignment hypotheses ofminimal number of span pairs.The first type of pruning is equivalent to mi-nimizing the number of hypernodes in a hyper-graph.
The task of ITG pruning is defined in thispaper as the first type of pruning; i.e.
the searchfor, given an F-span, the minimal number of E-spans which are the most likely counterpart ofthat F-span.1 The pruning method should main-tain a balance between efficiency (run as quicklyas possible) and performance (keep as many cor-rect span pairs as possible).1 Alternatively it can be defined as the search of the minimalnumber of E-spans per F-span.
That is simply an arbitrarydecision on how the data are organized in the ITG parser.B:[e1,e2]/[f1,f2]{e1/f2,e2/f1}C:[e1,e1]/[f2,f2]{e1/f2}C:[e2,e2]/[f1,f1]{e2/f1}C:[e3,e3]/[f3,f3]{e3/f3}A:[e1,e3]/[f1,f3]{e1/f2,e2/f1,e3/f3}(a)C:[e2,e2]/[f2,f2]{e2/f2}C:[e1,e1]/[f1,f1]{e1/f1}C:[e3,e3]/[f3,f3]{e3/f3}C:[e2,e2]/[f1,f1]{e2/f1}C:[e1,e1]/[f2,f2]{e1/f2}B:[e1,e2]/[f1,f2]{e1/f2}A:[e1,e2]/[f1,f2]{e2/f2}A:[e1,e3]/[f1,f3]{e1/f2,e2/f1,e3/f3} ,{e1/f1,e2/f2,e3,f3}(b)B?<C,C> A?[C,C]A?[A,C]A?
[B,C]Figure 1:  Example ITG parses in graph (a) and hypergraph (b).317A na?ve approach is that the required pruningmethod outputs a score given a span pair.
Thisscore is used to rank all E-spans for a particularF-span, and the score of the correct E-spanshould be in general higher than most of the in-correct ones.5 The DPDI FrameworkDPDI, the discriminative pruning model pro-posed in this paper, assigns score to a span pair?
, ?
as probability from a log-linear model:?
?
?
=???
(  ????
?
, ?
?
)???
(  ????(?
, ?
?))??
???
(1)where each ??(?
,? )
is some feature about thespan pair, and each ?
is the weight of the corres-ponding feature.
There are three major questionsto this model:1) How to acquire training samples?
(Section5.1)2) How to train the parameters ?
?
(Section 5.2)3) What are the features?
(Section 5.3)5.1 Training SamplesDiscriminative approaches to word alignment usemanually annotated alignment for sentence pairs.Discriminative pruning, however, handles notonly a sentence pair but every possible span pair.The required training samples consist of variousF-spans and their corresponding E-spans.Rather than recruiting annotators for markingspan pairs, we modify the parsing algorithm inSection 3 so as to produce span pair annotationout of sentence-level annotation.
In the base step,only the word pairs listed in sentence-level anno-tation are inserted in the hypergraph, and the re-cursive steps are just the same as usual.If the sentence-level annotation satisfies thealignment constraints of ITG, then each F-spanwill have only one E-span in the parse tree.
How-ever, in reality there are often the cases where aforeign word aligns to more than one Englishword.
In such cases the F-span covering that for-eign word has more than one corresponding E-spans.
Consider the example in Figure 2, wherethe golden links in the alignment annotation are?1/?1, ?2/?1, and ?3/?2; i.e.
the foreign word?1 aligns to both the English words ?1 and ?2.Therefore the F-span  ?1,?1  aligns to the E-span  ?1, ?1  in one hypernode and to the E-span?2, ?2  in another hypernode.
When such situa-tion happens, we calculate the product of the in-side and outside probability of each alignmenthypothesis of the span pair, based on the proba-bilities of the links from some simpler alignmentmodel2.
The E-span with the most probable hypo-thesis is selected as the alignment of the F-span.A?[C,C]Cw:[e1,e1]/[f1,f1]{e1/f1}Ce:[e1]/?Cw:[e2,e2]/[f1,f1]Ce:[e2]/?Cw:[e3,e3]/[f2,f2]C:[e1,e2]/[f1,f1]{e2/f1}C:[e2,e3]/[f2,f2]{e3/f2}A:[e1,e3]/[f1,f2]{e1/f1,e3/f2},{e2/f1,e3/f2}C?
[Ce,Cw]A?[C,C]C?
[Ce,Cw]{e1/f1} {e1/f1}(a) (b)[f1,f1][e1,e1][e1,e2][e2,e2][f2,f2][e2,e3][e3,e3][f1,f2] [e1,e3]Figure 2: Training sample collection.Table (b) lists, for the hypergraph in (a), the candidateE-spans for each F-span.It should be noted that this automatic span pairannotation may violate some of the links in theoriginal sentence-level alignment annotation.
Wehave already seen how the 1-to-1 constraint inITG leads to the violation.
Another situation isthe ?inside-out?
alignment pattern (c.f.
Figure 3).The ITG reordering constraint cannot be satisfiedunless one of the links in this pattern is removed.f1      f2      f3      f4e1     e2      e3      e4Figure 3: An example of inside-out alignmentThe training samples thus obtained are positivetraining samples.
If we apply some classifier forparameter training, then negative samples arealso needed.
Fortunately, our parameter trainingdoes not rely on any negative samples.5.2 MERT for PruningParameter training of DPDI is based on Mini-mum Error Rate Training (MERT) (Och, 2003), awidely used method in SMT.
MERT for SMTestimates model parameters with the objective ofminimizing certain measure of translation errors(or maximizing certain performance measure oftranslation quality) for a development corpus.Given an SMT system which produces, with2 The formulae of the inside and outside probability of aspan pair will be elaborated in Section 5.3.
The simpleralignment model we used is HMM.318model parameters ?1?, the K-best candidate trans-lations ?
(??
; ?1?)
for a source sentence ?
?, and anerror measure ?(??
, ??,?)
of a particular candidate??,?
with respect to the reference translation ??
,the optimal parameter values will be:?
1?
= ???????1??
??
, ?
??
; ?1??
?=1= ???????1??
??
, ??,?
?(?
??
; ?1?
, ??,?)??=1?
?=1DPDI applies the same equation for parametertuning, with different interpretation of the com-ponents in the equation.
Instead of a developmentcorpus with reference translations, we have a col-lection of training samples, each of which is apair of F-span (??)
and its corresponding E-span(??).
These samples are acquired from some ma-nually aligned dataset by the method elaboratedin Section 5.1.
The ITG parser outputs for each fsa K-best list of E-spans ?
??
; ?1?
based on thecurrent parameter values ?1?.The error function is based on the presence andthe rank of the correct E-span in the K-best list:?
??
, ?
??
; ?1?
=?????
??
??
??
?
?
??
; ?1????????
?????????
(2)where ????
??
is the (0-based) rank of the cor-rect E-span ??
in the K-best list  ?
??
; ?1?
.
If  ??
isnot in the K-best list at all, then the error is de-fined to be ??????
?, which is set as -100000 inour experiments.
The rationale underlying thiserror function is to keep as many correct E-spansas possible in the K-best lists of E-spans, andpush the correct E-spans upward as much aspossible in the K-best lists.This new error measure leads to a change indetails of the training algorithm.
In MERT forSMT, the interval boundaries at which the per-formance or error measure changes are definedby the upper envelope (illustrated by the dashline in Figure 4(a)), since the performance/errormeasure depends on the best candidate transla-tion.
In MERT for DPDI, however, the errormeasure depends on the correct E-span ratherthan the E-span leading to the highest systemscore.
Thus the interval boundaries are the inter-sections between the correct E-span and all othercandidate E-spans (as shown in Figure 4(b)).
Therank of the correct E-span in each interval canthen be figured out as shown in Figure 4(c).
Fi-nally, the error measure in each interval can becalculated by Equation (2) (as shown in Figure4(d)).
All other steps in MERT for DPDI are thesame as that for SMT.??mfm-indexloss?k-8-9-10-8-9-100,000gold?
?mfm?k(a)(b)(c)(d)?k?kFigure 4: MERT for DPDIPart (a) shows how intervals are defined for SMT andpart (b) for DPDI.
Part (c) obtains the rank of correctE-spans in each interval and part (d) the error measure.Note that the beam size (max number of E-spans) foreach F-span is 10.5.3 FeaturesThe features used in DPDI are divided into threecategories:1) Model 1-based probabilities.
Zhang and Gil-dea (2005) show that Model 1 (Brown et al,1993; Och and Ney., 2000) probabilities ofthe word pairs inside and outside a span pair( ?
?1 , ?
?2 /[?
?1 ,?
?2]) are useful.
Hence thesetwo features:a) Inside probability (i.e.
probability ofword pairs within the span pair):????
?
?1,?2 ?
?1,?2=1?2 ?
?1?
?1 ??
????
?1,?2 ??
?1,?2b) Outside probability (i.e.
probability ofthe word pairs outside the span pair):????
?
?1,?2 ??1,?2=1?
?
?2 + ?1?
?1 ??
???
?
?1,?2 ??
?1,?2where ?
is the length of the foreign sen-tence.2) Heuristics.
There are four features in this cat-egory.
The features are explained with the319example of Figure 5, in which the span pairin interest is  ?2, ?3 /[?1,?2].
The four linksare produced by some simpler alignmentmodel like HMM.
The word pair  ?2/?1  isthe only link in the span pair.
The links?4/?2  and ?3/?3 are inconsistent with thespan pair.3f1      f2      f3      f4e1     e2      e3      e4Figure 5: Example for heuristic featuresa) Link ratio:2?#?????????
+???
?where #?????
is the number of links inthe span pair, and ????
and ????
are thelength of the foreign and English spansrespectively.
The feature value of the ex-ample span pair is (2*1)/(2+2)=0.5.b) inconsistent link ratio:2?#?????
?????????
+???
?where #??????????
is the number of linkswhich are inconsistent with the phrasepair according to some simpler alignmentmodel (e.g.
HMM).
The feature value ofthe example is (2*2)/(2+2) =1.0.c) Length ratio:?????????
???????
?where ????????
is defined as the averageratio of foreign sentence length to Eng-lish sentence length, and it is estimated tobe around 1.15 in our training dataset.The rationale underlying this feature isthat the ratio of span length should not betoo deviated from the average ratio ofsentence length.
The feature value for theexample is |2/2-1.15|=0.15.d) Position Deviation: ????
?
???
?where ????
refers to the position of theF-span in the entire foreign sentence, andit is defined as12???????
+ ????
,??????
/????
being the position of thefirst/last word of the F-span in the for-eign sentence.
????
is defined similarly.The rationale behind this feature is themonotonic assumption, i.e.
a phrase ofthe foreign sentence usually occupiesroughly the same position of the equiva-lent English phrase.
The feature value for3An inconsistent link connects a word within the phrase pairto some word outside the phrase pair.
C.f.
Deng et al (2008)the example is |(1+2)/(2*4)-(2+3)/(2*4)|=0.25.3) HMM-based probabilities.
Haghighi et al(2009) show that posterior probabilities fromthe HMM alignment model is useful forpruning.
Therefore, we design two new fea-tures by replacing the link count in link ratioand inconsistent link ratio with the sum of thelink?s posterior probability.6 The DITG ModelsThe discriminative ITG alignment can be con-ceived as a two-staged process.
In the first stageDPDI selects good span pairs.
In the second stagegood alignment hypotheses are assigned to thespan pairs selected by DPDI.
Two discriminativeITG (DITG) models are investigated.
One isword-to-word DITG (henceforth W-DITG),which observes the 1-to-1 constraint on align-ment.
Another is DITG with hierarchical phrasepairs (henceforth HP-DITG), which relaxes the 1-to-1 constraint by adopting hierarchical phrasepairs in Chiang (2007).Each model selects the best alignment hypo-theses of each span pair, given a set of features.The contributions of these features are integratedthrough a log linear model (similar to Liu et al,2005; Moore, 2005) like Equation (1).
The dis-criminative training of the feature weights isagain MERT (Och, 2003).
The MERT modulefor DITG takes alignment F-score of a sentencepair as the performance measure.
Given an inputsentence pair and the reference annotated align-ment, MERT aims to maximize the F-score ofDITG-produced alignment.
Like SMT (and un-like DPDI), it is the upper envelope which de-fines the intervals where the performance meas-ure changes.6.1 Word-to-word DITGThe following features about alignment link areused in W-DITG:1) Word pair translation probabilities trainedfrom HMM model (Vogel, et.al., 1996)and IBM model 4 (Brown et.al., 1993;Och and Ney, 2000).2) Conditional link probability (Moore, 2005).3) Association score rank features (Moore etal., 2006).4) Distortion features: counts of inversionand concatenation.5) Difference between the relative positionsof the words.
The relative position of aword in a sentence is defined as the posi-320tion of the word divided by sentencelength.6) Boolean features like whether a word inthe word pair is a stop word.6.2 DITG with Hierarchical Phrase PairsThe 1-to-1 assumption in ITG is a serious limita-tion as in reality there are always segmentation ortokenization errors as well as idiomatic expres-sions.
Wu (1997) proposes a bilingual segmenta-tion grammar extending the terminal rules byincluding phrase pairs.
Cherry and Lin (2007)incorporate phrase pairs in phrase-based SMTinto ITG, and Haghighi et al (2009) introduceBlock ITG (BITG), which adds 1-to-many ormany-to-1 terminal unary rules.It is interesting to see if DPDI can benefit theparsing of a more realistic ITG.
HP-DITG ex-tends Cherry and Lin?s approach by not only em-ploying simple phrase pairs but also hierarchicalphrase pairs (Chiang, 2007).
The grammar isenriched with rules of the format: ??
?
?/?
?where ?
?
and ?
?
refer to the English and foreignside of the i-th (simple/hierarchical) phrase pairrespectively.As example, if there is a simple phrase pair??
?????
?????,?
??
, then it is trans-formed into the ITG rule ??
"North Korea"/"?
??".
During parsing, each span pair doesnot only examine all possible combinations ofsub-span pairs using binary rules, but also checksif the yield of that span pair is exactly the same asthat phrase pair.
If so, then the alignment linkswithin the phrase pair (which are obtained instandard phrase pair extraction procedure) aretaken as an alternative alignment hypothesis ofthat span pair.For a hierarchical phrase pair like??
?1 ??
?2 ,?2 ?
?1 , it is transformed intothe ITG rule  ??
"?1 ??
?2"/"?2 ?
?1"  duringparsing, each span pair checks if it contains thelexical anchors "of" and "?
", and if the remain-ing words in its yield can form two sub-spanpairs which fit the reordering constraint among?1 and ?2.
(Note that span pairs of any categoryin the ITG normal form grammar can substitutefor ?1 or ?2 .)
If both conditions hold, then thespan pair is assigned an alignment hypothesiswhich combines the alignment links among thelexical anchors (????
??/?)
and those linksamong the sub-span pairs.HP-ITG acquires the rules from HMM-basedword-aligned corpus using standard phrase pairextraction as stated in Chiang (2007).
The ruleprobabilities and lexical weights in both English-to-foreign and foreign-to-English directions areestimated and taken as features, in addition tothose features in W-DITG, in the discriminativemodel of alignment hypothesis selection.7 EvaluationDPDI is evaluated against the baselines of Tic-tac-toe (TTT) pruning (Zhang and Gildea, 2005)and Dynamic Program (DP) pruning (Haghighi etal., 2009; DeNero et al, 2009) with respect toChinese-to-English alignment and translation.Based on DPDI, HP-DITG is evaluated againstthe alignment systems GIZA++ and BITG.7.1 Evaluation CriteriaFour evaluation criteria are used in addition tothe time spent on ITG parsing.
We will first eva-luate pruning regarding the pruning decisionsthemselves.
That is, the first evaluation metric,pruning error rate (henceforth PER), measureshow many correct E-spans are discarded.
Themajor drawback of PER is that not all decisionsin pruning would impact on alignment quality,since certain F-spans are of little use to the entireITG parse tree.An alternative criterion is the upper bound onalignment F-score, which essentially measureshow many links in annotated alignment can bekept in ITG parse.
The calculation of F-score up-per bound is done in a bottom-up way like ITGparsing.
All leaf hypernodes which contain a cor-rect link are assigned a score (known as hit) of 1.The hit of a non-leaf hypernode is based on thesum of hits of its daughter hypernodes.
The max-imal sum among all hyperedges of a hypernode isassigned to that hypernode.
Formally,???
?
?
, ?
=????,?,?
1 ,?
1 ,?
2 ,?
2(???
?
?
1, ?
1  + ???[?
2, ?
2])???
??
?, ?
=1      ??
?, ?
?
?0        ????????????
??
= 0;???
??
= 0where ?,?,?
are variables for the categories inITG grammar, and ?
comprises the golden linksin annotated alignment.
??
, ??
, ??
are defined inAppendix A.Figure 6 illustrates the calculation of the hitscore for the example in Section 5.1/Figure 2.The upper bound of recall is the hit score dividedby the total number of golden links.
The upper321ID pruning beam size pruning/total time cost PER F-UB F-score1 DPDI 10 72??/3?03??
4.9% 88.5% 82.5%2 TTT 10 58??/2?38??
8.6% 87.5% 81.1%3 TTT 20 53??/6?55??
5.2% 88.6% 82.4%4 DP -- 11??/6?01??
12.1% 86.1% 80.5%Table 1: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for W-DITGID pruning beam size pruning/total time cost PER F-UB F-score1 DPDI 10 72??/5?18??
4.9% 93.9% 87.0%2 TTT 10 58??/4?51??
8.6% 93.0% 84.8%3 TTT 20 53??/12?5??
5.2% 94.0% 86.5%4 DP -- 11??/15?39??
12.1% 91.4% 83.6%Table 2: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for HP-DITG.bound of precision, which should be defined asthe hit score divided by the number of links pro-duced by the system, is almost always 1.0 inpractice.
The upper bound of alignment F-scorecan thus be calculated as well.A?
[C,C]Cw:[e1,e1]/[f1,f1]hit=1Ce:[e1]/?Cw:[e2,e2]/[f1,f1]Ce:[e2]/?Cw:[e3,e3]/[f2,f2]C:[e1,e2]/[f1,f1]hit=max{0+ }=1C:[e2,e3]/[f2,f2]hit=max{0+1}=1A:[e1,e3]/[f1,f2]hit=max{1+1,1+1}=2C?
[Ce,Cw]A?[C,C]C?
[Ce,Cw]hit=1 hit=1hit=0 hit=0Figure 6: Recall Upper Bound CalculationFinally, we also do end-to-end evaluation us-ing both F-score in alignment and Bleu score intranslation.
We use our implementation of hierar-chical phrase-based SMT (Chiang, 2007), withstandard features, for the SMT experiments.7.2 Experiment DataBoth discriminative pruning and alignment needtraining data and test data.
We use the manuallyaligned Chinese-English dataset as used in Hag-highi et al (2009).
The 491 sentence pairs in thisdataset are adapted to our own Chinese wordsegmentation standard.
250 sentence pairs areused as training data and the other 241 are testdata.
The corresponding numbers of F-spans intraining and test data are 4590 and 3951 respec-tively.In SMT experiments, the bilingual training da-taset is the NIST training set excluding the HongKong Law and Hong Kong Hansard, and our 5-gram language model is trained from the Xinhuasection of the Gigaword corpus.
The NIST?03test set is used as our development corpus and theNIST?05 and NIST?08 test sets are our test sets.7.3 Small-scale EvaluationThe first set of experiments evaluates the perfor-mance of the three pruning methods using thesmall 241-sentence set.
Each pruning method isplugged in both W-DITG and HP-DITG.
IBMModel 1 and HMM alignment model are re-implemented as they are required by the threeITG pruning methods.The results for W-DITG are listed in Table 1.Tests 1 and 2 show that with the same beam size(i.e.
number of E-spans per F-span), althoughDPDI spends a bit more time (due to the morecomplicated model), DPDI makes far less incor-rect pruning decisions than the TTT.
In terms ofF-score upper bound, DPDI is 1 percent higher.DPDI achieves even larger improvement in ac-tual F-score.To enable TTT achieving similar F-score or F-score upper bound, the beam size has to bedoubled and the time cost is more than twice theoriginal (c.f.
Tests 1 and 3 in Table 1) .The DP pruning in Haghighi et.al.
(2009) per-forms much poorer than the other two pruningmethods.
In fact, we fail to enable DP achieve thesame F-score upper bound as the other two me-thods before DP leads to intolerable memoryconsumption.
This may be due to the use of dif-ferent HMM model implementations between ourwork and Haghighi et.al.
(2009).Table 2 lists the results for HP-DITG.
Roughlythe same observation as in W-DITG can be made.In addition to the superiority of DPDI, it can alsobe noted that HP-DITG achieves much higher F-score and F-score upper bound.
This shows that322hierarchical phrase is a powerful tool in rectify-ing the 1-to-1 constraint in ITG.Note also that while TTT in Test 3 gets rough-ly the same F-score upper bound as DPDI in Test1, the corresponding F-score is slightly worse.
Apossible explanation is that better pruning notonly speeds up the parsing/alignment process butalso guides the search process to focus on themost promising region of the search space.7.4 Large-scale End-to-End ExperimentID Prun-ingbeamsizetimecostBleu-05Bleu-081 DPDI 10 1092h 38.57 28.312 TTT 10 972h 37.96 27.373 TTT 20 2376h 38.13 27.584 DP -- 2068h 37.43 27.12Table 3:  Evaluation of DPDI against TTT andDP for HP-DITGID WA-ModelF-Score Bleu-05 Bleu-081 HMM 80.1% 36.91 26.862 Giza++ 84.2% 37.70 27.333 BITG 85.9% 37.92 27.854 HP-DITG 87.0% 38.57 28.31Table 4:  Evaluation of DPDI against HMM, Gi-za++ and BITGTable 3 lists the word alignment time cost andSMT performance of different pruning methods.HP-DITG using DPDI achieves the best Bleuscore with acceptable time cost.
Table 4 com-pares HP-DITG to HMM (Vogel, et al, 1996),GIZA++ (Och and Ney, 2000) and BITG (Hag-highi et al, 2009).
It shows that HP-DITG (withDPDI) is better than the three baselines both inalignment F-score and Bleu score.
Note that theBleu score differences between HP-DITG and thethree baselines are statistically significant (Koehn,2004).An explanation of the better performance byHP-DITG is the better phrase pair extraction dueto DPDI.
On the one hand, a good phrase pairoften fails to be extracted due to a link inconsis-tent with the pair.
On the other hand, ITG prun-ing can be considered as phrase pair selection,and good ITG pruning like DPDI guides the sub-sequent ITG alignment process so that less linksinconsistent to good phrase pairs are produced.This also explains (in Tables 2 and 3) why DPDIwith beam size 10 leads to higher Bleu than TTTwith beam size 20, even though both pruning me-thods lead to roughly the same alignment F-score.8 Conclusion and Future WorkThis paper reviews word alignment through ITGparsing, and clarifies the problem of ITG pruning.A discriminative pruning model and two discri-minative ITG alignments systems are proposed.The pruning model is shown to be superior to allexisting ITG pruning methods, and the HP-DITGalignment system is shown to improve state-of-the-art alignment and translation quality.The current DPDI model employs a very li-mited set of features.
Many features are relatedonly to probabilities of word pairs.
As the successof HP-DITG illustrates the merit of hierarchicalphrase pair, in future we should investigate morefeatures on the relationship between span pairand hierarchical phrase pair.Appendix A.
The Normal Form GrammarTable 5 lists the ITG rules in normal form asused in this paper, which extend the normal formin Wu (1997) so as to handle the case of align-ment to null.1  ?
?
?|?|?2  ?
?
?
?
| ?
?
| ?
?
| ??
| ?
?
| ?
?3  ?
?
?
?
| ?
?
| ?
?
| ?
??
?
?
?
| ?
?4  ?
?
??
|???
|??
?5  ?
?
???
??
?6 ??
?
?/?7 ??
?
?/?;??
?
?/?8 ???
?
?
?| ???
??
;???
?
??
| ???
?
?9 ???
?
???
??
;???
?
???
?
?Table 5: ITG Rules in Normal FormIn these rules, ?
is the Start symbol; ?
is thecategory for concatenating combination whereas?
for inverted combination.
Rules (2) and (3) areinherited from Wu (1997).
Rules (4) divide theterminal category ?
into subcategories.
Ruleschema (6) subsumes all terminal unary rules forsome English word ?
and foreign word ?
, andrule schemas (7) are unary rules for alignment tonull.
Rules (8) ensure all words linked to null arecombined in left branching manner, while rules(9) ensure those words linked to null combinewith some following, rather than preceding, wordpair.
(Note: Accordingly, all sentences must beended by a special token  ???
, otherwise thelast word(s) of a sentence cannot be linked tonull.)
If there are both English and foreign wordslinked to null, rule (5) ensures that those English323words linked to null precede those foreign wordslinked to null.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Peitra, Robert L. Mercer.
1993.
The Mathe-matics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics,19(2):263-311.Colin Cherry and Dekang Lin.
2006.
Soft SyntacticConstraints for Word Alignment through Dis-criminative Training.
In Proceedings of ACL-COLING.Colin Cherry and Dekang Lin.
2007.
InversionTransduction Grammar for Joint PhrasalTranslation Modeling.
In Proceedings of SSST,NAACL-HLT, Pages:17-24.David Chiang.
2007.
Hierarchical Phrase-basedTranslation.
Computational Linguistics, 33(2).John DeNero, Mohit Bansal, Adam Pauls, and DanKlein.
2009.
Efficient Parsing for TransducerGrammars.
In Proceedings of NAACL, Pag-es:227-235.Alexander Fraser and Daniel Marcu.
2006.
Semi-Supervised Training for StatisticalWordAlignment.
In Proceedings of ACL, Pages:769-776.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better Word Alignments with Su-pervised ITG Models.
In Proceedings of ACL,Pages: 923-931.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In Proceedings of IWPT 2005, Pag-es:173-180.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings ofACL.
Pages: 440-447Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Pro-ceedings of ACL,  Pages:160-167.Dan Klein and Christopher D. Manning.
2001.
Pars-ing and Hypergraphs.
In Proceedings of IWPT,Pages:17-19Philipp Koehn.
2004.
Statistical Significance Testsfor Machine Translation Evaluation.
In Pro-ceedings of EMNLP,  Pages: 388-395.Yang Liu, Qun Liu and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceed-ings of ACL, Pages: 81-88.Robert Moore.
2005.
A Discriminative Frameworkfor Bilingual Word Alignment.
In Proceedings ofEMNLP 2005, Pages: 81-88.Robert Moore, Wen-tau Yih, and Andreas Bode.
2006.Improved Discriminative Bilingual WordAlignment.
In Proceedings of ACL, Pages: 513-520.Stephan Vogel, Hermann Ney, and Christoph Till-mann.
1996.
HMM-based word alignment instatistical translation.
In Proceedings of COL-ING, Pages: 836-841.Stephan Vogel.
2005.
PESA: Phrase Pair Extrac-tion as Sentence Splitting.
In Proceedings of MTSummit.Dekai Wu.
1997.
Stochastic Inversion Transduc-tion Grammars and Bilingual Parsing of Pa-rallel Corpora.
Computational Linguistics, 23(3).Hao Zhang and Daniel Gildea.
2005.
Stochastic Lex-icalized Inversion Transduction Grammar forAlignment.
In Proceedings of ACL.Hao Zhang, Chris Quirk, Robert Moore, and DanielGildea.
2008.
Bayesian learning of non-compositional phrases with synchronous pars-ing.
In Proceedings of ACL, Pages: 314-323.324
