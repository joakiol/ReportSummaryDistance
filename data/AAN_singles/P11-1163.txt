Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1626?1635,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsEvent Extraction as Dependency ParsingDavid McClosky, Mihai Surdeanu, and Christopher D. ManningDepartment of Computer ScienceStanford UniversityStanford, CA 94305{mcclosky,mihais,manning}@stanford.eduAbstractNested event structures are a common occur-rence in both open domain and domain spe-cific extraction tasks, e.g., a ?crime?
eventcan cause a ?investigation?
event, which canlead to an ?arrest?
event.
However, most cur-rent approaches address event extraction withhighly local models that extract each event andargument independently.
We propose a simpleapproach for the extraction of such structuresby taking the tree of event-argument relationsand using it directly as the representation in areranking dependency parser.
This provides asimple framework that captures global prop-erties of both nested and flat event structures.We explore a rich feature space that modelsboth the events to be parsed and context fromthe original supporting text.
Our approach ob-tains competitive results in the extraction ofbiomedical events from the BioNLP?09 sharedtask with a F1 score of 53.5% in developmentand 48.6% in testing.1 IntroductionEvent structures in open domain texts are frequentlyhighly complex and nested: a ?crime?
event cancause an ?investigation?
event, which can lead to an?arrest?
event (Chambers and Jurafsky, 2009).
Thesame observation holds in specific domains.
For ex-ample, the BioNLP?09 shared task (Kim et al, 2009)focuses on the extraction of nested biomolecularevents, where, e.g., a REGULATION event causes aTRANSCRIPTION event (see Figure 1a for a detailedexample).
Despite this observation, many state-of-the-art supervised event extraction models stillextract events and event arguments independently,ignoring their underlying structure (Bjo?rne et al,2009; Miwa et al, 2010b).In this paper, we propose a new approach for su-pervised event extraction where we take the tree ofrelations and their arguments and use it directly asthe representation in a dependency parser (ratherthan conventional syntactic relations).
Our approachis conceptually simple: we first convert the origi-nal representation of events and their arguments todependency trees by creating dependency arcs be-tween event anchors (phrases that anchor events inthe supporting text) and their corresponding argu-ments.1 Note that after conversion, only event an-chors and entities remain.
Figure 1 shows a sentenceand its converted form from the biomedical do-main with four events: two POSITIVE REGULATIONevents, anchored by the phrase ?acts as a costim-ulatory signal,?
and two TRANSCRIPTION events,both anchored on ?gene transcription.?
All eventstake either protein entity mentions (PROT) or otherevents as arguments.
The latter is what allows fornested event structures.
Existing dependency pars-ing models can be adapted to produce these seman-tic structures instead of syntactic dependencies.
Webuilt a global reranking parser model using multipledecoders from MSTParser (McDonald et al, 2005;McDonald et al, 2005b).
The main contributions ofthis paper are the following:1.
We demonstrate that parsing is an attractive ap-proach for extracting events, both nested andotherwise.1While our approach only works on trees, we show how wecan handle directed acyclic graphs in Section 5.1626(a) Original sentence with nested events (b) After conversion to event dependenciesFigure 1: Nested events in the text fragment: ?.
.
.
the HTLV-1 transactivator protein, tax, acts as a costim-ulatory signal for GM-CSF and IL-2 gene transcription .
.
.
?
Throughout this paper, bold text indicatesinstances of event anchors and italicized text denotes entities (PROTEINs in the BioNLP?09 domain).
Notethat in (a) there are two copies of each type of event, which are merged to single nodes in the dependencytree (Section 3.1).2.
We propose a wide range of features for eventextraction.
Our analysis indicates that fea-tures which model the global event structureyield considerable performance improvements,which proves that modeling event structurejointly is beneficial.3.
We evaluate on the biomolecular event corpusfrom the the BioNLP?09 shared task and showthat our approach obtains competitive results.2 Related WorkThe pioneering work of Miller et al (1997) wasthe first, to our knowledge, to propose parsing asa framework for information extraction.
They ex-tended the syntactic annotations of the Penn Tree-bank corpus (Marcus et al, 1993) with entity andrelation mentions specific to the MUC-7 evalua-tion (Chinchor et al, 1997) ?
e.g., EMPLOYEE OFrelations that hold between person and organizationnamed entities ?
and then trained a generative pars-ing model over this combined syntactic and seman-tic representation.
In the same spirit, Finkel andManning (2009) merged the syntactic annotationsand the named entity annotations of the OntoNotescorpus (Hovy et al, 2006) and trained a discrimina-tive parsing model for the joint problem of syntac-tic parsing and named entity recognition.
However,both these works require a unified annotation of syn-tactic and semantic elements, which is not alwaysfeasible, and focused only on named entities and bi-nary relations.
On the other hand, our approach fo-cuses on event structures that are nested and havean arbitrary number of arguments.
We do not needa unified syntactic and semantic representation (butwe can and do extract features from the underlyingsyntactic structure of the text).Finkel and Manning (2009b) also proposed aparsing model for the extraction of nested named en-tity mentions, which, like this work, parses just thecorresponding semantic annotations.
In this work,we focus on more complex structures (events insteadof named entities) and we explore more global fea-tures through our reranking layer.In the biomedical domain, two recent papers pro-posed joint models for event extraction based onMarkov logic networks (MLN) (Riedel et al, 2009;Poon and Vanderwende, 2010).
Both works proposeelegant frameworks where event anchors and argu-ments are jointly predicted for all events in the samesentence.
One disadvantage of MLN models is therequirement that a human expert develop domain-specific predicates and formulas, which can be acumbersome process because it requires thoroughdomain understanding.
On the other hand, our ap-proach maintains the joint modeling advantage, butour model is built over simple, domain-independentfeatures.
We also propose and analyze a richer fea-ture space that captures more information on theglobal event structure in a sentence.
Furthermore,since our approach is agnostic to the parsing modelused, it could easily be tuned for various scenarios,e.g., models with lower inference overhead such asshift-reduce parsers.Our work is conceptually close to the recentCoNLL shared tasks on semantic role labeling,where the predicate frames were converted to se-1627Events?to??Dependencies?Parser?1?
???Reranker?Dependencies??to?Events?Parser?k?Dependencies??to?Events?Event?
?Trigger?Recognizer?En?ty?Recognizer?Figure 2: Overview of the approach.
Rounded rect-angles indicate domain-independent components;regular rectangles mark domain-specific modules;blocks in dashed lines surround components not nec-essary for the domain presented in this paper.mantic dependencies between predicates and theirarguments (Surdeanu et al, 2008; Hajic et al, 2009).In this representation the dependency structure is adirected acyclic graph (DAG), i.e., the same nodecan be an argument to multiple predicates, and thereare no explicit dependencies between predicates.Due to this representation, all joint models proposedfor semantic role labeling handle semantic framesindependently.3 ApproachFigure 2 summarizes our architecture.
Our approachconverts the original event representation to depen-dency trees containing both event anchors and entitymentions, and trains a battery of parsers to recognizethese structures.
The trees are built using event an-chors predicted by a separate classifier.
In this work,we do not discuss entity recognition because inthe BioNLP?09 domain used for evaluation entities(PROTEINs) are given (but including entity recog-nition is an obvious extension of our model).
Ourparsers are several instances of MSTParser2 (Mc-Donald et al, 2005; McDonald et al, 2005b) con-figured with different decoders.
However, our ap-proach is agnostic to the actual parsing models usedand could easily be adapted to other dependencyparsers.
The output from the reranking parser is2http://sourceforge.net/projects/mstparser/converted back to the original event representationand passed to a reranker component (Collins, 2000;Charniak and Johnson, 2005), tailored to optimizethe task-specific evaluation metric.Note that although we use the biomedical eventdomain from the BioNLP?09 shared task to illustrateour work, the core of our approach is almost do-main independent.
Our only constraints are that eachevent mention be activated by a phrase that serves asan event anchor, and that the event-argument struc-tures be mapped to a dependency tree.
The conver-sion between event and dependency structures andthe reranker metric are the only domain dependentcomponents in our approach.3.1 Converting between Event Structures andDependenciesAs in previous work, we extract event structures atsentence granularity, i.e., we ignore events whichspan sentences (Bjo?rne et al, 2009; Riedel et al,2009; Poon and Vanderwende, 2010).
These formapproximately 5% of the events in the BioNLP?09corpus.
For each sentence, we convert theBioNLP?09 event representation to a graph (repre-senting a labeled dependency tree) as follows.
Thenodes in the graph are protein entity mentions, eventanchors, and a virtual ROOT node.
Thus, the onlywords in this dependency tree are those which par-ticipate in events.
We create edges in the graph inthe following way.
For each event anchor, we cre-ate one link to each of its arguments labeled with theslot name of the argument (for example, connectinggene transcription to IL-2 with the label THEME inFigure 1b).
We link the ROOT node to each entitythat does not participate in an event using the ROOT-LABEL dependency label.
Finally, we link the ROOTnode to each top-level event anchor, (those which donot serve as arguments to other events) again usingthe ROOT-LABEL label.
We follow the conventionthat the source of each dependency arc is the headwhile the target is the modifier.The output of this process is a directed graph,since a phrase can easily play a role in two or moreevents.
Furthermore, the graph may contain self-referential edges (self-loops) due to related eventssharing the same anchor (example below).
To guar-antee that the output of this process is a tree, wemust post-process the above graph with the follow-1628ing three heuristics:Step 1: We remove self-referential edges.
An exam-ple of these can be seen in the text ?the domain in-teracted preferentially with underphosphorylatedTRAF2,?
there are two events anchored by the sameunderphosphorylated phrase, a NEGATIVE REGU-LATION and a PHOSPHORYLATION event, and thelatter serves as a THEME argument for the former.Due to the shared anchor, our conversion compo-nent creates an self-referential THEME dependency.By removing these edges, 1.5% of the events in thetraining arguments are left without arguments, so weremove them as well.Step 2: We break structures where one argument par-ticipates in multiple events, by keeping only the de-pendency to the event that appears first in text.
Forexample, in the fragment ?by enhancing its inactiva-tion through binding to soluble TNF-alpha receptortype II,?
the protein TNF-alpha receptor type II isan argument in both a BINDING event (binding) andin a NEGATIVE REGULATION event (inactivation).As a consequence of this step, 4.7% of the events intraining are removed.Step 3: We unify events with the same types an-chored on the same anchor phrase.
For example,for the fragment ?Surface expression of intercellu-lar adhesion molecule-1, P-selectin, and E-selectin,?the BioNLP?09 annotation contains three distinctGENE EXPRESSION events anchored on the samephrase (expression), each having one of the proteinsas THEMEs.
In such cases, we migrate all argumentsto one of the events, and remove the empty events.21.5% of the events in training are removed in thisstep (but no dependencies are lost).Note that we do not guarantee that the resultingtree is projective.
In fact, our trees are more likelyto be non-projective than syntactic dependency treesof English sentences, because in our representationmany nodes can be linked directly to the ROOT node.Our analysis indicates that 2.9% of the dependenciesgenerated in the training corpus are non-projectiveand 7.9% of the sentences contain at least one non-projective dependency (for comparison, these num-bers for the English Penn Treebank are 0.3% and6.7%, respectively).After parsing, we implement the inverse process,i.e., we convert the generated dependency trees tothe BioNLP?09 representation.
In addition to theobvious conversions, this process implements theheuristics proposed by Bjo?rne et al (2009), whichreverse step 3 above, e.g., we duplicate GENE EX-PRESSION events with multiple THEME arguments.The heuristics are executed sequentially in the givenorder:1.
Since all non-BINDING events can have atmost one THEME argument, we duplicate non-BINDING events with multiple THEME argu-ments by creating one separate event for eachTHEME.2.
Similarly, since REGULATION events acceptsonly one CAUSE argument, we duplicate REG-ULATION events with multiple CAUSE argu-ments, obtaining one event per CAUSE.3.
Lastly, we implement the heuristic of Bjo?rne etal.
(2009) to handle the splitting of BINDINGevents with multiple THEME arguments.
This ismore complex because these events can acceptone or more THEMEs.
In such situations, wefirst group THEME arguments by the label of thefirst Stanford dependency (Marneffe and Man-ning, 2008) from the head word of the anchorto this argument.
Then we create one event foreach combination of THEME arguments in dif-ferent groups.3.2 Recognition of Event AnchorsFor anchor detection, we used a multiclass classifierthat labels each token independently.3 Since over92% of the anchor phrases in our evaluation domaincontain a single word, we simplify the task by re-ducing all multi-word anchor phrases in the trainingcorpus to their syntactic head word (e.g., ?acts?
forthe anchor ?acts as a costimulatory signal?
).We implemented this model using a logistic re-gression classifier with L2 regularization over thefollowing features:3We experimented with using conditional random fields as asequence labeler but did not see improvements in the biomed-ical domain.
We hypothesize that the sequence tagger fails tocapture potential dependencies between anchor labels ?
whichare its main advantage over an i.i.d.
classifier ?
because anchorwords are typically far apart in text.
This result is consistentwith observations in previous work (Bjo?rne et al, 2009).1629?
Token-level: The form, lemma, and whetherthe token is present in a gazetteer of known an-chor words.4?
Surface context: The above token features ex-tracted from a context of two words around thecurrent token.
Additionally, we build token bi-grams in this context window, and model themwith similar features.?
Syntactic context: We model all syntactic de-pendency paths up to depth two starting fromthe token to be classified.
These paths are builtfrom Stanford syntactic dependencies (Marn-effe and Manning, 2008).
We extract tokenfeatures from the first and last token in thesepaths.
We also generate combination featuresby concatenating: (a) the last token in each pathwith the sequence of dependency labels alongthe corresponding path; and (b) the word to beclassified, the last token in each path, and thesequence of dependency labels in that path.?
Bag-of-word and entity count: Extractedfrom (a) the entire sentence, and (b) a windowof five words around the token to be classified.3.3 Parsing Event StructuresGiven the entities and event anchors from the pre-vious stages in the pipeline, the parser generates la-beled dependency links between them.
Many de-pendency parsers are available and we chose MST-Parser for its ability to produce non-projective andn-best parses directly.
MSTParser frames parsingas a graph algorithm.
To parse a sentence, MST-Parser finds the tree covering all the words (nodes)in the sentence (graph) with the largest sum of edgeweights, i.e., the maximum weighted spanning tree.Each labeled, directed edge in the graph represents apossible dependency between its two endpoints andhas an associated score (weight).
Scores for edgescome from the dot product between the edge?s corre-sponding feature vector and learned feature weights.As a result, all features for MSTParser must be edge-factored, i.e., functions of both endpoints and the la-bel connecting them.
McDonald et al (2006) ex-tends the basic model to include second-order de-pendencies (i.e., two adjacent sibling nodes and their4These are automatically extracted from the training corpus.parent).
Both first and second-order modes includeprojective and non-projective decoders.Our features for MSTParser use both the eventstructures themselves as well as the surroundingEnglish sentences which include them.
By map-ping event anchors and entities back to the originaltext, we can incorporate information from the orig-inal English sentence as well its syntactic tree andcorresponding Stanford dependencies.
Both formsof context are valuable and complementary.
MST-Parser comes with a large number of features which,in our setup, operate on the event structure level(since this is the ?sentence?
from the parser?s pointof view).
The majority of additional features thatwe introduced take advantage of the original text ascontext (primarily its associated Stanford dependen-cies).
Our system includes the following first-orderfeatures:?
Path: Syntactic paths in the original sentencebetween nodes in an event dependency (as inprevious work by Bjo?rne et al (2009)).
Thesehave many variations including using Stanforddependencies (?collapsed?
and ?uncollapsed?
)or constituency trees as sources, optionally lex-icalizing the path, and using words or relationnames along the path.
Additionally, we includethe bucketed length of the paths.?
Original sentence words: Words from the fullEnglish sentence surrounding and between thenodes in event dependencies, and their buck-eted distances.
This additional context helpscompensate for how our anchor detection pro-vides only the head word of each anchor, whichdoes not necessarily provide the full context forevent disambiguation.?
Graph: Parents, children, and siblings ofnodes in the Stanford dependencies graphalong with label of the edge.
This provides ad-ditional syntactic context.?
Consistency: Soft constraints on edges be-tween anchors and their arguments (e.g., onlyregulation events can have edges labeled withCAUSE).
These features fire if their constraintsare violated.?
Ontology: Generalized types of the end-points of edges using a given type hierar-chy (e.g., POSITIVE REGULATION is a COM-1630PLEX EVENT5 is an EVENT).
Values ofthis feature are coded with the types of eachof the endpoints on an edge, running overthe cross-product of types for each endpoint.For instance, an edge between a BINDINGevent anchor and a POSITIVE REGULATIONcould cause this feature to fire with the val-ues [head:EVENT, child:COMPLEX EVENT] or[head:SIMPLE EVENT, child:EVENT].6 The lat-ter feature can capture generalizations such as?simple event anchors cannot take other eventsas arguments.
?Both Consistency and Ontology feature classes in-clude domain-specific information but can be usedon other domains under different constraints andtype hierarchies.
When using second-order de-pendencies, we use additional Path and Ontol-ogy features.
We include the syntactic paths be-tween sibling nodes (adjacent arguments of the sameevent anchor).
These Path features are as abovebut differentiated as paths between sibling nodes.The second-order Ontology features use the typehierarchy information on both sibling nodes andtheir parent.
For example, a POSITIVE REGULA-TION anchor attached to a PROTEIN and a BINDINGevent would produce an Ontology feature with thevalue [parent:COMPLEX EVENT, child1:PROTEIN,child2:SIMPLE EVENT] (among several other possi-ble combinations).To prune the number of features used, we employa simple entropy-based measure.
Our intuition isthat good features should typically appear with onlyone edge label.7 Given all edges enumerated duringtraining and their gold labels, we obtain a distribu-tion over edge labels (df ) for each feature f .
Giventhis distribution and the frequency of a feature, wecan score the feature with the following:score(f) = ??
log2(freq(f))?H(df )The ?
parameter adjusts the relative weight of thetwo components.
The log frequency component fa-vors more frequent features while the entropy com-ponent favors features with low entropy in their edge5We define complex events are those which can accept otherevents are arguments.
Simple events can only take PROTEINs.6We omit listing the other two combinations.7Labels include ROOT-LABEL, THEME, CAUSE, and NULL.We assign the NULL label to edges which aren?t in the gold data.label distribution.
Features are pruned by acceptingall features with a score above a certain threshold.3.4 Reranking Event StructuresWhen decoding, the parser finds the highest scoringtree which incorporates global properties of the sen-tence.
However, its features are edge-factored andthus unable to take into account larger contexts.
Toincorporate arbitrary global features, we employ atwo-step reranking parser.
For the first step, we ex-tend our parser to output its n-best parses insteadof just its top scoring parse.
In the second step, adiscriminative reranker rescores each parse and re-orders the n-best list.
Rerankers have been success-fully used in syntactic parsing (Collins, 2000; Char-niak and Johnson, 2005; Huang, 2008) and semanticrole labeling (Toutanova et al, 2008).Rerankers provide additional advantages in ourcase due to the mismatch between the dependencystructures that the parser operates on and their cor-responding event structures.
We convert the out-put from the parser to event structures (Section 3.1)before including them in the reranker.
This al-lows the reranker to capture features over the ac-tual event structures rather than their original de-pendency trees which may contain extraneous por-tions.8 Furthermore, this lets the reranker optimizethe actual BioNLP F1 score.
The parser, on the otherhand, attempts to optimize the Labeled AttachmentScore (LAS) between the dependency trees and con-verted gold dependency trees.
LAS is approximatefor two reasons.
First, it is much more local thanthe BioNLP metric.9 Second, the converted gold de-pendency trees lose information that doesn?t transferto trees (specifically, that event structures are reallymulti-DAGs and not trees).We adapt the maximum entropy reranker fromCharniak and Johnson (2005) by creating a cus-tomized feature extractor for event structures ?
inall other ways, the reranker model is unchanged.
Weuse the following types of features in the reranker:?
Source: Score and rank of the parse from the8For instance, event anchors with no arguments could beproposed by the parser.
These event anchors are automaticallydropped by the conversion process.9As an example, getting an edge label between an anchorand its argument correct is unimportant if the anchor is missingother arguments.1631Unreranked RerankedDecoder(s) R P F1 R P F11P 65.6 76.7 70.7 68.0 77.6 72.52P 67.4 77.1 71.9 67.9 77.3 72.31N 67.5 76.7 71.8 ?
?
?2N 68.9 77.1 72.7 ?
?
?1P, 2P, 2N ?
?
?
68.5 78.2 73.1(a) Gold event anchorsUnreranked RerankedDecoder(s) R P F1 R P F11P 44.7 62.2 52.0 47.8 59.6 53.12P 45.9 61.8 52.7 48.4 57.5 52.51N 46.0 61.2 52.5 ?
?
?2N 38.6 66.6 48.8 ?
?
?1P, 2P, 2N ?
?
?
48.7 59.3 53.5(b) Predicted event anchorsTable 1: BioNLP recall, precision, and F1 scores of individual decoders and the best decoder combinationon development data with the impact of event anchor detection and reranking.
Decoder names include thefeatures order (1 or 2) followed by the projectivity (P = projective, N = non-projective).decoder; number of different decoders produc-ing the parse (when using multiple decoders).?
Event path: Path from each node in the eventtree up to the root.
Unlike the Path featuresin the parser, these paths are over event struc-tures, not the syntactic dependency graphs fromthe original English sentence.
Variations of theEvent path features include whether to includeword forms (e.g., ?binds?
), types (BINDING),and/or argument slot names (THEME).
We alsoinclude the path length as a feature.?
Event frames: Event anchors with all their ar-guments and argument slot names.?
Consistency: Similar to the parser Consis-tency features, but capable of capturing largerclasses of errors (e.g., incorrect number ortypes of arguments).
We include the number ofviolations from four different classes of errors.To improve performance and robustness, featuresare pruned as in Charniak and Johnson (2005): se-lected features must distinguish a parse with thehighest F1 score in a n-best list, from a parse with asuboptimal F1 score at least five times.Rerankers can also be used to perform modelcombination (Toutanova et al, 2008; Zhang et al,2009; Johnson and Ural, 2010).
While we use a sin-gle parsing model, it has multiple decoders.10 Whencombining multiple decoders, we concatenate theirn-best lists and extract the unique parses.10We only have n-best versions of the projective decoders.For the non-projective decoders, we use their 1-best parse.4 Experimental ResultsOur experiments use the BioNLP?09 shared taskcorpus (Kim et al, 2009) which includes 800biomedical abstracts (7,449 sentences, 8,597 events)for training and 150 abstracts (1,450 sentences,1,809 events) for development.
The test set includes260 abstracts, 2,447 sentences, and 3,182 events.Throughout our experiments, we report BioNLP F1scores with approximate span and recursive eventmatching (as described in the shared task definition).For preprocessing, we parsed all documents us-ing the self-trained biomedical McClosky-Charniak-Johnson reranking parser (McClosky, 2010).
Webias the anchor detector to favor recall, allowing theparser and reranker to determine which event an-chors will ultimately be used.
When performing n-best parsing, n = 50.
For parser feature pruning,?
= 0.001.Table 1a shows the performance of each of the de-coders when using gold event anchors.
In both caseswhere n-best decoding is available, the reranker im-proves performance over the 1-best parsers.
We alsopresent the results from a reranker trained from mul-tiple decoders which is our highest scoring model.11In Table 1b, we present the output for the predictedanchor scenario.
In the case of the 2P decoder,the reranker does not improve performance, thoughthe drop is minimal.
This is because the rerankerchose an unfortunate regularization constant duringcrossvalidation, most likely due to the small size ofthe training data.
In later experiments where more11Including the 1N decoder as well provided no gains, possi-bly because its outputs are mostly subsumed by the 2N decoder.1632data is available, the reranker consistently improvesaccuracy (McClosky et al, 2011).
As before, thereranker trained from multiple decoders outperformsunreranked models and reranked single decoders.All in all, our best model in Table 1a scores 1 F1point higher than the best system at the BioNLP?09shared task, and the best model in Table 1b performssimilarly to the best shared task system (Bjo?rne etal., 2009), which also scores 53.5% on development.We show the effects of each system componentin Table 2.
Note how our upper limit is 87.1%due to our conversion process, which enforces thetree constraint, drops events spanning sentences, andperforms approximate reconstruction of BINDINGevents.
Given that state-of-the-art systems on thistask currently perform in the 50-60% range, we arenot troubled by this number as it still allows forplenty of potential.12 Bjo?rne et al (2009) list 94.7%as the upper limit for their system.
Consideringthis relatively large difference, we find the resultsin the previous table very encouraging.
As in otherBioNLP?09 systems, our performance drops whenswitching from gold to predicted anchor informa-tion.
Our decrease is similar to the one seen inBjo?rne et al (2009).To show the potential of reranking, we provide or-acle reranker scores in Table 3.
An oracle rerankerpicks the highest scoring parse from the availableparses.
We limit the n-best lists to the top k parseswhere k ?
{1, 2, 10,All}.
For single decoders,?All?
uses the entire 50-best list.
For multiple de-coders, the n-best lists are concatenated together.The oracle score with multiple decoders and goldanchors is only 0.4% lower than our upper limit (seeTable 2).
This indicates that parses which could haveachieved that limit were nearly always present.
Im-proving the features in the reranker as well as theoriginal parsers will help us move closer to the limit.With predicated anchors, the oracle score is about13% lower but still shows significant potential.Our final results on the test set, broken down byclass, are shown in Table 4.
As with other systems,complex events (e.g., REGULATION) prove harderthan simple events.
To get a complex event cor-rect, one must correctly detect and parse all events in12Additionally, improvements such as document-level pars-ing and DAG parsing would eliminate the need for much of theapproximate and lossy portions of the conversion process.AD Parse RR Conv R P F1X X X 45.9 61.8 52.7X X X X 48.7 59.3 53.5G X X 68.9 77.1 72.7G X X X 68.5 78.2 73.1G G G X 81.6 93.4 87.1Table 2: Effect of each major component to the over-all performance in the development corpus.
Compo-nents shown: AD ?
event anchor detection; Parse?
best individual parsing model; RR ?
rerankingmultiple parsers; Conv ?
conversion between theevent and dependency representations.
?G?
indicatesthat gold data was used; ?X?
indicates that the actualcomponent was used.n-best parses consideredAnchors Decoder(s) 1 2 10 AllGold1P 70.7 76.6 84.0 85.72P 71.8 77.5 84.8 86.21P, 2P, 2N ?
?
?
86.7Predicted1P 52.0 60.3 69.9 72.52P 52.7 60.7 70.1 72.51P, 2P, 2N ?
?
?
73.4Table 3: Oracle reranker BioNLP F1 scores forour n-best decoders and their combinations beforereranking on the development corpus.the event subtree allowing small errors to have largeeffects.
Top systems on this task obtain F1 scoresof 52.0% at the shared task evaluation (Bjo?rne etal., 2009) and 56.3% post evaluation (Miwa et al,2010a).
However, both systems are tailored to thebiomedical domain (the latter uses multiple syntac-tic parsers), whereas our system has a design that isvirtually domain independent.5 DiscussionWe believe that the potential of our approach ishigher than what the current experiments show.
Forexample, the reranker can be used to combine notonly several parsers but also multiple anchor rec-ognizers.
This passes the anchor selection decisionto the reranker, which uses global information notavailable to the current anchor recognizer or parser.Furthermore, our approach can be adapted to parseevent structures in entire documents (instead of in-1633Event Class Count R P F1Gene Expression 722 68.6 75.8 72.0Transcription 137 42.3 51.3 46.4Protein Catabolism 14 64.3 75.0 69.2Phosphorylation 135 80.0 82.4 81.2Localization 174 44.8 78.8 57.1Binding 347 42.9 51.7 46.9Regulation 291 23.0 36.6 28.3Positive Regulation 983 28.4 42.5 34.0Negative Regulation 379 29.3 43.5 35.0Total 3,182 42.6 56.6 48.6Table 4: Results in the test set broken by event class;scores generated with the main official metric of ap-proximate span and recursive event matching.dividual sentences) by using a representation with aunique ROOT node for all event structures in a doc-ument.
This representation has the advantage thatit maintains cross-sentence events (which accountfor 5% of BioNLP?09 events), and it allows fordocument-level features that model discourse struc-ture.
We plan to explore these ideas in future work.One current limitation of the proposed model isthat it constrains event structures to map to trees.
Inthe BioNLP?09 corpus this leads to the removal ofalmost 5% of the events, which generate DAGs in-stead of trees.
Local event extraction models (Bjo?rneet al, 2009) do not have this limitation, becausetheir local decisions are blind to (and hence notlimited by) the global event structure.
However,our approach is agnostic to the actual parsing mod-els used, so we can easily incorporate models thatcan parse DAGs (Sagae and Tsujii, 2008).
Addi-tionally, we are free to incorporate any new tech-niques from dependency parsing.
Parsing usingdual-decomposition (Rush et al, 2010) seems espe-cially promising in this area.6 ConclusionIn this paper we proposed a simple approach for thejoint extraction of event structures: we convertedthe representation of events and their arguments todependency trees with arcs between event anchorsand event arguments, and used a reranking parser toparse these structures.
Despite the fact that our ap-proach has very little domain-specific engineering,we obtain competitive results.
Most importantly, weshowed that the joint modeling of event structures isbeneficial: our reranker outperforms parsing modelswithout reranking in five out of the six configura-tions investigated.AcknowledgmentsThe authors would like to thank Mark Johnson forhelpful discussions on the reranker component andthe BioNLP shared task organizers, Sampo Pyysaloand Jin-Dong Kim, for answering questions.
Wegratefully acknowledge the support of the DefenseAdvanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181.
Any opinions, findings, and conclusionor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe view of DARPA, AFRL, or the US government.ReferencesJari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Ex-tracting Complex Biological Events with Rich Graph-Based Feature Sets.
Proceedings of the Workshop onBioNLP: Shared Task.Nate Chambers and Dan Jurafsky.
2009.
UnsupervisedLearning of Narrative Schemas and their Participants.Proceedings of ACL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the 2005 Meeting of the As-sociation for Computational Linguistics (ACL), pages173?180Nancy Chinchor.
1997.
Overview of MUC-7.
Pro-ceedings of the Message Understanding Conference(MUC-7).Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Machine Learning: Pro-ceedings of the Seventeenth International Conference(ICML 2000), pages 175?182.Jenny R. Finkel and Christopher D. Manning.
2009.Joint Parsing and Named Entity Recognition.
Pro-ceedings of NAACL.Jenny R. Finkel and Christopher D. Manning.
2009b.Nested Named Entity Recognition.
Proceedings ofEMNLP.Jan Hajic?, Massimiliano Ciaramita, Richard Johansson,Daisuke Kawahara, Maria A. Marti, Lluis Marquez,Adam Meyers, Joakim Nivre, Sebastian Pado, JanStepanek, Pavel Stranak, Mihai Surdeanu, Nianwen1634Xue, and Yi Zhang.
2009.
The CoNLL-2009 SharedTask: Syntactic and Semantic Dependencies in Multi-ple Languages.
Proceedings of CoNLL.Eduard Hovy, Mitchell P. Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% Solution.
Proceedings of the NAACL-HLT.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594, Association for Com-putational Linguistics.Mark Johnson and Ahmet Engin Ural.
2010.
Rerank-ing the Berkeley and Brown Parsers.
Proceedings ofNAACL.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof the BioNLP?09 Shared Task on Event Extrac-tion.
Proceedings of the NAACL-HLT 2009 Work-shop on Natural Language Processing in Biomedicine(BioNLP?09).Mitchell P. Marcus, Beatrice Santorini, and Marry AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford Typed Hierarchies Represen-tation.
Proceedings of the COLING Workshop onCross-Framework and Cross-Domain Parser Evalua-tion.David McClosky.
2010.
Any Domain Parsing: Auto-matic Domain Adaptation for Natural Language Pars-ing.
PhD thesis, Department of Computer Science,Brown University.David McClosky, Mihai Surdeanu, and Christopher D.Manning.
2011.
Event extraction as dependency pars-ing in BioNLP 2011.
In BioNLP 2011 Shared Task(submitted).Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online Large-Margin Training of DependencyParsers.
Proceedings of ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective Dependency Pars-ing using Spanning Tree Algorithms.
Proceedings ofHLT/EMNLP.Ryan McDonald and Fernando Pereira.
2006.
OnlineLearning of Approximate Dependency Parsing Algo-rithms.
Proceedings of EACL.Scott Miller, Michael Crystal, Heidi Fox, LanceRamshaw, Richard Schwartz, Rebecca Stone, andRalph Weischedel.
1997.
BBN: Description of theSIFT System as Used for MUC-7.
Proceedings of theMessage Understanding Conference (MUC-7).Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, andJun?ichi Tsujii.
2010a.
A Comparative Study of Syn-tactic Parsers for Event Extraction.
Proceedings ofthe 2010 Workshop on Biomedical Natural LanguageProcessing.Makoto Miwa, Rune Saetre, Jin-Dong Kim, and Jun?ichiTsujii.
2010b.
Event Extraction with Complex EventClassification Using Rich Features.
Journal of Bioin-formatics and Computational Biology, 8 (1).Hoifung Poon and Lucy Vanderwende.
2010.
Joint Infer-ence for Knowledge Extraction from Biomedical Liter-ature.
Proceedings of NAACL.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A Markov Logic Approachto Bio-Molecular Event Extraction.
Proceedings of theWorkshop on BioNLP: Shared Task.Alexander M. Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
Proceedings of EMNLP.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-Reduce De-pendency DAG Parsing.
Proceedings of the COLING.Mihai Surdeanu, Richard Johansson, Adam Meyers,Lluis Marquez, and Joakim Nivre.
2008.
The CoNLL-2008 Shared Task on Joint Parsing of Syntactic andSemantic Dependencies.
Proceedings of CoNLL.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A Global Joint Model for SemanticRole Labeling.
Computational Linguistics 34(2).Zhang, H. and Zhang, M. and Tan, C.L.
and Li, H. 2009.K-best combination of syntactic parsers.
Proceedingsof Empirical Methods in Natural Language Process-ing.1635
