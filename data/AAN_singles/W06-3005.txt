Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 33?40,New York City, NY, USA.
June 2006. c?2006 Association for Computational LinguisticsA Data Driven Approach to Relevancy Recognition forContextual Question AnsweringFan Yang?OGI School of Science & EngineeringOregon Health & Science Universityfly@cslu.ogi.eduJunlan Feng and Giuseppe Di FabbrizioAT&T Labs - Research180 Park Avenue, Florham Park, NJ, 07932 - USAjunlan@research.att.com, pino@research.att.comAbstractContextual question answering (QA), inwhich users?
information needs are satis-fied through an interactive QA dialogue,has recently attracted more research atten-tion.
One challenge of engaging dialogueinto QA systems is to determine whethera question is relevant to the previous inter-action context.
We refer to this task as rel-evancy recognition.
In this paper we pro-pose a data driven approach for the taskof relevancy recognition and evaluate iton two data sets: the TREC data and theHandQA data.
The results show that weachieve better performance than a previ-ous rule-based algorithm.
A detailed eval-uation analysis is presented.1 IntroductionQuestion Answering (QA) is an interactivehuman-machine process that aims to respondto users?
natural language questions with exactanswers rather than a list of documents.
In thelast few years, QA has attracted broader researchattention from both the information retrieval(Voorhees, 2004) and the computational linguisticfields (http://www.clt.mq.edu.au/Events/Conferences/acl04qa/).
Publicly ac-cessible web-based QA systems, such asAskJeeves (http://www.ask.com/) and START(http://start.csail.mit.edu/), have scaled up?The work was done when the first author was visitingAT&T Labs - Research.this technology to open-domain solutions.
Moretask-oriented QA systems are deployed as virtualcustomer care agents addressing questions aboutspecific domains.
For instance, the AT&T AskAllier agent (http://www.allie.att.com/) isable to answer questions about the AT&T plansand services; and the Ikea ?Just Ask Anna!?
agent(http://www.ikea.com/ms/en US/) targets ques-tions pertaining the company?s catalog.
Most ofthese QA systems, however, are limited to answerquestions in isolation.
The reality is that users oftenask questions naturally as part of contextualizedinteraction.
For instance, a question ?How do Isubscribe to the AT&T CallVantager service??
islikely to be followed by other related questionslike ?How much will the basic plan cost??
andso on.
Furthermore, many questions that usersfrequently want answers for cannot be satisfied witha simple answer.
Some of them are too complicated,broad, narrow, or vague resulting that there isn?t asimple good answer or there are many good answercandidates, which entails a clarification procedureto constrain or relax the search.
In all these cases,a question answering system that is able to answercontextual questions is more favored.Contextual question answering as a research chal-lenge has been fostered by TREC (Text RetrievalConference) since 2001.
The TREC 2001 QA trackmade the first attempt to evaluate QA systems?
abil-ity of tracking context through a series of questions.The TREC 2004 re-introduced this task and orga-nized all questions into 64 series, with each seriesfocusing on a specific topic.
The earlier questionsin a series provide context for the on-going ques-tion.
However, in reality, QA systems will not be33informed about the boundaries between series in ad-vance.One challenge of engaging dialogue into QA sys-tems is to determine the boundaries between topics.For each question, the system would need to deter-mine whether the question begins a new topic or itis a follow-up question related to the current exist-ing topic.
We refer to this procedure as relevancyrecognition.
If a question is recognized as a follow-up question, the next step is to make use of contextinformation to interpret it and retrieve the answer.We refer to this procedure as context information fu-sion.
Relevancy recognition is similar to text seg-mentation (Hearst, 1994), but relevancy recognitionfocuses on the current question with the previoustext while text segmentation has the full text avail-able and is allowed to look ahead.De Boni and Manandhar (2005) developed a rule-based algorithm for relevancy recognition.
Theirrules were manually deduced by carefully analyzingthe TREC 2001 QA data.
For example, if a questionhas no verbs, it is a follow-up question.
This rule-based algorithm achieves 81% in accuracy when rec-ognizing the question relevance in the TREC 2001QA data set.
The disadvantage of this approach isthat it involves a good deal of human effort to re-search on a specific data set and summarize the rules.For a new corpus from a different domain, it is verylikely that one would have to go over the data set andmodify the rules, which is time and human-effortconsuming.
An alternative is to pursue a data drivenapproach to automatically learn the rules from a dataset.
In this paper, we describe our experiments ofusing supervised learning classification techniquesfor the task of relevancy recognition.
Experimentsshow that machine learning approach achieves betterrecognition accuracy and can also be easily appliedto a new domain.The organization of this paper is as follows.
InSection 2, we summarize De Boni and Manandhar?srule-based algorithm.
We present our learning ap-proach in Section 3.
We ran our experiments ontwo data sets, namely, the TREC QA data and theHandQA data, and give the results in Section 4.
Insection 5, we report our preliminary study on con-text information fusion.
We conclude this paper inSection 6.2 Rule-Based ApproachDe Boni and Manandhar (2005) observed the fol-lowing cues to recognize follow-up questions:?
Pronouns and possessive adjectives.
For exam-ple, if a question has a pronoun that does not re-fer to an entity in the same sentence, this ques-tion could be a follow-up question.?
Cue words, such as ?precisely?
and ?exactly?.?
Ellipsis.
For example, if a question is not syn-tactically complete, this question could be afollow-up question.?
Semantic Similarity.
For example, if a ques-tion bears certain semantic similarity to previ-ous questions, this question might be a follow-up question.De Boni and Manandhar (2005) proposed analgorithm of calculating the semantic similar-ity between the current question Q and a pre-vious question Q?.
Supposed Q consists of alist of words (w1, w2, ..., wn) and Q?
consistsof (w?1, w?2, ..., w?m):SentenceSimilarity(Q, Q?)
(1)=?1?j?n( max1?i?mWordSimilarity(wj , w?i))The value of WordSimilarity(w, w?)
is thesimilarity between two words, calculated fromWordNet (Fellbaum, 1998).
It returns a valuebetween 0 (w and w?
have no semantic rela-tions) and 1 (w and w?
are the same).Motivated by these observations, De Boni andManandhar (2005) proposed the rule-based algo-rithm for relevancy recognition given in Figure 1.This approach can be easily mapped into an hand-crafted decision tree.
According to the algorithm,a question follows the current existing topic if it (1)contains reference to other questions; or (2) containscontext-related cue words; or (3) contains no verbs;or (4) bears certain semantic similarity to previousquestions or answer.
Evaluated on the TREC 2001QA context track data, the recall of the algorithmis 90% for recognizing first questions and 78% forfollow-up questions; the precision is 56% and 76%respectively.
The overall accuracy is 81%.34Given the current question Qi and a sequence of history ques-tions Qi?n, ..., Qi?1:1.
If Qi has a pronoun or possessive adjective which hasno references in the current question, Qi is a follow-upquestion.2.
If Qi has cue words such as ?precisely?
or ?exactly?, Qiis a follow-up question.3.
If Qi does not contain any verbs, Qi is a follow-up ques-tion.4.
Otherwise, calculate the semantic similarity measure ofQi asSimilarityMeasure(Qi)= max1?j?nf(j) ?
SentenceSimilarity(Qi, Qi?j)Here f(j) is a decay function.
If the similarity measure ishigher than a certain threshold, Qi is a follow-up ques-tion.5.
Otherwise, if answer is available, calculate the semanticdistance between Qi and the immediately previous an-swer Ai?1: SentenceSimilarity(Qi, Ai?1).
If it ishigher than a certain threshold, Qi is a follow-up ques-tion that is related to the previous answer.6.
Otherwise, Qi begins a new topic.Figure 1: Rule-based Algorithm3 Data Driven Approach3.1 Decision Tree LearningAs a move away from heuristic rules, in this paper,we make an attempt towards the task of relevancyrecognition using machine learning techniques.
Weformulate it as a binary classification problem: aquestion either begins a new topic or follows thecurrent existing topic.
This classification task canbe approached with a number of learning algorithmssuch as support vector machines, Adaboost and arti-ficial neural networks.
In this paper, we present ourexperiments using Decision Tree.
A decision treeis a tree in which each internal node represents achoice between a number of alternatives, and eachleaf node represents a decision.
Learning a decisiontree is fairly straightforward.
It begins from the rootnode which consists of all the training data, growingthe tree top-down by recursively splitting each nodebased on maximum information gain until certaincriteria is met.
Although the idea is simple, decisiontree learning is often able to yield good results.3.2 Feature ExtractionInspired by De Boni and Manandhar?s (2005) work,we selected two categories of features: syntactic fea-tures and semantic features.
Syntactic features cap-ture whether a question has certain syntactic compo-nents, such as verbs or pronouns.
Semantic featurescharacterize the semantic similarity between the cur-rent question and previous questions.3.2.1 Syntactic FeaturesAs the first step, we tagged each question withpart-of-speech tags using GATE (Cunningham et al,2002), a software tool set for text engineering.
Wethen extracted the following binary syntactic fea-tures:PRONOUN: whether the question has a pronounor not.
A more useful feature would be to la-bel whether a pronoun refers to an entity in theprevious questions or in the current question.However, the performances of currently avail-able tools for anaphora resolution are quite lim-ited for our task.
The tools we tried, includ-ing GATE (Cunningham et al, 2002), Ling-Pipe (http://www.alias-i.com/lingpipe/)and JavaRAP (Qiu et al, 2004), tend to usethe nearest noun phrase as the referents for pro-nouns.
While in the TREC questions, pronounstend to refer to the topic words (focus).
As aresult, unsupervised anaphora resolution intro-duced more noise than useful information.ProperNoun: whether the question has a propernoun or not.NOUN: whether the question has a noun or not.VERB: whether the question has a verb or not.DefiniteNoun: if a question has a definite nounphrase that refers to an entity in previous ques-tions, the question is very likely to be a follow-up question.
However, considering the diffi-culty in automatically identifying definite nounphrases and their referents, we ended up not us-ing this feature in our training because it in factintroduced misleading information.3.3 Semantic FeaturesTo compute the semantic similarity between twoquestions, we modified De Boni and Manandhar?sformula with a further normalization by the lengthof the questions; see formula (2).35SentenceSimilarity(Q, Q?)
(2)= 1n?1?j?n( max1?i?mWordSimilarity(wj , w?i))This normalization has pros and cons.
It removesthe bias towards long sentences by eliminating theaccumulating effect; but on the other hand, it mightcause the system to miss a related question, for ex-ample, when two related sentences have only onekey word in common.1Formula (2) shows that sentence level similaritydepends on word-word similarity.
Researchers haveproposed a variety of ways in measuring the seman-tic similarity or relatedness between two words (tobe exact, word senses) based on WordNet.
For ex-ample, the Path (path) measure is the inverse ofthe shortest path length between two word sensesin WordNet; the Wu and Palmer?s (wup) measure(Wu and Palmer, 1994) is to find the most spe-cific concept that two word senses share as ances-tor (least common subsumer), and then scale thepath length of this concept to the root note (sup-posed that there is a virtual root note in WordNet)by the sum of the path lengths of the individualword sense to the root node; the Lin?s (lin) mea-sure (Lin, 1998) is based on information content,which is a corpus based measure of the specificity ofa word; the Vector (vector) measure associates eachword with a gloss vector and calculates the similar-ity of two words as the cosine between their glossvectors (Patwardhan, 2003).
It was unclear whichmeasure(s) would contribute the best information tothe task of relevancy recognition, so we just exper-imented on all four measures, path, wup, lin, andvector, in our decision tree training.
We used Peder-sen et al?s (2004) tool WordNet::Similarity to com-pute these four measures.
WordNet::Similarity im-plements nine different measures of word similar-ity.
We here only used the four described above be-cause they return a value between 0 and 1, whichis suitable for using formula (2) to calculate sen-tence similarity, and we leave others as future work.Notice that the WordNet::Similarity implementation1Another idea is to feed the decision tree training both thenormalized and non-normalized semantic similarity informa-tion and see what would come out.
We tried it on the TREC dataand found out that the normalized features actually have higherinformation gain (i.e.
appear at the top levels of the learned tree.can only measure path, wup, and lin between twonouns or between two verbs, while it uses all thecontent words for the vector measure.
We thus havethe following semantic features:path noun: sentence similarity is based on thenouns2 similarity using the path measure.path verb: sentence similarity is based on the non-trivial verbs similarity using the path measure.Trivial verbs include ?does, been, has, have,had, was, were, am, will, do, did, would, might,could, is, are, can, should, shall, being?.wup noun: sentence similarity is based on thenouns similarity using the Wu and Palmer?smeasure.wup verb: sentence similarity is based on thenon-trivial verbs similarity using the Wu andPalmer?s measure.lin noun: sentence similarity is based on the nounssimilarity using the Lin?s measure.lin verb: sentence similarity is based on the non-trivial verbs similarity using the Lin?s measure.vector: sentence similarity is based on all contentwords (nouns, verbs, and adjectives) similarityusing the vector measure.4 ResultsWe ran the experiments on two sets of data: theTREC QA data and the HandQA data.4.1 Results on the TREC dataTREC has contextual questions in 2001 contexttrack and 2004 (Voorhees, 2001; Voorhees, 2004).Questions about a specific topic are organized into asession.
In reality, the boundaries between sessionsare not given.
The QA system would have to rec-ognize the start of a new session as the first step ofquestion answering.
We used the TREC 2004 dataas training and the TREC 2001 context track data astesting.
The training data contain 286 factoid and listquestions in 65 sessions3; the testing data contain 42questions in 10 sessions.
Averagely each session hasabout 4-5 questions.
Figure 2 shows some examplequestions (the first three sessions) from the TREC2001 context track data.2This is to filter out all other words but nouns from a sen-tence for measuring semantic similarity.3In the TREC 2004 data, each session of questions is as-signed a phrase as the topic, and thus the first question in a ses-sion might have pronouns referring to this topic phrase.
In suchcases, we manually replaced the pronouns by the topic phrase.36CTX1a Which museum in Florence was damaged by amajor bomb explosion in 1993?CTX1b On what day did this happen?CTX1c Which galleries were involved?CTX1d How many people were killed?CTX1e Where were these people located?CTX1f How much explosive was used?CTX2a Which industrial sector supplies the mostjobs in Toulouse?CTX2b How many foreign companies were based therein 1994?CTX2c Name a company that flies there.CTX3a What grape variety is used in Chateau PetrusBordeaus?CTX3b How much did the future cost for the 1989Vintage?CTX3c Where did the winery?s owner go to college?CTX3d What California winery does he own?Figure 2: Example TREC questions4.1.1 Confusion MatrixTable 1 shows the confusion matrix of the deci-sion tree learning results.
On the testing data, thelearned model performs with 90% in recall and 82%in precision for recognizing first questions; for rec-ognizing follow-up questions, the recall is 94% andprecision is 97%.
In contrast, De Boni and Man-andhar?s rule-based algorithm has 90% in recall and56% in precision for recognizing first questions; forfollow-up questions, the recall is 78% and precisionis 96%.
The recall and precision of our learnedmodel to recognize first questions and follow-upquestions are all better than or at least the sameas the rule-based algorithm.
The accuracy of ourlearned model is 93%, about 12% absolute improve-ment from the rule-based algorithm, which is 81% inaccuracy.
Although the size of the data is too smallto draw a more general conclusion, we do see thatthe data driven approach has better performance.Training DataPredicted ClassTrue Class First follow-up TotalFirst 63 2 65follow-up 1 220 221Total 64 222 286Testing DataPredicted ClassTrue Class First follow-up Total RecallFirst 9 1 10 90%follow-up 2 30 32 94%Total 11 31 42Precision 82% 97%Table 1: Confusion Matrix for TREC Data4.1.2 Trained TreeFigure 3 shows the first top two levels of the treelearned from the training data.
Not surprisingly,PRONOUN turns out to be the most important fea-ture which has the highest information gain.
In theTREC data, when there is a pronoun in a question,the question is very likely to be a follow-up ques-tion.
In fact, in the TREC 2004 data, the referentof pronouns very often is the topic phrase.
The fea-ture path noun, on the second level of the trainedtree, turns out to contribute most information in thisrecognition task among the four different semanticsimilarity measures.
The similarity measures usingwup, wup noun and wup verb, and the vector mea-sure do not appear in any node of the trained tree.Figure 3: Trained Tree on TREC DataThe following are rules generated from the train-ing data whose confidence is higher than 90%.
Con-fidence is defined as out of the training records forwhich the left hand side of the rule is true, the per-centage of records for which the right hand side isalso true.
This measures the accuracy of the rule.- If PRONOUN=1 then follow-up question- If path noun?0.31 then follow-up question- If lin noun?0.43 then follow-up question- If path noun<0.15 and PRONOUN=0 then first questionDe Boni and Manandhar?s algorithm has thisrule:?if a question has no verb, the question isfollow-up question?.
However, we did not learn thisrule from the data, nor the feature VERB appears inany node of the trained tree.
One possible reasonis that this rule has too little support in the trainingset (support is defined as the percentage of whichthe left hand side of the rule is true).
Another pos-sible reason is that this rule is not needed becausethe combination of other features is able to provideenough information for recognizing follow-up ques-tions.
In any case, the decision tree learns a (local)37optimized combination of features which capturesmost cases, and avoids redundant rules.4.1.3 Error AnalysisThe trained decision tree has 3 errors in the test-ing data.
Two of the errors are mis-recognition offollow-up questions to be first questions, and one isthe vice versa.The first error is failure to recognize the ques-tion ?which galleries were involved??
(CTX1c) asa follow-up question (see Figure 2 for context).
Itis a syntactically complete sentence, and there is nopronoun or definite noun in the sentence.
Seman-tic features are the most useful information to rec-ognize it as a follow-up question.
However, the se-mantic relatedness in WordNet between the words?gallery?
in the current question and ?museum?
inthe first question of this session (CTX1a in Figure 2)is not strong enough for the trained decision tree torelate the two questions together.The second error is failure to recognize the ques-tion ?Where did the winery?s owner go to college??
(CTX3c) as a follow-up question.
Similarly, partof the reason for this failure is due to the insuffi-cient semantic relatedness between the words ?win-ery?
and ?grape?
(in CTX3a) to connect the ques-tions together.
However, this question has a definitenoun phrase ?the winery?
which refers to ?ChateauPetrus Bordeaux?
in the first question in this session.We did not make use of the feature DefiniteNoun inour training, because it is not easy to automaticallyidentify the referents of a definite noun phrase, oreven whether it has a referent or not.
A lot of def-inite noun phrases, such as ?the sun?, ?the trees inChina?, ?the first movie?, and ?the space shuttles?,do not refer to any entity in the text.
This does notmean that the feature DefiniteNoun is not important,but instead that we just leave it as our future work tobetter incorporate this feature.The third error, is failure to recognize the question?What does transgenic mean??
as the first questionthat opens a session.
This error is due to the over-fitting of decision tree training.4.1.4 BoostingWe tried another machine learning approach, Ad-aboost (Schapire and Singer, 2000), which is resis-tant (but not always) to over-fitting.
It calls a givenweak learning algorithm repeatedly in a series ofrounds t = 1, ..., T .
Each time the weak learningalgorithm generates a rough ?rule of thumb?, andafter many rounds Adaboost combines these weakrules into a single prediction rule that, hopefully,will be more accurate than any one of the weakrules.
Figure 2 shows the confusion matrix of Ad-aboost learning results.
It shows that Adaboost isable to correctly recognize ?What does transgenicmean??
as beginning a new topic.
However, Ad-aboost has more errors in recognizing follow-upquestions, which results in an overall accuracy of88%, slightly lower than decision tree learning.Training DataPredicted ClassTrue Class First follow-up TotalFirst 64 1 65follow-up 1 220 221Total 65 221 286Testing DataPredicted ClassTrue Class First follow-up Total RecallFirst 10 0 10 100%follow-up 5 27 32 84%Total 15 27 42Precision 67% 100%Table 2: Confusion Matrix Using Adaboosting4.2 Results on the HandQA dataWe also conducted an experiment using real-worldcustomer-care related questions.
We selected ourtest data from the chat logs of a deployed onlineQA system.
We refer to this system as HandQA.HandQA is built using a telecommunication ontol-ogy database and 1600 pre-determined FAQ-answerpairs.
For every submitted customer question,HandQA chooses one of these 1600 answers as theresponse.
Each chat session contains about 3 ques-tions.
We assume the questions in a session arecontext-related.The HandQA data are different from the TRECdata in two ways.
First, HandQA questions are realtyped questions from motivated users.
The HandQAdata contain some noisy information, such as typosand bad grammars.
Some users even treated thissystem as a search engine and simply typed in thekeywords.
Second, questions in a chat session ba-sically asked for the same information.
Very often,when the system failed to get the correct answer to38the user?s question, the user would repeat or rephrasethe same question, until they gave up or the systemluckily found the answer.
As an example, Figure 4shows two chat sessions.
Again, we did not use thesystem?s answer in our relevancy recognition.How to make number non published?Non published numbersHow to make number non listed?Is my number switched to Call Vantage yet?When will my number be switched?When is number transferred?Figure 4: Example questions in HandQAA subset of the HandQA data, 5908 questions in2184 sessions are used for training and testing thedecision tree.
The data were randomly divided intotwo sets: 90% for training and 10% for testing.4.2.1 Confusion MatrixTable 3 shows the confusion matrix of the deci-sion tree learning results.
For recognizing first ques-tions, the learned model has 73% in recall and 62%in precision; for recognizing follow-up questions,the recall is 75% and precision is 84%.
The accuracyis 74%.
A base line model is to have all questionsexcept the first one as following up questions, whichresults in the accuracy of 64% (380/590).
Thus thelearned decision tree yields an absolute improve-ment of 10%.
However, the results on this data setare not as good as those on the TREC data.Training DataPredicted ClassTrue Class First follow-up TotalFirst 1483 490 1973follow-up 699 2646 3345Total 2182 3136 5318Testing DataPredicted ClassTrue Class First follow-up Total RecallFirst 153 58 211 73%follow-up 93 286 379 75%Total 246 344 590Precision 62% 84%Table 3: Confusion Matrix for HandQA Data4.2.2 Trained TreeTable 5 shows the top two levels of the treelearned from the training data, both of which areon the semantic measure path.
This again confirmsthat path best fits the task of relevancy recognitionamong the four semantic measures.No syntactical features appear in any node of thelearned tree.
This is not surprising because syntac-tic information is noisy in this data set.
Typos, badgrammars, and mis-capitalization affect automaticPOS tagging.
Keywords input also results in incom-plete sentences, which makes it unreliable to recog-nize follow-up questions based on whether a ques-tion is a complete sentence or not.
Furthermore,because questions in a session rarely refer to eachother, but just repeat or rephrase each other, the fea-ture PRONOUN does not help either.
All these makesyntactic features not useful.
Semantic features turnout to be more important for this data set.Figure 5: Trained Tree on HandQA Data4.2.3 Error AnalysisThere are two reasons for the decreased perfor-mance in this data set.
The first reason, as we ana-lyzed above, is that syntactical features do not con-tribute to the recognition task.
The second reason isthat consecutive chat sessions might ask for the sameinformation.
In the handQA data set, questions arebasically all about telecommunication service, andquestions in two consecutive chat sessions, althoughby different users, could be on very similar topics oreven have same words.
Thus, questions, although intwo separate chat sessions, could have high semanticsimilarity measure.
This would introduce confusinginformation to the decision tree learning.5 Making Use of Context InformationRelevancy recognition is the first step of contextualquestion answering.
If a question is recognized asfollowing the current existing topic, the next step isto make use of the context information to interpret it39and retrieve the answers.
To explore how context in-formation helps answer retrieval, we conducted pre-liminary experiments with the TREC 2004 QA data.We indexed the TREC documents using the Lucenesearch engine (Hatcher and Gospodnetic, 2004) fordocument retrieval.
The Lucene search engine takesas input a query (a list of keywords), and returns aranked list of relevant documents, of which the first50 were taken and analyzed in our experiments.
Wetried different strategies for query formulation.
Sim-ply using the questions as the query, only 20% ofthe follow-up questions find their answers in the first50 returned documents.
This percentage went upto 85% when we used the topic words, provided inTREC data for each section, as the query.
Becausetopic words are usually not available in real worldapplications, to be more practical, we tried using thenoun phrases in the first question as the query.
Inthis case, 81% of the questions are able to find theanswers in the returned documents.
When we com-bined the (follow-up) question with the noun phrasesin the first question as the query, the retrieved rateincreases to 84%.
Typically, document retrieval is acrucial step for QA systems.
These results suggestthat context information fusion has a big potential toimprove the performance of answer retrieval.
How-ever, we leave the topic of how to fuse context infor-mation into the follow-up questions as future work.6 ConclusionIn this paper, we present a data driven approach, de-cision tree learning, for the task of relevancy recog-nition in contextual question answering.
Experi-ments show that this approach achieves 93% accu-racy on the TREC data, about 12% improvementfrom the rule-based algorithm reported by De Boniand Mananhar (2005).
Moreover, this data drivenapproach requires much less human effort on inves-tigating a specific data set and less human exper-tise to summarize rules from the observation.
Allthe features we used in the training can be automat-ically extracted.
This makes it straightforward totrain a model in a new domain, such as the HandQA.Furthermore, decision tree learning is a white-boxmodel and the trained tree is human interpretable.
Itshows that the path measure has the best informationgain among the other semantic similarity measures.We also report our preliminary experiment results oncontext information fusion for question answering.7 AcknowledgementThe authors thank Srinivas Bangalore and Mazin E.Gilbert for helpful discussion.ReferencesHamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:A framework and graphical development environmentfor robust nlp tools and applications.
In Proceedingsof the 40th ACL.Marco De Boni and Suresh Manandhar.
2005.
Imple-menting clarification dialogues in open domain ques-tion answering.
Natural Language Engineering.
Ac-cepted.Christiane Fellbaum.
1998.
WordNet:An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.Erik Hatcher and Otis Gospodnetic.
2004.
Lucene inAction.
Manning.Marti A. Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of 32nd ACL, pages9?16.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of the International Con-ference on Machine Learning.Siddharth Patwardhan.
2003.
Incorporating dictionaryand corpus information into a context vector measureof semantic relatedness.
master?s thesis, University ofMinnesota, Duluth.Ted Pederson, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity - measuring the re-latedness of concepts.
In Proceedings of the 9th AAAI.Intelligent Systems Demonstration.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2004.
Apublic reference implementation of the rap anaphoraresolution algorithm.
In Proceedings of LREC, pages291?294.Robert E. Schapire and Yoram Singer.
2000.
BoosTex-ter: A boosting-based system for text categorization.Machine Learning, 39:135?168.Ellen M. Voorhees.
2001.
Overview of the TREC 2001question answering track.
In Proceedings of TREC-10.Ellen M. Voorhees.
2004.
Overview of the TREC 2004question answering track.
In Proceedings of TREC-13.Zhibiao Wu and Martha Palmer.
1994.
Verb semanticsand lexical selection.
In Proceedings of 32nd ACL,pages 133?138.40
