Corpus Variation and Parser PerformanceDaniel GildeaUniversity of California, Berkeley, andInternational Computer Science Institutegildea@cs.berkeley.eduAbstractMost work in statistical parsing has focused on asingle corpus: the Wall Street Journal portion of thePenn Treebank.
While this has allowed for quanti-tative comparison of parsing techniques, it has leftopen the question of how other types of text mightaect parser performance, and how portable pars-ing models are across corpora.
We examine thesequestions by comparing results for the Brown andWSJ corpora, and also consider which parts of theparser's probability model are particularly tuned tothe corpus on which it was trained.
This leads usto a technique for pruning parameters to reduce thesize of the parsing model.1 IntroductionThe past several years have seen great progress inthe eld of natural language parsing, through the useof statistical methods trained using large corpora ofhand-parsed training data.
The techniques of Char-niak (1997), Collins (1997), and Ratnaparkhi (1997)achieved roughly comparable results using the samesets of training and test data.
In each case, the cor-pus used was the Penn Treebank's hand-annotatedparses of Wall Street Journal articles.
Relativelyfew quantitative parsing results have been reportedon other corpora (though see Stolcke et al (1996)for results on Switchboard, as well as Collins etal.
(1999) for results on Czech and Hwa (1999) forbootstrapping from WSJ to ATIS).
The inclusion ofparses for the Brown corpus in the Penn Treebankallows us to compare parser performance across cor-pora.
In this paper we examine the following ques-tions: To what extent is the performance of statisticalparsers on the WSJ task due to its relativelyuniform style, and how might such parsers fareon the more varied Brown corpus? Can training data from one corpus be appliedto parsing another? What aspects of the parser's probability modelare particularly tuned to one corpus, and whichare more general?Our investigation of these questions leads us toa surprising result about parsing the WSJ corpus:over a third of the model's parameters can be elim-inated with little impact on performance.
Asidefrom cross-corpus considerations, this is an impor-tant nding if a lightweight parser is desired or mem-ory usage is a consideration.2 Previous Comparisons of CorporaA great deal of work has been done outside of theparsing community analyzing the variations betweencorpora and dierent genres of text.
Biber (1993)investigated variation in a number syntactic fea-tures over genres, or registers, of language.
Ofparticular importance to statistical parsers is theinvestigation of frequencies for verb subcategoriza-tions such as Roland and Jurafsky (1998).
Rolandet al (2000) nd that subcategorization frequen-cies for certain verbs vary signicantly between theWall Street Journal corpus and the mixed-genreBrown corpus, but that they vary less so betweengenre-balanced British and American corpora.
Ar-gument structure is essentially the task that auto-matic parsers attempt to solve, and the frequenciesof various structures in training data are reected ina statistical parser's probability model.
The varia-tion in verb argument structure found by previousresearch caused us to wonder to what extent a modeltrained on one corpus would be useful in parsing an-other.
The probability models of modern parsersinclude not only the number and syntactic type ofa word's arguments, but lexical information abouttheir llers.
Although we are not aware of previouscomparisons of the frequencies of argument llers,we can only assume that they vary at least as muchas the syntactic subcategorization frames.3 The Parsing ModelWe take as our baseline parser the statistical modelof Model 1 of Collins (1997).
The model is a history-based, generative model, in which the probability fora parse tree is found by expanding each node in thetree in turn into its child nodes, and multiplying theprobabilities for each action in the derivation.
It canbe thought of as a variety of lexicalized probabilis-tic context-free grammar, with the rule probabilitiesfactored into three distributions.
The rst distribu-tion gives probability of the syntactic category Hof the head child of a parent node with categoryP , head word Hhw with the head tag (the part ofspeech tag of the head word) Hht:Ph(H jP;Hht;Hhw)The head word and head tag of the new node H aredened to be the same as those of its parent.
Theremaining two distributions generate the non-headchildren one after the other.
A special #STOP#symbol is generated to terminate the sequence ofchildren for a given parent.
Each child is gener-ated in two steps: rst its syntactic category C andhead tag Cht are chosen given the parent's and headchild's features and a function  representing thedistance from the head child:Pc(C;ChtjP;H;Hht;Hhw;)Then the new child's head word Chw is chosen:Pcw(ChwjP;H;Hht;Hhw;; C; Cht)For each of the three distributions, the empirical dis-tribution of the training data is interpolated withless specic backo distributions, as we will see inSection 5.
Further details of the model, includingthe distance features used and special handling ofpunctuation, conjunctions, and base noun phrases,are described in Collins (1999).The fundamental features of used in the proba-bility distributions are the lexical heads and headtags of each constituent, the co-occurrences of par-ent nodes and their head children, and the co-occurrences of child nodes with their head siblingsand parents.
The probability models of Charniak(1997), Magerman (1995) and Ratnaparkhi (1997)dier in their details but are based on similar fea-tures.
Models 2 and 3 of Collins (1997) add someslightly more elaborate features to the probabilitymodel, as do the additions of Charniak (2000) tothe model of Charniak (1997).Our implementation of Collins' Model 1 performsat 86% precision and recall of labeled parse con-stituents on the standard Wall Street Journal train-ing and test sets.
While this does not reectthe state-of-the-art performance on the WSJ taskachieved by the more the complex models of Char-niak (2000) and Collins (2000), we regard it as areasonable baseline for the investigation of corpuseects on statistical parsing.4 Parsing Results on the BrownCorpusWe conducted separate experiments using WSJdata, Brown data, and a combination of the twoas training material.
For the WSJ data, we ob-served the standard division into training (sections2 through 21 of the treebank) and test (section 23)sets.
For the Brown data, we reserved every tenthsentence in the corpus as test data, using the othernine for training.
This may underestimate the dif-culty of the Brown corpus by including sentencesfrom the same documents in training and test sets.However, because of the variation within the Browncorpus, we felt that a single contiguous test sectionmight not be representative.
Only the subset of theBrown corpus available in the Treebank II bracket-ing format was used.
This subset consists primarilyof various ction genres.
Corpus sizes are shown inTable 1.Training Set Test SetCorpus Sentences Words Sentences WordsWSJ 39,832 950,028 2245 48,665Brown 21,818 413,198 2282 38,109Table 1: Corpus sizes.
Both test sets were restrictedto sentences of 40 words or less.
The Brown testset's average sentence was shorter despite the lengthrestriction.Training Data Test Set Recall Prec.WSJ WSJ 86.1 86.6WSJ Brown 80.3 81.0Brown Brown 83.6 84.6WSJ+Brown Brown 83.9 84.8WSJ+Brown WSJ 86.3 86.9Table 2: Parsing results by training and test corpusResults for the Brown corpus, along with WSJresults for comparison, are shown in Table 2.
Thebasic mismatch between the two corpora is shownin the signicantly lower performance of the WSJ-trained model on Brown data than on WSJ data(rows 1 and 2).
A model trained on Brown data onlydoes signicantly better, despite the smaller size ofthe training set.
Combining the WSJ and Browntraining data in one model improves performancefurther, but by less than 0.5% absolute.
Similarly,adding the Brown data to the WSJ model increasedperformance on WSJ by less than 0.5%.
Thus, evena large amount of additional data seems to have rel-atively little impact if it is not matched to the testmaterial.The more varied nature of the Brown corpus alsoseems to impact results, as all the results on Brownare lower than the WSJ result.5 The Eect of LexicalDependenciesThe parsers cited above all use some variety of lexicaldependency feature to capture statistics on the co-occurrence of pairs of words being found in parent-child relations within the parse tree.
These wordpair relations, also called lexical bigrams (Collins,1996), are reminiscent of dependency grammars suchas Melcuk (1988) and the link grammar of Sleatorand Temperley (1993).
In Collins' Model 1, the wordpair statistics occur in the distributionPcw(ChwjP;H;Hht;Hhw;; C; Cht)whereHhw represent the head word of a parent nodein the tree and Chw the head word of its (non-head)child.
(The head word of a parent is the same as thehead word of its head child.)
Because this is the onlypart of the model that involves pairs of words, it isalso where the bulk of the parameters are found.
Thelarge number of possible pairs of words in the vocab-ulary make the training data necessarily sparse.
Inorder to avoid assigning zero probability to unseenevents, it is necessary to smooth the training data.The Collins model uses linear interpolation to es-timate probabilities from empirical distributions ofvarying specicities:Pcw(ChwjP;H;Hht;Hhw;; C; Cht) =1~P (ChwjP;H;Hht;Hhw;; C; Cht) +(1  1)2~P (ChwjP;H;Hht;; C; Cht)+(1  2)~P (ChwjCht)(1)where~P represents the empirical distribution de-rived directly from the counts in the training data.The interpolation weights 1, 2are chosen as afunction of the number of examples seen for the con-ditioning events and the number of unique valuesseen for the predicted variable.
Only the rst distri-bution in this interpolation scheme involves pairs ofwords, and the third component is simply the prob-ability of a word given its part of speech.Because the word pair feature is the most spe-cic in the model, it is likely to be the most corpus-specic. The vocabularies used in corpora vary, asdo the word frequencies.
It is reasonable to ex-pect word co-occurrences to vary as well.
In or-der to test this hypothesis, we removed the distribu-tion~P (ChwjP;H;Hht;Hhw;C;Cht) from the pars-ing model entirely, relying on the interpolation of thetwo less specic distributions in the parser:Pcw2(ChwjP;H;Hht;; C; Cht) =2~P (ChwjP;H;Hht;; C; Cht) +(1  2)~P (ChwjCht) (2)We performed cross-corpus experiments as beforeto determine whether the simpler parsing modelmight be more robust to corpus eects.
Results areshown in Table 3.Perhaps the most striking result is just how littlethe elimination of lexical bigrams aects the baselinesystem: performance on the WSJ corpus decreasesby less than 0.5% absolute.
Moreover, the perfor-mance of a WSJ-trained system without lexical bi-grams on Brown test data is identical to the WSJ-trained system with lexical bigrams.
Lexical co-occurrence statistics seem to be of no benet whenattempting to generalize to a new corpus.6 Pruning Parser ParametersThe relatively high performance of a parsing modelwith no lexical bigram statistics on the WSJ taskled us to explore whether it might be possible tosignicantly reduce the size of the parsing modelby selectively removing parameters without sacri-cing performance.
Such a technique reduces theparser's memory requirements as well as the over-head of loading and storing the model, which couldbe desirable for an application where limited com-puting resources are available.Signicant eort has gone into developing tech-niques for pruning statistical language models forspeech recognition, and we borrow from this work,using the weighted dierence technique of Seymoreand Rosenfeld (1996).
This technique applies to anystatistical model which estimates probabilities bybacking o, that is, using probabilities from a lessspecic distribution when no data are available areavailable for the full distribution, as the followingequations show for the general case:P (ejh) = P1(ejh) if e 62 BO(h)= (h)P2(ejh0) if e 2 BO(h)Here e is the event to be predicted, h is the set ofconditioning events or history,  is a backo weight,and h0is the subset of conditioning events used forthe less specic backo distribution.
BO is the back-o set of events for which no data are present in thespecic distribution P1.
In the case of n-gram lan-guage modeling, e is the next word to be predicted,and the conditioning events are the n  1 precedingwords.
In our case the specic distribution P1of thebacko model is Pcwof equation 1, itself a linear in-terpolation of three empirical distributions from thetraining data.
The less specic distribution P2of thebacko model is Pcw2of equation 2, an interpolationof two empirical distributions.
The backo weightis simply 1   1in our linear interpolation model.The Seymore/Rosenfeld pruning technique can beused to prune backo probability models regardlessof whether the backo weights are derived from lin-ear interpolation weights or discounting techniquessuch as Good-Turing.
In order to ensure that themodel's probabilities still sum to one, the backow/ bigrams w/o bigramsTraining Data Test Set Recall Prec.
Recall Prec.WSJ WSJ 86.1 86.6 85.6 86.2WSJ Brown 80.3 81.0 80.3 81.0Brown Brown 83.6 84.6 83.5 84.4WSJ+Brown Brown 83.9 84.8 83.4 84.3WSJ+Brown WSJ 86.3 86.9 85.7 86.4Table 3: Parsing results by training and test corpusweight  must be adjusted whenever a parameter isremoved from the model.
In the Seymore/Rosenfeldapproach, parameters are pruned according to thefollowing criterion:N(e; h)(log p(ejh)  log p0(ejh0)) (3)where p0(ejh0) represents the new backed o proba-bility estimate after removing p(ejh) from the modeland adjusting the backo weight, and N(e; h) is thecount in the training data.
This criterion aims toprune probabilities that are similar to their back-o estimates, and that are not frequently used.
Asshown by Stolcke (1998), this criterion is an approx-imation of the relative entropy between the originaland pruned distributions, but does not take into ac-count the eect of changing the backo weight onother events' probabilities.Adjusting the threshold  below which parametersare pruned allows us to successively remove moreand more parameters.
Results for dierent values of are shown in Table 4.The complete parsing model derived from theWSJ training set has 735,850 parameters in a to-tal of nine distributions: three levels of backo foreach of the three distributions Ph, Pcand Pcw.
Thelexical bigrams are contained in the most specicdistribution for Pcw.
Removing all these parametersreduces the total model size by 43%.
The resultsshow a gradual degradation as more parameters arepruned.The ten lexical bigrams with the highest scores forthe pruning metric are shown in Table 5 for WSJand Table 6.
The pruning metric of equation 3 hasbeen normalized by corpus size to allow compari-son between WSJ and Brown.
The only overlapbetween the two sets is for pairs of unknown wordtokens.
The WSJ bigrams are almost all specicto nance, are all word pairs that are likely to ap-pear immediately adjacent to one another, and areall children of the base NP syntactic category.
TheBrown bigrams, which have lower correlation val-ues by our metric, include verb/subject and prepo-sition/object relations and seem more broadly ap-plicable as a model of English.
However, the pairsare not strongly related semantically, no doubt be-cause the rst term of the pruning criterion favorsthe most frequent words, such as forms of the verbs\be" and \have".Child word Head word Parent PruningChw Hhw P MetricNew York NPB .0778Stock Exchange NPB .0336< unk > < unk > NPB .0313vice president NPB .0312Wall Street NPB .0291San Francisco NPB .0291York Stock NPB .0243Mr.
< unk > NPB .0241third quarter NPB .0227Dow Jones NPB .0227Table 5: Ten most signicant lexical bigrams fromWSJ, with parent category (other syntactic contextvariables not shown) and pruning metric.
NPB is Collins' \base NP" category.Child word Head word Parent PruningChw Hhw P MetricIt was S .0174it was S .0169< unk > of PP .0156< unk > in PP .0097course Of PP .0090been had VP .0088< unk > < unk > NPB .0079they were S .0077I 'm S .0073time at PP .0073Table 6: Ten most signicant lexical bigrams fromBrown7 ConclusionOur results show strong corpus eects for statisticalparsing models: a small amount of matched train-ing data appears to be more useful than a largeamount of unmatched data.
The standard WSJtask seems to be simplied by its homogenous style.Adding training data from from an unmatched cor-pus doesn't hurt, but doesn't help a great deal either.In particular, lexical bigram statistics appear tobe corpus-specic, and our results show that theyThreshold # parameters % reduction removed model size Recall Prec.0 (full model) 0 0 86.1 86.61 96K 13 86.0 86.42 166K 23 85.9 86.23 213K 29 85.7 86.21 316K 43 85.6 86.2Table 4: Parsing results with pruned probability models.
The complete parsing model contains 736K pa-rameters in nine distributions.
Removing all lexical bigram parameters reducing the size of the model by43%.are of no use when attempting to generalize to newtraining data.
In fact, they are of surprisingly littlebenet even for matched training and test data |removing them from the model entirely reduces per-formance by less than 0.5% on the standard WSJparsing task.
Our selective pruning technique al-lows for a more ne grained tuning of parser modelsize, and would be particularly applicable to caseswhere large amounts of training data are availablebut memory usage is a consideration.
In our im-plementation, pruning allowed models to run within256MB that, unpruned, required larger machines.The parsing models of Charniak (2000) andCollins (2000) add more complex features to theparsing model that we use as our baseline.
Anarea for future work is investigation of the degreeto which such features apply across corpora, or, onthe other hand, further tune the parser to the pe-culiarities of the Wall Street Journal.
Of particu-lar interest are the automatic clusterings of lexicalco-occurrences used in Charniak (1997) and Mager-man (1995).
Cross-corpus experiments could revealwhether these clusters uncover generally applicablesemantic categories for the parser's use.Acknowledgments This work was undertaken aspart of the FrameNet project at ICSI, with fundingfrom National Science Foundation grant ITR/HCI#0086132.ReferencesDouglas Biber.
1993.
Using register-diversied cor-pora for general language studies.
ComputationalLinguistics, 19(2):219{241, June.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InAAAI97, Brown University, Providence, RhodeIsland, August.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st AnnualMeeting of the North American Chapter of theACL (NAACL), Seattle, Washington.Michael Collins, Jan Hajic, Lance Ramshaw, andChristoph Tillmann.
1999.
A statistical parser forczech.
In Proceedings of the 37th Annual Meetingof the ACL, College Park, Maryland.Michael Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the 34th Annual Meeting of the ACL.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofthe 35th Annual Meeting of the ACL.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theICML.Rebecca Hwa.
1999.
Supervised grammar inductionusing training data with limited constituent infor-mation.
In Proceedings of the 37th Annual Meet-ing of the ACL, College Park, Maryland.David Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rd An-nual Meeting of the ACL.Ivan A. Melcuk.
1988.
Dependency Syntax: Theoryand Practice.
State University of New York Press.Adwait Ratnaparkhi.
1997.
A linear observed timestatistical parser based on maximum entropymodels.
In Proceedings of the Second Conferenceon Empirical Methods in Natural Language Pro-cessing.Douglas Roland and Daniel Jurafsky.
1998.
Howverb subcategorization frequencies are aected bycorpus choice.
In Proceedings of COLING/ACL,pages 1122{1128.Douglas Roland, Daniel Jurafsky, Lise Menn, Su-sanne Gahl, Elizabeth Elder, and Chris Riddoch.2000.
Verb subcategorization frequency dier-ences between business-news and balanced cor-pora: the role of verb sense.
In Proceedings of theAssociation for Computational Linguistics (ACL-2000) Workshop on Comparing Corpora.Kristie Seymore and Roni Rosenfeld.
1996.
Scalablebacko language models.
In ICSLP-96, volume 1,pages 232{235, Philadelphia.Daniel Sleator and Davy Temperley.
1993.
Pars-ing english with a link grammar.
In Third Inter-national Workshop on Parsing Technologies, Au-gust.A.
Stolcke, C. Chelba, D. Engle, V. Jimenez,L.
Mangu, H. Printz, E. Ristad, R. Rosenfeld,D.
Wu, F. Jelinek, and S. Khudanpur.
1996.
De-pendency language modeling.
Summer WorkshopFinal Report 24, Center for Language and SpeechProcessing, Johns Hopkins University, Baltimore,April.Andreas Stolcke.
1998.
Entropy-based pruningof backo language models.
In Proc.
DARPABroadcast News Transcription and UnderstandingWorkshop, pages 270{274, Lansdowne, Va.
