Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125?128,Prague, June 2007. c?2007 Association for Computational LinguisticsAutomatically Assessing the Post Quality in Online Discussions on SoftwareMarkus Weimer and Iryna Gurevych and Max Mu?hlha?userUbiquitous Knowledge Processing Group, Division of TelecooperationDarmstadt University of Technology, Germanyhttp://www.ukp.informatik.tu-darmstadt.de[mweimer,gurevych,max]@tk.informatik.tu-darmstadt.deAbstractAssessing the quality of user generated con-tent is an important problem for many webforums.
While quality is currently assessedmanually, we propose an algorithm to as-sess the quality of forum posts automati-cally and test it on data provided by Nab-ble.com.
We use state-of-the-art classifi-cation techniques and experiment with fivefeature classes: Surface, Lexical, Syntactic,Forum specific and Similarity features.
Weachieve an accuracy of 89% on the task ofautomatically assessing post quality in thesoftware domain using forum specific fea-tures.
Without forum specific features, weachieve an accuracy of 82%.1 IntroductionWeb 2.0 leads to the proliferation of user generatedcontent, such as blogs, wikis and forums.
Key prop-erties of user generated content are: low publicationthreshold and a lack of editorial control.
Therefore,the quality of this content may vary.
The end userhas problems to navigate through large repositoriesof information and find information of high qual-ity quickly.
In order to address this problem, manyforum hosting companies like Google Groups1 andNabble2 introduce rating mechanisms, where userscan rate the information manually on a scale from 1(low quality) to 5 (high quality).
The ratings havebeen shown to be consistent with the user commu-nity by Lampe and Resnick (2004).
However, the1http://groups.google.com2http://www.nabble.compercentage of manually rated posts is very low (0.1%in Nabble).Departing from this, the main idea explored in thepresent paper is to investigate the feasibility of au-tomatically assessing the perceived quality of usergenerated content.
We test this idea for online fo-rum discussions in the domain of software.
The per-ceived quality is not an objective measure.
Rather, itmodels how the community at large perceives postquality.
We choose a machine learning approach toautomatically assess it.Our main contributions are: (1) An algorithm forautomatic quality assessment of forum posts thatlearns from human ratings.
We evaluate the systemon online discussions in the software domain.
(2)An analysis of the usefulness of different classes offeatures for the prediction of post quality.2 Related workTo the best of our knowledge, this is the first workwhich attempts to assess the quality of forum postsautomatically.
However, on the one hand work hasbeen done on automatic assessment of other types ofuser generated content, such as essays and productreviews.
On the other hand, student online discus-sions have been analyzed.Automatic text quality assessment has been stud-ied in the area of automatic essay scoring (Valentiet al, 2003; Chodorow and Burstein, 2004; Attaliand Burstein, 2006).
While there exist guidelinesfor writing and assessing essays, this is not the casefor forum posts, as different users cast their ratingwith possibly different quality criteria in mind.
Thesame argument applies to the automatic assessmentof product review usefulness (Kim et al, 2006c):125Stars Label on the website Number?
Poor Post 1251??
Below Average Post 44?
?
?
Average Post 69?
?
??
Above Average Post 183?
?
?
?
?
Excellent Post 421Table 1: Categories and their usage frequency.Readers of a review are asked ?Was this review help-ful to you??
with the answer choices Yes/No.
Thisis very well defined compared to forum posts, whichare typically rated on a five star scale that does notadvertise a specific semantics.Forums have been in the focus of another trackof research.
Kim et al (2006b) found that the re-lation between a student?s posting behavior and thegrade obtained by that student can be assessed au-tomatically.
The main features used are the num-ber of posts, the average post length and the aver-age number of replies to posts of the student.
Fenget al (2006) and Kim et al (2006a) describe a sys-tem to find the most authoritative answer in a fo-rum thread.
The latter add speech act analysis as afeature for this classification.
Another feature is theauthor?s trustworthiness, which could be computedbased on the automatic quality classification schemeproposed in the present paper.
Finding the most au-thoritative post could also be defined as a specialcase of the quality assessment.
However, it is def-initely different from the task studied in the presentpaper.
We assess the perceived quality of a givenpost, based solely on its intrinsic features.
Any dis-cussion thread may contain an indefinite number ofgood posts, rather than a single authoritative one.3 ExperimentsWe seek to develop a system that adapts to the qual-ity standards existing in a certain user communityby learning the relation between a set of featuresand the perceived quality of posts.
We experimentedwith features from five classes described in table 2:Surface, Lexical, Syntactic, Forum specific and Sim-ilarity features.We use forum discussions from the Software cat-egory of Nabble.com.5 The data consists of 1968rated posts in 1788 threads from 497 forums.
Postscan be rated by multiple users, but that happens5http://www.nabble.com/Software-f94.htmlrarely.
1927 posts were rated by one, 40 by two and1 post by three users.
Table 1 shows the distribu-tion of average ratings on a five star scale.
Fromthis statistics, it becomes evident that users at Nab-ble prefer extreme ratings.
Therefore, we decidedto treat the posts as being binary rated.
: Posts withless than three stars are rated ?bad?.
Posts with morethan three stars are ?good?.We removed 61 posts where all ratings are ex-actly three stars.
We removed additional 14 postsbecause they had contradictory ratings on the binaryscale.
Those posts were mostly spam, which wasvoted high for commercial interests and voted downfor being spam.
Additionally, we removed 30 poststhat did not contain any text but only attachmentslike pictures.
Finally, we removed 331 non Englishposts using a simple heuristics: Posts that containeda certain percentage of words above a pre-definedthreshold, which are non-English according to a dic-tionary, were considered to be non-English.This way, we obtained 1532 binary classifiedposts: 947 good posts and 585 bad posts.
For eachpost, we compiled a feature vector, and feature val-ues were normalized to the range [0.0, .
.
.
, 1.0].We use support vector machines as a state-of-the-art-algorithm for binary classification.
For all exper-iments, we used a C-SVM with a gaussian RBF ker-nel as implemented by LibSVM in the YALE toolkit(Chang and Lin, 2001; Mierswa et al, 2006).
Pa-rameters were set to C = 10 and ?
= 0.1.
We per-formed stratified ten-fold cross validation6 to esti-mate the performance of our algorithm.
We repeatedseveral experiments according to the leave-one-outevaluation scheme and found comparable results tothe ones reported in this paper.4 Results and AnalysisWe compared our algorithm to a majority class clas-sifier as a baseline, which achieves an accuracy of62%.
As it is evident from table 3, most system con-figurations outperform the baseline system.
The bestperforming single feature category are the Forumspecific features.
As we seek to build an adaptablesystem, analyzing the performance without thesefeatures is worthwhile: Using all other features, we6See (Witten and Frank, 2005), chapter 5.3 for an in-depthdescription.126Feature category Feature name DescriptionSurface FeaturesLength The number of tokens in a post.Question Frequency The percentage of sentences ending with ??
?.Exclamation Frequency The percentage of sentences ending with ?!
?.Capital Word Frequency The percentage of words in CAPITAL, which is often associated with shouting.Lexical FeaturesInformation aboutthe wording of thepostsSpelling Error Frequency The percentage of words that are not spelled correctly.3Swear Word Frequency The percentage of words that are on a list of swear words we compiled fromresources like WordNet and Wikipedia4, which contains more than eighty wordslike ?asshole?, but also common transcriptions like ?f*ckin?.Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set(Marcus et al, 1994).
We used TreeTagger (Schmid, 1995) based on the englishparameter files supplied with it.Forum specificfeaturesProperties of a postthat are onlypresent in forumpostingsIsHTML Whether or not a post contains HTML.
In our data, this is encoded explicitly,but it can also be determined by regular expressions matching HTML tags.IsMail Whether or not a post has been copied from a mailing list.
This is encodedexplicitly in our data.Quote Fraction The fraction of characters that are inside quotes of other posts.
These quotes aremarked explicitly in our data.URL and Path Count The number of URLs and filesystem paths.
Post quality in the software do-main may be influenced by the amount of tangible information, which is partlycaptured by these features.Similarity features Forums are focussed on a topic.
The relatedness of a post to the topic of theforum may influence post quality.
We capture this relatedness by the cosinebetween the posts unigram vector and the unigram vector of the forum.Table 2: Features used for the automatic quality assessment of posts.achieve an only slightly worse classification accu-racy.
Thus, the combination of all other featurescaptures the quality of a post fairly well.SUF LEX SYN FOR SIM Avg.
accuracyBaseline 61.82%?
?
?
?
?
89.10%?
?
?
?
?
61.82%?
?
?
?
?
71.82%?
?
?
?
?
82.64%?
?
?
?
?
85.05%?
?
?
?
?
62.01%?
?
?
?
?
89.10%?
?
?
?
?
89.36%?
?
?
?
?
85.03%?
?
?
?
?
82.90%?
?
?
?
?
88.97%?
?
?
?
?
88.56%?
?
?
?
?
85.12%?
?
?
?
?
88.74%Table 3: Accuracy with different feature sets.
SUF: Surface,LEX: Lexical, SYN: Syntax, FOR: Forum specific, SIM: simi-larity.
The baseline results from a majority class classifier.We performed additional experiments to identifythe most important features from the Forum specificones.
Table 4 shows that IsMail and Quote Frac-tion are the dominant features.
This is noteworthy,as those features are not based on the domain of dis-cussion.
Thus, we believe that these features willperform well in future experiments on other data.ISM ISH QFR URL PAC Avg.
accuracy?
?
?
?
?
85.05%?
?
?
?
?
73.30%?
?
?
?
?
61.82%?
?
?
?
?
73.76%?
?
?
?
?
61.29%?
?
?
?
?
61.82%?
?
?
?
?
74.41%?
?
?
?
?
85.05%?
?
?
?
?
73.30%?
?
?
?
?
85.05%?
?
?
?
?
85.05%?
?
?
?
?
84.99%?
?
?
?
?
85.05%Table 4: Accuracy with different forum specific features.ISM: IsMail, ISH: IsHTML, QFR: QuoteFraction, URL: URL-Count, PAC: PathCount.Error Analysis Table 5 shows the confusion ma-trix of the system using all features.
Many poststhat were misclassified as good ones show no ap-parent reason to be classified as bad posts to us.
Theunderstanding of their rating seems to require deepknowledge about the specific subject of discussion.The few remaining posts are either spam or ratednegatively to signalize dissent with the opinion ex-pressed in the post.
Posts that were misclassified asbad ones often contain program code, digital signa-tures or other non-textual parts in the body.
We planto address these issues with better preprocessing in127true good true bad sumpred.
good 490 72 562pred.
bad 95 875 970sum 585 947 1532Table 5: Confusion matrix for the system using all features.the future.
However, the relatively high accuracy al-ready achieved shows that these issues are rare.5 Conclusion and Future WorkAssessing post quality is an important problem formany forums on the web.
Currently, most forumsneed their users to rate the posts manually, which iserror prone, labour intensive and last but not leastmay lead to the problem of premature negative con-sent (Lampe and Resnick, 2004).We proposed an algorithm that has shown to beable to assess the quality of forum posts.
The al-gorithm applies state-of-the-art classification tech-niques using features such as Surface, Lexical, Syn-tactic, Forum specific and Similarity features todo so.
Our best performing system configurationachieves an accuracy of 89.1%, which is signifi-cantly higher than the baseline of 61.82%.
Our ex-periments show that forum specific features performbest.
However, slightly worse but still satisfactoryperformance can be obtained even without those.So far, we have not made use of the structural in-formation in forum threads yet.
We plan to performexperiments investigating speech act recognition inforums to improve the automatic quality assessment.We also plan to apply our system to further domainsof forum discussion, such as the discussions amongactive Wikipedia users.We believe that the proposed algorithm will sup-port important applications beyond content filteringlike automatic summarization systems and forumspecific search.AcknowledgmentsThis work was supported by the German Research Foundationas part of the Research Training Group ?Feedback-Based Qual-ity Management in eLearning?
under the grant 1223.
We arethankful to Nabble for providing their data.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essay scoringwith e-rater v.2.
The Journal of Technology, Learning, andAssessment, 4(3), February.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a libraryfor support vector machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Martin Chodorow and Jill Burstein.
2004.
Beyond essaylength: Evaluating e-raters performance on toefl essays.Technical report, ETS.Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006.Learning to detect conversation focus of threaded discus-sions.
In Proceedings of the Human Language TechnologyConference of the North American Chapter of the Associa-tion of Computational Linguistics (HLT-NNACL).Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and EduardHovya.
2006a.
Mining and assessing discussions on the webthrough speech act analysis.
In Proceedings of the Workshopon Web Content Mining with Human Language Technologiesat the 5th International Semantic Web Conference.Jihie Kim, Erin Shaw, Donghui Feng, Carole Beal, and EduardHovy.
2006b.
Modeling and assessing student activities inon-line discussions.
In Proceedings of the Workshop on Ed-ucational Data Mining at the conference of the American As-sociation of Artificial Intelligence (AAAI-06), Boston, MA.Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pen-neacchiotti.
2006c.
Automatically assessing review helpful-ness.
In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages 423 ?430, Sydney, Australia, July.Cliff Lampe and Paul Resnick.
2004.
Slash(dot) and burn:Distributed moderation in a large online conversation space.In Proceedings of ACM CHI 2004 Conference on HumanFactors in Computing Systems, Vienna Austria, pages 543?550.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a Large Annotated Corpusof English: The Penn Treebank.
Computational Linguistics,19(2):313?330.Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, MartinScholz, and Timm Euler.
2006.
YALE: Rapid prototypingfor complex data mining tasks.
In KDD ?06: Proceedings ofthe 12th ACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 935?940, New York,NY, USA.
ACM Press.Helmut Schmid.
1995.
Probabilistic Part-of-Speech TaggingUsing Decision Trees.
In International Conference on NewMethods in Language Processing, Manchester, UK.Salvatore Valenti, Francesca Neri, and Alessandro Cucchiarelli.2003.
An overview of current research on automated es-say grading.
Journal of Information Technology Education,2:319?329.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques.
Morgan Kaufmann,San Francisco, 2 edition.128
