Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 929?938,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsImproving Named Entity Recognition in Tweets via DetectingNon-Standard WordsChen Li and Yang LiuComputer Science Department, The University of Texas at DallasRichardson, Texas 75080, USA{chenli,yangl@hlt.utdallas.edu}AbstractMost previous work of text normalizationon informal text made a strong assumptionthat the system has already known whichtokens are non-standard words (NSW) andthus need normalization.
However, this isnot realistic.
In this paper, we proposea method for NSW detection.
In addi-tion to the information based on the dic-tionary, e.g., whether a word is out-of-vocabulary (OOV), we leverage novel in-formation derived from the normalizationresults for OOV words to help make deci-sions.
Second, this paper investigates twomethods using NSW detection results fornamed entity recognition (NER) in socialmedia data.
One adopts a pipeline strat-egy, and the other uses a joint decodingfashion.
We also create a new data setwith newly added normalization annota-tion beyond the existing named entity la-bels.
This is the first data set with suchannotation and we release it for researchpurpose.
Our experiment results demon-strate the effectiveness of our NSW detec-tion method and the benefit of NSW detec-tion for NER.
Our proposed methods per-form better than the state-of-the-art NERsystem.1 IntroductionShort text messages or comments from social me-dia websites such as Facebook and Twitter havebecome one of the most popular communicationforms in recent years.
However, abbreviations,misspelled words and many other non-standardwords are very common in short texts for vari-ous reasons (e.g., length limitation, need to con-vey much information, writing style).
They postproblems to many NLP techniques in this domain.There are many ways to improve language pro-cessing performance on the social media data.
Oneis to leverage normalization techniques to auto-matically convert the non-standard words into thecorresponding standard words (Aw et al, 2006;Cook and Stevenson, 2009; Pennell and Liu, 2011;Liu et al, 2012a; Li and Liu, 2014; Sonmez andOzgur, 2014).
Intuitively this will ease subsequentlanguage processing modules.
For example, if?2mr?
is converted to ?tomorrow?, a text-to-speechsystem will know how to pronounce it, a part-of-speech (POS) tagger can label it correctly, and aninformation extraction system can identify it as atime expression.
This normalization task has re-ceived an increasing attention in social media lan-guage processing.However, most of previous work on normaliza-tion assumed that they already knew which tokensare NSW that need normalization.
Then differ-ent methods are applied only to these tokens.
Toour knowledge, Han and Baldwin (2011) is theonly previous work which made a pilot research onNSW detection.
One straight forward method todo this is to use a dictionary to classify a token intoin-vocabulary (IV) words and out-of-vocabulary(OOV) words, and just treat all the OOV words asNSW.
The shortcoming of this method is obvious.For example, tokens like ?iPhone?, ?PES?
(a gamename) and ?Xbox?
will be considered as NSW,however, these words do not need normalization.Han and Baldwin (2011) called these OOV wordscorrect-OOV, and named those OOV words thatdo need normalization as ill-OOV.
We will followtheir naming convention and use these two termsin our study.
In this paper, we propose two meth-ods to classify tokens in informal text into threeclasses: IV, correct-OOV, and ill-OOV.
In the fol-lowing, we call this task the NSW detection task,and these three labels NSW labels or classes.
Thenovelty of our work is that we incorporate a to-ken?s normalization information to assist this clas-929sification process.
Our experiment results demon-strate that our proposed system gives a signifi-cant performance improvement on NSW detectioncompared with the dictionary baseline system.On the other hand, the impact of normalizationor NSW detection on NER has not been well stud-ied in social media domain.
In this paper, we pro-pose two methods to incorporate the NSW detec-tion information: one is a pipeline system that justuses the predicted NSW labels as additional fea-tures in an NER system; the other one uses jointdecoding, where we can simultaneously decide atoken?s NSW and NER labels.
Our experiment re-sults show that our proposed joint decoding per-forms better than the pipeline method, and it out-performs the state-of-the-art NER system.Our contributions in this paper are as follows:(1) We proposed a NSW detection model by lever-aging normalization information of the OOV to-kens.
(2) We created a data set with new NSWand normalization information, in addition to theexisting NER labels.
(3) It is the first time to ourknowledge that an effective and joint approach isproposed to combine the NSW detection and NERtechniques to improve the performance of thesetwo tasks at the same time on social media data.
(4) We demonstrate the effectiveness of our pro-posed method.
Our proposed NER system outper-forms the state-of-the-art system.2 Related WorkThere has been a surge of interest in lexical nor-malization with the advent of social media data.Lots of approaches have been developed for thistask, from using edit distance (Damerau, 1964;Levenshtein, 1966), to the noisy channel model(Cook and Stevenson, 2009; Pennell and Liu,2010; Liu et al, 2012a) and machine transla-tion method (Aw et al, 2006; Pennell and Liu,2011; Li and Liu, 2012b; Li and Liu, 2012a).Normalization performance on some benchmarkdata has been improved a lot.
Currently, unsuper-vised models are widely used to extract latent rela-tionship between non-standard words and correctwords from a huge corpus.
Hassan and Menezes(2013) applied the random walk algorithm on acontextual similarity bipartite graph, constructedfrom n-gram sequences on a large unlabeled textcorpus to build relation between non-standard to-kens and correct words.
Yang and Eisenstein(2013) presented a unified unsupervised statisticalmodel, in which the relationship between the stan-dard and non-standard words is characterized bya log-linear model, permitting the use of arbitraryfeatures.
Chrupa?a (2014) proposed a text normal-ization model based on learning edit operationsfrom labeled data while incorporating features in-duced from unlabeled data via recurrent networkderived character-level neural text embeddings.These studies only focused on how to normal-ize a given ill-OOV word and did not address theproblem of detecting an ill-OOV word.
Han andBaldwin (2011) is the only previous study thatconducted the detection work.
For any OOV word,they replaced it with its possible correct candi-date, then if the possible candidate together withOOV?s original context adheres to the knowledgethey learned from large formal corpora, the re-placement could be considered as a better choiceand that OOV token is classified as ill-OOV.
In thispaper, we propose a different method for NSWdetection.
Similar to (Han and Baldwin, 2011),we also use normalization information for OOVwords, but we use a feature based learning ap-proach.In order to improve robustness of NLP mod-ules in social media domain, some works choseto design specific linguistic information.
For ex-ample, by designing or annotating POS, chunkingand capitalized information on tweets, (Ritter etal., 2011) proposed a system which reduced thePOS tagging error by 41% compared with Stan-ford POS Tagger, and by 50% in NER comparedwith the baseline systems.
Gimpel et al (2011)created a specific set of POS tags for twitter data.With this tag set and word cluster information ex-tracted from a huge Twitter corpus, their proposedsystem obtained significant improvement on POStagging accuracy in Twitter data.At the same time, increasing research work hasbeen done to integrate lexical normalization intothe NLP tasks in social media data.
Kaji and Kit-suregawa (2014) combined lexical normalization,word segmentation and POS tagging on Japanesemicroblog.
They used rich character-level andword-level features from the state-of-the-art mod-els of joint word segmentation and POS taggingin Japanese (Kudo et al, 2004; Neubig et al,2011).
Their model can also be trained on a par-tially annotated corpus.
Li and Liu (2015) con-ducted a similar research on joint POS taggingand text normalization for English.
Wang and Kan930(2013) proposed a method of joint ill-OOV wordrecognition and word segmentation in Chinese Mi-croblog.
But with their method, ill-OOV wordsare merely recognized and not normalized.
There-fore, they did not investigate how to exploit theinformation that may be derived from normaliza-tion to increase word segmentation accuracy.
Liuet al (2012b) studied the problem of named entitynormalization (NEN) for tweets.
They proposed anovel graphical model to simultaneously conductNER and NEN on multiple tweets.
Although thiswork involved text normalization, it only focusedon the NER task, and there was no reported re-sult for normalization.
On Turkish tweets, Ku-cuk and Steinberger (2014) adapted NER rulesand resources to better fit Twitter language by re-laxing its capitalization constraint, expanding itslexical resources based on diacritics, and using anormalization scheme on tweets.
These showedpositive effect on the overall NER performance.Rangarajan Sridhar et al (2014) decoupled theSMS translation task into normalization followedby translation.
They exploited bi-text resources,and presented a normalization approach using dis-tributed representation of words learned throughneural networks.In this study, we propose new methods to ef-fectively integrate information of OOV words andtheir normalization for the NER task.
In particu-lar, by adopting joint decoding for both NSW de-tection and NER, we are able to outperform state-of-the-art results for both tasks.
This is the firststudy that systematically evaluates the effect ofOOV words and normalization on NER in socialmedia data.3 Proposed Method3.1 NSW Detection MethodsThe task of NSW detection is to find those wordsthat indeed need normalization.
Note that inthis study we only consider single-token ill-OOVwords (both before and after normalization).
Forexample, we would consider snds (sounds) as ill-OOV, but not smh (shaking my head).For a data set, our annotation process is as fol-lows.
We first manually label whether a token isill-OOV and if so its corresponding standard word.We only consider tokens consisting of alphanu-meric characters.
Then based on a dictionary, thetokes that are not labeled as ill-OOV can be cat-egorized into IV and OOV words.
These OOVwords will be considered as correct-OOV.
There-fore all the tokens will have these three labels: IV,ill-OOV, and correct-OOV.Throughout this paper, we use GNU spell dic-tionary (v0.60.6.1) to determine whether a token isOOV.1Twitter mentions (e.g., @twitter), hashtagsand urls are excluded from consideration for OOV.Dictionary lookup of Internet slang2is performedto filter those ill-OOV words whose correct formsare not single words.We propose two methods for NSW detection.The first one is a two-step method, where we firstlabel a token as IV or OOV based on the givendictionary and some filter rules, then a statisticalclassifier is applied on those OOV tokens to fur-ther decide their classes: ill-OOV or correct-OOV.We use a maximum entropy classifier for this.
Thesecond model directly does 3-way classification topredict a token?s label to be IV, correct-OOV, orill-OOV.
We use a CRF model in this method.3Table 1 shows the features used in these twomethods.
The first dictionary feature is not appli-cable for the two-step method because all the in-stances in that process have the same feature value?OOV?.
However, this dictionary feature is an im-portant feature for the 3-way classification model?
a token with a feature value ?IV?
has a very highprobability of being ?IV?.
Lexical features focuson a token?s surface information to judge whetherit is a regular English word or not.
It is becausemost of correct-OOV words (e.g., location andperson names) are still some regular words, com-plying with the general rules of word formation.For example, features 5-8 consider English wordformation rules that at least one vowel characteris needed for a correct word4.
Feature 9 consid-ers that a correct English word does not containmore than three consecutive same character.
Thecharacter level language model used in Feature 10is trained from a dictionary.
A higher probabilitymay indicate that it is a correct word.The motivation for the normalization features is1We remove all the one-character tokens, except a and I.25452 items are collected from http://www.noslang.com.3We can also use a maximum entropy classifier to imple-ment this model.
Our experiments showed that using CRFshas slightly better results.
But the main reason we adoptCRFs is because we use CRFs for NER, therefore we caneasily integrate the two models in joint decoding in Section3.2 for NER and NSW detection.
We do not use CRFs inthe two-step system because the labeling is performed on asubset of the words, not the entire sequence.4Although some exceptions exist, this rule applies to mostwords.931Dictionary Feature1.
is token categorized as IV or OOV by thegiven dictionary (Only used in 3-way classifi-cation)Lexical Features2.
word identity3.
whether token?s first character is capitalized4.
token?s length5.
how many vowel character chunks does thistoken have6.
how many consonant character chunks doesthis token have7.
the length of longest consecutive vowelcharacter chunk8.
the length of longest consecutive consonantcharacter chunk9.
whether this token contains more than 3 con-secutive same character10.
character level probability of this tokenbased on a character level language modelNormalization Features11.
whether each individual candidate list hasany candidates for this token12.
how many candidates each individual can-didate list has13.
whether each individual list?s top 10 candi-dates contain this token itself14.
the max number of lists that have the sametop one candidate15.
the similarity value between each in-dividual normalization system?s first candi-date w and this token t, calculated bylongest common string(w,t)length(t)16. the similarity value between each in-dividual normalization system?s first candi-date w and this token t, calculated bylongest common sequence(w,t)length(t)Table 1: Features used in NSW detection system.to leverage the normalization result of an OOV to-ken to help its classification.
Before we describethe reason why normalization information couldbenefit this task, we first introduce the normal-ization system we used.
We apply a state-of-the-art normalization system proposed by (Li and Liu,2014).
Briefly, in this normalization system thereare three supervised and two unsupervised sub-systems for each OOV token, resulting in six can-didate lists (one system provides two lists).
Thena maximum entropy reranking model is adoptedto combine and rerank these candidate lists, usinga rich set of features.
Please refer to (Li and Liu,2014) for more details.
By analyzing each individ-ual system, we find that for ill-OOV words mostnormalization systems can generate many candi-dates, which may contain a correct candidate; forcorrect-OOV words, many normalization systemshave few candidates or may not provide any can-didates.
For example, only two of the six lists havecandidates for the token Newsfeed and Metropcs.Therefore, we believe the patterns of these normal-ization results contain useful information to clas-sify OOVs.
Note that this kind of feature is onlyapplicable for those tokens that are judged as OOVby the given dictionary (normalization is done onthese OOV words).
The bottom of Table 1 showsthe normalization features we designed.3.2 NER MethodsThe NER task we study in this paper is just aboutsegmenting named entities, without identifyingtheir types (e.g., person, location, organization).Following most previous work, we model it as asequence-labeling task and use the BIO encodingmethod (each word either begins, is inside, or out-side of a named entity).Intuitively, NSW detection has an impact onNER, because many named entities may have thecorrect-OOV label.
Therefore, we investigate ifwe can leverage NSW label information for NER.First, we adopt a pipeline method, where we firstperform NSW detection and the results are usedas features in the NER system.
Table 2 shows thefeatures we designed.
One thing worth mentioningis that the POS tags we used are from (Gimpel etal., 2011).
This POS tag set consists of 25 coarse-grained tags designed for social media text.
Weuse CRFs for this NER system.The above method simply incorporates a to-ken?s predicted NSW label as features in the NERmodel.
Obviously it has an unavoidable limitation?
the errors from the NSW detection model wouldaffect the downstream NER process.
Therefore wepropose a second method, a joint decoding processto determine a token?s NSW and NER label at thesame time.
The 3-way classification method forNSW detection and the above NER system bothuse CRFs.
The decoding process for these twotasks is performed separately, using their corre-sponding trained models.
The motivation of ourproposed joint decoding process is to combine the932two processes together, therefore we can avoid theerror propagation in the pipeline system, and allowthe two models to benefit from each other.Part (A) and (B) of Figure 1 show the trellis fordecoding word sequence ?Messi is well-known?
inthe NER and NSW detection systems respectively.As shown in (A), every black box with dashed lineis a hidden state (possible BIO tag) for the corre-sponding token.
Two sources of information areused in decoding.
One is the label transition prob-ability p(yi|yj), from the trained model, where yiand yjare two BIO tags.
The other is p(yi|ti),where yiis a BIO label for token ti.
Similarly,during decoding in NSW detection, we need theBasic Features1.
Lexical features (word n-gram):Unigram: Wi(i = 0)Bigram: WiWi+1(i = ?2,?1, 0, 1)Trigram: Wi?1WiWi+1(i = ?2,?1, 0, 1)2.
POS features (POS n-gram):Unigram: Pi(i = 0)Bigram: PiPi+1(i = ?2,?1, 0, 1)Trigram: Pi?1PiPi+1(i = ?2,?1, 0, 1)3.
Token?s capitalization information:Trigram: Ci?1CiCi+1(i = 0) (Ci= 1 meansthis token?s first character is capitalized.
)Additional Features by Incorporating Pre-dicted NSW Label4.
Token?s dictionary categorization label:Unigram: Di(i = 0)Bigram: DiDi+1(i = ?2,?1, 0, 1)Trigram: Di?1DiDi+1(i = ?2,?1, 0, 1)5.
Token?s predicted NSW label:Unigram: Li(i = 0)Bigram: LiLi+1(i = ?2,?1, 0, 1)Trigram: Li?1LiLi+1(i = ?2,?1, 0, 1)6.
Compound features using lexical and NSWlabels: WiDi,WiLi,WiDiLi(i = 0)7.
Compound features using POS and NSWlabels: PiDi, PiLi, PiDiLi(i = 0)8.
Compound features using word, POS, andNSW labels:WiPiDiLi(i = 0)Table 2: Features used in the NER System.
Wand P represent word and POS.
D and L representlabels classified by the dictionary and 3-way NSWdetection system.
Subscripts i, i ?
1 and i + 1indicate the word position.
For example, when iequals to -1, i+ 1 means the current word.probability of p(oi|oj) and p(oi|ti).
The only dif-ference is that oiis a NSW label.
Part (C) of Figure1 shows the trellis used in our proposed joint de-coding approach for NSW detection and NER.
Inthis figure, three places are worth pointing out: (1)the label is a combination of NSW and NER la-bels, and thus there are nine in total; (2) the labeltransition probability is a linear sum of the previ-ous two transition probabilities: p(yioi|yjoj) =p(yi|yj) + ?
?
p(oi|oj), where yiand yjare BIOtags and oiand ojare NSW tags; (3) similarly,p(yioi|ti) equals to p(yi|ti)+?
?
p(oi|ti).
Pleasenote all these probabilities are log probabilitiesand they are trained separately from each system.4 Data and Experiment4.1 Data Set and Experiment SetupThe NSW detection model is trained using the datareleased by (Li and Liu, 2014).
It has 2,577 Twit-ter messages (selected from the Edinburgh Twit-ter corpus (Petrovic et al, 2010)), in which thereare 2,333 unique pairs of NSW and their standardwords.
This data is used for training the differentnormalization models.
We labeled this data set us-ing the given dictionary for NSW detection.
4,121tokens are labeled as ill-OOV, 1,455 as correct-OOV, and the rest 33,740 tokens are IV words.We have two test sets for evaluating the NSWdetection system.
One is from (Han and Baldwin,2011), which includes 549 tweets.
Each tweetcontains at least one ill-OOV and the correspond-ing correct word.
We call it Test set 1 in the fol-lowing.
The other is from (Li and Liu, 2015), whofurther processed the tweets data from (Owoputiet al, 2013).
Briefly, Owoputi et al (2013) re-leased 2,347 tweets with their designed POS tagsfor social media text, and then Li and Liu (2015)further annotated this data with normalization in-formation for each token.
The released data by (Liand Liu, 2015) contains 798 tweets with ill-OOV.We use these 798 tweets as the second data set forNSW detection, and call it Test set 2 in the follow-ing.
In addition, we use all of these 2,347 tweetsto train a POS model which then is used to predicttokens?
POS tags for NER (see Section 3.2 aboutthe POS tags).
The CRF model is implemented us-ing the pocket-CRF toolkit5.
The SRILM toolkit(Stolcke, 2002) is used to build the character-levellanguage model (LM) for generating the LM fea-tures in NSW detection system.5http://sourceforge.net/projects/pocket-crf-1/933is Messip(B|B)well-knownp(B|Messi)BIOBIOBIOp(I|Messi)p(O|Messi)p(I|B)p(O|B)is Messip(IV|IV)well-knownp(correct-OOV|Messi)IVcorrect-OOVIll-OOVIVcorrect-OOVIll-OOVIVcorrect-OOVIll-OOVp(Ill-OOV|Messi)p(IV|Messi)p(correct-OOV|IV)p(ill-OOV|IV)Messip(B|Messi)+ ?
* p(IV|Messi)B_IVB_correct-OOVB_ill-OOVI_IV...p(B|is)+ ?
* p(correct-OOV|is)B_IVB_correct-OOVB_ill-OOVI_IVisp(B|B)+ ?
* p(IVIIV)p(I|B)+ ?
* p(IVIIV)...p(B|well-known)+?
* p(ill-OOV|well-known)B_IVB_correct-OOVB_ill-OOVI_IVwell-known...p(B|B)+ ?
* p(ill-OOVIIV)p(I|B)+ ?
* p(IVIill-OOV)(C)(B) (A)Figure 1: Trellis Viterbi decoding for different systems.The data with the NER labels are from (Ritteret al, 2011) who annotated 2,396 tweets (34K to-kens) with named entities, but there is no infor-mation on the tweets?
ill-OOV words.
In order toevaluate the impact of ill-OOV on NER, we ask sixannotators to annotate the ill-OOV words and thecorresponding standard words in this data.
Thereare only 1,012 sentences with ill-OOV words.
Weuse all the sentences (2,396) for the NER exper-iments.
This data set,6to our knowledge, is thefirst one having both ill-OOV and NER annotationin social media domain.
For joint decoding, theparameters ?
and ?
are empirically set as 0.95 and0.5.4.2 Experiment Results4.2.1 NSW Detection ResultsFor NSW detection, we compared our two pro-posed systems on the two test sets describedabove, and also conducted different experiments toinvestigate the effectiveness of different features.We use the categorization of words by the dictio-nary as the baseline for this task.
Table 3 shows theresults for three NSW detection systems.
We useRecall, Precision and F value for the ill-OOV classas the evaluation metrics.
The Dictionary base-line can only recognize the token as IV and OOV,and thus label all the OOV words as ill-OOV.
Boththe two-step and the 3-way classification meth-ods in Table 3 leverage all the features described6http://www.hlt.utdallas.edu/?chenli/normalization nerin Table 1.
First note because of the property ofthe two-step method (it further divides the OOVwords from the dictionary-based method into ill-OOV and correct-OOV), the upper bound of itsrecall is the recall of the dictionary based method.We can see that in Test set 1, both the two-step andthe 3-way classification methods have a significantimprovement compared to the Dictionary method.However, in Test set 2, the two-step method per-forms much worse than that of the 3-way classifi-cation method, although it outperforms the dictio-nary method.
This can be attributed to the charac-teristics of that data set and also the system?s upperbounded recall.
We will provide a more detailedanalysis in the following feature analysis part.Table 4 and 5 show the performance of the twosystems on the two test sets with different features.Note that the dictionary feature is not applicable tothe two-step method, and the results for the two-step method using dictionary feature (feature 1,first line in the tables) are the same as the dictio-nary baseline in Table 3.
From these two tables,we can see that: (1) For both systems, normaliza-tion features (11?16) and lexical features (2?10)both perform better than the dictionary feature.
(2)In general, the combination of any two kinds offeatures has better performance than any one fea-ture type.
Using all the features (results shown inTable 3) yields the best performance, which signif-icantly improves the performance compared withthe baseline.
(3) There are some differences across934the two data sets in terms of the feature effective-ness on the two methods.
On Test set 2, whenlexical features are combined with other features(forth and fifth line of Table 5), the 3-way classifi-cation method significantly outperforms the two-step method.
It is because this data set has alarge number of ill-OOV words that are dictionarywords.
For example, token ?its?
appears 31 timesas ill-OOV, ?ya?
13 times, and ?bro?
10 times.
Suchill-OOV words occur more than two hundred timesin total.
Since these tokens are included in the dic-tionary, they are already classified as IV by thedictionary, and their label will not change in thesecond step.
This is also the reason why in Table3, the performance of 3-way classification is sig-nificantly better than that of the two-step methodusing all the features.
However, we also find thatwhen we only use lexical features (2?10), the twomethods have similar performance on Test set 2,but the two-step method has much better perfor-mance than the 3-way classifier method on Testset 1.
We believe this shows that lexical featuresthemselves are not reliable for the NSW detectiontask, and other information such as normalizationfeatures may be more stable.SystemTest Set 1 Test Set 2R P F R P FDictionary 88.73 72.35 79.71 67.87 69.59 68.72Two-step 81.66 88.74 85.05 57.60 90.04 70.263-way 87.63 83.49 85.51 73.53 90.42 81.10Table 3: NSW detection results.FeaturesTwo-Step 3-way ClassificationR P F R P F1 88.73 72.35 79.71 87.13 70.04 77.662?10 87.21 77.44 82.04 82.59 67.49 74.2811?16 86.45 78.77 82.43 91.75 74.97 82.511?10 76.78 92.87 84.07 77.12 93.09 84.362?16 81.16 89.02 84.90 87.13 86.54 85.301,11?16 78.30 91.00 84.17 78.55 93.77 85.48Table 4: Feature impact on NSW detection on TestSet 1.
The feature number corresponds to that inTable 1.4.2.2 NER ResultsFor the NER task, in order to make a fair compari-son with (Ritter et al, 2011), we conducted 4-foldcross validation experiments as they did.
First wepresent the result on the NSW detection task onthis date set when using our proposed joint de-FeaturesTwo-Step 3-way ClassificationR P F R P F1 67.86 69.59 68.72 66.45 64.27 65.342?10 64.33 79.52 71.12 69.56 76.26 72.7611?16 53.78 91.34 67.70 54.35 91.42 68.171?10 63.12 81.53 71.16 78.41 81.65 80.002?16 56.40 89.02 69.06 72.32 90.28 80.311,11?16 56.40 92.35 70.03 56.68 92.81 70.38Table 5: Feature impact on NSW detection on TestSet 2.coding method integrating NER and NSW.
Thisis done using the 1,012 sentences that contain ill-OOV words.
Table 6 shows such results on theNER data described in Section 4.1.
The 3-wayclassification method for NSW detection is usedas a baseline here.
It is the same model as usedin the previous section, and applied to the entireNER data.
For each cross validation experimentof the joint decoding method, the NSW detectionmodel is kept the same (from 3-way classifica-tion method), but NER model is tested on 1/4 ofthe data and trained from the remaining 3/4 of thedata.
From the Table 6, we can see that joint de-coding yields some marginal improvement for theNSW detection task.System R P F3-way classification 58.65 72.83 64.97Joint decoding w all features 59.53 72.96 65.56Table 6: NSW detection results on the data from(Ritter et al, 2011) with our new NSW annotation.In the following, we will focus on the impactof NSW detection on NER.
Table 7 shows theNER performance from different systems on thedata with NER and NSW labels.
From this table,we can see that when using our pipeline system,adding NSW label features has a significant im-provement compared to the basic features.
The Fvalue of 67.4% when using all the features is evenhigher than the state-of-the-art performance from(Ritter et al, 2011).
Please note that Ritter et al(2011) used much more information than us forthis task, such as dictionaries including a set oftype lists gathered from Freebase, brown clusters,and outputs of their specifically designed chunkand capitalization labels components7.
Then they7The chunk and capitalization components are speciallycreated by them for social media domain data.
Then theycreated a data set to train these models.935improved their baseline performance from 65% tothe reported best one at 67%.
However, we onlyadded our predicted NSW labels and related fea-tures, and we already achieved similar or slightlybetter results.
Using joint decoding can furtherboost the performance to 69%.System R P FPipeline w basic features 55.85 74.33 63.76Pipeline w all features 60.00 77.09 67.40Joint decoding w all features 73.56 65.02 69.00(Ritter et al, 2011) 73.00 61.00 67.00Table 7: NER results from different systems ondata from (Ritter et al, 2011).Table 8 shows the impact of different features.This analysis is based on the pipeline system.First, we can see that adding feature 4 and 5 (Uni-,Bi- and Tri-gram of the dictionary and predictedNSW labels) yields the most improvement com-pared with other features, and between these twokinds of features, using predicted NSW labels isbetter than the dictionary labels.
It also shows theeffectiveness of our NSW detection system.
Sec-ond, comparing adding feature 6 and 7, it showsthat combination of word/POS and its dictionaryor NSW label is not as good as only consideringthe label?s n-gram.
We also explored various othern-gram features, but did not find any that outper-formed feature 4 or 5.
Another finding is that thePOS related features are not as good as that ofwords.Features R P FBasic 55.85 74.33 63.76Basic + 4 57.71 75.04 65.23Basic + 5 57.47 75.87 65.37Basic + 6 56.53 74.20 64.12Basic + 7 56.13 74.66 64.06Basic + 8 57.14 74.55 64.66Table 8: Pipeline NER performance using differ-ent features.
The feature number corresponds tothat in Table 2.4.2.3 Error AnalysisA detailed error analysis further shows what im-provement our proposed method makes and whaterrors it is still making.
For example, for thetweet ?Watching the VMA pre-show again ...?, thetoken VMA is annotated as B-tvshow in NER la-bels.
Without using predicted NSW labels, thebaseline system labels this token as O (outside ofnamed entity).
However, after using the NSW pre-dicted label correct-OOV and related features, thepipeline NER system predicts its label as B. Wenoticed that joint decoding can solve some com-plicated cases that are hard for the pipeline sys-tem, especially for some OOVs, or when there areconsecutive named entity tokens.
For example, ina tweet, ?Let?s hope the Serie A continues to beon the tv schedule next week?, Seria A is a propernoun (meaning Italian soccer league).
The anno-tation for Seria and A is correct-OOV/B and IV/I.We find the joint decoding system successfully la-bels A as I after Seria is labeled as B.
However, thepipeline system labels A as O even it correctly la-bels Seria.
Take another example, in a tweet ?I wasgonna buy a Zune HD ...?, Zune HD is consecutivenamed entities.
The pipeline system recognizedZune as correct-OOV and HD as ill-OOV, thenlabeled both them as O.
But the joint decodingsystem identified HD as correct-OOV and labeled?Zune HD?
as B and I.
These changes may havehappened because of adjusting the transition prob-ability and observation probability during Viterbidecoding.5 Conclusion and Future WorkIn this paper, we proposed an approach to detectNSW.
This makes the lexical normalization taskas a complete applicable process.
The proposedNSW detection system leveraged normalizationinformation of an OOV and other useful lexicalinformation.
Our experimental results show bothkinds of information can help improve the predic-tion performance on two different data sets.
Fur-thermore, we applied the predicted labels as ad-ditional information for the NER task.
In thistask, we proposed a novel joint decoding approachto label every token?s NSW and NER label in atweet at the same time.
Again, experimental re-sults demonstrate that the NSW label has a sig-nificant impact on NER performance and our pro-posed method improves performance on both tasksand outperforms the best previous results in NER.In future work, we propose to pursue a numberof directions.
First, we plan to consider how toconduct NSW detection and normalization at thesame time.
Second, we like to try a joint method to936simultaneously train the NSW detection and NERmodels, rather than just combining models in de-coding.
Third, we want to investigate the impact ofNSW and normalization on other NLP tasks suchas parsing in social media data.AcknowledgmentsWe thank the anonymous reviewers for their de-tailed and insightful comments on earlier draftsof this paper.
The work is partially supported byDARPA Contract No.
FA8750-13-2-0041.
Anyopinions, findings, and conclusions or recommen-dations expressed are those of the authors and donot necessarily reflect the views of the fundingagencies.ReferencesAiti Aw, Min Zhang, Juan Xiao, Jian Su, and Jian Su.2006.
A phrase-based statistical model for sms textnormalization.
In Processing of COLING/ACL.Grzegorz Chrupa?a.
2014.
Normalizing tweets withedit scripts and recurrent neural embeddings.
InProceedings of ACL.Paul Cook and Suzanne Stevenson.
2009.
An unsu-pervised model for text message normalization.
InProceedings of NAACL.Fred J Damerau.
1964.
A technique for computer de-tection and correction of spelling errors.
Communi-cations of the ACM, 7(3):171?176.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor twitter: Annotation, features, and experiments.In Proceedings of ACL.Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a #twitter.In Proceeding of ACL.Hany Hassan and Arul Menezes.
2013.
Social textnormalization using contextual graph random walks.In Proceedings of ACL.Nobuhiro Kaji and Masaru Kitsuregawa.
2014.
Accu-rate word segmentation and pos tagging for Japanesemicroblogs: Corpus annotation and joint model-ing with lexical normalization.
In Proceedings ofEMNLP.Dilek Kucuk and Ralf Steinberger.
2014.
Experi-ments to improve named entity recognition on turk-ish tweets.
In Proceedings of Workshop on Lan-guage Analysis for Social Media (LASM) on EACL.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In Proceedings ofEMNLP.Vladimir I Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions and reversals.
InSoviet physics doklady, volume 10, page 707.Chen Li and Yang Liu.
2012a.
Improving text nor-malization using character-blocks based models andsystem combination.
In Proceedings of COLING2012.Chen Li and Yang Liu.
2012b.
Normalization of textmessages using character- and phone-based machinetranslation approaches.
In Proceedings of 13th In-terspeech.Chen Li and Yang Liu.
2014.
Improving text normal-ization via unsupervised model and discriminativereranking.
In Proceedings of ACL.Chen Li and Yang Liu.
2015.
Joint POS tagging andtext normalization for informal text.
In Proceedingsof IJCAI.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012a.
Abroad-coverage normalization system for social me-dia language.
In Proceedings of ACL.Xiaohua Liu, Ming Zhou, Xiangyang Zhou,Zhongyang Fu, and Furu Wei.
2012b.
Jointinference of named entity recognition and normal-ization for tweets.
In Proceedings of ACL.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptableJapanese morphological analysis.
In Proceedings ofACL.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL.Deana Pennell and Yang Liu.
2010.
Normalization oftext messages for text-to-speech.
In ICASSP.Deana Pennell and Yang Liu.
2011.
A character-levelmachine translation approach for normalization ofsms abbreviations.
In Proceedings of IJCNLP.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.2010.
The Edinburgh twitter corpus.
In Proceed-ings of NAACL.Vivek Kumar Rangarajan Sridhar, John Chen, SrinivasBangalore, and Ron Shacham.
2014.
A frameworkfor translating SMS messages.
In Proceedings ofCOLING.Alan Ritter, Sam Clark, and Oren Etzioni.
2011.Named entity recognition in tweets: an experimentalstudy.
In Proceedings of EMNLP.937Cagil Sonmez and Arzucan Ozgur.
2014.
A graph-based approach for contextual text normalization.
InProceedings of EMNLP.Andreas Stolcke.
2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference on Spoken Language Processing.Aobo Wang and Min-Yen Kan. 2013.
Mining informallanguage from Chinese microtext: Joint word recog-nition and segmentation.
In Proceedings of ACL.Yi Yang and Jacob Eisenstein.
2013.
A log-linearmodel for unsupervised text normalization.
In Pro-ceedings of EMNLP.938
