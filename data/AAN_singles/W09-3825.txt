Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 162?171,Paris, October 2009. c?2009 Association for Computational LinguisticsTransition-Based Parsing of the Chinese Treebank using a GlobalDiscriminative ModelYue ZhangOxford UniversityComputing Laboratoryyue.zhang@comlab.ox.ac.ukStephen ClarkCambridge UniversityComputer Laboratorystephen.clark@cl.cam.ac.ukAbstractTransition-based approaches have showncompetitive performance on constituentand dependency parsing of Chinese.
State-of-the-art accuracies have been achievedby a deterministic shift-reduce parsingmodel on parsing the Chinese Treebank 2data (Wang et al, 2006).
In this paper,we propose a global discriminative modelbased on the shift-reduce parsing process,combined with a beam-search decoder, ob-taining competitive accuracies on CTB2.We also report the performance of theparser on CTB5 data, obtaining the highestscores in the literature for a dependency-based evaluation.1 IntroductionTransition-based statistical parsing associatesscores with each decision in the parsing process,selecting the parse which is built by the highestscoring sequence of decisions (Briscoe and Car-roll, 1993; Nivre et al, 2006).
The parsing algo-rithm is typically some form of bottom-up shift-reduce algorithm, so that scores are associatedwith actions such as shift and reduce.
One ad-vantage of this approach is that the parsing can behighly efficient, for example by pursuing a greedystrategy in which a single action is chosen at eachdecision point.The alternative approach, exemplified byCollins (1997) and Charniak (2000), is to usea chart-based algorithm to build the space ofpossible parses, together with pruning of low-probability constituents and the Viterbi algorithmto find the highest scoring parse.
For English de-pendency parsing, the two approaches give similarresults (McDonald et al, 2005; Nivre et al, 2006).For English constituent-based parsing using thePenn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie, 2005).
In contrast, for Chi-nese, the best dependency parsers are currentlytransition-based (Duan et al, 2007; Zhang andClark, 2008).
For constituent-based parsing usingthe Chinese Treebank (CTB), Wang et al (2006)have shown that a shift-reduce parser can givecompetitive accuracy scores together with highspeeds, by using an SVM to make a single decisionat each point in the parsing process.In this paper we describe a global discrimina-tive model for Chinese shift-reduce parsing, andcompare it with Wang et al?s approach.
We ap-ply the same shift-reduce procedure as Wang etal.
(2006), but instead of using a local classifierfor each transition-based action, we train a gener-alized perceptron model over complete sequencesof actions, so that the parameters are learned inthe context of complete parses.
We apply beamsearch to decoding instead of greedy search.
Theparser still operates in linear time, but the use ofbeam-search allows the correction of local deci-sion errors by global comparison.
Using CTB2,our model achieved Parseval F-scores comparableto Wang et al?s approach.
We also present accu-racy scores for the much larger CTB5, using botha constituent-based and dependency-based evalu-ation.
The scores for the dependency-based eval-uation were higher than the state-of-the-art depen-dency parsers for the CTB5 data.2 The Shift-Reduce Parsing ProcessThe shift-reduce process used by our beam-searchdecoder is based on the greedy shift-reduce parsersof Sagae and Lavie (2005) and Wang et al (2006).162The process assumes binary-branching trees; sec-tion 2.1 explains how these are obtained from thearbitrary-branching trees in the Chinese Treebank.The input is assumed to be segmented and POStagged, and the word-POS pairs waiting to be pro-cessed are stored in a queue.
A stack holds thepartial parse trees that are built during the parsingprocess.
A parse state is defined as a ?stack,queue?pair.
Parser actions, including SHIFT and variouskinds of REDUCE, define functions from states tostates by shifting word-POS pairs onto the stackand building partial parse trees.The actions used by the parser are:?
SHIFT, which pushes the next word-POS pairin the queue onto the stack;?
REDUCE?unary?X, which makes a newunary-branching node with label X; the stackis popped and the popped node becomes thechild of the new node; the new node is pushedonto the stack;?
REDUCE?binary?
{L/R}?X, which makes anew binary-branching node with label X; thestack is popped twice, with the first poppednode becoming the right child of the newnode and the second popped node becomingthe left child; the new node is pushed onto thestack;?
TERMINATE, which pops the root node offthe stack and ends parsing.
This actionis novel in our parser.
Sagae and Lavie(2005) and Wang et al (2006) only used thefirst three transition actions, setting the fi-nal state as all incoming words having beenprocessed, and the stack containing only onenode.
However, there are a small number ofsentences (14 out of 3475 from the trainingdata) that have unary-branching roots.
Forthese sentences, Wang?s parser will be unableto produce the unary-branching roots becausethe parsing process terminates as soon as theroot is found.
We define a separate action toterminate parsing, allowing unary reduces tobe applied to the root item before parsing fin-ishes.The trees built by the parser are lexicalized, us-ing the head-finding rules from Zhang and Clark(2008).
The left (L) and right (R) versions of theREDUCE-binary rules indicate whether the head offor node Y = X1..Xm ?
T :if m > 2 :find the head node Xk(1 ?
k ?
m) of Ym?
= mwhile m?
> k and m?
> 2 :new node Y ?
= X1..Xm?
?1Y ?
Y ?Xm?m?
= m?
?
1n?
= 1while n?
< k and k ?
n?
> 1 :new node Y ?
= Xn?
..XkY ?
Xn?Y ?n?
= n?
+ 1Figure 2: the binarization algorithm with input Tthe new node is to be taken from the left or rightchild.
Note also that, since the parser is buildingbinary trees, the X label in the REDUCE rules canbe one of the temporary constituent labels, suchas NP?, which are needed for the binarization pro-cess described in Section 2.1.
Hence the numberof left and right binary reduce rules is the numberof constituent labels in the binarized grammar.Wang et al (2006) give a detailed exampleshowing how a segmented and POS-tagged sen-tence can be incrementally processed using theshift-reduce actions to produce a binary tree.
Weshow this example in Figure 1.2.1 The binarization processThe algorithm in Figure 2 is used to map CTBtrees into binarized trees, which are required bythe shift-reduce parsing process.
For any tree nodewith more than two child nodes, the algorithmworks by first finding the head node, and then pro-cessing its right-hand-side and left-hand-side, re-spectively.
The head-finding rules are taken fromZhang and Clark (2008).
Y = X1..Xm representsa tree node Y with child nodes X1...Xm(m ?
1).The label of the newly generated node Y ?
isbased on the constituent label of the original nodeY , but marked with an asterix.
Hence binariza-tion enlarges the set of constituent labels.
Wecall the constituents marked with ?
temporary con-stituents.
The binarization process is reversible, inthat output from the shift-reduce parser can be un-binarized into CTB format, which is required forevaluation.163Figure 1: An example shift-reduce parsing process, adopted from Wang et al (2006)2.2 Restrictions on the sequence of actionsNot all sequences of actions produce valid bina-rized trees.
In the deterministic parser of Wang etal.
(2006), the highest scoring action predicted bythe classifier may prevent a valid binary tree frombeing built.
In this case, Wang et al simply returna partial parse consisting of all the subtrees on thestack.In our parser a set of restrictions is appliedwhich guarantees a valid parse tree.
For example,two simple restrictions are that a SHIFT action canonly be applied if the queue of incoming words164Variables: state item item = (S,Q), whereS is stack and Q is incoming queue;the agenda agenda;list of state items next;Algorithm:for item ?
agenda:if item.score = agenda.bestScore anditem.isFinished:rval = itembreaknext = []for move ?
item.legalMoves:next.push(item.TakeAction(move))agenda = next.getBBest()Outputs: rvalFigure 3: the beam-search decoding algorithmis non-empty, and the binary reduce actions canonly be performed if the stack contains at least twonodes.
Some of the restrictions are more complexthan this; the full set is listed in the Appendix.3 Decoding with Beam SearchOur decoder is based on the incremental shift-reduce parsing process described in Section 2.
Weapply beam-search, keeping the B highest scoringstate items in an agenda during the parsing pro-cess.
The agenda is initialized with a state itemcontaining the starting state, i.e.
an empty stackand a queue consisting of all word-POS pairs fromthe sentence.At each stage in the decoding process, existingitems from the agenda are progressed by applyinglegal parsing actions.
From all newly generatedstate items, the B highest scoring are put back onthe agenda.
The decoding process is terminatedwhen the highest scored state item in the agendareaches the final state.
If multiple state items havethe same highest score, parsing terminates if anyof them are finished.
The algorithm is shown inFigure 3.4 Model and Learning AlgorithmWe use a linear model to score state items.
Recallthat a parser state is a ?stack,queue?
pair, with thestack holding subtrees and the queue holding in-coming words waiting to be processed.
The scoreInputs: training examples (xi, yi)Initialization: set ~w = 0Algorithm:for t = 1..T , i = 1..N :zi = parse(xi, ~w)if zi 6= yi:~w = ~w +?(yi)?
?
(zi)Outputs: ~wFigure 4: the perceptron learning algorithmfor state item Y is defined by:Score(Y ) = ~w ?
?
(Y ) =?i?i fi(Y )where ?
(Y ) is the global feature vector from Y ,and ~w is the weight vector defined by the model.Each element from ?
(Y ) represents the globalcount of a particular feature from Y .
The featureset consists of a large number of features whichpick out various configurations from the stack andqueue, based on the words and subtrees in the stateitem.
The features are described in Section 4.1.The weight values are set using the generalizedperceptron algorithm (Collins, 2002).The perceptron algorithm is shown in Figure 4.It initializes weight values as all zeros, and usesthe current model to decode training examples (theparse function in the pseudo-code).
If the outputis correct, it passes on to the next example.
Ifthe output is incorrect, it adjusts the weight val-ues by adding the feature vector from the gold-standard output and subtracting the feature vectorfrom the parser output.
Weight values are updatedfor each example (making the process online) andthe training data is iterated over T times.
In or-der to avoid overfitting we used the now-standardaveraged version of this algorithm (Collins, 2002).We also apply the early update modificationfrom Collins and Roark (2004).
If the agenda, atany point during the decoding process, does notcontain the correct partial parse, it is not possiblefor the decoder to produce the correct output.
Inthis case, decoding is stopped early and the weightvalues are updated using the highest scoring par-tial parse on the agenda.4.1 Feature setTable 1 shows the set of feature templates for themodel.
Individual features are generated from165Description Feature templatesUnigrams S0tc, S0wc, S1tc, S1wc,S2tc, S2wc, S3tc, S3wc,N0wt, N1wt, N2wt, N3wt,S0lwc, S0rwc, S0uwc,S1lwc, S1rwc, S1uwc,Bigrams S0wS1w, S0wS1c, S0cS1w, S0cS1c,S0wN0w, S0wN0t, S0cN0w, S0cN0t,N0wN1w, N0wN1t, N0tN1w, N0tN1tS1wN0w, S1wN0t, S1cN0w, S1cN0t,Trigrams S0cS1cS2c, S0wS1cS2c,S0cS1wS2c, S0cS1cS2w,S0cS1cN0t, S0wS1cN0t,S0cS1wN0t, S0cS1cN0wBracket S0wb, S0cbS0wS1cb, S0cS1wb, S0cS1cbS0wN0tb, S0cN0wb, S0cN0tbSeparator S0wp, S0wcp, S0wq, S0wcq,S1wp, S1wcp, S1wq, S1wcqS0cS1cp, S0cS1cqTable 1: Feature templatesthese templates by first instantiating a templatewith particular labels, words and tags, and thenpairing the instantiated template with a particu-lar action.
In the table, the symbols S0, S1, S2,and S3 represent the top four nodes on the stack,and the symbols N0, N1, N2 and N3 represent thefirst four words in the incoming queue.
S0L, S0Rand S0U represent the left and right child for bi-nary branching S0, and the single child for unarybranching S0, respectively; w represents the lex-ical head token for a node; c represents the labelfor a node.
When the corresponding node is a ter-minal, c represents its POS-tag, whereas when thecorresponding node is non-terminal, c representsits constituent label; t represents the POS-tag for aword.The context S0, S1, S2, S3 and N0, N1, N2, N3for the feature templates is taken from Wang et al(2006).
However, Wang et al (2006) used a poly-nomial kernel function with an SVM and did notmanually create feature combinations.
Since weused the linear perceptron algorithm we manuallycombined Unigram features into Bigram and Tri-gram features.The ?Bracket?
row shows bracket-related fea-tures, which were inspired by Wang et al (2006).Here brackets refer to left brackets including ???,???
and ???
and right brackets including ???,???
and ???.
In the table, b represents thematching status of the last left bracket (if any)on the stack.
It takes three different values:1 (no matching right bracket has been pushedonto stack), 2 (a matching right bracket has beenpushed onto stack) and 3 (a matching right brackethas been pushed onto stack, but then popped off).The ?Separator?
row shows features that in-clude one of the separator punctuations (i.e.
???,??
?, ???
and ???)
between the head words ofS0 and S1.
These templates apply only whenthe stack contains at least two nodes; p repre-sents a separator punctuation symbol.
Each uniqueseparator punctuation between S0 and S1 is onlycounted once when generating the global featurevector.
q represents the count of any separatorpunctuation between S0 and S1.Whenever an action is being considered at eachpoint in the beam-search process, templates fromTable 1 are matched with the context defined bythe parser state and combined with the action togenerate features.
Negative features, which are thefeatures from incorrect parser outputs but not fromany training example, are included in the model.There are around a million features in our experi-ments with the CTB2 dataset.Wang et al (2006) used a range of other fea-tures, including rhythmic features of S0 and S1(Sun and Jurafsky, 2003), features from the mostrecently found node that is to the left or right of S0and S1, the number of words and the number ofpunctuations in S0 and S1, the distance betweenS0 and S1 and so on.
We did not include thesefeatures in our parser, because they did not lead toimproved performance during development exper-iments.5 ExperimentsThe experiments were performed using the Chi-nese Treebank 2 and Chinese Treebank 5 data.Standard data preparation was performed beforethe experiments: empty terminal nodes were re-moved; any non-terminal nodes with no childrenwere removed; any unary X ?
X nodes resultingfrom the previous steps were collapsed into one Xnode.For all experiments, we used the EVALB tool1for evaluation, and used labeled recall (LR), la-beled precision (LP ) and F1 score (which is the1http://nlp.cs.nyu.edu/evalb/166Figure 5: The influence of beam-sizeSections Sentences WordsTraining 001?270 3475 85,058Development 301?325 355 6,821Test 271?300 348 8,008Table 2: The standard split of CTB2 dataharmonic mean of LR and LP ) to measure pars-ing accuracy.5.1 The influence of beam-sizeFigure 5 shows the accuracy curves using differ-ent beam-sizes for the decoder.
The number oftraining iterations is on the x-axis with F -scoreon the y-axis.
The tests were performed usingthe development test data and gold-standard POS-tags.
The figure shows the benefit of using a beamsize greater than 1, with comparatively little accu-racy gain being obtained beyond a beam size of 8.Hence we set the beam size to 16 for the rest of theexperiments.5.2 Test results on CTB2The experiments in this section were performedusing CTB2 to allow comparison with previouswork, with the CTB2 data extracted from ChineseTreebank 5 (CTB5).
The data was split into train-ing, development test and test sets, as shown in Ta-ble 2, which is consistent with Wang et al (2006)and earlier work.
The tests were performed us-ing both gold-standard POS-tags and POS-tags au-tomatically assigned by a POS-tagger.
We used ourModel LR LP F1Bikel Thesis 80.9% 84.5% 82.7%Wang 2006 SVM 87.2% 88.3% 87.8%Wang 2006 Stacked 88.3% 88.1% 88.2%Our parser 89.4% 90.1% 89.8%Table 3: Accuracies on CTB2 with gold-standardPOS-tagsown implementation of the perceptron-based tag-ger from Collins (2002).The results of various models measured usingsentences with less than 40 words and using gold-standard POS-tags are shown in Table 3.
Therows represent the model from Bikel and Chiang(2000), Bikel (2004), the SVM and ensemble mod-els from Wang et al (2006), and our parser, re-spectively.
The accuracy of our parser is competi-tive using this test set.The results of various models using automati-cally assigned POS-tags are shown in Table 4.
Therows in the table represent the models from Bikeland Chiang (2000), Levy and Manning (2003),Xiong et al (2005), Bikel (2004), Chiang andBikel (2002), the SVM model from Wang et al(2006) and the ensemble system from Wang etal.
(2006), and the parser of this paper, respec-tively.
Our parser gave comparable accuracies tothe SVM and ensemble models from Wang et al(2006).
However, comparison with Table 3 showsthat our parser is more sensitive to POS-tagging er-rors than some of the other models.
One possiblereason is that some of the other parsers, e.g.
Bikel(2004), use the parser model itself to resolve tag-ging ambiguities, whereas we rely on a POS tag-ger to accurately assign a single tag to each word.In fact, for the Chinese data, POS tagging accu-racy is not very high, with the perceptron-basedtagger achieving an accuracy of only 93%.
Thebeam-search decoding framework we use couldaccommodate joint parsing and tagging, althoughthe use of features based on the tags of incom-ing words complicates matters somewhat, sincethese features rely on tags having been assigned toall words in a pre-processing step.
We leave thisproblem for future work.In a recent paper, Petrov and Klein (2007) re-ported LR and LP of 85.7% and 86.9% for sen-tences with less than 40 words and 81.9% and84.8% for all sentences on the CTB2 test set, re-167?
40 words ?
100 words UnlimitedLR LP F1 POS LR LP F1 POS LR LP F1 POSBikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -Levy 2003 79.2% 78.4% 78.8% - - - - - - - - -Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - -Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -Wang 2006 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%Wang 2006 Stacked 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%Our parser 80.2% 80.5% 80.4% 93.5% 76.5% 77.7% 77.1% 93.1% 76.1% 77.4% 76.7% 93.0%Table 4: Accuracies on CTB2 with automatically assigned tags?
40 words UnlimitedLR LP F1 POS LR LP F1 POS87.9% 87.5% 87.7% 100% 86.9% 86.7% 86.8% 100%80.2% 79.1% 79.6% 94.1% 78.6% 78.0% 78.3% 93.9%Table 5: Accuracies on CTB5 using gold-standard and automatically assigned POS-tagsSections Sentences WordsSet A 001?270 3,484 84,873Set B Set A; 400?699 6,567 161,893Set C Set B; 700?931 9,707 236,051Table 6: Training sets with different sizesspectively.
These results are significantly betterthan any model from Table 4.
However, we didnot include their scores in the table because theyused a different training set from CTB5, which ismuch larger than the CTB2 training set used by allparsers in the table.
In order to make a compari-son, we split the data in the same way as Petrovand Klein (2007) and tested our parser using auto-matically assigned POS-tags.
It gave LR and LPof 82.0% and 80.9% for sentences with less than40 words and 77.8% and 77.4% for all sentences,significantly lower than Petrov and Klein (2007),which we partly attribute to the sensitivity of ourparser to pos tag errors (see Table 5).5.3 The effect of training data sizeCTB2 is a relatively small corpus, and so we in-vestigated the effect of adding more training datafrom CTB5.
Intuitively, more training data leadsto higher parsing accuracy.
By using increasedamount of training sentences (Table 6) from CTB5with the same development test data (Table 2),we draw the accuracy curves with different num-ber of training iterations (Figure 6).
This exper-iment confirmed that the accuracy increases withthe amount of training data.Figure 6: The influence of the size of training dataAnother motivation for us to use more trainingdata is to reduce overfitting.
We invested consid-erable effort into feature engineering using CTB2,and found that a small variation of feature tem-plates (e.g.
changing the feature template S0cS1cfrom Table 1 to S0tcS1tc) can lead to a compar-atively large change (up to 1%) in the accuracy.One possible reason for this variation is the smallsize of the CTB2 training data.
When performingexperiments using the larger set B from Table 6,we observed improved stability relative to smallfeature changes.168Sections Sentences WordsTraining 001?815; 16,118 437,8591001?1136Dev 886?931; 804 20,4531148?1151Test 816?885; 1,915 50,3191137?1147Table 7: Standard split of CTB5 dataNon-root Root CompleteZhang 2008 86.21% 76.26% 34.41%Our parser 86.95% 79.19% 36.08%Table 8: Comparison with state-of-the-art depen-dency parsing using CTB5 data5.4 Test accuracy using CTB5Table 5 presents the performance of the parser onCTB5.
We adopt the data split from Zhang andClark (2008), as shown in Table 7.
We used thesame parser configurations as Section 5.2.As an additional evaluation we also produceddependency output from the phrase-structuretrees, using the head-finding rules, so that wecan also compare with dependency parsers, forwhich the highest scores in the literature are cur-rently from our previous work in Zhang and Clark(2008).
We compare the dependencies read off ourconstituent parser using CTB5 data with the depen-dency parser from Zhang and Clark (2008).
Thesame measures are taken and the accuracies withgold-standard POS-tags are shown in Table 8.
Ourconstituent parser gave higher accuracy than thedependency parser.
It is interesting that, thoughthe constituent parser uses many fewer featuretemplates than the dependency parser, the featuresdo include constituent information, which is un-available to dependency parsers.6 Related workOur parser is based on the shift-reduce parsingprocess from Sagae and Lavie (2005) and Wanget al (2006), and therefore it can be classifiedas a transition-based parser (Nivre et al, 2006).An important difference between our parser andthe Wang et al (2006) parser is that our parseris based on a discriminative learning model withglobal features, whilst the parser from Wang et al(2006) is based on a local classifier that optimizeseach individual choice.
Instead of greedy local de-coding, we used beam search in the decoder.An early work that applies beam search to con-stituent parsing is Ratnaparkhi (1999).
The maindifference between our parser and Ratnaparkhi?s isthat we use a global discriminative model, whereasRatnaparkhi?s parser has separate probabilities ofactions chained together in a conditional model.Both our parser and the parser from Collins andRoark (2004) use a global discriminative modeland an incremental parsing process.
The majordifference is the use of different incremental pars-ing processes.
To achieve better performance forChinese parsing, our parser is based on the shift-reduce parsing process.
In addition, we did not in-clude a generative baseline model in the discrimi-native model, as did Collins and Roark (2004).Our parser in this paper shares similaritywith our transition-based dependency parser fromZhang and Clark (2008) in the use of a discrimina-tive model and beam search.
The main differenceis that our parser in this paper is for constituentparsing.
In fact, our parser is one of only a fewconstituent parsers which have successfully ap-plied global discriminative models, certainly with-out a generative baseline as a feature, whereasglobal models for dependency parsing have beencomparatively easier to develop.7 ConclusionThe contributions of this paper can be summarizedas follows.
First, we defined a global discrimina-tive model for Chinese constituent-based parsing,continuing recent work in this area which has fo-cused on English (Clark and Curran, 2007; Car-reras et al, 2008; Finkel et al, 2008).
Second, weshowed how such a model can be applied to shift-reduce parsing and combined with beam search,resulting in an accurate linear-time parser.
In stan-dard tests using CTB2 data, our parser achievedcomparable Parseval F-score to the state-of-the-art systems.
Moreover, we observed that moretraining data lead to improvements on both accu-racy and stability against feature variations, andreported performance of the parser using CTB5data.
By converting constituent-based output todependency relations using standard head-findingrules, our parser also obtained the highest scoresfor a CTB5 dependency evaluation in the literature.Due to the comparatively low accuracy for Chi-nese POS-tagging, the parsing accuracy dropped169significantly when using automatically assignedPOS-tags rather than gold-standard POS-tags.
Inour further work, we plan to investigate possiblemethods of joint POS-tagging and parsing underthe discriminative model and beam-search frame-work.A discriminative model allows consistent train-ing of a wide range of different features.
Weshowed in Zhang and Clark (2008) that it was pos-sible to combine graph and transition-based de-pendency parser into the same global discrimina-tive model.
Our parser framework in this paperallows the same integration of graph-based fea-tures.
However, preliminary experiments with fea-tures based on graph information did not showaccuracy improvements for our parser.
One pos-sible reason is that the transition actions for theparser in this paper already include graph infor-mation, such as the label of the newly gener-ated constituent, while for the dependency parserin Zhang and Clark (2008), transition actions donot contain graph information, and therefore theuse of transition-based features helped to makelarger improvements in accuracy.
The integrationof graph-based features for our shift-reduce con-stituent parser is worth further study.The source code of our parser is publicly avail-able at http://www.sourceforge.net/projects/zpar.2AppendixThe set of restrictions which ensures a valid binarytree is shown below.
The restriction on the num-ber of consecutive unary rule applications is takenfrom Sagae and Lavie (2005); it prevents infiniterunning of the parser by repetitive use of unary re-duce actions, and ensures linear time complexityin the length of the sentence.?
the shift action can only be performed whenthe queue of incoming words is not empty;?
when the node on top of the stack is tempo-rary and its head word is from the right child,no shift action can be performed;?
the unary reduce actions can be performedonly when the stack is not empty;?
a unary reduce with the same constituent la-bel (Y ?
Y ) is not allowed;?
no more than three unary reduce actions canbe performed consecutively;2The make target for the parser in this paper is chi-nese.conparser.?
the binary reduce actions can only be per-formed when the stack contains at least twonodes, with at least one of the two nodes ontop of stack (with R being the topmost and Lbeing the second) being non-temporary;?
if L is temporary with label X?, the result-ing node must be labeled X or X?
and left-headed (i.e.
to take the head word from L);similar restrictions apply when R is tempo-rary;?
when the incoming queue is empty and thestack contains only two nodes, binary reducecan be applied only if the resulting node isnon-temporary;?
when the stack contains only two nodes, tem-porary resulting nodes from binary reducemust be left-headed;?
when the queue is empty and the stack con-tains more than two nodes, with the thirdnode from the top being temporary, binary re-duce can be applied only if the resulting nodeis non-temporary;?
when the stack contains more than two nodes,with the third node from the top being tempo-rary, temporary resulting nodes from binaryreduce must be left-headed;?
the terminate action can be performed whenthe queue is empty, and the stack size is one.170ReferencesDaniel M. Bikel and David Chiang.
2000.
Two sta-tistical parsing models applied to the Chinese Tree-bank.
In Proceedings of SIGHAN Workshop, pages1?6, Morristown, NJ, USA.Daniel M. Bikel.
2004.
On the Parameter Space ofGenerative Lexicalized Statistical Parsing Models.Ph.D.
thesis, University of Pennsylvania.Ted Briscoe and John Carroll.
1993.
Generalized prob-abilistic LR parsing of natural language (corpora)with unification-based grammars.
ComputationalLinguistics, 19(1):25?59.Xavier Carreras, Michael Collins, and Terry Koo.2008.
Tag, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of CoNLL, pages 9?16, Manchester, England,August.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL, pages132?139, Seattle, WA.David Chiang and Daniel M. Bikel.
2002.
Recoveringlatent information in treebanks.
In Proceedings ofCOLING.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of ACL, pages 111?118, Barcelona, Spain, July.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofthe 35th Meeting of the ACL, pages 16?23, Madrid,Spain.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP, pages 1?8, Philadelphia, USA, July.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic models for action-based Chinese dependencyparsing.
In Proceedings of ECML/ECPPKDD, War-saw, Poland, September.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, con-ditional random field parsing.
In Proceedings ofACL/HLT, pages 959?967, Columbus, Ohio, June.Association for Computational Linguistics.Roger Levy and Christopher D. Manning.
2003.
Is itharder to parse Chinese, or the Chinese Treebank?In Proceedings of ACL, pages 439?446, Sapporo,Japan, July.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98, Ann Arbor, Michigan, June.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?enEryig?it, and Svetoslav Marinov.
2006.
Labeledpseudo-projective dependency parsing with supportvector machines.
In Proceedings of CoNLL, pages221?225, New York City, June.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofHLT/NAACL, pages 404?411, Rochester, New York,April.
Association for Computational Linguistics.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1-3):151?175.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of IWPT, pages 125?132, Vancouver, BritishColumbia, October.Honglin Sun and Daniel Jurafsky.
2003.
The effect ofrhythm on structural disambiguation in Chinese.
InProceedings of SIGHAN Workshop.Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.2006.
A fast, accurate deterministic parser for Chi-nese.
In Proceedings of COLING/ACL, pages 425?432, Sydney, Australia.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,and Yueliang Qian.
2005.
Parsing the Penn ChineseTreebank with semantic knowledge.
In Proceedingsof IJCNLP.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of EMNLP, pages562?571, Hawaii, USA, October.171
