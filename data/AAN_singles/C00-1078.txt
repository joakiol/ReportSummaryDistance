Chart-Based Transfer Rule Application in Machine TranslationAdam MeyersNew York Universitymeyers@cs.nyu.eduMich iko  KosakaMonlnouth Universitykosaka@monmouth.eduRalph GrishInanNew York Universitygr ishman@cs.nyu.eduAbstract35"ansfer-based Machine Translation systems re-quire a procedure for choosing the set; of transferrules for generating a target language transla-tion from a given source language sentence.
Inan MT system with many comI)eting transferrules, choosing t;he best, set of transfer ules fortranslation may involve the evaluation of an ex-plosive number of competing wets.
We propose asohltion t;o this problem l)ased on current best-first chart parsing algorithms.1 Introduct ionri~'ansfer-based Machine 'Kanslation systenls re-quire a procedure for choosing the set of trans-tier rules for generating a target  language I;rans-lation from a given source language sentence.This procedure is trivial tbr a system if, givena (:ontext, one transtb.r ule.
can l)e selected un-~mfl)iguously.
O|;herwise, choosing the besl; set;of transfer ules may involve the.
evaluation ofmmmrous competing sets.
In fact, the numberof l)ossible transfer ule combinations increas-es exponentially with the length of the source,language sentence,.
This situation mirrors thet)roblem of choosing productions in a nondeter-ministic parser, in this paI)er, we descril)e asystem for choosing transfer ules, based on s-tatistical chart parsing (Bol)row, 1990; Chitraoand Grishman, 1990; Caraballo and Charniak,1997; Charniak et al, 1998).In our Machine %'anslation system, transferrules are generated automatically from parsedparallel text along the lines of (Matsulnoto el;al,, 1993; Meyers et al, 1996; Meyers et al,1998b).
Our system tends to acquire a largenmnber of transt~r rules, due lnainly to 3,1terna-tive ways of translating the same sequences ofwords, non-literal translations in parallel textand parsing e, rrors.
It is therefore crucial thatour system choose the best set of rules efficient-ly.
While the technique discussed he.re obviouslyapplies to similar such systems, it could also ap-ply to hand-coded systems in which each wordor group of words is related to more than onetransfer ule.
D)r example, both Multra (Hein,1996) and the Eurotra system described in (Wayel; al., 1997) require components for decidingwhich combination of transtbr ules to use.
Theproi).osed technique may 1)e used with syst;emslike these, t)rovided that all transfer ules are as-signed initial scores rating thcqr at)propriatenessfor translation.
These al)t)rol)riateness ratingscouhl be dependent or independent of context.2 Previous WorkThe MT literature deserib(;s everal techniquestbr deriving the appropriate translation.
Statis-tical systems l;hal; do not incorporate linguisticanalysis (Brown el: al., 1993) typically choosethe most likely translation based on a statis-tical mode.l, i.e.., translation probability deter-mines the translation.
(Hein, 1996) reports a set;of (hand-coded) fea|;llre structure based prefi~r-ence rules to choose among alternatives in Mu\]-tra.
There is some discussion about addingsome transtbr ules automatically acquired flomcorpora to Multra?
Assuming that they over-generate rules (as we did), a system like the onewe propose should 1)e beneficial.
In (Way et al,1997), many ditDrent criteria are used to dloosetrmlsi~;r ules to execute including: pretbrmlcesfor specific rules over general ones, and comt)lexrule nol, ation that insures that tb.w rules can 21)-ply to the same set, of words.The Pangloss Mark III system (Nirenburg~This translatioll procedm'e would probably comple-menI~ not; replace exist, ing procedures in these systelns.2http : / / s tp .
l i ng .
uu.
se /~corpora /p lug / repor t  s /ansk_last/ is a report on this 1)reject; for Multra.537and Frederking, 1995) uses a chart-walk algo-rithm to combine the results of three MT en-gines: an example-based ngine, a knowledge-based engine, and a lexical-transfer engine.Each engine contributes its best edges and tilechart-walk algorithm uses dynamic program-ruing to find the combination of edges with thebest overall score that covers the input string.Scores of edges are normalized so that the scoresfi'om the different engines are comparable andweighted to favor engines which tend to producebetter results.
Pangloss's algorithm combineswhole MT systems.
In contrast, our algorith-m combines output of individual transfer uleswithin a single MT system.
Also, we use a best-first search that incorporates a probabilistic-based figure of merit, whereas Pangloss uses anempirically based weighting scheme and whatappears to be a top-down search.Best-first probabilistic chart parsers (Bo-brow, 1990; Chitrao and Grishman, 1990; Cara-ballo and Charniak, 1997; Charniak et al, 1998)strive to find the best parse, without exhaus-tively trying a l l  possible productions.
A proba-bilistic figure of merit (Caraballo and Charniak,1997; Charniak et al, 1998) is devised for rank-ing edges.
The highest ranking edges are pur-sued first and the parser halts after it producesa complete parse.
We propose an algorithm forchoosing and applying transthr ules based onprobability.
Each final translation is derivedfrom a specific set of transfer ules.
If the pro-cedure immediately selected these transfer rulesand applied them in tile correct order, we wouldarrive at tile final translation while creating theminimum number of edges.
Our procedure usesabout 4 tinms this minimum number of edges.With respect o chart parsing, (Charniak et al,1998) report that their parser can achieve goodresults while producing about three times tilemininmm number of edges required to producethe final parse.3 Test  DataWe conducted two experiments.
For experimen-t1, we parsed a sentence-aligned pair of Span-ish and English corpora, each containing 1155sentences of Microsoft Excel Help Text.
Thesepairs of parsed sentences were divided into dis-tinct training and test sets, ninety percent fortraining and ten percent fbr test.
The trainingSource Tree Target TreeD = vo lvcr  D' = reca lcu lates,,I,JA = Exce l  E = ca lcu la rObj~en A' = Excel I C' = workbookB' = va lues/ C = l ibrok ,B =va lores  \aeF = t raba joExcel vuelve a calcular Excel recalculatesvalores en libro de trabajo values iu workbookFigure 1: Spanish and English I{egularizedParse 2?eesset was used to acquire transfer ules (Meyerset al, 1998b) which were then used to translatetile sentences in tile test set.
This paper focus-es on our technique for applying these transferrules in order to translate the test sentences.The test and training sets in experiment1were rotated, assigning a different enth of thesentences to the test set in each rotation.
In thisww we tested tile program on the entire corpus.Only one test set (one tenth of the corpus) wasused for tuning the system (luring development.~:ansfer rules, 11.09 on average, were acquiredt'rom each training set and used for translationof the corresponding test set.
For Experiment2, we parsed 2617 pairs of aligned sentences andused the same rotation procedure for dividingtest and training corpora.
The Experiment 2corpus included the experinlentl corpus.
An av-erage of 2191 transfer ules were acquired froma given set of Experinmnt 2 training sentences.Experimentl isorchestrated in a carefld man-ner that may not be practical for extremelylarge corpora, and Experiment 2 shows how theprogram performs if we scale up and elilniuatesome of the fine-tuning.
Apart from corpus size,there are two main difference between the twoexperiments: (1) the experimentl corpus wasaligned completely by hand, whereas the Exper-iment 2 corpus was aligned automatically usingthe system described ill (Meyers et al, 1998a);and (2) the parsers were tuned to the experi-mentl sentences, but not the Experiment 2 sen-tences (that did not overlap with experinmntl).5381) A = Excel2) B =va loresC = l ibrovA' = ExcelB'  = values.~)rC' = workbookF = trabajoD = volvcr S.IJ.i~4)1 E = ealcularO b.
\ ]~en l2 31)' = recalculate1 2 3Figure 2: A S('t of %-ansfer Rules4 Parses  and  Trans fer  Ru lesFigure 1 is a pair of "regularized" parses t br acorresi)onding pair of Spanish and Fmglish sen-tences fi'om Microsoft Excel hell) text.
Theseat'(; F-structure-like dependency analyses of sen-tences that represent 1)redicate argument struc-ture.
This representation serves to neutralizesome ditfbrences between related sentence tyt)es,e.g., the regularized parse of related active andt)a,~sive senten(:es are identical, except tbr the{i'.ature value pair {Mood, Passive}.
Nodes (wfl-ues) are labeled with head words and arcs (fea-tures) are labeled with gramma~;ical thnetions(subject, object), 1)repositions (in) and subor-dinate conjunctions (beNre).
a For demonstra-tion purposes, the source tree in Figure 1 is theinput to our translation system and the targettree is the outl)ut.The t;ransfer rules in Figure 2 can beused to convert the intmt; tree into the out-1)at tree.
These transtbr rules are pairs ofcorresponding rooted substructures, where asubstructure (Matsumoto et al, 1993) is aconnected set of arcs and nodes.
A ruleaMorphologieal features and their values (Gram-Number: plural) are also represented as ares and nodes.consists of o, ither a pair of "open" substructures(rule 4) or a pair of "closed" substructures (rules1, 2 and 3).
Closed substructures consist of s-ingle nodes (A,A',B,B',C') or subtrees (the lefthand side of rule 3).
Open substructures con-tain one or more open arcs, arcs without heads(both sul)structures in rule 4).5 Simplif ied Translat ion withTree-based Transfer RulesThe rules in Figure 2 could combine by fillingin the open arcs in rule 4 with the roots of thesubstructures in rules 1, 2 and 3.
The resultwould be a closed edge which maps the left; treein l,'igure, 1 into the right tree.
Just as edges of achart parser are based on the context free rulesused by the chart parser, edges of our trans-lation system are, based on these trans~L'r ules.Initial edges are identical to transtb, r rules.
Oth-er edges result from combining one closed edgewith one open edge.
Figure 3 lists the sequenceof edges which wouhl result from combining theinitial edges based (m Rules 1-4 to replicate, thetrees in Figure 1.
The translation proceeds byincrementally matching the left hand sides ofRules 1-4 with the intmt tree (and insuring thatthe tree is completely covered by these rules).The right-hand sides of these comt)atil)le rulesare also (:ombined t;o 1)reduce the translal;iolLThis is an idealized view of our system in whicheach node in the input tree matches the left;-hand side of exactly one transfer rule: there isno ambiguity and no combinatorial explosion.The reality is that more than one transfer ulesmay be activated tbr each node, as suggestedin Figure 4.
4 If each of the six nodes of thesource tree corresponded to five transfer rules,there are 56 = 15625 possible combinations ofrules to consider.
To produce t lm output  in Fig-ure 3, a minimum of seven edges would be re-quired: four initial edges derived ti'om the o-riginal transfer ules plus three additional edgesrepresenting the combination of edges (steps 2,3 and 4 in Figure 3).
The speed of our system ismeasured by the number of actual edges dividedby this minimuln.4The third example listed would actually involve twotrm~sfer rules, one translating "volver" to "ret)cat" andthe second translating "calcular" to "calculal;e".5391)2)D = vo lverS u ~1 E = ca lcu la rObj~n2 3D = vo lverA = Exce l  E = ca lcu la rObj~n2 3vvD' = reca lcu la teI 2 3D' = reca lcu la teA' = Excel 2 33)D = volverA = Exce l  E = ca leu la rB = valores 3D'  = reca lcu la teA' = Excel / 3g' = values4)D = volverA = Excel E = ca lcu la rOb/~nB = va io res  C = l ib rodeF = t raba jovD'  = reca lcu la teA' = Excel \ C' = workbookB' = va luesFigure 3: An Idealized Translation Procedure6 Best  F i r s t  T rans la t ion  ProcedureThe following is an outline of our best firstsearch procedure for finding a single translation:1.
For each node N, find TN, the set of com-patible transfer ules2.
Create initial edges for all TN3.
Repeat until a "finished" edge is tbund oran edge limit is reached:(a) Find the highest scoring edge E(b) If complete, combine E with compati-ble incoml)lete dges(c) If incomplete, combine E with com-patible complete dges(d) Incomplete dge + complete edge =new edgeThe procedure creates one initial edgefor each matching transfer rule in thedatabase 5 and puts these edges in a'~The left-hand side of a matching transfer rule is com-patible with a substructure in the input source tree.540D'  = recalculateD = velvet 1 2 3 / %Sub, i / ~ a !)'
= calculate/ \/ E = \ '4 .
'+"3 againD = repeatSabj ~ b j1 E = calculationFigure 4: Multiple \[lYansfer Rules for Each Sub-structm:equeue prioritized by score.
The pro-cedure iteratively combines the bests(:oring edge with some other comt)al;ilfleedge to t)roduce a new edge.
and inserts the newedge in the queu('..
The score for each new edgeis a function of the scores of the edges used toproduce it:.
The process contimms m~til eitheran edge limit is reache(l (the system looks likeit; will take too long to terminate) or a completeedge is t)roduced whose left-hand side is theinput tree: we (:all this edge a "finished edge".We use the tbllowing technique for calculatingthe score tbr initial edges.
6 The score tbr eachinitial edge E rooted at N, based on rule/~, iscalculated as follows:1.
SCO17.F=I(S) " " F,.c.,~(n) = ~'?.q'~D~(~a  ~t N~)Where the fl'equency (Freq) of a rule is thenmnber of times it matched an exmnple inthe training corpus, during rule ~cquisition.The denominator is the combined fl'equen-cies of all rules that match N.aThis is somewhat det)cndent on the way these |;rans-fer rules are derived.
Other systems would t)robably haveto use some other scoring system.Ezperiment 1:1155 sentencesNorm No NormTotal TranslationsOver Edge LimitActual EdgesMiniature EdgesEdge RatioAccuracy1153293,71922,1253.370.9112728579,27820,1251.4.870.9Ezpcriment 2:2617 sentencesNorm No NormTotal TranslationsOver Edge LimitActual EdgesMinimum EdgesEdge RatioA(:curacy26107262,17248,5704.062.62544731,398,79642,77015.561.5Figure 5: Result:s2, S s ) = s ,o,.
(;.l ( S ) - No,., , , ,Where the Norm (normalization) t~ctor isequal to the highest SCORE1 for any rulematching N.Since the log.2 of probabilities are necessarilynegative, this has the effect of setting the E ofeach of the most t)rol)able initial edges to zero.The scores tbr non-initial edges are calculatedby ad(ling u I) the scores of the initial e(tges ofwhich they are comt)osed.
7Without any normMization (Score(S) =SCORE1 (,9)), small trees are favored over largetrees.
This slows down the process of finding thefinal result.
The normalization we use insuresthat the most probable set; of transihr ules areconsidered early on.7 Resu l tsFigure 5 gives our results for both experiments1 and 2, both with normalization (Norm) andwithout (No Norm).
"Total Translations" referto the number of sen|;ences which were translat-ed successfully 1)y the system and "Over EdgeLimit" refers to the numl)er of sentences whichcaused the system to exceed the edge limit, i.e.,once the system produces over 10,000 edges,trm~slation failure is assmned.
The system cur-7Scoring for special cases is not; included in this paper.These cases include rules for conjunctions and rules ibrwords that do not match any transfer ules in a givencontext (we currently leave the word untranslated.
)541rently will only fail to produce some transla-tion for any input if the edge limit is exceed-ed.
"Actual Edges" reibrs to the total numberof edges used tbr attempting to translate verysentence in the corpus.
"Minimum Edges" referto the total minimum number of edges requiredfor successful translations.
The "Edge Ratio"is a ratio between: (1) "Total Edges" less themnnber of edges used in failed translations; and(2) The "Minimum Edges".
This ratio, in com-l)ination with, the number of "Over Edge Limit"measures the efficiency of a given system.
"Ac-curacy" is an assessment of translation qualitywhich we will discuss in the next section.Normalization caused significant speed-up forboth experiments.
If you compare the totalnumber of edges used with and without nor-malization, speed-up is a factor of 6.2 for Ex-periment I and 5.3 for Experiment 2.
If youcompare actual edge ratios, speed-up is a factorof 4:.5 tbr Experiment 1 and 3.9 tbr Experiment2.
In addition, the number of failed parses wentdown by a fhctor of 10 for both experiments.
Asshould be expected, accuracy was virtually thesame with and without normalization, althoughnormalization <lid cause a slight improvemen-t. Normalization should produce the essentiallythe same result in less time.These results suggest that we can probablycount on a speed-up of at least 4 and a significant decline in failed parses by using normM-ization.
The ditferences in performance on thetwo corpora are most likely due to the degree ofhand-tuning for Experiment 1.7.1 Our  Accuracy  Measure"Accuracy" in Figure 5 is the average of thetbllowing score for each translated sentence:ITNYu ~ TMSI1/2 x (ITNYuI + ITMsl)TNZU is the set of words in NYU's translationand TMS is the set of words in the original Mi-crosoft translation.
If TNYU = "A B C D E"and TMS = "A B C F", then the intersectionset "A B C" is length 3 (the numerator) andthe average length of TNZU and TMS is 4 1/2(the denominator).
The accuracy score equals3 + 4 1/2 = 2/3.
This is a Dice coefficient com-parison of our translation with the original.
It isan inexpensive nmthod of measuring the pertbr-mance of a new version of our system, hnprove-ments in the average accuracy score for our san>ple set; of sentences usually reflect an improve-ment in overall translation quality.
While it issignificant hat the accuracy scores in Figure 5did not go down when we normalized the scores,the slight improvement in accuracy should notbe given nmch weight.
Our accuracy score isflawed in that it cannot account for the follow-ing facts: (1) good paraphrases are perfectly ac-ceptable; (2) some diflbrences in word selectionare more significant han others; and (3) errorsin syntax are not directly accounted tbr.NYU's system translates the Spanish sen-tence "1.
Selection la celda en la que deseaintroducir una rethrencia" as "1. select the cel-l that you want to enter a reference in".
Mi-crosoft translates this sentence as "1.
Select thecell in which you want; to enter the reference".Our system gives NYU's translation an accu-racy score of .75 due to the degree of overlapwith Microsoft's translation.
A truman reviewerwouhl probably rate NYU's translation as com-pletely acceptable.
In contrast, NYU's systemproduced the following unacceptable translationwhich also received a score of .75: the Spanishsentence "Elija la funcidn que desea pegar en laf6rmula en el cuadro de di~logo Asistente paraflmciones" is translated as " "Choose the flmc-tion that wants to paste Function Wizard in theformula in the dialog box", in contr,~st with Mi-crosoft's translation "Choose the flmction youwant to paste into the tbrmula fl'om the Func-tion Wizard dialog box".
In fact, some goodtranslations will get worse scores than somebad ones, e.g., an acceptable one word trans-lation can even get a score of 0, e.g.,"SUPR"was translated as "DEL" by Microsoft and as"Delete" by NYU.
Nevertheless, by averagingthis accuracy score over many examples, it hasproved a valuable measure for comparing differ-ent versions of a particular system: better sys-tems get better results.
Similarly, after tweak-ing the system, a better translation of a partic-ular sentence will usually yield a better score.8 Future  WorkFnture work should address two limitations ofour current system: (1) Bad parses yield badtransihr rules; and (2) sparse data limits the sizeof our transfer rule database and our options for542applying transfer ules selectively.
To nttack the"bad parse" problem, we are eonsideriug usingour MT system with less-detailed parsers, sincethese parsers typically produce less error-proneoutput.
We will have to conduct exl)erimcntsto determine the minimum level of detM1 thatis needed, aPrevious to the work reported in this paper,we ran our MT system on bilinguM corpora inwhich the sentences were Migned manuMly.
Thecost of manuM aligmnent limited the size of thecorpora we could use.
A lot of our recent MTresearch as bo.en tbcused on solving this sparsedata prol)lem through our develoi)ment of a sen-tence alignment progrmn (Meyers et al, 1998a).We now have 300,000 automaticMly aligned sen-tences in the Microsoft help text domain tbr fu-ture experiineni;s. In addition to provi(ting uswith many more transfer ules, this shouhl Mlowus to colh'.ct transfer rule co-occurrence infor-mation which we c~m then use to apply tr;mstbrrules more effectively, perhaps improving trans-b~tion quality.
In a preliminary experime, nt a-hmg these lines using the Experiment 1. tort)us,co-occurrence information had no noticeable ffeet.
However, we are hot)eflfl that flltm'e ex-t)eriments with 300,000 Migned sentences (300tinies as nnlch data) will 1)e more successful.Re ferencesRobert J. Bobrow.
1990.
S1;~Ltistical agendaparsing.
In I)ARPA Speech and Lang'uagcWorkshop, pages 222-224.Peter Brown~ Stephen A. Delb~ t)ietra, Vin-cent J. Della Pietra, and Robert L. Mer-cer.
1993.
The Mathematics of StatisticalM~zchine 'h'anslation: 1)arametcr Estimation.Computational Lin.quistics, 19:263-312.Sh;~ron A. Caraballo and Eugene Chm'niak.1997.
New figures of merit tbr best-tirst prot)-M)ilistie chart parsing.
Computational Lin-guistics, 24:275-298.Eugene Ctmrniak, Sharon Goldwater, and M~rkJohnson.
1998.
Edge-Based Best-First ChartParsing.
In Proceedings of the Sixth AnnualWorkshop for Very Lawc Corpora, Montreal.SOne could set u 1) a contimmm from detailed parser-s like Proteus down to shallow verb-group/noun-grouI)recognizers, with the Penn treetmnk based parsers ly-ing somewhere in the middle.
As one travels down t, heeonLinlmIn t;o t;he lower detail parsers, tim error rate nat-urally decreases.Mahesh V. Chitrao and RMph Gris}unan.
1990.St;~tisti('al pnrsing of messages.
In \])AIIPASpeech and La'n,g'uagc Workshop, pages 263266.Annn Sggvall ltein.
1996.
Pretbrence Mecha-nisms of the Multra Machine %'ansb~tion Sys-tem.
In Barbara H. Partee and Petr Sgall,editors, Discourse and Meaning: Papers in11onor of Eva 11aji~ovd.
John Benja.mins Pub-lishing Company, Amsterdam.Y.
Matsumoto, H. Ishimoto, T. Utsuro, andM.
Nagao.
1993.
Structural Matching ofParallel Texts.
In 31st Annual Meeting ofthe Association for Computational Linguis-tics: "Proceedings of the Uo~@rencc".Adam Meyers, Roman Ymlgm'ber, a.nd RalphGrishman.
1996.
Alignment of SharedForests fi)r BilinguM Corpora.
In Proceedingsof Coliw.I 1996: The 16th International Con-fercncc on Computational Linguistics, l)ages460 465.Adam Meyers, Miehiko Kosak~, and Ralph Gr-ishman.
1998m A Multilingual Procedurefor Dict;ionary-B;~sed Sentence Aligmnent.
InProceedings of AMTA '98: Machine Transla-tion and th, c ht:fo'rmation Soup, t)~ges 187.198.Adam Meyers, R,om~m Ym~g~rber, Ralph Gr-ishmml, Cntherine Macleod, mM AntonioMoreno-S~mdow~l.
1998|).
l)eriving ~l~:a.ns-fin: Rules from Domimmce-Preserving Align-ments.
In I)'rocccdim.ls o.f Coling-A CL98: Th.c171h International Conference on Computa-tional Ling,uistics and the 36th, Meeting of theAssociation for Computational Linguistics.Sergei Nirenlmrg mM Robert E. l~:ederking.1995.
The Pangloss Mark III Machine 'l?nms-lt~tion System: Multi-Engine System Archi-tecture.
Te(:hnical report, NMSU Oil,L, USCISI, ;rod CMU CMT.Andrew Way, Ian Crookston, and Jane Shell;on.1997.
A Typology of ~IYanslation Prol)lemsfor Eurotra Translation Machines.
Machine\[l}'anslation, 12:323 374.543
