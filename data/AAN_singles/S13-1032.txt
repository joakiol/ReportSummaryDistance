Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsCPN-CORE: A Text Semantic Similarity System Infusedwith Opinion KnowledgeCarmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?Bishan Yang?, Claire Cardie?, Rada Mihalcea[?, Janyce Wiebe?
[University of North TexasDenton, TX?University of PittsburghPittsburgh, PA?Google Inc.Mountain View, CA?Language Computer Corp.Richardson, TX?Cornell UniversityIthaca, NYAbstractThis article provides a detailed overview of theCPN text-to-text similarity system that we par-ticipated with in the Semantic Textual Similar-ity task evaluations hosted at *SEM 2013.
Inaddition to more traditional components, suchas knowledge-based and corpus-based met-rics leveraged in a machine learning frame-work, we also use opinion analysis features toachieve a stronger semantic representation oftextual units.
While the evaluation datasets arenot designed to test the similarity of opinions,as a component of textual similarity, nonethe-less, our system variations ranked number 38,39 and 45 among the 88 participating systems.1 IntroductionMeasures of text similarity have been used for a longtime in applications in natural language processingand related areas.
One of the earliest applicationsof text similarity is perhaps the vector-space modelused in information retrieval, where the documentmost relevant to an input query is determined byranking documents in a collection in reversed or-der of their angular distance with the given query(Salton and Lesk, 1971).
Text similarity has alsobeen used for relevance feedback and text classifi-cation (Rocchio, 1971), word sense disambiguation(Lesk, 1986; Schutze, 1998), and extractive summa-rization (Salton et al 1997), in the automatic evalu-ation of machine translation (Papineni et al 2002),?carmen.banea@gmail.com?
rada@cs.unt.edutext summarization (Lin and Hovy, 2003), text co-herence (Lapata and Barzilay, 2005) and in plagia-rism detection (Nawab et al 2011).Earlier work on this task has primarily focused onsimple lexical matching methods, which produce asimilarity score based on the number of lexical unitsthat occur in both input segments.
Improvementsto this simple method have considered stemming,stopword removal, part-of-speech tagging, longestsubsequence matching, as well as various weight-ing and normalization factors (Salton and Buckley,1997).
While successful to a certain degree, theselexical similarity methods cannot always identify thesemantic similarity of texts.
For instance, there is anobvious similarity between the text segments ?sheowns a dog?
and ?she has an animal,?
yet thesemethods will mostly fail to identify it.More recently, researchers have started to con-sider the possibility of combining the large numberof word-to-word semantic similarity measures (e.g.,(Jiang and Conrath, 1997; Leacock and Chodorow,1998; Lin, 1998; Resnik, 1995)) within a semanticsimilarity method that works for entire texts.
Themethods proposed to date in this direction mainlyconsist of either bipartite-graph matching strate-gies that aggregate word-to-word similarity into atext similarity score (Mihalcea et al 2006; Islamand Inkpen, 2009; Hassan and Mihalcea, 2011;Mohler et al 2011), or data-driven methods thatperform component-wise additions of semantic vec-tor representations as obtained with corpus mea-sures such as latent semantic analysis (Landauer etal., 1997), explicit semantic analysis (Gabrilovichand Markovitch, 2007), or salient semantic analysis221(Hassan and Mihalcea, 2011).In this paper, we describe the system variationswith which we participated in the *SEM 2013 taskon semantic textual similarity (Agirre et al 2013).The system builds upon our earlier work on corpus-based and knowledge-based methods of text seman-tic similarity (Mihalcea et al 2006; Hassan andMihalcea, 2011; Mohler et al 2011; Banea et al2012), while also incorporating opinion aware fea-tures.
Our observation is that text is not only similaron a semantic level, but also with respect to opin-ions.
Let us consider the following text segments:?she owns a dog?
and ?I believe she owns a dog.
?The question then becomes how similar these textfragments truly are.
Current systems will considerthe two sentences semantically equivalent, yet to ahuman, they are not.
A belief is not equivalent to afact (and for the case in point, the person may verywell have a cat or some other pet), and this shouldconsequently lower the relatedness score.
For thisreason, we advocate that STS systems should alsoconsider the opinions expressed and their equiva-lence.
While the *SEM STS task is not formulatedto evaluate this type of similarity, we complementmore traditional corpus and knowledge-based meth-ods with opinion aware features, and use them ina meta-learning framework in an arguably first at-tempt at incorporating this type of information to in-fer text-to-text similarity.2 Related WorkOver the past years, the research community hasfocused on computing semantic relatedness usingmethods that are either knowledge-based or corpus-based.
Knowledge-based methods derive a measureof relatedness by utilizing lexical resources and on-tologies such as WordNet (Miller, 1995) to measuredefinitional overlap, term distance within a graph-ical taxonomy, or term depth in the taxonomy asa measure of specificity.
We explore several ofthese measures in depth in Section 3.3.1.
On theother side, corpus-based measures such as LatentSemantic Analysis (LSA) (Landauer et al 1997),Explicit Semantic Analysis (ESA) (Gabrilovichand Markovitch, 2007), Salient Semantic Analysis(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-tual Information (PMI) (Church and Hanks, 1990),PMI-IR (Turney, 2001), Second Order PMI (Islamand Inkpen, 2006), Hyperspace Analogues to Lan-guage (Burgess et al 1998) and distributional simi-larity (Lin, 1998) employ probabilistic approachesto decode the semantics of words.
They consistof unsupervised methods that utilize the contextualinformation and patterns observed in raw text tobuild semantic profiles of words.
Unlike knowledge-based methods, which suffer from limited coverage,corpus-based measures are able to induce a similar-ity between any given two words, as long as theyappear in the very large corpus used as training.3 Semantic Textual Similarity System3.1 Task SetupThe STS task consists of labeling one sentence pairat a time, based on the semantic similarity existentbetween its two component sentences.
Human as-signed similarity scores range from 0 (no relation)to 5 (semantivally equivalent).
The *SEM 2013 STStask did not provide additional labeled data to thetraining and testing sets released as part of the STStask hosted at SEMEVAL 2012 (Agirre et al 2012);our system variations were trained on SEMEVAL2012 data.The test sets (Agirre et al 2013) consist oftext pairs extracted from headlines (headlines,750 pairs), sense definitions from WordNet andOntoNotes (OnWN, 561 pairs), sense definitionsfrom WordNet and FrameNet (FNWN, 189 pairs),and data used in the evaluation of machine transla-tion systems (SMT, 750 pairs).3.2 ResourcesVarious subparts of our framework use several re-sources that are described in more detail below.Wikipedia1 is the most comprehensive encyclo-pedia to date, and it is an open collaborative efforthosted on-line.
Its basic entry is an article which inaddition to describing an entity or an event also con-tains hyperlinks to other pages within or outside ofWikipedia.
This structure (articles and hyperlinks)is directly exploited by semantic similarity methodssuch as ESA (Gabrilovich and Markovitch, 2007),or SSA (Hassan and Mihalcea, 2011)2.1www.wikipedia.org2In the experiments reported in this paper, all the corpus-based methods are trained on the English Wikipedia downloadfrom October 2008.222WordNet (Miller, 1995) is a manually crafted lex-ical resource that maintains semantic relationshipssuch as synonymy, antonymy, hypernymy, etc., be-tween basic units of meaning, or synsets.
These rela-tionships are employed by various knowledge-basedmethods to derive semantic similarity.The MPQA corpus (Wiebe and Riloff, 2005) isa newswire data set that was manually annotatedat the expression level for opinion-related content.Some of the features derived by our opinion extrac-tion models were based on training on this corpus.3.3 FeaturesOur system variations derive the similarity score of agiven sentence-pair by integrating information fromknowledge, corpus, and opinion-based sources3.3.3.1 Knowledge-Based FeaturesFollowing prior work from our group (Mihalceaet al 2006; Mohler and Mihalcea, 2009), we em-ploy several WordNet-based similarity metrics forthe task of sentence-level similarity.
Briefly, for eachopen-class word in one of the input texts, we com-pute the maximum semantic similarity4 that can beobtained by pairing it with any open-class word inthe other input text.
All the word-to-word similarityscores obtained in this way are summed and normal-ized to the length of the two input texts.
We providebelow a short description for each of the similaritymetrics employed by this system.The shortest path (Path) similarity is equal to:Simpath =1length(1)where length is the length of the shortest path be-tween two concepts using node-counting.The Leacock & Chodorow (Leacock andChodorow, 1998) (LCH) metric is equal to:Simlch = ?
loglength2 ?D(2)where length is the length of the shortest path be-tween two concepts using node-counting, and D isthe maximum depth of the taxonomy.The Lesk (Lesk) similarity of two concepts is de-fined as a function of the overlap between the cor-responding definitions, as provided by a dictionary.3The abbreviation in italics accompanying each method al-lows for cross-referencing with the results listed in Table 2.4We use the WordNet::Similarity package (Pedersen et al2004).It is based on an algorithm proposed by Lesk (1986)as a solution for word sense disambiguation.The Wu & Palmer (Wu and Palmer, 1994) (WUP )similarity metric measures the depth of two givenconcepts in the WordNet taxonomy, and the depthof the least common subsumer (LCS), and combinesthese figures into a similarity score:Simwup =2 ?
depth(LCS)depth(concept1) + depth(concept2)(3)The measure introduced by Resnik (Resnik, 1995)(RES) returns the information content (IC) of theLCS of two concepts:Simres = IC(LCS) (4)where IC is defined as:IC(c) = ?
logP (c) (5)and P (c) is the probability of encountering an in-stance of concept c in a large corpus.The measure introduced by Lin (Lin, 1998) (Lin)builds on Resnik?s measure of similarity, and addsa normalization factor consisting of the informationcontent of the two input concepts:Simlin =2 ?
IC(LCS)IC(concept1) + IC(concept2)(6)We also consider the Jiang & Conrath (Jiang andConrath, 1997) (JCN ) measure of similarity:Simjnc =1IC(concept1) + IC(concept2)?
2 ?
IC(LCS)(7)3.3.2 Corpus Based FeaturesWhile most of the corpus-based methods inducesemantic profiles in a word-space, where the seman-tic profile of a word is expressed in terms of its co-occurrence with other words, LSA, ESA and SSArely on a concept-space representation, thus express-ing a word?s semantic profile in terms of the im-plicit (LSA), explicit (ESA), or salient (SSA) con-cepts.
This departure from the sparse word-space toa denser, richer, and unambiguous concept-space re-solves one of the fundamental problems in semanticrelatedness, namely the vocabulary mismatch.Latent Semantic Analysis (LSA) (Landauer et al1997).
In LSA, term-context associations are cap-tured by means of a dimensionality reduction op-erated by a singular value decomposition (SVD)223on the term-by-context matrix T, where the ma-trix is induced from a large corpus.
This reduc-tion entails the abstraction of meaning by collaps-ing similar contexts and discounting noisy and ir-relevant ones, hence transforming the real worldterm-context space into a word-latent-concept spacewhich achieves a much deeper and concrete seman-tic representation of words5.Random Projection (RP ) (Dasgupta, 1999).
In RP,a high dimensional space is projected onto a lowerdimensional one, using a randomly generated ma-trix.
(Bingham and Mannila, 2001) show that unlikeLSA or principal component analysis (PCA), RPis computationally efficient for large corpora, whilealso retaining accurate vector similarity and yieldingcomparable results.Explicit Semantic Analysis (ESA) (Gabrilovichand Markovitch, 2007).
ESA uses encyclopedicknowledge in an information retrieval framework togenerate a semantic interpretation of words.
It relieson the distribution of words inside Wikipedia arti-cles, thus building a semantic representation for agiven word using a word-document association.Salient Semantic Analysis (SSA) (Hassan and Mi-halcea, 2011).
SSA incorporates a similar seman-tic abstraction as ESA, yet it uses salient con-cepts gathered from encyclopedic knowledge, wherea ?concept?
represents an unambiguous expressionwhich affords an encyclopedic definition.
Saliencyin this case is determined based on the word beinghyperlinked in context, implying that it is highly rel-evant to the given text.In order to determine the similarity of two textfragments, we employ two variations: the typicalcosine similarity (cos) and a best alignment strat-egy (align), which we explain in more detail inthe paragraph below.
Both variations were pairedwith the ESA, and SSA systems resulting in foursimilarity scores that were used as features by ourmeta-system, namely ESAcos, ESAalign, SSAcos,and SSAalign; in addition, we also used BOWcos,LSAcos, and RPcos.Best Alignment Strategy (align).
Let Ta and Tb betwo text fragments of size a and b respectively.
Afterremoving all stopwords, we first determine the num-5We use the LSA implementation available at code.google.com/p/semanticvectors/.ber of shared terms (?)
between Ta and Tb.
Second,we calculate the semantic relatedness of all possiblepairings between non-shared terms in Ta and Tb.
Wefurther filter these possible combinations by creatinga list ?
which holds the strongest semantic pairingsbetween the fragments?
terms, such that each termcan only belong to one and only one pair.Sim(Ta, Tb) =(?
+?|?|i=1 ?i)?
(2ab)a+ b(8)where ?i is the similarity score for the ith pairing.3.3.3 Opinion Aware FeaturesWe design opinion-aware features to capture sen-tence similarity on the subjectivity level based on theoutput of three subjectivity analysis systems.
Intu-itively, two sentences are similar in terms of sub-jectivity if there exists similar opinion expressionswhich also share similar opinion holders.OpinionFinder (Wilson et al 2005) is a publiclyavailable opinion extraction model that annotates thesubjectivity of new text based on the presence (orabsence) of words or phrases in a large lexicon.
Thesystem consists of a two step process, by feedingthe sentences identified as subjective or objectiveby a rule-based high-precision classifier to a high-recall classifier that iteratively learns from the re-maining corpus.
For each sentence in a STS pair,the two classifiers provide two predictions; a subjec-tivity similarity score (SUBJSL) is computed as fol-lows.
If both sentences are classified as subjectiveor objective, the score is 1; if one is subjective andthe other one is objective, the score is -1; otherwiseit is 0.
We also make use of the output of the sub-jective expression identifier in OpinionFinder.
Wefirst record how many expressions the two sentenceshave: feature NUMEX1 and NUMEX2.
Then wecompare how many tokens these expressions shareand we normalize by the total number of expressions(feature EXPR).We compute the difference between the probabil-ities of the two sentences being subjective (SUBJD-IFF), by employing a logistic regression classifierusing LIBLINEAR (Fan et al 2008) trained on theMPQA corpus.
The smaller the difference, the moresimilar the sentences are in terms of subjectivity.We also employ features produced by the opinion-extraction model of Yang and Cardie (Yang andCardie, 2012), which is better suited to process ex-224pressions of arbitrary length.
Specifically, for eachsentence, we extract subjective expressions and gen-erate the following features.
SUBJCNT is a binaryfeature which is equal to 1 if both sentences con-tain a subjective expression.
DSEALGN marks thenumber of shared words between subjective expres-sions in two sentences, while DSESIM representstheir similarity beyond the word level.
We repre-sent the subjective expressions in each sentence asa feature vector, containing unigrams extracted fromthe expressions, their part-of-speech, their WordNethypernyms and their subjectivity label6, and com-pute the cosine similarity between the feature vec-tors.
The holder of the opinion expressions is ex-tracted with the aid of a dependency parser7.
In mostcases, the opinion holder and the opinion expressionare related by the dependency relation subj.
This re-lation is used to expand the verb dependents in theopinion expression and identify the opinion holderor AGENT.3.4 Meta-learningEach metric described above provides one individ-ual score for every sentence-pair in both the train-ing and test set.
These scores then serve as in-put to a meta-learner, which adjusts their impor-tance, and thus their bearing on the overall similar-ity score predicted by the system.
We experimentedwith regression and decision tree based algorithmsby performing 10-fold cross validation on the 2012training data; these types of learners are particularlywell suited to maintain the ordinality of the seman-tic similarity scores (i.e.
a score of 4.5 is closerto either 4 or 5, implying that the two sentencesare mostly or fully equivalent, while also being farfurther away from 0, implying no semantic relat-edness between the two sentences).
We obtainedconsistent results when using support vector regres-sion with polynomial kernel (Drucker et al 1997;Smola and Schoelkopf, 1998) (SV R) and randomsubspace meta-classification with tree learners (Ho,1998) (RandSubspace)8.We submitted three system variations basedon the training corpus (first word in the sys-6Label is based on the OpinionFinder subjectivity lexicon(Wiebe et al 2005).7nlp.stanford.edu/software/8Included with the Weka framework (Hall et al 2009); weused the default values for both algorithms.System FNWN headlines OnWN SMT Meancomb.RandSubSpace 0.331 0.677 0.514 0.337 0.494comb.SVR 0.362 0.669 0.510 0.341 0.494indv.RandSubspace 0.331 0.677 0.548 0.277 0.483baseline-tokencos 0.215 0.540 0.283 0.286 0.364Table 1: Evaluation results (Agirre et al 2013).tem name) or the learning methodology (secondword) used: comb.RandSubspace, comb.SV R andindv.RandSubspace.
For comb, training was per-formed on the merged version of the entire 2012 SE-MEVAL dataset.
For indv, predictions for OnWNand SMT test data were based on training onmatching OnWN and SMT 9 data from 2012, pre-dictions for the other test sets were computed usingthe combined version (comb).4 Results and DiscussionTable 2 lists the correlations obtained betweenthe scores assigned by each one of the featureswe used and the scores assigned by the humanjudges.
It is interesting to note that overall, corpus-based measures are stronger performers compared toknowledge-based measures.
The top contenders inthe former group are ESAalign, SSAalign, LSAcos,and RPcos, indicating that these methods are able toleverage a significant amount of semantic informa-tion from text.
While LSAcos achieves high corre-lations on many of the datasets, replacing the singu-lar value decomposition operation by random pro-jection to a lower-dimension space (RP ) achievescompetitive results while also being computation-ally efficient.
This observation is in line with priorliterature (Bingham and Mannila, 2001).
Amongthe knowledge-based methods, JCN and Pathachieve high performance on more than five of thedatasets.
In some cases, particularly on the 2013test data, the shortest path method (Path) peformsbetter or on par with the performance attained byother knowledge-based measures, despite its com-putational simplicity.
While opinion-based mea-sures do not exhibit the same high correlation, weshould remember that none of the datasets displaysconsistent opinion content, nor were they anno-tated with this aspect in mind, in order for this in-formation to be properly leveraged and evaluated.9The SMT training set is a combination of SMTeuroparl(in this paper abbreviated as SMTep) and SMTnews data.225Train 2012 Test 2012 Test 2013Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMTKnowledge-based measuresJCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25Corpus-based measuresBOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34Opinion-aware measuresAGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0Table 2: Correlation of individual features for the training and test sets with the gold standard.Nonetheless, we notice several promising features,such as DSEALIGN and EXPR.
Lower cor-relations seem to be associated with shorter spansof text, since when averaging all opinion-based cor-relations per dataset, MSRvid (x2), OnWN (x2),and headlines display the lowest average correla-tion, ranging from 0 to 0.03.
This matches theexpectation that opinionated content can be easieridentified in longer contexts, as additional subjectiveelements amount to a stronger prediction.
The otherseven datasets consist of longer spans of text; theydisplay an average opinion-based correlation be-tween 0.07 and 0.12, with the exception of FNWNand SMTnews at 0.04 and 0.01, respectively.Our systems performed well, ranking 38, 39 and45 among the 88 competing systems in *SEM 2013(see Table 1), with the best being comb.SVR andcomb.RandSubspace, both with a mean correlationof 0.494.
We noticed from our participation inSEMEVAL 2012 (Banea et al 2012), that trainingand testing on the same type of data achieves thebest results; this receives further support when con-sidering the performance of the indv.RandSubspacevariation on the OnWN data10, which exhibits a10The SMT test data is not part of the same corpus as either0.034 correlation increase over our next best sys-tem (comb.RandSubspace).
While we do surpass thebag-of-words cosine baseline (baseline-tokencos)computed by the task organizers by a 0.13 differ-ence in correlation, we fall short by 0.124 from theperformance of the best system in the STS task.5 ConclusionsTo participate in the STS *SEM 2013 task, we con-structed a meta-learner framework that combinestraditional knowledge and corpus-based methods,while also introducing novel opinion analysis basedmetrics.
While the *SEM data is not particularlysuited for evaluating the performance of opinion fea-tures, this is nonetheless a first step toward conduct-ing text similarity research while also consideringthe subjective dimension of text.
Our system varia-tions ranked 38, 39 and 45 among the 88 participat-ing systems.AcknowledgmentsThis material is based in part upon work sup-ported by the National Science Foundation CA-REER award #0747340 and IIS awards #1018613,SMTep or SMTnews.226#0208798 and #0916046.
This work was sup-ported in part by DARPA-BAA-12-47 DEFT grant#12475008.
Any opinions, findings, and conclu-sions or recommendations expressed in this materialare those of the authors and do not necessarily reflectthe views of the National Science Foundation or theDefense Advanced Research Projects Agency.ReferencesE.
Agirre, D. Cer, M. Diab, and A. Gonzalez.
2012.Semeval-2012 task 6: A pilot on semantic textual sim-ilarity.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), in con-junction with the First Joint Conference on Lexical andComputational Semantics (*SEM 2012).E.
Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.Guo.
2013.
*SEM 2013 Shared Task: Semantic Tex-tual Similarity, including a Pilot on Typed-Similarity.In Proceedings of the Second Joint Conference on Lex-ical and Computational Semantics (*SEM 2013), At-lanta, GA, USA.C.
Banea, S. Hassan, M. Mohler, and R. Mihalcea.
2012.UNT: A supervised synergistic approach to seman-tic text similarity.
In Proceedings of the First JointConference on Lexical and Computational Semantics(*SEM 2012), pages 635?642, Montreal, Canada.E.
Bingham and H. Mannila.
2001.
Random projectionin dimensionality reduction: applications to image andtext data.
In Proceedings of the seventh ACM SIGKDDinternational conference on Knowledge discovery anddata mining (KDD 2001), pages 245?250, San Fran-cisco, CA, USA.C.
Burgess, K. Livesay, and K. Lund.
1998.
Explorationsin context space: words, sentences, discourse.
Dis-course Processes, 25(2):211?257.K.
Church and P. Hanks.
1990.
Word association norms,mutual information, and lexicography.
ComputationalLinguistics, 16(1):22?29.S.
Dasgupta.
1999.
Learning mixtures of Gaussians.
In40th Annual Symposium on Foundations of ComputerScience (FOCS 1999), pages 634?644, New York, NY,USA.H.
Drucker, C. J. Burges, L. Kaufman, A. Smola, andVladimir Vapnik.
1997.
Support vector regressionmachines.
Advances in Neural Information Process-ing Systems, 9:155?161.R.
Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008.Liblinear: A library for large linear classification.
TheJournal of Machine Learning Research, 9:1871?1874.E.
Gabrilovich and S. Markovitch.
2007.
Comput-ing semantic relatedness using Wikipedia-based ex-plicit semantic analysis.
In Proceedings of the 20thAAAI International Conference on Artificial Intelli-gence (AAAI?07), pages 1606?1611, Hyderabad, In-dia.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and Ian H. Witten.
2009.
The WEKA datamining software: An update.
SIGKDD Explorations,11(1).S.
Hassan and R. Mihalcea.
2011.
Measuring semanticrelatedness using salient encyclopedic concepts.
Arti-ficial Intelligence, Special Issue.T.
K. Ho.
1998.
The Random Subspace Method forConstructing Decision Forests.
IEEE Transactions onPattern Analysis and Machine Intelligence, 20(8):832?844.A.
Islam and D. Inkpen.
2006.
Second order co-occurrence PMI for determining the semantic similar-ity of words.
In Proceedings of the 5th Conference onLanguage Resources and Evaluation (LREC 06), vol-ume 2, pages 1033?1038, Genoa, Italy, July.A.
Islam and D. Inkpen.
2009.
Semantic Similarity ofShort Texts.
In Nicolas Nicolov, Galia Angelova, andRuslan Mitkov, editors, Recent Advances in NaturalLanguage Processing V, volume 309 of Current Issuesin Linguistic Theory, pages 227?236.
John Benjamins,Amsterdam & Philadelphia.J.
J. Jiang and D. W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InInternational Conference Research on ComputationalLinguistics (ROCLING X), pages 9008+, September.T.
K. Landauer, T. K. L, D. Laham, B. Rehder, and M.E.
Schreiner.
1997.
How well can passage meaningbe derived without using word order?
a comparison oflatent semantic analysis and humans.M.
Lapata and R. Barzilay.
2005.
Automatic evaluationof text coherence: Models and representations.
In Pro-ceedings of the 19th International Joint Conference onArtificial Intelligence, Edinburgh.C.
Leacock and M. Chodorow.
1998.
Combining localcontext and WordNet similarity for word sense identi-fication.
In WordNet: An Electronic Lexical Database,pages 305?332.M.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In SIGDOC ?86: Pro-ceedings of the 5th annual international conference onSystems documentation, pages 24?26, New York, NY,USA.
ACM.C.
Lin and E. Hovy.
2003.
Automatic evaluation of sum-maries using n-gram co-occurrence statistics.
In Pro-ceedings of Human Language Technology Conference(HLT-NAACL 2003), Edmonton, Canada, May.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the Fifteenth Interna-227tional Conference on Machine Learning, pages 296?304, Madison, Wisconsin.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and knowledge-based measures of textsemantic similarity.
In Proceedings of the AmericanAssociation for Artificial Intelligence (AAAI 2006),pages 775?780, Boston, MA, US.G.
A. Miller.
1995.
WordNet: a Lexical database forEnglish.
Communications of the Association for Com-puting Machinery, 38(11):39?41.M.
Mohler and R. Mihalcea.
2009.
Text-to-text seman-tic similarity for automatic short answer grading.
InProceedings of the European Association for Compu-tational Linguistics (EACL 2009), Athens, Greece.M.
Mohler, R. Bunescu, and R. Mihalcea.
2011.
Learn-ing to grade short answer questions using semanticsimilarity measures and dependency graph alignments.In Proceedings of the Association for ComputationalLinguistics ?
Human Language Technologies (ACL-HLT 2011), Portland, Oregon, USA.R.
M. A. Nawab, M. Stevenson, and P. Clough.
2011.External plagiarism detection using information re-trieval and sequence alignment: Notebook for PAN atCLEF 2011.
In Proceedings of the 5th InternationalWorkshop on Uncovering Plagiarism, Authorship, andSocial Software Misuse (PAN 2011).K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 311?318, Philadelphia, PA.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet:: Similarity-Measuring the Relatedness ofConcepts.
Proceedings of the National Conference onArtificial Intelligence, pages 1024?1025.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In In Proceedingsof the 14th International Joint Conference on ArtificialIntelligence, pages 448?453.J.
Rocchio, 1971.
Relevance feedback in information re-trieval.
Prentice Hall, Ing.
Englewood Cliffs, New Jer-sey.G.
Salton and C. Buckley.
1997.
Term weighting ap-proaches in automatic text retrieval.
In Readings inInformation Retrieval.
Morgan Kaufmann Publishers,San Francisco, CA.G.
Salton and M. Lesk, 1971.
The SMART Retrieval Sys-tem: Experiments in Automatic Document Processing,chapter Computer evaluation of indexing and text pro-cessing.
Prentice Hall, Ing.
Englewood Cliffs, NewJersey.G.
Salton, A. Singhal, M. Mitra, and C. Buckley.
1997.Automatic text structuring and summarization.
Infor-mation Processing and Management, 2(32).H.
Schutze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?124.A.
Smola and B. Schoelkopf.
1998.
A tutorial on sup-port vector regression.
NeuroCOLT2 Technical Re-port NC2-TR-1998-030.P.
D. Turney.
2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe 12th European Conference on Machine Learning(ECML?01), pages 491?502, Freiburg, Germany.J.
Wiebe and E. Riloff.
2005.
Creating subjective andobjective sentence classifiers from unannotated texts.In Proceedings of the 6th international conference onComputational Linguistics and Intelligent Text Pro-cessing (CICLing 2005), pages 486?497, Mexico City,Mexico.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 39(2-3):165?210.T.
Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,and Siddharth Patwardhan.
2005.
OpinionFinder:A system for subjectivity analysis.
In Proceedingsof HLT/EMNLP on Interactive Demonstrations, pages34?35, Vancouver, BC, Canada.Z.
Wu and M. Palmer.
1994.
Verbs semantics and lexicalselection.
In Proceedings of the 32nd annual meetingon Association for Computational Linguistics, pages133?-138, Las Cruces, New Mexico.B.
Yang and C. Cardie.
2012.
Extracting opinion expres-sions with semi-markov conditional random fields.
InProceedings of the conference on Empirical Meth-ods in Natural Language Processing.
Association forComputational Linguistics.228
