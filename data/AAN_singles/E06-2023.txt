Unsupervised Discovery of Persian MorphemesMohsen ArabsorkhiComputer Science and Engineering Dept.,Shiraz University,Shiraz, Iranmarabsorkhi@cse.shirazu.ac.irMehrnoush ShamsfardElectrical and Computer Engineering Dept.,Shahid Beheshti University,Tehran, Iranm-shams@sbu.ac.irAbstractThis paper reports the present results of aresearch on unsupervised Persian mor-pheme discovery.
In this paper we pre-sent a method for discovering the mor-phemes of Persian language throughautomatic analysis of corpora.
We util-ized a Minimum Description Length(MDL) based algorithm with some im-provements and applied it to Persian cor-pus.
Our improvements include enhanc-ing the cost function using some heuris-tics, preventing the split of high fre-quency chunks, exploiting penalty forfirst and last letters and distinguishingpre-parts and post-parts.
Our improvedapproach has raised the precision, recalland f-measure of discovery by respec-tively %32, %17 and %23.1 IntroductionAccording to linguistic theory, morphemes areconsidered to be the smallest meaning-bearingelements of a language.
However, no adequatelanguage-independent definition of the word as aunit has been agreed upon.
If effective methodscan be devised for the unsupervised discovery ofmorphemes, they could aid the formulation of alinguistic theory of morphology for a new lan-guage.
The utilization of morphemes as basicrepresentational units in a statistical languagemodel instead of words seems a promisingcourse [Creutz, 2004].Many natural language processing tasks, includ-ing parsing, semantic modeling, information re-trieval, and machine translation, frequently re-quire a morphological analysis of the language athand.
The task of a morphological analyzer is toidentify the lexeme, citation form, or inflectionclass of surface word forms in a language.
Itseems that even approximate automated morpho-logical analysis would be beneficial for many NLapplications dealing with large vocabularies (e.g.text retrieval applications).
On the other hand,the construction of a comprehensivemorphological analyzer for a language based onlinguistic theory requires a considerable amountof work by experts.
This is both slow andexpensive and therefore not applicable to alllanguages.
Consequently, it is important todevelop methods that are able to discover andinduce morphology for a language based onunsupervised analysis of large amounts of data.Persian is the most-spoken of the modern Iranianlanguages, which, according to traditional classi-fication, with the Indo-Aryan language constitutethe Indo-Iranian group within the Satem branchof the Indo-European family.
Persian is writtenright-to-left in the Arabic alphabet with a fewmodifications.
Three of 32 Persian letters dodouble duty in representing both consonant andvowels: /h/, /v/, /y/, doubling, as /e/ (word fi-nally), /u/, and /I/ respectively [Mahootian 97].Persian morphology is an affixal system consist-ing mainly of suffixes and a few prefixes.
Thenominal paradigm consists of a relatively smallnumber of affixes [Megerdoomian 2000].
Theverbal inflectional system is quite regular andcan be obtained by the combination of prefixes,stems, inflections and auxiliaries.
Persian mor-phologically is a powerful language and there area lot of morphological rules in it.
For examplewe can derive more than 200 words from thestem of the verb ?raftan?
(to go).
Table 1 showssome morphological rules and table 2 illustratessome inflections and derivations as examples.There is no morphological irregularity in Persianand all of the words are stems or derived words,except some imported foreign words, that are notcompatible with Persian rules (such as irregularArabic plural forms imported to Persian.
)simple past verb past stem + identifiercontinuous present verb Mi+present stem+identifierNoun present stem +  (y)e?Table 1.
Some Persian morphological rules.175POS Persian TranslationVerb Infinitive Nega?t?n to writePresent Verb Stem Negar WritePast Verb Stem Nega?t wroteContinuous Present verb mi-negar-?m I am writingSimple Past verb nega?t-?m I wroteNoun from verb Neg?re?
WritingTable 2.
Some example words.2 Related WorksThere are several approaches for inducing mor-phemes from text.
Some of them are supervisedand use some information about words such aspart of speech (POS) tags, morphological rules,suffix list, lexicon, etc.
Other approaches are un-supervised and use only raw corpus to extractmorphemes.
In this section we concentrate onsome unsupervised methods as related works.
[Monson 2004] presents a framework for unsu-pervised induction of natural language morphol-ogy, wherein candidate suffixes are grouped intocandidate inflection classes, which are thenplaced in a lattice structure.
With similar ar-ranged inflection classes placed near one candi-date in the lattice, it proposes this structure to bean ideal search space in which to isolate the trueinflection classes of a language.
[Schone and Ju-rafsky 2000] presents an unsupervised model inwhich knowledge-free distributional cues arecombined orthography-based with informationautomatically extracted from semantic word co-occurrence patterns in the input corpus.Word induction from natural language textwithout word boundaries is also studied in[Deligne and Bimtol 1997], where MDL- basedmodel optimization measures are used.
Viterbi orthe forward- backward algorithm (an EM algo-rithm) is used for improving the segmentation ofthe corpus.
Some of the approaches removespaces from text and try to identify word bounda-ries utilizing e.g.
entropy- based measures, as in[Zellig and Harris, 1967; Redlich, 1993].
[Brent, 1999] presents a general, modular prob-abilistic model structure for word discovery.
Heuses a minimum representation length criterionfor model optimization and applies an incre-mental, greedy search algorithm which is suit-able for on- line learning such that childrenmight employ.
[Baroni, et al 2002] proposes an algorithmthat takes an unannotated corpus as its input, anda ranked list of probable returning related pairsas its output.
It discovers related pairs by lookingmorphologically for pairs that are both ortho-graphically and semantically similar.
[Goldsmith 2001] concentrates on stem+suffix-languages, in particular Indo-European lan-guages, and produces output that would match asclosely as possible with the analysis given by ahuman morphologist.
He further assumes thatstems form groups that he calls signatures, andeach signature shares a set of possible affixes.
Heapplies an MDL criterion for model optimiza-tion.3 Inducing Persian MorphemesOur task is to find the correct segmentation ofthe source text into morphemes while we don?thave any information about words or any struc-tural rules to make them.
So we use an algorithmthat works based on minimization of some heu-ristic cost function.
Our approach is based on avariation of MDL model and contains somemodifications to adopt it for Persian and improvethe results especially for this language.Minimum Description Length (MDL) analysis isbased on information theory [Rissanen 1989].Given a corpus, an MDL model defines a de-scription length of the corpus.
Given a probabil-istic model of the corpus, the description lengthis the sum of the most compact statement of themodel expressible in some universal language ofalgorithms, plus the length of the optimal com-pression of the corpus, when we use the prob-abilistic model to compress the data.
The lengthof the optimal compression of the corpus is thebase 2 logarithm of the reciprocal of the prob-ability assigned to the corpus by the model.Since we are concerned with morphologicalanalysis, we will henceforth use the more spe-cific term the morphology rather than model.
(1))|(log)(log),(22 MCpMpMModelCCorpusnLengthDescriptioMDL analysis proposes that the morphology Mwhich minimizes the objective function in (1) isthe best morphology of the corpus.
Intuitively,the first term (the length of the model, in bits)expresses the conciseness of the morphology,giving us strong motivation to find the simplestpossible morphology, while the second term ex-presses how well the model describes the corpusin question.The method proposed at [Creutz 2002; 2004] is aderivation of MDL algorithm which we use asthe basis of our approach.
In this algorithm, eachtime a new word token is read from the input,different ways of segmenting it into morphs areevaluated, and the one with minimum cost is se-lected.
First, the word as a whole is considered to176be a morph and added to the morph list.
Then,every possible splits of the word into two partsare evaluated.
The algorithm selects the split (orno split) that yields the minimum total cost.
Incase of no split, the processing of the word isfinished and the next word is read from input.Otherwise, the search for a split is performedrecursively on the two segments.
The order ofsplits can be represented as a binary tree for eachword, where the leaves represent the morphsmaking up the word, and the tree structure de-scribes the ordering of the splits.During model search, an overall hierarchical datastructure is used for keeping track of the currentsegmentation of every word type encountered sofar.
There is an occurrence counter field for eachmorph in morph list.
The occurrence counts fromsegments flow down through the hierarchicalstructure, so that the count of a child alwaysequals the sum of the counts of its parents.
Theoccurrence counts of the leaf nodes are used forcomputing the relative frequencies of themorphs.
To find out the morph sequence that aword consists of, we look up the chunk that isidentical to the word, and trace the split indicesrecursively until we reach the leaves, which arethe morphs.
This algorithm was applied on Per-sian corpus and results were not satisfiable.
Sowe gradually, applied some heuristic functions toget better results.
Our approach contains (1) Util-izing a heuristic function to compute cost moreprecisely, (2) Using Threshold to prevent split-ting high frequency chunks, (3) Exerting Penaltyfor first and last letters and (4) DistinguishingPre-parts and post-parts.After analyzing the results of the initial algo-rithm, we observed that the algorithm tries tosplit words into some morphemes to keep thecost minimum based on current morph list sorecognized morphemes may prevent extractingnew correct morphemes.
Therefore we applied anew reward function to find the best splittingwith respect to the next words.
In fact our func-tion (equation (2)) rewards to the morphemesthat are used in next words frequently.
(2)  }/)1)((*)({ WNLPlenLPfreqRFCWNRPlenRPfreq *}/)1)((*)({ In which LP is the left part of word, RP is theright part of it, Len (p) is the length of part P(number of characters), freq(p) is the frequencyof part P in corpus, WN is the number of words(corpus size) and C is a constant number.In this cost function freq(LP)/WN can be inter-preted as the probability of LP being a morph inthe corpus.
We use len(P) to increase the rewardfor long segments that are frequent and it is de-creased by 1 to avoid mono-letter splitting.
Wefound the parameter C empirically.
Figure 1shows the results of the algorithm for variousamounts of C.405060701 2 3 4 5 6 7 8 9 10RecallPrecisionf-measureFigure 1.
Algorithm results for various Cs.Our experiments showed that the best value for Cis 8.
It means that RP is 8 times more importantthat LP.
This may be because of the fact that Per-sian is written right-to-left and moreover most ofaffixes are suffixes.The final cost function in our algorithm is shownin equation (3).
(3) RFEF 'In which E is the description length, calculated inequation (1) and RF the cost function describedin equation (2).
Since RF values are in a limitedrange, they are large numbers (in comparisonwith other function values) in the first iterations,but after processing some words, cost functionvalues will become large so that the RF is notsignificant any more.
So we used the differenceof cost function in two sequential processes (twoiterations) instead of the cost function itself.
Inother words in our algorithm the cost function(E) is re-evaluated and replaced with its changes(?E).
This improvement causes better splitting insome words such as the words shown in table 3.
(Each word is shown by its written form in Eng-lish alphabet : its pronunciation (its translation)).word Initial alg.
Improved alg.
?n: ?en (sand) ?n ?n?nva: ?en?va(that can hear)?n + va ?nv (hear) +a (subjectiveadjective sign)mi-?nvm:mi-?en?v?m(I hear)mi (continuoustense sign) +?n + v + mmi + ?nv + m(first personpronoun)Table 3.
Comparing the results of the initial andimproved algorithm.We also used a frequency threshold T to avoidsplitting words that are observed as a substring inother words.
It means that in the current algo-rithm, for each word we first compute its fre-quency and it will be splitted just when it is used177less than the threshold.
Based on our experi-ments, the best value for T is 4.One of the mostwrong splitting is mono-letter splitting whichmeans that we split just the first or the last letterto be a morpheme.
Our experiments show thatthe first letter splitting occurs more than the lastletter.
So we apply a penalty factor on splitting inthese positions to avoid creating mono-lettermorphemes.Another improvement is that we distinguishedbetween pre-part and post-part.
So splittingbased on observed morphemes will become moreprecise.
In this process each morpheme that isobserved at the left corner of a word, in the firstsplitting phase, is post-part and each of them atthe right corner of a word is pre-part.
Other mor-phemes are added to both pre and post-part lists.4 Experimental ResultsWe applied improved algorithm on Persian cor-pus and observed significant improvements onour results.
Our corpus contains about 4000words from which 100 are selected randomly fortests.
We split selected words to their morphemesboth manually and automatically and computedprecision and recall factors.
For computing recalland precision, we numerated splitting positionsand compared with the gold data.
Precision is thenumber of correct splits divided to the total num-ber of splits done and recall is the number of cor-rect splits divided by total number of gold splits.Our experiments showed that our approach re-sults in increasing the recall measure from 45.53to 53.19, the precision from 48.24 to 63.29 and f-measure from 46.91 to 57.80.
Precision im-provement is significantly more than recall.
Thishas been predictable as we make algorithm toprevent unsure splitting.
So usually done splitsare correct whereas there are some necessarysplitting that have not been done.5 ConclusionIn this paper we proposed an improved approachfor morpheme discovery from Persian texts.
Ouralgorithm is an improvement of an existing algo-rithm based on MDL model.
The improvementsare done by adding some heuristic functions tothe split procedure and also introducing new costand reward functions.
Experiments showed verygood results obtained by our improvements.The main problems for our experiments were thelack of good, safe and large corpora and alsohandling the foreign words which do not obeythe morphological rules of Persian.Our proposed improvements are rarely language-dependent (such as right-to-left feature of Per-sian) and could be applied to other languageswith a little customization.
To extend the projectwe suppose to work on some probabilistic distri-bution functions which help to split words cor-rectly.
Moreover we plan to test our algorithm onlarge Persian and also English corpora.ReferencesMarco Baroni, Johannes Matiasek, Harald Trost 2002.Unsupervised discovery of morphologically relatedwords based on orthographic and semantic similar-ity, ACL Workshop on Morphological andPhonological Learning.Michael R. Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word dis-covery, Machine Learning, 34:71?105.Mathias Creutz, Krista Lagus, 2002.
Unsuperviseddiscovery of morphemes.
Workshop on Morpho-logical and Phonological Learning of ACL?02,Philadelphia, Pennsylvania, USA, 21?30.Mathias Creutz, Krista Lagus, 2004.
Induction of asimple morphology for highly inflecting languages.Proceedings of 7th Meeting of SIGPHON, Bar-celona.
43?51S.
Deligne and F. Bimbot.
1997.
Inference of vari-able-length linguistic and acoustic units by multi-grams.
Speech Communication, 23:223?241.John Goldsmith, 2001.
Unsupervised learning of themorphology of a natural language, ComputationalLinguistics, 27(2): 153?198Zellig.
Harris, 1967.
Morpheme Boundaries withinWords: Report on a Computer Test.
Transforma-tions and Discourse Analysis Papers, 73.Shahrzad Mahootian, 1997.
Persian, Routledge.Karine Megerdoomian, 2000 Persian ComputationalMorphology: A unification-based approach,NMSU, CLR, MCCS Report.Christian Monson.
2004.
A Framework for Unsuper-vised Natural Language Morphology Induction,The Student Workshop at ACL-04.A.
Norman Redlich.
1993.
Redundancy reduction as astrategy for unsupervised learning.
Neural Com-putation, 5:289?304.Jorma Rissanen 1989, Stochastic Complexity inStatistical Inquiry, World Scientific.P.
Schone and D. Jurafsky.
2000.
Knowldedge-freeinduction of morphology using latent semanticanalysis, Proceedings of the Conference onComputational Natural Language Learning.178
