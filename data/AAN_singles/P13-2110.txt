Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 623?627,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Lattice-based Framework for Joint Chinese Word Segmentation,POS Tagging and ParsingZhiguo Wang1, Chengqing Zong1 and Nianwen Xue21National Laboratory of Pattern Recognition,Institute of Automation, Chinese Academy of Sciences, Beijing, China, 1001902Computer Science Department, Brandeis University, Waltham, MA 02452{zgwang, cqzong}@nlpr.ia.ac.cn   xuen@brandeis.eduAbstractFor the cascaded task of Chinese word seg-mentation, POS tagging and parsing, the pipe-line approach suffers from error propagationwhile the joint learning approach suffers frominefficient decoding due to the large combinedsearch space.
In this paper, we present a novellattice-based framework in which a Chinesesentence is first segmented into a word lattice,and then a lattice-based POS tagger and a lat-tice-based parser are used to process the latticefrom two different viewpoints: sequential POStagging and hierarchical tree building.
A strat-egy is designed to exploit the complementarystrengths of the tagger and parser, and encour-age them to predict agreed structures.
Experi-mental results on Chinese Treebank show thatour lattice-based framework significantly im-proves the accuracy of the three sub-tasks.1 IntroductionPrevious work on syntactic parsing generallyassumes a processing pipeline where an inputsentence is first tokenized, POS-tagged and thenparsed (Collins, 1999; Charniak, 2000; Petrovand Klein, 2007).
This approach works well forlanguages like English where automatic tokeni-zation and POS tagging can be performed withhigh accuracy without the guidance of the high-level syntactic structure.
Such an approach, how-ever, is not optimal for languages like Chinesewhere there are no natural delimiters for wordboundaries, and word segmentation (or tokeniza-tion) is a non-trivial research problem by itself.Errors in word segmentation would propagate tolater processing stages such as POS tagging andsyntactic parsing.
More importantly, Chinese is alanguage that lacks the morphological clues thathelp determine the POS tag of a word.
For ex-ample, ??
(?investigate/investigation?)
caneither be a verb (?investigate?)
or a noun (?inves-tigation?
), and there is no morphological varia-tion between its verbal form and nominal form.This contributes to the relatively low accuracy(95% or below) in Chinese POS tagging whenevaluated as a stand-alone task (Sun and Uszko-reit, 2012), and the noun/verb ambiguity is a ma-jor source of error.More recently, joint inference approacheshave been proposed to address the shortcomingsof the pipeline approach.
Qian and Liu (2012)proposed a joint inference approach where syn-tactic parsing can provide feedback to wordsegmentation and POS tagging and showed thatthe joint inference approach leads to improve-ments in all three sub-tasks.
However, a majorchallenge for joint inference approach is that thelarge combined search space makes efficient de-coding and parameter estimation very hard.In this paper, we present a novel lattice-basedframework for Chinese.
An input Chinese sen-tence is first segmented into a word lattice,which is a compact representation of a small setof high-quality word segmentations.
Then, a lat-tice-based POS tagger and a lattice-based parserare used to process the word lattice from twodifferent viewpoints.
We next employ the dualdecomposition method to exploit the comple-mentary strengths of the tagger and parser, andencourage them to predict agreed structures.
Ex-perimental results show that our lattice-basedframework significantly improves the accuraciesof the three sub-tasks2 The Lattice-based FrameworkFigure 1 gives the organization of the framework.There are four types of linguistic structures: aChinese sentence, the word lattice, tagged wordsequence and parse tree of the Chinese sentence.An example for each structure is provided inFigure 2.
We can see that the terminals and pre-terminals of a parse tree constitute a tagged wordsequence.
Therefore, we define a comparatorbetween a tagged word sequence and a parse tree:if they contain the same word sequence and POStags, they are equal, otherwise unequal.623Figure 1 also shows the workflow of theframework.
First, the Chinese sentence is seg-mented into a word lattice using the word seg-mentation system.
Then the word lattice is fedinto the lattice-based POS tagger to produce atagged word sequence   and into the lattice-based parser to separately produce a parse tree  .We then compare   with   to see whether theyare equal.
If they are equal, we output   as thefinal result.
Otherwise, the guidance generatorgenerates some guidance orders based on thedifference between   and  , and guides the tag-ger and the parser to process the lattice again.This procedure may iterate many times until thetagger and parser predict equal structures.The motivation to design such a framework isas follows.
First, state-of-the-art word segmenta-tion systems can now perform with high accura-cy.
We can easily get an F1 score greater than96%, and an oracle (upper bound) F1 scoregreater than 99%  for the word lattice (Jiang etal., 2008).
Therefore, a word lattice provides us agood enough search space to allow sufficientinteraction among word segmentation, POS tag-ging and parsing systems.
Second, both the lat-tice-based POS tagger and the lattice-based pars-er can select word segmentation from the wordlattice and predict POS tags, but they do so fromtwo different perspectives.
The lattice-based POStagger looks at a path in a word lattice as a se-quence and performs sequence labeling based onlinear local context, while the lattice-based pars-er builds the parse trees in a hierarchical manner.They have different strengths with regard toword segmentation and POS tagging.
We hypo-thesize that exploring the complementarystrengths of the tagger and parser would improveeach of the sub-tasks.We build a character-based model (Xue, 2003)for the word segmentation system, and treatsegmentation as a sequence labeling task, whereeach Chinese character is labeled with a tag.
Weuse the tag set provided in Wang et al (2011)and use the same feature templates.
We use theMaximum Entropy (ME) model to estimate thefeature weights.
To get a word lattice, we firstgenerate N-best word segmentation results, andthen compact the N-best lists into a word latticeby collapsing all the identical words into oneedge.
We also assign a probability to each edge,which is calculated by multiplying the taggingprobabilities of each character in the word.The goal of the lattice-based POS tagger is topredict a tagged word sequence   for an inputword lattice  :   = argmax ?
( ) ?
( )where     ( ) represents the set of all possibletagged word sequences derived from the wordlattice  .
( ) is used to map   onto a global fea-ture vector, and   is the corresponding weightvector.
We use the same non-local feature tem-plates used in Jiang et al (2008) and a similardecoding algorithm.
We use the perceptron algo-rithm (Collins, 2002) for parameter estimation.Goldberg and Elhadad (2011) proposed a lat-tice-based parser for Heberw based on thePCFG-LA model (Matsuzaki et al, 2005).
Weadopted their approach, but found the un-weighted word lattice their parser takes as inputto be ineffective for our Chinese experiments.Instead, we use a weighted lattice as input andweigh each edge in the lattice with the wordprobability.
In our model, each syntactic catego-ry   is split into multiple subcategories  [ ] bylabeling a latent annotation  .
Then, a parse tree????????????
?Brown?s group will leave Shanghai to Guangzhou tonight.
(a) Chinese Sentence(b) Word Lattice??
????????
?NR NRVVNRPNTP PU?
?NNBrown .GuangzhougoShanghaileavetonightingroup(c) Tagged Word SequenceBrown.GuangzhougoShanghaileavetonightingroup??
???
???
?NR PNTNPPPVVNRNPVPPUNPIPVP?
?NNNP NP?
?VVNRNPVPVP(d) Parse TreeFigure 2: Linguistic structure examples.Chinese SentenceWord SegmentationWord LatticeLattice-based Parser Lattice-based POS TaggerGuidance GeneratorParse Tree Tagged WordSequenceThe Final Parse TreeNoYesEqual?Figure 1: The lattice-based framework.624is refined into  [ ], where X is the latent an-notation vector for all non-terminals in  .
Theprobability of  [ ] is calculated as:  ( [ ]) =   ( [ ] ?
[ ] [ ]) ?
( [ ] ?
)?
( )where the three terms are products of all syntac-tic rule probabilities, lexical rule probabilitiesand word probabilities in  [ ] respectively.3 Combined Optimization Between TheLattice-based POS Tagger and TheLattice-based ParserWe first define some variables to make it easierto compare a tagged word sequence   with aparse tree  .
We define   as the set of all POStags.
For  , we define  ( ,  , )=1 if   contains aPOS tag  ?
spanning from the i-th characterto the j-th character, otherwise  ( ,  , ) = 0.
Wealso define  ( ,  , #) = 1 if   contains the wordspanning from the i-th character to the j-th cha-racter, otherwise  ( ,  , #) = 0.
Similarly, for  ,we define  ( ,  , )=1 if   contains a POS tag  ?
spanning from the i-th character to the j-thcharacter, otherwise  ( ,  ,  ) = 0.
We also define  ( ,  , #)  = 1 if   contains the word spanningfrom the i-th character to the j-th character, oth-erwise  ( ,  , #) = 0.
Therefore,   and   are equal,only if  ( ,  ,  ) =  ( ,  ,  )  for all  ?
[0,  ] ,  ?
[ + 1,  ] and  ?
?
#, otherwise unequal.Our framework expects the tagger and theparser to predict equal structures and we formu-late it as a constraint optimization problem:    ,   = argmax ,    ( ) +   ( )Such that for all  ?
[0, ] ,  ?
[ + 1, ]  and  ?
?
#:  ( ,  ,  ) =  ( ,  , )where   ( ) =  ?
( )  is a scoring functionfrom the viewpoint of the lattice-based POS tag-ger, and   ( ) = log  ( ) is a scoring functionfrom the viewpoint of the lattice-based parser.The dual decomposition (a special case of La-grangian relaxation) method introduced in Ko-modakis et al (2007) is suitable for this problem.Using this method, we solve the primal con-straint optimization problem by optimizing thedual problem.
First, we introduce a vector of La-grange multipliers  ( ,  ,  )  for each equalityconstraint.
Then, the Lagrangian is formulated as:  ( ,  ,  ) =   ( ) +   ( ) +   ( ,  , )( ( ,  ,  )?
( ,  , )) , ,By grouping the terms that depend on   and  ,we rewrite the Lagrangian as  ( , , ) =    ( ) +   ( ,  , ) ( ,  , ) , ,   +   ( )?
( ,  , ) ( ,  , ) , ,Then, the dual objective is  ( ) = max ,  ( , , ) = max    ( ) +   ( ,  , ) ( ,  , ) , ,  + max    ( )?
( ,  , ) ( ,  , ) , ,The dual problem is to find min  ( ).We use the subgradient method (Boyd et al,2003) to minimize the dual.
Following Rush et al(2010), we define the subgradient of   ( ) as:  ( ,  , ) =  ( ,  , )?
( ,  ,  )  for all ( ,  , )Then, adjust  ( ,  ,  ) as follows:   ( ,  , ) =  ( ,  , )?
( ( ,  ,  )?
( ,  , ))where  >0 is a step size.Algorithm 1 presents the subgradient methodto solve the dual problem.
The algorithm initia-lizes the Lagrange multiplier values with 0 (line1) and then iterates many times.
In each iteration,the algorithm finds the best   ( )  and   ( )  byrunning the lattice-based POS tagger (line 3) andthe lattice-based parser (line 4).
If   ( ) and    ( )share the same tagged word sequence (line 5),then the algorithm returns the solution (line 6).Otherwise, the algorithm adjusts the Lagrangemultiplier values based on the differences be-tween    ( ) and   ( ) (line 8).
A crucial point isthat the argmax problems in line 3 and line 4 canbe solved efficiently using the original decodingalgorithms, because the Lagrange multiplier canbe regarded as adjustments for lexical rule prob-abilities and word probabilities.4 ExperimentsWe conduct experiments on the Chinese Tree-bank Version 5.0 and use the standard data splitAlgorithm 1: Combined Optimization1: Set  ( )( ,  , )=0, for all  ( ,  , )2: For k=1 to K3:     ( ) ?
argmax    ( ) + ?
(   )( ,  , ) ( ,  , )  , ,4:     ( ) ?
argmax    ( )?
?
(   )( ,  ,  ) ( ,  ,  )  , ,5:   If  ( )( ,  ,  ) =  ( )( ,  ,  ) for all ( ,  ,  )6:      Return (  ( ),   ( ))7:   Else8:       ( )( ,  ,  ) =  (   )( ,  ,  ) ?
( ( )( ,  ,  )?
( )( ,  , ))625(Petrov and Klein, 2007).
The traditional evalua-tion metrics for POS tagging and parsing are notsuitable for the joint task.
Following with Qianand Liu (2012), we redefine precision and recallby computing the span of a constituent based oncharacter offsets rather than word offsets.4.1 Performance of the Basic Sub-systemsWe train the word segmentation system with 100iterations of the Maximum Entropy model usingthe OpenNLP toolkit.
Table 1 shows the perfor-mance.
It shows that our word segmentation sys-tem is comparable with the state-of-the-art sys-tems and the upper bound F1 score of the wordlattice exceeds 99.6%.
This indicates that ourword segmentation system can provide a goodsearch space for the lattice-based POS tagger andthe lattice-based parser.To train the lattice-based POS tagger, we gen-erate the word lattice for each sentence in thetraining set using cross validation approach.
Wedivide the entire training set into 18 folds on av-erage (each fold contains 1,000 sentences).
Foreach fold, we segment each sentence in the foldinto a word lattice by compacting 20-best seg-mentation list produced with a model trained onthe other 17 folds.
Then, we train the lattice-based POS tagger with 20 iterations of the aver-age perceptron algorithm.
Table 2 presents thejoint word segmentation and POS tagging per-formance and shows that our lattice-based POStagger obtains results that are comparable withstate-of-the-art systems.We implement the lattice-based parser bymodifying the Berkeley Parser, and train it with5 iterations of the split-merge-smooth strategy(Petrov et al, 2006).
Table 3 shows the perfor-mance, where the ?Pipeline Parser?
representsthe system taking one-best segmentation resultfrom our word segmentation system as input and?Lattice-based Parser?
represents the system tak-ing the compacted word lattice as input.
We findthe lattice-based parser gets better performancethan the pipeline system among all three sub-tasks.4.2 Performance of the FrameworkFor the lattice-based framework, we set the max-imum iteration in Algorithm 1 as K = 20.
Thestep size   is tuned on the development set andempirically set to be 0.8.
Table 4 shows the pars-ing performance on the test set.
It shows that thelattice-based framework achieves improvementover the lattice-based parser alone among allthree sub-tasks: 0.16 points for word segmenta-tion, 1.19 points for POS tagging and 1.65 pointsfor parsing.
It also outperforms the lattice-basedPOS tagger by 0.65 points on POS tagging accu-racy.
Our lattice-based framework also improvesover the best joint inference parsing system(Qian and Liu, 2012) by 0.57 points.5 ConclusionIn this paper, we present a novel lattice-basedframework for the cascaded task of Chineseword segmentation, POS tagging and parsing.We first segment a Chinese sentence into a wordlattice, then process the lattice using a lattice-based POS tagger and a lattice-based parser.
Wealso design a strategy to exploit the complemen-tary strengths of the tagger and the parser andencourage them to predict agreed structures.
Ex-perimental results show that the lattice-basedframework significantly improves the accuraciesof the three tasks.
The parsing accuracy of theframework also outperforms the best joint pars-ing system reported in the literature.P R F(Qian and Liu,2012)Seg.
97.56 98.36 97.96POS 93.43 94.2 93.81Parse 83.03 82.66 82.85Lattice-basedFrameworkSeg.
97.82 97.9 97.86POS 94.36 94.44 94.40Parse 83.34 83.5 83.42Table 4: Lattice-based framework evaluation.P R FPipeline ParserSeg.
96.97 98.06 97.52POS 92.01 93.04 92.52Parse 80.86 81.47 81.17Lattice-basedParserSeg.
97.73 97.66 97.70POS 93.24 93.18 93.21Parse 81.83 81.71 81.77Table 3: Parsing evaluation.P R F (Kruengkrai et al, 2009) 93.28 94.07 93.67(Zhang and Clark, 2010) - - 93.67(Qian and Liu, 2012) 93.1 93.96 93.53(Sun, 2011) - - 94.02Lattice-based POS tagger 93.64 93.87 93.75Table 2: POS tagging evaluation.P R F(Kruengkrai et al, 2009) 97.46 98.29 97.87(Zhang and Clark, 2010) - - 97.78(Qian and Liu, 2012) 97.45 98.24 97.85(Sun, 2011) - - 98.17Our Word Seg.
System 96.97 98.06 97.52Word Lattice Upper Bound 99.55 99.75 99.65Table 1: Word segmentation evaluation.626AcknowledgmentsThe research work has been funded by the Hi-Tech Research and Development Program ("863"Program) of China under Grant No.2011AA01A207, 2012AA011101, and2012AA011102 and also supported by the KeyProject of Knowledge Innovation Program ofChinese Academy of Sciences under GrantNo.KGZD-EW-501.
This work is also supportedin part by the DAPRA via contract HR0011-11-C-0145 entitled "Linguistic Resources for Multi-lingual Processing".ReferencesS.
Boyd, L. Xiao and A. Mutapcic.
2003.
Subgradientmethods.
Lecture notes of EE392o, Stanford Uni-versity.E.
Charniak.
2000.
A maximum?entropy?inspiredparser.
In NAACL ?00, page 132?139.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Michael Collins.
2002.
Discriminative training me-thods for hidden markov models: Theory and expe-riments with perceptron algorithms.
In Proc.
ofEMNLP2002, pages 1-8.Yoav Goldberg and Michael Elhadad.
2011.
JointHebrew segmentation and parsing using a PCFG-LA lattice parser.
In Proc.
of ACL2011.Wenbin Jiang, Haitao Mi and Qun Liu.
2008.
Wordlattice reranking for Chinese word segmentationand part-of-speech tagging.
In Proc.
of Coling 2008,pages 385-392.Komodakis, N., Paragios, N., and Tziritas, G. 2007.MRF optimization via dual decomposition: Mes-sage-passing revisited.
In ICCV 2007.C.
Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K.Torisawa and H. Isahara.
2009.
An error-drivenword-character hybrid model for joint Chineseword segmentation and POS tagging.
In Proc.
ofACL2009, pages 513-521.Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc.
of ACL2005, pages 75-82.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proc.
of ACL2006,pages 433-440.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proc.
of NAACL2007,pages 404-411.Xian Qian and Yang Liu.
2012.
Joint Chinese Wordsegmentation, POS Tagging Parsing.
In Proc.
ofEMNLP 2012, pages 501-511.Alexander M. Rush, David Sontag, Michael Collinsand Tommi Jaakkola.
2010.
On dual decomposi-tion and linear programming relaxations for naturallanguage processing.
In Proc.
of EMNLP2010,pages 1-11.Weiwei Sun.
2011.
A stacked sub-word model forjoint Chinese word segmentation and part-of-speech tagging.
In Proc.
of ACL2011, pages 1385-1394.Weiwei Sun and Hans Uszkoreit.
Capturing paradig-matic and syntagmatic lexical relations: Towardsaccurate Chinese part-of-speech tagging.
In Proc.of ACL2012.Yiou Wang, Jun'ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang and Kentaro Torisa-wa.
2011.
Improving Chinese word segmentationand POS tagging with semi-supervised methods us-ing large auto-analyzed data.
In Proc.
ofIJCNLP2011, pages 309-317.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8 (1).
pages 29-48.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and POS-tagging usinga single discriminative model.
In Proc.
ofEMNLP2010, pages 843-852.627
