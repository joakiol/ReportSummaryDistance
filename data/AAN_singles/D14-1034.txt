Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278?289,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAn Unsupervised Model for Instance Level Subcategorization AcquisitionSimon BakerComputer LaboratoryUniversity of Cambridgesb895@cam.ac.ukRoi ReichartTechnion, IITHaifa, Israelroiri@ie.technion.ac.ilAnna KorhonenComputer LaboratoryUniversity of Cambridgealk23@cam.ac.ukAbstractMost existing systems for subcategoriza-tion frame (SCF) acquisition rely on su-pervised parsing and infer SCF distribu-tions at type, rather than instance level.These systems suffer from poor portabilityacross domains and their benefit for NLPtasks that involve sentence-level process-ing is limited.
We propose a new unsuper-vised, Markov Random Field-based modelfor SCF acquisition which is designedto address these problems.
The systemrelies on supervised POS tagging ratherthan parsing, and is capable of learningSCFs at instance level.
We perform eval-uation against gold standard data whichshows that our system outperforms severalsupervised and type-level SCF baselines.We also conduct task-based evaluation inthe context of verb similarity prediction,demonstrating that a vector space modelbased on our SCFs substantially outper-forms a lexical model and a model basedon a supervised parser1.1 IntroductionSubcategorization frame (SCF) acquisition in-volves identifying the arguments of a predicateand generalizing about its syntactic frames,where each frame specifies the syntactic type andnumber of arguments permitted by the predicate.For example, in sentences (1)-(3) the verb distin-guish takes three different frames, the differencebetween which is not evident when consideringthe phrase structure categorization:(1) Direct Transitive: [They]NP [distin-guished]VP [the mast]NP [of [ships on thehorizon ]NP ]PP .1The verb similarity dataset used for the evaluation of ourmodel is publicly available at ie.technion.ac.il/?roiri/.
(2) Indirect Transitive: [They]NP [distin-guished]VP [between [me and you]ADVP ]PP .
(3) Ditransitive: [They]NP [distinguished]VP[him]NP [from [the other boys]NP ]PP.As SCFs describe the syntactic realization ofthe verbal predicate-argument structure, they arehighly valuable for a variety of NLP tasks.
Forexample, verb subcategorization information hasproven useful for tasks such as parsing (Carrolland Fang, 2004; Arun and Keller, 2005; Cholakovand van Noord, 2010), semantic role labeling(Bharati et al., 2005; Moschitti and Basili, 2005),verb clustering, (Schulte im Walde, 2006; Sunand Korhonen, 2011) and machine translation (hyeHan et al., 2000; Haji?c et al., 2002; Weller et al.,2013).SCF induction is challenging.
The argument-adjunct distinction is difficult even for humans,and is further complicated by the fact that both ar-guments and adjuncts can appear frequently in po-tential argument head positions (Korhonen et al.,2000).
SCFs are also highly sensitive to domainvariation so that both the frames themselves andtheir probabilities vary depending on the meaningand behavior of predicates in the domain in ques-tion (e.g.
(Roland and Jurafsky, 1998; Lippincottet al., 2010; Rimell et al., 2013), Section 4).Because of the strong impact of domain vari-ation, SCF information is best acquired automat-ically.
Existing data-driven SCF induction sys-tems, however, do not port well between do-mains.
Most existing systems rely on hand-written rules (Briscoe and Carroll, 1997; Korho-nen, 2002; Preiss et al., 2007) or simple co-occurrence statistics (O?Donovan et al., 2005;Chesley and Salmon-Alt, 2006; Ienco et al., 2008;Messiant et al., 2008; Lenci et al., 2008; Al-tamirano and Alonso i Alemany, 2010; Kawa-hara and Kurohashi, 2010) applied to the gram-matical dependency output of supervised statisti-cal parsers.
Even the handful of recent systems278that use modern machine learning techniques (De-bowski, 2009; Lippincott et al., 2012; Van deCruys et al., 2012; Reichart and Korhonen, 2013)use supervised parsers to pre-process the data2.Supervised parsers are notoriously sensitive todomain variation (Lease and Charniak, 2005).
Asannotation of data for each new domain is un-realistic, current SCF systems suffer from poorportability.
This problem is compounded forthe many systems that employ manually devel-oped SCF rules because rules are inherently ig-norant to domain-specific preferences.
The fewSCF studies that focused on specific domains (e.g.biomedicine) have reported poor performance dueto these reasons (Rimell et al., 2013).Another limitation of most current SCF systemsis that they produce a type-level SCF lexicon (i.e.a lexicon which lists, for a given predicate, dif-ferent SCF types with their relative frequencies).Such a lexicon provides a useful high-level pro-file of the syntactic behavior of the predicate inquestion, but is less useful for downstream NLPtasks (e.g.
information extraction, parsing, ma-chine translation) that involve sentence processingand can therefore benefit from SCF informationat instance level.
Sentences (1)-(3) demonstratethis limitation - a prior distribution over the pos-sible syntactic frames of distinguish provides onlya weak signal to a sentence level NLP applicationthat needs to infer the verbal argument structure ofits input sentences.We propose a new unsupervised model for SCFinduction which addresses these problems withexisting systems.
Our model does not use a parseror hand-written rules, only a part-of-speech (POS)tagger is utilizes in order to produce features formachine learning.
While POS taggers are alsosensitive to domain variation, they can be adaptedto domains more easily than parsers because theyrequire much smaller amounts of annotated data(Lease and Charniak, 2005; Ringger et al., 2007).However, as we demonstrate in our experiments,domain adaptation of POS tagging may not evenbe necessary to obtain good results on the SCF ac-quisition task.Our model, based on the Markov Random Field(MRF) framework, performs instance-based SCFlearning.
It encodes syntactic similarities amongverb instances across different verb types (derived2(Lippincott et al., 2012) does not use a parser, but thesyntactic frames induced by the system do not capture sets ofarguments for verbs, so are not SCFs in a traditional sense.from a lexical and POS-based feature representa-tion of verb instances) as well as prior beliefs onthe tendencies of specific instances of the sameverb type to take the same SCF.We evaluate our model against corpora anno-tated with verb instance SCFs (Quochi et al.,2012).
In addition, following the Levin verbclustering tradition (Levin, 1993) which ties verbmeanings with their syntactic properties, we eval-uate the semantic predictive power of our clusters.In the former evaluation, our model outperforms anumber of strong baselines, including supervisedand type-level ones, achieving an accuracy of upto 69.2%.
In the latter evaluation a vector spacemodel that utilized our induced SCFs substantiallyoutperforms the output of a type-level SCF systemthat uses the fully trained Stanford parser.2 Previous WorkSeveral SCF acquisition systems are available forEnglish (O?Donovan et al., 2005; Preiss et al.,2007; Lippincott et al., 2012; Van de Cruys etal., 2012; Reichart and Korhonen, 2013) and otherlanguages, including French (Messiant, 2008),Italian (Lenci et al., 2008), Turkish (Uzun et al.,2008), Japanese (Kawahara and Kurohashi, 2010)and Chinese (Han et al., 2008).
The promi-nent input to these systems are grammatical re-lations (GRs) which express binary dependen-cies between words (e.g.
direct and indirect ob-jects, various types of complements and conjunc-tions).
These are generated by some parsers (e.g.
(Briscoe et al., 2006)) and can be extracted fromthe output of others (De-Marneffe et al., 2006).Two representative systems for English are theCambridge system (Preiss et al., 2007) and theBioLexicon system which was used to acquire asubstantial lexicon for biomedicine (Venturi et al.,2009).
These systems extract GRs at the verb in-stance level from the output of a parser: the RASPgeneral-language unlexicalized parser3(Briscoe etal., 2006) and the lexicalized Enju parser tuned tothe biomedical domain (Miyao and Tsujii, 2005),respectively.
They generate potential SCFs bymapping GRs to a predefined SCF inventory us-ing a set of manually developed rules (the Cam-bridge system) or by simply considering the setsof GRs including verbs in question as potentialSCFs (BioLexicon).
Finally, a type level lexicon3A so-called unlexicalized parser is a parser trained with-out explicit SCF annotations.279is built through noisy frame filtering (based onfrequencies or on external resources and annota-tions), which aims to remove errors from parsingand argument-adjunct distinction.
Clearly, thesesystems require extensive manual work: a-prioridefinition of an SCF inventory and rules, manu-ally annotated sentences for training a supervisedparser, SCF annotations for parser lexicalization,and manually developed resources for optimal fil-tering.A number of recent works have applied mod-ern machine learning techniques to SCF induc-tion, including point-wise co-occurrence of ar-guments (Debowski, 2009), a Bayesian networkmodel (Lippincott et al., 2012), multi-way tensorfactorization (Van de Cruys et al., 2012) and De-terminantal Point Processes (DPPs) -based clus-tering (Reichart and Korhonen, 2013).
However,all of these systems induce type-level SCF lexi-cons and, except from the system of (Lippincott etal., 2012) that is not capable of learning traditionalSCFs, they all rely on supervised parsers.Our new system differs from previous ones ina number of respects.
First, in contrast to mostprevious systems, our system provides SCF anal-ysis for each verb instance in its sentential con-text, yielding more precise SCF information forsystems benefiting from instance-based analysis.Secondly, it addresses SCF induction as an unsu-pervised clustering problem, avoiding the use ofsupervised parsing or any of the sources of man-ual supervision used in previous works.
Our sys-tem relies on POS tags - however, we show that itis not necessary to train a tagger with in-domaindata to obtain good performance on this task, andtherefore our approach provides a more domain-independent solution to SCF acquisition.We employ POS-tagging instead of unsuper-vised parsing for two main reasons.
First, whilea major progress has been made on unsupervisedparsing (e.g.
(Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010)), the performance is stillconsiderably behind that of supervised parsing.For example, the state-of-the-art discriminativemodel of (Berg-Kirkpatrick et al., 2010) achievesonly 63% directed arc accuracy for WSJ sentencesof up to 10 words, compared to more than 95%obtained with supervised parsers.
Second, currentunsupervised parsers produce unlabeled structureswhich are substantially less useful for SCF acqui-sition than labeled structures produced by super-vised parsers (e.g.
grammatical relations).Finally, a number of recent works addressed re-lated tasks such as argument role clustering forSRL (Lang and Lapata, 2011a; Lang and Lapata,2011b; Titvo and Klementiev, 2012) in an unsu-pervised manner.
While these works differ fromours in the task (clustering arguments rather thanverbs) and the level of supervision (applying a su-pervised parser), like us they analyze the verb ar-gument structure at the instance level.3 ModelWe address SCF induction as an unsupervisedverb instance clustering problem.
Given a set ofplain sentences, our algorithm aims to cluster theverb instances in its input into syntactic clustersthat strongly correlate with SCFs.
In this sec-tion we introduce a Markov Random Field (MRF)model for this task: Section 3.1 describes ourmodel?s structure, components and objective; Sec-tion 3.2 describes the model potentials and theknowledge they encode; and Section 3.3 describeshow clusters are induced from the model.3.1 Model StructureWe implement our model in the MRF framework(Koller and Friedman, 2009).
This enables us toencode the two main sources of information thatgovern SCF selection in verb instances: (1) Atthe sentential context, the verbal syntactic frameis encoded through syntactic features.
Verb in-stances with similar feature representations shouldtherefore take the same syntactic frame; and (2)At the global context, per verb type SCF distribu-tions tend to be Zipfian (Korhonen et al., 2000).Instances of the same verb type should thereforebe biased to take the same syntactic frame.Given a collection of plain input sentences, wedenote the number of verb instances in the col-lection with n, and the number of data-dependentequivalence classes (ECs) with K (see below fortheir definition), and define an undirected graphi-cal model (MRF), G = (V,E, L).
We define thevertex set as V = X ?C, with X = {x1, .
.
.
, xn}consisting of one vertex for every verb instance inthe input collection, and C = {c1.
.
.
cK} consist-ing of one vertex for each data-dependent EC.
Theset of labels used by the model, L, corresponds tothe syntactic frames taken by the verbs in the in-put data.
The edge set E is defined through themodel?s potentials that are described below.280We encode information in the model throughthree main sets of potentials: one set of single-ton potentials - defined over individual model ver-texes, and two sets of pairwise potentials - definedbetween pairs of vertexes.
The first set consists ofa singleton potential for each vertex in the model.Reflecting the Zipfian distribution of SCFs acrossthe instances of the same verb type, these poten-tials encourage the model to assign such verb in-stances to the same frame (cluster).
The infor-mation encoded in these potentials is induced viaa pre-processing clustering step.
The second setconsists of a pairwise potential for each pair of ver-texes xi, xj?
X - that is, for each verb instancepair in the input, across verb types.
These poten-tials encode the belief, computed as feature-basedsimilarity (see below), that their verb instance ar-guments implement the same SCF.Finally, potentials from the last set bias themodel to assign the same SCF to high cardinal-ity sets of cross-type verb instances based on theirsyntactic context.
While these are pairwise poten-tials defined between verb instance vertexes (X)and EC vertexes (C), they are designed so thatthey bias the assignment of all verb instance ver-texes that are connected to the same EC vertex to-wards the same frame assignment (l ?
L).
Thetwo types of pairwise potentials complement eachother by modeling syntactic similarities amongverb instance pairs, as well as among higher cardi-nality verb instance sets.The resulted maximum aposteriori problem(MAP) takes the following form:MAP (V ) = argmaxx,c?Vn?i=1?i(xi) +n?i=1n?j=1?i,j(xi, xj)+n?i=1K?j=1?i,j(xi, cj) ?
I(xi?
ECj) +K?i=1K?j=1?i,j(ci, cj)where the predicate I(xi?
ECj) returns 1 ifthe i-th verb instance belongs the j-th equivalenceclass and 0 otherwise.
The ?
pairwise potentialsdefined between EC vertexes are very simple po-tentials designed to promise different assignmentsfor each pair of EC vertexes.
They do so by assign-ing a ??
score to assignments where their argu-ment vertexes take the same frame and a 0 other-wise.
In the rest of this section we do not get backto this simple set of potentials.A graphical illustration of the model is givenin Figure 1.
Note that we could have selected aricher model structure, for example, by defininga similarity potential over all verb instance ver-texes that share an equivalence class.
However, asthe figure demonstrates, even the structure of thepruned version of our model (see Section 3.3) usu-ally contains cycles, which makes inference NP-hard (Shimony, 1994).
Our design choices aim tobalance between the expressivity of the model andthe complexity of inference.
In Section 3.3 we de-scribe the LP relaxation algorithm we use for in-ference.C1 C2Figure 1: A graphical illustration of our model(after pruning, see Sec.
3.3) for twenty verb in-stances (|X| = 20), each represented with a blackvertex, and two equivalence classes (ECs), eachrepresented with a gray vertex (|C| = 2).
Solidlines represent edges (and ?i,jpairwise potentials)between verb instance vertexes.
Dashed lines rep-resent edges between verb instance vertexes andEC vertexes (?i,jpairwise potentials) or betweenEC vertexes (?i,jpairwise potentials) .3.2 Potentials and Encoded KnowledgePairwise Syntactic Similarity Potentials.
Thepairwise syntactic similarity potentials are definedfor each pair of verb instance vertexes, xi, xj?
X .They are designed to encourage the model to as-sign verb instances with similar fine-grained fea-ture representations to the same frame (l ?
L)and verb instances with dissimilar representationsto different frames.
For this aim, for every verbpair i, j with feature representation vectors vi, vjand verb instance vertexes xi, xj?
X , we definethe following potential function:?i,j(xi= l1, xj= l2) ={?
(vi, vj) if l1= l20 otherwise}Where l1, l2?
L are label pairs and ?
is a verbinstance similarity function.
Below we describethe feature representation and the ?
function.The verb instance feature representation is de-fined through the following process.
For each281word instance in the input sentences we first builda basic feature representation (see below).
Then,for each verb instance we construct a final fea-ture representation defined to be the concatena-tion of that verb?s basic feature representation withthe basic representations of the words in a size2 window around the represented verb.
The fi-nal feature representation for the i-th verb in-stance in our dataset is therefore defined to bevi= [w?2, w?1, vbi, w+1, w+2], where w?kandw+kare the basic feature representations of thewords in distance ?k or +k from the i-th verb in-stance in its sentence, and vbiis the basic featurerepresentation of that verb instance.Our basic feature representation is inspiredfrom the feature representation of the MST parser(McDonald et al., 2005) except that in the parserthe features represent a directed edge in the com-plete directed graph defined over the words in asentence that is to be parsed, while our features aregenerated for word n-grams.
Particularly, our fea-ture set is a concatenation of two sets derived fromthe MST set described in Table 1 of (McDonald etal., 2005) in the following way: (1) In both sets theparent word in the parser?s set is replaced with therepresented word; (2) In one set every child wordin the parser?s set is replaced by the word to theleft of the represented word and in the other set itis replaced by the word to its right.
This choice offeatures allows us to take advantage of a provablyuseful syntactic feature representation without theapplication of any parse tree annotation or parser.We compute the similarity between the syntac-tic environments of two verb instances, i, j, usingthe following equation:?
(vi, vj) = W ?
cos(vi, vj)?
SWhere W is a hyperparameter designed to biasverb instances of the same verb type towards thesame frame.
Practically, W was tuned to be 3 forinstances of the same type, and 1 otherwise4.While the cosine function is the standard mea-sure of similarity between two vectors, its val-ues are in the [0, 1] range.
In the MRF modelingframework, however, we must encode a negativepairwise potential value between two vertexes inorder to encourage the model to assign differentlabels (frames) to them.
We therefore added thepositive hyperparameter S which was tuned, with-4All hyperparameters that require gold-standard annota-tion for tuning, were tuned using held-out data (Section 4).out access to gold standard manual annotations, sothat there is an even number of negative and pos-itive pairwise syntactic similarity potentials afterthe model is pruned (see Section 3.3)5.Type Level Singleton Potentials.
The goal ofthese potentials is to bias verb instances of thesame type to be assigned to the same syntacticframe while still keeping the instance based natureof our algorithm.
For this aim, we applied Algo-rithm 1 for pre-clustering of the verb instances andencoded the induced clusters into the local poten-tials of the corresponding x ?
X vertexes.
Forevery x ?
X the singleton potential is thereforedefined to be:?i(xi= l) ={F ?
max?
if l is induced by Algorithm 10 otherwise}where max?
is the maximum ?
score across allverb instance pairs in the model and F = 0.2 is ahyperparamter.Algorithm 1 has two hyperparameters: T andM , the first is a similarity cut-off value used to de-termine the initial set of clusters, while the secondis used to determine whether two clusters are simi-lar enough to be merged.
We tuned these hyperpa-rameters, without manually annotated data, so thatthe number of clusters induced by this algorithmwill be equal to the number of gold standard SCFs.T was tuned so that the first part of the algorithmgenerates an excessive number of clusters, and Mwas then tuned so that these clusters are merged tothe desired number of clusters.The ?
function, used to measure the similar-ity between two verbs, is designed to bias the in-stances of the same verb type to have a higher sim-ilarity score.
Algorithm 1 therefore tends to assignsuch instances to the same cluster.
In our experi-ments that was always the case for this algorithm.High Cardinality Verb Sets Potentials.
Thisset of potentials aims to bias larger sets of verbinstances to share the same SCF.
It is inspired by(Rush et al., 2012) who demonstrated, that syn-tactic structures that appear at the same syntac-tic context, in terms of the surrounding POS tags,tend to manifest similar syntactic behavior.
Whilethey demonstrated the usefulness of their methodfor dependency parsing and POS tagging, we im-plement it for higher level SCFs.We identified syntactic contexts that imply simi-lar SCFs for verb instances appearing inside them.5The values in practice are S = 0.43 for labour legislationand S = 0.38 for environment.282Algorithm 1 Verb instance pre-clustering algo-rithm.??
is the average ?
score between the mem-bers of its cluster arguments.
T and M are hyper-parametes tuned without access to gold standarddata.Require: K = ?for all x ?
X dofor all k ?
K dofor all u ?
k doif ?
(vx, vu) > T thenk = k ?
{x}Go to next xend ifend forend fork1 = {x}K = K ?
k1end forfor all k1, k2?
K: k16= k2doif??
(k1, k2) > M thenMerge (k1, k2)end ifend forContexts are characterized by the coarse POS tagto the left and to the right of the verb instance.While the number of context sets is bounded onlyby the number of frames our model is designedto induce, in practice we found that defining twoequivalence sets led to the best performance gain,and the sets we used are presented in Table 1.In order to encode this information into ourMRF, each set of syntactic contexts is associatedwith an equivalence class (EC) vertex c ?
C andthe verb instance vertexes of all verbs that appearin a context from that set are connected with anedge to c. The pairwise potential between a vertexx ?
X and its equivalence class is defined to be:?i,j(xi= l1, cj= l2) ={U if l1= l20 otherwise}U = 10 is a hyperparameter that strongly biases xvertexes to get the same SCF as their EC vertex.3.3 Verb Cluster InductionIn this section we describe how we induce verbinstance clusters from our model.
This processis based on the following three steps: (1) Graphpruning; (2) Induction of an Ensemble of approx-imate MAP inference solutions in the resultedgraphical model; and, (3) Induction of a final clus-tering solution based on the ensemble created atstep 2.
Below we explain the necessity of each ofthese steps and provide the algorithmic details.EC-1 EC-2Left Right Left Right, D V TN D R TV .
N DR D R NTable 1: POS contexts indicative for the syntacticframe of the verb instance they surround.
D: de-terminer, N: noun, V: verb, T: the preposition ?to?
(which has its own POS tag in the WSJ POS tag setwhich we use), R: adverb.
EC-1 and EC-2 standfor the first and second equivalence class respec-tively.
In addition, the following contexts whereassociated with both ECs: (T,D), (T,N), (N,N)and (V, I) where I stands for a preposition.Graph Pruning.
The edge set of our modelconsists of an edge for every pair of verb in-stance vertexes and of the edges that connect verbinstance vertexes and equivalence class vertexes.This results in a large tree-width graph which sub-stantially complicates MRF inference.
To alleviatethis we prune all edges with a positive score lowerthan p+and all edges with a negative score higherthan p?, where p+and p?are manually tuned hy-perparametes6.MAP Inference.
For most reasonable values ofp+and p?our graph still contains cycles even af-ter it is pruned, which makes inference NP-hard(Shimony, 1994).
Yet, thanks to our choice of anedge-factorized model, there are various approxi-mate inference algorithms suitable for our case.We applied the message passing algorithm forlinear-programming (LP) relaxation of the MAPassignment (MPLP, (Sontag et al., 2008)).
LP re-laxation algorithms for the MAP problem definean upper bound on the original objective whichtakes the form of a linear program.
Consequently,a minimum of this upper bound can be found us-ing standard LP solvers or, more efficiently, usingspecialized message passing algorithms (Yanoveret al., 2006).
The MPLP algorithm described in(Sontag et al., 2008) is appealing in that it itera-tively computes tighter upper bounds on the MAPobjective (for details see their paper).Cluster Ensemble Generation and a FinalSolution.
As our MAP objective is non-convex,6The values used in practice are p+= 0.28, p?= ?0.17for the labour legislation dataset, and p+= 0.25, p?=?0.20 for the environment set.283the convergent point of an optimization algorithmapplied to it is highly sensitive to its initializa-tion.
To avoid convergence to arbitrary local max-ima which may be of poor quality, we turn to aperturbation protocol where we repeatedly intro-duce random noise to the MRF?s potential func-tions and then compute the approximate MAP so-lution of the resulted model using the MPLP algo-rithm.
Noising was done by adding an  term tothe lambda values described in section 3.27.
Thisprotocol results in a set of cluster (label) assign-ments for the involved verb instances, which wetreat as an ensemble of experts from which a final,high quality, solution is to be induced.The basic idea in ensemble learning is that ifseveral experts independently cluster together twoverb instances, our belief that these verbs belongin the same cluster should increase.
(Reichart etal., 2012) implemented this idea through the k-way normalized cut clustering algorithm (Yu andShi, 2003).
Its input is an undirected graph?G =(?V ,?E,?W ) where?V is the set of vertexes,?E isthe set of edges and?W is a non-negative and sym-metric edge weight matrix.
To apply this modelto our task, we construct the input graph?G fromthe labelings (frame assignments) contained in theensemble.
The graph vertexes?V correspond to theverb instances and the (i, j)-th entry of the matrix?W is the number of ensemble members that assignthe same label to the i-th and j-th verb instances.For A,B ?
?V define:links(A,B) =?i?A,j?B?W (i, j)Using this definition, the normalized link ratioof A and B is defined to be:NormLinkRatio(A,B) =links(A,B)links(A,?V )The k-way normalized cut problem is to mini-mize the links that leave a cluster relative to thetotal weight of the cluster.
Denote the set of clus-terings of?V that consist of k clusters by?C ={c?1, .
.
.
c?t} and the j-th cluster of the i-th cluster-7 was accepted by first sampling a number in the [0, 1]range using the Java psuodorandom generator and then scal-ing it to 1% of cos(vi, vj).
This value was tuned, withoutaccess to gold standard manual annotations, so that there isan even number of negative and positive pairwise syntacticsimilarity potentials after the model is pruned (Section 3.3).ing by c?ij.
Thenc?= argminc?i?
?Ck?j=1NormLinkRatio(c?ij,?V ?
c?ij)The algorithm of (Yu and Shi, 2003) solves thisproblem very efficiently as it avoids the heavyeigenvalues and eigenvectors computations re-quired by traditional approaches.4 Experiments and ResultsOur model is unique compared to existing systemsin two respects.
First, it does not utilize supervi-sion in the form of either a supervised syntacticparser and/or manually crafted SCF rules.
Conse-quently, it induces unnamed frames (clusters) thatare not directly comparable to the named framesinduced by previous systems.
Second, it inducessyntactic frames at the verb instance, rather thantype, level.
Evaluation, and especially comparisonto previous work, is therefore challenging.We therefore evaluate our system in two ways.First, we compare its output, as well as the outputof a number of clustering baselines, to the goldstandard annotation of corpora from two differ-ent domains (the only publicly available ones withinstance level SCF annotation, to the best of ourknowledge).
Second, in order to compare the out-put of our system to a rule-based SCF system thatutilizes a supervised syntactic parser, we turn toa task-based evaluation.
We aim to predict thedegree of similarity between verb pairs and, fol-lowing (Pado and Lapata, 2007) , we do so usinga syntactic-based vector space model (VSM).
Weconstruct three VSMs - (a) one that derives fea-tures from our clusters; (b) one whose featurescome from the output of a state-of-the-art verbtype level, rule based, SCF system (Reichart andKorhonen, 2013) that uses a modern parser (Kleinand Manning, 2003); and (c) a standard lexicalVSM.
Below we show that our system comparesfavorably in both evaluations.Data.
We experimented with two datasets takenfrom different domains: labor legislation and en-vironment (Quochi et al., 2012).
These datasetswere created through web crawling followed bydomain filtering.
Each sentence in both datasetsmay contain multiple verbs but only one targetverb has been manually annotated with a SCF.The labour legislation domain dataset contains4415 annotated verb instances (and hence also284sentences) of 117 types, and the environmentaldomain dataset contains 4503 annotated verb in-stances of 116 types.
In both datasets no verb typeaccounts for more than 4% of the instances andonly up to 35 verb types account for 1% of theinstances or more.
The lexical difference betweenthe corpora is substantial: they share only 42 anno-tated verb types in total, of which only 2 verb types(responsible for 4.1% and 5.2% of the instances inthe environment and labor legislation domains re-spectively) belong to the 20 most frequent types(responsible for 37.9% and 46.85% of the verb in-stances in the respective domains) of each corpus.The 29 members of the SCF inventory are de-tailed in (Quochi et al., 2012).
Table 2, presentingthe distribution of the 5 highest frequency framesin each corpus, demonstrates that, in addition tothe significant lexical difference, the corpora differto some extent in their syntactic properties.
This isreflected by the substantially different frequenciesof the ?dobj:iobj-prep:su?
and ?dobj:su?
frames.As a pre-processing step we first POS taggedthe datasets with the Stanford tagger (Toutanovaet al., 2003) trained on the standard POS trainingsections of the WSJ PennTreebank corpus.4.1 Evaluation Against SCF Gold StandardExperimental Protocol The computational com-plexity of our algorithm does not allow us to run iton thousands of verb instances in a feasible time.We therefore repeatedly sampled 5% of the sen-tences from each dataset, ran our algorithm as wellas the baselines (see below) and report the averageperformance of each method.
The number of rep-etitions was 40 and samples were drawn from auniform distribution while still promising that thedistribution of gold standard SCFs in each sam-ple is identical to their distribution in the entiredataset.
Before running this protocol, 5% of eachcorpus was kept as held-out data on which hyper-parameter tuning was performed.EvaluationMeasures and Baselines.
We com-pare our system?s output to instance-level goldstandard annotation.
We use standard measuresfor clustering evaluation, one measure from eachof the two leading measure types: the V measure(Rosenberg and Hirschberg, 2007), which is an in-formation theoretic measure, and greedy many-to-one accuracy, which is a mapping-based measure.For the latter, each induced cluster is first mappedto the gold SCF frame that annotates the highestnumber of verb instances this induced cluster alsoannotates and then a standard instance-level accu-racy score is computed (see, e.g., (Reichart andRappoport, 2009)).
Both measures scale from 100(perfect match with gold standard) to 0 (no match).As mentioned above, comparing the perfor-mance of our system with respect to a gold stan-dard to the performance of previous type-levelsystems that used hand-crafted rules and/or su-pervised syntactic parsers would be challenging.We therefore compare our model to the follow-ing baselines: (a) The most frequent class (MFC)baseline which assigns all verb instances with theSCF that is the most frequent one in the gold stan-dard annotation of the data; (b) The Random base-line which simply assigns every verb instance witha randomly selected SCF; (c) Algorithm 1 of sec-tion 3.2 which generates unsupervised verb in-stance clustering such that verb instances of thesame type are assigned to the same cluster; and(d) Finally, we also compare our model againstversions where everything is kept fixed, except asubset of potentials which is omitted.
This enablesus to study the intricacies of our model and the rel-ative importance of its components.
For all mod-els, the number of induced clusters is equal to thenumber of SCFs in the gold standard.Results Table 3 presents the results, demon-strating that our full model substantially outper-forms all baselines.
For the first two simple heuris-tic baselines (MFC and Random) the margin ishigher than 20% for both the greedy M-1 mappingmeasure and the V measure.
Note tat the V scoreof the MFC baseline is 0 by definition, as it as-signs all items to the same cluster.
The poor per-formance of these simple baselines is an indicationof the difficulty of our task.Recall that the type level clustering induced byAlgorithm 1 is the main source of type level in-formation our model utilizes (through its single-ton potentials).
The comparison to the output ofthis algorithm (the Type Pre-clustering baseline)therefore shows the quality of the instance levelrefinement our model provides.
As seen in table 3,our model outperforms this baseline by 6.9% forthe M-1 measure and 5.2% for the V measure.In order to compare our model to its compo-nents we exclude either the EC potentials (?
and?)
only (Model - EC), or the EC and the singletonpotentials (?i, Model - EC - Type pre-clustering).The results show that our model gains much more285Environment Labour LegislationSCF Frequency SCF Frequencydobj:su 46% dobj:su 39%su 9% dobj:iobj-prep:su 15%iobj-prep:su 8% su 10%dobj:iobj-prep:su 6% su:xcompto-vbare 8%su:xcompto-vbare 6% iobj-prep:su 7%Table 2: Top 5 most frequent SCFs for the Environment and Labour Legislation datasets used in ourexperiments.Environment Labour LegislationM-1 V M-1 VFull Model 66.4 57.3 69.2 55.6BaselinesMFC 46.2 0 39.4 0Random 34.6 28.1 36.5 27.8Type Pre-clustering 60.1 52.1 62.3 51.4Model ComponentsModel - EC 64.9 56.2 67.4 54.6Model - EC - Type pre-clustering 48.3 48.9 45.7 44.7Table 3: Results for our full model, the baselines (Type Pre-clustering: the pre-clustering algorithm(Algorithm 1 of section 3.2), MFC: the most frequent class (SCF) in the gold standard annotation andRandom: random SCF assignment) and the model components.
The full model outperforms all othermodels across measures and datasets.from the type level information encoded throughthe singleton potentials than from the EC poten-tials.
Yet, EC potentials do lead to an improvementof up to 1.5% in M-1 and up to 1.1% in V and aretherefore responsible for up to 26.1% and 21.2%of the improvement over the type pre-clusteringbaseline in terms of M-1 and V, respectively.4.2 Task Based EvaluationWe next evaluate our model in the context of vec-tor space modeling for verb similarity prediction(Turney and Pantel, 2010).
Since most previousword similarity works used noun datasets, we con-structed a new verb pair dataset, following the pro-tocol used in the collection of the wordSimilarity-353 dataset (Finkelstein et al., 2002).Our dataset consists of 143 verb pairs, con-structed from 122 unique verb lemma types.
Theparticipating verbs appear ?
10 times in the con-catenation of the labour legislation and the envi-ronment datasets.
Only pairs of verbs that wereconsidered at least remotely similar by humanjudges (independent of those that provided thesimilarity scores) were included.
A similarityscore between 1 and 10 was assigned to each pairby 10 native English speaking annotators and werethen averaged in order to get a unique pair score.Our first baseline is a standard VSM based onlexical collocations.
In this model features corre-spond to the number of collocations inside a size2 window of the represented verb with each of the5000 most frequent nouns in the Google n-gramcorpus (Goldberg and Orwant, 2013).
Since ourcorpora are limited in size, we use the collocationcounts from the Google corpus.We used our model to generate a vector repre-sentation of each verb in the following way.
Werun the model 5000 times, each time over a set ofverbs consisting of one instance of each of the 122verb types participating in the verb similarity set.The output of each such run is transformed to abinary vector for each participating verb, whereall coordinates are assigned the value of 0, ex-cept from the one that corresponds to the cluster towhich the verb was assigned which has the valueof 1.
The final vector representation is a concate-nation of the 5000 binary vectors.
Note that forthis task we did not use the graph cut algorithm togenerate a final clustering from the multiple MRF286runs.
Instead we concatenated the output of allthese runs into one feature representation that fa-cilitates similarity prediction.
For our model weestimated the verb pair similarity using the Tani-mato similarity score for binary vectors:T (X,Y ) =?iXi?
Yi?ixi?
YiFor the baseline model, where the features arecollocation counts, we used the standard cosinesimilarity.Our second baseline is identical to our model,except that: (a) the data is parsed with the Stan-ford parser (version 3.3.0, (Klein and Manning,2003)) which was trained with sections 2-21 of theWSJ corpus; (b) the phrase structure output of theparser is transformed to the CoNLL dependencyformat using the official CoNLL 2007 conversionscript (Johansson and Nugues, 2007); and then (c)the SCF of each verb instance is inferred using therule-based system used by (Reichart and Korho-nen, 2013).
The vector space representation foreach verb is then created using the process we de-scribed for our model and the same holds for vec-tor comparison.
This baseline allows direct com-parison of frames induced by our SCF model withthose derived from a supervised parser?s output.We computed the Pearson correlation betweenthe scores of each of the models and the humanscores.
The results demonstrate the superiorityof our model in predicting verb similarity: thecorrelation of our model with the human scoresis 0.642 while the correlation of the lexical col-location baseline is 0.522 and that of the super-vised parser baseline is only 0.266.
The resultsindicate that in addition to their good alignmentwith SCFs, our clusters are also highly useful forverb meaning representation.
This is in line withthe verb clustering theory of the Levin tradition(Levin, 1993) which ties verb meaning with theirsyntactic properties.
We consider this an intrigu-ing direction of future work.5 ConclusionsWe presented an MRF-based unsupervised modelfor SCF acquisition which produces verb instancelevel SCFs as output.
As opposed to previous sys-tems for the task, our model uses only a POS tag-ger, avoiding the need for a statistical parser ormanually crafted rules.
The model is particularlyvaluable for NLP tasks benefiting from SCFs thatare applied across text domains, and for the manytasks that involve sentence-level processing.Our results show that the accuracy of the modelis promising, both when compared against goldstandard annotations and when evaluated in thecontext of a task.
In the future we intend to im-prove our model by encoding additional informa-tion in it.
We will also adapt it to a multilingualsetup, aiming to model a wide range of languages.AcknowledgmentsThe first author is supported by the Common-wealth Scholarship Commission (CSC) and theCambridge Trust.ReferencesIvana Romina Altamirano and Laura Alonso i Ale-many.
2010.
IRASubcat, a highly customizable,language independent tool for the acquisition of ver-bal subcategorization information from corpus.
InProceedings of the NAACL 2010 Workshop on Com-putational Approaches to Languages of the Ameri-cas.Abhishek Arun and Frank Keller.
2005.
Lexicalizationin crosslinguistic probabilistic parsing: The case offrench.
In Proceedings of ACL-05.Taylor Berg-Kirkpatrick, Alexander Bouchard-Cote,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceedings ofNAACL-HLT-10.Akshar Bharati, Sriram Venkatapathy, and PrashanthReddy.
2005.
Inferring semantic roles using sub-categorization frames and maximum entropy model.In Proceedings of CoNLL-05.Ted Briscoe and John Carroll.
1997.
Automatic ex-traction of subcategorization from corpora.
In Pro-ceedings of ANLP-97.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the rasp system.
In Proceed-ings of ACL-COLING-06.John Carroll and Alex Fang.
2004.
The automatic ac-quisition of verb subcategorisations and their impacton the performance of an HPSG parser.
In Proceed-ings of IJCNLP-04.Paula Chesley and Susanne Salmon-Alt.
2006.
Au-tomatic extraction of subcategorization frames forfrench.
In Proceedings of LREC-06.Kostadin Cholakov and Gertjan van Noord.
2010.
Us-ing unknown word techniques to learn known words.In Proceedings of EMNLP-10.287Shay Cohen and Noah Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in un-supervised grammar induction.
In Proceedings ofNAACL-HLT-09.Marie-Catherine De-Marneffe, Bill Maccartney, andChristopher Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
InProceedings of LREC-06.Lukasz Debowski.
2009.
Valence extraction using EMselection and co-occurrence matrices.
Proceedins ofLREC-09.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ei-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20:116?131.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large corpusof english books.
In Proceedings of (*SEM)-13.
As-sociation for Computational Linguistics.Jan Haji?c, Martin mejrek, Bonnie Dorr, Yuan Ding, Ja-son Eisner, Daniel Gildea, Terry Koo, Kristen Par-ton, Gerald Penn, Dragomir Radev, and Owen Ram-bow.
2002.
Natural language generation in the con-text of machine translation.
Technical report, Cen-ter for Language and Speech Processing, Johns Hop-kins University, Baltimore.
Summer Workshop FinalReport.Chung hye Han, Benoit Lavoie, Martha Palmer, OwenRambow, Richard Kittredge, Tanya Korelsky, andMyunghee Kim.
2000.
Handling structural diver-gences and recovering dropped arguments in a ko-rean/english machine translation system.
In Pro-ceedings of the AMTA-00.Dino Ienco, Serena Villata, and Cristina Bosco.
2008.Automatic extraction of subcategorization framesfor italian.
In Proceedings of LREC-08.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for en-glish.
In Proceedings of NODALIDA-07.Daisuke Kawahara and Sadao Kurohashi.
2010.
Ac-quiring reliable predicate-argument structures fromraw corpora for case frame compilation.
In Proceed-ings of LREC-10.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL-03.Daphne Koller and Nir Friedman.
2009.
Probabilisticgraphical models: principles and techniques.
TheMIT Press.Anna Korhonen, Genevieve Gorrell, and Diana Mc-Carthy.
2000.
Statistical filtering and subcate-gorization frame acquisition.
In Proceedings ofEMNLP-00.Anna Korhonen.
2002.
Semantically motivated sub-categorization acquisition.
In Proceedings of theACL-02 workshop on Unsupervised lexical acquisi-tion.Joel Lang and Mirella Lapata.
2011a.
Unsupervisedsemantic role induction via split-merge clustering.In Proceedings of ACL-11.Joel Lang and Mirella Lapata.
2011b.
Unsupervisedsemantic role induction with graph partitioning.
InProceedings of EMNLP-11.Matthew Lease and Eugene Charniak.
2005.
Parsingbiomedical literature.
In Proceedings of IJCNLP-05.Alessandro Lenci, Barbara Mcgillivray, SimonettaMontemagni, and Vito Pirrelli.
2008.
Unsupervisedacquisition of verb subcategorization frames fromshallow-parsed corpora.
In Proceedings of LREC-08.Beth Levin.
1993.
English verb classes and alterna-tions: A preliminary investigation.
Chicago, IL.Tom Lippincott, Anna Korhonen, and Diarmuid Os-eaghdha.
2010.
Exploring subdomain variation inbiomedical language.
BMC Bioinformatics.Tom Lippincott, Aanna Korhonen, and Diarmuid Os-eaghdha.
2012.
Learning syntactic verb frames us-ing graphical models.
In Proceedings of ACL-12.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL-05.Cedric Messiant, Anna Korhonen, and ThierryPoibeau.
2008.
LexSchem: A large subcategoriza-tion lexicon for French verbs.
In Proceedings ofLREC-08.Cedric Messiant.
2008.
A subcategorization acquis-tion system for french verbs.
In Proceedings ofACL08-SRW.Yusuke Miyao and Junichi Tsujii.
2005.
Probabilisticdisambiguaton models for wide-coverage hpsg pars-ing.
In Proceedings of ACL-05.Alessandro Moschitti and Roberto Basili.
2005.
Verbsubcategorization kernels for automatic semantic la-beling.
In Proceedings of the ACL-SIGLEX Work-shop on Deep Lexical Acquisition.Ruth O?Donovan, Michael Burke, Aoife Cahill, Josefvan Genabith, and Andy Way.
2005.
Large-scaleinduction and evaluation of lexical resources fromthe penn-ii and penn-iii treebanks.
ComputationalLinguistics, 31:328?365.Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33:161?199.288Judita Preiss, Ted Briscoe, and Anna Korhonen.
2007.A system for large-scale acquisition of verbal, nom-inal and adjectival subcategorization frames fromcorpora.
In Proceedings of ACL-07.Valeria Quochi, Francesca Frontini, Roberto Bartolini,Olivier Hamon, Marc Poch, Muntsa Padr, Nuria Bel,Gregor Thurmair, Antonio Toral, and Amir Kam-ram.
2012.
Third evaluation report.
evaluation ofpanacea v3 and produced resources.
Technical re-port.Roi Reichart and Anna Korhonen.
2013.
Improvedlexical acquisition through dpp-based verb cluster-ing.
In Proceedings of ACL-13.Roi Reichart and Ari Rappoport.
2009.
The nviclustering evaluation measure.
In Proceedings ofCoNLL-09.Roi Reichart, Gal Elidan, and Ari Rappoport.
2012.
Adiverse dirichlet process ensemble for unsupervisedinduction of syntactic categories.
In Proceedings ofCOLING-12.Laura Rimell, Thomas Lippincott, Karin Verspoor, He-len Johnson, and Anna Korhonen.
2013.
Acqui-sition and evaluation of verb subcategorization re-sources for biomedicine.
Journal of Biomedical In-formatics, 46:228?237.Eric Ringger, Peter McClanahan, Robbie Haertel,George Busby, Marc Carmen, James Carroll, KevinSeppi, and Deryle Lonsdale.
2007.
Active learningfor part-of-speech tagging: Accelerating corpus an-notation.
In Proceedings of the ACL-07 LinguisticAnnotation Workshop.Douglas Roland and Daniel Jurafsky.
1998. subcate-gorization frequencies are affected by corpus choice.In Proceedings of ACL-98.Andrew Rosenberg and Julia Hirschberg.
2007.
Vmeasure: a conditional entropybased external clusterevaluation measure.
In Proceedings of EMNLP-07.Alexander Rush, Roi Reichart, Michael Collins, andAmir Globerson.
2012.
Improved parsing and postagging using inter-sentence consistency constraints.In Proceedings of EMNLP-12.Sabine Schulte im Walde.
2006.
Experiments onthe automatic induction of german semantic verbclasses.
Computational Linguistics, 32(2):159?194.Solomon Shimony.
1994.
Finding the maps for beliefnetworks is np-hard.
Artificial Intelligence, 68:399?310.David Sontag, Talya Meltzer, Amir Globerson, TommiJaakkola, and Yair Weiss.
2008.
Tightening lp re-laxations for map using message passing.
In Pro-ceedings of UAI-08.Lin Sun and Anna Korhonen.
2011.
Hierarchical verbclustering using graph factorization.
In Proceedingsof EMNLP-11.Ivan Titvo and Alexandre Klementiev.
2012.
Abayesian approach to unsupervised semantic role in-duction.
In Proceedings of EMNLP-12.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of NAACL-03.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37:141?188.Tim Van de Cruys, Laura Rimell, Thierry Poibeau, andAnna Korhonen.
2012.
Multi-way tensor factor-ization for unsupervised lexical acquisition.
In Pro-ceedings of COLING-12.Giulia Venturi, Simonetta Montemagni, SimoneMarchi, Yutaka Sasaki, Paul Thompson, John Mc-Naught, and Sophia Ananiadou.
2009.
Bootstrap-ping a verb lexicon for biomedical information ex-traction.
Computational Linguistics and IntelligentText Processing, 5449:137?148.Marion Weller, Alexander Fraser, and Sabine Schulteim Walde.
2013.
Using subcategorization knowl-edge to improve case prediction for translation togerman.
In Proceedings of ACL-13.Chen Yanover, Talya Meltzer, and Yair Weiss.
2006.Linear programming relazations and belief pro-pogataion an empitical study.
JMLR Special Issueon Machine Learning and Large Scale Optimization.Stella Yu and Jianbo Shi.
2003.
Multiclass spectralclustering.
In Proceedings of ICCV-13.289
