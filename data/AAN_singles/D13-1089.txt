Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 884?890,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsScaling to Large3 Data: An efficient and effective methodto compute Distributional ThesauriMartin Riedl and Chris BiemannFG Language TechnologyComputer Science Department, Technische Universita?t DarmstadtHochschulstrasse 10, D-64289 Darmstadt, Germany{riedl,biem}@cs.tu-darmstadt.deAbstractWe introduce a new highly scalable approachfor computing Distributional Thesauri (DTs).By employing pruning techniques and a dis-tributed framework, we make the computationfor very large corpora feasible on comparablysmall computational resources.
We demon-strate this by releasing a DT for the whole vo-cabulary of Google Books syntactic n-grams.Evaluating against lexical resources using twomeasures, we show that our approach pro-duces higher quality DTs than previous ap-proaches, and is thus preferable in terms ofspeed and quality for large corpora.1 IntroductionUsing larger data to estimate models for machinelearning applications as well as for applications ofNatural Language Processing (NLP) has repeatedlyshown to be advantageous, see e.g.
(Banko andBrill, 2001; Brants et al 2007).
In this work, wetackle the influence of corpus size for building adistributional thesaurus (Lin, 1998).
Especially, weshed light on the interaction of similarity measuresand corpus size, as well as aspects of scalability.We shortly introduce the JoBimText frameworkfor distributional semantics and show its scalabilityfor large corpora.
For the computation of the datawe follow the MapReduce (Dean and Ghemawat,2004) paradigm.
The computation of similaritiesbetween terms becomes challenging on large cor-pora, as both the numbers of terms to be comparedand the number of context features increases.
Thismakes standard similarity calculations as proposedin (Lin, 1998; Curran, 2002; Lund and Burgess,1996; Weeds et al 2004) computationally infeasi-ble.
These approaches first calculate an informa-tion measure between each word and the accord-ing context and then calculate the similarity betweenall words, based on the information measure for allshared contexts.2 Related WorkA variety of approaches to compute DTs have beenproposed to tackle issues regarding size and run-time.
The reduction of the feature space seems tobe one possibility, but still requires the computationof such reduction cf.
(Blei et al 2003; Golub andKahan, 1965).
Other approaches use randomised in-dexing for storing counts or hashing functions to ap-proximate counts and measures (Gorman and Cur-ran, 2006; Goyal et al 2010; Sahlgren, 2006).
An-other possibility is the usage of distributed process-ing like MapReduce.
In (Pantel et al 2009; Agirreet al 2009) a DT is computed using MapReduceon 200 quad core nodes (for 5.2 billion sentences)respectively 2000 cores (1.6 Terawords), an amountof hardware only available to commercial search en-gines.
Whereas Agirre uses a ?2 test to measure theinformation between terms and context, Pantel usesthe Pointwise Mutual Information (PMI).
Then, bothapproaches use the cosine similarity to calculate thesimilarity between terms.
Furthermore, Pantel de-scribes an optimization for the calculation of the co-sine similarity.
Whereas Pantel and Lin (2002) de-scribe a method for sense clustering, they also usea method to calculate similarities between terms.Here, they propose a pruning scheme similar to ours,but do not explicitly evaluate its effect.The evaluation of DTs has been performed in ex-trinsic and intrinsic manner.
Extrinsic evaluationshave been performed using e.g.
DTs for automatic884set expansion (Pantel et al 2009) or phrase polar-ity identification (Goyal and Daume?, 2011).
In thiswork we will concentrate on intrinsic evaluations:Lin (1997; 1998) introduced two measures usingWordNet (Miller, 1995) and Roget?s Thesaurus.
Us-ing WordNet, he defines context features (synsets aword occurs in Wordnet or subsets when using Ro-get?s Thesaurus) and then builds a gold standard the-saurus using a similarity measure.
Then he evaluateshis generated Distributional Thesaurus (DT) with re-spect to the gold standard thesauri.
Weeds et al(2004) evaluate various similarity measures basedon 1000 frequent and 1000 infrequent words.
Curran(2004) created a gold standard thesaurus by manu-ally extracting entries from several English thesaurifor 70 words.
His automatically generated DTs areevaluated against this gold standard thesaurus usingseveral measures.
We will report on his measure andadditionally propose a measure based on WordNetpaths.3 Building a Distributional ThesaurusHere we present our scalable DT algorithm usingthe MapReduce paradigm, which is divided intotwo parts: The holing system and a computationalmethod to calculate distributional similarities.
Amore detailed description, especially for the MapRe-duce steps, can be found in (Biemann and Riedl,2013).3.1 Holing SystemThe holing operation splits an observation (e.g.
adependency relation) into a pair of two parts: aterm and a context feature.
This captures their first-order relationship.
These pairs are subsequentlyused for the computation of the similarities betweenterms, leading to a second-order relation.
The rep-resentation can be formalized by the pair <x,y>where x is the term and y represents the contextfeature.
The position of x in y is denoted by thehole symbol ?@?.
As an example the dependencyrelation (nsub;gave2;I1) could be transferred to<gave2,(nsub;@;I1)> and <I1,(nsub;gave2;@)>.This representation scheme is more generic then theschemes introduced in (Lin, 1998; Curran, 2002),as it allows to characterise pairs by several holes,which could be used to learn analogies, cf.
(Turneyand Littman, 2005).3.2 Distributional SimilarityFirst, we count the frequency for each first-orderrelation and remove all features that occur withmore than w terms, as these context features tend tobe too general to characterise the similarity betweenother words (Rychly?
and Kilgarriff, 2007; Goyalet al 2010, cmp.).
From this.
we calculate a sig-nificance score for all first-order relations.
For thiswork, we implemented two different significancemeasures: Pointwise Mutual Information (PMI):PMI(term, feature) = log2(f(term,feature)f(term)f(feature))(Church and Hanks, 1990) and Lexicographer?s Mu-tual Information (LMI): LMI(term, feature) =f(term, feature) log2(f(term,feature)f(term)f(feature)) (Evert,2005).We then prune all negatively correlated pairs(s<0).
The maximum number of context featuresper term are defined with p, as we argue that it issufficient to keep only the p most salient (ordereddescending by their significance score) context fea-tures per term.
Features of low saliency generallyshould not contribute much to the similarity of termsand also could lead to spurious similarity scores.
Af-terwards, all terms are aggregated by their features,which allows us to compute similarity scores be-tween all terms that share at least one such feature.Whereas the method introduced by (Pantel andLin, 2002) is very similar to the one proposed inthis paper (the similarity between terms is calculatedsolely by the number of features two terms share),they use PMI to rank features and do not use pruningto scale to large corpora, as they use a rather smallcorpus.
Additionally, they do not evaluate the effectof such pruning.In contrast to the best measures proposed by Lin(1998; Curran (2002; Pantel et al(2009; Goyal etal.
(2010) we do not calculate any information mea-sure using frequencies of features and terms (we usesignificance ranking instead), as shown in Table 1.Additionally, we avoid any similarity measure-ment using the information measure, as also done inthese approaches, to calculate the similarity over thefeature counts of each term: we merely count howmany salient features two terms share.
All these con-straints makes this approach more scalable to largercorpora, as we do not need to know the full list of885Information MeasuresLin?s formula I(term, feature) = lin(term, feature) = log f(term,feature)?f(relation(feature))P(f(word,relation(feature))f(word)Curran?s TTest I(term, feature) = ttest(term, feature) = p(term,feature)?p(feature)?p(term)?p(feature)?p(term)Similarity MeasuresLin?s formula sim(t1, t2) =Pf?features(t1)?features(t2)(I(t1,f)+I(t2,f))Pf?features(t1)I(t1,f)+Pf?features(w2)I(w2,f)Curran?s Dice sim(t1, t2) =Pf?features(t1)?features(t2)min(I(t1,f),I(t2,f))Pf?features(t1)?features(t2)(I(t1,f)+I(t2,f))with I(t, f) > 0Our Measure sim(t1, t2) =?f?features(t1)?features(t2)1 with s > 0Table 1: Similarity measures used for computing the distributional similarity between terms.features for a term pair at any time.
While our com-putations might seem simplistic, we demonstrate itsadequacy for large corpora in Section 5.4 EvaluationThe evaluation is performed using a recent dump ofEnglish Wikipedia, containing 36 million sentencesand a newspaper corpus, compiled from 120 millionsentences (about 2 Gigawords) from Leipzig Cor-pora Collection (Richter et al 2006) and the Giga-word corpus (Parker et al 2011).
The DTs are basedon collapsed dependencies from the Stanford Parser(Marneffe et al 2006) in the holing operation.
Forall DTs we use the pruning parameters s=0, p=1000and w=1000.
In a final evaluation, we use the syn-tactic n-grams built from Google Books (Goldbergand Orwant, 2013).To show the impact of corpus size, we down-sampled our corpora to 10 million, 1 million and100,000 sentences.
We compare our results againstDTs calculated using Lin?s (Lin, 1998) measure andthe best measure proposed by Curran (2002) (see Ta-ble 1).Our evaluation is performed using the same 1000frequent and 1000 infrequent nouns as previouslyemployed by Weeds et al(2004).
We create a goldstandard, by extracting reasonable entries of these2000 nouns using Roget?s 1911 thesaurus, MobyThesaurus, Merriam Webster?s Thesaurus, the BigHuge Thesaurus and the OpenOffice Thesaurus andemploy the inverse ranking measure (Curran, 2002)to evaluate the DTs.Furthermore, we introduce a WordNet-basedmethod.
To calculate the similarity between twoterms, we use the WordNet::Similarity path (Peder-sen et al 2004) measure.
While its absolute scoresare hard to interpret due to inhomogenity in the gran-ularity of WordNet, they are well-suited for relativecomparison.
The score between two terms is in-versely proportional to the shortest path between allthe synsets of both terms.
The highest possible scoreis one, if two terms share a synset.
We compare theaverage score of the top five (or ten) entries in theDT for each of the 2000 selected words for our com-parison.5 ResultsFirst, we inspect the results of Curran?s measure us-ing the Wikipedia and newspaper corpus for the fre-quent nouns, shown in Figure 1.Both graphs show the inverse ranking scoreagainst the size of the corpus.
Our method scoresconsistently higher when using LMI instead of PMIfor ranking the features per term.
The PMI measuredeclines when the corpus becomes larger.
This canbe attributed to the fact that PMI favors term-contextpairs involving rare contexts (Bordag, 2008).
Com-puting similarities between terms should not be per-formed on the basis of rare contexts, as these do notgeneralize well because of their sparseness.All other measures improve with larger corpora.It is surprising that recent works use PMI to calcu-late similarities between terms (Goyal et al 2010;Pantel et al 2009), who, however evaluate their ap-proach only with respect to their own implementa-tion or extrinsically, and do not prune on saliency.Apart from the PMI measure, Curran?s measureleads to the weakest results.
We could not confirmthat his measure outperforms Lin?s measure as statedin (Curran, 2002)1.
An explanation for this results1Regarding Curran?s Dice formula, it is not clear whether touse the intersection or the union of the features.
We use an inter-section, as it is unclear how to interpret the minimum functionotherwise, and the alternatives performed worse.886Figure 1: Inverse ranking for 1000 frequent nouns (Wikipedia left, Newspaper right) for different sized corpora.
The4 lines represent the scores of following DTs: our method using LMI (dashed black line) and the PMI significancemeasure (solid black line) and Curran?s (dash bray line) and Lin?s measure (solid tray line).might be the use of a different parser, very few testwords and also a different gold standard thesaurusin his evaluation.
Comparing our method using LMIto Lin?s method, we achieve lower scores with ourmethod using small corpora, but surpass Lin?s mea-sure from 10 million sentences onwards.Next, we show the results of the WordNet eval-uation measure in Figure 2.
Comparing the top 10(upper) to the top 5 words (lower) used for the eval-uation, we can observe higher scores for the top 5words, which validates the ranking.
These resultsare highly correlated to the results achieved with theinverse ranking measure.
This is a positive result,as the WordNet measure can be performed automat-ically using a single public resource2.
In Figure 3,we show results for the 1000 infrequent nouns usingthe inverse ranking (upper) and the WordNet mea-sure (lower).We can see that our method using PMI does notdecline for larger corpora, as the limit on first-orderfeatures is not reached and frequent features are stillbeing used.
Comparing our LMI DT is en par withLin?s measure for 10 million sentences, and makesbetter use of large data when using the completedataset.
Again, the inverse ranking and the Word-Net Path measure are highly correlated.2Building a gold standard thesaurus following Curran(2002) needs access to all the used thesauri.
Whereas for some,programming interfaces exist, often with limited access and li-cence restrictions, others have to be extracted manually.Figure 2: Results, using the WordNet:Path measure forfrequent nouns using the newspaper corpus.887Figure 3: WordNet::Path results for 1000 infrequentnounsThe results shown here validate our pruning ap-proach.
Whereas Lin and Curran propose ap-proaches to filter features that have low word featurescores, they do not remove features that occur withtoo many words, which is done in this work.
Usingthese pruning steps, a simplistic similarity measuredoes not only lead to reduced computation times, butalso to better results, when using larger corpora.5.1 Using a large3 corpusWe demonstrate the scalability of our method usingthe very large Google Books dataset (Goldberg andOrwant, 2013), consisting of dependencies extractedfrom 17.6 billion sentences.
The evaluation results,using different measures, are given in Table 2.Comparing the results for the Google Books DTto the ones achieved using Wikipedia and the news-Corpus Inv.
P@1 Path@5 Path@10frequentnounsNewspaper 2.0935 0.709 0.3277 0.2906Wikipedia 2.1213 0.703 0.3365 0.2968Google Books 2.3171 0.764 0.3712 0.3217infrequentnounsNewspaper 1.4097 0.516 0.2577 0.2269Wikipedia 1.3832 0.514 0.2565 0.2265Google Books 1.8125 0.641 0.2989 0.2565Table 2: Comparing results for different corpora.paper, we can observe a boost in the performance,both for the inverse ranking and the WordNet mea-sures.
Additionally, we show results for the P@1measure, which indicates the percentage of entries,whose first entry is in the gold standard thesaurus.Remarkably, we get a P@1 against our gold stan-dard thesaurus of 76% for frequent and 64% for in-frequent nouns using the Google Books DT.The most computation time was needed for thedependency parsing and took two weeks on a smallcluster (64 cores on 8 nodes) for the 120 millionNewspaper sentences.
The DT for the Google Bookswas calculated in under 30 hours on a Hadoop clus-ter (192 cores on 16 nodes) and could be calculatedwithin 10 hours for the Newspaper corpus.
The com-putation of a DT using this huge corpus would be in-tractable with standard vector-based measurements.Even computing Lin?s and Curran?s vector-basedsimilarity measure for the whole vocabulary of thenewspaper corpus was not possible with our Hadoopcluster, as too much memory would have been re-quired and thus we computed similarities only forthe 2000 test nouns on a server with 92GB of mainmemory.6 ConclusionWe have introduced a highly scalable approachto DT computation and showed its adequacy forvery large corpora.
Evaluating against thesauri andWordNet, we demonstrated that our similarity mea-sure yields better-quality DTs and scales to corporaof billions of sentences, even on comparably smallcompute clusters.
We achieve this by a number ofpruning operations, and distributed processing.
Theframework and the DTs for Google Books, News-paper and Wikipedia are available online3 under theASL 2.0 licence.3https://sf.net/projects/jobimtext/888AcknowledgmentsThis work has been supported by the Hessian re-search excellence program ?Landes-Offensive zurEntwicklung Wissenschaftlich-konomischer Exzel-lenz?
(LOEWE) as part of the research center ?Dig-ital Humanities?.
We would also thank the anony-mous reviewers for their comments, which greatlyhelped to improve the paper.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland wordnet-based approaches.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, NAACL ?09,pages 19?27, Boulder, Colorado, USA.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of the 39th Annual Meeting onAssociation for Computational Linguistics, ACL ?01,pages 26?33, Toulouse, France.Chris Biemann and Martin Riedl.
2013.
Text: Now in2D!
a framework for lexical expansion with contextualsimilarity.
Journal of Language Modelling, 1(1):55?95.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.Stefan Bordag.
2008.
A comparison of co-occurrenceand similarity measures as simulations of context.
InCICLing?08 Proceedings of the 9th international con-ference on Computational linguistics and intelligenttext processing, pages 52?63, Haifa, Israel.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 858?867, Prague, Czech Republic.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Computational Linguistics, 16(1):22?29.James R. Curran.
2002.
Ensemble methods for au-tomatic thesaurus extraction.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing - Volume 10, EMNLP ?02, pages222?229, Philadelphia, PA, USA.James R. Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.Jeffrey Dean and Sanjay Ghemawat.
2004.
MapReduce:Simplified Data Processing on Large Clusters.
In Pro-ceedings of Operating Systems, Desing & Implementa-tion (OSDI) ?04, pages 137?150, San Francisco, CA,USA.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis, In-stitut fu?r maschinelle Sprachverarbeitung, Universityof Stuttgart.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large corpusof english books.
In Second Joint Conference on Lex-ical and Computational Semantics (*SEM), Volume 1:Proceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 241?247, At-lanta, Georgia, USA.Gene H. Golub and William M. Kahan.
1965.
Calcu-lating the singular values and pseudo-inverse of a ma-trix.
J. Soc.
Indust.
Appl.
Math.
: Ser.
B, Numer.
Anal.,2:205?224.James Gorman and James R. Curran.
2006.
Scaling dis-tributional similarity to large corpora.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Associ-ation for Computational Linguistics, ACL-44, pages361?368, Sydney, Australia.Amit Goyal and Hal Daume?, III.
2011.
Generating se-mantic orientation lexicon using large data and the-saurus.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis, WASSA ?11, pages 37?43, Portland, Ore-gon, USA.Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume?, III, andSuresh Venkatasubramanian.
2010.
Sketch techniquesfor scaling distributional similarity to the web.
In Pro-ceedings of the 2010 Workshop on GEometrical Mod-els of Natural Language Semantics, GEMS ?10, pages51?56, Uppsala, Sweden.Dekang Lin.
1997.
Using syntactic dependency as localcontext to resolve word sense ambiguity.
In Proceed-ings of the 35th Annual Meeting of the Association forComputational Linguistics and Eighth Conference ofthe European Chapter of the Association for Compu-tational Linguistics, ACL ?98, pages 64?71, Madrid,Spain.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th interna-tional conference on Computational linguistics - Vol-ume 2, COLING ?98, pages 768?774, Montreal, Que-bec, Canada.889Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, 28(2):203?208.Marie-Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the International Conference on LanguageResources and Evaluation, LREC 2006, Genova, Italy.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38:39?41.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of the eighth ACMSIGKDD international conference on Knowledge dis-covery and data mining, KDD ?02, pages 613?619,Edmonton, Alberta, Canada.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume 2- Volume 2, EMNLP ?09, pages 938?947, Singapore.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword Fifth Edi-tion.
Linguistic Data Consortium, Philadelphia, PA,USA.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring therelatedness of concepts.
In Demonstration Papersat HLT-NAACL 2004, HLT-NAACL?Demonstrations?04, pages 38?41, Boston, Massachusetts, USA.Matthias Richter, Uwe Quasthoff, Erla Hallsteinsdo?ttir,and Chris Biemann.
2006.
Exploiting the leipzig cor-pora collection.
In Proceedings of the IS-LTC 2006,Ljubljana, Slovenia.Pavel Rychly?
and Adam Kilgarriff.
2007.
An efficientalgorithm for building a distributional thesaurus (andother sketch engine developments).
In Proceedingsof the 45th Annual Meeting of the ACL on InteractivePoster and Demonstration Sessions, ACL ?07, pages41?44, Prague, Czech Republic.Magnus Sahlgren.
2006.
The Word-Space Model: us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, StockholmUniversity.Peter D. Turney and Michael L. Littman.
2005.
Corpus-based learning of analogies and semantic relations.Machine Learning, 60(1-3):251?278.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional sim-ilarity.
In Proceedings of the 20th international con-ference on Computational Linguistics, COLING ?04,pages 1015?1021, Geneva, Switzerland.890
