INTEGRATING SPEECH AND NATURAL-LANGUAGE PROCESSINGRobert Moore, Fernando Pereira, and Hy MurveitSKI InternationalMenlo Park, California 94025ABSTRACTSRI has developed a new architecture for integrating speech and natural-language processing that applieslinguistic onstraints during recognition by incrementally expanding the state-transition network embodiedin a unification grammar.
We compare this dynamic-gralnlnar-network (DGN) approach to its principalalternative, word-lattice parsing, presenting preliminary experimental results that suggest the DGN approachrequires much less computation time than word-lattice parsing, while maintaining a very tractable recognitionsearch space.INTRODUCTIONWe report on an innovative and highly effective architecture for integrating speech and natural-language(NL) processing.
This architecture takes full advantage of the linguistic constraints supplied by an NLprocessor, yet it maintains a very tractable recognition search space and appears to require dramatically esscomputation by the NL processor than do other approaches.It is useful to begin by recalling why one would want to integrate speech and NL processing.
There aretwo primary reasons why this is desirable.
First, there are many applications of spoken-language processingthat require not just recognition but understanding, such as spoken-language systems for database queryand other computer interface applications.
Second, constraints on natural anguage can be used to reducethe speech recognition search space and therefore improve recognition accuracy.To take an example from our own system, one of the sentences in the 1987 DARPA resource managementspeaker-independent t s set is the following:What is the ETA at her destination of Fanning?Without any source of grammatical constraints (perplexity -- 1000), one version of the SRI system recognizesthis sentence as:Why added ETA at her destination of Fanning.This hypothesis contains three errors out of nine words in the original sentence.
With the fairly modestdegree of grammatical constraint represented by a perplexity-510 natural-language grammar, however, oursystem was able to recognize this sentence with no errors.The point is that even though the incorrect hypothesis represented a better phonetic and phonologicalmatch to the input signal according to the models used by our system, with an NL grammar the system wasable to rule out the incorrect hypothesis and choose the correct, although lower-scoring, hypothesis.
Whilethis is, of course, a carefully selected example of our system's performance, it is a good illustration of whatwe are trying to acheive.243OTHER APPROACHES TO SPEECH/NL  INTEGRATIONPrior to the work reported here, there have been two predominant approaches to integrating speech and NLprocessing.
The first is the serial connection of speech and NL processors, and the second is word-latticeparsing.In the serial approach, the speech recognizer makes its best guess as to what word string was uttered, andthe NL processor parses and interprets the word string as if it were text.
This approach as the advantagesthat it is simple to implement, and it is relatively fast because neither process increases the search spaceof the other.
Unfortunately, it takes no advantage of the NL constraints embodied in the NL processor toguide the recognizer, so we do not acheive the kind of integration we are looking for.In word-lattice parsing, this problem is addressed by having the recognizer refrain from making any finaldecisions as to what string of words has been uttered.
Instead the recognizer passes to the NL processor aword lattice--a set of word hypotheses and their acoustic match scores--and the NL processor has to findthe best scoring path, through the word lattice, that constitutes a well-formed utterance according to theconstraints he NL processor knows about.Word-lattice parsing thus succeeds in applying NL constraints to aid recognition, but it results in ex-tremely long processing times, at least in current implementations.
We have identified a number of factorsthat we believe are responsible for this.
First, the parser must deal with a word lattice containing thousandsof word hypotheses rather than a string of just a few words.
This, in effect, amounts to a massive degreeof lexical ambiguity, giving the parser many more possible analyses to consider than in text parsing.
Moreparticularly, the parser must deal with a large degree of word-boundary uncertainty.
Normally, a word lat-tice of adequate size for accurate recognition will contain dozens of instances of the same word with slightlydifferent start and end points.
A word-lattice parser must treat these, at least to some extent, as distincthypotheses.
The best approach to this problem proposed so far (Chow and Roukos, 1989) seems to be toassociate with each word or phrase a set of triples of start points, end points, and scores.
Each possibleparsing step is then performed only once, but a dynamic programming procedure must also be performedto compute the best score for the resulting phrase for each possible combination of start and end points forthe phrase.In addition to the parser having to do extra work, word-lattice parsing requires extra work of the rec-ognizer.
Because word hypotheses are put together to form paths by the parser ather than the recognizer,the recognizer must treat each word starting at each point in the input as an independent hypothesis.
Thus,much of the efficiency of the dynamic programming methods used in recognition systems is lost, and therecognizer faces a much larger search space than if it were running with no grammar or a simple finite-stategrammar.
Moreover, if the recognizer is using a pruned search to generate the word lattice, as it must tohave any hope of practicality, it cannot ake advantage of grammatical constraints on acceptable paths inmaking its pruning decisions.
The recognizer is unable to make use of the fact that a word hypothesis alignswith only low-scoring acceptable paths to help prune that hypothesis.OUR APPROACH:  DYNAMIC  GRAMMAR NETWORKSIn view of the efficiency problems of word-lattice parsing, we have developed an alternative approach based onthe concept of dynamic grammar networks.
In this approach, we use an NL parser to incrementally generatethe grammar-state-transition table used in the standard hidden-Markov-model (HMM) speech-recognitionarchitecture.
In an HMM speech recognizer, a finite-state grammar is used to predict what words can startin a particular ecognition state and to specify what recognition state the system should go into when aparticular word is recognized in a given predecessor state.
The Viterbi algorithm is used to efficiently assigna score to each recognition state the system may be in at a particular point in the signal.In HMM systems, the finite-state grammar is represented as a set of state-word-state (or state-word-state-word) transitions.
Any type of linguistic onstraints can, in fact, be represented assuch a set, but fora nontrivial NL grammar the set will be infinite.
Even a useful finite approximation would be too large to244compute given current hardware, and too large to store in current memories that would be fast enough forpractical applications of speech recognition.Our approach to this problem is to compute the state-transition table needed by the HHM systemincrementally, generating just the portion necessary to guide the pruned search carried out by the recognizerfor a particular utterance.
When the recognizer is started up, it contains an initial state and the words thatare possible in that state according to the NL grammar.
When a word is successfully recognized withoutbeing pruned; the recognizer sends the word and the state it started in to the NL processor, which returnsthe states that can follow and a specification of what words can start in each state.In our NL processor, we use a stack-based unification-grammar parser incorporating left-context con-straints.
The parser is roughly equivalent to an LK(0) parser that generates LR(0) item sets on the fly.
Therecognition states are derived from parser configurations, o that when the recognizer sends a state and aword to the parser, the parser can decode the state and advance the resulting parser configurations by therecognized word.
The parser then encodes the resulting configurations and returns a specification of a setofstate-word pairs to the recognizer.Although in this architecture the recognizer rather than the parser has control of the search, the parse orparses of the recognized string can easily be recovered for further NL processing.
The backtrace informationthe recognizer uses to recover the recognized string also contains the recognition states along the backtracepath.
Because these states encode parser configurations, it is possible to recover the parses of the recognizedstring with no additional search.It should be emphasized that this approach is computationally completely equivalent o word-latticeparsing, although it seems to be much faster.
One can think of the paths through a word lattice as repre-senting the recognition search space.
In word-lattice parsing the entire search space is generated, encodedin the word lattice, and NL constraints are brought to bear afterward to prune out the paths that violatethose constraints.
In the dynamic-grammar-network approach, the same constraints are brought o bear asthe search space is being generated, so the paths that violate the constraints are never produced.
In eithercase, the final set of paths allowed is the same.Thisarchitecture addresses the major efficiency problems we pointed out in word-lattice parsing.
Becausethe recognizer does all the tracking of input positions and scores, the parser has no need to deal with word-boundary uncertainty.
In fact, no information about word boundaries i  even passed to the parser.
This ispossible because the parser is stack rather than position based.
Because NL constraints are applied at everystate, the recognition search is confined to the states that arise in the grammar ather than treating everypoint in the input as a distinct recognition state.
Thus, the Viterbi search can collapse any word hypothesesthat begin in the same state and end at the same point, without having to keep track of their starting points.Finally, because the recognizer is following paths that are guaranteed to meet the NL constraints, its pruningcan take advantage of that information to help prune out word hypotheses that continue only low-scoringpaths.In addition to addressing the efficiency problems of word-lattice parsing, the dynamic-grammar-networkapproach as a number of other important advantages.
First, it presents a very clean interface between therecognizer and the NL processor, preserving the structure of existing recognition systems.
Thus, everythingthat has been learned about making HMM systems efficient can be applied in this design.
This carries overto implementations of the HMM architecture in special-purpose hardware currently under development.
Theonly modification of such designs that will be necessary is to transmit o the parser what recognition stateshave been reached, and to allow the parser to download state transitions into the tIHM system's grammarmemory.A particular advantage of our implementation of the dynamic-grammar-network approach is that it isbased on unification grammar, which is not only a superior formalism for encoding syntactic onstraints onnatural anguage, but which, as our prior work has shown, can be used to encode semantic onstraints aswell.
We believe this is of critical importance, because syntax alone seems to be fairly limited in its ability toconstrain recognition, but semantic onstraints seem to be far more restrictive.
These can also be expressedin our unification framework and applied by the unification 'parser in our current implementation.Finally, because recognition and NL processing are combined so tightly, it appears that our architecture245Med/Mean Med/Mean Med/Mean CumulativePruning Sentence Parsing Active Hyp WordThreshold Length Time (sec) per Frame Accuracy400K 6.5/7.1 63/102 539/592 85.5%500K 6.5/7.1 129/310 852/1523 87.8%Table 1: Experimental Resultsis ideally suited to exploring the interactions of prosody and phonology with syntax.
This should provideyet another source of constraint on the recognition problem, further improving recognition accuracy, and itshould also provide prosodic and phonological information to help resolve syntactic ambiguities.
It appearsthat many of these constraints can be encoded in the unification formalism in our system.RESULTS OF INITIAL EXPERIMENTSIn the preceding sections, many theoretical rguments have been advanced as to why the dynamic-grammar-network approach ought to be superior to its possible competitors.
Without empirical support, however,those predictions would be of little significance.
We have built an initial implementation f our architecturethat, in fact, seems to bear out those predictions in a fairly dramatic fashion.
The system has been testedon twenty-four sentences chosen from the 1987 DARPA resource management speaker-independent testset, using an 885-word subset of the standard 1000-word vocabulary.
The perplexity of this test set wasmeasured to be 510 with respect o the vocabulary and NL grammar used in the test.
1 Currently ourgrammar covers about half the sentences in the resource management database.
We do not expect heefficiency of our system to be seriously affected as coverage increases, however, because we already cover theconstructions that lead to most of the complexity in natural-language parsing: conjunction, relative clauses,WH-questions, prepositional phrases, and other postnominal nd postverbal modifiers.
On the test set usedin this experiment, Sl:tI's recognizer scores 82.6 percent word accuracy with no grammar (perplexity-1000),and 97.2 percent with a perplexity-60 word-pair grammar.The results of our experiments are summarized in Table 1.
The table presents summaries of the results ofrunning all twenty-four test sentences with two different pruning thresholds.
The smaller number epresentstighter pruning.
First, we note that at both levels of pruning there appears to be a significant improvementin recognition accuracy over the recognizer with no grammar.
There was only one sentence of the twenty-fouron which the no-grammar recognizer performed better than the run with the 400K pruning threshold, andno such sentences for the run with the 500K threshold.
The run with the 500K threshold produced a 30precent reduction in the word-error rate.
We believe that the really important results of this test, however,are the parsing times and the recognition search space size, the latter being measured by average number ofhypotheses active in a given frame.
We show both median and mean values for these measures.The parsing times are for a parser unning in Prolog on an 8-megabyte Sun 3/60 workstation.
At the 400Kpruning threshold, the median parsing time per sentence was just over one minute, and at 500K, just overtwo minutes.
This compares with two hours or more for word-lattice parsing experiments reported elsewhere.Thus, these times represent an improvement ofapproximately two orders of magnitude over previous resultswith word-lattice parsing.
The recognition search space size is an equally significant measure of the success ofthis architecture.
The median umber of active hypotheses per frame was under 1000 in both runs.
Becausethe real-time recognition hardware SRI is currently designing and building will ultimately have a capacity toprocess 6000 active hypotheses per frame, our architecture should easily support real-time recognition withmultithousand word vocabularies.1Although the test set is small, it does  represent (with the except ion  of  one sentence discussed below) all the sentences wehave had an opportunity to test our  sys tem on to date.
Thus it is not  specially selected to show our system to  advantage.246The parsing times we have acheived, as good as they are compared to previous results, still require morethan an order of magnitude improvement for real-time performance.
In addition, the fact that the meantimes were so much higher than the median is an indication that on difficult sentences, the parsing time canbecome xtraordinarily long.
These turn out to be sentences where the pruning thresholds fail to reduce thehypotheses considered to a reasonable number.
The parser then simply bogs down in the resulting mass ofdata.
2 We believe this problem can be addressed by more sophisticated pruning methods.Getting the median parsing time down to real-time levels can be addressed in a number of ways.
Thereare many algorithmic improvements we are already planning to try.
Moreover, simply moving to fasterhardware will decrease parsing time substantially, and another significant speedup would be obtained byrecoding in a lower-level language such as C. We believe that with these improvements and the advent ofreal-time recognition hardware, it is not unreasonable to hope for a complete real-time system within therelatively near future.SUMMARYWe believe the results reported in this paper have demonstated:?
A dramatic improvement in parsing times for integrated speech/NL systems?
A highly tractable recognition search space?
Significant improvement in recognition performance with natural-language grammars?
An architecture that can exploit existing recognition system designs and prospective r al-time hardwarewith only minimal modifications?
A clear path toward including important semantic onstraints on recognition, and possibly prosodic-phonological/syntactic onstraints as wellThese results suggest that the dynamic-grammar-network approach to the integration of speech and NLprocessing may well be the key technology needed to produce true spoken-language systems with real-timeperformance.AcknowledgmentsIn addition to the authors, significant contributions to the work reported here have been made by MaryDalrymple, Mike Cohen, and Mitch Weintraub.
This work has been supported by SKI International internalresearch and development funds.ReferencesChow, Y. L., and S. Roukos (1989) "Speech Understanding Using a Unification Grammar," in Proceedings ofthe IEEE International Conference on Acoustics, Speech, and Signal Processing, Glasgow, Scotland (May 23-26, 1989).2Indeed, a twenty-fith sentence was originally included in our test set, but  had to be discarded because the 8-megabyteworkstation the parser was runrdng on was too small to get the entire working set for this sentence in memory at the sametime.
Had that sentence been included, the mean parsing times would undoubtedly have increased significantly, but  the mediantimes would have changed only slightly.247
