Proceedings of the SIGDIAL 2013 Conference, pages 122?126,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsImproving Interaction Quality Recognition Using Error CorrectionStefan UltesUlm UniversityAlbert-Einstein-Allee 4389081 Ulm, Germanystefan.ultes@uni-ulm.deWolfgang MinkerUlm UniversityAlbert-Einstein-Allee 4389081 Ulm, Germanywolfgang.minker@uni-ulm.deAbstractDetermining the quality of an ongoing in-teraction in the field of Spoken DialogueSystems is a hard task.
While existingmethods employing automatic estimationalready achieve reasonable results, stillthere is a lot of room for improvement.Hence, we aim at tackling the task by es-timating the error of the applied statisticalclassification algorithms in a two-stage ap-proach.
Correcting the hypotheses usingthe estimated model error increases per-formance by up to 4.1 % relative improve-ment in Unweighted Average Recall.1 IntroductionEvaluating the quality of Spoken Dialogue Sys-tems (SDSs) has long since been a challengingtask.
While objective metrics like task completionand dialogue duration are not human-centered,subjective measures compensate for this by mod-eling the user?s subjective experience.
This infor-mation may be used to increase the dialogue sys-tem?s performance (cf.
(Ultes et al 2012b)).In human-machine dialogues, however, there isno easy way of deriving the user?s satisfactionlevel.
Moreover, asking real users for answeringquestions about the system performance requiresthem to spend more time talking to the machinethan necessary.
It can be assumed that a regularuser does not want to do this as human-machinedialogues usually have no conversational charac-ter but are task oriented.
Hence, automatic ap-proaches are the preferred choice.Famous work on determining the satisfactionlevel automatically is the PARADISE frameworkby Walker et al(1997).
Assuming a linear depen-dency between objective measures and User Satis-faction (US), a linear regression model is appliedto determine US on the dialogue level.
This is notonly very costly, as dialogues must be performedwith real users, but also inadequate if quality on afiner level is of interest, e.g., on the exchange level.To overcome this issue, work by Schmitt etal.
(2011) introduced a new metric for measuringthe performance of an SDS on the exchange levelcalled Interaction Quality (IQ).
They used statisti-cal classification methods to automatically derivethe quality based on interaction parameters.
Qual-ity labels were applied by expert raters after the di-alogue on the exchange level, i.e., for each system-user-exchange.
Automatically derived parameterswere then used as features for creating a statisticalclassification model using static feature vectors.Based on the same data, Ultes et al(2012a) putan emphasis on the sequential character of the IQmeasure by applying temporal statistical classifi-cation using Hidden Markov Models (HMMs) andContinuous Hidden Markov Models (CHMMs).However, statistical classifiers usually do notachieve perfect performance, i.e., there will al-ways be misclassification.
While most work fo-cuses on applying different statistical models andimproving them (Section 2), learning the error tocorrect the result afterwards represents a differentapproach.
Therefore, we present our approach onestimating the error of IQ recognition models tocorrect their hypothesis in order to eventually yieldbetter recognition rates (Section 4).
The definitionof IQ and data used for the evaluation of our ap-proach (Section 5) is presented in Section 3.
Ourapproach is also compared to a simple hierarchicalapproach also discussed in Section 5.2 Related Work on Dialogue QualityBesides Schmitt et al other research groups haveperformed numerous work on predicting subjec-tive quality measures on an exchange level, all notincorporating any form of error correction.Engelbrecht et al(2009) presented an approachusing Hidden Markov Models (HMMs) to model122e n?e n-1e n-2e1e n+1?exchange level parameterswindowlevel parametersdialogue levelparametersFigure 1: The three different modeling levels rep-resenting the interaction at exchange en.the SDS as a process evolving over time.
Perfor-mance ratings on a 5 point scale (?bad?, ?poor?,?fair?, ?good?, ?excellent?)
have been applied bythe users during the dialogue.Higashinaka et al(2010) proposed a model forpredicting turn-wise ratings for human-human dia-logues analyzed on a transcribed conversation andhuman-machine dialogues with text from a chatsystem.
Ratings ranging from 1 to 7 were ap-plied by two expert raters labeling for smoothness,closeness, and willingness.Hara et al(2010) derived turn level ratings fromoverall ratings of the dialogue which were appliedby the users afterwards on a five point scale.
Us-ing n-grams to model the dialogue, results for dis-tinguishing between six classes at any point in thedialogue showed to be hardly above chance.3 The LEGO CorpusFor estimating the Interaction Quality (IQ), theLEGO corpus published by Schmitt et al(2012)is used.
IQ is defined similarly to user satisfac-tion: While the latter represents the true disposi-tion of the user, IQ is the disposition of the user as-sumed by an expert rater.
The LEGO corpus con-tains 200 calls (4,885 system-user-exchanges) to abus information system (cf.
(Raux et al 2006)).Labels for IQ on a scale from 1 (extremely un-satisfied) to 5 (satisfied) have been assigned bythree expert raters with an inter-rater agreement of?
= 0.54.
In order to ensure consistent labeling,the expert raters had to follow labeling guidelines(cf.
(Schmitt et al 2012)).Parameters used as input variables for the IQmodel have been derived from the dialogue sys-tem modules automatically for each exchange onthree levels: the exchange level, the dialogue level,and the window level (see Figure 1).
As parame-ters like the confidence of the speech recognizercan directly be acquired from the dialogue mod-ules, they constitute the exchange level.
Based onthis, counts, sums, means, and frequencies of ex-change level parameters from multiple exchangesare computed to constitute the dialogue level (allexchanges up to the current one) and the windowlevel (the three previous exchanges).
A completelist of parameters is listed in (Schmitt et al 2012).Schmitt et al(2011) performed IQ recognitionon this data using linear SVMs.
They achieved anUnweighted Average Recall (UAR) of 0.58 basedon 10-fold cross-validation.
Ultes et al(2012a)applied HMMs and CHMMs using 6-fold crossvalidation and a reduced feature set achieving anUAR of 0.44 for HMMs and 0.39 for CHMMs.4 Error Estimation ModelError correction may be incorporated into the sta-tistical classification process by a two-stage ap-proach, which is depicted in Figure 2.At the first stage, a statistical classificationmodel is created using interaction parameters asinput and IQ as target variable.
For this work,a Support Vector Machine (SVM) and a RuleLearner are applied.
At the second stage, the er-ror er of the hypothesis h0 is calculated byer = h0 ?
r , (1)where the reference r denotes the true IQ value.In order to limit the number of error classes, thesignum function is applied.
It is defined assgn(x) :=?????
?1 if x < 0 ,0 if x = 0 ,1 if x > 0 .
(2)Therefore, the error is redefined aser = sgn(h0 ?
r) .
(3)Next, a statistical model is created similarly tostage one but targeting the error er.
The differenceis that the input parameter set is extended by the IQhypothesis h0 of stage one.
Here, two approachesare applied: Creating one model which estimatesall error classes (?1,0,1) and creating two mod-els where each estimates positive (0,1) or negativeerror (?1,0).
For the latter variant, the error ofthe class which is not estimated by the respectivemodel is mapped to 0.
By this, the final error hy-pothesis he may be calculated by simple additionof both estimated error values:he = he?1 + he+1 .
(4)Combining the hypothesis of the error estima-tion he with the hypothesis of the IQ estimation h012312IQmodelInteractionParametersHypothesisIQ h0ReferenceIQ r?
ReferenceError IQ er?Parameters+HypothesisError IQmodelError IQmodelHypothesisError IQ he?
FinalHypothesis IQhfinputtargetinputtargetFigure 2: The complete IQ estimation process including error correction.
After estimating IQ in Stage 1(upper frame), the error is estimated and the initial hypothesis is corrected in Stage 2 (lower frame).at stage one produces the final hypothesis hf de-noting the Interaction Quality estimation correctedby the estimated error of the statistical model:hf = h0 ?
he .
(5)As the error estimation will not work perfectly,it might recognize an error where there is none or?
even worse ?
it might recognize an error contraryto the real error, e.g., ?1 instead of +1.
Therefore,the corrected hypothesis might be out of range.
Tokeep hf within the defined bounds of IQ, a lim-iting functions is added to the computation of thefinal hypothesis resulting inhf = max(min(h0 ?
he), bu), bl) , (6)where bu denotes the upper bound of the IQ labelsand bl the lower bound.5 Experiments and ResultsAll experiments are conducted using the LEGOcorpus presented in Section 3.
By applying 5-foldcross validation, hypotheses for each system-user-exchange which is contained in the LEGO corpusare estimated.
Please note that some textual inter-action parameters are discarded due to their task-dependent nature leaving 45 parameters1.For evaluation, we rely on two measures: Theunweighted average recall (UAR) and the root1Removed parameters: Activity, LoopName, Prompt,RoleName, SemanticParse, SystemDialogueAct, UserDia-logueAct, Utterancemean squared error (RMSE).
UAR represents theaccuracy corrected by the effects of unbalanceddata and is also used by cited literature.
RMSE isused since the error correction method is limitedto correcting the results only by one.
For biggererrors, the true value cannot be reached.The performances of two different statisticalclassification methods are compared, both appliedfor stage one and stage two: Support Vector Ma-chine (SVM) (Vapnik, 1995) using a linear ker-nel, which is also used by Schmitt et al(2011),and Rule Induction (RI) based on Cohen (1995).Furthermore, a normalization component is addedperforming a range normalization of the input pa-rameters in both stages.
This is necessary for usingthe implementation of the statistical classificationalgorithms at hand.For error estimation, two variants are explored:using one combined model for all three errorclasses (?1,0,+1) and using two separate models,one for distinguishing between ?1 and 0 and onefor distinguishing between +1 and 0 with com-bining their results afterwards.
While using RI forerror estimation yields reasonable performance re-sults for the combined model, it is not suitable forerror estimation using two separate models as allinput vectors are mapped to 0.
Hence, for the twomodel approach, only the SVM is applied .Results for applying error correction (EC) arepresented in Table 1.
Having an SVM at stage one(column SVM), recognition performance is rela-tively improved by up to 4.6 % using EC.
With RI124Table 1: Results for IQ recognition: UAR andRMSE for IQ recognition without stage two, witherror correction at stage two, and with a simple hi-erarchical approach.UAR RMSEstage two SVM RI SVM RInone 51.1% 60.3% 0.97 0.88error correctionSVM 50.7% 59.6% 0.97 0.83RI 52.5% 58.1% 0.88 0.852xSVM 53.2% 60.6% 0.88 0.85simple hierarchical approachSVM 50.2% 57.6% 0.97 0.85RI 58.9% 58.7% 0.88 0.88at stage one, performance is only increased by upto 0.5 % which has shown to be not significant us-ing the Wilcoxon test.
The relative improvementsin UAR are depicted in Figure 3.Furthermore, these results are compared to asimple hierarchical approach (SH) where the hy-pothesis h0 of the stage one classifier is used asan additional feature for the stage two classifiertargeting IQ directly.
Here, the performance ofthe stage two classifier is of most interest sincethis approach can be viewed as one stage classi-fication with an additional feature.
The results inTable 1 show that RI does not benefit from addi-tional information (comparison of last row withone stage RI recognition).
SVM recognition atstage two, though, shows better results.
While itsperformance is reduced using the SVM hypothe-sis as additional feature, adding the RI hypothesisimproved UAR up to 12.6 % relatively.
However,there is no reasonable scenario where one wouldnot use the better performing RI in favor of usingits results as additional input for SVM recognition.The question remains why SVM benefits fromError Correction as well as from adding additionalinput parameters while RI does not.
It remains un-clear if this is an effect of the task characteristicscombined with the characteristics of the classifi-cation method.
It may as well be caused by lowclassification performance.
A classifier with lowperformance might be more likely to improve itsperformance by additional information or EC.6 ConclusionIn this work, we presented an approach for im-proving the recognition of Interaction Quality byestimating the error of the classifier in order to cor-rect the hypothesis.
For the resulting two-staged?0.8% ?1.3%2.7%?3.7%4.1%0.5%?4%?3%?2%?1%0%1%2%3%4%SVM RIerror?correction?(SVM)error?correction?(RI)error?correction?
(2?x?SVM)Figure 3: The relative improvement of EC in UARgrouped by stage one classifiers SVM and RI.approach, two different statistical classification al-gorithm were applied for both stages, i.e., SVMand Rule Learner.
Performance could be improvedfor both stage one classifiers using separate er-ror models relatively improving IQ recognition byup to 4.1 %.
The proposed error correction ap-proach has been compared to a simple hierarchi-cal approach where the hypohtesis of stage oneis used as additional feature of stage two classi-fication.
This apprach relatively improved SVMrecognition by up to 12.6 % using a Rule Learnerhypothesis as additional feature.
However, as one-stage Rule Learner classification already providesbetter results than this hierarchical approach, isdoes not seem reasonable to employ this config-uration.
Nonethelesse, why only the SVM couldbenefit from additional information (error correc-tion or simple hierarchical appraach) remains un-clear and should be investigated in future work.Moreover, some aspects of the error correc-tion approach have to be discussed controversially,e.g., applying the signum function for calculatingthe error.
While the obvious advantage is to limitthe number of error classes a statistical classifica-tion algorithm has to estimate, it also prohibits ofbeing able to correct all errors.
If the absolute er-ror is bigger than one it can never be corrected.AcknowledgmentsThis work was supported by the TransregionalCollaborative Research Centre SFB/TRR 62?Companion-Technology for Cognitive TechnicalSystems?
which is funded by the German Re-search Foundation (DFG).125ReferencesWilliam W. Cohen.
1995.
Fast effective rule induc-tion.
In Proceedings of the 12th International Con-ference on Machine Learning, pages 115?123.
Mor-gan Kaufmann, July.Klaus-Peter Engelbrecht, Florian Go?dde, Felix Har-tard, Hamed Ketabdar, and Sebastian Mo?ller.
2009.Modeling user satisfaction with hidden markovmodel.
In SIGDIAL ?09: Proceedings of the SIG-DIAL 2009 Conference, pages 170?177, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.2010.
Estimation method of user satisfaction us-ing n-gram-based dialog history model for spokendialog system.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Ros-ner, and Daniel Tapias, editors, Proceedings of theSeventh conference on International Language Re-sources and Evaluation (LREC?10), Valletta, Malta,May.
European Language Resources Association(ELRA).Ryuichiro Higashinaka, Yasuhiro Minami, KohjiDohsaka, and Toyomi Meguro.
2010.
Issues in pre-dicting user satisfaction transitions in dialogues: In-dividual differences, evaluation criteria, and predic-tion models.
In Gary Lee, Joseph Mariani, Wolf-gang Minker, and Satoshi Nakamura, editors, Spo-ken Dialogue Systems for Ambient Environments,volume 6392 of Lecture Notes in Computer Sci-ence, pages 48?60.
Springer Berlin / Heidelberg.10.1007/978-3-642-16202-2 5.Antoine Raux, Dan Bohus, Brian Langner, Alan W.Black, and Maxine Eskenazi.
2006.
Doing researchon a deployed spoken dialogue system: One yearof lets go!
experience.
In Proc.
of the Interna-tional Conference on Speech and Language Process-ing (ICSLP), September.Alexander Schmitt, Benjamin Schatz, and WolfgangMinker.
2011.
Modeling and predicting quality inspoken human-computer interaction.
In Proceed-ings of the SIGDIAL 2011 Conference, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Alexander Schmitt, Stefan Ultes, and WolfgangMinker.
2012.
A parameterized and annotated cor-pus of the cmu let?s go bus information system.
InInternational Conference on Language Resourcesand Evaluation (LREC).Stefan Ultes, Robert ElChabb, and Wolfgang Minker.2012a.
Application and evaluation of a conditionedhidden markov model for estimating interactionquality of spoken dialogue systems.
In Joseph Mari-ani, Laurence Devillers, Martine Garnier-Rizet, andSophie Rosset, editors, Proceedings of the 4th In-ternational Workshop on Spoken Language DialogSystem (IWSDS), pages 141?150.
Springer, Novem-ber.Stefan Ultes, Alexander Schmitt, and WolfgangMinker.
2012b.
Towards quality-adaptive spokendialogue management.
In NAACL-HLT Workshopon Future directions and needs in the Spoken Di-alog Community: Tools and Data (SDCTD 2012),pages 49?52, Montre?al, Canada, June.
Associationfor Computational Linguistics.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Marilyn Walker, Diane Litman, Candace A. Kamm,and Alicia Abella.
1997.
Paradise: a frameworkfor evaluating spoken dialogue agents.
In Proceed-ings of the eighth conference on European chap-ter of the Association for Computational Linguistics,pages 271?280, Morristown, NJ, USA.
Associationfor Computational Linguistics.126
