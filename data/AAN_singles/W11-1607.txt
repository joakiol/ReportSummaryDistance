Workshop on Monolingual Text-To-Text Generation, pages 54?63,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 54?63,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsLearning to Fuse Disparate SentencesMicha ElsnerSchool of InformaticsUniversity of Edinburghmelsner0@gmail.comDeepak SanthanamBrown Lab forLinguistic Information Processing (BLLIP)Department of Computer ScienceBrown University, Providence, RI 02912dsanthan@microsoft.comAbstractWe present a system for fusing sentenceswhich are drawn from the same source docu-ment but have different content.
Unlike previ-ous work, our approach is supervised, trainingon real-world examples of sentences fused byprofessional journalists in the process of edit-ing news articles.
Like Filippova and Strube(2008), our system merges dependency graphsusing Integer Linear Programming.
However,instead of aligning the inputs as a preprocess,we integrate the tasks of finding an alignmentand selecting a merged sentence into a jointoptimization problem, and learn parametersfor this optimization using a structured onlinealgorithm.
Evaluation by human judges showsthat our technique produces fused sentencesthat are both informative and readable.1 IntroductionSentence fusion is the process by which content fromtwo or more original sentences is transformed into asingle output sentence.
It is usually studied in thecontext of multidocument summarization, since fus-ing similar sentences can avoid repetition of materialwhich is shared by more than one input.
However,human editors and summarizers do not restrict them-selves to combining sentences which share most oftheir content.
This paper extends previous work onfusion to the case in which the input sentences aredrawn from the same document and express funda-mentally different content, while still remaining re-lated enough to make fusion sensible1.1Unfortunately, we cannot release our corpus due to li-censing agreements.
Our system is available at https://Our data comes from a corpus of news articles forwhich we have un-edited and edited versions.
Wesearch this corpus for sentences which were fused(or separated) by the editor; these constitute natu-rally occurring data for our system.
One examplefrom our dataset consists of input sentences (1) and(2) and output (3).
We show corresponding regionsof the input and output in boldface.
(1) The bodies showed signs of torture.
(2) They were left on the side of a highway inChilpancingo, about an hour north of thetourist resort of Acapulco in the southernstate of Guerrero, state police said.
(3) The bodies of the men, which showed signsof torture, were left on the side of a highwayin Chilpancingo, which is about an hournorth of the tourist resort of Acapulco, statepolice told Reuters.While the two original sentences are linked by acommon topic and reference to a shared entity, theyare not paraphrases of one another.
This could cre-ate a problem for traditional fusion systems whichfirst find an alignment between similar dependencygraphs, then extract a shared structure.
While oursystem has the same basic framework of alignmentand extraction, it performs the two jointly, as partsof a global optimization task.
This makes it robustto uncertainty about the hidden correspondences be-tween the sentences.
We use structured online learn-ing to find parameters for the system, allowing it tobitbucket.org/melsner/sentencefusion.54discover good ways to piece together input sentencesby examining examples from our corpus.Sentence fusion is a common strategy in human-authored summaries of single documents?
36% ofsentences in the summaries investigated by Jingand McKeown (1999) contain content from multiplesentences in the original document.
This suggeststhat a method to fuse dissimilar sentences couldbe useful for single-document summarization.
Ourdataset is evidence that editing also involves fusingsentences, and thus that models of this task couldcontribute to systems for automatic editing.In the remainder of the paper, we first give anoverview of related work (Section 2).
We next de-scribe our dataset and preprocessing in more detail(Section 3), describe the optimization we perform(Section 4), and explain how we learn parameters forit (Section 5).
Finally, we discuss our experimentalevaluation and give results (Section 6).2 Related workPrevious work on sentence fusion examines the taskin the context of multidocument summarization, tar-geting groups of sentences with mostly redundantcontent.
The pioneering work on fusion is Barzilayand McKeown (2005), which introduces the frame-work used by subsequent projects: they representthe inputs by dependency trees, align some words tomerge the input trees into a lattice, and then extracta single, connected dependency tree as the output.Our work most closely follows Filippova andStrube (2008), which proposes using Integer Lin-ear Programming (ILP) for extraction of an outputdependency tree.
ILP allows specification of gram-maticality constraints in terms of dependency rela-tionships (Clarke and Lapata, 2008), as opposed toprevious fusion methods (Barzilay and McKeown,2005; Marsi and Krahmer, 2005) which used lan-guage modeling to extract their output.In their ILP, Filippova and Strube (2008) optimizea function based on syntactic importance scoreslearned from a corpus of general text.
While similarmethods have been used for the related task of sen-tence compression, improvements can be obtainedusing supervised learning (Knight and Marcu, 2000;Turner and Charniak, 2005; Cohn and Lapata, 2009)if a suitable corpus of compressed sentences can beobtained.
This paper is the first we know of to adoptthe supervised strategy for sentence fusion.For supervised learning to be effective, it is nec-essary to find or produce example data.
Previouswork does produce some examples written by hu-mans, though these are used during evaluation, notfor learning (a large corpus of fusions (McKeown etal., 2010) was recently compiled as a first step to-ward a supervised fusion system).
However, theyelicit these examples by asking experimental sub-jects to fuse selected input sentences?
the choiceof which sentences to fuse is made by the system,not the subjects.
In contrast, our dataset consists ofsentences humans actually chose to fuse as part of apractical writing task.
Moreover, our sentences havedisparate content, while previous work focuses onsentences whose content mostly overlaps.Input sentences with differing content present achallenge to the models used in previous work.All these models use deterministic node alignmentheuristics to merge the input dependency graphs.Filippova and Strube (2008) align all content wordswith the same lemma and part of speech; Barzi-lay and McKeown (2005) and Marsi and Krahmer(2005) use syntactic methods based on tree simi-larity.
Neither method is likely to work well forour data.
Lexical methods over-align, since thereare many potential points of correspondence be-tween our sentences, only some of which shouldbe merged?
?the Doha trade round?
and ?U.S.
traderepresentative?
share a word, but probably ought toremain separate regardless.
Syntactic methods, onthe other hand, are unlikely to find any alignmentssince the input sentences are not paraphrases andhave very different trees.
Our system selects the setof nodes to merge during ILP optimization, allowingit to choose correspondences that lead to a sensibleoverall solution.3 Data and preprocessingOur sentence fusion examples are drawn from a cor-pus of 516 pre- and post-editing articles from theThomson-Reuters newswire, collected over a periodof three months in 2008.
We use a simple greedymethod based on bigram count overlaps to align thesentences of each original article to sentences in theedited version, allowing us to find fused sentences.55Since these sentences are relatively rare, we use bothmerges (where the editor fused two input sentences)and splits (where the editor splits an input sentenceinto multiple outputs) as examples for our system.In the case of a split, we take the edited sentencesas input for our method and attempt to produce theoriginal through fusion2.
This is suboptimal, sincethe editor?s decision to split the sentences probablymeans the fused version is too long, but is requiredin this small dataset to avoid sparsity.Out of a total of 9007 sentences in the corpus,our bigram method finds that 175 were split and 132were merged, for a total of 307.
We take 92 exam-ples for testing and 189 for training3.Following previous work (Barzilay and McKe-own, 2005), we adopt a labeled dependency formatfor our system?s input.
To produce this, we segmentsentences with MXTerminator (Reynar and Ratna-parkhi, 1997) and parse the corpus with the self-trained Charniak parser (McClosky et al, 2006).
Wethen convert to dependencies and apply rules to sim-plify and label the graph.
An example dependencygraph is shown in Figure 1.We augment the dependency tree by adding apotential dependency labeled ?relative clause?
be-tween each subject and its verb.
This allows oursystem to transform main clauses, like ?the bodiesshowed signs of torture?, into NPs like ?the bod-ies, which showed signs of torture?, a common para-phrase strategy in our dataset.We also add correspondences between the twosentences to the graph, marking nodes which thesystem might decide to merge while fusing the twosentences.
We introduce correspondence arcs be-tween pairs of probable synonyms4.
We also anno-tate pronoun coreference by hand and create a cor-respondence between each pronoun and the heads ofall coreferent NPs.
The example sentence has only asingle correspondence arc (?they?
and ?bodies?)
be-2In a few cases, this creates two examples which share asentence, since the editor sometimes splits content off from onesentence and merges it into another.3We originally had 100 testing and 207 training examples,but found 26 of our examples were spurious, caused by faultysentence segmentation.4Words with the same part of speech whose similarity isgreater than 3.0 according to the information-theoretic Word-Net based similarity measure of Resnik (1995), using the im-plementation of (Pedersen et al, 2004).cause input sentence (1) is extremely short, but mostsentences have more.bodies showedsigns torturesaidleftweretheyside highway chilpancingopolicestatenorth hour resort acapulcorootrootrelsbjobjpp ofrelsbjpp bypp of pp inpp aboutpp of pp oftheanauxobjsbjrelmerge?Figure 1: The labeled dependency graph for sentences (1)and (2).
Dashed lines show a correspondence arc (?bod-ies?
and ?they?)
and potential relative clauses betweensubjects and VPs.3.1 Retained informationSentence fusion can be thought of as a two-partprocess: first, the editor decides which informationfrom the input sentences to retain, and then they gen-erate a sentence incorporating it.
In this paper, wefocus on the generation stage.
To avoid having toperform content selection5 , we provide our systemwith the true information selected by the editor.
Todo this, we align the input sentences with the outputby repeatedly finding the longest common substring(LCS) until a substring containing a matching con-tent word can no longer be found.
(The LCS is com-puted by a dynamic program similar to that for editdistance, but unlike edit distance, repeated LCS canhandle reordering.)
We provide our system with theboundaries of the retained regions as part of the in-put.
For the example above, these are the regionsof sentences (1) and (2) marked in boldface.
Al-though this helps the system select the correct infor-mation, generating a grammatical and easy-to-readfused sentence is still non-trivial (see examples insection 7).4 Fusion via optimizationLike Filippova and Strube (2008), we model ourfusion task as a constrained optimization problem,which we solve using Integer Linear Programming(ILP).
For each dependency from word w to head5As pointed out by Daume III and Marcu (2004) and Krah-mer et al (2008), content selection is not only difficult, but alsosomewhat ill-defined without discourse context information.56h in the input sentences, we have a binary variablexh,w, which is 1 if the dependency is retained in theoutput and 0 otherwise.
However, unlike Filippovaand Strube (2008), we do not know the points of cor-respondence between the inputs, only a set of possi-ble points.
Therefore, we also introduce 0-1 integervariables ms,t for each correspondence arc, whichindicate whether word s in one sentence should bemerged with word t in another.
If the words aremerged, they form a link between the two sentences,and only one of the pair appears in the output.Each dependency x, each word w, and eachmerger m have an associated weight value v, whichis assigned based on its features and the learned pa-rameters of our system (explained in Section 5).
Ourobjective function (4) sums these weight values forthe structures we retain:max?h,wvh,w ?
vw ?
xh,w +?s,tvs,t ?ms,t (4)We use structural constraints to require the outputto form a single connected tree.
(In the followingequations, W denotes the set of words, X denotesthe set of dependencies and M denotes the poten-tial correspondence pairs.)
Constraint (5) requires aword to have at most one parent and (6) allows it tobe merged with at most one other word.
(7) and (8)require each merged node to have a single parent:?w ?
W,?hxh,w ?
1 (5)?w ?
W,?tms,t ?
1 (6)?s, t ?
M, ms,t ?
?hxh,s +?hxh,t (7)?s, t ?
M, ms,t +?hxh,s +?hxh,t ?
2 (8)(9) forces the output to be connected by ensuringthat if a node has children, it either has a parent or ismerged.
?w ?
W,?cxc,w?|W |?hxh,w ?
|W |?umu,w ?
0(9)Certain choices of nodes to merge or dependen-cies to follow can create a cycle, so we also intro-duce a rank variable rw ?
R for each word and con-strain each word (except the root) to have a higherrank than its parent (10).
Merged nodes must haveequal ranks (11).
?w,h ?
X,|X|xh,w + rh ?
rw ?
|X| ?
1 (10)?s,t ?
M,|X|ms,t + rs ?
rt ?
|X| (11)We also apply syntactic constraints to make surewe supply all the required arguments for each wordwe select.
We hand-write rules to prevent the sys-tem from pruning determiners, auxiliary verbs, sub-jects, objects, verbal particles and the word ?not?unless their head word is also pruned or it can finda replacement argument of the same type.
We learnprobabilities for prepositional and subclause argu-ments using the estimation method described in Fil-ippova and Strube (2008), which counts how oftenthe argument appears with the head word in a largecorpus.
While they use these probabilities in the ob-jective function, we threshold them and supply con-straints to make sure all argument types with proba-bility > 10% appear if the head is chosen.Word merging makes it more difficult to writeconstraints for required arguments, because a words might be merged with some other word t which isattached to the correct argument type (for instance, ifs and t are both verbs and they are merged, only oneof them must be attached to a subject).
This condi-tion is modeled by the expression ms,t ?xt,a, where ais a argument word of the appropriate type.
This ex-pression is non-linear and cannot appear directly ina constraint, but we can introduce an auxiliary vari-able gs,t,A which summarizes it for a set of poten-tial arguments A, while retaining a polynomial-sizedprogram:?s,t ?
M,?a?Axa,s+?a?Axa,t + |W |ms,t ?
|W + 1|gs,t,A ?
0(12)(13) then requires a word s to be connected to anargument in set A, either via a link or directly:57?hxs,h ?
2?t:{s,t?M}gs,t,A ?
2?a?Axa,s ?
0 (13)The resulting resulting ILP is usually solvablewithin a second using CPLEX (Ilog, Inc., 2003).4.1 LinearizationThe output of the ILP is a dependency tree, not anordered sentence.
We determine the final orderingmostly according to the original word order of theinput.
In the case of a merged node, however, wemust also interleave modifiers of the merged heads,which are not ordered with respect to one another.We use a simple heuristic, trying to place dependen-cies with the same arc label next to one another; thiscan cause errors.
We must also introduce conjunc-tions between arguments of the same syntactic type;our system always inserts ?and?.
Finally, we choosea realization for the dummy relative pronoun THATusing a trigram language model (Stolcke, 2002).
Amore sophisticated approach (Filippova and Strube,2009) might lead to better results.5 LearningThe solution which the system finds depends on theweights v which we provide for each dependency,word and merger.
We set the weights based on a dotproduct of features ?
and parameters ?, which welearn from data using a supervised structured tech-nique (Collins, 2002).
To do so, we define a lossfunction L(s, s?)
?
R which measures how poorsolution s is when the true solution is s?.
For each ofour training examples, we compute the oracle so-lution, the best solution accessible to our system,by minimizing the loss.
Finally, we use the struc-tured averaged perceptron update rule to push oursystem?s parameters away from bad solutions andtowards the oracle solutions for each example.Our loss function is designed to measure the high-level similarity between two dependency trees con-taining some aligned regions.
(For our system, theseare the regions found by LCS alignment of the in-put strings with the output.)
For two sentences to besimilar, they should have similar links between theregions.
Specifically, we define the paths P (s,C) ina tree s with a set of regions C as the set of wordpairs w,w?
where w is in one region, w?
is in an-other, and the dependency path between w and w?lies entirely outside all the regions.
An example isgiven in figure 2.left on the side of a highway...werebodies showedof the men, which signs of torturestate police told Reuters rootFigure 2: Paths between retained regions in sentence (3).Boxes indicate the retained regions.Our loss (equation 14) is defined as the number ofpaths in s and s?
which do not match, plus a penaltyK1 for keeping extra words, minus a bonus K2 forretaining words inside aligned regions:L(s,s?
;C,K) =|(P (s,C) ?
P (s?, C)) \ (P (s,C) ?
P (s?, C))|+K1|w ?
s \ C| ?K2|w ?
s ?
C|(14)To compute the oracle s?, we must minimize thisloss function with respect to the human-authoredreference sentence r over the space S of fused de-pendency trees our system can produce.s?
= argmins?S L(s, r) (15)We perform the minimization by again using ILP,keeping the constraints from the original programbut setting the objective to minimize the loss.
Thiscannot be done directly, since the existence of a pathfrom s to t must be modeled as a product of x vari-ables for the dependencies forming the path.
How-ever, we can again introduce a polynomial numberof auxiliary variables to solve the problem.
We in-troduce a 0-1 variable qsh,w for each path start words and dependency h,w, indicating whether the de-pendency from h to w is retained and forms part ofa path from s. Likewise, we create variables qsw foreach word and qsu,v for mergers6.
Using these vari-ables, we can state the loss function linearly as (16),6The q variables are constrained to have the appropriate val-ues in the same way as (12) constrains g. We will print thespecific equations in a technical report when this work is pub-lished.58where P (r, C) is the set of paths extracted from thereference solution.min?s,tqsh,t ?
2?s,t?P (r,C)qsh,t (16)The oracle fused sentence for the example (1) and(2) is (17).
The reference has a path from bodiesto showed, so the oracle includes one as well.
Todo so, follows a relative clause arc, which was notin the original dependency tree but was created asan alternative by our syntactic analysis.
(At thisstage of processing, we show the dummy relativepronoun as THAT.)
It creates a path from left to bod-ies by choosing to merge the pronoun they with itsantecedent.
Other options, such as linking the twooriginal sentences with ?and?, are penalized becausethey would create erroneous paths?
since there isno direct path between root and showed, the oracleshould not make showed the head of its own clause.
(17) the bodies THAT showed signs of torture wereleft on the side of a highway in Chilpancingoabout an hour north of the tourist resort ofAcapulco state police saidThe features which represent each merger, wordand dependency are listed in Table 1.
We use the firstletters of POS tags (in the Penn Treebank encoding)to capture coarse groupings such as all nouns and allverbs.
For mergers, we use two measures of seman-tic similarity, one based on Roget?s Thesaurus (Jar-masz and Szpakowicz, 2003) and another based onWordNet (Resnik, 1995).
As previously stated, wehand-annotate the corpus with true pronoun corefer-ence relationships (about 30% of sentences containa coreferent pronoun).
Finally, we provide the LCSretained region boundaries as explained above.Once we have defined the feature representationand the loss function, and can calculate the oraclefor each datapoint, we can easily apply any struc-tured online learning algorithm to optimize the pa-rameters.
We adopt the averaged perceptron, appliedto structured learning by (Collins, 2002).
For eachexample, we extract a current solution st by solvingthe ILP (with weights v dependent on our parame-ters ?
), then perform an update to ?
which forcesthe system away from st and towards the oracle so-lution s?.
The update at each timestep t (18) de-pends on the loss, the global feature vectors ?, andCOMPONENT FEATURESMERGER SAME WORDSAME POS TAGSSAME FIRST LETTER OF THE POS TAGSPOS TAG IF WORD IS SAMECOREFERENT PRONOUNSAME DEPENDENCY ARC LABEL TO PARENTROGET?S SIMILARITYWORDNET SIMILARITYFIRST LETTER OF BOTH POS TAGSWORD POS TAG AND ITS FIRST LETTERWORD IS PART OF RETAINED CHUNK IN EDITOR?S FUSIONDEPENDENCY POS TAGS OF THE PARENT AND CHILDFIRST LETTER OF THE POS TAGSTYPE OF THE DEPENDENCYDEPENDENCY IS AN INSERTED RELATIVE CLAUSE ARCPARENT IS RETAINED IN EDITOR?S SENTENCECHILD IS RETAINED IN EDITOR?S SENTENCETable 1: List of Features.a learning rate parameter ?.
(Note that the updateleaves the parameters unchanged if the loss relativeto the oracle is 0, or if the two solutions cannot bedistinguished in terms of their feature vectors.
)?t+1 = ?t + ?
(L(st, r)?L(s?, r))(?(s?)??
(st))(18)We do 100 passes over the training data, with ?decaying exponentially toward 0.
At the end of eachpass over the data, we set ??
to the average of all the?t for that pass (Freund and Schapire, 1999).
Fi-nally, at the end of training, we select the committeeof 10 ??
which achieved lowest overall loss and av-erage them to derive our final weights (Elsas et al,2008).
Since the loss function is nonsmooth, lossdoes not decrease on every pass, but it declines over-all as the algorithm proceeds.6 EvaluationEvaluating sentence fusion is a notoriously difficulttask (Filippova and Strube, 2008; Daume III andMarcu, 2004) with no accepted quantitative metrics,so we have to depend on human judges for evalu-ation.
We compare sentences produced by our sys-tem to three alternatives: the editor?s fused sentence,a readability upper-bound and a baseline formed bysplicing the input sentences together by inserting theword ?and?
between each one.
The readability upper59bound is the output of parsing and linearization onthe editor?s original sentence (Filippova and Strube,2008); it is designed to measure the loss in gram-maticality due to our preprocessing.Native English speakers rated the fused sentenceswith respect to readability and content on a scale of1 to 5 (we give a scoring rubric based on (Nomoto,2009)).
12 judges participated in the study, for atotal of 1062 evaluations7 .
Each judge saw the eachpair of inputs with the retained regions boldfaced,plus a single fusion drawn randomly from among thefour systems.
Results are displayed in Table 2.System Readability ContentEditor 4.55 4.56Readability UB 3.97 4.27?And?-splice 3.65 3.80Our System 3.12 3.83Table 2: Results of human evaluation.7 DiscussionReadability scores indicate that the judges preferhuman-authored sentences, then the readability up-per bound, then ?and?-splicing and finally our sys-tem.
This ordering is unsuprising considering thatour system is abstractive and can make grammaticalerrors, while the remaining systems are all based ongrammatical human-authored text.
The gap of .58between human sentences and the readability upperbound represents loss due to poor linearization; thisaccounts for over half the gap between our systemand human performance.For content, the human-authored sentencesslightly outperform the readability upper bound?this indicates that poor linearization has some ef-fect on content as well as readability.
Our system isslightly better than ?and?-splicing.
The distributionof scores is shown in Table 3.
The system gets morescores of 5 (perfect), but it occasionally fails drasti-cally and receives a very low score; ?and?-splicingshows less variance.Both metrics show that, while our system does notachieve human performance, it does not lag behind7One judge completed only the first 50 evaluations; the restdid all 92.1 2 3 4 5 Total?And?-splice 3 43 60 57 103 266System 24 24 39 58 115 260Table 3: Number of times each Content score was as-signed by human judges.by that much.
It performs quite well on some rel-atively hard sentences and gets easy fusions rightmost of the time.
For instance, the output on ourexample sentence is (19), matching the oracle (17).
(19) The bodies who showed signs of torture wereleft on the side of a highway in Chilpancingoabout an hour north of the tourist resort ofAcapulco state police said.In some cases, the system output correspondsto the ?and?-splice baseline, but in many cases,the ?and?-splice baseline adds extraneous content.While the average length of a human-authored fu-sion is 34 words, the average splice is 49 words long.Plainly, editors often prefer to produce compact fu-sions rather than splices.
Our own system?s out-put has an average length of 33 words per sentence,showing that it has properly learned to trim away ex-traneous information from the input.
We instructedparticipants to penalize the content score when fusedsentences lost important information or added extradetails.Our integration of node alignment into our solu-tion procedure helps the system to find good corre-spondences between the inputs.
For inputs (20) and(21), the system was allowed to match ?company?to ?unit?, but could also match ?terrorism?
to ?ad-ministration?
or to ?lawsuit?.
Our system correctlymerges ?company?
and ?unit?, but not the other twopairs, to form our output (22); the editor makes thesame decision in their fused sentence (23).
(20) The suit claims the company helped flyterrorism suspects abroad to secret prisons.
(21) Holder?s review was disclosed the same dayas Justice Department lawyers repeated aBush administration state-secret claim in alawsuit against a Boeing Co unit.60(22) Review was disclosed the same day as JusticeDepartment lawyers repeated a Bushadministration claim in a lawsuit against aBoeing Co unit that helped fly terrorismsuspects abroad to secret prisons.
(23) The review was disclosed the same day thatJustice Department lawyers repeated Bushadministration claims of state secrets in alawsuit against a Boeing Co <BA.N> unitclaiming it helped fly terrorism suspectsabroad to secret prisons.In many cases, even when the result is awkwardor ungrammatical, the ILP system makes reason-able choices of mergers and dependencies to retain.For inputs (24) and (25), the system (26) decides?Secretary-General?
belongs as a modifier on ?deMello?, which is in fact the choice made by the ed-itor (27).
In order to add the relative clause, theeditor paraphrased ?de Mello?s death?
as ?de Mellowas killed?.
Our system, without this paraphrase op-tion, is forced to produce the improper phrase ?deMello?s death who?
; a wider array of paraphrase op-tions might lead to better results.This example also demonstrates that the systemdoes not simply keep the LCS-aligned retained re-gions and throw away everything else, since the re-sult would be ungrammatical.
Here it links the se-lected content by also choosing to keep ?could havebeen?, ?an account?
and ?death?.
(24) Barker mixes an account of Vieira deMello?s death with scenes from his career,which included working in countries suchas Mozambique, Cyprus, Cambodia,Bangladesh, and the former Yugoslavia.
(25) Had he lived, he could have been a futureU.N.
Secretary-General.
(26) Barker mixes an account of Vieira de Mello?sdeath who could been a future U.N.secretary-general with scenes from careerwhich included working in countries as suchMozambique Cyprus Cambodia andBangladesh(27) Barker recounted the day Vieira de Mello, aBrazilian who was widely tipped as a futureU.N.
Secretary-General, was killed and mixesin the story of the 55-year-old?s career, whichincluded working in countries such asMozambique, Cyprus, Cambodia, Bangladesh,and Yugoslavia.Many of our errors are due to our simplistic lin-earization.
For instance, we produce a sentence be-ginning ?Biden a veteran Democratic senator fromDelaware that Vice president-elect and Joe...?, wherea correct linearization of the output tree would havebegun ?Vice President-elect Joe Biden, a veteranDemocratic senator from Delaware that...?.
Someerrors also occur during the ILP tree extraction pro-cess.
In (28), the system fails to mark the argumentsof ?took?
and ?position?
as required, leading to theiromission, which makes the output ungrammatical.
(28) The White House that took when Israelinvaded Lebanon in 2006 showed no signs ofpreparing to call for restraint by Israel and thestance echoed of the position.8 ConclusionWe present a supervised method for learning to fusedisparate sentences.
To the best of our knowl-edge, it is the first attempt at supervised learningfor this task.
We apply our method to naturally oc-curring sentences from editing data.
Despite usingtext generation, our system is comparable to a non-abstractive baseline.Our technique is general enough to apply to con-ventional fusion of similar sentences as well?
all thatis needed is a suitable training dataset.
We hopeto make use of the new corpus of McKeown et al(2010) for this purpose.
We are also interested inevaluating our approach on the fused sentences inabstractive single-document summaries.The performance of our readability upper boundsuggests we could improve our results using bet-ter tree linearization techniques and parsing.
Al-though we show results for our system using hand-annotated pronoun coreference, it should be possibleto use automatic coreference resolution instead.Paraphrase rules would help our system repli-cate some output structures it is currently unableto match (for instance, it cannot convert betweenthe copular ?X is Y?
and appositive ?X, a Y?
con-structions).
Currently, the system has just one such61rule, which converts main clauses to relatives.
Oth-ers could potentially be learned from a corpus, as in(Cohn and Lapata, 2009).Finally, in this study, we deliberately avoid in-vestigating the way editors choose which sentencesto fuse and what content from each of them to re-tain.
This is a challenging discourse problem thatdeserves further study.AcknowledgementsWe are very grateful to Alan Elsner, Howard Gollerand Thomas Kim at Thomson-Reuters for giving usaccess to this dataset.
We thank Eugene Charniak forhis supervision, our colleagues in BLLIP for theircomments, Kapil Thadani and Kathy McKeown fordiscussing the project with us, and our human eval-uators for completing a task which turned out to beextremely tedious.
Part of this work was funded by aGoogle Fellowship in Natural Language Processing.ReferencesRegina Barzilay and Kathleen McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: An integer linearprogramming approach.
J. Artif.
Intell.
Res.
(JAIR),31:399?429.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
J. Artif.
Intell.
Res.
(JAIR), 34:637?674.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics, July.Hal Daume III and Daniel Marcu.
2004.
Genericsentence fusion is an ill-defined summarization task.In Stan Szpakowicz Marie-Francine Moens, editor,Text Summarization Branches Out: Proceedings of theACL-04 Workshop, pages 96?103, Barcelona, Spain,July.
Association for Computational Linguistics.Jonathan L. Elsas, Vitor R. Carvalho, and Jaime G. Car-bonell.
2008.
Fast learning of document ranking func-tions with the committee perceptron.
In WSDM, pages55?64.Katja Filippova and Michael Strube.
2008.
Sentence fu-sion via dependency graph compression.
In Proceed-ings of the 2008 Conference on Empirical Methods inNatural Language Processing, pages 177?185, Hon-olulu, Hawaii, October.
Association for ComputationalLinguistics.Katja Filippova and Michael Strube.
2009.
Tree lin-earization in English: Improving language modelbased approaches.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, Companion Volume: ShortPapers, pages 225?228, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine Learning, 37(3):277?296.Ilog, Inc. 2003.
Cplex solver.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?s the-saurus and semantic similarity.
In Conference on Re-cent Advances in Natural Language Processing, pages212?219.Hongyan Jing and Kathleen McKeown.
1999.
The de-composition of human-written summary sentences.
InSIGIR, pages 129?136.Kevin Knight and Daniel Marcu.
2000.
Statistics-basedsummarization - step one: sentence compression.
InProceedings of the 17th National Conference on Arti-ficial Intelligence, pages 703?71.Emiel Krahmer, Erwin Marsi, and Paul van Pelt.
2008.Query-based sentence fusion is better defined andleads to more preferred results than generic sentencefusion.
In Proceedings of ACL-08: HLT, Short Pa-pers, pages 193?196, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Erwin Marsi and Emiel Krahmer.
2005.
Explorationsin sentence fusion.
In Proceedings of the 10th Eu-ropean Workshop on Natural Language Generation,pages 109?117.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 152?159.Kathleen McKeown, Sara Rosenthal, Kapil Thadani, andColeman Moore.
2010.
Time-efficient creation ofan accurate sentence fusion corpus.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 317?320, Los An-geles, California, June.
Association for ComputationalLinguistics.Tadashi Nomoto.
2009.
A comparison of model freeversus model intensive approaches to sentence com-pression.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 391?399, Singapore, August.
Association forComputational Linguistics.62Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::Similarity - measuring the re-latedness of concepts.
In Daniel Marcu Susan Du-mais and Salim Roukos, editors, HLT-NAACL 2004:Demonstration Papers, pages 38?41, Boston, Mas-sachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.Philip Resnik.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
In IJCAI?95:Proceedings of the 14th international joint conferenceon Artificial intelligence, pages 448?453, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proceedings of the Fifth Conference onApplied Natural Language Processing, pages 16?19,Washington D.C.Andreas Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, pages257?286, November.Jenine Turner and Eugene Charniak.
2005.
Supervisedand unsupervised learning for sentence compression.In Proc.
Assoc.
for Computational Linguistics (ACL),pages 290?297.63
