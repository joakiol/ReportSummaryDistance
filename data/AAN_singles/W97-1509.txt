Head-Driven Generation and Indexing in ALEGerald PennSFB 340K1.
Wilhelmstr.
11372074 Tfibingen, Germanygpenn@sfs.nphil.uni-tuebingen.deOctav PopescuComputational Linguistics ProgramCarnegie Mellon UniversityPittsburgh, PA 15213, USAoctav~cs.cmu.eduAbst rac tWe present a method for compiling ram-mars into efficient code for head-drivengeneration in ALE.
Like other compila-tion techniques already used in ALE, thismethod integrates ALE's compiled codefor logical operations with control-specificinformation from (SNMP90)'s algorithmalong with user-defined directives to iden-tify semantics-related substructures.
Thiscombination provides far better perfor-mance than typical bi-directional feature-based parser/generators, while requiring aminimum of adjustment to the grammarsignature itself, and a minimum of extracompilation.1 Mot ivat ionLarge-scale development systems for typed feature~based grammars have typically oriented themselvestowards parsing, either ignoring generation entirely(the usual case), or assuming that generation canbe achieved for free by using a bi-directional con-trol strategy with a semantically, rather than phono-logically, instantiated query.
In the latter case, theresult has inevitably been a system which is unac-ceptably slow in both directions.
At the same time,several lower-level logical operations over typed fea-ture structures, such as inferring a type from theexistence of an appropriate f ature, or the unifica-tion of two feature structures, are indeed commonto both parsing and generation; and generators out-side this logical domain, of course, can make no useof them.
What is required is a system which pro-vides a common pool of these operations optimizedfor this particular logic, while also providing modesof processing which are suited to the task at hand,namely parsing or generation.This is exactly how the situation has developedin other areas of logic programming.
The WarrenAbstract Machine and its various enhancements arenow the de facto standard for Prolog compilation, forexample; and with that standard come techniquesfor call stack management, heap data structures etc.
;but this does not mean that all Prolog programs arecreated equal - -  the more sophisticated compilersuse mode declarations in order to optimize particu-lar programs to being called with certain argumentinstantiations.The Attribute Logic Engine (ALE,(CP94)) is alogic programming language based on typed fea-ture structures, which can compile common logi-cal operations like type inferencing and unificationinto efficient lower-level code.
ALE also compilesgrammars themselves into lower-level instructions,rather than simply running an interpreter over them,which yields a substantial increase in efficiency.
ForALE, the question of efficient generation is thenhow to compile grammars for use in semantically-instantiated queries.
To date, however, ALE hasfallen within the class of systems which have ig-nored generation entirely.
Its only control strategieshave been a built-in bottom-up chart parser, andthe usual SLD-resolution strategy for its Prolog-likelanguage.On the other hand, only a few of the operations itcompiles are specific to the parsing direction.
ALE'slower level of instructions are expressed using Pro-log itself as the intermediate code.
ALE compilesthe various elements of a typed feature-based gram-mar (type signature, feature declarations, lexicalrules, phrase structure-like grammar rules) into Pro-log clauses which are then compiled further by a Pro-log compiler for use at run-time.
In fact, ALE alsohas a Prolog-like logic programming language of itsown, based on typed feature structures.
Goals fromthis language can be used as procedural ttachmentson lexical rules or grammar rules as well.62mmmmmmmmm\[\]mm\[\]mnmmmmmmmmmmThis paper describes a head-driven generatorwhich has recently been added to ALE ((Pop96)),which provides a smooth integration of generation-specific control information with the powerful logicalcompilation that ALE already performs.
We alsobriefly consider the use of a lexical indexing strat-egy for generation, which is compiled into efficientlower-level instructions as well.2 Head-Dr iven  Generat ionOur head-driven generator uses essentially the samecontrol strategy as proposed in (SNMP90), whichwas first used in the BUG system of (Noo89).
Thisalgorithm is quite well suited to large-scale HPSGgeneration, as it avoids the termination problemsinherent to top-down processing of strongly lexic-ocentric theories, and, at the same time, does notrequire of its grammar ules the same naive form ofcompositionality, known as semantic monotonicity,as Earley-based strategies do.
A semantically mono-tonic grammar ule is one in which the semanticcontribution of every daughter category subsumes aportion of the contribution of the mother category.In general, wide-coverage theories cannot guaranteethis.Control in this algorithm is guided by meaningrather than a particular direction over a string, andthus requires the user to distinguish two classes ofrules: those in which a mother has the same seman-tics as some daughter (a chain rule), and those inwhich it does not (non-chain rule).
The strategy is acombination of bottom-up and top-down steps basedon the location of a pivot, the lowest node in a deriva-tion tree which has the same semantics as the rootgoal.
Once a pivot is located, one can recursivelyprocess top-down from there with non-chain rules(since the pivot must be the lowest such node), andattach the pivot to the root bottom-up with chainrules.
A pivot can either be a lexical entry or emptycategory (the base cases), or the mother categoryof a non-chain rule.
The base case for bottom-upprocessing is when the pivot and root are taken tobe the same node, and thus unified.
The reader isreferred to (SNMP90) for the complete algorithm.What we will be concerned with here is the adap-tation of this algorithm to grammars based on alogic of typed feature structures, such as HPSG.
(SNMP90) uses definite clause grammars, while(Noo89) uses a Prolog-based extension of PATR-II, which has features and atoms, but no feature-bearing types, and thus no appropriateness.
Unlikeboth of these approaches, our goal is also to com-pile the grammar itself into lower-level code whichis specifically suited to the particular requirements ofhead-driven generation, very much as ALE alreadydoes for its parser, and much as one would compilea Prolog program for a particular set of mode spec-ifications.3 Input  Spec i f i ca t ionThe reader is referred to (CP94) for a complete spec-ification of ALE's syntax as it pertains to parsing.ALE allows the user to refer to feature structuresby means of descriptions, taken from a languagewhich allows reference to types (Prolog atoms), fea-ture values (colon-separated paths), conjunction anddisjunction (as in Prolog), and structure sharingthrough the use of variables (with Prolog variables).ALE grammar rules simply consist of a series of thesedescriptions, one for each daughter and one for themother, interspersed with procedural attachmentsfrom ALE's Prolog-like language.
The following is atypical S ~ NP  VP rule taken from a simple ALEgrammar:srule rule(s ,phon:SPhon,form:Form,sem:S) ===>cat> (phon : SubjPhon), Subj,seN_head> (vp, phon : VpPhon, form: Form,subcat : \[Subj \ ] ,  sem: S) ,goal> append (Subj Phon, VpPhon, SPhon).The description of a sentence-typed feature struc-ture before the ===> is the description of the mothercategory.
The operator, cat>, identifies a daughterdescription, here used for the subject NP, and goal>identifies a call to a procedural attachment, whosearguments are Prolog variables instantiated to theirrespective phonologies (the values of feature, phon).seN..head> is a new operator which identifies thedaughter description corresponding to the semantichead of a rule, according to (SNMP90)'s definition.Grammar ules can have at most one seN_head> dec-laration; and those which have one are identified aschain rules.The only other special information the user needsto provide is what constitutes the semantic ompo-nent of a feature structure.
ALE uses a distinguishedpredicate, seN_se lect  (+, - ) ,  from its procedural at-tachment language in order to identify this material,e.g.
:sem_select(seN:S,S) if true.In general, this material may be distributed over var-ious substructures of a given feature structure, inwhich case the predicate may be more complex:sem_seleet ( (s ign, synsem: coat : Coat,retrieved_quants : QR),(seN, c:Cont,q:QR)) ifno_free_vats (QR).63Notice that such grammars can still be compiledby ALE's parsing compiler: the sere_select/2 pred-icate can simply be ignored, and a sem~ead> oper-ator can be interpreted exactly as cat>.
In the gen-eral case, however, a particular grammar ule willnot compile into efficient, or even terminating, codein both modes, particularly when procedural attach-ments are used.
Just as in the case of Prolog, theuser is responsible for ordering the procedural at-tachments (subgoals) with respect o their daughtercategories and with respect o each other to ensureproper termination for a particular mode of process-ing.
Just as in Prolog, one could also modify ALEto assist, to an extent, by augmenting ALE's pro-cedural attachments with mode declarations whichcan be enforced by static analysis during compila-tion.
At this point, one could also adapt techniquesfor automatic mode reversal from logic programming((Str90; MGH93)) to grammar ules to obtain theminimum amount of manual modification ecessary.4 Compi la t ionAll ALE compilation up to, and including, the levelof descriptions applies to generation without change.This includes compiled type inferencing, featurevalue access functions, and the feature structure uni-fication code itself.
I This level is a very importantand convenient stage in compilation, because de-scriptions serve as the basic building blocks of allhigher-level components in ALE.
One of these com-ponents, ALE's procedural attachment language,can also be compiled as in the parsing case, sinceit uses the same SLD resolution strategy.
The restare described in the remainder of this section.4.1 Grammar  RulesChain rules and non-chain rules are compiled iffer-ently because (SNMP90)'s Mgorithm uses a differentcontrol strategy with each of them.
Both of them aredifferent from the strategy which ALE's bottom-upparser uses.
All three, however, vary only slightlyin their use of building blocks of code for enforcingdescriptions on feature structures.
These buildingblocks of code will be indicated by square brackets,e.g.
\[add Desc to FS\].4.1.1 Non-ch ina  Rules :Non-chain rules have no semantic head, and aresimply processed top-down, using the mother as apivot.
We also process the daughters from left toright.
So the non-chain rule:*(CP96) provides complete details about this level ofcompilation.DO ===> DI, ..., DN.consisting of descriptions DO through DN, is compiledto:non_cha in_ru le  (+PivotFS,  +RootFS, ?Ws,?WsRest) "-\[add DO to PivotFS\],exists_chain (PivotFS, RootFS),\[add D1 to FS1\],generat ?
(FS i, SubWs, SubWs 2),\[add D2 to FS2\],generate (FS2, SubWs2, SubWs3),\[add DN to FSN\],generate (FSN, SubWsN, SubWsRest),connect (PivotFS, RootFS, SubWs, SubWsRest,Ws, WsRest).non_chain_rule/4 is called whenever a non-chainrule's mother is selected as the pivot (by successfullyadding the mother's description, DO, to PivotFS),generating a string represented by the differencelist, Ws-WsRest.
The algorithm says one must re-cursively generate ach daughter (generate/3),  andthen connect his pivot-rooted erivation tree to theroot (connect/6).
Before we spend the effort onrecursive calls, we also want to know whether thispivot can in fact be connected to the root; this isaccomplished by ex ists_chain/2.
In general, themother category and daughter categories may sharesubstructures, through the co-instantiation of Pro-log variables in their descriptions.
After matchingthe mother's description, which will bind those vari-ables, we add each daughters' description to a newstructure gsi, initially a structure of type bot (themost general type in ALE), before making the re-spective recursive call.
In this way, the appropri-ate information shared between descriptions in theuser's grammar rule is passed between feature struc-tures at run-time.To generate, we use the user's distinguished selec-tion predicate to build a candidate pivot, and thentry to match it to the mother of a non-chain rule(the base cases will be considered below):generate (+GoalFS, ?Ws, ?WsRest) : -solve (sem_select (GoalFS, Sem) ),solve (sem_select (PivotFS, Sem) ),non_chain_rule (PivotFS, GoalFS, Ws, WsRest ).solve/1 is ALE's instruction for making calls toits procedural attachment language.
Its clauses arecompiled from the user's predicates, which have de-scription arguments, into predicates with featurestructure arguments as represented internally inALE.644.1.2 Cha in  Rules:Chain rules are used to connect pivots to goals.As a result, we use them bottom-up from semantichead to mother, and then recursively generate thenon-head daughters top-down, left to right.
So achain rule:DO ===> D1, .
.
.
,  DK, HI), D(K+I) .
.
.
.
.
DN.is compiled to:cha in_ru le  (+PivotFS, +RootFS, +SubWs,-SubWsRest, ?Ws, ?WsRest) ?
-\[add HI) to PivotFS\] ,\[add DO to MotherFS\]exist s_chain (MotherFS, RootFS),\[add D1 to FSI\],generate (FS1, SubWs, SubWs 2),\[add DK to FSK\],generate (FSK, SubWsK, SubWsK+1 ),\[add D(K+I) to FS(K+I)\],generate (FS (K+I), SubWsK+ i, SubWsK+2),.
.
.\[add DN to FSN\],generate (FSN, SubWsN, SubWsRes t ),connect (MotherFS, RootFS, SubWs, SubWsRest,Ws, WsRest).chain_ru le/6 is called whenever a chain rule is se-lected to connect a pivot (PivotFS) to a root goal(RootFS), yielding the string Ws-WsRest, which con-tains the substring, SubWs-SubWsRest.
In the caseof both chain and non-chain rules, calls to a procedu-ral attachment between daughter Di and D ( i+ l )  aresimply added between the code for Di and D( i+l) .Procedures which attach to the semantic head, in thecase of chain rules, must be distinguished as such,so that they can be called earlier.To connect a pivot to the root, we either unifythem (the base case):connect (PivotFS, RootFS, Ws, WsRest, Ws,WsRest) :-unify (Pivot FS, RootFS).or use a chain rule:connect (+PivotFS, +RootFS, +SubNs, -SubWsRest,?Ns, ?WsRest) :-chain_rul e (P ivotFS, RootFS, SubWs,SubWsRest ,Ws ,WsRest).Similarly, to discover whether a chain exists, we ei-ther unify, or attempt to use one or more chain rules.For each chain rule, we can, thus, compile a separateclause for exists_chain/2, for which that rule is thelast step in the chain.
In practice, a set of chain rulesmay have potentially unbounded length chains.
Forthis reason, we bound the length with a constant de-clared by the user directive, max_chain_length/1.4.2 Lexical  Ent r iesLexical entries are the base cases of the algorithm'stop-down processing, and can be chosen as pivotsinstead of the mothers of non-chain rules.
In fact,lexical entries can be compiled exactly as a non-chainrule with no daughters would be.
So a lexical entryfor W, with description, D, can be compiled into thenon_chain_rule/4 clause:non_chain_rule (PivotFS,  RootFS, Ws, WsRest) : -\[add D to P ivotFS\] ,connect  (PivotFS, RootFS, \[W ISubWs\], SubWs,Ws, WsRest).For ALE's bottom-up arser, lexical entries werecompiled into actual feature structures.
Now theyare being compiled into code which executes on analready existing feature structure, namely the mostgeneral satisfier of what is already known about thecurrent pivot.
Empty categories are compiled in thesame way, only with no phonological contribution.This method of compilation is re-evaluated in Sec-tion 6.4.3 Lexical  RulesALE's lexical rules consist simply of an input andoutput description, combined with a morphologi-cal translation and possibly some procedural attach-ments.
In this present hird singular lexical rule:pres_sg3 lex_rule (vp,form:nonfinite,subcat : Subcat,sem: Sem)**> (vp, form: f in i t  e,subcat : NewSubcat,sem: Sem)i f  add_sg3(Subcat,NewSubcat)morphs (X ,y )becomes  (X , i ,e , s ) ,X becomes (X ,s ) .a non-finite VP is mapped to a finite VP, providedthe attachment, add.Jg3/2 succeeds in transformingthe SUBCAT value to reflect agreement.For parsing, ALE unfolds the lexicon at compile-time under application of lexical rules, with an up-per bound on the depth of rule application.
Thiswas possible because lexical items were feature struc-tures to which the code for lexical rules could ap-ply.
In the generator, however, the lexical entriesthemselves are compiled into pieces of code.
Onesolution is to treat lexical rules as special unarynon-chain rules, whose daughters can only have piv-ots corresponding to lexical entries or other lexi-cal rules, and with bounded depth.
Because the65application depth is bounded, one can also unfoldthese lexical rule applications into the lexical entries'non_chain..rule/4 predicates themselves.
Givena lexical entry, W - - ->  DescLex, and lexical rule,DescIn **> DescOut morphs M, for example, wecan create the clause:non_ chain_rule (Pivot FS, RootFS, Ws, WsRest ) : -\[add DescOut to PivotFS\],\[add DescIn to LexFS\],\[add DescLex to LexFS\],connect (PivotFS, RootFS, \[Morp:hW I SubWs\],SubWs ,Ws ,WsRest).where MorphW is the result of applying N to W. Formost grammars, this code can be heavily optimizedby peephole filtering.
At least part of all three de-scriptions needs to be enforced if there are sharedstructures in the input and output of the lexical rule,in order to link this to information in the lexical en-try.5 ExampleAn example derivation is given in Figure 1 whichuses these grammar ules:sent rule(sentence,sem:(pred:decl,args:\[S\])) ===>cat> (s,form:finite,sem:S).s rule(s,form:Form,sem:S) ===>cat> Subj,sem_head> (vp,form:Form,subcat:\[Subj\],sem:S).vp rule(vp, form: Form, subcat : Subcat, sem: S) ===>sem_head> (vp, form: Form,subcat : \[Compl \[ Subcat\], sere: S),cat> Compl.The rules, s and vp, are chain rules, as evidencedby their possession of a semantic head.
sent is anon-chain rule.
Processing proceeds in alphabeticalorder of the labels.
Arrows show the direction ofcontrol-flow between the mother and daughters of arule.
Given the input feature structure shown in (a),we obtain its semantics with sere_select and unifyit with that of sent 's  mother category to obtain thefirst pivot, sent 's  daughter, (b), must then be re-cursively generated.
Its semantics matches that ofthe lexieal entry for "calls," (c), which must thenbe linked to (b) by chain rules.
The semantic headof chain rule vp matches (c), to produce a mother,(d), which must be further linked, and a non-headdaughter, (e), which is recursively generated by us-ing the lexical entry for "john."
A second applica-tion of vp matches (d), again producing a mother,(f), and a non-head daughter, (g), which is recur-sively generated by using the lexical entry for "up.
"An application of chain rule, s, then produces a non-head daughter, (h), and a mother.
This mother islinked to (b) directly by unification.6 IndexingIn grammars with very large lexica, generation canbe considerably expensive.
In the case of ALE'sbottom-up arser, our interaction with the lexiconwas confined simply to looking up feature structuresby their phonological strings; and no matter howlarge the lexicon was, Prolog first argument index-ing provided an adequate means of indexing by thosestrings.
In the case of generation, we need to lookup strings indexed by feature structures, which in-volves a much more expensive unification operationthan matching strings.
Given ALE's internal rep-resentation of feature structures, first argument in-dexing can only help us by selecting structures ofthe right type, which, in the case of a theory likeHPSG, is no help at all, because very lexical entryis of type, word.
(SNMP90) does not consider thisproblem, presumably because its data structures aremuch smaller.The same problem exists in feature-based chartparsing, too, since we need to find matching featurestructure chart edges given a description in a gram-mar rule.
In the case of HPSG, this is not quiteas critical given the small number of rules the the-ory requires.
In a grammar with a large number ofrules, however, a better indexing technique must beapplied to chart edges as well.The solution we adopt is to build a decision treewith features and types on the inner nodes and arcs,and code for lexical entries on the leaves.
This struc-ture can be built off-line for the entire lexicon andthen traversed on-line, using a feature structure inorder to avoid redundant, partially successful uni-fication operations.
Specifically, a node of the treeis labelled with a feature path in the feature struc-ture; and the arcs emanating from a node, with thepossible type values at that node's feature path.The chief concern in building this tree is decidingwhich feature paths should be checked, and in whichorder.
Our method, an admittedly preliminary one,simply indexes by all feature paths which reach intothe substructure(s) identified as semantics-relatedby sere_select/2, such that shorter paths are tra-versed earlier, and equally short paths are traversedalphabetically.
An example tree is shown in Figure 266(a) ntenceM: FRED:declIARGS:(FRED:calI-upL ALRGS :(PRED :mRry'PRED :jo(c)sent(non-chain)(d){'~F q I FORM:finite /I SEM: FRED:call-up "7.1 L IARGS :(PRED :mary'PRED :j?hn-'~JI n?x:sg3 I {FORM:finite /I SEM:\[PRED:mary, AROS"0\]{ I SUBCAT: ( \ [np ,AGR:sg3 ,SEM: \ [1 \ ] \ ] )  /~-  = I s~: F~q~,.,-u.
ql L ALRGs :(\[llPRED :mary'PggO :j?hn-\]Jmarymaryvp P 3 FORM:finite p, ARGS:SU BCAT:(\[p,SEM :PRED :up\]\[np,AG R:sg3,SEM:\[1\]\])SEM : FRED :call-up q up- -  \[ ARGS :(\[1\]PRED :mary,PRED :john n_\]B m vpFORM:finiteSUBCAT:(\[Hp,SEM:\[2\]\]\[p,SEM:PRED:up\]\[np,AGR:sg3,SEM :\[1\]\])SEM: FRED:call-up q-- I ARGS :( \[1\]PRED:mary'\[2\]PRED :j?hnn\](e)PGR:Sg 3 qM:\[PRED:john,ARGS:0\] \]johnjohncallscallsFigure 1: A Sample Generation Tree.67e INDEX RF_~TR INDEX:GENnom._OblN ~~xDEX: ~INDEX:PER (ithey ---> \[code for they\]plurwe ---> \[code for we\]Figure 2: A sample lexical decision tree.for the two HPSG-likele~calentries:they ---> word?
.
?CONT: nom_objINDEX: indexGEN: gendNUM: plurPER: 3rdRESTR: elistwe ---> word?
?
.CONT: nom_objINDEX: indexGEN: gendNUM: plurPER: IstRESTR: elist.
, .A~er the tree is built, a number is assigned to eachnode and the tree is compiled into a series of Prologpredicates to be used for traversal at run-time, whichare then compiled by Prolog.
The INDEX:PER nodein Figure 2 has the following compiled code:node(6,SemFS,PivotFS,RootFS,Ns,WsRest) "-IV := PivotFS~s value at INDEX:PER\],branch(6,V,SemFS,PivotFS,RootFS,Ws,WsRest).branch(6,V,SemFS,PivotFS,RootFS,Ws,NsRest) :-\[add type 3rd to V\],node(7,SemFS,PivotFS,RootFS,Ns,WsRest).branch(6,V,SemFS,PivotFS,RootFS,Ns,WsRest) :-\[add type ist to V\],node(8,Se~S,PivotFS,RootFS,Ws,WsRest).node (7, _, PivotFS ,RootFS ,Ws ,WsRest) ?
-\[add code for  he to PivotFS\],connect  (PivotFS ,RootFS, \[he \[ SubWs\],SubWs ,Ws ,WsRest).node (8, _, PivotFS, RootFS ,Ws, WsRest ) :-\[add code for i to PivotFS1,connect (PivotFS ,RootFS, \[i \[ SubWs\],SubNs, Ws, WsRest ).Each clause of a non-terminM node/2 finds the valueof the current pivot at the current node's featurepath, and then calls branch/3, which branches to anew node based on the type of that value.
Leaf nodeclauses add the code for one of possibly many lex-ical entries.
The non_chain.xule/4 clauses of Sec-tion 4.2 are then replaced by:non_chain_rule(PivotFS,RootFS,Ns ,NsRest) :-solve (sem_select (PivotFS, SemFS) )node (0, SemFS, P ivotFS, RootFS, Ns, NsRe st).As the type check on branches is made by unifi-cation, traversal of a tree can, in general, be non-deterministic.
Using ALE's internal data structurefor feature structures, a check to avoid infinite loopsthrough cyclic structures during compile-time can bemade in linear time.7 Resu l ts  and Future  WorkCompilation of control code for head-driven gener-ation, as outlined in Section 4, improves generationperformance by a factor of about 5 on three feature-based grammars we have written and tested.
Theuse of our indexing code independently improvesgeneration speed by a factor of roughly 3.
Thecombined compile-time cost for producing and com-piling the control and indexing code is a factor ofabout 1.5.
Taken as a function of maximum chainlength (also declared by the user), generation is, ofcourse, always slower with larger maxima; but per-formance degrades omewhat more rapidly with in-dexed generation than with non-indexed, and morerapidly still with compiled generation than with in-terpreted.
In our experience, the factor of improve-ment decreases no worse than logarithmically withrespect o maximum chain length in either case.There are several directions in which our approachcould be improved.
The most important is theuse of a better decision-tree growing method suchas impurity-based classification ((Qui83; Utg88;68Cho91)) or concept clustering over lexical entries((CR92)).
Our current approach only guaranteesthat semantics-related paths are favoured over unre-lated ones, and reduces redundant unifications whencompared with naive lookup in a table of featurestructures.
What is needed is a arrangement ofnodes which minimizes the average length of traver-sal to a failed match, in order to prune search as soonas possible.
For generation with fixed large-scalegrammars, this could also involve a training phaseover a corpus to refine the cost estimate based on alexical entry's frequency.
This direction is pursuedfurther in (Pen97).One could also explore the use of memoization forgeneration, to avoid regeneration f substrings, uchas the "chart-based" generator of (Shi88), which wasoriginally designed for a bottom-up generator.
Thebest kind of memoization for a semantically drivengenerator would be one in which a substring couldbe reused at any position of the final string, possiblyby indexing semantics values which could be checkedfor subsumption against later goals.Another direction is the incorporation of thisstrategy into a typed feature-based abstract ma-chine, such as the ones proposed in (Qu94; Win96).Abstract machines allow direct access to pointersand stack and heap structures, which can be usedto make the processing outlined here even more effi-cient, at both compile-time and run-time.
They canalso be used to perform smarter incremental compi-lation, which is very important for large-scale gram-mar development.
This direction is also consideredin (Pen97).8 Conc lus ionWe have presented the steps in compiling head-driven generation code for ALE grammar signatures,which can make use of ALE's efficient compilationof descriptions.
We have also outlined a method forcompiling feature-based decision trees which can beused to alleviate the lexicon indexing problem forgeneration, as well as the chart edge indexing prob-lem for large-scale feature-based parsers.All of these techniques have been implementedand will be available beginning with version 3.0of ALE, which will be released in Spring, 1997.By compiling both logical operations and, in aprocessing-specific ashion, higher-level control op-erations, ALE can be used for very efficient, large-scale feature-based grammar design.ReferencesCarpenter, B., and G. Penn, 1994.
The AttributeLogic Engine, User's Guide, Version 2.0.1, CMUTechnical Report.Carpenter, B., and G. Penn, 1996.
Compiling TypedAttribute-Value Logic Grammars, in H. Bunt, M.Tomita (eds.
), Recent Advances in Parsing Tech-nology, Kluwer.Carpineto, C. and G. Romano, 1992.
GALOIS: Anorder-theoretic approach to conceptual c ustering.Proceedings of AAALChou, P.A., 1991.
Optimal Partitioning for Clas-sification and Regression Trees.
IEEE Transac-tions on Pattern Analysis and Machine Intelli-gence, 13(4).Minnen, G., D. Gerdemann, and E.W.
Hinrichs,1993.
Direct Automated Inversion of Logic Gram-mars.
Proceedings of the 4th Workshop on NaturalLanguage Understanding and Logic Programming.van Noord, G., 1989.
BUG: A Directed BottomUp Generator for Unification Based Formalisms.Utrecht/Leuven working papers in Natural Lan-guage Processing 1989.Penn, G., forthcoming.
Statistical Optimization i aFeature Structure Abstract Machine.
CMU Doc-toral Thesis.Popescu, O., 1996.
Head-Driven Generation forTyped Feature Structures.
CMU Master's Thesis.Qu, Y., 1994.
An Abstract Machine for TypedAttribute-Value Logic.
CMU Master's Thesis.Quinlan, J., 1983.
Learning Efficient Classifica-tion Procedures.
In Michalski, Carbonell, Mitchell(eds.
), Machine Learning: an artificial intelli-gence approach, Morgan Kaufmann.Shieber, S.M., 1988.
A Uniform Architecture forParsing and Generation.
Proceedings of the 12thInternational Conference on Computational Lin-guistics, pp.
614-619.Shieber, S.M., G. van Noord, R.C.
Moore andF.C.N.
Pereira, 1990.
Semantic-head-driven G -eration.
Computational Linguistics, 16.Strzalkowski, T., 1990.
Reversible Logic Gram-mars for Natural Language Parsing and Genera-tion.
Canadian Computational Intelligence Jour-nal, 6(3), pp.
145-171.Utgoff, 1988.
ID5: an incremental ID3.
InternationalMachine Learning Conference, Ann-Arbor.Wintner, S., 1996.
An Abstract Machine for Unifi-cation Grammars.
Technion Doctoral Thesis.69
