Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555?1565,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Sentiment-Specific Word Embeddingfor Twitter Sentiment Classification?Duyu Tang?, Furu Wei?, Nan Yang\, Ming Zhou?, Ting Liu?, Bing Qin?
?Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China?Microsoft Research, Beijing, China\University of Science and Technology of China, Hefei, China{dytang, tliu, qinb}@ir.hit.edu.cn{fuwei, v-nayang, mingzhou}@microsoft.comAbstractWe present a method that learns word em-bedding for Twitter sentiment classifica-tion in this paper.
Most existing algorithm-s for learning continuous word represen-tations typically only model the syntacticcontext of words but ignore the sentimen-t of text.
This is problematic for senti-ment analysis as they usually map word-s with similar syntactic context but oppo-site sentiment polarity, such as good andbad, to neighboring word vectors.
Weaddress this issue by learning sentiment-specific word embedding (SSWE), whichencodes sentiment information in the con-tinuous representation of words.
Specif-ically, we develop three neural networksto effectively incorporate the supervisionfrom sentiment polarity of text (e.g.
sen-tences or tweets) in their loss function-s. To obtain large scale training corpora,we learn the sentiment-specific word em-bedding from massive distant-supervisedtweets collected by positive and negativeemoticons.
Experiments on applying SS-WE to a benchmark Twitter sentimen-t classification dataset in SemEval 2013show that (1) the SSWE feature performscomparably with hand-crafted features inthe top-performed system; (2) the perfor-mance is further improved by concatenat-ing SSWE with existing feature set.1 IntroductionTwitter sentiment classification has attracted in-creasing research interest in recent years (Jiang etal., 2011; Hu et al, 2013).
The objective is to clas-sify the sentiment polarity of a tweet as positive,?This work was done when the first and third authorswere visiting Microsoft Research Asia.negative or neutral.
The majority of existing ap-proaches follow Pang et al (2002) and employ ma-chine learning algorithms to build classifiers fromtweets with manually annotated sentiment polar-ity.
Under this direction, most studies focus ondesigning effective features to obtain better clas-sification performance.
For example, Mohammadet al (2013) build the top-performed system in theTwitter sentiment classification track of SemEval2013 (Nakov et al, 2013), using diverse sentimentlexicons and a variety of hand-crafted features.Feature engineering is important but labor-intensive.
It is therefore desirable to discover ex-planatory factors from the data and make the learn-ing algorithms less dependent on extensive fea-ture engineering (Bengio, 2013).
For the task ofsentiment classification, an effective feature learn-ing method is to compose the representation of asentence (or document) from the representation-s of the words or phrases it contains (Socher etal., 2013b; Yessenalina and Cardie, 2011).
Ac-cordingly, it is a crucial step to learn the wordrepresentation (or word embedding), which is adense, low-dimensional and real-valued vector fora word.
Although existing word embedding learn-ing algorithms (Collobert et al, 2011; Mikolov etal., 2013) are intuitive choices, they are not effec-tive enough if directly used for sentiment classi-fication.
The most serious problem is that tradi-tional methods typically model the syntactic con-text of words but ignore the sentiment informationof text.
As a result, words with opposite polari-ty, such as good and bad, are mapped into closevectors.
It is meaningful for some tasks such aspos-tagging (Zheng et al, 2013) as the two wordshave similar usages and grammatical roles, but itbecomes a disaster for sentiment analysis as theyhave the opposite sentiment polarity.In this paper, we propose learning sentiment-specific word embedding (SSWE) for sentimentanalysis.
We encode the sentiment information in-1555to the continuous representation of words, so thatit is able to separate good and bad to opposite endsof the spectrum.
To this end, we extend the ex-isting word embedding learning algorithm (Col-lobert et al, 2011) and develop three neural net-works to effectively incorporate the supervisionfrom sentiment polarity of text (e.g.
sentencesor tweets) in their loss functions.
We learn thesentiment-specific word embedding from tweet-s, leveraging massive tweets with emoticons asdistant-supervised corpora without any manual an-notations.
These automatically collected tweet-s contain noises so they cannot be directly usedas gold training data to build sentiment classifier-s, but they are effective enough to provide weak-ly supervised signals for training the sentiment-specific word embedding.We apply SSWE as features in a supervisedlearning framework for Twitter sentiment classi-fication, and evaluate it on the benchmark datasetin SemEval 2013.
In the task of predicting posi-tive/negative polarity of tweets, our method yields84.89% in macro-F1 by only using SSWE as fea-ture, which is comparable to the top-performedsystem based on hand-crafted features (84.70%).After concatenating the SSWE feature with ex-isting feature set, we push the state-of-the-art to86.58% in macro-F1.
The quality of SSWE is al-so directly evaluated by measuring the word sim-ilarity in the embedding space for sentiment lexi-cons.
In the accuracy of polarity consistency be-tween each sentiment word and its top N closestwords, SSWE outperforms existing word embed-ding learning algorithms.The major contributions of the work presentedin this paper are as follows.?
We develop three neural networks to learnsentiment-specific word embedding (SSWE)from massive distant-supervised tweets with-out any manual annotations;?
To our knowledge, this is the first work thatexploits word embedding for Twitter senti-ment classification.
We report the results thatthe SSWE feature performs comparably withhand-crafted features in the top-performedsystem in SemEval 2013;?
We release the sentiment-specific word em-bedding learned from 10 million tweets,which can be adopted off-the-shell in othersentiment analysis tasks.2 Related WorkIn this section, we present a brief review of therelated work from two perspectives, Twitter senti-ment classification and learning continuous repre-sentations for sentiment classification.2.1 Twitter Sentiment ClassificationTwitter sentiment classification, which identifiesthe sentiment polarity of short, informal tweets,has attracted increasing research interest (Jiang etal., 2011; Hu et al, 2013) in recent years.
Gen-erally, the methods employed in Twitter sentimentclassification follow traditional sentiment classifi-cation approaches.
The lexicon-based approaches(Turney, 2002; Ding et al, 2008; Taboada et al,2011; Thelwall et al, 2012) mostly use a dictio-nary of sentiment words with their associated sen-timent polarity, and incorporate negation and in-tensification to compute the sentiment polarity foreach sentence (or document).The learning based methods for Twitter sen-timent classification follow Pang et al (2002)?swork, which treat sentiment classification of textsas a special case of text categorization issue.
Manystudies on Twitter sentiment classification (Pakand Paroubek, 2010; Davidov et al, 2010; Bar-bosa and Feng, 2010; Kouloumpis et al, 2011;Zhao et al, 2012) leverage massive noisy-labeledtweets selected by positive and negative emoticon-s as training set and build sentiment classifiers di-rectly, which is called distant supervision (Go etal., 2009).
Instead of directly using the distant-supervised data as training set, Liu et al (2012)adopt the tweets with emoticons to smooth the lan-guage model and Hu et al (2013) incorporate theemotional signals into an unsupervised learningframework for Twitter sentiment classification.Many existing learning based methods on Twit-ter sentiment classification focus on feature engi-neering.
The reason is that the performance of sen-timent classifier being heavily dependent on thechoice of feature representation of tweets.
Themost representative system is introduced by Mo-hammad et al (2013), which is the state-of-the-art system (the top-performed system in SemEval2013 Twitter Sentiment Classification Track) byimplementing a number of hand-crafted features.Unlike the previous studies, we focus on learningdiscriminative features automatically from mas-sive distant-supervised tweets.15562.2 Learning Continuous Representations forSentiment ClassificationPang et al (2002) pioneer this field by using bag-of-word representation, representing each word asa one-hot vector.
It has the same length as the sizeof the vocabulary, and only one dimension is 1,with all others being 0.
Under this assumption,many feature learning algorithms are proposed toobtain better classification performance (Pang andLee, 2008; Liu, 2012; Feldman, 2013).
However,the one-hot word representation cannot sufficient-ly capture the complex linguistic characteristics ofwords.With the revival of interest in deep learn-ing (Bengio et al, 2013), incorporating the con-tinuous representation of a word as features hasbeen proving effective in a variety of NLP tasks,such as parsing (Socher et al, 2013a), languagemodeling (Bengio et al, 2003; Mnih and Hin-ton, 2009) and NER (Turian et al, 2010).
In thefield of sentiment analysis, Bespalov et al (2011;2012) initialize the word embedding by Laten-t Semantic Analysis and further represent eachdocument as the linear weighted of ngram vec-tors for sentiment classification.
Yessenalina andCardie (2011) model each word as a matrix andcombine words using iterated matrix multiplica-tion.
Glorot et al (2011) explore Stacked Denois-ing Autoencoders for domain adaptation in sen-timent classification.
Socher et al propose Re-cursive Neural Network (RNN) (2011b), matrix-vector RNN (2012) and Recursive Neural TensorNetwork (RNTN) (2013b) to learn the composi-tionality of phrases of any length based on therepresentation of each pair of children recursively.Hermann et al (2013) present Combinatory Cate-gorial Autoencoders to learn the compositionalityof sentence, which marries the Combinatory Cat-egorial Grammar with Recursive Autoencoder.The representation of words heavily relies onthe applications or tasks in which it is used (Lab-utov and Lipson, 2013).
This paper focuseson learning sentiment-specific word embedding,which is tailored for sentiment analysis.
Un-like Maas et al (2011) that follow the proba-bilistic document model (Blei et al, 2003) andgive an sentiment predictor function to each word,we develop neural networks and map each n-gram to the sentiment polarity of sentence.
Un-like Socher et al (2011c) that utilize manuallylabeled texts to learn the meaning of phrase (orsentence) through compositionality, we focus onlearning the meaning of word, namely word em-bedding, from massive distant-supervised tweets.Unlike Labutov and Lipson (2013) that producetask-specific embedding from an existing wordembedding, we learn sentiment-specific word em-bedding from scratch.3 Sentiment-Specific Word Embeddingfor Twitter Sentiment ClassificationIn this section, we present the details of learn-ing sentiment-specific word embedding (SSWE)for Twitter sentiment classification.
We pro-pose incorporating the sentiment information ofsentences to learn continuous representations forwords and phrases.
We extend the existing wordembedding learning algorithm (Collobert et al,2011) and develop three neural networks to learnSSWE.
In the following sections, we introduce thetraditional method before presenting the details ofSSWE learning algorithms.
We then describe theuse of SSWE in a supervised learning frameworkfor Twitter sentiment classification.3.1 C&W ModelCollobert et al (2011) introduce C&W model tolearn word embedding based on the syntactic con-texts of words.
Given an ngram ?cat chills on amat?, C&W replaces the center word with a ran-dom wordwrand derives a corrupted ngram ?catchills wra mat?.
The training objective is that theoriginal ngram is expected to obtain a higher lan-guage model score than the corrupted ngram by amargin of 1.
The ranking objective function canbe optimized by a hinge loss,losscw(t, tr) = max(0, 1?
fcw(t) + fcw(tr))(1)where t is the original ngram, tris the corruptedngram, fcw(?)
is a one-dimensional scalar repre-senting the language model score of the input n-gram.
Figure 1(a) illustrates the neural architec-ture of C&W, which consists of four layers, name-ly lookup ?
linear ?
hTanh ?
linear (frombottom to top).
The original and corrupted ngram-s are treated as inputs of the feed-forward neuralnetwork, respectively.
The output fcwis the lan-guage model score of the input, which is calculat-ed as given in Equation 2, where L is the lookuptable of word embedding,w1, w2, b1, b2are the pa-rameters of linear layers.fcw(t) = w2(a) + b2(2)1557so cooool :DlookuplinearhTanhlinearsoftmax(a) C&Wso cooool :D(b) SSWEhso cooool :D(c) SSWEusyntacticsentimentpositivenegativeFigure 1: The traditional C&W model and our neural networks (SSWEhand SSWEu) for learningsentiment-specific word embedding.a = hTanh(w1Lt+ b1) (3)hTanh(x) =?????
?1 if x < ?1x if ?
1 ?
x ?
11 if x > 1(4)3.2 Sentiment-Specific Word EmbeddingFollowing the traditional C&W model (Collobertet al, 2011), we incorporate the sentiment infor-mation into the neural network to learn sentiment-specific word embedding.
We develop three neuralnetworks with different strategies to integrate thesentiment information of tweets.Basic Model 1 (SSWEh).
As an unsupervisedapproach, C&W model does not explicitly capturethe sentiment information of texts.
An intuitivesolution to integrate the sentiment information ispredicting the sentiment distribution of text basedon input ngram.
We do not utilize the entire sen-tence as input because the length of different sen-tences might be variant.
We therefore slide thewindow of ngram across a sentence, and then pre-dict the sentiment polarity based on each ngramwith a shared neural network.
In the neural net-work, the distributed representation of higher lay-er are interpreted as features describing the input.Thus, we utilize the continuous vector of top layerto predict the sentiment distribution of text.Assuming there are K labels, we modify the di-mension of top layer in C&W model as K andadd a softmax layer upon the top layer.
Theneural network (SSWEh) is given in Figure 1(b).Softmax layer is suitable for this scenario be-cause its outputs are interpreted as conditionalprobabilities.
Unlike C&W, SSWEhdoes not gen-erate any corrupted ngram.
Let fg(t), where Kdenotes the number of sentiment polarity label-s, be the gold K-dimensional multinomial distri-bution of input t and?kfgk(t) = 1.
For pos-itive/negative classification, the distribution is ofthe form [1,0] for positive and [0,1] for negative.The cross-entropy error of the softmax layer is :lossh(t) = ?
?k={0,1}fgk(t) ?
log(fhk(t)) (5)where fg(t) is the gold sentiment distribution andfh(t) is the predicted sentiment distribution.Basic Model 2 (SSWEr).
SSWEhis trained bypredicting the positive ngram as [1,0] and the neg-ative ngram as [0,1].
However, the constraint ofSSWEhis too strict.
The distribution of [0.7,0.3]can also be interpreted as a positive label becausethe positive score is larger than the negative s-core.
Similarly, the distribution of [0.2,0.8] indi-cates negative polarity.
Based on the above obser-vation, the hard constraints in SSWEhshould berelaxed.
If the sentiment polarity of a tweet is pos-itive, the predicted positive score is expected to belarger than the predicted negative score, and theexact reverse if the tweet has negative polarity.We model the relaxed constraint with a rank-ing objective function and borrow the bottom fourlayers from SSWEh, namely lookup?
linear ?hTanh ?
linear in Figure 1(b), to build the re-laxed neural network (SSWEr).
Compared withSSWEh, the softmax layer is removed becauseSSWErdoes not require probabilistic interpreta-tion.
The hinge loss of SSWEris modeled as de-1558scribed below.lossr(t) = max(0, 1?
?s(t)fr0(t)+ ?s(t)fr1(t) )(6)where fr0is the predicted positive score, fr1isthe predicted negative score, ?s(t) is an indicatorfunction reflecting the sentiment polarity of a sen-tence,?s(t) ={1 if fg(t) = [1, 0]?1 if fg(t) = [0, 1](7)Similar with SSWEh, SSWEralso does not gen-erate the corrupted ngram.Unified Model (SSWEu).
The C&W modellearns word embedding by modeling syntacticcontexts of words but ignoring sentiment infor-mation.
By contrast, SSWEhand SSWErlearnsentiment-specific word embedding by integratingthe sentiment polarity of sentences but leaving outthe syntactic contexts of words.
We develop a uni-fied model (SSWEu) in this part, which capturesthe sentiment information of sentences as well asthe syntactic contexts of words.
SSWEuis illus-trated in Figure 1(c).Given an original (or corrupted) ngram andthe sentiment polarity of a sentence as the in-put, SSWEupredicts a two-dimensional vector foreach input ngram.
The two scalars (fu0, fu1) s-tand for language model score and sentiment s-core of the input ngram, respectively.
The trainingobjectives of SSWEuare that (1) the original n-gram should obtain a higher language model scorefu0(t) than the corrupted ngram fu0(tr), and (2) thesentiment score of original ngram fu1(t) should bemore consistent with the gold polarity annotationof sentence than corrupted ngram fu1(tr).
The lossfunction of SSWEuis the linear combination of t-wo hinge losses,lossu(t, tr) = ?
?
losscw(t, tr)+(1?
?)
?
lossus(t, tr)(8)where losscw(t, tr) is the syntactic loss as givenin Equation 1, lossus(t, tr) is the sentiment lossas described in Equation 9.
The hyper-parameter?
weighs the two parts.lossus(t, tr) = max(0, 1?
?s(t)fu1(t)+ ?s(t)fu1(tr) )(9)Model Training.
We train sentiment-specificword embedding from massive distant-supervisedtweets collected with positive and negative emoti-cons1.
We crawl tweets from April 1st, 2013 toApril 30th, 2013 with TwitterAPI.
We tokenizeeach tweet with TwitterNLP (Gimpel et al, 2011),remove the @user and URLs of each tweet, and fil-ter the tweets that are too short (< 7 words).
Final-ly, we collect 10M tweets, selected by 5M tweetswith positive emoticons and 5M tweets with nega-tive emoticons.We train SSWEh, SSWErand SSWEubytaking the derivative of the loss through back-propagation with respect to the whole set of pa-rameters (Collobert et al, 2011), and use Ada-Grad (Duchi et al, 2011) to update the parame-ters.
We empirically set the window size as 3, theembedding length as 50, the length of hidden lay-er as 20 and the learning rate of AdaGrad as 0.1for all baseline and our models.
We learn embed-ding for unigrams, bigrams and trigrams separate-ly with same neural network and same parametersetting.
The contexts of unigram (bigram/trigram)are the surrounding unigrams (bigrams/trigrams),respectively.3.3 Twitter Sentiment ClassificationWe apply sentiment-specific word embedding forTwitter sentiment classification under a supervisedlearning framework as in previous work (Pang etal., 2002).
Instead of hand-crafting features, weincorporate the continuous representation of word-s and phrases as the feature of a tweet.
The senti-ment classifier is built from tweets with manuallyannotated sentiment polarity.We explore min, average and max convolu-tional layers (Collobert et al, 2011; Socher etal., 2011a), which have been used as simple andeffective methods for compositionality learningin vector-based semantics (Mitchell and Lapata,2010), to obtain the tweet representation.
The re-sult is the concatenation of vectors derived fromdifferent convolutional layers.z(tw) = [zmax(tw), zmin(tw), zaverage(tw)]where z(tw) is the representation of tweet tw andzx(tw) is the results of the convolutional layer x ?
{min,max, average}.
Each convolutional layer1We use the emoticons selected by Hu et al (2013).
Thepositive emoticons are :) : ) :-) :D =), and the negative emoti-cons are :( : ( :-( .1559zxemploys the embedding of unigrams, bigramsand trigrams separately and conducts the matrix-vector operation of x on the sequence representedby columns in each lookup table.
The output ofzxis the concatenation of results obtained fromdifferent lookup tables.zx(tw) = [wx?Luni?tw, wx?Lbi?tw, wx?Ltri?tw]where wxis the convolutional function of zx,?L?twis the concatenated column vectors of thewords in the tweet.
Luni, Lbiand Ltriare thelookup tables of the unigram, bigram and trigramembedding, respectively.4 ExperimentWe conduct experiments to evaluate SSWE by in-corporating it into a supervised learning frame-work for Twitter sentiment classification.
We alsodirectly evaluate the effectiveness of the SSWE bymeasuring the word similarity in the embeddingspace for sentiment lexicons.4.1 Twitter Sentiment ClassificationExperiment Setup and Datasets.
We conductexperiments on the latest Twitter sentiment clas-sification benchmark dataset in SemEval 2013(Nakov et al, 2013).
The training and develop-ment sets were completely in full to task partici-pants.
However, we were unable to download allthe training and development sets because sometweets were deleted or not available due to mod-ified authorization status.
The test set is directlyprovided to the participants.
The distribution ofour dataset is given in Table 1.
We train sentimentclassifier with LibLinear (Fan et al, 2008) on thetraining set, tune parameter ?c on the dev set andevaluate on the test set.
Evaluation metric is theMacro-F1 of positive and negative categories2.Positive Negative Neutral TotalTrain 2,642 994 3,436 7,072Dev 408 219 493 1,120Test 1,570 601 1,639 3,810Table 1: Statistics of the SemEval 2013 Twittersentiment classification dataset.2We investigate 2-class Twitter sentiment classifica-tion (positive/negative) instead of 3-class Twitter sentimentclassification (positive/negative/neutral) in SemEval2013.Baseline Methods.
We compare our methodwith the following sentiment classification algo-rithms:(1) DistSuper: We use the 10 million tweets se-lected by positive and negative emoticons as train-ing data, and build sentiment classifier with Lib-Linear and ngram features (Go et al, 2009).
(2) SVM: The ngram features and Support Vec-tor Machine are widely used baseline methods tobuild sentiment classifiers (Pang et al, 2002).
Li-bLinear is used to train the SVM classifier.
(3) NBSVM: NBSVM (Wang and Manning,2012) is a state-of-the-art performer on many sen-timent classification datasets, which trades-off be-tween Naive Bayes and NB-enhanced SVM.
(4) RAE: Recursive Autoencoder (Socher et al,2011c) has been proven effective in many senti-ment analysis tasks by learning compositionalityautomatically.
We run RAE with randomly initial-ized word embedding.
(5) NRC: NRC builds the top-performed systemin SemEval 2013 Twitter sentiment classificationtrack which incorporates diverse sentiment lexi-cons and many manually designed features.
Were-implement this system because the codes arenot publicly available3.
NRC-ngram refers to thefeature set of NRC leaving out ngram features.Except for DistSuper, other baseline method-s are conducted in a supervised manner.
We donot compare with RNTN (Socher et al, 2013b) be-cause we cannot efficiently train the RNTN model.The reason lies in that the tweets in our dataset donot have accurately parsed results or fine grainedsentiment labels for phrases.
Another reason isthat the RNTN model trained on movie reviewscannot be directly applied on tweets due to the d-ifferences between domains (Blitzer et al, 2007).Results and Analysis.
Table 2 shows the macro-F1 of the baseline systems as well as the SSWE-based methods on positive/negative sentimen-t classification of tweets.
Distant supervision isrelatively weak because the noisy-labeled tweet-s are treated as the gold standard, which affectsthe performance of classifier.
The results of bag-of-ngram (uni/bi/tri-gram) features are not satis-fied because the one-hot word representation can-not capture the latent connections between words.NBSVM and RAE perform comparably and have3For 3-class sentiment classification in SemEval 2013,our re-implementation of NRC achieved 68.3%, 0.7% low-er than NRC (69%) due to less training data.1560Method Macro-F1DistSuper + unigram 61.74DistSuper + uni/bi/tri-gram 63.84SVM + unigram 74.50SVM + uni/bi/tri-gram 75.06NBSVM 75.28RAE 75.12NRC (Top System in SemEval) 84.73NRC - ngram 84.17SSWEu84.98SSWEu+NRC 86.58SSWEu+NRC-ngram 86.48Table 2: Macro-F1 on positive/negative classifica-tion of tweets.a big gap in comparison with the NRC and SSWE-based methods.
The reason is that RAE and NB-SVM learn the representation of tweets from thesmall-scale manually annotated training set, whichcannot well capture the comprehensive linguisticphenomenons of words.NRC implements a variety of features andreaches 84.73% in macro-F1, verifying the impor-tance of a better feature representation for Twit-ter sentiment classification.
We achieve 84.98%by using only SSWEuas features without borrow-ing any sentiment lexicons or hand-crafted rules.The results indicate that SSWEuautomaticallylearns discriminative features from massive tweetsand performs comparable with the state-of-the-artmanually designed features.
After concatenatingSSWEuwith the feature set of NRC, the perfor-mance is further improved to 86.58%.
We alsocompare SSWEuwith the ngram feature by inte-grating SSWE into NRC-ngram.
The concatenatedfeatures SSWEu+NRC-ngram (86.48%) outperfor-m the original feature set of NRC (84.73%).As a reference, we apply SSWEuon subjec-tive classification of tweets, and obtain 72.17% inmacro-F1 by using only SSWEuas feature.
Af-ter combining SSWEuwith the feature set of NR-C, we improve NRC from 74.86% to 75.39% forsubjective classification.Comparision between Different Word Embed-ding.
We compare sentiment-specific word em-bedding (SSWEh, SSWEr, SSWEu) with base-line embedding learning algorithms by only us-ing word embedding as features for Twitter sen-timent classification.
We use the embedding of u-nigrams, bigrams and trigrams in the experimen-t.
The embeddings of C&W (Collobert et al,2011), word2vec4, WVSA (Maas et al, 2011) andour models are trained with the same dataset andsame parameter setting.
We compare with C&Wand word2vec as they have been proved effectivein many NLP tasks.
The trade-off parameter ofReEmb (Labutov and Lipson, 2013) is tuned onthe development set of SemEval 2013.Table 3 shows the performance on the pos-itive/negative classification of tweets5.
ReEm-b(C&W) and ReEmb(w2v) stand for the useof embeddings learned from 10 million distant-supervised tweets with C&W and word2vec, re-spectively.
Each row of Table 3 represents a wordembedding learning algorithm.
Each column s-tands for a type of embedding used to composefeatures of tweets.
The column uni+bi denotes theuse of unigram and bigram embedding, and thecolumn uni+bi+tri indicates the use of unigram,bigram and trigram embedding.Embedding unigram uni+bi uni+bi+triC&W 74.89 75.24 75.89Word2vec 73.21 75.07 76.31ReEmb(C&W) 75.87 ?
?ReEmb(w2v) 75.21 ?
?WVSA 77.04 ?
?SSWEh81.33 83.16 83.37SSWEr80.45 81.52 82.60SSWEu83.70 84.70 84.98Table 3: Macro-F1 on positive/negative classifica-tion of tweets with different word embeddings.From the first column of Table 3, we can see thatthe performance of C&W and word2vec are obvi-ously lower than sentiment-specific word embed-dings by only using unigram embedding as fea-tures.
The reason is that C&W and word2vec donot explicitly exploit the sentiment information ofthe text, resulting in that the words with oppo-site polarity such as good and bad are mappedto close word vectors.
When such word embed-dings are fed as features to a Twitter sentimen-t classifier, the discriminative ability of sentimentwords are weakened thus the classification perfor-mance is affected.
Sentiment-specific word em-4Available at https://code.google.com/p/word2vec/.
Weutilize the Skip-gram model because it performs better thanCBOW in our experiments.5MVSA and ReEmb are not suitable for learning bigramand trigram embedding because their sentiment predictorfunctions only utilize the unigram embedding.1561beddings (SSWEh, SSWEr, SSWEu) effectivelydistinguish words with opposite sentiment polarityand perform best in three settings.
SSWE outper-forms MVSA by exploiting more contextual infor-mation in the sentiment predictor function.
SSWEoutperforms ReEmb by leveraging more senti-ment information from massive distant-supervisedtweets.
Among three sentiment-specific word em-beddings, SSWEucaptures more context informa-tion and yields best performance.
SSWEhandSSWErobtain comparative results.From each row of Table 3, we can see that thebigram and trigram embeddings consistently im-prove the performance of Twitter sentiment classi-fication.
The underlying reason is that a phrase,which cannot be accurately represented by uni-gram embedding, is directly encoded into the n-gram embedding as an idiomatic unit.
A typicalcase in sentiment analysis is that the composedphrase and multiword expression may have a dif-ferent sentiment polarity than the individual word-s it contains, such as not [bad] and [great] dealof (the word in the bracket has different sentimentpolarity with the ngram).
A very recent study byMikolov et al (2013) also verified the effective-ness of phrase embedding for analogically reason-ing phrases.Effect of ?
in SSWEuWe tune the hyper-parameter ?
of SSWEuon the development set byusing unigram embedding as features.
As givenin Equation 8, ?
is the weighting score of syntac-tic loss of SSWEuand trades-off the syntactic andsentiment losses.
SSWEuis trained from 10 mil-lion distant-supervised tweets.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.770.780.790.80.810.820.830.84?Macro?F1SSWEuFigure 2: Macro-F1 of SSWEuon the develop-ment set of SemEval 2013 with different ?.Figure 2 shows the macro-F1 of SSWEuon pos-itive/negative classification of tweets with differ-ent ?
on our development set.
We can see thatSSWEuperforms better when ?
is in the rangeof [0.5, 0.6], which balances the syntactic contextand sentiment information.
The model with ?=1stands for C&W model, which only encodes thesyntactic contexts of words.
The sharp decline at?=1 reflects the importance of sentiment informa-tion in learning word embedding for Twitter senti-ment classification.Effect of Distant-supervised Data in SSWEuWe investigate how the size of the distant-supervised data affects the performance of SSWEufeature for Twitter sentiment classification.
Wevary the number of distant-supervised tweets from1 million to 12 million, increased by 1 million.We set the ?
of SSWEuas 0.5, according to theexperiments shown in Figure 2.
Results of posi-tive/negative classification of tweets on our devel-opment set are given in Figure 3.1 2 3 4 5 6 7 8 9 10 11 12x 1060.770.780.790.80.810.820.830.84# of distant?supervised tweetsMacro?F1SSWEuFigure 3: Macro-F1 of SSWEuwith different sizeof distant-supervised data on our development set.We can see that when more distant-supervisedtweets are added, the accuracy of SSWEucon-sistently improves.
The underlying reason is thatwhen more tweets are incorporated, the word em-bedding is better estimated as the vocabulary sizeis larger and the context and sentiment informa-tion are richer.
When we have 10 million distant-supervised tweets, the SSWEufeature increasesthe macro-F1 of positive/negative classification oftweets to 82.94% on our development set.
Whenwe have more than 10 million tweets, the per-formance remains stable as the contexts of wordshave been mostly covered.4.2 Word Similarity of Sentiment LexiconsThe quality of SSWE has been implicitly evaluat-ed when applied in Twitter sentiment classificationin the previous subsection.
We explicitly evaluateit in this section through word similarity in the em-1562bedding space for sentiment lexicons.
The evalua-tion metric is the accuracy of polarity consistencybetween each sentiment word and its topN closestwords in the sentiment lexicon,Accuracy =?#Lexi=1?Nj=1?
(wi, cij)#Lex?N(10)where #Lex is the number of words in the senti-ment lexicon, wiis the i-th word in the lexicon, cijis the j-th closest word towiin the lexicon with co-sine similarity, ?
(wi, cij) is an indicator functionthat is equal to 1 if wiand cijhave the same sen-timent polarity and 0 for the opposite case.
Thehigher accuracy refers to a better polarity consis-tency of words in the sentiment lexicon.
We set Nas 100 in our experiment.Experiment Setup and Datasets We utilizethe widely-used sentiment lexicons, namely M-PQA (Wilson et al, 2005) and HL (Hu and Liu,2004), to evaluate the quality of word embedding.For each lexicon, we remove the words that donot appear in the lookup table of word embedding.We only use unigram embedding in this sectionbecause these sentiment lexicons do not containphrases.
The distribution of the lexicons used inthis paper is listed in Table 4.Lexicon Positive Negative TotalHL 1,331 2,647 3,978MPQA 1,932 2,817 4,749Joint 1,051 2,024 3,075Table 4: Statistics of the sentiment lexicons.
Join-t stands for the words that occur in both HL andMPQA with the same sentiment polarity.Results.
Table 5 shows our results com-pared to other word embedding learning al-gorithms.
The accuracy of random result is50% as positive and negative words are ran-domly occurred in the nearest neighbors ofeach word.
Sentiment-specific word embed-dings (SSWEh, SSWEr, SSWEu) outperform ex-isting neural models (C&W, word2vec) by largemargins.
SSWEuperforms best in three lexicon-s. SSWEhand SSWErhave comparable perfor-mances.
Experimental results further demonstratethat sentiment-specific word embeddings are ableto capture the sentiment information of texts anddistinguish words with opposite sentiment polari-ty, which are not well solved in traditional neuralEmbedding HL MPQA JointRandom 50.00 50.00 50.00C&W 63.10 58.13 62.58Word2vec 66.22 60.72 65.59ReEmb(C&W) 64.81 59.76 64.09ReEmb(w2v) 67.16 61.81 66.39WVSA 68.14 64.07 67.12SSWEh74.17 68.36 74.03SSWEr73.65 68.02 73.14SSWEu77.30 71.74 77.33Table 5: Accuracy of the polarity consistency ofwords in different sentiment lexicons.models like C&W and word2vec.
SSWE outper-forms MVSA and ReEmb by exploiting more con-text information of words and sentiment informa-tion of sentences, respectively.5 ConclusionIn this paper, we propose learning continuousword representations as features for Twitter sen-timent classification under a supervised learningframework.
We show that the word embeddinglearned by traditional neural networks are not ef-fective enough for Twitter sentiment classification.These methods typically only model the contex-t information of words so that they cannot dis-tinguish words with similar context but oppositesentiment polarity (e.g.
good and bad).
We learnsentiment-specific word embedding (SSWE) byintegrating the sentiment information into the lossfunctions of three neural networks.
We train SS-WE with massive distant-supervised tweets select-ed by positive and negative emoticons.
The ef-fectiveness of SSWE has been implicitly evaluat-ed by using it as features in sentiment classifica-tion on the benchmark dataset in SemEval 2013,and explicitly verified by measuring word similar-ity in the embedding space for sentiment lexicon-s. Our unified model combining syntactic contextof words and sentiment information of sentencesyields the best performance in both experiments.AcknowledgmentsWe thank Yajuan Duan, Shujie Liu, Zhenghua Li,Li Dong, Hong Sun and Lanjun Zhou for theirgreat help.
This research was partly supportedby National Natural Science Foundation of China(No.61133012, No.61273321, No.61300113).1563ReferencesLuciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy da-ta.
In Proceedings of International Conference onComputational Linguistics, pages 36?44.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Yoshua Bengio, Aaron Courville, and Pascal Vincent.2013.
Representation learning: A review and newperspectives.
IEEE Trans.
Pattern Analysis and Ma-chine Intelligence.Yoshua Bengio.
2013.
Deep learning of represen-tations: Looking forward.
arXiv preprint arX-iv:1305.0445.Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-oufandeh.
2011.
Sentiment classification based onsupervised latent n-gram analysis.
In Proceedingsof the Conference on Information and KnowledgeManagement, pages 375?382.Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shok-oufandeh.
2012.
Sentiment classification with su-pervised sequence embedding.
In Machine Learn-ing and Knowledge Discovery in Databases, pages159?174.
Springer.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Annual Meeting of the Association forComputational Linguistics, volume 7.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of International Con-ference on Computational Linguistics, pages 241?249.Xiaowen Ding, Bing Liu, and Philip S Yu.
2008.
Aholistic lexicon-based approach to opinion mining.In Proceedings of the International Conference onWeb Search and Data Mining, pages 231?240.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, pages 2121?2159.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Ronen Feldman.
2013.
Techniques and application-s for sentiment analysis.
Communications of theACM, 56(4):82?89.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor twitter: Annotation, features, and experiments.In Proceedings of the Annual Meeting of the Associ-ation for Computational Linguistics, pages 42?47.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
Proceed-ings of International Conference on Machine Learn-ing.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Karl Moritz Hermann and Phil Blunsom.
2013.
Therole of syntax in vector space models of compo-sitional semantics.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics, pages 894?904.Ming Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD Conference on Knowledge Discoveryand Data Mining, pages 168?177.Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.2013.
Unsupervised sentiment analysis with emo-tional signals.
In Proceedings of the InternationalWorld Wide Web Conference, pages 607?618.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent twitter sen-timent classification.
The Proceeding of AnnualMeeting of the Association for Computational Lin-guistics, 1:151?160.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In The International AAAIConference on Weblogs and Social Media.Igor Labutov and Hod Lipson.
2013.
Re-embeddingwords.
In Annual Meeting of the Association forComputational Linguistics.Kun-Lin Liu, Wu-Jun Li, and Minyi Guo.
2012.
E-moticon smoothed language models for twitter sen-timent analysis.
In The Association for the Advance-ment of Artificial Intelligence.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.1564Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
InProceedings of the Annual Meeting of the Associ-ation for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-do, and Jeffrey Dean.
2013.
Distributed representa-tions of words and phrases and their compositionali-ty.
The Conference on Neural Information Process-ing Systems.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Andriy Mnih and Geoffrey E Hinton.
2009.
A s-calable hierarchical distributed language model.
InAdvances in neural information processing systems,pages 1081?1088.Saif M Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
Proceedingsof the International Workshop on Semantic Evalua-tion.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Proceedings of the International Work-shop on Semantic Evaluation, volume 13.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In Proceedings of Language Resources and Evalua-tion Conference, volume 2010.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 79?86.Richard Socher, Eric H Huang, Jeffrey Pennington,Andrew Y Ng, and Christopher D Manning.
2011a.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
The Conferenceon Neural Information Processing Systems, 24:801?809.Richard Socher, Cliff C Lin, Andrew Ng, and ChrisManning.
2011b.
Parsing natural scenes and nat-ural language with recursive neural networks.
InProceedings of the International Conference on Ma-chine Learning, pages 129?136.Richard Socher, J. Pennington, E.H. Huang, A.Y.
Ng,and C.D.
Manning.
2011c.
Semi-supervised recur-sive autoencoders for predicting sentiment distribu-tions.
In Conference on Empirical Methods in Nat-ural Language Processing, pages 151?161.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Com-positionality Through Recursive Matrix-Vector S-paces.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013a.
Parsing with composi-tional vector grammars.
In Annual Meeting of theAssociation for Computational Linguistics.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013b.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Conference on Empirical Methods in Nat-ural Language Processing, pages 1631?1642.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional linguistics, 37(2):267?307.Mike Thelwall, Kevan Buckley, and Georgios Pal-toglou.
2012.
Sentiment strength detection for thesocial web.
Journal of the American Society for In-formation Science and Technology, 63(1):163?173.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
Annual Meeting of theAssociation for Computational Linguistics.Peter D Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of Annual Meet-ing of the Association for Computational Linguistic-s, pages 417?424.Sida Wang and Christopher D Manning.
2012.
Base-lines and bigrams: Simple, good sentiment and topicclassification.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistic-s, pages 90?94.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 347?354.Ainur Yessenalina and Claire Cardie.
2011.
Compo-sitional matrix-space models for sentiment analysis.In Proceedings of Conference on Empirical Methodsin Natural Language Processing, pages 172?182.Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu.
2012.Moodlens: an emoticon-based sentiment analysissystem for chinese tweets.
In Proceedings of the18th ACM SIGKDD international conference onKnowledge discovery and data mining.Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.2013.
Deep learning for chinese word segmenta-tion and pos tagging.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 647?657.1565
