Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 54?62,Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational LinguisticsDesperately Seeking Implicit Arguments in TextSara TonelliFondazione Bruno Kessler / Trento, Italysatonelli@fbk.euRodolfo DelmonteUniversit Ca?
Foscari / Venezia, Italydelmont@unive.itAbstractIn this paper, we address the issue of automat-ically identifying null instantiated argumentsin text.
We refer to Fillmore?s theory of prag-matically controlled zero anaphora (Fillmore,1986), which accounts for the phenomenon ofomissible arguments using a lexically-basedapproach, and we propose a strategy for iden-tifying implicit arguments in a text and findingtheir antecedents, given the overtly expressedsemantic roles in the form of frame elements.To this purpose, we primarily rely on linguis-tic knowledge enriched with role frequencyinformation collected from a training corpus.We evaluate our approach using the test setdeveloped for the SemEval task 10 and wehighlight some issues of our approach.
Be-sides, we also point out some open problemsrelated to the task definition and to the generalphenomenon of null instantiated arguments,which needs to be better investigated and de-scribed in order to be captured from a compu-tational point of view.1 IntroductionIn natural language, lexically unexpressed linguisticitems are very frequent and indirectly weaken anyattempt at computing the meaning of a text or dis-course.
However, the need to address semantic in-terpretation is strongly felt in current advanced NLPtasks, in particular, the issue of transforming a textor discourse into a set of explicitly interconnectedpredicate-argument/adjunct structures (hence PAS).The aim of this task would be to unambiguouslyidentify events and participants and their associationto spatiotemporal locations.
However, in order to dothat, symbolic and statistical approaches should bebased on the output representation of a deep parser,which is currently almost never the case.
CurrentNLP technologies usually address the surface levellinguistic information with good approximation independency or constituency structures, but miss im-plicit entities (IEs) altogether.
The difficulties todeal with lexically unexpressed items or implicit en-tities are related on the one hand to recall problems,i.e.
the problem of deciding whether an item is im-plicit or not, and on the other hand to precision prob-lems, i.e.
if an implicit entity is accessible to thereader from the discourse or its context, an appropri-ate antecedent has to be found.
However, a systemable to derive the presence of IEs may be a deter-mining factor in improving performance of QA sys-tems and, in general, in Informations Retrieval andExtraction systems.The current computational scene has witnessed anincreased interest in the creation and use of semanti-cally annotated computational lexica and their asso-ciated annotated corpora, like PropBank (Palmer etal., 2005), FrameNet (Baker et al, 1998) and Nom-Bank (Meyers, 2007), where the proposed annota-tion scheme has been applied in real contexts.
In allthese cases, what has been addressed is a basic se-mantic issue, i.e.
labeling PAS associated to seman-tic predicates like adjectives, verbs and nouns.
How-ever, what these corpora have not made available isinformation related to IEs.
For example, in the caseof eventive deverbal nominals, information about thesubject/object of the nominal predicate is often im-plicit and has to be understood from the previous54discourse or text, e.g.
?the development of a pro-totype [?
implicit subject]?.
As reported by Gerberand Chai (2010), introducing implicit arguments tonominal predicates in NomBank would increase theresource coverage of 65%.Other IEs can be found in agentless passive con-structions ( e.g.
?Our little problem will soon besolved ?
[?
unexpressed Agent ]?1) or as unex-pressed arguments such as addressee with verbs ofcommitment, for example ?I can promise ?
that oneof you will be troubled [?
unexpressed Addressee]?and ?I dare swear ?
that before tomorrow night hewill be fluttering in our net [?
unexpressed Ad-dressee]?.In this paper we discuss the issues related to theidentification of implicit entities in text, focussing inparticular on omissions of core arguments of pred-icates.
We investigate the topic from the perspec-tive proposed by (Fillmore, 1986) and base our ob-servations on null instantiated arguments annotatedfor the SemEval 2010 Task 10, ?Linking Events andTheir Participants in Discourse?
(Ruppenhofer et al,2010)2.
The paper is structured as follows: in Sec-tion 2 we detail the task of identifying null instan-tiated arguments from a theoretical perspective anddescribe related work.
In Section 3 we briefly in-troduce the SemEval task 10 for identifying implicitarguments in text, while in Section 4 we detail ourproposal for NI identification and binding.
In Sec-tion 5 we give a thorough description of the types ofnull instantiations annotated in the SemEval data setand we explain the behavior of our algorithm w.r.t.such cases.
We also compare our results with theoutput of the systems participating to the SemEvaltask.
Finally, we draw some conclusions in Section6.2 Related workIn this work, we focus on null complements, alsocalled pragmatically controlled zero anaphora (Fill-more, 1986), understood arguments or linguistically1Unless otherwise specified, the following examples aretaken from the data sets made available in the SemEval 2010task ?Linking Events and Their Participants in Discourse?.Some of them have been slightly simplified for purposes of ex-position.2http://semeval2.fbk.eu/semeval2.php?location=tasks&taskid=9unrealized arguments.
We focus on Fillmore?s the-ory because his approach represents the backbone ofthe FrameNet project, which in turn inspired the Se-mEval task we will describe below.
Fillmore (1986)shows that in English and many other languagessome verbs allow null complements and some oth-ers don?t.
The latter require that, when they ap-pear in a sentence, all core semantic roles relatedto the predicate are expressed.
For example, sen-tences like ?Mary locked ???
or ?John guaranteed???
are not grammatically well-formed, because theyboth require two mandatory linguistically inherentparticipants.
Fillmore tries to explain why seman-tic roles can sometimes be left unspoken and whatconstraints help the interpreter recover the missingroles.
He introduces different factors that can in-fluence the licensing of null complements.
Thesecan be lexically-based, (semantically close predi-cates like ?promise?
and ?guarantee?
can license theomission of the theme argument in different cases),motivated by the interpretation of the predicate (?Iwas eating ??
licenses a null object because it hasan existential interpretation) and depending on thecontext (see for example the use of impress in anepisodic context like ?She impressed the audience?,where the null complement is not allowed, comparedto ?She impresses ?
every time?
in habitual interpre-tation; examples from Ruppenhofer and Michaelis(2009)).The fact that Fillmore explains the phenomenonof omissible arguments with a lexically-based ap-proach implies that from his perspective neither apurely pragmatic nor a purely semantic approachcan account for the behavior of omissible arguments.For example, he argues that some verbs, such as tolock will never license a null complement, no matterin which pragmatic context they are used.
Besides,there are synonymous verbs which behave differ-ently as regards null complementation, which Fill-more sees as evidence against a purely semantic ex-planation of implicit arguments.Another relevant distinction drawn in Fillmore(1986) is the typology of omitted arguments, whichdepends on the type of licensor and on the interpre-tation of the null complement.
Fillmore claims thatwith some verbs the missing complement can be re-trieved from the context, i.e.
it is possible to find areferent previously mentioned in the text / discourse55and bearing a definite, precise meaning.
These casesare labeled as definite null complements or instantia-tions (DNI) and are lexically specific in that they ap-ply only to some predicates.
We report an exampleof DNI in (1), taken from the SemEval task 10 dataset (see Section 3).
The predicate ?waiting?
has anomitted object, which we understand from the dis-course context to refer to ?I?.
(1) I saw him rejoin his guest, and I crept quietlyback to where my companions were waiting ?to tell them what I had seen.DNIs can also occur with nominal predicates, asreported in (2), where the person having a thought,the baronet, is mentioned in the preceding sentence:(2) Stapleton was talking with animation, but thebaronet looked pale and distrait.
Perhaps thethought of that lonely walk across theill-omened moor was weighing heavily uponhis mind.In contrast to DNIs, Fillmore claims that withsome verbs and in some interpretations, a core ar-gument can be omitted without having a referentexpressing the meaning of the null argument.
Theidentity of the missing argument can be left un-known or indefinite.
These cases are labeled as in-definite null complements or instantiations (INI) andare constructionally licensed in that they apply toany predicate in a particular grammatical construc-tion.
See for example the following cases, where theomission of the agent is licensed by the passive con-struction:(3) One of them was suddenly shut off ?.
(4) I am reckoned fleet of foot ?.Cases of INI were annotated by the organizers ofthe SemEval task 10 also with nominal predicates,as shown in the example below, where the perceiverof the odour is left unspecified:(5) Rank reeds and lush, slimy water-plants sentan odour ?
of decay and a heavy miasmaticvapour.Few attempts have been done so far to automati-cally deal with the recovery of implicit informationin text.
One of the earliest systems for identifyingextra-sentential arguments is PUNDIT by Palmer etal.
(1986).
This Prolog-based system comprises asyntactic component for parsing, a semantic compo-nent, which decomposes predicates into componentmeanings and fills their semantic roles with syntacticconstituents based on a domain-specific model, anda reference resolution component, which is calledboth for explicit constituents and for obligatory im-plicit constituents.
The reference resolution processis based on a focus list with all potential pronominalreferents identified by the semantic component.
Theapproach, however, has not been evaluated on a dataset, so we cannot directly compare its performancewith other approaches.
Furthermore, it is stronglydomain-dependent.In a case study, Burchardt et al (2005) proposeto identify implicit arguments exploiting contex-tual relations from deep-parsing and lexico-semanticframe relations encoded in FrameNet.
In particu-lar, they suggest converting a text into a network oflexico-semantic predicate-argument relations con-nected through frame-to-frame relations and recur-rent anaphoric linking patterns.
However, the au-thors do not implement and evaluate this approach.Most recently, Gerber and Chai (2010) have pre-sented a supervised classification model for the re-covery of implicit arguments of nominal predicatesin NomBank.
The model features are quite differentfrom those usually considered in standard SRL tasksand include among others information from Verb-Net classes, pointwise mutual information betweensemantic arguments, collocation and frequency in-formation about the predicates, information aboutparent nodes and siblings of the predicates and dis-course information.
The authors show the feasibilityof their approach, which however relies on a selectedgroup of nominal predicates with a large number ofannotated instances.The first attempt to evaluate implicit argumentidentification over a common test set and consider-ing different kinds of predicates was made by Rup-penhofer et al (2010).
Further details are given inthe following section.56Data set Sentences Frame inst.
Frame types Overt FEs DNIs (resolved) INIsTrain 438 1,370 317 2,526 303 (245) 277Test 525 1,703 452 3,141 349 (259) 361Table 1: Data set statistics from SemEval task 103 SemEval 2010 task 10The SemEval-2010 task for linking events and theirparticipants in discourse (Ruppenhofer et al, 2010)introduced a new issue w.r.t.
the SemEval-2007task ?Frame Semantic Structure Extraction?
(Bakeret al, 2007), in that it focused on linking local se-mantic argument structures across sentence bound-aries.
Specifically, the task included first the identi-fication of frames and frame elements in a text fol-lowing the FrameNet paradigm (Baker et al, 1998),then the identification of locally uninstantiated roles(NIs).
If these roles are indefinite (INI), they haveto be marked as such and no antecedent has to befound.
On the contrary, if they are definite (DNI),their coreferents have to be found in the wider dis-course context.
The challenge comprised two tasks,namely the full task (semantic role recognition andlabeling + NI linking) and the NIs only task, i.e.
theidentification of null instantiations and their refer-ents given a test set with gold standard local seman-tic argument structure.
In this work, we focus on thelatter task.The data provided to the participants included atraining and a test set.
The training data comprised438 sentences from Arthur Conan Doyle?s novel?The Adventure of Wisteria Lodge?, manually an-notated with frame and INI/DNI information.
Thetest set included 2 chapters of the Sherlock Holmesstory ?The Hound of Baskervilles?
with a total of525 sentences, provided with gold standard frameinformation.
The participants had to i) assess if a lo-cal argument is implicit; ii) decide whether it is anINI or a DNI and iii) in the second case, find theantecedent of the implicit argument.
We report inTable 1 some statistics about the provided data setsfrom Ruppenhofer et al (2010).
Note that overt FEsare the explicit frame elements annotated in the dataset.Although 26 teams downloaded the data sets,there were only two submissions, probably depend-ing on the intrinsic difficulties of the task (see dis-cussion in Section 5).
The best performing system(Chen et al, 2010) is based on a supervised learn-ing approach using, among others, distributional se-mantic similarity between the heads of candidatereferents and role fillers in the training data, butits performance is strongly affected by data sparse-ness.
Indeed, only 438 sentences with annotatedNIs were made available in the training set, whichis clearly insufficient to capture such a multifacetedphenomenon with a supervised approach.
The sec-ond system participating in the task (Tonelli andDelmonte, 2010) was an adaptation of an exist-ing LFG-based system for deep semantic analysis(Delmonte, 2009), whose output was mapped toFrameNet-style annotation.
In this case, the majorchallenge was to cope with the classification of someNI phenomena which are very much dependent onframe specific information, and can hardly be gener-alized in the LFG framework.4 A linguistically motivated proposal forNI identification and bindingIn this section, we describe our proposal for dealingwith INI/DNI identification and evaluate our outputagainst SemEval gold standard data.
As discussed inthe previous section, existing systems dealing withthis task suffer on the one hand from a lack of train-ing data and on the other hand from the dependenceof the task on frame annotation, which makes it diffi-cult to adapt existing unsupervised approaches.
Weshow that, given this state of the art, better resultscan be achieved in the task by simply developing analgorithm that reflects as much as possible the lin-guistic motivations behind NI identification in theFrameNet paradigm.
Our approach is divided intotwo subtasks: i) identify INIs/DNIs and ii) for eachDNI, find the corresponding referent in text.We develop an algorithm that incorporates the fol-lowing linguistic information:FE coreness status Null instantiated arguments asdefined in FrameNet are always core arguments, i.e.57they are central to the meaning of a frame.
Sincethe coreness status of the arguments is encoded inFrameNet, we limit our search for an NI only if acore frame element is not overtly expressed in thetext.Incorporated FEs Although all lexical units be-longing to the same frame in the FrameNet databaseare characterized by the same set of core FEs, a fur-ther distinction should be introduced when dealingwith NIs identification.
For example, in PERCEP-TION ACTIVE, several predicates are listed, whichhowever have a different behavior w.r.t.
the coreBody part FE.
?Feel.v?, for instance, is underspec-ified as regards the body part perceiving the sensa-tion, so we can assume that when it is not overtlyexpressed, we have a case of null instantiation.
Forother verbs in the same frame, such as ?glance.v?
or?listen.v?, the coreness status of Body part seems tobe more questionable, because the perceiving organis already implied by the verb meaning.
For this rea-son, we argue that if Body part is not expressed with?glance.v?
or ?listen.v?, it is not a case of null instan-tiation.
Such FEs are defined as incorporated in thelexical unit and are encoded as such in FrameNet.Excludes and Includes relation In FrameNet,some information about the relationship betweencertain FEs is encoded.
In particular, some FEs areconnected by the Excludes relation, which meansthat they cannot occur together, and others by theRequires relation, which means that if a given FEis present, then also the other must be overtly orimplicitly present.
An example of Excludes is therelationship between the FE Entity 1 / Entity 2 andEntities, because if Entity 1 and Entity 2 are bothpresent in a sentence, then Entities cannot be co-present.
Conversely, Entity 1 and Entity 2 stand in aRequires relationship, because the first cannot occurwithout the second.
This kind of information canclearly be helpful in case we have to automaticallydecide whether an argument is implicit or is just notpresent because it is not required.INI/DNI preference Ruppenhofer and Michaelis(2009) suggest that omissible arguments in particu-lar frames tend to be always interpreted as definite orindefinite.
For example, they report that in a samplefrom the British National Corpus, the interpretationfor a null instantiated Goal argument is definite in97.5% of the observed cases.
We take this featureinto account by considering the frequency of an im-plicit argument being annotated as definite/indefinitein the training set.The algorithm incorporating all this linguistic in-formation is detailed in the following subsection.4.1 INI/DNI identificationIn a preliminary step, we collect for each frame thelist of arguments being annotated as DNI/INI withthe corresponding frequency in the training set.
Forexample, in the CALENDRIC UNIT frame, the Wholeargument has been annotated 11 times as INI and5 times as DNI.
Some implicit frame elements oc-cur only as INI or DNI, for example Goal, which isannotated 14 times as DNI and never as INI in theARRIVING frame.
This frequency list (FreqList)is collected in order to decide if candidate null in-stantiations have to be classified as DNI or INI.We consider each sentence in the test data pro-vided with FrameNet annotation, and for each pred-icate p annotated with a set of overt frame elementsFEs, we run the first module for DNI/INI identi-fication.
The steps followed are reported in Algo-rithm 1.
We first check if the annotated FEs con-tain all core frame elements C listed in FrameNet forp.
If the two sets are identical, we conclude that nocore frame element can be implicit and we return anempty set both for DNI and INI .
For example, inthe test sentence (6), the BODY MOVEMENT frameappears in the sentence with its two core frame el-ements, i.e.
Body part and Agent.
Therefore, noimplicit argument can be postulated.
(6) Finally [she]Agent openedBODY MOVEMENT [hereyes]Body part again.If the core FEs in C are not all overtly expressedin FEs, we run two routines to check if the miss-ing FEs CandNIs are likely to be null instantiatedelements.
First, we discard all candidate NIs that ap-pear as incorporated FEs for the given p. Second, wediscard as well candidate NIs if they are excluded bythe overtly annotated FEs.The last steps of the algorithm are devoted to de-ciding if the candidate null instantiation is definiteor indefinite.
For this step, we rely on the observa-tions collected in FreqList.
In particular, for each58candidate c we check if it was already present as INIor DNI in the training set.
If yes, we label c accord-ingly.
In case c was observed both as INI and asDNI, the most probable label is assigned based onits frequency in the training set.Input: TestSet with annotated core FEs;FreqListOutput: INI and DNI for pforeach p ?
TestSet doextract annotated core FEs;extract set C of core FEs for p in FrameNet;if C ?
FEs thenDNI = ?
;INI = ?
;elseC \ FEs = CandNIs;foreach c ?
CandNIs doif c is incorporated FE of p thendelete cforeach fe ?
FEs doif fe excludes c thendelete cendforeach nip ?
FreqListp doif c = nip thenif nip is only dnip thenc ?
DNIif nip is only inip thenc ?
INIif nip is inip and nip is dnipthenif Freq(inip) >Freq(dnip) thenc ?
INIelsec ?
DNIendendreturn(INI);return(DNI);endAlgorithm 1: DNI/INI identification4.2 DNI bindingGiven that both the supervised approach exploitedby Chen et al (2010) and the methodology pro-posed in Tonelli and Delmonte (2010) based ondeep-semantic parsing achieved quite poor resultsin the DNI-binding task, we devise a third approachthat relies on the observed heads of each FE in thetraining set and assigns a relevance score to eachcandidate antecedent.We first collect for each FE the list of headsHtrain assigned to FE in the training set, and we ex-tract for each head htrain ?
Htrain the correspond-ing frequency fhtrain .
Then, for each dni ?
DNIidentified with Algorithm 1 in the test set, we collectall nominal heads Htest occurring in a window of(plus/minus) 5 sentences and we assign to each can-didate head htest ?
Htest a relevance score relhtestw.r.t.
dni computed as follows:relhtest =fhtraindist(sentdni, senthtest)(7)where fhtrain is the number of times h has beenobserved in the training set with a FE label, anddist(sentdni, senthtest) is the distance between thesentence where the dni has been detected and thesentence where the candidate head htest occurs (0 ?dist(sentdni, senthtest) ?
5).The best candidate head for dni is the one withthe highest relhtest , given that it is (higher) than 0.The way we compute the relevance score is based onthe intuition that, if a head was frequently observedfor FE in the training set, it is likely that it is a goodcandidate.
However, the more distant it occurs fromdni, then less probable it is as antecedent.5 Evaluation and error analysisWe present here an evaluation of the system outputon test data.
We further comment on some difficultaspects of the task and suggest some solutions.5.1 ResultsEvaluation consists of different layers, which weconsider separately.
The first task was to decidewhether an argument is implicit or not.
We wereable to identify 53.8% of all null instantiated ar-guments in text, which is lower than the recall of63.4% achieved by SEMAFOR (Chen et al, 2010),the best performing system in the challenge.
How-ever, in the following subtask of deciding whether animplicit argument is an INI or a DNI, we achievedan accuracy of 74.6% (vs. 54.7% of SEMAFOR,59even if our result is based on fewer proposed clas-sifications).
Note that the majority-class accuracyreported by Ruppenhofer et al (2010) is 50.8%.In Table 2 we further report precision, recall andF1 scores computed separately on all DNIs and allINIs automatically detected.
Precision correspondsto the percentage of null instantiations found (eitherINI or DNI) that are correctly labeled as such, whilerecall indicates the amount of INI or DNI that werecorrectly identified compared to the gold standardones.
Our approach does not show significant dif-ferences between the result obtained with INIs andDNIs, while the evaluation of SEMAFOR (betweenparenthesis) shows that its performance suffers fromlow recall as regards DNIs and low precision as re-gards INIs.P R F1DNI 0.39 (0.57) 0.43 (0.03) 0.41 (0.06)INI 0.46 (0.20) 0.38 (0.61) 0.42 (0.30)Table 2: Evaluation of INI/DNI identification.SEMAFOR performance between parenthesis.Another evaluation step concerns the binding ofDNIs with the corresponding antecedents by apply-ing the equation reported in Section 4.2.
Results areshown in Table 3:P R F1DNI 0.13 (0.25) 0.06 (0.01) 0.08 (0.02)Table 3: Evaluation of DNI resolution.
SEMAFOR per-formance between parenthesis.Although the binding quality still needs to be im-proved, two main factors have a negative impact onour performance, which do not depend on our al-gorithm: first, 9% of the DNIs we bound to an an-tecedent don?t have a referent in the gold standard.Second, 26% of the wrong assignments concern an-tecedents found for the Topic frame element in testsentences where the STATEMENT frame has beenannotated together with the overtly expressed coreFE Message.
In all these gold cases, Topic is notconsidered null instantiated if the Message FE is ex-plicit in the clause.
Therefore, we can conclude thatthe mistake done by our algorithm depends on themissing Excludes relation between Topic and Mes-sage, i.e.
a rule should be introduced saying thatone of the two roles is redundant (and not null in-stantiated) if the other is overtly expressed.5.2 Open issues related to our approachEven if with a small set of rules our approachachieved state-of-the-art results in the SemEval task,our performance clearly requires further improve-ments.
Indeed, we currently rely only on the back-ground knowledge about core FEs from FrameNet,combined with statistical observations about rolefillers acquired from the training set.
Additionalmorphological, syntactic, semantic and discourse in-formation could be exploited in different ways.
Forexample, since the passive voice of a verb can con-structionally license INIs, this kind of informationwould greatly improve our performance with verbalpredicates (i.e.
46% of all annotated predicates inthe test set).As for nominal predicates, consider for examplesentence (8) extracted from the test set:(8) ?Excuse the admirationJUDGMENT [of aconnoisseur]Evaluee,?
said [he]Cognizer.In this case, ?admiration?
is a nominal predicatewith two explicit FEs, namely Evaluee and Cog-nizer.
The JUDGMENT frame includes also the Rea-son core FE, which can be a candidate for a null in-stantiation.
In fact, it is annotated as INI in the goldstandard data, because in the previous sentences areason for such admiration is not mentioned.
How-ever, this could have been annotated as DNI as well,if only some specific quality of the person had beenpreviously introduced.
This shows that the currentsentence does not present any inherent characteris-tic motivating the presence of a definite instantia-tion.
In this case, a strategy based on some kind ofhistory list may be very helpful.
This could con-tain, for example, all subjects and direct objects pre-viously mentioned in text and selected according tosome relevance criteria, as in (Tonelli and Delmonte,2010).
A further improvement may derive from theintegration of an anaphora resolution step, as firstproposed by Palmer et al (1986) and more recentlyby Gerber and Chai (2010).605.3 Open issues related to the taskOther open issues are related to the specification ofthe task and to the nature of implicit entities, whichmake it difficult to account for this phenomenonfrom a computational point of view.
We report be-low the main issues that need to be tackled:INI Linking: Table 1 shows that 28% of DNIsin the test set are not linked to any referent.
Thisputs into question one of the main assumptions ofthe task, that is the connection between a definiteinstantiation and a referent.
In the test set, there arealso 14 cases of indefinite null instantiations (out of361) that are provided with a referent.
Consider forexample the following sentence with gold standardannotation, in which the INI label Path is actuallyinstantiated and refers to ?we?
:(9) (We)Path allowed [him]Theme to passTRAVERSINGbefore we had recovered our nerve.This again may be a controversial annotation choice,since the annotation guidelines of the task reportedthat ?in cases of indefinite omission, there need notbe any overt mention of an indefinite NP in the lin-guistic context nor does there have to be a referentof the kind denoted by the omitted argument in thephysical discourse setting?
(Ruppenhofer, 2010).Position of referent: Although we suggested thatthe History List may represent a good starting pointfor finding antecedents to DNIs, searching only inthe context preceding the current predicate is notenough because the referent can occur after suchpredicate.
Also, the predicate with a DNI and thereferent can be divided by a very large text span.
Inthe test data, 38% of the DNIs referent occur in thesame sentence of the predicate, while 14% are men-tioned after that (in a text span of max.
4 sentences).Another 38% of DNIs are resolved in a text spanpreceding the current predicate of max.
5 sentences,while the rest has a very far antecedent (up to 116sentences before the current predicate).
The notionof context where the antecedent should be searchedfor is clearly lacking an appropriate definition.Diversity of lexical fillers: In general, it is pos-sible to successfully obtain information about thelikely fillers of a missing FE from annotated datasets only in case all FE labels are semantically wellidentifiable: in fact many FE labels are devoid ofany specific associated meaning.
Furthermore, lex-ical fillers of a given semantic role in the FrameNetdata sets can be as diverse as possible.
For exam-ple, a complete search in the FrameNet database forthe FE Charges will reveal heads like ?possession,innocent, actions?, where the significant portion oftext addressed by the FE would be in the specifica-tion - i.e.
?possession of a gun?
etc.
Only in case ofhighly specialized FEs there will be some help in thesemantic characterization of a possible antecedent.6 ConclusionsIn this paper, we have described the phenomenonof null instantiated arguments according to theFrameNet paradigm and we have proposed a strat-egy for identifying implicit arguments and find-ing their antecedents, if any, using a linguistically-motivated approach.
We have evaluated our systemusing the test set developed for the SemEval task10 and we have discussed some problems in our ap-proach affecting its performance.
Besides, we havealso pointed out some issues related to the task defi-nition and to the general phenomenon of null instan-tiated arguments that make the identification taskchallenging from a computational point of view.
Wehave shed some light on the syntactic, semantic anddiscourse information that we believe are necessaryto successfully handle the task.In the future, we plan to improve on our bindingapproach by making our model more flexible.
Morespecifically, we currently treat DNI referents occur-ring before and after the sentence containing thepredicate as equally probable.
Instead, we shouldpenalize less those preceding the predicate becausethey are more frequent in the training set.
For thisreason, the number of observations for the candi-date head and the distance should be representedas different weighted features.
Another direction toexplore is to extend the training set to the wholeFrameNet resource and not just to the SemEvaldata set.
However, our approach based on the ob-servations of lexical fillers is very much domain-dependent, and a larger training set may introducetoo much variability in the heads.
An approach ex-ploiting some kind of generalization, for example bylinking the fillers to WordNet synsets as proposed by(Gerber and Chai, 2010), may be more appropriate.61ReferencesCollin F. Baker, Charles J. Fillmore, and J.
B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of Coling/ACL, Montreal, Quebec, Canada.C.
F. Baker, M. Ellsworth, and K. Erk.
2007.
Semeval-2007 task 10: Frame semantic structure extraction.
InProceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 99?104,Prague, CZ, June.Aljoscha Burchardt, Annette Frank, and Manfred Pinkal.2005.
Building text meaning representations fromcontextually related frames - a case study.
In Proceed-ings of the Sixth International Workshop on Computa-tional Semantics, Tilburg, NL.Desai Chen, Nathan Schneider, Dipanjan Das, andNoah A. Smith.
2010.
SEMAFOR: Frame ArgumentResolution with Log-Linear Models.
In Proceedingsof SemEval-2010: 5th International Workshop on Se-mantic Evaluations, pages 264?267, Uppsala, Swe-den.
Association for Computational Linguistics.Rodolfo Delmonte.
2009.
Understanding Implicit Enti-ties and Events with Getaruns.
In Proceedings of theIEEE International Conference on Semantic Comput-ing, pages 25?32, Berkeley, California.Charles J. Fillmore.
1986.
Pragmatically ControlledZero Anaphora.
In V. Nikiforidou, M. Vanllay,M.
Niepokuj, and D. Felder, editors, Proceedings ofthe XII Annual Meeting of the Berkeley Linguistics So-ciety, Berkeley, California.
BLS.Matthew Gerber and Joyce Y. Chai.
2010.
Beyond Nom-Bank: A Study of Implicit Arguments for NominalPredicates.
In Proceedings of the 48th annual meet-ing of the Association for Computational Linguistics(ACL-10), pages 1583?1592, Uppsala, Sweden.
Asso-ciation for Computational Linguistics.Adam Meyers.
2007.
Annotation guidelines for Nom-Bank - noun argument structure for PropBank.
Tech-nical report, New York University.M.
Palmer, D. Dahl, R. Passonneau, L. Hirschman,M.
Linebarger, and J. Dowding.
1986.
Recovering im-plicit information.
In Proceedings of ACL 1986, pages96?113.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Corpusof Semantic Roles.
Computational Linguistics, 31.Josef Ruppenhofer and Laura A. Michaelis.
2009.Frames predict the interpretation of lexical omissions.Submitted.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin F. Baker, and Martha Palmer.
2010.
SemEval-2010 Task 10: Linking Events and Their Participantsin Discourse.
In Proceedings of SemEval-2010: 5thInternational Workshop on Semantic Evaluations.Josef Ruppenhofer, 2010.
Annotation guidelines used forSemeval task 10 - Linking Events and Their Partici-pants in Discourse.
(manuscript).Sara Tonelli and Rodolfo Delmonte.
2010.
VENSES++:Adapting a deep semantic processing system to theidentification of null instantiations.
In Proceedingsof SemEval-2010: 5th International Workshop on Se-mantic Evaluations, Uppsala, Sweden.62
