Chinese Word Segmentation with Conditional Support Vector In-spired Markov ModelsYu-Chieh Wu1Dep.
of Computer Science andInformation Engineering;National Central University2Finance DepartmentMing Chuan UniversityTaipei, Taiwanbcbb@db.csie.ncu.edu.twJie-Chi YangGraduate Institute of Net-work LearningNational Central UniversityTaoyuan, Taiwanyang@cl.ncu.edu.twYue-Shi LeeDep.
of Computer Scienceand Information EngineeringMing Chuan UniversityTaoyuan, Taiwanleeys@mcu.edu.twAbstractIn this paper, we present the proposed me-thod of participating SIGHAN-2010 Chi-nese word segmentation bake-off.
In thisyear, our focus aims to quick train and testthe given data.
Unlike the most structurallearning algorithms, such as conditionalrandom fields, we design an in-house devel-opment conditional support vector Markovmodel (CMM) framework.
The method isvery quick to train and also show better per-formance in accuracy than CRF.
To give afair comparison, we compare our method toCRF with three additional tasks, namely,CoNLL-2000 chunking, SIGHAN-3 Chi-nese word segmentation.
The results wereencourage and indicated that the proposedCMM produces better not only accuracy butalso training time efficiency.
The official re-sults in SIGHAN-2010 also demonstratesthat our method perform very well in tradi-tional Chinese with fine-tuned features set.1 IntroductionSince 2006 Chinese word segmentation bakeoffin SIGHAN-3 (Levow, 2006), this is the thirdtime to join the competition (Wu et al, 2006,2007).
In this year, we join the SIGHAN bakeofftask in both traditional and simplified Chineseclosed word segmentation.
Unlike most westernlanguages, there is no explicit space betweenwords.
The goal of word segmentation is to iden-tify words given the sentence.
This techniqueprovides important features for downstream pur-poses.
Examples include Chinese part-of-speech(POS) tagging (Wu et al, 2007), Chinese worddependency parsing (Wu et al, 2007, 2008).With the rapid growth of structural learningalgorithms, such as conditional random fields(CRFs) (Lafferty et al, 2001) and maximum-margin Markov models (M3N) (Taskar et al,2003) have received a great attention and be-come a prominent learning algorithm to manysequential labeling tasks.
Examples include part-of-speech (POS) tagging (Shen et al, 2007) andsyntactic phrase chunking (Suzuki et al, 2007).The Chinese word segmentation can also betreated as a character-based tagging task in (Xueand Converse, 2002).
One feature of sequentiallabeling is that it aims at finding non-recursivechunk fragments in a given sentence.
Amongthese approaches, CRF has been wildly used inrecent SIGHAN bakeoff tasks (Jin and Chen,2008; Levow, 2006).Although these approaches do not suffer fromso-called label-bias problems (Lafferty et al,2001), one limitation is that they are inefficientto train with large-scale, especially large catego-ry data.
On the other hand, non-structural learn-ing approaches (e.g.
maximum entropy models)which learn local predictors usually cost muchbetter training time performance than structurallearning algorithms.
These methods condition onlocal context features and incorporate fix-lengthhistory information.
Although higher order fea-ture (longer history) maybe useful to some tasks,the exponential scaled inference time is also in-tractable in practice.Support vector machines (SVMs) which is oneof the state-of-the-art supervised learning algo-rithms have been widely employed as local clas-sifiers to many sequential labeling tasks (Takuand Matsumoto, 2001; Wu et al, 2006, 2008).Specially, the training time of linear kernel SVMwith either L1-norm (Joachims, 2006; Keerthi etal., 2008) or L2-norm (Keerthi and DeCoste,2005; Hsieh et al, 2008) can now be obtained inlinear time.
Even local classifier-based ap-proaches have the drawbacks of label-bias prob-lems, training nonstructural linear SVM is scala-ble to large-scale data.
By means of so-calledone-versus-all multiclass SVM training, it is alsoscalable to large-category data.In this paper, we present our Chinese wordsegmentation based on the proposed conditionalsupport vector Markov models for sequentiallabeling tasks, especially Chinese word segmen-tation.
Unlike structural learning algorithms, ourmethod can be simply trained without consider-ing the entire structures and hence the trainingtime scales linearly with the number of trainingexamples.
In this framework, to alleviate the easeof label-bias problems, the state transition proba-bility is ignored.
Instead, we merely utilize theproperty of label relationships between chunks(Wu et al, 2008).
To demonstrate our method,we compare to several well-known structurallearning algorithms, like CRF (Kudo et al, 2004),and SVM-HMM (Joachims et al, 2009) on twowell-known data, namely, CoNLL-2000 syntac-tic chunking, SIGHAN-3 Chinese word segmen-tation tasks.
By following this, we apply themodel to the Chinese word segmentation tasks ofSIGHAN-2010 this year.
The empirical resultsshowed that our method is not only fast but alsoachieving more superior accuracy than structurallearning methods.
In traditional Chinese, our me-thod also achieves the state-of-the-art perfor-mance in accuracy with fined-tune features.2 Conditional support vector MarkovmodelsTraditional conditional Markov models (CMM)is to assign the tag sequence which maximizesthe observation sequence.
),...,,|,...,,( 2121 nn ooosssPWhere si is the tag of word i.
For the first orderleft-to-right CMM, the chain rule decomposesthe probabilistic function as:?niiiinn ossPooosssP112121 ),|(),...,,|,...,,(       (1)Therefore, we can employ a local classifier topredict ),|( 1 iii ossP  and the optimal tag sequencecan be efficiently searched by using conventionalViterbi algorithm.The graphic illustration of the K-th order left-to-right CMM is shown in Figure 1.
The chainprobability decompositions of the other K-th or-der CMM in Figure 1 are:?niii osPosP1)|(),(                                           (2)?niiii sosPosP21),|(),(                                    (3)?niiiii ssosPosP321 ),,|(),(                              (4)?niiiii ssosPosP311 )?,,|(),(                              (5)Equations (2), (3), and (4) are merely standardzero, first and second order decompositions,while equation (5) is the proposed greedy secondorder CMM decomposition which will be dis-cussed in next section.Figure 1: K-th order conditional Markov models: (a)the standard 0(zero) order CMM, (b) first order CMM,(c) second order CMM, and (d) the proposed secondorder CMMThe above decompositions merge the transi-tion and emission probability with single func-tion.
McCallum et al (2000) further combinedthe locally trained maximum entropy with theinfered transition score.
However, our condition-al support vector Markov models make differentchain probability.
We replace the original transi-tion probability with transition validity score, i.e.
?niiiii osPssPosP21 )|()|(~),(                           (6)?niiiiiii ssosPssPosP3111 )?,,|()|(?
),(               (7)The transition validity score is merely a Boo-lean flag which indicates the relationships be-tween two neighbor labels.
Equation (6) and (7)are zero-order and our second order chain prob-abilities.
We will introduce the proposed infe-rence algorithm and how to obtain the transitionvalidity score automatically without concerningthe change of chunk representation.2.1 Tag transitionsIn this paper, we do not explicitly adopt the statetransitions for our CMM.
Instead, a chunk-relation pair is used.
Nevertheless, one importantproperty to sequential chunk labeling is that thereis only one phrase type in a chunk.
For example,if the previous word is tagged as begin of nounphrase (B-NP), the current word must not be endof the other phrase (E-VP, E-PP, etc).
Therefore,we only model relationships between chunk tagsto generate valid phrase structure.Wu et al (2007, 2008) presented an automaticchunk pair relation construction algorithm whichcan handle so-called IOB1/IOB2/IOE1/IOE2(Kudo and Matsumoto, 2001) chunk representa-tion structures with either left-to-right or right-to-left directions.
Here, we extend this idea and ge-neralize to fit to more chunk tags.
That is we canmodel the S-tag, B2, B3 tags with dividing theleading tags into two categories.
For details canrefer the literatures.3 Empirical ResultsThree large-scale and large-category dataset isused to evaluate the proposed method, namely,CoNLL-2000 syntactic chunking (Tjong KimSang and Buchholz, 2000), Chinese POS tagging,and three of SIGHAN-3 word segmentation tasks.Table 1 shows the statistics of those datasets.CoNLL-2000 chunking task is a well-knownand widely evaluated in many literatures (Suzukiet al, 2007; Ando and Zhang, 2005; Kudo andMatsumoto, 2001; Wu et al, 2008; Daum?
IIIand Marcu, 2005).
The training data was derivedfrom Treebank WSJ section 15-18 while section20 was used for testing.
The goal is to find thenon-recursive phrase structures in a sentence,such as noun phrase (NP), verb phrase (VP), etc.There are 11 phrase types in this dataset.
We fol-low the previous best settings for SVMs (Kudoand Matsumoto, 2001; Wu et al, 2008).
TheIOE2 is used to represent the phrase structureand tagged the data with backward direction.The training and testing data of the ChinesePOS tagging is mainly derived from the Aca-demic Sinica?s balanced corpus (version 3.0).Seventy-five percent out of the data is used fortraining while the remaining 25% is used for test-ing.
However, the task of the Chinese POS tag-ging is very different from classical English POStagging in that there is no word boundary infor-mation in Chinese text.
To achieve this, Ng andLow (2004) gave a successful study on ChinesePOS tagging.
Just as English phrase chunking,the IOB-tags can be used to represent the Chi-nese word and its part-of-speech tag.
For exam-ple, the tag B-ADJ means the first character of aChinese word which POS tag is ADJ (adjective).n this task, we simply use the IOB2 to representthe chunk structure.
In this way, the tagger needsto recognize the chunk tag by considering 118(59*2) categories at once.As discussed in (Zhou and Kit, 2007), usingmore complex chunk representation bring bettersegmentation accuracy in several Chinese wordsegmentation benchmarks.
It is very useful inparticular to represent long Chinese word (in par-ticular proper nouns).
By following this line, weapply the six tags B, BI, I, IE, E, and S torepresent the Chinese word.
BI and IE are theinterior after begin and interior before end of achunk.
B/I/E/S tags indicate the be-gin/interior/end/single of a chunk.
Figure 2 liststhe used feature set in both experiments.3.1 SettingsWe included the Liblinear with square loss(Hsieh et al, 2008) into our conditional Markovmodels as classification algorithms.
In basic, theSVM was designed for binary classificationproblems.
To port to multiclass problems, weadopted the well-known one-versus-all (OVA)method.
One good property of OVA is that pa-rameter estimation process can be trained indivi-FeaturetypeCoNLL-2000 SIGHAN-3Unigram w-2~w+2 w-2~w+2Bigram (w-2,w-1),(w-1,w0),(w0,w+1),(w+1,w+2),(w+1,w-1)(w-2,w-1),(w-1,w0),(w0,w+1),(w+1,w+2),(w+1,w-1)POS p-2~p+2POS bigram (p-2,p-1),(p-1,p0),(p0,p+1),(p+1,p+2),(p+1,p-1)POS trigram (p-2,p-1,p0),(p-1,p0,p+1),(p-3,p-2,p-1),(p0,p+1,p+2), (p+1,p+2,p+3)(Word+POS)bigram(w-1,p0),(w-2,p-1) (w0,p+1),(w+1,p+2)Otherfeatures2~4 suffix letters AV feature of 2~6 grams(Zhou and Kit, 2007)2~4 prefix lettersOrthographic feature(Wu et al, 2008)Figure 2: Feature templates used in experimentsdually.
This is in particularly useful to the taskswhich involve training large number of featuresand categories (Wu et al, 2008).
To obtain theprobability output from SVM, we employ thesigmoid function with fixed parameter A=-2 andB=0 as (Platt, 1999).3.2 Comparison to structural learningThe overall experimental results are summarizedin Table 1.
&ROXPQ ?$OO? GHQRWHV DV WKH )?score of all chunk typeVZKLOH ?13? LV WKH )?score of the noun phrase only.
The final two col-umns list the entire training and testing times.As shown in Table 1, it is surprising that theproposed CMM outperforms the other structurallearning methods, CRF and SVM-HMM.
Interms of training time, our method shows sub-stantial faster than CRF.
However, in terms oftesting time, our method is worse than CRF.
Themain reason is that we do not optimize the codeand implementation.
We trust this can be furtherimproved.Table 1: Syntactic chunking results of the proposedCMM and the selected structural learning methods.Method All NP Training Time Testing TimeOur method 94.51 94.95 0.15 hr 13.72 sCRF 93.67 93.93 0.88 hr 6.20 sSVM-HMM 93.90 94.20 0.20 hr 13.60 sTable 2 shows the experimental results of theSIGHAN-3 bake-off tasks.
We ran and con-ducted the experiments with UPUC, MSRA, andCityU datasets.
The final two rows in Table 5 listthe top 1 and 2 scores of published papers.Here, the SVM-HMM still suffer from the sca-lability problems.
Similar to the findings in theChinese POS tagging task, the zero-order CMMachieved the optimal accuracy among first-order,full second order and the proposed inference al-gorithms.
The training time is still very efficientfor most CMMs.
In comparison to CRF, our me-thod did clearly perform better accuracy (ex-cepted for the CityU) and require much lesstraining time.
For example, for the CityU dataset,our 0-order CMM took less than 15 minutes totrain, while the CRF takes 4.34 hours in training.However, we observe that our CMM yieldedbetter testing time speed than CRF in this task.We further exploit the trained SVM models andfound that the produced weights were not asdense as CRF which produces many nonzeroweights per category.
In addition, we observedthat our implementation worked very efficient inthe small category tasks.For the three datasets, our method producedvery competitive results as previous best ap-proach which also made use of CRF as classifiers.Although we use the same techniques to de-rive global features (assessor variety (AV) fea-ture with 2~6 grams) from both training and test-ing data, our CMMs and the conducted CRFcould not perform as well as (Zhou and Kit,2007).
In our experiments, both CRF and CMMsreceived the same training set.
Hence the CRFand our CMMs is comparable in this experiment.3.3 Official Results in SIGHAN-2010To apply CMM to SIGHAN-2010, we design thefollowing strategy.
First the classifier parameters,Table 2: SIGHAN-3 word segmentation resultsSIGHAN-3 UPUC MSRA CityUMethod F?TrainingTimeTestingTimeF?TrainingTimeTestingTimeF?TrainingTimeTestingTimeOur method 93.86 0.06 hr 15.15 s 96.22 0.45 hr 15.41 s 97.26 0.26 hr 25.32 sCRF 93.76 1.17 hr 23.48 s 96.11 3.63 hr 17.06 s 97.29 4.34 hr 31.29 sSVM-HMM Out-of-memory Out-of-memory Out-of-memoryBest approach (Zhou andKit, 2007)94.28 N/A N/A 96.34 N/A N/A 97.43 N/A N/ASecond best approach 93.30 N/A N/A 96.30 N/A N/A 97.20 N/A N/ATable 3: Official evaluation results of the traditional and simplified Chinese word segmentation tasksTaskLiterature ComputerRecall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RRTraditional 0.942 0.942 0.942 0.788 0.958 0.948 0.957 0.952 0.666 0.977Simplified 0.936 0.932 0.934 0.564 0.964 0.915 0.915 0.915 0.594 0.972TaskMedicine FinanceRecall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RRTraditional 0.953 0.957 0.955 0.798 0.966 0.964 0.962 0.963 0.812 0.975Simplified 0.933 0.915 0.924 0.642 0.969 0.945 0.941 0.943 0.666 0.972feature set should be improved.
To achieve this,1/4 of the training data was used as developmentset, while the remaining 3/4 training data wasused to train the classifier.
Second, we combinemulti-classifier to enhance the accuracy.
TheCRF and our CMM with basic feature set weretrained to predict the initial labels of the testingdata.
Then the predicted labels were included asfeatures to train the final-stage classifier.
Thefinal classifier is still our CMM.
Third, the post-processing method (Low et al, 2005) is em-ployed to enhance the unknown word segmenta-tion.Table 4 lists the empirical results of the devel-opment set.
By validate with development data,we found that C=1.25 and use the E-BIES repre-sentation method (Wu et al, 2008) yields betteraccuracy than B-BIES (Zhou and Kit, 2007).Meanwhile, CRF seems to be suitable for B-BIES representation method.The classifier parameters were fixed and thenwe try to search the optimal feature set via theincremental add-and-check method.
That is, weuse the initial feature set as basis and add onefeature type from the pool and verify the good-ness of the feature with the development data.Figure 3 figures out the used features of eachpass.In this year, the process was completely run-through for the traditional Chinese task.
Unfor-tunately we have insufficient time to apply thesame technique to Simplified Chinese task.
Table3 lists the official results in the SIGHAN 2010Chinese word segmentation bake-off.Table 4: Empirical results of the development set ofsingle CRF and our CMMDevelopmentdatasetTraditionalChineseSimplifiedChineseB-BIES E-BIES B-BIES E-BIESOur method 97.40 97.42 97.34 97.37CRF 97.07 97.10 97.07 96.964 ConclusionIn this paper, we investigate the issues of sequen-tial chunk labeling and present the conditionalsupport vector Markov models for this purpose.The experiments were conducted with two well-known datasets, includes CoNLL-2000 textchunking and SIGHAN-3 Chinese word segmen-tation.
The experimental results showed that ourmethod scales very well while achieving surpris-ing good accuracy than structural learning me-thods.
On the SIGHAN-3 task, the proposed me-thod outperformed CRF, while substantially re-duced the training time.
We also apply such me-thod to the SIGHAN-2010 traditional Chinesesegmentation with fined tuned feature set.
Theresult was also encouraged.
Our approach ob-tains the best accuracy in this task.
In terms ofSimplified Chinese, we achieve mid-rank placedue to the very limited time-constraint.
In thefuture, we plan to completely adopt this methodto the Simplified Chinese word segmentationwith the elaborated feature selection metrics andthe same post-processing method.The full online demonstration of the proposedconditional support vector Markov models canbe found at the web site1.FeatureNamePass1: CRF/CMM Pass2: CMMCharacter w-2~w+2 Feature set ofPass1CharacterN-gram(w-2,w-1),(w-1,w0),(w0,w+1),(w+1,w+2),(w+1,w-1)SpecialCharacterflags (Lowet al, 2005)w-2~w+2Others 2AV feature and its 2-gram combinations2AV featureand its 2-gramand 3-gramcombinationsFutureflags1N/A t+1, t+2, t+3,(t0,t+2),(t+1,t+2),(w0,t+1),(w0,t+2)1Future flags: the predicted tags of previous classifierFigure 3: Feature templates used in experimentsReferencesRie K. Ando, and Tong Zhang.
2005.
A high-performance semi-supervised learning method fortext chunking, In Proc.
of ACL, pp.
1-9.Bernhard E. Boser, Isabelle M. Guyon, Vladimir N.Vapnik.
1992.
A training algorithm for optimalmargin classifiers, In Proc.
of COLT, pp.
144-152.Andrew McCallum, Dayne Freitag, and Fernando C.N.
Pereira.
2000.
Maximum entropy Markovmodels for information extraction andsegmentation, In Proc.
of  ICML, pp.
591.598.Hal Daum?
III and Daniel Marcu.
2005.
Learning assearch optimization: approximate large marginmethods for structured prediction, In Proc.
ofICML, pp.
169-176.Guangjin Jin and Xiao Chen.
2008.The Fourth International Chinese Language Processing Bakeoff: Chinese Word Segmentation NamedEntity Recognition and Chinese POS Tagging.
InProc.
of the SIGHAN Workshop on ChineseLanguage Processing, pp.
69-81.1 http://140.115.112.118/bcbb/Chunking.htmThorsten Joachims.
2006.
Training linear SVMs inlinear time, In Proc.
of KDD, pp.
217-226.Thorsten Joachims, Thomas Finley, and Chun-NamYu.
2009.
Cutting-Plane Training of StructuralSVMs, Machine Learning Journal, to appear.Sathiya Keerthi and Dennis DeCoste.
2005.
Amodified finite Newton method for fast solution oflarge scale linear SVMs, JMLR, 6: 341-361.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In Proc.
of NAACL,pp.
192-199.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis, In Proc.
ofEMNLP, pp.
230-237.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields:probabilistic models for segmenting and labelingsequence data, In Proc.
of ICML, pp.
282-289.Gina-Anne Levow.
2006.
The third internationalChinese language processing bakeoff: Wordsegmentation and named entity recognition.
InProc.
of the SIGHAN Workshop on ChineseLanguage Processing, pp.
108?117.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.2005.
A maximum entropy approach to Chineseword segmentation.
In Proc.
of the SIGHANWorkshop on Chinese Language Processing, pp.161-164.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging.
one-at-a-time or all-at-once?word-based or character-based?
In Proc.
ofEMNLP, pp.
277-284.John Platt.
1999.
Probabilistic outputs for supportvector machines and comparisons to regularizedlikelihood methods, In Advances in Large MarginClassifiers.Jun Suzuki, Akinori Fujino, and Hideki Isozaki.
2007.Semi-supervised structural output learning basedon a hybrid generative and discriminative approach,In Proc.
of EMNLP-CoNLL, pp.
791-800.Jun Suzuki and Hideki Isozaki.
2008.
Semi-Supervised Sequential Labeling and Segmentationusing Giga-word Scale Unlabeled Data.
In Proc.
ofACL, pp.
665-673.Ben Taskar, Carlos Guestrin, and Daphne Koller.2003.
Max-margin Markov networks, In Proc.
ofNIPS.Eric F. Tjong Kim Sang, and Sabine Buchholz.
2000.Introduction to the CoNLL-2000 shared task:chunking.
In Proc.
of CoNLL, pp.
127-132.Yu-Chieh Wu, Jie-Chi Yang, Yue-Shi Lee, andShow-Jane Yen.
2006.
Efficient and robust phrasechunking using support vector machines, In AsiaInformation Retrieval Symposium (AIRS), pp.
350-361.Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee.
2008.Description of the NCU Chinese WordSegmentation and Part-of-Speech Tagging forSIGHAN Bakeoff 2008, In Proc.
of the SIGHANWorkshop on Chinese Language Processing, pp.161-166, 2008.Yu-Chieh Wu, Yue-Shi Lee, and Jie-Chi Yang.Robust and efficient multiclass SVM models forphrase pattern recognition, Pattern recognition,41(9): 2874-2889, 2008.Yu-Chieh Wu, Jie-Chi Yang, and Qian Xiang Lin.2006.
Description of the NCU Chinese wordsegmentation and named entity recognition Systemfor SIGHAN Bakeoff 2006, In Proc.
of theSIGHAN Workshop on Chinese LanguageProcessing, pp.
209-212.Yu-Chieh Wu, Yue-Shi Lee, and Jie-Chi Yang.
2008.Robust and efficient Chinese word dependencyanalysis with linear kernel support vector machines,In Proc.
of the COLING, pp.
135-138.Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee.
2007.Multilingual deterministic dependency parsingframework using modified finite Newton methodSupport Vector Machines.
In Proc.
of theEMNLP/CoNLL, pp.1175-1181.Tong Zhang, Fred Damerau, and David Johnson.2002.
Text chunking based on a generalizationWinnow, JMLR, 2: 615-637.Hai Zhao and Chunyu Kit.
2007.
Incorporating globalinformation into supervised learning for Chineseword segmentation, In Proc.
of PACLIC, pp.66-74.
