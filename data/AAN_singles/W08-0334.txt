Proceedings of the Third Workshop on Statistical Machine Translation, pages 208?215,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAbstractThis paper presents a technique for class-dependent decoding for statistical machinetranslation (SMT).
The approach differs fromprevious methods of class-dependent transla-tion in that the class-dependent forms of allmodels are integrated directly into the decod-ing process.
We employ probabilistic mixtureweights between models that can change dy-namically on a segment-by-segment basisdepending on the characteristics of the sourcesegment.
The effectiveness of this approach isdemonstrated by evaluating its performanceon travel conversation data.
We used the ap-proach to tackle the translation of questionsand declarative sentences using class-dependent models.
To achieve this, our systemintegrated two sets of models specifically builtto deal with sentences that fall into one of twoclasses of dialog sentence: questions and dec-larations, with a third set of models built tohandle the general class.
The technique wasthoroughly evaluated on data from 17 lan-guage pairs using 6 machine translationevaluation metrics.
We found the results werecorpus-dependent, but in most cases our sys-tem was able to improve translation perform-ance, and for some languages the improve-ments were substantial.1 IntroductionTopic-dependent  modeling has proven to be aneffective way to improve quality the quality ofmodels in speech recognition (Iyer and Osendorf,1994; Carter, 1994).
Recently, experiments in thefield of machine translation (Hasan and Ney, 2005;Yamamoto and Sumita, 2007; Finch et al 2007,Foster and Kuhn, 2007) have shown that class-specific models are also useful for translation.In the method proposed by Yamamoto and Su-mita (2007), topic dependency was implementedby partitioning the data into sets before the decod-ing process commenced, and subsequently decod-ing these sets independently using different modelsthat were specific to the class predicted for thesource sentence by a classifier that  was run overthe source sentences in a pre-processing pass.
Ourapproach is in many ways a generalization of thiswork.
Our technique allows the use of multiple-model sets within the decoding process itself.
Thecontributions of each model set  can be controlleddynamically during the decoding through a set ofinterpolation weights.
These weights can bechanged on a sentence-by-sentence basis.
The pre-vious approach is, in essence, the case where theinterpolation weights are either 1 (indicating thatthe source sentence is the same topic as the model)or 0 (the source sentence is a different  topic).
Oneadvantage of our proposed technique is that it is asoft approach.
That is, the source sentence can be-long to multiple classes to varying degrees.
In thisrespect our approach is similar to that  of Foster andKuhn (2007), however we used a probabilisticclassifier to determine a vector of probabilities rep-resenting class-membership, rather than distance-based weights.
These probabilities were used di-rectly as the mixture weights for the respectivemodels in an interpolated model-set.
A second dif-ference between our approach and that of Fosterand Kuhn, is that  we include a general model builtfrom all of the data along with the set  of class-specific models.Our approach differs from all previous ap-proaches in the models that are class-dependent.Hasan and Ney (2005) used only a class-dependentlanguage model.
Both Yamamoto and Sumita(2007) and Foster and Kuhn (2007), extended thisto include the translation model.
In our approachwe combine all of the models, including the distor-tion and target length models, in the SMT systemwithin  a single framework.The contribution of this paper is two-fold.
Thefirst  is the proposal of a technique for combiningDynamic Model Interpolation for Statistical Machine TranslationAndrew FINCHNICT?-ATR?Kyoto, Japanandrew.finch@atr.jpEiichiro SUMITANICT?-ATR?Kyoto, Japaneiichiro.sumita@atr.jp?
National Institute for Science and Technology?
Advanced Telecommunications Research Laboratories208multiple SMT systems in a weighted manner toallow probabilistic soft  weighting between topic-dependent models for all models in the system.The second is the application of this technique toimprove the quality of dialog systems by buildingand combing class-based models for interrogativeand declarative sentences.For the purposes of this paper, we wish to makethe distinction between interrogative sentences andthose which are not.
For the sake of simplicity ofexpression we will call those sentences which areinterrogative, questions and those which are not,declarations for the remainder of this article.The techniques proposed here were evaluated ona variety of different languages.
We enumeratethem below as a key: Arabic (ar), Danish (da),German (de), English (en), Spanish (es), French(fr), Indonesian (Malay) (id), Italian (it), Japanese(ja), Korean (ko), Malaysian (Malay) (ms), Dutch(nl), Portugese (pt), Russian (ru), Thai (th), Viet-namese (vi) and Chinese (zh).2 System Overview2.1 Experimental DataTo evaluate the proposed technique, we conductedexperiments on a travel conversation corpus.
Theexperimental corpus was the travel arrangementtask of the BTEC corpus (Kikui et al, 2003) andused English as the target  and each of the otherlanguages as source languages.
The training, de-velopment, and evaluation corpus statistics areshown in Table 1.
The evaluation corpus had six-teen reference translations per sentence.
This train-ing corpus was also used in the IWSLT06 Evalua-tion Campaign on Spoken Language Translation(Paul 2006) J-E open track, and the evaluation cor-pus was used as the IWSLT05 evaluation set.2.2 System ArchitectureFigure 1 shows the overall structure of our system.We used punctuation (a sentence-final ???
charac-ter) on the target-side as the ground truth as to theclass of the target sentence.
Neither punctuationnor case information was used for any other pur-pose in the experiments.
The data were partitionedinto classes, and further sub-divided into trainingand development sets for each class.
1000 sen-tences were set  aside as development data, and theremainder was used for training.
Three completeSMT  systems were built: one for each class, andone on the data from both classes.
A probabilisticclassifier (described in the next section) was alsotrained from the full set of training data.The machine translation decoder used is able tolinearly interpolate all of the models models fromFigure 1.
The architecture of the class-based SMT system used in our experimentsModel interpolating decoderLabeled bilingual corpusAll databilingual declarationsProbabilisticClassifierQuestion-specificSMT SystemGeneral SMT SystemDeclaration-specificSMT SystemDEVTRAINbilingual questionsDEVTRAINDEV TRAINUnlabeled testcorpusGeneral weight(fixed during decoding)Question weight(dynamic)Declaration weight(dynamic)sentencesentencesentence209all of the sub-systems according to a vector of in-terpolation weights supplied for each source wordsequence to be decoded.
To do this, prior to thesearch, the decoder must  first merge the phrase-tables from each sub-system.
Every phrase from allof the phrase-tables is used during the decoding.Phrases that  occur in one sub-system?s table, butdo not  occur in another sub-system?s table will beused, but will receive no support  (zero probability)from those sub-systems that did not acquire thisphrase during training.
The search process pro-ceeds as in a typical multi-stack phrase-based de-coder.
The weight for the general model was set  bytuning the parameter on the general developmentset in order to maximize performance in terms ofBLEU score.
This weight determines the amountof probability mass to be assigned to the generalmodel, and it  remains fixed during the decoding ofall sentences.
The remainder of the probabilitymass is divided among the class-specific modelsdynamically sentence-by-sentence at  run-time.
Theproportion that  is assigned to each class is simplythe class membership probability of the source se-quence assigned by the classifier.3 Question Prediction3.1 Outline of the ProblemGiven a source sentence of a particular class (inter-rogative or declarative in our case), we wish toensure that  the target  sentence generated is of anappropriate class.
Note that this does not necessar-ily mean that given a question in the source, aquestion should be generated in the target.
How-ever, it seems reasonable to assume that, intuitivelyat  least, one should be able to generate a targetquestion from a source question, and a target decla-ration from a source declaration.
This is reason-able because the role of a machine translation en-gine is not  to be able to generate every possibletranslation from the source, but to be able to gener-ate one acceptable translation.
This assumptionleads us to two plausible ways to proceed.1.
To predict the class of the source sentence, anduse this to constrain the decoding process usedto generate the target2.
To predict the class of the targetIn our experiments, we chose the secondmethod, as it  seemed the most correct, but  feelthere is some merit in both strategies.3.2 The Maximum Entropy ClassifierWe used a Maximum Entropy (ME) classifier todetermine which class to which the input sourcesentence belongs using a set  of lexical features.That is, we use the classifier to set  the mixtureweights of the class-specific models.
In recentyears such classifiers have produced powerfulmodels utilizing large numbers of lexical featuresin a variety of natural language processing tasks,for example Rosenfeld (1996).
An ME model is anexponential model with the following form:where:t is the class being predicted;c is the context of t;?
is a normalization coefficient;K is the number of features in the model;?kis the weight of feature fk;fkare binary feature functions;p0is the default modelp(t, c) = ?K?k=0?fk(c,t)k p0Questions + Decls.
Questions Declarations TestTrain Dev Train Dev Train DevSentences 161317 1000 69684 1000 90633 1000 510Words1001671 6112 445676 6547 549375 6185 3169Table 1.
The corpus statistics of the target language corpus (en).
The number of sentences is the same asthese values for all source languaes.
The number of words in the source language differs, and dependson the segmentation granularity.210We used the set of all n-grams (n?3) occurringin the source sentences as features to predict  thesentence?s class.
Additionally we introduced be-ginning of sentence tokens (<s>) and end of sen-tence tokens into the word sequence to distinguishn-grams occurring at the start and end of sentencesfrom those occurring within the sentence.
This wasbased on the observation that ?question words?
orwords that indicate that the sentence is a questionwill frequently be found either at  the start of thesentence (as in the wh- <what, where, when>words in English or the -kah words in Malay <a-pakah, dimanakah, kapankah>), or at the end of thesentence (for example the Japanese ?ka?
or theChinese ?ma?).
In fact, in earlier models we usedfeatures consisting of n-grams occurring only atthe start  and end of the source sentence.
Theseclassifiers performed quite well (approximately 4%lower than the classifiers that used features fromall of the n-grams in the source), but  an erroranalysis showed that  n-grams from the interior ofthe sentence were necessary to handle sentencessuch as ?excuse me please where is ...?.
A simpleexample sentence and the set of features generatedfrom the sentence is shown in Figure 2.We used the ME modeling toolkit of (Zhang,2004) to implement our ME models.
The modelswere trained by using L-BFGS parameter estima-tion, and a Gaussian prior was used for smoothingduring training.3.3 Forcing the target to conformBefore adopting the mixture-based approach setout in this paper, we first pursued an obvious andintuitively appealing way of using this classifier.We applied it as a filter to the output of the de-coder, to force source sentences that the classifierpredicts should generate questions in the target toactually generate questions in the target.
This ap-proach was unsuccessful due to a number of issues.We took the n-best  output  from the decoder andselected the highest translation hypothesis on thelist that  had agreement  on class according to sourceand target  classifiers.
The issues we encounteredincluded, too much similarity in the n-best hy-potheses, errors of the MT system were correlatedwith errors of the classifier, and the number ofcases that were corrected by the system was small<2%.
As a consequence, the method proposed inthis paper was preferred.4 Experiments4.1 Experimental ConditionsDecoderThe decoder used to in the experiments, CleopA-TRa is an in-house phrase-based statistical decoderthat can operate on the same principles as thePHARAOH (Koehn, 2004) and MOSES (Koehn etSourceLanguageEnglishPunctuationOwnPunctuationar 98.0 N/Ada 97.3 98.0de 98.1 98.6en 98.9 98.9es 96.3 96.7fr 97.7 98.7id 97.9 98.5it 94.9 95.4ja 94.1 N/Ako 94.2 99.4ms 98.1 99.0nl 98.1 99.0pt 96.2 96.0ru 95.9 96.6th 98.2 N/Avi 97.7 98.0zh 93.2 98.8Table 2.
The classifcation accuracy (%) of theclassifier used to predict whether or not an inputsentence either is or should give rise to a question inthe target.<s> where is the<s> where is<s> where is the is the station </s>is the station </s>the station </s>Figure 2.
The set of n-gram (n?3) features extractedfrom the sentence <s> where is the station </s> foruse as predicates in the ME model to predict targetsentence class.211al, 2007) decoders.
The decoder was configured toproduce near-identical output to MOSES for theseexperiments.
The decoder was modified in order tohandle multiple-sets of models, accept  weightedinput, and to incorporate the dynamic interpolationprocess during the decoding.Practical IssuesPerhaps the largest concerns about the proposedapproach come from the heavy resource require-ments that could potentially occur when dealingwith large numbers of models.
However, one im-portant characteristic of the decoder used in ourexperiments is its ability to leave its models ondisk, loading only the parts of the models neces-Source BLEU NIST WER PER GTM METEORar0.4457(0.00)8.9386(0.00)0.4458(0.00)0.3742(0.00)0.7469(0.00)0.6766(0.00)da0.6640(0.64)11.4500(1.64)0.2560(0.08)0.2174(2.42)0.8338(0.68)0.8154(1.23)de0.6642(0.79)11.4107(0.44)0.2606(2.18)0.2105(0.14)0.8348(-0.13)0.8132(-0.07)es0.7345(0.00)12.1384(0.00)0.2117(0.00)0.1668(0.00)0.8519(0.00)0.8541(0.00)fr0.6666(0.95)11.7443(0.63)0.2548(4.82)0.2172(6.50)0.8408(0.48)0.8293(1.29)id0.5295(9.56)10.3459(4.11)0.3899(21.17)0.3239(4.65)0.7960(1.35)0.7521(2.35)it0.6702(1.01)11.5604(0.41)0.2590(3.25)0.2090(0.62)0.8351(0.36)0.8171(0.05)ja0.5971(3.47)10.6346(2.56)0.3779(5.53)0.2842(2.80)0.8125(0.74)0.7669(0.67)ko0.5898(1.78)10.2151(1.31)0.3891(0.74)0.3138(-0.10)0.7880(0.36)0.7397(0.35)ms0.5102(10.19)9.9775(2.75)0.4058(18.53)0.3355(3.59)0.7815(0.18)0.7247(2.49)nl0.6906(2.55)11.9092(1.47)0.2415(3.21)0.1872(1.73)0.8548(0.39)0.8399(0.36)pt0.6623(0.35)11.6913(0.26)0.2549(2.52)0.2110(2.68)0.8396(0.02)0.8265(-0.07)ru0.5877(0.34)10.1233(-1.10)0.3447(1.99)0.2928(1.71)0.7900(0.15)0.7537(-0.40)th0.4857(1.50)9.5901(1.17)0.4883(-0.23)0.3579(2.03)0.7608(0.45)0.7104(1.23)vi0.5118(0.67)9.8588(1.85)0.4274(-0.05)0.3301(0.12)0.7806(1.05)0.7254(0.43)zh0.5742(0.00)10.1263(0.00)0.3937(0.00)0.3172(0.00)0.7936(0.00)0.7343(0.00)Table 3.
Performance results translating from a number of source languages into English.
Figures in parentheses arethe percentage improvement in the score relative to the original score.
Bold-bordered cells indicate those conditionswhere performance degraded.
White cells indicate the proposed system?s performance is significanly different fromthe baseline (using 2000-sample bootstrap resampling with a 95% confidence level).
TER scores were not tested forsignificance due to technical difficulties.
ar, es and zh were also omitted since the systems were identical.212sary to decode the sentence in hand.
This reducedthe memory overhead considerably when loadingmultiple models, without noticeably affecting de-coding time.
Moreover, it is also possible to pre-compute the interpolated probabilities for most ofthe models for each sentence before the searchcommences, reducing both search memory andprocessing time.Decoding ConditionsFor tuning of the decoder's parameters, minimumerror training (Och 2003) with respect  to the BLEUscore using was conducted using the respectivedevelopment  corpus.
A 5-gram language model,built using the SRI language modeling toolkit(Stolcke, 1999) with Witten-Bell smoothing wasused.
The model included a length model, and alsothe simple distance-based distortion model used bythe PHARAOH decoder (Koehn, 2004).Source Baseline No Classifier Hard Proposedar 0.4457 (0.00) 0.4457 (0.00) 0.4457 (0.00) 0.4457da 0.6598 (0.64)0.6647 (-0.11)0.6591 (0.74) 0.664de 0.6590 (0.79)0.6651 (-0.14)0.6634 (0.12) 0.6642es 0.7345 (0.00)0.7345 (0.00)0.7345 (0.00) 0.7345fr 0.6603 (0.95) 0.6594 (1.09) 0.6605 (0.92) 0.6666id 0.4833 (9.56) 0.5029 (5.29) 0.5276 (0.36) 0.5295it 0.6635 (1.01) 0.6660 (0.63) 0.6644 (0.87) 0.6702ja 0.5771 (3.47) 0.5796 (3.02) 0.5667 (5.36) 0.5971ko 0.5795 (1.78) 0.5837 (1.05) 0.5922 (-0.41) 0.5898ms 0.4630 (10.19) 0.5015 (1.73) 0.5057 (0.89) 0.5102nl 0.6734 (2.55) 0.6902 (0.06) 0.6879 (0.39) 0.6906pt 0.6600 (0.35)0.6643 (-0.30)0.6598 (0.38) 0.6623ru 0.5857 (0.34)0.5885 (-0.14)0.5844 (0.56) 0.5877th 0.4785 (1.50)0.4815 (0.87)0.4831 (0.54) 0.4857vi 0.5084 (0.67) 0.5095 (0.45) 0.5041 (1.53) 0.5118zh 0.5742 (0.00) 0.5742 (0.00) 0.5742 (0.00) 0.5742Table 4.
Performance results comaparing our proposes method with other techniques.
The column labeled ?Baseline?is the same as in Table 3, for reference.
The column lableled ?No Classifier?, is the same system as our proposedmethod, except that the classifier was replaced with a default model that assigned a class membership probability of0.5 in every case.
The column labeled ?Hard?
corresponds to a system that used hard weights (either 1 or 0) for theclass-dependent models.
The column labeled ?Proposed?
are the results from our proposed method.
Figures inparentheses represent the percentage improvement of the proposed method?s score relative to the alternative method.Cells with bold borders indicate those conditions where performance was degraded.213Tuning the interpolation weightsThe interpolation weights were tuned by maximiz-ing the BLEU score on the development set  over aset of weights ranging from 0 to 1 in increments of0.1.
Figure 1 shows the behavior of two of ourmodels with respect to their weight parameter.Evaluation schemesTo obtain a balanced view of the merits of our pro-posed approach, in our experiments we used 6evaluation techniques to evaluate our systems.These were: BLEU (Papineni, 2001), NIST (Dod-dington, 2002), WER (Word Error Rate), PER(Position-independent WER), GTM (General TextMatcher), and METEOR (Banerjee and Lavie,2005).4.2 Classification AccuracyThe performance of the classifier (from 10-foldcross-validation on the training set) is shown inTable 2.
We give classification accuracy figures forpredicting both source (same language) and target(English) punctuation.
Unsurprisingly, all systemswere better at  predicting their own punctuation.The poorer scores in the table might reflect linguis-tic characteristics (perhaps questions in the sourcelanguage are often expressed as statements in thetarget), or characteristics of the corpus itself.
Forall languages the accuracy of the classifier seemedsatisfactory, especially considering the possibilityof inconsistencies in the corpus itself (and there-fore our test data for this experiment).4.3 Translation QualityThe performance of the SMT systems are shown inTable 3.
It  is clear from the table that  for most  ofthe experimental conditions evaluated the systemoutperformed a baseline system that consisted ofan SMT system trained on all of the data.
For thosemetrics in which performance degraded, in all-but-one the results were statistically insignificant, andin all cases most  of the other MT evaluation met-rics showed an improvement.
Some of the lan-guage pairs showed striking improvements, in par-ticular both of the Malay languages id and ms im-proved by over 3.5 BLEU points each using ourtechnique.
Interestingly Dutch, a relative of Malay,also improved substantially.
This evidence pointsto a linguistic explanation for the gains.
Malay hasvery simple and regular question structure, thequestion words appear at the front of question sen-tences (in the same way as the target  language) anddo not take any other function in the language (un-like the English ?do?
for example).
Perhaps thissimplicity of expression allowed our class-specificmodels to model the data well in spite of the re-duced data caused by dividing the data.
Anotherfactor might be the performance of the classifierwhich was high for all these languages (around98%).
Unfortunately, it is hard to know the reasonsbehind the variety of scores in the table.
One largefactor is likely to be differences in corpus quality,and also the relationship between the source andtarget  corpus.
Some corpora are direct translationsof each other, whereas others are translatedthrough another language.
Chinese was one suchlanguage, and this may explain why we were un-able to improve on the baseline for this languageeven though we were very successful for bothJapanese and Thai, which are relatives of Chinese.4.4  Comparison to Previous MethodsWe ran an experiment to compare our proposedmethod to an instance of our system that  used hardweights.
The aim was to come as close as possiblewithin our framework to the system proposed byYamamoto and Sumita (2007).
We used weights of1 and 0, instead of the classification probabilitiesto weight the class-specific models.
To achievethis, we thresholded the probabilities from the clas-sifier such that probabilities >0.5 gave a weight of1, otherwise a weight  of 0 was used.
The perform-ance of this system is shown in Table 4 under thecolumn heading ?Hard?.
In all-but-one of the con-Figure 3.
Graph showing the BLEU score on thedevelopmment set plotted against the generalmodel?s interpolation weight (a weight of 0meaning no contribution from the general model)for two systems in our experiments.0 0.2 0.4 0.6 0.8 1Model interpolation weight0.380.40.420.440.46BLEU scorezhid214ditions this system was outperformed by or equalto the proposed approach.The column labeled ?No Classifier?
in Table 4illustrates the effectiveness of the classifier in oursystem.
These results show the effect of usingequal weights (0.5) to interpolate between theQuestion and Declaration models.
This system,although not  as effective as the system with theclassifier, gave a respectable performance.5 ConclusionIn this paper we have presented a technique forcombining all models from multiple SMT  enginesinto a single decoding process.
This technique al-lows for topic-dependent decoding with probabilis-tic soft weighting between the component  models.We demonstrated the effectiveness of our approachon conversational data by building class-specificmodels for interrogative and declarative sentenceclasses.
We carried out an extensive evaluation ofthe technique using a large number of languagepairs and MT evaluation metrics.
In most cases wewere able to show significant improvements over asystem without model interpolation, and for somelanguage pairs the approach excelled.
The best im-provement of all the language pairs was for Malay-sian (Malay)-English which outperformed thebaseline system by 4.7 BLEU points (from 0.463to 0.510).
In future research we would like to trythe approach with larger sets of models, and also(possibly overlapping) subsets of the data producedusing automatic clustering methods.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:an automatic metric for MT  evaluation with im-proved correlation with human judgments.
ACL-2005: Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization, pp.
65-72.David Carter, 1994.
Improving Language Models byClustering Training Sentences, Proc.
ACL, pp.
59-64.J.
Civera and A. Juan.
Domain adaptation in statisticalmachine translation with mixture modelling.
In Pro-ceedings of the Second Workshop on ACL StatisticalMachine Translation, pp.
177-180, Prague,Czech Republic, June 2007.Andrew Finch, Etienne De-noual, Hideo Okuma, Michael Paul, Hirofumi Ya-mamoto, Keiji Yasuda, Ruiqiang Zhang, and EiichiroSumita.
2007.
The NICT/ATR speech translationsystem for IWSLT 2007.
IWSLT 2007, Trento, Italy.George Doddington.
2002 Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
Proc.
of Human Language TechnologyConference, San Diego, California, pp.
138-145.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, ACL,pp.
128-135, Prague, Czech Republic,Sasa Hasan and Hermann Ney.
2005.
Clustered Lan-guage Models Based on Regular Expressions forSMT, Proc.
EAMT, Budapest, Hungary.Rukmini Iyer and Mari Ostendorf.
1994.
ModelingLong Distance Dependence in Language: Topic mix-ture versus dynamic cache models, IEEE Transac-tions on Speech and Audio Processing.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the Human Language Technology Con-ference 2003, Edmonton, Canada.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
Machine translation: from real users to research:6th conference of AMTA,  Washington, DC, SpringerVerlag, pp.
115-124.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, Rich-ard Zens,  Chris Dyer, Ond?ej Bojar, Alexandra Con-stantin, Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation,  ACL 2007:proceedings of demo and poster sessions, Prague,Czech Republic, pp.
177-180.Franz J. Och, Hermann Ney, 2003.
A Systematic Com-parison of Various Statistical Alignment Models,Computational Linguistics, No.
1, Vol.
29, pp.
19-51.Franz J. Och, 2003.
Minimum error rate training forstatistical machine trainslation, Proc.
ACL.Michael Paul,  2006.
Overview of the IWSLT 2006Evaluation Campaign, IWSLT 2006.Kishore Papineni, Salim Roukos, Todd Ward, & Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalua-tion of machine translation.
IBM Research Report,RC22176, September 17.Ronald Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
ComputerSpeech and Language, 10:187?228.Matthew Snover, Bonnie Dorr,  Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion,  Proceedings of Association for Machine Trans-lation in the Americas.Andreas Stolcke.
1999.
SRILM - An Extensible Lan-guage Model Toolkit.http://www.speech.sri.com/projects/srilm/Hirofumi Yamamoto and Eiichiro Sumita.
2007.
Bilin-gual cluster based models for statistical machinetranslation.
EMNLP-CoNLL-2007, Prague, CzechRepublic; pp.
514-523.Le Zhang.
2004.
Maximum Entropy Modeling Toolkitfor Python and C++, [On-line].215
