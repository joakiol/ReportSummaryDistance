Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 89?95,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsVTEX System Description for the NLI 2013 Shared TaskVidas Daudaravic?iusVTEXAkademijos 2aVilnius, Lithuaniavidas.daudaravicius@vtex.ltAbstractThis paper describes the system developedfor the NLI 2013 Shared Task, requiringto identify a writer?s native language bysome text written in English.
I explore thegiven manually annotated data using wordfeatures such as the length, endings andcharacter trigrams.
Furthermore, I em-ploy k-NN classification.
Modified TFIDFis used to generate a stop-word list auto-matically.
The distance between two docu-ments is calculated combining n-grams ofword lengths and endings, and charactertrigrams.1 IntroductionNative Language Identification (NLI) is the taskof identifying the first spoken language (L1) ofa person based on the person?s written textin another language.
As a natural languageprocessing (NLP) task, it is properly catego-rized as text classification, and standard ap-proaches like support vector machines (SVM)are successufully applied to it.
Koppel et al(2005) trained SVM models with a set of stylis-tic features, including Part of Speech (POS) andcharacter n-grams (sequences), function words,and spelling error types, achieving 80% accu-racy in a 5-language task.
Tsur and Rappoport(2007) focused on character n-grams.
Wong andDras (2011) showed that syntactic patterns, de-rived by a parser, are more effective than otherstylistic features.
The Cambridge Learner Cor-pus has been used recently by Kochmar (2011),who concluded that character n-grams are themost promising features.
Brooke and Hirst(2012) investigated function words, character n-grams, POS n-grams, POS/function n-grams,CFG productions, dependencies, word n-grams.A notable problem in the recent NLI researchis a clear interaction between native languagesand topics in the corpora.
The solution in thementioned work was to avoid lexical featuresthat might carry topical information.2 DataThe NLI 2013 Shared Task uses the TOEFL11corpus (Blanchard et al 2013) which was de-signed specifically for the task of native languageidentification.
The corpus contains 12 100 En-glish essays from the TOEFL (Test of Englishas a Foreign Language) that were collectedthrough ETS (Educational Testing Service) op-erational test delivery system.
TOEFL11 con-tains eleven native languages: Arabic, Chinese,French, German, Hindi, Italian, Japanese, Ko-rean, Spanish, Telugu, and Turkish.
The sam-pling of essays ensures approximately equal rep-resentation of native languages across eight top-ics, labeled as prompts.
The corpus containsmore than 1000 essays for each L1 language.Each essay is labelled with an English languageproficiency level ?
high, medium, or low ?
givenby human assessment specialists.
The essays areusually 300 to 400 words long.
The corpus issplit into training, development and test data(9900, 1100 and 1100, respectively).
The corpuscontains plain text files and the index for these89File name Prompt NativelanguageLanguageproficiency1000025.txt P2 CHI high100021.txt P1 ARA low1000235.txt P8 TEL medium1000276.txt P4 TEL high1000392.txt P3 JPN medium1000599.txt P6 CHI medium1000617.txt P4 GER high1000719.txt P1 HIN high100082.txt P2 TUR mediumTable 1: The sample of the training data index.files.
Sample of this index is shown in Table 1.3 Nend transformationThe training and the development corpora con-tain a lot of spelling errors and no POS taggingis provided.
For instance, a sentence from thetraining corpus ?Acachely I write abawet maycommunitie and who the people support yougpeople?.
Therefore I needed to find featureswhich encode the information about native lan-guage of a writer in a more generalized way.Also, my primary interest was to build a sys-tem which does not utilize any language pro-cessing tool, such as part of speech or syntactictrees, and topic-related information, such as fullwords.
The reason for that is to have the possi-bility to apply the same techniques for the textswritten in other languages than English in thefuture.
Thus, I choose to use the word length asthe number of characters together with the lastn characters of that word.
Words in the essayswere transformed into tokens using five kinds oftransformations:0end ?
takes the pure length of a word (for ex-ample, make 7?
4 );1end ?
adds to the length of a word the lastcharacter (make 7?
4e);2end ?
adds to the length of a word the lasttwo characters (make 7?
4ke);3end ?
adds to the length of a word the lastthree characters (make 7?
4ake);4end ?
adds to the length of a word the lastfour characters (make 7?
4make).For instance, the sentence ?Difference makesa lot of opportunities .?
is translated to:0end: 10 5 1 3 2 13 11end: 10e 5s 1a 3t 2f 13s 1.2end: 10ce 5es 1a 3ot 2of 13es 1.3end: 10nce 5kes 1a 3lot 2of 13ies 1.4end: 10ence 5akes 1a 3lot 2of 13ties 1.4 N-gram featuresThe VTEX NLI 2013 system is based on n-gram features.
There are no strict rules forhow long n-grams should be.
Frequently used n-grams are unigrams, bigrams and trigrams as inBrooke and Hirst (2012; Wong and Dras (2011).The training NLI 2013 corpus is large enoughto build higher-order n-grams of nend tokens.I use unigrams, bigrams, trigrams, quad-gramsand five-grams based on nend tokens.
Some ex-amples of these n-grams are shown below:0end1-gram: 32-gram: 1 33-gram: 1 10 64-gram: 1 5 3 35-gram: 1 3 3 3 73end1-gram: 7ess2-gram: 2to 7ess3-gram: 4est 2to 7ess4-gram: 3but 3not 3for 7ess5-gram: 3try 5eir 4est 2to 7essBeside n-grams of nends, the character n-grams are of interest also.
Kochmar (2011)noted that character n-grams provide promiss-ing features for NLI task.
Therefore, I tried touse character trigrams also.
For instance, fromthe sentence ?Difference makes a lot of opportu-nities .?
the following trigrams were generated:Dif iff ffe fer ere ren enc nce ce e mma mak ake kes es s a a a l lo lotot t o of of f o op opp ppo por ortrtu tun uni nit iti tie ies es s .Whitespace is included in character trigramsand denotes the beginning or the end of a word.905 CTFIDF for weigthing featuresThe most widely used technique for weight-ing items in a list is Term-Frequency?Inverse-Document-Frequency, known as TF?IDF.
Dau-daravicius (2012) shows that the small change ofTF?IDF allows to the generation of stop-wordlists automatically.
For the NLI 2013 SharedTask I use Conditional TF?IDF :CTFIDF(x) = TF(x) ?
lnDmax ?
d(x) + 14 ?
d(x) + 1,where TF(x) is the frequency of the item x inthe training corpus, d(x) is the number of doc-uments in the training corpus where the itemx appears, known as document frequency, Dmaxis the maximum of document frequency of anyitem in the training corpus.The idea of my Conditional TF?IDF is as fol-lows: if a term occures in less than Dmax/4 doc-uments then this term is considered a normalterm, and the term is considered as stop-word ifit occures in more than Dmax/4 documents.
Therange of TF-IDF is between 0 and positive infin-ity.
The range of CTFIDF is from minus infinityto zero for items that are considered stop-words.And the range of CTFIDF is from zero to infin-ity for the rest of the items.For instance, the Dmax for the different n-gram length and different Nend transformationsis presented in Table 2.
The example list of 4endungrams with positive and negative CTFIDFsare shown in Tables 4 and 3, respectively.It is important to note that I count Dmax andd(x) for each training language separately; i.e.,when I measure the distance between a docu-ment and the document in the training data,The number of n-grams1 2 3 4 50end 900 899 834 444 1681end 900 759 358 320 1482end 899 581 354 319 1483end 899 572 320 303 1484end 899 572 320 303 148Table 2: The maximum of the document frequencyin the training corpus.I use Dmax and d(x) of the language which thetraining document denotes.token ctfidf token ctfidf token ctfidf5earn 0.00 4Most 1.16 10ents 2.517ally 0.04 7lity 1.20 4your 2.5910sion 0.10 2Of 1.22 7arly 2.597ieve 0.10 6ance 1.22 6eple 2.645hing 0.12 6mous 1.22 7tory 2.7110ence 0.12 5hier 1.24 8tics 2.949tion 0.15 3Now 1.25 9gers 3.002us 0.22 5eing 1.27 4cool 3.076rson 0.23 12tion 1.30 3Let 3.137hout 0.29 2He 1.30 4rule 3.293may 0.30 4ways 1.41 5imes 3.523say 0.31 6hers 1.43 3job 3.533see 0.34 5reat 1.45 13ties 3.603try 0.35 9rent 1.53 8cial 3.683did 0.36 3him 1.55 5eals 3.812?
0.42 5ower 1.61 6lent 3.812?
0.44 12ties 1.65 4lose 3.952he 0.46 3You 1.68 8naly 4.134hard 0.52 11lity 1.74 6skes 4.347pany 0.58 4cost 1.76 7cted 4.345akes 0.60 5ince 1.78 7test 4.344kind 0.68 6ills 1.82 6alth 4.367blem 0.70 5isks 1.82 5eall 4.605ever 0.71 5oney 1.89 9dent 4.734been 0.74 6rget 2.07 7cess 4.754same 0.81 5ired 2.10 7kers 5.368king 0.86 9nies 2.11 9ters 5.466king 0.93 4ever 2.15 2D.
5.525ften 0.96 6ates 2.15 5neof 5.526urse 0.97 3his 2.22 8idnt 5.527ling 0.97 10ered 2.24 8klin 5.524Even 0.98 4love 2.24 9velt 5.528ible 0.99 6ited 2.24 10sful 6.624used 1.02 9ties 2.27 4four 7.6210tely 1.07 4earn 2.30 3oil 8.054best 1.09 6llow 2.30 9cans 8.267ught 1.10 9ated 2.37 4jobs 8.964easy 1.12 3got 2.42 3FDR 11.044Then 1.12 8ngly 1.13Table 3: The list of 4end unigrams with positive CT-FIDFs of one document from the training corpus.91token ctfidf token ctfidf token ctfidf1.
-224.19 3but -3.48 3lot -0.921, -127.63 5bout -2.58 2we -0.882to -69.62 3get -2.57 5hich -0.852of -56.92 7mple -2.54 9ment -0.843the -45.09 2by -2.39 3who -0.843and -27.25 4from -2.26 3The -0.812is -24.79 4they -2.18 4them -0.791a -23.19 3can -2.12 3one -0.776ople -22.78 4will -2.11 4only -0.753not -22.31 3all -1.83 4much -0.703are -18.11 2If -1.72 4what -0.683for -15.82 2at -1.63 4also -0.644that -14.39 2In -1.50 4want -0.572do -13.16 6ings -1.38 6cond -0.562it -12.50 5irst -1.35 9tant -0.434have -11.53 3For -1.33 3how -0.354with -9.39 5gree -1.33 3new -0.311I -8.72 3you -1.31 6ould -0.317ause -7.73 2so -1.30 4need -0.202in -6.40 4time -1.15 5oing -0.155heir -6.23 3was -1.08 4take -0.112be -5.44 7ever -0.98 2So -0.104many -5.40 5ther -0.95 6ally -0.092as -5.06 4make -0.93 3But -0.085here -3.92 5hink -3.64Table 4: The list of 4end unigrams with negativeCTFIDFs of the same document as in Fig.
3.6 Distance between documentsCosine distance is a widely used technique tomeasure the distance between two feature vec-tors.
It is calculated as follows:cos(X,Y ) =?i(XiYi)?
?iX2i +?
?i Y2i.CTFIDF allows the splitting of feature vectorsinto the list of ?informative?
items and the listof functional items.
For the NLI 2013 Sharedtask, I combine two cosine distances of negativeand positive CTFIDFs as follows:cos?
(X,Y ) =2 cos(X ?, Y ?)
+ cos(X ?
?, Y ??
)3,whereX ?
= filter?0 X, Y?
= filter?0 Y,X ??
= abs(filter<0 X), Y??
= abs(filter<0 Y ),so X ?
and Y ?
contain features with positive CT-FIDF, while X ??
and Y ??
contain features withnegative CTFIDF.The cos?
combines two cosine distances givingthe weight for cosine of positive CTFIDFs equalto 2 and for the negative CTFIDFs equal to 1.I have also tested combinations of 1 to 0, 0 to1, 1 to 1, and 1 to 2.
But these combinationsdid not achieve better results.
Therefore, for allsubmitted system results I used the same com-bination of 2 to 1.I utilize 26 feature vectors and obtain 26 com-bined cosine distances for each document: onefor character trigrams and other 25 for tokenn-grams of diverse word transformations.
Eachcombined cosine distance has an assigned weightto get the final distance between two documents.The distance between two documents X and Yis calculated as follows:dist(X,Y ) =?iwi cos?
(Xi, Yi)?iwi?
[0, 1],where wi is the weight of ith feature vector.The most difficult task was to find the bestcombination of these 26 weights.
For the NLI2013 Shared Task I have used the combinationsshown in Table 5.
The n-gram weights in mostcases are diagonal with the highest value at the0end unigram and the lowest at the 4end five-gram.
In the beggining I tested the oppositecombination, but this led to worse results.
Also,the influence of character trigrams on the resultswas high.
The first and second combinations inTable 5 differ in the use of five-grams and 4endtransformations, while the leverage of charac-ter trigrams were kept the same.
The final of-ficial results show that richer features improveresults.
Also, I found that the higher leverageis for character trigrams over n-grams the bet-ter the results are.
But, the results of charactertrigrams only resulted in lower performance.
Itis a long way to find the optimal combination ofthe weights.92Token n-gram1 2 3 4 51-closedCharacter trigrams 640end 7 6 5 4 01end 6 5 4 3 02end 5 4 3 2 03end 4 3 2 1 04end 0 0 0 0 02-closedCharacter trigrams 1250end 9 8 7 6 51end 8 7 6 5 42end 7 6 5 4 33end 6 5 4 3 24end 5 4 3 2 13-closedCharacter trigrams 250end 1 1 1 1 11end 1 1 1 1 12end 1 1 1 1 13end 1 1 1 1 14end 1 1 1 1 14-closedCharacter trigrams 2250end 17 15 13 11 91end 15 13 11 9 72end 13 11 9 7 53end 11 9 7 5 34end 9 7 5 3 15-closedCharacter trigrams 5500end 17 15 13 11 91end 15 13 11 9 72end 13 11 9 7 53end 11 9 7 5 34end 9 7 5 3 1Table 5: Weights of the NLI 2013 different submis-sions.7 Assigning native language to a textI used the k-NN technique to assign native lan-guage to a text.
I counted the distances betweenthe test document and all training documents,and take some amount of closest documents foreach language.
To reduce the influnce of out-liers, I dropped off the n closest documents andonly then take some amount from the rest.
Atfirst, I remove the 10 top documents from eachlanguage, and then kept the 20 closest docu-ments for each language.
In total, I obtained 220documents and ranked them by distance.
Then,I employed voting for the closest 20 documents.A winner language is assigned to a document asthe native language.
This technique was used forVTEX-closed-(1, 2 and 3) system submitions.For the VTEX-closed-(4 and 5) I used anothernumber for outliers and the top closest ones:the 50 closest documents for each language weredropped off, the remianing 25 for each languagewere kept, and, finally, the closest 25 documentsare used for the voting of native language.8 ResultsMy primary interest in participating in the NLI2013 Shared Task was to investigate new fea-tures that were not used earlier, and what thevalue of each feature in the identification of awriter?s native language is.
The results of fivesubmitted systems are shown in Tables 6 and7.
The best submitted system had 31.9 percentaccuracy.
This result was the worst of all par-ticipating teams.
At the time of writing this re-port, I tested new combinations of outliers andtops, ?stop-words?
and significant items, nendn-grams and character trigram weights.
Newsettings improved my best submitted system ac-curacy from 31.9 to 63.9 percent.
This resultwas achieved with the following settings.
I tookthe last 50 percent of closest documents for eachlanguage.
I set to use only stop-words and toexclude significant items, i.e., items with onlynegative CTFIDF.
Finaly, I set n-gram weightsaccordingly: 84 for character trigrams, andfor nend 1,1,1,1,1, 1,3,3,3,1, 1,3,5,3,1, 1,3,3,3,1,1,1,1,1,1.
This result shows that 2end and 3endtransformation trigrams have the highest impacton the results.
Nevertheless, all tested transfor-mations help to improve the results.
In con-clusion, I investigated the influence of features,such as character trigrams and Nend n-grams,to the identification of writer?s native languageand found them very informative.93Results for VTEX-closed-1ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 30 5 2 5 5 11 12 6 10 13 1 26.3% 30.0% 28.0%CHI 4 20 2 5 5 6 21 20 5 9 3 24.1% 20.0% 21.9%FRE 6 8 9 13 3 14 14 9 8 10 6 28.1% 9.0% 13.6%GER 6 4 5 30 7 13 4 1 7 20 3 35.3% 30.0% 32.4%HIN 15 5 0 7 17 5 6 5 3 31 6 23.0% 17.0% 19.5%ITA 7 2 4 3 4 47 9 3 4 15 2 34.8% 47.0% 40.0%JPN 4 5 1 4 5 7 44 12 4 14 0 25.3% 44.0% 32.1%KOR 2 8 1 3 2 9 35 27 3 9 1 26.0% 27.0% 26.5%SPA 13 10 4 3 5 15 13 8 12 13 4 19.0% 12.0% 14.7%TEL 13 8 0 1 13 4 2 1 4 52 2 26.3% 52.0% 34.9%TUR 14 8 4 11 8 4 14 12 3 12 10 26.3% 10.0% 14.5%Accuracy = 27.1%Results for VTEX-closed-2ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 31 5 1 3 5 11 13 6 8 15 2 26.5% 31.0% 28.6%CHI 6 23 1 4 6 5 21 15 6 10 3 27.7% 23.0% 25.1%FRE 5 8 7 12 7 15 12 10 6 10 8 25.9% 7.0% 11.0%GER 7 4 4 28 9 12 6 1 6 20 3 35.0% 28.0% 31.1%HIN 13 5 2 6 17 4 6 5 4 30 8 20.2% 17.0% 18.5%ITA 7 2 4 3 4 47 9 3 4 15 2 35.1% 47.0% 40.2%JPN 4 7 0 5 6 7 36 16 3 15 1 22.0% 36.0% 27.3%KOR 3 7 1 3 2 9 34 26 4 9 2 25.7% 26.0% 25.9%SPA 15 7 3 5 6 17 10 7 10 15 5 16.4% 10.0% 12.4%TEL 13 6 1 0 15 2 2 1 6 52 2 25.5% 52.0% 34.2%TUR 13 9 3 11 7 5 15 11 4 13 9 20.0% 9.0% 12.4%Accuracy = 26.0%Results for VTEX-closed-3ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 27 6 1 5 6 11 11 7 11 13 2 25.2% 27.0% 26.1%CHI 6 22 2 6 8 2 21 14 5 12 2 27.2% 22.0% 24.3%FRE 6 8 6 12 8 14 15 7 5 10 9 17.1% 6.0% 8.9%GER 7 4 6 24 9 13 1 2 7 22 5 27.3% 24.0% 25.5%HIN 15 4 2 7 17 4 6 3 5 30 7 19.5% 17.0% 18.2%ITA 7 0 6 3 4 45 8 5 4 16 2 34.1% 45.0% 38.8%JPN 4 9 0 5 6 8 32 15 4 16 1 21.2% 32.0% 25.5%KOR 2 6 1 5 2 9 31 26 4 12 2 27.7% 26.0% 26.8%SPA 15 7 4 6 8 16 7 6 11 14 6 15.3% 11.0% 12.8%TEL 10 6 2 0 13 5 2 1 10 50 1 23.9% 50.0% 32.4%TUR 8 9 5 15 6 5 17 8 6 14 7 15.9% 7.0% 9.7%Accuracy = 24.3%Table 6: The results for closed-task VTEX systems.94Results for VTEX-closed-4ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 21 5 1 6 4 14 15 6 14 12 2 30.4% 21.0% 24.9%CHI 2 22 2 5 5 5 24 18 7 7 3 26.2% 22.0% 23.9%FRE 4 9 8 13 3 14 16 9 6 12 6 22.2% 8.0% 11.8%GER 5 4 8 25 8 13 5 2 6 19 5 28.7% 25.0% 26.7%HIN 7 7 1 7 15 5 7 7 4 31 9 22.1% 15.0% 17.9%ITA 2 3 3 4 2 48 12 3 4 16 3 33.8% 48.0% 39.7%JPN 1 5 1 5 4 8 42 17 4 13 0 21.8% 42.0% 28.7%KOR 1 6 1 2 1 7 36 33 2 10 1 30.0% 33.0% 31.4%SPA 9 11 5 6 4 18 14 5 10 14 4 15.9% 10.0% 12.3%TEL 8 5 3 1 15 5 2 1 4 53 3 27.0% 53.0% 35.8%TUR 9 7 3 13 7 5 20 9 2 9 16 30.8% 16.0% 21.1%Accuracy = 26.6%Results for VTEX-closed-5ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 40 7 0 2 2 14 10 4 7 11 3 33.9% 40.0% 36.7%CHI 6 32 4 0 4 4 21 16 4 8 1 27.8% 32.0% 29.8%FRE 5 13 13 9 2 15 14 8 6 12 3 28.9% 13.0% 17.9%GER 10 5 8 22 2 13 7 3 8 16 6 45.8% 22.0% 29.7%HIN 12 9 4 5 11 5 6 6 4 30 8 28.9% 11.0% 15.9%ITA 3 5 6 2 1 54 7 4 5 11 2 36.5% 54.0% 43.5%JPN 2 6 0 3 1 8 48 16 3 12 1 26.4% 48.0% 34.0%KOR 1 12 1 0 2 6 29 39 2 7 1 35.1% 39.0% 37.0%SPA 12 9 5 1 3 20 14 5 16 12 3 27.1% 16.0% 20.1%TEL 14 6 0 0 8 5 2 0 3 59 3 31.4% 59.0% 41.0%TUR 13 11 4 4 2 4 24 10 1 10 17 35.4% 17.0% 23.0%Accuracy = 31.9%Table 7: The results for closed-task VTEX systems.ReferencesBlanchard D., Tetreault J. and Cahill A.
2013.
Sum-mary Report on the First Shared Task on Na-tive Language Identification.
In Proceedings ofthe Eighth Workshop on Building Educational Ap-plications Using NLP, Association for Computa-tional Linguistics, Atlanta, GA, USABrooke, J. and Hirst, G. 2012.
Robust, LexicalizedNative Language Identification.
In Proceedings ofCOLING 2012, Mumbai, India, 391?408.Daudaravicius, V. 2012.
Collocation segmentationfor text chunking.
PhD thesis, Vytautas MagnusUniversity.Kochmar, E. 2011.
Identification of a Writer?s Na-tive Language by Error Analysis.
Master?s thesis,University of Cambridge.Koppel M., Schler J. and Zigdon, K. 2005.
Deter-mining an author?s native language by mining atext for errors.
In Proceedings of the 11th ACMSIGKDD International Conference on KnowledgeDiscovery in Data Mining (KDD ?05), 624-628.Tsur, O. and Rappoport, A.
2007.
Using classi-fier features for studying the effect of native lan-guage on the choice of written second languagewords.
In Proceedings of the Workshop on Cogni-tive Aspects of Computational Language Acquisi-tion (CACLA?07), 9-16.Wong, S.J.
and Dras, M. 2011.
Exploiting parsestructures for native language identification.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, 1600-1610.95
