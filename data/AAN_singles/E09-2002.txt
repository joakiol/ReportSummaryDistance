Proceedings of the EACL 2009 Demonstrations Session, pages 5?8,Athens, Greece, 3 April 2009. c?2009 Association for Computational LinguisticsCBSEAS, a Summarization SystemIntegration of Opinion Mining Techniques to Summarize BlogsAure?lien Bossard, Michel Ge?ne?reux and Thierry PoibeauLaboratoire d?Informatique de Paris-NordCNRS UMR 7030 and Universite?
Paris 1393430 Villetaneuse ?
France{firstname.lastname}@lipn.univ-paris13.frAbstractIn this paper, we present a novel approachfor automatic summarization.
Our system,called CBSEAS, integrates a new methodto detect redundancy at its very core, andproduce more expressive summaries thanprevious approaches.
Moreover, we showthat our system is versatile enough to in-tegrate opinion mining techniques, so thatit is capable of producing opinion orientedsummaries.
The very competitive resultsobtained during the last Text EvaluationConference (TAC 2008) show that our ap-proach is efficient.1 IntroductionDuring the past decade, automatic summarization,supported by evaluation campaigns and a large re-search community, has shown fast and deep im-provements.
Indeed, the research in this domain isguided by strong industrial needs: fast processingdespite ever increasing amount of data.In this paper, we present a novel approach forautomatic summarization.
Our system, called CB-SEAS, integrates a new method to detect redun-dancy at its very core, and produce more expres-sive summaries than previous approaches.
Thesystem is flexible enough to produce opinion ori-ented summaries by accommodating techniques tomine documents that express different views orcommentaries.
The very competitive results ob-tained during the last Text Evaluation Conference(TAC 2008) show that our approach is efficient.This short paper is structured as follows: wefirst give a quick overview of the state of the art.We then describe our system, focusing on the mostimportant novel features implemented.
Lastly, wegive the details of the results obtained on the TAC2008 Opinion Pilot task.2 Related worksInterest in creating automatic summaries has be-gun in the 1950s (Luhn, 1958).
(Edmundson andWyllys, 1961) proposed features to assign a scoreto each sentence of a corpus in order to rank thesesentences.
The ones with the highest scores arekept to produce the summary.
The features theyused were sentence position (in a news article forexample, the first sentences are the most impor-tant), proper names and keywords in the documenttitle, indicative phrases and sentence length.Later on, summarizers aimed at eliminating re-dundancy, especially for multi-documents summa-rizing purpose.
Identifying redundancy is a criti-cal task, as information appearing several times indifferent documents can be qualified as important.Among recent approaches, the ?centroid-basedsummarization?
method developed by (Radev etal., 2004) consists in identifying the centroidof a cluster of documents, in other words theterms which best suit the documents to summa-rize.
Then, the sentences to be extracted arethe ones that contain the greatest number of cen-troids.
Radev implemented this method in an on-line multi-document summarizer, MEAD.Radev further improved MEAD using a differ-ent method to extract sentences: ?Graph-basedcentrality?
extractor (Erkan and Radev, 2004).It consists in computing similarity between sen-tences, and then selecting sentences which areconsidered as ?central?
in a graph where nodes aresentences and edges are similarities.
Sentence se-lection is then performed by picking the sentenceswhich have been visited most after a random walkon the graph.The last two systems are dealing with redun-dancy as a post-processing step.
(Zhu et al, 2007),assuming that redundancy should be the concepton what is based multi-document summarization,offered a method to deal with redundancy at the5same time as sentence selection.
For that purpose,the authors used a ?Markov absorbing chain ran-dom walk?
on a graph representing the differentsentences of the corpus to summarize.MMR-MD, introduced by Carbonnel in (Car-bonell and Goldstein, 1998), is a measure whichneeds a passage clustering: all passages consid-ered as synonyms are grouped into the same clus-ters.
MMR-MD takes into account the similarityto a query, coverage of a passage (clusters thatit belongs to), content in the passage, similarityto passages already selected for the summary, be-longing to a cluster or to a document that has al-ready contributed a passage to the summary.The problem of this measure lies in the clus-tering method: in the literature, clustering is gen-erally fulfilled using a threshold.
If a passagehas a similarity to a cluster centroid higher thana threshold, then it is added to this cluster.
Thismakes it a supervised clustering method; an unsu-pervised clustering method is best suited for au-tomatic summarization, as the corpora we needto summarize are different from one to another.Moreover, sentence synonymy is also dependenton the corpus granularity and on the user compres-sion requirement.3 CBSEAS: A Clustering-BasedSentence Extractor for AutomaticSummarizationWe assume that, in multi-document summariza-tion, redundant pieces of information are the sin-gle most important element to produce a goodsummary.
Therefore, the sentences which carrythose pieces of information have to be extracted.Detecting these sentences conveying the same in-formation is the first step of our approach.
The de-veloped algorithm first establishes the similaritiesbetween all sentences of the documents to sum-marize, then applies a clustering algorithm ?
fastglobal k-means (Lo?pez-Escobar et al, 2006) ?
tothe similarity matrix in order to create clusters inwhich sentences convey the same information.First, our system ranks all the sentences accord-ing to their similarity to the documents centroid.We have chosen to build up the documents cen-troid with the m most important terms, their im-portance being reflected by the tf/idf of each term.We then select the n2 best ranked sentences to cre-ate a n sentences long summary.
We do so becausethe clustering algorithm we use to detect sentencesfor all ejinEC1 ?
ejfor i from 1 to k dofor j from 1 to icenter(Cj)?
em|emmaximizes?eninCjsim(em, en)for all ej in Eej ?
Cl|Clmaximizes sim(center(Cl, ej))add a new cluster: Ci.
It initially contains only itscenter, the worst represented element in its cluster.doneFigure 1: Fast global k-means algorithmconveying the same information, fast global k-means, behaves better when it has to group n2elements into n clusters.
The similarity with thecentroid is a weighted sum of terms appearing inboth centroid and sentence, normalized by sen-tence length.Similarity between sentences is computed usinga variant of the ?Jaccard?
measure.
If two termsare not equal, we test their synonymy/hyperonymyusing the Wordnet taxonomy (Fellbaum, 1998).
Incase they are synonyms or hyperonym/hyponym,these terms are taken into account in the similar-ity calculation, but weighted respectively half andquarter in order to reflect that term equality is moreimportant than term semantic relation.
We do thisin order to solve the problem pointed out in (Erkanand Radev, 2004) (synonymy was not taken intoaccount for sentence similarity measures) and soto enhance sentence similarity measure.
It is cru-cial to our system based on redundancy location asredundancy assumption is dependent on sentencesimilarities.Once the similarities are computed, we clusterthe sentences using fast global k-means (descrip-tion of the algorithm is in figure 1) using the simi-larity matrix.
It works well on a small data set witha small number of dimensions, although it has notyet scaled up as well as we would have expected.This clustering step completed, we select onesentence per cluster in order to produce a sum-mary that contains most of the relevant informa-tion/ideas in the original documents.
We do so bychoosing the central sentence in each cluster.
Thecentral sentence is the one which maximizes thesum of similarities with the other sentences of itscluster.
It should be the one that characterizes bestthe cluster in terms of information vehicled.64 TAC 2008: The OpinionSummarization TaskIn order to evaluate our system, we participatedin the Text Analysis Conference (TAC) that pro-posed in 2008 an opinion summarization task.
Thegoal is to produce fluent and well-organized sum-maries of blogs.
These summaries are orientedby complex user queries, such as ?Why do peoplelike.....??
or ?Why do people prefer...
to...?
?.The results were analyzed manually, using thePYRAMID method (Lin et al, 2006): the PYRA-MID score of a summary depends on the numberof simple semantic units, units considered as im-portant by the annotators.
The TAC evaluationfor this task also included grammaticality, non-redundancy, structure/coherence and overall flu-ency scores.5 CBSEAS Adaptation to the OpinionSummarization TaskBlog summarization is very different from anewswire article or a scientific paper summa-rization.
Linguistic quality as well as reason-ing structure are variable from one blogger to an-other.
We cannot use generalities on blog struc-ture, neither on linguistic markers to improveour summarization system.
The other problemwith blogs is the noise due to the use of un-usual language.
We had to clean the blogs in apre-processing step: sentences with a ratio num-ber of frequent words/total number of words belowa given threshold (0.35) were deemed too noisyand discarded.
Frequent words are the one hun-dred most frequent words in the English languagewhich on average make up approximately half ofwritten texts (Fry et al, 2000).Our system, CBSEAS, is a ?standard?
summa-rization system.
We had to adapt it in order todeal with the specific task of summarizing opin-ions.
All sentences from the set of documents tosummarize were tagged following the opinion de-tected in the blog post they originated from.
Weused for that purpose a two-class (positive or neg-ative) SVM classifier trained on movie reviews.The idea behind the opinion classifier is to im-prove summaries by selecting sentences havingthe same opinionated polarity as the query, whichwere tagged using a SVM trained on the manuallytagged queries from the training data provided ear-lier in TAC.As the Opinion Summarization Task was to pro-duce a query-oriented summary, the sentence pre-selection was changed, using the user query in-stead of the documents centroid.
We also changedthe sentence pre-selection ranking measure byweighting terms according to their lexical cate-gory; we have chosen to give more weight toproper names than verbs adjectives, adverbs andnouns.
Indeed, opinions we had to summarizewere mostly on products or people.While experimenting our system on TAC 2008training data, we noticed that extracting sentenceswhich are closest to their cluster center was notsatisfactory.
Some other sentences in the samecluster were best fitted to a query-oriented sum-mary.
We added the sentence ranking used for thesentence pre-selection to the final sentence extrac-tor.
Each sentence is given a score which is thedistance to the cluster center times the similarityto the query.6 TAC 2008 Results on OpinionSummarization TaskParticipants to the Opinion Summarization Taskwere allowed to use extra-information given byTAC organizers.
These pieces of information arecalled snippets.
The snippets contain the relevantinformation, and could be used as a stand-alonedataset.
Participants were classified into two dif-ferent groups: one for those who did not use snip-pets, and one for those who did.
We did not usesnippets at all, as it is a more realistic challengeto look directly at the blogs with no external help.The results we present here are those of the partic-ipants that were not using snippets.
Indeed, sys-tems using snippets obtained much higher scoresthan the other systems.
We cannot compare oursystem to systems using snippets.Our system obtained quite good results onthe ?opinion task?
: the scores can be found onfigure 2.
As one can see, our responsivenessscores are low compared to the others (responsive-ness score corresponds to the following question:?How much would you pay for that summary??
).We suppose that despite the grammaticality, flu-ency and pyramid scores of our summaries, judgesgave a bad responsiveness score to our summariesbecause they are too long: we made the choiceto produce summaries with a compression rate of10% when it was possible, the maximum lengthauthorized otherwise.7Evaluation CBSEAS Mean Best Worst RankPyramid .169 .151 .251 .101 5/20Grammatic.
5.95 5.14 7.54 3.54 3/20Non-redun.
6.64 5.88 7.91 4.36 4/20Structure 3.50 2.68 3.59 2.04 2/20Fluency 4.45 3.43 5.32 2.64 2/20Responsiv.
2.64 2.61 5.77 1.68 8/20Figure 2: Opinion task overall resultsFigure 3: Opinion task resultsHowever, we noticed that the quality of oursummaries was very erratic.
We assume this isdue to the length of our summaries, as the longestsummaries are the ones which get the worst scoresin terms of pyramid f-score (fig 3).
The length ofthe summaries is a ratio of the original documentslength.
The quality of the summaries would bedecreasing while the number of input sentences isincreasing.Solutions to fix this problem could be:?
Define a better score for the correspondenceto a user query and remove sentences whichare under a threshold;?
Extract sentences from the clusters that con-tain more than a predefined number of ele-ments only.This would result in improving the pertinenceof the extracted sentences.
The users reading thesummaries would also be less disturbed by thelarge amount of sentences a too long summaryprovides.
As the ?opinion summarization?
taskwas evaluated manually and reflects well the qual-ity of a summary for an operational use, the con-clusions of this evaluation are good indicators ofthe quality of the summaries produced by our sys-tem.7 ConclusionWe presented here a new approach for multi-document summarization.
It uses an unsuper-vised clustering method to group semantically re-lated sentences together.
It can be compared toapproaches using sentence neighbourhood (Erkanand Radev, 2004), because the sentences which arehighly related to the highest number of sentencesare those which will be extracted first.
How-ever, our approach is different since sentence se-lection is directly dependent on redundancy loca-tion.
Also, redundancy elimination, which is cru-cial in multi-document summarization, takes placein the same step as sentence selection.ReferencesJaime Carbonell and Jade Goldstein.
1998.
The useof MMR, diversity-based reranking for reorderingdocuments and producing summaries.
In SIGIR?98,pages 335?336, New York, NY, USA.
ACM.Harold P. Edmundson and Ronald E. Wyllys.
1961.Automatic abstracting and indexing?survey andrecommendations.
Commun.
ACM, 4(5):226?234.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Edward Bernard Fry, Jacqueline E. Kress, andDona Lee Fountoukidis.
2000.
The Reading Teach-ers Book of Lists.
Jossey-Bass, 4th edition.Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie.
2006.
An information-theoretic approachto automatic evaluation of summaries.
In Proceed-ings of HLT-NAACL, pages 463?470, Morristown,NJ, USA.Sau?l Lo?pez-Escobar, Jesu?s Ariel Carrasco-Ochoa, andJose?
Francisco Mart?
?nez Trinidad.
2006.
Fastglobal -means with similarity functions algorithm.In IDEAL, volume 4224 of Springer, Lecture Notesin Computer Science, pages 512?521.H.P.
Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal, 2(2):159?165.Dragomir Radev et al 2004.
MEAD - a platform formultidocument multilingual text summarization.
InProceedings of LREC 2004, Lisbon, Portugal.Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, andDavid Andrzejewski.
2007.
Improving diversityin ranking using absorbing random walks.
In Pro-ceedings of HLT-NAACL, pages 97?104, Rochester,USA.8
