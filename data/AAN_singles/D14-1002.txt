Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2?13,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsModeling Interestingness with Deep Neural NetworksJianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li DengMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USA{jfgao,ppantel,mgamon,xiaohe,deng}@microsoft.comAbstractThis paper presents a deep semantic simi-larity model (DSSM), a special type ofdeep neural networks designed for textanalysis, for recommending target docu-ments to be of interest to a user based on asource document that she is reading.
Weobserve, identify, and detect naturally oc-curring signals of interestingness in clicktransitions on the Web between source andtarget documents, which we collect fromcommercial Web browser logs.
The DSSMis trained on millions of Web transitions,and maps source-target document pairs tofeature vectors in a latent space in such away that the distance between source doc-uments and their corresponding interestingtargets in that space is minimized.
The ef-fectiveness of the DSSM is demonstratedusing two interestingness tasks: automatichighlighting and contextual entity search.The results on large-scale, real-world da-tasets show that the semantics of docu-ments are important for modeling interest-ingness and that the DSSM leads to signif-icant quality improvement on both tasks,outperforming not only the classic docu-ment models that do not use semantics butalso state-of-the-art topic models.1 IntroductionTasks of predicting what interests a user based onthe document she is reading are fundamental tomany online recommendation systems.
A recentsurvey is due to Ricci et al.
(2011).
In this paper,we exploit the use of a deep semantic model fortwo such interestingness tasks in which documentsemantics play a crucial role: automatic highlight-ing and contextual entity search.Automatic Highlighting.
In this task we wanta recommendation system to automatically dis-cover the entities (e.g., a person, location, organi-zation etc.)
that interest a user when reading a doc-ument and to highlight the corresponding textspans, referred to as keywords afterwards.
Weshow in this study that document semantics areamong the most important factors that influencewhat is perceived as interesting to the user.
Forexample, we observe in Web browsing logs thatwhen a user reads an article about a movie, she ismore likely to browse to an article about an actoror character than to another movie or the director.Contextual entity search.
After identifyingthe keywords that represent the entities of interestto the user, we also want the system to recommendnew, interesting documents by searching the Webfor supplementary information about these enti-ties.
The task is challenging because the same key-words often refer to different entities, and interest-ing supplementary information to the highlightedentity is highly sensitive to the semantic context.For example, ?Paul Simon?
can refer to many peo-ple, such as the singer and the senator.
Consideran article about the music of Paul Simon and an-other about his life.
Related content about his up-coming concert tour is much more interesting inthe first context, while an article about his familyis more interesting in the second.At the heart of these two tasks is the notion ofinterestingness.
In this paper, we model and makeuse of this notion of interestingness with a deepsemantic similarity model (DSSM).
The model,extending from the deep neural networks shownrecently to be highly effective for speech recogni-tion (Hinton et al., 2012; Deng et al., 2013) andcomputer vision (Krizhevsky et al., 2012; Mar-koff, 2014), is semantic because it maps docu-ments to feature vectors in a latent semantic space,also known as semantic representations.
Themodel is deep because it employs a neural net-work with several hidden layers including a spe-cial convolutional-pooling structure to identifykeywords and extract hidden semantic features atdifferent levels of abstractions, layer by layer.
Thesemantic representation is computed through adeep neural network after its training by back-propagation with respect to an objective tailored2to the respective interestingness tasks.
We obtainnaturally occurring ?interest?
signals by observ-ing Web browser transitions, from a source docu-ment to a target document, in Web usage logs of acommercial browser.
Our training data is sampledfrom these transitions.The use of the DSSM to model interestingnessis motivated by the recent success of applying re-lated deep neural networks to computer vision(Krizhevshy et al.
2012; Markoff, 2014), speechrecognition (Hinton et al.
2012), text processing(Collobert et al.
2011),  and Web search (Huanget al.
2013).
Among them, (Huang et al.
2013) ismost relevant to our work.
They also use a deepneural network to map documents to feature vec-tors in a latent semantic space.
However, theirmodel is designed to represent the relevance be-tween queries and documents, which differs fromthe notion of interestingness between documentsstudied in this paper.
It is often the case that a useris interested in a document because it providessupplementary information about the entities orconcepts she encounters when reading anotherdocument although the overall contents of the sec-ond documents is not highly relevant.
For exam-ple, a user may be interested in knowing moreabout the history of University of Washington af-ter reading the news about President Obama?svisit to Seattle.
To better model interestingness,we extend the model of Huang et al.
(2013) in twosignificant aspects.
First, while Huang et al.
treata document as a bag of words for semantic map-ping, the DSSM treats a document as a sequenceof words and tries to discover prominent key-words.
These keywords represent the entities orconcepts that might interest users, via the convo-lutional and max-pooling layers which are relatedto the deep models used for computer vision(Krizhevsky et al., 2013) and speech recognition(Deng  et al., 2013a) but are not used in Huang etal.
?s model.
The DSSM then forms the high-levelsemantic representation of the whole documentbased on these keywords.
Second, instead of di-rectly computing the document relevance scoreusing cosine similarity in the learned semanticspace, as in Huang et al.
(2013), we feed the fea-tures derived from the semantic representations ofdocuments to a ranker which is trained in a super-vised manner.
As a result, a document that is nothighly relevant to another document a user is read-ing (i.e., the distance between their derived feature1 We stress here that, although the click signal is available toform a dataset and a gold standard ranker (to be described invectors is big) may still have a high score of inter-estingness because the former provides useful in-formation about an entity mentioned in the latter.Such information and entity are encoded, respec-tively, by (some subsets of) the semantic featuresin their corresponding documents.
In Sections 4and 5, we empirically demonstrate that the afore-mentioned two extensions lead to significant qual-ity improvements for the two interestingness taskspresented in this paper.Before giving a formal description of theDSSM in Section 3, we formally define the inter-estingness function, and then introduce our dataset of naturally occurring interest signals.2 The Notion of InterestingnessLet ?
be the set of all documents.
FollowingGamon et al.
(2013), we formally define the inter-estingness modeling task as learning the mappingfunction:?
: ?
?
?
?
?
?where the function ??
?, ??
is the quantified degreeof interest that the user has  in the target document?
?
?
after or while reading the source document?
?
?.Our notion of a document is meant in its mostgeneral form as a string of raw unstructured text.That is, the interestingness function should notrely on any document structure such as title tags,hyperlinks, etc., or Web interaction data.
In ourtasks, documents can be formed either from theplain text of a webpage or as a text span in thatplain text, as will be discussed in Sections 4 and 5.2.1 DataWe can observe many naturally occurring mani-festations of interestingness on the Web.
For ex-ample, on Twitter, users follow shared links em-bedded in tweets.
Arguably the most frequent sig-nal, however, occurs in Web browsing eventswhere users click from one webpage to anothervia hyperlinks.
When a user clicks on a hyperlink,it is reasonable to assume that she is interested inlearning more about the anchor, modulo cases oferroneous clicks.
Aggregate clicks can thereforeserve as a proxy for interestingness.
That is, for agiven source document, target documents that at-tract the most clicks are more interesting than doc-uments that attract fewer clicks1.Section 4), our task is to model interestingness between un-structured documents, i.e., without access to any documentstructure or Web interaction data.
Thus, in our experiments,3We collect a large dataset of user browsingevents from a commercial Web browser.
Specifi-cally, we sample 18 million occurrences of a userclick from one Wikipedia page to another duringa one year period.
We restrict our browsing eventsto Wikipedia since its pages tend to contain manyanchors (79 on average, where on average 42 havea unique target URL).
Thus, they attract enoughtraffic for us to obtain robust browsing transitiondata2.
We group together all transitions originat-ing from the same page and randomly hold out20% of the transitions for our evaluation data(EVAL), 20% for training the DSSM described inSection 3.2 (TRAIN_1), and the remaining 60%for training our task specific rankers described inSection 3.3 (TRAIN_2).
In our experiments, weused different settings for the two interestingnesstasks.
Thus, we postpone the detailed descriptionof these datasets and other task-specific datasetsto Sections 4 and 5.3 A Deep Semantic Similarity Model(DSSM)This section presents the architecture of theDSSM, describes the parameter estimation, andthe way the DSSM is used in our tasks.we remove all structural information (e.g., hyperlinks andXML tags) in our documents, except that in the highlightingexperiments (Section 4) we use anchor texts to simulate thecandidate keywords to be highlighted.
We then convert each3.1 Network ArchitectureThe heart of the DSSM is a deep neural networkwith convolutional structure, as shown in Figure1.
In what follows, we use lower-case bold letters,such as ?, to denote column vectors, ????
to de-note the ???
element of ?, and upper-case letters,such as ?, to denote matrices.Input Layer ?.
It takes two steps to convert a doc-ument ?, which is a sequence of words, into a vec-tor representation ?
for the input layer of the net-work: (1) convert each word in ?
to a word vector,and (2) build ?
by concatenating these word vec-tors.
To convert a word ?
into a word vector, wefirst represent ?
by a one-hot vector using a vo-cabulary that contains ?
high frequent words(?
?
150K in this study).
Then, following Huanget al.
(2013), we map ?
to a separate tri-letter vec-tor.
Consider the word ?#dog#?, where # is a wordboundary symbol.
The nonzero elements in its tri-letter vector are ?#do?, ?dog?, and ?og#?.
We thenform the word vector of ?
by concatenating itsone-hot vector and its tri-letter vector.
It is worthnoting that the tri-letter vector complements theone-hot vector representation in two aspects.
First,different OOV (out of vocabulary) words can berepresented by tri-letter vectors with few colli-sions.
Second, spelling variations of the sameword can be mapped to the points that are close toeach other in the tri-letter space.
Although thenumber of unique English words on the Web isextremely large, the total number of distinct tri-letters in English is limited (restricted to the mostfrequent 30K in this study).
As a result, incorpo-rating tri-letter vectors substantially improves therepresentation power of word vectors while keep-ing their size small.To form our input layer ?
using word vectors,we first identify a text span with a high degree ofrelevance, called focus, in ?
using task-specificheuristics (see Sections 4 and 5 respectively).
Sec-ond, we form ?
by concatenating each word vec-tor in the focus and a vector that is the summationof all other word vectors, as shown in Figure 1.Since the length of the focus is much smaller thanthat of its document, ?
is able to capture the con-textual information (for the words in the focus)Web document into plain text, which is white-space to-kenized and lowercased.
Numbers are retained and no stem-ming is performed.2 We utilize the May 3, 2013 English Wikipedia dump con-sisting of roughly 4.1 million articles from http://dumps.wiki-media.org.Figure 1: Illustration of the network architec-ture and information flow of the DSSM4useful to the corresponding tasks, with a manage-able vector size.Convolutional Layer ?
.
A convolutional layerextracts local features around each word ??
in aword sequence of length ?
as follows.
We firstgenerate a contextual vector ??
by concatenatingthe word vectors of ??
and its surrounding words defined by a window (the window size is set to 3in this paper).
Then, we generate for each word alocal feature vector ??
using a tanh  activationfunction and a linear projection matrix ?
?, whichis the same across all windows ?
in the word se-quence, as:??
?
tanh???????
, where	?
?
1?
1) ?
)Max-pooling Layer ?.
The size of the output ?depends on the number of words in the word se-quence.
Local feature vectors have to be com-bined to obtain a global feature vector, with afixed size independent of the document length, inorder to apply subsequent standard affine layers.We design ?
by adopting the max operation overeach ?time?
?
of the sequence of vectors computedby (1), which forces the network to retain only themost useful, partially invariant local features pro-duced by the convolutional layer:????
?
max???,?,??u?????
(2)where the max operation is performed for each di-mension of ?
across ?
?
1,?
, ?
respectively.That convolutional and max-pooling layers areable to discover prominent keywords of a docu-ment can be demonstrated using the procedure inFigure 2 using a toy example.
First, the convolu-tional layer of (1) generates for each word in a 5-word document a 4-dimensional local feature vec-tor, which represents a distribution of four topics.For example, the most prominent topic of ??
within its three word context window is the firsttopic, denoted by ??
?1?, and the most prominenttopic of ??
is ???3?.
Second, we use max-pooling of (2) to form a global feature vector, which rep-resents the topic distribution of the whole docu-ment.
We see that ??1?
and ??3?
are two promi-nent topics.
Then, for each prominent topic, wetrace back to the local feature vector that survivesmax-pooling:??1?
?
max???,?,?????1??
?
???1???3?
?
max???,?,?????3??
?
??
?3?.Finally, we label the corresponding words of theselocal feature vectors, ??
and ?
?, as keywords of the document.Figure 3 presents a sample of document snip-pets and their keywords detected by the DSSM ac-cording to the procedure elaborated in Figure 2.
Itis interesting to see that many names are identifiedas keywords although the DSSM is not designedexplicitly for named entity recognition.Fully-Connected Layers ?
and ?
.
The fixedsized global feature vector ?
of (2) is then fed toseveral standard affine network layers, which arestacked and interleaved with nonlinear activationfunctions, to extract highly non-linear features ?at the output layer.
In our model, shown in Figure1, we have:?
?
tanh??????
(3)?
?
tanh??????
(4)where ??
and ??
are learned linear projection matri-ces.3.2 Training the DSSMTo optimize the parameters of the DSSM of Fig-ure 1, i.e., ?
?
???,??,??
?, we use a pair-wise rank loss as objective (Yih et al.
2011).
Considera source document ?
and two candidate targetdocuments ??
and ?
?, where ??
is more interestingthan ??
to a user when reading ?.
We constructtwo pairs of documents ?
?, ???
and ?
?, ??
?, where the former is preferred and should have a higheru1 u2 u3 u4 u5w1 w2 w3 w4 w52341w1 w2 w3 w4 w5v2341Figure 2: Toy example of (upper) a 5-worddocument and its local feature vectors ex-tracted using a convolutional layer, and (bot-tom) the global feature vector of the documentgenerated after max-pooling.5interestingness score.
Let ?
be the difference oftheir interestingness scores: ?
?
??
?, ???
???
?, ???
, where ?
is the interestingness score, computed as the cosine similarity:??
?, ??
?
sim??
?, ??
??????????????
(5)where ??
and ??
are the feature vectors of ?
and ?, respectively, which are generated using theDSSM, parameterized by ?.
Intuitively, we wantto learn ?
to maximize ?.
That is, the DSSM islearned to represent documents as points in a hid-den interestingness space, where the similarity be-tween a document and its interesting documents ismaximized.We use the following logistic loss over ?
,which can be shown to upper bound the pairwiseaccuracy:???
; ??
?
log?1 ?
exp??????
(6)3 In our experiments, we observed better results by samplingmore negative training examples (e.g., up to 100) althoughthis makes the training much slower.
An alternative approachThe loss function in (6) has a shape similar to thehinge loss used in SVMs.
Because of the use ofthe cosine similarity function, we add a scalingfactor ?
that magnifies ?
from [-2, 2] to a largerrange.
Empirically, the value of ?
makes no dif-ference as long as it is large enough.
In the exper-iments, we set ?
?
10.
Because the loss functionis differentiable, optimizing the model parameterscan be done using gradient-based methods.
Due tospace limitations, we omit the derivation of thegradient of the loss function, for which readers arereferred to related derivations (e.g., Collobert etal.
2011; Huang et al.
2013; Shen et al.
2014).In our experiments we trained DSSMs usingmini-batch Stochastic Gradient Descent.
Eachmini-batch consists of 256 source-target docu-ment pairs.
For each source document ?, we ran-domly select from that batch four target docu-ments which are not paired with ?
as negativetraining samples3.
The DSSM trainer is imple-mented using a GPU-accelerated linear algebra li-brary, which is developed on CUDA 5.5.
Giventhe training set (TRAIN_1 in Section 2), it takesapproximately 30 hours to train a DSSM as shownin Figure 1, on a Xeon E5-2670 2.60GHz machinewith one Tesla K20 GPU card.In principle, the loss function of (6) can be fur-ther regularized (e.g.
by adding a term of 2?
norm)to deal with overfitting.
However, we did not finda clear empirical advantage over the simpler earlystop approach in a pilot study, hence we adoptedthe latter in the experiments in this paper.
Our ap-proach adjusts the learning rate ?
during thecourse of model training.
Starting with ?
?
1.0,after each epoch (a pass over the entire trainingdata), the learning rate is adjusted as ?
?
0.5 ?
?if the loss on validation data (held-out fromTRAIN_1) is not reduced.
The training stops ifeither ?
is smaller than a preset threshold(0.0001) or the loss on training data can no longerbe reduced significantly.
In our experiments, theDSSM training typically converges within 20epochs.3.3 Using the DSSMWe experiment with two ways of using the DSSMfor the two interestingness tasks.
First, we use theDSSM as a feature generator.
The output layer ofthe DSSM can be seen as a set of semantic fea-tures, which can be incorporated in a boosted treeis to approximate the partition function using Noise Contras-tive Estimation (Gutmann and Hyvarinen 2010).
We leave itto future work.?
the comedy festival formerly known asthe us comedy arts festival is a comedyfestival held each year in las vegasnevada from its 1985 inception to 2008. it was held annually at the wheeleropera house and other venues in aspencolorado .
the primary sponsor of thefestival was hbo with co-sponsorship bycaesars palace .
the primary venue tbsgeico insurance twix candy bars andsmirnoff vodka hbo exited the festivalbusiness in 2007 and tbs became the pri-mary sponsor the festival includesstandup comedy performances appearancesby the casts of television shows??
bad samaritans is an american comedyseries produced by walt becker kellyhayes and ross putman .
it premiered onnetflix on march 31 2013 cast and char-acters .
the show focuses on a communityservice parole group and their paroleofficer brian kubach as jake gibson anaspiring professional starcraft playerwho gets sentenced to 2000 hours of com-munity service for starting a forestfire during his breakup with drew priorto community service he had no real am-bition in life other than to be a pro-fessional gamer and become wealthyovernight like mark zuckerberg as inlife his goal during ?Figure 3: A sample of document snippets andthe keywords (in bold) detected by the DSSM.6based ranker (Friedman 1999) trained discrimina-tively on the task-specific data.
Given a source-target document pair ?
?, ?
?, the DSSM generates600 features (300 from the output layers ??
and ?
?for each ?
and ?, respectively).Second, we use the DSSM as a direct imple-mentation of the interestingness function ?.
Re-call from Section 3.2 that in model training, wemeasure the interestingness score for a documentpair using the cosine similarity between their cor-responding feature vectors (??
and ??).
Similarlyat runtime, we define	?
?
sim??
?, ??
as (5).4 Experiments on HighlightingRecall from Section 1 that in this task, a systemmust select ?
most interesting keywords in a doc-ument that a user is reading.
To evaluate our mod-els using the click transition data described in Sec-tion 2, we simulate the task as follows.
We use theset of anchors in a source document ?
to simulatethe set of candidate keywords that may be of in-terest to the user while reading ?, and treat the textof a document that is linked by an anchor in ?
as atarget document ?.
As shown in Figure 1, to applyDSSM to a specific task, we need to define the fo-cus in source and target documents.
In this task,the focus in s is defined as the anchor text, and thefocus in t is defined as the first 10 tokens in t.We evaluate the performance of a highlightingsystem against a gold standard interestingnessfunction ??
which scores the interestingness of ananchor as the number of user clicks on ?
from theanchor in ?
in our data.
We consider the ideal se-lection to then consist of the ?
most interestinganchors according to ??.
A natural metric for thistask is Normalized Discounted Cumulative Gain(NDCG) (Jarvelin and Kekalainen 2000).We evaluate our models on the EVAL datasetdescribed in Section 2.
We utilize the transitiondistributions in EVAL to create three other testsets, following the stratified sampling methodol-ogy commonly employed in the IR community,for the frequently, less frequently, and rarelyviewed source pages, referred to as HEAD,TORSO, and TAIL, respectively.
We obtainthese sets by first sorting the unique source docu-ments according to their frequency of occurrencein EVAL.
We then partition the set so that HEADcorresponds to all transitions from the sourcepages at the top of the list that account for 20% ofthe transitions in EVAL; TAIL corresponds to thetransitions at the bottom also accounting for 20%of the transitions in EVAL; and TORSO corre-sponds to the remaining transitions.4.1 Main ResultsTable 1 summarizes the results of various modelsover the three test sets using NDCG at truncationlevels 1, 5, and 10.Rows 1 to 3 are simple heuristic baselines.RAND selects ?
random anchors, 1stK selectsthe first ?
anchors and LastK the last ?
anchors.The other models in Table 1 are boosted treebased rankers trained on TRAIN_2 described inSection 2.
They vary only in their features.
Theranker in Row 4 uses Non-Semantic Features(NSF) only.
These features are derived from the# Models HEAD TORSO TAIL@1 @5 @10 @1 @5 @10 @1 @5 @10srconly1 RAND 0.041 0.062 0.081 0.036 0.076 0.109 0.062 0.195 0.2582 1stK 0.010 0.177 0.243 0.072 0.171 0.240 0.091 0.274 0.3483 LastK 0.170 0.022 0.027 0.022 0.044 0.062 0.058 0.166 0.2194 NSF 0.215 0.253 0.295 0.139 0.229 0.282 0.109 0.293 0.3655 NSF+WCAT 0.438 0.424 0.463 0.194 0.290 0.346 0.118 0.317 0.3866 NSF+JTT 0.220 0.302 0.343 0.141 0.241 0.295 0.111 0.300 0.3697 NSF+DSSM_BOW 0.312 0.351 0.391 0.162 0.258 0.313 0.110 0.299 0.3728 NSF+DSSM 0.362 0.386 0.421 0.178 0.275 0.330 0.116 0.312 0.382src+tar 9 NSF+WCAT 0.505 0.475 0.501 0.224 0.304 0.356 0.129 0.324 0.391 10 NSF+JTT 0.345 0.380 0.418 0.183 0.280 0.332 0.131 0.321 0.39011 NSF+DSSM_BOW 0.416 0.393 0.428 0.197 0.274 0.325 0.123 0.311 0.38012 NSF+DSSM 0.554 0.524 0.547 0.241 0.317 0.367 0.135 0.329 0.398Table 1: Highlighting task performance (NDCG @ K) of interest models over HEAD, TORSO andTAIL test sets.
Bold indicates statistical significance over all non-shaded results using t-test (?
?0.05).7source document s and from user session infor-mation in the browser log.
The document featuresinclude: position of the anchor in the document,frequency of the anchor, and anchor density in theparagraph.The rankers in Rows 5 to 12 use the NSF andthe semantic features computed from source andtarget documents of a browsing transition.
Wecompare semantic features derived from three dif-ferent sources.
The first feature source comesfrom our DSSMs (DSSM and DSSM_BOW) us-ing the output layers as feature generators as de-scribed in Section 3.3.
DSSM is the model de-scribed in Section 3 and DSSM_BOW is themodel proposed by Huang et al.
(2013) wheredocuments are view as bag of words (BOW) andthe convolutional and max-pooling layers are notused.
The two other sources of semantic featuresare used as a point of comparison to the DSSM.One is a generative semantic model (Joint Transi-tion Topic model, or JTT) (Gamon et al.
2013).JTT is an LDA-style model (Blei et al.
2003) thatis trained jointly on source and target documentslinked by browsing transitions.
JTT generates atotal of 150 features from its latent variables, 50each for the source topic model, the target topicmodel and the transition model.
The other seman-tic model of contrast is a manually defined one,which we use to assess the effectiveness of auto-matically learned models against human model-ers.
To this effect, we use the page categories thateditors assign in Wikipedia as semantic features(WCAT).
These features number in the multiplethousands.
Using features such as WCAT is not aviable solution in general since Wikipedia catego-ries are not available for all documents.
As such,we use it solely as a point of comparison againstDSSM and JTT.We also distinguish between two types oflearned rankers: those which draw their featuresonly from the source (src only) document andthose that draw their features from both the sourceand target (src+tar) documents.
Although ourtask setting allows access to the content of bothsource and target documents, there are practicalscenarios where a system should predict what in-terests the user without looking at the target doc-ument because the extra step of identifying a suit-able target document for each candidate conceptor entity of interest is computationally expensive.4.2 Analysis of ResultsAs shown in Table 1, NSF+DSSM, which incor-porates our DSSM, is the overall best performingsystem across test sets.
The task is hard as evi-denced by the weak baseline scores.
One reason isthe large average number of candidates per page.On HEAD, we found an average of 170 anchors(of which 95 point to a unique target URL).
ForTORSO and TAIL, we found the average numberof anchors to be 94 (52 unique targets) and 41 (19unique targets), respectively.Clearly, the semantics of the documents formimportant signals for this task: WCAT, JTT,DSSM_BOW, and DSSM all significantly boostthe performance over NSF alone.
There are twointeresting comparisons to consider: (a) manualsemantics vs. learned semantics; and (b) deep se-mantic models vs. generative topic models.
On(a), we observe somewhat surprisingly that thelearned DSSM produces features that outperformthe thousands of features coming from manually(editor) assigned Wikipedia category features(WCAT), in all but the TAIL where the two per-form statistically the same.
In contrast, featuresfrom the generative model (JTT) perform worsethan WCAT across the board except on TAILwhere JTT and WCAT are statistically tied.
On(b), we observe that DSSM outperforms a state-of-the-art generative model (JTT) on HEAD andTORSO.
On TAIL, they are statistically indistin-guishable.We turn now to inspecting the scenario wherefeatures are only drawn from the source document(Rows 1-8 in Table 1).
Again we observe that se-mantic features significantly boost the perfor-mance against NSF alone, however they signifi-cantly deteriorate when compared to using fea-tures from both source and target documents.
Inthis scenario, the manual semantics from WCAToutperform all other models, but with a diminish-ing effect as we move from HEAD throughTORSO to TAIL.
DSSM is the best performinglearned semantic model.Finally, we present the results to justify the twomodifications we made to extend the model ofHuang et al.
(2013) to the DSSM, as described inSection 1.
First, we see in Table 1 thatDSSM_BOW, which has the same network struc-ture of Huang et al.
?s model, is much weaker thanDSSM, demonstrating the benefits of using con-volutional and max-pooling layers to extract se-mantic features for the highlighting task.
Second,we conduct several experiments by using the co-sine scores between the output layers of DSSMfor ?
and ?
as features (following the procedure inSection 3.3 for using the DSSM as a direct imple-mentation of ?).
We found that adding the cosine8features to NSF+DSSM does not lead to any im-provement.
We also combined NSF with solelythe cosine features from DSSM (i.e., without theother semantic features drawn from its output lay-ers).
But we still found no improvement over us-ing NSF alone.
Thus, we conclude that for thistask it is much more effective to feed the featuresderived from DSSM to a supervised ranker thandirectly computing the interestingness score usingcosine similarity in the learned semantic space, asin Huang et al.
(2013).5 Experiments on Entity SearchWe construct the evaluation data set for this sec-ond task by randomly sampling a set of documentsfrom a traffic-weighted set of Web documents.
Ina second step, we identify the entity names in eachdocument using an in-house named entity recog-nizer.
We issue each entity name as a query to acommercial search engine, and retain up to thetop-100 retrieved documents as candidate targetdocuments.
We form for each entity a source doc-ument which consists of the entity text and its sur-rounding text defined by a 200-word window.
Wedefine the focus (as in Figure 1) in ?
as the entitytext, and the focus in ?
as the first 10 tokens in ?.The final evaluation data set contains 10,000source documents.
On average, each source docu-ment is associated with 87 target documents.
Fi-nally, the source-target document pairs are labeledin terms of interestingness by paid annotators.
Thelabel is on a 5-level scale, 0 to 4, with 4 meaningthe target document is the most interesting to thesource document and 0 meaning the target is of nointerest.We test our models on two scenarios.
The firstis a ranking scenario where ?
interesting docu-ments are displayed to the user.
Here, we selectthe top-?
ranked documents according to their in-terestingness scores.
We measure the performancevia NDCG at truncation levels 1 and 3.
The sec-ond scenario is to display to the user all interestingresults.
In this scenario, we select all target docu-ments with an interestingness score exceeding apredefined threshold.
We evaluate this scenariousing ROC analysis and, specifically, the area un-der the curve (AUC).5.1 Main ResultsThe main results are summarized in Table 2.
Rows1 to 6 are single model results, where each modelis used as a direct implementation of the interest-ingness function ?.
Rows 7 to 9 are ranker results,where ?
is defined as a boosted tree based rankerthat incorporates different sets of features ex-tracted from source and target documents, includ-ing the features derived from single models.
As inthe highlighting experiments, all the machine-learned single models, including the DSSM, aretrained on TRAIN_1, and all the rankers aretrained on TRAIN_2.5.2 Analysis of ResultsBM25 (Rows 1 and 2 in Table 2) is the classicdocument model (Robertson and Zaragoza 2009).It uses the bag-of-words document representationand the BM25 term weighting function.
In our set-ting, we define the interestingness score of a doc-ument pair as the dot product of their BM25-weighted term vectors.
To verify the importanceof using contextual information, we compare twodifferent ways of forming the term vector of asource document.
The first only uses the entitytext (Row 1).
The second (Row 2) uses both theentity text and and its surrounding text in a 200-word window (i.e., the entire source document).Results show that the model using contextual in-formation is significantly better.
Therefore, all theother models in this section use both the entitytexts and their surrounding text.WTM (Row 3) is our implementation of theword translation model for IR (Berger and Laf-ferty 1999; Gao et al.
2010).
WTM defines the in-terestingness score as:??
?, ??
?
?
?
????|???????|??????????
,# Models @1 @3 AUC1 BM25 (entity)  0.133 0.195 0.5832 BM25 0.142 0.227 0.6753 WTM 0.191 0.287 0.6784 BLTM 0.214 0.306 0.7045 DSSM 0.259* 0.356* 0.711*6 DSSM_BOW 0.223 0.322 0.6997 Baseline ranker 0.283 0.360 0.7238 7 + DSSM(1) 0.301# 0.385# 0.758#9 7 + DSSM(600) 0.327## 0.402## 0.782##Table 2: Contextual entity search task perfor-mance (NDCG @ K and AUC).
* indicates sta-tistical significance over all non-shaded singlemodel results (Rows 1 to 6) using t-test (?
?0.05).
# indicates statistical significance over re-sults in Row 7.
## indicates statistical signifi-cance over results in Rows 7 and 8.9where ????|??
is the unigram probability of word??
in ?, and ????|???
is the probability of trans-lating ??
into ?
?, trained on source-target docu-ment pairs using EM (Brown et al.
1993).
Thetranslation-based approach allows any pair ofnon-identical but semantically related words tohave a nonzero matching score.
As a result, it sig-nificantly outperforms BM25.BTLM (Row 4) follows the best performingbilingual topic model described in Gao et al.
(2011), which is an extension of PLSA (Hofmann1999).
The model is trained on source-target doc-ument pairs using the EM algorithm with a con-straint enforcing a source document ?
and its tar-get document ?
to not only share the same priortopic distribution, but to also have similar frac-tions of words assigned to each topic.
BLTM de-fines the interestingness score between s and t as:??
?, ??
?
?
?
????|??????|????????
.The model assumes the following story of gener-ating ?
from ?.
First, for each topic ?
a word dis-tribution ??
is selected from a Dirichlet prior withconcentration parameter ?
.
Second, given ?
, atopic distribution ??
is drawn from a Dirichletprior with parameter ?
.
Finally, ?
is generatedword by word.
Each word ??
is generated by firstselecting a topic ?
according to ??
, and thendrawing a word from ??
.
We see that BLTM models interestingness by taking into account thesemantic topic distribution of the entire docu-ments.
Our results in Table 2 show that BLTMoutperforms WTM by a significant margin inboth NDCG and AUC.DSSM (Row 5) outperforms all the competingsingle models, including the state-of-the-art topicmodel BLTM.
Now, we inspect the difference be-tween DSSM and BLTM in detail.
Although bothmodels strive to generate the semantic representa-tion of a document, they use different modelingapproaches.
BLTM by nature is a generativemodel.
The semantic representation in BLTM is adistribution of hidden semantic topics.
Such a dis-tribution is learned using Maximum LikelihoodEstimation in an unsupervised manner, i.e., max-imizing the log-likelihood of the source-targetdocument pairs in the training data.
On the otherhand, DSSM represents documents as points in ahidden semantic space using a supervised learningmethod, i.e., paired documents are closer in thatlatent space than unpaired ones.
We believe thatthe superior performance of DSSM is largely dueto the fact that the model parameters are discrimi-natively trained using an objective that is tailoredto the interestingness task.In addition to the difference in training meth-ods, DSSM and BLTM also use different modelstructures.
BLTM treats a document as a bag ofwords (thus losing some important contextual in-formation such as word order and inter-word de-pendencies), and generates semantic representa-tions of documents using linear projection.DSSM, on the other hand, treats text as a sequenceof words and better captures local and global con-text, and generates highly non-linear semanticfeatures via a deep neural network.
To further ver-ify our analysis, we inspect the results of a variantof DSSM, denoted as DSSM_BOW (Row 6),where the convolution and max-pooling layers areremoved.
This model treats a document as a bagof words, just like BLTM.
These results demon-strate that the effectiveness of DSSM can also beattributed to the convolutional architecture in theneural network, in addition to being deep and be-ing discriminative.We turn now to discussing the ranker results inRows 7 to 9.
The baseline ranker (Row 7) uses 158features, including many counts and single modelscores, such as BM25 and WMT.
DSSM (Row 5)alone is quite effective, being close in perfor-mance to the baseline ranker with non-DSSM fea-tures.
Integrating the DSSM score computed in (5)as one single feature into the ranker (Row 8) leadsto a significant improvement over the baseline.The best performing combination (Row 9) is ob-tained by incorporating the DSSM feature vectorsof source and target documents (i.e., 600 featuresin total) in the ranker.We thus conclude that on both tasks, automatichighlighting and contextual entity search, featuresdrawn from the output layers of our deep semanticmodel result in significant gains after being addedto a set of non-semantic features, and in compari-son to other types of semantic models used in thepast.6 Related WorkIn addition to the notion of relevance as describedin Section 1, related to interestingness is also thenotion of salience (also called aboutness) (Gamonet al.
2013; 2014; Parajpe 2009; Yih et al.
2006).Salience is the centrality of a term to the contentof a document.
Although salience and interesting-ness interact, the two are not the same.
For exam-ple, in a news article about President Obama?svisit to Seattle, Obama is salient, yet the averageuser would probably not be interested in learningmore about Obama while reading that article.10There are many systems that identify popularcontent in the Web or recommend content (e.g.,Bandari et al.
2012; Lerman and Hogg 2010;Szabo and Huberman 2010), which is closely re-lated to the highlighting task.
In contrast to theseapproaches, we strive to predict what term a useris likely to be interested in when reading content,which may or may not be the same as the mostpopular content that is related to the current docu-ment.
It has empirically been demonstrated inGamon et al.
(2013) that popularity is in fact a ra-ther poor predictor for interestingness.
The task ofcontextual entity search, which is formulated as aninformation retrieval problem in this paper, is alsorelated to research on entity resolution (Stefanidiset al.
2013).Latent Semantic Analysis (Deerwester et al.1990) is arguably the earliest semantic model de-signed for IR.
Generative topic models widelyused for IR include PLSA (Hofmann 1990) andLDA (Blei et al.
2003).
Recently, these modelshave been extended to handle cross-lingual cases,where there are pairs of corresponding documentsin different languages (e.g., Dumais et al.
1997;Gao et al.
2011; Platt et al.
2010; Yih et al.
2011).By exploiting deep architectures, deep learningtechniques are able to automatically discover fromtraining data the hidden structures and the associ-ated features at different levels of abstraction use-ful for a variety of tasks (e.g., Collobert et al.2011; Hinton et al.
2012; Socher et al.
2012;Krizhevsky et al., 2012; Gao et al.
2014).
Hintonand Salakhutdinov (2010) propose the most origi-nal approach based on an unsupervised version ofthe deep neural network to discover the hierar-chical semantic structure embedded in queries anddocuments.
Huang et al.
(2013) significantly ex-tends the approach so that the deep neural networkcan be trained on large-scale query-documentpairs giving much better performance.
The use ofthe convolutional neural network for text pro-cessing, central to our DSSM, was also describedin Collobert et al.
(2011) and Shen et al.
(2014)but with very different applications.
The DSSMdescribed in Section 3 can be viewed as a variantof the deep neural network models used in theseprevious studies.7 ConclusionsModeling interestingness is fundamental to manyonline recommendation systems.
We obtain natu-rally occurring interest signals by observing Webbrowsing transitions where users click from onewebpage to another.
We propose to model this?interestingness?
with a deep semantic similaritymodel (DSSM), based on deep neural networkswith special convolutional-pooling structure,mapping source-target document pairs to featurevectors in a latent semantic space.
We train theDSSM using browsing transitions between docu-ments.
Finally, we demonstrate the effectivenessof our model on two interestingness tasks: auto-matic highlighting and contextual entity search.Our results on large-scale, real-world datasetsshow that the semantics of documents computedby the DSSM are important for modeling interest-ingness and that the new model leads to signifi-cant improvements on both tasks.
DSSM is shownto outperform not only the classic document mod-els that do not use (latent) semantics but also state-of-the-art topic models that do not have the deepand convolutional architecture characterizing theDSSM.One area of future work is to extend ourmethod to model interestingness given an entireuser session, which consists of a sequence ofbrowsing events.
We believe that the prior brows-ing and interaction history recorded in the sessionprovides additional signals for predicting interest-ingness.
To capture such signals, our model needsto be extended to adequately represent time series(e.g., causal relations and consequences of ac-tions).
One potentially effective model for such apurpose is based on the architecture of recurrentneural networks (e.g., Mikolov et al.
2010; Chenand Deng, 2014), which can be incorporated intothe deep semantic model proposed in this paper.Additional AuthorsYelong Shen (Microsoft Research, One MicrosoftWay, Redmond, WA 98052, USA, email:yeshen@microsoft.com).AcknowledgmentsThe authors thank Johnson Apacible, PradeepChilakamarri, Edward Guo, Bernhard Kohlmeier,Xiaolong Li, Kevin Powell, Xinying Song andYe-Yi Wang for their guidance and valuable dis-cussions.
We also thank the three anonymous re-viewers for their comments.ReferencesBandari, R., Asur, S., and Huberman, B.
A.
2012.The pulse of news in social media: forecastingpopularity.
In ICWSM.11Bengio, Y., 2009.
Learning deep architectures forAI.
Fundamental Trends in Machine Learning,2(1):1?127.Berger, A., and Lafferty, J.
1999.
Information re-trieval as statistical translation.
In SIGIR, pp.222-229.Blei, D. M., Ng, A. Y., and Jordan, M. J.
2003.Latent Dirichlet allocation.
Journal of MachineLearning Research, 3.Broder, A., Fontoura, M., Josifovski, V., andRiedel, L. 2007.
A semantic approach to contex-tual advertising.
In SIGIR.Brown, P. F., Della Pietra, S. A., Della Pietra, V.J., and Mercer, R. L. 1993.
The mathematics ofstatistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263-311.Burges, C., Shaked, T., Renshaw, E., Lazier, A.,Deeds, M., Hamilton, and Hullender, G. 2005.Learning to rank using gradient descent.
InICML, pp.
89-96.Chen, J. and Deng, L. 2014.
A primal-dual methodfor training recurrent neural networks con-strained by the echo-state property.
In ICLR.Collobert, R., Weston, J., Bottou, L., Karlen, M.,Kavukcuoglu, K., and Kuksa, P., 2011.
Naturallanguage processing (almost) from scratch.Journal of Machine Learning Research, vol.
12.Deerwester, S., Dumais, S. T., Furnas, G. W.,Landauer, T., and Harshman, R. 1990.
Indexingby latent semantic analysis.
Journal of theAmerican Society for Information Science,41(6): 391-407Deng, L., Hinton, G., and Kingsbury, B.
2013.New types of deep neural network learning forspeech recognition and related applications: Anoverview.
In ICASSP.Deng, L., Abdel-Hamid, O., and Yu, D., 2013a.
Adeep convolutional neural network using heter-ogeneous pooling for trading acoustic invari-ance with phonetic confusion.
In ICASSP.Dumais, S. T., Letsche, T. A., Littman, M. L., andLandauer, T. K. 1997.
Automatic cross-linguis-tic information retrieval using latent semanticindexing.
In AAAI-97 Spring Symposium Series:Cross-Language Text and Speech Retrieval.Friedman, J. H. 1999.
Greedy function approxi-mation: a gradient boosting machine.
Annals ofStatistics, 29:1189-1232.Gamon, M., Mukherjee, A., Pantel, P. 2014.
Pre-dicting interesting things in text.
In COLING.Gamon, M., Yano, T., Song, X., Apacible, J. andPantel, P. 2013.
Identifying salient entities inweb pages.
In CIKM.Gao, J., He, X., and Nie, J-Y.
2010.
Clickthrough-based translation models for web search: fromword models to phrase models.
In CIKM.
pp.1139-1148.Gao, J., He, X., Yih, W-t., and Deng, L. 2014.Learning continuous phrase representations fortranslation modeling.
In ACL.Gao, J., Toutanova, K., Yih., W-T. 2011.
Click-through-based latent semantic models for websearch.
In SIGIR.
pp.
675-684.Graves, A., Mohamed, A., and Hinton, G. 2013.Speech recognition with deep recurrent neuralnetworks.
In ICASSP.Gutmann, M. and Hyvarinen, A.
2010.
Noise-con-trastive estimation: a new estimation principlefor unnormalized statistical models.
In Proc.Int.
Conf.
on Artificial Intelligence and Statis-tics (AISTATS2010).Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed,A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-yen, P., Sainath, T., and Kingsbury, B., 2012.Deep neural networks for acoustic modeling inspeech recognition.
IEEE Signal ProcessingMagazine, 29:82-97.Hinton, G., and Salakhutdinov, R., 2010.
Discov-ering binary codes for documents by learningdeep generative models.
Topics in CognitiveScience, pp.
1-18.Hofmann, T. 1999.
Probabilistic latent semanticindexing.
In SIGIR.
pp.
50-57.Huang, P., He, X., Gao, J., Deng, L., Acero, A.,and Heck, L. 2013.
Learning deep structured se-mantic models for web search using click-through data.
In CIKM.Jarvelin, K. and Kekalainen, J.
2000.
IR evalua-tion methods for retrieving highly relevant doc-uments.
In SIGIR.
pp.
41-48.Krizhevsky, A., Sutskever, I. and Hinton, G.2012.
ImageNet classification with deep convo-lutional neural networks.
In NIPS.Lerman, K., and Hogg, T. 2010.
Using a model ofsocial dynamics to predict popularity of news.In WWW.
pp.
621-630.Markoff, J.
2014.
Computer eyesight gets a lotmore accurate.
In New York Times.Mikolov, T.. Karafiat, M., Burget, L., Cernocky,J., and Khudanpur, S. 2010.
Recurrent neuralnetwork based language model.
InINTERSPEECH.
pp.
1045-1048.Paranjpe, D. 2009.
Learning document aboutnessfrom implicit user feedback and documentstructure.
In CIKM.12Platt, J., Toutanova, K., and Yih, W. 2010.Translingual document representations fromdiscriminative projections.
In EMNLP.
pp.
251-261.Ricci, F., Rokach, L., Shapira, B., and Kantor, P.B.
(eds) 2011.
Recommender System Handbook,Springer.Robertson, S., and Zaragoza, H. 2009.
The proba-bilistic relevance framework: BM25 and be-yond.
Foundations and Trends in InformationRetrieval, 3(4):333-389.Shen, Y., He, X., Gao.
J., Deng, L., and Mesnil, G.2014.
A latent semantic model with convolu-tional-pooling structure for information re-trieval.
In CIKM.Socher, R., Huval, B., Manning, C., Ng, A., 2012.Semantic compositionality through recursivematrix-vector spaces.
In EMNLP.Stefanidis, K., Efthymiou, V., Herschel, M., andChristophides, V. 2013.
Entity resolution in theweb of data.
CIKM?13 Tutorial.Szabo, G., and Huberman, B.
A.
2010.
Predictingthe popularity of online content.
Communica-tions of the ACM, 53(8).Wu, Q., Burges, C.J.C., Svore, K., and Gao, J.2009.
Adapting boosting for information re-trieval measures.
Journal of Information Re-trieval, 13(3):254-270.Yih, W., Goodman, J., and Carvalho, V. R. 2006.Finding advertising keywords on web pages.
InWWW.Yih, W., Toutanova, K., Platt, J., and Meek, C.2011.
Learning discriminative projections fortext similarity measures.
In CoNLL.13
