A Memory-Based Approach to Learning Shallow NaturalLanguage PatternsSh lomo Argamon and Ido Dagan and Yuva l  K rymolowsk iDepar tment  of Mathemat ics  and Computer  ScienceBar- I lan University52900 Ramat  Gan, Israel{ argamon, dagan, yuvalk}@cs, b iu .
ac.
ilAbst rac tRecognizing shallow linguistic patterns, such as ba-sic syntactic relationships between words, is a com-mon task in applied natural language and text pro-cessing.
The common practice for approaching thistask is by tedious manual definition of possible pat-tern structures, often in the form of regular expres-sions or finite automata.
This paper presents a novelmemory-based l arning method that recognizes shal-low patterns in new text based on a bracketed train-ing corpus.
The training data are stored as-is, inefficient suffix-tree data structures.
Generalizationis performed on-line at recognition time by compar-ing subsequences of the new text to positive andnegative vidence in the corpus.
This way, no in-formation in the training is lost, as can happen inother learning systems that construct a single gen-eralized model at the time of training.
The paperpresents experimental results for recognizing nounphrase, subject-verb and verb-object patterns in En-glish.
Since the learning approach enables easy port-ing to new domains, we plan to apply it to syntac-tic patterns in other languages and to sub-languagepatterns for information extraction.1 In t roduct ionIdentifying local patterns of syntactic sequences andrelationships i  a fundamental task in natural an-guage processing (NLP).
Such patterns may corre-spond to syntactic phrases, like noun phrases, or topairs of words that participate in a syntactic rela-tionship, like the heads of a verb-object relation.Such patterns have been found useful in variousapplication areas, including information extraction,text summarization, and bilingual alignment.
Syn-tactic patterns are useful also for many basic com-putational linguistic tasks, such as statistical wordsimilarity and various disambiguation problems.One approach for detecting syntactic patterns i toobtain a full parse of a sentence and then extract herequired patterns.
However, obtaining a completeparse tree for a sentence is difficult in many cases,and may not be necessary at all for identifying mostinstances of local syntactic patterns.An alternative approach is to avoid the complex-ity of full parsing and instead to rely only on localinformation.
A variety of methods have been devel-oped within this framework, nown as shallow pars-ing, chunking, local parsing etc.
(e.g., (Abney, 1991;Greffenstette, 1993)).
These works have shown thatit is possible to identify most instances of local syn-tactic patterns by rules that examine only the pat-tern itself and its nearby context.
Often, the rulesare applied to sentences that were tagged by part-of-speech (POS) and are phrased by some form ofregular expressions or finite state automata.Manual writing of local syntactic rules has becomea common practice for many applications.
However,writing rules is often tedious and time consuming.Furthermore, xtending the rules to different lan-guages or sub-language domains can require sub-stantial resources and expertise that are often notavailable.
As in many areas of NLP, a learning ap-proach is appealing.
Surprisingly, though, rather lit-tle work has been devoted to learning local syntacticpatterns, mostly noun phrases (Ramshaw and Mar-cus, 1995; Vilain and Day, 1996).This paper presents a novel general learning ap-proach for recognizing local sequential patterns, thatmay be perceived as falling within the memory-based learning paradigm.
The method utilizes apart-of-speech tagged training corpus in which all in-stances of the target pattern are marked (bracketed).The training data are stored as-is in suffix-tree datastructures, which enable linear time searching forsubsequences in the corpus.The memory-based nature of the presented algo-rithm stems from its deduction strategy: a new in-stance of the target pattern is recognized by exam-ining the raw training corpus, searching for positiveand negative vidence with respect o the given testsequence.
No model is created for the training cor-pus, and the raw examples are not converted to anyother representation.Consider the following example 1.
Suppose we1We use here the POS tags: DT ---- determiner, ADJ= adjective, hDV = adverb, C0NJ = conjunction, VB=verb,PP=preposition, NN = singular noun, and NNP ---- plural noun.67want to decide whether the candidate sequenceDT ADJ ADJ NN NNPis a noun phrase (NP) by comparing it to the train-ing corpus.
A good match would be if the entiresequence appears as-is several times in the corpus.However, due to data sparseness, an exact matchcannot always be expected.A somewhat weaker match may be obtained if weconsider sub-parts of the candidate sequence (calledtiles).
For example, suppose the corpus containsnoun phrase instances with the following structures:(i) DT ADJ ADJ NN NN(2) DT ADJ NN NNPThe first structure provides positive evidence thatthe sequence "DT ADJ ADJ NN" is a possible NP pre-fix while the second structure provides evidence for"ADJ NN NNP" being an NP suffix.
Together, thesetwo training instances provide positive evidence thatcovers the entire candidate.
Considering evidencefor sub-parts of the pattern enables us to general-ize over the exact structures that are present in thecorpus.
Similarly, we also consider the negative vi-dence for such sub-parts by noting where they occurin the corpus without being a corresponding part ofa target instance.The proposed method, as described in detail inthe next section, formalizes this type of reasoning.
Itsearches specialized ata structures for both positiveand negative vidence for sub-parts of the candidatestructure, and considers additional factors such ascontext and evidence overlap.
Section 3 presents ex-perimental results for three target syntactic patternsin English, and Section 4 describes related work.2 The  A lgor i thmThe input to the Memory-Based Sequence Learning(MBSL) algorithm is a sentence represented as a se-quence of POS tags, and its output is a bracketedsentence, indicating which subsequences of the sen-tence are to be considered instances of the targetpattern (target instances).
MBSL determines thebracketing by first considering each subsequence ofthe sentence as a candidate to be a target instance.It computes a score for each candidate by comparingit to the training corpus, which consists of a set ofpre-bracketed sentences.
The algorithm then finds aconsistent bracketing for the input sentence, givingpreference to high scoring subsequences.
In the re-mainder of this section we describe the scoring andbracketing methods in more detail.2.1 Scor ing cand idatesWe first describe the mechanism for scoring an in-dividual candidate.
The input is a candidate sub-sequence, along with its context, i.e., the other tagsin the input sentence.
The method is presented attwo levels: a general memory-based learning schemaand a particular instantiation of it.
Further instan-tiations of the schema re expected in future work.2.1.1 The  genera l  MBSL  schemaThe MBSL scoring algorithm works by consideringsituated candidates.
A situated candidate is a sen-tence containing one pair of brackets, indicating acandidate to be a target instance.
The portion ofthe sentence between the brackets is the candidate(as above), while the portion before and after thecandidate is its context.
(Although we describe thealgorithm here for the general case of unlimited con-text, for computational reasons our implementationonly considers a limited amount of context on eitherside of the candidate.)
This subsection describeshow to compute the score of a situated candidatefrom the training corpus.The idea of the MBSL scoring algorithm is to con-struct a tiling of subsequences of a situated candi-date which covers the entire candidate.
We con-sider as tiles subsequences of the situated candidatewhich contain a bracket.
(We thus consider only tileswithin or adjacent to the candidate that also includea candidate boundary.
)Each tile is assigned a score based on its occur-rence in the training memory.
Since brackets cor-respond to the boundaries of potential target in-stances, it is important o consider how the bracketpositions in the tile correspond to those in the train-ing memory.For example, consider the training sentence\[ NN \] VB \[ ADJ NN NN \] ADV PP \[ NN \]We may now examine the occurrence in this sentenceof several possible tiles:VB \[ ADJ NN occurs positively in the sentence, andNN NN \] ADV also occurs positively, whileNN \[ NN ADV occurs negatively in the training sen-tence, since the bracket does not correspond.The positive evidence for a tile is measured by itspositive count, the number of times the tile (in-cluding brackets) occurs in the training memorywith corresponding brackets.
Similarly, the nega-tive evidence for a tile is measured by its negativecount, the number of times that the POS sequenceof the tile occurs in the training memory with non-corresponding brackets (either brackets in the train-ing where they do not occur in the tile, or vice versa).The total count of a tile is its positive count plus itsnegative count, that is, the total count of the POSsequence of the tile, regardless of bracket position.The score \](t) of a tile t is a function of its positiveand negative counts.68Candidate: NN VB \[ ADJ NN NN \] ADVMTile I: VB \[ ADJ NN NN \]MTile 2: VB \[ ADJMTile 3: \[ ADJ NNMTile 4: NN NN \]MTile 5: NN \] ADVFigure 1: A candidate subsequence with some of itscontext, and 5 matching tiles found in the trainingcorpus.The overall score of a situated candidate is gen-erally a function of the scores of all the tiles for thecandidate, as well as the relations between the tiles'positions.
These relations include tile adjacency,overlap between tiles, the amount of context in atile, and so on.2.1.2 An ins tant ia t ion  of  the  MBSL  schemaIn our instantiation of the MBSL schema, we definethe score f i t )  of a tile t as the ratio of its positivecount pos(t) and its total count total(t):1 if -P-P--~!
!- > 0 total(t)I(t) = 0 otherwisefor a predefined threshold O. Tiles with a score of1, and so with sufficient positive evidence, are calledmatching tiles.Each matching tile gives supporting evidence thata part of the candidate can be a part of a target in-stance.
In order to combine this evidence, we try tocover the entire candidate by a set of matching tiles,with no gaps.
Such a covering constitutes evidencethat the entire candidate is a target instance.
Forexample, consider the matching tiles shown for thecandidate in Figure 1.
The set of matching tiles 2,4, and 5 covers the candidate, as does the set of tiles1 and 5.
Also note that tile 1 constitutes a cover onits own.To make this precise, we first say that a tile T1connects to a tile T2 if (i) T2 starts after T1 starts,(ii) there is no gap between the end of T1 and thestart of T2 (there may be some overlap), and (iii) T2ends after T1 (neither tile includes the other).
Forexample, tiles 2 and 4 in the figure connect, whiletiles 2 and 5 do not, and neither do tiles 1 and 4(since tile 1 includes tile 4 as a subsequence).A cover for a situated candidate c is a sequenceof matching tiles which collectively cover the en-tire candidate, including the boundary brackets, andpossibly some context, such that each tile connectsto the following one.
A cover thus provides posi-tive evidence for the entire sequence of tags in thecandidate.The set of all the covers for a candidate summa-rizes all of the evidence for the candidate being atarget instance.
We therefore compute the score ofa candidate as a function of some statistics of theset of all its covers.
For example, if a candidate hasmany different covers, it is more likely to be a targetinstance, since many different pieces of evidence canbe brought o bear.We have empirically found several statistics of thecover set to be useful.
These include, for each cover,the number of tiles it contains, the total number ofcontext ags it contains, and the number of positionswhich more than one tile covers (the amount of over-lap).
We thus compute, for the set of all covers of acandidate c, the?
Total number of different covers, num(c),* Minimum number of matches in any cover,minsize(c),?
Maximum amount of context in any cover,maxcontext (c ) ,  and?
Maximum total overlap between tiles for anycover, maxover lap(c) .Each of these items gives an indication regarding theoverall strength of the cover-based evidence for thecandidate.The score of the candidate is a linear function ofits statistics:f(c) = anum(c)  - 13minsize(c)+3' maxcontext  (c) +maxover lap  (c)If candidate c has no covers, we set f(c) = O. Notethat mins ize is weighted negatively, since a coverwith fewer tiles provides stronger evidence for thecandidate.In the current implementation, the weights werechosen so as to give a lexicographic ordering, pre-ferring first candidates with more covers, then thosewith covers containing fewer tiles, then those withlarger contexts, and finally, when all else is equal,preferring candidates with more overlap betweentiles.
We plan to investigate in the future a data-driven approach (based on the Winnow algorithm)for optimal selection and weighting of statistical fea-tures of the score.We compute a candidate's statistics efficiently byperforming a depth-first traversal of the cover graphof the candidate.
The cover graph is a directedacyclic graph (DAG) whose nodes represent match-ing tiles of the candidate, such that an arc existsbetween odes n and n', if tile n connects to n'.
Aspecial start node is added as the root of the DAG,that connects to all of the nodes (tiles) that containan open bracket.
There is a cover corresponding toeach path from the start node to a node (tile) thatcontains a close bracket.
Thus the statistics of all thecovers may be efficiently computed by traversing thecover graph.692.1.3 SummaryGiven a candidate sequence and its context (a situ-ated candidate):1.
Consider all the subsequences of the situatedcandidate which include a bracket as tiles;2.
Compute a tile score as a function of its positivecount and total counts, by searching the train-ing corpus.
Determine which tiles are matchingtiles;3.
Construct the set of all possible covers forthe candidate, that is, sequences of connectedmatching tiles that cover the entire candidate;4.
Compute the candidate score based on thestatistics of its covers.2.2 Searching the training memoryThe MBSL scoring algorithm searches the trainingcorpus for each subsequence of the sentence in or-der to find matching tiles.
Implementing this searchefficiently is therefore of prime importance.
We doso by encoding the training corpus using suffix trees(Edward and McCreight, 1976), which provide stringsearching in time which is linear in the length of thesearched string.Inspired by Satta (1997), we build two suffix treesfor retrieving the positive and total counts for a tile.The first suffix tree holds all pattern instances fromthe training corpus surrounded by bracket symbolsand a fixed amount of context.
Searching a giventile (which includes a bracket symbol) in this treeyields the positive count for the tile.
The secondsuffix tree holds an unbracketed version of the en-tire training corpus.
This tree is used for searchingthe POS sequence of a tile, with brackets omitted,yielding the total count for the tile (recall that thenegative count is the difference between the totaland positive counts).2.3 Selecting candidatesAfter the above procedure, each situated candidateis assigned a score.
In order to select a bracketing forthe input sentence, we assume that target instancesare non-overlapping (this is usually the case for thetypes of patterns with which we experimented).
Weuse a simple constraint propagation algorithm thatfinds the best choice of non-overlapping candidatesin an input sentence:1.
Examine ach situated candidate c withf(c) > 0, in descending order of f(c):(a) Add c's brackets to the sentence;(b) Remove all situated candidates overlappingwith c which have not yet been examined.2.
Return the bracketed sentence.NPVOSVNPVOSVTrain Data:sentences words8936 22959816397 45437516397 454375patterns547601427125024Test Data:sentences words patterns2012 51401 123351921 53604 16261921 53604 3044Table 1: Sizes of training and test dataLen1 16959 312 21577 39 3203 22 7613 303 10264 19 5922 41 7265 294 3630 7 2952 21 3284 135 1460 3 1242 9 1697 76 521 1 506 4 1112 47 199 0 242 2 806 38 69 0 119 1 ,592 29 40 0 44 0 446 210 18 0 20 0 392 2>10 23 0 23 0 1917 8total 54760 14271 25024avg.
len 2.2 3.4 4.5Table 2: Distribution of pattern lengths, total num-ber of patterns and average length in the trainingdata.3 Eva luat ion3.1 The DataWe have tested our algorithm in recognizing threesyntactic patterns: noun phrase sequences (NP),verb-object (VO), and subject-verb (SV) relations.The NP patterns were delimited by ' \[' and ' \ ] 'symbols at the borders of the phrase.
For VO pat-terns, we have put the starting delimiter before themain verb and the ending delimiter after the objecthead, thus covering the whole noun phrase compris-ing the object; for example:... invest igators  s tarted to\[ v iew the lower pr ice levels  \]as at t ract ive  ...We used a similar policy for SV patterns, definingthe start of the pattern at the start of the subjectnoun phrase and the end at the first verb encoun-tered (not including auxiliaries and medals); for ex-ample:... argue that\[ the U.S. should regu late  \]the class ...7090t~OLo.
8070I , , t , I s , , \[ , , ,70 80 90RecallFigure 2: Recall-Precision curves for NP, VO, andSV; 0.1 < 8 < 0.99The subject and object noun-phrase borders werethose specified by the annotators, phrases which con-tain conjunctions or appositives were not further an-alyzed.The training and testing data were derived fromthe Penn TreeBank.
We used the NP data preparedby Ramshaw and Marcus (1995), hereafter RM95.The SV and VO data were obtained using T (Tree-Bank's search script language) scripts.
2 Table 1summarizes the sizes of the training and test datasets and the number of examples in each.The T scripts did not attempt o match depen-dencies over very complex structures, since we areconcerned with shallow, or local, patterns.
Table 2shows the distribution of pattern length in the traindata.
We also did not attempt o extract passive-voice VO relations.3.2 Testing Methodo logyThe test procedure has two parameters: (a) maxi-mum context size of a candidate, which limits whatqueries are performed on the memory, and (b) thethreshold 8 used for establishing a matching tile,which determines how to make use of the query re-sults.Recall and precision figures were obtained for var-ious parameter values.
F~ (van Rijsbergen, 1979), acommon measure in information retrieval, was used2The scripts may be found at the URLhttp://www.cs.biu.ac.il/,-~yuvalk/MBSL.as a single-figure measure of performance:(f12 + 1).
P .
nF~ = f12 .
P + RWe use ~ = 1 which gives no preference to eitherrecall or precision.3.3 ResultsTable 3 summarizes the optimal parameter settingsand results for NP, VO, and SV on the test set.
Inorder to find the optimal values of the context sizeand threshold, we tried 0.1 < t~ < 0.95, and maxi-mum context sizes of 1,2, and 3.
Our experimentsused 5-fold cross-validation  the training data todetermine the optimal parameter settings.In experimenting with the maximum context sizeparameter, we found that the difference between thevalues of F~ for context sizes of 2 and 3 is less than0.5% for the optimal threshold.
Scores for a contextsize of 1 yielded F~ values smaller by more than 1%than the values for the larger contexts.Figure 2 shows recall/precision curves for thethree data sets, obtained by varying 8 while keepingthe maximum context size at its optimal value.
Thedifference between F~=I values for different hresh-olds was always less than 2%.Performance may be measured also on a word-byword basis, counting as a success any word whichwas identified correctly as being part of the tar-get pattern.
That method was employed, alongwith recall/precision, by RM95.
We preferred tomeasure performance by recall and precision forcomplete patterns.
Most errors involved identifica-tions of slightly shifted, shorter or longer sequences.Given a pattern consisting of five words, for example,identifying only a four-word portion of this patternwould yield both a recall and precision errors.
Tag-assignment scoring, on the other hand, will give it ascore of 80%.
We hold the view that such an identi-fication is an error, rather than a partial success.We used the datasets created by RM95 for NPlearning; their results are shown in Table 3.
3 TheF~ difference is small (0.4%), yet they use a richerfeature set, which incorporates lexicai nformation aswell.
The method of Ramshaw and Marcus makes adecision per word, relying on predefined rule tem-plates.
The method presented here makes deci-sions on sequences and uses sequences as its mem-ory, thereby attaining a dynamic perspective of theSNot ice  that our results, as well as those we cite fromRM95, pertains to a training set of 229,000 words.
RM95report also results for a larger training set, of 950,000 words,for which recall/precision is 93.5%/93.1%, correspondingly(F~=93.3%).
Our system needs to be further optimized inorder to handle that amount of data, though our major con-cern in future work is to reduce the overall amount of labeledtraining data.71Thresh.VO 2 0.5 81.3SV 3 0.6 86.1NP 3 0.6 91.4aM95 (NP) I I - IRecall (%) Precision (%)89.8 77.184.5 88.691.6 91.6I 92.3 91.883.086.591.6192.0Table 3: Results with optimal parameter settings for context size and threshold, and breakeven points.
Thelast line shows the results of Ramshaw and Marcus (1995) (recognizing NP's) with the same train/test data.The optimal parameters were obtained by 5-fold cross-validation.9085J80t 1 t. .
.
.
.
.
.
.
.
.
.
.
.
.  '
-  - _ ' _  _ - ,///// NP.
8=0.7 Con.=290/ .
/  SV.
8=0.8  Con.
-3.
//l/=i/IiJL VO.
0~0.3  Con .
-2n2000085Y,8075 ' ' \] ~ ' ~ \] 750 40000' ' ' ' I ' ' ' ' I _ ' -  ' ' ' I ' ' ' i 1 ' ' '/t//NP .
e=o.7  Con .=20ExamplesFigure 3: Learning curves for NP, VO, and SV by number ofSV, 8~0.6  Con .~3 .~ '~i .
I. i "  ~ - - ~/ /i / /I /I~ !
vo.
,-o.~ ~ .
.
.
.
, / /i 00000 200000 300000 400000Wordsexamples (left) and words (right)pattern structure.
We aim to incorporate l xical in-formation as well in the future, it is still unclearwhether that will improve the results.Figure 3 shows the learning curves by amount oftraining examples and number of words in the train-ing data, for particular parameter settings.4 Re la ted  WorkTwo previous methods for learning local syntacticpatterns follow the transformation-based paradigmintroduced by Brill (1992).
Vilain and Day (1996)identify (and classify) name phrases such as com-pany names, locations, etc.
Ramshaw and Marcus(1995) detect noun phrases, by classifying each wordas being inside a phrase, outside or on the boundarybetween phrases.Finite state machines (FSMs) are a natural for-malism for learning linear sequences.
It was usedfor learning linguistic structures other than shallowsyntax.
Gold (1978) showed that learning regularlanguages from positive examples is undecidable inthe limit.
Recently, however, several learning meth-ods have been proposed for restricted classes of FSM.OSTIA (Onward Subsequential Transducer Infer-ence Algorithm; Oncina, Garcia, and Vidal 1993),learns a subsequential transducer in the limit.
Thisalgorithm was used for natural-language tasks by Vi-lar, Marzal, and Vidal (1994) for learning translationof a limited-domain language, as well as by Gildeaand Jurafsky (1994) for learning phonological rules.Ahonen et al (1994) describe an algorithm for learn-ing (k,h)-contextual regular languages, which theyuse for learning the structure of SGML documents.Apart from deterministic FSMs, there are a num-ber of algorithms for learning stochastic models,eg., (Stolcke and Omohundro, 1992; Carrasco andOncina, 1994; Ron et al, 1995).
These algorithmsdiffer mainly by their state-merging strategies, usedfor generalizing from the training data.A major difference between the abovementionedlearning methods and our memory-based approach isthat the former employ generalized models that werecreated at training time while the latter uses thetraining corpus as-is and generalizes only at recog-nition time.Much work aimed at learning models for full pars-ing, i.e., learning hierarchical structures.
We re-fer here only to the DOP (Data Oriented Parsing)method (Bod, 1992) which, like the present work, isa memory-based approach.
This method constructsparse alternatives for a sentence based on combina-tions of subtrees in the training corpus.
The MBSLapproach may be viewed as a linear analogy to DOPin that it constructs a cover for a candidate based72on subsequences of training instances.Other implementations of the memory-basedparadigm for NLP tasks include Daelemans et al(1996), for POS tagging; Cardie (1993), for syntacticand semantic tagging; and Stanfill and Waltz (1986),for word pronunciation.
In all these works, examplesare represented as sets of features and the deductionis carried out by finding the most similar cases.
Themethod presented here is radically different in thatit makes use of the raw sequential form of the data,and generalizes by reconstructing test examples fromdifferent pieces of the training data.5 Conc lus ionsWe have presented a novel general schema nd a par-ticular instantiation of it for learning sequential pat-terns.
Applying the method to three syntactic pat-terns in English yielded positive results, suggestingits applicability for recognizing local linguistic pat-terns.
In future work we plan to investigate a data-driven approach for optimal selection and weightingof statistical features of candidate scores, as well asto apply the method to syntactic patterns of Hebrewand to domain-specific patterns for information ex-traction.6 acknowledgementsThe authors wish to thank Yoram Singer for hiscollaboration in an earlier phase of this researchproject, and Giorgio Satta for helpful discussions.We also thank the anonymous reviewers for their in-structive comments.
This research was supportedin part by grant 498/95-1 from the Israel ScienceFoundation, and by grant 8560296 from the IsraeliMinistry of Science.Re ferencesS.
P. Abney.
1991.
Parsing by chunks.
In R. C.Berwick, S. P. Abney, and C. Tenny, editors,Principle-Based Parsing: Computation and Psy-cholinguistics, pages 257-278.
Kluwer, Dordrecht.H.
Ahonen, H. Mannila, and E. Nikunen.
1994.Forming grammars for structured ocuments: Anapplication of grammatical inference.
In R. C.Carrasco and J. Oncina, editors, Grammatical In-ference and Applications (ICGI-9~), pages 153-167.
Springer, Berlin, Heidelberg.R.
Bod.
1992.
A computational model of languageperformance: Data oriented parsing.
In Coling,pages 855-859, Nantes, France.E.
Brill.
1992.
A simple rule-based part of speechtagger.
In proc.
of the DARPA Workshop onSpeech and Natural Language.C.
Cardie.
1993.
A case-based approach to knowl-edge acquisition for domain-specific sentence anal-ysis.
In Proceedings of the 11th National Con-ference on Artificial Intelligence, pages 798-803,Menlo Park, CA, USA, July.
AAAI Press.R.
C. Carrasco and J. Oncina.
1994.
Learn-ing stochastic regular grammars by means of astate merging method.
In R. C. Carrasco andJ.
Oncina, editors, Grammatical Inference andApplications (ICGI-94), pages 139-152.
Springer,Berlin, Heidelberg.W.
Daelemans, J. Zavrel, Berck P., and Gillis S.1996.
Mbt: A memory-based part of speech tag-ger generator.
In Eva Ejerhed and Ido Dagan, edi-tors, Proceedings of the Fourth Workshop on VeryLarge Corpora, pages 14-27.
ACL SIGDAT.T.
Edward and M. McCreight.
1976. space-economical suffix tree construction algorithm.Journal of the ACM, 23(2):262-272, April.D.
Gildea and D. Jurafsky.
1994.
Automatic induc-tion of finite state transducers for simple phono-logical rules.
Technical Report TR-94-052, In-ternational Computer Science Institute, Berkeley,CA, October.E.
M. Gold.
1978.
Complexity of automaton iden-tification from given data.
Information and Con-trol, 37:302-320.Gregory Greffenstette.
1993.
Evaluation techniquesfor automatic semantic extraction: Comparingsyntactic and window based approaches.
In A CLWorkshop on Acquisition of Lexical KnowledgeFrom Text, Ohio State University, June.L.
A. Ramshaw and M. P. Marcus.
1995.
Textchunking using transformation-based l arning.
InProceedings of the Third Workshop on Very LargeCorpora.D.
Ron, Y.
Singer, and N. Tishby.
1995.
On thelearnability and usage of acyclic probabilistic fi-nite automata.
In Proceedings of the 8th AnnualConference on Computational Learning Theory(COLT'95), pages 31-40, New York, NY, USA,July.
ACM Press.G.
Satta.
1997.
String transformation learning.
InProc.
of the ACL/EACL Annual Meeting, pages444-451, Madrid, Spain, July.C.
Stanfill and D. Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM,29(12):1213-1228, December.A.
Stolcke and S. Omohundro.
1992.
Hiddenmarkov model induction by bayesian model merg-ing.
In Proceedings of Neural Information Pro-cessing Systems 5 (NIPS-5).C.
J. van Rijsbergen.
1979.
Information Retrieval.Buttersworth.M.
B. Vilain and D. S. Day.
1996.
Finite-statephrase parsing by rule sequences.
In Proc.
ofCOLING, Copenhagen, Denmark.73
