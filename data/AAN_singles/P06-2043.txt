Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 331?336,Sydney, July 2006. c?2006 Association for Computational LinguisticsImproving English Subcategorization Acquisition with Diathesis Al-ternations as Heuristic InformationXiwu HanInstitute of ComputationalLinguisticsHeilongjiang UniversityHarbin City 150080 Chinahxw@hlju.edu.cnTiejun ZhaoSchool of Computer Science andTechnologyHarbin Institute of TechnologyHarbin City 150001 Chinatjzhao@mtlab.hit.edu.cnXingshang FuInstitute of ComputationalLinguisticsHeilongjiang UniversityHarbin City 150080 Chinafxs@hlju.edu.cnAbstractAutomatically acquired lexicons withsubcategorization information have al-ready proved accurate and useful enoughfor some purposes but their accuracy stillshows room for improvement.
By meansof diathesis alternation, this paper pro-poses a new filtering method, which im-proved the performance of Korhonen?sacquisition system remarkably, with theprecision increased to 91.18% and recallunchanged, making the acquired lexiconmuch more practical for further manualproofreading and other NLP uses.1 IntroductionSubcategorization is the process that further clas-sifies a syntactic category into its subsets.
Chom-sky (1965) defines the function of strict subcate-gorization features as appointing a set of con-straints that dominate the selection of verbs andother arguments in deep structure.
Large sub-categorized verbal lexicons have proved to becrucially important for many tasks of naturallanguage processing, such as probabilistic pars-ers (Korhonen, 2001, 2002) and verb classifica-tions (Schulte im Walde, 2002; Korhonen, 2003).Since Brent (1993) a considerable amount of re-search focusing on large-scaled automatic acqui-sition of subcategorization frames (SCF) has metwith some success not only in English but also inmany other languages, including German(Schulte im Walde, 2002), Spanish (Chrupala,2003), Czech (Sarkar and Zeman, 2000), Portu-guese (Gamallo et.
al, 2002), and Chinese (Hanet al 2004).
The general objective of this re-search is to acquire from a given corpus the SCFtypes and numbers for predicate verbs.
Two typi-cal steps during the process of automatic acquisi-tion are hypothesis generation and selection.Usually based on heuristic rules, the first stepgenerates SCF hypotheses for involved verbs;and the second selects reliable ones via statisticalmethods, such as BHT (binomial hypothesis test-ing), LLR (log likelihood ratio) and MLE(maximum likelihood estimation).
This secondstep is also called statistical filtering and hasbeen widely regarded as problematic.
Englishresearchers have proposed some methods adjust-ing the corpus hypothesis frequencies before orwhile filtering.
These methods are often calledbackoff techniques for SCF acquisition.
Some ofthem represent a remarkable improvement in theacquisition performance, for example diathesisalternation and semantic motivation (Korhonen,1998, 2001, 2002).For the convenience of comparison betweenperformances of different SCF acquisition meth-ods, we define absolute and relative recall in thispaper.
By absolute recall, we mean the figurecomputed against the background of input corpus,while relative recall is against the set of gener-ated hypotheses.At present, automatically acquired verb lexi-cons with SCF information have already provedaccurate and useful enough for some NLP pur-poses (Korhonen, 2001; Han et al 2004).
As forEnglish, Korhonen (2002) reported that semanti-cally motivated SCF acquisition achieved a pre-cision of 87.1%, an absolute recall of 71.2% anda relative recall of 85.27%, thus making the ac-quired lexicon much more accurate and useful.However, the accuracy still shows room for im-provement, especially for those SCF hypotheseswith low frequencies.
Detailed analysis on theacquisition system and some resulting datashows that three main causes should account forthe comparatively unsatisfactory performance: a.the imperfect hypothesis generator, b. the Zipfian331distribution of syntactic patterns, c. the incom-plete partition over SCF types of a given verb.The first problem mainly comes from the inade-quate parsing performance and noises existing inthe corpus, while the other two problems are in-herent to natural languages and should be solvedin terms of acquisition techniques particularlyduring the process of hypothesis selection.2 Related WorkThe empirical background of this paper is thepublic resource for subcategorization acquisitionof English verbs, provided by Anna Korhonen(2005) in her personal home page.
The data in-clude 30 verbs, as shown in Table 1, and theirunfiltered SCF hypotheses, which were auto-matically generated via Briscoe and Carroll?s(1997) SCF acquisition system, and the manuallyestablished standard.Precision  + Recall2 * Precision * Recall|True positives|+|False positives||True positives||True positives|+|False negatives||True positives|Table 1.
English Verbs in Use.add agree attachbring carry carvechop cling clipfly  cut traveldrag communicate givelend lock marrymeet mix moveoffer provide visitpush sail sendslice supply swingFor each verb, there is a corpus of 1000 sen-tences extracted from the BNC, and all together42 SCF types are involved in the corpus.
Theframework of Briscoe and Carroll?s system con-sists of six overall components, which are ap-plied in sequence to sentences containing a spe-cific predicate in order to retrieve a set of SCFsfor that verb:z A tagger, a first-order Hidden MarkovModel POS and punctuation tag disam-biguator.z A lemmatizer, an enhanced version of theGeneral Architecture for Text Engineeringproject stemmer.z A probabilistic LR parser, trained on atree-bank derived semi-automatically fromthe SUSANNE corpus, returns rankedanalyses using a feature-based unificationgrammar.z A pattern extractor, which extractssubcategorization patterns, i.e.
localsyntactic frames, including the syntacticframes, including the syntactic categoriesand head lemmas.z A pattern classifier, which assigns patternsto SCFs or rejects them as unclassifiable.z A SCF filter, which evaluates sets of SCFsgathered for a predicate verb.Nowadays, in most related researches, the per-formances of subcategorization acquisition sys-tems are often evaluated in terms of precision,recall and F measure of SCF types (Korhonen,2001, 2002).
Generally, precision is the percent-age of SCFs that the system proposes correctly,while recall is the percentage of SCFs in the goldstandard that the system proposes:Precision =Recall =F-measure =Here, true positives are correct SCF types pro-posed by the system, false positives are incorrectSCF types proposed by system, and false nega-tives are correct SCF types not proposed by thesystem.3 The MLE Filtering MethodThe present SCF acquisition system for Englishverbs employs a MLE filter to test the automati-cally generated SCF hypotheses.
Due to noisesaccumulated while tagging, lemmatizing andparsing the corpus, even though correction is im-plemented for some typical errors when classify-ing the extracted patterns, the hypothesis genera-tor does not perform as efficiently as hoped.Sampling analysis on the unfiltered hypothesesin Korhonen?s evaluation corpus indicates thatabout 74% incorrectly proposed and rejectedSCF types come from the defects of the MLEfiltering method.Performance of the MLE filter is closely re-lated to the actual distributions p(scfi|v) overpredicates and SCF types in the input corpus.First, from the overall corpus a training set isdrawn randomly; it must be large enough to en-sure a similar distribution.
Then, the frequencyof a subcategorization frame scfi occurring with averb v is recorded and used to estimate the prob-ability p(scfi|v).
Thirdly, an empirical threshold ?is determined, which ensures that a maximum332value of the F-measure will result for the trainingset.
Finally, the threshold is used to filter outfrom the total set those SCF hypotheses with fre-quencies lower than ?.Therefore, the statistical foundation of this fil-tering method is the assumption of independenceamong the SCFs that a verb enters, which can beprobabilistically expressed in two formulas asfollows:0),|(,,, =???
vscfscfpjiji ji ?
(1)?==nii vscfp11)|(                          ?
(2)Here, i and j are natural numbers, scfi and scfj aretwo SCF types that verb v enters, and variables informulas henceforth will hold the same meanings.In actual application, the probability p(scfi|v) isestimated from the observed frequency f(scfi, v),and the conditional probability p(scfi|scfj, v) isassumed to be zero.
This means any two SCFtypes entered by a given verb are taken forgranted to be probabilistically independent fromeach other.
However, this assumption can some-times be far from appropriate.4 Diathesis Alternations and FilteringMuch linguistic research focusing on child lan-guage acquisition has revealed that many chil-dren are able to produce new grammatical sen-tences from what they have learned (Peters, 1983;Ellis, 1985).
This implies that the widely-usedindependence assumption in the field of NLPmay not be very appropriate, at least for syntacticpatterns.
If this assumption should be removed, apossible heuristic could be the information ofdiathesis alternations, which is also another con-vincing counterargument.
Diathesis alternationsare generally regarded as alternative ways inwhich verbs express their arguments.
Examplesare as follows:a.
He broke the glass.b.
The glass broke.c.
Ta1 chi1 le0 pin2guo3.?
?
?
???
(         )d. Ta1 ba3 pin2guo3 chi1 le01.??
??
???
(         )In the above examples, the English verb breaktakes the causative-inchoative alternation asshown in sentences a and b, while sentences cand d indicate that the Chinese verb chi1 ?
( , eat)may enter the ba-object-raising alternation wherethe object is shifted forward by the prepositionba3 ?
( ) to the location between the subject andthe predicate, as illustrated in Figure 1.1 The numbers in sentences c and d, which are pinyin nota-tions, show tones of the Chinese syllables, and the two sen-tences, in English, generally mean He ate an apple.Figure 1.
Ba-object-raising Alternation.ba3Ta1 chi1 le0 pin2guo3.ba-object-raisingSubcategorization of verbs has much to dowith diathesis alternations, and most SCF re-searchers regard information of diathesis alterna-tion as an indispensable part of subcategorization(Korhonen, 2001; McCarthy, 2001).
Therefore,one may conclude that, for subcategorizationacquisition, the independence assumption sup-porting the MLE filter is not as appropriate aspreviously thought.For a given verb, the assumption will be ap-propriate and sufficient if and only if there is nodiathesis alternation between all the SCFs it en-ters, and formula (1) and (2) in Section 3 are ef-ficient enough to serve as a foundation for theMLE filtering method.
Otherwise, if there arediathesis alternations between some of the SCFsthat a verb enters, then formula (1) and (2) mustbe modified as illustrated in formula (3) and (4).In either case, for the sake of convenience, itwould be better to combine the formulas asshown in (5) and (6).0),|(,,, >???
vscfscfpjiji ji  ?
(3)?=>nii vscfp11)|(                           ?
(4)0),|(,,, ????
vscfscfpjiji ji  ?
(5)?=?nii vscfp11)|(                            ?
(6)For English verbs, previous research hasachieved great progress in diathesis alternationand relative applications, such as the work ofLevin (1993) and McCarthy (2001).
Besides,Korhonen (1998) has proved that diathesis alter-nation could be used as heuristic information forbackoff estimates to improve the general per-formance of subcategorization acquisition.
How-ever, determining where and how to seed theheuristic remains difficult.Korhonen (1998) employed diathesis alterna-tions in Briscoe and Carroll?s system to improvethe performance of their BHT filter.
Althoughthe precision rate increased from 61.22% to33369.42% and the recall rate from 44.70% to50.81%, the results were still not accurateenough for possible practical NLP uses.Korhonen obtained her one-way diathesis al-ternations from the ANLT dictionary (Boguraevand Briscoe, 1987), calculated the alternatingprobability p(scfj|scfi) according to the number ofcommon verbs that took the alternation(scfi?scfj), and used formula (7) and (8), wherew is an empirical weight, to adjust the previouslyestimated p(scfi|v):If p(scfi|scfj, v)>0,p(scfi|v) = p(scfi|v)?w(p(scfi|v)?p(scfj| scfi))                  ?
(7)If p(scfi|v)>0 & p(scfj|v)=0,p(scfi|v) = p(scfi|v)+w(p(scfi|v)?p(scfj| scfi))                   ?
(8)2Following the adjustment, a BHT filter with aconfidence rate of 95% was used to check theSCF hypotheses.This method removes the assumption of inde-pendence among SCF types but establishes an-other assumption of independence betweenp(scfj|scfi) and certain verbs, which assumes thatall verbs take each diathesis alternation with thesame probability.
Nevertheless, linguistic knowl-edge tells us that verbs often enter different dia-thesis alternations and can be classified accord-ingly.
Consider the following examples:e. He broke the glass.
/ The glass broke.f.
The police dispersed the crowd./ The crowd dispersed.g.
Mum cut the bread.
/ *The bread cut.Both of the English verbs ?break?
and ?disperse?can take the causative-inchoative alternation and,hence, may be classified together, while the verb?cut?
does not take this alternation.
Therefore,the newly established assumption doesn?t fit theactual situation either, and the probability sums?ip(scfi|v) and ?i,jp(scfi|scfj, v) neither need orcan be normalized.Based on the above methodology, we formed anew filtering method with diathesis alternationsas heuristic information, which is, in fact, de-rived from the simple MLE filter and based onformula (5) and (6).
The algorithm can be brieflyexpressed as shown in Table 2.2 For the sake of consistency in this paper and for the con-venience of understanding, formulae formats here are modi-fied.
They may look different from those of Korhonen(1998), but they are actually the same.Table 2.
The New Filtering Method.For hypotheses of a given verb v,1.
if p(scfi|v) > ?1,accept scfi into the output set S;2. elseif p(scfi|v) > ?2,& p(scfi|scfj, v) > 0,& scfj?S,accept scfi into set S;3.
Go to step 1 until S doesn?t increase.In our method, two filters are employed.
Foreach verb involved, first a common MLE filter isused, but it employs a threshold ?1 that is muchhigher than usual, and those SCF hypotheses thatsatisfy the requirement are accepted.
Then, all ofthe remainder of the hypotheses are checked byanother MLE filter seeded with diathesis alterna-tions as heuristic information and equipped witha much lower threshold ?2.
Any hypothesis scfileft out by the first filter will be accepted if itsprobability exceeds ?2 and it is an alternative ofan SCF type scfj that has been accepted by thefirst filter, which means that p(scfi|scfj, v)>0 andscfj?S.
The filtering process will be performedrepeatedly for those unaccepted hypotheses untilno more hypotheses can be accepted for the verb.5 Experimental EvaluationWe implemented an acquisition experiment onKorhonen?s evaluation resources with the above-mentioned filtering method.The diathesis alternations in use are also thoseprovided by Korhonen, except that we used themin a two-way manner (scfi?
?scfj) instead ofone-way (scfi?scfj), because the two involvedSCF types are usually alternative pragmatic for-mats of the concerned verb, as shown in exam-ples in Section 3 and 4.In the experiment we empirically set ?1= 0.2,which is ten times of Korhonen?s threshold forher MLE filter; ?2= 0.002, which is one tenth ofKorhonen?s.
Thus, in a token set of hypothesesno more than 1000, an SCF type scfi will be ac-cepted if it occurs two times or more and has adiathesis alternative type scfj already accepted forthe verb.The gold standard was the manually analysedresults by Korhonen.
Precision, recall and F-measure were calculated via expressions given inSection 2.Table 3 lists the performances of the baselinemethod of non-filtering (No_f), MLE filteringwith ?
= 0.02, and our filtering method on the334evaluation corpus, and also gives the best resultsof Korhonen's method that is using extra seman-tic information (Kor) to make a comparison.Here, Ab_R is the absolute recall ratio, Re_R therelative recall ratio, Ab_F the absolute F-measure that is calculated from Precision andAb_R, and Re_F the relative F-measure that isfrom Precision and Re_R.Table 3.
Performance Comparison.Methods No-f MLE  ours KorP(%) 47.85 67.89 91.18 87.1Ab_R(%) 34.62 32.52 32.52 71.2Re_R(%) 100 93.93 93.93 85.27Ab_F 40.17 43.98 47.94 78.35Re_F 64.73 78.81 92.53 86.18The evaluation shows that our new filteringmethod improved the acquisition performanceremarkably: a.
Compared with MLE, precisionincreased by 23.29%, recall ratio remained un-changed, absolute F-measure increased by 3.96,and relative F-measure increased by 13.72; b.Compared with Korhonen?s best results, preci-sion, Re_R and Re_F also increased respec-tively 3 .
Thus, the general performance of ourfiltering method makes the acquired lexiconmuch more practical for further manual proof-reading and other NLP uses.What?s more, the data shown in Table 3 im-plies that there is little room left for improvementof the statistical filter, since the absolute recallratio is only 2.1% lower than that of the non-filtering method.
Whereas, detailed analysis ofthe evaluation corpus shows that the hypothesisgenerator accounts for about 95% of those unre-called and wrongly recalled SCF types, whichindicates, for the present time, more improve-ment efforts need to be made on the first step ofsubcategorization acquisition, i.e.
hypothesisgeneration.6 ConclusionOur new filtering method removed the inappro-priate assumptions and takes much more advan-3 Korhonen (2002) reported the non-filtering absolute recallratio of her experiment was about 83.5%.
She didn?t giveany explanation with her evaluation resources why herenon-filtering Ab_R was so much lower.
Therefore, theAb_R and Ab_F figures are not comparable here.tage of what can be observed in the corpus bydrawing on the alternative relationship betweenSCF hypotheses with higher and lower frequen-cies.
Unlike the semantically motivated method(Korhonen, 2001, 2002), which is dependent onverb classifications that linguistic resources areable to provide, our filter needs no prior knowl-edge other than reasonable diathesis alternationinformation and may work well for most verbs inother languages with sufficient predicative to-kens.Our experimental results suggest that the pro-posed technique improves the general perform-ance of the English subcategorization acquisitionsystem, and leaves only a little room for furtherimprovement in statistical filtering methods.However, approaches that are more complicatedstill exist theoretically, for instance, some SCFtypes unseen by the hypothesis generator may berecalled by integrating semantic verb-classification information into the system.More essential aspects of our future work,however, will focus on improving the perform-ance of the hypothesis generator, and testing andapplying the acquired subcategorization lexiconsin some concrete NLP tasks.Acknowledgement This research has beenjointly sponsored by the NSFC project No.60373101 and the post-doctor scholarship of for-eign linguistics and literature in HeilongjiangUniversity.
And at the same time, our greatthanks go to Dr. Anna Korhonen for her publicevaluation resources, and Dr. Chrys Chrystellofor his helpful advice on the English writing ofthis paper.ReferencesBoguraev B. K., E. J. Briscoe.
Large lexicons fornatural language processing utilizing the grammarcoding system of the Longman Dictionary of Con-temporary English.
Computational Linguistics,1987: 219-240Brent, M., From Grammar to Lexicon: unsupervisedlearning of lexical syntax, Computational Linguis-tics 19(3) 1993: 243-262.Briscoe, Ted and John Carroll, Automatic extractionof subcategorization from corpora, Proceedings ofthe 5th ACL Conference on Applied Natural Lan-guage Processing, Washington, DC, 1997: 356-363.Chomsky, Noam, Aspects of the Theory of Syntax,MIT Press, Cambridge, 1965.Chrupala, Grzegorz, Acquiring Verb Subcategoriza-tion from Spanish Corpora, PhD program ?Cogni-335tive Science and Language?, Universitat de Barce-lona, 2003: 67-68.Ellis, R?Understanding Second language Acquisi-tion, Oxford University Press.1985Gamallo, P., Agustini, A. and Lopes Gabriel P., UsingCo-Composition for Acquiring Syntactic and Se-mantic Subcategorisation, Proceedings of theWorkshop of the ACL Special Interest Group onthe Lexicon (SIGLEX), Philadelphia, 2002: 34-41.Han, Xiwu, Tiejun Zhao, Haoliang Qi, and Hao Yu,Subcategorization Acquisition and Evaluation forChinese Verbs, Proceedings of the COLING 2004,2004: 723-728.Korhonen, Anna, Automatic Extraction of Subcatego-rization Frames from Corpora ?Improving Filteringwith Diathesis Alternations, 1998.
Please refer tohttp://www.folli.uva.nl/CD/1998/pdf/keller/korhonen.pdfKorhonen, Anna, Subcategorization Acquisition, Dis-sertation for PhD, Trinity Hall University of Cam-bridge, 2001.Korhonen, Anna, Subcategorization Acquisition,Technical Report Number 530, Trinity Hall Uni-versity of Cambridge, 2002.Korhonen, Anna, Yuval Krymolowski, Zvika Marx,Clustering Polysemic Subcategorization FrameDistributions Semantically, Proceedings of the 41stAnnual Meeting of the Association for Computa-tional Linguistics, 2003: 64-71.Korhonen, Anna.
Subcategorization Evaluation Re-sources.
http://www.cl.cam.ac.uk/users/alk23/sub-cat/subcat.html.
2005Levin, B., English Verb Classes and Alternations,Chicago University Press, Chicago, 1993.McCarthy, D., Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Sub-categorization Frames and Selectional Preferences,PhD thesis, University of Sussex, 2001.Peters, A.
The Unit of Language Acquisition, Cam-bridge University Press.
1983.Sarkar, A. and Zeman, D., Automatic Extraction ofSubcategorization Frames for Czech, Proceedingsof the 19th International Conference on Computa-tional Linguistics, Saarbrucken, Germany, 2000.Please refer to http://www.sfu.ca/~anoop/papers/pdf/coling0final.pdfShulte im Walde, Sabine, Inducing German SemanticVerb Classes from Purely Syntactic Subcategoriza-tion Information, Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics, 2002: 223-230.336
