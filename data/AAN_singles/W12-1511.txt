INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 67?75,Utica, May 2012. c?2012 Association for Computational Linguistics?Hidden semantics?
: what can we learn from the names in an ontology?
?Allan ThirdComputing Department, Open University, UKa.third@open.ac.ukAbstractDespite their flat, semantics-free structure, on-tology identifiers are often given names or la-bels corresponding to natural language wordsor phrases which are very dense with informa-tion as to their intended referents.
We arguethat by taking advantage of this informationdensity, NLG systems applied to ontologiescan guide the choice and construction of sen-tences to express useful ontological informa-tion, solely through the verbalisations of iden-tifier names, and that by doing so, they can re-place the extremely fussy and repetitive textsproduced by ontology verbalisers with shorterand simpler texts which are clearer and eas-ier for human readers to understand.
We spec-ify which axioms in an ontology are ?defin-ing axioms?
for linguistically-complex identi-fiers and analyse a large corpus of OWL on-tologies to identify common patterns amongall defining axioms.
By generating texts fromontologies, and selectively including or omit-ting these defining axioms, we show by sur-veys that human readers are typically capableof inferring information implicitly encoded inidentifier phrases, and that texts which do notmake such ?obvious?
information explicit arepreferred by readers and yet communicate thesame information as the longer texts in whichsuch information is spelled out explicitly.1 IntroductionThere has been increasing interest in recent years inthe generation of natural language texts from, or us-?Many thanks to Richard Power and Sandra Williams fortheir help and comments.
This work was supported by En-gineering and Physical Sciences Research Council Grant Ref.G033579/1.ing, ontologies ((Cregan et al, 2007; Kaljurand andFuchs, 2007; Smart, 2008), for example).
Such ?ver-balisations?
?
translations of the logic of, for exam-ple, OWL (W3C Consortium, 2012), into human-readable natural language ?
can be useful for a vari-ety of purposes, such as communicating the resultsof ontology inference, generating custom texts tosuit a particular application domain or assisting non-ontology-experts in authoring, reviewing and vali-dating ontologies.
This paper takes as its startingpoint an observation about ontology structure anduse.
The purpose of an ontology (specifically, theso-called ?T-box?1) is to define the terms of a par-ticular domain in order to allow automated infer-ence of the semantics of that domain.
Given thatmachines are essentially tabulae rasae with regardto nearly any kind of world knowledge, it is there-fore necessary to spell out the meanings of mostterms in what (to a human) would be excruciatingdetail.
In most, if not all, ontology languages, andcertainly in OWL, identifiers ?
the ?names?
for in-dividual entities, classes and relations2 ?
are atomicunits.
That is to say, every identifier is treated bythe machine as simply a flat string, with no internalstructure or semantics.
The corresponding naturallanguage constructions ?
noun and verb phrases ?by contrast have a very rich internal structure whichcan communicate very subtle semantic distinctions.Best practice for human ontology developers recom-mends that for every entity in an ontology, either itsidentifier should be a meaningful simple or complexterm, or it should have a (localised) label which isa meaningful simple or complex natural language1?Terminology box?2?Property?
is the OWL terminology for a relation betweentwo entities67term.
For example, in the domain of education, aclass intended to represent the real-world class ofjunior schools ought to have (in English) an iden-tifier such as junior school or a label such as?junior school?.
Ontology developers who followthis best practice (and, according to (Power, 2010),the vast majority do) produce ontologies in whichthe entities are easily recognisable and understoodby human readers who can parse these identifiers, toinfer, for example, that ?junior school?
is a subclassof the class ?school?.
As it stands, however, a ma-chine will not make this inference.
In order for themachine to comprehend the semantics of this exam-ple, there must additionally be an axiom equivalentto ?a junior school is a school?.The motivation for this work is the desire to iden-tify which kinds of identifier or label are ?obvious?in this way.
That is to say, if we treat an OWL iden-tifier as if it were in fact a multi-word natural lan-guage expression, can we infer at least some of itssemantics from its properties as a noun phrase, forexample?
This has two overall purposes: given anexisting ontology, definitional axioms for ?obvious?identifiers can be omitted when verbalising for a hu-man user, in order to shorten the text and make itmore relevant, and, conversely, during the process ofontology creation, if a human uses an obvious iden-tifier, a reasonable guess can be made as to its defi-nitional axiom(s), and these can be presented to theuser for confirmation, thus saving the user the needto spend time and energy spelling out the obviousfor the machine?s purposes.
This paper addressesthe first of these two purposes.
Note that the aim ofthis work is not particular to consider how best to re-alise entity names in a verbalisation, but rather, howto use the names of entities to guide the choice andconstruction of sentences.This work was undertaken in the context ofthe SWAT (Semantic Web Authoring Tool) project,which is investigating the application of NLG/NLPto ontology authoring and editing (Williams etal., 2011),(Williams and Power, 2010),(Powerand Third, 2010),(Stevens et al, 2011), (Power,2010),(The SWAT Project, 2012).2 Existing workOther researchers have attempted to take advantageof the internal structure of ontology identifiers to in-fer semantics, but these have exclusively been con-cerned with the question of checking or improvingan ontology?s coverage of its domain.
Examplesinclude (Wroe et al, 2003; Fernandez-Breis et al,2010; Egan?a Aranguren et al, 2008).
To the bestof our knowledge, our current research is the first totake advantage of identifier structure to infer seman-tics in order to improve verbalisation and producemore human-focused texts.3 HypothesisInformal feedback from existing work indicates thatmany readers are dissatisfied with the kinds of textproduced by ontology verbalisers, feeling them to besomewhat fussy and unnatural.
Some of this can nodoubt be put down to the verbalisations themselves?
it is very difficult to find a natural way to expressthat one property is the inverse of another withoutresorting to mathematical variables ?
but, as withother generation tasks, the problem is not necessar-ily just how things are said, but also in the selectionof which things to say at all.
Our hypothesis takestwo parts:1. linguistically-complex identifiers/labels are of-ten defined by ?obvious?
axioms in the OWLontologies containing them, and2.
ontology verbalisations which omit such ?obvi-ous?
axioms lead to a better reading experiencefor users without sacrificing content.A prerequisite for these is also the claim thatlinguistically-complex identifiers are reasonablycommon in ontologies.
(Power, 2010) demonstratedvery clearly that recommended best-practice is infact followed very well in much ontology develop-ment, and entities do tend to be given meaningfulnames.One caveat is necessary here.
We are talkingabout what an average human reader might reason-ably expect to follow from a given use of language.However, observing that a black horse is a horse,a grey horse is a horse, a brown horse is a horse,and so on, does not guarantee the truth of any infer-ences we might make on encountering a reference68to a clothes-horse.
There will always be situationsin which ordinary uses of language will need to bemade more precise.
An interesting future directionof this work would be to investigate whether it ispossible to detect exactly when such clarification isnecessary, in the context of ontology verbalisation,at least.4 DefinitionsOf course, ?complex?, ?obvious?, and so on can beloaded terms, and it is necessary to make them pre-cise before continuing.4.1 Simple and complex identifiersIdentifiers3 may consist of a single natural languageword, in which case we call it simple, or multiplewords, in which case it is complex.
The words in acomplex identifier may be separated by spaces (?ju-nior school?
), punctuation (junior school) orcapitalisation (juniorSchool).
In any case, it istrivial to separate these words into a list automati-cally.4.2 Content wordsIn looking for ?defining?
axioms, we often need toignore words occurring in complex identifiers whichhave some grammatical function.
For example, ifcomparing ?has as father?
to other identifiers in thesame ontology, we may ignore ?has?
and ?as?
andconsider only the content word ?father?.
?Has?
oc-curs far too frequently to be of any use in identifyingaxioms relating to the semantics of ?has as father?,although it is of course relevant to what those se-mantics actually are in any one of such axioms.4.3 Constructed identifiersA complex identifier is constructed if its componentwords (or just its content words) are themselves sim-ple identifiers in the containing ontology.
For exam-ple, if an ontology contains identifiers correspond-ing to ?French?, ?woman?
and ?French woman?,then ?French woman?
is a constructed identifier.
Wemay wish to relax this definition slightly to considerconstructed identifiers where most of the component3Henceforth, for brevity, ?identifier?
may mean ?OWL iden-tifier?, if that is human-meaningful, or it may mean ?label?, oth-erwise.
(content) words are also identifiers, or where com-ponent words are morphological variants of otheridentifiers.4.4 Defining axiomsThe meaning of a constructed identifier can be de-fined in an ontology by axioms in which all, or most,of its component or content words occur as, or in,other identifiers.
For example, if there is an iden-tifier van driver, there is likely to be an axiomsimilar toA van driver drives a van.So, for a complex identifier I , we take an axiomA to be a defining axiom if:?
A contains at least two distinct identifiers,?
I occurs in A, and either?
for each identifier J 6= I in A, the contentwords in J are a subset of the content wordsin I , OR?
the content words in I are a subset of the unionof the content words of at least two other iden-tifiers in A.The third condition is relatively straightforward ?a phrase such as ?white van man?
can be defined inOWL using at most terms corresponding to ?white?,?van?
and ?man?, but not every word in a complexphrase must appear in its defining axiom.
Adjec-tives often work like this: we accept ?a junior schoolis a school?
as being a defining axiom of ?juniorschool?, but ?junior?
only appears in the definien-dum.
It is worth noting here that a defining ax-iom need not be the whole of the definition of itsdefiniendum; a complex identifier may have morethan one defining axiom associate with it, in whichcase its definition would be the set of all of its defin-ing axioms.The fourth condition perhaps seems stranger.
Theintention here is to capture defining axioms such asA French woman is a woman whose nationality isFrenchwhere ?nationality?
is not a content word of ?Frenchwoman?, and yet there is an underlying relation-ship between this ?extra?
word/phrase and one of69the content words of ?French woman?, namely inthis case that ?French?
is a nationality.
One goal offuture work might be to look into ways to identifysuch underlying relationships from OWL semanticsin order to use them in new contexts.5 Corpus studySo far, we have given criteria for which identifierswe consider to be linguistically-complex and forwhich axioms we believe serve as definitions forsuch identifiers.
The immediately obvious questionis whether these criteria are useful.
To test this, weevaluate them against a corpus of 548 OWL ontolo-gies collected from a variety of sources, include theTones repository (TONES, 2012), the Swoogle se-mantic web search engine (Finin et al, 2004) andthe Ontology Design Patterns corpus (ODP, 2012).The corpus includes ontologies on a wide range oftopics and featuring a variety of authoring styles.By using the OWL API (OWL API, 2012), aJava program was written to scan through the corpusfor identifiers matching the definition of ?complex?above, and for each such identifier found, look fordefining axioms for it.
Of the logical entity typespossible in OWL ?
Named Individuals, Classes,Data- and Object-Properties ?
it was decided to omitNamed Individuals from the current study.
Much aswith proper nouns in natural language, the names ofindividuals typically have less internal structure thanother kinds of entity or relation names, and thosewhich do have such structure (such as, e.g., ?LakeWindermere?)
are usually very simple.
Individualsare also not really ?defined?
as such.
One may statewhat are deemed to be highly-salient features aboutthem, such as that Lake Windermere is a lake, butthis is not a definition.
Had we included individualsin this study, it was thought that the large numberof non-defined names would artificially depress thestatistics and give an inaccurate view of the phenom-ena being studied.
Re-running the analysis includingNamed Individuals confirmed this hypothesis: lessthan 10 ontologies in the whole corpus containedany defining axioms for named individuals, with themost common pattern having only 77 occurrences inthe whole corpus ?
a negligible frequency.
It wouldbe interesting to look at these cases in more detail,however, to examine what kinds of individual are de-fined in this way.Having identified defining axioms across the cor-pus, the results were then abstracted, by replacingthe occurrences of content words of each identifierin an axiom with alphabetic variables, so thatSubClassOf(junior school school)andSubClassOf(domestic mammal mammal)both becomeSubClassOf(AB B).The occurrences of each abstracted axiom patterncould then be counted and tabulated.
Table 1 showsthe most frequent 10 patterns, comprising 43% of allresults.
Across the whole corpus, 69% of all identi-fiers were complex, according to our definition, andof those, 45% had at least one defining axiom.
Thesefigures indicate that the phenomenon we are investi-gating is in fact a very common one, and hence thatany improvements to ontology verbalisation basedon taking advantage of identifier semantics are likelyto be significantly useful.Of all the patterns identified, 64% involve theSubClassOf axiom type (?A junior school is aschool?).
A further 14% involve InverseObjectProp-erties (?Bill is father of John if and only if John hasfather Bill?
), and another 14% involve ObjectProp-ertyDomain or ObjectPropertyRange (?If somethinghas as segment X, then X is a segment?).
Col-lectively, then, these axiom types cover 92% of alldefining axioms.
An informal glance at the resultsinvolving SubClassOf axioms shows that what ap-pears to be the case in Table 1 is generally true ?
thebulk of the SubClassOf axioms involve some formof adjective construction.It should be noted here that the intention was touse the absolute bare minimum of linguistic knowl-edge in identifying these axioms ?
almost everythingis done by matching substrings ?
in order to avoidinfluencing the results with our own intuitions abouthow we think it ought to look.
It is reassuring to seenonetheless how far it is possible to get without in-volving linguistic knowledge.
Indeed, one of the on-tologies in the test corpus has identifiers in Italian,and it was confirmed by a bilingual English/Italianspeaker that the axiom patterns our software identi-fied for that ontology were just as ?obvious?
in Ital-70Table 1: 10 most frequent patterns of defining axiomNo.
of Pattern Exampleoccurrences1430 SubClassOf(AB B ) SubClassOf(representation-activity activity)1179 SubClassOf(ABC BC ) SubClassOf(Quantified set builder Set builder)455 InverseObjectProperties(hasA isAof ) InverseObjectProperties(HasInput IsInputOf)387 SubClassOf(ABCD BCD ) SubClassOf(Continental-Statistical-Water-Area Statistical-Water-Area)348 SubClassOf(ABCD CD ) SubClassOf(NonWikipediaWebPage WebPage)240 SubClassOf(ABC AC ) SubClassOf(Process-Resource-Relation Process-Relation)229 ObjectPropertyRange(hasA A ) ObjectPropertyRange(hasAnnotation Annotation)192 ObjectPropertyRange(hasAB AB ) ObjectPropertyRange(hasTrustValue TrustValue)188 InverseObjectProperties(AB ABof ) InverseObjectProperties(situation-place situation-place-of)179 InverseObjectProperties(Aof hasA ) InverseObjectProperties(contentOf hasContent)ian as they are in English.
There are limitations,of course.
A defining axiom such as ?a pet owneris a person who owns a pet?
would not be pickedup by this software, as ?owner?
and ?owns?
do notmatch each other as strings.
To bypass this particularlimitation, the software has been modified to allowthe optional use of a (language-specific) stemmingalgorithm before substring matching, so that both?owner?
and ?owns?
would be matched as ?own?,for example.
The current work, however, focuseson the non-stemmed results for reasons of simplic-ity and time; we intend to carry out further researchusing the stemmed results in future.6 Generation study6.1 DesignA core part of our claim for these defining axiomsis that their semantics are in some sense ?obvious?.A human reading a phrase such as ?junior school?is unlikely to need to be told explicitly that a juniorschool is a school.
This claim needs to be tested.Furthermore, it was suggested above that ontologyverbalisations would be improved in quality for hu-man readers if such ?obvious?
sentences were omit-ted and the semantics implied by the internal struc-ture of noun and verb phrases were used to improveverbalisation.
Again, it is necessary to test whetherany improvement does occur.In order to test the first of these claims, a sur-vey was designed, in which each question wouldconsist of a (verbalised) identifier phrase, followedby three sentences containing that identifier phrase.Respondents were asked to indicate which of thosesentences, if any, they were able to deduce fromthe phrase itself, without relying on any domainknowledge.
The questions were based on the top8 patterns of defining axiom from Table 1, and thecontaining ontology of each was verbalised usingthe SWAT ontology verbalisation tool ((The SWATProject, 2012)).
The choice of 8 was motivated byan intended survey size of 10 to 14 questions allow-ing for some duplication of patterns in order to vary,e.g., the order of elements in sentences, and to min-imise the effects of possible domain knowledge onbehalf of respondents.Figure 1 shows an example of a question fromthe first survey.
The prediction was that respondentswould be more likely to select sentences based on adefining axiom pattern than sentences which are notbased on any such pattern.The second claim required a more involved test.It was decided to present respondents with two para-graphs of text, both verbalised from the same setof axioms ?about?
the same class or property.
Oneparagraph of each pair contains verbalisations of ev-ery initial axiom, possibly with common subjectsor objects aggregated between sentences (the ?full?paragraph).
The other omitted the verbalisationsof any defining axioms, and allowed aggregationof common elements from within identifiers wherethat was justified by one of the omitted defining ax-iom.
For example, the already-aggregated (in thefull paragraph) sentenceThe following are kinds of atrium cavity: leftatrium cavity, right atrium cavitywas further aggregated toThe following are kinds of atrium cavity: left, right.because of the defining axiomsA left (right) atrium cavity is an atrium cavity.This second paragraph is the ?simplified?
para-graph.
Both paragraphs were checked in each caseto ensure that the original set of axioms could be71Figure 1: Sample question from survey 1inferred without any external information, provid-ing an objective guarantee that both carried the samesemantic content even if one only did so implicitly.Respondents were asked to judge whether each pairof paragraphs expressed the same information, to ex-press a preference (or lack of preference) for oneparagraph over the other, and to select those sen-tences from each paragraph which conveyed infor-mation which was not conveyed by the other para-graph.
Figure 2 shows an example survey question.Three hypotheses were tested simultaneously bythis survey.
The first was that respondents wouldbe able to detect when two paragraphs containedthe same information at a probability significantlygreater than chance and the second that respondentswould tend to prefer the simplified paragraphs.
Thethird hypothesis was that respondents would be un-likely to label information as being ?missing?
from aparagraph when that information was implicitly ex-pressed.Our initial survey design also included pairs ofparagraphs which genuinely did contain different in-formation, to serve as a control, and so respondents?ability to judge pairs of paragraphs as carrying thesame information would be compared to their abil-ity to judge when the presence of different informa-tion.
However, in piloting that design, nearly everyrespondent reported such examples as being highlyconfusing and distracting.
This is perhaps not sur-prising; the task of telling when two texts expressthe same content is not symmetrical with the taskof telling when they express different content.
Thelatter is considerably easier, by virtue of the factthat different content will involve different words orphrases, or noticably different sentence structures.Because of this, the decision was taken only to testtexts which objectively did contain the same logi-cal content, and to compare the results to chance.Each paragraph pair was controlled to minimise theeffects of ordering of information and, where possi-ble, of length.To maximise take-up and completion, it was de-cided to try to keep the length of time taken to com-plete each survey down to around five minutes.
Con-sequently, survey 1 had 14 of the relatively sim-ple identifier inference questions and survey 2 had4 of the more complex paragraph-comparison ques-tions.
Both surveys were published via Survey-Monkey (Monkey, 2012) and were publicised viathe social networking sites Twitter (Twitter, 2012),Facebook (Facebook, 2012) and Google+ (Google+,2012).6.2 Results and evaluationThe first survey attracted 30 respondents, the second29.
The data collected from the first survey are sum-marised in Table 2, where S is ?sentence predictedto be obvious by a defining axiom pattern?
and J is?sentence judged inferrable from the given identi-fier?.
Applying a 2 ?
2 ?2 contingency test resultsin ?2 = 342.917, df = 1 and P < 0.0001, indi-cating an extremely significant association betweenthe predicted obviousness of a sentence and respon-dent judgement of that sentence as following fromthe given identifier.It is interesting, however, to note the top row ofTable 2: for sentences which are predicted to hold,human judges are ambivalent as to whether to judgeit as definitely true or not.
One interpretation of thisresult is that, while it is very clear that non-definingaxioms can not be inferred from identifier phrases,people are hesitant to commit to asserting these ax-ioms in an unfamiliar domain, perhaps for fear of anunknown exception to the general rule.
For example,72Figure 2: Sample question from survey 2while ?a Qualifier Noun is a Noun?
is usually a goodrule of thumb, ?a clothes-horse is a horse?
is a clearcounterexample.
So perhaps the better interpreta-tion of these results would be to say that, presentedfor example with a phrase of the form ?QualifierNoun?, a reader would not be surprised if it turnedout that the entity referred to is also a ?Noun?.
Ei-ther way, these statistics suggest that it could well besafe, when generating texts, to omit defining axiomsand allow readers?
default assumptions to apply.
Asimple improvement suggests itself.
In the situationwhere a particular defining axiom pattern would bepredicted, but its negation is in fact present, the saidnegation is automatically highly-salient.
It is alwayslikely to be worthwhile verbalising ?a clothes-horseis not a horse.
?Table 2: Results of the survey on identifier inference.J not JS 224 211not S 44 739It is also interesting to separate out the results ofthis survey by type of axiom.
There were three gen-eral families of defining axiom type tested ?
Sub-ClassOf (?A junior school is a school?
), InverseOb-jectProperties (?Bill is father of John if and only ifJohn has father Bill?)
and ObjectPropertyRange (?Ifsomething has as segment X, then X is a segment?
).Table 3 shows the results broken down by these cat-egories, where ?SC?
is SubClassOf, ?IOP?
is In-verseObjectProperties?
and ?OPR?
is ObjectProper-tyRange.Table 3: Breakdown of identifier inference results by ax-iom type.J not JSC 152 109IOP 52 64OPR 20 38A 3 ?
2 ?2 test results in ?2 = 13.54, df = 2and P = 0.001148, indicating that the judgementof a sentence as obvious or not varies to a signif-icant degree with the type of sentence it is.
Thisis perhaps to be expected, given that not all axiom-types can be verbalised by sentences of similar lin-guistic complexities.
In particular, it is very difficultto see how to verbalise ObjectPropertyRange sen-tences without appealing to the use of variables suchas X and Y, which tend to lead to rather clunky sen-tences.
Sentences corresponding to SubClassOf ax-ioms are most likely to be judged as obvious.
Fur-ther work is necessary to determine the reasons for73these differences empirically.Table 4: Results of paragraph comparison survey (I)Yes NoSame info 74 22Prefer simplified 61 24paragraphTable 4 summarises the results of the ?sameinformation?
and ?preference?
questions from theparagraph-comparison survey, aggregated acrossquestions.
Comparing each of these to a randomdistribution of Yes/No answers gives, in turn, ?2 =15.198, df = 1 and P < 0.0001 (same information)and ?2 = 8.498, df = 1 and P = 0.0036 (prefer-ence), indicating an extremely significant likelihoodof judging two paragraphs containing the same in-formation as in fact doing so, and a significant like-lihood of preferring the more concise of such para-graphs.More interesting are the results shown in Table5.
Here, taken across all paragraph-pairs, E denotesthat the information expressed by a sentence in oneparagraph is explicitly expressed in the other para-graph, and J denotes the judgement as to whethereach sentence was judged to express information notalso expressed in the other paragraph.
These dis-tributions of observations need to be compared, forexplicit and implicit in turn, to the expected distri-butions of judgements as to whether the informationis missing or not.
For explicit information, the ex-pected distribution is zero judgements of ?missing??
where sentences were explicit in both paragraphs,they were in fact identical in both paragraphs andso should never have been judged missing ?
and 696judgements of ?not missing?.
It scarcely needs a sta-tistical test to show that the actual observations of3, and 693, respectively, do not differ significantlyfrom these expectations.
Nonetheless, Fisher?s exacttest (since one of the expected values is 0, ruling out?2) gives P=0.2495.
For implicit information, thenull hypothesis is that implicit information is indis-tinguishable from absent information, and so the ex-pected distribution is 290 judgements of ?missing?and zero judgements of ?not missing?, compared toobservations of 33, and 257, respectively.
Apply-ing Fisher?s exact test gives P less than 0.0001, indi-cating an extremely significant difference.
In otherwords, implicit information is readily distinguish-able from absent information, as predicted.Table 5: Results of paragraph comparison survey (II)J not JE 3 693not E 33 2577 Conclusion and further workBeginning from some observations about identifieruse and semantics in ontologies, we posed two hy-potheses, that linguistically-complex identifiers areoften defined by ?obvious?
axiom patterns in termsof the content words contained in those identifiers,and that these ?obvious?
axiom patterns could beomitted from ontology verbalisations in order to pro-duce clearer texts without significant informationloss.
By means of an ontology corpus study, and thesurvey evaluation of generated NL texts with humanreaders, we have confirmed these hypotheses.
As aresult, these generation strategies have already beenincorporated into the SWAT ontology verbaliser andontology authoring tool and are already being eval-uated in use by ontology developers as those toolsprogress.Of course, there are many avenues along whichthis work could be taken further.
We have barelyscratched the surface when it comes to using under-lying logical formalisms, and the information ?hid-den?
in identifiers to improve generated text.
Furtherinvestigation of the possibilities of language-specificstemming algorithms in defining-axiom-pattern de-tection, the interactions between multiple definingaxioms for the same entities to form whole defini-tions, and exploitation of the logical contents of anontology to determine the salience of ?usual?
or ?un-usual?
features in order to aid text organisation, alloffer rich opportunities to improve natural-languagegeneration from ontologies.
We look forward to be-ing able to look further into these areas, and to iden-tifying which of these phenomena can perhaps begeneralised to other NLG applications by means ofontologies.74ReferencesAnne Cregan, Rolf Schwitter, and Thomas Meyer.
2007.Sydney owl syntax - towards a controlled natural lan-guage syntax for owl 1.1.
In OWLED.M.
Egan?a Aranguren, C. Wroe, C. Goble, and R. Stevens.2008.
In situ migration of handcrafted ontologies toreason-able forms.
Data & Knowledge Engineering,66(1):147?162.Facebook.
2012.
Facebook.
http://www.facebook.com.Last checked: 10th February 2012.J.
Fernandez-Breis, L. Iannone, I. Palmisano, A. Rec-tor, and R. Stevens.
2010.
Enriching the gene on-tology via the dissection of labels using the ontologypre-processor language.
Knowledge Engineering andManagement by the Masses, pages 59?73.Tim Finin, Yun Peng, R. Scott, Cost Joel, Sachs Anu-pam Joshi, Pavan Reddivari, Rong Pan, Vishal Doshi,and Li Ding.
2004.
Swoogle: A search and meta-data engine for the semantic web.
In In Proceedingsof the Thirteenth ACM Conference on Information andKnowledge Management, pages 652?659.
ACM Press.Google+.
2012.
Google+.
http://plus.google.com,February.
Last checked: 10th February 2012.Kaarel Kaljurand and Norbert E. Fuchs.
2007.
Verbaliz-ing owl in attempto controlled english.
In Proceed-ings of Third International Workshop on OWL: Ex-periences and Directions, Innsbruck, Austria (6th?7thJune 2007), volume 258.Survey Monkey.
2012.
Survey monkey.http://www.surveymonkey.com.
Last checked:10th February 2012.ODP.
2012.
Ontology design patterns.http://ontologydesignpatterns.org.
Last checked:10th February 2012.OWL API.
2012.
The OWL API.http://owlapi.sourceforge.net.
Last checked: 10thFebruary 2012.Richard Power and Allan Third.
2010.
Expressing OWLaxioms by English sentences: dubious in theory, fea-sible in practice.
In 23rd International Conference onComputational Linguistics.Richard Power.
2010.
Complexity assumptions in on-tology verbalisation.
In 48th Annual Meeting of theAssociation for Computational Linguistics.Paul R Smart.
2008.
Controlled natural languages andthe semantic web.
July.R.
Stevens, J. Malone, S. Williams, R. Power, andA.
Third.
2011.
Automating generation of textualclass definitions from owl to english.
Journal ofBiomedical Semantics, 2(Suppl 2):S5.The SWAT Project.
2012.
Last checked: 10th February2012.TONES.
2012.
The TONES ontology repository.http://owl.cs.manchester.ac.uk/repository/browser.Last checked: 10th February 2012.Twitter.
2012.
Twitter.
http://twitter.com.
Last checked:10th February 2012.W3C Consortium.
2012.
Last checked: 10th February2012.Sandra Williams and Richard Power.
2010.
Groupingaxioms for more coherent ontology descriptions.
In6th International Natural Language Generation Con-ference, pages 197?202.Sandra Williams, Allan Third, and Richard Power.
2011.Levels of organisation in ontology verbalisation.
InProceedings of the 13th European Workshop on Natu-ral Language Generation (forthcoming).CJ Wroe, R. Stevens, CA Goble, and M. Ashburner.2003.
A methodology to migrate the gene ontology toa description logic environment using.
In Pacific Sym-posium on Biocomputing, volume 8, pages 624?635.75
