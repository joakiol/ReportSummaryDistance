BUILDING A LARGE THESAURUS FOR INFORMATION RETRIEVALEdward A.
Fox** and J. Terry NutterDepartment of Computer ScienceVirginia Tech, Blacksburg VA 24061Thomas Ahlswede and Martha EvensComputer Science DepartmentIllinois Institute of Technology, Chicago IL 60616Judith MarkowitzNavistar InternationalOakbrook Terrace, IL 60181ABSTRACTInformation retrieval systems that support searchingof large textual databases are typically accessed by trainedsearch intermediaries who provide assistance to end usersin bridging the gap between the languages of authors andinquirers.
We are building a thesaurus in the form of alarge semantic network .to support interactive query expan-sion and search by end users.
Our lexicon is being built byanalyzing and merging data from several large English dic-tionaries; testing of its value for rea'ieval is with theSMART and CODER systems.1.
IntroductionThough computer systems aiding retrieval frombibliographic or full-text databases have been available formore than two decades, it is only in recent years that manypeople are becoming concerned about the serious limi-tations of those systems regarding effectiveness in findingdesired references (Blair and Maron 1985).
Like others,though, we are convinced that easy-to-apply automaticmethods can help solve this problem (see argument inSalton 1986).
Indeed, automatic approaches seem essentialsince many end-users want to search without involvingtrained intermediaries (Ojala 1986).
However, since thefundamental issue is one of mismatch between languageuse of document authors and inquirers, leading touncertainties regarding whether a particular item should beretrieved (Chen and Dhar 1987), we are also convinced thatcomputational linguistics is essential for a completesolution.Since many queries are simply sets of lexemes orphrases, and all queries can be reduced to that form, webelieve that focusing on lexical and phrasal issues may bethe most appropriate strategy for applying computationallinguistics to information retrieval.
While good resultshave been achieved in applying automatic procedures tofind lexically related words based on local context in a* This material is based in part upon work supported bythe National Science Foundation under Grant No's.
IST-8418877, IST-8510069, and IRI-8703580; by the VirginiaCenter for Innovative Technology under Grant No.
INF-85-016; and by AT&T equipment contributions.
**All correspondence r garding this paper should beaddressed to the first author.particular collection (Attar and Fraenkel 1977), no reliabletechniques exist for using collection statistics to build athesaurus automatically, and manual construction ofthesauri is both problematic and expensive (Svenonius1986).
Efforts to represent a significant portion of commonsense knowledge will take years (Lenat et al 1986), butdeveloping knowledge resources to aid text processing isnow feasible (Walker 1985).
Encouraged by activities inidentifying lexical and semantic information in otherlanguages (especially since that reported in Apresyan et al1970), we decided to build a large, comprehensive lexiconfor English that would include lexical and semanticrelations (see comments below and survey in Evens et al1980) and be of use for information retrieval.
Clearly thismight also help with question answering (Evens and Smith1978).Lexical and semantic relations provide a formalmeans for expressing relationships between words andconcepts.
Most commercial dictionaries give explicitmention to the classical relations, synonymy and antonymy,but many other relations are used in the definitions.Perhaps the most common is taxonomy, the relationbetween a word and its genus term, as in l i on  - an ima l .Many noun definitions also contain the part-whole relation.Grading and queuing relations help describe words thatcome in sequences like Monday  - Tuesday  - Wednesdayand  hot  - warm - coo l  - co ld .
Collocation relations areused to express the relationships between words thatcooccur frequently like ho le  and  d ig .
As a basis forautomated thesaurus construction, we are trying to extractfrom machine-readable dictionaries triples, consisting ofwords or phrases linked by a labelled arc representing therelation.
We plan to include phrases as well as words forboth practical and theoretical reasons.
It is well known thatthesauri that include phrases are much more effective thanthose without, and we are believers in the phrasal exicondescribed by Becker (1975).Our approach is first (see section 2) to apply textprocessing methods to machine readable dictionaries(Amsler 1984); next (see section 3) to analyze definitionsfrom those dictionaries; then (see section 4) to merge thatinformation (along with data in a large synonym file madeavailable by Microlytics Inc.) into a large semantic network(see early work in Quillian 1968, and survey in Ritchie andHanna 1984); and finally (see section 5) to test the utilityof the resulting thesaurus using the SMART and CODER101experimental retrieval systems.
We discuss preliminaryresults of all these aspects of our research program.2.
Dictionary text processingSince we wish to build a thesaurus with broadcoverage, and since each dictionary has unique advantages,we plan to use several.
The bulk of our work to date hasbeen with the Collins Dictionary of the English Language(CDEL) and Webster' s Seventh New Collegiate Dictionary(W7).2.1.
Collins Dictionary of the English LanguageIn 1985 we obtained a magnetic tape containing thetypesetter's form of CDEL from the Oxford Text Archiveand embarked upon the task of converting that to a factbase in the form of Prolog relations (Wohiwend 1986).CDEL is a large dictionary with about 82,000 headwords.Thus it is larger than W7 (which has roughly 69,000headwords), and also has separate fields not found in W7,such as sample usages (roughly 17,000), first names,compare-to lists, related adjectives, and abbreviations.Like W7, there are on average two definitions per head-word, though while W7 has at most 26 senses per entrythere are many words in CDEL with more senses, on upthrough one entry with 50 senses.Wohlwend used various UNIX (trademark ofAT&T Bell Laboratories) text processing tools anddeveloped analyzers with lex and yacc.
The mainprocessing involved nine passes through the data by ouranalyzers, with a small amount of manual checking andcorrection between steps.
By the fall of 1986 Wohlwend,France and Chert had extracted all suitable data from theCDEL tape, placed it in the form of facts that could beloaded into a Prolog system, and collected statisticsregarding occurrences (Fox et al 1986).
Valuableinformation was present o aid in our work with theCODER "expert" information retrieval system (France andFox 1986).
Recently J. Weiss has completed manualchecking and editing of the data, and has refined some ofthe automatic analysis (e.g., separating past tense formsfrom other irregulars).
Later we will load the bulk of thatinto a semantic network and carry out further processing onthe definition portions.2.2.
Webster's Seventh Collegiate DictionaryWe received our machine-readable W7 from RaoulSmith on five tapes in Ohaey's original format (Olney1968).
Our first piece of text processing was to compressthis huge mass of data (approximately 120 megabytes) intoa manageable format.
Ahlswede wrote a C programcompress which converted the tape data into a format basedon that of Peterson (1982), with a few differences tosimplify out analysis.
The resulting version occupied15,676,249 bytes.The synonymy relation is particularly easy torecognize, since it is explicitly tagged -- in the printeddictionary synonyms appear in small capitals.
Thus thefirst step in adding relations to out lexical database was toextract the 45,910 synonymy relationships markedexplicitly in this way using the UNIX awk utility and insertthem in a table of word-relation-word triples (Ahlswede1985).Morphology also provides an analytical tool for theextraction of relations.
One fruitful source is the prefix listsof words beginning with non-, re-, and un- that are printedin the dictionary but are never defined.
These were left outof the Olney version of the dictionary, but one of ourcolleagues, Sharon King, typed in the lists and wroteroutines that generate definitions for these words, e.g.,readjust vt to adjust againredefinition n the act or process of definingagainreexporter n that which exports againunflattering aj not flatteringFor many of these words it is possible to derive relationalinformation automatically also:redefinition ACT redefineredefine REPEAT definereexporter AGENT reexportunflattering ANTI flatteringAlmost twenty percent of the words and phrases presentedin W7 do not have main entries of their own but appear as"run-ons" at the foot of other entries.
Most of the run-onsare formed by adding productive suffixes to the main entry,such as mess, -able, and -ly.
Fortunately, the suffixesthemselves are often defined in ena'ies of their own thatmake clear how the root and the derived word are related.Word-relation-word t iples can easily be generated betweenwords derived using these suffixes and their roots.We have long agreed with Becker's (1975)assertion that many phrases are best treated as lexical units.We were delighted to find that the editors of W7 apparentlyagree: out of 68,656 main entries, 8,878 are phrases ofmore than one word (separated by spaces).
Another studentis currently analyzing phrasal entries in W7, and lookingfor systematic relationships.3.
Analysis of Definitions3.1.
Preprocessing for the parserThe definition texts proper are the largest and (tous) the most interesting part of dictionary entries.Considerable preprocessing was necessary in order toprepare the W7 definition texts for parsing with the LSP.The Peterson/Ahlswede format uses semicolons asdelimiters to separate several fields in a definition text.Since the LSP treats the semicolon as a word, we replacedthe semicolon delimiters with spaces.
The entry word,homograph number, sense number, and part of speech werenecessary to identify the sense being defined and must bepart of the text input to the LSP.
Texts longer than 72characters had to be broken up into multiple lines and textshad to be converted toall upper case.
Finally, each text hadto end with a period (separated from the last word by aspace).A shell script, LSPformat, used the UNDC utilitiessed and tr to perform all of these operations except hebreaking of long lines.
Since no UNIX utility performedline breaks in quite the way we needed, we wrote a Cprogram foldx which did.
However, we have not mademuch use of LSPformca because we have found it useful tocombine preprocessing of definition texts with the solutionto another problem.3.2.
The LSP Word Dictiotuu'y102The LSP uses a lexicon of its own, the WordDictionary, and cannot parse any text unless all words(strings separated by blanks) contained in the text aredefined in the Word Dictionary.
The Linguistic StringProject has created a Word Dictionary of over 10,000words (White 1983), but this cannot be used to parse evenone W7 definition text without some additions.In particular, W7's part of speech codes (n, vi, vt,aj, etc.)
are absent from the Word Dictionary.
These wereeasily added.
The homograph and sense numbers were alsoabsent.
We found it convenient to generate a list of all thedifferent homograph and sense numbers that appeared inW7 (there were 410) and enter them as "words" in theWord Dictionary.
With these additions it became possibleto parse a total of 8,832 of the 126,879 definition textswithout adding any more entries to the Word Dictionary.Identification of this subset required another specialprogram, showsubset, which steps through all of the W7definition texts, comparing each word with a list of entriesin the Word Dictionary.
It prints those texts out again, eachword in upper ease if it appears in the Word Dictionary andin lower case if it does not.
A shell script, showsubset.com,combines the text processing functions of LSPformat with acall to showsubset to generate a complete set of definitiontexts in this mixed upper and lower case format.
Definitiontexts consisting entirely of upper case words are then readyto parse.Further enlarging the Word Dictionary to includethe entire defining vocabulary (as well as all the entrywords) in W7 is a major effort, still in the early stages.There are two kinds of words occurring in W7 for whichwe need only trivial Word Dictionary entries.
These are (1)words defined but not used in definitions, and-(2) propernames, of which the most numerous and easiest o identifyare scientific names.
The LSP definition grammar requiresno information about the word or phrase in the entry-wordposition except its part of speech (and it needs this only toavoid creating numerous "junk" parses).
All proper nameshave a simple, fixed set of Word Dictionary parameters,thus their Word Dictionary entries are all identical exceptfor the names themselves.
Adding these two categories tothe Word Dictionary would allow parsing of 29,692definitions.We have not yet added these words to the WordDictionary because up to now we have concentratedexclusively on development of the definition grammarbased on the original 8,832 word subset.
There arelogistical problems involved in dealing with a much largerWord Dictionary; furthermore there is at least one majorand distinctive type of definition text (those involvingbiological taxonomy) missing from the subset.3.3.
Determining what relations to look for indefinitionsOne relation, taxonomy, is at least formally presentin virtually all defirfitions.
A definition text consists of aphrase (sometimes a very long one) consisting of one ormore head words plus zero or more modifying phrases.The head word is a taxonomic superordinate of the wordbeing defined.
Sometimes the taxonomy relation isintuitively clear and useful:hole 0 1 n an opening into or through a thingcurtain 1 1 n a hanging screen usu.
capable ofbeing drawn.
.
.Sometimes the formal taxonomy is obscure or of littlesemantic value, and the important relations are tomodifying words or phrases:customer 0 la n one that purchases a commodityor serv ice.
.
.
(customer AGENT purchase)aged 0 la aj of an advanced age(aged JSIMPL age)Other times the head word is best understood as a pointer toanother elation:baptistery 0 0 n a part of a church .
.
.
used forbaptism(baptistery PART church)agglutinate 2 1 vt to cause to adhere(agglutinate v-v-CAUSE adhere)Computational study of dictionary definitions hasfocused heavily on taxonomy because it can often beidentified fairly reliably without parsing the definitions(Amsler 1980, Chodorow et al 1985).
Our emphasis inidentifying relations, whether for information retrieval orfor other purposes, has been on relations other thantaxonomy (Evens et al 1987).An important ool for identifying relations is thephrase count.
A frequently occurring phrase, especially atthe beginning of a defirtition, is often a "defining formula"(Smith 1985, Ahlswede and Evens 1987) that marks arelation.Another useful tool was very simplified handparsing of definitions.
This consisted of manuallyidentifying the headword(s) and blocking off the "leftadjuncts" and "right adjuncts" (to use LSP terminology) ofthe headwords.
For noun definitions, this was a veryproductive procedure; the right adjuncts of nounheadwords, in particular, included many stereotyped phrasestructures which could be associated with relations.Adjuncts in verb definitions were more varied and of lesshelp in identifying relations.3.4.
Developing a graramarThe grammar of dictionary definitions is based onthe grammar provided in Sager (1981).
The great majorityof changes to that grammar have been of two types:1.
Adaptations to the top-level syntax of W7.
TheLSP grammar is based on a medical sublanguage consistingof complete declarative sentences, as well as questions,imperatives, and some other sentence types.
The texts ofW7 are never complete sentences; rather they are phrasessyntactically parallel to the words they define: nounphrases for nouns, infinitive verb phrases for verbs, etc.It would be possible in principle to use existing LSPphrase structures to represent most definition texts,although some enhancement of the grammar would still benecessary to include the entry words, homograph and sensenumbers, and part-of-speech symbols, as well as somesyntactic peculiarities like the zeroed direct object which ispart of every transitive verb definition.
However, we havechosen, for greater ease of finding relational arcs as well asto make the grammar more efficient, to define specialphrase structures for the various definition types.
Shownbelow is part of the grammar for noun definitions:103<NDEF> %%= <NDEF-HEAD> <NDEF-RN> .<NDEF-HEAD> %%= <PROHEAD> / <NOUNHEAD> .<PROHEAD> %%= ANY / ONE / SOMETHING .<NOUNHEAD> %%= <LN> <NVAR> .<NDEF-RN> %%= <OFX> / <PVINGO> / <PN> /<ASX> / <ASOFX> / <ASDFROMX> /<ADJ INRN> / <WHETHS> / <PWHS> /<THATS-N> / <TOVO> / <TOBE> /<VENPASS> / -<VENO> / <VINGO> /<WHS-N> / NULL  .<OFX> %%= <VN-P> <NDEF> .
\ [note  recurs ion \ ]<ASX> %%= ' (' AS <NSTG> ')' <NDEF-RN> .<ASOFX> %%= ' (' AS  <PN> ' ) ' <NDEF-RN> .<ASDFROMX> %%= AS D IST INGUISHED FROM <NSTG> .<VN-P> %%= OF / FOR .2.
Routines to identify and print out relational arcs.The following is an example of a routine which identifiesthe formula used tolinlfor and prints out a relational triple(X) imtr (Y):T-W-N-USE = IN VENPASS%IF IMMEDIATE-NODE IS NDEF-RN THEN $i.$ i  = DESCEND TO LVENR;  STORE IN Xl;AT  COELEMENT PASSOBJ  X2 IF SUSE-FORMULATHEN SN-N-USE.SUSE-FORMULA = BOTH X1 SUBSUMES ' USE'AND X2 SUBSUMES 'TO' OR ' IN' OR 'FOR' .SN-N-USE = DESCEND TO VERB OR NSTGO OR V INGOPASS ING THROUGH STRING;  DO GET-DWORD;WRITE  ' ins t r  '; DO WRITECORE;  WRITE  ENDOF L INE .A few miscellaneous changes have also been made.For example, adverbs are used more freely in W7 than inthe LSP grammar's sublanguage.
Nouns appear more oftenwithout an article, and transitive verbs are more often usedintransitively.
The most spectacular difference between thesublanguages at this level is that W7 uses the conjunctionor with a freedom that is barely if at all acceptable instandard English, especially in adjective definitions:abbatial 0 0 aj of or relating to an abbot .
.
.abaxial 0 0 aj situated out of or directed awayfrom the axis3.5.
Results of parsing2949 noun definitions, 1451 adjective defimtions,1272 intransitive verb definitions, and 2549 transitive verbdefinitions were parsed.
The LSP's performance wassignificantly different in these four categories:Part Percent Parses Time (see.)
Arcsof speech success per success per parse generatednouns 77.63 1.70 11.05 26225adjectives 68.15 1.85 10.59 5393int.
verbs 64.62 1.59 11.96 5438tr.
verbs 60.29 1.50 43.33 14059average/total 68.65 1.66 18.89 51115The count of arcs generated includes duplications;the total number of unique ares was 25,178.
Theseincluded 5,086 taxonomies, 7,971 modification relations(e.g., the definition "puppy n a young dog" yielded themodification arc (puppy) rood (young)), and 125 otherrelations, kn three principal categories.The first category was "traditional" relations uch astaxonomy, part-whole, etc., which we felt were amenable toaxiomatic treatmenL Parsing produced relatively few ofthese: 334 causation arcs, 232 part-whole arcs, and a fewhundred others.The second category was a set of recurring syntacticrelations that we speculated would prove to have consistentsemantic significance.
Some of these were familiar caserelations: there were 448 verbal nouns, 124 adjectivesderived in one way or another from verbs, etc.
Thiscategory also included, for example, relations uch as "v-obj", the relation between averb and the direct object of itsdefining headword.The third category consisted of syntactic relationswhich we simply noted with the idea of later doing clusteranalysis to determine selectional categories in thedictionary, much as described by Sager (1981).
Theseincluded 2,694 "permissible modifiers", adjectivesmodifier-noun pairs; 182 "permissible subjects", nounsappearing as subjects of verbs; and so on.Definitions which failed to parse did so for a greatmany reasons; we may be near the point of diminishingreturns in terms of refining the grammar to parse everydefinition.
As the third column indicates, many definitionsyielded multiple parses.
Multiple parses were responsiblefor most of the duplicate relational arcs that weregenerated.The quality of these parses is an important issue.
A"good" parse is one consistent with the way competentEnglish readers would agree to interpret he text; flawsinclude acceptance of ungrammatical forms (which must becorrected by changing the grammar) or, more often,resolution of syntactic or semantic ambiguities in wayswhich the human reader can recognize as not intended.Quality analysis of the 8,910 parse trees is still at an earlystage.It is not clear why transitive verbs took so muchtime to parse; our best guess at this time is that this wascaused by the difficulty of locating the zeroed direct object(see above).3.6.
Extracting relational triples by text processingAs the "time per parse" column suggests, parsingdefinitions is slow.
(Total parsing time for our 8,832definitions was 176 hours.)
It is also intensive with respectto development effort.
Although we have by no meansgiven up on parsing as a powerful tool of analysis fordictionary definitions, it seems unsuited for the task offinding relational triples.
Consequently, we have experi-mented with a text processing approach based on the identi-fication of defining formulas; it yields more triples in farless time, and in many cases their quality is much better.Our initial text processing effort involved isolatingintransitive verb definitions containing three of the mostcommon intransitive verb headwords: become (774occurrences in 8,482 definitions), make (526 occurrences),and move.
Become, in intransitive verb definitions, isalmost invariably followed by an adjective - -  in a handfulof cases it is followed by a noun (marked by the article a)or an adverb which, in turn, precedes an adjective.
Theseexceptions can be easily weeded out of the 774 definitiontexts.
Conjoined adjectives to the right of become are also104easily identified and an awk program expands thedefinitions containing them into pairs (triplets, etc.)
ofdefinitions.
These are then reduced to triple form:ablate 0 0 vi to become ablated(ablate 0 0 vi) ineep (ablated)abort 0 2 vi to become checked in develop-ment so as to remain rudimentary or toshrink away(abort 0 2 vi) incep (checked)abound 0 2 vi to become copiously supplied(abound 0 2 vi) incep (supplied)addle 2 1 vi to become rotten(addle 2 1 vi) incep (rotten)age 2 2b vi to become mellow or mature(age 2 2b vi) incep (mature)(age 2 2b vi) incep (mellow)By this means we extracted 2,076 triplesrepresenting five relations, which we called incep (frombecome), vncause (from make preceding a noun), move(from move), vacause (from make preceding an adjective),and vnbe (from several head verbs followed by as and anoun phrase - -  the verb be, though occurring 389 times asa headword, governed a variety of definition structures andtherefore conveyed no consistent relational meaning).
Thewhole process took about hree hours.We have also tried to extract taxonomies ofintransitive verbs, assuming as a first approximation thatthe head verb constitutes a genus term.
We have extracted9,520 triples; but the quality of these was not as good asthat of the other relations, since our head finding algorithmdoes not yet catch adverbial particles uch as those in giveup or bring about, or idiomatic direct objects as in takeplace.We axe now in the process of extracting relations bythis process for nouns, adjectives, and transitive verbs.
Theimprovement in performance time is less dramatic,particularly in the case of noun definitions, whereposmominal phrases are much more common than inintransitive verb definitions.
These phrases are harder toidentify, so more manual intervention is necessary.4.
Constructing the semantic networkThe SNePS semantic network system (Shapiro1979b) is one of relatively few knowledge representationschemes that permit a unified representation f associativeinformation and predicate-logic-style inference (for detailson the logic, see Shapiro 1979a and Hull 1986) enhancedwith default reasoning capability (Nutter 1983).
In SNePS,every node represents a concept (concepts include lexemes,word senses, individuals, relations, propositions, and anyother potential objects of the system's knowledge).Conversely, every concept is represented by a node, and notwo nodes represent the same concept (although theconcepts they represent may refer to the same object; formore on this and other principles underlying the design ofSNePS, see Shapiro and Rapaport 1986).
Logical andstructuring information are carried on arcs.
All explicitinformation about a concept is directly linked to its node,with structure sharing across propositions.
It follows thatevery detected synonym of a given word sense, forinstance, is connected to that sense by simply definablepaths to the nodes representing the original word sense andthe lexical relation SYNONYM.
This simplifies findingrelated terms, eliminating much of the look-up necessary inschemes that are superficially more logic-like.The lexicon we are forming contains three differentkinds of information: lexical-semantic relations amongspecific word senses, axioms for lexical relations, anddefinitional assertions.
Most of the semantic information iscaptured in the form of lexical relations between the sensebeing defined and some other word or word sense.Instances of lexical relations have a natural implementationin SNePS, with base nodes representing lexemes, wordsenses, and individual lexical-semantic relations andmolecular nodes (non-base nodes labeled Mx for someinteger x) representing propositions and rules about thebase nodes.
To give content o individual exical relations,relation axioms are included for the various relations.
Forinstance, the lexical relation CHILD has a single axiomwhich says that if an x is a child y, then any x is a y and anyx is young.
While most word sense meanings determinedfrom definitions can be represented fully by lexicalrelations, occasionally some aspect of meaning will remainafter full analysis of the lexical relations between this andother word senses.
In that case the entry is completed by adefinitional assertion.The SNePS representations of instances of lexicalrelations, the relation axioms, and assertions completingdefinitions are not only compatible but linked, and hencecan readily be combined.
In addition, because thesevarious kinds of information all have representations aSNePS subnetworks, they are all immediately available tothe inferencing package (SNIP) without conversion orinstantiation.Thus in most cases the semantic representations ofthe different kinds of lexical information is straightforward,and has the advantage that all information is uniformlyavailable to the inference system.
There is a secondinferential advantage which derives from the nature of thesemantic network representation.
There are two differentkinds of inference available in semantic networks: rule-based inference and path-based inference.
Rule-basedinference, the more common form of inference in mostSNePS applications, involves predicate-calculus-likereasoning based on the existence of rule nodes (nodesrepresenting complex predicate calculus formulas).
Insofaras SNIP operates like a (slightly exotic) notational variantof predicate logic, the inference in use is rule-based.
Inrule-based reasoning, the semantic information resides inthe nodes; the arcs are used for accessing and to manageimplementation.
Path-based inference can be viewed as theprecise inverse: inference which relies only on theexistence of paths (that is, concatenations of arcs) from onenode to another.
Conceptually, rule-based inferencerepresents conscious reasoning from principles, whilepath-based inference represents (unconscious) reasoning bytraversing associational links.
The most naturalimplementation for path-based reasoning is hierarchicalinheritance, but it can be applied more generally, forexample to locate synonyms.
In choosing related terms forexpanding retrieval queries, it turns out that path-based105wool n \[U\] 1ramn Iewe nlamb n 1 \[C\]From these definitions,following lexical relations:reasoning is by far the more important.
Path-basedinference is more efficient than rule-based inference,because given a starting point it eliminates the need tosearch the network for unifying matches.
That is, wherepath-based inference is possible the system does not haveto look for rules which might apply; it need only traverse avery limited subgraph from a given starting point along alimited set of predefined paths.As a simple example, consider the followingdefinitions (quoted directly from the Oxford AdvancedLearner's Dictionary of Current English; in each ease, thedefinition given is the first sense for the first homonym;pronunciations, references and examples have beenomitteA).sheep n (pl unchanged) grass-eating animalkept for its flesh as food(mutton) and its wool.soft hair of sheep, goats, andsome other animals...uncastrated male sheep.female sheep.young of the sheep.. .we extract (among others) thesheep TAX animalsheep TFOOD grasswool PART sheepwool PART goatwool TAX hairram MALE sheepewe FEMALE sheeplamb CHILD sheepA simple post-processor transforms each triple of the formx R y into a SNePS User Language command of the form"(build argl x rel R arg2 y)", which results in creating amolecular node representing the proposition that x bears therelation R to y, yielding the network shown in Figure 1(simplified by omitting relation axioms and the networklinking nodes representing lexemes to those representing aparticular word sense).
Since all SNePS arcs have inverses,simple arc traversals from the node for sheep will locatesuch relate, d terms as ewe, ram, lamb, wool, animal, and soon.
L~ewise, starting from a query containing wool, thesystem can rapidly find sheep, etc.
A more complete net-work, including other definitions, would allow going downas well as up the taxonomic tree, finding e.g.
"merino" andother sheep varieties from either sheep or wool.~ FEMALEFigure 1.
ReNesentation f Lexical Relations involvingSheep5.
Testing with SMART and CODERSeveral studies have been undertaken regarding theuse of lexical and semantic relations in informationretrieval.
Though one investigation i volved use of aspecial system constructed atlIT (Wang et al 1985), mostof the other work has involved the SMART system.
Thefirst version of SMART ran on IBM mainframes; a moremodem form was developed to run under the UNIXoperating system (Fox 1983b).
In SMART, queries anddocuments are represented simply as sets of terms, so amulti-dimensional vector space can be constructed whereineach term is associated with a different dimension in thatspace.
Queries and documents can then be associated withpoints in that space, and documents can be retrieved if"near" to the query.
But since queries are typically short, itcan be valuable to expand aquery with terms related to theoriginal set (especially due to variations in namingpractices like those considered inFumas et al 1982).In our first experiment, involving a small collectionof 82 documents, we found a mild improvement i  systemperformance when all types of related terms (exceptantonyms) were involved in query expansion (Fox 1980).Similar benefits resulted when using a different, largercollection (Evens et al 1985).
In two later studies we usedSMART but worked with Boolean queries.
Queryexpansion then involved "ORing" in related terms with theoriginal ones.
Once again, improvements resulted,especially when the p-norm scheme for interpretingBoolean queries was applied (Fox 1983a, 1984).
In all ofthese studies, lexical-semantic relations were identifiedmanually for all query terms that were expanded.In other ecent work with SMART, Fox, Miller andSridaran used the same Boolean queries, but varied the106source of related words.
They compared with the base caseof original queries the results of the following sources forexpansion: all words based on manually derived lexical-semantic relations, all words (except for antonyms) takenfrom the Merriam Webster Thesaurus, and all words(except hose in a "stop" list) from the definition appearingin a dictionary for the correct word sense.
All expansionschemes gave better esults hart the base case.
While thelexical-semantic relation method seemed best overall, thedictionary results were comparable and the thesaurusapproach was only slightly worse.We are convinced that much larger improvementsare possible ff end-users can be more directly involved inthe process, so they can decide which words should beexpanded, and can select which related terms to includefrom the lists produced from our thesaurus.
Testing thishypothesis, however, requires a more flexible processingparadigm than we have employed in the past.
Furthermore,we believe that inferencing using the information in thesemantic network we are building earl allow us to developan effective automatic or semi-automatic s heme for"intelligent" query expansion.
The CODER system shouldsupport these approaches.Building upon early efforts to build intelligentretrieval systems (Guida and Tasso 1983, Pollitt 1984) andlearning from experiences with similar systems (Croft andThompson 1987), we have been developing the CODER(COmposite Document Expert/effective/extended R triev-al) system (Fox and France 1987, Fox 1987) for the lastthree years.
Though part of that effort deals with newapproaches to automatic text analysis (Fox art d Chen 1987),in the current context the most important aspect of CODERis that it is built as a distributed collection of "expert'"modules (according to the models discussed in Belkin et al1987) programmed in Prolog or C, to support flexibletesting of various AI approaches to information retrieval.Weaver and France have developed modules for handlinglexical and semantic relations and a server moduleproviding access to our version of the contents of CDEL.In the future, a module will be added to interface CODERwith the SNePS semantic network so that furtherexperiments can be undertaken.6.
Conclusions and Future WorkBased on preliminary investigations, it appears thatlexical-semantic relations can be used to give smallimprovements in the effectiveness of information retrievalsystems.
Work with Webster's Seventh New CollegiateDictionary and the Collins Dictionary of the EnglishLanguage has demonstrated that text processing and naturallanguage parsing techniques can be used to extract andorganize important data about English that will be of valuefor information retrieval and for a variety of naturallanguage processing applications.
Furthermore, it is clearthat the SNePS semantic network system can be used tostore that type of data in a form that will permit both ruleand path-based inference.Future work will include further processing ofdictionaries (including the Longman Dictionary ofContemporary English and others from the Oxford TextArchive), merging the resulting output into a large semanticnetwork, extending the capabilities of SNePS to handle avery large thesaurus, integrating SNePS with the SMARTand CODER systems, and further testing of the utility ofthat thesaurus to support interactive information retrievalsessions.REFERENCESAhlswede, Thomas, 1985.
"A Tool Kit for Lexicon Build-ing."
In Proc.
23rd Annual Meeting of the Associa.tion for Computational Linguistics, 268-276.Ahlswede, Thomas and Martha Evens, 1987.
"Generatinga Relational Lexicon from a Machine-ReadableDictionary."
In Visible Language, W. Frawley andR.
Smith (eds.
), Oxford University Press, to appear.Amsler, Robert A., 1980.
"The Structure of the Merriam-Webster Pocket Dictionary."
Ph.D. Dissertation.TR-164, Computer Science Dept., Univ.
of Texas,Austin, Dec. 1980.Amsler, Robert A., 1984.
"Machine-ReadableDictionaries."
Annual Review of InformationScience and Technology, 19:161-209.Apresyan, Yu.
D., I.
A. Mel'~uk and A. K. ~olkovsky,1970.
"Semantics and Lexicography: Towards aNew Type of Unilingual Dictionary."
In Studies inSyntax and Semantics, F. Kiefer (ed.
), Reidel,Dordrecht, Holland, 1-33.Attar, R. and Aviezri S. Fraenkel, 1977.
"Local Feedbackin Full-Text Retrieval Systems."
J. ACM, 24(3):397-417.Becker, Joseph, 1975.
"The Phrasal Lexicon."
InTheoretical Issues in Natural Language Processing,R.
Schank and B. Nash-Webber (eds.
), ACL, 38-41.Belkin, N. et al, 1987.
"Distributed Expert-Based Infor-mation Systems: An Interdisciplinary Approach.
"Information Processing and Management, toappear.Blair, D.C. and M.E.
Maron, 1985.
"An Evaluation ofRetrieval Effectiveness for a Full-Text Document-Retrieval System."
Cornmun.
ACM, 28(3):289-299.Chert, Hsinchun and Vasant Dhar, 1987.
"ReducingIndeterminism in Consultation: A Cognitive Modelof User/Librarian I teractions."
In Proc.
AAAI-87,285-289.Chodorow, Martin S., Roy Byrd, and George Heidorn,1985.
"Extracting Semantic Hierarchies from aLarge On-Line Dictionary."
In Proc.
of the 23rdAnnual Meeting of the Association forComputational Linguistics, 299-304.Croft, W. Bruce and Roger Thompson, 1987.
"I3R: A NewApproach to the Design of Document RetrievalSystems."
J.
Am.
Soc.
lnf.
Sci., in press.Evens, Martha W. and Raoul N. Smith, 1978.
"A Lexiconfor a Computer Question-Answering System."
Am.J.
Comp.
Ling., No.
4, Microfiche 83: 1-96.Evens, Martha W., Bonnie C. Litowitz, Judith A.Markowitz, Raoul N. Smith, and Oswald Wemer,1980.
Lexical-Semantic Relations: A ComparativeSurvey.
Linguistic Research, Inc., Edmonton,Alberta.Evens, Martha W., James Vandendorpe, and Yih-ChenWang, 1985.
"Lexical-Semantic Relations inInformation Retrieval."
In Humans and Machines:107The Interface Through Language, S. Williams (ed.
),Ablex, Norwood, NJ, 73-100.Evens, Martha W., Judith Markowitz, Thomas Ahlswede,and Kay Rossi, 1987.
"Digging in the Dictionary:Building a Relational Lexicon to Support NaturalLanguage Processing Applications," IDEAL (Issuesand Developments in English and AppliedLinguistics) 2(33-44).Fox, E.A., 1980.
"Lexical Relations: EnhancingEffectiveness of Information Retrieval Systems.
"ACM SIGIR Forum, 15(3):5-36.Fox, Edward A., 1983a.
"Extending the Boolean andVector Space Models of Information Retrieval withP-Norm Queries and Multiple Concept Types,"Dissertation, Comell University, UniversityMicrofilms Int., Ann Arbor MI.Fox, Edward A., 1983b.
"Some Considerations forImplementing the SMART Information RetrievalSystem under UNIX."
TR 83-560, ComeU Univ.,Dept.
of Comp.
Sci..Fox, Edward A., 1984.
"Improved Retrieval Using aRelational Thesaurus Expansion of Boolean LogicQueries."
In Proc.
Workshop Relational Models ofthe Lexicon, Martha W. Evens (ed.
), Stanford, CA,to appear.Fox" Edward A., 1987.
"Development of the CODERSystem: A Testbed for Artificial IntelligenceMethods in Information Retrieval."
InformationProcessing and Management, 23(4).Fox, Edward A. and Qi-Fang Chert, 1987.
"Text Analysisin the CODER System."
In Proc.
Fourth AnnualUSC Computer Science Symposium: Language andData in Systems, Columbia SC, 7-14.Fox, Edward A. and Robert K. France, 1987.
"Architectureof an Expert System for Composite DocumentAnalysis, Representation a d Retrieval."
Int.
J. ofApproximate Reasoning, 1(2).Fox, Edward A., Robert C. Wohlwend, Phyllis R. Sheldon,Qi Fan Chen, and Robert K. France, 1986.
"Building the CODER Lexicon The Collins EnglishDictionary and Its Adverb Definitions.'"
TR-86-23,VPI&SU Computer Science Dept., Blacksburg, VA.France, Robert K. and Edward A.
Fox, 1986.
"KnowledgeStructures for Information Retrieval: Representationin the CODER Project."
In Proceedings IEEEExpert Systems in Government Conference, October20-24, 1986, McLean VA, 135-141.Fumas, George W. et al, 1982.
"Statistical semantics: howcan a computer use what people name things toguess what things people mean when they namethings."
In Proc.
of the Human Factors in ComputerSystems Conference, Gaithersburg, MD, Assoc.
forComputing Machinery, New York.Guida, G., and C. Tasso, 1983.
"An expert intermediarysystem for interactive document retrieval.
"Automatica 19(6): 759-766.Hull, Richard G,, 1986.
"'A New Design for SNIP theSNePS Inference Package."
SNeRG TechnicalReport 86-10, Deparmaent of Computer Science,SUNY at Buffalo.Lenat, Doug et al, 1986.
"CYC: Using Common SenseKnowledge to Overcome Brittleness andKnowledge Acquisition Bottlenecks."
The AIMagazine, 65-84.Nutter, J. Terry, 1983.
"Default reasoning using monotoniclogic: a modest proposal."
In Proc.
AAA1-83, TheNational Conf.
on AL Washington D.C., 297-300.Ojala, Marydee, 1986.
"Views on End-User Searching."
J.Am.
Soc.
Inf.
Sci., 37(4), 197-203.Olney, John, 1968.
''To All Interested in the Merriam-Webster Transcripts and Data Derived from Them.
"Systems Development Corporation Document L-13579.Peterson, James L., 1982.
"Webster's Seventh NewCollegiate Dictionary: A Computer-Readable FileFormat."
TR-196, Dept.
of Comp.
Sci., Univ.
ofTexas, Austin.PoUitt, S.E., 1984.
"A 'front-end' system: an ExpertSystem as an online search intermediary."
AslibProceedings, 36(5): 229-234.Quillian.
M. Ross, 1968.
"Semantic Memory."
In SemanticInformation Processing, Marvin Minsky ed.Cambridge, MA: MIT Press, 227-270.Ritchie, G.D. and Hanna, F.K., 1984.
"Semantic Networks:a General Definition and a Survey."
Inf.
Tech.
: Res.Dev.
Applications, 3(1):33-42.Sager, Naomi, 1981o Natural Language InformationProcessing."
Addison-Wesley, Reading, MA.Salton, G., 1986.
"Another Look at Automatic Text-Re-trieval Systems."
Commun.
ACM, 29(7): 648-656.Shapiro, Smart C., 1979a.
"Generalized AugmentedTransition Network Grammars for Generation fromSemantic Networks."
In Proc.
of the 17th AnnualMeeting of the Association for ComputationalLinguistics, 25-29.Shapiro, Smart C., 1979b.
"The SNePS Semantic NetworkProcessing System."
In Associative Networks, NickFindler (ed.
), Academic Press, NY, 179-203.Shapiro, Stuart C. and William Rapaport, 1986.
"SNePSConsidered as a Fully Intensional PropositionalSemantic Network."
In Proc.
AAAI-86, FifthNational Conf.
on AI, Phil.
PA, 278-283.Smith, Raoul N., 1985.
"Conceptual primitives in theEnglish lexicon."
Papers in Linguistics 18: 99-137.Svenouius, Elaine, 1986.
"Unanswered Questions in theDesign of Controlled Vocabularies."
J.
Am.
Soc.
Inf.Sci., 37(5):331-340.Walker, Donald E., 1985.
"Knowledge Resource Tools forAccessing Large Text Files."
In Proc.
FirstConference of the UW Centre for the New OxfordEnglish Dictionary: Information in Data.
Nov. 6-7,1985, Waterloo, Canada, 11-24.Wang, Y~-Chen, James Vandendorpe, and Martha Evens,1985.
"Relational Thesauri in InformationRetrieval."
J.
Am.
Soc.
Inf.
Sci., 36(1): 15-27.Whim, Carolyn, 1983.
"The Linguistic String ProjectDictionary for Automatic Text Analysis."
In Proc.Workshop on Machine Readable Dictionaries, SRI,Menlo Park, CA.Wohlwend, Robert C., 1986.
"Creation of a Prolog FactBase from the Collins English Dictionary."
MSReport, VPI&SU Computer Science Dept.,Blacksburg, VA.108
