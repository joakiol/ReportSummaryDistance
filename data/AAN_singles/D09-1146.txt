Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1408?1417,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPCross-Cultural Analysis of Blogs and Forumswith Mixed-Collection Topic ModelsMichael Paul and Roxana GirjuUniversity of Illinois at Urbana ChampaignUrbana, IL 61801{mjpaul2, girju}@illinois.eduAbstractThis paper presents preliminary results onthe detection of cultural differences frompeople?s experiences in various countriesfrom two perspectives: tourists and lo-cals.
Our approach is to develop proba-bilistic models that would provide a goodframework for such studies.
Thus, we pro-pose here a new model, ccLDA, whichextends over the Latent Dirichlet Alloca-tion (LDA) (Blei et al, 2003) and cross-collection mixture (ccMix) (Zhai et al,2004) models on blogs and forums.
Wealso provide a qualitative and quantitativeanalysis of the model on the cross-culturaldata.1 IntroductionIn today?s society, people from different culturalbackgrounds have to understand each other, inter-act on a daily base and travel to or work in morethan one country.
Understanding cultural diver-sity, as well as addressing the need to communi-cate effectively across cultural divides, have be-come imperative in almost every aspect of life.These constitute an important language aspectsince the lack of such cultural awareness can leadto misinterpretations.This paper presents preliminary results on thedetection of cultural differences from people?s ex-periences in various countries from two perspec-tives: tourists and locals.
Since the advent of Web2.0, user-generated data in the form of blogs andnewsgroup messages have reached high propor-tions.
In this paper we take advantage of suchresources of blogs and forums to perform variouscross-cultural analyses.Our approach is to develop probabilistic mod-els that would provide a good framework for suchstudies.
Thus, we propose here a new model,ccLDA, which extends over the Latent DirichletAllocation (LDA) (Blei et al, 2003) and cross-collection mixture (ccMix) (Zhai et al, 2004)models.
Our contribution is as follows:(1) Unsupervised topic models such as LDA areelegant and flexible approaches to clustering largecollections of unannotated data.
These models,however, have conceptually focused on one singlecollection of text which is inadequate for compar-ative analyses of text.We thus develop an LDA-based model that cannot only discover topics but also model their simi-larities and differences across multiple text collec-tions.
(2) We improve on similar previous work by craft-ing a model that can better generalize data and isless reliant on user-defined parameters.
(3) We apply our new model on blogs and forumsto identify cross-cultural differences.Thus, different models can be compared to re-flect different hypotheses about the data.The paper is organized as follows.
In Section2 we summarize relevant previous work and givea detailed description of the model in Section 3.Section 4 details the model?s parameter estima-tion.
Experimental results are presented in Sec-tion 5, followed by discussion, future work, andconclusions.2 Previous WorkA topic model for comparing text collections(ccMix) was previously introduced by Zhai etal.
(2004) for a problem called comparative textmining (CTM).
Given news articles from differ-ent sources (about the same event), ccMix can ex-tract what is common to all the sources and whatis unique to one specific source.Our model improves over ccMix by replacingtheir probabilistic latent semantic indexing (pLSI)(Hofmann, 1999) framework with that of LDA.1408Under the ccMix model, the probability of gen-erating the ith word in a document belonging tocollection c is:P (wi) = (1?
?B)?z?ZP (z)(?CP (wi|z) +(1?
?C)P (wi|z, c)) + ?BP (wi|B),where each topic is denoted z.
?Bis the prob-ability of choosing a word from the backgroundword distribution and is user-defined.
?Cis alsodefined by the user and is the probability of draw-ing a word from the collection-independent worddistribution instead of the collection-specific dis-tribution.
The parameters can be estimated usingthe Expectation-Maximization algorithm (Demp-ster et al, 1977).However, in addition to the advantages of LDAover pLSI such as the incorporation of Dirichletpriors and a natural way to deal with new docu-ments, our model avoids the limitations of using asingle user-defined parameter ?C?
this probabil-ity is learned automatically under our model.
Fur-thermore, we allow this probability to depend onthe collection and topic, which is a less restrictiveassumption.Our model, ccLDA, shares with the LDA-Collocation (Griffiths et al, 2007) and Topical N-Grams (Wang et al, 2007) models the assump-tion that each word can come from two differentword distributions, one of which depends on an-other observable variable.
In these models, a wordcan come from either its topic?s word distribution,or it can come from a word distribution associatedwith the previous word, in the case that the wordis determined to be part of a collocation.
The keydifference here is that in these models, the alter-native word distribution depends on the word pre-ceding a token, while in ccLDA, this depends onthe document?s collection.The model is also related to hierarchicalvariants of LDA, in particular the hierarchicalPachinko allocation (hPAM) (Mimno et al, 2007)model, in which both a topic and hierarchy depthare chosen, and there is a different word dis-tribution at different levels in the hierarchy.
Anatural way to view our model is as a two-level hierarchy where the top level represents thecollection-independent distributions and the bot-tom level represents the collection-specific distri-butions.
One of the main differences here is thatthe discovered hierarchies in hPAM can be arbi-trary, whereas the graphical structure of our modelis pre-determined such that each topic has exactlyone ?sub-topic?
representing each collection.Wang et al recently introduced Markov topicmodels (MTM) (2009), a family of models whichcan simultaneously learn the topic structure of asingle collection while discovering correlated top-ics in other collections.
This is promising in thatthis type of model makes no assentation that eachtopic is in some way shared across all collections.However, it does not explicitly model the similar-ities and differences between collections as we doin this research.In computational linguistics, topic models havebeen used in various applications, such as predict-ing response to political webposts (Yano et al,2009), analyzing Enron and academic emails (Mc-Callum et al, 2007a), analyzing voting recordsand corresponding text of resolutions from theU.S.
Senate and the U.N. (McCallum et al,2007b), as well as studying the history of ideasin various research fields (Hall et al, 2008; Pauland Girju, 2009).
To our knowledge, the applica-tion of topic models to identifying cross-culturaldifferences is novel.3 The ModelIn this section we first review the basic pLSI andLDA models.
We then introduce our extension toLDA: cross-collection LDA (ccLDA).3.1 Basic Topic ModelingThe most basic generative model that assumesdocument topicality is the standard Na?
?ve Bayesmodel, where each document is assumed to be-long to exactly one topic, and each topic is asso-ciated with a probability distribution over words(Mitchell, 1997).While this single-topic approach can be suffi-cient for classification tasks, it is often too limitingfor unsupervised grouping of semantically relatedwords into topics.
A better assumption is that eachdocument is a mixture of topics.
For example, anews article about a natural disaster may includetopics about the causes of such disasters, the dam-age/death toll, and relief aid/efforts.
Probabilisticlatent semantic indexing (pLSI) (Hofmann, 1999)is one such model.
Under this model, the proba-bility of seeing the ith word in a document is:P (wi|d) =?z?ZP (wi|z)P (z|d)1409One of the main criticisms of pLSI is that eachdocument is represented as a variable d and it isnot clear how to label previously unseen docu-ments.
This issue is addressed by Blei et al withlatent Dirichlet alocation (2003).
Furthermore,the probabilities under this model have Dirichletpriors, which results in more reasonable mixturesand less overfitting.
In LDA, a document is gener-ated as follows:1) Draw a multinomial distribution of words ?zfrom Dirichlet(?)
for each topic z2) For each document d1, draw a topic mixturedistribution ?
(d)from Dirichlet(?).
Then for eachword wiin d:a) Sample a topic zifrom ?
(d)b) Sample a word wifrom ?zThe Dirichlet parameters ?
and ?
are vectorswhich represent the average of the respective dis-tributions.
In many applications, it is sufficientto assume that these vectors are uniform and tofix them at a value pre-defined by the user.
Inthis case, the Dirichlet priors simply function assmoothing factors.3.2 Cross-Collection LDAIn this subsection we introduce our extensionof LDA for comparing multiple text collec-tions, which we refer to as cross-collection LDA(ccLDA).
Under this model, each topic is as-sociated with two classes of word distributions:one that is shared among all collections, and onethat is unique to the collection from which thedocument comes.
For example, when modelingreviews of different laptops, the topic describ-ing the preloaded software contains the words?software?, ?application?, ?programs?, etc.
inits shared distribution with high probability, andthe Apple-specific word distribution contains thewords ?itunes?, ?appleworks?, and ?iphoto?.When generating a document under this model,one first samples a collection c (which is ob-servable in the data), then chooses a topic z andflips a coin x to determine whether to draw fromthe shared topic-word distribution or the topic?scollection-specific distribution.
The probability ofx being 1 or 0 comes from a Beta distribution (thebivariate analog of the Dirichlet distribution) and1One should also assume that a document length is sam-pled from an arbitrary distribution, but this does not affect thederivation of the model, so we ignore this here and elsewhere.is dependent on the collection and topic of the cur-rent token.Figure 1: Graphical representation of ccLDA.
C is thenumber of collections, T is the number of topics, D is thenumber of documents, and N is the length of each document.The generative process is thus:1) Draw a collection-independent multinomialword distribution ?zfrom Dirichlet(?)
for eachtopic z2) Draw a collection-specific multinomial worddistribution ?z,cfrom Dirichlet(?)
for each topicz and each collection c3) Draw a Bernoulli distribution ?z,cfromBeta(?0, ?1) for each topic z and each collectionc4) For each document d, choose a collection c anddraw a topic mixture ?(d)fromDirichlet(?c).
Thenfor each word wiin d:a) Sample a topic zifrom ?
(d)b) Sample xifrom ?z,cc) If xi= 0, sample a word wifrom ?z;else if xi= 1, sample wifrom ?z,cAs mentioned in section 2, this model is insome respects an LDA-based analog of the Zhaiet al (2004) model (ccMix), and thus it offers thesame improvements that LDA offers over pLSI(described in the previous subsection), but thereare some other differences.
An obvious structuraldifference between the models is that ccMix hasa special topic for background words, whereas wesimply address this by removing stop words dur-ing preprocessing, which seems to give reasonableperformance in this respect.
This could easily beincorporated into our model such that x can take a1410third value that designates that a word comes fromthe background, but removing stop words hugelyreduces the number of tokens in the data, and thusvery significantly improves the time needed to es-timate the model.In the ccMix model, the probability that aword comes from the collection-specific distribu-tion versus the shared distribution depends on asingle user-defined parameter ?C.
Since it is notclear how to set this parameter2, in our model, welearn this probability automatically.
Furthermore,the nature of the ?Cparameter is quite restrictivein that it is the same regardless of the topic andcollection.
In our model, this probability dependson the collection and topic, which should allow fora more accurate fitting of the data, as some topicsmay be shared across the collections to a differentdegree than others.Additionally, our model allows the topic dis-tributions for each document to come from non-uniform Dirichlet priors (parameterized by thevector ?c) that depends on the document?s collec-tion.
Because the learned Dirichlet parameters canbe interpreted as the average mixing level of eachtopic in the different collections, we can easily de-termine if a topic is not shared among all collec-tions, and thus we can automatically remove or setaside such topics.4 Parameter EstimationExact inference is often intractable in complexBayesian models and approximate methods mustbe used.
Blei et al (2003) offer a variational EMalgorithm for LDA.
Griffiths and Steyvers (2004)show how Gibbs sampling can be used for approx-imate inference in LDA.
Gibbs sampling is a typeof Markov chain Monte Carlo algorithm and iswhat we employ in this paper, as it is simple toderive, comparable in speed to other estimators,and it approximates a global maximum (whereasEM algorithms may only converge to a local max-imum).In a Gibbs sampler, one iteratively samples newassignments of hidden variables by drawing fromthe distributions conditioned on the previous stateof the model (Gilks et al, 1995).
In each Gibbssampling iteration we alternately sample new as-signments of z and xwith the following equations:2If needed, one can effectively set this probability manu-ally in ccLDA as well by using a large prior.P (zi|xi= 0, z?i,w, ?, ?)
?
(ndzi+ ?cz)?nziwi+ ?nzi.+ W?
(1)P (zi|xi= 1, z?i,w, ?, ?)
?
(ndzi+ ?cz)?nzi,cwi+ ?nzi,c.+ W?
(2)P (xi= 0|x?i, z,w, ?, ?)
?nz,cx=0+ ?0nz,c.+ ?0+ ?1?nziwi+ ?nzi.+ W?
(3)P (xi= 1|x?i, z,w, ?, ?)
?nz,cx=1+ ?1nz,c.+ ?0+ ?1?nzi,cwi+ ?nzi,c.+ W?
(4)Because of the conjugacy of the Beta/Dirichletand binomial/multinomial distributions, we canintegrate out ?, ?, ?
and ?
to obtain these equa-tions, a technique known as ?collapsed?
Gibbssampling (Heinrich, 2008).nbadenotes the number of times a has beenassigned to b, excluding the assignment of thecurrent token i. W is the size of the vocabu-lary.
x should be initialized as 0 for all tokens;that is, we initially assume that everything comesfrom the shared word distributions, otherwise thecollection-specific word distributions will form in-dependently.
?cis a non-uniform vector that is collection-specific.
A simple and efficient way to approxi-mate this is through moment-matching such that?cz?1Nc?dndznd., where d belongs to collection candNcis the number of documents in c (details in(Minka, 2003); (Li and McCallum, 2006)).
Theother hyperparameters can be updated similarly,although in our research we simply keep that atfixed, uniform values, as they do not largely affectthe sampling procedure at small values.5 Experimental ResultsOur experiments focus on discovering cultural dif-ferences by running our model on text from orabout three countries: the UK, India, and Singa-pore.
We explore the notion of perspective by ex-perimenting with datasets with two distinctly dif-ferent perspectives: one in which the text is abouteach country (tourists), and one in which the textis authored by residents of each country (locals).5.1 The DataIn our first experiment, we model 3,266 discus-sions from the forums at lonelyplanet.com, thelargest blog website for travelers with a forum fornearly every potential travel destination.
We showhow this can be used for comparative content ag-gregation and summarization, and we show how1411our model improves upon previous work on suchdatasets.
In the second experiment, we compareby authorship (blogs written by locals), and werun our model on 7,388 English-language weblogsfrom the same set of three different countries3.
Weshow how this is a solid step toward automatic dis-covery of cultural differences.Moreover, we compare the two perspectiveson the topic of food.
We show that there aresome strong similarities between the topic in eachdataset (thereby enforcing our inferences fromeach experiment individually), but we also showsome differences in the foods tourists find inter-esting and what locals actually eat.In all of our experiments, we ran the Gibbs sam-pler for a burn-in period of 3000 iterations, thenwe collected and averaged 15 samples, each sep-arated by a 100-iteration lag.
We used ?
= ?
=0.01 and ?0= ?1= 1.0.Our implementation is loosely based on theLDA Gibbs sampler4by Phan and Nguyen (2008).5.2 Analysis Along the Tourists DimensionIn the first experiment we consider data aboutthree destination countries.
Using the data pro-vided by lonelyplanet.com, we crawled 1,108threads from the UK forum, 1,112 from the Indiaforum, and 1,046 from the Singapore forum.
Mes-sages are predominantly written by people whohave traveled or plan to travel to that country.Since we are not interested in the thread discus-sions on a particular travel topic, we treated eachthread or discussion of multiple messages as a sin-gle document.
We were able to use simple pat-tern matching to extract only the discussion text.We removed HTML tags, stop words, and wordswith a corpus frequency less than 10.
There were703,551 tokens after preprocessing.Wemodeled this dataset with 25 topics.
Generaltopical words were grouped into the shared worddistribution of each topic, but each collection-specific distribution contained words in the topicthat best describe that country.
For example, thetopic on weather is characterized by words likeweather, rain and snow, but each collection?s dis-tribution might give one a sense of the weather ineach country.
Table 1 shows that travelers in In-dia, for example, should be aware of monsoon sea-son, and travelers to Singapore can expect to be3The dataset is available for download athttp://apfel.ai.uiuc.edu/resources.html4http://gibbslda.sourceforge.netweather time day going rainsummer month high days thanksUK India Singaporewind leh hotwaterproof monsoon humidending road humidityrolling manali heatwalkers ladakh degreerochdale trekking equatorlayers trek sweatsnow season bringfootwear rains rainankle monsoons umbrellaTable 1: The topic of weather, modeled across travel forumsfor three different countries.hot and sweaty.
The UK distribution suggests thatcampers should prepare for potentially hazardousweather with the appropriate clothing and gear.As another example, let?s consider the topicwhose shared words are english, school, language,and speak.
The results show that English is com-mon to all three, but the collection-specific worddistributions indicate that Irish language is foundin the UK region, Hindi is common in India, andMandarin is common in Singapore.Other common topics include immigration re-quirements, monetary issues, air and rail travel,etc., all containing information specific to eachcountry.
This could be used for automatic sum-marization by topic which would be useful eitherto travelers who are visiting multiple destinations,or for a potential traveler in the process of choos-ing where to go.
Someone interested in shoppingfor music should go to the UK while someone in-terested in electronics should go to Singapore, forexample (at least according to one of the topicsdiscovered).5.3 Analysis Along the Locals DimensionThe results of the first experiment offer an unsu-pervised aggregation of factual information that isimportant to travelers such as a destination?s cli-mate, law, and infrastructure; however, the datadid not offer much in terms of cultural informa-tion.
We would now like to see if we can get bet-ter insight into this problem by modeling text au-thored by residents of these same countries.
In do-ing this we can compare what they talk about andin what manner they talk about certain topics.For this experiment we downloaded 2,715 blogsfrom the UK, 2,630 blogs from India, and 2,043blogs from Singapore.
We found these English-language blogs through blogcatalog.com, a blog1412directory which lists a blog?s language and coun-try of origin.
We downloaded only the front pageof each blog, which usually included multiple ar-ticles or postings.We removed HTML tags from the documents,but we made no attempt to segment the documentsinto article text ?
there are efficient methods ofdoing this (Pasternack and Roth, 2009) and thismay be worth experimenting with, but we foundthat noise such as navigation menus and advertise-ments would mostly get grouped into their owntopics.
We removed stop words and words witha corpus frequency less than 20.
All punctua-tion was treated as word separators.
There were8,599,751 tokens in the end.Table 2 shows 3 topics induced from modelingthis data with 50 topics.
By looking at these wecan see some clear differences between the threegroups of native bloggers.
For example, Topic 1 isabout fashion, and we can compare which fashionsare popular in each country.
Shoes are popular inthe UK; leather and jewelry are more popular inIndia.
Singapore bloggers seem to focus on pricesand the shopping aspect of apparel.From Topic 2 (about pets) it seems that Britonsslightly prefer dogs and Singaporians slightly pre-fer cats.
In general, it seems that Singaporianshave an affinity for small animals, considering thepresence of hamster and rabbit in their word dis-tribution.Topic 3 is about religion, in which we see thatChristianity is common to all of them, but Hin-duism is prominent in India as well.There are many topics not shown here includ-ing politics, gardening, health, etc.
The healthtopic is interesting in that homeopathy and herbalmedicines are discussed in Indian blogs.
Smokingis a bigger topic in the UK than the others.It is also interesting to compare what technolo-gies and web services people use.
Twitter andFacebook are popular in the UK whereas Orkutis more popular in India.
Blogging services likeWordpress are popular in Singapore.From the travel topic, shown in Table 5, wesee that people travel close to home, so to speak.Britons travel around Europe, especially Spain,Paris and London, while Singaporians travel topopular destinations in that part of the world, suchas Hong Kong, Thailand and Bali.5.4 Differences in Perspective: Tourists vs.LocalsHaving modeled the same countries from two dif-ferent perspectives (that of travelers and that oflocals), it would be interesting to see how topicscompare between the two perspectives.Do people have the same view of themselves asoutsiders see them?
Are locals interested in thesame things as tourists?We hope to answer these questions by examin-ing related topics within these two datasets.
Whilethe two datasets consist of mostly different top-ics, there are a few that would be interesting tocompare.
In particular, we examine the topic offood and eating.
The top words from this topic areshown in Table 3.We first examine this topic from the blog data(that is, from the perspective of residents).
Bylooking at each collection-specific word distribu-tion we can see which foods are more popular ineach country ?
cheese and soup in the UK, curry inIndia, and seafood in Singapore.
We also noticedthat tea and coffee are more popular in Singapore,wine and beer are more popular in the UK, whilein Indian blogs beverages are not commonly men-tioned.
Perhaps a less trivial observation is thatthe words restaurant and chef are frequent in UKblogs, but the Indian word distribution is domi-nated by words pertaining to recipes.
From thisone might infer that people in the UK (and to alesser extent in Singapore) eat out more often thanpeople in India, who do more home cooking.Looking now at the topics induced from thelonelyplanet.com forums (that is, from the per-spective of travelers), we see some interesting sim-ilarities.
Most notably, the Indian distributionagain consists of words related to cooking, af-firming our observation that dining out is not aspopular in India.
The Singapore distribution alsomatches that in the other dataset ?
the commonwords include seafood and noodles.
The UK dis-tribution, however, shows that tourists are mostlyinterested in local specialties (such as fish andchips and haggis).To see where these perspectives on food differthe most, we computed the ratio of the probabil-ity of each word given the topic between the twodatasets.
That is, if p = P (w|z) in the localsdata and q = P (w|z) in the tourists data, then?
= p/q gives us a measure of how much more (orless) prominent that word is among locals than it1413Topic 1 Topic 2 Topic 3fashion style look dress wear dog dogs pet animals animal god jesus lord life faithnew collection accessories black comments cat like food plant holy man christ church loveUK India Singapore UK India Singapore UK India Singaporeshoes fashion price garden water cat church krishna godfashion women posted dog energy cats god religion sinclothing indian earrings pet carbon dog john religious johnhigh designer length cat earth pet todd spiritual spiritdesigner sarees item dogs green training bentley guru thingsstyle leather sgd pets solar pets jesus lord lamblove girls silver gardening jai hamster christ sri exoduslondon china clothes cats climate cute luke shri sufferingshirts jewellery shop puppy environment hamsters bible baba crossbag jewelry code flowers warming rabbit christian hindu livesTable 2: A sample of topics induced on a set of blogs from 3 countries.
Shown are the top 10 words from the sharedtopic-word distribution P (word|x = 0, topic) and the top 10 words from P (word|x = 1, topic, class) for each collection.Perspective of Locals Perspective of Touristsfood add chicken recipe cooking food eat restaurant restaurants teataste rice recipes sugar soup cheap meal eating cafe drinkUK India Singapore UK India Singaporefood recipe coffee chips cooking hawkerawine recipes cup haggis spices satayrestaurant powder oil fish sick stallscoffee indian comments respectability flour noodlescheese salt fried decent tomato rotisoup tsp add veggie batter stalleat rice restaurant pudding ate seafoodchef masala rice photoblog cook malayenglish oil tea sausages olive rochesterdrink coriander seafood sandwiches recipe noodleaA hawker centre is an open-air complex with manyfood stalls, commonly found in Singapore and Malaysia.Table 3: A comparison of the food topic from two differentdatasets, one of which comes from a travel forum and theother of which consists of blogs authored by residents of eachrespective country.is among tourists in the food topic.
Table 4 showsthe words with the highest (left) and lowest (right)values of ?.Preferred by Locals Preferred by Touristsrecipe bowl lemon tomato simple street cheap couple yeah crowdspring spoon vanilla stir pour old road floor run localsUK India Singapore UK India Singaporefood indian cup pubs mother quayhealthy recipes comments music ate coastshop cup tea lane tree parkwayfavorite chicken mins brick party reasonablywine minutes pot fish fields airicing kitchen note jazz base sultancoffee mustard nice pints rock tumleeds fried salt dancing toilet viewsduck ginger tarts arms bottled plentyextra salt fish recommend olive rochesterTable 4: This table shows words in the food topic that aremore popular in the tourists data than the locals data or viceversa.The prominent trend, which is largely a logis-tical matter, is that travelers are more interestedin restaurants and locals talk more about cooking.Most of the words that are more prominent fromthe tourist perspective have to do with eating loca-tions.
We also noticed that wine and coffee rankmore prominently among the locals, whereas trav-elers are more likely to ask about beer and liquor.5.5 Model EvaluationIn this subsection we evaluate ccLDA againstccMix and LDA both qualitatively, through blindjudgments of cluster quality, and quantitatively,by measuring the likelihood of held-out data witheach model.5.5.1 Cluster CoherenceBecause our research relies on analyses of discov-ered topics, it is important that we use a model thatgives the best empirical quality of word clusters.We compare against ccMix (Zhai et al, 2004),the only related model that is naturally suited toour task.
Using blind human judgments we showthat ccLDA unquestionably delivers topics that aremore coherent than those obtained with the ccMixmodel.A direct comparison with ccMix is tricky be-cause it incorporates a model for backgroundwords, whereas our model expects stop words tobe removed during preprocessing.
So that theyare fully comparable, we set the parameter ?B(the probability that a word comes from the back-ground) to 0 and fed the model the same input aswe did ccLDA.
We set the parameter ?C, analo-gous to P (x = 0), to 0.6, which is the averagevalue learned by ccLDA on this data, and it seemsquite reasonable.
Using an implementation pro-vided by the authors of ccMix, we ran the EM pro-cedure for 20 trials and saved the model with thebest log-likelihood.We performed human judgments of the 25 top-ics induced by ccLDA in the first experiment1414above and by the ccMix model with the number oftopics set again to 25.
We aligned the topics auto-matically using a symmetric KL-divergence scorecomputed on the collection-independent distribu-tions ?
specifically, D(P ||Q) + D(Q||P ) whereD(P ||Q) is the KL-divergence5of the distribu-tions P and Q.Each aligned pair of topics (ordered randomlyfor each topic to avoid bias) was presented to twonatural language processing researchers who wereasked to choose which one was better, based on thefollowing criteria: (1) semantic coherence of thetopic as a whole (e.g.
are the words in the clustersrelated?)
and (2) coherence across collections, thatis, are the collection-specific distributions relatedto each other and to the common one?
The judgeswere also given the option to rate a pair as ?noopinion?
in the case that the aligned topics weretoo dissimilar to compare (because the two mod-els did not discover the same topic), or that thetopics did not carry enough semantic informationto judge (i.e.
topics composed mostly of functionwords).Of the 25 pairs, there were 10 that both judgesrated.
Of these 10, the judges disagreed on 3.
Theother 7 were all rated in favor of ccLDA.Similarly, the 50 topics from the second exper-iment were judged against 50 topics formed us-ing ccMix.
There were 22 topics that both judgesrated.
Among these, they disagreed on only 3; ofthe remaining topics they voted in favor of ccMixfor 1 topic and in favor of ccLDA for 18 topics.It has been observed that the performance of amodel can largely depend on the estimator used(Girolami and Kab?an, 2003), so it may be that theweaker performance of ccMix is because the EMalgorithm is getting stuck in local maxima, evenafter several trials.Table 5 shows the topic of travel compared withboth ccMix and LDA.
To compare against LDA,we performed a post-hoc estimation of the topic?sword distribution for each collection by consider-ing topic assignments of documents within eachcollection.
We see that the ccLDA distributionsare much more coherent than that of ccMix.
Fur-thermore, the advantage over LDA is clear ?
withLDA, we do not get a separation of the wordsthat are common to all of the collections, and thusit is hard to detect the important differences at a5Kullback-Leibler divergence is a commonly used mea-surement of the similarity of two probability distributions.glance.5.5.2 Likelihood ComparisonTo measure how well our model can generalizeunseen documents, we compute the likelihood ofheld-out data using ccLDA compared with ccMixand LDA.
We partitioned the forum dataset fromthe first experiment into a subset of 80% of thedata on which the models are learned, and an eval-uation set of the remaining 20%.To calculate the likelihood of the held-out doc-uments with ccMix, we use the ?fold-in?
method(Hofmann, 1999) in which the mixing proportionsexcept for P (z|d) are fixed during the EM pro-cess.
As with our cluster evaluation above, we set?B= 0 and ?C= 0.6.
With LDA and ccLDA, weapproximate P (z|d) through another Gibbs sam-pling procedure, by averaging 10 samples col-lected after 100 iterations with a 10-iteration lagin between each sample.The log-likelihood of the three models is shownat various numbers of topics in Figure 2.
As ex-pected, ccLDA generally achieves a higher like-lihood than ccMix, although the difference be-tween them diminishes at higher numbers of top-ics.
This appears to be because the pLSI-basedccMix does not regularize the topic mixtures andcan thus achieve higher values of P (z|d), and thesmoothing of ccLDA has a greater effect at highernumbers of topics.Both cross-collection models achieve a higherlikelihood than LDA, which is not too surprising,given that these models utilize extra information(specifically, the document?s collection) to assigna higher probability to words more likely to appearin a document given that information.It should be noted that even though the like-lihood of both cross-collection models increaseswith the number of topics up to 100, we observedempirically that the best cluster quality in thisdataset occurs around 20 to 30 topics; more thanthat results in clusters that are repeated and arelargely specific to only one collection.6 Discussion and Future WorkWhile there are obvious limitations of the unigramapproach used here, our system was neverthelessable to capture some interesting details.
It is im-portant, however, to point out some limitations forpossible future extensions.Consider Topic 2 in Table 2.
The UK and Singa-pore word distributions are both clearly pertinent1415ccLDA ccMix LDAtravel hotel hotels city best travel hotel comments hotels city travel city hotel park holidayplace holiday visit trip world posted road trip labels airport hotels place beach road visitUK India Singapore UK India Singapore UK India Singaporeholiday india singapore yang india yang travel travel travelholidays delhi kong train delhi dan holiday city hotelhotels indian hong london tourism ini hotel beach cityspain mumbai spa saya dubai dengan city place parklondon bangalore hotel nie indian untuk london hotel placegreat tour beach travel tour itu park temple beachsurf air chinese flight bangalore saya hotel road tripbreaks dubai pictures luxury mahindra orang place park hotelstrain city restaurant dan hotels tidak holidays hotels spaski mahindra bangkok advert marathi dalam hall tourism visitTable 5: The topic of travel as discovered by the 3 different models.Figure 2: Comparison of the log-likelihood of held-out datawith the 3 models.to the topic of pets, but the India distribution seemsentirely unrelated, being about energy and the en-vironment.
This could be because the environmenttopic was statistically too strong to ignore, but notfound in other collections, so it made its way intoa largely unrelated topic.
(In fact, the formation ofthe environment cluster within this topic is not en-tirely random, as the pets topic also includes somewords related to gardening, including ?water?
and?plant?, which are likely to also co-occur with en-vironmental words.
)This is perhaps the main weakness of the model.If an emerging topic is not shared among all col-lections, it will either form as a primary topicthat is unique to only a subset of collections (andthus some of the collection-specific distributionswill be noisy), or it will form as a collection-specific distribution that is not strongly relatedto the main collection-independent distribution.This can make the results difficult to interpret,although an automated solution would be to re-move or flag topics that are not evenly shared,which could be done by comparing the learnedcollection-dependent Dirichlet parameters ?c.This is also a matter of how the model performswith different numbers of collections.
It wouldbe interesting to see what results we would getby modeling UK-India, UK-Singapore, and India-Singapore as only a pair at a time.
The perfor-mance should not degrade with larger numbers ofcollections if the collections are fully compara-ble, but in practice, with more collections thereare likely to be more topics that are difficult to fitacross all collections.In future work, we would like to enrich themodel and/or feature set to move beyond the lim-itations of a bag-of-words analysis.
For example,by considering negation and word polarity, we canbetter capture the opinions of the authors, which isan important component of such cultural analysis.Certainly, there are many other possible appli-cations of this model, including product compar-ison, media bias detection, and interdisciplinaryliterature analysis.
Cultural awareness is also im-portant in marketing and we can use this model toinvestigate, for example what products and whataspects of life people in different regions focus on.AcknowledgmentsWe would like to thank ChengXiang Zhai forthoughtful discussions and for providing an imple-mentation of the ccMix model.
We would also liketo thank the reviewers for their constructive com-ments.1416ReferencesD.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research,3.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theem algorithm.
Journal of the Royal Statistical Soci-ety.
Series B (Methodological), 39(1):1?38.W.R.
Gilks, S. Richardson, and D.J.
Spiegelhalter.1995.
Markov Chain Monte Carlo in Practice.
CRCPress.M.
Girolami and A. Kab?an.
2003.
On an equivalencebetween plsi and lda.
In SIGIR ?03: Proceedingsof the 26th annual international ACM SIGIR confer-ence on Research and development in informaion re-trieval, pages 433?434, New York, NY, USA.
ACM.T.
Griffiths and M. Steyvers.
2004.
Finding scientifictopics.
In Proceedings of the National Academy ofSciences of the United States of America.Tl Griffiths, M. Steyvers, and Jb Tenenbaum.
2007.Topics in semantic representation.
PsychologicalReview, 114(2):211?244.D.
Hall, D. Jurafsky, and C. Manning.
2008.
Study-ing the history of ideas using topic models.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages363?371.G.
Heinrich.
2008.
Parameter estimation for text anal-ysis.
Technical report, University of Leipzig.T.
Hofmann.
1999.
Probabilistic latent semantic in-dexing.
In SIGIR ?99: Proceedings of the 22nd an-nual international ACM SIGIR conference on Re-search and development in information retrieval,pages 50?57, New York, NY, USA.
ACM.W.
Li and A. McCallum.
2006.
Pachinko allocation:Dag-structured mixture models of topic correlations.In International Conference on Machine Learning.A.
McCallum, X. Wang, and A. Corrada-Emmanuel.2007a.
Topic and role discovery in social networkswith experiments on enron and academic email.Journal of Artificial Intelligence Research (JAIR),30:249?272.A.
McCallum, X. Wang, and N. Mohanty.
2007b.
Jointgroup and topic discovery from relations and text.In Statistical Network Analysis: Models, Issues andNew Directions - Lecture Notes in Computer Science4503, pages 28?44.D.
Mimno, W. Li, and A. McCallum.
2007.
Mixturesof hierarchical topics with pachinko allocation.
InInternational Conference on Machine Learning.T.
Minka.
2003.
Estimating a dirichlet distribution.T.
Mitchell.
1997.
Machine Learning.
McGraw-Hill,Boston.J.
Pasternack and D. Roth.
2009.
Extracting article textfrom the web with maximum subsequence segmen-tation.
In The International World Wide Web Con-ference, April.M.
Paul and R. Girju.
2009.
Topic modeling of re-search fields: An interdisciplinary perspective.
InProceedings of the the International Conference onRecent Advances in Natural Language Processing(RANLP) (to appear).X.
Phan, L. Nguyen, and S. Horiguchi.
2008.
Learningto classify short and sparse text & web with hiddentopics from large-scale data collections.
In WWW?08: Proceeding of the 17th international conferenceon World Wide Web, pages 91?100, New York, NY,USA.
ACM.X.
Wang, A. McCallum, and X. Wei.
2007.
Top-ical n-grams: Phrase and topic discovery, with anapplication to information retrieval.
In ICDM ?07:Proceedings of the 2007 Seventh IEEE InternationalConference on Data Mining, pages 697?702.
IEEEComputer Society.C.
Wang, B. Thiesson, C. Meek, and D. Blei.
2009.Markov topic models.
In The Twelfth InternationalConference on Artificial Intelligence and Statistics(AISTATS), pages 583?590.T.
Yano, W. Cohen, and N. Smith.
2009.
Predictingresponse to political blog posts with topic models.In The 7th Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL).C.
Zhai, A. Velivelli, and B. Yu.
2004.
A cross-collection mixture model for comparative text min-ing.
In Proceedings of KDD 04, pages 743?748.1417
