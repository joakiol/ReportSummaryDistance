Word Sense Disambiguation Based on Structured Semantic Space*Ji Donghong Huang ChangningDepartment o f  Computer ScienceTsinghua UniversityBeijing, 100084, P. R. ChinaEmail: jdh@s1000e.cs.tsinghua.edu.cnhcn@mail.tsinghua.edu.cnAbstractIn this paper, we propose a framework, structuredsemantic space, as a foundation for word sensedisarnbiguation tasks, and present a strategy toidentify the correct sense of a word in somecontext based on the space.
The semantic space isa set of multidimensional real-valued vectors,which formally describe the contexts of words.Instead of locating all word senses in the space,we only make use of mono-sense words tooutline it.
We design a merging procedure toestablish the dendrogram structure of the spaceand give an heuristic algorithm to find the nodes(sense clusters) corresponding with sets ofsimilar senses in the dendrogram.
Given a wordin a particular context' he context would activatesome clusters in the dendrogram, based on itssimilarity with the contexts of the words in theclusters, then the correct sense of the word couldbe determined by comparing its definitions withthose of the words in the clusters.1.
IntroductionWord sense disambiguation has long been one ofthe major concerns in natural anguage processingarea (e.g., Bruce et al, 1994; Choueka et al, 1985;Gale et al, 1993; McRoy, 1992; Yarowsky 1992,1994, 1995), whose aim is to identify the correctsense of a word in a particular context, among all ofits senses defined in a dictionary or a thesaurus.Undoubtedly, effective disambiguation techniquesare of great use in many natural language processingtasks, e.g., machine translation and informationretrieving (Allen, 1995; Ng and Lee, 1996; Resnik,1995), etc.Previous strategies for word sensedisambiguation mainly fall into two categories:statistics-based method and exemplar-based method.Statistics-based method often requires large-scalecorpora (e.g., Hirst, 1987; Luk, 1995), sense-taggingor not, monolingual or aligned bilingual, as trainingdata to specify significant clues for each word sense.The method generally suffers from the problem ofdata sparseness.
Moreover, huge corpora, especiallysense-tagged or aligned ones, are not generallyavailable in all domains for all languages.Exemplar-based method makes use of typicalcontexts (exemplars) of a word sense, e.g., verb-noun collocations or adjective-noun collocations,and identifies the correct sense of a word in aparticular context by comparing the context with theexemplars (Ng and Lee, 1996).
Recently, somekinds of learning techniques have been applied tocumulatively acquire exemplars form large corpora(Yarowsky, 1994, 1995).
But ideal resources fromwhich to learn exemplars are not generally availablefor any languages.
Moreover, the effectiveness ofthis method on disambiguating words in large-scalecorpora into fine-grained sense distinctions needs tobe further investigated (Ng and Lee, 1996).
* The work is supported by National Science Foundation fChina.187A common assumption held by bothapproaches i  that neighboring words provide strongand consistent clues for the correct sense of a targetword in some context.
In this paper, we also holdthe same assumption, but start from a different point.We see the senses of all words in a particularlanguage as forming a space, which we callsemantic space, for any word of the language, eachof its senses is regarded as a point in the space.
Sothe task of disambiguating a word in a particularcontext is to locate an appropriate point in the spacebased on the context.Now that word senses can be generallysuggested by their distributional contexts, we modelsenses with their contexts.
In this paper, weformalize the contexts as a kind of multidimensionalreal-valued vectors, so the semantic space can beseen as a vector space.
The similar idea aboutrepresenting contexts with vectors has beenproposed by Schuetze (1993), but what his workfocuses on is the contexts of  words, while what weconcern is the contexts of word senses.
Furthermore,his formulation of contexts is based on wordfrequencies, while we formalize them with semanticcodes given in a thesaurus and their salience withrespect to senses.It seems that we should first have a large-scalesense-tagged corpus in order to build semantic space,but establishing such a corpus is obviously too time-consuming.
To simplify it, we only try to outline thesemantic space by locating the mono-sense words inthe space, rather than build it completely by spottingall word senses in the space.Now that we don't try to specify all wordsenses in the semantic space, for a word in aparticular context, it may be the case that we cannotdirectly spot its correct sense in the space, becausethe space may not contain the sense at all.
But wecould locate some senses in the space which aresimilar with it according to their contexts, and basedon their definitions given in a dictionary, we couldmake out the correct sense of the word in thecontext.In our implementation, we first build thesemantic space based on the contexts of the mono-sense words, and structure the senses in the space asa dendrogram, which we call structured semanticspace.
Then we make use of an heuristic method todetermine some nodes in the dendrogram whichcorrespond with sets of similar senses, which wecall sense clusters.
Finally, given a target word in aparticular context, some clusters in the dendrogramcan be activated by the context, then we can makeuse of the definitions of the target word and thewords ~ in the clusters to determine its correct sensein the context.The remainder of the paper is organized asfollows: Section 2 defines the notion of semanticspace, and discuss how to outline it by establishingthe context vectors for mono-sense words.
Section 3examines the structure of the semantic space, andintroduces algorithms to merge the senses into adendrogram and specify the nodes in it whichcorrespond with sets of similar senses.
Section 4discusses the disambiguation procedure based on thecontexts.
Section 5 describes ome experiments andtheir results.
Section 6 presents ome conclusionsand discusses the future work2 Semantic SpaceIn general, a word may have several senses and mayappear in several different kinds of contexts.
From apoint of empirical view, we suppose that each senseof a word is corresponded with a particular kind ofcontext it appears, and the similarity between wordsenses can be measured by their correspondingcontexts.
For a particular kind of language, weregard its semantic space as the set of all wordsenses of the language, with similarity relationbetween them.Now that word senses are in accordance withtheir contexts, we use the contexts to model wordsenses.
Due to the unavailability of  large~-scalei Because the senses in the semantic space are of mono-sensewords, we don't distinguish "words" from "senses" strictlyhere.188sense-tagged corpus, we try to outline the semanticspace by only taking into consideration the mona-sense words, instead of locating all word senses inthe space.In order to formally represent word senses, weformalize the notion of context as multidimensionalreal-valued vectors.
For any word, we first annotateits neighboring words within certain distances in thecorpus with all of their semantic codes in athesaurus respectively, then make use of such codesand their salience with respect to the word toformalize its contexts.
Suppose w is a mona-senseword, and there are n occurrences of the word in acorpus, i.e., wt, we .
.
.
.
, w,, (1) lists their neighborwords within d word distances respectively.
(l)al,.d, al,-(d.l), .
.
.
,  al,- iaz-a, az-ca-O .
.
.
.
, az.tan,~, a,,-(d-O .
.
.
.
.
an, aWI at, l, al,2, .
.
.
,  al ,dwe azt, az2 .
.
.
.
.
azaWn an, l, an, e, .
.
.
,  an, dSuppose Cr is the set of all the semantic codesdefined in a thesaurus, for any occurrence wt, 1_< i.~_n,let NCi.
be the set of all the semantic odes of itsneighboring words which are given in the thesaurus,for any c ~ Cr, we define its salience with respect tow, denoted as Sal(c, w), as (2).
(2) Sal(c, w) -Itw, Nc,)InSo we can build a context vector for w as (3),denoted as CVw, whose dimension is \[Cr\[.Chinese thesaurus, iii) a Chinese corpus consistingof 80 million Chinese characters.
In the Chinesedictionary, 37,824 words have only one sense,among which only 27,034 words occur in the corpus,we select 15,000 most frequent mona=sense wordsin the corpus to build the semantic space forChinese.
In the Chinese thesaurus, the words aredivided into 12 major classes, 94 medium classesand 1428 minor classes respectively, and each classis given a semantic ode, we select the semanticcodes for the minor classes to formalize the contextsof the words.
So k=\[ CT\[ =1428.3.
Structure of Semantic Space?
Due to the similarity/dissimilarity relation betweenword senses, those in the semantic space cannot bedistributed in an uniform way.
We suppose that thesenses form some clusters, and the senses in eachcluster are similar with each other.
In order to makeout the clusters, we first construct a dendrogram ofthe senses based on their similarity, then make useof an heuristic strategy to select some appropriatenodes in the dendrogram which most likelycorrespond with the clusters.Now that word senses occur in accordance withtheir contexts, we measure their similarity/dissimilarity by their contexts.
For any two sensesst, seeS, let cvt=(xt xe ... xk), cve=( Yt Ye ... yk) betheir context vectors respectively, we define thedistance between st and se; denoted as dis(st, se),based on the cosine of the angle between the twovectors.
(4) dis(st, s2)=l-cos(cvt, cv2) 2(3) cvw=<Sal(ct, w), Sal(c2, w) .
.
.
.
.
Sal(ck, w)>where k= I CTI.When building the semantic space for Chineselanguage, we make use of the following resources, i)Xiandai Hanyu Cidian(1978), a Modem ChineseDictionary, ii) Tongyici Cilin(Mei et al 1983), aObviously, dis(st, s2) is a normalized coefficient: itsvalue ranges from 0 to 1.Suppose S is the set of the mona-senses in the189I V l~i~k l~i~ksemantic space, for any sense si~S, we create apreliminary node dj, and let Idjl=l, which denotesthe number of senses related with d~.
Suppose D bethe set of all preliminary nodes, the following is thealgorithm to construct the dendrogram.0.105O.
078O.
04.3 O.
058ai shang/ /b e ishartg/ /b e it ong/ /b e iai/O.
067~.t~/ shartgbei/ / a:i.t o r t~Fig.
1 A subtree of the dendrogram for Chinese mono-sense wordsAlgorithm 1.Procedure Den-construct(D)begini) select dr and d2 among all in D,whose distance is the smallest;ii) merge dl and d2 into a new node d,and let Idl=l dl I+l de l;iii) remove dr and d2 from D, and putd into D;iv) compute the context vector of dbased on the vectors of dl and d/;v) go to i) until there is only onenode;end;Obviously, the algorithm is a bottom-up mergingprocedure.
In each step, two closest nodes areselected and merged into a new one.
In (n-1)th step,where n is the number of word senses in S, a finalnode is produced.
The complexity of the algorithmis O(n 3) when implementing it directly, but can bereduced to  O(n 2) by sorting the distances betweenall nodes in each previous tep.Fig.
1 is a sub-tree of the dendrogram we buildfor Chinese.
It contains six mono-sense words,whose English correspondences are sad, sorrowful,etc.
In the sub-tree, we mark each non-preliminarynode with the distance between the two merged sub-nodes, which we also refer to as the weight of thenode.It can be proved that the distances between themerged nodes in earlier merging steps are smallerthan those in later merging steps 4.
According to thesimilarity/dissimilarity relation between the senses,there should exist a level across the dendrogramsuch that the weights of the nodes above it arebigger, while the weights of the nodes below itsmaller, in other words, the ratio between the meanweight of the nodes above the level and that of thenodes below the level is the biggest.
Furthermore wesuppose that the nodes immediately below the levelcorrespond with the clusters of similar senses.
So, in3 We call (zl z2 ... zk ) the context vector of  d, where for all i,l_<i_<k, zi = (\[ d, \]e x, +1 d21- y,)/I d I.4 This can be seen from the fact that the context vector o fd  isa linear composit ion of  the vectors of  dl and d2.190order to make out the sense clusters, we only need todetermine the level.Unfortunately, the complexity of determiningsuch a level is exponential to the edges in thedendrogram, which demonstrates that he problem ishard.
So we adopt an heuristic strategy to determinean optimal level.Suppose T is the dendrogram, sub_T is the sub-tree of T, which takes the same root as T, we alsouse T and sub T to denote the sets of non-preliminary nodes in T and in sub_T respectively,for any de T, let Wei(d) be the weight of the node d,we define an object function, as (5):(5)~ Wei ( d) /d~sub T /sub_  7~Oby(sub_T)-  Wei(d)/de(T-sub T) /- /JZ- ub/* v.L is not a sense cluster */else Tee-- Tc ~ {v.L};/* v.L is a sense cluster */if Obj(sub T+v.R)>Obj(sub T)then Clustering(v.R, sub T)/* v.R is not a sense cluster */else Tc?-- Tc u {v.R};/* v.R is a sense cluster */end;The algorithm is a depth-first searchprocedure.
Its complexity is O(n), where n is thenumber of the leaf nodes in the dendrogram, i.e., thenumber of the mono-sense words in the semanticspace.When building the dendrograrn for the Chinesesemantic space, we found 726 sense clusters in thespace.
The distribution of the senses in the clustersis demonstrated in Table 1.where the numerator is the mean weight of thenodes in sub_T, while the denominator is the meanweight of the nodes in T-sub_T.In order to specify the sense clusters, we onlyneed to determine a sub-tree of T which makes (5)get its biggest value.
We adopt depth-first searchstrategy to determine the sub-tree.
Suppose vo is theroot ofT, for any veT, we use v.L andv.R to denoteits two sub-nodes, let Tc be the set of all the nodescorresponding with the sense clusters, we can get Tcby Clustering(vo, Nlff) calling the followingprocedure.Algorithm 2Clustering(v, sub_T)beginsub_T~ sub_T+ {v} ;/* add node v to the subtree*/if Obj(sub_T+v.L)>Obj(sub_T)then Clustering(v.L, subT)5 NIL is a preliminary value for sub_T, which demonstratesthe tree includes no nodes.Number of senses Number of clusters\[1, 10)\[10, 20)\[20, 30 )\[30, 40), \[40, 58),921572971764All: 726Table 1.
The distribution of senses in the clusters4.
Disambiguation ProcedureGiven a word in some context, we suppose thatsome clusters in the space can be activated by thecontext, which reflects the fact that the contexts ofthe clusters are similar with the given context.
Butthe given context may contain much noise, so theremay be some activated clusters in which the sensesare not similar with the correct sense of the word inthe given context.
But due to the fact that the givencontext can suggest the correct sense of the word,there should be clusters, among all activated ones, inwhich the senses are similar with the correct sense.191To make out these clusters, we make use of thedefinitions of the words in the Modem ChineseDictionary, and determine the correct sense of theword in the context by measuring the similaritybetween their definitions.percent10.90.80.70.60.50.40.30.20.10090.2~3.66-/0.87"90%/ .
.70 .60'/'21"20?
'2-30% a %, ~ ,0.1 0.2 0.3 0.4 0.5distance4.1 ActivationGiven a word w in some context, we consider thecontext as consisting of n words to the left of theword, i.e., w.,,, w.(,.
0 .... , w.l and n words to the rightof the word, i.e., wl, w2, w3 ..... w,,.
We make use ofthe semantic odes given in the Chinese thesaurus to.
.
.
.
.
.
.
.
.
.
.
.
,-  .
.
.
.
,,nnnno/_ l nno /^ 100%.
l l .#v /o  ~u.uv /0  Jo .
I J T l l r  .
.
.
.
.
.
.
- .
.
.
.I I !
I I0.6 0.7 0.8 0.9 1Fig.2 The distribution ofdisl(clu~, w).create a context vector to formally model thecontext,.
Suppose NCw be the set of all semanticcodes of the words in the context, then cvw=<x~,x2 ..... xp,  where if c~eNC,, then x~=l; otherwisex~=O.For any cluster clu in the space, let cvau be itscontext vector, we also define its distance from wbased on the cosine of the angle between theircontext vectors as (6).
(6) disl(clu, w)=l-cos(cvdu, cvw)We say clu is activated, if disl(clu, w).~dl, where dtis a threshold.
Here we don't define the activatedcluster as the one which makes disl(clu, w) smallest,this is because that the context may contain muchnoise, and the senses in the cluster which makesdisj(clu, w) smallest may not be similar with thevery sense of the word in the context.To estimate a reasonable value for dl, we cancompute the distance between the context vector ofeach mono-sense word occurrence in the corpus andthe context vector of the cluster containing the word,then select a reasonable value ford1 based on thesedistances as the threshold.
Suppose CLU is the setof all sense clusters in the space, O is the set of alloccurrences of the mono-sense word in the corpus,for any weO,  let cluw be the sense cluster containingthe sense in the space, we compute all distancesdist(cluw, w), for all weO.
It should be the case thatmost values for disl(cluw, w) will be smaller than athreshold, but some will be bigger, even close to 1,this is because most contexts in which the mono-sense words occur would contain meaningful wordsfor the senses, while other contexts contain muchnoise, and less words, even no words in the contextsare meaningful for the senses.When estimating the parameter di for theChinese semantic space, we let n=5, i.e., we onlytake 5 words to the left or the right of a word as itscontext.
Fig.
2 demonstrates the distribution of thevalues of disl(cluw, w), where X axle denotes the192distance, and Y axle denotes the percent of thedistances whose values are smaller than x~\[0, 1\]among all distances.
We produce a function fix) tomodel the distribution based on commonly usedsmoothing tools and locate its inflection point bysettingf"(x)=0.
Finally we get x=0.378, and let it bethe threshold r.(7)I{w,\[csat(c, clu) =nWe call (8) definition vector ofclu, denoted as dvd~.
(8) dvau =< sal(cl, clu), sal(c2, clu) ..... sal(ck, clu)>4.2 Definition-Based DisambiguationGiven a word w in some context c, suppose CLU~ isthe set of all the clusters in the semantic spaceactivated by the context, he problem is to determinethe correct sense of the word in the context, amongall of its senses defined in the modem Chinesedictionary.The activation of the clusters in CLUw by thecontext c demonstrates that c is similar with thecontexts of the clusters in CLUw, so there should beat least one cluster in CLU~, in which the senses aresimilar with the correct sesne of w in c. On the otherhand, now that the senses in a cluster are similar inmeaning, their definitions in the dictionary shouldcontain similar words, which can be characterized asholding the same semantic odes in the thesaurus.So the definitions of all the words in the clusterscontain strong and meaningful information about hevery sense of the word in the context.We first construct wo definition vectors tomodel the definitions of all the words in a clusterand the definitions of w based on the semantic codesof the definition words 6, then determine the sense ofw in the context by measuring the similarity betweeneach definition of w and the definitions of all thewords in a cluster.For any clu~CLU~, suppose clu={wJ l_<ig_n},let C~ be the set of all semantic odes of all thewords in w;s definition, CT be defined as above, i.e.,the set of all the semantic codes in the thesaurus, forany CeCT, we define its salience with respect to clu,denoted as sal(c, clu), as (7).6 The words in the definitions are called efinition words.Suppose Sw is the set ofw's senses defined in thedictionary, for any sense s~Sw, let Cs be the set of allthe semantic odes of its definition words, we calldvs=<xl, x2 ... .
.
xk> definition vector of s, where forall i, ifci~C,, x~=l; otherwise x~=0.We define the distance between an activatedcluster in the semantic space and the sense of a wordas (9) again in terms of the cosine of the anglebetween their definition vectors.
(9) dis2(clu, s)=l-cos(dvau, dv,)Intuitively the distance can be seen as a measureof the similarity between the definitions of thewords in the cluster and each definition of the word.Compared with the distance defined in (6), thisdistance is to measure the similarity betweendefinitions, while the distance in (6) is to measurethe similarity between contexts.Thus it is reasonable toselect he sense s* amongall as the correct one in the context, such that thereexists clu'~CLUw, and dis2(clu*, s*) gets the smallestvalue as (10), for clu~CLUw, and s~Sw.
(1 O) MIN dis 2 ( clu, s)c IueCLU w ,s~.S,,5.
Experiments and ResultsIn order to evaluate the application of the Chinesesemantic space to WSD tasks, we make use ofanother Chinese lexical resource, i.e., XiandaiHanyu Cihai (Zhang et al, 1994), a Chinesecollocation dictionary.
The sense distinctions in thedictionary are the same as those in the modemChinese dictionary, and for each sense in the193collocation dictionary, some words are listed as itscollocations.
We see these collocations as thecontexts of the word senses, and evaluate ouralgorithm automatically.
We randomly select 40ambiguous words contained in the dictionary, andthere are altogether 1240 words listed as theircollocations.
Table 2 lists the distribution of thenumber of the sense clusters activated by thesecollocation words.Table 3 lists the distribution of the smallestdistances between the word senses and the activatedclusters, and the accuracy of the disambiguation.From Table 3, we can see that smaller distancesbetween the senses and the activated clusters meanhigher accuracy of disambiguation.Number of activated Number of collocationsclusters1 4202 3803 2504 100~5 90All: 1240Table 2.
Collocation words and the numberof activated clustersoccurrences of the word in the corpus, andimplement our algorithm on them respectively.
Theresult is 66 occurrences are tagged with the secondsense (6 occurrences wrongly tagged), and theothers tagged with the first sense (2 occurrenceswrongly tagged).
The overall accuracy is 92%.
Toexamine the reasonableness of the result, weformalize four context vectors again based onsemantic codes to represent the contexts of fourgroups of the occurrences:cvl: the context of the 60 occurrencescorrectly tagged with the second sense;cv2: the context of the 6 occurrences wronglytagged with the second sense;cv3: the context of the 32 occurrencescorrectly tagged with the first sense;cv4: the context of the 2 occurrences wronglytagged with the first sense;The distances between these vectors are listed inTable 4:CVLcv2cv3cv4CV I CV 20.3640.3640.9140.8250.941cv3 cv40.914 0.8250.941 0.8760.3200.876 0.320Distance area\[0.0 0.2)\[0.2 0.4)\[0.4 0.6)\[0.6 1.0)Percent(%)27.3Accuracy(%)94.258.2 90.59.6 40.54.9 10.4Table 3.
Distribution of distances anddisambiguation accuracyIn another experiment, we examine theambiguous Chinese word --0-~ (/danbo/7), it hastwo senses, one is less clothes taken by a man, theother is thin and weak.
We randomly select 1007 The Piyin of the word.Table 4.
The distances between the contexts ofthe four groupsFrom Table 4, we find that both the distancebetween cvl and cv4 and that between cv2 and cv3are very high, which reflects the fact that they arenot similar with each other.
This demonstrates thatone main reason for tagging errors is that theconsidered contexts of the words contain lessmeaningful information for determining the correctsenses.In third experiment, we implement our algorithmon 100 occurrences of the ambiguous word f~l~(/bianji/), it also has two senses, one is editor, theother is to edit.
We find the tagging accuracy is verylow.
To explore the reason for the errors, we194compute the distances between its definitions andthose of the words in the activated clusters, and findthat the smallest distances fall in \[0.34, 0.87\].
Thisdemonstrates that another main reason for thetagging errors is the sparseness of the clusters in thespace.6.
Conclusions and Future workIn this paper, we propose a formal resource oflanguage, structured semantic space, as afoundation for word sense disambiguation tasks.
Fora word in some context, the context can activatesome sense clusters in the semantic space, due to itssimilarity with the contexts of the senses in theclusters, and the correct sense of the word can bedetermined by comparing its definitions and those ofthe words in the clusters.Structured semantic space can be seen as ageneral model to deal with WSD problems, becauseit doesn't concern any language-specific knowledgeat all.
For a language, we can first make use of itsmono-sense word to outline its semantic space, andproduce a dendrogram according to their similarity,then word sense disambiguation can be carried outbased on the dendrogram and the definitions of thewords given in a dictionary.As can be seen that ideal structured semanticspace should be homogeneous, i.e., the clusters in itshould be well-distributed, neither too dense nor toosparse.
If it is too dense, there may be too manyclusters activated by a context.
On the contrary, if itis too sparse, there may be no clusters activated by acontext, even if there is any, it may be the case thatthe senses in the clusters are not similar with thecorrect sense of the target word.
So future workincludes how to evaluate the homogeneity of thesemantic space, how to locate the non-homogeneousareas in the space, and how to make themhomogeneous.Obviously, the disambiguation accuracy will bereduced if the cluster contains less words, becauseless words in the cluster will lead to invalidity of itsdefinition vectors in revealing the similar wordsincluded in their definitions.
But it seems to beimpossible to ensure that every cluster containsenough words, with only mono-sense words takeninto consideration when building the semantic space.In order to make the cluster contain more words, wemust make use of ambiguous words.
So future workincludes how to add ambiguous words into clustersbased on their contexts.Another problem is about the length of thecontexts to be considered.
With longer contextstaken into consideration, there may be too manyclusters activated.
But if we consider shortercontexts, the meaningful information for word sensedisambiguation may be lost.
So future work alsoincludes how to make an appropriate decision on thelength of the contexts to be considered, meanwhilemake out the meaningful information carried by thewords outside the considered contexts.ReferencesJ.
Allen.
1995.
Natural Language Understanding,The Benjamin/Cumming Publishing Company,INC.Rebecca Bruce, Janyce Wiebe.
1994.
Word sensedisarnbiguation using decomposable models.
InProceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics, LasCruces, New Mexico.Y.
Choueka nd S. Lusignan.
1985.
Disambiguationby short contexts.
Computers and theHumanities, 19:147-157.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992.
Estimating upper and lowerbounds on the performance of word-sensedisambiguation programs.
In Proceedings of the30th Annual Meeting of the Association forComputational Linguistics, Newark, Delaware.Graeme Hirst.
1987.
Semantic Interpretation a dthe Resolution of Ambiguity.
CambridgeUniversity Press, Cambridge.Alpha K.Luk.
1995.
Statistical sense disambiguationwith relatively small corpora using dictionarydefinitions.
In Proceedings of the 33th Annual195Meeting of the Association for ComputationalLinguistics, Cambridge, Massachusetts.Susan W. McRoy.
199Z Using multiple knowledgesources for word  sense disambiguation.Computational Linguistics, 18(1): 1-30.J.J.Mei et al 1983.
TongYiCi CiLin (A ChineseThesaurus), Shanghai Cishu press, Shanghai.Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguatingword sense: an exemplar-based approach.
InProceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics.P.
Resnik.
1995.
Disambiguating noun groupingswith respect to WordNet senses, In Proceedingsof 3rd Workshop on Very Large Corpus, MIT,USA, 54-68.H.
Schutz?.
1993.
Part-of-speech induction fromscratch.
In Proceedings of the 31st AnnualMeeting of the Association for ComputationalLinguistics, Columbus, OH.Xiandai Hanyu Cidian.
(a Modern ChineseDictionary).
1978.
Shnagwu Press, Beijing (inChinese).D.
Yarowsky.
1992.
Word sense disambiguationusing statistical models of Roget's categoriestrained on large corpora, Proceedings ofCOLING '92, Nantas, France, 454-460.David Yarowsky.
1994.
Decision lists for lexicalambiguity resolution: Application to accentrestoration in Spanish and French.
InProceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics, LasCruces, New Mexico.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33th Annual Meeting of theAssociation for Computational Linguistics,Cambridge, Massachusetts.Zhang et al 1994.
Xiandai Hanyu Cihai.
(a ChineseCollocation Dictionary), Renmin ZhongguoPress (in Chinese).196
