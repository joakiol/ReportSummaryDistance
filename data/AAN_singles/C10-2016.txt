Coling 2010: Poster Volume, pages 135?143,Beijing, August 2010A comparison of unsupervised methods forPart-of-Speech Tagging in ChineseAlex ChengMicrosoft Corporationalcheng@microsoft.comFei XiaUniv.
of Washingtonfxia@uw.eduJianfeng GaoMicrosoft Researchjfgao@microsoft.comAbstractWe conduct a series of Part-of-Speech(POS) Tagging experiments using Ex-pectation Maximization (EM), Varia-tional Bayes (VB) and Gibbs Sampling(GS) against the Chinese Penn Tree-bank.
We want to first establish a base-line for unsupervised POS tagging inChinese, which will facilitate future re-search in this area.
Secondly, by com-paring and analyzing the results betweenChinese and English, we highlight someof the strengths and weaknesses of eachof the algorithms in POS tagging taskand attempt to explain the differencesbased on some preliminary linguisticsanalysis.
Comparing to English, we findthat all algorithms perform rather poorlyin Chinese in 1-to-1 accuracy result butare more competitive in many-to-1 accu-racy.
We attribute one possible explana-tion of this to the algorithms?
inability tocorrectly produce tags that match thedesired tag count distribution.1 IntroductionRecently, there has been much work onunsupervised POS tagging using HiddenMarkov Models (Johnson, 2007; Goldwater &Griffiths, 2007).
Three common approaches areExpectation Maximization (EM), VariationalBayes (VB) and Gibbs Sampling (GS).
EM wasfirst used in POS tagging in (Merialdo, 1994)which showed that except in conditions wherethere are no labeled training data at all, EMperforms very poorly.
Gao and Johnson (2008)compared EM, VB and GS in English againstthe Penn Treebank Wall Street Journal (WSJ)text.
Their experiments on English showed thatGS outperforms EM and VB in almost all cases.Other notable studies in the unsupervised andsemi-supervised POS domain include the use ofprototype examples (Haghighi & Klien, 2006),dictionary constraints to guide the algorithms(Elworthy 1994; Banko & Moore 2004) andBayseian LDA-based model (Toutanova andJohnson, 2007).To our knowledge, little work has been doneon unsupervised POS tagging in Chinese againstthe Chinese Penn Treebank (CTB).
The workin Chinese POS tagging has been predominatelyin the supervised fashion (Huang et al 2009;Chang & Chen, 1993; Ng & Low, 2004) andachieve accuracy of 92.25% using a traditionalngram HMM tagger.
For English, a supervisedtrigram tagger achieves an accuracy of  96.7%against the Penn Treebank (Thorsten, 2000).In this study, we analyze and compare theperformance of three classes of unsupervisedlearning algorithms on Chinese and report theexperimental results on the CTB.
We establisha baseline for unsupervised POS tagging inChinese.
We then compare and analyze theresults between Chinese and English, weexplore some of the strengths and weaknessesof each of the algorithms in POS tagging taskand attempt to explain the differences based onsome preliminary linguistics analysis.2 ModelsIn this section, we provide a brief overview ofthe three unsupervised learning methods forPOS tagging as described in (Gao & Johnson,2008), which all uses a traditional bigram Hid-den Markov Model (HMM).
HMM is a well-135known statistical model, used for sequentialmodeling.
To put it formally, letbe the set of possiblestates and                  be the setof possible observations.
In the case for POStagging using a bigram model, the set   corres-ponds to the set of POS tags and the set  cor-responds to the set of words in the language.Figure 1: Graphical model of an HMM for abigram POS tagger.
The top row represents asequence of hidden states where each is condi-tionally dependent only on the previous stateand the bottom row represents a sequence ofobservations where each is conditionally depen-dent only on the current state.An HMM models a sequence of discrete ob-servations                wherethat are produced by a sequence of hiddenstates                 where       .
Thesequence of states is produced by a first orderMarkov process such that the current statedepends only on its previous state     ; corres-pondingly each of the observations    dependsonly on the state   :where                 is the probability oftransition to state       from         andis the probability of observa-tion       produced by      .
The para-meter   for the HMM is defined by the transi-tion probability distribution        , emission(observation) probability distributionand the initial probability            .Direct calculation of the likelihood          iscomputationally inefficient, and we can use dy-namic programming techniques to speed up thecalculation by calculating the forward probabili-ty:and backward probability.See (Mannings & Schutze, 1999) for details onthe calculation.2.1 Expectation Maximization (EM)EM is a general class of algorithms for findingthe maximum likelihood estimator ofparameters in probabilistic models.
It is aniterative algorithm where we alternate betweencalculating the expectation of the log likelihoodof the model given the parameters:and then finding the parameters that maximizesthe expected log likelihood.
Using Lagrangemultipliers with constraint that each parameteris a probability distribution, we have theseupdate steps for the well-known forward-backward Algorithm for EM HMM:where.2.2 Variational Bayes (VB)One of the drawbacks of EM is that the result-ing distribution is very uniform; that is, EM ap-plies roughly the same number of observationsfor each state.
Instead of using only the bestmodel for decoding, the Bayesian approach usesand considers all the models; that is, the modelis treated as a hidden variable.
This is done byassigning a probability distribution over themodel parameters as a prior distribution,      .In HMM, we calculate the probability of theobservation by considering all models and inte-grating over the distribution over the priors:(5)(1)(2)(3)(4)136where                  .As with the standard in the literature, we useDirichlet Prior as it allows us to model the tagdistribution more closely and because they arein the same conjugate exponential family as thelog likelihood.
The Dirichlet distribution is pa-rameterized by a vector of real values   (hyper-parameters).
There are two ways that we canview the vector  .
First, the parameter controlsthe sharpness of distribution for each of thecomponents.
This is in contrast to the EM mod-el where we essentially have a uniform prior.Thus, we can view   as our prior beliefs on theshape of the distribution and we can make ourchoices based on our linguistics knowledge.Second, we can view the role of   in terms ofpredictive distribution based on the statisticsfrom observed counts.
For HMM, we can set aseparate prior for each state-state transition andword-state emission distribution, effectivelygiving us control over the distribution of eachentry in the transition matrix.
However, to sim-plify the model and without the need to finetune each parameters, we use two fixed hyper-parameters: all of the state-state probability willhave the hyper-parameter      and all of theword-state probability will have hyper-parameter    .To begin our estimation and maximizationprocedure, we createas an approximation of the post-erior of the log likelihood:By taking the functional derivative with respectto      to find the distribution that maximizesthe log likelihood, and following the derivationfrom (Beal, 2003), we arrive at the followingEM-like procedure:This is the Expectation step where   and   isthe forward and backward probabilities andis the indicator function as in EM.The Maximization step is as follows:where                                     ,and    isthe digamma function.2.3 Gibbs Sampling (GS)Gibbs sampling (Geman & Geman, 1984) is awidely used MCMC algorithm designed espe-cially for cases where we can sample from theconditional probability easily.
It is astraightforward application of the MetropolisHasting algorithm where we sample a variablewhile keeping     constant where.
We set the proposaldistribution to.So the sampling procedure is the following:initialize the components of             .Then sample    from              ,    from, and so on for each compo-nent of  .
For POS tagging, the main idea isthat we sample the tag   based on theand        distribution.The main idea for using GS for POS taggingis that in  each iteration, we sample the tagbased on the         and        distribution.
(7)(6)(8)(9)(10)137Then from the samples, we count the numberfor each state-state and word-state pairs and up-date the probabilities accordingly.
How wesample the data depends on whether we are us-ing word based or sentence based sampling (theExpectation Step).
Whereas how we update theprobabilities depend on whether we are using acollapsed or explicit Gibbs sampler (the Max-imization Step).Word Based vs.
Sentence Based: Word-basedand sentence-based approaches to GS determinehow we sample the each tag   at position   inthe data set.
For the word-based approach, in-stead of going through sentence by sentence (asin EM and VB procedures), we pick a word po-sition in the corpus at random (without repeti-tion) and sample a new tag    at position   usingthe probability:Notice that since we are selecting each positionat random, the tag      at position n-1 andat position n+1 are our samples at the previousiteration or an already updated samples at thecurrent iteration.The sentence-based approach use the forwardand backward probability to sample the tagbased on the sentence (Besag, 2004).
Specifical-ly, we use the backward probabilityto sample the sentence fromstart (     to finish (    .
We sample anew tag    at position   using the probability:where the transition and emission probabilitydistribution are from the current model parame-ters.
Again      is our ?guess?
at the previoussampling step of the tag of     .Explicit vs.
Collapsed Based: We use the tagsestimated at the previous step to maximize theparameters.
Our choice of using Dirichlet dis-tributions over the parameters      andgive us some nice mathematical properties.
Weshow that           and           also calcu-late to be Dirichlet distributions.
Following(MacKay & Peto, 1994), the posterior probabili-ty of   can be derived as follows:where          is the number of times    is fol-lowed by    in the sample from the previousiteration.Similarly, we can define           using thecount          to show that:For the collapsed Gibbs sampler, we want tointegrate over all possible model parametersto maximize the new transition probabilitiesusing Maximum a posteriori (MAP) estimator:The last equality uses the following result:We can derive a similar result for.
Then we can use the samplecount to update the new parameter values.An explicit sampler samples the HMM para-meters   in addition to the states.
Specifically,in the Bayesian model, we will need to samplefrom the Dirichlet distribution for the parame-ters(11)(12)(13)(14)(15)(16)138derived above.
An  -dimensional Dirichlet dis-tribution variable can be generated from gammavariate (Wolfram Mathematica, 2009):we can update the transition probability by ge-nerating the gamma variate for the Dirichletdistribution:.Similarly, we sample the emission probabilityusing the count for word-tag withas the hyper-parameter.3 Experiment SetupOur experiment setup is similar to the ones usedin (Gao & Johnson, 2007).
They are summa-rized in Table 1:Parameters ValuesData Size 24k, 120k, 500kAlgorithm EM, VB, GS(c,w), GS(c,s),GS(e,s), GS(e,w)# of states Chinese: 33  English: 500.0001, 0.1, 0.5, 10.0001, 0.1, 0.5, 1Table 1: The list of experiments conducted.
Forthe hyper-parameters          , we try thecombination of the adjacent pairs ?
(0.0001,0.0001), (0.1,0.0001), (0.0001,0.1), (0.1,0.1), (0.1, 0.5), etc.3.1 DataFor our experiments, we use the data set Chi-nese Penn Treebank (CTB) v5.0.
The ChineseTreebank project began at the University ofPennsylvania in 1998 and the team created a setof annotation guidelines for word segmentation,POS tagging and bracketing (Xia, 2000; Xue etal., 2002; Xue et al, 2005).
The version used inthis paper is the Chinese Treebank 5.0 whichconsists of over 500k words and over 800k Chi-nese characters.
The text comes from varioussources including newswire, magazine articles,website news, transcripts from various broad-cast news program.Chinese POS tagging faces additional chal-lenges because it has very little, if any, inflec-tional morphology.
Words are not inflected withnumber, gender, case, or tense.
For example, aword such as ??
in Chinese corresponds todestroy /destroys /destroyed/destruction in Eng-lish.
This fuels the discussion in Chinese NLPcommunities on whether the POS tags should bebased on meaning or on syntactic distribution(Xia, 2000).
If only the meaning is used, ?
?should be a verb all the time.
If syntactic distri-bution is used, the word is a verb or a noun de-pending on the context.
For the CTB, syntacticdistribution is used, which complies with theprinciples of contemporary linguistics theories.Following the experiment done for English in(Gao & Johnson, 2008), we split the data intothree sizes: 24k words, 120k words and allwords (500k), and used the same data set fortraining and testing.
The idea is to track the ef-fectiveness of an algorithm across different cor-pus sizes.
Instead of using two different tag setsizes (17 and 50) as it is done for English POStagging, we opt to keep the original 33 tag setfor Chinese without further modification.
Inaddition to reporting the results for Englishfrom (Gao & Johnson, 2008), we run additionalexperiments on English using only 500k wordsfor comparison.3.2 DecodingFor decoding, we use max marginal likelihoodestimator (as opposed to using Viterbi algorithm)to assign a tag for each word in the result tag.
(Gao & Johnson, 2008) finds that max marginaldecoder performs as well as Viterbi algorithmand runs significantly faster as we can reuse theforward and backwards probabilities alreadycalculated during the estimation and update step.3.3    HyperparametersFor the Bayesian approaches (VB and GS), wehave a choice of hyperparameters.
We chooseuniform hyperparameters     and     instead(17)(18)(19)139of choosing a specific hyper-parameter for eachof the tag-tag and word-tag distribution.
Thevalues for the hyper-parameters are chosen suchthat we can see more clearly the interactionsbetween the two values.
For GS, we use thenotation GS(c,s) to denote collapsed sentence-based approach, GS(e,s) for explicit sentencebased, GS(c,w) for collapsed word-based andGS(e,w) for explicit word based.3.4   Evaluation MetricsWe use POS tagging accuracy as our primaryevaluation method.
There are two commonlyused methods to map the state sequences fromthe system output to POS tags.
In both methods,we first create a matrix where each row corres-ponds to a hidden state, each column corres-ponds to a POS tag, and each cellrepresents the number of times a word positionin the test data comes from the hidden stateaccording to the system output and the positionhas tag    according to the gold standard.
Ingreedy 1-to-1 mapping, we find the largest val-ue in the table ?
suppose the value is for the cell.
We map state i to tag j, and remove bothrow i and column j from the table.
We repeatthe process until all the rows have been re-moved.
Greedy many-to-1 allow multiple hid-den states to map to a single POS tag.
That is,when the highest value in the table is found,only the corresponding row is removed.
In otherwords, we simply map each hidden state to thePOS tag that the hidden state co-occurs with themost.4 Results and AnalysisWe compare and analyze the results betweenthe different algorithms and between Chineseand English using Greedy 1-to-1 accuracy,Greedy many-to-1 accuracy.4.1 Greedy 1-to-1 accuracyWhen measure using 1-to-1 mapping, the bestalgorithm ?
Collapsed word based Gibbs Sam-pling GS(c,w) - achieve 0.358 in Chinese on thefull data set but remains close to 0.499 in Eng-lish for the full dataset.
GS(c,w) outperformsother algorithm in almost all categories.
ButEM posts the highest  relative improvementwith an increase of 70% when the data size in-creases  from  24k to 500k words.
The full re-sult is listed in Table 2.Greedy 1-to-124k 120k 500kChineseEM 0.1483 0.1838 0.2406VB 0.1925 0.2498 0.3105GS(e,w) 0.2167 0.3108 0.3475GS(e,s) 0.2262 0.2596 0.3572GS(c,s) 0.2351 0.2931 0.3577GS(c,w) 0.2932 0.3289 0.3558EngEM 0.1862 0.2930 0.3837VB 0.2382 0.3468 0.4327GS(c,w) 0.3918 0.4276 0.4348Table 2: Tagging accuracy for Chinese andEnglish with greedy 1-to-1 mapping.
The Eng-lish 24k and 120k results are taken from (Gao &Johnson 2008) with the 50-tag set.Figure 2: Tag distribution for 1-to-1 greedymapping in Chinese 500k.
Only the top 18 tagsare shown.
The figure compares the tag distri-bution between the gold standard for Chinese(33 tags) and the algorithm?s results.
The goldtags are shown as lines, and each algorithm?sresult is shown as bar graphs.As expected, the increase in data size improvesthe accuracy as EM algorithm optimizes thelikelihood better with more data.
We ran addi-tional experiments on English using a reduced500k dataset to match the dataset used for Chi-nese; EM in this setting achieve an accuracy of0.384 on average for 50 tags (down from0.405).
So even in the reduced data size setting,EM on English performs better than Chinesealthough the difference is reduced.
We analyzethe tag distribution of the 1-to-1 mapping.
(Johnson, 2007) finds that EM generally assignsroughly as equal number of words for eachstate.
In Figure 2, we find the same phenome-non for Chinese.050000100000150000EMVBGS(c,w)Gold140One of the advantages of Bayesian approaches(VB and GS) is that we can assign a prior toattempt to encourage a sparse model distribu-tion.
Despite using small values 0.0001 ashyperparameters, we find that the resulting dis-tribution for number of words mapping to a par-ticular state is very different  from the goldstandard.4.2 Greedy many-to-1 accuracyCollapsed Word Based Gibbs Sampler GS(c,w)is the clear winner for both English and Chineseunsupervised POS tagging.
Table 3 shows theresult of Greedy many-to-1 mapping for Chi-nese in different data size as well as Englishwith the full data set.
In Greedy many-to-1mapping, GS(c,w) in both Chinese and Englishachieve 60%+ accuracy.
In addition, the size ofthe dataset does not affect GS(c,w) as much asthe other algorithms.
In fact, the change from24k to 500k dataset only increases the relativeaccuracy by less than 6%.Greedy many-to-124k 120k 500kChineseEM 0.4049 0.4564 0.4791VB 0.4411 0.5023 0.5390GS(e,w) 0.4758 0.4969 0.5499GS(e,s) 0.4904 0.5369 0.5658GS(c,s) 0.5070 0.5701 0.5757GS(c,w) 0.5874 0.6180 0.6213EngEM 0.2828 0.44135 0.5872VB 0.3595 0.48427 0.6025GS(c,w) 0.5815 0.6529 0.6644Table  3: Many-to-1 accuracy for Chinese andEnglish.
The English 24k and 120k results aretaken from (Gao & Johnson 2008) with the 50-tag set.However, despite the relatively high  accuracy,when analyzing the result, we notice that thereare overwhelmingly many states which maps toa single POS tag (NN).
Figure 3 shows thenumber of states mapping to different POS tagsin Chinese over the 500k data size.
There are alarge number of states mapping to relatively fewPOS tags.
In the most extreme example, for thePOS tag NN, GS(e,s) assigns 18 (the most) hid-den states, accounts for 44% of the word tokensmapping to NN whereas GS(e,w) assigns 13states, which is actually the least among all thealgorithms and accounts for 31% of the wordtokens mapping to NN.
Notice that we haveonly a total of 33 hidden states in our model.This means that over half the states are mappedto NN, which is a rather disappointing result.The actual empirical result for the gold standardin CTB is that only 27% of the word should bemapped to NN.
For EM in particular, we see 17states accounting for 42% of the words taggedas NN.Figure 3: The distribution of POS tags based onthe output EM algorithm in Chinese using the500k dataset.
Tag T-N-y% means that there areN hidden states mapped to the specific POS tagT accounting for y% of word tokens tagged withthese N states by the EM algorithm.Figure 4: English tag distribution for EM using500k dataset with 50 states mapping to the 17pos tag set.
Tag T-N-y% means that there are Nhidden states mapped to the specific POS tag Taccounting for y% of word tokens tagged withthese N states.We also ran additional experiments on the algo-rithms for English using a reduced data size of500k to match that of our Chinese experiment tosee whether we see the same phenomena.
Wenotice that the tag distribution for English EM ismore consistent to the empirical distributionfound in the gold standard.AD-3state(s)9%NN-17state(s)42%NR-2state(s)8%PU-3state(s)14%VV-3state(s)10%N-16state(s)37%DET-7state(s)15%V-7state(s)12%PREP-4state(s)10%ADJ-7state(s)6%141With the English 50 tag set with 500k words,we experiment with mapping the English 50 tagset result to the 17 tag set, we see that in Figure4, 16 (of 50) states mapped to the N tag, ac-counting for 37% of the words in the dataset.This is close to the actual empirical distributionfor English for 17 tags where N accounts forabout 32%.4.3 ConvergenceWe analyze how each algorithm converges to itslocal maxima.
Figure 5 shows the change ingreedy 1-to-1 accuracy over the 50% of the run.Figure 5: Greedy 1-to-1 accuracy of EM, VBand GS(c,w) over the first 50% of the algo-rithms' iterations for the Chinese 500k dataset.Note: the percentage of iterations is used herebecause each algorithms converge at a differentnumber of iterations, thus the progress is scaledaccordingly.The greedy 1-to-1 accuracy actually fluctuatesthrough the run.
VB has an interesting dip ataround 80% of its iteration before climbing toits max (not showing in the graph).
All theGibbs sampling variations follow a relativelysteady hill climb before converging (onlyGS(c,w) is shown in Figure 5).
EM is particu-larly interesting; Looking at the initial 15% ofthe algorithm?s run, we can see that EM climbsto a ?local?
max very quickly before droppingand then slowly improving in its accuracy.
Thegreedy 1-to-1 accuracy in the initial top is ac-tually higher than the final convergence value inmost runs.
This initial peak in value followingby a drop and then a slow hill climb in EM forChinese POS tagging is consistent with the find-ing in (Johnson, 2007) for English POS tagging.5 Conclusion and Future WorkWe have only scratched the surface of the re-search in unsupervised techniques in ChineseNLP.
We have established a baseline of EM,VB and GS against the CTB 5.0.
The experi-ment shows that for both Chinese and English,GS(c,w) produces the best result.
We have alsofound that Chinese performs rather poorly in the1-to-1 accuracy  when comparing against Eng-lish in the same data size.
We find that inmany-to-1 mapping, we have a disproportionatelarge number of states mapping to individualPOS tags comparing to the gold distribution andalso in comparison to English against its golddistribution.Gra?a et al (2009) addresses the problem weobserve in our resulting tag distributions in ourmodel where EM, VB and GS fails to capturethe shape of the true distribution.
They proposea Posterior Regularization framework where itposes linear constraints on the posterior expec-tation.
They define a set distributions Q overhidden states with a constraint on the expecta-tion over the features.
The log likelihood is pe-nalized using the KL-divergence between the Qdistribution and the model.
The distributionsthat their model predicted are far more similarto the gold standard than traditional EM.Liang and Klein (2009) propose some inter-esting error analysis techniques for unsuper-vised POS tagging.
One of their analyses onEM is done by observing the approximationerrors being created during each iteration of thealgorithm?s execution.
We can also performthese analyses on VB and GS and observe thechanges of output tags by starting from the GoldStandard distribution in EM and VB, and goldstandard tags in GS.
We can then follow howand which set of tags start to deviate from thegold standard.
This will allow us to see whichcategories of errors (ex.
noun-verb, adj-adv er-rors) occur most in these algorithms and howthe error progresses.Acknowledgment: This work is partly sup-ported by the National Science FoundationGrant BCS-0748919.
We would also like tothank three anonymous reviewers for their valu-able comments.15%20%25%30%35%0% 16% 32% 48%% of Total IterationsEMVBGS(c,w)142ReferencesBanko, M., & Moore, R. C. 2004.
Part of SpeechTagging in Context.
In Proc.
of the 20th Interna-tional Conference on Computational Linguistics(COLING), pp 556-561.Beal, M. 2003.
Variational Algorithms for Approx-imate Bayesian Inference.
Ph.D. thesis, Gatsby-Computational Neuroscience unit, University Col-lege London.Besag, J.
2004.
An introduction to Markov ChainMonte Carlo methods.
Mathematical Foundationsof Speech and Language Processing, pages 247?270.
Springer, New York.Chang, C.-H., & Chen, C.-D. 1993.
HMM-BasedPart-Of-Speech Tagging For Chinese Corpora.Workshop On Very Large Corpora: AcademicAnd Industrial Perspectives.Elworthy, D. 1994.
Does Baum-Welch Re-estimation Help Taggers?
In Proc.
of AppliedNatural Language Processing Conference(ANLP),  pp 53-58.Gao, J., & Johnson, M.  2008.
A comparison ofBayesian estimators for unsupervised HiddenMarkov Model POS taggers.
In Proceedings ofthe 2008 Conference on Empirical Methods inNatural Language Processing (EMNLP), pp 344-352.Geman, S., & Geman, D. 1984.
Stochastic Relaxa-tion, Gibbs Distributions, and the Bayesian Resto-ration of Images.
IEEE Transactions on PatternAnalysis and Machine Intelligence, pp 721?741.Goldwater, S., & Griffiths, T. 2007.
A Fully Baye-sian Approach to Unsupervised Part-of-SpeechTagging.
Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics(ACL) , pp 744-751.Graca, M.J., Ganchev, K., Taskar B.
& Pereira, F.2009.
Posterior vs. Parameter Sparsity in LatentVariable.
Advances in Neural InformationProcessing Systems 22 (NIPS).
MIT Press.Haghighi, A., & Klein, D. 2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference(HLT- NAACL) , pp 320-327.Huang, Z., Eidelman, V., Harper, M.  2009.
Improv-ing A Simple Bigram HMM Part-of-Speech Tag-ger by Latent Annotation and Self-Training.
InProc.
of  Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (NAACL), Companion Volume: ShortPapers.Johnson, M. 2007.
Why Doesn?t EM Find GoodHMM POS-Taggers?
In Proceedings of the 2007Joint Conference on Empirical Methods in Natu-ral Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pp296-305.Liang, P., & Klein, D. 2008.
Analyzing the errors ofunsupervised learning.
The Forty Sixth AnnualMeeting of the Association for ComputationalLinguistics (ACL), pp 879?887.
Columbus, OH.MacKay, D. J., & Peto, L. C. 1994.
A HierarchicalDirichlet Language Model.
Natural LanguageEngineering, 1-19.Manning, C. &  Schutze, H. 1999.
Foundations ofStatistical Natural Language Processing.
TheMIT Press, Cambridge, Massachusetts.Merialdo, B.
1994.
Tagging English text with aprobabilistic model.
Computational Linguistics,20(2).Ng, H. T., & Low, J. K. 2004.
Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once?
Word-Based Or Character-Based?
In Proc.of  EMNLP.Thorsten Brants, 2000.
TnT - A Statistical Part-of-Speech Tagger.
In Proceedings of the Sixth Ap-plied Natural Language Processing Conference(ANLP), Seattle, WA.Toutanova, K., & Johnson, M. 2007.
A BayesianLDA-based model for semi-supervised.
In Pro-ceedings of NIPS 21 .Wolfram Mathematica.
(2009, 10 3).
Random Num-ber Generation.
http://reference.wolfram.com/mathematica/tutorial/RandomNumberGeneration.html .Xia, F. 2000.
The Part-of-Speech Guidelines for thePenn Chinese Treebank (3.0).
University  ofPennsylvania: IRCS Report 00-07.Xue, N., Chiou, F.-D., & Palmer, M. 2002.
Buildinga Large-Scale Annotated Chinese Corpus.
Pro-ceedings of the 19th.
International Conference onComputational Linguistics (COLING).
Taipei,Taiwan.Xue, N., Xia, F., Chiou, F.-D., & Palmer, M. 2005.The Penn Chinese TreeBank: Phrase StructureAnnotation of a Large Corpus.
Natural LanguageEngineering, 11(2), pp 207-238.143
