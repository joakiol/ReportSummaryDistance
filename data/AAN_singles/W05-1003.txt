Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 18?27,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsIdentifying Concept Attributes Using a ClassifierMassimo PoesioUniversity of EssexComputer Science /Language and Computationpoesio at essex.ac.ukAbdulrahman AlmuharebUniversity of EssexComputer Science /Language and Computationaalmuh at essex.ac.ukAbstractWe developed a novel classification ofconcept attributes and two supervisedclassifiers using this classification to iden-tify concept attributes from candidate at-tributes extracted from the Web.
Ourbinary (attribute / non-attribute) classifierachieves an accuracy of 81.82% whereasour 5-way classifier achieves 80.35%.1 IntroductionThe assumption that concept attributes and, morein general, features1 are an important aspect ofconceptual representation is widespread in all dis-ciplines involved with conceptual representations,from Artificial Intelligence / Knowledge Represen-tation (starting with at least (Woods, 1975) anddown to (Baader et al 2003)), Linguistics (e.g., inthe theories of the lexicon based on typed featurestructures and/or Pustejovsky?s Generative Lexi-con theory: (Pustejovsky 1995)) and Psychology(Murphy 2002, Vinson et al2003).
This being thecase, it is surprising how little attention has beendevoted to this aspect of lexical representation inwork on large-scale lexical semantics in Computa-tional Linguistics.
The most extensive resource at1 The term attribute is used informally here to indicate thetype of relational information about concepts that is expressedusing so-called roles in Description Logics (Baader et al2003)?i.e., excluding IS-A style information (that cars arevehicles, for instance).
It is meant to be a more restrictiveterm than the term feature, often used to indicate any propertyof concepts, particularly in Psychology.
We are carrying out asystematic analysis of the sets of features used in work such as(Vinson et al 2003) (see Discussion).our disposal, WordNet (Fellbaum, 1998) containsvery little information that would be considered asbeing about ?attributes?
?only information aboutparts, not about qualities such as height, or even tothe values of such attributes in the adjective net-work?and this information is still very sparse.
Onthe other hand, the only work on the extraction oflexical semantic relations we are aware of has con-centrated on the type of relations found in Word-Net: hyponymy (Hearst, 1998; Caraballo, 1999)and meronymy (Berland and Charniak, 1999; Poe-sio et al 2002).2The work discussed here could be perhaps bestdescribed as an example of empirical ontology:using linguistics and philosophical ideas to im-prove the results of empirical work on lexical / on-tology acquisition, and vice versa, using findingsfrom empirical analysis to question some of theassumptions of theoretical work on ontology andthe lexicon.
Specifically, we discuss work on theacquisition of (nominal) concept attributes whosegoal is twofold: on the one hand, to clarify the no-tion of ?attribute?
and its role in lexical semantics,if any; on the other, to develop methods to acquiresuch information automatically (e.g., to supple-ment WordNet).The structure of the paper is as follows.
After ashort review of relevant literature on extractingsemantic relations and on attributes in the lexicon,we discuss our classification of attributes, followedby the features we used to classify them.
We thendiscuss our training methods and the results weachieved.2 In work on the acquisition of lexical information about verbsthere has been some work on the acquisition of thematic roles,(e.g., Merlo and Stevenson, 2001).182 Background2.1 Using Patterns to Extract Semantic Rela-tionsThe work discussed here belongs to a line of re-search attempting to acquire information aboutlexical and other semantic relations other thansimilarity / synonymy by identifying syntacticconstructions that are often (but not always!)
usedto express such relations.
The earliest work of thistype we are aware of is the work by Hearst (1998)on acquiring information about hyponymy (= IS-Alinks) by searching for instances of patterns such asNP {, NP}* or other NP(as in, e.g., bruises ?.
broken bones and otherINJURIES).
A similar approach was used by Ber-land and Charniak (1999) and Poesio et al(2002)to extract information about part-of relations usingpatterns such asthe N of the N is ?.
(as in the wheel of the CAR is) and by Girju andMoldovan (2002) and Sanchez-Graillet and Poesio(2004) to extract causal relations.
In previous work(Almuhareb and Poesio, 2004) we used this sameapproach to extract attributes, using the pattern?the * of the C [is|was]?
(suggested by, e.g., (Woods, 1975) as a test for?attributehood?)
to search for attributes of conceptC in the Web, using the Google API.
Although theinformation extracted this way proved a useful ad-dition to our lexical representations from a cluster-ing perspective, from the point of view of lexiconbuilding this approach results in too many falsepositives, as very few syntactic constructions areused to express exclusively one type of semanticrelation.
For example, the ?attributes?
of deer ex-tracted using the text pattern above include ?themajority of the deer,?
?the lake of the deer,?
and?the picture of the deer.?
Girju and Moldovan(2002) addressed the problem of false positives forcausal relations by developing WordNet-based fil-ters to remove unlikely candidates.
In this work,we developed a semantic filter for attributes basedon a linguistic theory of attributes which does notrely on WordNet except as a source of morpho-logical information (see below).2.2 Two Theories of AttributesThe earliest attempt to classify attributes and otherproperties of substances we are aware of goes backto Aristotle, e.g., in Categories,3 but our classifica-tion of attributes was inspired primarily by thework of Pustejovsky (1995) and Guarino (e.g.,(1992)).
According to Pustejovsky?s GenerativeLexicon theory (1995), an integral part of a lexicalentry is its Qualia Structure, which consists offour ?roles?
:4 the Formal Role, specifying whattype of object it is: e.g., in the case of a book, thatit has a shape, a color, etc.
; the Constitutive Role,specifying the stuff and parts that it consists of(e.g., in the case of a book,  that it is made of pa-per, it has chapters and an index, etc.
); the TelicRole, specifying the purpose of the object (e.g., inthe case of a book, reading);  and the AgentiveRole, specifying how the object was created (e.g.,in the case of a book, by writing).Guarino (1992) argues that there are two typesof attributes: relational and non-relational.
Rela-tional attributes include qualities such as color andposition, and relational social roles such as sonand spouse.
Non-relational attributes include partssuch as wheel and engine.
Activities are notviewed as attributes in Guarino?s classification.3 Attribute Extraction and ClassificationThe goal of this work is to identify genuine attrib-utes by classifying candidate attributes collectedusing text patterns as discussed in (Almuhareb andPoesio, 2004) according to a scheme inspired bythose proposed by Guarino and Pustejovsky.The scheme we used to classify the trainingdata in the experiment discussed below consists ofsix categories:?
Qualities: Analogous to Guarino?s qualitiesand Pustejovsky?s formal ?role?.
(E.g., ?thecolor of the car?.)?
Parts: Related to Guarino?s non-relationalattributes and Pustejovsky?s constitutive?roles?.
(E.g., ?the hood of the car?).?
Related-Objects: A new category intro-duced to cover the numerous physical ob-jects which are ?related?
to an object but arenot part of it?e.g., ?the track of the deer?.3 E.g., http://plato.stanford.edu/entries/substance.
Thanks toone of the referees for drawing our attention to this.4 ?Facets?
would be perhaps a more appropriate term to avoidconfusions with the use of the term ?role?
in Knowledge Rep-resentation.19?
Activities: These include both the types ofactivities which are part of Pustejovsky?stelic ?role?
and those which would be in-cluded in his agentive ?role?.
(E.g., ?the re-pairing of the car?.)?
Related-Agents: For the activities in whichthe concept in question is acted upon, theagent of the activity: e.g., ?the writer of thebook?, ?the driver of the car?.?
Non-Attributes: This category covers thecases in which the construction ?the N of theN?
expresses other semantic relations, as in:?the last of the deer?, ?the majority of thedeer,?
?the lake of the deer,?
and ?in thecase of the deer?.We will quickly add that (i) we do not view thisclassification as definitive?in fact, we alreadycollapsed the classes ?part?
and ?related objects?
inthe  experiments discussed below?and (ii) not allof these distinctions are very easy even for humanjudges to do.
For example, design, as an attributeof a car, can be judged to be a quality if we thinkof it as taking values such as modern and standard;on the other hand, design might also be viewed asan activity in other contexts discussing the design-ing process.
Another type of difficulty is that agiven attribute may express different things fordifferent objects.
For example, introduction is apart of a book, and an activity for a product.
Anadditional difficulty results from the strong similar-ity between parts and related-objects.
For example,?key?
is a related-object to a car but it is not partof it.
We will return to this issue and to agreementon this classification scheme when discussing theexperiment.One difference from previous work is that weuse additional linguistic constructions to extractcandidate attributes.
The construction ?the X of theY is?
used in our previous work is only one exam-ple of genitive construction.
Quirk et al(1985) listeight types of genitives in English, four of whichare useful for our purposes:?
Possessive Genitive: used to express quali-ties, parts, related-objects, and related-agents.?
Genitive of Measure: used to express quali-ties.?
Subjective & Objective Genitives: used toexpress activities.We used all of these constructions in the workdiscussed here.4 Information Used to Classify AttributesOur attribute classifier uses four types of informa-tion: morphological information, an attributemodel, a question model, and an attributive-usagemodel.
In this section we discuss how this informa-tion is automatically computed.4.1 Morphological InformationOur use of morphological information is based onthe noun classification scheme proposed by Dixon(1991).
According to Dixon, derivational morphol-ogy provides some information about attribute typ-e. Parts are concrete objects and almost all of themare expressed using basic noun roots (i.e., not de-rived from adjectives or verbs).
Most of qualitiesand properties are either basic noun roots or de-rived from adjectives.
Finally, activities are mostlynouns derived from verbs.
Although these rulesonly have a heuristic value, we found that morpho-logically based heuristics did provide useful cueswhen used in combination with the other types ofinformation discussed below.As we are not aware of any publicly availablesoftware performing automatic derivational mor-phology, we developed our own (and very basic)heuristic methods.
The techniques we used involveusing information from WordNet, suffix-checking,and a POS tagger.WordNet was used to find nouns that are de-rived from verbs and to filter out words that are notin the noun database.
Nouns in WordNet are linkedto their derivationally related verbs, but there is noindication about which is derived from which.
Weuse a heuristic based on length to decide this: thesystem checks if the noun contains more lettersthan the most similar related verb.
If this is thecase, then the noun is judged to be derived fromthe verb.
If the same word is used both as a nounand as a verb, then we check the usage familiarityof the word, which can also be found in WordNet.If the word is used more as a verb and the verbalusage is not rare, then again the system treats thenoun as derived from the verb.20To find nouns that are derived from adjectiveswe used simple heuristics based on suffix-checking.
(This was also done by Berland andCharniak (1999).)
All words that end with ?ity?
or?ness?
are considered to be derived from adjec-tives.
A noun not found to be derived from a verbor an adjective is assumed to be a basic noun root.In addition to derivational morphology, we usedthe Brill tagger (Brill, 1995) to filter out adjectivesand other types of words that can occasionally beused as nouns such as better, first, and whole be-fore training.
Only nouns, base form verbs, andgerund form verbs were kept in the candidate at-tribute list.4.2 Clustering AttributesAttributes are themselves concepts, at least in thesense that they have their own attributes: for ex-ample, a part of a car, such as a wheel, has its ownparts (the tyre) its qualities (weight, diameter) etc.This observation suggests that it should be possibleto find similar attributes in an unsupervised fashionby looking at their attributes, just as we did earlierfor concepts (Almuhareb and Poesio, 2004).
Inorder to do this, we used our text patterns for find-ing attributes to collect from the Web up to 500pattern instances for each of the candidate attrib-utes.
The collected data were used to build a vecto-rial representation of attributes as done in(Almuhareb and Poesio, 2004).
We then usedCLUTO (Karypis, 2002) to cluster attributes usingthese vectorial representations.
In a first round ofexperiments we found that the classes ?parts?
and?related objects?
were difficult to differentiate, andtherefore we merged them.
The final model clus-ters candidate attributes into five classes: activities,parts & related-objects, qualities, related-agents,and non-attributes.
This classification was used asone of the input features in our supervised classi-fier for attributes.We also developed a measure to identify par-ticularly distinctive ?attributes of attributes?
?attributes which have a strong tendency to occurprimarily with attributes (or any concept) of agiven class?which has proven to work pretty well.This measure, which we call Uniqueness, actuallyis the product of two factors: the degree of unique-ness proper, i.e., the probability P(classi | attrib-utej) that  an attribute (or, in fact, any other noun)will belong to class i given than it has attribute j;and a measure of ?definitional power?
?the prob-ability P(attribute j | classi) that a concept belong-ing to a given class will have a certain attribute.Using MLE to estimate these probabilities, the de-gree of uniqueness of attributesj of classi is com-puted as follows:)(),( 2,jijiji attributeCnattributeclassCUniqueness ?=where ni is the number of concepts in classi.
C is acount function that counts concepts that are associ-ated with the given attribute.
Uniqueness rangesfrom 0 to 1.Table 1 shows the 10 most distinctive attributesfor each of the five attribute classes, as determinedby the Uniqueness measure just introduced, for the1,155 candidate attributes in the training data forthe experiment discussed below.Class Top 10 Distinctive AttributesRelated-Agent(0.39)identity, hands, duty, consent,responsibility, part, attention,voice, death, jobPart &Related-Object(0.40)inside, shape, top, outside, sur-face, bottom, center, front, size,interiorActivity(0.29)time, result, process, results,timing, date, effect, beginning,cause, purposeQuality(0.23)measure, basis, determination,question, extent, issue, meas-urement, light, result, increaseNon-Attribute(0.18)content, value, rest, nature,meaning, format, interpretation,essence, size, sourceTable 1: Top 10 distinctive attributes of the fiveclasses of candidate attributes.
Average distinct-iveness (uniqueness) for the top 10 attributes isshown between parenthesesMost of the top 10 attributes of related-agents,parts & related-objects, and activities are genuinelydistinctive attributes for such classes.
Thus, attrib-utes of related-agents reflect the ?intentionality?aspect typical of members of this class: identity,duty, and responsibility.
Attributes of parts arecommon attributes of physical objects (e.g., inside,shape).
Most attributes of activities have to do withtemporal properties and causal structure: e.g., be-ginning, cause.
The ?distinctive?
attributes of the21quality class are less distinctive, but four such at-tributes (measure, extent, measurement, and in-crease) are related to values since many of thequalities can have different values (e.g., small andlarge for the quality size).
There are however sev-eral attributes in common between these classes ofattributes, emphasizing yet again how some ofthese distinctions at least are not completely clearcut:  e.g., result, in common between activities andqualities (two classes which are sometimes diffi-cult to distinguish).
Finally, as one would expect,the attributes of the non-attribute class are notreally distinctive: their average uniqueness score isthe lowest.
This is because ?non-attribute?
is a het-erogeneous class.4.3 The Question ModelCertain types of attributes can only be used whenasking certain types of questions.
For example, it ispossible to ask ?What is the color of the car??
butnot ?
?When is the color of the car?
?.We created a text pattern for each type of ques-tion and used these patterns to search the Web andcollect counts of occurrences of particular ques-tions.
An example of such patterns would be:?
?what is|are the A  of the?where A is the candidate attribute under investiga-tion.
Patterns for who, when, where, and how aresimilar.After collecting occurrence frequencies for allthe candidate attributes, we transform these countsinto weights using the t-test weighting function asdone for all of our counts, using the following for-mula from Manning and Schuetze (1999):22, ),(N)()(),(NattributequestionCattributeCquestionCNattributequestionCtjijijiji??
?where N is the total number of relations, and C is acount function.Table 2 shows the 10 most frequent attributesfor each question type.
This data was collected us-ing a more restricted form of the question patternsand a varying number of instances for each type ofquestions.
The restricted form includes a questionmark at the end of the phrase and was used to im-prove the precision.
For example, the what-patternwould be ?what is the * of the *?
?.Question Top 10 Attributeswhat purpose, name, nature, role, cost, func-tion, significance, size, source, statuswho author, owner, head, leader, president, sponsor, god, lord, father, kingwhere rest, location, house, fury, word, edge, center, end, ark, voicehowquality, rest, pace, level, length, mo-rale, performance, content, organiza-tion, cleanlinesswhen end, day, time, beginning, date, onset, running, birthday, fast, openingTable 2: Frequent attributes for each question typeInstances of the what-pattern are frequent in theWeb: the Google count was more than 2,000,000for a query issued in mid 2004.
The who-pattern isnext in terms of occurrence, with about 350,000instances.
The when-pattern is the most infrequentpattern, about 5,300 instances.The counts broadly reflected our intuitionsabout the use of such questions.
What-questionsare mainly used with qualities, whereas who-questions are used with related-agents.
Attributesoccurring with when-questions have some tempo-ral aspects; attributes occurring with how-questionsare mostly qualities and activities, and attributes inwhere-questions are of different types but some arerelated to locations.
Parts usually do not occur withthese types of questions.4.4 Attributive UseFinally, we exploited the fact that certain types ofattributes are used more in language as conceptsrather than as attributes.
For instance, it is morecommon to encounter the phrase ?the size of the??
than ?the ?
of the size?.
On the other hand, it ismore common to encounter the phrase ?the * ofthe window?
than ?the window of the *?.
Gener-ally speaking, parts, related-objects, and related-agents are more likely to have more attributes thanqualities and activities.
We used the two patterns?the * of the A?
and ?the A of the *?
to collectGoogle counts for all of the candidate attributes.These counts were also weighted using the t-test asin the question model.Table 3 illustrates the attributive and conceptualusage for each attribute class using a training dataof 1,155 attributes.
The usage averages confirm theinitial assumption.22Average T-Test Score Attribute Class Conceptual AttributiveParts &Related-Objects 18.81 3.00Non-Attributes 13.29 11.07Related-Agents 12.15 2.54Activities 3.22 5.08Qualities 0.23 17.09Table 3: Conceptual and attributive usage averagesfor each attribute class5 The ExperimentWe trained two classifiers: a 2-way classifier thatsimply classifies candidate attributes into attributesand non-attributes, and a 5-way classifier that clas-sifies candidate attributes into activities, parts &related-objects, qualities, related-agents, and non-attributes.
These classifiers were trained using de-cision trees algorithm (J48) from WEKA (Wittenand Frank, 1999).FeatureelectionabdomenaciditycreatorproblemCluster Id 1 2 4 0 3What 0.00 0.00 0.00 0.00 3.80When 2.62 0.00 0.00 0.00 0.00Where 0.78 0.94 0.00 0.00 0.00Who 0.00 0.00 0.00 30.28 0.00How 2.05 0.00 1.54 0.00 2.61Conceptual 38.16 20.15 0.00 0.00 135.40Attributive 0.00 0.00 10.22 1.60 0.00Morph DV BN DA DV BNAttributeClass(Output)Activity Part Quality RelatedAgentNon-AttributeTable 4: Five examples of training instances.
Thevalues for morph are as follows: DV: derived fromverb; BN: basic noun; DA: derived from adjectiveOur training and testing material was acquiredas follows.
We started from the 24,178 candidateattributes collected for the concepts in the balancedconcept dataset we recently developed (Almuhareband Poesio, 2005).
We threw out every candidateattribute with a Google frequency less than 20; thisreduced the number of candidate attributes to4,728.
We then removed words other than nounsand gerunds as discussed above, obtaining 4,296candidate attributes.The four types of input features for this filteredset of candidate attributes were computed as dis-cussed in the previous section.
The best resultswere obtained using all of these features.
A train-ing set of 1,155 candidate attributes was selectedand hand-classified (see below for agreement fig-ures).
We tried to include enough samples for eachattribute class in the training set.
Table 4 shows theinput features for five different training examples,one for each attribute class.6  EvaluationFor a qualitative idea of the behavior of our classi-fier, the best attributes for some concepts are listedin Appendix A.
We concentrate here on quantita-tive analyses.6.1 Classifier Evaluation 1: Cross-ValidationOur two classifiers were evaluated, first of all, us-ing 10-fold cross-validation.
The 2-way classifiercorrectly classified 81.82% of the candidate attrib-utes (the baseline accuracy is 80.61%).
The 5-wayclassifier correctly classified 80.35% of the attrib-utes (the baseline accuracy is 23.55%).
The preci-sion / recall results are shown in Table 5.Attribute Class P R F2-Way ClassifierAttribute 0.854 0.934 0.892Non-Attribute 0.551 0.335 0.4175-Way ClassifierRelated-Agent 0.930 0.970 0.950Part & Related-Object 0.842 0.882 0.862Activity 0.822 0.878 0.849Quality 0.799 0.821 0.810Non-Attribute 0.602 0.487 0.538Table 5: Cross-validation results for the twoattribute classifiersAs it can be seen from Table 5, both classifiersachieve good F values for all classes except for thenon-attribute class: F-measures range from 81% to95%.
With the 2-way classifier, the valid attributeclass has an F-measure of 89.2%.
With the 5-wayclassifier, related-agent is the most accurate class(F = 95%) followed by part & related-object,activity, and quality (86.2%, 84.9%, and 81.0%,23respectively).
With non-attribute, however, wefind an F of 41.7% in the 2-way classification, and53.8% in the 5-way classification.
This suggeststhat the best strategy for lexicon building would beto use these classifiers to ?find?
attributes ratherthan ?filter?
non-attributes.6.2 Classifier Evaluation 2: Human JudgesNext, we evaluated the accuracy of the attributeclassifiers against two human judges (the authors).We randomly selected a concept from each of the21 classes in the balanced dataset.
Next, we usedthe classifiers to classify the 20 best candidate at-tributes of each concept, as determined by their t-test scores.
Then, the judges decided if the as-signed classes are correct or not.
For the 5-wayclassifier, the judges also assigned the correct classif the automatic assigned class is incorrect.After a preliminary examination we decided notto consider two troublesome concepts: constructorand future.
The reason for eliminating constructoris that we discovered it is ambiguous: in additionto the sense of ?a person who builds things?, wediscovered that constructor is used widely in theWeb as a name for a fundamental method in objectoriented programming languages such as Java.Most of the best candidate attributes (e.g., call,arguments, code, and version) related to the lattersense, that doesn?t exist in WordNet.
Our system iscurrently not able to do word sense discrimination,but we are currently working on this issue.
Thereason for ignoring the concept future was that thisword is most commonly used as a modifier inphrases such as: ?the car of the future?, and ?theoffice of the future?, and that all of the best candi-date attributes occurred in this type of construction.This reduced the number of evaluated concepts to19.According to the judges, the 2-way classifierwas on average able to correctly assign attributeclasses for 82.57% of the candidate attributes.
Thisis very close to its performance in evaluation 1.The results using the F-measure reveal similar re-sults too.
Table 6 shows the results of the two clas-sifiers based on the precision and recall measures.According to the judges, the 5-way classifiercorrectly classified 68.72% on average.
This per-formance is good but not as good as its perform-ance in evaluation 1 (80.35%).
The decrease in theperformance was also shown in the F-measure.The F-measure ranges from 0.712 to 0.839 exclud-ing the non-attribute class.Attribute Class P R F2-Way ClassifierAttribute 0.928 0.872 0.899Non-Attribute 0.311 0.459 0.3695-Way ClassifierRelated-Agent 0.813 0.868 0.839Part & Related-Object 0.814 0.753 0.781Activity 0.870 0.602 0.712Quality 0.821 0.658 0.730Non-Attribute 0.308 0.632 0.414Table 6: Evaluation against human judges resultsfor the two classifiersAn important question when using humanjudges is the degree of agreement among them.The K-statistic was used to measure this agree-ment.
The values of K are shown in Table 7.
In the2-way classification, the judges agreed on 89.84%of the cases.
On the other hand, the K-statistic forthis classification task is 0.452.
This indicates thatpart of this strong agreement is because that themajority of the candidate attributes are valid attrib-utes.
It also shows the difficulty of identifying non-attributes even for human judges.
In the 5-wayclassification, the two judges have a high level ofagreement; Kappa statistic is 0.749.
The judgesand the 5-way classifier agreed on 63.71% of thecases.Description 2-Way 5-WayHuman Judges 89.84% 80.69%Human Judges (Kappa) 0.452 0.749Human Judges & Classifier 78.36% 63.71%Table 7: Level of agreement between the humanjudges and the classifiers6.3 Re-Clustering the Balanced DatasetFinally, we looked at whether using the classifiersresults in a better lexical description for the pur-poses of clustering (Almuhareb and Poesio, 2004).In Table 8 we show the results obtained using theoutput of the 2-way classifier to re-cluster the 402concepts of our balanced dataset, comparing theseresults with those obtained using all attributes (firstcolumn) and all attributes that remain after fre-quency cutoff and POS filtering (column 2).
Theresults are based on the CLUTO evaluation meas-24ures: Purity (which measures the degree of cohe-sion of the clusters obtained) and Entropy.
Thepurity and entropy formulas are shown in Table 9.DescriptionAllCandidateAttributesFilteredCandidateAttributes2-WayAttributesPurity 0.657 0.672 0.693Entropy 0.335 0.319  0.302Vector Size 24,178 4,296 3,824Table 8: Results of re-clustering concepts usingdifferent sets of attributesClustering the concepts using only filtered can-didate attributes improved the clustering purityfrom 0.657 to 0.672.
This improvement in purity isnot significant.
However, clustering using only theattributes sanctioned by  the 2-way classifier im-proved the purity further to 0.693, and this im-provement in purity from the initial purity  wassignificant (t = 2.646, df = 801, p < 0.05).Entropy PuritySingleCluster ?=?=qi rirrirr nnnnqSE1loglog1)(  )(max1)( irirr nnSP =Over-all )(1rkrr SEnnEntropy ?==  )(1rkrr SPnnPurity ?==Table 9: Entropy and Purity in CLUTO.Sr is a cluster, nr is the size of the cluster, q is the number ofclasses, nir is the number of concepts from the  ith class thatwere assigned to the rth cluster, n is the number of concepts,and k is the number of clusters.7 Discussion and ConclusionsThe lexicon does not simply contain informationabout synonymy and hyponymy relations; it alsocontains information about the attributes of theconcepts expressed by senses, as in Qualia struc-tures.
In previous work, we developed techniquesfor mining candidate attributes from the Web; inthis paper we presented a method for improvingthe quality of attributes thus extracted, based on aclassification for attributes derived from work inlinguistics and philosophy, and a classifier thatautomatically tags candidate attributes with suchclasses.
Both the 2-way and the 5-way classifiersachieve good precision and recall.
Our work alsoreveals, however, that the notion of attribute is notfully understood.
On the one hand, that attributejudgments are not always easy for humans evengiven a scheme; on the other hand, the results forcertain types of attributes, especially activities andqualities, could certainly be improved.
We alsofound that whereas attributes of physical objectsare relatively easy to classify, the attributes ofother types of concepts are harder ?particularlywith activities.
(See the Appendix for examples.
)Our longer term goal is thus to further clarify thenotion of attribute, possibly refining our classifica-tion scheme, in collaboration with linguists, phi-losophers, and psycholinguists.
One comparisonwe are particularly interested in pursuing at themoment is that with feature lists used by psycholo-gist, for whom knowledge representation is en-tirely concept-based, and virtually every propertyof a concept counts as an attribute, including prop-erties that would be viewed as IS-A links and whatwould be considered a value.
Is it possible to makea principled, yet cognitively based distinction?AcknowledgmentsAbdulrahman Almuhareb is supported by KingAbdulaziz City for Science and Technology(KACST), Riyadh, Saudi Arabia.
We wish to thankthe anonymous referees for many helpful sugges-tions.ReferencesAlmuhareb, A. and Poesio, M. (2004).
Attribute-Basedand Value-Based Clustering: An Evaluation.
In Proc.of EMNLP.
Barcelona, July.Almuhareb, A. and Poesio, M. (2005).
Concept Learn-ing and Categorization from the Web.
In Proc.
ofCogSci.
Italy, July.Baader, F., Calvanese, D., McGuinness, D., Nardi, D.and Patel-Schneider, P. (Editors).
(2003).
The De-scription Logic Handbook.
Cambridge UniversityPress.Berland, M. and Charniak, E. (1999).
Finding parts invery large corpora.
In Proc.
of the 37th ACL, (pp.57?64).
University of Maryland.Brill, E. (1995).
Transformation-Based Error-DrivenLearning and Natural Language Processing: A CaseStudy in Part of Speech Tagging.
ComputationalLinguistics.25Caraballo, S. A.
(1999).
Automatic construction of ahypernym-labeled noun hierarchy from text.
In Proc.of  the 37th  ACL.Dixon, R. M. W. (1991).
A New Approach to EnglishGrammar, on Semantic Principles.
Clarendon Press,Oxford.Fellbaum, C. (Editor).
(1998).
WordNet: An electroniclexical database.
The MIT Press.Girju, R. and Moldovan, D. (2002).
Mining answers forcausal questions.
In Proc.
AAAI.Guarino, N. (1992).
Concepts, attributes and arbitraryrelations: some linguistic and ontological criteria forstructuring knowledge base.
Data and KnowledgeEngineering, 8, (pp.
249?261).Hearst, M. A.
(1998).
Automated discovery of WordNetrelations.
In Fellbaum, C. (Editor).
WordNet: AnElectronic Lexical Database.
MIT Press.Karypis, G. (2002).
CLUTO: A clustering toolkit.
Tech-nical Report 02-017.
University of Minnesota.
Athttp://www-users.cs.umn.edu/~karypis/cluto/.Manning, C. D. and Schuetze H. (1999).
Foundations ofStatistical NLP.
MIT Press.Merlo, P. and Stevenson, S. (2001).
Automatic VerbClassification Based on Statistical Distributions ofArgument Structure.
Computational Linguistics.
27:3, 373-408.Murphy, G. L. (2002).
The Big Book of Concepts.
TheMIT Press.Poesio, M., Ishikawa, T., Schulte im Walde, S. andVieira, R. (2002).
Acquiring lexical knowledge foranaphora resolution.
In Proc.
Of LREC.Pustejovsky, J.
(1995).
The generative lexicon.
MITPress.Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J.(1985).
A comprehensive grammar of the Englishlanguage.
London: Longman.Sanchez-Graillet, O. and Poesio, M. (2004).
BuildingBayesian Networks from text.
In Proc.
of LREC, Lis-bon, May.Vinson, D. P., Vigliocco, G., Cappa, S., and Siri, S.(2003).
The breakdown of semantic knowledge: in-sights from a statistical model of meaning representa-tion.
Brain and Language, 86(3), 347-365(19).Witten, I. H. and Frank, E. (1999).
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations, Morgan Kaufmann.Woods, W. A.
(1975).
What?s in a link: Foundations forsemantic networks.
In Daniel G. Bobrow and AlanM.
Collins, editors, Representation and Understand-ing: Studies in Cognitive Science, (pp.
35-82).
Aca-demic Press, New York.Appendix A.
5-Way Automatic Classification of the Best Candidate Attributes ofSome ConceptsCarClass Best AttributesActivity acceleration, performance, styling, construction, propulsion, insurance, stance, ride, move-mentPart &Related-Objectfront, body, mass, underside, hood, roof, nose, graphics, side, trunk, engine, boot, frame, bot-tom, backseat, chassis, wheelbase, silhouette, floor, battery, windshield, seat, undercarriage,tank, window, steering, drive, finishQuality  speed, weight, handling, velocity, color, condition, width, look, colour, feel, momentum, heritage, shape, appearance, ownership, make, convenience, age, quality, reliabilityRelated-Agent  driver, owner, buyer, sponsor, occupant, sellerNon-Attributerest, price, design, balance, motion, lure, control, use, future, cost, inertia, model, wheel,style, position, setup, sale, supply, safety26CamelClass Best AttributesActivity introduction, selling, argument, exhaustionPart &Related-Objectnose, hump, furniture, saddle, hair, flesh, neck, milk, head, reins, foot, eye, hooves, humps,ass, feet, hoof, flanks, bones, ears, bag, skin, haunches, stomach, legs, urine, meat, penis,load, breast, backside, testicles, rope, corpse, house, nostrils, foam, bell, sight, butt, fur, bod-ies, toe, hoofs, heads, knees, pancreas, mouth, coat, uterus, necks, chin, uddersQuality  origins, gait, domestication, usefulness, pace, fleetness, smell, existence, appeal, birth, awk-wardnessRelated-Agent  ghostNon-Attribute  gift, rhythm, physiology, battle, case, example, dance, manner, descriptionCancerClass Best AttributesActivitygrowth, development, removal, treatment, recurrence, diagnosis, pain, spreading, metastasis,detection, eradication, elimination, production, discovery, remission, advance, excision, pre-vention, evolution, disappearance, anxietyPart &Related-Objectlocation, site, lump, nature, root, cells, margin, formation, margins, roots, world, regionQualityextent, size, seriousness, progression, severity, aggressiveness, cause, progress, symptoms,effects, risk, incidence, staging, biology, onset, characteristics, histology, ability, status, ap-pearance, thickness, sensitivity, causes, prevalence, responsiveness, ravages, frequency, aeti-ology, circumstances, rarity, outcome, behavior, geneticsRelated-Agent  club, patientNon-Attributestage, spread, grade, origin, course, power, return, area, response, presence, type, particulars,occurrence, prognosis, pathogenesis, source, news, cure, pathology, properties, genesis,boundaries, drama, stages, chapterFamilyClass Best AttributesActivity disintegration, protection, decline, destruction, breakup, abolition, participation, reunifica-tion, reconciliation, dissolution, composition, restorationPart &Related-Objecthead, institution, support, flower, core, fabric, culture, dimension, food, lineage, cornerstone,communityQualitybreakdown, importance, honor, structure, sociology, integrity, unity, sanctity, health, privacy,survival, definition, influence, honour, involvement, continuity, stability, size, preservation,upbringing, centrality, ancestry, solidarity, hallmark, status, functioning, primacy, autonomyRelated-Agentfather, baby, member, mother, members, patriarch, breadwinner, matriarch, man, foundation,founder, heir, daughterNon-Attributerest, role, income, history, concept, welfare, pedigree, genealogy, presence, context, origin,bond, tradition, taxonomy, system, wealth, lifestyle, surname, crisis, ideology, rights, eco-nomics, safety27
