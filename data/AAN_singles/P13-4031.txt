Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 181?186,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticstSEARCH: Flexible and Fast Search over Automatic Translations forImproved Quality/Error AnalysisMeritxell Gonza`lez and Laura Mascarell and Llu?
?s Ma`rquezTALP Research CenterUniversitat Polite`cnica de Catalunya{mgonzalez,lmascarell,lluism}@lsi.upc.eduAbstractThis work presents tSEARCH, a web-basedapplication that provides mechanisms fordoing complex searches over a collectionof translation cases evaluated with a largeset of diverse measures.
tSEARCH uses theevaluation results obtained with the ASIYAtoolkit for MT evaluation and it is connectedto its on-line GUI, which makes possiblea graphical visualization and interactive ac-cess to the evaluation results.
The searchengine offers a flexible query language al-lowing to find translation examples match-ing a combination of numerical and struc-tural features associated to the calculation ofthe quality metrics.
Its database design per-mits a fast response time for all queries sup-ported on realistic-size test beds.
In sum-mary, tSEARCH, used with ASIYA, offersdevelopers of MT systems and evaluationmetrics a powerful tool for helping transla-tion and error analysis.1 IntroductionIn Machine Translation (MT) system develop-ment, a qualitative analysis of the translations is afundamental step in order to spot the limitations ofa system, compare the linguistic abilities of differ-ent systems or tune the parameters during systemrefinement.
This is especially true in statisticalMT systems, where usually no special structuredknowledge is used other than parallel data and lan-guage models, but also on systems that need toreason over linguistic structures.
The need for an-alyzing and comparing automatic translations withrespect to evaluation metrics is also paramountfor developers of translation quality metrics, whoneed elements of analysis to better understand thebehavior of their evaluation measures.This paper presents tSEARCH, a web applica-tion that aims to alleviate the burden of manualanalysis that developers have to conduct to as-sess the translation quality aspects involved in theabove mentioned situations.
As a toy example,consider for instance an evaluation setting withtwo systems, s1 and s2, and two evaluation met-rics m1 and m2.
Assume also that m1 scores s1 tobe better than s2 in a particular test set, while m2predicts just the contrary.
In order to analyze thiscontradictory evaluation one might be interested ininspecting from the test set the particular transla-tion examples that contribute to these results, i.e.,text segments t for which the translation providedby s1 is scored better by m1 than the translationprovided by s2 and the opposite behavior regard-ing metric m2.
tSEARCH allows to retrieve (vi-sualize and export) these sentences with a simplequery in a fast time response.
The search can befurther constrained, by requiring certain marginson the differences, by including other systems ormetrics, or by requiring some specific syntactic orsemantic constructs to appear in the examples.tSEARCH is build on top of ASIYA (Gime?nezand Ma`rquez, 2010), an open-source toolkit forMT evaluation; and it can be used along withthe ASIYA ON-LINE INTERFACE (Gonza`lez et al2012), which provides an interactive environmentto examine the sentences.
ASIYA allows to ana-lyze a wide range of linguistic aspects of candi-date and reference translations using a large setof automatic and heterogeneous evaluation met-rics.
In particular, it offers a especially rich setof measures that use syntactic and semantic infor-mation.
The intermediate structures generated bythe parsers, and used to compute the scoring mea-sures, could be priceless for MT developers, whocan use them to compare the structures of severaltranslations and see how they affect the perfor-mance of the metrics, providing more understand-ing in order to interpret the actual performance ofthe automatic translation systems.tSEARCH consists of: 1) a database that stores181the resources generated by ASIYA, 2) a query lan-guage and a search engine able to look throughthe information gathered in the database, and 3) agraphical user interface that assists the user towrite a query, returns the set of sentences that ful-fill the conditions, and allows to export these re-sults in XML format.
The application is publiclyaccessible on-line1, and a brief explanation of itsmost important features is given in the demonstra-tive video.In the following, Section 2 gives an overviewof the ASIYA toolkit and the information gatheredfrom the evaluation output.
Section 3 and Sec-tion 4 describe in depth the tSEARCH applicationand the on-line interface, respectively.
Finally,Section 5 reviews similar applications in compari-son to the functionalities addressed by tSEARCH.2 MT Evaluation with the ASIYA ToolkitCurrently, ASIYA contains more than 800 variantsof MT metrics to measure the similarity betweentwo translations at several linguistic dimensions.Moreover, the scores can be calculated at threegranularity levels: system (entire test-set), docu-ment and sentence (or segment).As shown in Figure 1, ASIYA requires the userto provide a test suite.
Then, the input files areprocessed in order to calculate the annotations, theparsing trees and the final metric scores.
Sev-eral external components are used for both, met-ric computation and automatic linguistic analysis2.The use of these tools depends on the languagessupported and the type of measures that one needsto obtain.
Hence, for instance, lexical-basedmeasures are computed using the last versionof most popular metrics, such as BLEU, NIST,METEOR or ROUGE.
The syntax-wise measuresneed the output of taggers, lemmatizers, parsers1http://asiya.lsi.upc.edu/demo2A complete list of external components can be found inthe Technical Manual at the ASIYA web-siteFigure 1: ASIYA processes and data filesand other analyzers.
In those cases, ASIYA usesthe SVMTool (Gime?nez and Ma`rquez, 2004),BIOS (Surdeanu et al 2005), the Charniak-Johnson and Berkeley constituent parsers (Char-niak and Johnson, 2005; Petrov and Klein, 2007),and the MALT dependency parser (Nivre et al2007), among others.In the tSEARCH platform, the system managesthe communication with an instance of the ASIYAtoolkit running on the server.
For every test suite,the system maintains a synchronized representa-tion of the input data, the evaluation results and thelinguistic information generated.
Then, the systemupdates a database where the test suites are storedfor further analysis using the tSEARCH tool, as de-scribed next.3 The tSEARCH TooltSEARCH offers a graphical search engine to ana-lyze a given test suite.
The system core retrievesall translation examples that satisfy certain prop-erties related to either the evaluation scores or thelinguistic structures.
The query language designedis simple and flexible, and it allows to combinemany properties to build sophisticated searches.The tSEARCH architecture consists of the threecomponents illustrated in Figure 2: the web-basedinterface, the storage system based on NoSQLtechnology and the tSEARCH core, composed ofa query parser and a search engine.The databases (Section 3.1) are fed through thetSearch Data Loader API used by ASIYA.
Atrun-time, during the calculation of the measures,ASIYA inserts all the information being calcu-lated (metrics and parses) and a number of pre-calculated variables (e.g., average, mean and per-centiles).
These operations are made in parallel,which makes the overhead of filling the databasemarginal.The query parser (Section 3.2) receives thequery from the on-line interface and converts itFigure 2: tSEARCH architecture182(a) Scores Column Family (b) Statistics Column Family(c) Linguistic Elements Column FamilyFigure 3: tSEARCH data modelinto a binary tree structure where each leaf is a sin-gle part of an operation and each node combinesthe partial results of the children.
The search en-gine obtains the final results by processing the treebottom-up until the root is reached.3.1 Data Representation, Storage and AccessThe amount of data generated by ASIYA can bevery large for test sets with thousands of sen-tences.
In order to handle the high volume ofinformation, we decided to use the Apache Cas-sandra database3, a NoSQL (also known as notonly SQL) solution that deals successfully withthis problem.It is important to remark that there is no similar-ity between NoSQL and the traditional relationaldatabase management system model (RDBMS).Actually, RDBMS uses SQL as its query languageand requires a relational model, whereas NoSQLdatabases do not.
Besides, the tSEARCH querylanguage can be complex, with several conditions,which makes RDBMS perform poorly due thecomplexity of the tables.
In contrast, NoSQL-databases use big-tables having many queryinginformation precalculated as key values, whichyields for direct access to the results.The Cassandra data model is based on columnfamilies (CF).
A CF consists of a set of rows thatare uniquely identified by its key and have a setof columns as values.
So far, the tSEARCH datamodel has the three CFs shown in Figure 3.
Thescores CF in Figure 3(a) stores information relatedto metrics and score values.
Each row slot containsthe list of segments that matches the column key.3http://cassandra.apache.org/The statistics CF in Figure 3(b) stores basic statis-tics, such as the minimum, maximum, average,median and percentiles values for every evaluationmetric.
The CF having the linguistic elements inFigure 3(c) stores the results of the parsers, suchas part-of-speech, grammatical categories and de-pendency relationships.One of the goals of NoSQL databases is to ob-tain the information required in the minimum ac-cess time.
Therefore, the data is stored in theway required by the tSEARCH application.
Forinstance, the query BLEU > 0.4 looks for allsegments in the test suite having a BLEU scoregreater than 0.4.
Thus, in order to get the queryresult in constant time, we use the metric identi-fier as a part of the key for the scores CF, and thescore 0.4 as the column key.3.2 The Query Language and ParserThe Query Parser module is one of the key ingre-dients in the tSEARCH application because it de-termines the query grammar and the allowed op-erations, and it provides a parsing method to an-alyze any query and produce a machine-readableversion of its semantics.
It is also necessary in or-der to validate the query.There are several types of queries, depending onthe operations used: arithmetic comparisons, sta-tistical functions (e.g., average, quartiles), rangeof values, linguistic elements and logical opera-tors.
Furthermore, the queries can be applied atsegment-, document- and/or system-level, and itis even possible to create any group of systemsor metrics.
This is useful, for instance, in or-der to limit the search to certain type of systems(e.g., rule-based vs. statistical) and specific met-183Figure 4: (top) Query operations and functions, (bottom) Queries for group of systems and metricsrics (e.g., lexical vs. syntactic).
All possible querytypes are described in the following subsections(3.2.1 to 3.2.3) and listed in Figure 4.3.2.1 Segment-level and Metric-basedQueriesThe most basic queries are those related tosegment level scores, i.e., obtain all segmentsscored above/below a value for a concrete met-ric.
The common comparison operators are sup-ported, such as for instance, BLEU > 0.4 andBLEU gt 0.4, that are both correct and equiva-lent queries.Basic statistics are also calculated at run-time,which allows to use statistic variables as values,e.g., obtain the segments scored in the fourth quar-tile of BLEU.
The maximum, minimum, average,median and percentile values of each metric areprecalculated and saved into the MAX, MIN, AVG,MEDIAN and PERC variables, respectively.
Thethresholds and quartiles (TH,Q) are calculated atrun-time based on percentiles.
MIN and MAX canalso be used and allow to get alsegments in thetest set (i.e.,BLEU ge MIN).The threshold function implies a percentage.The query BLEU > TH(20) gets all segmentsthat have a BLEU score greater than the scorevalue of the bottom 20% of the sentences.It is also possible to specify an interval of valuesusing the operator IN[x,y].
The use of paren-thesis is allowed in order to exclude the bound-aries.
The arguments for this operator can beeither numerical values or the predefined func-tions for quartiles and percentiles.
Therefore,the following example BLEU IN [TH(20),TH(30)] returns all segments with a BLEU scorein the range between the threshold of the 20% (in-cluded) and the 30% (excluded).The quartile function Q(X) takes a value be-tween 1 and 4 and returns all segments thathave their score in that quartile.
In contrast,the percentile function generalizes the previous:PERC(n,M), where 1 < M <= 100; 1 <= n <=M , returns all the segments with a score in the nthpart, when the range of scores is divided in M partsof equal size.Finally, a query can be composed of more thanone criterion.
To do so, the logical operators ANDand OR are used to specify intersection and union,respectively.3.2.2 System- and Document-level QueriesThe queries described next implement the searchprocedures for more sophisticated queries involv-ing system and document level properties, andalso the linguistic information used in the calcu-184lation of the evaluation measures.
The purpose ofthis functionality is to answer questions related togroups of systems and/or metrics.As explained in the introduction, one maywant to find the segments with good scoresfor lexical metrics and, simultaneously, badscores for syntactic-based ones, or viceversa.The following query illustrates how to doit: ((srb[LEX] > AVG) OR (s3[LEX]< AVG)) AND ((srb[SYN] < AVG) OR(s3[SYN] > AVG) ), where srb = {s1, s2}is the definition of a group of the rule-basedsystems s1 and s2, s3 is another transla-tion system, and LEX={BLEU,NIST} andSYN={CP-Op(*),SP-Oc(*)} are two groupsof lexical- and syntactic-based measures, respec-tively.
The output of this kind of queries can helpdevelopers to inspect the differences between thesystems that meet these criteria.Concerning queries at document level, its struc-ture is the same but applied at document scope.They may help to find divergences when translat-ing documents from different domains.3.2.3 Linguistic Element-based QueriesThe last functionality in tSEARCH allows search-ing the segments that contain specific linguisticelements (LE), estimated with any of the ana-lyzers used to calculate the linguistic structures.Linguistic-wise queries will allow the user tofind segments which match the criteria for anylinguistic feature calculated by ASIYA: part-of-speech, lemmas, named entities, grammatical cat-egories, dependency relations, semantic roles anddiscourse structures.We have implemented queries that matchn-grams of lemmas (lemma), parts-of-speech(pos) and items of shallow (SP) or constituentparsing (CP), dependency relations (DP) and se-mantic roles SR, such as LE[lemma(be),pos(NN,adj), SP(NP,ADJP,VP),CP(VP,PP)].
The DP function allows alsospecifying a compositional criterion (i.e., thecategories of two words and their dependencyrelationship) and even a chain of relations, e.g.,LE[DP(N,nsubj,V,dep,V)].
In turn, theSR function obtains the segments that match averb and its list of arguments, e.g., LE[SR(ask,A0, A1)].The asterisk symbol can be used to substi-tute any LE-item, e.g., LE[SP(NP,*,PP),DP(*,*,V)].
When combined with semanticroles, one asterisk substitutes any verb that has allthe arguments specified, e.g., LE[SR(*, A0,A1)], whereas two asterisks in a row allowarguments to belong to different verbs in thesame sentence.
For instance, LE[SR(**, A1,AM-TMP)] matches the sentence Those who pre-fer to save money, may try to wait a few more days,where the verb wait has the argument AM-TMPand the verb prefer has the argument A1.4 On-line Interface and Export of theResultstSEARCH is fully accessible on-line through theASIYA ON-LINE INTERFACE.
The web applica-tion runs ASIYA remotely, calculates the scoresand fills the tSEARCH database.
It also offers thechance to upload the results of a test suite previ-ously processed.
This way it feeds the databasedirectly, without the need to run ASIYA.Anyhow, once the tSEARCH interface is alreadyaccessible, one can see a tools icon on the rightof the search box.
It shows the toolbar with allavailable metrics, functions and operations.
Thesearch box allows to query the database using thequery language described in Section 3.2.After typing a query, the user can navigate theresults using three different views that organizethem according to the user preferences: 1) Allsegments shows all segments and metrics men-tioned in the query, the segments can be sortedby the score, in ascendent or descendent order,just tapping on the metric name; 2) Grouped bysystem groups the segments by system and, forFigure 5: The tSEARCH Interface185each system, by document; 3) Grouped by segmentdisplays the segment organization, which allowsan easy comparison between several translations.Each group contains all the information related toa segment number, such as the source and the ref-erence sentences along with the candidate transla-tions that matched the query.Additionally, moving the mouse over the seg-ments displays a floating box as illustrated in Fig-ure 5.
It shows some relevant information, suchas the source and references segments, the systemthat generated the translation, the document whichthe segment belongs to, and the scores.Finally, all output data obtained during thesearch can be exported as an XML file.
It is possi-ble to export all segments, or the results structuredby system, by segment, or more specific informa-tion from the views.5 Related Work and ConclusionsThe ultimate goal of tSEARCH is to provide thecommunity with a user-friendly tool that facilitatesthe qualitative analysis of automatic translations.Currently, there are no freely available automatictools for aiding MT evaluation tasks.
For this rea-son, we believe that tSEARCH can be a useful toolfor MT system and evaluation metric developers.So far, related works in the field address (semi)-automatic error analysis from different perspec-tives.
A framework for error analysis and classifi-cation was proposed in (Vilar et al 2006), whichhas inspired more recent works in the area, suchas (Fishel et al 2011).
They propose a methodfor automatic identification of various error types.The methodology proposed is language indepen-dent and tackles lexical information.
Nonetheless,it can also take into account language-dependentinformation if linguistic analyzers are available.The user interface presented in (Berka et al 2012)provides also automatic error detection and clas-sification.
It is the result of merging the Hjer-son tool (Popovic?, 2011) and Addicter (Zeman etal., 2011).
This web application shows alignmentsand different types of errors colored.In contrast, the ASIYA interface and thetSEARCH tool together facilitate the qualitativeanalysis of the evaluation results yet providinga framework to obtain multiple evaluation met-rics and linguistic analysis of the translations.They also provide the mechanisms to search andfind relevant translation examples using a flexiblequery language, and to export the results.AcknowledgmentsThis research has been partially funded bythe Spanish Ministry of Education and Science(OpenMT-2, TIN2009-14675-C03), the EuropeanCommunity?s Seventh Framework Programme un-der grant agreement number 247762 (FAUST,FP7-ICT-2009-4-247762) and the EAMT Spon-sorhip of Activities: Small research and develop-ment project, 2012.ReferencesJan Berka, Ondrej Bojar, Mark Fishel, Maja Popovic,and Daniel Zeman.
2012.
Automatic MT error anal-ysis: Hjerson helping Addicter.
In Proc.
8th LREC.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine N-best Parsing and MaxEnt DiscriminativeReranking.
In Proc.
43rd Meeting of the ACL.Mark Fishel, Ondr?ej Bojar, Daniel Zeman, and JanBerka.
2011.
Automatic Translation Error Analy-sis.
In Proc.
14th Text, Speech and Dialogue (TSD).Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool:A general POS tagger generator based on SupportVector Machines.
In Proc.
4th Intl.
Conf.
LREC.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010.
Asiya: AnOpen Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathe-matical Linguistics, 94.Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??sMa`rquez.
2012.
A Graphical Interface for MT Eval-uation and Error Analysis.
In Proc.
50th Meeting ofthe ACL.
System Demonstration.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser: Alanguage-independent system for data-driven depen-dency parsing.
Natural Language Engineering, 13.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Proc.
HLT.Maja Popovic?.
2011.
Hjerson: An Open SourceTool for Automatic Error Classification of MachineTranslation Output.
The Prague Bulletin of Mathe-matical Linguistics, 96.Mihai Surdeanu, Jordi Turmo, and Eli Comelles.
2005.Named Entity Recognition from Spontaneous Open-Domain Speech.
In Proc.
9th INTERSPEECH.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Machine Trans-lation Output.
In Proc.
5th LREC.Daniel Zeman, Mark Fishel, Jan Berka, and Ondrej Bo-jar.
2011.
Addicter: What Is Wrong with My Trans-lations?
The Prague Bulletin of Mathematical Lin-guistics, 96.186
