Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1977?1985,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsOnline Topic Model for Twitter Considering Dynamics of User Interestsand Topic TrendsKentaro Sasaki, Tomohiro Yoshikawa, Takeshi FuruhashiGraduate School of Engineering Nagoya Universitysasaki@cmplx.cse.nagoya-u.ac.jpyoshikawa, furuhashi@cse.nagoya-u.ac.jpAbstractLatent Dirichlet allocation (LDA) is atopic model that has been applied to var-ious fields, including user profiling andevent summarization on Twitter.
WhenLDA is applied to tweet collections, it gen-erally treats all aggregated tweets of a useras a single document.
Twitter-LDA, whichassumes a single tweet consists of a singletopic, has been proposed and has shownthat it is superior in topic semantic coher-ence.
However, Twitter-LDA is not capa-ble of online inference.
In this study, weextend Twitter-LDA in the following twoways.
First, we model the generation pro-cess of tweets more accurately by estimat-ing the ratio between topic words and gen-eral words for each user.
Second, we en-able it to estimate the dynamics of user in-terests and topic trends online based on thetopic tracking model (TTM), which mod-els consumer purchase behaviors.1 IntroductionMicroblogs such as Twitter, have prevailed rapidlyin our society recently.
Twitter users post a mes-sage using 140 characters, which is called a tweet.The characters limit allows users to post tweetseasily about not only personal interest or real lifebut also public events such as traffic accidentsor earthquakes.
There have been many studieson how to extract and utilize such informationon tweets (Diao et al., 2012; Pennacchiotti andPopescu, 2011; Sakaki et al., 2010; Weng et al.,2010).Topic models, such as latent Dirichlet alloca-tion (LDA) (Blei et al., 2003) are widely used toidentify latent topic structure in large collectionsof documents.
Recently, some studies have ap-plied LDA to Twitter for user classification (Pen-nacchiotti and Popescu, 2011), detection of influ-ential users (Weng et al., 2010), and so on.
LDAis a generative document model, which assumesthat each document is represented as a probabil-ity distribution over some topics, and that eachword has a latent topic.
When we apply LDAto tweets, each tweet is treated as a single docu-ment.
This direct application does not work wellbecause a tweet is very short compared with tradi-tional media such as newspapers.
To deal with theshortness of a tweet, some studies aggregated allthe tweets of a user as a single document (Hongand Davison, 2010; Pennacchiotti and Popescu,2011; Weng et al., 2010).
On the other hand, Zhaoet al.
(2011) proposed ?Twitter-LDA,?
which isa model that considers the shortness of a tweet.Twitter-LDA assumes that a single tweet consistsof a single topic, and that tweets consist of topicand background words.
Zhao et al.
(2011) showthat it works well at the point of semantic coher-ence of topics compared with LDA.
However, aswith the case of LDA, Twitter-LDA cannot con-sider a sequence of tweets because it assumes thatsamples are exchangeable.
In Twitter, user inter-ests and topic trends are dynamically changing.In addition, when new data comes along, a newmodel must be generated again with all the datain Twitter-LDA because it does not assume onlineinference.
Therefore, it cannot efficiently analyzethe large number of tweets generated everyday.
Toovercome these difficulties, a model that considersthe time sequence and has the capability of onlineinference is required.In this study, we first propose an improvedmodel based on Twitter-LDA, which assumes thatthe ratio between topic and background words dif-fers for each user.
This study evaluates the pro-posed method based on perplexity and shows theefficacy of the new assumption in the improvedmodel.
Second, we propose a new topic modelcalled ?Twitter-TTM?
by extending the improved1977model based on the topic tracking model (TTM)(Iwata et al., 2009), which models the purchasebehavior of consumers and is capable of onlineinference.
Finally, we demonstrate that Twitter-TTM can effectively capture the dynamics of userinterests and topic trends in Twitter.2 Improvement of Twitter-LDA2.1 Improved-ModelFigure 1(a) shows the graphical representation ofTwitter-LDA based on the following assumptions.There are K topics in Twitter and each topic is rep-resented by a topic word distribution.
Each userhas his/her topic interests ?urepresented by a dis-tribution over K topics.
Topic k is assigned toeach tweet of user u depending on the topic inter-ests ?u.
Each word in the tweet assigned by topick is generated from a background word distribu-tion ?Bor a topic word distribution ?k.
Whetherthe word is a background word or a topic wordis determined by a latent value y.
When y = 0,the word is generated from the background worddistribution ?B, and from the topic word distribu-tion ?kwhen y = 1.
The latent value y is chosenaccording to a distribution ?.
In other words, theratio between background and topic words is de-termined by ?.In Twitter-LDA, ?
is common for all users,meaning that the rate between background andtopic words is the same for each user.
However,this assumption could be incorrect, and the ratecould differ for each user.
Thus, we develop animproved model based on Twitter-LDA, which as-sumes that ?
is different for each user, as shownin Figure 1(b).
In the improved model, the ratebetween background and topic words for user u isdetermined by a user-specific distribution ?u.
Theimproved model is expected to infer the generativeprocess of tweets more efficiently.2.2 Experiment for Improved ModelWe performed an experiment to compare the pre-dictive performances of LDA, TTM, and the im-proved model shown in Section 2.1.
In this ex-periment, LDA was applied as the method to ag-gregate all tweets of a user as a single document.The original Twitter data set contains 14,305 usersand 292,105 tweets collected on October 18, 2013.We then removed words that occurred less than20 times and stop words.
Retweets1 were treated1Republishing a tweet written by another Twitter user.as the same as other general tweets because theyreflected the user?s interests.
After the abovepreprocessing, we obtained the final dataset with14,139 users, 252,842 tweets, and 7,763 vocab-ularies.
Each model was inferred with collapsedGibbs sampling (Griffiths and Steyvers, 2004) andthe iteration was set at 500.
For a fair comparison,the hyper parameters in these models were opti-mized in each Gibbs sampling iteration by max-imizing likelihood using fixed iterations (Minka,2000).This study employs perplexity as the evaluationindex, which is the standard metric in informationretrieval literature.
The perplexity of a held-outtest set is defined asperplexity = exp(?1N?ulog p(wu))(1)where wurepresents words are contained in thetweets of user u and N is the number of words inthe test set.
A lower perplexity means higher pre-dictive performance.
We set the number of topicsK at 50, 100, 150, 200, and 250 and evaluated theperplexity for each model in each K via a 10-foldcross-validation.The results are shown in Table 1, which showsthat the improved model performs better than theother models for any K. Therefore, the new as-sumption of the improved model, that the rate be-tween background and topic words is different foreach user, could be more appropriate.
LDA per-formance worsens with an increase in K becausethe aggregated tweets of a single user neglect thetopic of each tweet.Table 2 shows examples of the tweets of userswith high and low rates of background words.
Theusers with a high background words rate tend touse basic words that are often used in any top-ics, such as ?like,?
?about,?
and ?people,?
and theytend to tweet about their personal lives.
On theother hand, for users with a low background wordsrate, topical words are often used such as ?Arse-nal,?
?Justin,?
and ?Google?.
They tend to tweetabout their interests, including music, sports, andmovies.3 Twitter-TTM3.1 Model Extension based on TopicTracking ModelWe extend the improved model shown in Section2.1 considering the time sequence and capabil-1978w?kKNu,s?zUNuy ?
?B???
?
(a) Twitter-LDAw?kKNu,s?zUNuy??B????
(b) Improved-modelFigure 1: Graphical representation of Twitter-LDA and Improved-modelTable 1: Perplexity of each model in 10 runsNumber of topic K LDA Twitter-LDA Improved-model50 1586.7 (14.4) 2191.0 (28.4) 1555.3 (36.7)100 1612.7 (11.9) 1933.9 (23.6) 1471.7 (22.3)150 1635.3 (11.2) 1760.1 (15.7) 1372.3 (20.0)200 1655.2 (13.0) 1635.4 (22.1) 1289.5 (13.3)250 1672.7 (17.2) 1542.8 (12.5) 1231.1 (11.9)Table 2: Example of tweets of users with high and low rate of background wordsHigh rate of background words Low rate of background wordsI hope today goes quickly Team Arsenal v will Ozil beI want to work in a cake Making Justin smile and laugh as he is working on musicAll need your support please Google nexus briefly appears in Google play storeity of online inference based on TTM (Iwata etal., 2009).
TTM is a probabilistic consumer pur-chase behavior model based on LDA for track-ing the interests of each user and the trends ineach topic.
Other topic models considering the dy-namics of topics include the dynamic topic model(DTM) (Blei and Lafferty, 2006) and topic overtime (ToT) (Wang and McCallum, 2006).
DTM isa model for analyzing the time evolution of top-ics in time-ordered document collections.
It doesnot track the interests of each user as shown inFigure 2(a) because it assumes that a user (doc-ument) has only one time stamp.
ToT requires allthe data over time for inference, thus, it is not ap-propriate for application to continuously generateddata such as Twitter.
We consider a model must becapable of online inference and track the dynam-ics of user interests and topic trends for modelingtweets.
Since TTM has these abilities, we adapt itto the improved model described in Section 2.Figure 2(b) shows the graphical representationof TTM.
TTM assumes that the mean of user in-terests at the current time is the same as that at theprevious time, unless new data is observed.
For-mally, the current interest ?t,uare drawn from thefollowing Dirichlet distribution in which the meanis the previous interest ?
?t?1,uand the precision is1979w??kK?zUNuw?kK?zUNu?
?t-1 t(a) DTMw??kK?zUNuw?kK?zUNu?
??
?t -1 t(b) TTMFigure 2: Graphical representation of DTM and TTM?t,up(?t,u|?
?t?1,u, ?t,u) ??k??t,u?
?t?1,u,k?1t,u,k(2)where ?t,u,krepresents the probability that user uis interested in topic k at time t. t is a discretevariable and can be arbitrarily set as the unit timeinterval, e.g., at one day or one week.
The preci-sion ?t,urepresents the interest persistence of howconsistently user u maintains his/her interests attime t compared with the previous time t?1.
?t,uis estimated for each time period and each userbecause interest persistence depends on both timeand users.
As mentioned above, the current topictrend ?t,kis drawn from the following Dirichletdistribution with the previous trend ??t?1,kp(?t,k|?
?t?1,k, ?t,k) ??v??t,k?
?t?1,k,v?1t,k,v(3)where ?t,k,vrepresents the probability that word vis chosen in topic k at time t.Here our proposed Twitter-TTM adapts theabove TTM assumptions to the improved model.That is, we extend the improved model wherebyuser interest ?t,uand topic trend ?t,kdepend onprevious states.
Time dependency is not consid-ered on ?Band ?ubecause they can be regardedas being independent of time.Figures 3 and 4 show the generative processand a graphical representation of Twitter-TTM, re-spectively.
Twitter-TTM can capture the dynam-ics of user interests and topic trends in Twitterconsidering the features of tweets online.
More-over, Twitter-TTM can be extended to capturelong-term dependences, as described in Iwata etal.
(2009).3.2 Model InferenceWe use a stochastic expectation-maximization al-gorithm for Twitter-TTM inference, as describedin Wallach (2006) in which Gibbs sampling of la-tent values and maximum joint likelihood estima-tion of parameters are alternately iterated.
At timet, we estimate user interests ?t= {?
?t,u}Uu=1,topic trends ?t= {?
?t,k}Kk=1, background worddistribution ?t,B, word usage rate distribution ?t,u,interest persistence parameters ?t = {?t,u}Uu=1,and trend persistence parameters ?t = {?t,k}Kk=1using the previous time interests ?
?t?1and trends?
?t?1.We employ collapsed Gibbs sampling to inferthe latent variables.
Let Dtbe a set of tweets andZt, Ytbe a set of latent variables z, y at time t. Wecan integrate the parameters in the joint distribu-1980tion as follows:p(Dt, Yt, Zt|??t?1,?
?t?1,?t,?t, ?, ?)=(?(2?)?(?)2)U?u?(?
+ nt,u,B)?(?
+ nt,u,K)?(2?
+ nt,u)??
(V ?)?(?)V?v?
(nt,B,v+ ?)?
(nt,B+ V ?)??k?(?t,k)?
(nt,k+ ?t,k)?v?
(nt,k,v+ ?t,k??t?1,k,v)?(?t,k??t?1,k,v)??u?(?t,u)?
(ct,u+ ?t,u)?k?
(ct,u,k+ ?t,u??t?1,u,k)?(?t,u?
?t?1,u,k),(4)where nt,u,Band nt,u,Kare the number of back-ground and topic words of user u at time t, nt,B,vis the number of times that word v is assigned asa background word at time t, nt,k,vis the num-ber of times that word v is assigned to topic kat time t, ct,u,kis the number of tweets assignedto topic k for user u at time t. In addition,nt,u= nt,u,B+ nt,uK, nt,B=?vnt,B,v, nt,K=?knt,k=?k?vnt,k,v, nt,u=?knt,u,k, andct,u=?kct,u,k.Given the assignment of all other latent vari-ables, we derive the following formula calculatedfrom eq.
(4) to infer a latent topic,p(zi= k|Dt, Yt, Zt\i,??t?1,?
?t?1,?t,?t)?ct,u,k\i+ ?t,u?
?t?1,u,kct,u\i+ ?t,u?
(nt,k\i+ ?t,k)?
(nt,k+ ?t,k)??v?
(nt,k,v+ ?t,k??t?1,k,v)?
(nt,k,v\i+ ?t,k?
?t?1,k,v), (5)where i = (t, u, s), thus zirepresents a topic as-signed to the s-th tweet of user u at time t, and \irepresents a count excluding the i-th tweet.Then, when zi= k is given, we derive the fol-lowing formula to infer a latent variable yj,p(yj= 0|Dt, Yt\j, Zt, ?, ?
)?nt,B,v\j+ ?nt,B\j+ V ?nt,u,B\j+ ?nt,u\j+ 2?, (6)p(yj= 1|Dt, Yt\j, Zt,?
?t?1,?t, ?
)?nt,k,v\j+ ?t,k?
?t?1,k,vnt,k\j+ ?t,knt,u,K\j+ ?nt,u\j+ 2?, (7)where j = (t, u, s, n), thus yjrepresents a latentvariable assigned to the n-th word in the s-th tweetof user u at time t, and \j represents a count ex-cluding the j-th word.The persistence parameters ?t and ?t are esti-mated by maximizing the joint likelihood eq.
(4),using a fixed point iteration (Minka, 2000).
Theupdate formulas are as follows:?newt,u= ?t,u?k??t?1,u,kAt,u,k?
(ct,u+ ?t,u)??
(?t,u),(8)where At,u,k= ?
(ct,u,k+ ?t,u?
?t?1,u,k) ??(?t,u?
?t?1,u,k), and?newt,k= ?t,k?v??t?1,k,vBt,k,v?
(nt,k+ ?t,k)??
(?t,k),(9)where Bt,k,v= ?
(nt,k,v+ ?t,k?
?t?1,k,v) ??(?t,k??t?1,k,v).
We can estimate latent variablesZt, Yt, and parameters ?t and ?t by iteratingGibbs sampling with eq.
(5), eq.
(6), and eq.
(7) andmaximum joint likelihood with eq.
(8) and eq.
(9).After the iterations, the means of ?t,u,kand ?t,k,vare obtained as follows.?
?t,u,k=ct,u,k+ ?t,u?
?t?1,u,kct,u+ ?t,u, (10)?
?t,k,v=nt,k,v+ ?t,k?
?t?1,k,vnt,k+ ?t,k.
(11)These estimates are used as the hyper parametersof the prior distributions at the next time periodt + 1.4 Related WorkRecently, topic models for Twitter have been pro-posed.
Diao et al.
(2012) proposed a topicmodel that considers both the temporal informa-tion of tweets and user?s personal interests.
Theyapplied their model to find bursty topics fromTwitter.
Yan et al.
(2013) proposed a bitermtopic model (BTM), which assumes that a word-pair is independently drawn from a specific topic.They demonstrated that BTM can effectively cap-ture the topics within short texts such as tweetscompared with LDA.
Chua and Asur (2013) pro-posed two topic models considering time orderand tweet intervals to extract the tweets summa-rizing a given event.
The models mentioned abovedo not consider the dynamics of user interests, nor19811.
Draw ?t,B?Dirichlet(?)2.
For each topic k = 1, ...,K,(a) draw ?t,k?Dirichlet(?t,k??t?1,k)3.
For each user u = 1, ..., U ,(a) draw ?t,u?Dirichlet(?t,u?
?t?1,u)(b) draw ?t,u?Beta(?
)(c) for each tweet s = 1, ..., Nui.
draw zt,u,s?Multinomial(?t,u)ii.
for each word n = 1, ..., Nu,sA.
draw yt,u,s,n?Bernoulli(?t,u)B. draw wt,u,s,n?Multinomial(?t,B) if yt,u,s,n= 0or Multinomial(?t,zt,u,s)if yt,u,s,n= 1Figure 3: Generative process of tweets in Twitter-TTMdo they have the capability of online inference;thus, they cannot efficiently model the large num-ber of tweets generated everyday, whereas Twitter-TTM can capture the dynamics of user interestsand topic trends and has the capability of onlineinference.Some online topic models have also been pro-posed.
TM-LDA was proposed by Wang et al.
(2012), which can efficiently model online the top-ics and topic transitions that naturally arise in atweet stream.
Their model learns the transitionparameters among topics by minimizing the pre-diction error on topic distribution in subsequenttweets.
However, the TM-LDA does not con-sider dynamic word distributions.
In other words,their model can not capture the dynamics of topictrends.
Lau et al.
(2012) proposed a topic modelimplementing a dynamic vocabulary based on on-line LDA (OLDA) (AlSumait et al., 2008) and ap-plied it to track emerging events on Twitter.
Anonline variational Bayes algorithm for LDA is alsoproposed (Hoffman et al., 2010).
However, thesemethods are based on LDA and do not considerthe shortness of a tweet.
Twitter-TTM tacklesthe shortness of a tweet by assuming that a singletweet consists of a single topic.
This assumptionis based on the following observation: a tweet ismuch shorter than a normal document, so a singletweet rarely contains multiple topics but rather asingle one.w??kKNu,s?zUNuy?
?B w?kKNu,s?zUNuy??B???
????
?t -1 tFigure 4: Graphical model of Twitter-TTM5 Experiment5.1 SettingWe evaluated the effectiveness of the proposedTwitter-TTM using an actual Twitter data set.
Theoriginal Twitter data set contains 15,962 users and4,146,672 tweets collected from October 18 to 31,2013.
We then removed words that occurred lessthan 30 times and stop words.
After this prepro-cessing, we obtained the final data set with 15,944users, 3,679,481 tweets, and 30,096 vocabularies.We compared the predictive performance ofTwitter-TTM with LDA, TTM, Twitter-LDA,Twitter-LDA+TTM, and the improved modelbased on the perplexity for the next time tweets.Twitter-LDA+TTM is a combination of Twitter-LDA and TTM.
It is equivalent to Twitter-TTM,except that the rate between background and topicwords is different for each user.
We set the num-ber of topics K at 100, the iteration of each modelat 500, and the unit time interval at one day.
Thehyper parameters in these models were optimizedin each Gibbs sampling iteration by maximizinglikelihood using fixed iterations (Minka, 2000).The inferences of LDA, Twitter-LDA, and the im-proved model were made for current time tweets.5.2 ResultFigure 5 shows the perplexity of each model foreach time, where t = 1 in the horizontal axis rep-resents October 18, t = 2 represents October 19,..., and t = 13 represents October 31.
The perplex-ity at time t represents the predictive performanceof each model inferred by previous time tweets to1982the current time tweets.
Note that at t = 1, the per-formance of LDA and TTM, that of Twitter-LDAand Twitter-LDA+TTM, and that of Twitter-TTMand the improved model were found to be equiva-lent.As shown in Figure 5(a), the proposed Twitter-TTM shows lower perplexity compared with con-ventional models, such as LDA, Twitter-LDA, andTTM at any time, which implies that Twitter-TTMcan appropriately model the dynamics of user in-terests and topic trends in Twitter.
TTM couldnot have perplexity lower than LDA although itconsiders the dynamics.
If LDA could not ap-propriately model the tweets, then the user inter-ests ?
?t?1and topic trends ?
?t?1in the previoustime are not estimated well in TTM.
Figure 5(b)shows the perplexities of the improved model andTwitter-TTM.
From t = 2, Twitter-TTM showslower perplexity than the improved model for eachtime.
The reason for the high perplexity of the im-proved model is that it does not consider the dy-namics.
Twitter-TTM also shows lower perplexitythan Twitter-LDA+TTM for each time, as shownin Figure 5(c), because Twitter-TTM?s assumptionthat the rate between background and topic wordsis different for each user is more appropriate, asdemonstrated in Section 2.2.
These results implythat Twitter-TTM also outperforms other conven-tional methods, such as DTM, OLDA, and TM-LDA, which do not consider the shortness of atweet or the dynamics of user interests or topictrends .Table 3 shows two topic examples of the topicevolution analyzed by Twitter-TTM, and Figure 6shows the trend persistence parameters ?
of eachtopic at each time.
The persistence parameters ofthe topic ?Football?
are lower than those of ?Birth-day?
because it is strongly affected by trends in thereal world.
In fact, the top words in ?Football?change more dynamically than those of ?Birth-day.?
For example, in the ?Football?
topic, though?Arsenal?
is usually popular, ?Madrid?
becomesmore popular on October 24.6 ConclusionWe first proposed an improved model basedon Twitter-LDA, which estimates the rate be-tween background and topic words for each user.We demonstrated that the improved model couldmodel tweets more efficiently than LDA andTwitter-LDA.
Next we proposed a novel proba-100015002000250030003500400045001 2 3 4 5 6 7 8 9 10 11 12 13perplexitytLDATwitter-LDATTMTwitter-TTM(a) Comparison with LDA, Twitter-LDA, and TTM100015002000250030003500400045001 2 3 4 5 6 7 8 9 10 11 12 13perplexitytImproved -ModelTwitter-TTM(b) Comparison with Improved-model100015002000250030003500400045001 2 3 4 5 6 7 8 9 10 11 12 13perplexitytTwitter-LDA+TTMTwitter-TTM(c) Comparison with Twitter-LDA+TTMFigure 5: Perplexity for each timebilistic topic model for Twitter, called Twitter-TTM, which can capture the dynamics of user in-terests and topic trends and is capable of onlineinference.
We evaluated Twitter-TTM using an ac-tual Twitter data set and demonstrated that it couldmodel more accurately tweets than conventional1983methods.The proposed method currently needs to prede-termine the number of topics each time, and it isfixed.
In future work, we plan to extend the pro-posed method to capture the birth and death oftopics along the timeline with a variable numberof topics, such as the model proposed by Ahmed(Ahmed and Xing, 2010).
We also plan to ap-ply the proposed method to content recommenda-tions and trend analysis in Twitter to investigatethis method further.ReferencesAmr Ahmed and Eric P. Xing.
2010.
Timeline: Adynamic hierarchical Dirichlet process model for re-covering birth/death and evolution of topics in textstream.
In Proceedings of the 26th Conference onUncertainty in Artificial Intelligence (UAI), 20?29.Loulwah AlSumait, Daniel Barbara?
and CarlottaDomeniconi.
2008.
On-line LDA: Adaptive topicmodels for mining text streams with applicationsto topic detection and tracking.
In Proceedings ofthe IEEE International Conference on Data Mining(ICDM), 3-12.David M.
Blei., and John D. Lafferty.
2006.
Dynamictopic models.
In Proceedings of the 23rd Inter-national Conference on Machine learning (ICML),113-120.David M. Blei, Andrew Y. Ng and Michael I. Jordan.2003.
Latent dirichlet allocation.
Journal of Ma-chine Learning Research, 3: 993-1022.Freddy C. T. Chua and Sitaram Asur.
2013.
Automaticsummarization of events from social media.
In Pro-ceedings of the International AAAI Conference onWeblogs and Social Media (ICWSM).Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim2012.
Finding bursty topics from microblogs.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), 536?544.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
In Proceedings of the NationalAcademy of Sciences of the United States of Amer-ica, 101(1):5228-5235.Matthew D. Hoffman, Francis Bach and David M. Blei.2010.
Online learning for latent dirichlet allocation.In Proceedings of the Advances in Neural Informa-tion Processing Systems (NIPS), 856?864.Liangjie Hong and Brian D. Davison.
2010.
Empiri-cal study of topic modeling in twitter.
In Proceed-ings of the First Workshop on Social Media Analyt-ics (SOMA), 80?88.Tomoharu Iwata, Shinji Watanabe, Takeshi Yamada.and Naonori Ueda.
2009.
Topic tracking model foranalyzing consumer purchase behavior.
In Proceed-ings of the International Joint Conferences on Arti-ficial Intelligence (IJCAI),1427?1432.JeyHan Lau, Nigel Collier and Timothy Baldwin.2012.
On-line trend analysis with topic models:#twitter trends detection topic model online.
In Pro-ceedings of the 23th International Conference onComputational Linguistics (COLING), 1519?1534.Thomas P. Minka 2000.
Estimating a Dirichlet distri-bution Technical report, MIT.Marco Pennacchiotti and Ana-Maria Popescu.
2011.A machine learning approach to Twitter user clas-sification.
In Proceedings of the International AAAIConference on Weblogs and Social Media (ICWSM),281?288.Takeshi Sakaki, Makoto Okazaki and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: realtimeevent detection by social sensors.
In Proceedings ofthe World Wide Web Conference (WWW), 851?860.Hanna M. Wallach 2006.
Topic modeling: beyondbag-of-words.
In Proceedings of the 23rd Inter-national Conference on Machine Learning (ICML),977?984.Xuerui Wang and Andrew McCallum.
2006.
Topicsover time: a non-Markov continuous-time model oftopical trends.
In Proceedings of the InternationalConference on Knowledge Discovery and Data Min-ing (KDD), 424?433.Yu Wang, Eugene Agichtein and Michele Benz.
2012.TM-LDA: efficient online modeling of the latenttopic transitions in social media.
In Proceedings ofthe International Conference on Knowledge Discov-ery and Data Mining (KDD), 123?131.Jianshu Weng, Ee Peng Lim, Jing Jiang and Qi He.2010.
Twitterrank: finding topic-sensitive influen-tial twitterers.
In Proceedings of the 3rd ACM Inter-national Conference on Web Search and Data Min-ing (WSDM), 261?270.Xiaohui Yan, Jiafeng Guo, Yanyan Lan and XueqiCheng 2013.
A biterm topic model for short texts.In Proceedings of the World Wide Web Conference(WWW), 1445?1456.Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,Ee-Peng Lim, Hongfei Yan and Xiaoming Li.
2011.Comparing twitter and traditional media using topicmodels.
In Advances in Information Retrieval, 338?349.1984010020030040050060010/19  10/20  10/21  10/22  10/23  10/24?dateBirthdayFootballFigure 6: Trend persistence parameters ?
of each topic at each time estimated by Twitter-TTMTable 3: Two examples of topic evolution analyzed by Twitter-TTMLabel Date Top wordsBirthday 10/18 birthday,happy,maria,hope,good,love,thanks,bday,lovely,enjoy10/19 happy,birthday,good,hope,thank,enjoy,love,bday,lovely,great10/20 birthday,happy,hope,good,love,lovely,great,enjoy,thank,beautiful10/21 birthday,happy,hope,good,beautiful,love,lovely,bday,great,thank10/22 birthday,happy,hope,good,beautiful,love,bless,thank,today,bday10/23 birthday,happy,thank,good,love,hope,beautiful,enjoy,channing,wish10/24 birthday,happy,thank,love,hope,good,beautiful,fresh,thanks,jamzFootball 10/18 arsenal,ozil,game,team,cazorla,league,wenger,play,season,good10/19 goal,liverpool,gerrard,arsenal,ozil,league,newcastle,suarez,goals,team10/20 arsenal,ozil,goal,ramsey,norwich,goals,league,wilshere,mesut,premier10/21 arsenal,goal,goals,league,townsend,spurs,player,season,wenger,ozil10/22 arsenal,goal,wenger,ozil,league,arsene,goals,birthday,happy,team10/23 arsenal,dortmund,ozil,fans,wilshere,borussia,ramsey,lewandowski,giroud,league10/24 madrid,goals,ronaldo,cska,real,league,city,moscow,champions,yaya1985
