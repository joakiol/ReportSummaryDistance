Rapid Prototyping of Scalable Grammars: Towards Modularity inExtensions to a Language-Independent CoreEmily M. BenderDepartment of LinguisticsUniversity of WashingtonBox 354340Seattle WA 98195-4340 USAebender@u.washington.eduDan FlickingerCenter for the Study of Language and InformationStanford UniversityStanford CA 94305-2150 USAdanf@csli.stanford.eduAbstractWe present a new way to simplify theconstruction of precise broad-coveragegrammars, employing typologically-motivated, customizable extensions toa language-independent core grammar.Each ?module?
represents a salient di-mension of cross-linguistic variation,and presents the grammar developerwith simple choices that result in auto-matically generated language-specificsoftware.
We illustrate the approach forseveral phenomena and explore the in-terdependence of the modules.1 IntroductionManual development of precise broad-coveragegrammar implementations, useful in a range ofnatural language processing/understanding tasks,is a labor-intensive undertaking, requiring manyyears of work by highly trained linguists.
Manyrecent efforts toward reducing the time and levelof expertise needed to produce a new grammarhave focused on adapting an existing grammar ofanother language (Butt et al, 2002; Kim et al,2003; Bateman et al, ip).
Our work on the ?Gram-mar Matrix?
has pursued an alternative approach,identifying a set of language-independent gram-mar constraints to which language-specific con-straints can be added (Bender et al, 2002).
Thisapproach has the hitherto unexploited potentialto benefit from the substantial theoretical workon language typology.
In this paper, we presenta prototype Grammar Matrix customization sys-tem.
This system draws on phenomenon-specificmodules encoding dimensions of linguistic varia-tion, presents the grammar developer with simplechoices for each phenomenon, and then automati-cally generates a working starter-grammar, incor-porating both the cross-linguistic Matrix core andlanguage-specific constraints.
The prototype ad-dresses basic word order, sentential negation, yes-no questions, and a small range of lexical entries.2 The Grammar MatrixWide-coverage grammars representing deep lin-guistic analysis exist in several frameworks, in-cluding Head-Driven Phrase Structure Grammar(HPSG), Lexical-Functional Grammar, and Lex-icalized Tree Adjoining Grammar.
In HPSG (P.and Sag, 1994), the most extensive grammarsare those of English (Flickinger, 2000), German(Hinrichs et al, 1997; Mu?ller and Kasper, 2000;Crysmann, ip), and Japanese (Siegel, 2000; Siegeland Bender, 2002).
The Grammar Matrix is an at-tempt to distill the wisdom of existing grammarsand document it in a form that can be used as thebasis for new grammars.
The main goals of theproject are: (i) to develop in detail semantic rep-resentations and the syntax-semantics interface,consistent with other work in HPSG; (ii) to repre-sent generalizations across linguistic objects andacross languages; and (iii) to allow for very quickstart-up as the Matrix is applied to new languages.The original Grammar Matrix consisted oftypes defining the basic feature geometry, typesassociated with Minimal Recursion Semantics(e.g., (Copestake et al, 2001)), types for lex-203ical and syntactic rules, and configuration filesfor the LKB grammar development environment(Copestake, 2002) and the PET system (Callmeier,2000).
Subsequent releases have refined the orig-inal types and developed a lexical hierarchy.
Theconstraints in this ?core?
Matrix are intended to belanguage-independent and monotonically exten-sible in any given grammar.
With the typology-based modules presented here, we extend the con-straint definitions which can be supplied to gram-mar developers to those that capture generaliza-tions holding only for subsets of languages.3 Typology-based modulesIn general, we find two kinds of typological vari-ation across languages.
On the one hand, thereare systems (formal or functional) which must berepresented in every language.
For example, ev-ery language has some set of permissible word or-ders (formal) and a means of expressing sententialnegation (functional).
On the other hand, thereare linguistic phenomena which appear in onlysome languages, and are not typically conceptual-ized as alternative realizations of some universalfunction, phenomena such as noun incorporation,numeral classifiers, and auxiliary verbs.
Each ofthese phenomena are found in recurring varietiesthat can be subjected to typological analysis (see,e.g., (Mithun, 1984)).
Our approach is designedto handle both kinds of typological variation.As with earlier versions of the Matrix, we aimto support rapid prototyping of precision gram-mars that can scale up to broad-coverage (as havethe NorSource (Hellan and Haugereid, 2003) andModern Greek (Kordoni and Neu, 2003) gram-mars, based on early versions of the Matrix).
Thissets a high bar for the modules themselves, requir-ing them to be good early approximations whichmay need to be refined but not thrown out.
It alsorequires that the automatically generated gram-mar files maintain a high degree of readability sothat they may be effectively modified.
In futurework, we intend to extend the system to allow thelinguist to revise decisions in the face of new in-formation or improved linguistic analyses.The core Matrix and modular extensions to itmay appear analogous to the Principles and Pa-rameters proposed by Chomsky (1981) and oth-ers.
However, whereas Parameters are meant tobe abstract ?switches?
which simultaneously con-trol multiple different, apparently unrelated phe-nomena, the modules in the Matrix each encodethe constraints necessary to handle one particu-lar phenomenon.
Nonetheless, this does not makethe modules trivial: they need to be carefully de-signed in order to be mutually consistent, ide-ally across all possible combinations.
Our strat-egy is thus consistent with a bottom-up, data-driven investigation of linguistic universals andconstraints on cross-linguistic variation.
As thenumber and breadth of implemented grammarsgrows, we expect linguistic predictions to emergeand become part of improved modules, particu-larly with respect to interactions among the dis-tinct phenomena covered.
Our approach should intime be instrumental in assisting large-scale typo-logical investigations (covering hundreds of lan-guages), making use of the linguistically preciseconstraints encoded in these modules to uncoverdeeper and more subtle facts about languages.4 Implementations of prototype systemWe have implemented a prototype system witha small set of modules targeting basic word or-der, main-clause yes-no questions, and senten-tial negation.1 The corresponding choices anda questionnaire for creating a small lexicon arepresented to the user through an html form inter-face.
A perl/cgi back-end produces a starter gram-mar from the user input and an internal knowl-edge base.
The resulting grammars can be usedimmediately to parse and generate a fragment ofthe target language.
The system can be accessedat http://www.delph-in.net/matrix/modules.html.This section describes its linguistic coverage.4.1 Word orderThe word order module addresses the so-calledbasic word order in a language: the relative or-der of subjects, verbs, and verbal complements.Languages vary in their rigidity in this respect,and the question of how to determine the basicword-order of a language is notoriously complex.Nonetheless, we believe that most linguists work-ing on linguistic description analyze some ordersas primary and others as derived.
Thus the word1Drellishak and Bender (ta) present a module for coordi-nation which is integrated with those described here.204order module is meant to capture the relative or-dering of major constituents in clauses withoutword-order changing phenomena such as topical-ization, extraposition, subject-verb inversion, etc.Modules for such phenomena will need to interactappropriately with the basic word-order module.The Matrix core grammar provides defini-tions of basic head-complement and head-subjectschemata which are consistent with our imple-mentation of compositional semantics (Flickingerand Bender, 2003), as well as definitions of head-initial and head-final phrase types.
The wordorder module creates subtypes joining the head-complement and head-subject schemata with thetypes specifying head/dependent order, creates in-stances of those types as required by the LKBparser, and constrains the rules to eliminate spu-rious ambiguity in the case of free word order.
Itcurrently handles SOV, SVO, VSO, VOS, OVS,OSV, V-final, V-initial, and free word order.
Weleave to future work variations such as V2 or-der, differing word order in main v. subordinateclauses, and flexible ordering among comple-ments in otherwise strict word order languages.4.2 Yes-no questionsFor yes-no questions, we implement four alterna-tives: inversion of the subject and a main or aux-iliary verb relative to declarative word order andsentence-initial or final question particles.Inversion of the subject and the main verb isimplemented with a lexical rule which relocatesthe subject (the value of SUBJ in the valence spec-ifications) to be the first on the COMPS list, andfurther assigns a positive value for an additionalfeature INV (inverted) on verbs.
This feature maywell have independent syntactic motivation in thelanguage, but is in any case used here so thedeclarative/interrogative distinction can be madein the semantics once the clause is constructed.Subject-aux inversion is a minor extension of thebasic inversion type, constraining the lexical ruleto only apply to auxiliary verbs.
This module han-dles ?support?
verbs like do in English in not li-censing inversion with main verbs, while licens-ing similar strings with a semantically empty sup-port verb (if it is in the lexicon).
The third type ofmechanism employs a distinct question particle,here treated as a pre- or post-modifying sentenceadverb.
The grammar developer is prompted forthis positional distinction, and for the spelling ofthe particle; the code for the relevant lexical en-try is then autogenerated, instantiating a questionparticle type which supplies the remaining syn-tactic and semantic constraints needed.Future work on this module includes supportfor ?intonation questions?, where the same stringcan be associated with either proposition or ques-tion semantics, as well as the integration ofdeclarative/interrogative punctuation contrasts.4.3 Sentential negationThe sentential negation module handles two gen-eral negation strategies, several variants on each,and allows for both to coexist in a single grammar.The first strategy is negation via verbal inflec-tion.
For this strategy, the grammar developerspecifies whether the inflection attaches to mainverbs, auxiliaries, or either; whether it is a prefixor a suffix; and the form of the affix.
We cur-rently only allow for strictly concatenative mor-phology.
In a more fully developed system, thesyntax-semantics modules here would be inter-faced with a separate means of specifying mor-phophonology (cf.
(Bender and Good, ip)).The second strategy is negation via a negativeadverb, with two sub-cases: The negative adverbmay be an independent modifier (of V, VP, or Sand pre- or post-head) or it may be a selectedcomplement of the verb (main verbs only, aux-iliaries only, or both) (Kim, 2000).
The grammardeveloper specifies the form of the adverb.Neither, either or both of these strategies maybe selected.
If neither, the grammar produced willnot contain an analysis of negation.
If both, thegrammar developer must specify how the strate-gies interact, from among five choices: (i) the twostrategies are in complementary distribution, (ii)the two strategies can appear independently or to-gether, (iii) both inflection and an adverb are re-quired to express sentential negation, (iv) the ad-verb is obligatory, but it may appear with or with-out the inflection, and (v) the inflection is obliga-tory, but it may appear with or without the adverb.In the generated grammars, independent ad-verbs are implemented by adding appropriate lex-ical types and lexical entries.
Selected adverbsand inflection are handled via lexical rules similar205to those presented in (Sag et al, 2003).
For exam-ple, in a language where sentential negation canbe expressed by inflection alone or inflection incombination with a (selected) adverb, we gener-ate two lexical rules.
One changes the form of theverb and adds the negative semantics.
The otherchanges the form of the verb and adds the nega-tive adverb to its complements list.4.4 LexiconAs HPSG is a strongly lexicalist theory, wordstend to carry quite a bit of information.
Thisinformation is encoded in lexical types; lexicalentries merely specify the type they instantiate,their orthographic form, and their semantic predi-cate.
Many of the constraints required (e.g., forthe linking of syntactic to semantic arguments)are already provided by the core Matrix.
How-ever, there is also cross-linguistic variation.We ask the grammar developer to specify twonouns and two verbs (one transitive and one in-transitive), as well as an auxiliary, two deter-miners, two case-marking adpositions, a nega-tive adverb and a question particle, if appropriate.Nouns are specified as to whether they require,allow, or disallow determiners.
Verbs are speci-fied as to whether each argument is expressed asan NP or a PP, and optionally for an additional(non-finite) form.
Auxiliaries are specified as towhether they introduce independent predicates oronly carry tense/aspect; take S, VP or V com-plements; appear to the left, right or either sideof their complements; and take NP or PP sub-jects.
Case-marking adpositions must be specifiedas either prepositions or postpositions.
Finally,the questionnaire requires orthographic forms andpredicate names.
Note that the forms are assumedto be fully inflected (modulo negation), supportmorphological processes awaiting future work.We use this information and the knowledgebase to produce a set of lexical types inherit-ing from the types defined in the core Matrixand specifying appropriate language-specific con-straints, and a set of lexical entries.5 Limits of modularityRecent computational work in HPSG has askedwhether different parts of a single grammar canbe abstracted into separate, independent mod-ules, either for processing (Kasper and Krieger,1996; Theofilidis et al, 1997) or grammar devel-opment (Kes?elj, 2001).
Our work is most simi-lar to Kes?elj?s though we are pursuing differentgoals: Kes?elj is looking to support a division oflabor among multiple individuals working on thesame grammar and to support variants of a singlegrammar for different domains.
His modules eachhave private and public features and types, and heillustrates the approach with a small-scale ques-tion answering system.
In contrast, we are ap-proaching this issue from the perspective of reuseof grammar code in the context of multilingualgrammar engineering (a possibility suggested, butnot developed, by Theofilidis et al.Our notion of modularity is influenced by thefollowing constraints: (i) The questions in thecustomization interface must be sensible to theworking linguist; (ii) The resulting starter gram-mars must be highly readable so that they canbe extended by the grammar developer (typicallyonly one per grammar); and (iii) HPSG prac-tice values capturing linguistic generalizations byhaving single types encode many different con-straints and, ideally, single constraints contributeto the analysis of many different phenomena.Even with the modest linguistic coverage ofthe existing system, we have found many casesof non-trivial interaction between the modules:Our phrase structure rules, following HPSG prac-tice, capture cross-categorial generalizations: ifboth verbs and adpositions follow their comple-ments, then a single complement-head rule servesfor both.
However, few languages (if any) arecompletely consistent in their ordering of headsand dependents.
Thus, before defining the typesand instances for these rules, we must determinewhether the fragment requires auxiliaries (fornegation or yes-no questions) or case-marking ad-positions, and whether their order with respect totheir complements is consistent with that of mainverbs.
A second example is the lexical type formain verbs, whose definition depends on whetherthe language has auxiliaries (requiring a featureAUX distinguishing the two kinds of verbs anda feature FORM governing the distribution of fi-nite and non-finite verbs).
As a third example, thenegation and question modules each have optionsrequiring auxiliaries, but we must posit the asso-206ciated types and constraints at most once.Thus we find that, for our purposes, the relevantnotion of modularity is modularity from the pointof view of the linguist who uses the system to cre-ate a starter grammar.
To support this, we strive tomake the questions we ask of the linguist be as in-dependent of each other as possible, and to makeit clear when one particular choice (e.g., negationas inflection) requires further information (suffixv.
prefix).
The fact that the questions we presentto the linguist don?t correspond to neatly isolatedparts of the underlying knowledge base is not afailure of the approach, but rather a reflection ofthe complexity of language.
The very intercon-nectedness of grammatical phenomena is at theheart of research in theoretical syntax.
We in-tend our system to provide a data-driven cross-linguistic exploration of that interconnection.6 Validation of prototype systemTo verify the mutual consistency of the mod-ules developed so far and to illustrate their ap-plicability to a interesting range of languages,we developed abstract test suites for seven lan-guages.
This convenience sample of languages isnot representative, either typologically or geneti-cally.
The grammatical and ungrammatical exam-ples in each test suite use a small, artificial lexi-con, and reflect the typological properties of eachlanguage along the dimensions of basic word or-der, sentential negation, and yes-no questions (Ta-ble 1).
Table 2 presents the performance of eachgrammar (as generated by our prototype systemwith appropriate input) on its associated test suite.Language2 Order Negation Yes-no Q3English SVO aux-selected adv aux invHindi SOV pre-V adv S-init part.Japanese V-final verbal suffix S-final partMandarin SVO pre-V adv S-final part,A-not-APolish free pre-V adv S-init partSlave SOV post-V adv S-init partSpanish SVO pre-V adv main V invTable 1: Languages used in testingWhile these test suites are quite modest, we be-lieve they show that the prototype system is able2Sources: Hindi: Snell and Weightman, 2000, Mandarin:Li and Thompson, 1981, Polish: Adam Przepio?rkowski, p.c.,Slave (Athabaskan): Rice, 19893In addition to intonation questions, if any.Language Pos.
Coverage Neg.
Overgen.English 5 100% 10 10%Hindi 5 100% 10 0%Japanese 6 100% 10 0%Mandarin 4 75% 9 0%Polish 14 100% 8 0%Slave 3 100% 6 0%Spanish 5 100% 7 0%Table 2: Parsing evaluation resultsto produce good first-pass grammar fragments foran interesting variety of languages.
More study isneeded to develop a means of testing the cross-compatibility of all choices on all modules, toevaluate the coverage against a typologically jus-tified sample, and to gauge the success of thisstrategy in producing grammars which are com-prehensible to beginning grammar developers.7 Conclusion and outlookWe have described a method for extendinga language-independent core grammar like theGrammar Matrix with modules handling cross-linguistically variable but still recurring patterns.This method allows for extremely rapid prototyp-ing of deep precision grammars in such a waythat the prototypes themselves can serve as thebasis for sustained development.
We envision atleast four potential uses for this kind of grammarprototyping: (i) in pedagogical contexts, whereit would allow grammar engineering students tomore quickly work on cutting-edge problems, (ii)in language documentation, where a documen-tary linguist in the field might be collaboratingremotely with a grammar engineer to propose andtest hypotheses, (iii) in leveraging the results fromeconomically powerful languages to reduce thecost of creating resources for minority languages,and (iv) in supporting typological or comparativestudies of linguistic phenomena or interactionsbetween phenomena across languages.AcknowledgmentsWe thank Scott Drellishak, Stephan Oepen, Lau-rie Poulson, and the 2004 and 2005 multilingualgrammar engineering classes at the University ofWashington for valuable input and NTT Com-munication Science Laboratories for their supportthrough a grant to CSLI (Stanford).
All remainingerrors are our own.207ReferencesJ.A.
Bateman, I.
Kruijff-Korbayova?, and G.-J.
Krui-jff.
ip.
Multilingual resource sharing across bothrelated and unrelated languages: an implemented,open-source framework for practical natural lan-guage generation.
Res.
on Lang.
and Computation.E.M.
Bender and J.
Good.
ip.
Implementation fordiscovery: A bipartite lexicon to support morpho-logical and syntactic analysis.
In CLS 41.E.M.
Bender, D. Flickinger, and S. Oepen.
2002.The grammar matrix.
COLING 2002 Workshop onGrammar Engineering and Evaluation.M.
Butt, H. Dyvik, T.H.
King, H. Masuichi, andC.
Rohrer.
2002.
The parallel grammar project.
InCOLING 2002 Workshop on Grammar Engineeringand Evaluation.U.
Callmeier.
2000.
PET ?
A platform for ex-perimentation with efficient HPSG processing tech-niques.
Natural Lang.
Engineering, 6 (1):99 ?
108.N.
Chomsky.
1981.
Lectures on Government andBinding.
Foris, Dordrecht.A.
Copestake, A. Lascarides, and D. Flickinger.
2001.An algebra for semantic construction in constraint-based grammars.
In ACL 2001.A.
Copestake.
2002.
Implementing Typed FeatureStructure Grammars.
CSLI, Stanford, CA.B.
Crysmann.
ip.
Relative clause extraposition in ger-man: An efficient and portable implementation.
Re-search on Lang.
and Computation.S.
Drellishak and E.M. Bender.
ta.
Coordinationmodules for a crosslinguistic grammar resource.
InProc.
of HPSG 2005.D.
Flickinger and E.M. Bender.
2003.
Compositionalsemantics in a multilingual grammar resource.
InProc.
of the Workshop on Ideas and Strategies forMultilingual Grammar Development, ESSLLI 2003,pages 33?42.D.
Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural Lang.
En-gineering, 6 (1):15 ?
28.L.
Hellan and P. Haugereid.
2003.
Norsource: Anexercise in matrix grammar-building design.
InProc.
of the Workshop on Ideas and Strategies forMultilingual Grammar Development, ESSLLI 2003,pages 41?48.W.D.
Hinrichs, E.and Meurers, F. Richter, M. Sailer,and H. Winhart.
1997.
Ein HPSG-Fragement desDeutschen.
Arbeitspapiere des Sonderforschungs-bereichs 340, Bericht Nr.
95.W.
Kasper and H.-U.
Krieger.
1996.
Modularizingcodescriptive grammars for efficient parsing.
InCOLING 1996, pages 628?633.V.
Kes?elj.
2001.
Modular HPSG.
Technical ReportCS-2001-05, Department of Computer Science,University of Waterloo, Waterloo, Ont., Canada.R.
Kim, M. Dalrymple, R.M.
Kaplan, T.H.
King,H.
Masuichi, and T. Ohkuma.
2003.
Multlingualgrammar development via grammar porting.
InProc.
of the Workshop on Ideas and Strategies forMultilingual Grammar Development, ESSLLI 2003,pages 49?56.J.
Kim.
2000.
The Grammar of Negation: AConstraint-Based Approach.
CSLI, Stanford, CA.V.
Kordoni and J. Neu.
2003.
Deep gramamr develop-ment for Modern Greek.
In Proc.
of the Workshopon Ideas and Strategies for Multilingual GrammarDevelopment, ESSLLI 2003, pages 65?72.C.N.
Li and S.A. Thompson.
1981.
Mandarin Chi-nese: A Functional Reference Grammar.
Univer-sity of California Press, Berkeley, CA.M.
Mithun.
1984.
The evolution of noun incorpora-tion.
Language, 60(4):847?894.S.
Mu?ller and W. Kasper.
2000.
HPSG analy-sis of German.
In W. Wahlster, editor, Verbmo-bil.
Foundations of Speech-to-Speech Translation,pages 238 ?
253.
Springer, Berlin, Germany.Carl P. and I.A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
The Univeristy of ChicagoPress, Chicago, IL.K.
Rice.
1989.
A Grammar of Slave.
Mouton deGruyter, Berlin.I.A.
Sag, T. Wasow, and E.M. Bender.
2003.
SynacticTheory: A Formal Introduction.
CSLI, Stanford,CA, 2nd edition.M.
Siegel and E.M. Bender.
2002.
Efficient deepprocessing of Japanese.
In Proc.
of the 3rd Work-shop on Asian Language Resources and Interna-tional Standardization at COLING 2002.M.
Siegel.
2000.
HPSG analysis of Japanese.In W. Wahlster, editor, Verbmobil.
Foundationsof Speech-to-Speech Translation, pages 265 ?
280.Springer, Berlin, Germany.R.
Snell and S. Weightman.
2000.
Hindi.
Teach Your-self Books.A.
Theofilidis, P. Schmidt, and T. Declerck.
1997.Grammar modularization for efficient processing:Language engineering devices and their instantia-tions.
In Proc.
of the DGFS/CL.208
