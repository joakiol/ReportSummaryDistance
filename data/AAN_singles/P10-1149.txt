Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1473?1481,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsExperiments in Graph-based Semi-Supervised Learning Methods forClass-Instance AcquisitionPartha Pratim Talukdar?Search Labs, Microsoft ResearchMountain View, CA 94043partha@talukdar.netFernando PereiraGoogle, Inc.Mountain View, CA 94043pereira@google.comAbstractGraph-based semi-supervised learning(SSL) algorithms have been successfullyused to extract class-instance pairs fromlarge unstructured and structured text col-lections.
However, a careful comparisonof different graph-based SSL algorithmson that task has been lacking.
We com-pare three graph-based SSL algorithmsfor class-instance acquisition on a varietyof graphs constructed from different do-mains.
We find that the recently proposedMAD algorithm is the most effective.
Wealso show that class-instance extractioncan be significantly improved by addingsemantic information in the form ofinstance-attribute edges derived froman independently developed knowledgebase.
All of our code and data will bemade publicly available to encouragereproducible research in this area.1 IntroductionTraditionally, named-entity recognition (NER) hasfocused on a small number of broad classes suchas person, location, organization.
However, thoseclasses are too coarse to support important ap-plications such as sense disambiguation, seman-tic matching, and textual inference in Web search.For those tasks, we need a much larger inventoryof specific classes and accurate classification ofterms into those classes.
While supervised learn-ing methods perform well for traditional NER,they are impractical for fine-grained classificationbecause sufficient labeled data to train classifiersfor all the classes is unavailable and would be veryexpensive to obtain.?
Research carried out while at the University of Penn-sylvania, Philadelphia, PA, USA.To overcome these difficulties, seed-based in-formation extraction methods have been devel-oped over the years (Hearst, 1992; Riloff andJones, 1999; Etzioni et al, 2005; Talukdar etal., 2006; Van Durme and Pas?ca, 2008).
Start-ing with a few seed instances for some classes,these methods, through analysis of unstructuredtext, extract new instances of the same class.
Thisline of work has evolved to incorporate ideas fromgraph-based semi-supervised learning in extrac-tion from semi-structured text (Wang and Cohen,2007), and in combining extractions from freetext and from structured sources (Talukdar et al,2008).
The benefits of combining multiple sourceshave also been demonstrated recently (Pennac-chiotti and Pantel, 2009).We make the following contributions:?
Even though graph-based SSL algorithmshave achieved early success in class-instanceacquisition, there is no study comparing dif-ferent graph-based SSL methods on this task.We address this gap with a series of experi-ments comparing three graph-based SSL al-gorithms (Section 2) on graphs constructedfrom several sources (Metaweb Technolo-gies, 2009; Banko et al, 2007).?
We investigate whether semantic informa-tion in the form of instance-attribute edgesderived from an independent knowledgebase (Suchanek et al, 2007) can improveclass-instance acquisition.
The intuition be-hind this is that instances that share attributesare more likely to belong to the same class.We demonstrate that instance-attribute edgessignificantly improve the accuracy of class-instance extraction.
In addition, useful class-attribute relationships are learned as a by-product of this process.?
In contrast to previous studies involving pro-1473prietary datasets (Van Durme and Pas?ca,2008; Talukdar et al, 2008; Pennacchiottiand Pantel, 2009), all of our experiments usepublicly available datasets and we plan to re-lease our code1.In Section 2, we review three graph-basedSSL algorithms that are compared for the class-instance acquisition task in Section 3.
In Section3.6, we show how additional instance-attributebased semantic constraints can be used to improveclass-instance acquisition performance.
We sum-marize the results and outline future work in Sec-tion 4.2 Graph-based SSLWe now review the three graph-based SSL algo-rithms for class inference over graphs that we haveevaluated.2.1 NotationAll the algorithms compute a soft assignment oflabels to the nodes of a graph G = (V,E,W ),where V is the set of nodes with |V | = n, E isthe set of edges, and W is an edge weight ma-trix.
Out of the n = nl + nu nodes in G, nlnodes are labeled, while the remaining nu nodesare unlabeled.
If edge (u, v) 6?
E, Wuv = 0.The (unnormalized) Laplacian, L, ofG is given byL = D?W , whereD is an n?n diagonal degreematrix with Duu =?vWuv.
Let S be an n ?
ndiagonal matrix with Suu = 1 iff node u ?
V islabeled.
That is, S identifies the labeled nodes inthe graph.
C is the set of labels, with |C| = mrepresenting the total number of labels.
Y is then ?
m matrix storing training label information,if any.
Y?
is an n ?m matrix of soft label assign-ments, with Y?vl representing the score of label lon node v. A graph-based SSL computes Y?
from{G,SY }.2.2 Label Propagation (LP-ZGL)The label propagation method presented by Zhuet al (2003), which we shall refer to as LP-ZGLin this paper, is one of the first graph-based SSLmethods.
The objective minimized by LP-ZGL is:minY??l?CY?
>l LY?l, s.t.
SYl = SY?l (1)1http://www.talukdar.net/datasets/class inst/where Y?l of size n ?
1 is the lth column of Y?
.The constraint SY = SY?
makes sure that the su-pervised labels are not changed during inference.The above objective can be rewritten as:?l?CY?
>l LY?l =?u,v?V,l?CWuv(Y?ul ?
Y?vl)2From this, we observe that LP-ZGL penalizes anylabel assignment where two nodes connected by ahighly weighted edge are assigned different labels.In other words, LP-ZGL prefers smooth labelingsover the graph.
This property is also shared by thetwo algorithms we shall review next.
LP-ZGL hasbeen the basis for much subsequent work in thegraph-based SSL area, and is still one of the mosteffective graph-based SSL algorithms.2.3 AdsorptionAdsorption (Baluja et al, 2008) is a graph-basedSSL algorithm which has been used for open-domain class-instance acquisition (Talukdar et al,2008).
Adsorption is an iterative algorithm, wherelabel estimates on node v in the (t+ 1)th iterationare updated using estimates from the tth iteration:Y?
(t+1)v ?
pinjv ?Yv+pcontv ?B(t)v +pabndv ?r (2)where,B(t)v =?uWuv?u?
Wu?vY?
(t)uIn (2), pinjv , pcontv , and pabndv are three proba-bilities defined on each node v ?
V by Ad-sorption; and r is a vector used by Adsorptionto express label uncertainty at a node.
On eachnode v, the three probabilities sum to one, i.e.,pinjv + pcontv + pabndv = 1, and they are based onthe random-walk interpretation of the Adsorptionalgorithm (Talukdar et al, 2008).
The main ideaof Adsorption is to control label propagation moretightly by limiting the amount of information thatpasses through a node.
For instance, Adsorptioncan reduce the importance of a high-degree nodev during the label inference process by increas-ing pabndv on that node.
For more details on these,please refer to Section 2 of (Talukdar and Cram-mer, 2009).
In contrast to LP-ZGL, Adsorptionallows labels on labeled (seed) nodes to change,which is desirable in case of noisy input labels.14742.4 Modified Adsorption (MAD)Talukdar and Crammer (2009) introduced a modi-fication of Adsorption called MAD, which sharesAdsorption?s desirable properties but can be ex-pressed as an unconstrained optimization problem:minY?
?l?C[?1(Yl ?
Y?l)>S(Yl ?
Y?l)+?2Y?>l L?Y?l + ?3?????
?Y?l ?Rl?????
?2](3)where ?1, ?2, and ?3 are hyperparameters; L?is the Laplacian of an undirected graph derivedfrom G, but with revised edge weights; and R isan n ?
m matrix of per-node label prior, if any,with Rl representing the lth column of R. As inAdsorption, MAD allows labels on seed nodes tochange.
In case of MAD, the three random-walkprobabilities, pinjv , pcontv , and pabndv , defined byAdsorption on each node are folded inside the ma-trices S,L?, andR, respectively.
The optimizationproblem in (3) can be solved with an efficient iter-ative algorithm described in detail by Talukdar andCrammer (2009).These three algorithms are all easily paralleliz-able in a MapReduce framework (Talukdar et al,2008; Rao and Yarowsky, 2009), which makesthem suitable for SSL on large datasets.
Addition-ally, all three algorithms have similar space andtime complexity.3 ExperimentsWe now compare the experimental performanceof the three graph-based SSL algorithms reviewedin the previous section, using graphs constructedfrom a variety of sources described below.
Fol-lowing previous work (Talukdar et al, 2008), weuse Mean Reciprocal Rank (MRR) as the evalua-tion metric in all experiments:MRR =1|Q|?v?Q1rv(4)where Q ?
V is the set of test nodes, and rv is therank of the gold label among the labels assigned tonode v. Higher MRR reflects better performance.We used iterative implementations of the graph-based SSL algorithms, and the number of itera-tions was treated as a hyperparameter which wastuned, along with other hyperparameters, on sep-arate held-out sets, as detailed in a longer versionof this paper.
Statistics of the graphs used duringexperiments in this section are presented in Table1.3.1 Freebase-1 Graph with Pantel ClassesTable ID: people-personName Place of Birth Gender?
?
?
?
?
?
?
?
?Isaac Newton Lincolnshire MaleBob Dylan Duluth MaleJohnny Cash Kingsland Male?
?
?
?
?
?
?
?
?Table ID: film-music contributorName Film Music Credits?
?
?
?
?
?Bob Dylan No Direction Home?
?
?
?
?
?Figure 1: Examples of two tables from Freebase,one table is from the people domain while theother is from the film domain.0.50.5750.650.7250.823 x 2 23 x 10Freebase-1 Graph, 23 Pantel ClassesMeanReciprocalRank(MRR)Amount of Supervision (# classes x seeds per class)LP-ZGL Adsorption MADFigure 3: Comparison of three graph transductionmethods on a graph constructed from the Freebasedataset (see Section 3.1), with 23 classes.
All re-sults are averaged over 4 random trials.
In eachgroup, MAD is the rightmost bar.Freebase (Metaweb Technologies, 2009)2 isa large collaborative knowledge base.
Theknowledge base harvests information from manyopen data sets (for instance Wikipedia and Mu-sicBrainz), as well as from user contributions.
Forour current purposes, we can think of the Freebase2http://www.freebase.com/1475Graph Vertices Edges Avg.
Min.
Max.Deg.
Deg.
Deg.Freebase-1 (Section 3.1) 32970 957076 29.03 1 13222Freebase-2 (Section 3.2) 301638 2310002 7.66 1 137553TextRunner (Section 3.3) 175818 529557 3.01 1 2738YAGO (Section 3.6) 142704 777906 5.45 0 74389TextRunner + YAGO (Section 3.6) 237967 1307463 5.49 1 74389Table 1: Statistics of various graphs used in experiments in Section 3.
Some of the test instances in theYAGO graph, added for fair comparison with the TextRunner graph in Section 3.6, had no attributes inYAGO KB, and hence these instance nodes had degree 0 in the YAGO graph.Bob Dylanfilm-music_contributor-nameJohnnyCashpeople-person-nameIsaac NewtonBob Dylanfilm-music_contributor-nameJohnnyCashpeople-person-nameIsaac Newtonhas_attribute:albums(a) (b)Figure 2: (a) Example of a section of the graph constructed from the two tables in Figure 1.
Rectangularnodes are properties, oval nodes are entities or cell values.
(b) The graph in part (a) augmented withan attribute node, has attribue:albums, along with the edges incident on it.
This results is additionalconstraints for the nodes Johnny Cash and Bob Dylan to have similar labels (see Section 3.6).dataset as a collection of relational tables, whereeach table is assigned a unique ID.
A table con-sists of one or more properties (column names)and their corresponding cell values (column en-tries).
Examples of two Freebase tables are shownin Figure 1.
In this figure, Gender is a propertyin the table people-person, and Male is a corre-sponding cell value.
We use the following processto convert the Freebase data tables into a singlegraph:?
Create a node for each unique cell value?
Create a node for each unique property name,where unique property name is obtained byprefixing the unique table ID to the prop-erty name.
For example, in Figure 1, people-person-gender is a unique property name.?
Add an edge of weight 1.0 from cell-valuenode v to unique property node p, iff valuev is present in the column corresponding toproperty p. Similarly, add an edge in the re-verse direction.By applying this graph construction process onthe first column of the two tables in Figure 1, weend up with the graph shown in Figure 2 (a).
Wenote that even though the resulting graph consistsof edges connecting nodes of different types: cellvalue nodes to property nodes; the graph-basedSSL methods (Section 2) can still be applied onsuch graphs as a cell value node and a propertynode connected by an edge should be assignedsame or similar class labels.
In other words, the la-bel smoothness assumption (see Section 2.2) holdson such graphs.We applied the same graph construction pro-cess on a subset of the Freebase dataset consist-ing of topics from 18 randomly selected domains:astronomy, automotive, biology, book, business,1476chemistry, comic books, computer, film, food, ge-ography, location, people, religion, spaceflight,tennis, travel, and wine.
The topics in this subsetwere further filtered so that only cell-value nodeswith frequency 10 or more were retained.
We callthe resulting graph Freebase-1 (see Table 1).Pantel et al (2009) have made availablea set of gold class-instance pairs derivedfrom Wikipedia, which is downloadable fromhttp://ow.ly/13B57.
From this set, we selectedall classes which had more than 10 instancesoverlapping with the Freebase graph constructedabove.
This resulted in 23 classes, which alongwith their overlapping instances were used as thegold standard set for the experiments in this sec-tion.Experimental results with 2 and 10 seeds (la-beled nodes) per class are shown in Figure 3.
Fromthe figure, we see that that LP-ZGL and Adsorp-tion performed comparably on this dataset, withMAD significantly outperforming both methods.3.2 Freebase-2 Graph with WordNet Classes0.250.2850.320.3550.39192 x 2 192 x 10Freebase-2 Graph, 192 WordNet ClassesMeanReciprocalRank(MRR)Amount of Supervision (# classes x seeds per class)LP-ZGL Adsorption MADFigure 4: Comparison of graph transduction meth-ods on a graph constructed from the Freebasedataset (see Section 3.2).
All results are averagedover 10 random trials.
In each group, MAD is therightmost bar.To evaluate how the algorithms scale up, weconstruct a larger graph from the same 18 domainsas in Section 3.1, and using the same graph con-struction process.
We shall call the resulting graphFreebase-2 (see Table 1).
In order to scale up thenumber of classes, we selected all Wordnet (WN)classes, available in the YAGO KB (Suchanek etal., 2007), that had more than 100 instances over-lapping with the larger Freebase graph constructedabove.
This resulted in 192 WN classes which weuse for the experiments in this section.
The reasonbehind imposing such frequency constraints dur-ing class selection is to make sure that each classis left with a sufficient number of instances duringtesting.Experimental results comparing LP-ZGL, Ad-sorption, and MAD with 2 and 10 seeds per classare shown in Figure 4.
A total of 292k test nodeswere used for testing in the 10 seeds per class con-dition, showing that these methods can be appliedto large datasets.
Once again, we observe MADoutperforming both LP-ZGL and Adsorption.
It isinteresting to note that MAD with 2 seeds per classoutperforms LP-ZGL and adsorption even with 10seeds per class.3.3 TextRunner Graph with WordNetClasses0.150.20.250.30.35170 x 2 170 x 10TextRunner Graph, 170 WordNet ClassesMeanReciprocalRank(MRR)Amount of Supervision (# classes x seeds per class)LP-ZGL Adsorption MADFigure 5: Comparison of graph transduction meth-ods on a graph constructed from the hypernym tu-ples extracted by the TextRunner system (Bankoet al, 2007) (see Section 3.3).
All results are aver-aged over 10 random trials.
In each group, MADis the rightmost bar.In contrast to graph construction from struc-tured tables as in Sections 3.1, 3.2, in this sectionwe use hypernym tuples extracted by TextRun-ner (Banko et al, 2007), an open domain IE sys-tem, to construct the graph.
Example of a hyper-nym tuple extracted by TextRunner is (http, proto-col, 0.92), where 0.92 is the extraction confidence.To convert such a tuple into a graph, we create anode for the instance (http) and a node for the class(protocol), and then connect the nodes with two1477directed edges in both directions, with the extrac-tion confidence (0.92) as edge weights.
The graphcreated with this process from TextRunner out-put is called the TextRunner Graph (see Table 1).As in Section 3.2, we use WordNet class-instancepairs as the gold set.
In this case, we consideredall WordNet classes, once again from YAGO KB(Suchanek et al, 2007), which had more than 50instances overlapping with the constructed graph.This resulted in 170 WordNet classes being usedfor the experiments in this section.Experimental results with 2 and 10 seeds perclass are shown in Figure 5.
The three methodsare comparable in this setting, with MAD achiev-ing the highest overall MRR.3.4 DiscussionIf we correlate the graph statistics in Table 1 withthe results of sections 3.1, 3.2, and 3.3, we seethat MAD is most effective for graphs with highaverage degree, that is, graphs where nodes tendto connect to many other nodes.
For instance,the Freebase-1 graph has a high average degreeof 29.03, with a corresponding large advantagefor MAD over the other methods.
Even thoughthis might seem mysterious at first, it becomesclearer if we look at the objectives minimizedby different algorithms.
We find that the objec-tive minimized by LP-ZGL (Equation 1) is under-regularized, i.e., its model parameters (Y? )
are notconstrained enough, compared to MAD (Equation3, specifically the third term), resulting in overfit-ting in case of highly connected graphs.
In con-trast, MAD is able to avoid such overfitting be-cause of its minimization of a well regularized ob-jective (Equation 3).
Based on this, we suggestthat average degree, an easily computable struc-tural property of the graph, may be a useful indica-tor in choosing which graph-based SSL algorithmshould be applied on a given graph.Unlike MAD, Adsorption does not optimizeany well defined objective (Talukdar and Cram-mer, 2009), and hence any analysis along the linesdescribed above is not possible.
The heuristicchoices made in Adsorption may have lead to itssub-optimal performance compared to MAD; weleave it as a topic for future investigation.3.5 Effect of Per-Node Class SparsityFor all the experiments in Sections 3.1, 3.2, and3.6, each node was allowed to have a maximumof 15 classes during inference.
After each update0.30.330.360.390.425 15 25 35 45Effect of Per-node Sparsity ConstraintMeanReciprocalRank(MRR)Maximum Allowed Classes per NodeFigure 6: Effect of per node class sparsity (maxi-mum number of classes allowed per node) duringMAD inference in the experimental setting of Fig-ure 4 (one random split).on a node, all classes except for the top scoring15 classes were discarded.
Without such sparsityconstraints, a node in a connected graph will endup acquiring all the labels injected into the graph.This is undesirable for two reasons: (1) for ex-periments involving a large numbers of classes (asin the previous section and in the general case ofopen domain IE), this increases the space require-ment and also slows down inference; (2) a partic-ular node is unlikely to belong to a large num-ber of classes.
In order to estimate the effect ofsuch sparsity constraints, we varied the numberof classes allowed per node from 5 to 45 on thegraph and experimental setup of Figure 4, with 10seeds per class.
The results for MAD inferenceover the development split are shown in Figure6.
We observe that performance can vary signifi-cantly as the maximum number of classes allowedper node is changed, with the performance peak-ing at 25.
This suggests that sparsity constraintsduring graph based SSL may have a crucial role toplay, a question that needs further investigation.3.6 TextRunner Graph with additionalSemantic Constraints from YAGORecently, the problem of instance-attribute extrac-tion has started to receive attention (Probst et al,2007; Bellare et al, 2007; Pasca and Durme,2007).
An example of an instance-attribute pairis (Bob Dylan, albums).
Given a set of seedinstance-attribute pairs, these methods attempt toextract more instance-attribute pairs automatically14780.180.230.280.330.38LP-ZGL Adsorption MAD170 WordNet Classes, 2 Seeds per ClassMeanReciprocalRank(MRR)AlgorithmsTextRunner GraphYAGO GraphTextRunner + YAGO Graph0.30.3380.3750.4130.45LP-ZGL Adsorption MAD170 WordNet Classes, 10 Seeds per ClassMeanReciprocalRank(MRR)AlgorithmsTextRunner GraphYAGO GraphTextRunner + YAGO GraphFigure 7: Comparison of class-instance acquisition performance on the three different graphs describedin Section 3.6.
All results are averaged over 10 random trials.
Addition of YAGO attributes to theTextRunner graph significantly improves performance.YAGO Top-2 WordNet Classes Assigned by MADAttribute (example instances for each class are shown in brackets)has currency wordnet country 108544813 (Burma, Afghanistan)wordnet region 108630039 (Aosta Valley, Southern Flinders Ranges)works at wordnet scientist 110560637 (Aage Niels Bohr, Adi Shamir)wordnet person 100007846 (Catherine Cornelius, Jamie White)has capital wordnet state 108654360 (Agusan del Norte, Bali)wordnet region 108630039 (Aosta Valley, Southern Flinders Ranges)born in wordnet boxer 109870208 (George Chuvalo, Fernando Montiel)wordnet chancellor 109906986 (Godon Brown, Bill Bryson)has isbn wordnet book 106410904 (Past Imperfect, Berlin Diary)wordnet magazine 106595351 (Railway Age, Investors Chronicle)Table 2: Top 2 (out of 170) WordNet classes assigned by MAD on 5 randomly chosen YAGO attributenodes (out of 80) in the TextRunner + YAGO graph used in Figure 7 (see Section 3.6), with 10 seeds perclass used.
A few example instances of each WordNet class is shown within brackets.
Top ranked classfor each attribute is shown in bold.from various sources.
In this section, we ex-plore whether class-instance assignment can beimproved by incorporating new semantic con-straints derived from (instance, attribute) pairs.
Inparticular, we experiment with the following typeof constraint: two instances with a common at-tribute are likely to belong to the same class.
Forexample, in Figure 2 (b), instances Johnny Cashand Bob Dylan are more likely to belong to thesame class as they have a common attribute, al-bums.
Because of the smooth labeling bias ofgraph-based SSL methods (see Section 2.2), suchconstraints are naturally captured by the methodsreviewed in Section 2.
All that is necessary is theintroduction of bidirectional (instance, attribute)edges to the graph, as shown in Figure 2 (b).In Figure 7, we compare class-instance acqui-sition performance of the three graph-based SSLmethods (Section 2) on the following three graphs(also see Table 1):TextRunner Graph: Graph constructedfrom the hypernym tuples extracted by Tex-tRunner, as in Figure 5 (Section 3.3), with175k vertices and 529k edges.YAGO Graph: Graph constructed from the(instance, attribute) pairs obtained from theYAGO KB (Suchanek et al, 2007), with 142knodes and 777k edges.TextRunner + YAGO Graph: Union of the1479two graphs above, with 237k nodes and 1.3medges.In all experimental conditions with 2 and 10seeds per class in Figure 7, we observe that thethree methods consistently achieved the best per-formance on the TextRunner + YAGO graph.
Thissuggests that addition of attribute based seman-tic constraints from YAGO to the TextRunnergraph results in a better connected graph whichin turn results in better inference by the graph-based SSL algorithms, compared to using eitherof the sources, i.e., TextRunner output or YAGOattributes, in isolation.
This further illustratesthe advantage of aggregating information acrosssources (Talukdar et al, 2008; Pennacchiotti andPantel, 2009).
However, we are the first, to thebest of our knowledge, to demonstrate the effec-tiveness of attributes in class-instance acquisition.We note that this work is similar in spirit to therecent work by Carlson et al (2010) which alsodemonstrates the benefits of additional constraintsin SSL.Because of the label propagation behavior,graph-based SSL algorithms assign classes to allnodes reachable in the graph from at least oneof the labeled instance nodes.
This allows usto check the classes assigned to nodes corre-sponding to YAGO attributes in the TextRunner+ YAGO graph, as shown in Table 2.
Eventhough the experiments were designed for class-instance acquisition, it is encouraging to see thatthe graph-based SSL algorithm (MAD in Table2) is able to learn class-attribute relationships,an important by-product that has been the fo-cus of recent studies (Reisinger and Pasca, 2009).For example, the algorithm is able to learn thatworks at is an attribute of the WordNet class word-net scientist 110560637, and thereby its instances(e.g.
Aage Niels Bohr, Adi Shamir).4 ConclusionWe have started a systematic experimental com-parison of graph-based SSL algorithms for class-instance acquisition on a variety of graphs con-structed from different domains.
We found thatMAD, a recently proposed graph-based SSL algo-rithm, is consistently the most effective across thevarious experimental conditions.
We also showedthat class-instance acquisition performance can besignificantly improved by incorporating additionalsemantic constraints in the class-instance acqui-sition process, which for the experiments in thispaper were derived from instance-attribute pairsavailable in an independently developed knowl-edge base.
All the data used in these experimentswas drawn from publicly available datasets and weplan to release our code3 to foster reproducibleresearch in this area.
Topics for future work in-clude the incorporation of other kinds of semanticconstraint for improved class-instance acquisition,further investigation into per-node sparsity con-straints in graph-based SSL, and moving beyondbipartite graph constructions.AcknowledgmentsWe thank William Cohen for valuable discussions,and Jennifer Gillenwater, Alex Kulesza, and Gre-gory Malecha for detailed comments on a draft ofthis paper.
We are also very grateful to the authorsof (Banko et al, 2007), Oren Etzioni and StephenSoderland in particular, for providing TextRunneroutput.
This work was supported in part by NSFIIS-0447972 and DARPA HRO1107-1-0029.ReferencesS.
Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,S.
Kumar, D. Ravichandran, and M. Aly.
2008.Video suggestion and discovery for youtube: takingrandom walks through the view graph.
Proceedingsof WWW-2008.M.
Banko, M.J. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the web.
Procs.
of IJCAI.K.
Bellare, P. Talukdar, G. Kumaran, F. Pereira,M.
Liberman, A. McCallum, and M. Dredze.
2007.Lightly-Supervised Attribute Extraction.
NIPS 2007Workshop on Machine Learning for Web Search.A.
Carlson, J. Betteridge, R.C.
Wang, E.R.
Hruschka Jr,and T.M.
Mitchell.
2010.
Coupled Semi-SupervisedLearning for Information Extraction.
In Proceed-ings of the Third ACM International Conference onWeb Search and Data Mining (WSDM), volume 2,page 110.O.
Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the web - anexperimental study.
Artificial Intelligence Journal.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Fourteenth International3http://www.talukdar.net/datasets/class inst/1480Conference on Computational Linguistics, Nantes,France.Metaweb Technologies.
2009.
Freebase data dumps.http://download.freebase.com/datadumps/.P.
Pantel, E. Crestan, A. Borkovsky, A.M. Popescu, andV.
Vyas.
2009.
Web-scale distributional similarityand entity set expansion.
Proceedings of EMNLP-09, Singapore.M.
Pasca and Benjamin Van Durme.
2007.
What youseek is what you get: Extraction of class attributesfrom query logs.
In IJCAI-07.
Ferbruary, 2007.M.
Pennacchiotti and P. Pantel.
2009.
Entity Ex-traction via Ensemble Semantics.
Proceedings ofEMNLP-09, Singapore.K.
Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu.2007.
Semi-supervised learning of attribute-valuepairs from product descriptions.
In IJCAI-07, Fer-bruary, 2007.D.
Rao and D. Yarowsky.
2009.
Ranking and Semi-supervised Classification on Large Scale Graphs Us-ing Map-Reduce.
TextGraphs.J.
Reisinger and M. Pasca.
2009.
Bootstrapped extrac-tion of class attributes.
In Proceedings of the 18thinternational conference on World wide web, pages1235?1236.
ACM.E.
Riloff and R. Jones.
1999.
Learning dictionar-ies for information extraction by multi-level boot-strapping.
In Proceedings of the 16th National Con-ference on Artificial Intelligence (AAAI-99), pages474?479, Orlando, Florida.F.M.
Suchanek, G. Kasneci, and G. Weikum.
2007.Yago: a core of semantic knowledge.
In Proceed-ings of the 16th international conference on WorldWide Web, page 706.
ACM.P.
P. Talukdar and Koby Crammer.
2009.
New regular-ized algorithms for transductive learning.
In ECML-PKDD.P.
P. Talukdar, T. Brants, F. Pereira, and M. Liberman.2006.
A context pattern induction method for namedentity extraction.
In Tenth Conference on Computa-tional Natural Language Learning, page 141.P.
P. Talukdar, J. Reisinger, M. Pasca, D. Ravichan-dran, R. Bhagat, and F. Pereira.
2008.
Weakly-Supervised Acquisition of Labeled Class Instancesusing Graph Random Walks.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 581?589.B.
Van Durme and M. Pas?ca.
2008.
Finding cars, god-desses and enzymes: Parametrizable acquisition oflabeled instances for open-domain information ex-traction.
Twenty-Third AAAI Conference on Artifi-cial Intelligence.R.
Wang and W. Cohen.
2007.
Language-IndependentSet Expansion of Named Entities Using the Web.Data Mining, 2007.
ICDM 2007.
Seventh IEEE In-ternational Conference on, pages 342?350.X.
Zhu, Z. Ghahramani, and J. Lafferty.
2003.
Semi-supervised learning using gaussian fields and har-monic functions.
ICML-03, 20th International Con-ference on Machine Learning.1481
