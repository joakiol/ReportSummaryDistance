Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 531?540,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsTowards a Convex HMM Surrogate for Word AlignmentAndrei Arsene SimionColumbia University ?New York, NY, 10011aas2148@columbia.eduMichael CollinsColumbia University?Computer ScienceNew York, NY, 10027mc3354@columbia.eduClifford SteinColumbia UniversityIEOR DepartmentNew York, NY, 10027cs2035@columbia.eduAbstractAmong the alignment models used in statis-tical machine translation (SMT), the hiddenMarkov model (HMM) is arguably the mostelegant: it performs consistently better thanIBM Model 3 and is very close in perfor-mance to the much more complex IBM Model4.
In this paper we discuss a model whichcombines the structure of the HMM and IBMModel 2.
Using this surrogate, our experi-ments show that we can attain a similar levelof alignment quality as the HMM model im-plemented in GIZA++ (Och and Ney, 2003).For this model, we derive its convex relaxationand show that it too has strong performancedespite not having the local optima problemsof non-convex objectives.
In particular, theword alignment quality of this new convexmodel is significantly above that of the stan-dard IBM Models 2 and 3, as well as the pop-ular (and still non-convex) IBM Model 2 vari-ant of (Dyer et al, 2013).1 IntroductionThe IBM translation models are widely used in mod-ern statistical translation systems.
Typically, oneseeds more complex models with simpler models,and the parameters of each model are estimatedthrough an Expectation Maximization (EM) proce-dure.
Among the IBM Models, perhaps the mostelegant is the HMM model (Vogel et al, 1996).
TheHMM is the last model whose expectation step is?Currently at Google.
?Currently on leave at Google.both exact and simple, and it attains a level of ac-curacy that is very close to the results achieved bymuch more complex models.
In particular, experi-ments have shown that IBM Models 1, 2, and 3 allperform worse than the HMM and Model 4 benefitsgreatly from being seeded by the HMM (Och andNey, 2003).In this paper we make the following contributions:?
We derive a new alignment model which com-bines the structure of the HMM and IBMModel 2 and show that its performance is veryclose to that of the HMM.
There are severalreasons why such a result would be of value(for more on this, see (Simion et al, 2013) and(Simion et al, 2015a), for example).?
The main goal of this work is not to eliminatehighly non-convex models such as the HMMentirely but, rather, to develop a new, power-ful, convex alignment model and thus push theboundary of these theoretically justified tech-niques further.
Building on the work of (Simionet al, 2015a), we derive a convex relaxation forthe new model and show that its performanceis close to that of the HMM.
Although it doesnot beat the HMM, the new convex model im-proves upon the standard IBM Model 2 signif-icantly.
Moreover, the convex relaxation alsoperforms better than the strong IBM 2 vari-ant FastAlign (Dyer et al, 2013), IBM Model3, and the other available convex alignmentmodels detailed in (Simion et al, 2015a) and(Simion et al, 2013).?
We derive a parameter estimation algorithm for531new model and its convex relaxation based onthe EM algorithm.
Our model has both HMMemission probabilities and IBM Model 2?s dis-tortions, so we can use Model 2 to seed both themodel?s lexical and distortion parameters.
Forthe convex model, we need not use any initial-ization heuristics since the EM algorithm wederive is guaranteed to converge to a local op-tima that is also global.The goal of our work is to present a model whichis convex and has state of the art empirical perfor-mance.
Although one step of this task was achievedfor IBM Model 2 (Simion et al, 2015a), our tar-get goal deals with a much more local-optima-laden,non-convex objective.
Finally, whereas IBM 2 insome ways leads to a clear method of attack, we willdiscuss why the HMM presents challenges that re-quire the insertion of this new surrogate.Notation.
We adopt the notation introduced in(Och and Ney, 2003) of having 1m2n denote thetraining scheme of m IBM Model 1 EM iterationsfollowed by initializing Model 2 with these parame-ters and running n IBM Model 2 EM iterations.
Wedenote by H the HMM and note that it too can beseeded by running Model 1 followed by Model 2.Additionally, we denote our model as 2H, and notethat it has distortion parameters like IBM Model2 and emission parameters like that of the HMM.Under this notation, we let 1m2n2oH denote runningModel 1 for m iterations, then Model 2 for n iter-ation, and then finally our Model for o iterations.As before, we are seeding from the more basic tothe more complex model in turn.
We denote theconvex relaxation of 2H by 2HC.
Throughout thispaper, for any integer N , we use [N ] to denote{1 .
.
.
N} and [N ]0 to denote {0 .
.
.
N}.
Finally, inour presentation, ?convex function?
means a func-tion for which a local maxima also global, for exam-ple, f(x) = ?x2.2 IBM Models 1 and 2 and the HMMIn this section we give a brief review of IBM Models1, 2, and the HMM, as well as the the optimizationproblems arising from these models.
The standardapproach for optimization within these latent vari-able models is the EM algorithm.Throughout this section, and the remainder of thepaper, we assume that our set of training examplesis (e(k), f (k)) for k = 1 .
.
.
n, where e(k) is the k?thEnglish sentence and f (k) is the k?th French sen-tence.
Following standard convention, we assumethe task is to translate from French (the ?source?
lan-guage) into English (the ?target?
language) 1.
Weuse E to denote the English vocabulary (set of pos-sible English words), and F to denote the Frenchvocabulary.
The k?th English sentence is a sequenceof words e(k)1 .
.
.
e(k)lk where lk is the length of thek?th English sentence, and each e(k)i ?
E; similarlythe k?th French sentence is a sequence f (k)1 .
.
.
f (k)mk ,where mk is the length of the k?th French sentence,and each f (k)j ?
F .
We define e(k)0 for k = 1 .
.
.
nto be a special NULL word (note that E contains theNULL word).For each English word e ?
E, we will assumethat D(e) is a dictionary specifying the set of possi-ble French words that can be translations of e. Theset D(e) is a subset of F .
In practice, D(e) can bederived in various ways; in our experiments we sim-ply define D(e) to include all French words f suchthat e and f are seen in a translation pair.Given these definitions, the IBM Model 2 opti-mization problem is presented in several sources, forexample, (Simion et al, 2013).
The parameters inthis problem are t(f |e) and d(i|j, l,m).
The ob-jective function for IBM Model 2 is then the log-likelihood of the training data; we can simplify thelog-likelihood (Koehn, 2008) as1nn?k=1mk?j=1log p(f (k)j |e(k)) ,wherep(f (k)j |e(k)) =lk?i=0t(f (k)j |e(k)i )d(i|j, lk,mk) .1Technically, in most standard sources (Koehn, 2008), thisgoes as follows: when we want to translate from French to En-glish we note that p(e|f) ?
p(f |e)p(e) by Bayes?s Theorem.When translating, the alignment models we consider are con-cerned with modeling p(f |e) while the rest of the translation ishandled by the language model p(e).
Therefore, in the contextof the original task, we have that English is the target languagewhile French is the source.
However, for the sake of clarity, weemphasize that the alignment models we study are concernedwith the development of p(f |e).532This last simplification is crucial as it allows for asimple multinomial EM implementation, and can bedone for IBM Model 1 as well (Koehn, 2008).
Fur-thermore, the ability to write out the marginal like-lihood per sentence in this manner has seen otherapplications: it was crucial, for example, in derivinga convex relaxation of IBM Model 2 and solving thenew problem using subgradient methods (Simion etal., 2013).An improvement on IBM Model 2, called theHMM alignment model, was introduced by Vogelet al(Vogel et al, 1996).
For this model, the dis-tortion parameters are replaced by emission parame-ters d(aj |aj?1, l).
These emission parameters spec-ify the probability of the next alignment variablefor the jth target word is aj , given that the previ-ous source word was aligned to a target word whoseposition was aj?1 in a target sentence with length ofl.
The objective of the HMM is given by1nn?k=1?a(k)1 ...a(k)mklogmk?j=1t(f (k)j |e(k)akj)d(a(k)j |a(k)j?1, lk)and we present this in Fig 1.
We note that unlikeIBM Model 2, we cannot simplify the exponentialsum within the log-likelihood of the HMM, and soEM training for this model requires the use of a spe-cial EM implementation knows as the Baum-Welchalgorithm (Rabiner and Juang., 1986).Once these models are trained, each model?s high-est probability (Viterbi) alignment is computed.
ForIBM Models 1 and 2, the Viterbi alignment splitseasily (Koehn, 2008).
For the HMM, dynamic pro-gramming is used (Vogel et al, 1996).
Although itis non-convex and thus its initialization is important,the HMM is the last alignment model in the classi-cal setting that has an exact EM procedure (Och andNey, 2003): from IBM Model 3 onwards heuristicsare used within the expectation and maximizationsteps of each model?s associated EM procedure.3 Distortion and emission parameterstructureThe structure of IBM Model 2?s distortion param-eters and the HMM?s emission parameters is im-portant and is used in our model as well, so weInput: Define E, F , (e(k), f (k), lk,mk) for k =1 .
.
.
n, D(e) for e ?
E as in Section 2.Parameters:?
A parameter t(f |e) for each e ?
E, f ?
D(e).?
A parameter d(i|i, lk) for each i ?
[lk]0, i?
?
[lk]0.Constraints:?e ?
E, f ?
D(e), t(f |e) ?
0 (1)?e ?
E,?f?D(e)t(f |e) = 1 (2)?i, i?
?
[lk]0, d(i?|i, lk) ?
0 (3)?i ?
[lk]0,?i??
[lk]0d(i?|i, lk) = 1 (4)Objective: Maximize1nn?k=1?a(k)1 ...a(k)mklogmk?j=1t(f (k)j |e(k)akj)d(a(k)j |a(k)j?1, lk)with respect to the t(f |e) parameters d(i?|i, l).Figure 1: The HMM Optimization Problemdetail this here.
We are using the roughly samestructure as (Liang et al, 2006) and (Dyer et al,2013): the distortions and emissions of our modelare parametrized by forcing the model to concentrateits alignments on the diagonal.3.1 Distortion Parameters for IBM2Let ?
> 0.
For the IBM Model 2 distortions weset the NULL word probability as d(0|j, l,m) = p0,where p0 = 1l+1 and note that this will generally de-pend on the target sentence length within a bitexttraining pair that we are considering.
For i 6= 0which satisfies we setd(i|j, l,m) = (1?
p0)e?
?| il?jm |Z?
(j, l,m),where Z?
(j, l,m) is a normalization constant as in(Dyer et al, 2013).3.2 Emission Parameters for HMMLet ?
> 0.
For the HMM emissions we first setthe NULL word generation to d(0|i, l) = p0, with533p0 = 1l+1 .
For target word position i, i?
6= 0, we setd(i?|i, l) = (1?
p0)e?
?| i?l ?il |Z?
(i, l,m),where Z?
(i, l,m) is a suitable normalization con-stant.
Lastly, if i = 0 so that we are jump-ing from the NULL word onto a possibly differentword, we set d(i?|0, l) = p0.
Aside from makingthe NULL word have uniform jump probability, theabove emission parameters are modeled to favor ajumping to an adjacent English word.4 Combining IBM Model 2 and the HMMIn deriving the new HMM surrogate, our main goalwas to allow the current alignment to know as muchas possible about the previous alignment variableand still have a likelihood that factors as that of IBMModel 2 (Simion et al, 2013; Koehn, 2008).
Wecombine IBM Model 2 and the HMM by incorpo-rating the generation of words using the structureof both models.
The model we introduce, IBM2-HMM, is displayed in Fig 2.Consider a target-source sentence pair (e, f) with|e| = l and |f | = m. For source sentence positions jand j+ 1 we have source words fj and fj+1 and weassign a joint probability involving the alignmentsaj and aj+1 as:q(j, aj , aj+1, l,m) = (5)t(fj |eaj )d(aj |j, l,m)t(fj+1|eaj+1)d(aj+1|aj , l) .
(6)From the equation above, we note that we use theIBM Model 2?s word generation method for posi-tion j and the HMM generative structure for positionj + 1.
The generative nature of the above procedureintroduces dependency between adjacent words twoat a time.
Since we want to mimic the HMM?s struc-ture as much as possible, we devise our likelihoodfunction to mimic the HMM?s dependency betweenalignments using q.
Essentially, we move the sourceword position j from 1 to m and allow for overlap-ping terms when j ?
{2, .
.
.
,m ?
1}.
In what fol-lows, we describe this representation in detail.The likelihood in Eq.
16 is actually the sum of twolikelihoods which use equations Eq.
5 and 6 repeat-edly.
To this end, we will discuss how our objectiveis actually1nn?k=1log?a(k),b(k)p(f (k), a(k), b(k)|e(k)) , (7)where a(k) and b(k) both are alignment vectorswhose components are independent and can take onany values in [lk]0.
To see how p(f, a, b|e) comesabout, note that we could generate the sentence f bygenerating pairs (1, 2), (3, 4), (5, 6), .
.
.
using equa-tions Eqs.
5 and 6 for each pair.
Taking all this to-gether, the upshot of our discussion is that generat-ing the pair (e, f) in this way gives us that the like-lihood for an alignment a would be given by:p1(f, a|e) =m?1?j oddq(j, aj , aj+1, l,m) .
(8)Using the same idea as above, we could also skipthe first target word position and generate pairs(2, 3), (4, 5), .
.
.
using Eqs.
5 and 6.
Under this sec-ond generative method, the joint probability for fand alignment b is:p2(f, b|e) =m?1?j evenq(j, bj , bj+1, l,m) , (9)Finally, we note that if m is even we do notgenerate f1 and fm under p2 but we do generatethese words under p1.
Similarly, if m is odd wedo not generate f1 under p2 and we do not gen-erate fm under p1; however in this case as in thefirst, we still generate these missing words underthe other generative method.
Using p(f, a, b|e) =p1(f, a|e)p2(f, b|e) and factoring the log-likelihoodas in IBM Model 1 and 2 (Koehn, 2008), we get thelog-likelihood in Fig 2.
Finally, we note that ourmodel?s log-likelihood could be viewed as the sumof the log-likelihoods of a model which generates(e, f) using p1 and another model which generatessentences using p2.
These models share parametersbut generate words using different recipes, as dis-cussed above.5 The parameter estimation forIBM2-HMMTo fully optimize our new model (over t, ?, and ?
),we can use an EM algorithm in the same fashion as534Input: Define E, F , (e(k), f (k), lk,mk) for k =1 .
.
.
n, D(e) for e ?
E as in Section 2.Parameters:?
A parameter t(f |e) for each e ?
E, f ?
D(e).?
A parameter ?
> 0 for distortion centering.?
A parameter ?
> 0 for emission centering.Constraints:?e ?
E, f ?
D(e), t(f |e) ?
0 (10)?e ?
E,?f?D(e)t(f |e) = 1 (11)?i ?
[lk]0, j ?
[mk], d(i|j, lk,mk) ?
0 (12)?j ?
[mk],?i?
[lk]0d(i|j, lk,mk) = 1 (13)?i, i?
?
[lk]0, d(i?|i, lk) ?
0 (14)?i ?
[lk]0,?i??
[lk]0d(i?|i, lk) = 1 (15)Objective: Maximize1nn?k=1mk?1?j=1loglk?i=0lk?i?=0q(j, i, i?, lk,mk) (16)with respect to the parameters t(f |e), d(i?|i, l)d(i|j, l,m), and q(j, i, i?, lk,mk) set ast(f (k)j |e(k)i )d(i|j, l,m)t(f(k)j+1|ei?
)d(i?|i, l) (17)Figure 2: The IBM2-HMM Optimization Problem.
We useequation (5) within the likelihood definition.
(Dyer et al, 2013).
Specifically, for the model inquestion the EM algorithm still applies but we haveto use a gradient-based algorithm within the learningstep.
On the other hand, since such a gradient-basedmethod introduces the necessary complication of alearning rate, we could also optimize the objectiveby picking ?
and ?
via cross-validation and usinga multinomial EM algorithm for the learning of thelexical t terms.
For our experiments, we opted forthis simpler choice: we derived a multinomial EMalgorithm and cross-validated the centering param-eters for the distortion and emission terms.
With ?and ?
fixed, the derivation of this algorithm is verysimilar to the one used for IBM2-HMM?s convex re-laxation and this uses the path discussed in (Simionet al, 2015a) and (Simion et al, 2015b).
We detailthe EM algorithm for the convex relaxation below.6 A Convex HMM SurrogateIn this section we detail a procedure to get a con-vex relaxation for IBM2-HMM.
Let (t,d) be all theparameters of the HMM.
As a first step in gettinga convex HMM, one could follow the path devel-oped in (Simion et al, 2015a) and directly replacethe HMM?s objective termsmk?j=1t(f (k)j |e(k)akj)d(a(k)j |a(k)j?1, lk)by(mk?j=1t(f (k)j |e(k)akj)d(a(k)j |a(k)j?1, lk))12mk .In particular, the geometric mean functionh(x1, .
.
.
, h2mk) = (?2mkj=1 xj)12mk is convex((Boyd and Vandenberghe, 2004)) and, for agiven sentence pair (e(k), f (k)) with alignmenta(k) we can find a projection matrix P so thatP(t,d) = (t?, d?)
where t?
= {t(f (k)j |e(k)akj )}mkj=1and d?
= {d(a(k)j |a(k)j?1, lk)}mkj=1 are exactly theparameters used in the term above (in particular,t,d are the set of all parameters while t?, d?
arethe set of parameters for the specific training pairk; P projects from the full space onto only theparameters used for training pair k).
Given this, wethen have that g(t,d) = h(P(t,d)) = h(t?, d?)
isconvex and, by composition, so is log g(t,d) (see(Simion et al, 2015a; Boyd and Vandenberghe,2004) for details; the main idea lies in the fact thatas linear transformations preserve convexity, so docompositions of convex functions with increasingconvex functions such as log).
Finally, if we run thisplan for all terms in the objective, the new objectiveis convex since it is the sum of convex functions (thenew optimization problem is convex as it has linearconstraints).
Although this gives a convex program,we observed that the powers being so small madethe optimized probabilities very uninformative (i.e.uniform).
The above makes sense: no matter whatthe parameters are, we will easily get the 1 we seek535for each term in the objective since all terms aretaken to a low ( 12mk ) power .Since this direct relaxation does not yield fruit,we next could turn to our model.
Developing its re-laxation in the vein of (Simion et al, 2015a), wecould be to let d(i|j, l,m) and d(i?|i, l) be multino-mial probabilities (that is, these parameters wouldnot have centering parameters ?
and ?
and wouldbe just standard probabilities as in the GIZA++ ver-sions of the HMM and IBM Model 2 (Och and Ney,2003)) and replace all the terms q(j, i?, i, l,m) in(16) by (q(j, i?, i, l,m)) 14 .
Although this method isfeasible, experiments showed that the relaxation isnot very competitive and performs on par with IBMModel 2; this relaxation is far in performance fromthe HMM even though we are relaxing (only) theproduct of 4 terms (lastly, we mention that we triedother variants were we replaced d(i|j, l,m)d(i?|i, l)by d(i, i?|j, l,m) so that we would have only threeterms; unfortunately, this last attempt also producedparameters that were ?too uniform?
).The above analysis motivates why we defined ourmodel as we did: we now have only two terms torelax.
In particular, to rectify the above, we leftin place the structure discussed in Section 3 andmade ?
and ?
be tuning parameters which we cancross-validate for on a small held-out data set.
Thislast constraint effectively removes the distortion andemission parameters from the model but we stillmaintain the structural property of these parame-ters: we maintain their favoring the diagonal or ad-jacent alignment.
To get the relaxation, we replaceq(j, i, i?, l,m) byp(j, i, i?, l,m) ?
?t(f (k)j |e(k)i )t(f(k)j+1|ei?
)and set the proportionality constant to bed(i|j, l,m)d(i?|i, l).
Using this setup we nowhave a convex objective to optimize over.
Inparticular, we?ve formulated a convex relaxation ofthe IBM2-HMM problem which, like the SupportVector Machine, includes parameters that can becross-validated over (Boyd and Vandenberghe,2004).Input: Define E, F , (e(k), f (k), lk,mk) for k =1 .
.
.
n, D(e) for e ?
E as in Section 2.
Pick?, ?
> 0 as in Section 3 via cross-validation.Parameters:?
A parameter t(f |e) for each e ?
E, f ?
D(e).Constraints:?e ?
E, f ?
D(e), t(f |e) ?
0 (18)?e ?
E,?f?D(e)t(f |e) = 1 (19)Objective: Maximize1nn?k=1mk?1?j=1loglk?i=0lk?i?=0p(j, i, i?, lk,mk) (20)with respect to the parameters t(f |e) andp(j, i, i?, lk,mk) set as?t(f (k)j |e(k)i )d(i|j, l,m)?t(f (k)j+1|ei?
)d(i?|i, l)Figure 3: The IBM2-HMM convex relaxation optimizationproblem.
Note that the distortions d(i|j, l, ,m) and emissionsd(i?|i, l) are constants held fixed and parameterized by cross-validated parameters ?
and ?
as in Section 3.7 An EM algorithm for the convexsurrogateThe EM algorithm for the convex relaxation of oursurrogate is given in Fig 4.
As the model?s objectiveis the sum of the objectives of two models generatedby a multinomial rule, we can get a very succinctEM algorithm.
For more details on this and a simi-lar derivation, please refer to (Simion et al, 2015a),(Koehn, 2008) or (Simion et al, 2015b).
For this al-gorithm, we again note that the distortion and emis-sion parameters are constants so that the only esti-mation that needs to be conducted is on the lexical tterms.To be specific, we have that the M step requiresoptimizing1nn?k=1log?a(k),b(k)q(a(k), b(k)|e(k), f(k))p(f(k), a(k), b(k)|e(k)) .In the above, we have that536q(a(k), b(k)|e(k), f (k))are constants proportional tomk?1?j=1?t(f(k)j |e(k)a(k)j)t(f(k)j+1|e(k)a(k)j+1)mk?j=2?t(f(k)j |e(k)b(k)j)t(f(k)j+1|e(k)b(k)j+1)and gotten through the E step.
This optimizationstep is very similar to the regular Model 2 M stepsince the ?
drops down using log t?
= ?
log t; theexact same count-based method can be applied.
Theupshot of this is given in Fig 4; similar to the logicabove for 2HC, we can get the EM algorithm for 2H.8 Decoding methods for IBM2-HMMWhen computing the optimal alignment we wantedto compare our model with the HMM as closely aspossible.
Because of this, the most natural methodof evaluating the quality of the parameters would beto use the same rule as the HMM.
Specifically, fora sentence pair (e, f) with |e| = l and |f | = m,in HMM decoding we aim to find (a1 .
.
.
am) whichmaximizesmaxa1,...,amm?j=1t(fj |eaj )d(aj |aj?1, l).As is standard, dynamic programming can now beused to find the Viterbi alignment.
Although thereare a number of ways we could define the opti-mal alignment, we felt that the above would be thebest since it tests dependance between alignmentvariables and allows for easy comparison with theGIZA++ HMM.
Finding the optimal alignment un-der the HMM setting is labelled ?HMM?
in Table 1.We can also find the optimal alignment by takingthe objective literally (see (Simion et al, 2014) for asimilar argument dealing with the convex relaxationof IBM Model 2) and computingmaxa1...amp1(f, a|e)p2(f, a|e).Above, we are asking for the optimal alignmentthat yields the highest probability alignment throughgenerating technique p1 and p2.
This method of de-coding is a lot like the HMM style and also relies1: Input: Define E, F , (e(k), f (k), lk,mk) for k =1 .
.
.
n, D(e) for e ?
E as in Section 2.
Two pa-rameters ?, ?
> 0 picked by cross-validation so thatthe distortions and emissions are constants obeyingthe structure in Section 3.
An integer T specifyingthe number of passes over the data.2: Parameters:?
A parameter t(f |e) for each e ?
E, f ?
D(e).3: Initialization:?
?e ?
E, f ?
D(e), set t(f |e) = 1D(e) .4: EM Algorithm: Expectation5: for all k = 1 .
.
.
N do6: for all j = 1 .
.
.mk do7: ?
= 08: ?
= 09: for all i = 0 .
.
.
lk do10: for all i?
= 0 .
.
.
lk do11: ?
[i, i?]
= p(j, i?, i, lk,mk)12: ?+ = ?
[i, i?
]13: for all i = 0 .
.
.
lk do14: for all i?
= 0 .
.
.
lk do15: ?
[i, i?]
= ?[i,i?
]?16: counts(f (k)j , e(k)i )+ = ?
[i, i?
]17: counts(e(k)i )+ = ?
[i, i?
]18: counts(f (k)j+1, e(k)i?
)+ = ?
[i, i?
]19: counts(e(k)i?
)+ = ?
[i, i?
]20: EM Algorithm: Maximization21: for all e ?
E do22: for all f ?
D(e) do23: t(f |e) = counts(e,f)counts(e)24: Output: t parameters.Figure 4: Pseudocode for the EM algorithm of the IBM2-HMM?s convex relaxation.
As the distortion and emission pa-rameters are constants, the algorithm is very similar to that ofIBM Model 1.on dynamic programming.
In this case we have therecursion for QJoint given byQJoint(1, i) = t(f1|ei)d2(i|1, l,m) ,?i ?
[l]0, andQJoint(j, i?)
= t2(fj |ei?
)d(i?|j, l,m)MJoint(j ?
1, i?)
,where MJoint(j ?
1, i?)
isMJoint(j ?
1, i?)
=lmaxi=0{d(i?|i, l)QJoint(j ?
1, i)} ,?
2 ?
j ?
m,?
i?
?
[l]0.
The alignment results got-ten by decoding with this method is labelled ?Joint?in Table 1.5379 ExperimentsIn this section we describe experiments using theIBM2-HMM optimization problem combined withthe EM algorithm for parameter estimation.9.1 Data SetsWe use data from the bilingual word alignmentworkshop held at HLT-NAACL 2003 (Michalceaand Pederson, 2003).
We use the Canadian Hansardsbilingual corpus, with 743,989 English-French sen-tence pairs as training data, 37 sentences of devel-opment data, and 447 sentences of test data (notethat we use a randomly chosen subset of the origi-nal training set of 1.1 million sentences, similar tothe setting used in (Moore, 2004)).
The develop-ment and test data have been manually aligned at theword level, annotating alignments between sourceand target words in the corpus as either ?sure?
(S)or ?possible?
(P ) alignments, as described in (Ochand Ney, 2003).
As is standard, we lower-cased allwords before giving the data to GIZA++ and we ig-nored NULL word alignments in our computation ofalignment quality scores.9.2 MethodologyWe test several models in our experiments.
In par-ticular, we empirically evaluate our models againstthe GIZA++ IBM Model 3 and HMM, as well as theFastAlign IBM Model 2 implementation of (Dyer etal., 2013) that uses Variational Bayes.
For each ofour models, we estimated parameters and got align-ments in turn using models in the source-target andtarget-source directions; using the same setup as(Simion et al, 2013), we present the gotten inter-sected alignments.
In training, we employ the stan-dard practice of initializing non-convex alignmentmodels with simpler non-convex models.
In par-ticular, we initialize, the GIZA++ HMM with IBMModel 2, IBM Model 2 with IBM Model 1, andIBM2-HMM and IBM Model 3 with IBM Model2 preceded by Model 1.
Lastly, for FastAlign, weinitialized all parameters uniformly since this em-pirically was a more favorable initialization, as dis-cussed in (Dyer et al, 2013).We measure the performance of the models interms of Precision, Recall, F-Measure, and AER us-ing only sure alignments in the definitions of the firstthree metrics and sure and possible alignments in thedefinition of AER , as in (Simion et al, 2013) and(Marcu et al, 2006).
For our experiments, we reportresults in both AER (lower is better) and F-Measure(higher is better) (Och and Ney, 2003).Table 1 shows the alignment summary statisticsfor the 447 sentences present in the Hansard testdata.
We present alignments quality scores usingeither the FastAlign IBM Model 2, the GIZA++HMM, and our model and its relaxation using eitherthe ?HMM?
or ?Joint?
decoding.
First, we note thatin deciding the decoding style for IBM2-HMM, theHMM method is better than the Joint method.
Weexpected this type of performance since HMM de-coding introduces positional dependance among theentire set of words in the sentence, which is shown tobe a good modeling assumption (Vogel et al, 1996).From the results in Table 1 we see that the HMMoutperforms all other models, including IBM2-HMM and its convex relaxation.
However, IBM2-HMM is not far in AER performance from the HMMand both it and its relaxation do better than FastAl-ign or IBM Model 3 (the results for IBM Model 3are not presented; a one-directional English-Frenchrun of 1525315 gave AER and F-Measure numbers of0.1768 and 0.6588, respectively, and this was behindboth the IBM Model 2 FastAlign and our models).As a further set of experiments, we also appendedan IBM Model 1 or IBM Model 2 objective to ourmodels?s original objectives, so that the constraintsand parameters are the same but now we are maxi-mizing the average of two log-likelihoods.
With re-gard to the EM optimization, we would only needto add another ?
parameter: we?d now have proba-bilities ?1[i] ?
t(f (k)j |e(k)i )d(i|j, ll,mk) (this is forIBM Model 2 smoothing; we have d = 1 for IBM1 smoothing) and ?2[i, i?]
?
p(j, i, i?.lk,mk) in theEM Algorithm that results (for more, see (Simion etal., 2015a)).
We note that the appended IBM Model2 objective is still convex if we fix the distortions??
parameter and then optimize for the t parametersvia EM (thus, model 2HC is still convex).
For us,there were significant gains, especially in the con-vex model.
The results for all these experiments areshown in Table 2, with IBM 2 smoothing for the con-vex model displayed in the rightmost column.Finally, we also tested our model in the full538Training 15210H 15210H 210HC 210HC FA10 1525H10Decoding HMM Joint HMM Joint IBM2 HMMIteration AER1 0.0956 0.1076 0.1538 0.1814 0.5406 0.17612 0.0884 0.0943 0.1093 0.1343 0.1625 0.08733 0.0844 0.0916 0.1023 0.1234 0.1254 0.07864 0.0828 0.0904 0.0996 0.1204 0.1169 0.07535 0.0808 0.0907 0.0992 0.1197 0.1131 0.07376 0.0804 0.0906 0.0989 0.1199 0.1128 0.07197 0.0795 0.0910 0.0986 0.1197 0.1116 0.07178 0.0789 0.0900 0.0988 0.1195 0.1086 0.07259 0.0793 0.0904 0.0986 0.1195 0.1076 0.073810 0.0793 0.0902 0.0986 0.1195 0.1072 0.0734Iteration F-Measure1 0.7829 0.7797 0.7199 0.6914 0.2951 0.72192 0.7854 0.7805 0.7594 0.7330 0.7111 0.80393 0.7899 0.7806 0.7651 0.7427 0.7484 0.81124 0.7908 0.7813 0.7668 0.7457 0.7589 0.80945 0.7928 0.7806 0.7673 0.7461 0.7624 0.80586 0.7928 0.7807 0.7678 0.7457 0.7630 0.80567 0.7939 0.7817 0.7679 0.7457 0.7633 0.80468 0.7942 0.7814 0.7679 0.7458 0.7658 0.80249 0.7937 0.7813 0.7680 0.7457 0.7672 0.800710 0.7927 0.7816 0.7680 0.7457 0.7679 0.8010Table 1: Alignment quality results for IBM2-HMM (2H) andits convex relaxation (2HC) using either HMM-style dynamicprogramming or ?Joint?
decoding.
The first and last columnsabove are for the GIZA++ HMM initialized either with IBMModel 1 or Model 1 followed by Model 2.
FA above refers tothe improved IBM Model 2 (FastAlign) of (Dyer et al, 2013).SMT pipeline using the cdec system (Dyer et al,2013).
For our experiments, we compared ourmodels?
alignments (gotten by training 1525H and25HC) against the alignments gotten by the HMM(1525H5), IBM Model 4 (15H53343), and FastAl-ign.
Unfortunately, we found that all 4 systemsled to roughly the same BLEU score of 40 on aSpanish-English training set of size 250000 whichwas a subset of version 7 of the Europarl dataset(Dyer et al, 2013).
For our development and testsets, we used data each of size roughly 1800 andwe preprocessed all data by considering only sen-tences of size less than 80 and filtering out sentenceswhich had a very large (or small) ratio of target andsource sentence lengths.
Although the SMT resultswere not a success in that our gains were not signif-icant, we felt that the experiments at least highlightthat our model mimics the HMM?s alignments eventhough its structure is much more local.
Lastly, wein regards to the new convex model?s performance,we observe much better alignment quality than anyother convex alignment models in print, for exam-ple, (Simion et al, 2015a).Training 15210H 15210H 210HC 210HCSmoothing IBM1 IBM2 IBM1 IBM2Decoding HMM HMM HMM HMMIteration AER1 0.1003 0.0958 0.1703 0.14822 0.0949 0.0890 0.1172 0.10573 0.0904 0.0840 0.1039 0.09554 0.0886 0.0816 0.0984 0.09275 0.0866 0.0795 0.0948 0.08946 0.0851 0.0794 0.0933 0.08887 0.0837 0.0790 0.0922 0.08868 0.0825 0.0788 0.0921 0.08809 0.0820 0.0785 0.0921 0.088110 0.0820 0.0777 0.0920 0.0881Iteration F-Measure1 0.7791 0.7817 0.7065 0.72512 0.7822 0.7839 0.7559 0.76373 0.7856 0.7897 0.7689 0.77404 0.7873 0.7923 0.7729 0.77605 0.7899 0.7938 0.7771 0.77826 0.7904 0.7943 0.7789 0.77887 0.7917 0.7946 0.7800 0.77918 0.7928 0.7944 0.7806 0.77959 0.7930 0.7941 0.7806 0.779710 0.7925 0.7947 0.7806 0.7796Table 2: Alignment quality results for IBM2-HMM and itsrelaxation using IBM 1 and IBM 2 smoothing (in this case,?smoothing?
means adding these log-likelihoods to the originalobjective as in (Simion et al, 2013).
For the convex relaxationof IBM2-HMM, we can only smooth by adding in the convexIBM Model 1 objective, or by adding in an IBM Model 2 objec-tive where the distortions are taken to be constants (these distor-tions are identical to the ones that are used within the relaxationitself and are cross-validated for optimal ?
).10 Conclusions and Future WorkOur work has explored some of the details of a newmodel which combines the structure of IBM Model2 the alignment HMM Model.
We?ve shown thatthis new model and its convex relaxation performsvery close to the standard GIZA++ implementationof the HMM.
Bridging the gap between the HMMand convex models proves difficult for a number ofreasons (Guo and Schuurmans, 2007).
In this pa-per, we have introduced a new set of ideas aimed attightening this gap.AcknowledgmentsAndrei Simion was supported by a Google researchaward.
Cliff Stein was partially supported by NSFgrant CCF-1421161.
We thank the reviewers fortheir insightful commentary and suggestions.539ReferencesSteven Boyd and Lieven Vandenberghe.
2004.
ConvexOptimization.
Cambridge University Press.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert.
L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19:263-311.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood From Incomplete Data via theEM Algorithm.
Journal of the royal statistical society,series B, 39(1):1-38.Chris Dyer , Victor Chahuneau, Noah A. Smith.
2013.A Simple, Fast, and Effective Reparameterization ofIBM Model 2.
In Proceedings of NAACL.Alexander Fraser and Daniel Marcu.
2007.
Measur-ing Word Alignment Quality for Statistical Ma-chine Translation.
Journal Computational Linguistics,33(3): 293-303.Joao V. Graca, Kuzman Ganchev and Ben Taskar.
2007.Expectation Maximization and Posterior Constraints.In Proceedings of NIPS.Yuhong Guo and Dale Schuurmans.
2007.
Convex Re-laxations of Latent Variable Training.
In Proceedingsof NIPS.Simon Lacoste-Julien, Ben Taskar, Dan Klein, andMichael Jordan.
2008.
Word Alignment via QuadraticAssignment.
In Proceedings of the HLT-NAACL.Phillip Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings of theEMNLP.Phillip Koehn.
2008.
Statistical Machine Translation.Cambridge University Press.Percy Liang, Ben Taskar and Dan Klein.
2006.
Alignmentby Agreement.
In Proceedings of NAACL.Daniel Marcu, Wei Wang, Abdessamad Echihabi,and Kevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target LanguagePhrases.
In Proceedings of the EMNLP.Rada Michalcea and Ted Pederson.
2003.
An Evalua-tion Exercise in Word Alignment.
HLT-NAACL 2003:Workshop in building and using Parallel Texts: DataDriven Machine Translation and Beyond.Robert C. Moore.
2004.
Improving IBM Word-Alignment Model 1.
In Proceedings of the ACL.Stephan Vogel, Hermann Ney and Christoph Tillman.1996.
HMM-Based Word Alignment in StatisticalTranslation.
In Proceedings of COLING.Franz Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational-Linguistics, 29(1): 19-52.L.R.
Rabiner and B.H.
Juang.
1986.
An Introduction toHidden Markov Models.
In IEEE ASSP Magazine.Andrei Simion, Michael Collins and Cliff Stein.
2013.
AConvex Alternative to IBM Model 2.
In Proceedingsof EMNLP.Andrei Simion, Michael Collins and Cliff Stein.
2013.Some Experiments with a Convex IBM Model 2.
InProceedings of EACL.Andrei Simion, Michael Collins and Cliff Stein.
2015.A Family of Latent Variable Convex Relaxations forIBM Model 2.
In Proceedings of the AAAI.Andrei Simion, Michael Collins and Cliff Stein.
2015.On a Strictly Concave IBM Model 1.
In Proceedingsof EMNLP.Kristina Toutanova and Michel Galley.
2011.
Why Ini-tialization Matters for IBM Model 1: Multiple Optimaand Non-Strict Convexity.
In Proceedings of the ACL.Ashish Vaswani, Liang Huang and David Chiang.
2012.Smaller Alignment Models for Better Translations:Unsupervised Word Alignment with the L0-norm.
InProceedings of the ACL.540
