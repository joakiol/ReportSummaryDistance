Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353?1361,Beijing, August 2010A Monolingual Tree-based Translation Model for Sentence Simplification?Zhemin Zhu1Department of Computer ScienceTechnische Universita?t DarmstadtDelphine Bernhard2LIMSI-CNRSIryna Gurevych1Department of Computer ScienceTechnische Universita?t Darmstadt1http://www.ukp.tu-darmstadt.de 2delphine.bernhard@limsi.frAbstractIn this paper, we consider sentence sim-plification as a special form of translationwith the complex sentence as the sourceand the simple sentence as the target.We propose a Tree-based SimplificationModel (TSM), which, to our knowledge,is the first statistical simplification modelcovering splitting, dropping, reorderingand substitution integrally.
We also de-scribe an efficient method to train ourmodel with a large-scale parallel datasetobtained from the Wikipedia and SimpleWikipedia.
The evaluation shows that ourmodel achieves better readability scoresthan a set of baseline systems.1 IntroductionSentence simplification transforms long and dif-ficult sentences into shorter and more readableones.
This helps humans read texts more easilyand faster.
Reading assistance is thus an impor-tant application of sentence simplification, espe-cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read-ers (Watanabe et al, 2009), or non-native speakers(Siddharthan, 2002).Not only human readers but also NLP ap-plications can benefit from sentence simplifica-tion.
The original motivation for sentence sim-plification is using it as a preprocessor to facili-tate parsing or translation tasks (Chandrasekar etal., 1996).
Complex sentences are considered asstumbling blocks for such systems.
More recently,sentence simplification has also been shown help-ful for summarization (Knight and Marcu, 2000),?
This work has been supported by the Emmy NoetherProgram of the German Research Foundation (DFG) underthe grant No.
GU 798/3-1, and by the Volkswagen Founda-tion as part of the Lichtenberg-Professorship Program underthe grant No.
I/82806.sentence fusion (Filippova and Strube, 2008b), se-mantic role labeling (Vickrey and Koller, 2008),question generation (Heilman and Smith, 2009),paraphrase generation (Zhao et al, 2009) andbiomedical information extraction (Jonnalagaddaand Gonzalez, 2009).At sentence level, reading difficulty stems ei-ther from lexical or syntactic complexity.
Sen-tence simplification can therefore be classifiedinto two types: lexical simplification and syntac-tic simplification (Carroll et al, 1999).
These twotypes of simplification can be further implementedby a set of simplification operations.
Splitting,dropping, reordering, and substitution are widelyaccepted as important simplification operations.The splitting operation splits a long sentence intoseveral shorter sentences to decrease the complex-ity of the long sentence.
The dropping operationfurther removes unimportant parts of a sentence tomake it more concise.
The reordering operationinterchanges the order of the split sentences (Sid-dharthan, 2006) or parts in a sentence (Watanabeet al, 2009).
Finally, the substitution operation re-places difficult phrases or words with their simplersynonyms.In most cases, different simplification opera-tions happen simultaneously.
It is therefore nec-essary to consider the simplification process asa combination of different operations and treatthem as a whole.
However, most of the ex-isting models only consider one of these opera-tions.
Siddharthan (2006) and Petersen and Osten-dorf (2007) focus on sentence splitting, while sen-tence compression systems (Filippova and Strube,2008a) mainly use the dropping operation.
As faras lexical simplification is concerned, word sub-stitution is usually done by selecting simpler syn-onyms from Wordnet based on word frequency(Carroll et al, 1999).In this paper, we propose a sentence simplifica-tion model by tree transformation which is based1353on techniques from statistical machine translation(SMT) (Yamada and Knight, 2001; Yamada andKnight, 2002; Graehl et al, 2008).
Our model in-tegrally covers splitting, dropping, reordering andphrase/word substitution.
The parameters of ourmodel can be efficiently learned from complex-simple parallel datasets.
The transformation froma complex sentence to a simple sentence is con-ducted by applying a sequence of simplificationoperations.
An expectation maximization (EM)algorithm is used to iteratively train our model.We also propose a method based on monolingualword mapping which speeds up the training pro-cess significantly.
Finally, a decoder is designed togenerate the simplified sentences using a greedystrategy and integrates language models.In order to train our model, we further com-pile a large-scale complex-simple parallel dataset(PWKP) from Simple English Wikipedia1 and En-glish Wikipedia2, as such datasets are rare.We organize the remainder of the paper as fol-lows: Section 2 describes the PWKP dataset.
Sec-tion 3 presents our TSM model.
Sections 4 and 5are devoted to training and decoding, respectively.Section 6 details the evaluation.
The conclusionsfollow in the final section.2 Wikipedia Dataset: PWKPWe collected a paired dataset from the EnglishWikipedia and Simple English Wikipedia.
Thetargeted audience of Simple Wikipedia includes?children and adults who are learning English lan-guage?.
The authors are requested to ?use easywords and short sentences?
to compose articles.We processed the dataset as follows:Article Pairing 65,133 articles from SimpleWikipedia3 and Wikipedia4 were paired by fol-lowing the ?language link?
using the dump filesin Wikimedia.5 Administration articles were fur-ther removed.Plain Text Extraction We use JWPL (Zesch etal., 2008) to extract plain texts from Wikipedia ar-ticles by removing specific Wiki tags.Pre-processing including sentence boundarydetection and tokenization with the Stanford1http://simple.wikipedia.org2http://en.wikipedia.org3As of Aug 17th, 20094As of Aug 22nd, 20095http://download.wikimedia.orgParser package (Klein and Manning, 2003),and lemmatization with the TreeTagger (Schmid,1994).Monolingual Sentence Alignment As we needa parallel dataset algned at the sentence level,we further applied monolingual sentence align-ment on the article pairs.
In order to achievethe best sentence alignment on our dataset, wetested three similarity measures: (i) sentence-levelTF*IDF (Nelken and Shieber, 2006), (ii) wordoverlap (Barzilay and Elhadad, 2003) and (iii)word-based maximum edit distance (MED) (Lev-enshtein, 1966) with costs of insertion, deletionand substitution set to 1.
To evaluate their perfor-mance we manually annotated 120 sentence pairsfrom the article pairs.
Tab.
1 reports the precisionand recall of these three measures.
We manuallyadjusted the similarity threshold to obtain a recallvalue as close as possible to 55.8% which was pre-viously adopted by Nelken and Shieber (2006).Similarity Precision RecallTF*IDF 91.3% 55.4%Word Overlap 50.5% 55.1%MED 13.9% 54.7%Table 1: Monolingual Sentence AlignmentThe results in Tab.
1 show that sentence-levelTF*IDF clearly outperforms the other two mea-sures, which is consistent with the results reportedby Nelken and Shieber (2006).
We henceforthchose sentence-level TF*IDF to align our dataset.As shown in Tab.
2, PWKP contains morethan 108k sentence pairs.
The sentences fromWikipedia and Simple Wikipedia are consideredas ?complex?
and ?simple?
respectively.
Both theaverage sentence length and average token lengthin Simple Wikipedia are shorter than those inWikipedia, which is in compliance with the pur-pose of Simple Wikipedia.Avg.
Sen. Len Avg.
Tok.
Len #Sen.Pairscomplex simple complex simple -25.01 20.87 5.06 4.89 108,016Table 2: Statistics for the PWKP datasetIn order to account for sentence splitting, we al-low 1 to n sentence alignment to map one complexsentence to several simple sentences.
We first per-form 1 to 1 mapping with sentence-level TF*IDFand then combine the pairs with the same complexsentence and adjacent simple sentences.3 The Simplification Model: TSMWe apply the following simplification operationsto the parse tree of a complex sentence: splitting,1354dropping, reordering and substitution.
In this sec-tion, we use a running example to illustrate thisprocess.
c is the complex sentence to be simpli-fied in our example.
Fig.
1 shows the parse tree ofc (we skip the POS level).c: August was the sixth month in the ancient Ro-man calendar which started in 735BC.NP VPSAugust wasNPinsixththeSBARNPNP PPWHNP SVPstarted PPin 735BCancient calendar whichthe RomanmonthFigure 1: Parse Tree of c3.1 SplittingThe first operation is sentence splitting, which wefurther decompose into two subtasks: (i) segmen-tation, which decides where and whether to splita sentence and (ii) completion, which makes thenew split sentences complete.First, we decide where we can split a sentence.In our model, the splitting point is judged by thesyntactic constituent of the split boundary wordin the complex sentence.
The decision whether asentence should be split is based on the length ofthe complex sentence.
The features used in thesegmentation step are shown in Tab.
3.Word Constituent iLength isSplit Prob.?which?
SBAR 1 true 0.0016?which?
SBAR 1 false 0.9984?which?
SBAR 2 true 0.0835?which?
SBAR 2 false 0.9165Table 3: Segmentation Feature Table (SFT)Actually, we do not use the direct constituent ofa word in the parse tree.
In our example, the directconstituent of the word ?which?
is ?WHNP?.
In-stead, we use Alg.
1 to calculate the constituentof a word.
Alg.
1 returns ?SBAR?
as the ad-justed constituent for ?which?.
Moreover, di-rectly using the length of the complex sentenceis affected by the data sparseness problem.
In-stead, we use iLength as the feature which iscalculated as iLength = ceiling( comLengthavgSimLength),where comLength is the length of the complexsentence and avgSimLength is the average lengthof simple sentences in the training dataset.
The?Prob.?
column shows the probabilities obtainedafter training on our dataset.Algorithm 1 adjustConstituent(word, tree)constituent?
word.father;father ?
constituent.father;while father 6= NULL AND constituent is the mostleft child of father doconstituent?
father;father ?
father.father;end whilereturn constituent;In our model, one complex sentence can be splitinto two or more sentences.
Since many splittingoperations are possible, we need to select the mostlikely one.
The probability of a segmentation op-eration is calculated as:P (seg|c) =?w:cSFT (w|c) (1)where w is a word in the complex sentence c andSFT (w|c) is the probability of the word w in theSegmentation Feature Table (SFT); Fig.
2 showsa possible segmentation result of our example.NP VPSAugust wasNPinsixththeSBARNPNP PPWHNP SVPstarted PPin 735BCancient calendarwhichthe RomanmonthFigure 2: SegmentationThe second step is completion.
In this step,we try to make the split sentences complete andgrammatical.
In our example, to make the secondsentence ?which started in 735BC?
complete andgrammatical we should first drop the border word?which?
and then copy the dependent NP ?theancient Roman calendar?
to the left of ?started?to obtain the complete sentence ?the ancient Ro-man calendar started in 735BC?.
In our model,whether the border word should be dropped orretained depends on two features of the borderword: the direct constituent of the word and theword itself, as shown in Tab.
4.Const.
Word isDropped Prob.WHNP which True 1.0WHNP which False Prob.MinTable 4: Border Drop Feature Table (BDFT)In order to copy the necessary parts to completethe new sentences, we must decide which partsshould be copied and where to put these parts inthe new sentences.
In our model, this is judgedby two features: the dependency relation and theconstituent.
We use the Stanford Parser for pars-ing the dependencies.
In our example, the de-1355pendency relation between ?calendar?
in the com-plex sentence and the verb ?started?
in the secondsplit sentence is ?gov nsubj?.6 The direct con-stituent of ?started?
is ?VP?
and the word ?calen-dar?
should be put on the ?left?
of ?started?, seeTab.
5.Dep.
Const.
isCopied Pos.
Prob.gov nsubj VP(VBD) True left 0.9000gov nsubj VP(VBD) True right 0.0994gov nsubj VP(VBD) False - 0.0006Table 5: Copy Feature Table (CFT)For dependent NPs, we copy the whole NPphrase rather than only the head noun.7 In ourexample, we copy the whole NP phrase ?the an-cient Roman calendar?
to the new position ratherthan only the word ?calendar?.
The probability ofa completion operation can be calculated asP (com|seg) =Ybw:sBDFT (bw|s)Yw:sYdep:wCFT (dep).where s are the split sentences, bw is a borderword in s, w is a word in s, dep is a dependencyof w which is out of the scope of s. Fig.
3 showsthe most likely result of the completion operationfor our example.NP VPpt1August wasNPinsixththeNPNP PPpt2VPstarted PPin 735BCancient calendarthe RomanNPancient calendarthe RomanmonthFigure 3: Completion3.2 Dropping and ReorderingWe first apply dropping and then reordering toeach non-terminal node in the parse tree from topto bottom.
We use the same features for both drop-ping and reordering: the node?s direct constituentand its children?s constituents pattern, see Tab.
6and Tab.
7.Constituent Children Drop Prob.NP DT JJ NNP NN 1101 7.66E-4NP DT JJ NNP NN 0001 1.26E-7Table 6: Dropping Feature Table (DFT)6With Stanford Parser, ?which?
is a referent of ?calender?and the nsubj of ?started?.
?calender?
thus can be consideredto be the nsubj of ?started?
with ?started?
as the governor.7The copied NP phrase can be further simplified in thefollowing steps.Constituent Children Reorder Prob.NP DT JJ NN 012 0.8303NP DT JJ NN 210 0.0039Table 7: Reordering Feature Table (RFT)The bits ?1?
and ?0?
in the ?Drop?
column indi-cate whether the corresponding constituent is re-tained or dropped.
The number in the ?Reorder?column represents the new order for the children.The probabilities of the dropping and reorderingoperations can be calculated as Equ.
2 and Equ.
3.P (dp|node) = DFT (node) (2)P (ro|node) = RFT (node) (3)In our example, one of the possible results isdropping the NNP ?Roman?, as shown in Fig.
4.NP VPpt1August wasNPinsixththeNPNP PPpt2VPstarted PPin 735BCancient calendartheNPancient calendarthemonthFigure 4: Dropping & Reordering3.3 Substitution3.3.1 Word SubstitutionWord substitution only happens on the termi-nal nodes of the parse tree.
In our model, theconditioning features include the original wordand the substitution.
The substitution for a wordcan be another word or a multi-word expression(see Tab.
8).
The probability of a word substitu-tion operation can be calculated as P (sub|w) =SubFT (Substitution|Origin).Origin Substitution Prob.ancient ancient 0.963ancient old 0.0183ancient than transport 1.83E-102old ancient 0.005Table 8: Substitution Feature Table (SubFT)3.3.2 Phrase SubstitutionPhrase substitution happens on the non-terminal nodes and uses the same conditioningfeatures as word substitution.
The ?Origin?
con-sists of the leaves of the subtree rooted at thenode.
When we apply phrase substitution on anon-terminal node, then any simplification opera-tion (including dropping, reordering and substitu-tion) cannot happen on its descendants any more1356because when a node has been replaced then itsdescendants are no longer existing.
Therefore, foreach non-terminal node we must decide whether asubstitution should take place at this node or at itsdescendants.
We perform substitution for a non-terminal node if the following constraint is met:Max(SubFT (?|node)) ?Ych:nodeMax(SubFT (?|ch)).where ch is a child of the node.
???
canbe any substitution in the SubFT.
The proba-bility of the phrase substitution is calculated asP (sub|node) = SubFT (Substitution|Origin).Fig.
5 shows one of the possible substitution re-sults for our example where ?ancient?
is replacedby ?old?.NP VPpt1August wasNPinsixththeNPNP PPpt2VPstarted PPin 735BCold calendartheNPold calendarthemonthFigure 5: SubstitutionAs a result of all the simplification operations,we obtain the following two sentences: s1 =Str(pt1)=?August was the sixth month in the oldcalendar.?
and s2 = Str(pt2)=?The old calendarstarted in 735BC.
?3.4 The Probabilistic ModelOur model can be formalized as a direct transla-tion model from complex to simple P (s|c) multi-plied by a language model P (s) as shown in Equ.4.s = argmaxsP (s|c)P (s) (4)We combine the parts described in the previoussections to get the direct translation model:P (s|c) =??:Str(?
(c))=s(P (seg|c)P (com|seg)(5)?nodeP (dp|node)P (ro|node)P (sub|node)?w(sub|w)).where ?
is a sequence of simplification operationsand Str(?
(c)) corresponds to the leaves of a sim-plified tree.
There can be many sequences of op-erations that result in the same simplified sentenceand we sum up all of their probabilities.4 TrainingIn this section, we describe how we train the prob-abilities in the tables.
Following the work ofYamada and Knight (2001), we train our modelby maximizing P (s|c) over the training corpuswith the EM algorithm described in Alg.
2, us-ing a constructed graph structure.
We develop theTraining Tree (Fig.
6) to calculate P (s|c).
P (s|c)is equal to the inside probability of the root in theTraining Tree.
Alg.
3 and Alg.
4 are used to cal-culate the inside and outside probabilities.
We re-fer readers to Yamada and Knight (2001) for moredetails.Algorithm 2 EM Training (dataset)Initialize all probability tables using the uniform distribu-tion;for several iterations doreset al cnt = 0;for each sentence pair < c, s > in dataset dott = buildTrainingTree(< c, s >);calcInsideProb(tt);calcOutsideProb(tt);update cnt for each conditioning feature in eachnode of tt: cnt = cnt + node.insideProb ?node.outsideProb/root.insideProb;end forupdateProbability();end forrootspsp_res1 sp_res2dprompmp_res1 mp_res2submpmp_ressubsubdpromp_resrootspsp_res sp_resdproro_res ro_ressubro_ressubsubdproro_ressub_ressub_res sub_resFigure 6: Training Tree (Left) and Decoding Tree(Right)We illustrate the construction of the trainingtree with our running example.
There are twokinds of nodes in the training tree: data nodes inrectangles and operation nodes in circles.
Datanodes contain data and operation nodes executeoperations.
The training is a supervised learning1357process with the parse tree of c as input and thetwo strings s1 and s2 as the desired output.
rootstores the parse tree of c and also s1 and s2.
sp,ro, mp and sub are splitting, reordering, mappingand substitution operations.
sp res and mp resstore the results of sp and mp.
In our example,sp splits the parse tree into two parse trees pt1and pt2 (Fig.
3).
sp res1 contains pt1 and s1.sp res2 contains pt2 and s2.
Then dp, ro and mpare iteratively applied to each non-terminal nodeat each level of pt1 and pt2 from top to down.This process continues until the terminal nodesare reached or is stopped by a sub node.
The func-tion of mp operation is similar to the word map-ping operation in the string-based machine trans-lation.
It maps substrings in the complex sentencewhich are dominated by the children of the currentnode to proper substrings in the simple sentences.Speeding Up The example above is only oneof the possible paths.
We try all of the promis-ing paths in training.
Promising paths are thepaths which are likely to succeed in transform-ing the parse tree of c into s1 and s2.
We selectthe promising candidates using monolingual wordmapping as shown in Fig.
7.
In this example,only the word ?which?
can be a promising can-didate for splitting.
We can select the promisingcandidates for the dropping, reordering and map-ping operations similarly.
With this improvement,we can train on the PWKP dataset within 1 hourexcluding the parsing time taken by the StanfordParser.We initialize the probabilities with the uniformdistribution.
The binary features, such as SFT andBDFT, are assigned the initial value of 0.5.
ForDFT and RFT, the initial probability is 1N!
, whereN is the number of the children.
CFT is initial-ized as 0.25.
SubFT is initialized as 1.0 for anysubstitution at the first iteration.
After each itera-tion, the updateProbability function recalculatesthese probabilities based on the cnt for each fea-ture.Algorithm 3 calcInsideProb (TrainingTree tt)for each node from level = N to root of tt doif node is a sub node thennode.insideProb = P (sub|node);else if node is a mp OR sp node thennode.insideProb =Qchild child.insideProb;elsenode.insideProb =Pchild child.insideProb;end ifend forAlgorithm 4 calcOutsideProb (TrainingTree tt)for each node from root to level = N of tt doif node is the root thennode.outsideProb = 1.0;else if node is a sp res OR mp res node then{COMMENT: father are the fathers of the currentnode, sibling are the children of father excludingthe current node}node.outsideProb =Pfatherfather.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node thennode.outsideProb = father.outsideProb ?
1.0;else if node is a sp, ro, dp or sub node thennode.outsideProb = father.outsideProb ?P (sp or ro or dp or sub|node);end ifend forAugust was the sixth in the ancient Roman calendar statedwhich in 735BCAugust was the sixth in the old Roman calendar stated in 735BCThe old calendar...Complex sentenceSimple sentencesmonthmonthFigure 7: Monolingual Word Mapping5 DecodingFor decoding, we construct the decoding tree(Fig.
6) similarly to the construction of the train-ing tree.
The decoding tree does not have mp op-erations and there can be more than one sub nodesattached to a single ro res.
The root contains theparse tree of the complex sentence.
Due to spacelimitations, we cannot provide all the details of thedecoder.We calculate the inside probability and out-side probability for each node in the decodingtree.
When we simplify a complex sentence, westart from the root and greedily select the branchwith the highest outside probability.
For the sub-stitution operation, we also integrate a trigramlanguage model to make the generated sentencesmore fluent.
We train the language model withSRILM (Stolcke, 2002).
All the articles from theSimple Wikipedia are used as the training corpus,amounting to about 54 MB.6 EvaluationOur evaluation dataset consists of 100 complexsentences and 131 parallel simple sentences fromPWKP.
They have not been used for training.Four baseline systems are compared in our eval-uation.
The first is Moses which is a state ofthe art SMT system widely used as a baseline inMT community.
Obviously, the purpose of Mosesis cross-lingual translation rather than monolin-1358gual simplification.
The goal of our comparisonis therefore to assess how well a standard SMTsystem may perform simplification when fed witha proper training dataset.
We train Moses with thesame part of PWKP as our model.
The secondbaseline system is a sentence compression sys-tem (Filippova and Strube, 2008a) whose demosystem is available online.8 As the compressionsystem can only perform dropping, we further ex-tend it to our third and fourth baseline systems,in order to make a reasonable comparison.
In ourthird baseline system, we substitute the words inthe output of the compression system with theirsimpler synonyms.
This is done by looking upthe synonyms in Wordnet and selecting the mostfrequent synonym for replacement.
The word fre-quency is counted using the articles from SimpleWikipedia.
The fourth system performs sentencesplitting on the output of the third system.
Thisis simply done by splitting the sentences at ?and?,?or?, ?but?, ?which?, ?who?
and ?that?, and dis-carding the border words.
In total, there are 5systems in our evaluation: Moses, the MT sys-tem; C, the compression system; CS, the com-pression+substitution system; CSS, the compres-sion+substitution+split system; TSM, our model.We also provide evaluation measures for the sen-tences in the evaluation dataset: CW: complexsentences from Normal Wikipedia and SW: par-allel simple sentences from Simple Wikipedia.6.1 Basic Statistics and ExamplesThe first three columns in Tab.
9 present the ba-sic statistics for the evaluation sentences and theoutput of the five systems.
tokenLen is the aver-age length of tokens which may roughly reflect thelexical difficulty.
TSM achieves an average tokenlength which is the same as the Simple Wikipedia(SW).
senLen is the average number of tokens inone sentence, which may roughly reflect the syn-tactic complexity.
Both TSM and CSS produceshorter sentences than SW. Moses is very close toCW.
#sen gives the number of sentences.
Moses,C and CS cannot split sentences and thus produceabout the same number of sentences as availablein CW.Here are two example results obtained with ourTSM system.Example 1.
CW: ?Genetic engineering has ex-panded the genes available to breeders to utilizein creating desired germlines for new crops.?
SW:8http://212.126.215.106/compression/?New plants were created with genetic engineer-ing.?
TSM: ?Engineering has expanded the genesavailable to breeders to use in making germlinesfor new crops.
?Example 2.
CW: ?An umbrella term is a word thatprovides a superset or grouping of related con-cepts, also called a hypernym.?
SW: ?An umbrellaterm is a word that provides a superset or group-ing of related concepts.?
TSM: ?An umbrella termis a word.
A word provides a superset of relatedconcepts, called a hypernym.
?In the first example, both substitution and drop-ping happen.
TSM replaces ?utilize?
and ?cre-ating?
with ?use?
and ?making?.
?Genetic?
isdropped.
In the second example, the complex sen-tence is split and ?also?
is dropped.6.2 Translation AssessmentIn this part of the evaluation, we use traditionalmeasures used for evaluating MT systems.
Tab.
9shows the BLEU and NIST scores.
We use?mteval-v11b.pl?9 as the evaluation tool.
CWand SW are used respectively as source and ref-erence sentences.
TSM obtains a very high BLEUscore (0.38) but not as high as Moses (0.55).However, the original complex sentences (CW)from Normal Wikipedia get a rather high BLEU(0.50), when compared to the simple sentences.We also find that most of the sentences generatedby Moses are exactly the same as those in CW:this shows that Moses only performs few modi-fications to the original complex sentences.
Thisis confirmed by MT evaluation measures: if weset CW as both source and reference, the BLEUscore obtained by Moses is 0.78.
TSM gets 0.55in the same setting which is significantly smallerthan Moses and demonstrates that TSM is able togenerate simplifications with a greater amount ofvariation from the original sentence.
As shown inthe ?#Same?
column of Tab.
9, 25 sentences gen-erated by Moses are exactly identical to the com-plex sentences, while the number for TSM is 2which is closer to SW.
It is however not clear howwell BLEU and NIST discriminate simplificationsystems.
As discussed in Jurafsky and Martin(2008), ?BLEU does poorly at comparing systemswith radically different architectures and is mostappropriate when evaluating incremental changeswith similar architectures.?
In our case, TSM andCSS can be considered as having similar architec-tures as both of them can do splitting, dropping9http://www.statmt.org/moses/1359TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPLCW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353Table 9: Evaluationand substitution.
But Moses mostly cannot splitand drop.
We may conclude that TSM and Moseshave different architectures and BLEU or NIST isnot suitable for comparing them.
Here is an exam-ple to illustrate this: (CW): ?Almost as soon as heleaves, Annius and the guard Publius arrive to es-cort Vitellia to Titus, who has now chosen her ashis empress.?
(SW): ?Almost as soon as he leaves,Annius and the guard Publius arrive to take Vitel-lia to Titus, who has now chosen her as his em-press.?
(Moses): The same as (SW).
(TSM): ?An-nius and the guard Publius arrive to take Vitelliato Titus.
Titus has now chosen her as his empress.
?In this example, Moses generates an exactly iden-tical sentence to SW, thus the BLUE and NISTscores of Moses is the highest.
TSM simplifiesthe complex sentence by dropping, splitting andsubstitution, which results in two sentences thatare quite different from the SW sentence and thusgets lower BLUE and NIST scores.
Nevertheless,the sentences generated by TSM seem better thanMoses in terms of simplification.6.3 Readability AssessmentIntuitively, readability scores should be suitablemetrics for simplification systems.
We use theLinux ?style?
command to calculate the Fleschand Lix readability scores.
The results are pre-sented in Tab.
9.
?PE?
in the Flesch column standsfor ?Plain English?
and the ?Grade?
in Lix repre-sents the school year.
TSM achieves significantlybetter scores than Moses which has the best BLEUscore.
This implies that good monolingual trans-lation is not necessarily good simplification.
OOVis the percentage of words that are not in the Ba-sic English BE850 list.10 TSM is ranked as thesecond best system for this criterion.The perplexity (PPL) is a score of text proba-bility measured by a language model and normal-ized by the number of words in the text (Equ.
6).10http://simple.wikipedia.org/wiki/Wikipedia:Basic_English_alphabetical_wordlistPPL can be used to measure how tight the lan-guage model fits the text.
Language models con-stitute an important feature for assessing readabil-ity (Schwarm and Ostendorf, 2005).
We train atrigram LM using the simple sentences in PWKPand calculate the PPL with SRILM.
TSM gets thebest PPL score.
From this table, we can concludethat TSM achieves better overall readability thanthe baseline systems.PPL(text) = P (w1w2...wN )?1N (6)There are still some important issues to be con-sidered in future.
Based on our observations, thecurrent model performs well for word substitutionand segmentation.
But the completion of the newsentences is still problematic.
For example, wecopy the dependent NP to the new sentences.
Thismay break the coherence between sentences.
Abetter solution would be to use a pronoun to re-place the NP.
Sometimes, excessive droppings oc-cur, e.g., ?older?
and ?twin?
are dropped in ?Shehas an older brother and a twin brother...?.
Thisresults in a problematic sentence: ?She has anbrother and a brother...?.
There are also some er-rors which stem from the dependency parser.
InExample 2, ?An umbrella term?
should be a de-pendency of ?called?.
But the parser returns ?su-perset?
as the dependency.
In the future, we willinvestigate more sophisticated features and rulesto enhance TSM.7 ConclusionsIn this paper, we presented a novel large-scale par-allel dataset PWKP for sentence simplification.We proposed TSM, a tree-based translation modelfor sentence simplification which covers splitting,dropping, reordering and word/phrase substitutionintegrally for the first time.
We also described anefficient training method with speeding up tech-niques for TSM.
The evaluation shows that TSMcan achieve better overall readability scores thana set of baseline systems.1360ReferencesBarzilay, Regina and Noemie Elhadad.
2003.
Sen-tence alignment for monolingual comparable cor-pora.
In Proceedings of the 2003 Conference onEmpirical Methods in Natural Language Process-ing, pages 25?32.Carroll, John, Guido Minnen, Darren Pearce, YvonneCanning, Siobhan Devlin, and John Tait.
1999.Simplifying text for language-impaired readers.
InProceedings of the 9th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL?99), pages 269?270.Chandrasekar, R., Christine Doran, and B. Srinivas.1996.
Motivations and methods for text simpli-fication.
In Proceedings of the Sixteenth Inter-national Conference on Computational Linguistics(COLING?96), pages 1041?1044.Filippova, Katja and Michael Strube.
2008a.
Depen-dency tree based sentence compression.
In Inter-national Natural Language Generation Conference(INLG?08), pages 25?32.Filippova, Katja and Michael Strube.
2008b.
Sen-tence fusion via dependency graph compression.
InEMNLP ?08: Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 177?185.Graehl, Jonathan, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
In ComputationalLinguistics, volume 34, pages 391?427.
MIT Press.Heilman, M. and N. A. Smith.
2009.
Question gener-ation via overgenerating transformations and rank-ing.
Technical Report CMU-LTI-09-013, LanguageTechnologies Institute, Carnegie Mellon University.Inui, Kentaro, Atsushi Fujita, Tetsuro Takahashi, RyuIida, and Tomoya Iwakura.
2003.
Text simplifi-cation for reading assistance: A project note.
InProceedings of the 2nd International Workshop onParaphrasing: Paraphrase Acquisition and Appli-cations (IWP), pages 9?16.Jonnalagadda, Siddhartha and Graciela Gonzalez.2009.
Sentence simplification aids protein-proteininteraction extraction.
In Proceedings of the 3rdInternational Symposium on Languages in Biologyand Medicine.Jurafsky, Daniel and James H. Martin.
2008.
Speechand Language Processing (2nd Edition).
PrenticeHall, 2 edition.Klein, Dan and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems 15 (NISP?02), pages 3?10.Knight, Kevin and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In AAAI, pages 703?710.Levenshtein.
1966.
Binary code capable of correct-ing deletions, insertions and reversals.
In SovietPhysics, pages 707?710.Nelken, Rani and Stuart M. Shieber.
2006.
To-wards robust context-sensitive sentence alignmentfor monolingual corpora.
In Proceedings of 11thConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 161?168.Petersen, Sarah E. and Mari Ostendorf.
2007.
Textsimplification for language learners: a corpus anal-ysis.
In Proc.
of Workshop on Speech and LanguageTechnology for Education, pages 69?72.Schmid, Helmut.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing,pages 44?49.Schwarm, Sarah E. and Mari Ostendorf.
2005.
Read-ing level assessment using support vector machinesand statistical language models.
In ACL?05: Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 523?530.Siddharthan, Advaith.
2002.
An architecture for atext simplification system.
In Proceedings of theLanguage Engineering Conference (LEC?02), pages64?71.Siddharthan, Advaith.
2006.
Syntactic simplifica-tion and text cohesion.
In Research on Language& Computation, volume 4, pages 77?109.
SpringerNetherlands, June.Stolcke, Andreas.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
pages 901?904.Vickrey, David and Daphne Koller.
2008.
Sentencesimplification for semantic role labeling.
In Pro-ceedings of ACL-08: HLT, pages 344?352, June.Watanabe, Willian Massami, Arnaldo Candido Junior,Vin?
?cius Rodriguez Uze?da, Renata Pontin de Mat-tos Fortes, Thiago Alexandre Salgueiro Pardo, andSandra Maria Alu??sio.
2009.
Facilita: reading as-sistance for low-literacy readers.
In SIGDOC ?09:Proceedings of the 27th ACM international confer-ence on Design of communication, pages 29?36.ACM.Yamada, Kenji and Kevin Knight.
2001.
A syntax-based statistical translation model.
In ACL?01: Pro-ceedings of the 39th Annual Meeting on Associationfor Computational Linguistics, pages 523?530.Yamada, Kenji and Kevin Knight.
2002.
A decoder forsyntax-based statistical mt.
In ACL?02: Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics, pages 303?310.Zesch, Torsten, Christof Mu?ller, and Iryna Gurevych.2008.
Extracting Lexical Semantic Knowledgefrom Wikipedia and Wiktionary.
In Proceedingsof the Sixth International Language Resources andEvaluation (LREC?08), pages 1646?1652.Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li.2009.
Application-driven statistical paraphrase gen-eration.
In Proceedings of ACL-IJCNLP, pages834?842, Suntec, Singapore, August.1361
