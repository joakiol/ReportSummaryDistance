Dialogue Act Tagging with Transformation-Based LearningKen Samuel  and Sandra  Carber ry  and K.  V i jay -ShankerDepartment  of Computer  and Information SciencesUniversity of DelawareNewark, Delaware 19716 USA{samuel,carberry, vijay}@cis.udel.eduht tp : / /www.eec is .ude l .edu/ -  { samuel,carberry, vij ay } /Abst rac tFor the task of recognizing dialogue acts, we areapplying the Transformation-Based Learning(TBL) machine learning algorithm.
To circum-vent a sparse data problem, we extract valuesof well-motivated features of utterances, suchas speaker direction, punctuation marks, and anew feature, called dialogue act cues, which wefind to be more effective than cue phrases andword n-grams in practice.
We present strate-gies for constructing a set of dialogue act cuesautomatically by minimizing the entropy of thedistribution of dialogue acts in a training cor-pus, filtering out irrelevant dialogue act cues,and clustering semantically-related words.
Inaddition, to address limitations of TBL, we in-troduce a Monte Carlo strategy for training ef-ficiently and a committee method for comput-ing confidence measures.
These ideas are com-bined in our working implementation, which la-bels held-out data as accurately as any otherreported system for the dialogue act taggingtask.In t roduct ionAlthough machine learning approaches haveachieved success in many areas of Natural Lan-guage Processing, researchers have only recentlybegun to investigate applying machine learn-ing methods to discourse-level problems (Re-ithinger and Klesen, 1997; Di Eugenio et al,1997; Wiebe et al, 1997; Andernach, 1996; Lit-man, 1994).
An important ask in discourseunderstanding is to interpret an utterance's di-alogue act, which is a concise abstraction of aspeaker's intention, such as SUGGEST and AC-CEPT.
Recognizing dialogue acts is critical fordiscourse-level understanding and can also beuseful for other applications, such as resolvingambiguity in speech recognition.
However, com-puting dialogue acts is a challenging task, be-cause often a dialogue act cannot be directlyinferred from a literal interpretation of an ut-terance.We have investigated applying Transforma-tion-Based Learning (TBL) to the task of com-puting dialogue acts.
This method, which hasnot been used previously in discourse, has anumber of attractive characteristics for our task.However, it also has some limitations, which weaddress with a Monte Carlo strategy that sig-nificantly improves the training time efficiencywithout compromising accuracy and a commit-tee method that enables TBL to compute con-fidence measures for the dialogue acts assignedto utterances.Our machine learning algorithm makes useof abstract features extracted from utterances.In addition, we utilize an entropy-minimizationapproach to automatically identify dialogue actcues, which are words and short phrases thatserve as signals for dialogue acts.
Our experi-ments demonstrate hat dialogue act cues tendto be more effective than cue phrases and wordn-grams, and this strategy can be further im-proved by adding a filtering mechanism and asemantic-clustering method.
Although we stillplan to implement more modifications, our sys-tem has already achieved success rates compa-rable to the best reported results for computingdialogue acts.T rans format ion -Based  Learn ingTo compute dialogue acts, we are using a mod-ified version of Brill's (1995a) Transformation-Based Learning method.
Given a tagged train-ing corpus, TBL develops a learned model thatconsists of a sequence of rules.
For example, inone experiment, our system produced 213 rules;the first five rules are presented in Figure 1.
Tolabel a new corpus of dialogues with dialogue1150acts, the rules are applied, in turn, to every ut-terance in the corpus, and each utterance thatsatisfies the conditions of a rule is relabeled withthat rule's new tag.
For example, the first rulein Figure 1 labels every utterance with the tagSUGGEST.
Then, after the second, third, andfourth rules are applied, the fifth rule changesan utterance's tag to REJECT if it includes theword "no", and the preceding utterance is cur-rently tagged SUGGEST.
Note that an utter-ance's tag may change several times as the dif-ferent rules in the sequence are applied.#1234Condition(s) New Tagnone SUGGESTIncludes "see" & "you" BYEIncludes "sounds" ACCEPTLength < 4 words GREETPrec.
tag is none 1Includes "no" REJECTPrec.
tag is SUGGESTFigure h Rules produced by the systemTo develop a sequence of rules from a taggedtraining corpus, TBL attempts to produce rulesthat will correctly label many of the utterancesin the training data.
The system first gener-ates all of the potential rules that would makeat least one label in the training corpus correct.For each potential rule, its improvement score isdefined to be the number of correct tags in thetraining corpus after the rule is applied minusthe number of correct tags in the training cor-pus before the rule is applied.
The potential rulewith the highest improvement score is appliedto the entire training corpus and output as thenext rule in the learned model.
This process re-peats (using the new tags assigned to utterancesin the training corpus), producing one rule foreach pass through the training data, until norule can be found with an improvement scorethat surpasses some predefined threshold, O.Since there are potentially an infinite numberof rules that could produce the dialogue actsin the training data, it is necessary to restrictthe range of patterns that the system canconsider by providing a set of rule templates.The system replaces variables in the templateswith appropriate values to generate rules.For example, the following template can be1This condition is true only for the first utterance ofa dialogue.instantiated with w="no", X=SUGGEST,and Y=REJECT to produce the last rule inFigure 1.IF utterance u contains the word wAND the tag on the utterance preceding u is XTHEN change u's tag to Y__We have observed that TBL has a numberof attractive characteristics for the task of com-puting dialogue acts.
TBL has been effective ona similar 2 task, Part-of-Speech Tagging (Brill,1995a).
Also, TBL's rules are relatively intu-itive, so a human can analyze the rules to deter-mine what the system has learned and perhapsdevelop a theory.
TBL is very good at discard-ing irrelevant rules, because the effect of irrel-evant rules on a training corpus is essentiallyrandom, resulting in low improvement scores.In addition, our implementation can accommo-date a wide variety of different ypes of features,including set-valued features, features that con-sider the context of surrounding utterances, andfeatures that can take distant context into ac-count.
These and other attractive characteris-tics of TBL are discussed further in Samuel etal.
(1998b).D ia logue  Act  Tagg ingTo address a significant concern in machinelearning, called the sparse data problem, wemust select an appropriate set of features.
Re-searchers in discourse, such as Grosz and Sidner(1986), Lambert (1993), Hirschberg and Litman(1993), Chen (1995), Andernach (1996), Samuel(1996), and Chu-Carroll (1998) have suggestedseveral features that might be relevant for thetask of computing dialogue acts.
Our systemcan consider the following features of an ut-terance: 1) the cue phrases 3 in the utterance;2) the word n-grams 3 in the utterance; 3) thedialogue act cues 3 in the utterance; 4) the en-tire utterance for one-, two-, or three-word ut-terances; 5) speaker information 4 for the utter-2The part-of-speech tag of a word is dependent on theword's internal features and on the surrounding words;similarly, the dialogue act of an utterance is dependenton the utterance's internal features and on the surround-ing utterances.~This feature is defined later in this section.4In our system, we are handling speaker informationdifferently from the previous research.
For example, Rei-thinger and Klesen (1997) combine the speaker direction1151ance; 6) the punctuation marks found in theutterance; 7) the number of words in the ut-terance; 8) the dialogue acts on the precedingutterances; and 9) the dialogue acts on the fol-lowing 5 utterances.
Other features that we stillplan to implement include: 10) surface speechacts, to represent the syntactic structure of theutterance in an abstract format; 11) the focus-ing information, specifying which preceding ut-terance should be considered the most salientwhen interpreting the current utterance; 12) thetype of the subject of the utterance; and 13) thetype of the main verb of the utterance.Like other researchers, we recognize thatthe specific word substr ings (words and shortphrases) in an utterance can provide impor-tant clues for discourse processing, so we shouldutilize a feature that captures this informa-tion.
Hirschberg and Litman (1993) and Knott(1996) have identified sets of cue phrases.
Un-fortunately, we have found that these manually-selected sets of cue phrases are insufficient forour task, as they were motivated by differentdomains and tasks, and these sets may be in-complete.Reithinger and Klesen (1997) utilized wordn-grams, which are all of the word substrings(with a reasonable bound on the length) in thetraining corpus.
However, although TBL is ca-pable of discarding irrelevant rules, if it is bom-barded by an overwhelming number of irrele-vant rules, performance may begin to suffer.This is because the improvement scores of ir-relevant rules are random, so if the system gen-erates too many of these rules, some of theirscores might, by chance, be high enough for se-lection in the final model, where they can affectperformance on new data.As a happy medium between the two ex-with the dialogue act to make act-speaker pairs, such as<SUGGEST,A-+B> and <REJECT,B-~A>.
But webelieve it is more effective to use the change of speakerfeature, which is defined to be false if the speaker of thecurrent utterance is the same as the speaker of the im-mediately preceding utterance, and true otherwise.5If the system is participating in the dialogue, ratherthan simply listening, the future context may not alwaysbe available.
But for an utterance that is in the middleof a speaker's turn, it is reasonable to consider the subse-quent utterances within that same turn.
And also, whenutterances from the later turns do become available, itmay be important o use this information to re-evaluateany dialogue acts that were computed and determine ifthe system might have misunderstood.tremes of using a small set of hand-picked cuephrases and considering the complete set ofword n-grams, we are automating the analy-sis of the training corpus to determine whichword substrings are relevant.
We introduce anew feature called dialogue act cues: word sub-strings that appear frequently in dialogue andprovide useful clues to help determine the ap-propriate dialogue acts.
To collect dialogue actcues automatically from a training corpus, ourstrategy is to select word substrings of one, two,or three words to minimize the entropy of thedistribution of dialogue acts given a substring.A substring is selected if the dialogue acts co-occurring with it have a sufficiently low entropy,discarding sparse data.
Specifically,C de=f {sES  \[ H(D Is )  <01 A #(s )>02}where C is the set of dialogue act cues, Sis the set of word substrings, D is the setof dialogue acts, 01 and 02 are predefinedthresholds, #(x) is the number of times anevent, x, occurs in the training corpus, andentropy 6 is defined in the standard way: 7H(D\[s) de__f __ ~"~dED P(dls)log 2P(d\[s).The desirable dialogue act cues produced byour experiments can be organized into three cat-egories.
Tradit ional  cues are those cue phrasesthat have previously been reported in the lit-erature, such as "but" and "so"; potent ia l  cuesconsist of other useful word substrings that havenot been considered, such as "thanks" and "seeyou"; and for dialogues from a particular do-main, there may be domain  cues - -  for example,the appointment-scheduling corpora have dia-logue act cues, such as "what time" and "busy".Dialogue act cues in the first two categoriescan be utilized for learning general rules thatshould apply across domains, while the thirdcategory constitutes information that can fine-tune a model for a particular domain.But this method is not sufficiently restrictive;it selects many word substrings that do not sig-6The entropy is capturing the distribution of dialogueacts for utterances with a given word substring.
By min-imizing entropy, we are selecting a word substring if itproduces a highly skewed distribution of the dialogueacts, and thus, if this word substring is found in an ut-terance, it is relatively easy to determine the proper di-alogue act.Tin practice, we estimate the probabilities with:#(d&:s) P(dJs) ~ #(,) .1152CategoryTraditional cuesPotential cuesDomain cuesSuperstring cues...with filteringUndesirable cuesI #567142690472170I Examples"and", "because", "but", "so", "then""bye", "how 'bout", "see you", "sounds", "thanks""busy", "meet", "o'clock", "tomorrow", "what time""and then", "but the", "how 'bout the", "okay I", "so we""and then", "but the", "no I", "okay with", "so we""a", "be", "had", "in the", "to"Figure 2: A set of dialogue act cues divided into five categoriesnal dialogue acts.
In many cases, an undesirabledialogue act cue contains a useful dialogue actcue as a substring, so it should be relatively easyto eliminate.
Examples of these superstring cuesinclude "but the" and "okay I".
We have im-plemented a straightforward filtering functionto address this problem.
If a dialogue act cue,such as "how 'bout the" is subsumed by a moregeneral dialogue act cue with a better entropyscore, such as "how 'bout", then the first di-alogue act cue only offers redundant informa-tion, and so it should be removed from the setof dialogue act cues to minimize the number ofirrelevant rules that are generated.
Our filterdeletes a dialogue act cue if one of its substringshappens to be another dialogue act cue with abetter or equivalent entropy score.Another effective heuristic is to cluster cer-tain dialogue act cues into semantic classes,which can collapse several potential rules intoa single rule with significantly more data sup-porting it.
For example, in the appointment-scheduling corpora, there is a strong correla-tion between weekdays and the SUGGEST di-alogue act, but to express this fact, it is nec-essary to generate five separate rules.
How-ever, if the five weekdays are combined un-der one label, "$weekday$", then the same in-formation can be captured by a single rulethat has five times as much data supportingit: "$weekday$" ==v SUGGEST.
We have ex-perimented with clusters, such as "$weekday$","$month$", "$number$", "$ordinal-number$",and "$proper-name$".We collected a set of dialogue act cues,clustering words in six semantic lasses, with01 = H(T) (the entropy of the dialogue acts)and 02 = 6.
As shown in Figure 2, these dia-logue act cues were distributed among the fourcategories described above, with an additionalcategory for the remaining undesirable cues.Note that our simple filtering technique success-fully eliminated 218 of the superstring cues.
Weplan to investigate more sophisticated filteringapproaches to target the remaining 472 super-string cues.L imi ta t ions  o f  TBLAlthough we have argued for the use ofTransformation-Based Learning for dialogue acttagging, we have discovered a significant limita-tion of the algorithm: The rule templates usedby TBL must be developed by a human, in ad-vance.
Since the omission of any relevant em-plates would handicap the system, it is essentialthat these choices be made carefully.
But, in di-alogue act tagging, nobody knows exactly whichfeatures and feature interactions are relevant, sowe would prefer to err on the side of caution byconstructing an overly-general set of templates,allowing the system to learn which templatesare effective.
Unfortunately, in training, TBLmust generate all of the potential rules for eachutterance during each pass through the train-ing data, and our experimental results indicatethat it is necessary to severely limit the numberof potential rules that may be generated, or thememory and time costs are so exorbitant hatthe method becomes intractable.Our solution to this problem is to implementa Monte Carlo version of TBL to relax the re-striction that TBL must perform an exhaus-tive search.
In a given pass through the train-ing data, for each utterance that is incorrectlytagged, only R of the possible template instan-tiations are randomly selected, where R is a pa-rameter that is set in advance.
As long as Ris large enough, there doesn't appear to be anysignificant degradation i performance.
We be-lieve that this is because the best rules tendto be effective for many different utterances, othere are many opportunities to find these rulesduring training; the better a rule is, the morelikely it is to be generated.
So, although ran-1153dom sampling will miss many rules, it is stillhighly likely to find the best rules.Experimental tests show that this extensionenables the system to efficiently and effectivelyconsider a large number of potential rules.
Thisincreases the applicability of the TBL methodto tasks where the relevant features and featureinteractions are not known in advance as wellas tasks where there are many relevant featuresand feature interactions.
In addition, it is nolonger critical that the human developer iden-tify a minimal set of templates, and so this im-provement decreases the labor demands on thehuman developer.Unlike probabilistic machine learning ap-proaches, TBL fails to offer any measure of con-fidence in the tags that it produces.
Confidencemeasures are useful in a wide variety of ways;for example, we foresee that our module for tag-ging dialogue acts can potentially be integratedinto a larger system so that, when TBL cannotproduce a tag with high confidence, other mod-ules may be invoked to provide more evidence.Unfortunately, due to the nature of the TBLmethod, straightforward approaches for track-ing the confidence of a rule during training havebeen unsuccessful.
To address this problem,we are using the Committee-Based Samplingmethod (Dagan and Engelson, 1995) and theBoosting method (Freund and Schapire, 1996)in a novel way: The system is trained multi-ple times, to produce a few different but rea-sonable models for the training data.
s To con-struct these models, we adopted the strategyintroduced in the Boosting method, by biasingthe later models to focus on those utterances (inthe training set) that the earlier models taggedincorrectly.
Then, given new data, each modelindependently tags the input, and the responsesare compared.
A given tag's confidence measure~s based on how well the different models agreeon that tag.
Our preliminary results with fivemodels show that this strategy produces use-ful confidence measures - -  for nearly half of theutterances, all five models agreed on the tag,and over 90% of those tags were correct.
Inaddition, the overall accuracy of our system in-SWith the efficiencies introduced by our use of fea-tures, dialogue act cue selection, and the Monte Carloapproach, we can implement modifications that requiremultiple xecutions of the algorithm, which would be in-feasible otherwise.creased significantly.
More details on this workare presented in Samuel et al (1998b).Experimental  ResultsA survey of the other research projects thathave applied machine learning methods to thedialogue act tagging task is presented in Samuelet al (1998a).
The highest success rate was re-ported by Reithinger and Klesen (1997), whosesystem could correctly label 74.7% of the utter-ances in a test corpus.
Their work utilized anN-Grams approach, in which an utterance's di-alogue act was based on substrings of words aswell as the dialogue acts and speaker informa-tion from the preceding two utterances.
Vari-ous probabilities were estimated from a trainingcorpus by counting the frequencies of specificevents, such as the number of times that eachpair of consecutive words co-occurred with eachdialogue act.As a direct comparison, we applied our sys-tem to Reithinger and Klesen's training set (143dialogues, 2701 utterances) and disjoint testingset (20 dialogues, 328 utterances), which consistof utterances labeled with 18 different dialogueacts.
Using semantic lustering, (9 -- 1 (the im-provement score threshold), R = 14 (the MonteCarlo sample size), a set of dialogue act cues,change of speaker, the dialogue act on the pre-ceding utterance, and other features, our sys-tem achieved an average accuracy score overfive 9 runs of 75.12% (a=1.34%), including ahigh score of 77.44%.
We have also run di-rect comparisons between our system and Deci-sion Trees, determining that our system's per-formance is also comparable to this popular ma-chine learning method (Samuel et al, 1998b).Figure 3 presents a series of experimentswhich vary the set of word substrings utilizedby the system, l?
Each experiment was run tentimes, and the results were compared using atwo-tailed t test to determine that all of the ac-curacy differences were significant at the 0.05level, except for the differences between rows 3& 4, rows 4 &: 5, rows 4 & 6, rows 5 & 6, rows5 & 7, and rows 6 & 7.9This is to factor out the random aspect of the MonteCarlo method.1?Note that these results cannot be compared with theresults presented above, since several parameter valuesdiffer between the two sets of experiments.11There are only 478 different cue phrases in the set,but for our system, it was necessary to manipulate the1154Word SubstringsNoneCue phrases (from previous literature) nWord n-gramsEntropy minimizationEntropy minimization with clusteringEntropy minimization with filteringEntropy minimization with filtering and clustering#09361627110531029826811Accuracy41.16% (a=O.O0%)61.74% (a--0.69%)69.21% (a=0.94%)69.54% (a=1.97%)70.18% (a=0.75%)70.70% (a=1.31%)71.22% (a=1.25%)Figure 3: Tagging accuracy on held-out data, using different sets of word substrings in trainingAs the figure shows, when the system was re-stricted from using any word substrings, its ac-curacy on unseen data was only 41.16%.
Whengiven access to all of the cue phrases proposedin previous work, 12 the accuracy rises signifi-cantly (p < 0.001) to 61.74%.
But this result issignificantly lower (p < 0.001) than the 69.21%accuracy produced by using all substrings ofone, two, or three words (word n-grams) in thetraining data, as Reithinger and Klesen (1997)did.
And the entropy-minimization approachwith the filtering and clustering techniques pro-duce dialogue act cues that cause the accu-racy to rise significantly further (p = 0.003) to71.22%.Our experimental results show that the cuephrases identified in the literature do not cap-ture all of the word substrings that signal di-alogue acts.
On the other hand, the completeset of word n-grams causes the performance ofTBL to suffer.
Our dialogue act cues generatethe highest accuracy scores, using significantlyfewer word substrings than the word n-gramsapproach.D iscuss ionThis paper has presented the first attemptto apply Transformation-Based Learning todiscourse-level problems.
We utilized variousfeatures of utterances to learn effectively from arelatively small amount of data, and we have de-veloped an entropy-minimization approach withfiltering and clustering that automatically col-lects useful dialogue act cues from tagged train-ing data.
In addition, we have devised a Montedata in various ways, such as including acapitalized ver-sion of each cue phrase and splitting up contractions.12See Hirschberg and Litman (1993) and Knott (1996)for these lists of cue phrases.
We also included 45 cuephrases that we pinpointed by manually analyzing acompletely different set of dialogues, two years beforewe began working with the VERBMOBIL corpora.Carlo strategy and a committee method to ad-dress some limitations of TBL.
Although wehave only begun implementing our ideas, oursystem has already matched Reithinger andKlesen's success rate in computing dialogueacts.In the future, we plan to implement more fea-tures, improve our method for collecting dia-logue act cues, and investigate how these mod-ifications improve our system's performance.Also, for the semantic-clustering technique, weselected the clusters of words by hand, but itwould be interesting to see how a taxonomy,such as WordNet could be used to automate thisprocess.When there is not enough tagged train-ing data available, we would like the systemto learn from untagged data.
Dagan andEngelson's (1995) Committee-Based Samplingmethod constructed multiple learned modelsfrom a small set of tagged data, and then, onlywhen the models disagreed on a tag, a hu-man was consulted for the correct tag.
Brill(1995b) developed an unsupervised version ofTBL for Part-of-Speech Tagging, but this algo-rithm must be initialized with words that canbe tagged unambiguously, 13 and in discourse,there are very few unambiguous examples.
Weintend to investigate a weakly-supervised ap-proach that utilizes the confidence measures de-scribed above.
First, the system will be trainedon a relatively small set of tagged data, pro-ducing a few different models.
Then, given un-tagged data, it will use the models to derivedialogue acts with confidence measures.
Thosetags that receive high confidence can be usedas unambiguous examples to drive the unsuper-vised version of TBL.While we contend that machine learning canbe an effective tool for identifying dialogue acts,13For example, "the" is always a Determiner.1155we do realize that machine learning may not beable to completely solve this problem, as it isunable to capture some relevant factors, suchas common-sense world knowledge.
We envisionthat our system may potentially be integratedinto a larger system that uses confidence mea-sures to determine when world knowledge infor-mation is required.AcknowledgmentsWe wish to thank the members of the VERBMo-BIL research group at DFKI in Germany, partic-ularly Norbert Reithinger, Jan Alexandersson,and Elisabeth Maier, for providing us with theopportunity to work with them and generouslygranting us access to the VERBMOBIL corpora.This work was partially supported by the NSFGrant #GER-9354869.ReferencesToine Andernach.
1996.
A machine learning ap-proach to the classification of dialogue utter-ances.
In Proceedings of NeMLaP-2.Eric Brill.
1995a.
Transformation-based error-driven learning and natural anguage process-ing: A case study in part-of-speech tagging.Computational Linguistics, 21(4):543-566.Eric Drill.
1995b.
Unsupervised learning ofdisambiguation rules for part of speech tag-ging.
In Proceedings of the Very Large Cor-pora Workshop.Kuang-Hua Chen.
1995.
Topic identificationin discourse.
In Proceedings of the Sev-enth Meeting of the European Association forComputational Linguistics, pages 267-271.Jennifer Chu-Carroll.
1998.
A statistical modelfor discourse act recognition in dialogue in-teractions.
In Applying Machine Learning toDiscourse Processing: Papers from the 1998AAAISpring Symposium, pages 12-17.
Tech-nical Report ~SS-98-01.Ido Dagan and Sean P. Engelson.
1995.Committee-based sampling for training prob-abilistic classifiers.
In Proceedings of the 12thInternational Conference on Machine Learn-ing, pages 150-157.Barbara Di Eugenio, Johanna D. Moore, andMassimo Paolucci.
1997.
Learning featuresthat predict cue usage.
In Proceedings of the35th Annual Meeting of the A CL, pages 80-87.Yoav Freund and Robert E. Schapire.
1996.Experiments with a new boosting algorithm.In Proceedings of the Thirteenth InternationalConference on Machine Learning.Barbara Grosz and Candace Sidner.
1986.Attention, intentions, and the structureof discourse.
Computational Linguistics,12(3):175-204.Julia Hirschberg and Diane Litman.
1993.Empirical studies on the disambiguationof cue phrases.
Computational Linguistics,19(3):501-530.Alistair Knott.
1996.
A Data-Driven Methodol-ogy for Motivating a Set of Coherence Rela-tions.
Ph.D. thesis, University of Edinburgh.Lynn Lambert.
1993.
Recognizing ComplexDiscourse Acts: A Tripartite Plan-BasedModel of Dialogue.
Ph.D. thesis, The Univer-sity of Delaware.
Technical Report #93-19.Diane J. Litman.
1994.
Classifying cue phrasesin text and speech using machine learning.
InProceedings of the 12th National Conferenceon Artificial Intelligence, pages 806-813.Norbert Reithinger and Martin Klesen.
1997.Dialogue act classification using languagemodels.
In Proceedings of EuroSpeech-97,pages 2235-2238.Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998a.
Computing dialogue actsfrom features with transformation-basedlearning.
In Applying Machine Learning toDiscourse Processing: Papers from the 1998AAAI Spring Symposium, pages 90-97.
Tech-nical Report #SS-98-01.Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998b.
An investigation oftransformation-based l arning in discourse.In Machine Learning: Proceedings of the Fif-teenth International Conference.Kenneth B. Samuel.
1996.
Using statisticallearning algorithms to compute discourse in-formation.
Technical Report #97-11, TheUniversity of Delaware.
Dissertation pro-posal.Janyce Wiebe, Tom O'Hara, Kenneth McKee-ver, and Thorsten Oehrstroem-Sandgren.1997.
An empirical approach to temporal ref-erence resolution.
In Proceedings of the Sec-ond Conference on Empirical Methods in Nat-ural Language Processing, pages 174-186.1156
