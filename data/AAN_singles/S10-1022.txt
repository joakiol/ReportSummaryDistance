Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 108?111,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTANL-1: Coreference Resolution by Parse Analysis andSimilarity ClusteringGiuseppe AttardiDipartimento di InformaticaUniversit?
di PisaLargo B. Pontecorvo, 3attardi@di.unipi.itStefano Dei RossiDipartimento di InformaticaUniversit?
di PisaLargo B. Pontecorvo, 3deirossi@di.unipi.itMaria SimiDipartimento di InformaticaUniversit?
di PisaLargo B. Pontecorvo, 3simi@di.unipi.itAbstractOur submission to the Semeval 2010 taskon coreference resolution in multiple lan-guages is based on parse analysis and si-milarity clustering.
The system uses a bi-nary classifier, based on Maximum En-tropy, to decide whether or not there is arelationship between each pair of men-tions extracted from a textual document.Mention detection is based on the analy-sis of the dependency parse tree.1 OverviewCoreference resolution can be described as theproblem of clustering noun phrases (NP), alsocalled mentions, into sets referring to the samediscourse entity.The ?Coreference Resolution in Multiple Lan-guages task?
at SemEval-2010 is meant to assessdifferent machine learning techniques in a multi-lingual context, and by means of differentevaluation metrics.
Two different scenarios areconsidered: a gold standard scenario (only avail-able for Catalan and Spanish), where correctmention boundaries are provided to the partici-pants, and a regular scenario, where mentionboundaries are to be inferred from other linguis-tic annotations provided in the input data.
In par-ticular the linguistic annotations provided foreach token in a sentence are: position in sentence(ID), word (TOKEN), lemma and predictedlemma (LEMMA and PLEMMA), morpho-syntactic information, both gold and/or predicted(POS and PPOS, FEAT and PFEAT), depend-ency parsing annotations (HEAD and PHEAD,DEPREL and PDEPREL), named entities (NEand PNE), and semantic roles (PRED, PPRED,and corresponding roles in the following col-umns).
In the gold scenario, mention boundariesannotations (in column COREF) can also be usedas input.Our approach to the task was to split corefer-ence resolution into two sub-problems: mentionidentification and creation of entities.
Mentionrecognition was based on the analysis of parsetrees produced from input data, which were pro-duced by manual annotation or state-of-the-artdependency parsers.
Once the mentions are iden-tified, coreference resolution involves partition-ing them into subsets corresponding to the sameentity.
This problem is cast into the binary classi-fication problem of deciding whether two givenmentions are coreferent.
A Maximum Entropyclassifier is trained to predict how likely twomentions refer to the same entity.
This is fol-lowed by a greedy procedure whose purpose is tocluster mentions into entities.According to Ng (2005), most learning basedcoreference systems can be defined by four ele-ments: the learning algorithm used to train thecoreference classifier, the method of creatingtraining instances for the learner, the feature setused to represent a training or test instance, andthe clustering algorithm used to coordinate thecoreference classification decisions.
In the fol-lowing we will detail our approach by makingexplicit the strategies used in each of above men-tioned components.The data model used by our system is basedon the concepts of entity and mention.
The col-lection of mentions referring to the same objectin a document forms an entity.
A mention is aninstance referring to an object: it is representedby the start and end positions in a sentence, atype and a sequence number.
For convenience italso contains a frequency count and a referenceto the containing sentence.1082 Mention detectionThe first stage of the coreference resolutionprocess tries to identify the occurrence of men-tions in documents.In the training phase mentions are obtainedfrom the NE (or PNE) column of the corpus andare partitioned into entities using the informationprovided in the COREF column.In the regular setting, we used an algorithm forpredicting boundaries that relies on the parse treeof the sentence produced from the gold annota-tions in columns HEAD and DEP, if available, orelse from columns PHEAD and PDEP, the out-put of a dependency parser provided as input da-ta.This analysis relied on minimal languageknowledge, in order to determine possible headsof sub-trees counting as mentions, i.e.
nounphrases or adverbial phrases referring to quanti-ties, times and locations.
POS tags and morpho-logical features, when available, were mostlytaken into account in determining mention heads.The leaves of the sub-trees of each detected headwere collected as possible mentions.The mentions identified by the NE columnwere then added to this set, discarding duplicatesor partial overlaps.
Partial overlaps in principleshould not occur, but were present occasionallyin the data.
When this occurred, we applied astrategy to split them into a pair of mentions.The same mention detection strategy was usedalso in the gold task, where we could have justreturned the boundaries present in the data, scor-ing 100% in accuracy.
This explains the smallloss in accuracy we achieved in mention identifi-cation in the gold setting.Relying on parse trees turned out to be quiteeffective, especially for languages where goldparses where available.
For some other languag-es, the strategy was less effective.
This was dueto different annotation policies across differentlanguages, and, in part, to inconsistencies in thedata.
For example in the Italian data set, namedentities may include prepositions, which are typ-ically the head of the noun phrase, while ourstrategy of looking for noun heads leaves thepreposition out of the mention boundaries.Moreover this strategy obviously fails whenmentions span across sentences as was the case,again, for Italian.3 Determining coreferenceFor determining which mentions belong to thesame entity, we applied a machine learning tech-nique.
We trained a Maximum Entropy classifierwritten in Python (Le, 2004) to determinewhether two mentions refer to the same entity.We did do not make any effort to optimize thenumber of training instances for the pair-wiselearner: a positive instance is created for eachanaphoric NP, paired with each of its antecedentswith the same number, and a negative instance iscreated by pairing each NP with each of its pre-ceding non-coreferent noun phrases.The classifier is trained using the followingfeatures, extracted for each pair of mentions.Lexical featuresSame: whether two mentions are equal;Prefix: whether one mention is a prefix ofthe other;Suffix: whether one mention is a suffix ofthe other;Acronym: whether one mention is theacronym of the other.Edit distance: quantized editing distancebetween two mentions.Distance featuresSentence distance: quantized distance be-tween the sentences containing the twomentions;Token distance: quantized distance be-tween the start tokens of the two mentions;Mention distance: quantized number ofother mentions between two mentions.Syntax featuresHead: whether the heads of two mentionshave the same POS;Head POS: pairs of POS of the two men-tions heads;Count featuresCount: pairs of quantized numbers, eachcounting how many times a mention oc-curs.Type featuresType: whether two mentions have thesame associated NE (Named Entity) type.Pronoun features109When the most recent mention is a pronominalanaphora, the following features are extracted:Gender: pair of attributes {female, male orundetermined};Number: pair of attributes {singular, plur-al, undetermined};Pronoun type: this feature is language de-pendent and represents the type of prono-minal mention, i.e.
whether the pronoun isreflexive, possessive, relative, ?In the submitted run we used the GIS (Genera-lized Iterative Scaling) algorithm for parameterestimation, with 600 iterations, which appearedto provide better results than using L-BFGS (alimited-memory algorithm for unconstrained op-timization).
Training times ranged from oneminute for German to 8 minutes for Italian,hence the slower speed of GIS was not an issue.3.1 Entity creationThe mentions detected in the first phase wereclustered, according to the output of the classifi-er, using a greedy clustering algorithm.Each mention is compared to all previousmentions, which are collected in a global men-tions table.
If the pair-wise classifier assigns aprobability greater than a given threshold to thefact that a new mention belongs to a previouslyidentified entity, it is assigned to that entity.
Incase more than one entity has a probability great-er than the threshold, the mention is assigned tothe one with highest probability.
This strategyhas been described as best-first clustering by Ng(2005).In principle the process is not optimal since,once a mention is assigned to an entity, it cannotbe later assigned to another entity to which itmore likely refers.
Luo et al (2004) propose anapproach based on the Bell tree to address thisproblem.
Despite this potential limitation, oursystem performed quite well.4 Data preparationWe used the data as supplied by the task organ-izers for all languages except Italian.
A modifiedversion of the Hunpos tagger (Hal?csy, Kornai &Oravecz, 2007; Attardi et al, 2009) was used toadd to the Italian training and development cor-pora more accurate POS tags than those supplied,as well as missing information about morphol-ogy.
The POS tagger we used, in fact is capableof tagging sentences with detailed POS tags,which include morphological information; thiswas added to column PFEATS in the data.
Justfor this reason our submission for Italian is to beconsidered an open task submission.The Italian training corpus appears to containseveral errors related to mention boundaries.
Inparticular there are cases of entities starting in asentence and ending in the following one.
Thisappears to be due to sentence splitting (for in-stance at semicolons) performed after named ent-ities had been tagged.
As explained in section 2,our system was not prepared to deal with thesesituations.Other errors in the annotations of entities oc-curred in the Italian test data, in particular incor-rect balancing of openings and closings namedentities, which caused problems to our submis-sion.
We could only complete the run after thedeadline, so we could only report unofficial re-sults for Italian.5 ResultsWe submitted results to the gold and regularchallenges for the following languages: Catalan,English, German and Spanish.Table 1 summarizes the performance of oursystem, according to the different accuracyscores for the gold task, Table 2 for the regulartask.
We have outlined in bold the cases wherewe achieved the best scores among the partici-pating systems.Mention CEAF MUC B3 BLANCCatalan 98.4 64.9 26.5 76.2 54.4German 100 77.7 25.9 85.9 57.4English 89.8 67.6 24.0 73.4 52.1Spanish 98.4 65.8 25.7 76.8 54.1Table 1.
Gold task, Accuracy scores.Mention CEAF MUC B3 BLANCCatalan 82.7 57.1 22.9 64.6 51.0German 59.2 49.5 15.4 50.7 44.7English 73.9 57.3 24.6 61.3 49.3Spanish 83.1 59.3 21.7 66.0 51.4Table 2.
Regular task.
Accuracy scores.6 Error analysisWe performed some preliminary error analysis.The goal was to identify systematic errors andpossible corrections for improving the perfor-mance of our system.We limited our analysis to the mention boun-daries detection for the regular tasks.
A similar110analysis for coreference detection, would requirethe availability of gold test data.7 Mention detection errorsAs described above, the strategy used for the ex-traction of mentions boundaries is based on de-pendency parse trees and named entities.
Thisproved to be a good strategy in some languagessuch as Catalan (F1 score: 82.7) and Spanish (F1score: 83.1) in which the dependency data avail-able in the corpora were very accurate and con-sistent with the annotation of named entities.
In-stead, there have been unexpected problems inother languages like English or German, wherethe dependencies information were annotatedusing a different approach.For German, while we achieved the best B3accuracy on coreference analysis in the gold set-tings, we had a quite low accuracy in mentiondetection (F1: 59.2), which was responsible of asignificant drop in coreference accuracy for theregular task.
This degradation in performancewas mainly due to punctuations, which in Ger-man are linked to the sub-tree containing thenoun phrase rather than to the root of the sen-tence or tokens outside the noun phrase, as ithappens in Catalan and Spanish.
This misled ourmention detection algorithm to create many men-tions with wrong boundaries, just because punc-tuation marks were included.In the English corpus different conventionswere apparently used for dependency parsing andnamed entity annotations (Table 3), which pro-duced discrepancies between the boundaries ofthe named entities present in the data and thosepredicted by our algorithm.
This in turn affectednegatively the coreference detection algorithmthat uses both types of information.ID TOKEN HEAD DEPREL NE COREF1 Defense 2 NAME (org) (252 Secretary 4 NMOD _ _3 William 4 NAME (person _4 Cohen 5 SBJ person) 25)Table 3.
Example of different conventions for NE andCOREF in the English corpus.Error analysis also has shown that further im-provements could be obtained, for all languages,by using more accurate language specific extrac-tion rules.
For example, we missed to consider anumber of specific POS tags as possible identifi-ers for the head of noun phrases.
By some simpletuning of the algorithm we obtained some im-provements.8 ConclusionsWe reported our experiments on coreference res-olution in multiple languages.
We applied an ap-proach based on analyzing the parse trees in or-der to detect mention boundaries and a Maxi-mum Entropy classifier to cluster mentions intoentities.Despite a very simplistic approach, the resultswere satisfactory and further improvements arepossible by tuning the parameters of the algo-rithms.ReferencesG.
Attardi et al, 2009.
Tanl (Text Analytics and Natu-ral Language Processing).
SemaWiki project:http://medialab.di.unipi.it/wiki/SemaWiki.P.
Hal?csy, A. Kornai, and C. Oravecz, 2007.
Hun-Pos: an open source trigram tagger.
Proceedings ofthe ACL 2007, Prague.Z.
Le, Maximum Entropy Modeling Toolkit for Pythoand C++, Reference Manual.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla & S.Roukos.
2004.
A Mention-Synchronous Corefer-ence Resolution Algorithm Based on the Bell Tree.Proceedings of the ACL 2004, Barcelona.V.
Ng, Machine Learning for Coreference Resolution:From Local Classification to Global Ranking, Pro-ceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics (ACL), AnnArbor, MI, June 2005, pp.
157-164.M.
Recasens, L. M?rquez,  E. Sapena, M. A.
Mart?,M.
Taul?, V. Hoste, M. Poesio and Y. Versley,SemEval-2010 Task 1: Coreference resolution inmultiple languages, in Proceedings of the 5th In-ternational Workshop on Semantic Evaluations(SemEval-2010), Uppsala, Sweden, 2010.111
