An Empirical Comparison of Goodness Measures forUnsupervised Chinese Word Segmentation with a Unified FrameworkHai Zhao?and Chunyu KitDepartment of Chinese, Translation and Linguistics,City University of Hong Kong,83 Tat Chee Avenue, Kowloon, Hong Kong, ChinaEmail: haizhao@cityu.edu.hk, ctckit@cityu.edu.hkAbstractThis paper reports our empirical evaluationand comparison of several popular good-ness measures for unsupervised segmenta-tion of Chinese texts using Bakeoff-3 datasets with a unified framework.
Assuming noprior knowledge about Chinese, this frame-work relies on a goodness measure to iden-tify word candidates from unlabeled textsand then applies a generalized decoding al-gorithm to find the optimal segmentationof a sentence into such candidates with thegreatest sum of goodness scores.
Exper-iments show that description length gainoutperforms other measures because of itsstrength for identifying short words.
Furtherperformance improvement is also reported,achieved by proper candidate pruning andby assemble segmentation to integrate thestrengths of individual measures.1 IntroductionUnsupervised Chinese word segmentation was ex-plored in a number of previous works for variouspurposes and by various methods (Ge et al, 1999;Fu and Wang, 1999; Peng and Schuurmans, 2001;?The research described in this paper was supported by theResearch Grants Council of Hong Kong S.A.R., China, throughthe CERG grant 9040861 (CityU 1318/03H) and by City Uni-versity of Hong Kong through the Strategic Research Grant7002037.
Dr. Hai Zhao was supported by a postdoctoral Re-search Fellowship in the Department of Chinese, Translationand Linguistics, City University of Hong Kong.
Thanks fouranonymous reviewers for their insightful comments!SUN et al, 2004; Jin and Tanaka-Ishii, 2006).
How-ever, various heuristic rules are often involved inmost existing works, and there has not been a com-prehensive comparison of their performance in aunified way with available large-scale ?gold stan-dard?
data sets, especially, multi-standard ones sinceBakeoff-1 1.In this paper we will propose a unified frame-work for unsupervised segmentation of Chinese text.Four existing approaches to unsupervised segmenta-tions or word extraction are considered as its specialcases, each with its own goodness measurement toquantify word likelihood.
The output by each ap-proach will be evaluated using benchmark data setsof Bakeoff-32 (Levow, 2006).
Note that unsuper-vised segmentation is different from, if not morecomplex than, word extraction, in that the formermust carry out the segmentation task for a text, forwhich a segmentation (decoding) algorithm is indis-pensable, whereas the latter only acquires a wordcandidate list as output (Chang and Su, 1997; Zhanget al, 2000).2 Generalized FrameworkWe propose a generalized framework to unify theexisting methods for unsupervised segmentation, as-suming the availability of a list of word candidateseach associated with a goodness for how likely it isto be a true word.
Let W = {{wi, g(wi)}i=1,...,n} besuch a list, where wi is a word candidate and g(wi)1First International Chinese Word Segmentation Bakeoff, athttp://www.sighan.org/bakeoff20032The Third International Chinese Language ProcessingBakeoff, at http://www.sighan.org/bakeoff2006.9its goodness function.Two generalized decoding algorithms, (1) and (2),are formulated for optimal segmentation of a givenplain text.
The first one, decoding algorithm (1), is aViterbi-style one to search for the best segmentationS?
for a text T , as follows,S?
= argmaxw1???wi??
?wn =Tn?i=1g(wi), (1)with all {wi, g(wi)} ?
W .Another algorithm, decoding algorithm (2), is amaximal-matching one with respect to a goodnessscore.
It works on T to output the best current wordw?
repeatedly with T=t?
for the next round as fol-lows,{w?, t?}
= argmaxwt=Tg(w) (2)with each {w, g(w)} ?
W .
This algorithm will backoff to forward maximal matching algorithm if thegoodness function is set to word length.
Thus theformer may be regarded as a generalization of thelatter.
Symmetrically, it has an inverse version thatworks the other way around.3 Goodness MeasurementAn unsupervised segmentation strategy has to reston some predefined criterion, e.g., mutual informa-tion (MI), in order to recognize a substring in the textas a word.
Sproat and Shih (1990) is an early inves-tigation in this direction.
In this study, we examinefour types of goodness measurement for a candidatesubstring3.
In principle, the higher goodness scorefor a candidate, the more possible it is to be a trueword.Frequency of Substring with Reduction A lin-ear algorithm was proposed in (Lu?
et al, 2004) toproduce a list of such reduced substrings for a givencorpus.
The basic idea is that if two partially over-lapped n-grams have the same frequency in the inputcorpus, then the shorter one is discarded as a redun-dant word candidate.
We take the logarithm of FSR3Although there have been many existing works in this di-rection (Lua and Gan, 1994; Chien, 1997; Sun et al, 1998;Zhang et al, 2000; SUN et al, 2004), we have to skip the de-tails of comparing MI due to the length limitation of this paper.However, our experiments with MI provide no evidence againstthe conclusions in this paper.as the goodness for a word candidate, i.e.,gFSR(w) = log(p?
(w)) (3)where p?
(w) is w?s frequency in the corpus.
Thisallows the arithmetic addition in (1).
According toZipf?s Law (Zipf, 1949), it approximates the use ofthe rank of w as its goodness, which would give itsome statistical significance.
For the sake of effi-ciency, only those substrings that occur more thanonce are considered qualified word candidates.Description Length Gain (DLG) The goodnessmeasure is proposed in (Kit and Wilks, 1999) forcompression-based unsupervised segmentation.
TheDLG from extracting all occurrences of xixi+1...xj(also denoted as xi..j) from a corpus X= x1x2...xnas a word is defined asDLG(xi..j) = L(X)?
L(X[r ?
xi..j ]?
xi..j) (4)where X[r ?
xi..j ] represents the resultant corpusfrom replacing all instances of xi..j with a new sym-bol r throughout X and ?
denotes the concatenationof two substrings.
L(?)
is the empirical descriptionlength of a corpus in bits that can be estimated by theShannon-Fano code or Huffman code as below, fol-lowing classic information theory (Shannon, 1948).L(X) .= ?|X|?x?Vp?(x)log2p?
(x) (5)where | ?
| denotes string length, V is the charactervocabulary of X and p?
(x) x?s frequency in X .
Fora given word candidate w, we define gDLG(w) =DLG(w).
In principle, a substring with a negativeDLG do not bring any positive compression effectby itself.
Thus only substrings with a positive DLGvalue are added into our word candidate list.Accessor Variety (AV) Feng et al (2004) proposeAV as a statistical criterion to measure how likely asubstring is a word.
It is reported to handle low-frequent words particularly well.
The AV of a sub-string xi..j is defined asAV (xi..j) = min{Lav(xi..j), Rav(xi..j)} (6)where the left and right accessor variety Lav(xi..j)and Rav(xi..j) are, respectively, the number of dis-tinct predecessor and successor characters.
For asimilar reason as to FSR, the logarithm of AV is used10as goodness measure, and only substrings with AV> 1 are considered word candidates.
That is, wehave gAV (w) = logAV (w) for a word candidate w.Boundary Entropy (Branching Entropy, BE) Itis proposed as a criterion for unsupervised segmen-tation in some existing works (Tung and Lee, 1994;Chang and Su, 1997; Huang and Powers, 2003; Jinand Tanaka-Ishii, 2006).
The local entropy for agiven xi..j , defined ash(xi..j) = ?
?x?Vp(x|xi..j)log p(x|xi..j), (7)indicates the average uncertainty after (or before)xi..j in the text, where p(x|xi..j) is the co-occurrenceprobability for x and xi..j .
Two types of h(xi..j),namely hL(xi..j) and hR(xi..j), can be defined forthe two directions to extend xi..j (Tung and Lee,1994).
Also, we can define hmin = min{hR, hL} ina similar way as in (6).
In this study, only substringswith BE > 0 are considered word candidates.
For acandidate w, we have gBE (w) = hmin(w)4.4 EvaluationThe evaluation is conducted with all four corporafrom Bakeoff-3 (Levow, 2006), as summarized inTable 1 with corpus size in number of characters.For unsupervised segmentation, the annotation inthe training corpora is not used.
Instead, theyare used for our evaluation, for they are large andthus provide more reliable statistics than small ones.Segmentation performance is evaluated by word F-measure F = 2RP/(R + P ).
The recall R andprecision P are, respectively, the proportions of thecorrectly segmented words to all words in the gold-standard and a segmenter?s output5.Note that a decoding algorithm always requiresthe goodness score of a single-character candidate4Both AV and BE share a similar idea from Harris (1970):If the uncertainty of successive token increases, then it is likelyto be at a boundary.
In this sense, one may consider them thediscrete and continuous formulation of the same idea.5All evaluations will be represented in terms of wordF-measure if not otherwise specified.
A standard scoringtool with this metric can be found in SIGHAN website,http://www.sighan.org/bakeoff2003/score.
However, to com-pare with related work, we will also adopt boundary F-measureFb = 2RbPb/(Rb + Pb), where the boundary recall Rb andboundary precision Pb are, respectively, the proportions of thecorrectly recognized boundaries to all boundaries in the gold-standard and a segmenter?s output (Ando and Lee, 2000).Table 1: Bakeoff-3 CorporaCorpus AS CityU CTB MSRATraining(M) 8.42 2.71 0.83 2.17Test(K) 146 364 256 173Table 2: Performance with decoding algorithm (1)M. Good- Training corpusL.a ness AS CityU CTB MSRAFSR .400 .454 .462 .4322 DLG/d .592 .610 .604 .603AV .568 .595 .596 .577BE .559 .587 .592 .572FSR .193 .251 .268 .2357 DLG/d .331 .397 .409 .379AV .399 .423 .430 .407BE .390 .419 .428 .403aM.L.
: Maximal length allowable for word candidates.for computation.
There are two ways to get thisscore: (1) computed by the goodness measure,which is applicable only if the measure allows; (2)set to zero as default value, which is always appli-cable even to single-character candidates not in theword candidate list in use.
For example, all single-character candidates given up by DLG because oftheir negative DLG scores will have a default valueduring decoding.
We will use a ?/d?
to indicate ex-periments using such a default value.4.1 ComparisonWe apply the decoding algorithm (1) to segment allBakeoff-3 corpora with the above goodness mea-sures.
Both word candidates and goodness valuesare derived from the raw text of each training cor-pus.
The performance of these measures is presentedin Table 2.
From the table we can see that DLGand FSR have the strongest and the weakest perfor-mance, respectively, whereas AV and BE are highlycomparable to each other.Decoding algorithm (2) runs the forward andbackward segmentation with the respective AVand BE criteria, i.e., LAV /hL for backward andRAV /hR forward, and the output is the union of twosegmentations 6.
A performance comparison of AVand BE with both algorithms (1) and (2) is presentedin Table 3.
We can see that the former has a rela-6That is, all segmented points by either segmentation will beaccounted into the final segmentation.11Table 3: Performance comparison: AV vs. BEM.
Good- Training corpusL.
ness AS CityU CTB MSRAAV(1) .568 .595 .596 .577AV(2)/d .485 .489 .508 .471AV(2) .445 .366 .367 .3872 BE(1) .559 .587 .592 .572BE(2)/d .485 .489 .508 .471BE(2) .504 .428 .446 .446AV(1) .399 .423 .430 .407AV(2)/d .570 .581 .588 .572AV(2) .445 .366 .368 .3877 BE(1) .390 .419 .428 .403BE(2)/d .597 .604 .605 .593BE(2) .508 .431 .449 .4462 3 4 5 6 70.350.40.450.50.550.6The Range of Word LengthF?measureBE/(2): ASBE/(2): CityUBE/(2): CTBBE/(2): MSRADLG/(1): ASDLG/(1): CityUDLG/(1): CTBDLG/(1): MSRAFigure 1: Performance vs. word lengthtively better performance on shorter words and thelatter outperforms on longer ones.How segmentation performance varies along withword length is exemplified with DLG and BE as ex-amples in Figure 1, with (1) and (2) indicating a re-spective decoding algorithm in use.
It shows thatDLG outperforms on two-character words and BEon longer ones.4.2 Word Candidate PruningUp to now, word candidates are determined by thedefault goodness threshold 0.
The number of themfor each of the four goodness measures is presentedin Table 4.
We can see that FSR generates the largestset of word candidates and DLG the smallest.
Moreinterestingly or even surprising, AV and BE generateexactly the same candidate list for all corpora.In addition to word length, another crucial factorto affect segmentation performance is the quality ofthe word candidates as a whole.
Since each candi-date is associated with a goodness score to indicatehow good it is, a straightforward way to ensure, andfurther enhance, the overall quality of a candidateset is to prune off those with low goodness scores.Table 4: Word candidate number by threshold 0Good- Training Corpusness AS CityU CTB MSRAFSR 2,009K 832K 294K 661KDLG 543K 265K 96K 232KAV 1,153K 443K 160K 337KBE 1,153K 443K 160K 337K2 3 4 5 6 70.40.450.50.550.60.65The Range of Word LengthF?measure100% size89% size79% size74% size70% size65% size62% size48% size38% sizeFigure 2: Performance by candidate pruning: DLGTo examine how segmentation performance changesalong with word candidate pruning and decide theoptimal pruning rate, we conduct a series of experi-ments with each goodness measurements.
Figures 2and 3 present, as an illustration, the outcomes of twoseries of our experiments with DLG by decoding al-gorithm (1) and BE by decoding algorithm (1) and(2) on CityU training corpus.
We find that appro-priate pruning does lead to significant performanceimprovement and that both DLG and BE keep theirsuperior performance respectively on two-characterwords and others.
We also observe that each good-ness measure has a stable and similar performancein a range of pruning rates around the optimal one,e.g., 79-62% around 70% in Figure 2.The optimal pruning rates found through our ex-periments for the four goodness measures are givenin Table 5, and their correspondent segmentationperformance in Table 6.
These results show a re-markable performance improvement beyond the de-2 3 4 5 6 70.40.450.50.550.60.65The Range of Word LengthF?measure 100% size/(1)38% size/(1)32% size/(1)19% size/(1)10% size/(1)100% size/(2)27% size/(2)19% size/(2)16% size/(2)13.5% size/(2)11% size/(2)4.5% size/(2)Figure 3: Performance by candidate pruning: BE12Table 5: Optimal rates for candidate pruning (%)Decoding Goodness measurealgorithm FSR DLG AV BE(1) 1.8 70 12.5 20(2) ?
?
8 12.5Table 6: Performance via optimal candidate pruningM.
Good- Training corpusL.
ness AS CityU CTB MSRAFSR(1) .501 .525 .513 .522DLG(1)/d .710 .650 .664 .6382 AV(1) .616 .625 .609 .618BE(1) .613 .614 .605 .611AV(2)/d .585 .602 .589 .599BE(2)/d .591 .599 .596 .593FSR(1) .444 .491 .486 .486DLG(1)/d .420 .447 .460 .4237 AV(1) .517 .568 .549 .544BE(1) .501 .539 .510 .519AV(2)/d .623 .624 .604 .615BE(2)/d .630 .631 .620 .622fault threshold setting.
What remains unchanged isthe advantage of DLG for two-character words andthat of AV/BE for longer words.
However, DLGachieves the best overall performance among thefour, although it uses only single- and two-characterword candidates.
The overwhelming number of two-character words in Chinese allows it to triumph.4.3 Ensemble SegmentationAlthough proper pruning of word candidates bringsamazing performance improvement, it is unlikelyfor one to determine an optimal pruning rate in prac-tice for an unlabeled corpus.
Here we put forth aparameter-free method to tackle this problem withthe aids of all available goodness measures.The first step of this method to do is to derive anoptimal set of word candidates from the input.
Wehave shown above that quality candidates play a crit-ical role in achieving quality segmentation.
Withoutany better goodness criterion available, the best wecan opt for is the intersection of all word candidatelists generated by available goodness measures withthe default threshold.
A good reason for this is thatthe agreement of them can give a more reliable de-cision than any individual one of them.
In fact, weonly need DLG and AV/BE to get this intersection,because AV and BE give the same word candidatesTable 7: Performances of ensemble segmentationM.
Good- Training corpusL.
ness AS CityU CTB MSRAFSR(1) .629 .635 .624 .6232 DLG(1)/d .664 .653 .643 .650AV(1) .641 .644 .631 .634BE(1) .640 .643 .632 .6347 AV(2)/d .595 .637 .624 .610BE(2)/d .593 .635 .620 .609DLG(1)/d+AV(2)/d .672 .684 .663 .665DLG(1)/d+BE(2)/d .660 .681 .656 .653and DLG generates only a subset of what FSR does.The next step is to use this intersection set ofword candidates to perform optimal segmentationwith each goodness measures, to see if any fur-ther improvement can be achieved.
The best re-sults are given in Table 7, showing that decoding al-gorithm (1) achieves marvelous improvement usingshort word candidates with all other goodness mea-sures than DLG.
Interestingly, DLG still remains atthe top by performance despite of some slip-back.To explore further improvement, we also tryto combine the strengths of DLG and AV/BE re-spectively for recognizing two- and multi-characterword.
Our strategy to combine them together is toenforce the multi-character words in AV/BE seg-mentation upon the correspondent parts of DLG seg-mentation.
This ensemble method gives a betteroverall performance than all others that we havetried so far, as presented at the bottom of Table 7.4.4 Yet Another Decoding AlgorithmJin and Tanaka-Ishii (2006) give an unsupervisedsegmentation criterion, henceforth referred to as de-coding algorithm (3), to work with BE.
It works asfollows: if g(xi..j+1) > g(xi..j) for any two over-lapped substrings xi..j and xi..j+1, then a segment-ing point should be located right after xi..j+1.
Thisalgorithm has a forward and a backward version.The union of the segmentation outputs by both ver-sions is taken as the final output of the algorithm,in exactly the same way as how decoding algorithm(2) works7.
This algorithm is evaluated in (Jin andTanaka-Ishii, 2006) using Peking University (PKU)7Three segmentation criteria are given in (Jin and Tanaka-Ishii, 2006), among which the entropy increase criterion,namely, decoding algorithm (3), proves to be the best.
Here wewould like to thank JIN Zhihui and Prof. Kumiko Tanaka-Ishiifor presenting the details of their algorithms.13Table 8: Performance comparison by word andboundary F-measure on PKU corpus (M. L. = 6)Good- Decoding algorithmness (1)/d (1) (2)/d (2) (3)/d (3)AV .313 .325 .588 .373 .376 .453F AV?
.372 .372 .663 .663 .445 .445BE .309 .319 .624 .501 .376 .624BE?
.370 .370 .676 .676 .447 .447AV .695 .700 .830 .762 .762 .728Fb AV?
.728 .728 .865 .865 .783 .783BE .696 .699 .849 .810 .762 .837aBE?
.728 .728 .872 .872 .784 .784aWith the same hyperparameters, (Jin and Tanaka-Ishii, 2006)report their best result of boundary precision 0.88 and boundaryrecall 0.79, equal to boundary F-measure 0.833.Corpus of 1.1M words8 as gold standard with a wordcandidate list extracted from the 200M Contempo-rary Chinese Corpus that mostly consists of severalyears of Peoples?
Daily9.
Here, we carry out evalu-ation with similar data: we extract word candidatesfrom the unlabeled texts of People?s Daily (1993 -1997), of 213M and about 100M characters, in termsof the AV and BE criteria, yielding a list of 4.42 mil-lion candidates up to 6-character long10 for each cri-terion.
Then, the evaluation of the three decodingalgorithms is performed on PKU corpus.The evaluation results with both word and bound-ary F-measure are presented for the same segmenta-tion outputs in Table 8, with ?*?
to indicate candi-date pruning by DLG > 0 as reported before.
Notethat boundary F-measure gives much more higherscore than word F-measure for the same segmenta-tion output.
However, in either of metric, we canfind no evidence in favor of decoding algorithm (3).Undesirably, this algorithm does not guarantee a sta-ble performance improvement with the BE measurethrough candidate pruning.4.5 Comparison against SupervisedSegmentationHuang and Zhao (2007) provide empirical evidenceto estimate the degree to which the four segmenta-tion standards involved in the Bakeoff-3 differ fromeach other.
As quoted in Table 9, a consistency rate8http://icl.pku.edu.cn/icl groups/corpus/dwldform1.asp9http://ccl.pku.edu.cn:8080/ccl corpus/jsearch/index.jsp10This is to keep consistence with (Jin and Tanaka-Ishii,2006), where 6 is set as the maximum n-gram length.Table 9: Consistency rate among Bakeoff-3 segmen-tation standards (Huang and Zhao, 2007)Test Training corpuscorpus AS CityU CTB MSRAAS 1.000 0.926 0.959 0.858CityU 0.932 1.000 0.935 0.849CTB 0.942 0.910 1.000 0.877MSRA 0.857 0.848 0.887 1.000beyond 84.8% is found among the four standards.If we do not over-expect unsupervised segmentationto achieve beyond what these standards agree witheach other, it is reasonable to take this figure as thetopline for evaluation.
On the other hand, Zhao et al(2006) show that the words of 1 to 2 characters longaccount for 95% of all words in Chinese texts, andsingle-character words alone for about 50%.
Thus,we can take the result of the brute-force guess of ev-ery single character as a word as a baseline.To compare to supervised segmentation, whichusually involves training using an annotated train-ing corpus and, then, evaluation using test corpus,we carry out unsupervised segmentation in a com-parable manner.
For each data track, we first ex-tract word candidates from both the training and testcorpora, all unannotated, and then evaluate the un-supervised segmentation with reference to the gold-standard segmentation of the test corpus.
The re-sults are presented in Table 10, together with bestand worst official results of the Bakeoff closed test.This comparison shows that unsupervised segmen-tation cannot compete against supervised segmenta-tion in terms of performance.
However, the experi-ments generate positive results that the best combi-nation of the four goodness measures can achieve anF-measure in the range of 0.65-0.7 on all test corporain use without using any prior knowledge, but ex-tracting word candidates from the unlabeled trainingand test corpora in terms of their goodness scores.5 Discussion: How Things HappenNote that DLG criterion is to perform segmentationwith the intension to maximize the compression ef-fect, which is a global effect through the text.
Thusit works well incorporated with a probability maxi-mization framework, where high frequent but inde-pendent substrings are effectively extracted and re-14Table 10: Comparison of performances against su-pervised segmentationType Test corpusAS CityU CTB MSRABaseline .389 .345 .337 .353DLG(1)/d .597 .616 .601 .602DLG?
(1)/d .655 .659 .632 .6552 AV(1) .577 .603 .597 .583AV?
(1) .630 .650 .618 .638BE(1) .570 .598 .594 .580BE?
(1) .629 .649 .618 .638AV(2)/d .512 .551 .543 .526AV?
(2)/d .591 .644 .618 .6047 BE(2)/d .518 .554 .546 .533BE?
(2)/d .587 .641 .614 .605DLG?
(1)/d +AV?
(2)/d .663 .692 .658 .667DLG?
(1)/d +BE?
(2)/d .650 .689 .650 .656Worst closed .710 .589 0.818 .819Best closed .958 .972 0.933 .963combined.
We know that most unsupervised seg-mentation criteria will bring up long word bias prob-lem, so does DLG measure.
This explains why itgives the worse results as long candidates are added.As for AV and BE measures, both of them give themetric of the uncertainty before or after the currentsubstring.
This means that they are more concernedwith local uncertainty information near the currentsubstring, instead of global information among thewhole text as DLG.
Thus local greedy search inmaximal matching style is more suitable for thesetwo measures than Viterbi search.Our empirical results about word candidate listwith default threshold 0, where the same list is fromAV and BE, give another proof that both AV and BEreflect the same uncertainty.
The only difference isbehind the fact that the former and the latter is in thediscrete and continuous formulation, respectively.6 Conclusion and Future WorkThis paper reported our empirical comparison of anumber of goodness measures for unsupervised seg-mentation of Chinese texts with the aid two gener-alized decoding algorithms.
We learn no previouswork by others for a similar attempt.
The compari-son is carried out with Bakeoff-3 data sets, showingthat all goodness measures exhibit their strengths forrecognizing words of different lengths and achieve aperformance far beyond the baseline.
Among them,DLG with decoding algorithm (1) can achieve thebest segmentation performance for single- and two-character words identification and the best overallperformance as well.
Our experiments also showthat the quality of word candidates plays a criti-cal role in ensuring segmentation performance 11.Proper pruning of candidates with low goodnessscores to enhance this quality enhances the seg-mentation performance significantly.
Also, the suc-cess of unsupervised segmentation depends stronglyon an appropriate decoding algorithm.
Generally,Viterbi-style decoding produces better results thanbest-first maximal-matching.
But the latter is not shyfrom exhibiting its particular strength for identifyingmulti-character words.Finally, the ensemble segmentation we put forthto combine the strengths of different goodness mea-sures proves to be a remarkable success.
It achievesan impressive performance improvement on top ofindividual goodness measures.As for future work, it would be natural for re-searchers to enhance supervised learning for Chi-nese word segmentation with goodness measures in-troduced here.
There does be two successful exam-ples in our existing work (Zhao and Kit, 2007).
Thisis still an ongoing work.ReferencesRie Kubota Ando and Lillian Lee.
2000.
Mostly-unsupervised statistical segmentation of Japanese: Ap-plications to kanji.
In Proceedings of the first Confer-ence on North American Chapter of the Associationfor Computational Linguistics and the 6th Conferenceon Applied Natural Language Processing, pages 241?248, Seattle, Washington, April 30.Jing-Shin Chang and Keh-Yih Su.
1997.
An unsuper-vised iterative method for Chinese new lexicon ex-traction.
Computational Linguistics and Chinese Lan-guage Processing, 2(2):97?148.Lee-Feng Chien.
1997.
PAT-tree-based keyword extrac-tion for Chinese information retrieval.
In Proceedingsof the 20th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, pages 50?58, Philadelphia.Haodi Feng, Kang Chen, Xiaotie Deng, and WeiminZheng.
2004.
Accessor variety criteria for Chi-nese word extraction.
Computational Linguistics,30(1):75?93.11This observation is shared by other researchers, e.g., (Penget al, 2002).15Guo-Hong Fu and Xiao-Long Wang.
1999.
Unsu-pervised Chinese word segmentation and unknownword identification.
In 5th Natural Language Process-ing Pacific Rim Symposium 1999 (NLPRS?99), ?Clos-ing the Millennium?, pages 32?37, Beijing, China,November 5-7.Xianping Ge, Wanda Pratt, and Padhraic Smyth.
1999.Discovering Chinese words from unsegmented text.
InSIGIR ?99: Proceedings of the 22nd Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 271?272,Berkeley, CA, USA, August 15-19.
ACM.Zellig Sabbetai Harris.
1970.
Morpheme boundarieswithin words.
In Papers in Structural and Transfor-mational Linguistics, page 68?77.Jin Hu Huang and David Powers.
2003.
Chineseword segmentation based on contextual entropy.
InDong Hong Ji and Kim-Ten Lua, editors, Proceedingsof the 17th Asian Pacific Conference on Language, In-formation and Computation, pages 152?158, Sentosa,Singapore, October, 1-3.
COLIPS Publication.Chang-Ning Huang and Hai Zhao.
2007.
Chinese wordsegmentation: A decade review.
Journal of ChineseInformation Processing, 21(3):8?20.Zhihui Jin and Kumiko Tanaka-Ishii.
2006.
Unsuper-vised segmentation of Chinese text by use of branch-ing entropy.
In COLING/ACL 2006, pages 428?435,Sidney, Australia.Chunyu Kit and Yorick Wilks.
1999.
Unsupervisedlearning of word boundary with description lengthgain.
In M. Osborne and E. T. K. Sang, editors,CoNLL-99, pages 1?6, Bergen, Norway.Gina-Anne Levow.
2006.
The third international Chi-nese language processing bakeoff: Word segmentationand named entity recognition.
In Proceedings of theFifth SIGHAN Workshop on Chinese Language Pro-cessing, pages 108?117, Sydney, Australia, July.Xueqiang Lu?, Le Zhang, and Junfeng Hu.
2004.
Sta-tistical substring reduction in linear time.
In Keh-Yih Su et al, editor, Proceeding of the 1st Interna-tional Joint Conference on Natural Language Process-ing (IJCNLP-2004), volume 3248 of Lecture Notesin Computer Science, pages 320?327, Sanya City,Hainan Island, China, March 22-24.
Springer.Kim-Teng Lua and Kok-Wee Gan.
1994.
An applica-tion of information theory in Chinese word segmenta-tion.
Computer Processing of Chinese and OrientalLanguages, 8(1):115?123.Fuchun Peng and Dale Schuurmans.
2001.
Self-supervised Chinese word segmentation.
In The FourthInternational Symposium on Intelligent Data Analysis,pages 238?247, Lisbon, Portugal, September, 13-15.Fuchun Peng, Xiangji Huang, Dale Schuurmans, NickCercone, and Stephen Robertson.
2002.
Using self-supervised word segmentation in Chinese informationretrieval.
In Proceedings of the 25th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 349?350,Tampere, Finland, August, 11-15.Claude E. Shannon.
1948.
A mathematical theory ofcommunication.
The Bell System Technical Journal,27:379?423, 623?656, July, October.Richard Sproat and Chilin Shih.
1990.
A statisticalmethod for finding word boundaries in Chinese text.Computer Processing of Chinese and Oriental Lan-guages, 4(4):336?351.Maosong Sun, Dayang Shen, and Benjamin K. Tsou.1998.
Chinese word segmentation without using lexi-con and hand-crafted training data.
In COLING-ACL?98, 36th Annual Meeting of the Association for Com-putational Linguistics and 17th International Confer-ence on Computational Linguistics, volume 2, pages1265?1271, Montreal, Quebec, Canada.Mao Song SUN, Ming XIAO, and Benjamin K. Tsou.2004.
Chinese word segmentation without using dic-tionary based on unsupervised learning strategy (inChinese) (???????????????????????).
Chinese Journal of Computers,27(6):736?742.Cheng-Huang Tung and His-Jian Lee.
1994.
Iden-tification of unknown words from corpus.
Compu-tational Proceedings of Chinese and Oriental Lan-guages, 8:131?145.Jian Zhang, Jianfeng Gao, and Ming Zhou.
2000.
Ex-traction of Chinese compound words ?
an experimen-tal study on a very large corpus.
In Proceedings ofthe Second Chinese Language Processing Workshop,pages 132?139, Hong Kong, China.Hai Zhao and Chunyu Kit.
2007.
Incorporating globalinformation into supervised learning for Chinese wordsegmentation.
In Proceedings of the 10th Conferenceof the Pacific Association for Computational Linguis-tics, pages 66?74, Melbourne, Australia, September19-21.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006.
Effective tag set selection in Chinese wordsegmentation via conditional random field modeling.In Proceedings of the 20th Asian Pacific Conference onLanguage, Information and Computation, pages 87?94, Wuhan, China, November 1-3.George Kingsley Zipf.
1949.
Human Behavior andthe Principle of Least Effort.
Addison-Wesley, Cam-bridge, MA.16
