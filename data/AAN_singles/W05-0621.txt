Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 165?168, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsInferring semantic roles using sub-categorization frames andmaximum entropy modelAkshar Bharati, Sriram Venkatapathy and Prashanth ReddyLanguage Technologies Research Centre, IIIT - Hyderabad, India.
{sriram,prashanth}@research.iiit.ac.inAbstractIn this paper, we propose an approachfor inferring semantic role using sub-categorization frames and maximumentropy model.
Our approach aims touse the sub-categorization informationof the verb to label the mandatory ar-guments of the verb in various possi-ble ways.
The ambiguity between theassignment of roles to mandatory argu-ments is resolved using the maximumentropy model.
The unlabelled manda-tory arguments and the optional argu-ments are labelled directly using themaximum entropy model such that theirlabels are not one among the frame el-ements of the sub-categorization frameused.
Maximum entropy model is pre-ferred because of its novel approachof smoothing.
Using this approach,we obtained an F-measure of 68.14%on the development set of the dataprovided for the CONLL-2005 sharedtask.
We show that this approach per-forms well in comparison to an ap-proach which uses only the maximumentropy model.1 IntroductionSemantic role labelling is the task of assigningappropriate semantic roles to the arguments ofa verb.
The semantic role information is impor-tant for various applications in NLP such as Ma-chine Translation, Question Answering, Informa-tion Extraction etc.
In general, semantic role in-formation is useful for sentence understanding.We submitted our system for closed challengeat CONLL-2005 shared task.
This task encour-ages participants to use novel machine learningtechniques suited to the task of semantic role la-belling.
Previous approaches on semantic rolelabelling can be classified into three categories(1) Explicit Probabilistic methods (Gildea andJurafsky, 2002).
(2) General machine learningalgorithms (Pradhan et al, 2003) (Lim et al,2004) and (3) Generative model (Thompson etal., 2003).Our approach has two stages; first, identifica-tion whether the argument is mandatory or op-tional and second, the classification or labellingof the arguments.
In the first stage, the argumentsof a verb are put into three classes, (1) mandatory,(2) optional or (3) null.
Null stands for the factthat the constituent of the verb in the sentence isnot an semantic argument of the verb.
It is used torule out the false argument of the verb which wereobtained using the parser.
The maximum entropybased classifier is used to classify the argumentsinto one of the above three labels.After obtaining information about the nature ofthe non-null arguments, we proceed in the secondstage to classify the mandatory and optional ar-guments into their semantic roles.
The propbanksub-categorization frames are used to assign rolesto the mandatory arguments.
For example, in thesentence ?John saw a tree?, the sub-categorizationframe ?A0 v A1?
would assign the roles A0 toJohn and A1 to tree respectively.
After usingall the sub-categorization frames of the verb irre-165spective of the verb sense, there could be ambigu-ity in the assignment of semantic roles to manda-tory arguments.
The unlabelled mandatory argu-ments and the optional arguments are assignedthe most probable semantic role which is not oneof the frame elements of the sub-categorizationframe using the maximum entropy model.
Now,among all the sequences of roles assigned to thenon-null arguments, the sequence which has themaximum joint probability is chosen.
We ob-tained an accuracy of 68.14% using our approach.We also show that our approach performs betterin comparision to an approach with uses a simplemaximum entropy model.
In section 4, we willtalk about our approach in greater detail.This paper is organised as follows, (2) Features,(3) Maximum entropy model, (4) Description ofour system, (5) Results, (6) Comparison with ourother experiments, (7) Conclusion and (8) Futurework.2 FeaturesThe following are the features used to train themaximum entropy classifier for both the argumentidentification and argument classification.
Weused only simple features for these experiments,we are planning to use richer features in the nearfuture.1.
Verb/Predicate.2.
Voice of the verb.3.
Constituent head and Part of Speech tag.4.
Label of the constituent.5.
Relative position of the constituent with re-spect to the verb.6.
The path of the constituent to the verbphrase.7.
Preposition of the constituent, NULL if itdoesn?t exist.3 Maximum entropy modelThe maximum entropy approach became the pre-ferred approach of probabilistic model buildersfor its flexibility and its novel approach tosmoothing (Ratnaparakhi, 1999).Many classification tasks are most naturallyhandled by representing the instance to be classi-fied as a vector of features.
We represent featuresas binary functions of two arguments, f(a,H),where ?a?
is the observation or the class and ?H?
isthe history.
For example, a feature fi(a, H) is trueif ?a?
is Ram and ?H?
is ?AGENT of a verb?.
In alog linear model, the probability function P (a|H)with a set of features f1, f2, ....fj that connects ?a?to the history ?H?, takes the following form.P (a|H) = e?i ?i(a,H)?fi(a,H)Z(H)Here ?i?s are weights between negative andpositive infinity that indicate the relative impor-tance of a feature: the more relevant the feature tothe value of the probability, the higher the abso-lute value of the associated lambda.
Z(H), calledthe partition function, is the normalizing constant(for a fixed H).4 Description of our systemOur approach labels the semantic roles in twostages, (1) argument identification and (2) ar-gument classification.
As input to our sys-tem, we use full syntactic information (Collins,1999), Named-entities, Verb senses and Propbankframes.
For our experiments, we use Zhang Le?sMaxent Toolkit 1, and the L-BFGS parameter esti-mation algorithm with Gaussian prior smoothing(Chen and Rosenfield, 1999).4.1 Argument identificationThe first task in this stage is to find the candidatearguments and their boundaries using a parser.We use Collins parser to infer a list of candidatearguments for every predicate.
The following aresome of the sub-stages in this task.?
Convert the CFG tree given by Collins parserto a dependency tree.?
Eliminate auxilliary verbs etc.?
Mark the head of relative clause as an argu-ment of the verb.1http://www.nlplab.cn/zhangle/maxent toolkit.html166?
If a verb is modified by another verb, thesyntactic arguments of the superior verbare considered as shared arguments betweenboth the verbs.?
If a prepositional phrase attached to a verbcontains more than one noun phrase, attachthe second noun phrase to the verb.The second task is to filter out the constituentswhich are not really the arguments of the pred-icate.
Given our approach towards argumentclassification, we also need information aboutwhether an argument is mandatory or optional.Hence, in this stage the constituents are markedusing three labels, (1) MANDATORY argument,(2) OPTIONAL argument and (3) NULL, using amaximum entropy classifier.
For example, a sen-tence ?John was playing football in the evening?,?John?
is marked MANDATORY, ?football?
ismarked MANDATORY and ?in the evening?
ismarked OPTIONAL.For training, the Collins parser is run on thetraining data and the syntactic arguments areidentified.
Among these arguments, the oneswhich do not exist in the propbank annotation ofthe training data are marked as null.
Among theremaining arguments, the arguments are markedas mandatory or optional according to the prop-bank frame information.
Mandatory roles arethose appearing in the propbank frames of theverb and its sense, the rest are marked as optional.A propbank frame contains information as illus-trated by the following example:If Verb = play, sense = 01,then the roles A0, A1 are MANDATORY.4.2 Argument classificationArgument classification is done in two steps.
Inthe first step, the propbank sub-categorizationframes are used to assign the semantic roles to themandatory arguments in the order specified by thesub-categorization frames.
Sometimes, the num-ber of mandatory arguments of a verb in the sen-tence may be less than the number of roles whichcan be assigned by the sub-categorization frame.For example, in the sentence?MAN1 MAN2 V MAN3 OPT1?, roles couldbe assigned in the following two possible ways bythe sub-categorization frame ?A0 v A1?
of verbV1.?
A0[MAN1] MAN2 V1 A1[MAN3] OPT1?
MAN1 A0[MAN2] V A1[MAN3] OPT1In the second step, the task is to label the un-labelled mandatory arguments and the argumentswhich are marked as optional.
This is done bymarking these arguments with the most probablesemantic role which is not one of the frame ele-ments of the sub-categorization frame ?A0 v A1?.In the above example, the unlabelled mandatoryarguments and the optional arguments cannot belabelled as either A0 or A1.
Hence, after this step,the following might be the role-labelling for thesentence ?MAN1 MAN2 V1 MAN3 OPT1?.?
A0[MAN1] AM-TMP[MAN2] V1A1[MAN3] AM-LOC[OPT1]?
AM-MNC[MAN1] A0[MAN2] V1A1[MAN3] AM-LOC[OPT1]The best possible sequence of semantic roles(R?)
is decided by the taking the product of prob-abilities of individual assignments.
This also dis-ambiguates the ambiguity in the assignment ofmandatory roles.
The individual probabilities arecomputed using the maximum entropy model.For a sequence ~R, the product of the probabilitiesis defined asP (~R) = ?Ri?~RP (Ri|Argi)The best sequence of semantic roles R?
is de-fined asR?
= argmax P (~R)For training the maximum entropy model, theoutcomes are all the possible semantic roles.
Thelist of sub-categorization frames for a verb is ob-tained from the training data using informationabout mandatory roles from the propbank.
Thepropbank sub-categorization frames are also ap-pended to this list.We present our results in the next section.167Precision Recall F?=1Development 71.88% 64.76% 68.14Test WSJ 73.76% 65.52% 69.40Test Brown 65.25% 55.72% 60.11Test WSJ+Brown 72.66% 64.21% 68.17Test WSJ Precision Recall F?=1Overall 73.76% 65.52% 69.40A0 85.17% 73.34% 78.81A1 74.08% 66.08% 69.86A2 54.51% 48.47% 51.31A3 52.54% 35.84% 42.61A4 71.13% 67.65% 69.35A5 25.00% 20.00% 22.22AM-ADV 52.18% 47.23% 49.59AM-CAU 60.42% 39.73% 47.93AM-DIR 45.65% 24.71% 32.06AM-DIS 75.24% 73.12% 74.17AM-EXT 73.68% 43.75% 54.90AM-LOC 50.80% 43.53% 46.88AM-MNR 47.24% 49.71% 48.44AM-MOD 93.67% 91.29% 92.46AM-NEG 94.67% 92.61% 93.63AM-PNC 42.02% 43.48% 42.74AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 74.13% 66.97% 70.37R-A0 82.27% 80.80% 81.53R-A1 73.28% 61.54% 66.90R-A2 75.00% 37.50% 50.00R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 100.00% 57.14% 72.73R-AM-MNR 25.00% 16.67% 20.00R-AM-TMP 70.00% 53.85% 60.87V 97.28% 97.28% 97.28Table 1: Overall results (top) and detailed resultson the WSJ test (bottom).5 ResultsThe results of our approach are presented in table1.When we used an approach which uses a sim-ple maximum entropy model, we obtained an F-measure of 67.03%.
Hence, we show that thesub-categorization frames help in predicting thesemantic roles of the mandatory arguments, thusimproving the overall performance.6 ConclusionIn this paper, we propose an approach for in-ferring semantic role using sub-categorizationframes and maximum entropy model.
Using thisapproach, we obtained an F-measure of 68.14%on the development set of the data provided forthe CONLL-2005 shared task.7 Future workWe have observed that the main limitation of oursystem was in argument identification.
Currently,the recall of the arguments inferred from the out-put of the parser is 75.52% which makes it the up-per bound of recall of our system.
In near future,we would focus on increasing the upper boundof recall.
In this direction, we would also usethe partial syntactic information.
The accuracyof the first stage of our approach would increaseif we include the mandatory/optional informationfor training the parser (Yi and Palmer, 1999).8 AcknowledgementsWe would like to thank Prof. Rajeev Sangal, Dr.Sushama Bendre and Dr. Dipti Misra Sharma forguiding us in this project.
We would like to thankSzu-ting for giving some valuable advice.ReferencesS.
Chen and R. Rosenfield.
1999.
A gaussian prior forsmoothing maximum entropy models.M.
Collins.
1999.
Head driven statistical models fornatural language processing.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.Hwang Young Sook Lim, Joon-H and, So-Young Park,and Hae-Chang Rim.
2004.
Semantic role labellingusing maximum entropy model.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James.
H. Martin, and Daniel Juraf-sky.
2003.
Support Vector Learning for SemanticArgument Classification.Adwait Ratnaparakhi.
1999.
Learning to parse naturallanguage with maximum entropy models.Cynthia A. Thompson, Roger Levy, and Christo-pher D. Manning.
2003.
A generative model forsemantic role labelling.Szu-ting Yi and M. Palmer.
1999.
The integration ofsyntactic parsing and semantic role labeling.168
