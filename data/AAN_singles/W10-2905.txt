Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 38?45,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsIdentifying Patterns for Unsupervised Grammar InductionJesu?s Santamar??aU.
Nacional de Educacio?n a DistanciaNLP-IR Group, Madrid, Spain.jsant@lsi.uned.esLourdes AraujoU.
Nacional de Educacio?n a DistanciaNLP-IR Group, Madrid, Spain.lurdes@lsi.uned.esAbstractThis paper describes a new method for un-supervised grammar induction based onthe automatic extraction of certain pat-terns in the texts.
Our starting hypoth-esis is that there exist some classes ofwords that function as separators, mark-ing the beginning or the end of new con-stituents.
Among these separators we dis-tinguish those which trigger new levels inthe parse tree.
If we are able to detect theseseparators we can follow a very simpleprocedure to identify the constituents of asentence by taking the classes of words be-tween separators.
This paper is devoted todescribe the process that we have followedto automatically identify the set of sepa-rators from a corpus only annotated withPart-of-Speech (POS) tags.
The proposedapproach has allowed us to improve the re-sults of previous proposals when parsingsentences from the Wall Street Journal cor-pus.1 IntroductionMost works dealing with Grammar Induction (GI)are focused on Supervised Grammar Induction,using a corpus of syntactically annotated sen-tences, or treebank, as a reference to extract thegrammar.
The existence of a treebank for the lan-guage and for a particular type of texts from whichwe want to extract the grammar is a great help toGI, even taking into account the theoretical limi-tations of GI, such as the fact that grammars can-not be correctly identified from positive examplesalone (Gold, 1967).
But the manual annotationof thousands of sentences is a very expensive taskand thus there are many languages for which thereare not treebanks available.
Even in languages forwhich there is a treebank, it is usually composedof a particular kind of texts (newspaper articles,for example) and may not be appropriate for otherkind of texts, such as tales or poetry.
These rea-sons have led to the appearance of several worksfocused on unsupervised GI.Thanks to our knowledge of the language weknow that some classes of words are particularlyinfluential to determine the structure of a sentence.For example, let us consider the tree in Figure 1,for which the meaning of the POS tags appears inTable 1.
We can observe that the tag MD (Modal)breaks the sentence into two parts.
Analogously,in the tree appearing in Figure 2 the POS tag VBZbreaks the sentence.
In both cases, we can see thatafter the breaking tag, a new level appears in theparse tree.
A similar effect is observed for otherPOS tags, such as VB in the tree of Figure 1 andIN in the tree of Figure 2.
We call these kind ofPOS tags separators.
There are also other POStags which are frequently the beginning or the endof a constituent1.
For example in the tree in Fig-ure 1 we can find the sequences (DT NN) and (DTJJ NN), which according to the parse tree are con-stituents.
In the tree in Figure 2 we find the se-quence (DT NNP VBG NN).
In both trees we canalso find sequences beginning with the tag NNP:(NNP NNP) and (NNP CD) in the tree in Figure 1and (NNP NNP), which appears twice, in the treein Figure 2.
This suggests that there are classesof words with a trend to be the beginning or theend of constituents without giving rise to new lev-els in the parse tree.
We call these POS tags sub-separators.
These observations reflect some of ourintuitions, such as the fact that most sentences arecomposed of a noun phrase and a verb phrase, be-ing frequently the verb the beginning of the verbalphrase, which usually leads to a new level of theparse tree.
We also know that determiners (DT)are frequently the beginning of the noun phrases.1Constituents are language units in which we can arrangethe structure of a sentence.38SNP-SBJNPNNP NNPADJPNPCD NNSJJVPMD VPVB NPDT NNPP-CLRIN NPDT JJ NNNP-TMPNNP CDFigure 1: Parse tree for the sentence Pierre Vinken,61 years old, will join the board as a nonexecutivedirector Nov. 29. from the Penn TreebankSNP-SBJNNP NNPVPVBZ NP-PRDNPNNPPIN NPNPNNPNNPNPDT NNPVBG NNFigure 2: Parse tree for the sentence Mr. Vinkenis chairman of Elsevier N.V., the Dutch publishinggroup.
from the Penn TreebankAt this point we could either try to figure outwhat is the set of tags which work as separators, orto compute them from a parsed corpus for the con-sidered language, provided it is available.
How-ever, because we do not want to rely on the exis-tence of a treebank for the corresponding languageand type of texts we have done something differ-ent: we have devised a statistical procedure to au-tomatically capture the word classes which func-tion as separators.
In this way our approach canbe applied to most languages, and apart from pro-viding a tool for extracting grammars and parsingsentences, it can be useful to study the differentclasses of words that work as separators in differ-ent languages.Our statistical mechanism to detect separatorsis applied to a corpus of sentences annotated withPOS tags.
This is not a strong requirement sincethere are very accurate POS taggers (about 97%)for many languages.
The grammar that we obtaindoes not specify the left-hand-side of the rules, butonly sequences of POS tags that are constituents.CC Coordinating conjunctionCD Cardinal numberDT DeterminerEX Existential thereFW Foreign wordIN Preposition / subordinating conjunctionJJ AdjectiveJJR Adjective, comparativeJJS Adjective, superlativeLS List item markerMD ModalNN Noun, singular or massNNS Noun, pluralNNP Proper noun, singularNNPS Proper noun, pluralPDT PredeterminerPOS Possessive endingPRP Personal pronounPP$ Possessive pronounRB Adverb.RBR Adverb, comparativeRBS Adverb., superlativeRP ParticleSYM Symbol (mathematical or scientific)TO ToUH InterjectionVB Verb, base formVBD Verb, past tenseVBG Verb, gerund / present participleVBN Verb, past participleVBP Verb, non-3rd ps.
sing.
presentVBZ Verb, 3rd ps.
sing.
presentWDT wh-determinerWP wh-pronounWP$ Possessive wh-pronounWRB wh-adverbTable 1: Alphabetical list of part-of-speech tagsused in the Penn Treebank, the corpus used in ourexperimentsAt this point we have followed the Klein and Man-ning (2005) setting for the problem, which allowsus to compare our results to theirs.
As far as weknow these are the best results obtained so far forunsupervised GI using a monolingual corpus.
Asthey do, we have used the Penn treebank (Mar-cus et al, 1994) for our experiments, employingthe syntactic annotations that it provides for eval-uation purposes only.
Specifically, we have usedWSJ10, composed of 6842 sentences, which is thesubset of the Wall Street Journal section of thePenn Treebank, containing only those sentences of10 words or less after removing punctuation andnull elements, such as $, ?, etc.The rest of the paper is organized as follows:section 2 reviews some related works; section 3describes the details of the proposal to automati-cally extract the separators from a POS tagged cor-pus; section 4 is devoted to describe the procedureto find a parse tree using the separators; section395 presents and discusses the experimental results,and section 6 draws the main conclusions of thiswork.2 State of the ArtA growing interest in unsupervised GI has beenobserved recently with the appearance of severalworks in the topic.
Some of these works have fo-cused on finding patterns of words (Solan et al,2005) more than syntactic structures.
It has beennoted that the rules produced by GI can also be in-terpreted semantically (David et al, 2003), wherea non-terminal describes interchangeable elementswhich are instances of the same concepts.Distributional approaches to unsupervised GIexploit the principle of substitutability: con-stituents of the same type may be exchanged withone another without affecting the syntax of thesurrounding context.
Distributional approaches togrammar induction fall into two categories, de-pending on their treatment of nested structure.
Thefirst category covers Expectation-Maximization(EM) systems (Dempster et al, 1977).
These sys-tems propose constituents based on analysis of thetext, and then select a non-contradictory combina-tion of constituents for each sentence that maxi-mizes a given metric, usually parsing probability.One of the most successful proposals in this areais the one by Klein and Manning (2005), which,as mentioned before, starts from a corpus labelledonly with POS tags.
The key idea of the modelproposed in this work is that constituents appearin constituent contexts.
However, the EM algo-rithm presents some serious problems: it is veryslow (Lari and Young, 1990), and is easily trappedin local maxima (Carroll and Charniak, 1992).Alignment Based Learning (ABL) (van Zaanenand Leeds, 2000) is the only EM system applieddirectly to raw text.
However, ABL is relativelyinefficient and has only been applied to small cor-pora.
Brooks (Brooks, 2006) reverses the notionof distributional approaches: if we can identify?surrounding context?
by observation, we can hy-pothesize that word sequences occurring in thatcontext will be constituents of the same type.
Hedescribes a simplified model of distributional anal-ysis (for raw test) which uses heuristics to reducethe number of candidate constituents under con-sideration.
This is an interesting idea in spite thatBrook showed that the system was only capable oflearning a small subset of constituent structures ina large test corpus.The second category is that of incrementallearning systems.
An incremental system analyzesa corpus in a bottom-up fashion: each time a newconstituent type is found it is inserted into the cor-pus to provide data for later learning.
The EMILE(Adriaans, 1999) and ADIOS (David et al, 2003)systems are examples for this category, not yetevaluated on large corpora.Bilingual experiments have been also conductedwith the aim to exploit information from one lan-guage to disambiguate another.
Usually such asetting requires a parallel corpus or another an-notated data that ties the two languages.
Co-hen and Smith (2009) use the English and Chi-nese treebanks, which are not parallel corpora, totrain parsers for both languages jointly.
Their re-sults shown that the performance on English im-proved in the bilingual setting.
Another relatedwork (Snyder et al, 2009) uses three corpora ofparallel text.
Their approach is closer to the un-supervised bilingual parsing model developed byKuhn (2004), which aims to improve monolingualperformance.The approach considered in this work follows adifferent direction, trying to identify certain pat-terns that can determine the structure of the parsetrees.3 Extracting Separators from the CorpusTo automatically extract the set of separators andsub-separators from a corpus of POS tagged sen-tences we start from some assumptions:?
The most frequent sequence (of any length)of POS tags in the corpus is a constituent,that we call safe constituent (sc).
It is quite asensible assumption, since we can expect thatat least for the most frequent constituent thenumber of occurrences overwhelms the num-ber of sequences appearing by chance.?
We also assume that the POS tag on the left,Lsc, and on the right, Rsc, of the safe con-stituent are a kind of context for other se-quences that play the same role.
Accord-ing to this, other extended sequences withLsc and Rsc at the ends but with other POStags inside are also considered constituents.This assumption is somehow related to theKlein and Manning?s (2005) idea underlyingtheir unsupervised GI proposal.
According40to them, constituents appear in constituentcontexts.
Their model exploits the fact thatlong constituents often have short, commonequivalents, which appear in similar contextsand whose constituency as a grammar rule ismore easily found.?
According to the previous point, we use thetag on the left (Lsc) and on the right (Rsc) ofthe safe constituent as discriminant with re-spect to which to study the behavior of eachPOS tag.
A POS tag E can have a bias to beinside the safe constituent, to be outside thesafe constituent (separator), or not to have abias at all (sub-separator).
We define the de-termining side of a tag E, as the end tag, Lscor Rsc, of the sc with the greater differenceon the number of occurrences of E on bothsides of the end tag.
For example, if the ra-tio of occurrences of E on the left and on theright of Lsc is smaller (they are more differ-ent) than the ratio of E on the left and on theright of Rsc, then Lsc is the determining sideof E, ds(E)2.
Then:?
E is considered a separator in the fol-lowing cases:?
if Lsc is the determining side for Eand E appears a 75% more often tothe left of Lsc than to the right (the75% has been fixed after some esti-mates described below), or?
if Rsc is the determining side for Eand E appears a 75% more often tothe right of Rsc than to the left.?
E is considered a sub-separator if thefollowing conditions hold:?
if Lsc is the determining side for Eand E appears a 75% less often tothe left of Lsc than to the right (theratios are very similar) , or?
if Rsc is the determining side for Eand E appears a 75% less often tothe right of Rsc than to the left.?
In the remaining cases E is consideredto be part of a constituent (the prefer-ence is to be inside).Let us introduce some notation to define moreformally the separators and sub-separators.
Let2If the number of occurrences of E on any side of Lscor Rsc is zero, then we compare differences between occur-rences instead of ratios.#(E1, ?
?
?
, En) be the number of occurrences ofthe sequence of tags (E1, ?
?
?
, En).
We define apredicate sim to denote the similarity between thenumber of occurrences of a sequence of two tagsand the one with reverse order, assim(E1, E2)) =#(E1, E2)#(E2, E1)?
0.75 if #(E1, E2) ?
#(E2, E1)#(E2, E1)#(E1, E2)?
0.75 if #(E2, E1) ?
#(E1, E2)Then a tag E is considered a separator if thefollowing predicate is true:sep(Lsc, E, Rsc) =(sd(Lsc) ?
(#(E, Lsc) > #(Lsc, E)?
?sim(E, Lsc)))?
(sd(Rsc) ?
(#(Rsc, E) > #(E, Rsc) ?
?sim(E, Rsc)).
A tag is considered a sub-separator when thefollowing predicate is true:subsep(Lsc, E, Rsc) =(sd(Lsc) ?
sim(E, Lsc))?
(sd(Rsc) ?
sim(E, Rsc)).We have computed the number of occurrencesof every sequence of POS tags in the corpus, find-ing that the most frequent sequence of tags is(DT,NN).
This sequence, which is our safe con-stituent, appears 2222 times in the considered cor-pus WSJ10.Applying our procedure to the corpus we haveobtained the following sets of separators and sub-separators:Separators MD, PRP, IN, RB, RBR,CC, TO, VB, VBD, VBN,VBZ, VBP, VBG, EX, LS,RP, UH, WP, WRB, WDTSub-separators DT, PDT, POS, SYM, NN,NNS, NNP, NNPSFor selecting a threshold value to discriminatethe preference of a POS tag to be inside or out-side of a constituent we have studied the resultsobtained for different threshold values greater than50%.
Table 2 shows the results.
We can observeall of them are very similar for all the thresholds,as long as they are greater than 50%.
Analyzingthe set of POS-tags that have been classified asseparators and sub-separators with each thresholdwe have found that the only differences are that thetag POS (Possessive ending), which is classifiedas sub-separator using a threshold between 50%41and 75%, is classified as separator using higherthresholds, and the tag SYM (Symbol), which isclassified as sub-separator using a threshold be-tween 50% and 75%, is classified neither as a sep-arator nor as a sub-separator using higher thresh-olds.
We have adopted a threshold value of 75%because higher values can be too restrictive, and infact provide worse results.Similarity F155% 74.5565% 74.5575% 74.5585% 72.2495% 72.24Table 2: F-measure results obtained for differentvalues of the threshold used to classify the set ofPOS-tags.Sub-separators can be grouped to their right orto their left, depending on the case.
In order tomeasure the bias of each of them for one direc-tion or another we have compared the number ofoccurrences of the most frequent sequence com-posed of the sub-separator and a POS tag on theright and on the left.
We choose as preference di-rection for a sub-separator the corresponding tothe most frequent sequence.
Table 3 shows theresults obtained, the preference direction of eachsub-separator appearing in the last column.
In thecase of NNP, for which the frequency of the mostfrequent tag to the right and to the left are thesame, we have looked at the second most frequentsequence to choose the grouping direction.sub-sep left freq.
right freq.
DDT (DT, NN)(2222) (IN,DT)(894) LPDT (PDT,DT)(28) (NN,PDT)(14) LPOS (POS, NN)(169) (NNP, POS)(223) RSYM (SYM, IN)(11) (NN,SYM)(4) LNN (NN, IN)(892) (DT,NN)(2222) RNNS (NNS, VBP)(591) (JJ,NNS)(797) RNNP (NNP, NNP)(2127) (NNP,NNP)(2127) RNNPS (NNPS, NNP)(42) (NNP,NNPS)(82) RTable 3: Preference direction to which each sub-separator clusters.
The first column correspondsto the sub-separator, the second one to the mostfrequent sequence composed of the sub-separatorand a tag on its right, the third one to the most fre-quent sequence of the sub-separator and a tag onits left, and the last column to the resulting direc-tion.4 Identifying ConstituentsOnce we have the sets of separators and sub-separators the procedure to identify the con-stituents of each sentence is as follows:?
We identify the separators in the sentence.For example, if we consider the sentence:CC DT NN IN NNP NNP POS NN VBZthe separators are marked in boldface:CC DT NN IN NNP NNP POS NN VBZ?
The next step is to split the sentence ac-cording to the separators.
The first separatorwhich is a verb, if any, is used to split the sen-tence into two parts.
Each separator can giverise to two groups: one composed of the tagsequence between the separator and the nextseparator, and another one which includes theseparator and the POS tags up to the end ofthe part of the sentence in which it appears(usually sentences are divided into two partsusing the first separator which is a verb).
Inour example, this mechanism leads to the fol-lowing structure:[[CC [DT NN] [IN [NNP NNP POS NN]]][VBZ]]?
Now it is the turn of the sub-separators (DT,PDT, POS, SYM, NN, NNS, NNP, NNPS),which are underlined in the sentence:[[CC [DT NN] [IN [NNP NNP POS NN]]][VBZ]]?
Finally, each group of the sentence is splitaccording to the sub-separators.
Each sub-separator has been assigned a preference di-rection to form the group with the next POStag.
Looking at Table 3, which tells us thedirection in which each sub-separator formsthe group, we apply this step to our sentenceexample, obtaining:[[CC [DT NN] [IN [[NNP NNP POS] NN]]][VBZ]]42The sub-separator DT is grouped with thetags on its right, while NN is grouped withthe tags on its left, thus composing the group(DT NN).
When two or more sub-separatorsappear in a sequence, they are grouped to-gether in a unique constituent whenever theyhave the same grouping direction.
In our sen-tence example this criterion leads to [NNPNNP POS] instead of [NNP[NNP[POS]]].
Aconstituent finishes if the next POS tag is aseparator or if it is a sub-separator that makesgroups towards the left.
Since POS (Pos-sessive ending) tends to be grouped with thePOS tag on its left, it is the end of the con-stituent.Figure 3 represents the obtained structure as aparse tree.
Figure 4 represents the correct parsetree according to the Penn treebank.
We can ob-serve that both structures are very similar.
Themethod based on separators has been able to cap-ture most of the constituent appearing in the parsetree: (DT, NN), (NNP, NNP, POS), (NNP, NNP,POS, NN), (IN, NNP, NNP, POS, NN).
The differ-ences between both trees come from our criterionof splitting the sequence of tags into two subse-quences using the first verb.
This problem will betackled in the future in a more refined model.CCCC CDT NNCIN CCNNP NNP POSNNVBZFigure 3: Parse tree for the sentence And the noseon Mr. Courter?s face grows from the Penn tree-bank (WSJ), obtained with our separators method.5 EvaluationOur proposal has been evaluated by comparing thetree structures produced by the system to the gold-standard trees produced by linguists, which can befound in the Penn Treebank.
Because we do notassign class name to our constituents, i.e.
a lefthand side symbol for the grammar rules, as the lin-guists do in treebanks, the comparison ignores theclass labels, considering only groups of tags.SCC NP-SBJNPDT NNPP-LOCIN NPNPNNP NNP POSNNVPVBZFigure 4: Parse tree appearing in the Penn tree-bank (WSJ) for the sentence And the nose on Mr.Courter?s face grows.The results presented in the work by Klein andManning (2005) have been our reference, since asfar we know they are the best ones obtained so farfor unsupervised GI.
For the sake of comparison,we have considered the same corpus and the samemeasures.
Accordingly, we performed the experi-ments on the 6842 sentences3 of the WSJ10 selec-tion from the Penn treebank Wall Street Journalsection.In order to evaluate the quality of the obtainedgrammar we have used the most common mea-sures for parsing and grammar induction evalua-tion: recall, precision, and their harmonic mean(F-measure).
They are defined assuming a bracketrepresentation of a parse tree.Precision is given by the number of bracketsin the parse to evaluate which match those in thecorrect tree and recall measures how many of thebrackets in the correct tree are in the parse.
Thesemeasures have counterparts for unlabeled trees,the ones considered in this work ?
in which thelabel assigned to each constituent is not checked.Constituents which could not be wrong (those ofsize one and those spanning the whole sentence)have not been included in the measures.The definitions of Unlabeled Precision (UP) andRecall (UR) of a proposed corpus P = [Pi] againsta gold corpus G = [Gi] are:UP (P, G) =?i |brackets(Pi) ?
brackets(Gi)|?i |brackets(Pi)|,UR(P, G) =?i |brackets(Pi) ?
brackets(Gi)|?i |brackets(Gi)|.Finally, UF (Unlabeled F-measure) is given by:UF = 2 ?
UP (P, G) ?
UR(P, G)UP (P, G) + UR(P, G) .3More precisely sequences of POS tags431 2 3 4 5 6 7 8 9 10Constituent size60708090100%RecallPrecisionF-measureFigure 5: Results obtained per constituent size:unlabeled recall, precision, and F-measure.Figure 5 shows the results of unlabeled recall,precision and F-measure obtained per constituentsize.
We can observe that recall and precision, andthus the corresponding F-measure, are quite sim-ilar for every constituent size.
This is important,because obtaining a high F-measure thanks to avery high recall but with a poor precision, is notso useful.
We can also observe that the best resultsare obtained for short and long constituents, withlower values for middle lengths, such as 5 and 6.We believe that this is because intermediate sizeconstituents present more variability.
Moreover,for intermediate sizes, the composition of the con-stituents is more dependent on sub-separators, forwhich the statistical differences are less significantthan for separators.2 3 4 5 6 7 8 9Constituent size5060708090F-measureSeparator approachKlein-Manning approachFigure 6: Comparison of the separator approachand Klein and Manning?s approach per constituentsize.We have compared our results to those obtainedby Klein and Manning (2005) for the same corpus.Table 4 shows the obtained results for WSJ10.
Wecan observe that we have obtained more balancedvalues of recall and precision, as well as a bettervalue for the F-measure.
Thus the method pro-posed in this work, that we expect to refine byassigning different probabilities to separators andsub-separators, depending on the context they ap-pear in, provides a very promising approach.UR UP UFSepar.
A.
77,63% 71,71% 74,55%KM 80.2% 63.8% 71.1%Table 4: Results (unlabeled recall, precision, andF-measure), obtained with the separator approach(first row) and with the Klein and Manning ap-proach (second row) for the WSJ10 corpus.Figure 6 compares the F-measure for the twoapproaches by constituents length.
We can ob-serve that the separator approach obtains better re-sults for all the lengths.
The figure also shows thatthe results per constituent length follow the sametrend in both approaches, thus reflecting that thedifficulty for middle length constituents is greater.6 ConclusionsWe have proposed a novel approach for unsuper-vised grammar induction which is based on iden-tifying certain POS tags that very often divide thesentences in particular manners.
These separatorsare obtained from POS tagged texts, thus makingthe model valid for many languages.
The con-stituents corresponding to a sentence are found bymeans of a simple procedure based on the sepa-rators.
This simple method has allowed us to im-prove the results of previous proposals.We are currently working in defining a more re-fined statistical model which takes into accountthe probability of a tag to be a separator or sub-separator, depending on its context.
We plan toapply a similar study to other languages, in orderto study the different classes of words that func-tion as separator in each of them.AcknowledgementsThis paper has been funded in part by the Span-ish MICINN project QEAVis-Catiex (SpanishMinisterio de Educacio?n y Ciencia - TIN2007-67581), as well as by the Regional Government ofMadrid under the Research Network MA2VICMR(S2009/TIC-1542).44ReferencesPieter Adriaans.
1999.
Learning Shallow Context-FreeLanguages under Simple Distributions.
TechnicalReport, Institute for Logic, Language, and Compu-tation, Amsterdam.David J. Brooks.
2006.
Unsupervised grammar in-duction by distribution and attachment.
In CoNLL-X?06: Proceedings of the Tenth Conference on Com-putational Natural Language Learning, pages 117?124.
Association for Computational Linguistics.Glenn Carroll and Eugene Charniak.
1992.
Two exper-iments on learning probabilistic dependency gram-mars from corpora.
In Working Notes of the Work-shop Statistically-Based NLP Techniques, pages 1?13.
AAAI.Shay B. Cohen and Noah A. Smith.
2009.
Sharedlogistic normal distributions for soft parameter ty-ing in unsupervised grammar induction.
In NAACL?09: Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 74?82.
Association forComputational Linguistics.Zach Solan David, David Horn, and Shimon Edelman.2003.
Unsupervised efficient learning and represen-tation of language structure.
In Proc.
25th Confer-ence of the Cognitive Science Society, pages 2577?3596.
Erlbaum.A.
Dempster, N. Laird, and D. Rubin.
1977.
Max-imum likelihood from incomplete data via the EMalgorithm.
Royal statistical Society B, 39:1?38.E.
Mark Gold.
1967.
Language identification in thelimit.
Information and Control, 10(5):447?474.Dan Klein and Christopher D. Manning.
2005.
Nat-ural language grammar induction with a genera-tive constituent-context model.
Pattern Recognition,38(9):1407?1419.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In ACL ?04: Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, page 470.
Association for Com-putational Linguistics.K.
Lari and S. J.
Young.
1990.
The estimation ofstochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Language,4:35?56.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.Benjamin Snyder, Tahira Naseem, and Regina Barzi-lay.
2009.
Unsupervised multilingual grammar in-duction.
In ACL-IJCNLP ?09: Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 1, pages 73?81.
Association for ComputationalLinguistics.Zach Solan, David Horn, Eytan Ruppin, and Shi-mon Edelman.
2005.
Unsupervised learning ofnatural languages.
Proceedings of the NationalAcademy of Sciences of the United States of Amer-ica, 102(33):11629?11634.Menno van Zaanen and Ls Jt Leeds.
2000.
Learningstructure using alignment based learning.
In Univer-sities of Brighton and Sussex, pages 75?82.45
