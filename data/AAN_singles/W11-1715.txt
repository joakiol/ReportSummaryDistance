Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 118?124,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsMining Subjective Knowledge from Customer Reviews:A Specific Case of Irony DetectionAntonio Reyes and Paolo RossoNatural Language Engineering Lab - ELiRFDepartamento de Sistemas Informa?ticos y Computacio?nUniversidad Polite?cnica de Valencia, Spain{areyes,prosso}@dsic.upv.esAbstractThe research described in this work focuseson identifying key components for the task ofirony detection.
By means of analyzing a setof customer reviews, which are considered asironic both in social and mass media, we tryto find hints about how to deal with this taskfrom a computational point of view.
Our ob-jective is to gather a set of discriminating el-ements to represent irony.
In particular, thekind of irony expressed in such reviews.
Tothis end, we built a freely available data setwith ironic reviews collected from Amazon.Such reviews were posted on the basis of anonline viral effect; i.e.
contents whose ef-fect triggers a chain reaction on people.
Thefindings were assessed employing three clas-sifiers.
The results show interesting hints re-garding the patterns and, especially, regardingthe implications for sentiment analysis.1 IntroductionVerbal communication is not a trivial process.
It im-plies to share a common code as well as being ableto infer information beyond the semantic meaning.A lot of communicative acts imply information notgrammatically expressed to be able to decode thewhole sense: if the hearer is not capable to infer thatinformation, the communicative process is incom-plete.
Let us consider a joke.
The amusing effectsometimes relies on not given information.
If suchinformation is not filled, the result is a bad, or bettersaid, a misunderstood joke.
This information, whichis not expressed with ?physical?
words, supposes agreat challenge, even from a linguistic analysis, be-cause it points to social and cognitive layers quitedifficult to be computationally represented.
One ofthe communicative phenomena which better repre-sents this problem is irony.
According to Wilsonand Sperber (2007), irony is essentially a commu-nicative act which expresses an opposite meaning ofwhat was literally said.Due to irony is common in texts that express sub-jective and deeply-felt opinions, its presence repre-sents a significant obstacle to the accurate analysisof sentiment in such texts (cf.
Councill et al (2010)).In this research work we aim at gathering a set ofdiscriminating elements to represent irony.
In par-ticular, we focus on analyzing a set of customer re-views (posted on the basis of an online viral effect)in order to obtain a set of key components to face thetask of irony detection.This paper is organized as follows.
Section 2 in-troduces the theoretical problem of irony.
Section 3presents the related work as well as the evaluationcorpus.
Section 4 describes our model and the ex-periments that were performed.
Section 5 assessesthe model and presents the discussion of the results.Finally, Section 6 draws some final remarks and ad-dresses the future work.2 Pragmatic Theories of IronyLiterature divides two primaries classes of irony:verbal and situational.
Most theories agree on themain property of the former: verbal irony conveysan opposite meaning; i.e.
a speaker says some-thing that seems to be the opposite of what s/hemeans (Colston and Gibbs, 2007).
In contrast, sit-uational irony is a state of the world which is per-ceived as ironical (Attardo, 2007); i.e.
situations thatshould not be (Lucariello, 2007).
Our work focuseson verbal irony.
This kind of irony is defined as away of intentionally denying what it is literally ex-pressed (Curco?, 2007); i.e.
a kind of indirect nega-tion (Giora, 1995).
On the basis of some pragmaticframeworks, authors focus on certain fine-grainedaspects of this term.
For instance, Grice (1975) con-118siders that an utterance is ironic if it intentionallyviolates some conversational maxims.
Wilson andSperber (2007) assume that verbal irony must beunderstood as echoic; i.e.
as a distinction betweenuse and mention.
Utsumi (1996), in contrast, sug-gests an ironic environment which causes a nega-tive emotional attitude.
According to these pointsof view, the elements to conceive a verbal expres-sion as ironic point to different ways of explainingthe same underlying concept of opposition, but spe-cially note, however, that most of them rely on lit-erary studies (Attardo, 2007); thus, their computa-tional formalization is quite challenging.
Further-more, consider that people have their own conceptof irony, which often does not match with the rulessuggested by the experts.
For instance, consider thefollowing expressions retrieved from the web:1.
?If you find it hard to laugh at yourself, I would be happy to doit for you.?2.
?Let?s pray that the human race never escapes from Earth tospread its iniquity elsewhere.
?These examples, according to some user-generated tags, could be either ironic, or sarcastic,or even satiric.
However, the issue we want to fo-cus does not lie on what tag should be the rightfor every expression, but on the fact that there isnot a clear distinction about the boundaries amongthese terms.
For Colston (2007), sarcasm is a termcommonly used to describe an expression of verbalirony; whereas for Gibbs (2007), sarcasm along withjocularity, hyperbole, rhetorical questions, and un-derstatement, are types of irony.
Attardo (2007) inturn, considers that sarcasm is an overtly aggressivetype of irony.
Furthermore, according to Gibbs andColston (2007), irony is often compared to satire andparody.In accordance with these statements, the limitsamong these figurative devices are not clearly dif-ferentiable.
Their differences rely indeed on mattersof usage, tone, and obviousness, which are not soevident in ordinary communication acts.
Therefore,if there are no formal boundaries to separate theseconcepts, even from a theoretical perspective, peo-ple will not be able to produce ironic expressions asthe experts suggest.
Instead, there will be a mix-ture of expressions pretending to be ironic but beingsarcastic, satiric, or even humorous.
This get worsewhen dealing with non prototypical examples.
Ob-serve the following fragment from our corpus:3.
?I am giving this product [a t-shirt] 5 stars because not everyoneout there is a ladies?
man.
In the hands of lesser beings, it canhelp you find love.
In the hands of a playa like me, it can onlybreak hearts.
That?s why I say use with caution.
I am passing thetorch onto you, be careful out there folks.
?In this text irony is perceived as a mixture of sar-casm and satire, whose effect is not only based onexpressing an opposite or negative meaning, but ahumorous one as well.Taking into account these assumptions, we beginby defining irony as a verbal subjective expressionwhose formal constituents attempt to communicatean underlying meaning, focusing on negative or hu-morous aspects, which is opposite to the one ex-pressed.
Based on this definition, we consider sar-casm, satire, and figures such as the ones suggestedin (Gibbs, 2007), as specific extensions of a gen-eral concept of irony, and consequently, we will notmake any fine-grained distinction among them; i.e.irony will include them.3 Approaching Irony DetectionAs far as we know, very few attempts have beencarried out in order to integrate irony in a compu-tational framework.
The research described by Ut-sumi (1996) was one of the first approaches to com-putationally formalize irony.
However, his modelis too abstract to represent irony beyond an ide-alized hearer-listener interaction.
Recently, froma computational creativity perspective, Veale andHao (2009) focused on studying irony by analyz-ing humorous similes.
Their approach gives somehints to explain the cognitive processes that underlyirony in such structures.
In contrast, Carvalho etal.
(2009) suggested some clues for automaticallyidentifying ironic sentences by means of identifyingfeatures such as emoticons, onomatopoeic expres-sions, punctuation and quotation marks.
Further-more, there are others approaches which are focusedon particular devices such as sarcasm and satire,rather than on the whole concept of irony.
For in-stance, Tsur et al (2010) and Davidov et al (2010)address the problem of finding linguistic elementsthat mark the use of sarcasm in online product re-views and tweets, respectively.
Finally, Burfoot andBaldwin (2009) explore the task of automatic satire119detection by evaluating features related to headlineelements, offensive language and slang.3.1 Evaluation CorpusDue to the scarce work on automatic irony process-ing, and to the intrinsic features of irony, it is quitedifficult and subjective to obtain a corpus with ironicdata.
Therefore, we decided to rely on the wisdomof the crowd and use a collection of customer re-views from the Amazon web site.
These reviewsare considered as ironic by customers, as well as bymany journalists, both in mass and social media.
Ac-cording to such means, all these reviews deal withirony, sarcasm, humor, satire and parody (hence,they are consistent with our definition of irony).
Allof them were posted by means of an online viraleffect, which in most cases, increased the popular-ity and sales of the reviewed products.
The ThreeWolf Moon T-shirt is the clearest example.
This itembecame one of the most popular products, both inAmazon as well as in social networks, due to theironic reviews posted by people1.Our positive data are thus integrated with reviewsof five different products published by Amazon.
Allof them were posted through the online viral effect.The list of products is: i) Three Wolf Moon T-shirt(product id: B002HJ377A); ii) Tuscan Whole Milk(product id: B00032G1S0); iii) Zubaz Pants (prod-uct id: B000WVXM0W); iv) Uranium Ore (prod-uct id: B000796XXM); and v) Platinum RadiantCut 3-Stone (product id: B001G603AE).
A total of3,163 reviews were retrieved.
Then, in order to au-tomatically filter the ones more likely to be ironicwithout performing a manual annotation (which isplanned to be carried out in the near feature), we re-moved the reviews whose customer rating, accord-ing to the Amazon rating criteria, was lesser thanfour stars.
The assumptions behind this decision relyon two facts: i) the viral purpose, and ii) the ironiceffect.
The former caused that people to post reviewswhose main purpose, and perhaps the only one, wasto exalt superficial properties and non-existent con-sequences; thus the possibilities to find real reviewswere minimal.
Considering this scenario, the lat-1According to results obtained with Google, apart from themore than one million of results retrieved when searching thisproduct, there are more than 10,000 blogs which comment theeffect caused by these reviews.ter supposes that, if someone ironically wants to re-flect properties and consequences such as the previ-ous ones, s/he will not do it by rating the productswith one or two stars, instead, s/he will rate themwith the highest scores.After applying this filter, we obtained an ironic setintegrated with 2,861 documents.
On the other hand,two negative sets were automatically collected fromtwo sites: Amazon.com (AMA) and Slashdot.com(SLA).
Each contains 3,000 documents.
The prod-ucts selected from AMA were: Bananagrams (toy),The Help by Kathryn Stockett (book), Flip Ul-traHD Camcorder (camera), I Dreamed A Dream(CD), Wii Fit Plus with Balance Board (Videogameconsole).
Finally, the data collected from SLAcontain web comments categorized as funny in acommunity-driven process.
The whole evaluationcorpus is integrated with 8,861 documents.
It isavailable at http://users.dsic.upv.es/grupos/nle.4 ModelWe define a model with six categories which at-tempts to represent irony from different linguisticlayers.
These categories are: n-grams, POS n-grams, funny profiling, positive/negative profiling,affective profiling, and pleasantness profiling.4.1 N-gramsThis category focuses on representing the ironicdocuments in the simplest way: with sequences ofn-grams (from order 2 up to 7) in order to find a setof recurrent words which might express irony.
Notethat all the documents were preprocessed.
Firstly,the stopwords were removed, and then, all the doc-uments were stemmed.
The next process consistedin removing irrelevant terms by applying a tf ?
idfmeasure.
This measure assesses how relevant a wordis, given its frequency both in a document as in theentire corpus.
Irrelevant words such as t-shirt, wolf,tuscan, milk, etc., were then automatically elimi-nated.
The complete list of filtered words, stopwordsincluded, contains 824 items.
Examples of the mostfrequent sequences are given in Table 1.4.2 POS n-gramsThe goal of this category is to obtain recurrent se-quences of morphosyntactic patterns.
According to120Table 1: Statistics of the most frequent word n-grams.Order Sequences Examples2-grams 160 opposit sex; american flag; alpha male3-grams 82 sex sex sex; fun educ game4-grams 78 fun hit reload page; remov danger reef pirat5-grams 76 later minut custom contribut product6-grams 72 fals function player sex sex sex7-grams 69 remov danger reef pirat fewer shipwreck survivour definition, irony looks for expressing an oppo-site meaning; however, the ways of transmitting thatmeaning are enormous.
Therefore, we pretend tosymbolize an abstract structure through sequencesof POS tags (hereafter, POS-grams) instead of onlywords.
It is worth highlighting that a statistical sub-string reduction algorithm (Lu?
et al, 2004) was em-ployed in order to eliminate redundant sequences.For instance, if the sequences ?he is going to look sohot in this shirt?
and ?he is going to look hot in thisshirt?
occur with similar frequencies in the corpus,then, the algorithm removes the last one because isa substring of the first one.
Later on, we labeled thedocuments employing the FreeLing resource (Atse-rias et al, 2006).
The N-best sequences of POS-grams, according to orders 2 up to 7, are given inTable 2.4.3 Funny profilingIrony takes advantage of humor aspects to produceits effect.
This category intends to characterize thedocuments in terms of humorous properties.
In or-der to represent this category, we selected some ofthe best humor features reported in the literature:stylistic features, human centeredness, and keyness.The stylistic features, according to the experimentsreported in (Mihalcea and Strapparava, 2006), wereobtained by collecting all the words labeled with thetag ?sexuality?
in WordNet Domains (Bentivogli etal., 2004).
The second feature focuses on social re-lationships.
In order to retrieve these words, the el-ements registered in WordNet (Miller, 1995), whichbelong to the synsets relation, relationship andrelative, were retrieved.
The last feature is repre-sented by obtaining the keyness value of the words(cf.
(Reyes et al, 2009)).
This value is calculatedcomparing the word frequencies in the ironic doc-uments against their frequencies in a reference cor-pus.
Google N-grams (Brants and Franz, 2006) wasTable 2: Statistics of the most frequent POS-grams.Order Sequences Examples2-grams 300 dt nn; nn in; jj nn; nn nn3-grams 298 dt nn in; dt jj nn; jj nn nn4-grams 282 nn in dt nn; vb dt jj nn5-grams 159 vbd dt vbg nn jj6-grams 39 nnp vbd dt vbg nn jj7-grams 65 nns vbd dt vbg nn jj fdused as the reference corpus.
Only the words whosekeyness was ?
100 were kept.4.4 Positive/Negative ProfilingAs we have already pointed out, one of the most im-portant properties of irony relies on the communi-cation of negative information through positive one.This category intends to be an indicator about thecorrelation between positive and negative elementsin the data.
The Macquarie Semantic OrientationLexicon (MSOL) (Saif et al, 2009) was used to la-bel the data.
This lexicon contains 76,400 entries(30,458 positive and 45,942 negative ones).4.5 Affective ProfilingIn order to enhance the quality of the informationrelated to the expression of irony, we considered torepresent information linked to psychological lay-ers.
The affective profiling category is an attemptto characterize the documents in terms of wordswhich symbolize subjective contents such as emo-tions, feelings, moods, etc.
The WordNet-Affectresource (Strapparava and Valitutti, 2004) was em-ployed for obtaining the affective terms.
This re-source contains 11 classes to represent affectiveness.According to the authors, these classes representhow speakers convey affective meanings by meansof selecting certain words and not others.4.6 Pleasantness ProfilingThe last category is an attempt to represent idealcognitive scenarios to express irony.
This meansthat, like words, the contexts in which irony ap-pears are enormous.
Therefore, since it is impos-sible to make out all the possibilities, we pretend todefine a schema to represent favorable and unfavor-able ironic contexts on the basis of pleasantness val-ues.
In order to represent those values, we used theDictionary of Affect in Language (Whissell, 1989).This dictionary assigns a score of pleasantness to121?
9,000 English words.
The scores were obtainedfrom human ratings.
The range of scores goes from1 (unpleasant) to 3 (pleasant).5 EvaluationIn order to verify the effectiveness of our model, weevaluated it through a classification task.
Two un-derlying goals were analyzed: a) feature relevance;and b) the possibility of automatically finding ironicdocuments.The classifiers were evaluated by comparing thepositive set against each of the two negative subsets(AMA and SLA, respectively).
All the documentswere represented as frequency-weighted term vec-tors according to a representativeness ratio.
This ra-tio was estimated using Formula 1:?
(dk) =?i,j fdfi,j|d|(1)where i is the i-th conceptual category (i = 1. .
.
6);j is the j-th feature of i; fdfi,j (feature dimensionfrequency) is the frequency of features j of cate-gory i; and |d| is the length of the k-th documentdk.
For categories funny, positive/negative, affec-tive, and pleasantness, we determined an empiricalthreshold of representativeness ?
0.5.
A documentwas assigned the value = 1 (presence) if its ?
ex-ceeded the threshold, otherwise a value = 0 (ab-sence) was assigned.
A different criterion was de-termined for the n-grams and POS-grams becausewe were not only interested in knowing whether ornot the sequences appeared in the corpus, but also inobtaining a measure to represent the degree of simi-larity among the sets.
In order to define a similarityscore, we used the Jaccard similarity coefficient.The classification accuracy was assessed employ-ing three classifiers: Na?
?ve Bayes (NB), support vec-tor machines (SVM), and decision trees (DT).
Thesets were trained with 5,861 instances (2,861 posi-tive and 3,000 negative ones).
10-fold cross valida-tion method was used as test.
Global accuracy aswell as detailed performance in terms of precision,recall, and F ?measure, are given in Table 3.5.1 DiscussionRegarding the first goal (feature relevance), our a-priori aim of representing some irony features inTable 3: Classification results.Accuracy Precision Recall F-MeasureAMA 72,18% 0,745 0,666 0,703NB SLA 75,19% 0,700 0,886 0,782AMA 75,75% 0,771 0,725 0,747SVM SLA 73,34% 0,706 0,804 0,752AMA 74,13% 0,737 0,741 0,739DT SLA 75,12% 0,728 0,806 0,765terms of six general categories seems to be accept-able.
According to the results depicted in Table 3,the proposed model achieves good rates of classifi-cation which support this assumption: from 72% upto 89%, whereas a classifier that labels all texts asnon-ironic would achieve an accuracy around 54%.Moreover, both precision and recall, as well as F-measure rates corroborate the effectiveness of suchperformance: most of classifiers obtained scores >0.7.
This means that, at least regarding the data setsemployed in the experiments, the capabilities for dif-ferentiating an ironic review from a non-ironic one,or a web comment, are satisfactory.With respect to the second goal, an informationgain filter was applied in order to verify the rel-evance of the model for finding ironic documentsregarding the different discourses profiled in eachnegative subset.
In Table 4 we detailed the most dis-criminating categories per subset according to theirinformation gain scores.
On the basis of the re-sults depicted in this table, it is evident how therelevance of the categories varies in function of thenegative subset.
For instance, when classifying theAMA subset, it is clear how the POS-grams (order3), pleasantness and funny categories, are the mostinformative ones; in contrast, the pleasantness, n-grams (order 5) and funny categories, are the mostrelevant ones regarding the SLA subset.
Moreover,it is important to note how the negative words, with-out being the most differentiable ones, function asdiscriminating elements.Table 4: The 5 most discriminating categories regardinginformation gain results.AMA POS 3-grams Pleasantness Funny POS 2-grams POS 4-gramsSLA Pleasantness 5-grams Funny Affectiveness 6-gramsTaking into consideration all previous remarks,we would like to stress some observations with re-122spect to each category.
Regarding the n-grams, itis important to note the presence of some interestingsequences which are not common to the three sub-sets.
For instance: pleasantly surprised.
How-ever, we cannot define irony only in terms of thesesequences because they might represent domain-specific information such as the bigram: customerservice.With respect to the POS-grams, the fact offocusing on morphosyntactic templates instead ofonly on words seem to be more affective.
Forinstance, the sequence noun + verb + noun +adjective would represent more information thanthe sum of simple words: [grandpa/hotel/bed]+ [looks/appears/seems] + [years/days/months] +[younger/bigger/dirtier].
These sequences of POStags show how an abstract representation could bemore useful than a simple word representation.The funny category seems to be a relevant ele-ment to express irony.
However, its relevance mightbe supported by the kind of information profiled inthe positive set.
Considering the comic trend in thereviews posted by Amazon?s customers, it is likelythat many of the words belonging to this categoryappeared in such reviews.
For instance, in the fol-lowing example the words in italics represent funnyelements: ?I am an attractive guy.
Slender, weak,and I have never shaved in my 19 years, but sexy ashell, and I cannot tell you how many women haveflocked to me since my purchase?.
Regardless, itis important to stress that this category is equallydiscriminating for all sets, funny web comments in-cluded.Concerning the positive/negative profiling, it isnecessary to emphasize that, despite the greaternumber of negative words in the MSOL (more than15,000 words of difference; cf.
Section 4.4), thepositive elements are the most representative inthe ironic documents.
This fact corroborates theassumption about the use of positive informationin order to express an underlying negative mean-ing: ?The coolPOS, refreshingPOS tastePOS of themilkPOS washed away my painNEG and its kosherPOSsourcePOS of calciumPOS wash away my fearNEG?.Regarding the affective category, its relevance isnot as important as we have a-priori considered, de-spite it is one of the categories used to discriminatethe SLA subset: ?Man, that was weird .
.
.
I think isfunny, because there?s a good overlap?.
However,if we take into account the whole accuracy for thissubset, then we can conclude that its relevance is mi-nor.
Nonetheless, we still consider that the affectiveinformation is a valuable factor which must be takeninto account in order to provide rich knowledge re-lated to subjective layers of linguistic representation.The role played by the pleasantness category onthe classifications is significant.
Despite the cate-gory is not the most discriminating, its effectivenessfor increasing the classification accuracy is remark-able.
For instance, consider the following ironic sen-tence: ?I became the man I always dreamed Icould be all those nights staying up late watchingwrestling?, where most of its constituents are wordswhose pleasantness score is ?
2.5; i.e.
these words(in italics) should communicate information relatedto favorable pleasant contexts.6 Conclusions and Future WorkIrony is one of the most subjective phenomena re-lated to linguistic analysis.
Its automatic processingis a real challenge, not only from a computationalperspective but from a linguistic one as well.
In thiswork we have suggested a model of six categorieswhich attempts to describe salient characteristics ofirony.
They intend to symbolize low and high levelproperties of irony on the basis of formal linguis-tic elements.
This model was assessed by creatinga freely available data set with ironic reviews.
Theresults achieved with three different classifiers aresatisfactory, both in terms of classification accuracy,as well as precision, recall, and F-measure.
Furtherwork consists of improving the quality of every cat-egory, as well as of identifying new ones in order tocome up with an improved model capable to detectbetter ironic patterns in different kinds of texts.AcknowledgmentsThe National Council for Science and Technol-ogy (CONACyT - Me?xico) has funded the researchof the first author.
This work was carried outin the framework of the MICINN Text-Enterprise(TIN2009-13391-C04-03) research project and theMicrocluster VLC/Campus (International Campusof Excellence) on Multimodal Intelligent Systems.123ReferencesJ.
Atserias, B. Casas, E. Comelles, M. Gonza?lez,L.
Padro?, and M Padro?.
2006.
Freeling 1.3: Syntac-tic and semantic services in an open-source nlp library.In Proceedings of the 5th International Conference onLanguage Resources and Evaluation, pages 48?55.S.
Attardo.
2007.
Irony as relevant inappropriateness.In R. Gibbs and H. Colston, editors, Irony in Lan-guage and Thought, pages 135?174.
Taylor and Fran-cis Group.L.
Bentivogli, P. Forner, B. Magnini, and E. Pianta.
2004.Revising the wordnet domains hierarchy: semantics,coverage and balancing.
In Gilles Se?rasset, editor,Multilingual Linguistic Resources, pages 94?101.T.
Brants and A. Franz.
2006.
Web 1t 5-gram corpusversion 1.C.
Burfoot and T. Baldwin.
2009.
Automatic satire de-tection: Are you having a laugh?
In ACL-IJCNLP?09: Proceedings of the ACL-IJCNLP 2009 Confer-ence Short Papers, pages 161?164.P.
Carvalho, L. Sarmento, M. Silva, and E. de Oliveira.2009.
Clues for detecting irony in user-generated con-tents: oh...!!
it?s ?so easy?
;-).
In TSA ?09: Proceed-ing of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 53?56.H.
Colston and R. Gibbs.
2007.
A brief history of irony.In R. Gibbs and H. Colston, editors, Irony in Languageand Thought, pages 3?24.
Taylor and Francis Group.H.
Colston.
2007.
On necessary conditions for verbalirony comprehension.
In R. Gibbs and H. Colston, ed-itors, Irony in Language and Thought, pages 97?134.Taylor and Francis Group.I.
Councill, R. McDonald, and L. Velikovich.
2010.What?s great and what?s not: learning to classify thescope of negation for improved sentiment analysis.
InProceedings of the Workshop on Negation and Specu-lation in Natural Language Processing, pages 51?59,July.C.
Curco?.
2007.
Irony: Negation, echo, and metarepre-sentation.
In R. Gibbs and H. Colston, editors, Ironyin Language and Thought, pages 269?296.
Taylor andFrancis Group.D.
Davidov, O. Tsur, and A. Rappoport.
2010.
Semi-supervised recognition of sarcastic sentences in Twit-ter and Amazon.
In Proceeding of the 23rd interna-tional conference on Computational Linguistics, July.R.
Gibbs and H. Colston.
2007.
The future of irony stud-ies.
In R. Gibbs and H. Colston, editors, Irony in Lan-guage and Thought, pages 339?360.
Taylor and Fran-cis Group.R.
Gibbs.
2007.
Irony in talk among friends.
InR.
Gibbs and H. Colston, editors, Irony in Lan-guage and Thought, pages 339?360.
Taylor and Fran-cis Group.R.
Giora.
1995.
On irony and negation.
Discourse Pro-cesses, 19(2):239?264.H.
Grice.
1975.
Logic and conversation.
In Peter Coleand Jerry L. Morgan, editors, Syntax and semantics,volume 3, pages 41?58.
New York: Academic Press.X.
Lu?, L. Zhang, and J. Hu.
2004.
Statistical substringreduction in linear time.
In Proceedings of IJCNLP-04, HaiNan island.J.
Lucariello.
2007.
Situational irony: A concept ofevents gone away.
In R. Gibbs and H. Colston, edi-tors, Irony in Language and Thought, pages 467?498.Taylor and Francis Group.R.
Mihalcea and C. Strapparava.
2006.
Learning toLaugh (Automatically): Computational Models forHumor Recognition.
Journal of Computational Intel-ligence, 22(2):126?142.G.
Miller.
1995.
Wordnet: A lexical database for english.Communications of the ACM, 38(11):39?41.A.
Reyes, P. Rosso, and D. Buscaldi.
2009.
Humor in theblogosphere: First clues for a verbal humor taxonomy.Journal of Intelligent Systems, 18(4):311?331.M.
Saif, D. Cody, and D. Bonnie.
2009.
Generat-ing high-coverage semantic orientation lexicons fromovertly marked words and a thesaurus.
In Proceedingsof the 2009 Conference on EMNLP, pages 599?608,Morristown, NJ, USA.
Association for ComputationalLinguistics.C.
Strapparava and A. Valitutti.
2004.
WordNet-affect:an affective extension of WordNet.
In Proceedingsof the 4th International Conference on Language Re-sources and Evaluation, volume 4, pages 1083?1086.O.
Tsur, D. Davidov, and A. Rappoport.
2010.
{ICWSM} ?
a great catchy name: Semi-supervisedrecognition of sarcastic sentences in online product re-views.
In William W. Cohen and Samuel Gosling,editors, Proceedings of the Fourth International AAAIConference on Weblogs and Social Media, pages 162?169, Washington, D.C., 23-26 May.
The AAAI Press.A.
Utsumi.
1996.
A unified theory of irony and its com-putational formalization.
In Proceedings of the 16thconference on Computational Linguistics, pages 962?967, Morristown, NJ, USA.
Association for Computa-tional Linguistics.T.
Veale and Y. Hao.
2009.
Support structures for lin-guistic creativity: A computational analysis of creativeirony in similes.
In Proceedings of CogSci 2009, the31st Annual Meeting of the Cognitive Science Society,pages 1376?1381.C.
Whissell.
1989.
The dictionary of affect in language.Emotion: Theory, Research, and Experience, 4:113?131.D.
Wilson and D. Sperber.
2007.
On verbal irony.
InR.
Gibbs and H. Colston, editors, Irony in Languageand Thought, pages 35?56.
Taylor and Francis Group.124
