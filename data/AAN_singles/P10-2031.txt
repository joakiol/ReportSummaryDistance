Proceedings of the ACL 2010 Conference Short Papers, pages 168?172,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsImproving Chinese Semantic Role Labeling with Rich Syntactic FeaturesWeiwei Sun?Department of Computational Linguistics, Saarland UniversityGerman Research Center for Artificial Intelligence (DFKI)D-66123, Saarbru?cken, Germanywsun@coli.uni-saarland.deAbstractDeveloping features has been shown cru-cial to advancing the state-of-the-art in Se-mantic Role Labeling (SRL).
To improveChinese SRL, we propose a set of ad-ditional features, some of which are de-signed to better capture structural infor-mation.
Our system achieves 93.49 F-measure, a significant improvement overthe best reported performance 92.0.
Weare further concerned with the effectof parsing in Chinese SRL.
We empiri-cally analyze the two-fold effect, groupingwords into constituents and providing syn-tactic information.
We also give some pre-liminary linguistic explanations.1 IntroductionPrevious work on Chinese Semantic Role La-beling (SRL) mainly focused on how to imple-ment SRL methods which are successful on En-glish.
Similar to English, parsing is a standardpre-processing for Chinese SRL.
Many featuresare extracted to represent constituents in the inputparses (Sun and Jurafsky, 2004; Xue, 2008; Dingand Chang, 2008).
By using these features, se-mantic classifiers are trained to predict whether aconstituent fills a semantic role.
Developing fea-tures that capture the right kind of information en-coded in the input parses has been shown crucialto advancing the state-of-the-art.
Though therehas been some work on feature design in ChineseSRL, information encoded in the syntactic trees isnot fully exploited and requires more research ef-fort.
In this paper, we propose a set of additional?The work was partially completed while this author wasat Peking University.features, some of which are designed to better cap-ture structural information of sub-trees in a givenparse.
With help of these new features, our sys-tem achieves 93.49 F-measure with hand-craftedparses.
Comparison with the best reported results,92.0 (Xue, 2008), shows that these features yield asignificant improvement of the state-of-the-art.We further analyze the effect of syntactic pars-ing in Chinese SRL.
The main effect of parsingin SRL is two-fold.
First, grouping words intoconstituents, parsing helps to find argument candi-dates.
Second, parsers provide semantic classifiersplenty of syntactic information, not to only recog-nize arguments from all candidate constituents butalso to classify their detailed semantic types.
Weempirically analyze each effect in turn.
We alsogive some preliminary linguistic explanations forthe phenomena.2 Chinese SRLThe Chinese PropBank (CPB) is a semantic anno-tation for the syntactic trees of the Chinese Tree-Bank (CTB).
The arguments of a predicate are la-beled with a contiguous sequence of integers, inthe form of AN (N is a natural number); the ad-juncts are annotated as such with the label AMfollowed by a secondary tag that represents the se-mantic classification of the adjunct.
The assign-ment of semantic roles is illustrated in Figure 1,where the predicate is the verb ??
?/investigate?.E.g., the NP ????
?/the cause of the accident?is labeled as A1, meaning that it is the Patient.In previous research, SRL methods that are suc-cessful on English are adopted to resolve ChineseSRL (Sun and Jurafsky, 2004; Xue, 2008; Dingand Chang, 2008, 2009; Sun et al, 2009; Sun,2010).
Xue (2008) produced complete and sys-tematic research on full parsing based methods.168IPbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbA0 VPddddddddddddddddddddddiiiiiiiiiiiiNP AM-TMP AM-MNR VPZZZZZZZZZZZZZZZZZZZZZZNN ADVP ADVP Rel A1?
?policeAD AD VV NPiiiiiiiiiiii??now??thoroughly?
?investigateNN NN??accident?
?causeFigure 1: An example sentence: The police arethoroughly investigating the cause of the accident.Their method divided SRL into three sub-tasks: 1)pruning with a heuristic rule, 2) Argument Identi-fication (AI) to recognize arguments, and 3) Se-mantic Role Classification (SRC) to predict se-mantic types.
The main two sub-tasks, AI andSRC, are formulated as two classification prob-lems.
Ding and Chang (2008) divided SRC intotwo sub-tasks in sequence: Each argument shouldfirst be determined whether it is a core argument oran adjunct, and then be classified into fine-grainedcategories.
However, delicately designed featuresare more important and our experiments suggestthat by using rich features, a better SRC solvercan be directly trained without using hierarchicalarchitecture.
There are also some attempts at re-laxing the necessity of using full syntactic parses,and semantic chunking methods have been intro-duced by (Sun et al, 2009; Sun, 2010; Ding andChang, 2009).2.1 Our SystemWe implement a three-stage (i.e.
pruning, AI andSRC) SRL system.
In the pruning step, our sys-tem keeps all constituents (except punctuations)that c-command1 current predicate in focus as ar-gument candidates.
In the AI step, a lot of syntac-tic features are extracted to distinguish argumentand non-argument.
In other words, a binary classi-fier is trained to classify each argument candidateas either an argument or not.
Finally, a multi-classclassifier is trained to label each argument recog-nized in the former stage with a specific semanticrole label.
In both AI and SRC, the main job is toselect strong syntactic features.1See (Sun et al, 2008) for detailed definition.3 FeaturesA majority of features used in our system are acombination of features described in (Xue, 2008;Ding and Chang, 2008) as well as the word for-mation and coarse frame features introduced in(Sun et al, 2009), the c-command thread fea-tures proposed in (Sun et al, 2008).
We givea brief description of features used in previouswork, but explain new features in details.
Formore information, readers can refer to relevantpapers and our source codes2 that are well com-mented.
To conveniently illustrate, we denotea candidate constituent ck with a fixed contextwi?1[ckwi...wh...wj ]wj+1, where wh is the headword of ck, and denote predicate in focus witha context wv?2wv?1wvwv+1wv+2, where wv is thepredicate in focus.3.1 Baseline FeaturesThe following features are introduced in previousChinese SRL systems.
We use them as baseline.Word content of wv, wh, wi, wj and wi+wj ;POS tag of wv, wh.
subcategorization frame, verbclass of wv; position, phrase type ck, path from ckto wv (from (Xue, 2008; Ding and Chang, 2008))First character, last character and word lengthof wv, first character+length, last character+wordlength, first character+position, last charac-ter+position, coarse frame, frame+wv, frame+leftcharacter, frame+verb class, frame+ck (from (Sunet al, 2009)).Head word POS, head word of PP phrases, cat-egory of ck?s lift and right siblings, CFG rewriterule that expands ck and ck?s parent (from (Dingand Chang, 2008)).3.2 New Word FeaturesWe introduce some new features which can beextracted without syntactic structure.
We denotethem as word features.
They include:Word content of wv?1, wv+1, wi?1 and wj+1;POS tag of wv?1, wv+1, wv?2, wv+2, wi?1, wi, wj ,wj+1, wi+2 and wj?2.Length of ck: how many words are there in ck.Word before ?LC?
: If the POS of wj is ?LC?
(localizer), we use wj?1 and its POS tag as twonew features.NT: Does ck contain a word with POS ?NT?
(temporal noun)?2Available at http://code.google.com/p/csrler/.169Combination features: wi?s POS+wj?s POS,wv+Position3.3 New Syntactic FeaturesTaking complex syntax trees as inputs, the clas-sifiers should characterize their structural proper-ties.
We put forward a number of new features toencode the structural information.Category of ck?s parent; head word and POS ofhead word of parent, left sibling and right siblingof ck.Lexicalized Rewrite rules: Conjuction ofrewrite rule and head word of its correspondingRHS.
These features of candidate (lrw-c) and itsparent (lrw-p) are used.
For example, this lrw-c feature of the NP ??????
in Figure 1 isNP ?
NN +NN (??
).Partial Path: Path from the ck or wv to the low-est common ancestor of ck and wv.
One path fea-ture, hence, is divided into left path and right path.Clustered Path: We use the manually createdclusters (see (Sun and Sui, 2009)) of categories ofall nodes in the path (cpath) and right path.C-commander thread between ck and wv (cct):(proposed by (Sun et al, 2008)).
For example, thisfeature of the NP ????
in Figure 1 is NP +ADV P +ADV P + V V .Head Trace: The sequential container of thehead down upon the phrase (from (Sun and Sui,2009)).
We design two kinds of traces (htr-p, htr-w): one uses POS of the head word; the other usesthe head word word itself.
E.g., the head word of????
is ????
therefore these feature of thisNP are NP?NN and NP??
?.Combination features: verb class+ck, wh+wv,wh+Position, wh+wv+Position, path+wv,wh+right path, wv+left path, frame+wv+wh,and wv+cct.4 Experiments and Analysis4.1 Experimental SettingTo facilitate comparison with previous work, weuse CPB 1.0 and CTB 5.0, the same data set-ting with (Xue, 2008).
The data is divided intothree parts: files from 081 to 899 are used astraining set; files from 041 to 080 as develop-ment set; files from 001 to 040, and 900 to 931as test set.
Nearly all previous research on con-stituency based SRL evaluation use this setting,also including (Ding and Chang, 2008, 2009; Sunet al, 2009; Sun, 2010).
All parsing and SRL ex-periments use this data setting.
To resolve clas-sification problems, we use a linear SVM classi-fier SVMlin3, along with One-Vs-All approach formulti-class classification.
To evaluate SRL withautomatic parsing, we use a state-of-the-art parser,Bikel parser4 (Bikel, 2004).
We use gold segmen-tation and POS as input to the Bikel parser anduse it parsing results as input to our SRL system.The overall LP/LR/F performance of Bikel parseris 79.98%/82.95%/81.43.4.2 Overall PerformanceTable 1 summarizes precision, recall and F-measure of AI, SRC and the whole task (AI+SRC)of our system respectively.
The forth line isthe best published SRC performance reported in(Ding and Chang, 2008), and the sixth line is thebest SRL performance reported in (Xue, 2008).Other lines show the performance of our system.These results indicate a significant improvementover previous systems due to the new features.Test P(%) R(%) F/AAI 98.56 97.91 98.24SRC - - - - 95.04(Ding and Chang, 2008) - - - - 94.68AI + SRC 93.80 93.18 93.49(Xue, 2008) 93.0 91.0 92.0Table 1: SRL performance on the test data withgold standard parses.4.3 Two-fold Effect of Parsing in SRLThe effect of parsing in SRL is two-fold.
On theone hand, SRL systems should group words as ar-gument candidates, which are also constituents ina given sentence.
Full parsing provides bound-ary information of all constituents.
As argumentsshould c-command the predicate, a full parser canfurther prune a majority of useless constituents.
Inother words, parsing can effectively supply SRLwith argument candidates.
Unfortunately, it isvery hard to rightly produce full parses for Chi-nese text.
On the other hand, given a constituent,SRL systems should identify whether it is an argu-ment and further predict detailed semantic types if3http://people.cs.uchicago.edu/?vikass/svmlin.html4http://www.cis.upenn.edu/?dbikel/software.html170Task Parser Bracket Feat P(%) R(%) F/AAI - - Gold W 82.44 86.78 84.55CTB Gold W+S 98.69 98.11 98.40Bikel Bikel W+S 77.54 71.62 74.46SRC - - Gold W - - - - 93.93CTB Gold W+S - - - - 95.80Bikel Gold W+S - - - - 92.62Table 2: Classification perfromance on develop-ment data.
In the Feat column, W means wordfeatures; W+S means word and syntactic feautres.it is an argument.
For the two classification prob-lems, parsing can provide complex syntactic infor-mation such as path features.4.3.1 The Effect of Parsing in AIIn AI, full parsing is very important for bothgrouping words and classification.
Table 2 sum-marizes relative experimental results.
Line 2 is theAI performance when gold candidate boundariesand word features are used; Line 3 is the perfor-mance with additional syntactic features.
Line 4shows the performance by using automatic parsesgenerated by Bikel parser.
We can see that: 1)word features only cannot train good classifiers toidentify arguments; 2) it is very easy to recognizearguments with good enough syntactic parses; 3)there is a severe performance decline when auto-matic parses are used.
The third observation is asimilar conclusion in English SRL.
However thisproblem in Chinese is much more serious due tothe state-of-the-art of Chinese parsing.Information theoretic criteria are popular cri-teria in variable selection (Guyon and Elisse-eff, 2003).
This paper uses empirical mutualinformation between each variable and the tar-get, I(X,Y ) =?x?X,y?Y p(x, y) logp(x,y)p(x)p(y) , toroughly rank the importance of features.
Table 3shows the ten most useful features in AI.
We cansee that the most important features all based onfull parsing information.
Nine of these top 10 use-ful features are our new features.Rank Feature Rank Feature1 wv cct 2 ?
wh+wv+Position3 htr-w 4 htr-p5 path 6 ?
wh+wv7 cpath 8 cct9 path+wv 10 lrw-pTable 3: Top 10 useful features for AI.
?
meansword features.4.3.2 The Effect of Parsing in SRCThe second block in Table 2 summarizes the SRCperformance with gold argument boundaries.
Line5 is the accuracy when word features are used;Line 6 is the accuracy when additional syntacticfeatures are added; The last row is the accuracywhen syntactic features used are extracted fromautomatic parses (Bikel+Gold).
We can see thatdifferent from AI, word features only can trainreasonable good semantic classifiers.
The com-parison between Line 5 and 7 suggests that withparsing errors, automatic parsed syntactic featurescause noise to the semantic role classifiers.4.4 Why Word Features Are Effective forSRC?Rank Feature Rank Feature1 ?frame+wh+wv 2 ?wh+wv+position3 ?wh+wv 4 wv+cct5 lrw-p 6 ?wi+wj7 lrw-c 8 ?wh+Postion9 ?frame+wv 10 htr-pTable 4: Top 10 useful features for SRC.Table 4 shows the ten most useful features inSRC.
We can see that two of these ten featuresare word features (denoted by ?).
Namely, wordfeatures play a more important role in SRC thanin AI.
Though the other eight features are basedon full parsing, four of them (denoted by ?)
usethe head word which can be well approximatedby word features, according to some language spe-cific properties.
The head rules described in (Sunand Jurafsky, 2004) are very popular in Chineseparsing research, such as in (Duan et al, 2007;Zhang and Clark, 2008).
From these head rules,we can see that head words of most phrases inChinese are located at the first or the last position.We implement these rules on Chinese Tree Bankand find that 84.12% 5 nodes realize their heads aseither their first or last word.
Head position sug-gests that boundary words are good approximationof head word features.
If head words have goodapproximation word features, then it is not strangethat the four features denoted by ?
can be effec-tively represented by word features.
Similar withfeature effect in AI, most of most useful featuresin SRC are our new features.5This statistics excludes all empty categories in CTB.1715 ConclusionThis paper proposes an additional set of featuresto improve Chinese SRL.
These new features yielda significant improvement over the best publishedperformance.
We further analyze the effect ofparsing in Chinese SRL, and linguistically explainsome phenomena.
We found that (1) full syntacticinformation playes an essential role only in AI andthat (2) due to the head word position distribution,SRC is easy to resolve in Chinese SRL.AcknowledgmentsThe author is funded both by German AcademicExchange Service (DAAD) and German ResearchCenter for Artificial Intelligence (DFKI).The author would like to thank the anonymousreviewers for their helpful comments.ReferencesDaniel M. Bikel.
2004.
A distributional analysisof a lexicalized statistical parsing model.
InDekang Lin and Dekai Wu, editors, Proceed-ings of EMNLP 2004, pages 182?189.
Associa-tion for Computational Linguistics, Barcelona,Spain.Weiwei Ding and Baobao Chang.
2008.
Improv-ing Chinese semantic role classification with hi-erarchical feature selection strategy.
In Pro-ceedings of the EMNLP 2008, pages 324?333.
Association for Computational Linguis-tics, Honolulu, Hawaii.Weiwei Ding and Baobao Chang.
2009.
Fast se-mantic role labeling for Chinese based on se-mantic chunking.
In ICCPOL ?09: Proceed-ings of the 22nd International Conference onComputer Processing of Oriental Languages.Language Technology for the Knowledge-based Economy, pages 79?90.
Springer-Verlag,Berlin, Heidelberg.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.Probabilistic models for action-based Chinesedependency parsing.
In ECML ?07: Pro-ceedings of the 18th European conference onMachine Learning, pages 559?566.
Springer-Verlag, Berlin, Heidelberg.Isabelle Guyon and Andre?
Elisseeff.
2003.
Anintroduction to variable and feature selec-tion.
Journal of Machine Learning Research,3:1157?1182.Honglin Sun and Daniel Jurafsky.
2004.
Shallowsemantc parsing of Chinese.
In Daniel MarcuSusan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings.Weiwei Sun.
2010.
Semantics-driven shallowparsing for Chinese semantic role labeling.
InProceedings of the ACL 2010.Weiwei Sun and Zhifang Sui.
2009.
Chinese func-tion tag labeling.
In Proceedings of the 23rdPacific Asia Conference on Language, Informa-tion and Computation.
Hong Kong.Weiwei Sun, Zhifang Sui, and Haifeng Wang.2008.
Prediction of maximal projection for se-mantic role labeling.
In Proceedings of the22nd International Conference on Computa-tional Linguistics.Weiwei Sun, Zhifang Sui, Meng Wang, and XinWang.
2009.
Chinese semantic role labelingwith shallow parsing.
In Proceedings of the2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 1475?1483.Association for Computational Linguistics, Sin-gapore.Nianwen Xue.
2008.
Labeling Chinese predi-cates with semantic roles.
Comput.
Linguist.,34(2):225?255.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-based and transition-based dependency parsing.In Proceedings of the 2008 Conference on Em-pirical Methods in Natural Language Process-ing, pages 562?571.
Association for Computa-tional Linguistics, Honolulu, Hawaii.172
