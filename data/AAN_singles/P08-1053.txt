Proceedings of ACL-08: HLT, pages 461?469,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCombining Speech Retrieval Results with Generalized Additive ModelsJ.
Scott Olsson?
and Douglas W. Oard?UMIACS Laboratory for Computational Linguistics and Information ProcessingUniversity of Maryland, College Park, MD 20742Human Language Technology Center of ExcellenceJohn Hopkins University, Baltimore, MD 21211olsson@math.umd.edu, oard@umd.eduAbstractRapid and inexpensive techniques for auto-matic transcription of speech have the po-tential to dramatically expand the types ofcontent to which information retrieval tech-niques can be productively applied, but lim-itations in accuracy and robustness must beovercome before that promise can be fullyrealized.
Combining retrieval results fromsystems built on various errorful representa-tions of the same collection offers some po-tential to address these challenges.
This pa-per explores that potential by applying Gener-alized Additive Models to optimize the combi-nation of ranked retrieval results obtained us-ing transcripts produced automatically for thesame spoken content by substantially differ-ent recognition systems.
Topic-averaged re-trieval effectiveness better than any previouslyreported for the same collection was obtained,and even larger gains are apparent when usingan alternative measure emphasizing results onthe most difficult topics.1 IntroductionSpeech retrieval, like other tasks that require trans-forming the representation of language, suffers fromboth random and systematic errors that are intro-duced by the speech-to-text transducer.
Limita-tions in signal processing, acoustic modeling, pro-nunciation, vocabulary, and language modeling canbe accommodated in several ways, each of whichmake different trade-offs and thus induce different?
Dept.
of Mathematics/AMSC, UMD?
College of Information Studies, UMDerror characteristics.
Moreover, different applica-tions produce different types of challenges and dif-ferent opportunities.
As a result, optimizing a sin-gle recognition system for all transcription tasks iswell beyond the reach of present technology, andeven systems that are apparently similar on averagecan make different mistakes on different sources.
Anatural response to this challenge is to combine re-trieval results from multiple systems, each imper-fect, to achieve reasonably robust behavior over abroader range of tasks.
In this paper, we comparealternative ways of combining these ranked lists.Note, we do not assume access to the internal work-ings of the recognition systems, or even to the tran-scripts produced by those systems.System combination has a long history in infor-mation retrieval.
Most often, the goal is to combineresults from systems that search different content(?collection fusion?)
or to combine results from dif-ferent systems on the same content (?data fusion?
).When working with multiple transcriptions of thesame content, we are again presented with new op-portunities.
In this paper we compare some wellknown techniques for combination of retrieval re-sults with a new evidence combination techniquebased on a general framework known as Gener-alized Additive Models (GAMs).
We show thatthis new technique significantly outperforms sev-eral well known information retrieval fusion tech-niques, and we present evidence that it is the abilityof GAMs to combine inputs non-linearly that at leastpartly explains our improvements.The remainder of this paper is organized as fol-lows.
We first review prior work on evidence com-461bination in information retrieval in Section 2, andthen introduce Generalized Additive Models in Sec-tion 3.
Section 4 describes the design of our ex-periments with a 589 hour collection of conversa-tional speech for which information retrieval queriesand relevance judgments are available.
Section 5presents the results of our experiments, and we con-clude in Section 6 with a brief discussion of implica-tions of our results and the potential for future workon this important problem.2 Previous WorkOne approach for combining ranked retrieval resultsis to simply linearly combine the multiple systemscores for each topic and document.
This approachhas been extensively applied in the literature (Bartellet al, 1994; Callan et al, 1995; Powell et al, 2000;Vogt and Cottrell, 1999), with varying degrees ofsuccess, owing in part to the potential difficulty ofnormalizing scores across retrieval systems.
In thisstudy, we partially abstract away from this poten-tial difficulty by using the same retrieval system onboth representations of the collection documents (sothat we don?t expect score distributions to be signif-icantly different for the combination inputs).Of course, many fusion techniques using more ad-vanced score normalization methods have been pro-posed.
Shaw and Fox (1994) proposed a numberof such techniques, perhaps the most successful ofwhich is known as CombMNZ.
CombMNZ has beenshown to achieve strong performance and has beenused in many subsequent studies (Lee, 1997; Mon-tague and Aslam, 2002; Beitzel et al, 2004; Lillis etal., 2006).
In this study, we also use CombMNZas a baseline for comparison, and following Lil-lis et al (2006) and Lee (1997), compute it in thefollowing way.
First, we normalize each score sias norm(si) =si?min(s)max(s)?min(s) , where max(s) andmin(s) are the maximum and minimum scores seenin the input result list.
After normalization, theCombMNZ score for a document d is computed asCombMNZd =L?`Ns,d ?
|Nd > 0|.Here, L is the number of ranked lists to be com-bined, N`,d is the normalized score of document din ranked list `, and |Nd > 0| is the number of non-zero normalized scores given to d by any result set.Manmatha et al (2001) showed that retrievalscores from IR systems could be modeled using aNormal distribution for relevant documents and ex-ponential distribution for non-relevant documents.However, in their study, fusion results using thesecomparatively complex normalization approachesachieved performance no better than the much sim-pler CombMNZ.A simple rank-based fusion technique is inter-leaving (Voorhees et al, 1994).
In this approach,the highest ranked document from each list is takenin turn (ignoring duplicates) and placed at the top ofthe new, combined list.Many probabilistic combination approaches havealso been developed, a recent example being Lilliset al (2006).
Perhaps the most closely related pro-posal, using logistic regression, was made first bySavoy et al (1988).
Logistic regression is one exam-ple from the broad class of models which GAMs en-compass.
Unlike GAMs in their full generality how-ever, logistic regression imposes a comparativelyhigh degree of linearity in the model structure.2.1 Combining speech retrieval resultsPrevious work on single-collection result fusion hasnaturally focused on combining results from multi-ple retrieval systems.
In this case, the potential forperformance improvements depends critically on theuniqueness of the different input systems being com-bined.
Accordingly, small variations in the samesystem often do not combine to produce results bet-ter than the best of their inputs (Beitzel et al, 2004).Errorful document collections such as conversa-tional speech introduce new difficulties and oppor-tunities for data fusion.
This is so, in particular,because even the same system can produce drasti-cally different retrieval results when multiple repre-sentations of the documents (e.g., multiple transcripthypotheses) are available.
Consider, for example,Figure 1 which shows, for each term in each of ourtitle queries, the proportion of relevant documentscontaining that term in only one of our two tran-script hypotheses.
Critically, by plotting this propor-tion against the term?s inverse document frequency,we observe that the most discriminative query termsare often not available in both document represen-462123450.00.20.40.60.81.0Inverse Document FrequencyProportion of relevant docs with term in only one transcript sourceFigure 1: For each term in each query, the proportion ofrelevant documents containing the term vs. inverse doc-ument frequency.
For increasingly discriminative terms(higher idf ), we observe that the probability of only onetranscript containing the term increases dramatically.tations.
As these high-idf terms make large contri-butions to retrieval scores, this suggests that even anidentical retrieval system may return a large scoreusing one transcript hypothesis, and yet a very lowscore using another.
Accordingly, a linear combina-tion of scores is unlikely to be optimal.A second example illustrates the difficulty.
Sup-pose recognition system A can recognize a particu-lar high-idf query term, but system B never can.
Inthe extreme case, the term may simply be out of vo-cabulary, although this may occur for various otherreasons (e.g., poor language modeling or pronuncia-tion dictionaries).
Here again, a linear combinationof scores will fail, as will rank-based interleaving.In the latter case, we will alternate between taking aplausible document from systemA and an inevitablyworse result from the crippled system B.As a potential solution for these difficulties, weconsider the use of generalized additive models forretrieval fusion.3 Generalized Additive ModelsGeneralized Additive Models (GAMs) are a gen-eralization of Generalized Linear Models (GLMs),while GLMs are a generalization of the well knownlinear model.
In a GLM, the distribution of an ob-served random variable Yi is related to the linear pre-dictor ?i through a smooth monotonic link functiong,g(?i) = ?i = Xi?.Here, Xi is the ith row of the model matrix X (oneset of observations corresponding to one observedyi) and ?
is a vector of unknown parameters to belearned from the data.
If we constrain our link func-tion g to be the identity transformation, and assumeYi is Normal, then our GLM reduces to a simple lin-ear model.But GLMs are considerably more versatile thanlinear models.
First, rather than only the Normal dis-tribution, the response Yi is free to have any distribu-tion belonging to the exponential family of distribu-tions.
This family includes many useful distributionssuch as the Binomial, Normal, Gamma, and Poisson.Secondly, by allowing non-identity link functions g,some degree of non-linearity may be incorporated inthe model structure.A well known GLM in the NLP community is lo-gistic regression (which may alternatively be derivedas a maximum entropy classifier).
In logistic regres-sion, the response is assumed to be Binomial and thechosen link function is the logit transformation,g(?i) = logit(?i) = log(?i1?
?i).Generalized additive models allow for additionalmodel flexibility by allowing the linear predictor tonow also contain learned smooth functions fj of thecovariates xk.
For example,g(?i) = X?i ?
+ f1(x1i) + f2(x2i) + f3(x3i, x4i).As in a GLM, ?i ?
E(Yi) and Yi belongs to theexponential family.
Strictly parametric model com-ponents are still permitted, which we represent as arow of the model matrix X?i (with associated param-eters ?
).GAMs may be thought of as GLMs where oneor more covariate has been transformed by a basisexpansion, f(x) =?qj bj(x)?j .
Given a set of qbasis functions bj spanning a q-dimensional space463of smooth transformations, we are back to the lin-ear problem of learning coefficients ?j which ?opti-mally?
fit the data.
If we knew the appropriate trans-formation of our covariates (say the logarithm), wecould simply apply it ourselves.
GAMs allow us tolearn these transformations from the data, when weexpect some transformation to be useful but don?tknow it?s form a priori.
In practice, these smoothfunctions may be represented and the model pa-rameters may be learned in various ways.
In thiswork, we use the excellent open source packagemgcv (Wood, 2006), which uses penalized likeli-hood maximization to prevent arbitrarily ?wiggly?smooth functions (i.e., overfitting).
Smooths (in-cluding multidimensional smooths) are representedby thin plate regression splines (Wood, 2003).3.1 Combining speech retrieval results withGAMsThe chief difficulty introduced in combining rankedspeech retrieval results is the severe disagreement in-troduced by differing document hypotheses.
As wesaw in Figure 1, it is often the case that the most dis-criminative query terms occur in only one transcriptsource.3.1.1 GLM with factorsOur first new approach for handling differences intranscripts is an extension of the logistic regressionmodel previously used in data fusion work, (Savoyet al, 1988).
Specifically, we augment the modelwith the first-order interaction of scores x1x2 andthe factor ?i, so thatlogit{E(Ri)} = ?0+?i+x1?1+x2?2+x1x2?3,where the relevance Ri ?
Binomial.
A factor isessentially a learned intercept for different subsetsof the response.
In this case,?i =???
?BOTH if both representations matched qi?IBM only di,IBM matched qi?BBN only di,BBN matched qiwhere ?i corresponds to data row i, with associ-ated document representations di,source and queryqi.
The intuition is simply that we?d like our modelto have different biases for or against relevancebased on which transcript source retrieved the doc-ument.
This is a small-dimensional way of damp-ening the effects of significant disagreements in thedocument representations.3.1.2 GAM with multidimensional smoothIf a document?s score is large in both systems, weexpect it to have high probability of relevance.
How-ever, as a document?s score increases linearly in onesource, we have no reason to expect its probabilityof relevance to also increase linearly.
Moreover, be-cause the most discriminative terms are likely to befound in only one transcript source, even an absentscore for a document does not ensure a documentis not relevant.
It is clear then that the mappingfrom document scores to probability of relevance isin general a complex nonlinear surface.
The limiteddegree of nonlinear structure afforded to GLMs bynon-identity link functions is unlikely to sufficientlycapture this intuition.Instead, we can model this non-linearity using ageneralized additive model with multidimensionalsmooth f(xIBM , xBBN ), so thatlogit{E(Ri)} = ?0 + f(xIBM , xBBN ).Again, Ri ?
Binomial and ?0 is a learned inter-cept (which, alternatively, may be absorbed by thesmooth f ).Figure 2 shows the smoothing transformation flearned during our evaluation.
Note the small de-crease in predicted probability of relevance as theretrieval score from one system decreases, while theprobability curves upward again as the disagreementincreases.
This captures our intuition that systemsoften disagree strongly because discriminative termsare often not recognized in all transcript sources.We can think of the probability of relevance map-ping learned by the factor model of Section 3.1.1 asalso being a surface defined over the space of inputdocument scores.
That model, however, was con-strained to be linear.
It may be visualized as a col-lection of affine planes (with common normal vec-tors, but each shifted upwards by their factor level?sweight and the common intercept).4644 Experiments4.1 DatasetOur dataset is a collection of 272 oral history inter-views from the MALACH collection.
The task isto retrieve short speech segments which were man-ually designated as being topically coherent by pro-fessional indexers.
There are 8,104 such segments(corresponding to roughly 589 hours of conversa-tional speech) and 96 assessed topics.
We follow thetopic partition used for the 2007 evaluation by theCross Language Evaluation Forum?s cross-languagespeech retrieval track (Pecina et al, 2007).
Thisgives us 63 topics on which to train our combinationsystems and 33 topics for evaluation.4.2 Evaluation4.2.1 Geometric Mean Average PrecisionAverage precision (AP) is the average of the pre-cision values obtained after each document relevantto a particular query is retrieved.
To assess theeffectiveness of a system across multiple queries,a commonly used measure is mean average preci-sion (MAP).
Mean average precision is defined asthe arithmetic mean of per-topic average precision,MAP = 1n?n APn.
A consequence of the arith-metic mean is that, if a system improvement dou-bles AP for one topic from 0.02 to 0.04, while si-multaneously decreasing AP on another from 0.4 to0.38, the MAP will be unchanged.
If we prefer tohighlight performance differences on the lowest per-forming topics, a widely used alternative is the geo-metric mean of average precision (GMAP), first in-troduced in the TREC 2004 robust track (Voorhees,2006).GMAP = n?
?nAPnRobertson (2006) presents a justification and analy-sis of GMAP and notes that it may alternatively becomputed as an arithmetic mean of logs,GMAP = exp1n?nlog APn.4.2.2 Significance Testing for GMAPA standard way of measuring the significance ofsystem improvements in MAP is to compare aver-age precision (AP) on each of the evaluation queriesusing the Wilcoxon signed-rank test.
This test, whilenot requiring a particular distribution on the mea-surements, does assume that they belong to an in-terval scale.
Similarly, the arithmetic mean of MAPassumes AP has interval scale.
As Robertson (2006)has pointed out, it is in no sense clear that AP(prior to any transformation) satisfies this assump-tion.
This becomes an argument for GMAP, since itmay also be defined using an arithmetic mean of log-transformed average precisions.
That is to say, thelogarithm is simply one possible monotonic trans-formation which is arguably as good as any other,including the identify transform, in terms of whetherthe transformed value satisfies the interval assump-tion.
This log transform (and hence GMAP) is use-ful simply because it highlights improvements onthe most difficult queries.We apply the same reasoning to test for statisticalsignificance in GMAP improvements.
That is, wetest for significant improvements in GMAP by ap-plying the Wilcoxon signed rank test to the paired,transformed average precisions, log AP.
We handletied pairs and compute exact p-values using the Stre-itberg & Ro?hmel Shift-Algorithm (1990).
For topicswith AP = 0, we follow the Robust Track conven-tion and add  = 0.00001.
The authors are not awareof significance tests having been previously reportedon GMAP.4.3 Retrieval SystemWe use Okapi BM25 (Robertson et al, 1996) asour basic retrieval system, which defines a documentD?s retrieval score for query Q ass(D,Q) =n?i=1idf(qi)(k3+1)qfik3+qfi )f(qi, D)(k1 + 1)f(qi, D) + k1(1?
b+ b|D|avgdl ),where the inverse document frequency (idf ) is de-fined asidf(qi) = logN ?
n(qi) + 0.5n(qi) + 0.5,N is the size of the collection, n(qi) is the docu-ment frequency for term qi, qfi is the frequency ofterm qi in query Q, f(qi, D) is the term frequencyof query term qi in document D, |D| is the lengthof the matching document, and avgdl is the averagelength of a document in the collection.
We set the465BBNScoreIBM Scorelinear predictorFigure 2: The two dimensional smooth f(sIBM, sBBN)learned to predict relevance given input scores from IBMand BBN transcripts.parameters to k1 = 1, k3 = 1, b = .5, which gavegood results on a single transcript.4.4 Speech Recognition TranscriptsOur first set of speech recognition transcripts wasproduced by IBM for the MALACH project, andused for several years in the CLEF cross-languagespeech retrieval (CL-SR) track (Pecina et al, 2007).The IBM recognizer was built using a manuallyproduced pronunciation dictionary and 200 hoursof transcribed audio.
The resulting interview tran-scripts have a reported mean word error rate (WER)of approximately 25% on held out data, which wasobtained by priming the language model with meta-data available from pre-interview questionnaires.This represents significant improvements over IBMtranscripts used in earlier CL-SR evaluations, whichhad a best reported WER of 39.6% (Byrne et al,2004).
This system is reported to have run at ap-proximately 10 times real time.4.4.1 New Transcripts for MALACHWe were graciously permitted to use BBN Tech-nology?s speech recognition system to produce asecond set of ASR transcripts for our experiments(Prasad et al, 2005; Matsoukas et al, 2005).
We se-lected the one side of the audio having largest RMSamplitude for training and decoding.
This channelwas down-sampled to 8kHz and segmented using anavailable broadcast news segmenter.
Because we didnot have a pronunciation dictionary which coveredthe transcribed audio, we automatically generatedpronunciations for roughly 14k words using a rule-based transliterator and the CMU lexicon.
Usingthe same 200 hours of transcribed audio, we trainedacoustic models as described in (Prasad et al, 2005).We use a mixture of the training transcripts and var-ious newswire sources for our language model train-ing.
We did not attempt to prime the language modelfor particular interviewees or otherwise utilize anyinterview metadata.
For decoding, we ran a fast (ap-proximately 1 times real time) system, as describedin (Matsoukas et al, 2005).
Unfortunately, as we donot have the same development set used by IBM, adirect comparison of WER is not possible.
Testingon a small held out set of 4.3 hours, we observed oursystem had a WER of 32.4%.4.5 Combination MethodsFor baseline comparisons, we ran our evaluation oneach of the two transcript sources (IBM and our newtranscripts), the linear combination chosen to opti-mize MAP (LC-MAP), the linear combination cho-sen to optimize GMAP (LC-GMAP), interleaving(IL), and CombMNZ.
We denote our additive fac-tor model as Factor GLM, and our multidimensionalsmooth GAM model as MD-GAM.Linear combination parameters were chosen tooptimize performance on the training set, sweepingthe weight for each source at intervals of 0.01.
Forthe generalized additive models, we maximized thepenalized likelihood of the training examples underour model, as described in Section 3.5 ResultsTable 1 shows our complete set of results.
Thisincludes baseline scores from our new set oftranscripts, each of our baseline combination ap-proaches, and results from our proposed combina-tion models.
Although we are chiefly interested inimprovements on difficult topics (i.e., GMAP), wepresent MAP for comparison.
Results in bold in-dicate the largest mean value of the measure (ei-ther AP or log AP), while daggers (?)
indicate the466Type Model MAP GMAPT IBM 0.0531 (-.2) 0.0134 (-11.8)- BBN 0.0532 0.0152- LC-MAP 0.0564 (+6.0) 0.0158 (+3.9)- LC-GMAP 0.0587 (+10.3) 0.0154 (+1.3)- IL 0.0592 (+11.3) 0.0165 (+8.6)- CombMNZ 0.0550 (+3.4) 0.0150 (-1.3)- Factor GLM 0.0611 (+14.9)?
0.0161 (+5.9)- MD-GAM 0.0561 (+5.5)?
0.0180 (+18.4)?TD IBM 0.0415 (-15.1) 0.0173 (-9.9)- BBN 0.0489 0.0192- LC-MAP 0.0519 (+6.1)?
0.0201 (+4.7)?- LC-GMAP 0.0531 (+8.6)?
0.0200 (+4.2)- IL 0.0507 (+3.7) 0.0210 (+9.4)- CombMNZ 0.0495 (+1.2)?
0.0196 (+2.1)- Factor GLM 0.0526 (+7.6)?
0.0198 (+3.1)- MD-GAM 0.0529 (+8.2)?
0.0223 (+16.2)?Table 1: MAP and GMAP for each combination ap-proach, using the evaluation query set from the CLEF-2007 CL-SR (MALACH) collection.
Shown in paren-theses is the relative improvement in score over the bestsingle transcripts results (i.e., using our new set of tran-scripts).
The best (mean) score for each condition is inbold.combination is a statistically significant improve-ment (?
= 0.05) over our new transcript set (thatis, over the best single transcript result).
Tests forstatistically significant improvements in GMAP arecomputed using our paired log AP test, as discussedin Section 4.2.2.First, we note that the GAM model with multi-dimensional smooth gives the largest GMAP im-provement for both title and title-description runs.Secondly, it is the only combination approach ableto produce statistically significant relative improve-ments on both measures for both conditions.
ForGMAP, our measure of interest, these improve-ments are 18.4% and 16.2% respectively.One surprising observation from Table 1 is thatthe mean improvement in log AP for interleaving isfairly large and yet not statistically significant (it isin fact a larger mean improvement than several otherbaseline combination approaches which are signifi-cant improvements.
This may suggest that interleav-ing suffers from a large disparity between its bestand worst performance on the query set.0.001 0.002 0.005 0.010 0.020 0.050 0.100 0.2000.0010.0020.0050.0100.0200.050Term recall in IBM transcriptsTermrecall in BBN transcriptsimpact guiltattitudzionismpreviouassemblFigure 3: The proportion of relevant documents returnedin IBM and BBN transcripts for discriminative title words(title words occurring in less than .01 of the collection).Point size is proportional to the improvement in averageprecision using (1) the best linear combination chosen tooptimize GMAP (4) and (2) the combination using MD-GAM (?
).Figure 3 examines whether our improvementscome systematically from only one of the transcriptsources.
It shows the proportion of relevant docu-ments in each transcript source containing the mostdiscriminative title words (words occurring in lessthan .01 of the collection).
Each point representsone term for one topic.
The size of the point is pro-portional to the difference in AP observed on thattopic by using MD-GAM and by using LC-GMAP.If the difference is positive (MD-GAM wins), weplot ?, otherwise 4.
First, we observe that, whenit wins, MD-GAM tends to increase AP much morethan when LC-GMAP wins.
While there are manywins also for LC-GMAP, the effects of the largerMD-GAM improvements will dominate for many ofthe most difficult queries.
Secondly, there does notappear to be any evidence that one transcript sourcehas much higher term-recall than the other.5.1 Oracle linear combinationA chief advantage of our MD-GAM combinationmodel is that it is able to map input scores non-linearly onto a probability of document relevance.467Type Model GMAPT Oracle-LC-GMAP 0.0168- MD-GAM 0.0180 (+7.1)TD Oracle-LC-GMAP 0.0222- MD-GAM 0.0223 (+0.5)Table 2: GMAP results for an oracle experiment inwhich MD-GAM was fairly trained and LC-GMAP wasunfairly optimized on the test queries.To make an assessment of how much this capabil-ity helps the system, we performed an oracle exper-iment where we again constrained MD-GAM to befairly trained but allowed LC-GMAP to cheat andchoose the combination optimizing GMAP on thetest data.
Table 2 lists the results.
While the im-provement with MD-GAM is now not statisticallysignificant (primarily because of our small queryset), we found it still out-performed the oracle linearcombination.
For title-only queries, this improve-ment was surprisingly large at 7.1% relative.6 ConclusionWhile speech retrieval is one example of retrievalunder errorful document representations, other sim-ilar tasks may also benefit from these combinationmodels.
This includes the task of cross-language re-trieval, as well as the retrieval of documents obtainedby optical character recognition.Within speech retrieval, further work also remainsto be done.
For example, various other features arelikely to be useful in predicting optimal system com-bination.
These might include, for example, confi-dence scores, acoustic confusability, or other strongcues that one recognition system is unlikely to haveproperly recognized a query term.
We look forwardto investigating these possibilities in future work.The question of how much a system should ex-pose its internal workings (e.g., its document rep-resentations) to external systems is a long standingproblem in meta-search.
We?ve taken the rather nar-row view that systems might only expose the list ofscores they assigned to retrieved documents, a plau-sible scenario considering the many systems nowemerging which are effectively doing this already.Some examples include EveryZing,1 the MIT Lec-1http://www.everyzing.com/ture Browser,2 and Comcast?s video search.3 Thistrend is likely to continue as the underlying repre-sentations of the content are themselves becomingincreasingly complex (e.g., word and subword levellattices or confusion networks).
The cost of expos-ing such a vast quantity of such complex data rapidlybecomes difficult to justify.But if the various representations of the con-tent are available, there are almost certainly othercombination approaches worth investigating.
Somepossible approaches include simple linear combi-nations of the putative term frequencies, combina-tions of one best transcript hypotheses (e.g., us-ing ROVER (Fiscus, 1997)), or methods exploitingword-lattice information (Evermann and Woodland,2000).Our planet?s 6.6 billion people speak many morewords every day than even the largest Web searchengines presently index.
While much of this issurely not worth hearing again (or even once!
), someof it is surely precious beyond measure.
Separatingthe wheat from the chaff in this cacophony is the rai-son d?etre for information retrieval, and it is hard toconceive of an information retrieval challenge withgreater scope or greater potential to impact our soci-ety than improving our access to the spoken word.AcknowledgementsThe authors are grateful to BBN Technologies, whogenerously provided access to their speech recogni-tion system for this research.ReferencesBrian T. Bartell, Garrison W. Cottrell, and Richard K.Belew.
1994.
Automatic combination of multi-ple ranked retrieval systems.
In Proceedings of the17th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 173?181.Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury,David Grossman, Ophir Frieder, and Nazli Goharian.2004.
Fusion of effective retrieval strategies in thesame information retrieval system.
J.
Am.
Soc.
Inf.
Sci.Technol., 55(10):859?868.W.
Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic,D.W.
Oard, M. Picheny, J. Psutka, B. Ramabhadran,2http://web.sls.csail.mit.edu/lectures/3http://videosearch.comcast.net468D.
Soergel, T. Ward, and Wei-Jing Zhu.
2004.
Au-tomatic recognition of spontaneous speech for accessto multilingual oral history archives.
IEEE Transac-tions on Speech and Audio Processing, Special Issueon Spontaneous Speech Processing, 12(4):420?435,July.J.
P. Callan, Z. Lu, and W. Bruce Croft.
1995.
Search-ing Distributed Collections with Inference Networks .In E. A.
Fox, P. Ingwersen, and R. Fidel, editors, Pro-ceedings of the 18th Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 21?28, Seattle, Washington.ACM Press.G.
Evermann and P.C.
Woodland.
2000.
Posterior prob-ability decoding, confidence estimation and systemcombination.
In Proceedings of the Speech Transcrip-tion Workshop, May.Jonathan G. Fiscus.
1997.
A Post-Processing System toYield Reduced Word Error Rates: Recogniser OutputVoting Error Reduction (ROVER).
In Proceedings ofthe IEEE ASRU Workshop, pages 347?352.Jong-Hak Lee.
1997.
Analyses of multiple evidencecombination.
In SIGIR Forum, pages 267?276.David Lillis, Fergus Toolan, Rem Collier, and John Dun-nion.
2006.
Probfuse: a probabilistic approach to datafusion.
In SIGIR ?06: Proceedings of the 29th annualinternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 139?146,New York, NY, USA.
ACM.R.
Manmatha, T. Rath, and F. Feng.
2001.
Modelingscore distributions for combining the outputs of searchengines.
In SIGIR ?01: Proceedings of the 24th annualinternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 267?275,New York, NY, USA.
ACM.Spyros Matsoukas, Rohit Prasad, Srinivas Laxminarayan,Bing Xiang, Long Nguyen, and Richard Schwartz.2005.
The 2004 BBN 1xRT Recognition Systemsfor English Broadcast News and Conversational Tele-phone Speech.
In Interspeech 2005, pages 1641?1644.Mark Montague and Javed A. Aslam.
2002.
Condorcetfusion for improved retrieval.
In CIKM ?02: Proceed-ings of the eleventh international conference on Infor-mation and knowledge management, pages 538?548,New York, NY, USA.
ACM.Pavel Pecina, Petra Hoffmannova, Gareth J.F.
Jones, Jian-qiang Wang, and Douglas W. Oard.
2007.
Overviewof the CLEF-2007 Cross-Language Speech RetrievalTrack.
In Proceedings of the CLEF 2007 Workshopon Cross-Language Information Retrieval and Evalu-ation, September.Allison L. Powell, James C. French, James P. Callan,Margaret E. Connell, and Charles L. Viles.
2000.The impact of database selection on distributed search-ing.
In Research and Development in Information Re-trieval, pages 232?239.R.
Prasad, S. Matsoukas, C.L.
Kao, J. Ma, D.X.
Xu,T.
Colthurst, O. Kimball, R. Schwartz, J.L.
Gauvain,L.
Lamel, H. Schwenk, G. Adda, and F. Lefevre.2005.
The 2004 BBN/LIMSI 20xRT English Conver-sational Telephone Speech Recognition System.
In In-terspeech 2005.S.
Robertson, S. Walker, S. Jones, and M. Hancock-Beaulieu M. Gatford.
1996.
Okapi at TREC-3.
InText REtrieval Conference, pages 21?30.Stephen Robertson.
2006.
On GMAP: and other trans-formations.
In CIKM ?06: Proceedings of the 15thACM international conference on Information andknowledge management, pages 78?83, New York, NY,USA.
ACM.J.
Savoy, A.
Le Calve?, and D. Vrajitoru.
1988.
Report onthe TREC-5 experiment: Data fusion and collectionfusion.Joseph A. Shaw and Edward A.
Fox.
1994.
Combinationof multiple searches.
In Proceedings of the 2nd TextREtrieval Conference (TREC-2).Bernd Streitberg and Joachim Ro?hmel.
1990.
On teststhat are uniformly more powerful than the Wilcoxon-Mann-Whitney test.
Biometrics, 46(2):481?484.Christopher C. Vogt and Garrison W. Cottrell.
1999.
Fu-sion via a linear combination of scores.
InformationRetrieval, 1(3):151?173.Ellen M. Voorhees, Narendra Kumar Gupta, and BenJohnson-Laird.
1994.
The collection fusion problem.In D. K. Harman, editor, The Third Text REtrieval Con-ference (TREC-3), pages 500?225.
National Instituteof Standards and Technology.Ellen M. Voorhees.
2006.
Overview of the TREC 2005robust retrieval track.
In Ellem M. Voorhees and L.P.Buckland, editors, The Fourteenth Text REtrieval Con-ference, (TREC 2005), Gaithersburg, MD: NIST.Simon N. Wood.
2003.
Thin plate regression splines.Journal Of The Royal Statistical Society Series B,65(1):95?114.Simon Wood.
2006.
Generalized Additive Models: AnIntroduction with R. Chapman and Hall/CRC.469
