Word Sense Disambiguation of AdjectivesUsing Probabilistic NetworksGera ld  Chao and Michae l  G. DyerCoin l )uter  Science Det )ar tment ,Un ivers i ty  of Cal i forn ia ,  Los AngelesLos Angeles,  Ca l i fo rn ia  90095gera ld@cs.uc la .edu,  dyer~cs .uc la .eduAbst rac tIn this paper, word sense dismnbiguation (WSD) ac-curacy achievable by a probabilistic lassifier, usingvery milfimal training sets, is investigated.
\Ve madethe assuml)tiou that there are no tagged corporaavailable and identified what information, needed byan accurate WSD system, can and cmmot be auto-matically obtained.
The lesson learned can then beused to locus on what knowledge needs malmal an-notation.
Our system, named Bayesian HierarchicalDisambiguator (BHD), uses the Internet, arguablytile largest corlms in existence, to address the st)arsedata problem, and uses WordNet's hierarchy tbr se-mantic contextual features.
In addition, Bayesiannetworks are automatically constructed to representknowledge learned from training sets by lnodelingthe selectional i)retbrence of adjectives.
These net-works are then applied to disaml)iguation by per-tbrming inferences on unseen adjective-noun pairs.We demonstrate that this system is able to disam-biguate adjectives in um'estricl;ed text at good initialaccuracy rates without tile need tbr tagged corpora.The learning and extensibility aspects of the modelare also discussed, showing how tagged corpora andadditional context can be incorporated easily to im-prove accm'acy, and how this technique can be usedto disambiguate other types of word pairs, such asverb-noun and adverb-verb pairs.1 In t roduct ionWord sense disambiguation (WSD) remains an openprobleln in Natural Language Processing (NLP).
Be-ing able to identify tile correct sense of an mnbigu-ous word is ilnportant for many NLP tasks, such asmachine translation, information retrieval, and dis-course analysis.
The WSD problem is exacerbatedby the large number of senses of colmnonly usedwords and by the difficulty in determining relevantcontextual features most suitable to the task.
Tileabsence of semantically tagged corpora makes prob-abilistic techniques, shown to be very effective byspeech recognition and syntactic tagging research,difficult to employ due to the sparse data problem.Early NLP systeins limited their domain and re-quired manual knowledge ngineering.
More recentworks take advantage of machine readable dictio-naries such as WordNet (Miller, 1990) and Roget'sOnline Thesaurus.
Statistical techniques, both su-pervised learning from tagged corpora (Yarowsky,1992), (Ng and Lee, 1.996), and unsupervised learn-ing (Yarowsky, 1995), (Resnik, 1997), have been in-vestigated.
There are also hybrid inodcls that in-corporate both statistical and symbolic knowledge(Wiebe et al, 1998), (Agirre and I{igau, 1996).Supervised models have shown promising results,but the lack of sense tagged corpora often requiresthe need tbr laboriously tagging trailfing sets man-ually.
Depending on the technique, unsupervisedmodels can result in ill-defined senses.
Many havenot been evaluated with large vocabularies or flfllsets of senses.
Hybrid models, using various heuris-tics, have demonstrated good accuracy but are ditfi-cult to compare clue to variations in the evahlationprocedures, as discussed in Resnik and Yarowsky(\]997).In our Bayesian Hierarchical Disambiguator(BHD) model, we attempt to address some of themain issues faced by today's WSD systelns, namely:1) the sparse data problem; 2) the selection of afi;ature set that can be trained upon easily withoutsacrificing accuracy; and 3) the scalability of the sys-tein to disambiguate um'estricted text.
The first twoproblems can be attributed to the lack of taggedcorpora, while the third results from the need forlland-annotated text as a method of circumventingthe first two problems.
We will address the first twoissues by identiflying contexts in which knowledgecan be obtained automatically, as opposed to thosethat require minimal manual tagging.
The effective-hess of the BHD model is then tested on unrestrictedtext, thus addressing the third issue.2 Prob lem Formulat ionWSD can t)e described as a classification task,where the i th sense (W#i )  of a word (W) is clas-sified as the correct tag, given a word and usuallysome surrounding context.
For example, to disam-biguate the adjective "great" in the sentence "Tile152great hm'riealm devastated the region", a \VSI) sys-tem should disambiguate "great" as left.q(: in .s'izcrather than the good or c:cccllent meaning.
Us-ing l)robability notations, this t)rocedure can best~ted as maxi(Pr(great#i \[ "great", "ihe", "lnn'-ricane", "devastated", "the", "region")).
That is,given the word "great" and its context, elassit~y thesense great#/  with the highest probability as thecorrect  o11e.
However, a large COllteX\[;~ sll(:h as thewhole sentence, is rarely used, due to the dillicultyin estimating the 1)robability of this particular set ofwords oceurring.
Therefore, the context is usuallynarrowed, such as ,z number of surromMing words.Additionally, surrollnding syntactic tbatures aim se-mantic knowledge are sometimes used.
The diffi-culty is in choosing the right context, or the setof features, that will ol)timize the classification.
Alarger (:ontext intl)roves t;11(!
classiti(:ation a(:(:ura(:y althe exl)ense of increasing the mlml)er of l)arameters(typically learned fr(nn large training data).in our BHI) model, a minimal context (:Oml)OSe(lof only the adje(:tive, noun and the llOllll;S Sel l lal l -tie features obtained fl'()m \VordNel is used.
Us-ing the al)ove examl)le , only "great", "hurri(:ane"and hun'icane's t)atures encoded in WordNet's hi-erarchy, as in hurricane lISA cyclone lISA wind-storm ISA violent storm..., are used its (;Olltext.Therefore, the classitieation 1)ertbrnmd 1)y Bil l) (:ant)e written as nlaxi(lh(great?~i \[ "great", "\]nlrri-cane", cyt:loltc, windslorm ...)), or more generi-cally, ,m(,,:~( l ' r ( ,d j#i l ( ,d j  , ,,>,,.,,, < N \],',; >)), ,vher(,<N/Zs> denotes the n(mn features.
By using theBayesian inversion fornmla, this equati()n l)e(:onw.s.,,,.<,:,.~(Pr(,.4i,'l,.o,.,., < N\],'.s > I .
, !
i# i )  x P,(,,4i#i)).Pr(,d:i,'no'u,., < Nt;'.s" >)(1)This eonte?t is chosen because it does not; need anannotated training set, and these semantic fe.aturesare used to l)uiht a belief about the nouns ,:ill adjec-tive SellSe typically lnodifies, i.e., the se\]ectional pref-erences of a(ljectiv('.s.
For examlfle , having learnedabout lmrrieane, the system can infer the most prob-able dismnbiguation of "great typhoon", "great tor-nado", or ltlore distal concepts uch as earthquakesand tloods.3 Establishing the ParametersAs shown in equation 1, BH1) requirestwo parameters: 1) the likelihood termPr(adj, noun,< NFs  > ladj#i) and 2) the priorterm Pr(ad j#i ) .
The prior term represents theknowledge of how frequently a sense of an adjectiveis used without any contextual infornmtion.
Dn"example, if great#2 (sense: 9ood, excellent) isnsed frequently while great#l  is less commonlyused, then Pr(great#2) wtmld be larger thanPr (great#l ) ,  in l)rOl)ortion to the usage of thetwo senses.
Although WordNet orders the sensesof a t)olysenl(ms word according to usage, theactual t)rot)ortions are not quantified.
Therefore, to(:omtmte the priors, one can iterate over all Englishnouns and sunl the instmlces of great#l-nounversus great#-2-noun t)airs.
But siilce we assumethat no training set exists (the worst possible easeof the sparse data l)rol)lem), these counts need to1)e estinmted from ilMireet sources.3.1 The Sparse  Data  P rob lemThe techuique used to address data sI)arsity, as firstproposed by Mihalcea and Moldovan (1998), treal;sthe Internet as a cortms to automatically (lisam-biguate word 1)airs.
Using the previous examl)le , todisambiguate he adjective in "great hurricane", twosynonym lists of ("great, large, t)ig") and ("great,neat, good") are retrieved from V?ordNet.
(SOlllesynonyms ~111(1 el;her SellSeS i/re olllitted here for1)revity.)
Two queries, ("great hurricane" or "largehurrit:ane?'
or "t)ig hurricane") and ("great hurri-cane" or "neat hurricane" or "good hurriemm"), areissued to Altavista, which reporl, s that 1100 and 914:1)ages contain these terlllS, respectively.
The querywith the higher count (#1) is classitied its the correctsense.
For fllrther details, please refer to Mihalecaanti Moldovau (1998).In our luo(lel, the (:omd;s front Altavista are im:or-l)orated as \[)aralltel;er stilllatiollS within our proba-bilislh: frlt111(~work.
Jill addition to disaml)igualinglhe adjectives, we also need to estimale lhe us-age of the a(ljec|;ive#i-noml pair.
For simt)licil;y ,the counts fronl Altavista are assigued wholesale 1othe disambiguated adjective sense, e.g., the usageof great#l-hurr icane is 1100 times and great#2-hurricane is zero times.
This is a great simplifi-cation since in many adjective-noun pairs nmltil)lemeanings are likely.
For instance, in "great sl;eak",both sense of '"great" (large steak vs. tasty steak)are e, qually likely.
However, given no other infor-mation, this sinlt)lification is used as a gross ap-t)roximation of Counts(adj#i-noun), which becoinesPr(adj#i-noun) by dividing the counts by a nornml-izing constant, ~ Com~ts(adj#i-all nouns).
Theseprolmbilities are then used to compute the priors,described in the next section.Using this technique, two major probleins are ad-dressed.
Not only are the adjectives automaticallydisambiguated, but the number of occurrenees of theword pairs is also estimated.
The need lot haud-annotated semantic orpora is thus avoided.
How-ever, the statistics gathered by this technique areat)proxinmtions, so the noise they introduce does re-quire supervised training to nfinimize error, as willlm described.1533.2 Comput ing  the PriorsUsing the methods described above, the priors catsbe automatically computed by iterating over allnouns and smnnfing tile counts tbr each adjectivesense.
Untbrtunately, the automatic disambiguationof tim adjective is not reliable enough and results it,inaccurate priors.
Therefore, manual classificationof assigning nouns into one of the adjective sensesis needed, constituting the first of two manual tasksneeded by this model.
However, instead of classi-fying all English nouns, Altavista is again used toprovide collocation data on 5,000 nouns for each ad-jective.
The collocation frequency is then sorted andthe top 100 nouns are manually classified.
For ex-ample, the top 10 nouns that collocate after "great"are "deal", "site", "job", "place", "time", "way","American", "page", "book", and "work".
They arethen all classified as being modified by the great#2sense except for tile last one, which is classified intoanother sense, as defined by WordNet.
Tim priorfor each sense is then coinputed by smmning thecounts Don, t)airing the adjective with the nounsclassified into that sense and dividing by the sumof all adjective-noun pairs.
The top 100 collocatednouns fbr each adjective are used as an al)proxima-tion for all adjective-noun pairs since considering allnouns would be impractical.To validate these priors, a Naive Bayes classifierthat comt)utesv,.azi Pr(adj, ,!,o'l,r@,dj#i) x Pr(,,dj #i)Pr  ( a4i, ~o , .
,.
)is used, with the noun as the only context.
Thissimpler likelihood term is approxinmted by the sameInternet counts used to establish the priors, i.e.,Counts(adj#i-noun) / normalizing constant.
In Ta-ble 1, the accuracy of disambiguating 135 adjective-noun pairs Dora the br-a01 file of the semanticallytagged corlms SemCor (Miller et al, 1993) is com-pared to the baseline, wlfich was calculated by usingthe first WordNet sense of the adjective.
As men-tioned earlier, disambiguating using, simply the high-est count Dot** Altavista ("Before Prior" it, Table1) achieved a low accuracy of 56%, whereas usingthe sense with the highest prior ("Prior Only") isslightly better than the baseline.
This result vali-dates the fact that the priors established here pre-serve WordNet's ordering of sense usage, with tileimprovement that tim relative usages between sensesare now quantified.Combining both the prior and the likelihood termsdid not significantly improve or degrade tile accu-racy.
This would indicate that either the likelihoodterm is uniformly distributed across tile i senses,which is contradicted by the accuracy without thepriors (second row) being significantly higher thanthe average number of senses per adjective of 3.98,Accur&cyBefore Prior 56.3%Prior Only 77.0%Combined 77.8%Baseline 75.6%Table 1: Accuracy rates from using a Naive Bayesclassifier to wflidate the priors.
These results showthat the priors established in this model are as ac-curate as the WordNet's ordering according to senseusage (Baseline).or, more likely that this parameter is subsumed bythe priors due to the limited context.
Therefore,more contextual infbrmation is needed to improvetim model's peribrmance.3.3 Contextual  FeaturesInstead of adding other types of context such asthe surrounding words and syntactic features, tlmsemantic features of tim noun (as encoded in theWordNet ISA hierarchy) is investigated for its effec-tiveness.
These features are readily available and areorganized into a well-defined structus'e.
Tim hierar-chy provides a systematic and intuitive method ofdistance measurements between feature vectors, i.e.,the semantic distance between concepts.
This prop-erty is very important for inthrring the classificationof the novel pair "great flood" into the sense tha.tcontains hurricane as a member of its prototypicalnouns.
These prototypical nouns describe tile se-lectional preferences of adjective senses of "great",and the semantic distance between them and a newnoun ineasures 1;11o "semantic fit" between the con-cepts.
Tile closer the.y are, as with "hurricane" and"flood", the higher the prot)ability of the likelihoodterm, whereas distal concepts uch as "hurricane"and "taste" would have a lower value.Representing these prototyt)ical 11011218 prot)abilis-tically, however, is difficult due to the exponentialnumber of 1)robal)ilit, ies with respect o the nmnberof features.
For exmnl)le, ret)resenting hurricmm 1)e-ing I)resent in a selectional t)reference list requires2 s probabilities ince there are 8 features, or ISAl)arents, in the WordNet hierarchy.
It, addition , tilesparse data t)roblem resurfaces because ach one ofthe 2 s probabilities has to l)e quantified.
To ad-dress these two issues, belief networks are used, asdescribed it, detail in the next section.4 Probab i l i s t i c  Network 'sThere are many advantages to using Bayesian net-works over the traditional probabilistic models.
Themost notable is that the mlmber of probabilitiesneeded to represent he distribution can be significantly reduced by making independence assump-tions between variables, with each node condition-154~A~ p(AIB, c)P(h,B,C3)3?,l~)=l'(All~,C)xl'(BIb,i;)xP(Cl}))xP(\])llV,)xl)(l~)xl)(F)Figure :1: A n eXamlih; of a Bayesiml nel;work and thet)rolialiilities at ('a(:h node, thai  (h'lin(; lhe rehtl.ion-shills 1)('tweeu a no(h; and its parents.
'\]'he quational; the l)ott()m shows how t.h(, (listrilmtion across all(if the variat)les is (:Oml)Ut(',(l.ally d(,l/endenl; ltl)on only il;s t)arenls (\]'earl, 1988).\]Pigllr(!
\] shows all (LxCallll)\](; \]{ayesiall l(H:Woll( rel)-r(;senl;ing l;h(' disl;ribul;i()n \])(;\,B;(',,1);F,,\]").
\]nsl;('a(lof having oue large (,able wilh 2" lirol)al)ilit.i(~s (withall \]}o()\](;an nod(!s), the (lisl;ribution is rel)resenl(;(tby the (:onditional I)rolial)ility tal)les (C,\])Ts) at ea('hnod(,, su(:h as I>(B I l), F), requiring a (olal of only 2dprol)al)ilitie~s.
Not only (lo th(, savings he(:ome moresignificant with larger networks, lmi; (.he sparse datat/r()l)h;m b(!
(:omes m()r(; manag(;abh, as well.
Th(!I;l';/illitlg~ s(!l; 11o lollg:er lice(is Ix) (:()v(!r all l)(!l'lllllta-l;ions (if l;he f(!a(;m'e sets, lml only smaller sul)setsdictated l)y l.he sets of Valial)h!s (if lira CPTs.The n(;l.w(/rk shown in Figure 1 lo()ks simi-hn to any pOll;ion of th(~ \Vor(1N(~l; hierar(:hy fora reason.
In BII\]), belief networks with thesame stru(:tur(; as the \Vor(tNet hierar(:hy are au-tomatical ly (:onstru(:t;ed to rel)l"esent he seh'cl;ionalI)reference of an a(1.iective sense.
S1)ecifically,the network rei)resents the prol)aliilistic (tistribu-l;ion over all of the i)rotol;yl)i(:al nouns of an a(1-ject ive~i  mid the nouns' semanti(: t'ealures, i.e.,P(v,.ot,,,,,,,,..,., < v,.o~,oNF, > I,-!i#i).
'rh(; ,,s(, ofBayesian networks for WSI) has I)een l)rop()sed byothers such as Wiebe eL al (1.998), but a differentfl)rmulation is used in this mod('l.
Th(, constructionof the networks in BHD can be divided into threesteps: defining 1) the training sets, 2) the structure,and 3) the probabil it ies, as described in the tbllowingsections.4.1 ~lYaining SetsThe training set for each of the adje(:tive s(',nses is(;onstructed by ('.xi;ra(:l;ing l;he exenll)lary adje(:tive-noun pairs from tile WordNet glossary.
The glossarycontains the examlih; usage of the a(lje(:tiv('~s, andl;\]l(', nouns from thein are taken as the training sets" /> ,  t'"il, - \  / ' \'""@t :':, (?.
*ib,,}~ " ,'..,,,.,:p, .
.
.
.
.
.
.
.
H/ ,(,,,,:,,,,@,,.
(,.,,~,) ( , ,&  ,," ............... ~.............\ /a~",'"') (.,,,5 (.,,,4-.
..... ~ ...............Figure 2: The stru(:ture (if the belief n(,l;w(/rk thati'el)resenls the, s(~h~ctional l)refer(!n(:(~ (if f lrcal,#\[.The leaf nodes are 1.he nouns within the training set,mid lhe int(~rm('xlial.
(; no(les r('th;(:l; the ISA hierar(:hyfr()m \\;()r(1Nel;.
The 1)rol)ahilities al each node at(',used i;o (lisanfl)iguat(!
novel a(lj(,(:l;ive-noun 1)airs.for th(, a(1.i('(:iives.
For exmnl)h!
, the nouns "auk","oak", "steak", "delay" and "amount" (:ompose thetraining set fl)r g reat#l  (SellSC" lalT/e iv, size,).
Notel\]lal; WordNet in(:huh'd "sl;e~al~" ill l;he glossary ofgreal;#\],  l)ul: il; al)l)ears thai  1;\]le 9ood or e:ccelhe'H,l,5(!lise would lm lliOl'(!
aplWol)rial;e. Neverlheh~ss, the}isis of exelnl)lary mmns are sysl:(;malit:aily rel;rh'vcdand nol ('dile(l.Th(" sets (if l/r()tolypi(:al n(/mlS f(/r each a(lj(~(:liv(~sense have to lie (lisaml)igual;ed lie(:ause, the S(~lllall-li(: features (lifter 1)etween ambiguous nouns.
Sincethese n(mns cmmol; lie autonmti(:ally disamlAguatedwith high accuracy, lhey have to be done mamm\]ly.This is the second t)art of (,lie mmmal process need(;d1) 3" BIID sitl(:(!
the W(/rdNel; gh)ssary is not selnallti-(:ally tagged.4.2 Be l ie f  Network  S t ruetm'eThe belief networks have the same structure as the\VordNet 1SA hierarchy with the ext;et)tion that theedges are direcl;ed from the child nodes to their par-ents.
I lhlstrated in Figure 2, the BItD-eonstructednetwork represents the selectional t)referelme of theto I) level nod(;, great#l .
The leaf nodes are tile evi-(len(:e nodes from the training set mM the internm(li-ale ilo(les are |;t1(; sema.ntie featm'es (if the leaf nodes.This organization emd)les the belief gathered fromthe leaf nodes 1;o lie tirot)agated uI) to the tol) levelnode during inferencing, as described in a later sec-LioIl.
1}11|; first, th(' l)robability table ae(:oml)anyiugea(:h node needs to be constructed.1554.3 Quanti fy ing the NetworkThe two parameters the belief networks require arethe CPTs tbr each intermediate node and the pri-ors of the leaf nodes, such as P(great#l ,  hurri-cane).
The latter is estilnated by tile counts ob-tained fronl Altavista, as described earlier, and ashortcut is used to specit~y the CPTs.
Normallythe CPTs ill a flflly specified Bayesian etwork con-tain all instantiations of the child and parent valuesand their corresponding probabilities.
For example,the CPT at node D in Figure 1 would have fourrows: e r (D=t lE : t ) ,  Pr (D=t lE=f) ,  Pr(D=flE=t) ,and Pr (D=f lE=f  ).
This is needed to perform flfllinferencing, where queries can be issued for any in-stantiation of the variables.
However, since the net-works in this model are used only for one specificquery, where all nodes are instantiated to be true,only the row with all w~riables equal to true, e.g.,Pr (D=t lE=t) ,  has to be specified.
The nature ofthis query will be described in more detail in tilenext section.To calculate the probability that an intermediatenode and all of its parents are true, one divides thenumber of parents present by the number of possi-ble parents as specified ill WordNet.
hi Figure 2,the small clotted nodes denote the absent parents,which deternfine how the probabilities are specifiedat each node.
Recall that tile parents in the beliefnetwork are actually the children in the.
WordNethierarchy, so this probability can be seen as the per-centage of children actually present, hltuitively, thisprobability is a form of assigning weights to parts ofthe network where more related nouns are presentin the training set, silnilar to the concept of seman-tic density.
Tile probability, in conjunction with thestructure of the belief network, also implicitly en-codes the semantic distance between concepts with-out necessarily 1)enalizing concepts with deep hier-archies.
A discount is taken at each ancestral nodeduring inferencing (next section) only wlmn someof its WordNet children are absent in the network.Therefore, the semantic distance can be seen as thenumber of traversals 11I) the network weighted by thenumber of siblings present in tile tree (and not bydirect edge counting).4.4 Querying the NetworkWith the probability between nodes specified, thenetwork becomes a representation of the selectionalprefbrence of an adjective sense, with features fromthe WordNet ISA hierarchy providing additionalknowledge on both semantic densities and semanticdistances.
To disambiguate a novel adjective-nounpair such as "great flood", the great#l  and great#2networks (along with 7 other great#/networks  notshown here) infer the likelihood that "flood" be-longs to the network by comtmting the probabilityPr(great, flood, <flood NFs>, proto nouns, <l)rotoNFs> I adj #i), even though neither network has everencountered the noun "flood" before.To perform these inferences, the noun and its fea-tures are tenlporarily inserted into the network ac-cording to the WordNet hierarchy (if not alreadypresent).
The prior for this "hypothetical evidence"is obtained the same way as the training set, i.e., byquerying Altavista, and the CPTs are updated toreflect this new addition.
To calculate the probabil-ity at the top node, any Bayesian network inferenc-ing algorithm can be used.
However, a query whereall nodes are instantiated to true is a special casesince the probability can be comlmted by multiply-ing together all priors and the CPT entries where allvariables are true.Ill Figure 3, tile network for g reat#l  is shown with"flood" as tile hypothetical evidence added on theright.
The CPT of the node "natural phenomenon"is updated to reflect the newly added evidence.
Thepropagation of the probabilities from the leaf nodesup the network is shown and illustrates how dis-counts art taken at each intermediate node.
When-ever more related concepts are 1)resent in the net-work, such as "typhoon" and "tornado '~, less dis-counts are taken and thus a higher probability willresult at the root node.
Converso.ly, one can see thatwith a distal concept, such as "taste" (which is ill acompletely different branch), the knowledge about"hurricane" will have little or no influence on dis-ambiguating "great taste".The calculation above can be computed in lineartime with respect to the det/th of tlle query nounnode (depth=5 in the case of f lood#l)  and not thethe nmnber of nodes ill the network.
This is impor-tant for scaling the network to represent the largenuinber of nouns needed to accurately model tile se-lectional preferences of adjective senses.
Tile onlycost incurred is storage for a summary probabilityof the children at each internlediate node and timefor ut)dating these values when a new piece of evi-dence is added, which is also linear with respect othe depth of the node.Finally, the probabilities comt)uted by the infer-ence algorithnl are combilmd with the priors estab-lished in the earlier section.
Tile combined proba-bilities represent P(adj#i  \] adj, noun, <NFs>),  andtilt one with the highest probability is classified byBHD as tile most plausible sense of the adjective.4.5 EvahmtionTo test the accuracy of BHD, the same proceduredescribed earlier was used.
Tile same 135 adjective-noun pairs from SelnCor were disambiguated byBHD and compared to the baseline.
Table 2 showsthe accuracy results froln evaluating either the firstsense of the nouns or all senses of the nouns.
The re-sults of the accuracy without the priors Pr(adj#i) in-1561/16x 1/30x'I'?i?al) 2/-$,S/cLmst x 1/~Illka,d} 1/8t tlood) - 1770~x 1/7 x I/IFigure 3: (~ll{2ry of {;he flrcrtt#-I 1}elief netw{)rl(t(} infe.r l;h{', probal}ility of tlood being m{}{lified l}y.q'reat#l..
The left branch of the network has 1}{'.enomitted for clarity.dicate~ the imI}rovements l}rovided by the likeliho{}{tterm alone.
The itnl}r(}vement gain(~{l from the ad-dii.ional conl;extual featm:es hows th(~ ell'{~{:liv{m{~ssof the belief networks, l!
'~v(m with only 3 t}rol{)tyt)-ica.l mmns l}er a(ljecl,ive se.nse (}n av{u'ag{~ (hardly aCOml)lete deseril)tion of the sel(B(:tional pr(ff(w(u\]{:{;s);the gain is very encouraging.
Wit;h the 1)ri(}rs t'a{:-tored in, 13IID iml}r(}ved even flu:ttmr (81%), signifi-('antly surllassing the baseline (75.6%), a feat a{:{:om-plished 1)3; {rely (me (}ther m{}del that we are aware (}f(airi Sl;el;iIla and Nagao, 1998).
Not;e thai; l;h{!
l)esl~/.C{;lll;}/{;y \v~ls a{'hieved by evaluating all senses of th{~ll(}llllS} }IS exi)e{:ted, since the sele{:l;iollal t)r{~.fer{,Jl{'eis modele{t l;hr{mgh senmnti(: feai;mes (}t" the glos-sary nouns, not just their word forms.
The.
r{;as{mfor the good accuracy from using only the tirst nounsense is because 72% of them hal}pen to be the first;S0,IlSe.
r\]_~heso results are very ('JH;ouragillg si\]lco 11otagged corl)us and minimal training data were.
used.\?e believe that  with a bigger training set, \]3HD'st)erf(}rnlance will iml}rove even further.4.6 Co ln l )a r i son  w i th  Other  Mo{h,JsrE() our kn{}wledge, there are {rely two oLher sysl;enlsthat  (lisanfl)iguate a.
(lj{~{:tive-noml llairs from unre-st.ri{:l;e{l l;ex{;.
Results fl'om both models were evalu-ated against SemCor and thus a comparison is mean-ingful.
In Table 3, each model's accuracy (as well as\'Vithoutlh'iorWith1}l.i{}rContext11Ol111 only+SP11Ol111 oll ly+SP1.
st IIOllllSOIISO56.3%6O.O%77.8%80.
{}%all nounS{BI IH(~S53.3%60.0%77.8%81.4%Baseline 75.6% 75.6%Ta.1}le 2: Accuracy results from the seleetional pref-erence model (+SP) ,  showing the improvements overthe baseline by either considering the first llOllIlsense or all noun senses.Model Results \ [Basel ineHIII) 81.4% 75.6%Mihah:ea and Moldovan 79.8% 81.8%(1999)SW.tina el; al.
(1998) 83.6% 81.9%Table 3: Colnt/arison of atljectiv(~ disalnl}iguation at:-curay with other inodels.lhe baseline) is provided since different adjet:tive-noun pairs were e, valuated.
We find t;he BIt\]) re-suits conlpa.ralfle, if not bel;ter, espcc, ially when ihe}IlllOllll{; Of inq)rovenw, nl; ()vet th0, \])aseline is eonsi(l-ered.
The Ino<lel 1) 3, ,SI;e,l;ina (1.998) was {,rai\]md tmSemCor that was merged with a flfll senl;ential parsetree, the determinat ion of which is considered a dif-ficult l)rolflem of its own (Collins, 1997).
\Ve belie, rethai; by int:ori}oral;ing tim data from SemC(n (dis-(:llSse(l ill I;he fllI;llre work sc, ct;ion)~ {;11o, \]}erforlll}lllCe(ff our sysi;em will surpass Stetina's.5 Conc lus ion  and  Future  WorkWe have 1)resenl;etl a t)rol}~fl}ilistic disanfl}iguationmodel that  ix sysl;e, ln~l;i{,, a(tcllral;e, all(l require llHlll-ual intervention ill only ill two places.
The morel;illle (:OllSlllllill~ of tim l;wo manual l;asks is to (:\]as-sitS' th(~ toll 100 nouns needed for the priors.
Theel;her task~ of disanfl)igual;ing l)rol;olTpical lOllllS, isrelal;ively simple due to the limited nunfl)er of glos-sary nouns per sense.
IIowever, it would l}e straight-forward to incorporate semantically tagged corpora,such as SemCor, to avoid these mamml tasks.
Thepriors are the number  of instances of each adjectivesense divided by all of the adjectives in the corpus.The disambiguated adjectiveT~i-noun#.\] pairs fromthe corpus ean be used as training sets to build bet-{e,r ret/resental;ion of selectional preferences l}y in-serting tim nounT~j node mid the ac(:omf}any featm'esinto the l}elief network of a{ljectivegfii.
The insertionis the same prot:c/hue used to add the hyllothe|;icalevidence dm'ing the inferoncing stage.
The Ul)(latedbelief networks could then be, used tbr disambigua-lion wii;h improve.d at:curacy.
Furthernlore, the per-formance of BI ID (:(mid a.lso be improved by exl)and-157ing the context or using statistical learning methodssuch as the EM algorithln (DemI)ster et al, 1977).Using Bayesian networks gives the model ttexibilityto incorporate additional contexts, such as syntac-tical and morphological features, without incurringexorbitant costs.It is l)ossible that, with an extended model thataccurately disambiguates adjective-noun pairs, theselectional preference ofadjective senses coutd be au-tomatically learned.
Having all improved knowledgeal)out the selectional 1)references would then providebetter parameters for disanfl)iguation.
The modelcan be seen as a bootstrapping learning process tbrdisambiguation, where the information gained fromone part (selectional preference) is used to improvetile other (disambiguation) and vice versa, reminis-cent of the work by Riloff and Jones (1.999) andYarowsky (1995).Lastly, the techniques used in this paper could bescaled to disambiguate not only all adjective-nounpairs, but also other word pairs, such as subject-verb, verb-object, adverb-verb, y obtaining most ofthe paraineters from the Internet and WordNet.
Ifthe information fi'oln SemCor is also used, then thesystem could be automatically trained to pertbrmdisambiguation tasks on all content words within aSellteI1Ce.In this paper, we have addressed three of what webelieve to be the main issues timed 1)y current WSDsystems.
We demonstrated the effectiveness of theteclmiques used, while identii~ying two mmmal tasksthat don't necessarily require a semantically taggedcorpus.
By establishing accurate priors a.nd smalltraining sets, our system achieved good initial dis-ambiguation accuracy.
The salne methods could 1)eflflly automated to disami)iguate all content wordpairs if infbrmation from semantically tagged cor-pora is used.
Our goal is to create a system that candisambiguate all content words to an accuracy levelsufficient for automatic tagging with tummn valida-tion, which could then be used to improve or fa-cilitate new probabilistic semantic taggers accurateenough for other NLP applications.ReferencesEneko Agirre and German Rigau.
1996.
Word sensedismnbiguation using conceptual density.
In Pro-ceedings of COLING-96, Copenhagen.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical 1)arsing.
In Proceedings ofthe 351h Annual Meeting of the ACL, pages 16-23, Madrid, SI)ain.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, 39 (B): 1-38.Sadao Kurohashi Jiri Stetina and Makoto Nagao.1998.
General word sense disambiguation methodbased oll a flfll sentential context.
In Proceedingsof COLING-ACL Workshop on Usage of Word-Net in Natural Language Processing, Montreal,Canada, .July.Rada Mihalcea and Dan Moldovan.
1998.
Wordsense disambiguation base(!
on semantic density.In Proceedings of COLING-ACL Workshop on Us-age of WordNct in Natural Language Proecssing,Montreal, Canada, July.G.
Miller, C. Leacock, and R. Tengi.
1993.
A seman-tic concordance.
In Procccdings of ARPA I\]uTnanLanguage Technology, Princeton.G.
Miller.
1990.
WordNet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4:).Hwee Tou Ng and Hian Beng Lee.
1996.
Integratinginultiple knowledge sources to disambiguate wordsense: An exemplar-based approaclL In Proceed-ings of the 3/tth, Annual Meeting of ACL, SantaCruz, June.Judea Pearl.
1988.
Probabilistic Reasoning in Intel-ligent Systems: Networks of Plausible Inference.Morgan Kaufmalm, San Mateo, CA.Philit) Resnik an(1 David Yarowsky.
1997.
A per-spective ell word sense disambiguation methodsmid their evaluation.
In ANLP Workshop on Tag-ging Text with, Lexical Semantics, Washington,D.C., June.Philip Resnik.
1997.
Selectional preference andsense disambiguation.
In ANLP Worksh.op onTagging Text with, Lcxical Semantics, Wash, ing-ton, D.C., June.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries fbr information extraction by multi-levelbootstrapping.
In Proceedings of AAAI-O9, Oflando, Florida.aanyce Wiebe, Tom O'Hara, and Rebecca Bruce.1998.
Constru(:ting bayesian etworks from Word-Net for word-sense disambiguation: I/.el)resenta-tional and I)rocessing issues.
In Proceedings ofCOLING-ACL Workshop on Usage of WordNct inNatural Language Processing, Montreal, Canada,July.David Yarowsky.
1992.
Word-sense disambigua-tion using statistical model of Roget's cate-gories trained on large corpora.
In Proceedings ofCOLING-92, Nantes, France.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProccedings of the 33rd Annual Meeting of theACL.158
