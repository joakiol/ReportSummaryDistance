Proceedings of NAACL HLT 2007, pages 97?104,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsImproving Diversity in Ranking using Absorbing Random WalksXiaojin Zhu Andrew B. Goldberg Jurgen Van Gael David AndrzejewskiDepartment of Computer SciencesUniversity of Wisconsin, MadisonMadison, WI 53705{jerryzhu, goldberg, jvangael, andrzeje}@cs.wisc.eduAbstractWe introduce a novel ranking algorithmcalled GRASSHOPPER, which ranks itemswith an emphasis on diversity.
That is, thetop items should be different from eachother in order to have a broad coverageof the whole item set.
Many natural lan-guage processing tasks can benefit fromsuch diversity ranking.
Our algorithm isbased on random walks in an absorbingMarkov chain.
We turn ranked items intoabsorbing states, which effectively pre-vents redundant items from receiving ahigh rank.
We demonstrate GRASSHOP-PER?s effectiveness on extractive text sum-marization: our algorithm ranks betweenthe 1st and 2nd systems on DUC 2004Task 2; and on a social network analy-sis task that identifies movie stars of theworld.1 IntroductionMany natural language processing tasks involveranking a set of items.
Sometimes we want the topitems to be not only good individually but also di-verse collectively.
For example, extractive text sum-marization generates a summary by selecting a fewgood sentences from one or more articles on thesame topic (Goldstein et al, 2000).
This can be for-mulated as ranking all the sentences, and taking thetop ones.
A good sentence is one that is represen-tative, i.e., similar to many other sentences, so thatit likely conveys the central meaning of the articles.On the other hand, we do not want multiple near-identical sentences.
The top sentences should be di-verse.As another example, in information retrieval onnews events, an article is often published by multi-ple newspapers with only minor changes.
It is unde-sirable to rank all copies of the same article highly,even though it may be the most relevant.
Instead,the top results should be different and complemen-tary.
In other words, one wants ?subtopic diversity?in retrieval results (Zhai et al, 2003).The need for diversity in ranking is not unique tonatural language processing.
In social network anal-ysis, people are connected by their interactions, e.g.,phone calls.
Active groups of people have strong in-teractions among them, but many groups may existwith fewer interactions.
If we want a list of peoplethat represent various groups, it is important to con-sider both activity and diversity, and not to fill thelist with people from the same active groups.Given the importance of diversity in ranking,there has been significant research in this area.
Per-haps the most well-known method is maximummarginal relevance (MMR) (Carbonell and Gold-stein, 1998), as well as cross-sentence informationalsubsumption (Radev, 2000), mixture models (Zhanget al, 2002), subtopic diversity (Zhai et al, 2003),diversity penalty (Zhang et al, 2005), and others.The basic idea is to penalize redundancy by loweringan item?s rank if it is similar to items already ranked.However, these methods often treat centrality rank-ing and diversity ranking separately, sometimes withheuristic procedures.97We propose GRASSHOPPER (Graph Random-walkwith Absorbing StateS that HOPs among PEaksfor Ranking), a novel ranking algorithm that en-courages diversity.
GRASSHOPPER is an alternativeto MMR and variants, with a principled mathemat-ical model and strong empirical performance.
Itranks a set of items such that: 1.
A highly rankeditem is representative of a local group in the set,i.e., it is similar to many other items (centrality);2.
The top items cover as many distinct groups aspossible (diversity); 3.
It incorporates an arbitrarypre-specified ranking as prior knowledge (prior).Importantly GRASSHOPPER achieves these in a uni-fied framework of absorbing Markov chain randomwalks.
The key idea is the following: We definea random walk on a graph over the items.
Itemswhich have been ranked so far become absorbingstates.
These absorbing states ?drag down?
the im-portance of similar unranked states, thus encourag-ing diversity.
Our model naturally balances central-ity, diversity, and prior.
We discuss the algorithmin Section 2.
We present GRASSHOPPER?s empiri-cal results on text summarization and social networkanalysis in Section 3.2 The GRASSHOPPER Algorithm2.1 The InputGRASSHOPPER requires three inputs: a graph W , aprobability distribution r that encodes the prior rank-ing, and a weight ?
?
[0, 1] that balances the two.The user needs to supply a graph with n nodes,one for each item.
The graph is represented by ann?
n weight matrix W , where wij is the weight onthe edge from i to j.
It can be either directed or undi-rected.
W is symmetric for undirected graphs.
Theweights are non-negative.
The graph does not needto be fully connected: if there is no edge from itemi to j, then wij = 0.
Self-edges are allowed.
For ex-ample, in text summarization one can create an undi-rected, fully connected graph on the sentences.
Theedge between sentences i, j has weight wij , their co-sine similarity.
In social network analysis one cancreate a directed graph with wij being the numberof phone calls i made to j.
The graph should beconstructed carefully to reflect domain knowledge.For examples, see (Erkan and Radev, 2004; Mihal-cea and Tarau, 2004; Pang and Lee, 2004).The user can optionally supply an arbitrary rank-ing on the items as prior knowledge.
In thiscase GRASSHOPPER can be viewed as a re-rankingmethod.
For example, in information retrieval,the prior ranking can be the ranking by relevancescores.
In text summarization, it can be the po-sition of sentences in the original article.
(Thereis evidence that the first few sentences in an ar-ticle are likely good summaries.)
Somewhat un-conventionally, the prior ranking is represented asa probability distribution r = (r1, ?
?
?
, rn)?
suchthat ri ?
0,?ni=1 ri = 1.
The highest-ranked itemhas the largest probability, the next item has smallerprobability, and so on.
A distribution gives the usermore control.
For example ra = (0.1, 0.7, 0.2)?and rb = (0.3, 0.37, 0.33)?
both represent the sameranking of items 2, 3, 1, but with different strengths.When there is no prior ranking, one can let r =(1/n, ?
?
?
, 1/n)?, the uniform distribution.2.2 Finding the First ItemWe find the first item in GRASSHOPPER ranking byteleporting random walks.
Imagine a random walkeron the graph.
At each step, the walker may do one oftwo things: with probability ?, she moves to a neigh-bor state1 according to the edge weights; otherwiseshe is teleported to a random state according to thedistribution r. Under mild conditions (which are sat-isfied in our setting, see below), the stationary distri-bution of the random walk defines the visiting prob-abilities of the nodes.
The states with large probabil-ities can be regarded as central items, an idea usedin Google PageRank (Page et al, 1998) and other in-formation retrieval systems (Kurland and Lee, 2005;Zhang et al, 2005), text summarization (Erkan andRadev, 2004), keyword extraction (Mihalcea and Ta-rau, 2004) and so on.
Depending on ?, items high onthe user-supplied prior ranking r may also have largestationary probabilities, which is a way to incorpo-rate the prior ranking.As an example, we created a toy data set with 300points in Figure 1(a).
There are roughly three groupswith different densities.
We created a fully con-nected graph on the data, with larger edge weightsif points are closer2.
Figure 1(b) shows the station-ary distribution of the random walk on the graph.1We use state, node and item interchangeably.2We use wij = exp(?
?xi ?
xj?2/0.16), ?
= 1.980 5 10024680 510051000.0050.010.015g10 51005100246g20 510051000.511.5g3(a) (b) (c) (d)Figure 1: (a) A toy data set.
(b) The stationary distribution pi reflects centrality.
The item with the largestprobability is selected as the first item g1.
(c) The expected number of visits v to each node after g1 becomesan absorbing state.
(d) After both g1 and g2 become absorbing states.
Note the diversity in g1, g2, g3 as theycome from different groups.Items at group centers have higher probabilities, andtighter groups have overall higher probabilities.However, the stationary distribution does not ad-dress diversity at all.
If we were to rank the itemsby their stationary distribution, the top list would bedominated by items from the center group in Fig-ure 1(b).
Therefore we only use the stationary dis-tribution to find the first item, and use a methoddescribed in the next section to rank the remainingitems.Formally we first define an n ?
n raw transitionmatrix P?
by normalizing the rows of W : P?ij =wij/?nk=1 wik, so that P?ij is the probability that thewalker moves to j from i.
We then make the walka teleporting random walk P by interpolating eachrow with the user-supplied initial distribution r:P = ?P?
+ (1 ?
?
)1r?, (1)where 1 is an all-1 vector, and 1r?
is the outer prod-uct.
If ?
< 1 and r does not have zero elements,our teleporting random walk P is irreducible (possi-ble to go to any state from any state by teleporting),aperiodic (the walk can return to a state after anynumber of steps), all states are positive recurrent (theexpected return time to any state is finite) and thusergodic (Grimmett and Stirzaker, 2001).
ThereforeP has a unique stationary distribution pi = P?pi.We take the state with the largest stationary proba-bility to be the first item g1 in GRASSHOPPER rank-ing: g1 = argmaxni=1 pii.2.3 Ranking the Remaining ItemsAs mentioned early, the key idea of GRASSHOPPERis to turn ranked items into absorbing states.
Wefirst turn g1 into an absorbing state.
Once the ran-dom walk reaches an absorbing state, the walk is ab-sorbed and stays there.
It is no longer informative tocompute the stationary distribution of an absorbingMarkov chain, because the walk will eventually beabsorbed.
Nonetheless, it is useful to compute theexpected number of visits to each node before ab-sorption.
Intuitively, those nodes strongly connectedto g1 will have many fewer visits by the randomwalk, because the walk tends to be absorbed soonafter visiting them.
In contrast, groups of nodes faraway from g1 still allow the random walk to lingeramong them, and thus have more visits.
In Fig-ure 1(c), once g1 becomes an absorbing node (rep-resented by a circle ?on the floor?
), the center groupis no longer the most prominent: nodes in this grouphave fewer visits than the left group.
Note now they-axis is the number of visits instead of probability.GRASSHOPPER selects the second item g2 with thelargest expected number of visits in this absorbingMarkov chain.
This naturally inhibits items similarto g1 and encourages diversity.
In Figure 1(c), theitem near the center of the left group is selected asg2.
Once g2 is selected, it is converted into an ab-sorbing state, too.
This is shown in Figure 1(d).
Theright group now becomes the most prominent, sinceboth the left and center groups contain an absorbingstate.
The next item g3 in ranking will come from theright group.
Also note the range of y-axis is smaller:99with more absorbing states, the random walk will beabsorbed sooner.
The procedure is repeated until allitems are ranked.
The name GRASSHOPPER reflectsthe ?hopping?
behavior on the peaks.It is therefore important to compute the expectednumber of visits in an absorbing Markov chain.
LetG be the set of items ranked so far.
We turn the statesg ?
G into absorbing states by setting Pgg = 1 andPgi = 0,?i 6= g. If we arrange items so that rankedones are listed before unranked ones, we can writeP asP =[IG 0R Q].
(2)Here IG is the identity matrix on G. Submatrices Rand Q correspond to rows of unranked items, thosefrom (1).
It is known that the fundamental matrixN = (I ?Q)?1 (3)gives the expected number of visits in the absorbingrandom walk (Doyle and Snell, 1984).
In particularNij is the expected number of visits to state j be-fore absorption, if the random walk started at state i.We then average over all starting states to obtain vj ,the expected number of visits to state j.
In matrixnotation,v = N?1n?
|G| , (4)where |G| is the size of G. We select the state withthe largest expected number of visits as the next itemg|G|+1 in GRASSHOPPER ranking:g|G|+1 = argmaxni=|G|+1 vi.
(5)The complete GRASSHOPPER algorithm is summa-rized in Figure 2.2.4 Some DiscussionsTo see how ?
controls the tradeoff, note when ?
= 1we ignore the user-supplied prior ranking r, whilewhen ?
= 0 one can show that GRASSHOPPER re-turns the ranking specified by r.Our data in Figure 1(a) has a cluster struc-ture.
Many methods have exploited such structure,e.g., (Hearst and Pedersen, 1996; Leuski, 2001; Liuand Croft, 2004).
In fact, a heuristic algorithm isto first cluster the items, then pick the central itemsfrom each cluster in turn.
But it can be difficult toInput: W , r, ?1.
Create the initial Markov chain P fromW, r, ?
(1).2.
Compute P ?s stationary distribution pi.
Pick thefirst item g1 = argmaxi pii.3.
Repeat until all items are ranked:(a) Turn ranked items into absorbingstates (2).
(b) Compute the expected number of visits vfor all remaining items (4).
Pick the nextitem g|G|+1 = argmaxi viFigure 2: The GRASSHOPPER algorithmdetermine the appropriate number and control theshape of clusters.
In contrast, GRASSHOPPER doesnot involve clustering.
However it is still able toautomatically take advantage of cluster structures inthe data.In each iteration we need to compute the fun-damental matrix (3).
This involves inverting an(n ?
|G|) ?
(n ?
|G|) matrix, which is expensive.However the Q matrix is reduced by one row andone column in every iteration, but is otherwise un-changed.
This allows us to apply the matrix in-version lemma (Sherman-Morrison-Woodbury for-mula) (Press et al, 1992).
Then we only need toinvert the matrix once in the first iteration, but not insubsequent iterations.
Space precludes a full discus-sion, but we point out that it presents a significantspeed up.
A Matlab implementation can be foundat http://www.cs.wisc.edu/?jerryzhu/pub/grasshopper.m.3 Experiments3.1 Text SummarizationMulti-document extractive text summarization is aprime application for GRASSHOPPER.
In this task, wemust select and rank sentences originating from aset of documents about a particular topic or event.The goal is to produce a summary that includes allthe relevant facts, yet avoids repetition that mayresult from using similar sentences from multipledocuments.
In this section, we demonstrate that100GRASSHOPPER?s balance of centrality and diversitymakes it successful at this task.
We present em-pirical evidence that GRASSHOPPER achieves resultscompetitive with the top text summarizers in the2004 Document Understanding Conference (http://duc.nist.gov).
DUC is a yearly text summa-rization community evaluation, with several tasks inrecent years concentrating on multi-document sum-marization (described in more detail below).Many successful text summarization systemsachieve a balance between sentence centrality anddiversity in a two-step process.
Here we review theLexRank system (Erkan and Radev, 2004), whichis most similar to our current approach.
LexRankworks by placing sentences in a graph, with edgesbased on the lexical similarity between the sentences(as determined by a cosine measure).
Each sen-tence is then assigned a centrality score by findingits probability under the stationary distribution ofa random walk on this graph.
Unlike the similarPageRank algorithm (Page et al, 1998), LexRankuses an undirected graph of sentences rather thanWeb pages, and the edge weights are either cosinevalues or 0/1 with thresholding.
The LexRank cen-trality can be combined with other centrality mea-sures, as well as sentence position information.
Af-ter this first step of computing centrality, a sec-ond step performs re-ranking to avoid redundancyin the highly ranked sentences.
LexRank uses cross-sentence informational subsumption (Radev, 2000)to this end, but MMR (Carbonell and Goldstein,1998) has also been widely used in the text sum-marization community.
These methods essentiallydisqualify sentences that are too lexically similar tosentences ranked higher by centrality.
In short, sim-ilar graph-based approaches to text summarizationrely on two distinct processes to measure each sen-tence?s importance and ensure some degree of diver-sity.
GRASSHOPPER, on the other hand, achieves thesame goal in a unified procedure.We apply GRASSHOPPER to text summarization inthe following manner.
Our graph contains nodesfor all the sentences in a document set.
Weused the Clair Library (http://tangra.si.umich.edu/clair/clairlib) to split docu-ments into sentences, apply stemming, and createa cosine matrix for the stemmed sentences.
Cosinevalues are computed using TF-IDF vectors.
As inLexRank, edges in the graph correspond to text sim-ilarity.
To create a sparse graph, we use the cosinethreshold value of 0.1 obtained in (Erkan and Radev,2004).
Specifically, the edge weight between sen-tence vectors si and sj is defined aswij ={1 if s?i sj?si???sj?
> 0.10 otherwise.
(6)The second input for GRASSHOPPER is an initialranking distribution, which we derive from the po-sition of each sentence in its originating document.Position forms the basis for lead-based summaries(i.e., using the first N sentences as the summary)and leads to very competitive summaries (Brandowet al, 1995).
We form an initial ranking for eachsentence by computing p?
?, where p is the positionof the sentence in its document, and ?
is a posi-tive parameter trained on a development dataset.
Wethen normalize over all sentences in all documentsto form a valid distribution r ?
p??
that gives highprobability to sentences closer to the beginning ofdocuments.
With a larger ?, the probability assignedto later sentences decays more rapidly.To evaluate GRASSHOPPER, we experimented withDUC datasets.
We train our parameters (?
and ?
)using the DUC 2003 Task 2 data.
This dataset con-tains 30 document sets, each with an average of 10documents about a news event.
We test GRASSHOP-PER?s performance on the DUC 2004 Task 2, Tasks4a and 4b data.
DUC 2004 Task 2 has 50 documentsets of 10 documents each.
Tasks 4a and 4b exploredcross-lingual summarization.
These datasets consistof Arabic-to-English translations of news stories.The documents in Task 4a are machine-translated,while Task 4b?s are manually-translated.
Note thatwe handle the translated documents in exactly thesame manner as the English documents.We evaluate our results using the standard textsummarization metric ROUGE (http://www.isi.edu/?cyl/ROUGE/).
This is a recall-basedmeasure of text co-occurrence between a machine-generated summary and model summaries manuallycreated by judges.
ROUGE metrics exist based onbigram, trigram, and 4-gram overlap, but ROUGE-1(based on unigram matching) has been found to cor-relate best with human judgments (Lin and Hovy,2003).101Using the DUC 2003 training data, we tuned ?and ?
on a small grid (?
?
{0.125, 0.25, 0.5, 1.0};?
?
{0.0, 0.0625, 0.125, 0.25, 0.5, 0.95}).
Specifi-cally, for each of the 30 DUC 2003 Task 2 documentsets, we computed ROUGE-1 scores comparing ourgenerated summary to 4 model summaries.
We av-eraged the resulting ROUGE-1 scores across all 30sets to produce a single average ROUGE-1 score toassess a particular parameter configuration.
Afterexamining the results for all 24 configurations, weselected the best one: ?
= 0.25 and ?
= 0.5.Table 1 presents our results using these parame-ter values to generate summaries for the three DUC2004 datasets.
Note that the averages listed are ac-tually averages over 4 model summaries per set, andover all the sets.
Following the standard DUC pro-tocol, we list the confidence intervals calculated byROUGE using a bootstrapping technique.
The fi-nal column compares our results to the official sys-tems that participated in the DUC 2004 evaluation.GRASSHOPPER is highly competitive in these textsummarization tasks: in particular it ranks betweenthe 1st and 2nd automatic systems on 2004 Task 2.The lower performance in Task 4a is potentially dueto the documents being machine-translated.
If theycontain poorly translated sentences, graph edgesbased on cosine similarity could be less meaning-ful.
For such a task, more advanced text processingis probably required.3.2 Social Network AnalysisAs another application of GRASSHOPPER, we iden-tify the nodes in a social network that are the mostprominent, and at the same time maximally coverthe network.
A node?s prominence comes from itsintrinsic stature, as well as the prominence of thenodes it touches.
However, to ensure that the top-ranked nodes are representative of the larger graphstructure, it is important to make sure the results arenot dominated by a small group of highly prominentnodes who are closely linked to one another.
This re-quirement makes GRASSHOPPER a useful algorithmfor this task.We created a dataset from the Internet MovieDatabase (IMDb) that consists of all comedy moviesproduced between 2000 and 2006, and have receivedmore than 500 votes by IMDb users.
This results in1027 movies.
We form a social network of actors byco-star relationship.
Not surprisingly, actors fromthe United States dominate our dataset, although atotal of 30 distinct countries are represented.
Weseek an actor ranking such that the top actors areprominent.
However, we also want the top actors tobe diverse, so they represent comedians from aroundthe world.This problem is framed as a GRASSHOPPER rank-ing problem.
For each movie, we considered onlythe main stars, i.e., the first five cast members, whotend to be the most important.
The resulting list con-tains 3452 unique actors.
We formed a social net-work where the nodes are the actors, and undirectedweighted edges connect actors who have appeared ina movie together.
The edge weights are equal to thenumber of movies from our dataset in which bothactors were main stars.
Actors are also given a self-edge with weight 1.
The co-star graph is given toGRASSHOPPER as an input.
For the prior actor rank-ing, we simply let r be proportional to the numberof movies in our dataset in which an actor has ap-peared.
We set the weight ?
= 0.95.
It is importantto note that no country information is ever given toGRASSHOPPER.We use two measurements, ?country coverage?and ?movie coverage?, to study the diversity andprominence of the ranking produced by GRASSHOP-PER.
We compare GRASSHOPPER to two baselines:ranking based solely on the number of movies an ac-tor has appeared in, MOVIECOUNT, and a randomlygenerated ranking, RANDOM.First, we calculate ?country coverage?
as the num-ber of different countries represented by the top k ac-tors, for all k values.
Each actor represents a singlecountry?the country that the actor has appeared inthe most.
We hypothesize that actors are more likelyto have co-star connections to actors within the samecountry, so our social network may have, to someextent, a clustering structure by country.
?Countrycoverage?
approximates the number of clusters rep-resented at different ranks.Figure 3(a) shows that country coverage growsmuch more rapidly for GRASSHOPPER than forMOVIECOUNT.
That is, we see more comedians fromaround the world ranked highly by GRASSHOPPER.In contrast, the top ranks of MOVIECOUNT are dom-inated by US actors, due to the relative abundanceof US movies on IMDb.
Many other countries are102Number of Average GRASSHOPPERDataset Doc.
Sets ROUGE-1 95% C.I.
Unofficial RankDUC 2004 Task 2 50 0.3755 [0.3622, 0.3888] Between 1 & 2 of 34DUC 2004 Task 4a 24 0.3785 [0.3613, 0.3958] Between 5 & 6 of 11DUC 2004 Task 4b 24 0.4067 [0.3883, 0.4251] Between 2 & 3 of 11Table 1: Text summarization results on DUC 2004 datasets.
GRASSHOPPER was configured using parameterstuned on the DUC 2003 Task 2 dataset.
The rightmost column lists what our rank would have been if wehad participated in the DUC 2004 evaluation.not represented until further down in the rankedlist.
This demonstrates that GRASSHOPPER ranking issuccessful in returning a more diverse ranking.
Be-cause of the absorbing states in GRASSHOPPER, thefirst few highly ranked US actors encourage the se-lection of actors from other regions of the co-stargraph, which roughly correspond to different coun-tries.
RANDOM achieves even higher country cover-age initially, but is quickly surpassed by GRASSHOP-PER.
The initial high coverage comes from the ran-dom selection of actors.
However these randomlyselected actors are often not prominent, as we shownext.Second, we calculate ?movie coverage?
as the to-tal number of unique movies the top k actors arein.
We expect that actors who have been in moremovies are more prominent.
This is reasonable be-cause we count an actor in a movie only if the actoris among the top five actors from that movie.
Ourcounts thus exclude actors who had only small rolesin numerous movies.
Therefore high movie cov-erage roughly corresponds to ranking more promi-nent actors highly.
It is worth noting that this mea-sure also partially accounts for diversity, since anactor whose movies completely overlap with thoseof higher-ranked actors contributes nothing to moviecoverage (i.e., his/her movies are already covered byhigher-ranked actors).Figure 3(b) shows that the movie cover-age of GRASSHOPPER grows more rapidly thanMOVIECOUNT, and much more rapidly than RAN-DOM.
The results show that, while the RANDOMranking is diverse, it is not of high quality be-cause it fails to include many prominent actors inits high ranks.
This is to be expected of a ran-dom ranking.
Since the vast majority of the ac-tors appear in only one movie, the movie cover-age curve is roughly linear in the number of ac-tors.
By ranking more prominent actors highly, theGRASSHOPPER and MOVIECOUNT movie coveragecurves grow faster.
Many of the US actors highlyranked by MOVIECOUNT are co-stars of one an-other, so GRASSHOPPER outperforms MOVIECOUNTin terms of movie coverage too.We inspect the GRASSHOPPER ranking, and findthe top 5 actors to be Ben Stiller, Anthony Anderson,Johnny Knoxville, Eddie Murphy and Adam San-dler.
GRASSHOPPER also brings many countries, andmajor stars from those countries, into the high ranks.Examples include Mads Mikkelsen (?synonym tothe great success the Danish film industry has had?
),Cem Yilmaz (?famous Turkish comedy actor, cari-caturist and scenarist?
), Jun Ji-Hyun (?face of SouthKorean cinema?
), Tadanobu Asano (?Japan?s an-swer to Johnny Depp?
), Aamir Khan (?prominentBollywood film actor?
), and so on3.
These actorsare ranked significantly lower by MOVIECOUNT.These results indicate that GRASSHOPPERachieves both prominence and diversity in rankingactors in the IMDb co-star graph.4 ConclusionsGRASSHOPPER ranking provides a unified approachfor achieving both diversity and centrality.
We haveshown its effectiveness in text summarization andsocial network analysis.
As future work, one direc-tion is ?partial absorption,?
where at each absorbingstate the random walk has an escape probability tocontinue the random walk instead of being absorbed.Tuning the escape probability creates a continuumbetween PageRank (if the walk always escapes) andGRASSHOPPER (if always absorbed).
In addition, wewill explore the issue of parameter learning, and3Quotes from IMDb and Wikipedia.1030 100 200 300 400 500051015202530k (number of actors)Number of countriescoveredGRASSHOPPERMOVIECOUNTRANDOM0 100 200 300 400 50001002003004005006007008009001000k (number of actors)Number of moviescoveredGRASSHOPPERMOVIECOUNTRANDOM(a) Country coverage (b) Movie coverageFigure 3: (a) Country coverage at ranks up to 500, showing that GRASSHOPPER and RANDOM rankings aremore diverse than MOVIECOUNT.
(b) Movie coverage at ranks up to 500, showing that GRASSHOPPER andMOVIECOUNT have more prominent actors than RANDOM.
Overall, GRASSHOPPER is the best.user feedback (e.g., ?This item should be rankedhigher.?).
We also plan to apply GRASSHOPPER to avariety of tasks, including information retrieval (forexample ranking news articles on the same event asin Google News, where many newspapers might usethe same report and thus result in a lack of diversity),image collection summarization, and social networkanalysis for national security and business intelli-gence.Acknowledgment We thank Mark Craven and the anony-mous reviewers for helpful comments.
This work is supportedin part by Wisconsin Alumni Research Foundation (WARF) andNLM training grant 5T15LM07359.ReferencesR.
Brandow, K. Mitze, and Lisa F. Rau.
1995.
Automatic con-densation of electronic publications by sentence selection.Inf.
Process.
Manage., 31(5):675?685.Jaime Carbonell and Jade Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documents and pro-ducing summaries.
In SIGIR?98.P.G.
Doyle and J.L.
Snell.
1984.
Random Walks and ElectricNetworks.
Mathematical Assoc.
of America.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
LexRank: Graph-based centrality as salience in text summarization.
Journalof Artificial Intelligence Research.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and MarkKantrowitz.
2000.
Multi-document summarization by sen-tence extraction.
In NAACL-ANLP 2000 Workshop on Auto-matic summarization, pages 40?48.Geoffrey R. Grimmett and David R. Stirzaker.
2001.
Proba-bility and Random Processes.
Oxford Science Publications,third edition.Marti A. Hearst and Jan O. Pedersen.
1996.
Reexaminingthe cluster hypothesis: Scatter/gather on retrieval results.
InSIGIR-96.Oren Kurland and Lillian Lee.
2005.
PageRank without hyper-links: Structural re-ranking using links induced by languagemodels.
In SIGIR?05.Anton Leuski.
2001.
Evaluating document clustering for inter-active information retrieval.
In CIKM?01.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalua-tion of summaries using n-gram co-occurrence statistics.
InNAACL?03, pages 71?78.Xiaoyong Liu and W. Bruce Croft.
2004.
Cluster-based re-trieval using language models.
In SIGIR?04.Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bringingorder into texts.
In EMNLP?04.Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Wino-grad.
1998.
The PageRank citation ranking: Bringing orderto the web.
Technical report, Stanford Digital Library Tech-nologies Project.Bo Pang and Lillian Lee.
2004.
A sentimental education: Sen-timent analysis using subjectivity summarization based onminimum cuts.
In ACL, pages 271?278.W.H.
Press, S.A. Teukolsky, W.T.
Vetterling, and B.P.
Flannery.1992.
Numerical recipes in C: the art of scientific comput-ing.
Cambridge University Press New York, NY, USA.Dragomir Radev.
2000.
A common theory of information fu-sion from multiple text sources, step one: Cross-documentstructure.
In Proceedings of the 1st ACL SIGDIAL Workshopon Discourse and Dialogue.ChengXiang Zhai, William W. Cohen, and John Lafferty.
2003.Beyond independent relevance: Methods and evaluationmetrics for subtopic retrieval.
In SIGIR?03.Yi Zhang, Jamie Callan, and Thomas Minka.
2002.
Noveltyand redundancy detection in adaptive filtering.
In SIGIR?02.Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Weiguo Fan,Zheng Chen, and Wei-Ying Ma.
2005.
Improving websearch results using affinity graph.
In SIGIR?05.104
