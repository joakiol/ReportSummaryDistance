A Salience-Based Approach to Gesture-Speech AlignmentJacob Eisenstein and C. Mario ChristoudiasMIT Computer Science and Artificial Intelligence Laboratory32 Vassar StreetCambridge, MA 02139{jacobe+cmch}@csail.mit.eduAbstractOne of the first steps towards understandingnatural multimodal language is aligning ges-ture and speech, so that the appropriate ges-tures ground referential pronouns in the speech.This paper presents a novel technique forgesture-speech alignment, inspired by salience-based approaches to anaphoric pronoun reso-lution.
We use a hybrid between data-drivenand knowledge-based mtehods: the basic struc-ture is derived from a set of rules about gesturesalience, but the salience weights themselvesare learned from a corpus.
Our system achieves95% recall and precision on a corpus of tran-scriptions of unconstrained multimodal mono-logues, significantly outperforming a competi-tive baseline.1 IntroductionIn face to face communication, speakers frequently usegesture to supplement speech (Chovil, 1992), using theadditional modality to provide unique, non-redundant in-formation (McNeill, 1992).
In the context of pen/speechuser interfaces, Oviatt finds that ?multimodal ... languageis briefer, syntactically simpler, and less disfluent thanusers?
unimodal speech.?
(Oviatt, 1999)One of the simplest and most direct ways in which ges-ture can supplement verbal communication is by ground-ing references, usually through deixis.
For example, it isimpossible to extract the semantic content of the verbalutterance ?I?ll take this one?
without an accompanyingpointing gesture indicating the thing that is desired.
Theproblem of gesture-speech alignment involves choosingthe appropriate gesture to ground each verbal utterance.This paper describes a novel technique for this problem.We evaluate our system on a corpus of multimodal mono-logues with no fixed grammar or vocabulary.1.1 Example[This]_1 thing goes over [here]_2 sothat it goes back ...--------------------1.
Deictic: Hand rests on latch mechanism2.
Iconic: Hand draws trajectory fromright to leftIn this example, there are three verbal references.
Theword ?this?
refers to the latch mechanism, which is indi-cated by the rest position of the hand.
?Here?
refers to theendpoint of the trajectory indicated by the iconic gesture.?It?
is an anaphoric reference to a noun phrase definedearlier in the sentence; there is no accompanying gesture.The word ?that?
does not act as a reference, although itcould in other cases.
Not every pronoun keyword (e.g.,this, here, it, that, etc.)
will act as a reference in all cases.In addition, there will be many gestures that do not re-solve any keyword.2 Related WorkThis research draws mainly from two streams of re-lated work.
Researchers in human-computer interactionhave worked towards developing multimodal user inter-faces, which allow spoken and gestural input.
These sys-tems often feature powerful algorithms for fusing modal-ities; however, they also restrict communication to shortgrammatically-constrained commands over a very lim-ited vocabulary.
Since our goal is to handle more com-plex linguistic phenomena, these systems were of littlehelp in the design of our algorithm.
Conversely, we foundthat the problem of anaphora resolution faces a very sim-ilar set of challenges as gesture-speech alignment.
Wewere able to apply techniques from anaphora resolutionto gesture-speech alignment.2.1 Multimodal User InterfacesDiscussion of multimodal user interfaces begins with theseminal ?Put-That-There?
system (Bolt, 1980), which al-lowed users to issue natural language commands and usedeictic hand gestures to resolve references from speech.Commands were subject to a strict grammar and align-ment was straightforward: keywords created holes in thesemantic frame, and temporally-aligned gestures filledthe holes.More recent systems have extended this approachsomewhat.
Johnston and Bangalore describe a multi-modal parsing algorithm that is built using a 3-tape, finitestate transducer (FST) (Johnston and Bangalore, 2000).The speech and gestures of each multimodal utteranceare provided as input to an FST whose output is a se-mantic representation conveying the combined meaning.A similar system, based on a graph-matching algorithm,is described in (Chai et al, 2004).
These systems per-form mutual disambiguation, where each modality helpsto correct errors in the others.
However, both approachesrestrict users to a predefined grammar and lexicon, andrely heavily on having a complete, formal ontology ofthe domain.In (Kettebekov et al, 2002), a co-occurrence modelrelates the salient prosodic features of the speech (pitchvariation and pause) to characteristic features of gestic-ulation (velocity and acceleration).
The goal was to im-prove performance of gesture recognition, rather than toaddress the problem of alignment directly.
Their ap-proach also differs from ours in that they operate at thelevel of speech signals, rather than recognized words.Potentially, the two approaches could compliment eachother in a unified system.2.2 Anaphora ResolutionAnaphora resolution involves linking an anaphor to itscorresponding antecedent in the same or previous sen-tence.
In many cases, speech/gesture multimodal fusionworks in a very similar way, with gestures groundingsome of the same anaphoric pronouns (e.g., ?this?, ?that?,?here?
).One approach to anaphora resolution is to assign asalience value to each noun phrase that is a candidatefor acting as a grounding referent, and then to choosethe noun phrase with the greatest salience (Lappin andLeass, 1994).
Mitkov showed that a salience-based ap-proach can be applied across genres and without com-plex syntactic, semantic, and discourse analysis (Mitkov,1998).
Salience values are typically computed by apply-ing linguistic knowledge; e.g., recent noun phrases aremore salient, gender and number should agree, etc.
Thisknowledge is applied to derive a salience value throughthe application of a set of predefined salience weightson each feature.
Salience weights may be defined byhand, as in (Lappin and Leass, 1994), or learned fromdata (Mitkov et al, 2002).Anaphora resolution and gesture-speech alignment arevery similar problems.
Both involve resolving ambigu-ous words which reference other parts of the utterance.In the case of anaphora resolution, pronomial referencesresolve to previously uttered noun phrases; in gesture-speech alignment, keywords are resolved by gestures,which usually precede the keyword.
The salience-basedapproach works for anaphora resolution because the fac-tors that contribute to noun-phrase salience are well un-derstood.
We define a parallel set of factors for evaluatingthe salience of gestures.3 Our ApproachThe most important goal of our system is the ability tohandle natural, human-to-human language usage.
Thisincludes disfluencies and grammatically incorrect utter-ances, which become even more problematic when con-sidering that the output of speech recognizers is far fromperfect.
Any approach that requires significant parsingor other grammatical analysis may be ill-suited to meetthese goals.Instead, we identify keywords that are likely to requiregestural referents for resolution.
Our goal is to producean alignment ?
a set of bindings ?
that match at least someof the identified keywords with one or more gestures.There are several things that are known to contribute tothe salience of candidate gesture-speech bindings:?
The relevant gesture is usually close in time to thekeyword (Oviatt et al, 1997; Cohen et al, 2002)?
The gesture usually precedes the keyword (Oviatt etal., 1997).?
A one-to-one mapping is preferred.
Multiple key-words rarely align with a single gesture, and mul-tiple gestures almost never align with a single key-word (Eisenstein and Davis, 2003).?
Some types of gestures, such as deictic pointing ges-tures, are more likely to take part in keyword bind-ings.
Other gestures (i.e., beats) do not carry thistype of semantic content, and instead act to moder-ate turn taking or indicate emphasis.
These gesturesare unlikely to take part in keyword bindings (Cas-sell, 1998).?
Some keyword/gesture combinations may be partic-ularly likely; for example, the keyword ?this?
and adeictic pointing gesture.These rules mirror the salience weighting features em-ployed by the anaphora resolution methods described inthe previous section.
We define a parameterizable penaltyfunction that prefers alignments that adhere to as many ofthese rules as possible.
Given a set of verbal utterancesand gestures, we then try to find the set of bindings withthe minimal penalty.
This is essentially an optimizationapproach, and we use the simplest possible optimizationtechnique: greedy hill-climbing.
Of course, given a setof penalties and the appropriate representation, any op-timization technique could be applied.
In the evaluationsection, we discuss whether and how much our systemwould benefit from using a more sophisticated optimiza-tion technique.
Later in this section, we formalize theproblem and our proposed solution.3.1 Leveraging Empirical DataOne of the advantages of the salience-based approach isthat it enables the creation of a hybrid system that ben-efits both from our intuitions about multimodal commu-nication and from a corpus of annotated data.
The formof the salience metric, and the choice of features that fac-tor into it, is governed by our knowledge about the wayspeech and gesture work.
However, the penalty func-tion also requires parameters that weigh the importanceof each factor.
These parameters can be crafted by handif no corpus is available, but they can also be learned fromdata.
By using knowledge about multimodal language toderive the form and features of the salience metric, andusing a corpus to fine-tune the parameters of this metric,we can leverage the strengths of both knowledge-basedand data-driven approaches.4 FormalizationWe define a multimodal transcript M to consist of a setof spoken utterances S and gestures G. S contains a setof references R that must be ground by a gestural ref-erent.
We define a binding, b ?
B, as a tuple relatinga gesture, g ?
G, to a corresponding speech reference,r ?
R. Provided G and R, the set B enumerates allpossible bindings between them.
Formally, each gesture,reference, and binding are defined asg = ?tgs , tge , Gtype?r = ?trs, tre, w?b = ?g, r?
(1)where ts, te describe the start and ending time of a gestureor reference, w ?
S is the word corresponding to r, andGtype is the type of gesture (e.g.
deictic or trajectory).An alternative, useful description of the set B is as thefunction b(g) which returns for each gesture a set of cor-responding references.
This function is defined asb(g) = {r|?g, r?
?
B} (2)4.1 RulesIn this section we provide the analytical form for thepenalty functions of Section 3.
We have designed thesefunctions to penalize bindings that violate the preferencesthat model our intuitions about the relationship betweenspeech and gesture.
We begin by presenting the analyticalform for the binding penalty function, ?b.It is most often the case that verbal references closelyfollow the gestures that they refer to; the verbal referencerarely precedes the gesture.
To reflect this knowledge,we parameterize ?b using a time gap penalty, ?tg, and awrong order penalty, ?wo as follows,?b(b) = ?tgwtg(b) + ?wowwo(b) (3)where,wwo(b) ={0 trs ?
tgs1 trs < tgsand wtg = |trs ?
tgs |In addition to temporal agreement, specific words orparts-of-speech have varying affinities for different typesof gestures.
We incorporate these penalties into ?b by in-troducing a binding agreement penalty, ?
(b), as follows:?b(b) = ?tgwwo(b) + ?
(b) (4)The remaining penalty functions model binding fertil-ity.
Specifically, we assign a penalty for each unassignedgesture and reference, ?g(g) and ?r(r) respectively, thatreflect our desire for the algorithm to produce bindings.Certain gesture types (e.g., deictics) are much more likelyto participate in bindings than others (e.g., beats).
Anunassigned gesture penalty is associated with each ges-ture type, given by ?g(g).
Similarly, we expect refer-ences to have a likelihood of being bound that is condi-tioned on their word or part-of-speech tag.
However, wecurrently handle all keywords in the same way, with aconstant penalty ?r(r) for unassigned keywords.4.2 Minimization AlgorithmGivenG andR we wish to find aB?
?
B that minimizesthe penalty function ?(B,G,R):B?
= arg minB??
(B?,G,R) (5)Using the penalty functions of Section 4.1 ?
(B?,G,R) isdefined as,?
(B?,G,R) =?b?B?
?b(b) + ?g(Ga) + ?r(Ra) (6)whereGa = {g|b(g) = ?
}Ra = {r|b(r) = ?
}.Although there are numerous optimization techniquesthat may be applied to minimize Equation 5, we havechosen to implement a naive gradient decent algorithmpresented below as Algorithm 1.
Observing the prob-lem, note we could have initialized B?
= B; in otherAlgorithm 1 Gradient DescentInitialize B?
= ?
and B?
= BrepeatLet b0 be the first element in B?
?max = ?
(B?, G,R)?
?
({B?, b0}, G,R)bmax = b0for all b ?
B?, b 6= b0 do?
= ?
(B?, G,R)?
?
({B?, b}, G,R)if ?
> ?max thenbmax = b?max = ?end ifend forif ?max > 0 thenB?
= {B?, bmax}B?
= B?
?
bmaxend ifConvergence test: is ?max < limit?until convergencewords, start off with all possible bindings, and gradu-ally prune away the bad ones.
But it seems likely that|B?| ?
min(|R|, |G|); thus, starting from the empty setwill converge faster.
The time complexity of this algo-rithm is given by O(|B?||B|).
Since |B| = |G||R|, andassuming |B?| ?
|G| ?
|R|, this simplifies to O(|B?|3),cubic in the number of bindings returned.4.3 Learning ParametersWe explored a number of different techniques for find-ing the parameters of the penalty function: setting themby hand, gradient descent, simulated annealing, and a ge-netic algorithm.
A detailed comparison of the results witheach approach is beyond the scope of this paper, but thegenetic algorithm outperformed the other approaches inboth accuracy and rate of convergence.The genome representation consisted of a thirteen bitstring for each penalty parameter; three bits were usedfor the exponent, and the remaining ten were used forthe base.
Parameters were allowed to vary from 10?4to 103.
Since there were eleven parameters, the overallstring length was 143.
A population size of 200 was used,and training proceeded for 50 generations.
Single-pointcrossover was applied at a rate of 90%, and the mutationrate was set to 3% per bit.
Tournament selection was usedrather than straightforward fitness-based selection (Gold-berg, 1989).5 EvaluationWe evaluated our system by testing its performanceon a set of 26 transcriptions of unconstrained human-to-human communication, from nine different speak-Baseline Training TestRecall 84.2% 94.6% 95.1%?
n/a 1.2% 5.1%Precision 82.8% 94.5% 94.5%?
n/a 1.2% 5.0%Table 1: Performance of our system versus a baselineers (Eisenstein and Davis, 2003).
Of the four women andfive men who participated, eight were right-handed, andone was a non-native English speaker.
The participantsranged in age from 22 to 28.
All had extensive computerexperience, but none had any experience in the task do-main, which required explaining the behavior of simplemechanical devices.The participants were presented with three conditions,each of which involved describing the operation of a me-chanical device based on a computer simulation.
Theconditions were shown in order of increasing complexity,as measured by the number of moving parts: a latchbox, apiston, and a pinball machine.
Monologues ranged in du-ration from 15 to 90 seconds; the number of gestures usedranged from six to 58.
In total, 574 gesture phrases weretranscribed, of which 239 participated in gesture-speechbindings.In explaining the devices, the participants were al-lowed ?
but not instructed ?
to refer to a predrawn di-agram that corresponded to the simulation.
Vocabulary,grammar, and gesture were not constrained in any way.The monologues were videotaped, transcribed, and an-notated by hand.
No gesture or speech recognition wasperformed.
The decision to use transcriptions rather thanspeech and gesture recognizers will be discussed in detailbelow.5.1 Empirical ResultsWe averaged results over ten experiments, in which 20%of the data was selected randomly and held out as a testset.
Entire transcripts were held out, rather than parts ofeach transcript.
This was necessary because the systemconsiders the entire transcript holistically when choosingan alignment.For a baseline, we evaluated the performance of choos-ing the temporally closest gesture to each keyword.While simplistic, this approach is used in several imple-mented multimodal user interfaces (Bolt, 1980; Koonset al, 1993).
Kettebekov and Sharma even reported that93.7% of gesture phrases were ?temporally aligned?
withthe semantically associated keyword in their corpus (Ket-tebekov and Sharma, 2001).
Our results with this base-line were somewhat lower, for reasons discussed below.Table 1 shows the results of our system and the base-line on our corpus.
Our system significantly outperformsthe baseline on both recall and precision on this corpus(p < 0.05, two-tailed).
Precision and recall differ slightlybecause there are keywords that do not bind to any ges-ture.
Our system does not assume a one-to-one mappingbetween keywords and gestures, and will refuse to bindsome keywords if there is no gesture with a high enoughsalience.
One benefit of our penalty-based approach isthat it allows us to easily trade off between recall andprecision.
Reducing the penalties for unassigned ges-tures and keywords will cause the system to create feweralignments, increasing precision and decreasing recall.This could be useful in a system where mistaken ges-ture/speech alignments are particularly undesirable.
Byincreasing these same penalties, the opposite effect canalso be achieved.Both systems perform worse on longer monologues.On the top quartile of monologues by length (measuredin number of keywords), the recall of the baseline systemfalls to 75%, and the recall of our system falls to 90%.For the baseline system, we found a correlation of -0.55(df = 23, p < 0.01) between F-measure and monologuelength.This may help to explain why Kettebekov and Sharmafound such success with the baseline algorithm.
The mul-timodal utterances in their corpus consisted of relativelyshort commands.
The longer monologues in our corpustended to be more grammatically complex and includedmore disfluency.
Consequently, alignment was more dif-ficult, and a relatively na?
?ve strategy, such as the baselinealgorithm, was less effective.6 DiscussionTo our knowledge, very few multimodal understandingsystems have been evaluated using natural, unconstrainedspeech and gesture.
One exception is (Quek et al, 2002),which describes a system that extracts discourse struc-ture from gesture on a corpus of unconstrained human-to-human communication; however, no quantitative anal-ysis is provided.
Of the systems that are more relevantto the specific problem of gesture-speech alignment (Co-hen et al, 1997; Johnston and Bangalore, 2000; Kette-bekov and Sharma, 2001), evaluation is always conductedfrom an HCI perspective, in which participants act asusers of a computer system and communicate in short,grammatically-constrained multimodal commands.
Asshown in Section 5.1, such commands are significantlyeasier to align than the natural multimodal communica-tion found in our corpus.6.1 The CorpusA number of considerations went into gathering this cor-pus.1 One of our goals was to minimize the use ofdiscourse-related ?beat?
gestures, so as to better focus onthe deictic and iconic gestures that are more closely re-lated to the content of the speech; that is why we focusedon monologues rather than dialogues.
We also wanted thecorpus to be relevant to the HCI community.
That is whywe provided a diagram to gesture at, which we believeserves a similar function to a computer display, providingreference points for deictic gestures.
We used a predrawndiagram ?
rather than letting participants draw the dia-gram themselves ?
because interleaved speech, gesture,and sketching is a much more complicated problem, tobe addressed only after bimodal speech-gesture commu-nication is better understood.For a number of reasons, we decided to focus on tran-scriptions of speech and gesture, rather than using speechand gesture recognition systems.
Foremost is that wewanted the language in our corpus to be as natural as pos-sible; in particular, we wanted to avoid restricting speak-ers to a finite list of gestures.
Building a recognizer thatcould handle such unconstrained gesture would be a sub-stantial undertaking and an important research contribu-tion in its own right.
However, we are sensitive to the con-cern that our system should scale to handle possibly erro-neous recognition data.
There are three relevant classes oferrors that our system may need to handle: speech recog-nition, gesture recognition, and gesture segmentation.?
Speech Recognition ErrorsThe speech recognizer could fail to recognize a key-word; in this case, a binding would simply not becreated.
If the speech recognizer misrecognizeda non-keyword as a keyword, a spurious bindingmight be created.
However, since our system doesnot require that all keywords have bindings, we feelthat our approach is likely to degrade gracefully inthe face of this type of error.?
Gesture Recognition ErrorsThis type of error would imply a gestural misclas-sification, e.g., classifying a deictic pointing gestureas an iconic.
Again, we feel that a salience-basedsystem will degrade gracefully with this type of er-ror, since there are no hard requirements on the typeof gesture for forming a binding.
In contrast, a sys-tem that required, say, a deictic gesture to accom-pany a certain type of command would be very sen-sitive to a gesture misclassification.1We also considered using the recently released FORM2corpus from the Linguistic Data Consortium.
However, thiscorpus is presently more focused on the kinematics of hand andupper body movement, rather than on higher-level linguistic in-formation relating to gestures and speech.?
Gesture Segmentation ErrorsGesture segmentation errors are probably the mostdangerous, since this could involve incorrectlygrouping two separate gestures into a single gesture,or vice versa.
It seems that this type of error wouldbe problematic for any approach, and we have noreason to believe that our salience-based approachwould fare differently from any other approach.6.2 Success CasesOur system outperformed the baseline by more than 10%.There were several types of phenomena that the base-line failed to handle.
In this corpus, each gesture pre-cedes the semantically-associated keyword 85% of thetime.
Guided by this fact, we first created a baseline sys-tem that selected the nearest preceding gesture for eachkeyword; clearly, the maximum performance for such abaseline is 85%.
Slightly better results were achieved bysimply choosing the nearest gesture regardless of whetherit precedes the keyword; this is the baseline shown in Ta-ble 1.
However, this baseline incorrectly bound severalcataphoric gestures.
The best strategy is to accept just afew cataphoric gestures in unusual circumstances, but ana?
?ve baseline approach is unable to do this.Most of the other baseline errors came about whenthe mapping from gesture to speech was not one-to-one.For example, in the utterance ?this piece here,?
the twokeywords actually refer to a single deictic gesture.
Inthe salience-based approach, the two keywords were cor-rectly bound to a single gesture, but the baseline insistedon finding two gestures.
The baseline similarly mishan-dled situations where a keyword was used without refer-ring to any gesture.6.3 Failure CasesAlthough the recall and precision of our system neared95%, investigating the causes of error could suggestpotential improvements.
We were particularly interestedin errors on the training set, where overfitting could notbe blamed.
This section describes two sources of error,and suggests some potential improvements.6.3.1 DisfluenciesWe adopted a keyword-based approach so that our sys-tem would be more robust to disfluency than alternativeapproaches that depended on parsing.
While we wereable to handle many instances of disfluent speech, wefound that disfluencies occasionally disturbed the usualrelationship between gesture and speech.
For example,consider the following utterance:It has this... this spinning thing...Our system attempted to bind gestures to each occur-rence of ?this?, and ended up binding each reference to adifferent gesture.
Moreover, both references were boundincorrectly.
The relevant gesture in this case occurs afterboth references.
This is an uncommon phenomenon, andas such, was penalized highly.
However, anecdotallyit appears that the presence of a disfluency makes thisphenomenon more likely.
A disfluency is frequentlyaccompanied by an abortive gesture, followed by thefull gesture occurring somewhat later than the spokenreference.
It is possible that a system that could detectdisfluency in the speech transcript could account for thisphenomenon.6.3.2 Greedy SearchOur system applies a greedy hill-climbing opti-mization to minimize the penalty.
While this greedyoptimization performs surprisingly well, we were ableto identify a few cases of errors that were caused by thegreedy nature of our optimization, e.g....once it hits this, this thing is blocked.In this example, the two references are right next toeach other.
The relevant gestures are also very near eachother.
The ideal bindings are shown in Figure 1a.
Theearlier ?this?
is considered first, but from the system?sperspective, the best possible binding is the second ges-ture, since it overlaps almost completely with the spokenutterance (Figure 1b).
However, once the second gestureis bound to the first reference, it is removed from the listof unassigned gestures.
Thus, if the second gesture werealso bound to the second utterance, the penalty wouldstill be relatively high.
Even though the earlier gestureis farther away from the second reference, it is still on thelist of unassigned gestures, and the system can reduce theoverall penalty considerably by binding it.
The systemends up crisscrossing, and binding the earlier gesture tothe later reference, and vice versa (Figure 1c).7 Future WorkThe errors discussed in the previous section suggest somepotential improvements to our system.
In this section, wedescribe four possible avenues of future work: dynamicprogramming, deeper syntactic analysis, other anaphoraresolution techniques, and user adaptation.7.1 Dynamic ProgrammingAlgorithm 1 provides only an approximate solution toEquation 5.
As demonstrated in Section 6.3.2, the greedychoice is not always optimal.
Using dynamic program-ming, an exhaustive search of the space of bindings canbe performed within polynomial time.
(a) (b) (c)Figure 1: The greedy binding problem.
(a) The correct binding, (b) the greedy binding, (c) the result.We define m[i, j] to be the penalty of the optimal sub-set B?
?
{bi, ..., bj} ?
B, i ?
j. m[i, j] is implementedas a k ?
k lookup table, where k = |B| = |G||R|.
Eachentry of this table is recursively defined by preceding ta-ble entries.
Specifically, m[i, j] is computed by perform-ing exhaustive search on its subsets of bindings.
Usingthis lookup table, an optimal solution to Equation 5 istherefore found as ?
(B?, G,R) = m[1, k].
Again as-suming |B?| ?
|G| ?
|R|, the size of the lookup table isgiven by O(|B?|4).
Thus, it is possible to find the glob-ally optimal set of bindings, by moving from an O(n3)algorithm to O(n4).
The precise definition of a recur-rence relation for m[i, j] and a proof of correctness willbe described in a future publication.7.2 Syntactic AnalysisOne obvious possibility for improvement would be to in-clude more sophisticated syntactic information beyondkeyword spotting.
However, we require that our systemremain robust to disfluency and recognition errors.
Partof speech tagging is a robust method of syntactic anal-ysis which could allow us to refine the penalty functiondepending on the usage case.
Consider that there at leastthree relevant uses of the keyword ?this.?1.
This movie is better than A.I.2.
This is the bicycle ridden by E.T.3.
The wheel moves like this.When ?this?
is followed by a noun (case 1), a deic-tic gesture is likely, although not strictly necessary.
Butwhen ?this?
is followed by a verb (case 2), a deicticgesture is usually crucial for understanding the sentence.Thus, the penalty for not assigning this keyword shouldbe very high.
Finally, in the third case, when the keywordfollows a preposition, a trajectory gesture is more likely,and the penalty for any such binding should be lowered.7.3 Other Anaphora Resolution TechniquesWe have based this research on salience values, whichis just one of several possible alternative approaches toanaphora resolution.
One such alternative is the use ofconstraints: rules that eliminate candidates from the listof possible antecedents (Rich and Luperfoy, 1988).
Anexample of a constraint in anaphora resolution is a rulerequiring the elimination of all candidates that disagreein gender or number with the referential pronoun.
Con-straints may be used in combination with a salience met-ric, to prune away unlikely choices before searching.The advantage is that enforcing constraints could be sub-stantially less computationally expensive than searchingthrough the space of all possible bindings for the one withthe highest salience.
One possible future project would beto develop a set of constraints for speech-gesture align-ment, and investigate the effect of these constraints onboth accuracy and speed.Ge, Hale, and Charniak propose a data-driven ap-proach to anaphora resolution (Ge et al, 1998).
For agiven pronoun, their system can compute a probabilityfor each candidate antecedent.
Their approach of seek-ing to maximize this probability is similar to the salience-maximizing approach that we have described.
However,instead of using a parametric salience function, they learna set of conditional probability distributions directly fromthe data.
If this approach could be applied to gesture-speech alignment, it would be advantageous because thebinding probabilities could be combined with the outputof probabilistic recognizers to produce a pipeline archi-tecture, similar to that proposed in (Wu et al, 1999).
Suchan architecture would provide multimodal disambigua-tion, where the errors of each component are correctedby other components.7.4 Multimodal AdaptationSpeakers have remarkably entrenched multimodal com-munication patterns, with some users overlapping ges-ture and speech, and others using each modality sequen-tially (Oviatt et al, 1997).
Moreover, these multimodalintegration patterns do not seem to be malleable, sug-gesting that multimodal user interfaces should adapt tothe user?s tendencies.
We have already shown how theweights of the salience metric can adapt for optimal per-formance against a corpus of user data; this approachcould also be extended to adapt over time to an individualuser.8 ConclusionsThis work represents one of the first efforts at aligninggesture and speech on a corpus of natural multimodalcommunication.
Using greedy optimization and only aminimum of linguistic processing, we significantly out-perform a competitive baseline, which has actually beenimplemented in existing multimodal user interfaces.
Ourapproach is shown to be robust to spoken English, evenwith a high level of disfluency.
By blending some of thebenefits of empirical and knowledge-based approaches,our system can learn from a large corpus of data, but de-grades gracefully when limited data is available.Obviously, alignment is only one small component ofa comprehensive system for recognizing and understand-ing multimodal communication.
Putting aside the issueof gesture recognition, there is still the problem of de-riving semantic information from aligned speech-gestureunits.
The solutions to this problem will likely have to bespecially tailored to the application domain.
While ourevaluation indicates that our approach achieves what ap-pears to be a high level of accuracy, the true test will bewhether our system can actually support semantic infor-mation extraction from multimodal data.
Only the con-struction of such a comprehensive end-to-end system willreveal whether the algorithm and features that we havechosen are sufficient, or whether a more sophisticated ap-proach is required.AcknowledgementsWe thank Robert Berwick, Michael Collins, Trevor Darrell,Randall Davis, Tracy Hammond, Sanshzar Kettebekov, ?OzlemUzuner, and the anonymous reviewers for their helpful com-ments on this paper.ReferencesRichard A. Bolt.
1980.
Put-That-There: Voice and gesture atthe graphics interface.
Computer Graphics, 14(3):262?270.Justine Cassell.
1998.
A framework for gesture generation andinterpretation.
In Computer Vision in Human-Machine Inter-action, pages 191?215.
Cambridge University Press.Joyce Y. Chai, Pengyu Hong, , and Michelle X. Zhou.
2004.
Aprobabilistic approach to reference resolution in multimodaluser interfaces.
In Proceedings of 2004 International Con-ference on Intelligent User Intefaces (IUI?04), pages 70?77.Nicole Chovil.
1992.
Discourse-oriented facial displays in con-versation.
Research on Language and Social Interaction,25:163?194.Philip R. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman,I.
Smith, L. Chen, and J. Clow.
1997.
Quickset: Multimodalinteraction for distributed applications.
In ACM Multime-dia?97, pages 31?40.
ACM Press.Philip R. Cohen, Rachel Coulston, and Kelly Krout.
2002.Multimodal interaction during multiparty dialogues: Initialresults.
In IEEE Conference on Multimodal Interfaces.Jacob Eisenstein and Randall Davis.
2003.
Natural gesture indescriptive monologues.
In UIST?03 Supplemental Proceed-ings, pages 69?70.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A statisticalapproach to anaphora resolution.
In Proceedings of the SixthWorkshop on Very Large Corpora, pages 161?171.David E. Goldberg.
1989.
Genetic Algorithms in Search, Opti-mization, and Machine Learning.
Addison-Wesley.Michael Johnston and Srinivas Bangalore.
2000.
Finite-statemultimodal parsing and understanding.
In Proceedings ofCOLING-2000.
ICCL.Sanshzar Kettebekov and Rajeev Sharma.
2001.
Toward natu-ral gesture/speech control of a large display.
In Engineeringfor Human-Computer Interaction (EHCI?01).
Lecture Notesin Computer Science.
Springer Verlag.Sanshzar Kettebekov, Mohammed Yeasin, and Rajeev Sharma.2002.
Prosody based co-analysis for continuous recognitionof coverbal gestures.
In International Conference on Mul-timodal Interfaces (ICMI?02), pages 161?166, Pittsburgh,USA.David B. Koons, Carlton J. Sparrell, and Kristinn R. Thorisson.1993.
Integrating simultaneous input from speech, gaze, andhand gestures.
In Intelligent Multimedia Interfaces, pages257?276.
AAAI Press.Shalom Lappin and Herbert J. Leass.
1994.
An algorithm forpronominal anaphora resolution.
Computational Linguistics,20(4):535?561.David McNeill.
1992.
Hand and Mind.
The University ofChicago Press.Ruslan Mitkov, Richard Evans, and Constantin Ora?san.
2002.A new, fully automatic version of mitkov?s knowledge-poorpronoun resolution method.
In Intelligent Text Processingand Computational Linguistics (CICLing?02), Mexico City,Mexico, February, 17 ?
23.Ruslan Mitkov.
1998.
Robust pronoun resolution with limitedknowledge.
In COLING-ACL, pages 869?875.Sharon L. Oviatt, Antonella DeAngeli, and Karen Kuhn.
1997.Integration and synchronization of input modes during mul-timodal human-computer interaction.
In Human Factors inComputing Systems (CHI?97), pages 415?422.
ACM Press.Sharon L. Oviatt.
1999.
Ten myths of multimodal interaction.Communications of the ACM, 42(11):74?81.Francis Quek, David McNeill, Robert Bryll, Susan Duncan,Xin-Feng Ma, Cemil Kirbas, Karl E. McCullough, andRashid Ansari.
2002.
Multimodal human discourse: gestureand speech.
Transactions on Computer-Human Interaction,9(3):171?193.Elaine Rich and Susann Luperfoy.
1988.
An architecture foranaphora resolution.
In Proceedings of the Second Confer-ence on Applied Natural Language Processing (ANLP-2),pages 18?24, Texas, USA.Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen.
1999.Multimodal integration - a statistical view.
IEEE Transac-tions on Multimedia, 1(4):334?341.
