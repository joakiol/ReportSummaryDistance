Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 38?45,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsA Method for Compiling Two-level Rules with Multiple ContextsKimmo KoskenniemiUniversity of HelsinkiHelsinki, Finlandkimmo.koskenniemi@helsinki.fiMiikka SilfverbergUniversity of HelsinkiHelsinki, Finlandmiikka.silfverberg@helsinki.fiAbstractA novel method is presented for compilingtwo-level rules which have multiple contextparts.
The same method can also be appliedto the resolution of so-called right-arrow ruleconflicts.
The method makes use of the factthat one can efficiently compose sets of two-level rules with a lexicon transducer.
By in-troducing variant characters and using simplepre-processing of multi-context rules, allrules can be reduced into single-context rules.After the modified rules have been combinedwith the lexicon transducer, the variant char-acters may be reverted back to the originalsurface characters.
The proposed method ap-pears to be efficient but only partial evidenceis presented yet.1 IntroductionTwo-level rules can be compiled into length-preserving transducers whose intersection effec-tively reflects the constraints and the correspon-dences imposed by the two-level grammar.
Two-level rules relate input strings (lexical representa-tions) with output strings (surface representa-tions).
The pairs of strings are treated as charac-ter pairs x:z consisting of lexical (input) char-acters x and surface (output) characters z, andregular expressions based on such pairs.
Two-level rule transducers are made length-preserving(epsilon-free) by using a place holder zero (0)within the rules and in the representations.
Thezero is then removed after the rules have beencombined by (virtual) intersection, before theresult is composed with the lexicon.
There arefour kinds of two-level rules:1. right-arrow rules or restriction rules,(x:z => LC _ RC) saying that thecorrespondence pair is allowed only ifimmediately preceded by left context LCand followed by right context RC,2.
left-arrow rules or surface coercionrules,  (x:z <= LC _ RC) which saythat in this context, the lexical characterx may  only  correspond  to  the  surfacecharacter z,3.
double-arrow rules (<=>), a shorthandcombining these two requirements, and4.
exclusion rules (x:z /<= LC _ RC)which forbid the pair x:z  to occur inthis context.All types of rules may have more than onecontext part.
In particular, the right-arrow rulex:z => LC1 _ RC1; LC2 _ RC2 wouldsay that the pair x:z (which we call the centreof the rule) may occur in either one of these twocontexts.
For various formulations of two-levelrules, see e.g.
(Koskenniemi, 1983), (Grimley-Evans et al, 1996), (Black et.al., 1987), (Ruess-ink, 1989), (Ritchie, 1992), (Kiraz, 2001) and acomprehensive survey on their formal interpreta-tions, see (Vaillette, 2004).Compiling two-level rules into transducers iseasy in all other cases except for right-arrowrules with multiple context-parts; see e.g.Koskenniemi (1983).
Compiling right-arrowrules with multiple context parts is more difficultbecause the compilation of the whole rule is notin a simple relation to the component expressionsin the rule; see e.g.
Karttunen et al (1987).The method proposed here reduces multi-context rules into a set of separate simple rules,one for each context, by introducing some auxil-iary variant characters.
These auxiliary charac-ters are then normalized back into the originalsurface characters after the intersecting composi-tion of the lexicon and the modified rules.
Themethod is presented in section 3.
The compila-tion of multiple contexts using the proposedscheme appears to be very simple and fast.
Pre-liminary results and discussion about the compu-tational complexity are presented in section 4.381.1 The compilation task with an exampleWe make use of a simplified linguistic examplewhere a stop k is realized as v between identicalrounded close vowels (u, y).
The example re-sembles one detail of Finnish consonant grada-tion but it is grossly simplified.
According to therule in the example, the lexical representationpukun would be realized as the surface repre-sentation puvun.
This correspondence is tradi-tionally represented as:p u k u np u v u nwhere the upper tier represents the lexical ormorphophonemic representation which we inter-pret as the input, and the lower one correspondsto the surface representation which we consideras the output.
1   This two-tier representation isusually represented on a single line as a sequenceof input and output character pairs where pairs ofidentical characters, such as p:p are abbreviatedas a single p.  E.g.
the above pair  of  strings  be-comes a string of pairs:p u k:v u nIn our example we require that the correspon-dence k:v may occur only between two identi-cal rounded close vowels, i.e.
either between twoletters u or between two letters y.
Multiple con-texts are needed in the right-arrow rule whichexpresses this constraint.
As a two-level gram-mar, this would be:Alphabet a b ?
k ?
u v w ?k:v;Rulesk:v => u _ u;y _ y;This grammar would permit sequences such as:p u k:v u nk y k:v y np u k:v u k:v u nl u k:v u n k y k:v y nt u k k ubut it would exclude sequences:p u k:v y nt u k:v a n1 In Xerox terminology, the input or lexical charactersare called the upper characters, and the output or sur-face characters are called the lower characters.
Otherorientations are used by some authors.Whereas one can always express  multi-context left-arrow rules (<=) and exclusion rules(/<=)  equivalently  as  separate  rules,  this  doesnot  hold  for  right-arrow rules.
The  two  separaterulesk:v => u _ u;k:v => y _ y;would be in conflict with each other permittingno occurrences of k:v at all, (unless we applyso-called conflict resolution which would effec-tively combine the two rules back to a single rulewith two context parts).2 Previous compilation methodsThe first compiler of two-level rules was imple-mented by the first author in 1985 and it handledalso multi-context rules (Koskenniemi, 1985).The compiler used a finite-state package writtenby Ronald Kaplan and Martin Kay at XeroxPARC, and a variant of a formula they used forcompiling cascaded rewrite rules.
Their ownwork was not published until 1994.
Kosken-niemi?s compiler was re-implemented in LISP bya student in her master?s thesis (Kinnunen,1987).Compilation of two-level rules in general re-quires  some  care  because  the  centres  may  occurseveral times in pair strings, the contexts mayoverlap and the centres may act as part of a con-text for another occurrence of the same centre.For other rules than right-arrow rules, each con-text is yet another condition for excluding un-grammatical  strings  of  pairs,  which  is  how  therules are related to each other.
The context partsof a right-arrow rule are, however, permissions,one of which has to be satisfied.
Expressing un-ions of context parts was initially a problemwhich required complicated algorithms.Some of the earlier compilation methods arementioned below.
They all produce a singletransducer out of each multi-context right-arrowrule.2.1 Method based on Kaplan and KayKaplan and Kay (1994) developed a methodaround 1980 for compiling rewriting rules intofinite-state transducers 2 .
The method wasadapted by Koskenniemi to the compilation oftwo-level rules by modifying the formula2 Douglas Johnson (1972) presented a similar tech-nique earlier but his work was not well known inearly 1980s.39slightly.
In this method, auxiliary left and rightbracket characters (<1, >1, <2, >2, ...)were freely added in order to facilitate the check-ing of the context conditions.
A unique left andright bracket was dedicated for each context partof  the  rule.
For  each  context  part  of  a  rule,  se-quences with freely added brackets were thenfiltered with the context expressions so that onlysuch sequences remained where occurrences ofthe brackets were delimited with the particularleft or right context (allowing free occurrence ofbrackets for other context parts).
Thereafter, itwas easy to check that all occurrences of the cen-tre  (i.e.
the  left  hand  part  of  the  rule  before  therule operator) were delimited by some matchingpair of brackets.
As all component transducers inthis expression were length-preserving (epsilon-free), the constraints could be intersected witheach other resulting in a single rule transducerfor the multi-context rule (and finally the brack-ets could be removed).2.2 Method of Grimley-Evans, Kiraz andPulmanGrimley-Evans, Kiraz and Pulman presented asimpler compilation formula for two-level rules(1996).
The method is prepared to handle morethan two levels of representation, and it does notneed the freely added brackets in the intermedi-ate  stages.
Instead,  it  uses  a  marker  for  the rulecentre and can with it express disjunctions ofcontexts.
Subtracting such a disjunction from allstrings where the centre occurs expresses all pairstrings which violate the multi-context rule.Thus, the negation of such a transducer is thedesired result.2.3 Yli-Jyr?
?s methodYli-Jyr?
(Yli-Jyr?
et al, 2006) introduced aconcept of Generalized Restriction (GR) whereexpressions with auxiliary boundary characters made it possible to express context parts of rulesin a natural way, e.g.
as:Pi* LC  Pi  RC Pi*Here Pi is the set of feasible pairs of charactersand LC and RC are the left and right contexts.The two context parts of our example would cor-respond to the following two expressions:Pi* u  Pi  u Pi*Pi* y  Pi  y Pi*Using such expressions, it is easy to express dis-junctions of contexts as unions of the above ex-pressions.
This makes it logically simple to com-pile multi-context right-arrow rules.
The rulecentre x:z can be expressed simply as:Pi*  x:z  Pi*The right-arrow rule  can be expressed as  an im-plication where the expression for the centre im-plies the union of the context parts.
Thereafter,one may just remove the auxiliary boundarycharacters, and the result is the rule-transducer.
(It is easy to see that only one auxiliary characteris needed when the length of the centres is one.
)The compilation of rules with centres whoselength is one using the GR seems very similar tothat  of  Grimley-Evans  et  al.
The  nice  thingabout GR is that one can easily express variousrule types, including but not limited to the fourtypes listed above.2.4 Intersecting composeIt was observed somewhere around 1990 atXerox that the rule sets may be composed withthe lexicon transducers in an efficient way andthat the resulting transducer was roughly similarin size as the lexicon transducer itself (Karttunenet al, 1992).
This observation gives room to thenew approach presented below.At that time, it was not practical to intersectcomplete two-level grammars if they containedmany elaborate rules (and this is still a fairlyheavy operation).
Another useful observationwas that the intersection of the rules could bedone in a joint single operation with the compo-sition (Karttunen, 1994).
Avoiding the separateintersection made the combining of the lexiconand rules feasible and faster.
In addition toXerox LEXC program, e.g.
the HFST finite-statesoftware contains this operation and it is rou-tinely used when lexicons and two-level gram-mars are combined into lexicon transducers(Lind?n et al, 2009).M?ns Huld?n has noted (2009) that the com-posing of the lexicon and the rules is sometimesa heavy operation, but can be optimized if onefirst composes the output side of the lexicontransducer with the rules, and thereafter theoriginal lexicon with this intermediate result.3 Proposed method for compilationThe  idea  is  to  modify  the  two-level  grammar  sothat the rules become simpler.
The modifiedgrammar will contain only simple rules with sin-gle context parts.
This is done at the cost that thegrammar will transform lexical representationsinto slightly modified surface representations.40The surface representations are, however, fixedafter the rules have been combined with the lexi-con so that the resulting lexicon transducer isequivalent to the result produced using earliermethods.3.1 The method through the exampleLet us return to the example in the introduction.The modified surface representation differs fromthe ultimate representation by having a slightlyextended alphabet where some surface charactersare expressed as their variants, i.e.
there might bev1 or v2 in addition to v.  In particular, the firstvariant v1 will  be  used  exactly  where  the  firstcontext of the original multi-context rule for k:vis satisfied, and v2 where the second context issatisfied.
After extending the alphabet and split-ting  the  rule,  our  example  grammar  will  be  asfollows:Alphabet a b ?
k ?
u v w x y ?k:v1 k:v2;Rulesk:v1 => u _ u;k:v2 => y _ y;These rules would permit sequences such as:p u k:v1 u nk y k:v2 y np u k:v1 u k:v1 u nbut exclude a sequencep u k:v2 u nThe output of the modified grammar is now asrequired, except that it includes these variants v1and v2 instead of v.   If  we first  perform the in-tersecting composition of the rules and the lexi-con, we then can compose the result with a trivialtransducer which simply transforms both v1 andv2 into v.It  should  be  noted  that  here  the  context  ex-pressions of these example rules do not contain von the output side, and therefore the introductionof the variants v1 and v2 causes no furthercomplications.
In  the  general  case,  the  variantsshould be added as alternatives of v in  the  con-text expressions, see the explanation below.3.2 More general casesThe strategy is to pre-process the two-levelgrammar in steps by splitting more complex con-structions into simpler ones until we have unitswhose components are trivial to compile.
Theintersection of the components will have the de-sired effect when composed with a lexicon and atrivial correction module.
Assume, for the timebeing, that all centres (i.e.
the left-hand parts) ofthe rules are of length one.
(1) Split double-arrow (<=>) rules into oneright-arrow (=>)  rule  and  one  left-arrow  (<=)rule with centres and context parts identical tothose of the original double-arrow rule.
(2) Unfold the iterative where clauses in left-arrow rules by establishing a separate left-arrowrule for each value of the iterator variable, e.g.V:Vb <= [a | o | u] ?
* _;where V in (A O U)Vb in (a o u) matched;becomesA:a <= [a | o | u] ?
* _;O:o <= [a | o | u] ?
* _;U:u <= [a | o | u] ?
* _;Unfold the where clauses in right-arrow rulesin  either  of  the  two  ways:  (a)  If  the  whereclauses create disjoint centres (as above), thenestablish a separate right-arrow rule for eachvalue  of  the  variable,  and  (b)  if  the  clause  doesnot affect the centre, then create a single multi-context right-arrow rule whose contexts consistof the context parts of the original rule by replac-ing the where clause variable by its values, onevalue at a time, e.g.k:v => Vu _ Vu; where Vu in (u y);becomesk:v => u _ u;y _ y;If  there  are  set  symbols  or  disjunctions  in  thecentres  of  a  right-arrow  rule,  then  split  the  ruleinto separate rules where each rule has just a sin-gle pair as its centre, and the context part is iden-tical to the context part (after the unfolding of thewhere clauses).Note that these two first steps would probablybe common to any method of compiling multi-context rules.
After these two steps, we haveright-arrow, left-arrow and exclusion rules.
Theright-arrow  rules  have  single  pairs  as  their  cen-tres.
(3) Identify the right-arrow rules which, afterthe unfolding, have multiple contexts, and recordeach  pair  which  is  the  centre  of  such  a  rule.Suppose that the output character (i.e.
the surfacecharacter) of such a rule is z and there are n con-text parts in the rule, then create n new auxiliarycharacters z1, z2, ..., zn  and denote the set consist-ing of them by S(z).41Split the rule into n distinct single-contextright-arrow rules by replacing the z of the centreby each zi in turn.Our simple example rule becomes now.k:v1 => u _ u;k:v2 => y _ y;(4) When all rules have been split according tothe above steps, we need a post-processing phasefor the whole grammar.
We have to extend thealphabet by adding the new auxiliary charactersin it.
If original surface characters (which nowhave variants) were referred to in the rules, eachsuch reference must be replaced with the unionof the original character and its variants.
Thisreplacement has to be done throughout thegrammar.
For any existing pairs x:z listed in thealphabet, we add there also the pairs x:z1, ..., x:zn.The same is done for all declarations of setswhere z occurs  (as  an output  character).
Insert  adeclaration for a new character set correspondingto S(z).
In  all  define  clauses  and  in  all  rule-context expressions where z occurs as an outputcharacter,  it  is  replaced  by  the  set  S(z).
In  allcentres of left-arrow rules where z occurs  as  theoutput character, it is replaced by S(z).The  purpose  of  this  step  is  just  to  make  themodified two-level grammar consistent in termsof its alphabet, and to make the modified rulestreat  the occurrence of  any of  the output  charac-ters z1,  z2,  ?, zn in the same way as the originalrule treated z wherever it occurred in its contexts.After this pre-processing we only have right-arrow, left-arrow and exclusion rules with a sin-gle context part.
All rules are independent ofeach  other  in  such  a  way  that  their  intersectionwould have the effect we wish the grammar tohave.
Thus, we may compile the rule set as suchand  each  of  these  simple  rules  separately.
Anyof the existing compilation formulas will do.After compiling the individual rules, they haveto be intersected and composed with the lexicontransducer which transforms base forms and in-flectional feature symbols into the morphopho-nemic representation of the word-forms.
Thecomposing and intersecting is efficiently done asa single operation because it then avoids the pos-sible explosion which can occur if intermediateresult of the intersection is computed in full.The rules are mostly independent of eachother, capable of recurring freely.
Thereforesomething near the worst case complexity islikely  to  occur,  i.e.
the  size  of  the  intersectionwould have many states, roughly proportional tothe  product  of  the  numbers  of  the  states  in  theindividual rule transducers.The composition of the lexicon and the logicalintersection of the modified rules is almost iden-tical  to  the  composition  of  the  lexicon  and  thelogical intersection of the original rules.
The onlydifference is that the output (i.e.
the surface) rep-resentation contains some auxiliary characters ziinstead of the original surface characters z. Asimple transducer will correct this.
(The trans-ducer has just one (final) state and identity transi-tions for all original surface characters and a re-duction zi:z for  each of  the auxiliary characters.
)This composition with the correcting transducercan be made only after the rules have been com-bined with the lexicon.3.3 Right-arrow conflictsRight-arrow rules are often considered as per-missions.
A  rule  could  be  interpreted  as  ?thiscorrespondence pair may occur if the followingcontext condition is met?.
Further permissionsmight  be  stated  in  other  rules.
As  a  whole,  anyoccurrence must get at least one permission inorder to be allowed.The right-arrow conflict resolution schemepresented by Karttunen implemented thisthrough an extensive pre-processing where theconflicts were first detected and then resolved(Karttunen et al, 1987).
The resolution was doneby copying context parts among the rules in con-flict.
Thus, what was compiled was a grammarwith rules extended with copies of context partsfrom other rules.The scenario outlined above could be slightlymodified in order to implement the simple right-arrow rule  conflict  resolution  in  a  way  which  isequivalent to the solution presented by Kart-tunen.
All  that  is  needed is  that  one would firstsplit the right-arrow rules with multiple contextparts into separate rules.
Only after that, onewould consider all right-arrow rules and recordrules  with identical  centres.
For  groups of  ruleswith identical centres, one would introduce thefurther variants of the surface characters, a sepa-rate  variant  for  each  rule.
In  this  scheme,  theconflict resolution of right-arrow rules is imple-mented fairly naturally in a way analogous to thehandling of multi-context rules.3.4 Note on longer centres in rulesIn the above discussion, the left-hand parts ofrules,  i.e.
their  centres,  were  always  of  lengthone.
In fact, one may define rules with longercentres by a scheme which reduces them into42rules with length one centres.
It appears that thebasic rule types (the left and right-arrow rules)with longer centres can be expressed in terms oflength one centres, if we apply conflict resolutionfor the right-arrow rules.We replace a right-arrow rule, e.g.x1:z1 x2:z2 ... xk:zk => LC _ RC;with k separate rulesx1:z1 => LC _ x2:z2 ... xk:zk RC;x2:z2 => LC x1:z1 _ ... xk:zk RC;...xk:zk => LC x1:z1 x2:z2 ... _ RC;Effectively, each input character may be realizedaccording  to  the  original  rule  only  if  the  rest  ofthe  centre  will  also  be  realized  according  to  theoriginal rule.Respectively, we replace a left-arrow rule, e.g.x1:z1 x2:z2 ... xk:zk <= LC _ RC;with k separate rulesx1:z1 <= LC _ x2: ... xk: RC;x2:z2 <= LC x1: _ ... xk: RC;...xk:zk <= LC x1: x2: ... _ RC;Here the realization of the surface string is forcedfor each of its character of the centre separately,without reference to what happens to other char-acters  in  the  centre.
(Otherwise  the  contexts  ofthe separate rules would be too restrictive, andallow the default realization as well.
)4 Complexity and implementationIn order to implement the proposed method, onecould write a pre-processor which just transformsthe  grammar  into  the  simplified  form,  and  thenuse an existing two-level compiler.
Alternatively,one could modify an existing compiler, or write anew compiler which would be somewhat simplerthan the existing ones.
We have not implementedthe proposed method yet, but rather simulated theeffects using existing two-level rule compilers.Because the pre-processing would be very fastanyway, we decided to estimate the efficiency ofthe proposed method through compiling hand-modified rules with the existing HFST-TWOLC(Lind?n et al, 2009) and Xerox TWOLC3 two-3 We used an old version 3.4.10 (2.17.7) which wethought would make use of the Kaplan and Kay for-mula.
We suspected that the most recent versionsmight have gone over to the GR formula.level rule compilers.
The HFST tools are built ontop of existing open source finite-state packagesOpenFST (Allauzen et al, 2007) and HelmutSchmid?s SFST (2005).It appears that all normal morphographemictwo-level grammars can be compiled with themethods of Kaplan and Kay, Grimley-Evans andYli-Jyr?.Initial tests of the proposed scheme are prom-ising.
The  compilation  speed  was  tested  with  agrammar of consisting of 12 rules including onemulti-context rule for Finnish consonant grada-tion with some 8 contexts and a full Finnish lexi-con.
When the multi-context rule was split intoseparate rules, the compilation was somewhatfaster (12.4 sec) to than when the rule was com-piled a multi-context rule using the GR formula(13.9 sec).
The gain in the speed by splitting waslost at the additional work needed in the inter-secting compose of the rules and the full lexiconand the final fixing of the variants.
On the whole,the proposed method had no advantage over theGR method.In  order  to  see  how  the  number  of  contextparts affects the compilation speed, we madetests with an extreme grammar simulating Dutchhyphenation rules.
The hyphenation logic wastaken out of TeX hyphenation patterns which hadbeen converted into two-level rules.
The firstgrammar consisted of a single two-level rulewhich had some 3700 context parts.
This gram-mar could not be compiled using Xerox TWOLCwhich applies the Kaplan and Kay method be-cause more than 5 days on a dedicated Linuxmachine with 64 GB core memory was notenough for completing the computation.
Whenusing  of  GR  method  of  HFST-TWOLC,  thecompilation time was not a problem (34 min-utes).
The method of Grimley-Evans et alwould probably have been equally feasible.Compiling the grammar after splitting it intoseparate rules as proposed above was also feasi-ble: about one hour with Xerox TWOLC andabout 20 hours with HFST-TWOLC.
The differ-ence between these two implementations de-pends most likely on the way they handle alpha-bets.
The Xerox tool makes use of a so-called'other' symbol which stands for characters notmentioned in the rule.
It also optimizes the com-putation by using equivalence classes of charac-ter pairs.
These make the compilation less sensi-tive to the 3700 new symbols added to the alpha-bet than what happens in the HFST routines.Another test was made using a 50 pattern sub-set of the above hyphenation grammar.
Using43the  Xerox  TWOLC,  the  subset  compiled  as  amulti-context rule in 28.4 seconds, and whensplit according to the method proposed here, itcompiled in 0.04 seconds.
Using the HFST-TWOLC, the timings were 3.1 seconds and 5.4seconds, respectively.
These results corroboratethe intuition that the Kaplan and Kay formula issensitive  to  the  number  of  context  parts  in  ruleswhereas  the  GR  formula  is  less  sensitive  to  thenumber of context parts in rules.There are factors which affect the speed ofHFST-TWOLC, including the implementationdetail including the way of treating characters orcharacter pairs which are not specifically men-tioned in a particular transducer.
We anticipatethat there is much room for improvement intreating larger alphabets in HFST internal rou-tines and there is no inherent reason why itshould be slower than the Xerox tool.
The nextrelease of HFST will use Huld?n?s FOMA finite-state package.
FOMA implements the ?other?symbol and is expected to improve the process-ing of larger alphabets.Our intuition and observation is that the pro-posed compilation phase requires linear timewith  respect  to  the  number  of  context  parts  in  arule.
Whether the proposed compilation methodhas an advantage over the compilation using theGR  or  Grimley-Evans  formula  remains  to  beseen.5 AcknowledgementsMiikka Silfverberg, a PhD student at Finnish graduateschool Langnet and the author of HFST-TWOLCcompiler.
His contribution consists of making all testsused here to estimate and compare the efficiency ofthe compilation methods.The current work is part of the FIN-CLARIN infra-structure project at the University of Helsinki fundedby the Finnish Ministry of Education.ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk,Wojciech Skut and Mehryar Mohri.
2007.OpenFst: A General and Efficient Weighted Finite-State Transducer Library.
In Implementation andApplication of Automata, Lecture Notes in Com-puter Science.
Springer, Vol.
4783/2007, 11-23.Alan Black, Graeme Ritchie, Steve Pulman, and Gra-ham Russell.
1987.
?Formalisms for morphogra-phemic description?.
In Proceedings of the ThirdConference of the European Chapter of the Asso-ciation for Computational Linguistics, 11?18.Edmund Grimley-Evans, Georg A. Kiraz, Stephen G.Pulman.
1996.
Compiling a Partition-Based Two-Level Formalism.
In COLING 1996, Volume 1:The 16th International Conference on Computa-tional Linguistics, pp.
454-459.Huld?n, M?ns.
2009.
Finite-State Machine Construc-tion Methods and Algorithms for Phonology andMorphology.
PhD Thesis, University of Arizona.Douglas C. Johnson.
1972.
Formal Aspects of Phono-logical Description.
Mouton, The Hague.Ronald M. Kaplan and Martin Kay.
1994.
RegularModels of Phonological Rule Systems.
Computa-tional Linguistics 20(3): 331?378.Lauri Karttunen.
1994.
Constructing lexical transduc-ers.
In Proceedings of the 15th conference onComputational linguistics, Volume 1. pp.
406-411.Lauri Karttunen, Ronald M. Kaplan, and Annie Zae-nen.
1992.
Two-Level Morphology with Composi-tion.
Proceedings of the 14th conference on Com-putational linguistics, August 23-28, 1992, Nantes,France.
141-148.Lauri Karttunen, Kimmo Koskenniemi, and Ronald.M.
Kaplan.
1987.
A Compiler for Two-level Pho-nological Rules.
In Dalrymple, M. et al Tools forMorphological Analysis.
Center for the Study ofLanguage and Information.
Stanford University.Palo Alto.Maarit Kinnunen.
1987.
Morfologisten s??nt?jenk?
?nt?minen ?
?rellisiksi automaateiksi.
(Translat-ing morphological rules into finite-state automata.Master?s thesis.).
Department of Computer Sci-ence, University of HelsinkiGeorge Anton Kiraz.
2001.
Computational NonlinearMorphology: With Emphasis on Semitic Lan-guages.
Studies in Natural Language Processing.Cambridge University Press, Cambridge.Kimmo Koskenniemi.
1983.
Two-Level Morph-ology: A General Computational Model forWord-form Recognition and Production.
Uni-versity of Helsinki, Department of General Lin-guistics, Publications No.
11.Kimmo Koskenniemi.
1985.
Compilation of automatafrom morphological two-level rules.
In F.
Karlsson(ed.
), Papers from the fifth Scandinavian Confer-ence of Computational Linguistics, Helsinki, De-cember 11-12, 1985. pp.
143-149.Krister Lind?n, Miikka Silfverberg and Tommi Piri-nen.
2009.
HFST Tools for Morphology ?
An Effi-cient Open-Source Package for Construction ofMorphological Analyzers.
In State of the Art inComputational Morphology (Proceedings of Work-shop on Systems and Frameworks for Computa-tional Morphology, SFCM 2009).
Springer.Graeme Ritchie.
1992.
Languages generated by two-level morphological rules?.
Computational Lin-guistics, 18(1):41?59.44H.
A. Ruessink.
1989.
Two level formalisms.
UtrechtWorking Papers in NLP.
Technical Report 5.Helmut Schmid.
2005.
A Programming Language forFinite State Transducers.
In Proceedings of the 5thInternational Workshop on Finite State Methods inNatural Language Processing (FSMNLP 2005).pp.
50-51.Nathan Vaillette.
2004.
Logical Specification of Fi-nite-State Transductions for Natural LanguageProcessing.
PhD Thesis, Ohio State University.Anssi Yli-Jyr?
and Kimmo Koskenniemi.
2006.Compiling Generalized Two-Level Rules andGrammars.
International Conference on NLP: Ad-vances in natural language processing.
Springer.174 ?
185.45
