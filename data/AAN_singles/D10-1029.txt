Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 293?303,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsPredicting the Semantic Compositionality of Prefix VerbsShane Bergsma, Aditya Bhargava, Hua He, Grzegorz KondrakDepartment of Computing ScienceUniversity of Alberta{bergsma,abhargava,hhe,kondrak}@cs.ualberta.caAbstractIn many applications, replacing a complexword form by its stem can reduce sparsity, re-vealing connections in the data that would nototherwise be apparent.
In this paper, we focuson prefix verbs: verbs formed by adding a pre-fix to an existing verb stem.
A prefix verb isconsidered compositional if it can be decom-posed into a semantically equivalent expres-sion involving its stem.
We develop a clas-sifier to predict compositionality via a rangeof lexical and distributional features, includ-ing novel features derived from web-scale N-gram data.
Results on a new annotated cor-pus show that prefix verb compositionality canbe predicted with high accuracy.
Our systemalso performs well when trained and tested onconventional morphological segmentations ofprefix verbs.1 IntroductionMany verbs are formed by adding prefixes to exist-ing verbs.
For example, remarry is composed of aprefix, re-, and a stem, marry.
We present an ap-proach to predicting the compositionality of prefixverbs.
The verb remarry is compositional; it meansto marry again.
On the other hand, retire is gener-ally non-compositional; it rarely means to tire again.There is a continuum of compositionality in prefixverbs, as in other complex word forms and multi-word expressions (Bannard et al, 2003; Creutz andLagus, 2005; Fazly et al, 2009; Xu et al, 2009).We adopt a definition of compositionality specifi-cally designed to support downstream applicationsthat might benefit from knowledge of verb stems.For example, suppose our corpus contains the fol-lowing sentence: ?Pope Clement VII denied HenryVIII permission to marry again before a decisionwas given in Rome.?
A user might submit the ques-tion, ?Which pope refused Henry VIII permission toremarry??
If we can determine that the meaning ofremarry could also be provided via the stem marry,we could add marry to our search terms.
This isknown as morphological query expansion (Bilotti etal., 2004).
Here, such an expansion leads to a bettermatch between question and answer.Previous work has shown that ?full morpholog-ical analysis provides at most very modest bene-fits for retrieval?
(Manning et al, 2008).
Stem-ming, lemmatization, and compound-splitting oftenincrease recall at the expense of precision, but theresults depend on the morphological complexity ofthe text?s language (Hollink et al, 2004).The lack of success in applying morphologicalanalysis in IR is unsurprising given that most pre-vious systems are not designed with applicationsin mind.
For example, the objective of the influ-ential Linguistica program is ?to produce an out-put that matches as closely as possible the analy-sis that would be given by a human morphologist?
(Goldsmith, 2001).
Unsupervised systems achievethis aim by exploiting learning biases such as min-imum description length for lexicons (Goldsmith,2001; Creutz and Lagus, 2007) and high entropyacross morpheme boundaries (Keshava and Pitler,2006).
Supervised approaches learn directly fromwords annotated by morphologists (Van den Boschand Daelemans, 1999; Toutanova and Cherry, 2009),often using CELEX, a lexical database that includes293morphological information (Baayen et al, 1996).The conventional approach in morphology is tosegment words into separate morphemes even whenthe words are not entirely compositional combina-tions of their parts (Creutz and Lagus, 2005).
Forexample, while co- is considered a separate mor-pheme in the verb cooperate, the meaning of coop-erate is not simply to operate jointly.
These formsare sometimes viewed as perturbations of compo-sition (de Marken, 1996).
In practice, a user mayquery, ?Which nations do not cooperate with the In-ternational Criminal Court??
An expansion of thequery to include operate may have undesirable con-sequences.Rather than relying on conventional standards, wepresent an algorithm whose objective is to find onlythose prefix verbs that exhibit semantic composi-tionality; i.e., prefix verbs that are fully meaning-preserving, sums-of-their-parts.
We produce a newcorpus, annotated according to this definition.
Weuse these annotated examples to learn a discrimina-tive model of semantic compositionality.Our classifier relies on a variety of features thatexploit the distributional patterns of verbs and stems.We build on previous work that applies semanticsto morphology (Yarowsky and Wicentowski, 2000;Schone and Jurafsky, 2001; Baroni et al, 2002), andalso on work that exploits web-scale data for seman-tic analysis (Turney, 2001; Nakov, 2007; Kummer-feld and Curran, 2008).
For example, we measurehow often a prefix verb appears with a hyphen be-tween the prefix and stem.
We also look at the dis-tribution of the stem as a separate word: we calculatethe probability of the prefix verb and the separatedstem?s co-occurrence in a segment of discourse; wealso calculate the distributional similarity betweenthe verb and the separated stem.
High scores forthese measures indicate compositionality.
We ex-tract counts from a web-scale N-gram corpus, allow-ing us to efficiently leverage huge volumes of unla-beled text.Our system achieves 93.6% accuracy on held-outdata, well above several baselines and comparisonsystems.
We also train and test our system on con-ventional morphological segmentations.
Our clas-sifier remains reliable in this setting, making halfas many errors as the state-of-the-art unsupervisedMorfessor system (Creutz and Lagus, 2007).2 Problem Definition and SettingA prefix verb is a derived word with a bound mor-pheme as prefix.
While derivation can change boththe meaning and part-of-speech of a word (as op-posed to inflection, which does not change ?referen-tial or cognitive meaning?
(Katamba, 1993)), herethe derived form remains a verb.We define prefix-verb compositionality as a se-mantic equivalence between a verb and a paraphraseinvolving the verb?s stem.
The stem must be used asa verb in the paraphrase.
Words can be introduced,if needed, to account for the meaning contributed bythe prefix, e.g., outbuild?build more/better/fasterthan.
A bidirectional entailment between the prefixverb and the paraphrase is required.Words can have different meanings in differentcontexts.
For example, a nation might ?resort toforce,?
(non-compositional) while a computer pro-gram can ?resort a linked list?
(compositional).
Wetherefore define prefix-verb compositionality as acontext-specific property of verb tokens rather thana global property of verb types.
However, it is worthnoting that we ultimately found the compositionalityof types to be very consistent across contexts (Sec-tion 5.1.2), and we were unable to leverage contex-tual information to improve classification accuracy;our final system is essentially type-based.
Other re-cent morphological analyzers have also been type-based (Keshava and Pitler, 2006; Poon et al, 2009).Our system takes as input a verb token in unin-flected form along with its sentence as context.
Theverb must be divisible into an initial string and a fol-lowing remainder such that the initial string is onour list of prefixes and the remainder is on our list ofstems.
Hyphenation is allowed, e.g., both re-enterand reenter are acceptable inputs.
The system deter-mines whether the prefix/stem combination is com-positional in the current context.
For example, theverb unionize in, ?The workers must unionize,?
canbe divided into a prefix un- and a stem ionize.
Thesystem should determine that here unionize is not acompositional combination of these parts.The algorithm requires a list of prefixes and stemsin a given language.
For our experiments, we useboth dictionary and corpus-based methods to con-struct these lists (Section 4).2943 Supervised Compositionality DetectionWe use a variety of lexical and statistical informa-tion when deciding whether a prefix verb is compo-sitional.
We adopt a discriminative approach.
Weassume some labeled examples are available to traina classifier.
Relevant information is encoded in afeature vector, and a learning algorithm determinesa set of weights for the features using the trainingdata.
As compositionality is a binary decision, wecan adopt any standard package for binary classifi-cation.
In our experiments we use support vectormachines.Our features include both local information thatdepends only on the verb string (sometimes referredto as lexical features) and also global informationthat depends on the verb and the stem?s distributionin text.
Our approach can therefore be regarded as asimple form of semi-supervised learning; we lever-age both a small number of labeled examples and alarge volume of unlabeled text.If a frequency or similarity is undefined in our cor-pus, we indicate this with a separate feature; weightson these features act as a kind of smoothing.3.1 Features based on Web-Scale N-gram DataWe use web-scale N-gram data to extract distribu-tional features.
The most widely-used N-gram cor-pus is the Google 5-gram Corpus (Brants and Franz,2006).
We use Google V2: a new N-gram corpus(also with N-grams of length one-to-five) createdfrom the same one-trillion-word snapshot of the webas the Google 5-gram Corpus, but with enhanced fil-tering and processing of the source text (Lin et al,2010).
For Google V2, the source text was also part-of-speech tagged, and the resulting part-of-speechtag distribution is included for each N-gram.
Thereare 4.1 billion N-grams in the corpus.The part-of-speech tag distributions are particu-larly useful, as they allow us to collect verb-specificcounts.
For example, while a string like reuse oc-curs 1.1 million times in the web corpus, it is onlytagged as a verb 270 thousand times.
Conflating thenoun/verb senses can lead to misleading scores forcertain features.
E.g., the hyphenation frequencyof re-use would appear relatively low, even thoughreuse is semantically compositional.Lin et al (2010) also provide a high-coverage,10-million-phrase set of clusters extracted from theN-grams; we use these for our similarity features(Section 3.1.3).
There are 1000 clusters in total.The data does not provide the context vectors foreach phrase; rather, each phrase is listed with its 20most similar clusters, measured by cosine similar-ity with the cluster centroid.
We use these centroidsimilarities as values in a 1000-dimensional cluster-membership feature space.
To calculate the similar-ity between two verbs, we calculate the cosine simi-larity between their cluster-membership vectors.The feature classes in the following four subsec-tions each make use of web-scale N-gram data.3.1.1 HYPH featuresHyphenated verbs are usually compositional (e.g.,re-elect).
Of course, a particular instance of a com-positional verb may or may not occur in hyphenatedform.
However, across a large corpus, compositionalprefix verbs tend to occur in a hyphenated form moreoften than do non-compositional prefix verbs.
Wetherefore provide real-valued features for how oftenthe verb was hyphenated and unhyphenated on theweb.
For example, we collect counts for the fre-quencies of re-elect (33K) and reelect (9K) in ourweb corpus, and we convert the frequencies to log-counts.
We also give real-valued features for the hy-phenated/unhyphenated log-counts using only thoseoccurrences of the verb that were tagged as a verb,exploiting the tag distributions in our web corpus asdescribed above.Nakov and Hearst (2005) previously used hy-phenation counts as an indication of a syntactic re-lationship between nouns.
In contrast, we leveragehyphenation counts as an indication of a semanticproperty of verbs.3.1.2 COOC featuresCOOC features, and also the SIM (Section 3.1.3)and YAH (Section 3.2.2) features, concern the asso-ciation in text between the prefix verb and its stem,where the stem occurs as a separate word.
We callthis the separated stem.If a prefix verb is compositional, it is more likelyto occur near its separated stem in text.
We oftensee agree and disagree, read and reread, etc.
occur-ring in the same segment of discourse.
We createfeatures for the association of the prefix verb and its295separated stem in a discourse.
We include the log-count of how often the verb and stem occur in thesame N-gram (of length 2-to-5) in our N-gram cor-pus.
Note that the 2-to-4-gram counts are not strictlya subset of the 5-gram counts, since fewer 5-gramspass the data?s minimum frequency threshold.We also include a real-valued pointwise mutualinformation (PMI) feature for the verb and separatedstem?s co-occurrence in an N-gram.
For the PMI, weregard occurrence in an N-gram as an event, and cal-culate the probability that a verb and separated stemjointly occur in an N-gram, divided by the probabil-ity of their occurring in an N-gram independently.3.1.3 SIM featuresIf a prefix verb is compositional, it should oc-cur in similar contexts to its stem.
The idea thata stem and stem+affix should be semantically sim-ilar has been exploited previously for morphologicalanalysis (Schone and Jurafsky, 2000).
We includea real-valued feature for the distributional similar-ity of the verb and stem using Lin?s thesaurus (Lin,1998).
The coverage of this measure was low: itwas non-zero for only 93 of the 1000 prefix verbs inour training set.
We therefore also include distribu-tional similarity calculated using the web-scale 10-million-phrase clustering as described above.
Us-ing this data, similarity is defined for 615 of the1000 training verbs.
We also explored a variety ofWordNet-based similarity measures, but these ulti-mately did not prove helpful on development data.3.1.4 FRQ featuresWe include real-valued features for the raw fre-quencies of the verb and the stem on the web.
Ifthese frequencies are widely different, it may in-dicate a non-compositional usage.
Yarowsky andWicentowski (2000) use similar statistics to iden-tify words related by inflection, but they gather theircounts from a much smaller corpus.
In addition,higher-frequency prefix verbs may be a priori morelikely to be non-compositional.
A certain frequencyis required for an irregular usage to become famil-iar to language speakers.
The potential correlationbetween frequency and non-compositionality couldthus also be exploited by the classifier via the FRQfeatures.3.2 Other Features3.2.1 LEX featuresWe provide lexical features for various aspectsof a prefix verb.
Binary features indicate the oc-currence of particular verbs, prefixes, and stems,and whether the prefix verb is hyphenated.
Whilehyphenated prefix verbs are usually compositional,even non-compositional prefix verbs may be hy-phenated if the prefix and stem terminate and be-gin with a vowel, respectively.
For example, non-compositional uses of co-operate are often hyphen-ated, whereas the compositional remarry is rarelyhyphenated.
We therefore have indicator featuresfor the conjunction of the prefix and the first letterof the stem (e.g., co-o), and also for the prefix con-joined with a flag indicating whether the stem beginswith a vowel (e.g., co+vowel).3.2.2 YAH featuresWhile the COOC features capture many caseswhere the verb and separated stem occur in closeproximity (especially, but not limited to, conjunc-tions), there are many other cases where a longerdistance might separate a compositional verb andits separated stem.
For example, consider the sen-tence, ?Brush the varnish on, but do not overbrush.
?Here, the verb and separated stem do not co-occurwithin a 5-gram window, and their co-occurrencewill therefore not be recorded in our N-gram cor-pus.
As an approximation for co-occurrence countswithin a longer segment of discourse, we count thenumber of pages on the web where the verb and sep-arated stem co-occur.
We use hit-counts returnedby the Yahoo search engine API.1 Similar to ourCOOC features, we include a real-valued feature forthe pointwise mutual information of the prefix verband separated stem?s co-occurrence on a web page,i.e., we use Turney?s PMI-IR (Turney, 2001).Baroni et al (2002) use similar statistics to helpdiscover morphologically-related words.
In contrastto our features, however, their counts are derivedfrom source text that is several orders of magnitudesmaller in size.1http://developer.yahoo.com/search/boss/2963.2.3 DIC featuresOne potentially useful resource, when available,is a dictionary of the conventional morphologicalsegmentations of words in the language.
Althoughthese segmentations have been created for a differ-ent objective than that of our annotations, we hy-pothesize that knowledge of morphology can helpinform our system?s predictions.
For each prefixverb, we include features for whether or not the pre-fix and stem are conventionally segmented into sep-arate morphemes, according to a morphological dic-tionary.
Similar to the count-based features, we in-clude a DIC-undefined feature for the verbs that arenot in the dictionary; any precompiled dictionarywill have imperfect coverage of actual test examples.Interestingly, DIC features are found to be amongour least useful features in the final evaluation.4 Experiments4.1 ResourcesWe use CELEX (Baayen et al, 1996) as our dictio-nary for the DIC features.
We also use CELEX to helpextract our lists of prefixes and stems.
We take ev-ery prefix that is marked in CELEX as forming a newverb by attaching to an existing verb.
For stems, weuse every verb that occurs in CELEX, but we alsoextend this list by automatically collecting a largenumber of words that were automatically tagged asverbs in the NYT section of Gigaword (Graff, 2003).To be included in the extra-verb list, a verb must oc-cur more than ten times and be tagged as a verb morethan 70% of the time by a part-of-speech tagger.
Wethereby obtain 43 prefixes and 6613 stems.2 Weaimed for an automatic, high-precision list for ourinitial experiments.
This procedure is also amenableto human intervention; one could alternatively cast awider net for possible stems and then manually filterfalse positives.4.2 Annotated DataWe carried out a medium-scale annotation to providetraining and evaluation data for our experiments.32The 43 prefixes are: a- ab- ac- ad- as- be- circum- co- col-com- con- cor- counter- cross- de- dis- e- em- en- ex- fore- im-in- inter- ir- mis- out- over- per- photo- post- pre- pro- psycho-re- sub- super- sur- tele- trans- un- under- with-3Our annotated data is publicly available at:http://www.cs.ualberta.ca/?ab31/verbcomp/The data for our annotations also comes from theNYT section of Gigaword.
We first build a list ofpossible prefix verbs.
We include any verb that a) iscomposed of a valid prefix and stem; and b) occursat least twice in the corpus.4 If the verb occurs lessthan 50 times in the corpus, we also require that itwas tagged as a verb in at least 70% of cases.
Thisresults in 2077 possible prefix verbs for annotation.For each verb type in our list of possible prefixverbs, we randomly select for annotation sentencesfrom Gigaword containing the verb.
We take at mostthree sentences for each verb type so that a few verycommon types (such as become, understand, and im-prove) do not comprise the majority of annotated ex-amples.
The resulting set of sentences includes asmall number of sentences with incorrectly-taggednon-verbs; these are simply marked as non-verbsby our annotators and excluded from our final datasets.
A graphical program was created for the an-notation; the program automatically links to the on-line Merriam-Webster dictionary entries for the pre-fix verb and separated stem.
When in doubt abouta verb?s meaning, our annotators adhere to the dic-tionary definitions.
A single annotator labeled 1718examples, indicating for each sentence whether theprefix verb was compositional.
A second annota-tor then labeled a random subset of 150 of these ex-amples, and agreement was calculated.
The annota-tors agreed on 137 of the 150 examples.
The Kappastatistic (Jurafsky and Martin, 2000, page 315), withP(E) computed from the confusion matrices, is 0.82,above the 0.80 level considered to indicate good re-liability.For our experiments, the 1718 annotated exam-ples are randomly divided into 1000 training, 359development, and 359 held-out test examples.4.3 Classifier SettingsWe train a linear support vector machine classifierusing the efficient LIBLINEAR package (Fan et al,2008).
We use L2-loss and L2-regularization.
We4We found that the majority of single-occurrence verbs inthe Gigaword data were typos.
We would expect true hapaxlegomena to be largely compositional, and we could potentiallyderive better statistics if we include them (Baayen and Sproat,1996).
One possible option, employed in previous work, is toensure words of interest are ?manually corrected for typing er-rors before further analysis?
(Baayen and Renouf, 1996).297optimize the choice of features and regularizationhyperparameter on development data, attaining amaximum when C = 0.1.4.4 EvaluationWe compare the following systems:1.
Base1: always choose compositional (the ma-jority class).2.
Base2: for each prefix, choose the majorityclass over the verbs having that prefix in train-ing data.3.
Morf: the unsupervised Morfessor sys-tem (Creutz and Lagus, 2007) (Categories-ML, from 110K-word corpus).
If Morfessorsplits the prefix and stem into separate mor-phemes, we take the prediction as composi-tional.
If it does anything else, we take it asnon-compositional.4.
SCD: Supervised Compositionality Detection:the system proposed in this paper.We evaluate using accuracy: the percentage of ex-amples classified correctly in held-out test data.5 ResultsWe first analyze our annotations, gaining insight intothe relation between our definition and conventionalsegmentations.
We also note the consistency of ourannotations across contexts.
We then provide themain results of our system.
Finally, we provide theresults of our system when trained and tested on con-ventional morphological segmentations.5.1 Analysis of Annotations5.1.1 Annotation consistency with dictionariesThe majority of our examples are not present ina morphological dictionary, even in one as compre-hensive as CELEX.
The prefix verbs are in CELEXfor only 670 of the 1718 total annotated instances.For those that are in CELEX, Table 1 providesthe confusion matrix that relates the CELEX seg-mentations to our annotations.
The table showsthat the major difference between our annotationsand CELEX is that our definition of compositionalityis more strict than conventional morphological seg-mentations.
When CELEX does not segment the pre-fix from the stem (case 0), our annotations agree inCELEX segmentation1 0Compositionality 1 227 10annotation 0 250 183Table 1: Confusion matrix on the subset of prefix verbannotations that are also in CELEX.
1 indicates that theprefix and stem are segmented into separate morphemes,0 indicates otherwise.183 of 193 cases.
When CELEX does split the prefixfrom the stem (case 1), the meaning is semanticallycompositional in less than half the cases.
This isa key difference between conventional morphologyand our semantic definition.It is also instructive to analyze the 10 cases thatare semantically compositional but which CELEXdid not segment.
Most of these are verbs that areconventionally viewed as single morphemes becausethey entered English as complete words.
For exam-ple, await comes from the Old North French await-ier, itself from waitier.
In practice, it is useful toknow that await is compositional, i.e.
that it can berephrased as wait for.
Downstream applications canexploit the compositionality of await, but miss theopportunity if using the conventional lack of seg-mentation.5.1.2 Annotation consistency across contextsWe next analyze our annotated data to determinethe consistency of compositionality across differentoccurrences of the same prefix-verb type.
There are1248 unique prefix verbs in our 1718 labeled exam-ples: 45 verbs occur three times, 380 occur twiceand 823 occur only once.
Of the 425 verbs that oc-cur multiple times, only 6 had different annotationsin different examples (i.e., six verbs occur in bothcompositional and non-compositional usages in ourdataset).
These six instances are subtle, debatable,and largely uninteresting, depending on distinctionslike whether the proclaim sense of blazon can sub-stitute for the celebrate sense of emblazon, etc.It is easy to find clearer ambiguities online,such as compositional examples of typically non-compositional verbs (how to recover a couch, whento redress a wound, etc.).
However, in our data verbslike recover and redress always occur in their moredominant non-compositional sense.
People may298Set # Base1 Base2 Morf SCDTest 359 65.7 87.2 73.8 93.6?
CELEX 128 30.5 73.4 50.8 89.8/?
CELEX 231 85.3 94.8 86.6 95.7?
train 107 69.2 93.5 74.8 97.2/?
train 252 64.3 84.5 73.4 92.1Table 2: Number of examples (#) and accuracy (%) ontest data, and on in-CELEX vs. not-in-CELEX, and in-training-data vs. not-in-training splits.consciously or unconsciously recognize the possi-bility for confusion and systematically hyphenateprefixes from the stem if a less-common composi-tional usage is employed.
For example, our data has?repress your feelings?
for the non-compositionalcase but the hyphenated ?re-press the center?
for thecompositional usage.5Due to the consistency of compositionality acrosscontexts, context-based features may simply not bevery useful for classification.
All the features we de-scribe in Section 3 depend only on the prefix verbitself and not the verb context.
Various context-dependent features did not improve accuracy on ourdevelopment data and were thus excluded from thefinal system.5.2 Main ResultsThe first row of Table 2 gives the results of allsystems on test data.
SCD achieves 93.6% ac-curacy, making one fifth as many errors as themajority-class baseline (Base1) and half as many er-rors as the more competitive prefix-based predictor(Base2).
The substantial difference between SCDand Base2 shows that SCD is exploiting much infor-mation beyond the trivial memorization of a deci-sion for each prefix.
Morfessor performs better thanBase1 but significantly worse than Base2.
This indi-cates that state-of-the-art unsupervised morpholog-ical segmentation is not yet practical for semanticpreprocessing.
Of course, Morfessor was also de-signed with a different objective; in Section 5.3 wecompare Morfessor and SCD on conventional mor-5Note that many examples like recover, repress and redressare only ambiguous in text, not in speech.
Pronunciation re-duces ambiguity in the same way that hyphens do in text.
Con-versely, observe that knowledge of compositionality could po-tentially help speech synthesis.Prefix # Tot # Comp SCDre- 166 147 95.8over- 26 25 96.2out- 23 18 91.3de- 21 0 100.0pre- 19 16 94.7un- 17 1 94.1dis- 10 0 90.0under- 9 7 77.8co- 7 6 100.0en- 5 2 60.0Table 3: Total number of examples (# Tot), number ofexamples that are compositional (# Comp), and accuracy(%) of SCD on test data, by prefix.phological segmentations.We further analyzed the systems by splitting thetest data two ways.First, we separate verbs that occur in our mor-phological dictionary (?
CELEX) from those thatdo not (/?
CELEX).
Despite using the dictionarysegmentation itself as a feature, the performanceof SCD is worse on the ?
CELEX verbs (89.8%).The comparison systems drop even more dramati-cally on this subset.
The ?
CELEX verbs comprisethe more frequent, irregular verbs in English.
Non-compositionality is the majority class on the exam-ples that are in the dictionary.On the other hand, one would expect verbs thatare not in a comprehensive dictionary to be largelycompositional, and indeed most of the /?
CELEXverbs are compositional.
However, there is stillmuch to be gained from applying SCD, which makesa third as many errors as the system which alwaysassigns compositional (95.7% for SCD vs. 85.3%for Base1).Our second way of splitting the data is to divideour test set into prefix verbs that also occurred intraining sentences (?
train) and those that did not (/?train).
Over 70% did not occur in training.
SCDscores 97.2% accuracy on those that did.
The clas-sifier is thus able to exploit the consistency of anno-tations across different contexts (Section 5.1.2).
The92.1% accuracy on the /?-train portion also showsthe features allow the system to generalize well tonew, previously-unseen verbs.Table 3 gives the results of our system on sets of299-LEX -HYPH -COOC -SIM -YAH -FRQ -DIC85.0 92.8 92.5 93.6 93.6 93.6 93.685.5 93.6 92.8 93.0 93.3 93.986.9 90.5 93.3 93.6 93.684.1 90.3 93.3 93.687.5 90.5 93.085.5 89.4Table 4: Accuracy (%) of SCD as different feature classesare removed.
Performance with all features is 93.6%.verbs divided according to their prefix.
The table in-cludes those prefixes that occurred at least 5 timesin the test set.
Note that the prefixes have a longtail: these ten prefixes cover only 303 of the 359test examples.
Accuracy is fairly high across all thedifferent prefixes.
Note also that the three prefixesde-, un-, and dis- almost always correspond to non-compositional verbs.
Each of these prefixes corre-sponds to a subtle form of negation, and it is usuallydifficult to paraphrase the negation using the stem.For example, to demilitarize does not mean to notmilitarize (or any other simple re-phrasing using thestem as a verb), and so our annotation marks it asnon-compositional.
Whether such a strict strategy isultimately best may depend on the target application.Feature AnalysisWe perform experiments to evaluate which featuresare most useful for this task.
Table 4 gives the ac-curacy of our system as different feature classes areremoved.
A similar table was previously used forfeature analysis in Daume?
III and Marcu (2005).Each row corresponds to performance with a groupof features; each entry is performance with a par-ticular feature class individually removed the group.We remove the least helpful feature class from eachgroup in succession moving group-to-group downthe rows.We first remove the DIC features.
These do notimpact performance on test data.
The last row givesthe performance with only HYPH features (85.5, re-moving LEX), and only LEX features (89.4, remov-ing HYPH).
These are found to be the two most ef-fective features for this task, followed by the COOCstatistics.
The other features, while marginally help-ful on development data, are relatively ineffective onthe test set.
In all cases, removing LEX features hurtsBase1 Base2 Morf SCD76.0 79.6 72.4 86.4Table 5: Accuracy (%) on CELEX.the most.
Removing LEX not only removes usefulstem, prefix, and hyphen information, but it also im-pairs the ability of the classifier to use the other fea-tures to separate the examples.5.3 CELEX Experiments and ResultsFinally, we train and test our system on prefix verbswhere the segmentation decisions are provided bya morphological dictionary.
We are interested inwhether the strong results of our system could trans-fer to conventional morphological segmentations.We extract all verbs in CELEX that are valid verbsfor our system (divisible into a prefix and verb stem),and take the CELEX segmentation as the label; i.e.,whether the prefix and stem are separated into dis-tinct morphemes.
We extract 1006 total verbs.We take 506 verbs for training, 250 verbs as adevelopment set (to tune our classifier?s regulariza-tion parameter) and 250 verbs as a final held-out testset.
We use the same features and classifier as inour main results, except we remove the DIC featureswhich are now the instance labels.Table 5 shows the performance of our two base-line systems along with Morfessor and SCD.
Whilethe majority-class baseline is much higher, theprefix-based baseline is 7% lower, indicating thatknowledge of prefixes, and lexical features in gen-eral, are less helpful for conventional segmentations.In fact, performance only drops 2% when we re-move the LEX features, showing that web-scale in-formation alone can enable solid performance onthis task.
Surprisingly, Morfessor performs worsehere, below both baselines and substantially belowthe supervised system.
We confirmed our Morfessorprogram was generating the same segmentations asthe online demo.
We also experimented with Lin-guistica (Goldsmith, 2001), training on a large cor-pus, but results were worse than with Morfessor.Accurate segmentation of prefix verbs is clearlypart of the mandate of these systems; prefix verbsegmentation is simply a very challenging task.
Un-like other, less-ambiguous tasks in morphology, aprefix/stem segmentation is plausible for all of our300input verbs, since the putative morphemes are bydefinition valid morphemes in the language.Overall, the results confirm and extend previousstudies that show semantic information is helpful inmorphology (Schone and Jurafsky, 2000; Yarowskyand Wicentowski, 2000).
However, we reiterate thatoptimizing systems according to conventional mor-phology may not be optimal for downstream ap-plications.
Furthermore, accuracy is substantiallylower in this setting than in our main results.
Target-ing conventional segmentations may be both morechallenging and less useful than focusing on seman-tic compositionality.6 Related WorkThere is a large body of work on morphologicalanalysis of English, but most of this work does nothandle prefixes.
Porter?s stemmer is a well-knownsuffix-stripping algorithm (Porter, 1980), whilepublicly-available lemmatizers like morpha (Min-nen et al, 2001) and PC-KIMMO (Karp et al, 1992)only process inflectional morphology.
FreeLing (At-serias et al, 2006) comes with a few simple rulesfor deterministically stripping prefixes in some lan-guages, but not English (e.g., only semi- and re- canbe stripped when analyzing OOV Spanish verbs).A number of modern morphological analyzers usesupervised machine learning.
These systems couldall potentially benefit from the novel distributionalfeatures used in our model.
Van den Bosch andDaelemans (1999) use memory-based learning toanalyze Dutch.
Wicentowski (2004)?s supervisedWordFrame model includes a prefixation compo-nent.
Results are presented on over 30 languages.Erjavec and Dz?eroski (2004) present a supervisedlemmatizer for Slovene.
Dreyer et al (2008) per-form supervised lemmatization on Basque, English,Irish and Tagalog; like us they include results whenthe set of lemmas is given.
Toutanova and Cherry(2009) present a discriminative lemmatizer for En-glish, Bulgarian, Czech and Slovene, but only han-dle suffix morphology.
Poon et al (2009) present anunsupervised segmenter, but one that is based on alog-linear model that can include arbitrary and in-terdependent features of the type proposed in ourwork.
We see potential in combining the best el-ements of both approaches to obtain a system thatdoes not need annotated training data, but can makeuse of powerful web-scale features.Our approach follows previous systems for mor-phological analysis that leverage semantic as wellas orthographic information (Yarowsky and Wicen-towski, 2000; Schone and Jurafsky, 2001; Baroni etal., 2002).
Similar problems also arise in core se-mantics, such as how to detect the compositionalityof multi-word expressions (Lin, 1999; Baldwin etal., 2003; Fazly et al, 2009).
Our problem is sim-ilar to the analysis of verb-particle constructions orVPCs (e.g., round up, sell off, etc.)
(Bannard et al,2003).
Web-scale data can be used for a variety ofproblems in semantics (Lin et al, 2010), includingclassifying VPCs (Kummerfeld and Curran, 2008).We motivated our work by describing applicationsin information retrieval, and here Google is clearlythe elephant in the room.
It is widely reported thatGoogle has been using stemming since 2003; for ex-ample, a search today for Porter stemming returnspages describing the Porter stemmer, and the re-turned snippets have words like stemming, stem-mer, and stem in bold text.
Google can of coursedevelop high-quality lists of morphological variantsby paying attention to how users reformulate theirqueries.
User query sessions have previously beenused to expand queries using similar terms, such assubstituting feline for cat (Jones et al, 2006).
Weshow that high-quality, IR-friendly stemming is pos-sible even without query data.
Furthermore, querydata could be combined with our other features forhighly discriminative word stemming in context.Beyond information retrieval, suffix-based stem-ming and lemmatization have been used in a rangeof NLP applications, including text categorization,textual entailment, and statistical machine transla-tion.
We believe accurate prefix-stripping can alsohave an impact in these areas.7 Conclusions and Future WorkWe presented a system for predicting the semanticcompositionality of prefix verbs.
We proposed anew, well-defined and practical definition of compo-sitionality, and we annotated a corpus of sentencesaccording to this definition.
We trained a discrimina-tive model to predict compositionality using a rangeof lexical and web-scale statistical features.
Novel301features include measures of the frequency of prefix-stem hyphenation, and statistics for the likelihood ofthe verb and stem co-occurring as separate words inan N-gram.
The classifier is highly accurate across arange of prefixes, correctly predicting composition-ality for 93.6% of examples.Our preliminary results provide strong motiva-tion for investigating and applying new distribu-tional features in the prediction of both conventionalmorphology and in task-directed semantic composi-tionality.
Our techniques could be used on a varietyof other complex word forms.
In particular, manyof our features extend naturally to identifying stem-stem compounds (like panfry or healthcare).
Also, itwould be possible for our system to handle inflectedforms by first converting them to their lemmas us-ing a morphological analyzer.
We could also jointlylearn the compositionality of words across their in-flections, along the lines of Yarowsky and Wicen-towski (2000).There are also other N-gram-derived features thatwarrant further investigation.
One source of in-formation that has not previously been exploited isthe ?lexical fixedness?
(Fazly et al, 2009) of non-compositional prefix verbs.
If prefix verbs are rarelyrephrased in another form, they are likely to be non-compositional.
For example, in our N-gram data,the count of quest again is relatively low comparedto the count of request, indicating request is non-compositional.
On the other hand, marry again isrelatively frequent, indicating that remarry is com-positional.
Incorporation of these and other N-gramcounts could further improve classification accuracy.ReferencesJordi Atserias, Bernardino Casas, Elisabet Comelles,Meritxell Gonza?lez, Llu?
?s Padro?, and Muntsa Padro?.2006.
FreeLing 1.3: Syntactic and semantic servicesin an open-source NLP library.
In LREC.R.
Harald Baayen and Antoinette Renouf.
1996.
Chron-icling the Times: Productive lexical innovations in anEnglish newspaper.
Language, 72(1).Harald Baayen and Richard Sproat.
1996.
Estimatinglexical priors for low-frequency morphologically am-biguous forms.
Comput.
Linguist., 22(2):155?166.R.
Harald Baayen, Richard Piepenbrock, and LeonGulikers.
1996.
The CELEX2 lexical database.LDC96L14.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In ACL 2003Workshop on Multiword Expressions.Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics of verb-particles.
In ACL 2003 Workshop on Multiword Ex-pressions.Marco Baroni, Johannes Matiasek, and Harald Trost.2002.
Unsupervised discovery of morphologically re-lated words based on orthographic and semantic sim-ilarity.
In ACL-02 Workshop on Morphological andPhonological Learning (SIGPHON), pages 48?57.Matthew W. Bilotti, Boris Katz, and Jimmy Lin.
2004.What works better for question answering: Stemmingor morphological query expansion?
In InformationRetrieval for Question Answering (IR4QA) Workshopat SIGIR 2004.Thorsten Brants and Alex Franz.
2006.
The Google Web1T 5-gram Corpus Version 1.1.
LDC2006T13.Mathias Creutz and Krista Lagus.
2005.
Inducingthe morphological lexicon of a natural language fromunannotated text.
In International and Interdisci-plinary Conference on Adaptive Knowledge Represen-tation and Reasoning.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Trans.
Speech Lang.
Process., 4(1):1?34.Hal Daume?
III and Daniel Marcu.
2005.
A large-scaleexploration of effective global features for a joint en-tity detection and tracking model.
In HLT-EMNLP.Carl de Marken.
1996.
Linguistic structure as composi-tion and perturbation.
In ACL.Markus Dreyer, Jason Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In EMNLP.Tomaz?
Erjavec and Sas?o Dz?eroski.
2004.
Machine learn-ing of morphosyntactic structure: Lemmatising un-known Slovene words.
Applied Artificial Intelligence,18:17?41.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
JMLR, 9:1871?1874.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identification ofidiomatic expressions.
Comput.
Linguist., 35(1):61?103.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
Comput.
Linguist.,27(2):153?198.David Graff.
2003.
English Gigaword.
LDC2003T05.302Vera Hollink, Jaap Kamps, Christof Monz, and Maartende Rijke.
2004.
Monolingual document retrieval forEuropean languages.
IR, 7(1):33?52.Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substitutions.
InWWW.Daniel Jurafsky and James H. Martin.
2000.
Speech andlanguage processing.
Prentice Hall.Daniel Karp, Yves Schabes, Martin Zaidel, and DaniaEgedi.
1992.
A freely available wide coverage mor-phological analyzer for English.
In COLING.Francis Katamba.
1993.
Morphology.
MacMillan Press.Samarth Keshava and Emily Pitler.
2006.
A simpler, in-tuitive approach to morpheme induction.
In 2nd Pas-cal Challenges Workshop.Jonathan K. Kummerfeld and James R. Curran.
2008.Classification of verb particle constructions with theGoogle Web1T Corpus.
In Australasian LanguageTechnology Association Workshop.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scaleN-grams.
In LREC.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In COLING-ACL.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In ACL.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat.Lang.
Eng., 7(3):207?223.Preslav Nakov and Marti Hearst.
2005.
Search en-gine statistics beyond the n-gram: Application to nouncompound bracketing.
In CoNLL.Preslav Ivanov Nakov.
2007.
Using the Web as an Im-plicit Training Set: Application to Noun CompoundSyntax and Semantics.
Ph.D. thesis, University of Cal-ifornia, Berkeley.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In HLT-NAACL.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3).Patrick Schone and Daniel Jurafsky.
2000.
Knowledge-free induction of morphology using latent semanticanalysis.
In LLL/CoNLL.Patrick Schone and Daniel Jurafsky.
2001.
Knowledge-free induction of inflectional morphologies.
InNAACL.Kristina Toutanova and Colin Cherry.
2009.
A globalmodel for joint lemmatization and part-of-speech pre-diction.
In ACL-IJCNLP.Peter D. Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In European Confer-ence on Machine Learning.Antal Van den Bosch and Walter Daelemans.
1999.Memory-based morphological analysis.
In ACL.Richard Wicentowski.
2004.
Multilingual noise-robustsupervised morphological analysis using the word-frame model.
In ACL SIGPHON.Ying Xu, Christoph Ringlstetter, and Randy Goebel.2009.
A continuum-based approach for tightness anal-ysis of Chinese semantic units.
In PACLIC.David Yarowsky and Richard Wicentowski.
2000.
Min-imally supervised morphological analysis by multi-modal alignment.
In ACL.303
