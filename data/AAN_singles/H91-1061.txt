Evaluating Text Categorization IDavid D. LewisComputer  and  In fo rmat ion  Science Dept .Un ivers i ty  of  Massachuset tsAmhers t ,  MA 01003ABSTRACTWhile certain standard procedures are widely used for evalu-ating text retrieval systems and algorithms, the sarne is not truefor text categorization.
Omission of important data from reportsis common and methods of measuring effectiveness vary widely.This \]'ms m~de judging the relative merits of techniques for textcategorization dif~.cult and has disguised important research is-sues .In this paper I discuss a variety of ways of evaluating theeffectiveness of text categorization systems, drawing both on re-ported categorization experiments and on methods used in eval-uating query-driven retrieval.
I also consider the extent o whichthe same evaluation methods may be used with systems for textextraction, a more complex task.
In evaluatlng either kind ofsystem, the purpose for which the output is to be used is crucialin choosing appropriate valuation methods.INTRODUCTIONText classification systems, i.e.
systems which can makedistinctions between meaningful classes of texts, have beenwidely studied in information retrieval and natural  anguageprocessing.
The major i ty  of information retrieval researchhas been devoted to a part icular form of text classif ication--~ext raft/eva/.
Text  retrieval systems find or route texts inresponse to arbitrary user queries or interest profiles.
Eval-uation has been a focus of research in text retrieval sincethe beginning, and standard evaluation methods are in wideuse.A smaller, but significant, body of  work has examined atask variously known as machine-aided indexing, automatedindexing, authority control, or text categorization.
Text cat-egorization is the assignment of texts to one or more of apre-existing set of categories, rather than classifying themin response to an arbitrary query.
Categorization may beperformed for a wide range of reasons, either as an end initself or as a component of a larger system.i Current Address: Center for Information and Language Studies;University of Chicago; Chicago, IL 60637; le~ig@tira.uchicago.eduThe l i terature on text categorization is widely scatteredand shows l ittle agreement on evaluation methods.
Thismakes it very difficult to draw conclusions about the relativeeffectiveness of techniques o that, unlike the situation inquery-driven retrieval, there is no consensus on a set of basicevaluation methods for text categorization.In this paper I discuss measures of effectiveness for textcategorization systems and algorithms.
Effectiveness refersto the abil ity of a categorization to supply information toa system or user that wants to access the texts.
Measuringeffectiveness i just one of several kinds of evaluation thatshould be considered \[Spa81a, CH88, PFg0\].After considering effectiveness evaluation for text cate-gorization we will turn to a related task, text  extraction,and consider what role the effectiveness measures discussedfor categorization have there.
A common theme is the needto consider in an evaluation the purpose for which informa-tion is generated from the text.I will have occasion in the following to repeatedly referto a chapter by Tague \[Tag81\] in Sparck Jones'  collection oninformation retrieval exper imentat ion \[Spagla\].
This collec-tion discusses a wide range of evaluation issues, and is animportant  resource for anyone interested in the evaluationof text-based systems.EFFECT IVENESS MEASURESWhile a number of different effectiveness measures havebeen used in evaluating text categorization i  the past, al-most all have been based on the same model of decision mak-ing by the categorization system.
I begin by discussing thiscontingency table model, which motivates a small numberof simple and widely used effectiveness measures.
Complex-ities arise, however, in how to compute and interpret hesemeasures in the context of a text categorization experiment.The bulk of the discussion concerns these complexities.312Decides YesDecides Noa+bc+d~z+b+e+d=~Table it Contingency Table for a Set of Binary DecisionsThe Cont ingency  Tab leConsider a system that is required to make n binary de-cisions, each of which has exactly one correct answer (eitherYes or No).
The result of n such decisions can be summa-rized in a contingency table, as shown in Table 1.
Eachentry in the table specifies the number of decisions of thespecified type.
For instance, a is the number of times thesystem decided Yes, and Yes was in fact the correct answer.Given the contingency table, three important measuresof the system's effectiveness are:(1) recall = ~/(~ + c)(2) precision = ~/(~ + b)(3) fallout = ~/(~ + d)Measures equivalent to recall and fallout made their firstappearance in signal detection theory \[Swe64\], where theyplay a central role.
Recall and precision are ubiquitous ininformation retrieval, where they measure the proportionof relevant documents retrieved and the proportion of re-trieved documents which are relevant, respectively.
Falloutmeasures the proportion of nonrelevant documents whichare retrieved, and has also seen considerable use.A decision maker can achieve very high recall by rarelydeciding No, or very high precision (and low fallout) byrarely deciding Yes.
For this reason either recall and pre-cision, or recall and fallout, are necessary to ensure a non-trivial evaluation of a decision maker's effectiveness underthe above model.Another measure sometimes used in categorization ex-periments i overlap:(4) overlap = a/(a + b + c)This measure is symmetric with respect o b and c, andso is sometimes used to measure how much two categoriza-tions are alike without defining one or the other to be cor-rect.It is appropriate at this point to mention some of thelimitations of the contingency table model.
It does not takeinto account he possibility that different errors have differ-ent costs; doing so requires more general decision theoreticmodels.
The contingency table also requires all decisions tobe binary.
It may be desirable for category assignments obe weighted rather than binary, and we will discuss laterone approach to evaluation i  this case.Def in ing  Dec is ions  and  Averag ing  E f fec -t i venessThe contingency table model presented above is appli-cable to a wide range of decision making situations.
In thissection, I will first consider how query-driven text retrievalhas been evaluated under this model, and then consider howtext categorization can be evaluated under the same model.In both cases it will be necessary to interpret he system'sbehavior as a set of binary decisions.In a query-driven retrieval systems, the basic decisionis whether or not to retrieve a particular document for aparticular query.
For a set of q queries and d documentsa total of n = qd decisions are made.
Given those qd de-cisions, two ways of computing effectiveness are available.Microaueraging considers all qd decisions as a single groupand computes recall, precision, fallout, or overlap as definedabove.
Macroaveraging computes these effectiveness mea-sures separately for the set of d documents associated witheach query, and then computes the mean of the resulting qeffectiveness values.Macroaveraging has been favored in evaluating query-driven retrieval, partly because it gives equal weight o eachuser query.
A microaveraged recall measurement, for in-stance, would be disproportionately affected by recall per-formance on queries from users who desired large numbersof documents.An obvious analogy exists between categories in a textcategorization system and queries in a text retrieval system.The most common view taken of categorization is that anassignment decision is made for each category/documentpair.
A categorization experiment will compare the cate-gorization decisions made by a computer system with somestandard of correctness, usually human category assignment.In contrast o evaluations of query-driven retrieval, evalu-ations of categorization have usually used microaveragingrather than macroaveraglng.
Many ad hoc variants of bothforms of averaging have also been used.Whether microaveraging or macroaveraging is more in-formative depends on the purpose for the categorization.For instance, if categorization is used to index documentsfor text retrieval, and each category appears in user queriesat about the same frequency it appears in documents, then313Category Set Cor~ Assigned ~ b F 0ABCDEFGHIJKL A CTable 2: Recall (R), Precision (P), and Fallout (F) of Catego-rlzer X on One Documentmicroaveraging seems very appropriate.
On the other hand,if categorization were used to route documents to divisionsof a company, with each division viewed as being equally im-portant, then macroaveraging would be more informative.The choice will often not be clearcut.
I assume microaverag-ing in the following discussion unless otherwise mentioned.Precision Versus FalloutPrecision and fallout both measure (in roughly inverseways) the tendency of the categorizer to assign incorrect cat-egories.
However, in doing so they capture different prop-erties of the categorization.In the context of query-driven retrieval, Salton has pointedout how systems which maintain constant precision reactdifferently to increasing numbers of documents than thosewhich maintain constant fallout \[Sal72\].
Similar effects canarise for categorizers as the number or nature of categorieschanges.Table 2 shows the hypothetical performance of catego-rizer X as the category set is expanded to include new top-ics.
Decreasing fallout suggests that the categorizer X in-correctly assigns categories in proportion to the number ofcorrect categories to be assigned.
A different categorizer,Y, might show the pattern in Table 3, suggesting categoriesare incorrectly assigned in proportion to the total numberof incorrect categories (or in proportion to the total numberof all categories).In extreme cases a system could actually improve on pre-cision while worsening on fallout, or vice versa.
Having bothmeasures, plus recall, available is useful in quickly apprais-ing a method's behavior under changing circumstances.Partitioning of ResultsThe basic tools of microaveraging and macroaveragingcan be applied to arbitrary subsets of categorization deci-sions.
Subsets of decisions can be defined in terms of sub-CategorYABcD Set ~ A Assignedc ~~bF0ABCDEFGH A CEF 5150 125 150ABCDEFGHIJKL A CEFIJKTable 3: Recall (R), Precision (P), and Fallout (F) of Catego-rlzer Y on One Documentsets of categories, subsets of documents, or gradations inthe correctness standard.Categories can be partitioned by importance, frequency,similarity of meaning, or strategy used in assigning them.Presenting effectiveness measures averaged over categorygroups defined by frequency in the training set would beextremely informative, but does not appear to have beendone in any published study.
If the number of categories ismall enough, effectiveness can be presented separately foreach category \[HKC88\].Subsets of the set of test documents can be defined aswell, particularly if the behavior of the system on texts ofdifferent kinds is of interest.
Maron grouped documents on?
the basis of the amount of evidence they provided for mak-ing a categorization decision, and showed that effectivenessincreased in proportion to the amount of evidence \[lVlar61\].Finally, it is sometimes appropriate to partition resultsby degree of correctness of a category/document pair.
Whilethe contingency table model assumes that an assignment de-cision is either correct or incorrect, the standard they arebeing tested against may actually have gradations of cor-rectness.
The model can still be used if gradations are par-titioned into two disjoint classes, for instance correct andmarginal being considered correct, and ineffective and in-correct being considered incorrect.
In this circumstance, itmay be desirable to present results under several plausiblepartitions.The appropriate partitions to make will depend on manyfactors that cannot be anticipated here.
A crucial point tostress, however, is that care should be taken to partitionsupporting data on the task and system in the same fashion\[Lew91\].
For instance, if effectiveness measures are pre-sented for subsets of documents, then statistics uch as av-erage number of words per document, etc.
should be givenfor the same groups of documents.Arithmetic Anomal iesThe above discussion assumed that computing the ef-fectiveness measures is always straightforward.
Referringto equations (1) to (3) shows that 0 denominators arisewhen there exist no correct category assignments, no in-correct category assignments, or when the system never as-signs a category.
All these situations are extremely unlikelywhen microaveraging is used, but are quite possible undermacroaveraging.For evaluating query-drlven retrieval, Tague suggests ei-ther treating 0/0 as 1.0 or throwing out the query, but saysneither solution is entirely satisfactory.
For a categoriza-tion system, we also have the option of partitioning thecategories and macroaveraging only over the categories forwhich these anomalies don't arise.
As discussed above, the314same partitioning should be used for any background atapresented on the testset and task.One Category or Many?Evaluations of systems which assign multiple categoriesto a document have often been flawed, particularly for cate-gorizers which use statistical techniques.
For instance, someof the results in \[Mar61\] and ~KW75\] were obtained underassumptions equivalent o the categorizer knowing in ad-vance how many correct categories each test document has.This knowledge is not available in an operational setting.Better attempts to both produce and evaluate multi-ple category assignments are found in work by Fuhr andKnorz, and by Field.
Field uses the strategy of assigningthe top k categories to a document, but unlike the abovestudies does this without knowledge of the proper numberof categories for any particular document.
He then plotsthe recall value achieved for variations in the number ofcategories assigned \[Fie75\].
Fuhr and Knorz plot a curveshowing tradeoff between recall and precision as a categoryassignment threshold varies \[FK84\].When categories are completely disjoint and a catego-rizer always assigns exactly 1 of the M categories to atext, we really have a single M-ary decision, rather thanM binary decisions.
The contingency table model providesone way of summarizing M-ary decision effectiveness, butother approaches, uch as confusion matrices \[Swe64\], maybe more revealing.Standard of CorrectnessThe effectiveness measures described above require thatcorrect categorizations are known for a set of test docu-ments.
In cases where an automated categorizer is beingdeveloped to replace or aid manual categorization, catego-rizations from the operational human system may be usedas the standard.
Otherwise, it may be necessary to havehuman indexers categorize some texts specifically for thepurposes of the experiment.Many studies have found that even professional bibli-ographic indexers disagree on a substantial proportion ofcategorization decisions \[Bor64, Fie75, HZ80\].
This callsinto question the validity of human category assignment asa standard against which to judge mechanical assignment.One approach to this problem has been to have an especiallycareful indexing done \[Fie75, HZS0\].
Sometimes evaluationis done against several indexings \[Fie75, HKC88\].Another approach is to accept that there will alwaysbe some degree of inconsistency in human categorization,and that this imposes an upper limit on the effectiveness ofmachine categorization.
The degree of consistency betweenseveral human indexers can be measured, typically usingoverlap, as defined in Equation (4), or some variant of this.How measures of consistency between human indexersmight best aid the interpretation of machine categoriza-tion effectiveness is unclear.
Overlap between the machine-assigned categories and each human indexers' categories canbe measured and compared to overlap among humans.
It isless clear how to interpret recall, precision, or fallout in thepresence of a known level of inconsistency.The possibility also exists that machine categorizationcould be better than human categorization, making consis-tency with human categorization a questionable measureunder any circumstance.
Indirect evaluation, discussed inthe next section, is the best way to address this possibility.Indirect EvaluationThe output of a text categorization system is often usedby another system in performing text retrieval, text extrac-tion, or some other task.
When this is the case, it is possibleto evaluate the categorization i directly, by measuring theperformance of the system which uses the categorization.This indirect evaluation of the categorization can be an im-portant complement to direct evaluation, particularly whenmultiple categorizations are available to be compared.How an indirect evaluation is done depends on the kindof system using the categorized text.
Most categorizers havebeen intended to index documents for query-driven text re-trieval.
Despite this, there have been surprisingly few stud-ies \[Hat82, FK84\] comparing text retrieval performance un-der different automatic ategory assignments.The focus on manual categorization as a standard ap-pears to have led categorization researchers to ignore somepromising research directions.
For instance, I know of nostudy that has evaluated weighted assignment of categoriesto documents, despite early recognition of the potential ofthis technique \[Mar61\] and the strong evidence that weight-ing free text terms in documents improves retrieval perfor-mance \[Sa186\].Categorization of documents may be desired for otherpurposes than supporting query-driven retrieval.
Separa-tion of a text stream by category may allowing packaging ofthe text stream as different products \[Hay90\].
Some com-parison of average retrieval effectiveness across text streamsmight be an appropriate measure in this case.Categorization may also be used to select a subset oftexts for more sophisticated processing, such as extractionof information or question answering \[JR90\].
Evaluating thequality of the extracted information may give some insightinto categorization performance, though the connection canbe distant here.315There are drawbacks to indirect evaluation, of course.Tague questions why any particular set of queries shouldserve as a test of an indexing.
Cleaxly, if a categorizationis to be evaluated by text retrieval performance, the queryset needs to be as large as possible, and representative ofthe actual usage the system will experience.
When cate-gorization is used as a component in a complex languageunderstanding system, that system itself may be difficult toevaluate \[21190\] or differences in categorization quality maybe hard to discern from overall system behavior.
A singlecategorization may also be intended to serve several pur-poses, some possibly not yet defined.
Using both direct andindirect evaluation will be the best approach, when practi-cal.Other IssuesThe evaluation of natural anguage processing (NLP)systems is an area of active research \[PF90\], and a greatdeal remains to be learned.
Much more could be said evenabout evaluating categorization systems.
In particular, Ihave focused entirely on numerical measures.
Carefully cho-sen examples, examined in detail, can also be quite revealing\[HKC88\].
However, the numerical measures described aboveprovide a useful standard for understanding the differencesbetween methods under a variety of conditions.Comparison between categorization methods would beaided by the use of common testsets, something which hasrarely been done.
(An exception is \[BB64\].)
Developmentof standard collections would be an important first step tobetter understanding of text categorization.Categorization is an important facet of many kinds oftext processing systems.
The effectiveness measures definedabove may be useful for evaluating some aspects of thesesystems.
In the next section we consider the evaluation oftext extraction systems from this standpoint.IMPL ICAT IONS FOR EVALUATINGTEXT EXTRACTIONSystems for test e.ztraction generate formatted ata fromnatural anguage text.
Some forms of extraction, for in-stance specifying the highest level of action in a naval report\[Sun89\], are in fact categorization decisions.
Other forms ofextraction are very different, and do not fit well into thecontingency table model.In the following I briefly consider evaluation of text ex-traction systems using the effectiveness measures describedfor categorization.
Two perspectives are taken--one focus-ing on the type of data extracted and the other focusing onthe purpose for which extraction is done.Types of Extracted DataExtracted ata can include binary or M-axy categoriza-tions, quantities, and templates or database records withatomic or structured fillers \[Sun89, McC90, Hal90\].
Thenumber of desired records per text may depend on text con-tent, and cross references between fillers of record fields maybe required.Using the effectiveness measures described above requiresinterpreting the system output in terms of a set of binarydecisions which can be either correct or incorrect.
The mea-sures become less meaningful as the extraction task becomesless a matter of making isolated ecisions with easily definedcorrectness, and more a matter of generating a legal expres-sion from some potentially infinite language.Binary data, either as the sole output of extraction or asthe filler of a fixed subpart of a larger structure, fits easilyinto the contingency table model of evaluation.
This in-cludes the case where a slot can have 0 or more fillers froma fixed set of possible fillers.
Each pair of the form (slot,possible filler) can be treated as a category in the catego-rization model.
Micro- or macroaveraging across slot/fillerpairs for a single slot or for all slots in a template can bedone.
The situation where exactly one of a fixed set of Mfillers must fill a slot is an M-ary decision, as mentionedabove for categorization.Another common extraction task is to recognize all hu-man names in a piece of text, and produce a canonical stringfor each name as part of the extracted ata.
Effectivenessmeasures from categorization begin to break down here.Treating the assignment of each possible canonical nameas a binary decision is likely to be uninformative, given thevery large set of legal names.
(And is impossible if insteadof a fixed set of canonical names there axe rules defining anunbounded number of them.)
The situation is even moredifficult when arbitrary strings may be slot fillers.The MUC-3 evaluation \[HalP0\] has taken the approachof retaining the contingency table measures but redefiningthe set of possible decisions.
Rather than taking the cross-product of the set of all fillers and the set of all documents,the set of decisions is implicitly considered to be the unionof all correct string/document assignments and all system-produced string/document assignments.
This is equivalentto setting cell d of the contingency table to 0, while re-taining the others.
Fallout is thus eliminated as a measurebut recall, precision, and overlap can still be computed.
Ascheme for assigning partial credit is also used.While this approach as been quite useful, it may notbe ideal.
Two processes are being evaluated at once--recognition of an extractable concept, and selection of astring (canonical or arbitrary) to represent that concept.It may be preferable, for instance, to evaluate these pro-cesses eparately.
This approach also requires ubtle human316judgments of the relative correctness ofvarious trings thatmight be extracted.
Finally, when comparing systems usingthis approach, the underlying decision spaces may be differ-ent for each system, making interpreting the effectivenessmeasures more diffcult.When a system goes beyond string fills to filling slotswith arbitrary structures, the contingency table model be-comes very difficult to apply.
At best there may be somehopes of capturing some parts of the task in this way, suchas getting the right category of structure in a slot.
Moreresearch on evaluation is clearly needed here.Purposes for Extracted DataThe data type of extracted information affects what ef-fectiveness measures can be computed.
Even more impor-tant, however, is the purpose for which information is beingextracted.
This issue has been given surprisingly little at-tention in published iscussions of text extraction systems.In the following, I give three examples to suggest hat ex-plicit consideration of how extracted ata will be used iscrucial in choosing appropriate ffectiveness measures.Statistical Analysis of Real-World Events A data-base of extracted information may be meant to supportqueries about real-world events described in the texts.
Ananalyst might want to check for correlations between um-bers of naval equipment failures and servicing in certainports, or list the countries where plastic explosives havebeen used in terrorist bombings, to give examples.Accurate answers to questions about numbers of eventsdepend on recognizing when multiple event references inthe same or in different documents in fact refer to a sin-gle real world event, and on proper handling of phenomenasuch as plurals, numbers, and quantification.
High preci-sion and low fallout may be favored over high recall.
If it isexpected that the same event will be described by multiplesources, a single failure to recognize it may not be impor-tant.
Evaluation might focus on effectiveness in extractingdetails necessary to uniquely identify each event.
On theother hand, if support of arbitrary existence queries (Hasplastic ee~plosive b en used...) is important, then recall forall recognizable details of events may be the most importantthing to evaluate.The degree of connection between reports of events andactual events will vary from reliable (intra-agency traffc)to dubious (political propaganda).
This makes it likely thatthe extraction system will at best be an aid to a humananalyst, who will need to make judgment calls on the tell-ability of textual descriptions.
The most useful evaluationmay be of the analyst's performance with and without theextraction system.Content Analysis Content a,alysi8 has been definedin many different ways (\[Ho169\], pp.
2-3) but here I focusparticularly on the analysis of texts to gain insight into themotivations and plans of the texts' authors.
In its simplestform content analysis involves counting the number of oc-currences of members of particular linguistic classes.
Forinstance, one might count how often words with positive ornegative connotations are used in referring to a neighboringcountry.
The great potential of the computer to aid withthe drudgery of analyzing large corpora of text has longbeen recognized, as has the potential for NLP to improvethe effectiveness of this process.In content analysis, faithfulness to the text rather thanfaithfulness to the world may be the primary concern.
Ofparticular importance is that the number of instances ofa particular linguistic item extracted is not a~ected by ex-traneous variables.
Consider a comparison of the number ofreferences to a particular border skirmish in political broad-casts from two countries.
In this case, one would want con-fidence that extraction effectiveness was about the same fortexts from the two countries and was not affected by, forinstance, differing capitalization conventions.
The absolutelevel of e/Fectiveness might be a lesser concern.Indexing for Query-Drlven Text Retrieval In thiscase, the extracted ata is used only indirectly.
An an-alyst will use either a text retrieval system or a conven-tional database system to retrieve documents indexed byextracted ata.
The analyst may want the documents forany of a number of purposes, including the ones describedabove.
The difference is that extracted information partici-pates in the analysis only to the extent of influencing whichdocuments the analyst sees.
No numeric values are deriveddirectly from the extracted ata.In evaluating formatted ata extracted for this purpose,a number of results from information retrieval research areimportant to consider.
One is the fact, mentioned earlier,that document-specific weighting of indexing units is likelyto substantially increase performance.
Since NLP systemscan potentially use many sources of evidence in decidingwhether to extract a particular piece of information, thereis a rich opportunity for such weighting.Another lesson from IK research is that people find itvery difficult o judge the quality of indexing in the absenceof retrieval data.
Strongly held intuitions about the relativeeffectiveness of indexing languages and indexing methodsfor supporting document retrieval have often been shownby experiment to be incorrect \[Spaglb\].
If the primary pur-pose of extracted information is to support querying, thenindirect evaluation, i.e.
testing with actual queries, is veryimportant.317CONCLUSIONText categorization plays a variety of roles in text-basedsystems.
Evaluation of categorization effectiveness i  im-portant, both for confidence in operational systems and forprogress in research.
Several good measures, based on amodel of binary decision making, are available for evaluat-ing the effectiveness of a categorization.
I have discussedsome of the issues to consider in using these measures, andstressed that the purpose for which categorization is beingdone needs to be considered.
The use of both indirect anddirect evaluation is preferable.
I also discussed how someof the work done by text extraction systems can be viewedas categorization and evaluated in a similar fashion, thoughnew measures are needed as well.AcknowledgmentsThe author is preparing a longer article on this topicILew91\] so comments on the above would be most welcome.This research as already greatly benefited from discussionswith Laura Balcom, Nancy Chinchor, Bruce Croft, RalphGrishman, Jerry Hobbs, Adele Howe, Lisa R.au, PenelopeSibun, Beth Sundheim, and Carl Weir.
The research asbeen supported by AFOSIt  under grant AFOSR-90-0110, bythe NSF under grant IRI-8814790, and by an NSF GraduateFellowship.REFERENCES\[BB64\] Harold Borko and Myrna Bernlck.
Automatic documentclassificatlonpart II.
additional experiments.
3 Associationfor Computing Machinery, 11(2):138-151, April 1964.\[Bor64\] Harold Borko.
Measuring the reliability of subject clas-sification by men and machines.
American Documentation,pages 268--273, October 1964.\[CH88\] Paul Ft. Cohen and Adele E. Howe.
How evaluationguides AI research.
AI Magazine, pages 35-43, Winter 1988.\[Fie75\] B. J.
Field.
Towards automatic indexing: Automatic as-signment of controlled-language indexing and classificationfrom free indexing..IT.
Documentation, 31(4):246-265, De-cember 1975.\[FK84\] N. Fuhr and G. E. Knorz.
Retrieval test evaluation ofa rule based automatic indexing (AIR/PHYS).
In C. J.van Rijsbergen, editor, Research and Development in In-formation Retrieval, pages 391--408, Cambridge.
CambridgeUniversity Press.\[Hal90\] Peter C. Halverson.
MUC scoring system: user's man-ual.
Edition 1.5, General Electric Corporate Ftesearch andDevelopment, Schenectady, N'Y', November 2 1990.\[Har82\] P. Harding.
Automatic Indexing and Classification forMecha.nised Information Ftetrieval.
BLRDD Fteport No.5723, British Library Ft & D Department, London, February1982.\[HKC88\] Philip J. Hayes, Laura E. Knecht, and Monica J. Cellio.A news story categorization system.
In Second Conferenceon Applied Natural Language Proceuing, pages 9--17, 1988.\[Hay90\] Philip J. Hayes.
Intelligent hlgh-volume t xt processirq~using shallow, domaln-speciflc techniques.
In P. S. Jacobs,editor, Tezt*Based Intelligent Systems, pages 70---74, Sch-enectady, NY, 1990.
GE Ft & D Center.
Report Number90CFtD198.\[Ho169\] Ole Ft. Hoist.i.
Content Analysis for the Soci?l Sciencesand Humanities.
Addison-Wesley, Fteadlng, MA, 1969.\[HZS0\] Harsh A. Ham;l\] and Antonio Zaraora.
The use of titlesfor automatic document c~ssificatlon.
J. American Societyfor Information Science, pages 396-402, 1980.\[JFt90\] Paul S. Jacobs and Lisa F. Ftau.
SCISOFt: Extracting in-formation from on-llne news.
Communications of the A C~/~,33(11):88--97, November 1990.\[KW75\] B. Gautam Kar and L. J.
White.
A distance measure forautomatic sequentisd document classification.
Technical Re-port OSU-CISFtC-TR,-75-7, Computer end Information Sci-ence Ftesearch Center; Ohio State Univ., Col'umbus, Ohio,August 1975.\[Lew91\] David D. Lewis.
Evaluating text classification systems.In preparation., 1991.\[Mar61\] M. E. March.
Automatic indexing: An experimentalinquiry.
J. of the Association for Computing Machinery,8:404--417, 1961.\[McC90\] Rite McCarden.
Evaluating natural anguage gener-ated database records.
In Proceedings of Speech and Nat-ural Language Workshop, pages 64-70.
Defense AdvancedFtesearch Projects Agency, Morgan Kaufmann, June 1990.\[PFg0\] Martha Palmer and Tim Finin.
Workshop on the evalua-tion of natural language processlng syst eros.
ComputationalLinguistics, 16:175-181, September 1990.\[Sal72\] G. Salton.
The "generality" effect and the retrieval eval-uation for large collections.
J American Societ# for Infor-mation Science, pages 11-22, January-February 1972.\[Ss186\] Gerard Salton.
Another look at automatic text-retrievalsystems.
Communications offthe A CM, 29(7):648-656, July1986.\[Spa81a\] Karen Sparck Jones, editor.
Information Retrieral Ez-pertinent.
Butterworths, London, 1981.\[Spa81b\] Karen Sparck Jones.
Ftetrleval system teats 1958-1978.In Karen Sparck Jones, editor, Information Retrierai Exper-iment, chapter 12.
Butterworths, London, 1981.\[Sun89\] Beth M. Sunclhelm.
Plans for task-oriented evaluationof natural langx~ge understanding systems.
In Proceedingsof the Speech and Natural Language Workshop, pages 197-202.
Defeuse Advanced Ftesearch Projects Agency, MorganKaufmmm, February 1989.\[Swe64\] John A. Swets, editor.
Signal Detection and Recognitionby Human Obserrers.
John Wiley & Sons, New York, 1964.\[Tag81\] Jean M. Tague.
The pragmatlcs of information retrievalexperimentation.
I  Karen Sparck Jones, editor, Informs.tion Retrieral Ezperiment, chapter 5.
Butterworthe, Lon-don, 1981.318
