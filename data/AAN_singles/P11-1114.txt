Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1137?1147,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsPeeling Back the Layers: Detecting Event Role Fillers in Secondary ContextsRuihong Huang and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{huangrh,riloff}@cs.utah.eduAbstractThe goal of our research is to improveevent extraction by learning to identify sec-ondary role filler contexts in the absenceof event keywords.
We propose a multi-layered event extraction architecture that pro-gressively ?zooms in?
on relevant informa-tion.
Our extraction model includes a docu-ment genre classifier to recognize event nar-ratives, two types of sentence classifiers, andnoun phrase classifiers to extract role fillers.These modules are organized as a pipeline togradually zero in on event-related information.We present results on the MUC-4 event ex-traction data set and show that this model per-forms better than previous systems.1 IntroductionEvent extraction is an information extraction (IE)task that involves identifying the role fillers forevents in a particular domain.
For example, theMessage Understanding Conferences (MUCs) chal-lenged NLP researchers to create event extractionsystems for domains such as terrorism (e.g., to iden-tify the perpetrators, victims, and targets of terrorismevents) and management succession (e.g., to iden-tify the people and companies involved in corporatemanagement changes).Most event extraction systems use either alearning-based classifier to label words as rolefillers, or lexico-syntactic patterns to extract rolefillers from pattern contexts.
Both approaches, how-ever, generally tackle event recognition and rolefiller extraction at the same time.
In other words,most event extraction systems primarily recognizecontexts that explicitly refer to a relevant event.
Forexample, a system that extracts information aboutmurders will recognize expressions associated withmurder (e.g., ?killed?, ?assassinated?, or ?shot todeath?)
and extract role fillers from the surround-ing context.
But many role fillers occur in contextsthat do not explicitly mention the event, and thosefillers are often overlooked.
For example, the per-petrator of a murder may be mentioned in the con-text of an arrest, an eyewitness report, or specula-tion about possible suspects.
Victims may be namedin sentences that discuss the aftermath of the event,such as the identification of bodies, transportationof the injured to a hospital, or conclusions drawnfrom an investigation.
We will refer to these types ofsentences as ?secondary contexts?
because they aregenerally not part of the main event description.
Dis-course analysis is one option to explicitly link thesesecondary contexts to the event, but discourse mod-elling is itself a difficult problem.The goal of our research is to improve event ex-traction by learning to identify secondary role fillercontexts in the absence of event keywords.
We cre-ate a set of classifiers to recognize role-specific con-texts that suggest the presence of a likely role fillerregardless of whether a relevant event is mentionedor not.
For example, our model should recognizethat a sentence describing an arrest probably in-cludes a reference to a perpetrator, even though thecrime itself is reported elsewhere.Extracting information from these secondary con-texts can be risky, however, unless we know thatthe larger context is discussing a relevant event.
To1137address this, we adopt a two-pronged strategy forevent extraction that handles event narrative docu-ments differently from other documents.
We definean event narrative as an article whose main purposeis to report the details of an event.
We apply the role-specific sentence classifiers only to event narrativesto aggressively search for role fillers in these sto-ries.
However, other types of documents can men-tion relevant events too.
The MUC-4 corpus, for ex-ample, includes interviews, speeches, and terroristpropaganda that contain information about terroristevents.
We will refer to these documents as fleet-ing reference texts because they mention a relevantevent somewhere in the document, albeit briefly.
Toensure that relevant information is extracted from alldocuments, we also apply a conservative extractionprocess to every document to extract facts from ex-plicit event sentences.Our complete event extraction model, calledTIER, incorporates both document genre and role-specific context recognition into 3 layers of analy-sis: document analysis, sentence analysis, and nounphrase (NP) analysis.
At the top level, we train atext genre classifier to identify event narrative doc-uments.
At the middle level, we create two typesof sentence classifiers.
Event sentence classifiersidentify sentences that are associated with relevantevents, and role-specific context classifiers identifysentences that contain possible role fillers irrespec-tive of whether an event is mentioned.
At the low-est level, we use role filler extractors to label indi-vidual noun phrases as role fillers.
As documentspass through the pipeline, they are analyzed at dif-ferent levels of granularity.
All documents passthrough the event sentence classifier, and event sen-tences are given to the role filler extractors.
Docu-ments identified as event narratives additionally passthrough role-specific sentence classifiers, and therole-specific sentences are also given to the role fillerextractors.
This multi-layered approach creates anevent extraction system that can discover role fillersin a variety of different contexts, while maintaininggood precision.In the following sections, we position our researchwith respect to related work, present the details ofour multi-layered event extraction model, and showexperimental results for five event roles using theMUC-4 data set.2 Related WorkSome event extraction data sets only include doc-uments that describe relevant events (e.g., well-known data sets for the domains of corporate ac-quisitions (Freitag, 1998b; Freitag and McCallum,2000; Finn and Kushmerick, 2004), job postings(Califf and Mooney, 2003; Freitag and McCallum,2000), and seminar announcements (Freitag, 1998b;Ciravegna, 2001; Chieu and Ng, 2002; Finn andKushmerick, 2004; Gu and Cercone, 2006).
Butmany IE data sets present a more realistic task wherethe IE system must determine whether a relevantevent is present in the document, and if so, extractits role fillers.
Most of the Message Understand-ing Conference data sets represent this type of eventextraction task, containing (roughly) a 50/50 mixof relevant and irrelevant documents (e.g., MUC-3,MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)).Our research focuses on this setting where the eventextraction system is not assured of getting only rele-vant documents to process.Most event extraction models can be character-ized as either pattern-based or classifier-based ap-proaches.
Early event extraction systems used hand-crafted patterns (e.g., (Appelt et al, 1993; Lehn-ert et al, 1991)), but more recent systems gener-ate patterns or rules automatically using supervisedlearning (e.g., (Kim and Moldovan, 1993; Riloff,1993; Soderland et al, 1995; Huffman, 1996; Fre-itag, 1998b; Ciravegna, 2001; Califf and Mooney,2003)), weakly supervised learning (e.g., (Riloff,1996; Riloff and Jones, 1999; Yangarber et al,2000; Sudo et al, 2003; Stevenson and Greenwood,2005)), or unsupervised learning (e.g., (Shinyamaand Sekine, 2006; Sekine, 2006)).
In addition, manyclassifiers have been created to sequentially labelevent role fillers in a sentence (e.g., (Freitag, 1998a;Chieu and Ng, 2002; Finn and Kushmerick, 2004;Li et al, 2005; Yu et al, 2005)).
Research hasalso been done on relation extraction (e.g., (Rothand Yih, 2001; Zelenko et al, 2003; Bunescu andMooney, 2007)), but that task is different from eventextraction because it focuses on isolated relationsrather than template-based event analysis.Most event extraction systems scan a text andsearch small context windows using patterns or aclassifier.
However, recent work has begun to ex-1138Figure 1: TIER: A Multi-Layered Architecture for Event Extractionplore more global approaches.
(Maslennikov andChua, 2007) use discourse trees and local syntacticdependencies in a pattern-based framework to incor-porate wider context.
Ji and Grishman (2008) en-force event role consistency across different docu-ments.
(Liao and Grishman, 2010) use cross-eventinference to help with the extraction of role fillersshared across events.
And there have been severalrecent IE models that explore the idea of identify-ing relevant sentences to gain a wider contextualview and then extracting role fillers.
(Gu and Cer-cone, 2006) created HMMs to first identify relevantsentences, but their research focused on eliminatingredundant extractions and worked with seminar an-nouncements, where the system was only given rel-evant documents.
(Patwardhan and Riloff, 2007) de-veloped a system that learns to recognize event sen-tences and uses patterns that have a semantic affinityfor an event role to extract role fillers.
GLACIER(Patwardhan and Riloff, 2009) jointly considers sen-tential evidence and phrasal evidence in a unifiedprobabilistic framework.
Our research follows inthe same spirit as these approaches by performingmultiple levels of text analysis.
But our event ex-traction model includes two novel contributions: (1)we develop a set of role-specific sentence classifiersto learn to recognize secondary contexts associatedwith each type of event role , and (2) we exploit textgenre to incorporate a third level of analysis that en-ables the system to aggressively hunt for role fillersin documents that are event narratives.
In Section 5,we compare the performance of our model with boththe GLACIER system and Patwardhan & Riloff?ssemantic affinity model.3 A Multi-Layered Approach to EventExtractionThe main idea behind our approach is to analyzedocuments at multiple levels of granularity in orderto identify role fillers that occur in different types ofcontexts.
Our event extraction model progressively?zooms in?
on relevant information by first identi-fying the document type, then identifying sentencesthat are likely to contain relevant information, andfinally analyzing individual noun phrases to identifyrole fillers.
The key advantage of this architecture isthat it allows us to search for information using twodifferent principles: (1) we look for contexts that di-rectly refer to the event, as per most traditional eventextraction systems, and (2) we look for secondarycontexts that are often associated with a specific typeof role filler.
Identifying these role-specific contextscan root out important facts would have been oth-erwise missed.
Figure 1 shows the multi-layeredpipeline of our event extraction system.An important aspect of our model is that two dif-ferent strategies are employed to handle documentsof different types.
The event extraction task is tofind any description of a relevant event, even if theevent is not the topic of the article.1 Consequently,all documents are given to the event sentence recog-nizers and their mission is to identify any sentencethat mentions a relevant event.
This path through thepipeline is conservative because information is ex-tracted only from event sentences, but all documentsare processed, including stories that contain only afleeting reference to a relevant event.1Per the MUC-4 task definition (MUC-4 Proceedings,1992).1139The second path through the pipeline performsadditional processing for documents that belong tothe event narrative text genre.
For event narratives,we assume that most of the document discusses arelevant event so we can more aggressively hunt forevent-related information in secondary contexts.In this section, we explain how we create the twotypes of sentence classifiers and the role filler extrac-tors.
We will return to the issue of document genreand the event narrative classifier in Section 4.3.1 Sentence ClassificationWe have argued that event role fillers commonly oc-cur in two types of contexts: event contexts androle-specific secondary contexts.
For the purposesof this research, we use sentences as our definitionof a ?context?, although there are obviously manyother possible definitions.
An event context is a sen-tence that describes the actual event.
A secondarycontext is a sentence that provides information re-lated to an event but in the context of other activitiesthat precede or follow the event.For both types of classifiers, we use exactly thesame feature set, but we train them in different ways.The MUC-4 corpus used in our experiments in-cludes a training set consisting of documents and an-swer keys.
Each document that describes a relevantevent has answer key templates with the role fillers(answer key strings) for each event.
To train theevent sentence recognizer, we consider a sentenceto be a positive training instance if it contains one ormore answer key strings from any of the event roles.This produced 3,092 positive training sentences.
Allremaining sentences that do not contain any answerkey strings are used as negative instances.
This pro-duced 19,313 negative training sentences, yielding aroughly 6:1 ratio of negative to positive instances.There is no guarantee that a classifier trained inthis way will identify event sentences, but our hy-pothesis was that training across all of the eventroles together would produce a classifier that learnsto recognize general event contexts.
This approachwas also used to train GLACIER?s sentential eventrecognizer (Patwardhan and Riloff, 2009), and theydemonstrated that this approach worked reasonablywell when compared to training with event sentenceslabelled by human judges.The main contribution of our work is introducingadditional role-specific sentence classifiers to seekout role fillers that appear in less obvious secondarycontexts.
We train a set of role-specific sentenceclassifiers, one for each type of event role.
Everysentence that contains a role filler of the appropri-ate type is used as a positive training instance.
Sen-tences that do not contain any answer key strings arenegative instances.2 In this way, we force each clas-sifier to focus on the contexts specific to its particu-lar event role.
We expect the role-specific sentenceclassifiers to find some secondary contexts that theevent sentence classifier will miss, although somesentences may be classified as both.Using all possible negative instances would pro-duce an extremely skewed ratio of negative to pos-itive instances.
To control the skew and keep thetraining set-up consistent with the event sentenceclassifier, we randomly choose from the negative in-stances to produce a 6:1 ratio of negative to positiveinstances.Both types of classifiers use an SVM model cre-ated with SVMlin (Keerthi and DeCoste, 2005), andexactly the same features.
The feature set consistsof the unigrams and bigrams that appear in the train-ing texts, the semantic class of each noun phrase3,plus a few additional features to represent the tenseof the main verb phrase in the sentence and whetherthe document is long (> 35 words) or short (< 5words).
All of the feature values are binary.3.2 Role Filler ExtractorsOur extraction model also includes a set of role fillerextractors, one per event role.
Each extractor re-ceives a sentence as input and determines whichnoun phrases (NPs) in the sentence are fillers for theevent role.
To train an SVM classifier, noun phrasescorresponding to answer key strings for the eventrole are positive instances.
We randomly chooseamong all noun phrases that are not in the answerkeys to create a 10:1 ratio of negative to positive in-stances.2We intentionally do not use sentences that contain fillersfor competing event roles as negative instances because sen-tences often contain multiple role fillers of different types (e.g.,a weapon may be found near a body).
Sentences without anyrole fillers are certain to be irrelevant contexts.3We used the Sundance parser (Riloff and Phillips, 2004) toidentify noun phrases and assign semantic class labels.1140The feature set for the role filler extractors ismuch richer than that of the sentence classifiers be-cause they must carefully consider the local contextsurrounding a noun phrase.
We will refer to the nounphrase being labelled as the targeted NP.
The rolefiller extractors use three types of features:Lexical features: we represent four words to theleft and four words to the right of the targeted NP, aswell as the head noun and modifiers (adjectives andnoun modifiers) of the targeted NP itself.Lexico-syntactic patterns: we use the AutoSlogpattern generator (Riloff, 1993) to automaticallycreate lexico-syntactic patterns around each nounphrase in the sentence.
These patterns are similarto dependency relations in that they typically repre-sent the syntactic role of the NP with respect to otherconstituents (e.g., subject-of, object-of, and noun ar-guments).Semantic features: we use the Stanford NER tag-ger (Finkel et al, 2005) to determine if the targetedNP is a named entity, and we use the Sundanceparser (Riloff and Phillips, 2004) to assign seman-tic class labels to each NP?s head noun.4 Event Narrative Document ClassificationOne of our goals was to explore the use of documentgenre to permit more aggressive strategies for ex-tracting role fillers.
In this section, we first presentan analysis of the MUC-4 data set which reveals thedistribution of event narratives in the corpus, andthen explain how we train a classifier to automati-cally identify event narrative stories.4.1 Manual AnalysisWe define an event narrative as an article whosemain focus is on reporting the details of an event.For the purposes of this research, we are only con-cerned with events that are relevant to the event ex-traction task (i.e., terrorism).
An irrelevant docu-ment is an article that does not mention any rele-vant events.
In between these extremes is anothercategory of documents that briefly mention a rele-vant event, but the event is not the focus of the ar-ticle.
We will refer to these documents as fleetingreference documents.
Many of the fleeting referencedocuments in the MUC-4 corpus are transcripts ofinterviews, speeches, or terrorist propaganda com-muniques that refer to a terrorist event and mentionat least one role filler, but within a discussion abouta different topic (e.g., the political ramifications of aterrorist incident).To gain a better understanding of how we mightcreate a system to automatically distinguish eventnarrative documents from fleeting reference docu-ments, we manually labelled the 116 relevant docu-ments in our tuning set.
This was an informal studysolely to help us understand the nature of these texts.# of Event # of FleetingNarratives Ref.
Docs AccGold Standard 54 62Heuristics 40 55 .82Table 1: Manual Analysis of Document TypesThe first row of Table 1 shows the distribution ofevent narratives and fleeting references based on our?gold standard?
manual annotations.
We see thatmore than half of the relevant documents (62/116)are not focused on reporting a terrorist event, eventhough they contain information about a terroristevent somewhere in the document.4.2 Heuristics for Event NarrativeIdentificationOur goal is to train a document classifier to automat-ically identify event narratives.
The MUC-4 answerkeys reveal which documents are relevant and irrel-evant with respect to the terrorism domain, but theydo not tell us which relevant documents are eventnarratives and which are fleeting reference stories.Based on our manual analysis of the tuning set, wedeveloped several heuristics to help separate them.We observed two types of clues: the location ofthe relevant information, and the density of rele-vant information.
First, we noticed that event nar-ratives tend to mention relevant information withinthe first several sentences, whereas fleeting refer-ence texts usually mention relevant information onlyin the middle or end of the document.
Therefore ourfirst heuristic requires that an event narrative men-tion a role filler within the first 7 sentences.Second, event narratives generally have a higherdensity of relevant information.
We use several cri-teria to estimate information density because a sin-gle criterion was inadequate to cover different sce-1141narios.
For example, some documents mention rolefillers throughout the document.
Other documentscontain a high concentration of role fillers in someparts of the document but no role fillers in otherparts.
We developed three density heuristics to ac-count for different situations.
All of these heuristicscount distinct role fillers.
The first density heuristicrequires that more than 50% of the sentences containat least one role filler ( |RelSents||AllSents| > 0.5) .
Figure 2shows histograms for different values of this ratio inthe event narrative (a) vs. the fleeting reference doc-uments (b).
The histograms clearly show that docu-ments with a high (> 50%) ratio are almost alwaysevent narratives.0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1051015Ratio of Relevant Sentences#ofDocuments(a)0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1051015Ratio of Relevant Sentences#ofDocuments(b)Figure 2: Histograms of Density Heuristic #1 in EventNarratives (a) vs.
Fleeting References (b).A second density heuristic requires that the ratioof different types of roles filled to sentences be >50% ( |Roles||AllSents| > 0.5).
A third density heuristicrequires that the ratio of distinct role fillers to sen-tences be > 70% ( |RoleF illers||AllSents| > 0.7).
If any ofthese three criteria are satisfied, then the documentis considered to have a high density of relevant in-formation.4We use these heuristics to label a document as anevent narrative if: (1) it has a high density of relevantinformation, and (2) it mentions a role filler withinthe first 7 sentences.The second row of Table 1 shows the performanceof these heuristics on the tuning set.
The heuristicscorrectly identify 4054 event narratives and5562 fleetingreference stories, to achieve an overall accuracy of82%.
These results are undoubtedly optimistic be-cause the heuristics were derived from analysis ofthe tuning set.
But we felt confident enough to moveforward with using these heuristics to generate train-4Heuristic #1 covers most of the event narratives.ing data for an event narrative classifier.4.3 Event Narrative ClassifierThe heuristics above use the answer keys to help de-termine whether a story belongs to the event narra-tive genre, but our goal is to create a classifier thatcan identify event narrative documents without thebenefit of answer keys.
So we used the heuristicsto automatically create training data for a classifierby labelling each relevant document in the trainingset as an event narrative or a fleeting reference doc-ument.
Of the 700 relevant documents, 292 werelabeled as event narratives.
We then trained a doc-ument classifier using the 292 event narrative docu-ments as positive instances and all irrelevent trainingdocuments as negative instances.
The 308 relevantdocuments that were not identified as event narra-tives were discarded to minimize noise (i.e., we es-timate that our heuristics fail to identify 25% of theevent narratives).
We then trained an SVM classifierusing bag-of-words (unigram) features.Table 2 shows the performance of the event nar-rative classifier on the manually labeled tuning set.The classifier identified 69% of the event narrativeswith 63% precision.
Overall accuracy was 81%.Recall Precision Accuracy.69 .63 .81Table 2: Event Narrative Classifier ResultsAt first glance, the performance of this classifieris mediocre.
However, these results should be inter-preted loosely because there is not always a clear di-viding line between event narratives and other doc-uments.
For example, some documents begin witha specific event description in the first few para-graphs but then digress to discuss other topics.
For-tunately, it is not essential for TIER to have a per-fect event narrative classifier since all documentswill be processed by the event sentence recognizeranyway.
The recall of the event narrative classifiermeans that nearly 70% of the event narratives willget additional scrutiny, which should help to find ad-ditional role fillers.
Its precision of 63% means thatsome documents that are not event narratives willalso get additional scrutiny, but information will beextracted only if both the role-specific sentence rec-ognizer and NP extractors believe they have found1142Method PerpInd PerpOrg Target Victim Weapon AverageBaselinesAutoSlog-TS 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46Semantic Affinity 48/39/43 36/58/45 56/46/50 46/44/45 53/46/50 48/47/47GLACIER 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52New Results without document classificationAllSent 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42EventSent 52/54/53 50/44/47 52/67/59 55/51/53 56/57/56 53/54/54RoleSent 37/54/44 37/58/45 49/75/59 52/60/55 38/66/48 43/63/51EventSent+RoleSent 38/60/46 36/63/46 47/78/59 52/64/57 36/66/47 42/66/51New Results with document classificationDomDoc/EventSent+DomDoc/RoleSent 45/54/49 42/51/46 51/68/58 54/56/55 46/63/53 48/58/52EventSent+DomDoc/RoleSent 43/59/50 45/61/52 51/77/61 52/61/56 44/66/53 47/65/54EventSent+ENarrDoc/RoleSent 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56Table 3: Experimental results, reported as Precision/Recall/F-scoresomething relevant.4.4 Domain-relevant Document ClassifierFor comparison?s sake, we also created a docu-ment classifier to identify domain-relevant docu-ments.
That is, we trained a classifier to determinewhether a document is relevant to the domain ofterrorism, irrespective of the style of the document.We trained an SVM classifier with the same bag-of-words feature set, using all relevant documents in thetraining set as positive instances and all irrelevantdocuments as negative instances.
We use this clas-sifier for several experiments described in the nextsection.5 Evaluation5.1 Data Set and MetricsWe evaluated our approach on a standard benchmarkcollection for event extraction systems, the MUC-4data set (MUC-4 Proceedings, 1992).
The MUC-4corpus consists of 1700 documents with associatedanswer key templates.
To be consistent with previ-ously reported results on this data set, we use the1300 DEV documents for training, 200 documents(TST1+TST2) as a tuning set and 200 documents(TST3+TST4) as the test set.
Roughly half of thedocuments are relevant (i.e., they mention at least 1terrorist event) and the rest are irrelevant.We evaluate our system on the five MUC-4?string-fill?
event roles: perpetrator individuals,perpetrator organizations, physical targets, victimsand weapons.
The complete IE task involves tem-plate generation, which is complex because manydocuments have multiple templates (i.e., they dis-cuss multiple events).
Our work focuses on extract-ing individual facts and not on template generationper se (e.g., we do not perform coreference resolu-tion or event tracking).
Consequently, our evalua-tion follows that of other recent work and evaluatesthe accuracy of the extractions themselves by match-ing the head nouns of extracted NPs with the headnouns of answer key strings (e.g., ?armed guerril-las?
is considered to match ?guerrillas?
)5 .
Our re-sults are reported as Precision/Recall/F(1)-score foreach event role separately.
We also show an overallaverage for all event roles combined.65.2 BaselinesAs baselines, we compare the performance of ourIE system with three other event extraction sys-tems.
The first baseline is AutoSlog-TS (Riloff,1996), which uses domain-specific extraction pat-terns.
AutoSlog-TS applies its patterns to every sen-tence in every document, so does not attempt toexplicitly identify relevant sentences or documents.The next two baselines are more recent systems:the (Patwardhan and Riloff, 2007) semantic affin-ity model and the (Patwardhan and Riloff, 2009)GLACIER system.
The semantic affinity approach5Pronouns were discarded since we do not perform corefer-ence resolution.
Duplicate extractions with the same head nounwere counted as one hit or one miss.6We generated the Average scores ourselves by macro-averaging over the scores reported for the individual event roles.1143explicitly identifies event sentences and uses pat-terns that have a semantic affinity for an event roleto extract role fillers.
GLACIER is a probabilisticmodel that incorporates both phrasal and sententialevidence jointly to label role fillers.The first 3 rows in Table 3 show the results foreach of these systems on the MUC-4 data set.
Theyall used the same evaluation criteria as our results.5.3 Experimental ResultsThe lower portion of Table 3 shows the results ofa variety of event extraction models that we cre-ated using different components of our system.
TheAllSent row shows the performance of our RoleFiller Extractors when applied to every sentence inevery document.
This system produced high recall,but precision was consistently low.The EventSent row shows the performance ofour Role Filler Extractors applied only to the eventsentences identified by our event sentence classi-fier.
This boosts precision across all event roles, butwith a sharp reduction in recall.
We see a roughly20 point swing from recall to precision.
These re-sults are similar to GLACIER?s results on most eventroles, which isn?t surprising because GLACIER alsoincorporates event sentence identification.The RoleSent row shows the results of our RoleFiller Extractors applied only to the role-specificsentences identified by our classifiers.
We see a 12-13 point swing from recall to precision comparedto the AllSent row.
This result is consistent withour hypothesis that many role fillers exist in role-specific contexts that are not event sentences.
As ex-pected, extracting facts from role-specific contextsthat do not necessarily refer to an event is less reli-able.
The EventSent+RoleSent row shows the re-sults when information is extracted from both typesof sentences.
We see slightly higher recall, whichconfirms that one set of extractions is not a strictsubset of the other, but precision is still relativelylow.The next set of experiments incorporates docu-ment classification as the third layer of text analy-sis.
The DomDoc/EventSent+DomDoc/RoleSentrow shows the results of applying both types ofsentence classifiers only to documents identified asdomain-relevant by the Domain-relevant Document(DomDoc) Classifier described in Section 4.4.
Ex-tracting information only from domain-relevant doc-uments improves precision by +6, but also sacrifices8 points of recall.The EventSent row reveals that informationfound in event sentences has the highest precision,even without relying on document classification.
Weconcluded that evidence of an event sentence isprobably sufficient to warrant role filler extractionirrespective of the style of the document.
As we dis-cussed in Section 4, many documents contain onlya fleeting reference to an event, so it is importantto be able to extract information from those isolatedevent descriptions as well.
Consequently, we cre-ated a system, EventSent+DomDoc/RoleSent, thatextracts information from event sentences in all doc-uments, but extracts information from role-specificsentences only if they appear in a domain-relevantdocument.
This architecture captured the best ofboth worlds: recall improved from 58% to 65% withonly a one point drop in precision.Finally, we evaluated the idea of using documentgenre as a filter instead of domain relevance.
Thelast row, EventSent+ENarrDoc/RoleSent, showsthe results of our final architecture which extractsinformation from event sentences in all documents,but extracts information from role-specific sentencesonly in Event Narrative documents.
This architec-ture produced the best F1 score of 56.
This model in-creases precision by an additional 4 points and pro-duces the best balance of recall and precision.Overall, TIER?s multi-layered extraction architec-ture produced higher F1 scores than previous sys-tems on four of the five event roles.
The improvedrecall is due to the additional extractions from sec-ondary contexts.
The improved precision comesfrom our two-pronged strategy of treating event nar-ratives differently from other documents.
TIER ag-gressively searches for extractions in event narrativestories but is conservative and extracts informationonly from event sentences in all other documents.5.4 AnalysisWe looked through some examples of TIER?s outputto try to gain insight about its strengths and limita-tions.
TIER?s role-specific sentence classifiers didcorrectly identify some sentences containing rolefillers that were not classified as event sentences.Several examples are shown below, with the role1144fillers in italics:(1) ?The victims were identified as David Lecky, directorof the Columbus school, and James Arthur Donnelly.?
(2) ?There were seven children, including four of theVice President?s children, in the home at the time.?
(3) ?The woman fled and sought refuge inside thefacilities of the Salvadoran Alberto Masferrer University,where she took a group of students as hostages, threaten-ing them with hand grenades.?
(4) ?The FMLN stated that several homes were damagedand that animals were killed in the surrounding hamletsand villages.
?The first two sentences identify victims, but theterrorist event itself was mentioned earlier in thedocument.
The third sentence contains a perpetrator(the woman), victims (students), and weapons (handgrenades) in the context of a hostage situation afterthe main event (a bus attack), when the perpetratorescaped.
The fourth sentence describes incidentaldamage to civilian homes following clashes betweengovernment forces and guerrillas.However there is substantial room for improve-ment in each of TIER?s subcomponents, and manyrole fillers are still overlooked.
One reason is that itcan be difficult to recognize acts of terrorism.
Manysentences refer to a potentially relevant subevent(e.g., injury or physical damage) but recognizingthat the event is part of a terrorist incident dependson the larger discourse.
For example, consider theexamples below that TIER did not recognize asrelevant sentences:(5) ?Later, two individuals in a Chevrolet Opala automo-bile pointed AK rifles at the students, fired some shots,and quickly drove away.?
(6) ?Meanwhile, national police members who weredressed in civilian clothes seized university studentsHugo Martinez and Raul Ramirez, who are still missing.?
(7) ?All labor union offices in San Salvador were looted.
?In the first sentence, the event is described assomeone pointing rifles at people and the perpetra-tors are referred to simply as individuals.
There areno strong keywords in this sentence that reveal thisis a terrorist attack.
In the second sentence, policeare being accused of state-sponsored terrorism whenthey seize civilians.
The verb ?seize?
is commonin this corpus, but usually refers to the seizing ofweapons or drug stashes, not people.
The third sen-tence describes a looting subevent.
Acts of lootingand vandalism are not usually considered to be ter-rorism, but in this article it is in the context of accu-sations of terrorist acts by government officials.6 ConclusionsWe have presented a new approach to event extrac-tion that uses three levels of analysis: documentgenre classification to identify event narrative sto-ries, two types of sentence classifiers, and nounphrase classifiers.
A key contribution of our work isthe creation of role-specific sentence classifiers thatcan detect role fillers in secondary contexts that donot directly refer to the event.
Another important as-pect of our approach is a two-pronged strategy thathandles event narratives differently from other doc-uments.
TIER aggressively hunts for role fillers inevent narratives, but is conservative about extract-ing information from other documents.
This strategyproduced improvements in both recall and precisionover previous state-of-the-art systems.This work just scratches the surface of using doc-ument genre identification to improve informationextraction accuracy.
In future work, we hope toidentify additional types of document genre stylesand incorporate genre directly into the extractionmodel.
Coreference resolution and discourse anal-ysis will also be important to further improve eventextraction performance.7 AcknowledgmentsWe gratefully acknowledge the support of the Na-tional Science Foundation under grant IIS-1018314and the Defense Advanced Research ProjectsAgency (DARPA) Machine Reading Program underAir Force Research Laboratory (AFRL) prime con-tract no.
FA8750-09-C-0172.
Any opinions, find-ings, and conclusion or recommendations expressedin this material are those of the authors and do notnecessarily reflect the view of the DARPA, AFRL,or the U.S. government.1145ReferencesD.
Appelt, J. Hobbs, J.
Bear, D. Israel, and M. Tyson.1993.
FASTUS: a finite-state processor for informa-tion extraction from real-world text.
In Proceedings ofthe Thirteenth International Joint Conference on Arti-ficial Intelligence.R.
Bunescu and R. Mooney.
2007.
Learning to ExtractRelations from the Web using Minimal Supervision.In Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics.M.E.
Califf and R. Mooney.
2003.
Bottom-up RelationalLearning of Pattern Matching rules for InformationExtraction.
Journal of Machine Learning Research,4:177?210.H.L.
Chieu and H.T.
Ng.
2002.
A Maximum En-tropy Approach to Information Extraction from Semi-Structured and Free Text.
In Proceedings of the 18thNational Conference on Artificial Intelligence.F.
Ciravegna.
2001.
Adaptive Information Extractionfrom Text by Rule Induction and Generalisation.
InProceedings of the 17th International Joint Confer-ence on Artificial Intelligence.J.
Finkel, T. Grenager, and C. Manning.
2005.
Incor-porating Non-local Information into Information Ex-traction Systems by Gibbs Sampling.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics, pages 363?370, Ann Ar-bor, MI, June.A.
Finn and N. Kushmerick.
2004.
Multi-level BoundaryClassification for Information Extraction.
In In Pro-ceedings of the 15th European Conference on MachineLearning, pages 111?122, Pisa, Italy, September.D.
Freitag and A. McCallum.
2000.
Information Ex-traction with HMM Structures Learned by Stochas-tic Optimization.
In Proceedings of the SeventeenthNational Conference on Artificial Intelligence, pages584?589, Austin, TX, August.Dayne Freitag.
1998a.
Multistrategy Learning for In-formation Extraction.
In Proceedings of the FifteenthInternational Conference on Machine Learning.
Mor-gan Kaufmann Publishers.Dayne Freitag.
1998b.
Toward General-Purpose Learn-ing for Information Extraction.
In Proceedings of the36th Annual Meeting of the Association for Computa-tional Linguistics.Z.
Gu and N. Cercone.
2006.
Segment-Based HiddenMarkov Models for Information Extraction.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages481?488, Sydney, Australia, July.L.
Hirschman.
1998.
?The Evolution of Evaluation:Lessons from the Message Understanding Confer-ences.
Computer Speech and Language, 12.S.
Huffman.
1996.
Learning Information Extraction Pat-terns from Examples.
In Stefan Wermter, Ellen Riloff,and Gabriele Scheler, editors, Connectionist, Statisti-cal, and Symbolic Approaches to Learning for Nat-ural Language Processing, pages 246?260.
Springer-Verlag, Berlin.H.
Ji and R. Grishman.
2008.
Refining Event Extractionthrough Cross-Document Inference.
In Proceedings ofACL-08: HLT, pages 254?262, Columbus, OH, June.S.
Keerthi and D. DeCoste.
2005.
A Modified FiniteNewton Method for Fast Solution of Large Scale Lin-ear SVMs.
Journal of Machine Learning Research.J.
Kim and D. Moldovan.
1993.
Acquisition of SemanticPatterns for Information Extraction from Corpora.
InProceedings of the Ninth IEEE Conference on Artifi-cial Intelligence for Applications, pages 171?176, LosAlamitos, CA.
IEEE Computer Society Press.W.
Lehnert, C. Cardie, D. Fisher, E. Riloff, andR.
Williams.
1991.
University of Massachusetts: De-scription of the CIRCUS System as Used for MUC-3.
In Proceedings of the Third Message Understand-ing Conference (MUC-3), pages 223?233, San Mateo,CA.
Morgan Kaufmann.Y.
Li, K. Bontcheva, and H. Cunningham.
2005.
Us-ing Uneven Margins SVM and Perceptron for Infor-mation Extraction.
In Proceedings of Ninth Confer-ence on Computational Natural Language Learning,pages 72?79, Ann Arbor, MI, June.Shasha Liao and Ralph Grishman.
2010.
Using docu-ment level cross-event inference to improve event ex-traction.
In Proceedings of the 48st Annual Meeting onAssociation for Computational Linguistics (ACL-10).M.
Maslennikov and T. Chua.
2007.
A Multi-ResolutionFramework for Information Extraction from Free Text.In Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics.MUC-4 Proceedings.
1992.
Proceedings of the FourthMessage Understanding Conference (MUC-4).
Mor-gan Kaufmann.S.
Patwardhan and E. Riloff.
2007.
Effective InformationExtraction with Semantic Affinity Patterns and Rele-vant Regions.
In Proceedings of 2007 the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-2007).S.
Patwardhan and E. Riloff.
2009.
A Unified Model ofPhrasal and Sentential Evidence for Information Ex-traction.
In Proceedings of 2009 the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2009).E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence.1146E.
Riloff and W. Phillips.
2004.
An Introduction to theSundance and AutoSlog Systems.
Technical ReportUUCS-04-015, School of Computing, University ofUtah.E.
Riloff.
1993.
Automatically Constructing a Dictio-nary for Information Extraction Tasks.
In Proceedingsof the 11th National Conference on Artificial Intelli-gence.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Artificial Intelli-gence, pages 1044?1049.
The AAAI Press/MIT Press.D.
Roth and W. Yih.
2001.
Relational Learning viaPropositional Algorithms: An Information ExtractionCase Study.
In Proceedings of the Seventeenth In-ternational Joint Conference on Artificial Intelligence,pages 1257?1263, Seattle, WA, August.Satoshi Sekine.
2006.
On-demand information ex-traction.
In Proceedings of Joint Conference of theInternational Committee on Computational Linguis-tics and the Association for Computational Linguistics(COLING/ACL-06.Y.
Shinyama and S. Sekine.
2006.
Preemptive Informa-tion Extraction using Unrestricted Relation Discovery.In Proceedings of the Human Language TechnologyConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 304?311, New York City, NY, June.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.1995.
CRYSTAL: Inducing a conceptual dictionary.In Proc.
of the Fourteenth International Joint Confer-ence on Artificial Intelligence, pages 1314?1319.M.
Stevenson and M. Greenwood.
2005.
A Seman-tic Approach to IE Pattern Induction.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics, pages 379?386, Ann Ar-bor, MI, June.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An Im-proved Extraction Pattern Representation Model forAutomatic IE Pattern Acquisition.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-03).R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic Acquisition of DomainKnowledge for Information Extraction.
In Proceed-ings of the Eighteenth International Conference onComputational Linguistics (COLING 2000).K.
Yu, G. Guan, and M. Zhou.
2005.
Resume?
Infor-mation Extraction with Cascaded Hybrid Model.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 499?506,Ann Arbor, MI, June.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel Methods for RelationExtraction.
Journal of Machine Learning Research, 3.1147
