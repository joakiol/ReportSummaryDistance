Proceedings of the 8th International Conference on Computational Semantics, pages 45?60,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsAutomatic identification of semantic relationsin Italian complex nominalsFabio CelliCLIC-CIMeCUniversity of Trentofabio.celli@email.unitn.itMalvina NissimDipartimento di Studi Linguistici e OrientaliUniversity of Bolognamalvina.nissim@unibo.itAbstractThis paper addresses the problem of the identification of the seman-tic relations in Italian complex nominals (CNs) of the type N+P+N.We exploit the fact that the semantic relation, which is underspeci-fied in most cases, is partially made explicit by the preposition.
Wedevelop an annotation framework around five different semantic rela-tions, which we use to create a corpus of 1700 Italian CNs, obtaining aninter-annotator agreement of K=.695.
Exploiting this data, for eachpreposition p we train a classifier to assign one of the five semanticrelations to any CN of the type N+p+N, by using both string andsupersense features.
To obtain supersenses, we experiment with a se-quential tagger as well as a plain lookup in MultiWordNet, and findthat using information obtained from the former yields better results.1 IntroductionComplex nominals are pervasive in language, and include noun-noun (N+N)and adjective-noun (A+N) combinations (Levi, 1978), as in Ex.
1 and 2.
(1) dessert fork(2) medieval historian45A ?dessert fork?
is ?a fork for eating dessert?, and a ?medieval historian?can be also described as ?a historian who studies medieval times?.1Inboth cases the relation is not overtly marked.
Indeed, syntactically, there isnothing that tells us that the semantic relation between ?dessert?
and ?fork?in Ex.
1 is different than the one binding ?plastic?
and ?fork?
in Ex.
3.
(3) plastic forkHowever, it is well known that whereas English composes CNs of the typeN+N, Romance languages must glue the two nouns by means of a prepo-sition, thus yielding CNs of the form N+P+N, thereby partially makingexplicit the underlying semantic relation (Busa and Johnston, 1996).
So, inEx.
4, the ?purpose?
relation between dessert and fork is (partially) madeexplicit by the preposition ?da?.
In contrast, the ?property?
relation bindingplastic and fork (a fork made of plastic) is expressed using ?di?
(Ex.
5).
(4) forchetta da dessert (en: dessert fork)(5) forchetta di plastica (en: plastic fork)Recently, Girju (2007) has exploited this observation including cross-languageinformation in a system for the automatic interpretation of NN compoundsin English.
However, whereas it is true that the overt preposition restrictsthe set of possible relations, it is also true that prepositions are still se-mantically ambiguous, since there is no one-to-one correspondence betweenprepositions and relations.
So, ?di?, used in a ?property?
relation above,can also express a ?part-whole?
(Ex.
6), a ?theme?
(Ex.
7), and several otherrelations.
(6) dorso della mano (the back of the hand)(7) suonatore di chitarra (guitar player)In this work, we also exploit the presence of a preposition in Italian CNs asan aid to detect the semantic relation.
We extract and annotate CNs in acorpus of written Italian, and develop a supervised system for determiningthe semantics of the CN, comparing the contribution of plain nouns withthat of hypernym classes, and different ways in which such hypernyms can beobtained.
In the next section, we discuss previous work on the semantics ofcomplex nominals.
In Section 3, we define a set of five semantic relations forthe annotation of Italian CNs and the details of the annotation framework,and discuss the corpus distribution.
In Section 4 we describe the experimentsfor the automatic identification of semantic relations, and discuss the results.We conclude with ideas for future work in Section 5.1In this work we will only consider N+N CNs, thereby excluding A+N CNs.462 Previous workGiven their underspecified nature, CNs, especially in English, have receiveda large amount of attention in the linguistic and computational linguisticliterature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; John-ston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007,among others).
Current interest in NLP is also shown in the organisation ofa SemEval task especially dedicated to noun-noun compound interpretation(Task 4, (Girju et al, 2007)).
Indeed, NLP systems which aim at full textunderstanding for higher NLP tasks, such as question answering, recognis-ing textual entailment and machine translation, need to grasp the semanticrelation which noun compounds mostly leave underspecified.One main issue in noun-noun compound interpretation is the lack ofgeneral agreement on a well-defined set of semantic relations.
Nastase andSzpakowicz (2003), for instance, propose a two-level taxonomy, in whichfifteen fine-grained relations are subsumed into five general classes (causal,participant, spatial, temporal, quality).
An example of a causal relation(with subtype ?purpose?)
is ?concert hall?, and an example of a participantrelation (with subtype ?beneficiary?)
is ?student discount?.Girju et al (2007) propose the smaller set reported in Table 1, whichwas tested on English N+N complex nominals within the SemEval 2007 task.They specifically spell out semantic relations as two-poles relationships: forexample an effect is an effect always with respect to a cause.Table 1: The set of 7 semantic relations from Girju et al (2007)Semantic relation ExamplesCause-Effect laugh (cause) wrinkles (effect)Instrument-Agency laser (instrument) printer (agency)Product-Producer honey (product) bee (producer)Origin-Entity message (entity) from outer-space (origin)Theme-Tool news (theme) conference(tool)Part-Whole the door (part) of the car (whole)Content-Container apples (content) in the basket (container)As far as relation detection is concerned, Johnston and Busa (1996),working specifically on Italian, have suggested using information includedin qualia structures (Pustejovsky, 1995) for deriving the compound?s inter-pretation.
The use of qualia structures for this task is appropriate andsemantically sound but absolutely not straightforward to implement, sincethere does not exist an electronic repository of qualias, so that the structures47would need to be constructed by hand, thereby involving a large amount ofmanual work.
Recent work has shown that the automatic acquisition ofqualias can be performed with reasonable success exploiting informationobtained using lexico-syntactic patterns over the Web (Cimiano and Wen-deroth, 2005).
For our purposes, though, if lexico-syntactic patterns can beused successfully to induce qualia roles, we could directly use the informa-tion we obtain from them, thus bypassing the qualia structure representa-tion.
We plan to include features based on such kinds of patterns in futuredevelopment of this work (see also (Nakov and Hearst, 2008)).More purely computational approaches include both supervised (Lauer,1995) as well as unsupervised models, such as (Lapata and Keller, 2005),who use frequencies obtained over the Web.
Some researchers also suggestsolutions to the data sparsness problem, which affects our approach as well,by using lexical similarity (Turney, 2006) or clustering techniques (Panteland Pennacchiotti, 2006).Finally, there exists specific work on compound nouns whose head isderived from a verb (Lapata, 2002), and information about verbs deverbalnouns are linked to has proved a useful feature in previous approaches (Girju,2007).
Whereas we have exploited this information in the annotation phase,we have not included corresponding features yet in the statistical model weuse, but we plan to do so in future extensions.3 Annotation Framework and DataFor developing an annotation framework, we built on Italian grammars,existing classifications (see Section 2), and a preliminary study of corpusdata.3.1 Annotation frameworkIn determining the set of relations to be annotated, following (Girju, 2007),we also define two-pole relations between the involved nominals.We assume that relations can be extracted and subsumed in generalclasses starting from ?-roles, which are partially made explicit by the prepo-sitional phrase.
Since there is no general agreement on a complete list of?-roles we chose to work with types of complements, which are provided bytraditional Italian grammars and can be found in almost every Italian dic-tionary.
In (Zingarelli, 2008), we found 33 different types of prepositionalphrases (PPs), which we grouped into 21 classes (for instance, all of the48location-related PPs were grouped under a single LOC class).
This infor-mation was included in the annotation scheme (Celli, 2008), although is notused in the current relation identification model.Following (Langacker, 1987), the nouns within each CNs were also re-visited within a trajector (Tr) and landmark (Lm) approach mirroring thetwo-pole interpretation of the semantic relations.The set of five semantic relations we arrived at is given in Table 2.
Thesefive relations are the target of our classification experiments (Section 4).Table 2: Relations for Italian prepositions.Relation(Tag) Description Examplescause-effect (CE) tr.
causes lm.
deathLmby privationsTrlocated-location (LL) lm.
localizes tr.
windowLmpassageTrowner-property (OP) tr.
possess lm.
stoneLmstatueTrincluded-set (IS) lm.
includes tr.
thousandsTrof menLmbound-bounded (RR) lm.
undergoes tr.
cityLmdestructionTrIn the cause-effect (CE) relation the trajector is the cause or the agentand the landmark is the product or the effect produced by the agent/causer,as in ?morte per stenti?
(en: death by privations).
In located-location (LL),a trajector is located in space or time with respect to a landmark, as ?casain montagna?
(en: mountain house).
The owner-property (OP) relation as-sociates a trajector (owner) with its property, part, or characteristic, whichis the landmark.
Examples are ?statua di pietra?
(en: stone statue) and?cane da caccia?
(en: hunting dog).
In included-set (IS) the trajector is theincluded object and the landmark is the set: in ?migliaia di uomini?
(en:thousands of men), ?migliaia?
(en: thousands) is the subset and ?uomini?
(en: men) is the set.
The bound-bounded (RR) relation is a direct rela-tionship between an event, usually a deverbal (trajector), and its undergoer(landmark), as ?distruzione della citta`?
(en: destruction of the city).
Clas-sic relations such as part-whole, producer-product, and is-a are covered inthis account by the owner-property, cause-effect and included-set relations,respectively.Annotation categories Each extracted CN (see Section 3.2) was anno-tated with the following information:?
the lemma (A, CON, DI, DA, IN, PER, SU, TRA)22The preposition ?tra?
can also be written as ?fra?.
They are semantically equivalent.Occurrences of both variants were extracted, but we refer to them always as ?tra?.49?
the relation (CE, OP, LL, IS, RR)?
the type of prepositional phrase (21 tags)?
the semantic type of n1/n2 (natural, abstract, artifact, metaphorical usage)?
the position of trajector and landmark in the CN (TL, LT)?
the order of the head and the modifier in the CN (HM, MH)The following CN types were to be excluded from annotation:?
CNs including proper nouns, such as ?problema di Marco?
(en: Mark?s prob-lem);?
CNs involving complex prepositions, such as ?hotel nel mezzo del deserto?
(en: hotel in the middle of the desert);?
CNs involving n1 and/or n2 of categories other than noun, due to POS-tagging errors;?
CNs containing bisyllabic prepositions, such as ?macchina senza benzina?
(en: car without fuel);3?
CNs used as adverbs, e.g.
?accordo di massima?
(en: generally agreed with)3.2 DataCorpus Selection We used CORISsmall, a reduced version of CORIS,a 100M-word, balanced corpus of written Italian (Rossini Favretti, 2000).CORISsmall was sampled by randomly extracting sentences with a lengthbetween 2 and 40 words.
We discarded a few domain-specific subcorporawhich were likely to contain prepositions used in ways different from commonusage, as the legal subcorpus.
The resulting corpus, henceforth CORISnom-inals, contains 75,000 words.
The corpus was then automatically taggedwith part-of-speech information, using TreeTagger (Schmid, 1994).CN detection We chose to annotate monosyllabic prepositions only, namelya (to), con (with), di (of), da (from), in (in),per (for), su (on) and tra(within), because they are more frequent in CNs, more polysemous andnot occurring as any other grammatical category, differently from bisyllabicprepositions which can be used adverbially.
In any case, bisyllabic preposi-tions occurr in less than 2% of all the extracted CNs (42 out of 2298).Exploiting part-of-speech information, we extracted all the N+P+N com-binations with a context window of 10 words left and right.
The frequencyof the CNs found in CORISnominals is reported in Table 3.3Prepositions which incorporate the determiner, such as ?della?
(di+la, en: of the)or ?sulla?
(su+la, en: on the), although possible bisyllabic, are definite variants of theircorresponding monosyllabic prepositions, and are therefore included in the dataset.50Table 3: Frequency of CN types in CORISnominalsCNs extracted #inst exampleN+P+N 1125 lampada a olio (oil lamp)N+Pdet+N 1044 dorso della mano (back of the hand)N+P+D+N 129 casa per le vacanze (holiday home)total 2298Annotation procedure and evaluation The annotation was performedby a native speaker of Italian, with experience in the semantic analysis ofcomplex nominals.
After discarding some CNs according to the rules definedin the annotation scheme, the final number of annotated instances is 1700.In order to assess the difficulty of the relation assignment task, a randomlyextracted portion of the data (186 CNs) was further annotated by a secondnative speaker of Italian.
The second annotator marked them up followingspecific guidelines and some training material composed of about 50 alreadyannotated CNs as examples.
We calculated inter-annotator agreement usingCohen?s kappa statistics (Cohen, 1960), obtaining a kappa of .695.
Whilethis relatively not so high value can be considered satisfactory in the fieldof semantic annotation (this score is also in the same ballpark as the 70.3%agreement reported for the SemeEval Task 4 annotation (Girju et al, 2007)),it still indicates that the phenomenon involves a good amount of ambiguitythus making the classification task far from straightforward.
Table 4 reportsthe confusion matrix for the annotated subset.Table 4: Confusion matrix for annotator A and annotator BA/B CE IS LL OP RR totalCE 2 ?
?
?
?
2IS 1 22 4 5 1 33LL ?
?
12 4 1 17OP 34 4 8 44 6 96RR 5 ?
1 3 29 38total 42 26 25 56 37 186The largest area of disagreement is in the opposition between CE andOP: annotator B assigned the type CE to a large number of CNs whichannotator A had marked as OP.
This might be due to the fact that CErelations can be triggered by parts of objects (or features of concepts), whichare expressed by the OP relation.
A prime example of such overlap is ?fumo51di sigaretta?
(en: cigarette smoke), which can be seen both as a cause-effectrelation as well as a owner-property relation.
Thus, future work will involvea reassessment of these two categories and a revision of the guidelines.Corpus Distribution Table 5 illustrates the distribution of semantic re-lations across each preposition.Table 5: Distribution of relations across prepositions in CORISnominalsprep/rel CE IS LL OP RR totala 0 8 29 34 28 99con 0 5 0 10 14 29di 62 262 69 646 289 1328da 2 0 7 18 8 35in 0 5 50 31 14 100per 8 2 2 29 7 48su 3 0 18 12 11 44tra 0 0 4 3 10 17total 75 282 179 783 381 1700The most striking figure is the overwhelming predominance of ?di?,which features in 78% of all CNs.
This is in line with the extremely highoverall frequency of ?di?
in Italian, which is ranked as the most frequentword in CoLFIS (an Italian frequency lexicon based on a 3M word corpus,Laudanna et al (1995)), and also with Girju?s 2007 observation that 77.7%of the English noun-noun compounds in her data can be rephrased as ?of?phrases.
We can also observe that some prepositions, namely ?a?
and ?con?,show more than one predominant relation usage in CNs.
Overall, OP is byfar the most frequent relation, occurring in nearly half of the CNs.As an additional observation, for each preposition we compared its fre-quency of occurrence in CNs and in any other constructions.
We found thatwhile ?di?
and ?su?
are particularly CN-oriented prepositions, both withover 55% of their occurrences being in CNs, the others appear in CNs about10% or less of their total occurrences.4 Automatic identification of CN relationsWe can see the problem of semantic relations in CNs from at least two (con-verging) points of view.
From a more language understanding side, given aCN (two nouns connected by a preposition), we might want to know what the52Table 6: Accuracy for most frequent relation baseline and for basic systemprep #inst most freq rel baseline basic systema 99 OP (34) 34.34 47.47con 29 RR (14) 48.28 48.28da 35 OP (18) 51.43 51.43di 1328 OP (646) 48.64 56.40in 100 LL (50) 50.00 52.00per 48 OP (29) 60.42 60.42tra 17 RR (10) 58.82 58.82su 44 LL (18) 40.91 50.00underlying semantic relation is.
From a more language generation perspec-tive, though, we might want to be able to select the appropriate preposition,given two nouns and a relation between the concepts they express.This translates into two different classification tasks.
One where thetarget categories are relations, the other where they are prepositions.
Inthe work we describe in this paper we concentrate on the first task.
Foreach preposition we build a supervised model where the target categoriescorrespond to the annotation tags for the semantic relations: CE, IS, LL,OP, RR.
As evaluation measures, we report accuracy and coverage.
Coverageamounts to the portion of data for which supersenses could be found for bothn1 and n2, thus providing insights in assessing the contribution of differentsupersense assignment methods (see Section 4.2 and Section 4.3).For assessing the difficulty of the task, beside inter-annotator agreement,we take a simple baseline where we assign to each CN the semantic relationwhich is most frequently associated with the CN?s preposition (Table 6).In the learning experiments, we use the Weka implementation (Wit-ten and Frank, 2000) of the sequential minimal optimization algorithm fortraining a support vector classifier, within a ten-fold cross-validation setting.Girju (2007) has shown SVMs to be most efficient for this task.4.1 Basic systemThe basic system uses as features only n1 and n2 as simple strings.
Table 6shows accuracy per preposition for the basic system and for the baseline.The most evident limitation of this basic approach is data sparseness.Out of 1700 CNs, 1662 involve a combination of n1 and n2 which occurs onlyonce, independently of the preposition used.
The most frequent n1 (?parte?,part) occurs 13 times with two different prepositions, and the most frequent53n2 (?lavoro?, job/work) 16 across four different prepositions.One intuitive way to alleviate the data sparseness problem without in-creasing the corpus size, is to cluster instances.
Following Girju (2007), whouses hypernyms obtained from WordNet (Fellbaum, 1998) in place of strings,we reduce each noun in our data set to its hypernym.
In this supersenseassignment, we experimented with two procedures: a more sophisticated oneinvolving sequential sense tagging, thus dealing with sense disambiguation,and a simpler one involving plain assignment of hypernyms.4.2 Hypernym selection via sense taggingTwo major problems related to finding a hypernym for a word are senseambiguity (one term can easily have more than one hypernym if it hasmore than one sense) and coverage (even large ontologies/databases mightnot include some of the encountered terms).
A supersense tagger alleviatessuch limitations by tagging words in context, thus tackling the ambiguityissue, and by using a combination of features rather than just the lexicalentry, thereby being able to classify also words that are not included in thedictionary.
Picca et al (2008) have developed such a tagger for Italian,building on an existing version for English (Ciaramita and Altun, 2006),retrained on MultiSemCor (Bentivogli and Pianta, 2005), a word-alignedEnglish-Italian corpus which contains the translation of the English textsin SemCor.
The set of 26 noun supersense labels come from MultiWordNet(Pianta et al, 2002), a multilingual lexical database in which the ItalianWordNet is strictly aligned with Princeton WordNet 1.6, and which is linkedto MultiSemCor.The average reported performance of the tagger is about 60% (Piccaet al, 2008).
This relatively low accuracy introduces a large portion of errorsin the classification, thus reducing the advantage of dealing with supersensesrather than words in the identification of semantic relations in CNs.
Errorscan be of three types: (i) the assignment of a wrong noun class, (ii) theassignment of a class of the wrong part-of-speech type (any non-noun tag),and (iii) the non-assignment of any class (tag ?0?).
Whereas errors of type(i) can only be spotted via manual investigation, mistakes of type (ii) and(iii) can be detected automatically and a backoff strategy can be deployed.In 228 CNs out of 1700 both nouns have been assigned a ?0?
tag.
In afurther 751 CNs, one of the two nouns is tagged as ?0?.
Out of these, thereare 33 cases where the other noun is assigned a non-noun tag (adj or verb).A non-noun tag for n1 or n2 is also found in a further 57 cases.As a backoff strategy for all cases that fall under (ii) and (iii), we searched54Table 7: Results using supersenses obtained via tagging, in combinationwith string features, and alone, and with and without backoff.no backoff backoffprep #inst cov%acc%#inst cov%acc%string nostring string nostringa 32 32.32 68.75 75.00 99 100 45.46 44.44con 11 37.93 72.73 63.64 29 100 62.07 58.62da 14 40.00 64.29 57.14 35 100 65.71 65.71di 526 39.61 58.55 51.71 1328 100 59.71 50.75in 36 36.00 63.89 61.11 100 100 64.00 62.00per 16 33.33 68.75 56.25 48 100 56.25 54.17tra 10 58.82 70.00 70.00 17 100 64.71 64.71su 20 45.45 45.00 50.00 44 100 54.54 47.73hypernyms directly in MultiWordNet (MWN).
(The set of possible hyper-nyms is identical to the set of the 26 supersenses used by the tagger.)
As afirst step, we lemmatised the string using Morph-it!, an existing lemmatiserfor Italian (Zanchetta and Baroni, 2005), since MWN contains lemmata butnot their morphological variants.
Whenever we found more than one synsetassociated to a term, a corresponding number of hypernyms was also found.If one of the hypernyms was recurring more than the others, this was se-lected.
Otherwise, the hypernym associated to the first sense was selected.4Whenever the lemmatised noun was not in MWN (106 cases), we assignedthe most frequent supersense in the dataset (?act?
for both n1 and n2).We then ran classification experiments using the obtained supersensesfor n1 and n2 as additional features, as well as on their own (thus ignoringthe original string?this is reported as ?nostring?
in Tables 7?8), both withand without the backoff strategy.
In the latter case, we excluded all CNswhere at least one of the two nouns had been tagged as a non-noun or had nosupersense assignment.
Under these settings coverage was seriously affected,but accuracy was generally higher than when deploying the backoff strategy.Table 7 reports results.4Optimally, we would select the hypernym for the most frequent sense (the one rankedfirst in Princeton WordNet).
However, synsets for a given term are not ordered by fre-quency in MWN.
One option would be to exploit frequencies from MultiSemCor, but thecorpus is rather small and might not be very reliable.55Table 8: Results using supersenses obtained via plain assignment, in com-bination with string features, and alone, and with and without backoff.no backoff backoffprep #inst cov%acc%#inst cov%acc%string nostring string nostringa 90 90.91 47.78 42.22 99 100 39.39 34.34con 26 89.65 61.54 57.69 29 100 55.17 55.17da 30 85.71 60.00 63.33 35 100 62.86 65.71di 1178 88.70 61.88 52.63 1328 100 60.54 51.13in 88 88.00 50.00 52.27 100 100 56.00 53.00per 41 85.42 65.85 60.98 48 100 56.25 47.92tra 14 82.35 42.86 42.86 17 100 47.06 35.29su 36 81.82 52.78 52.78 44 100 59.09 40.914.3 Hypernym selection via plain assignmentGiven the large number of cases where we had to resort to a backoff strategyon the tagger?s output, we tried to obtain hypernyms from MWN directly,thus bypassing the tagging stage.
Whenever necessary, we employed thebackoff strategies described above: most frequent hypernym found for anambiguous term (or first sense?s hypernym in case of equal frequency), andoverall most frequent assigned hypernym in the corpus (?act?
in this case aswell) for all those nouns that were not found in MWN.
This direct lookupapproach should improve on coverage but suffer more from ambiguity-relatedproblems.
Table 8 summarises the results.4.4 DiscussionUnder the best settings, at full coverage, our average performance is around59% (using tagger-assigned supersenses, backoff, the string feature), withwide variation across prepositions.
Given the currently limited set of fea-tures, results are in general promising, especially if compared to the inter-annotator agreement, and to previous work (see below).When using supersenses obtained from the tagger, results are steadilybetter than when using hypernyms directly looked up in MWN (both withand without backoff) with the exception of ?di?
and ?su?.
The low coveragebut higher accuracy yielded when using the tagger?s senses without resortingto a backoff strategy were both expected, as mentioned above.Results suggest that the utility of a backoff strategy varies from one56preposition to another.
For instance, for ?a?, ?con?, and ?per?, backoffappears to lower performance, independently on how the supersenses wereobtained.
These three prepositions had the three lowest coverage scoreswhen using the tagger, which suggests that if too large a proportion is leftto the approximation of backoff, the benefits of accurate sense tagging arelost.
This is not however true for the MWN lookup, where the coverage forthese three prepositions is rather high.Additionally, we can observe that in most cases, in the back-off settings,including the string as a feature helps improve the performance (both in thetagging and in the plain assignment).
This is likely due to the fact that theapproximation given by not having precise information about the supersenseand needing to resort to a backoff strategy is (partially) compensated bytaking into account the original noun.
In contrast, using the string withoutthe backoff strategy on the tagger?s output yields a decrease in performance,proving supersenses useful.For a better assessment of the actual contribution of using hypernymsfor detecting the semantic relation without incurring in the noise introducedby wrong hypernym assignments or the backoff strategy, we manually cor-rected the tagger?s output in 60% of the data.
This allowed us to evaluatethe tagger?s performance on supersense assignment for this 60% portion aswell as to compare on this subset, contaning 1024 CNs, an algorithm us-ing ?gold?
supersenses with that built on the tagger?s output (using thebackoff strategy, and including string features, see Section 4.2).
We foundthat supersenses were assigned by the tagger with an accuracy of 63.9%,a result in line with previously reported performance (Picca et al, 2008).We also observed that using the manually assigned hypernyms yielded anaverage improvement of about seven percentage points over using the tag-ger?s senses, although for some prepositions, instances in this smaller datasetwere just too few to draw any solid conclusion.
Although more accurate,the gold tags do not boost the performance as much as one might expect.On the one hand, this might suggest that hypernyms can contribute onlyto a certain extent to this task, and other more expressive features must befound.
On the other, it is also possible that the chosen set of 26 supersensesis too large, especially for a dataset like ours which is rather small, therebynot really overcoming the data sparseness problem.Comparison to previous work in terms of performance is not straight-forward, because of the language difference, the relation sets used, and theevaluation settings.
In the SemEval-2007 exercise, for example, for eachof the seven semantic relations used (see Table 1), a system must decidewhether a given instance expresses that relation or not within an ad hoc-57built dataset, so that the overall semantic relation identification of the taskis actually split in seven different binary classification tasks, one per relation.The highest reported average accuracy is 76.3% (Girju et al, 2007).Girju (2007) classifies noun-noun compounds in 22 different semanticrelations.
Best results on English are obtained when using a rich featureset including cross-linguistic information.
Reported figures differ slightly ac-cording to the dataset used, with an average accuracy of 76.1%.
When usingonly language-internal supersense features, the average accuracy is 44.15%.Girju (2007) also trains and tests another state-of-the-art supervised modelfor English, namely Semantic Scattering (Moldovan and Badulescu, 2005),reporting an average accuracy of 59.07%.5 Conclusions and future workWe have presented a framework for the annotation of Italian complex nom-inals in a very high data sparseness condition, and supervised models forthe identification of the underlying semantic relation for monosyllabic Ital-ian prepositions.
We exploited both string and supersense features, showingthat the importance of including string information varies from one prepo-sition to another and from whether we are using backoff strategies or not.We have also seen that for obtaining the supersenses, a sequential sense tag-ging approach yields better overall results than a simple lookup in MWN,although it dramatically cuts on coverage.Future work will involve further classification experiments with addi-tional features, including web counts obtained via lexico-syntactic patterns(Lapata and Keller, 2005; Nakov and Hearst, 2008).
We will exploit part ofthe annotation which we have not considered in this study (see Section 3),namely the type of prepositional phrase (see Appendix), a very general con-ceptual clustering which also marks metaphorical usage, the position of tra-jector and landmark in the CN, and the order of the head and the modifier.ReferencesBentivogli, L. and E. Pianta (2005).
Exploiting parallel texts in the cre-ation of multilingual semantically annotated resources: the MultiSemCorCorpus.
Natural Language Engineering 11 (3), 247?261.Busa, F. and M. Johnston (1996).
Cross-linguistic semantics for complexnominals in the generative lexicon.
In AISB Workshop on Multilingualityin the Lexicon.58Celli, F. (2008).
La semantica delle preposizioni italiane nella combinazioneconcettuale.
Master thesis in Linguistics, Universita` di Bologna.Ciaramita, M. and Y. Altun (2006).
Broad-coverage sense disambiguationand information extraction with a supersense sequence tagger.
In Pro-ceedings of EMNLP 2006, pp.
594?602.Cimiano, P. and J. Wenderoth (2005).
Automatically learning qualia struc-tures from the web.
In Proceedings of the ACL-SIGLEX Workshop onDeep Lexical Acquisition, Ann Arbor, Michigan, pp.
28?37.Cohen, J.
(1960).
A coefficient of agreement for nominal scales.
Educationaland Psychological Measurement 20, 37?46.Downing, P. (1977).
On the creation and use of English compound nouns.Language 53, 810?842.Fellbaum, C.
(Ed.)
(1998).
WordNet: An Electronic Lexical Database.
Cam-bridge: MIT Press.Girju, R. (2007).
Improving the interpretation of noun phrases with cross-linguistic information.
In Proceedings of ACL?07, pp.
568?575.Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret(2007, June).
SemEval-2007 Task 04: Classification of Semantic Relationsbetween Nominals.
In Proceedings of SemEval-2007, pp.
13?18.Johnston, M. and F. Busa (1996).
Qualia structure and the compositionalinterpretation of compounds.
In Proceedings of the ACL Workshop onbreadth and depth of semantic lexicons.Langacker, R.W.
(1987).
Foundations of cognitive grammar.
Univ.
Press.Lapata, M. (2002).
The disambiguation of nominalisations.
ComputationalLinguistics 28 (3), 357?388.Lapata, M. and F. Keller (2005).
Web-based models for natural languageprocessing.
ACM Transactions on Speech and Language Processing 2.Laudanna, A., A. Thornton, G. Brown, C. Burani, and L. Marconi (1995).Un corpus dell?italiano scritto contemporaneo dalla parte del ricevente.
InS.
Bolasco, L. Lebart, and A. Salem (Eds.
), III Giornate internazionalidi Analisi Statistica dei Dati Testuali.
Volume I, pp.
103?109.
Cisu.Lauer, M. (1995).
Corpus statistics meet the noun compound: some empir-ical results.
In Proceedings of ACL?95.Levi, J.
(1978).
The Syntax and Semantics of Complex Nominals.
AcademicPress.Moldovan, D. and A. Badulescu (2005).
A semantic scattering model forthe automatic interpretation of genitives.
In Proceedings of HLT-EMNLP2005, pp.
891?898.59Nakov, P. and M. A. Hearst (2008).
Solving relational similarity problemsusing the web as a corpus.
In Proceedings of ACL-08: HLT, Columbus,Ohio, pp.
452?460.
Association for Computational Linguistics.Nastase, V. and S. Szpakowicz (2003).
Exploring noun-modifier semanticrelations.
In Proceedings of IWCS-5, pp.
285?301.Pantel, P. and M. Pennacchiotti (2006).
Espresso: Leveraging generic pat-terns for automatically harvesting semantic relations.
In Proceedings ofACL?06, Sydney, Australia, pp.
113?120.Pianta, E., L. Bentivogli, and C. Girardi (2002).
MultiWordNet: developingan aligned multilingual database.
In Proceedings of the First InternationalConference on Global WordNet, pp.
293?302.Picca, D., A. M. Gliozzo, and M. Ciaramita (2008).
Supersense Tagger forItalian.
In Proceedings of LREC 2008.Pustejovsky, J.
(1995).
The Generative Lexicon.
The MIT Press.Rosario, B. and M. Hearst (2001).
Classifying the semantic relations innoun compounds via a domain-specific lexical hierarchy.
In L. Lee andD.
Harman (Eds.
), Proceedings of EMNLP 2001, pp.
82?90.Rossini Favretti, R. (2000).
Progettazione e costruzione di un corpus di ital-iano scritto: CORIS/CODIS.
In R. Rossini Favretti (Ed.
), Linguistica einformatica.
Multimedialita`, corpora e percorsi di apprendimento.
Bulzoni.Schmid, H. (1994).
Probabilistic part-of-speech tagging using decision trees.In Proc.
of the Conference on New Methods in Language Processing, 44-49.Turney, P. D. (2006).
Expressing implicit semantic relations without super-vision.
In Proceedings of ACL?06, Sydney, Australia, pp.
313?320.Warren, B.
(1978).
Semantic patterns of noun-noun compounds.
GothenburgStudies in English 41.Witten, I. H. and E. Frank (2000).
Data Mining: Practical Machine LearningTools and Techniques with Java Implementations.
Morgan Kaufmann.Zanchetta, E. and M. Baroni (2005).
Morph-it!
a free corpus-based morpho-logical resource for the italian language.
Corpus Linguistics 2005 1 (1).Zingarelli, N. (2008).
Lo Zingarelli 2008.
Vocabolario della Lingua Italiana.Zanichelli.60
