Proceedings of EACL '99Japanese Dependency Structure AnalysisBased on Maximum Entropy ModelsK iyotaka  Uch imoto  t Satosh i  Sekine$ H i tosh i  I sahara  ttCommunicat ions Research LaboratoryMinistry of Posts and Telecommunications588-2, Iwaoka, Iwaoka-cho, Nishi-kuKobe, Hyogo, 651-2401, Japan\[uchimot o i isahara\] ?crl.
go.
j pSNew York University715 Broadway, 7th floorNew York, NY 10003, USAsekine~cs, nyu.
eduAbst rac tThis paper describes a dependencystructure analysis of Japanese sentencesbased on the maximum entropy mod-els.
Our model is created by learningthe weights of some features from a train-ing corpus to predict he dependency be-tween bunsetsus or phrasal units.
Thedependency accuracy of our system is87.2% using the Kyoto University cor-pus.
We discuss the contribution of eachfeature set and the relationship betweenthe number of training data and the ac-curacy.1 I n t roduct ionDependency structure analysis is one of the ba-sic techniques in Japanese sentence analysis.
TheJapanese dependency structure is usually repre-sented by the relationship between phrasal unitscalled 'bunsetsu.'
The analysis has two concep-tual steps.
In the first step, a dependency matrixis prepared.
Each element of the matrix repre-sents how likely one bunsetsu is to depend on theother.
In the second step, an optimal set of de-pendencies for the entire sentence is found.
Inthis paper, we will mainly discuss the first step, amodel for estimating dependency likelihood.So far there have been two different approachesto estimating the dependency likelihood, One isthe rule-based approach, in which the rules arecreated by experts and likelihoods are calculatedby some means, including semiautomatic corpus-based methods but also by manual assignment ofscores for rules.
However, hand-crafted rules havethe following problems.?
They have a problem with their coverage.
Be-cause there are many features to find correctdependencies, it is difficult o find them man-ually.?
They also have a problem with their consis-tency, since many of the features competewith each other and humans cannot createconsistent rules or assign consistent scores.?
As syntactic haracteristics differ across dif-ferent domains, the rules have to be changedwhen the target domain changes.
It is costlyto create a new hand-made rule for each do-main.At/other approach is a fully automatic orpus-based approach.
This approach as the poten-tial to overcome the problems of the rule-basedapproach.
It automatically learns the likelihoodsof dependencies from a tagged corpus and calcu-lates the best dependencies for an input sentence.We take this approach.
This approach is taken bysome other systems (Collins, 1996; Fujio and Mat-sumoto, 1998; Haruno et ah, 1998).
The parserproposed by Ratnaparkhi (Ratnaparkhi, 1997) isconsidered to be one of the most accurate parsersin English.
Its probability estimation is based onthe maximum entropy models.
We also use themaximum entropy model.
This model earns theweights of given features from a training corpus.The weights are calculated based on the frequen-cies of the features in the training data.
The set offeatures i  defined by a human.
In our model, weuse features of bunsetsu, such as character strings,parts of speech, and inflection types of bunsetsu,as well as information between bunsetsus, uch asthe existence of punctuation, and the distance be-tween bunsetsus.
The probabilities of dependen-cies are estimated from the model by using thosefeatures in input sentences.
We assume that theoverall dependencies in a whole sentence can bedetermined as the product of the probabilities ofall the dependencies in the sentence.196Proceedings of EACL '99Now, we briefly describe the algorithm of de-pendency analysis.
It is said that Japanese de-pendencies have the following characteristics.
(1) Dependencies are directed from left to right(2) Dependencies do not cross(3) A bunsetsu, except for the rightmost one, de-pends on only one bunsetsu(4) In many cases, the left context is not neces-sary to determine a dependency 1The analysis method proposed in this paper is de-signed to utilize these features.
Based on theseproperties, we detect the dependencies in a sen-tence by analyzing it backwards (from right toleft).
In the past, such a backward algorithm hasbeen used with rule-based parsers (e.g., (Fujita,1988)).
We applied it to our statistically basedapproach.
Because of the statistical property, wecan incorporate a beam search, an effective way oflimiting the search space in a backward analysis.2 The  Probab i l i ty  Mode lGiven a tokenization of a test corpus, the prob-lem of dependency structure analysis in Japanesecan be reduced to the problem of assigning oneof two tags to each relationship which consists oftwo bunsetsus.
A relationship could be tagged as"0" or "1" to indicate whether or not there is adependency between the bunsetsus, respectively.The two tags form the space of "futures" for amaximum entropy formulation of our dependencyproblem between bunsetsus.
A maximum entropysolution to this, or any other similar problem al-lows the computation of P( f \ [h)  for any f from thespace of possible futures, F, for every h from thespace of possible histories, H. A "history" in max-imum entropy is all of the conditioning data whichenables you to make a decision among the spaceof futures.
In the dependency problem, we couldreformulate this in terms of finding the probabil-ity of f associated with the relationship at indext in the test corpus as:P( f \ ]ht )  = P ( f l  Information derivablefrom the test corpusrelated to relationship t)The computation of P( f \ ]h)  in M.E.
is depen-dent on a set of '`features" which, hopefully, arehelpful in making a prediction about the future.Like most current M.E.
modeling efforts in com-putational linguistics, we restrict ourselves to fea-tures which are binary functions of the history andaAssumption (4) has not been discussed very much,but our investigation with humans howed that it istrue in more than 90% of cases.future.
For instance, one of our features isg 1 :g(h, f )  =t 0 :Here "has(h,z)" is a binary function which re-turns true if the history h has an attribute x. Wefocus on attributes on a bunsetsu itself and thosebetween bunsetsus.
Section 3 will mention theseattributes.Given a set of features and some training data,the maximum entropy estimation process pro-duces a model in which every feature gi has as-sociated with it a parameter ai.
This allows usto compute the conditional probability as follows(Berger et al, 1996):P(f lh)  - Y I ia \ [  '(n'l) z~(h) (2)~,i ?
(3)I iThe maximum entropy estimation techniqueguarantees that for every feature gi, the expectedvalue of gi according to the M.E.
model will equalthe empirical expectation of gi in the training cor-pus.
In other words:y\]~ P(h, f).
g,(h, f)h,!= y-~P(h).y~P~(Slh)-g,(h,1).
(41h !Here /3 is an empirical probability and PME isthe probability assigned by the M.E.
model.We assume that dependencies in a sentence areindependent of each other and the overall depen-dencies in a sentence can be determined based onthe product of probability of all dependencies inthe sentence.if has(h, x) = ture,= "Poster ior -  Head-POS(Major) : ~\[J'~(verb)" (1)&f=lotherwise.3 Exper iments  and  D iscuss ionIn our experiment, we used the Kyoto Universitytext corpus (version 2) (Kurohashi and Nagao,1997), a tagged corpus of the Mainichi newspaper.For training we used 7,958 sentences from news-paper articles appearing from January 1st to Jan-uary 8th, and for testing we used 1,246 sentencesfrom articles appearing on January 9th.
The inputsentences were morphologically analyzed and theirbunsetsus were identified.
We assumed that thispreprocessing was done correctly before parsinginput sentences.
If we used automatic morpholog-ical analysis and bunsetsu identification, the pars-ing accuracy would not decrease so much becausethe rightmost element in a bunsetsu is usually acase marker, a verb ending, or a adjective end-ing, and each of these is easily recognized.
Theautomatic preprocessing by using public domain197Proceedings of EACL '99tools, for example, can achieve 97% for morpho-logical analysis (Kitauchi et al, 1998) and 99% forbunsetsu identification (Murata et al, 1998).We employed the Maximum Entropy tool madeby Ristad (Ristad, 1998), which requires one tospecify the number of iterations for learning.
Weset this number to 400 in all our experiments.In the following sections, we show the featuresused in our experiments and the results.
Then wedescribe some interesting statistics that we foundin our experiments.
Finally, we compare our workwith some related systems.3.1 Results  of  Exper imentsThe features used in our experiments are listed inTables 1 and 2.
Each row in Table 1 contains afeature type, feature values, and an experimentalresult that will be explained later.
Each featureconsists of a type and a value.
The features arebasically some attributes of a bunsetsu itself orthose between bunsetsus.
We call them 'basic fea-tures.'
The list is expanded from tIaruno's list(Haruno et al, 1998).
The features in the list areclassified into five categories that are related tothe "Head" part of the anterior bunsetsu (cate-gory "a"), the '~rype" part of the anterior bun-setsu (category "b"), the "Head" part of the pos-terior bunsetsu (category "c"), the '~l~ype " partof the posterior bunsetsu (category "d"), and thefeatures between bunsetsus (category "e") respec-tively.
The term "Head" basically means a right-most content word in a bunsetsu, and the term"Type" basically means a function word followinga "Head" word or an inflection type of a "Head"word.
The terms are defined in the following para-graph.
The features in Table 2 are combinationsof basic features ('combined features').
They arerepresented by the corresponding category nameof basic features, and each feature set is repre-sented by the feature numbers of the correspond-ing basic features.
They are classified into ninecategories we constructed manually.
For exam-ple, twin features are combinations of the featuresrelated to the categories %" and "c." Triplet,quadruplet and quintuplet features basically con-sist of the twin features plus the features of theremainder categories "a," "d" and "e." The to-tal number of features is about 600,000.
Amongthem, 40,893 were observed in the training corpus,and we used them in our experiment.The terms used in the table are the following:Anter ior:  left bunsetsu of the dependencyPosterior:  right bunsetsu of the dependencyHead: the rightmost word in a bunsetsu otherthan those whose major part-of-speech 2 cat-egory is "~ (special marks)," "1~ (post-positional particles)," or " ~  (suffix)"2Part-of-speech categories follow those of JU-MAN(Kurohashi and Nagao, 1998).Head-Lex:  the fundamental form (uninflectedform) of the head word.
Only words witha frequency of three or more are used.Head-Inf :  the inflection type of a headType: the rightmost word other than thosewhose major part-of-speech category is "~(special marks)."
If the major category ofthe word is neither "IIJJ~-~-\] (post-positional par-ticles)" nor "~\[~:~.
(suffix)," and the word isinflectable 3, then the type is represented bythe inflection type.JOSt i I l :  the rightmost post-positional particlein the bunsetsuJOStt I2 :  the second rightmost post-positionalparticle in the bunsetsu if there are two ormore post-positional particles in the bunsetsuTOUTEN,  WA: TOUTEN means if a comma(Touten) exists in the bunsetsu.
WA meansif the word WA (a topic marker) exists in thebunsetsuBW:  BW means "between bunsetsus"BW-Distance:  the distance between the bunset-susBW-TOUTEN:  if TOUTEN exists betweenbunsetsusBW- IDto -Anter io r -Type:BW-IDto-Anterior-Type means if there is abunsetsu whose type is identical to that ofthe anterior bunsetsu between bunsetsusBW- IDto -Anter io r -Type-Head-P  OS: thepart-of-speech category of the head word ofthe bunsetsu of "BW-IDto-Anterior-Type"BW- IDto -Poster io r -Head:  if there is betweenbunsetsus a bunsetsu whose head is identicalto that of the posterior bunsetsuBW- IDto -Poster io r -  Head-Type(St r ing) :the lexical information of the bunsetsu "BW-IDto-Posterior-Head"The results of our experiment are listed in Ta-ble 3.
The dependency accuracy means the per-centage of correct dependencies out of all depen-dencies.
The sentence accuracy means the per-centage of sentences in which all dependencieswere analyzed correctly.
We used input sentencesthat had already been morphologically analyzedand for which bunsetsus had been identified.
Thefirst line in Table 3 (deterministic) shows the ac-curacy achieved when the test sentences were an-alyzed deterministically (beam width k = 1).
Thesecond line in Table 3 (best beam search) showsthe best accuracy among the experiments whenchanging the beam breadth k from 1 to 20.
Thebest accuracy was achieved when k = 11, althoughthe variation in accuracy was very small.
This re-sult supports assumption (4) in Chapter 1 because3The inflection types follow those of JUMAN.198Proceedings of EACL  '99Category \] Feature number \[ Feature typeTable 1: Features (basic features)Bas ic  features  (5 categor ies ,  43 types)  \[?
Feature values .
.
.
(Number of values) Accuracy without I each feature12a 3456789b 10111213141516171819202122232425262728293031323334353637383940414243Anterior-Head-LexAnter ior-Head-POS(Major)Anter ior-Head-POS(Minor)Anter ior-Head-lnf(Major)Anterior-Head-I  nf(Minor)Anter ior-Type(Str ing)Anter ior-Type(Major)Anter ior-Type(Minor)Anterior-J  OSHl l (St r ing)Anterior- JOSHI 1/Minor )Anterior-J  OSHI2(Str ing)Anterior- JOSHI2(Minor)Anter ior-punctuat ionAnterior-bracket-openAnterior-bracket-close(2204)(verb), ~I#~-\] (adjective), ~ (noun) .... (117~1~ ~\] (common noun), ~ (quantif ier) .
.
.
.
(24)~j \ [ t \ ]~  (vowel verb) .
.
.
.
(307~(stem) ,  ~r~ (fundamental form) .
.
.
.
(6O)~,  ~ a, ~c L-C, ~ ,  &, tO, t .
.
.
.
(73)(post-posit ional particle), (43):~\]\]J3~ (case marker),  ~.zx.~ ( imperat ive form).
.
.
.
(lO2)~b,  ~'~*, a)Jk, ~, ~t~., ... (63)\[nil\], ;~ J~ (case marker) .... (5)YJ'~:', ~ ,  A e', ,\];:, ~*, .
.
.
(63);~gJJ~ (case marker) .... (4)\[ml\], comma, pemod (3)nil ,\[nil\]' /< , ,  , >, :: 111 ,Posterior-Head-LexPost erior- Head- P OS (Maj or)Posterior-Head-POS (Minor)Posterior-Head-Inf(Maj or 7Post erior-Head-Inf(Minor)Posterior-Type(String)Posterior-Type(Major)Posterior-Type(Minor~Poster ior - JOSHl l (St rmg)Poster ior - JOSHI l (Minor)Posterior- J OS HI2( St r ing)Posterior- JOSHI 2(Minor)Posterior- punct UatlonPost erior-bracket- openPosterior-bracket-closeBW-Dist anceBW-TOU'I'EINBW-WABW-bracketsBW-IDt o-Ant erior-TypeBW-  IDto-Anterior-Type-Head-POS(Major)B W- IDt o-Ant erior-Type-Head-POS(Minor)BW-  IDto-Ant erior-Type-Head-lnf(Major)BW-  IDtc-Ant erior-Type-Head-lnf(Minor)BW-IDto-Posterior-HeadBW-  IDto-Posterior- Head-Type(String)BW-  IDt o- Posterior-Head-Type(Major)BW- IDt o-Post erior-Head-Type(Minor)The same values as those of feature number 1.The same values as those of feature number 2.The same values as those of feature number 3.The same values as those of feature number 4.The same values as those of feature number 5.The same values as those of feature number 6.The same values as those of feature number 7.The same values as those of feature number 8.The same values as those of feature number 9.The same values as those of feature number 10.The same values as those of feature number 11.The same values as those of feature number 12.The same values as those of feature number 13.The same values as those of feature number 14.The same values as those of feature number 15.A(1), B~2 ~ 5), C(6 or more) (3)\[nil\], \[extstJ (2~\[hill, \[exist\] (27\[nil\], close, open, open-close (4)\[nil\], \[existJ (2)The same values as those of feature number 2.The same values as those of feature number 3.The same values as those of feature number 4.The same values as those of feature number 5.\[nilJ, \[exist\] (2)The same values as those of feature number 6.The same values as those of feature number 7.The same values as those of feature number 8.86.96% (-0.16%)86.43% (--0.71%)87.14% (4-0%)69.73% (--17.41%)87.11% (-0.03%)87.08% (-0.06%)85.47~ (-- 1.67v?87.12% ~--0.02%87.10% (--0.04%86.31% ( -0 .83%76.15~ (--10.99%)87.14% (4---0% 786.06% ( -  1.08%)87.16% (+0.02% 787.11% ( -0 .03%)s4.62~ (-2.52%)s6.s7z ~-o.27~'o)66.85% (-0.29%)84.64% (-2.50%)66.81% (-0.33%)86.96% (--0.18,%)86.08% ~-- 1.06%)86.99% (--0.15%)86.75% (-o.39%)Combination typeTwin  features :related to the "Type" part ofthe anterior bunsetsu and the"Head" part of the posteriorbunsetsu.T r ip le t  features:basically consist of the twinfeatures plus the featuresbetween bunsetsus.Quadrup le t  features:basically consist of the twinfeatures plus the featuresrelated to the "Head" part ofthe anterior bunsetsu, and the"Type" part of the posteriorbunsetsu.Table  2: Features (combined features)Combined  features (9 categories, 134 types)Combinat ionsCategory(b, c)(bx, b2, c)(b, c, e)(d l ,  d2, e)(b l ,  b2, c, d)(b, c, el, e2)(a, b, c, d)Feature setb = {6, 7, 8}, c = {16, 17, 18}(b l ,  b2) = {(9, 11),(10, 12)}, c = {17, 18}b = {6, 7, 8}, c = {17, lS},e = {31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43}(dl, d,, e) = (29, 30, 34)b I = {6, 7, 8}, c = {17, 18},(b2, d) = (13, 28)b = {6, 7, 8), c = {17, 18} , (e l ,e2)  = (35, 40)(a, c) = {(1, 16), (2, 17), (3, 18)},(b, d) = {(6, 21), (7, 22), (8, 23)}Accuracy withoutthe feature86.99% (-o.15%)66.47%(-0.67%)85.65% ( -1 .49%)Quintup le t  features:  (a, b l ,  b2, c, d) (a, c) = {(2, 17), (3, 18)}, 86.96% ( -0 .18%)basically consist of the (bl ,  b2) = {(9, 11), (I0, 12)}, d = {21,22,23}quadruplet features plus the (a, b, c, d, e) (a, c) = {(1, 16), (2, 17), (3, 18)},features between bunsetsus.
(b, d) = {(6, 21), (7, 22), (8, 23}, e = 31199Proceedings of EACL '99Table 3: Results of dependency analysisDeterministic (k = 1)Best beam search(k = 11)BaselineDependency accuracy87.14%(9814/11263)87.21%(9822/11263)64.09%(7219/11263)Sentence accuracy40.60% (503/1239)40.60% (503/1239)6.38% (79/1239)1.00.8714 .
.
.
.
.
.
.0.8Dependency accuracy0.60.40.2- - ~ - -, , i i I i10 20 30Number of bunsetsus in a sentenceFigure 1: Relationship between the number of bunsetsus in a sentence and dependency accuracy.it shows that the previous context has almost noeffect on the accuracy.
The last line in Table 3 rep-resents the accuracy when we assumed that everybunsetsu depended on the next one (baseline).Figure 1 shows the relationship between thesentence length (the number of bunsetsus) andthe dependency accuracy.
The data for sentenceslonger than 28 segments are not shown, becausethere was at most one sentence of each length.Figure 1 shows that the accuracy degradation dueto increasing sentence length is not significant.For the entire test corpus the average running timeon a SUN Sparc Station 20 was 0.08 seconds persentence.3.2 Features  and  AccuracyThis section describes how much each feature setcontributes to improve the accuracy.The rightmost column in Tables 1 and 2 showsthe performance of the analysis without each fea-ture set.
In parenthesis, the percentage of im-provement or degradation to the formal experi-ment is shown.
In the experiments, when a basicfeature was deleted, the combined features thatincluded the basic feature were also deleted.We also conducted some experiments in whichseveral types of features were deleted together.The results are shown in Table 4.
All of the resultsin the experiments were carried out deterministi-cally (beam width k = 1).The results shown in Table 1 were very closeto our expectation.
The most useful features arethe type of the anterior bunsetsu and the part-of-speech tag of the head word on the posteriorbunsetsu.
Next important features are the dis-tance between bunsetsus, the existence of punctu-ation in the bunsetsu, and the existence of brack-ets.
These results indicate preferential rules withrespect o the features.The accuracy obtained with the lexical fea-tures of the head word was better than thatwithout them.
In the experiment with the fea-tures, we found many idiomatic expressions, forexample, "~,, 15-C (oujile, according to ) - -  b}~b(kimeru, decide)" and "~'~" (katachi_de, in theform of ) - -  ~b~ (okonawareru, be held)."
Wewould expect to collect more of such expressionsif we use more training data.The experiments without some combined fea-tures are reported in Tables 2 and 4.
As canbe seen from the results, the combined featuresare very useful to improve the accuracy.
We usedthese combined features in addition to the basicfeatures because we thought that the basic fea-tures were actually related to each other.
With-out the combined features, the features are inde-pendent of each other in the maximum entropyframework.We manually selected combined features, whichare shown in Table 2.
If we had used all combi-200Proceedings of EACL '99Table 4: Accuracy without several types of featuresFeaturesWithout features 1 and 16 (lexical information about the head word)Without features 35 to 43Without quadruplet and quintuplet featuresWithout triplet, quadruplet, and quintuplet featuresWithout all combinationsAccuracy86.30% (-0.84%)86.83% (-0.31%)84.27% (-2.87%)81.28% (-5.86%)68.83% (-18.31%)nations, the number of combined features wouldhave been very large, and the training wouldnot have been completed on the available ma-chine.
Furthermore, we found that the accuracydecreased when several new features were addedin our preliminary experiments.
So, we shouldnot use all combinations of the basic features.
Weselected the combined features based on our intu-ition.In our future work, we believe some methodsfor automatic feature selection should be studied.One of the simplest ways of selecting features isto select features according to their frequencies inthe training corpus.
But using this method in ourcurrent experiments, the accuracy decreased in allof the experiments.
Other methods that have beenproposed are one based on using the gain (Bergeret al, 1996) and an approximate method for se-lecting informative features (Shirai et al, 1998a),and several criteria for feature selection were pro-posed and compared with other criteria (Bergerand Printz, 1998).
We would like to try thesemethods.Investigating the sentences which could not beanalyzed correctly, we found that many of thosesentences included coordinate structures.
We be-lieve that coordinate structures can be detected toa certain extent by considering new features whichtake a wide range of information into account.3.3 Number  of  Tra in ing Data  andAccuracyFigure 2 shows the relationship between the num-ber of training data (the number of sentences) andthe accuracy.
This figure shows dependency accu-racies for the training corpus and the test corpus.Accuracy of 81.84% was achieved even with a verysmall training set (250 sentences).
We believe thatthis is due to the strong characteristic of the max-imum entropy framework to the data sparsenessproblem.
From the learning curve, we can expecta certain amount of improvement if we have moretraining data.3.4 Compar ison  wi th  Re la ted  WorksThis section compares our work with relatedstatistical dependency structure analyses inJapanese.Comparison withShirai's work (Shirai et al, 1998b)Shirai proposed a framework of statistical lan-guage modeling using several corpora: the EDRcorpus, RWC corpus, and Kyoto University cor-pus.
He combines a parser based on a hand-madeCFG and a probabilistic dependency model.
Healso used the maximum entropy model to estimatethe dependency probabilities between two or threepost-positional particles and a verb.
Accuracy of84.34% was achieved using 500 test sentences oflength 7 to 9 bunsetsus.
In both his and our ex-periments, the input sentences were morphologi-cally analyzed and their bunsetsus were identified.The comparison of the results cannot strictly bedone because the conditions were different.
How-ever, it should be noted that the accuracy achievedby our model using sentences of the same lengthwas about 3% higher than that of Shirai's model,although we used a much smaller set of trainingdata.
We believe that it is because his approachis based on a hand-made CFG.Comparison with Ehara's work (Ehara, 1998)Ehara also used the Maximum Entropy model,and a set of similar kinds of features to ours.
How-ever, there is a big difference in the number of fea-tures between Ehara's model and ours.
Besidesthe difference in the number of basic features,Ehara uses only the combination of two features,but we also use triplet, quadruplet, and quintupletfeatures.
As shown in Section 3.2, the accuracy in-creased more than 5% using triplet or larger com-binations.
We believe that the difference in thecombination features between Ehara's model andours may have led to the difference in the accuracy.The accuracy of his system was about 10% lowerthan ours.
Note that Ehara used TV news articlesfor training and testing, which are different fromour corpus.
The average sentence length in thosearticles was 17.8, much longer than that (average:10.0) in the Kyoto University text corpus.Comparison withFujio's work (Fujio and Matsumoto, 1998)and Haruno's work (Haruno et al, 1998)Fujio used the Maximum Likelihood modelwith similar features to our model in his parser.Haruno proposed a parser that uses decision tree201Proceedings of EACL '99A0<O,.94929088868482800'2raining" - -*-"testing .
.
.
.
.
.
.. .
.
.
,+.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.+- .
.
.
.
.
.
.
.
.
.
.
./4I I I I I I I1000 2000 3000 4000 6000 6000 7000 8000Number o!
Training Data (sentences)Figure 2: Relationship between the number of training data and the parsing accuracy.
(beam breadthk=l )models and a boosting method.
It is difficult todirectly compare these models with ours becausethey use a different corpus, the EDR corpus whichis ten times as large as our corpus, for trainingand testing, and the way of collecting test datais also different.
But they reported an accuracyof around 85%, which is slightly worse than ourmodel.We carried out two experiments using almostthe same attributes as those used in their exper-iments.
The results are shown in Table 5, wherethe lines "Feature set(l)" and "Feature set(2)"show the accuracies achieved by using Fujio'sattributes and Haruno's attributes respectively.Considering that both results are around 85% to86%, which is about the same as ours.
From theseexperiments, we believe that the important factorin the statistical approaches i  not the model, i.e.Maximum Entropy, Maximum Likelihood, or De-cision Tree, but the feature selection.
However,it may be interesting to compare these modelsin terms of the number of training data, as wecan imagine that some models are better at cop-ing with the data sparseness problem than others.This is our future work.4 Conc lus ionThis paper described a Japanese dependencystructure analysis based on the maximum en-tropy model.
Our model is created by learningthe weights of some features from a training cor-pus to predict the dependency between bunset-sus or phrasal units.
The probabilities of depen-dencies between bunsetsus are estimated by thismodel.
The dependency accuracy of our systemwas 87.2% using the Kyoto University corpus.In our experiments without the feature setsshown in Tables 1 and 2, we found that some basicand combined features trongly contribute to im-prove the accuracy.
Investigating the relationshipbetween the number of training data and the accu-racy, we found that good accuracy can be achievedeven with a very small set of training data.
Webelieve that the maximum entropy framework hassuitable characteristics for overcoming the datasparseness problem.There are several future directions.
In particu-lar, we are interested in how to deal with coordi-nate structures, since that seems to be the largestproblem at the moment.Re ferencesAdam Berger and Harry Printz.
1998.
A com-parison of criteria for maximum entropy / min-imum divergence feature selection.
Proceedingsof Third Conference on Empirical Methods inNatural Language Processing, pages 97-106.Adam L. Berger, Stephen A. Della Pietra, andVincent J. Della Pietra.
1996.
A maximum en-tropy approach to natural anguage processing.Computational Linguistics, 22(1):39-71.Michael Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
Proceed-ings of the 34th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL),pages 184-191.Terumasa Ehara.
1998.
Japanese bunsetsu de-pendency estimation using maximum entropymethod.
Proceedings of The Fourth Annual202Proceedings of EACL '99Table 5: Simulation of Fujio's and Haruno's experimentsFeature setFeature set (1)(Without features 4, 5, 9--12, 14, 15, 19, 20, 24--27, 29, 30, 34--43.
)Feature set (2)(Without features 4, 5, 9--12, 19, 20, 24--27, 34-43.
)Accuracy85.71% (-1.43%)86.47% (-0.67%)Meeting of The Association for Natural Lan-guage Processing, pages 382-385.
(in Japanese).Masakazu Fujio and Yuuji Matsumoto.
1998.Japanese dependency structure analysis basedon lexicalized statistics.
Proceedings of ThirdConference on Empirical Methods in NaturalLanguage Processing, pages 87-96.Katsuhiko Fujita.
1988.
A deterministic parserbased on karari-uke grammar, pages 399-402.Masahiko Haruno, Satoshi Shiral, and YoshifumiOoyama.
1998.
Using decision trees to con-struct a practical parser.
Proceedings of theCOLING-ACL '98.Akira Kitauchi, Takehito Utsuro, and Yuji Mat-sumoto.
1998.
Error-driven model learningof Japanese morphological analysis.
IPSJ-WGNL, NL124-6:41--48.
(in Japanese).Sadao Kurohashi and Makoto Nagao.
1997.
Ky-oto university text corpus project, pages 115-118.
(in Japanese).Sadao Kurohashi and Makoto Nagao, 1998.Japanese Morphological Analysis System JU-MAN version 3.5.
Department of Informatics,Kyoto University.Masaki Murata, Kiyotaka Uchimoto, Qing Ma,and Hitoshi Isahara.
1998.
Machine learningapproach to bunsetsu identification - -  compar-ison of decision tree, maximum entropy model,example-based approach, and a new method us-ing category-exclusive rules --.
IPSJ-WGNL,NL128-4:23-30.
(in Japanese).Adwait Ratnaparkhi.
1997.
A linear observedtime statistical parser based on maximum en-tropy models.
Conference on Empirical Meth-ods in Natural Language Processing.Eric Sven Ristad.
1998.
Maximum en-tropy modeling toolkit, release 1.6 beta.http ://www.mnemonic.com/software/memt.Kiyoaki Shirai, Kentaro Inui, Takenobu Toku-naga, and I-Iozumi Tanaka.
1998a.
Learningdependencies between case frames using max-imum entropy method, pages 356-359.
(inJapanese).Kiyoaki Shirai, Kentaro Inui, Takenobu Toku-naga, and Hozumi Tanaka.
1998b.
A frame-work of integrating syntactic and lexical statis-tics in statistical parsing.
Journal of Nat-ural Language Processing, 5(3):85-106.Japanese).
(in203
