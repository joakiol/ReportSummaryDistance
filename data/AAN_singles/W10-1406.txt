Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 49?57,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsFactors Affecting the Accuracy of Korean ParsingTagyoung Chung, Matt Post and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe investigate parsing accuracy on the Ko-rean Treebank 2.0 with a number of differentgrammars.
Comparisons among these gram-mars and to their English counterparts suggestdifferent aspects of Korean that contribute toparsing difficulty.
Our results indicate that thecoarseness of the Treebank?s nonterminal setis a even greater problem than in the EnglishTreebank.
We also find that Korean?s rela-tively free word order does not impact parsingresults as much as one might expect, but infact the prevalence of zero pronouns accountsfor a large portion of the difference betweenKorean and English parsing scores.1 IntroductionKorean is a head-final, agglutinative, and mor-phologically productive language.
The languagepresents multiple challenges for syntactic pars-ing.
Like some other head-final languages suchas German, Japanese, and Hindi, Korean exhibitslong-distance scrambling (Rambow and Lee, 1994;Kallmeyer and Yoon, 2004).
Compound nouns areformed freely (Park et al, 2004), and verbs havewell over 400 paradigmatic endings (Martin, 1992).Korean Treebank 2.0 (LDC2006T09) (Han andRyu, 2005) is a subset of a Korean newswire corpus(LDC2000T45) annotated with morphological andsyntactic information.
The corpus contains roughly5K sentences, 132K words, and 14K unique mor-phemes.
The syntactic bracketing rules are mostlythe same as the previous version of the treebank(Han et al, 2001) and the phrase structure annota-tion schemes used are very similar to the ones usedin Penn English treebank.
The Korean Treebank isconstructed over text that has been morphologicallyanalyzed; not only is the text tokenized into mor-phemes, but all allomorphs are neutralized.To our knowledge, there have been only a few pa-pers focusing on syntactic parsing of Korean.
Herm-jakob (2000) implemented a shift-reduce parser forKorean trained on very limited (1K sentences) data,and Sarkar and Han (2002) used an earlier versionof the Treebank to train a lexicalized tree adjoininggrammar.
In this paper, we conduct a range of ex-periments using the Korean Treebank 2.0 (hereafter,KTB) as our training data and provide analyses thatreveal insights into parsing morphologically rich lan-guages like Korean.
We try to provide comparisonswith English parsing using parsers trained on a simi-lar amount of data wherever applicable.2 Difficulties parsing KoreanThere are several challenges in parsing Korean com-pared to languages like English.
At the root of manyof these challenges is the fact that it is highly in-flected and morphologically productive.
Effectivemorphological segmentation is essential to learninggrammar rules that can generalize beyond the train-ing data by limiting the number of out-of-vocabularywords.
Fortunately, there are good techniques for do-ing so.
The sentences in KTB have been segmentedinto basic morphological units.Second, Korean is a pro-drop language: subjectsand objects are dropped wherever they are pragmati-cally inferable, which is often possible given its richmorphology.
Zero pronouns are a remarkably fre-quent phenomenon in general (Han, 2006), occuring49an average of 1.8 times per sentence in the KTB.The standard approach in parsing English is to ig-nore NULL elements entirely by removing them (andrecursively removing unary parents of empty nodesin a bottom-up fashion).
This is less of a problem inEnglish because these empty nodes are mostly traceelements that denote constituent movement.
In theKTB, these elements are removed altogether and acrucial cue to grammatical inference is often lost.Later we will show the profound effect this has onparsing accuracy.Third, word order in Korean is relatively free.This is also partly due to the richer morphology,since morphemes (rather than word order) are usedto denote semantic roles of phrases.
Consider thefollowing example:??
????
??
???
.John-NOM Mary-DAT book-ACC give-PAST .In the example, any permutation of the first threewords produces a perfectly acceptable sentence.This freedom of word order could potentially resultin a large number of rules, which could complicateanalysis with new ambiguities.
However, formalwritten Korean generally conforms to a canonicalword order (SOV).3 Initial experimentsThere has been some work on Korean morphologi-cal analysis showing that common statistical meth-ods such as maximum entropy modeling and condi-tional random fields perform quite well (Lee et al,2000; Sarkar and Han, 2002; Han and Palmer, 2004;Lee and Rim, 2005).
Most claim accuracy rate over95%.
In light of this, we focus on the parsing part ofthe problem utilizing morphology analysis alreadypresent in the data.3.1 SetupFor our experiments we used all 5,010 sentences inthe Korean Treebank (KTB), which are already seg-mented.
Due to the small size of the corpus, we usedten-fold cross validation for all of our experiments,unless otherwise noted.
Sentences were assigned tofolds in blocks of one (i.e., fold 1 contained sen-tences 1, 11, 21, and so on.).
Within each fold, 80%of the data was assigned to training, 10% to devel-opment, and 10% to testing.
Each fold?s vocabularymodel F1 F1?40 types tokensKorean 52.78 56.55 6.6K 194KEnglish (?02?03) 71.06 72.26 5.5K 96KEnglish (?02?04) 72.20 73.29 7.5K 147KEnglish (?02?21) 71.61 72.74 23K 950KTable 1: Parser scores for Treebank PCFGs in Koreanand English.
For English, we vary the size of the trainingdata to provide a better point of comparison against Ko-rean.
Types and tokens denote vocabulary sizes (whichfor Korean is the mean over the folds).was set to all words occurring more than once in itstraining data, with a handful of count one tokens re-placing unknown words based on properties of theword?s surface form (all Korean words were placedin a single bin, and English words were binned fol-lowing the rules of Petrov et al (2006)).
We reportscores on the development set.We report parser accuracy scores using the stan-dard F1 metric, which balances precision and recallof the labeled constituents recovered by the parser:2PR/(P + R).
Throughout the paper, all evalua-tion occurs against gold standard trees that containno NULL elements or nonterminal function tags orannotations, which in some cases requires the re-moval of those elements from parse trees output bythe parser.3.2 Treebank grammarsWe begin by presenting in Table 1 scores for thestandard Treebank grammar, obtained by reading astandard context-free grammar from the trees in thetraining data and setting rule probabilities to rela-tive frequency (Charniak, 1996).
For these initialexperiments, we follow standard practice in Englishparsing and remove all (a) nonterminal function tagsand (b) NULL elements from the parse trees beforelearning the grammar.
For comparison purposes, wepresent scores from parsing the Wall Street Journalportion of the English Penn Treebank (PTB), usingboth the standard training set and subsets of it cho-sen to be similar in size to the KTB.
All Englishscores are tested on section 22.There are two interesting results in this table.First, Korean parsing accuracy is much lower thanEnglish parsing accuracy, and second, the accuracydifference does not appear to be due to a differencein the size of the training data, since reducing the50size of the English training data did not affect accu-racy scores very much.Before attempting to explain this empirically, wenote that Rehbein and van Genabith (2007) demon-strate that the F1 metric is biased towards parse treeswith a high ratio of nonterminals to terminals, be-cause mistakes made by the parser have a smallereffect on the overall evaluation score.1 They rec-ommend that F1 not be used for comparing parsingaccuracy across different annotation schemes.
Thenonterminal to terminal ratio in the KTB and PTBare 0.40 and 0.45, respectively.
It is a good idea tokeep this bias in mind, but we believe that this smallratio difference is unlikely to account for the hugegap in scores displayed in Table 1.The gap in parsing accuracy is unsurprising inlight of the basic known difficulties parsing Korean,summarized earlier in the paper.
Here we observe anumber of features of the KTB that contribute to thisdifficulty.Sentence length On average, KTB sentences aremuch longer than PTB sentences (23 words versus48 words, respectively).
Sentence-level F1 is in-versely correlated with sentence length, and the rel-atively larger drop in F1 score going from column 3to 2 in Table 1 is partially accounted for by the factthat column 3 represents 33% of the KTB sentences,but 92% of the English sentences.Flat annotation scheme The KTB makes rela-tively frequent use of very flat and ambiguous rules.For example, consider the extreme cases of rule am-biguity in which the lefthand side nonterminal ispresent three or more times on its righthand side.There are only three instances of such ?triple+-recursive?
NPs among the?40K trees in the trainingportion of the PTB, each occurring only once.NP?
NP NP NP , CC NPNP?
NP NP NP CC NPNP?
NP NP NP NP .The KTB is an eighth of the size of this, but hasfifteen instances of such NPs (listed here with theirfrequencies):1We thank one of our anonymous reviewers for bringing thisto our attention.NP?
NP NP NP NP (6)NP?
NP NP NP NP NP (3)NP?
NP NP NP NP NP NP (2)NP?
NP NP NP NP NP NP NP (2)NP?
SLQ NP NP NP SRQ PAD (1)NP?
SLQ NP NP NP NP SRQ PAN (1)Similar rules are common for other nonterminals aswell.
Generally, flatter rules are easier to parse withbecause they contribute to parse trees with fewernodes (and thus fewer independent decision points).However, the presence of a single nonterminal onboth the left and righthand side of a rule means thatthe annotation scheme is failing to capture distribu-tional differences which must be present.Nonterminal granularity This brings us to a finalpoint about the granularity of the nonterminals in theKTB.
After removing function tags, there are only43 nonterminal symbols in the KTB (33 of thempreterminals), versus 72 English nonterminals (44of them preterminals).
Nonterminal granularity isa well-studied problem in English parsing, and thereis a long, successful history of automatically refin-ing English nonterminals to discover distributionaldifferences.
In light of this success, we speculatethat the disparity in parsing performance might beexplained by this disparity in the number of nonter-minals.
In the next section, we provide evidence thatthis is indeed the case.4 Nonterminal granularityThere are many ways to refine the set of nontermi-nals in a Treebank.
A simple approach suggestedby Johnson (1998) is to simply annotate each nodewith its parent?s label.
The effect of this is to re-fine the distribution of each nonterminal over se-quences of children according to its position in thesentence; for example, a VP beneath an SBAR nodewill have a different distribution over children than aVP beneath an S node.
This simple technique aloneproduces a large improvement in English Treebankparsing.
Klein and Manning (2003) expanded thisidea with a series of experiments wherein they manu-ally refined nonterminals to different degrees, whichresulted in parsing accuracy rivaling that of bilexi-calized parsing models of the time.
More recently,Petrov et al (2006) refined techniques originallyproposed by Matsuzaki et al (2005) and Prescher51SBJ subject with nominative case markerOBJ complement with accusative case markerCOMP complement with adverbial postpositionADV NP that function as adverbial phraseVOC noun with vocative case makerLV NP coupled with ?light?
verb constructionTable 2: Function tags in the Korean treebankmodel F1 F1?40Koreancoarse 52.78 56.55w/ function tags 56.18 60.21English (small)coarse 72.20 73.29w/ function tags 70.50 71.78English (standard)coarse 71.61 72.74w/ function tags 72.82 74.05Table 3: Parser scores for Treebank PCFGs in Koreanand English with and without function tags.
The smallEnglish results were produced by training on ?02?04.
(2005) for automatically learning latent annotations,resulting in state of the art parsing performance withcubic-time parsing algorithms.We begin this section by conducting some sim-ple experiments with the existing function tags, andthen apply the latent annotation learning proceduresof Petrov et al (2006) to the KTB.4.1 Function tagsThe KTB has function tags that mark grammaticalfunctions of NP and S nodes (Han et al, 2001),which we list all of them in Table 2.
These functiontags are principally grammatical markers.
As men-tioned above, the parsing scores for both Englishand Korean presented in Table 1 were produced withgrammars stripped of their function tags.
This isstandard practice in English, where the existing tagsare known not to help very much.
Table 3 presentsresults of parsing with grammars with nonterminalsthat retain these function tags (we include resultsfrom Section 3 for comparison).
Note that evalua-tion is done against the unannotated gold standardparse trees by removing the function tags after pars-ing with them.The results for Korean are quite pronounced:we see a nearly seven-point improvement when re-taining the existing tags.
This very strongly sug-gests that the KTB nonterminals are too coarsewhen stripped of their function tags, and raises thequestion of whether further improvement might begained from latent annotations.The English scores allow us to make another point.Retaining the provided function tags results in asolid performance increase with the standard train-ing corpus, but actually hurts performance whentraining on the small dataset.
Note clearly that thisdoes not suggest that parsing performance with thegrammar from the small English data could not beimproved with latent annotations (indeed, we willshow that they can), but only that the given annota-tions do not help improve parsing accuracy.
Takingthe Korean and English accuracy results from this ta-ble together provides another piece of evidence thatthe Korean nonterminal set is too coarse.4.2 Latent annotationsWe applied the latent annotation learning proceduresof Petrov et al2 to refine the nonterminals in theKTB.
The trainer learns refinements over the coarseversion of the KTB (with function tags removed).
Inthis experiment, rather than doing 10-fold cross vali-dation, we split the KTB into training, development,and test sets that roughly match the 80/10/10 splitsof the folds:section file IDstraining 302000 to 316999development 317000 to 317999testing 320000 to 320999This procedure results in grammars which can thenbe used to parse new sentences.
Table 4 displays theparsing accuracy results for parsing with the gram-mar (after smoothing) at the end of each split-merge-smooth cycle.3 The scores in this table show that,just as with the PTB, nonterminal refinement makesa huge difference in parser performance.Again with the caveat that direct comparison ofparsing scores across annotation schemes must betaken loosely, we note that the KTB parsing accu-racy is still about 10 points lower than the best ac-2http://code.google.com/p/berkeleyparser/3As described in Petrov et al (2006), to score a parse treeproduced with a refined grammar, we can either take the Viterbiderivation or approximate a sum over derivations before project-ing back to the coarse tree for scoring.52Viterbi max-sumcycle F1 F1?40 F1 F1?401 56.93 61.11 61.04 64.232 63.82 67.94 66.31 68.903 69.86 72.83 72.85 75.634 74.36 77.15 77.18 78.185 78.07 80.09 79.93 82.046 78.91 81.55 80.85 82.75Table 4: Parsing accuracy on Korean test data from thegrammars output by the Berkeley state-splitting grammartrainer.
For comparison, parsing all sentences of ?22 inthe PTB with the same trainer scored 89.58 (max-sumparsing with five cycles) with the standard training corpusand 85.21 when trained on ?2?4.curacy scores produced in parsing the PTB which,in our experiments, were 89.58 (using max-sum toparse all sentences with the grammar obtained afterfive cycles of training).An obvious suspect for the difference in parsingaccuracy with latent grammars between English andKorean is the difference in training set sizes.
Thisturns out not to be the case.
We learned latent anno-tations on sections 2?4 of the PTB and again testedon section 22.
The accuracy scores on the test setpeak at 85.21 (max-sum, all sentences, five cycles oftraining).
This is about five points lower than the En-glish grammar trained on sections 2?21, but is stillover four points higher than the KTB results.In the next section, we turn to one of the theoret-ical difficulties with Korean parsing with which webegan the paper.5 NULL elementsBoth the PTB and KTB include many NULL ele-ments.
For English, these elements are traces de-noting constituent movement.
In the KTB, thereare many more kinds of NULL elements, in includ-ing trace markers, zero pronouns, relative clause re-duction, verb deletions, verb ellipsis, and other un-known categories.
Standard practice in English pars-ing is to remove NULL elements in order to avoidthe complexity of parsing with ?-productions.
How-ever, another approach to parsing that avoids suchproductions is to retain the NULL elements whenreading the grammar; at test time, the parser is givensentences that contain markers denoting the emptyelements.
To evaluate, we remove these elementsmodel F1 F1?40 tokensEnglish (standard training corpus)coarse 71.61 72.74 950Kw/ function tags 72.82 74.05 950Kw/ NULLs 73.29 74.38 1,014KKoreanw/ verb ellipses 52.85 56.52 3,200w/ traces 55.88 59.42 3,868w/ r.c.
markers 56.74 59.87 3,794w/ zero pronouns 57.56 61.17 4,101latent (5) w/ NULLs 89.56 91.03 22,437Table 5: Parser scores for Treebank PCFGs in Englishand Korean with NULL elements.
Tokens denotes thenumber of words in the test data.
The latent grammarwas trained for five iterations.from the resulting parse trees output by the parserand compare against the stripped-down gold stan-dard used in previous sections, in order to providea fair point of comparison.Parsing in this manner helps us to answer the ques-tion of how much easier or more difficult parsingwould be if the NULL elements were present.
Inthis section, we present results from a variety of ex-periments parsing will NULL tokens in this manner.These results can be seen in Table 5.
The first ob-servation from this table is that in English, retainingNULL elements makes a few points difference.The first four rows of the KTB portion of Table 5contains results with retaining different classes ofNULL elements, one at a time, according to the man-ner described above.
Restoring deleted pronounsand relative clause markers has the largest effect,suggesting that the absence of these optional ele-ments removes key cues needed for parsing.In order to provide a more complete picture ofthe effect of empty elements, we train the Berkeleylatent annotation system on a version of the KTBin which all empty elements are retained.
The fi-nal row of Table 5 contains the score obtained whenevaluating parse trees produced from parsing withthe grammar after the fifth iteration (after which per-formance began to fall).
With the empty elements,we have achieved accuracy scores that are on parwith the best accuracy scores obtained parsing theEnglish Treebank.536 Tree substitution grammarsWe have shown that coarse labels and the prevalenceof NULL elements in Korean both contribute to pars-ing difficulty.
We now turn to grammar formalismsthat allow us to work with larger fragments of parsetrees than the height-one rules of standard context-free grammars.
Tree substitution grammars (TSGs)have been shown to improve upon the standard En-glish Treebank grammar (Bod, 2001) in parser ac-curacy, and more recently, techniques for inferringTSG subtrees in a Bayesian framework have enabledlearning more efficiently representable grammars,permitting some interesting analysis (O?Donnell etal., 2009; Cohn et al, 2009; Post and Gildea, 2009).In this section, we try parsing the KTB with TSGs.We experiment with different methods of learningTSGs to see whether they can reveal any insightsinto the difficulties parsing Korean.6.1 Head rulesTSGs present some difficulties in learning and rep-resentation, but a simple extraction heuristic calleda spinal grammar has been shown to be very use-ful (Chiang, 2000; Sangati and Zuidema, 2009; Postand Gildea, 2009).
Spinal subtrees are extractedfrom a parse tree by using a set of head rules tomaximally project each lexical item (a word or mor-pheme).
Each node in the parse tree having a differ-ent head from its parent becomes the root of a newsubtree, which induces a spinal TSG derivation inthe parse tree (see Figure 1).
A probabilistic gram-mar is derived by taking counts from these trees,smoothing them with counts of all depth-one rulesfrom the same training set, and setting rule probabil-ities to relative frequency.This heuristic requires a set of head rules, whichwe present in Table 6.
As an evaluation of our rules,we list in Table 7 the accuracy results for parsingwith spinal grammars extracted using the head ruleswe developed as well as with two head rule heuris-tics (head-left and head-right).
As a point of compar-ison, we provide the same results for English, usingthe standard Magerman/Collins head rules for En-glish (Magerman, 1995; Collins, 1997).
Functiontags were retained for Korean but not for English.We observe a number of things from Table 7.First, the relative performance of the head-left andNT RC ruleS SFN second rightmost childVV EFN rightmost XSVVX EFN rightmost VJ or COADJP EFN rightmost VJCV EFN rightmost VVLV EFN rightmost VVNP EFN rightmost COVJ EFN rightmost XSV or XSJVP EFN rightmost VX, XSV, or VV?
?
rightmost childTable 6: Head rules for the Korean Treebank.
NT is thenonterminal whose head is being determined, RC identi-fies the label of its rightmost child.
The default is to takethe rightmost child as the head.head-right spinal grammars between English andKorean capture the linguistic fact that English is pre-dominantly head-first and Korean is predominantlyhead-final.
In fact, head-finalness in Korean was sostrong that our head rules consist of only a handfulof exceptions to it.
The default rule makes headsof postpositions (case and information clitics) suchas dative case marker and topic marker.
It is thesewords that often have dependencies with words inthe rest of the sentence.
The exceptions concernpredicates that occur in the sentence-final position.As an example, predicates in Korean are composedof several morphemes, the final one of which indi-cates the mood of the sentence.
However, this mor-pheme often does not require any inflection to re-flect long-distance agreement with the rest of thesentence.
Therefore, we choose the morpheme thatwould be considered the root of the phrase, whichin Korean is the verbalization/adjectivization suf-fix, verb, adjective, auxiliary predicate, and copula(XSV, XSJ, VV, VJ, VX, CO).
These items often in-clude the information about valency of the predicate.Second, in both languages, finer-grained specifi-cation of head rules results in performance abovethat of the heuristics (and in particular, the head-left heuristic for English and head-right heuristic forKorean).
The relative improvements in the two lan-guages are in line with each other: significant, butnot nearly as large as the difference between thehead-left and head-right heuristics.Finally, we note that the test results together sug-gest that parsing with spinal grammars may be a54(a) TOPSNP-SBJNPR???NNC??PAU?VPNP-ADVDAN?NNC?VPVVNNC??XSV??EPF?EFN?SFN.
(b) SNP-SBJNPR??
?NNC PAUVP SFN(c) SNP-SBJ VP SFN.Figure 1: (a) A KTB parse tree; the bold nodes denote the top-level spinal subtree using our head selection rules.
(b)The top-level spinal subtree using the head-left and (c) head-right extraction heuristics.
A gloss of the sentence isDoctor Schwartz was fired afterward.model F1 F1?40 sizeKoreanspinal (head left) 59.49 63.33 49Kspinal (head right) 66.05 69.96 29Kspinal (head rules) 66.28 70.61 29KEnglishspinal (head left) 77.92 78.94 158Kspinal (head right) 72.73 74.09 172Kspinal (head rules) 78.82 79.79 189KTable 7: Spinal grammar scores on the KTB and on PTBsection 22.good evaluation of a set of head selection rules.6.2 Induced tree substitution grammarsRecent work in applying nonparametric machinelearning techniques to TSG induction has shown thatthe resulting grammars improve upon standard En-glish treebank grammars (O?Donnell et al, 2009;Cohn et al, 2009; Post and Gildea, 2009).
Thesetechniques use a Dirichlet Process prior over the sub-tree rewrites of each nonterminal (Ferguson, 1973);this defines a model of subtree generation that pro-duces new subtrees in proportion to the number oftimes they have previously been generated.
Infer-ence under this model takes a treebank and usesGibbs sampling to determine how to deconstruct aparse tree into a single TSG derivation.
In this sec-tion, we apply these techniques to Korean.This TSG induction requires one to specify a basemeasure, which assigns probabilities to subtrees be-ing generated for the first time in the model.
Onebase measure employed in previous work scored asubtree by multiplying together the probabilities ofthe height-one rules inside the subtree with a ge-ometric distribution on the number of such rules.Since Korean is considered to be a free word-orderlanguage, we modified this base measure to treat thechildren of a height-one rule as a multiset (instead ofa sequence).
This has the effect of producing equiva-lence classes among the sets of children of each non-terminal, concentrating the mass on these classes in-stead of spreading it across their different instantia-tions.To build the sampled grammars, we initialized thesamplers from the best spinal grammar derivationsand ran them for 100 iterations (once again, func-tion tags were retained).
We then took the state ofthe training data at every tenth iteration, smoothedtogether with the height-one rules from the standardTreebank.
The best score on the development datafor a sampled grammar was 68.93 (all sentences)and 73.29 (sentences with forty or fewer words):well above the standard Treebank scores from ear-lier sections and above the spinal heuristics, but wellbelow the scores produced by the latent annotationlearning procedures (a result that is consistent withEnglish).This performance increase reflects the results forEnglish demonstrated in the above works.
We see alarge performance increase above the baseline Tree-bank grammar, and a few points above the bestspinal grammar.
One nice feature of these inducedTSGs is that the rules learned lend themselves toanalysis, which we turn to next.6.3 Word orderIn addition to the base measure mentioned above,we also experimented with the standard base mea-55NPNPR NNC?
?NNU NNX?Figure 2: Example of a long distance dependency learnedby TSG induction.sure proposed by Cohn et al and Post & Gildea, thattreats the children of a nonterminal as a sequence.The grammars produced sampling under a modelwith this base measure were not substantively differ-ent from those of the unordered base measure.
A par-tial explanation for this is that although Korean doespermit a significant amount of reordering relative toEnglish, the sentences in the KTB come from writ-ten newswire text, where word order is more stan-dardized.
Korean sentences are characterized as hav-ing a subject-object-verb (SOV) word order.
Thereis some flexibility; OSV, in particular, is commonin spoken Korean.
In formal writing, though, SOVword order is overwhelmingly preferred.
We see thisreflected in the KTB, where SOV sentences are 63.5times more numerous that OSV among sentencesthat have explicitly marked both the subject and theobject.
However, word order is not completely fixedeven in the formal writing.
NP-ADV is most likelyto occur right before the VP it modifies, but can bemoved earlier.
For example,S?
NP-SBJ NP-ADV VPis 2.4 times more frequent than the alternative withthe order of the NPs reversed.Furthermore, the notion of free(er) word orderdoes not apply to all constituents.
An example isnonterminals directly above preterminals.
A Koreanverb may have up to seven affixes; however, they al-ways agglutinate in a fixed order.6.4 Long distance dependenciesThe TSG inference procedure can be thought ofas discovering structural collocations in parse trees.The model prefers subtrees that are common in thedata set and that comprise highly probable height-one rules.
The parsing accuracy of these grammarsis well below state of the art, but the grammars aresmaller, and the subtrees learned can help us analyzethe parse structure of the Treebank.
One particularclass of subtree is one that includes multiple lexicalitems with intervening nonterminals, which repre-sent long distance dependencies that commonly co-occur.
In Korean, a certain class of nouns must ac-company a particular class of measure word (a mor-pheme) when counting the noun.
In the exampleshown in Figure 2, (NNC ??)
(members of as-sembly) is followed by NNU, which expands to in-dicate ordinal, cardinal, and numeral nouns; NNU isin turn followed by (NNX?
), the politeness neutralmeasure word for counting people.7 Summary & future workIn this paper, we addressed several difficult aspectsof parsing Korean and showed that good parsing ac-curacy for Korean can be achieved despite the smallsize of the corpus.Analysis of different parsing results from differ-ent grammatical formalisms yielded a number ofuseful observations.
We found, for example, that theset of nonterminals in the KTB is not differentiatedenough for accurate parsing; however, parsing accu-racy improves substantially from latent annotationsand state-splitting techniques that have been devel-oped with English as a testbed.
We found that freerword order may not be as important as might havebeen thought from basic a priori linguistic knowl-edge of Korean.The prevalence of NULL elements in Korean isperhaps the most interesting difficulty in develop-ing good parsing approaches for Korean; this isa key difference from English parsing that to ourknowledge is not addressed by any available tech-niques.
One potential approach is a special an-notation of parents with deleted nodes in order toavoid conflating rewrite distributions.
For example,S ?
VP is the most common rule in the Koreantreebank after stripping away empty elements; how-ever, this is a result of condensing the rule S?
(NP-SBJ *pro*) VP and S?VP, which presumably havedifferent distributions.
Another approach would beto attempt automatic recovery of empty elements asa pre-processing step.Acknowledgments We thank the anonymous re-viewers for their helpful comments.
This workwas supported by NSF grants IIS-0546554 and ITR-0428020.56ReferencesRens Bod.
2001.
What is the minimal set of fragmentsthat achieves maximal parse accuracy?
In Proc.
ACL,Toulouse, France, July.Eugene Charniak.
1996.
Tree-bank grammars.
In Proc.of the National Conference on Artificial Intelligence,pages 1031?1036.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProc.
ACL, Hong Kong.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In Proc.
NAACL.Michael Collins.
1997.
Three penerative, lexicalisedmodels for statistical parsing.
In Proc.
ACL/EACL.Thomas S. Ferguson.
1973.
A Bayesian analysis ofsome nonparametric problems.
Annals of Mathemat-ical Statistics, 1(2):209?230.Chung-Hye Han and Martha Palmer.
2004.
A mor-phological tagger for Korean: Statistical tagging com-bined with corpus-based morphological rule applica-tion.
Machine Translation, 18(4):275?297.Na-Rae Han and Shijong Ryu.
2005.
Guidelines forPenn Korean Treebank version 2.0.
Technical report,IRCS, University of Pennsylvania.Chung-hye Han, Na-Rae Han, and Eon-Suk Ko.
2001.Bracketing guidelines for Penn Korean Treebank.Technical report, IRCS, University of Pennsylvania.Na-Rae Han.
2006.
Korean zero pronouns: analysis andresolution.
Ph.D. thesis, University of Pennsylvania,Philadelphia, PA, USA.Ulf Hermjakob.
2000.
Rapid parser development: a ma-chine learning approach for Korean.
In Proc.
NAACL,pages 118?123, May.Mark Johnson.
1998.
PCFGmodels of linguistic tree rep-resentations.
Computational Linguistics, 24(4):613?632.Laura Kallmeyer and SinWon Yoon.
2004.
Tree-localMCTAG with shared nodes: Word order variation inGerman and Korean.
In Proc.
TAG+7, Vancouver,May.Dan Klein and Chris Manning.
2003.
Accurate unlexi-calized parsing.
In Proc.
ACL.Do-Gil Lee and Hae-Chang Rim.
2005.
Probabilisticmodels for Korean morphological analysis.
In Com-panion to the Proceedings of the International JointConference on Natural Language Processing, pages197?202.Sang-zoo Lee, Jun-ichi Tsujii, and Hae-Chang Rim.2000.
Hidden markov model-based Korean part-of-speech tagging considering high agglutinativity, word-spacing, and lexical correlativity.
In Proc.
ACL.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proc.
ACL.Samuel E. Martin.
1992.
Reference Grammar of Korean:A Complete Guide to the Grammar and History of theKorean Language.
Tuttle Publishing.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc.
ACL, Ann Arbor, Michigan.Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.Tenenbaum.
2009.
Fragment grammar: Exploringreuse in hierarchical generative processes.
Technicalreport, MIT.Seong-Bae Park, Jeong-Ho Chang, and Byoung-TakZhang.
2004.
Korean compound noun decompositionusing syllabic information only.
In ComputationalLinguistics and Intelligent Text Processing (CICLing),pages 146?157.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
COLING/ACL, Syd-ney, Australia, July.Matt Post and Daniel Gildea.
2009.
Bayesian learning ofa tree substitution grammar.
In Proc.
ACL, Singapore,Singapore, August.Detlef Prescher.
2005.
Inducing head-driven PCFGswith latent heads: Refining a tree-bank grammar forparsing.
Machine Learning: ECML 2005, pages 292?304.Owen Rambow and Young-Suk Lee.
1994.
Word ordervariation and tree-adjoining grammar.
ComputationalIntelligence, 10:386?400.Ines Rehbein and Josef van Genabith.
2007.
Eval-uating evaluation measures.
In Proceedings of the16th Nordic Conference of Computational Linguistics(NODALIDA).Federico Sangati and Willem Zuidema.
2009.
Unsuper-vised methods for head assignments.
In Proc.
EACL.Anoop Sarkar and Chung-hye Han.
2002.
Statisticalmorphological tagging and parsing of Korean with anLTAG grammar.
In Proc.
TAG+6.57
