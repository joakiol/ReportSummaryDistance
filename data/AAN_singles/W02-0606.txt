Unsupervised discovery of morphologically related words based onorthographic and semantic similarityMarco Baroni?OFAISchottengasse 3A-1010 Vienna, Austriamarco@oefai.atJohannes Matiasek?OFAISchottengasse 3A-1010 Vienna, Austriajohn@oefai.atHarald TrostIMKAIFreyung 6A-1010 Vienna, Austriaharald@ai.univie.ac.atAbstractWe present an algorithm that takes anunannotated corpus as its input, and re-turns a ranked list of probable morpho-logically related pairs as its output.
Thealgorithm tries to discover morphologi-cally related pairs by looking for pairsthat are both orthographically and seman-tically similar, where orthographic simi-larity is measured in terms of minimumedit distance, and semantic similarity ismeasured in terms of mutual information.The procedure does not rely on a mor-pheme concatenation model, nor on dis-tributional properties of word substrings(such as affix frequency).
Experimentswith German and English input give en-couraging results, both in terms of pre-cision (proportion of good pairs found atvarious cutoff points of the ranked list),and in terms of a qualitative analysis ofthe types of morphological patterns dis-covered by the algorithm.1 IntroductionIn recent years, there has been much interest in com-putational models that learn aspects of the morphol-ogy of a natural language from raw or structureddata.
Such models are of great practical interest astools for descriptive linguistic analysis and for mini-mizing the expert resources needed to develop mor-phological analyzers and stemmers.
From a theo-retical point of view, morphological learning algo-rithms can help answer questions related to humanlanguage acquisition.In this study, we present a system that, given acorpus of raw text from a language, returns a rankedlist of probable morphologically related word pairs.For example, when run with the Brown corpus asits input, our system returned a list with pairs suchas pencil/pencils and structured/unstructured at thetop.Our algorithm is completely knowledge-free, inthe sense that it processes raw corpus data, and itdoes not require any form of a priori informationabout the language it is applied to.
The algorithmperforms unsupervised learning, in the sense that itdoes not require a correctly-coded standard to (iter-atively) compare its output against.The algorithm is based on the simple idea that acombination of formal and semantic cues should beexploited to identify morphologically related pairs.In particular, we use minimum edit distance to mea-sure orthographic similarity,1 and mutual informa-tion to measure semantic similarity.
The algo-rithm does not rely on the notion of affix, and itdoes not depend on global distributional propertiesof substrings (such as affix frequency).
Thus, atleast in principle, the algorithm is well-suited todiscover pairs that are related by rare and/or non-concatenative morphological processes.The algorithm returns a list of related pairs, butit does not attempt to extract the patterns that relatethe pairs.
As such, it can be used as a tool to pre-1Given phonetically transcribed input, our model wouldcompute phonetic similarity instead of orthographic similarity.July 2002, pp.
48-57.
Association for Computational Linguistics.ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,Morphological and Phonological Learning: Proceedings of the 6th Workshop of theprocess corpus data for an analysis to be performedby a human morphologist, or as the first step of afully automated morphological learning program, tobe followed, for example, by a rule induction pro-cedure that extracts correspondence patterns frompaired forms.
See the last section of this paper forfurther discussion of possible applications.We tested our model with German and Englishinput.
Our results indicate that the algorithm isable to identify a number of pairs related by a va-riety of derivational and inflectional processes witha remarkably high precision rate.
The algorithm isalso discovering morphological relationships (suchas German plural formation with umlaut) that wouldprobably be harder to discover using affix-based ap-proaches.The remainder of the paper is organized as fol-lows: In section 2, we shortly review related work.In section 3, we present our model.
In section 4, wediscuss the results of experiments with German andEnglish input.
Finally, in section 5 we summarizeour main results, we sketch possible directions thatour current work could take, and we discuss somepotential uses for the output of our algorithm.2 Related workFor space reason, we discuss here only three ap-proaches that are closely related to ours.
See, forexample, Goldsmith (2001) for a very different (pos-sibly complementary) approach, and for a review ofother relevant work.2.1 Jacquemin (1997)Jacquemin (1997) presents a model that automati-cally extracts morphologically related forms from alist of English two-word medical terms and a corpusfrom the medical domain.The algorithm looks for correspondences betweentwo-word terms and orthographically similar pairsof words that are adjacent in the corpus.
For exam-ple, the list contains the term artificial ventilation,and the corpus contains the phrase artificially ven-tilated.
Jacquemin?s algorithm thus postulates the(paired) morphological analyses artificial ventilat-ion and artificial-ly ventilat-ed.Similar words, for the purposes of this pairingprocedure, are simply words that share a commonleft substring (with constraints that we do not dis-cuss here).Jacquemin?s procedure then builds upon theseearly steps by clustering together sets that follow thesame patterns, and using these larger classes to lookfor spurious analyses.
Finally, the algorithm tries tocluster classes that are related by similar, rather thanidentical, suffixation patterns.
Again, we will notdescribe here how this is accomplished.Our basic idea is related to that of Jacquemin, butwe propose an approach that is more general bothin terms of orthography and in terms of semantics.In terms of orthography, we do not require that twostrings share the left (or right) substring in order toconstitute a candidate pair.
Thus, we are not limitedto affixal morphological patterns.
Moreover, our al-gorithm extracts semantic information directly fromthe input corpus, and thus it does not require a pre-compiled list of semantically related pairs.2.2 Schone and Jurafsky (2000)Schone and Jurafsky (2000) present a knowledge-free unsupervised model in which orthography-based distributional cues are combined with seman-tic information automatically extracted from wordco-occurrence patterns in the input corpus.They first look for potential suffixes by search-ing for frequent word-final substrings.
Then, theylook for potentially morphologically related pairs,i.e., pairs that end in potential suffixes and share theleft substring preceding those suffixes.
Finally, theylook, among those pairs, for those whose semanticvectors (computed using latent semantic analysis)are significantly correlated.
In short, the idea behindthe semantic component of their model is that wordsthat tend to co-occur with the same set of words,within a certain window of text, are likely to be se-mantically correlated words.While we follow Schone and Jurafsky?s idea ofcombining orthographic and semantic cues, our al-gorithm differs from them in both respects.
From thepoint of view of orthography, we rely on the com-parison between individual word pairs, without re-quiring that the two pairs share a frequent affix, andindeed without requiring that they share an affix atall.From the point of view of semantics, we computescores based on mutual information instead of latentsemantic analysis.
Thus, we only look at the co-occurrence patterns of target words, rather than atthe similarity of their contexts.Future research should try to assess to what extentthese two approaches produce significantly differentresults, and/or to what extent they are complemen-tary.2.3 Yarowsky and Wicentowski (2000)Yarowsky and Wicentowski (2000) propose an algo-rithm that extracts morphological rules relating rootsand inflected forms of verbs (but the algorithm canbe extended to other morphological relations).Their algorithm performs unsupervised, but notcompletely knowledge-free, learning.
It requiresa table of canonical suffixes for the relevant partsof speech of the target language, a list of the con-tent word roots with their POS (and some informa-tion about the possible POS/inflectional features ofother words), a list of the consonants and vowels ofthe language, information about some characteristicsyntactic patterns and, if available, a list of functionwords.The algorithm uses a combination of differentprobabilistic models to find pairs that are likely to bemorphologically related.
One model matches root+ inflected form pairs that have a similar frequencyprofile.
Another model matches root + inflectedform pairs that tend to co-occur with the same sub-jects and objects (identified using simple regular ex-pressions).
Yet another model looks for words thatare orthographically similar, in terms of a minimumedit distance score that penalizes consonant changesmore than vowel changes.
Finally, the rules relatingstems and inflected forms that the algorithm extractsfrom the pairs it finds in an iteration are used as afourth probabilistic model in the subsequent itera-tions.Yarowsky and Wicentowski show that the al-gorithm is extremely accurate in identifying En-glish root + past tense form pairs, including thosepairs that are related by non-affixal patterns (e.g.,think/thought.
)The main issue with this model is, of course, thatit cannot be applied to a new target language with-out having some a priori knowledge about some ofits linguistic properties.
Thus, the algorithm can-not be applied in cases in which the grammar ofthe target language has not been properly describedyet, or when the relevant information is not availablefor other reasons.
Moreover, even when such infor-mation is in principle available, trying to determineto what extent morphology could be learned with-out relying on any other knowledge source remainsan interesting theoretical pursuit, and one whose an-swer could shed some light on the problem of humanlanguage acquisition.3 The current approach: Morphologicalrelatedness as a function of orthographicand semantic similarityThe basic intuition behind the model presented hereis extremely simple: Morphologically related wordstend to be both orthographically and semanticallysimilar.
Obviously, there are many words that are or-thographically similar, but are not morphologicallyrelated; for example, blue and glue.
At the sametime, many semantically related words are not mor-phologically related (for example, blue and green).However, if two words have a similar shape and arelated meaning (e.g., green and greenish), they arevery likely to be also morphologically related.In order to make this idea concrete, we use min-imum edit distance to identify words that are ortho-graphically similar, and mutual information betweenwords to identify semantically related words.3.1 Outline of the procedureGiven an unannotated input corpus, the algorithm(after some elementary tokenization) extracts a listof candidate content words.
This is simply a listof all the alphabetic space- or punctuation-delimitedstrings in the corpus that have a corpus frequencybelow .01% of the total token count.2Preliminary experiments indicated that our proce-dure does not perform as well without this trimming.Notice in any case that function words tend to be oflittle morphological interest, as they display highlylexicalized, often suppletive morphological patterns.The word list extracted as described above and theinput corpus are used to compute two lists of wordpairs: An orthographic similarity list, in which the2In future versions of the algorithm, we plan to make thishigh frequency threshold dependent on the size of the input cor-pus.pairs are scored on the basis of their minimum editdistance, and a semantic similarity list, based on mu-tual information.
Because of minimum thresholdsthat are enforced during the computation of the twomeasures, neither list contains all the pairs that canin principle be constructed from the input list.Before computing the combined score, we get ridof the pairs that do not occur in both lists (the ra-tionale being that we do not want to guess the mor-phological status of a pair on the sole basis of ortho-graphic or semantic evidence).We then compute a weighted sum of the ortho-graphic and semantic similarity scores of each re-maining pair.
In the experiments reported below, theweights are chosen so that the maximum weightedscores for the two measures are in the same orderof magnitude (we prefer to align maxima rather thanmeans because both lists are trimmed at the bottom,making means and other measures of central ten-dency less meaningful).The pairs are finally ranked on the basis of theresulting combined scores.In the next subsections, we describe how the or-thographic and semantic similarity lists are con-structed, and some properties of the measures weadopted.3.2 Scoring the orthographic similarity ofword pairsLike Yarowsky and Wicentowski, we use mini-mum edit distance to measure orthographic simi-larity.
The minimum edit distance between twostrings is the minimum number of editing oper-ations (insertion, deletion, substitution) needed totransform one string into the other (see section 5.6of Jurafsky and Martin (2000) and the referencesquoted there).Unlike Yarowsky and Wicentowski, we do not at-tempt to define a phonologically sensible edit dis-tance scoring function, as this would require makingassumptions about how the phonology of the targetlanguage maps onto its orthography, thus falling out-side the domain of knowledge-free induction.
In-stead, we assign a cost of 1 to all editing operations,independently of the nature of the source and targetsegments.
Thus, in our system, the pairs dog/Dog,man/men, bat/mat and day/dry are all assigned aminimum edit distance of 1.3Rather than computing absolute minimum editdistance, we normalize this measure by dividingit by the length of the longest string (this corre-sponds to the intuition that, say, two substitutionsare less significant if we are comparing two eight-letter words than if we are comparing two three-letter words).
Moreover, since we want to rank pairson the basis of orthographic similarity, rather thandissimilarity, we compute (1 - normalized minimumedit distance), obtaining a measure that ranges from1 for identical forms to 0 for forms that do not shareany character.This measure is computed for all pairs of words inthe potential content word list.
However, for reasonsof size, only pairs that have a score of .5 or higher(i.e., where the two members share at least half oftheir characters) are recorded in the output list.Notice that orthographic similarity does not favorconcatenative affixal morphology over other typesof morphological processes.
For example, the pairswoman/women and park/parks both have an ortho-graphic similarity score of .8.Moreover, orthographic similarity depends onlyon the two words being compared, and not on globaldistributional properties of these words and theirsubstrings.
Thus, words related by a rare morpho-logical pattern can have the same score as wordsrelated by a very frequent pattern, as long as theminimum edit distance is the same.
For example,both nucleus/nuclei and bench/benches have an or-thographic similarity score of .714, despite the factthat the latter pair reflects a much more common plu-ralization pattern.Of course, this emancipation from edge-anchoredconcatenation and global distributional salience alsoimplies that orthographic similarity will assign high3Following a suggestion by two reviewers, we are currentlyexperimenting with an iterative version of our algorithm, alongthe lines of the one described by Yarowsky and Wicentowski.We start with the cost matrix described in the text, but we re-estimate the editing costs on the basis of the empirical character-to-character (or character-to-zero/zero-to-character) probabili-ties observed in the output of the previous run of the algorithm.Surprisingly, the revised version of the algorithm leads to (mod-erately) worse results than the single-run version described inthis paper.
Further experimentation with edit cost re-estimationis needed, in order to understand which aspects of our iterativeprocedure make it worse than the single-run model, and how itcould be improved.scores to many pairs that are not morphologicallyrelated ?
for example, the pair friends/trends also hasan orthographic similarity score of .714.Furthermore, since in most languages the range ofpossible word lengths is narrow, orthographic simi-larity as a ranking measure tends to suffer of a ?mas-sive tying?
problem.
For example, when pairs fromthe German corpus described below are ranked onthe sole basis of orthographic similarity, the result-ing list is headed by a block of 19,597 pairs that allhave the same score.
These are all pairs where oneword has 9 characters, the other 9 or 8 characters,and the two differ in only one character.4For the above reasons, it is crucial that ortho-graphic similarity is combined with an independentmeasure that allows us to distinguish between simi-larity due to morphological relatedness vs. similar-ity due to chance or other reasons.3.3 Scoring the semantic similarity of wordpairsMeasuring the semantic similarity of words on thebasis of raw corpus data is obviously a much hardertask than measuring the orthographic similarity ofwords.Mutual information (first introduced to compu-tational linguistics by Church and Hanks (1989)) isone of many measures that seems to be roughlycorrelated to the degree of semantic relatedness be-tween words.
The mutual information between twowords A and B is given by:I(A,B) = logPr(A,B)Pr(A)Pr(B)(1)Intuitively, the larger the deviation between theempirical frequency of co-occurrence of two wordsand the expected frequency of co-occurrence if theywere independent, the more likely it is that the oc-currence of one of the two words is not independentfrom the occurrence of the other.Brown et ali (1990) observed that when mutualinformation is computed in a bi-directional fashion,and by counting co-occurrences of words within a4Most of the pairs in this block ?
78% ?
are actually morpho-logically related.
However, given that all pairs contain wordsof length 9 and 8/9 that differ in one character only, they arebound to reflect only a very small subset of the morphologicalprocesses present in German.relatively large window, but excluding ?close?
co-occurrences (which would tend to capture colloca-tions and lexicalized phrases), the measure identifiessemantically related pairs.It is particularly interesting for our purposes thatmost of the examples of English word clusters con-structed on the basis of this interpretation of mutualinformation by Brown and colleagues (reported intheir table 6) include morphologically related words.A similar pattern emerges among the examples ofGerman words clustered in a similar manner byBaroni et ali (2002).
Rosenfeld (1996) reports thatmorphologically related pairs are common amongwords with a high (average) mutual information.We computed mutual information by considering,for each pair, only co-occurrences within a maxi-mal window of 500 words and outside a minimalwindow of 3 words.
Given that mutual informa-tion is notoriously unreliable at low frequencies (see,for example, Manning and Schu?tze (1999), section5.4), we only collected mutual information scoresfor pairs that co-occurred at least three times (withinthe relevant window) in the input corpus.
Obvi-ously, occurrences across article boundaries werenot counted.
Notice however that the version of theBrown corpus we used does not mark article bound-aries.
Thus, in this case the whole corpus was treatedas a single article.Our ?semantic?
similarity measure is based on thenotion that related words will tend to often occur inthe nears of each other.
This differs from the (moregeneral) approach of Schone and Jurafsky (2000),who look for words that tend to occur in the samecontext.
It remains an open question whether thetwo approaches produce complementary or redun-dant results.5Taken by itself, mutual information is a worsepredictor of morphological relatedness than mini-mum edit distance.
For example, among the top onehundred pairs ranked by mutual information in eachlanguage, only one German pair and five Englishpairs are morphologically motivated.
This poor per-formance is not too surprising, given that there are5We are currently experimenting with a measure based onsemantic context similarity (determined on the basis of class-based left-to-right and right-to-left bigrams), but the current im-plementation of this requires ad hoc corpus-specific settings toproduce interesting results with both our test corpora.plenty of words that often co-occur together withoutbeing morphologically related.
Consider for exam-ple (from our English list) the pairs index/operandand orthodontist/teeth.4 Empirical evaluation4.1 MaterialsWe tested our procedure on the German APA corpus,a corpus of newswire containing over twenty-eightmillion word tokens, and on the English Brown cor-pus (Kuc?era and Francis, 1967), a balanced corpuscontaining less than one million two hundred thou-sand word tokens.
Of course, the most important dif-ference between these two corpora is that they rep-resent different languages.
However, observe alsothat they have very different sizes, and that they aredifferent in terms of the types of texts constitutingthem.Besides the high frequency trimming proceduredescribed above, for both languages we removedfrom the potential content word lists those wordsthat were not recognized by the XEROX morpholog-ical analyzer for the relevant language.
The reasonfor this is that, as we describe below, we use this toolto build the reference sets for evaluation purposes.Thus, morphologically related pairs composed ofwords not recognized by the analyzer would unfairlylower the precision of our algorithm.Moreover, after some preliminary experimenta-tion, we also decided to remove words longer than 9characters from the German list (this corresponds totrimming words whose length is one standard devi-ation or more above the average token length).
Thisactually lowers the performance of our system, butmakes the results easier to analyze ?
otherwise, thetop of the German list would be cluttered by a highnumber of rather uninteresting morphological pairsformed by inflected forms from the paradigm ofvery long nominal compounds (such as Wirtschafts-forschungsinstitut ?institute for economic research?
).Unlike high frequency trimming, the two opera-tions we just described are meant to facilitate empir-ical evaluation, and they do not constitute necessarysteps of the core algorithm.4.2 PrecisionIn order to evaluate the precision obtained by ourprocedure, we constructed a list of all the pairs that,according to the analysis provided by the XEROXanalyzer for the relevant language, are morpholog-ically related (i.e., share one of their stems).6 Werefer to the lists constructed in the way we just de-scribed as reference sets.The XEROX tools we used do not provide deriva-tional analysis for English, and a limited form ofderivational analysis for German.
Our algorithm,however, finds both inflectionally and derivationallyrelated pairs.
Thus, basing our evaluation on a com-parison with the XEROX parses leads to an underes-timation of the precision of the algorithm.
We foundthat this problem is particularly evident in English,since English, unlike German, has a rather poor in-flectional morphology, and thus the discrepanciesbetween our output and the analyzer parses in termsof derivational morphology have a more visible im-pact on the results of the comparison.
For example,the English analyzer does not treat pairs related bythe adverbial suffix -ly or by the prefix un- as mor-phologically related, whereas our algorithm foundpairs such as soft/softly and load/unload.In order to obtain a more fair assessment of thealgorithm, we went manually through the first 2,000English pairs found by our algorithm but not parsedas related by the analyzer, looking for items to beadded to the reference set.
We were extremelyconservative, and we added to the reference setonly those pairs that are related by a transparentand synchronically productive morphological pat-tern.
When in doubt, we did not correct the analyzer-based analysis.
Thus, for example, we did not countpairs such as machine/machinery, variables/variesor electric/electronic as related.We did not perform any manual post-processingon the German reference set.Tables 1 and 2 report percentage precision (i.e.,the percentage of pairs that are in the reference setover the total number of ranked pairs up to the rele-vant threshold) at various cutoff points, for Germanand English respectively.6The XEROX morphological analyzers are state-of-the-art,knowledge-driven morphological analysis tools (see for exam-ple Karttunen et ali (1997)).# of pairs precision500 97%1000 96%1500 96%2000 94%3000 81%4000 65%5000 53%5279 50%Table 1: German precision at various cutoff points(5279 = total number of pairs)# of pairs precision500 98%1000 95%1500 91%2000 83%3000 72%4000 58%5000 48%8902 29%Table 2: English precision at various cutoff points(8902 = total number of pairs)For both languages we notice a remarkably highprecision rate (> 90%) up to the 1500-pair cutoffpoint.After that, there is a sharper drop in the Englishprecision, whereas the decline in German is moregradual.
This is perhaps due in part to the problemswith the English reference set we discussed above,but notice also that English has an overall poorermorphological system and that the English corpus isconsiderably smaller than the German one.
Indeed,our reference set for German contains more than tentimes the forms in the English reference set.Notice anyway that, for both languages, the preci-sion rate is still around 50% at the 5000-pair cutoff.77Yarowsky and Wicentowski (2000) report an accuracy ofover 99% for their best model and a test set of 3888 pairs.
Ourprecision rate at a comparable cutoff point is much lower (58%at the 4000-pair cutoff).
However, Yarowksy and Wicentowskirestricted the possible matchings to pairs in which one memberis an inflected verb form, and the other member is a potentialverbal root, whereas in our experiments any word in the corpus(as long as it was below a certain frequency threshold, and it wasrecognized by the XEROX analyzer) could be matched with anyother word in the corpus.
Thus, on the one hand, Yarowsky andWicentowski forced the algorithm to produce a matching for acertain set of words (their set of inflected forms), whereas ouralgorithm was not subject to an analogous constraint.
On theother hand, though, our algorithm had to explore a much largerpossible matching space, and it could (and did) make a highnumber of mistakes on pairs (such as, e.g., sorry and worry) thatOf course, what counts as a ?good?
precision ratedepends on what we want to do with the output ofour procedure.
We show below that even a verynaive morphological rule extraction algorithm canextract sensible rules by taking whole output lists asits input, since, although the number of false pos-itives is high, they are mostly related by patternsthat are not attested as frequently in the list as thepatterns relating true morphological pairs.
In otherwords, true morphological pairs tend to be relatedby patterns that are distributionally more robust thanthose displayed by false positives.
Thus, rule ex-tractors and other procedures processing the outputof our algorithm can probably tolerate a high falsepositive rate if they take frequency and other distri-butional properties of patterns into account.Notice that we discussed only precision, and notrecall.
This is because we believe that the goal of amorphological discovery procedure is not to find theexhaustive list of all morphologically related formsin a language (indeed, because of morphologicalproductivity, such list is infinite), but rather to dis-cover all the possible (synchronically active and/orcommon) morphological processes present in a lan-guage.
It is much harder to measure how good ouralgorithm performed in this respect, but the qualita-tive analysis we present in the next subsection indi-cates that, at least, the algorithm discovers a variedand interesting set of morphological processes.4.3 Morphological patterns discovered by thealgorithmThe precision tables confirm that the algorithmfound a good number of morphologically relatedpairs.
However, if it turned out that all of thesepairs were examples of the same morphological pat-tern (say, nominal plural formation in -s), the al-gorithm would not be of much use.
Moreover, westated at the beginning that, since our algorithm doesnot assume an edge-based stem+affix concatenationmodel of morphology, it should be well suited to dis-cover relations that cannot be characterized in theseYarowksy and Wicentowski?s algorithm did not have to con-sider.
Schone and Jurafsky (2000) report a maximum precisionof 92%.
It is hard to compare this with our results, since theyuse a more sophisticated scoring method (based on paradigmsrather than pairs) and a different type of gold standard.
More-over, they do not specify what was the size of the input theyused for evaluation.terms (e.g., pairs related by circumfixation, stemchanges, etc.).
It is interesting to check whether thealgorithm was indeed able to find relations of thissort.Thus, we performed a qualitative analysis of theoutput of the algorithm, trying to understand whatkind of morphological processes were captured byit.In order to look for morphological processes inthe algorithm output, we wrote a program that ex-tracts ?correspondence rules?
in the following sim-ple way: For each pair, the program looks for thelongest shared (case-insensitive) left- and right-edgesubstrings (i.e., for a stem + suffix parse and for aprefix + stem parse).
The program then chooses theparse with the longest stem (assuming that one of thetwo parses has a non-zero stem), and extracts the rel-evant edge-bound correspondence rule.
If there is atie, the stem + suffix parse is preferred.
The programthen ranks the correspondence rules on the basis oftheir frequency of occurrence in the original outputlist.8We want to stress that we are adopting this proce-dure as a method to explore the results, and we areby no means proposing it as a serious rule inductionalgorithm.
One of the most obvious drawbacks ofthe current rule extraction procedure is that it is onlyable to extract linear, concatenative, edge-bound suf-fixation and prefixation patterns, and thus it missesor fails to correctly generalize some of the most in-teresting patterns in the output.
Indeed, looking atthe patterns missed by the algorithm (as we do inpart below) is as instructive as looking at the rules itfound.Tables 3 and 4 report the top five suffixation andprefixation patterns found by the rule extractor bytaking the entire German and English output lists asits input.These tables show that our morphological pairscoring procedure found many instances of variouscommon morphological patterns.
With the excep-tion of the German ?prefixation?
rule ers?drit (ac-tually relating the roots of the ordinals ?first?
and?second?
), and of the compounding pattern  ?
?Ol(?Oil?
), all the rules in these lists correspond to re-alistic affixation patterns.
Not surprisingly, in both8Ranking by cumulative score yields analogous results.rule example fq?s Jelzin?Jelzins 921?n lautete?lauteten 670?en digital?digitalen 225?e rot?rote 201?es Papst?Papstes 113?ge stiegen?gestiegen 9?
?Ol Embargo?
?Olembargo 6?vor Mittag?Vormittag 5aus?ein ausfuhren?einfuhren 4ers?drit Erstens?Drittens 4Table 3: The most common German suffixation andprefixation patternsrule example fq?s allotment?allotments 860?ed accomplish?accomplished 98ed?ing established?establishing 87?ing experiment?experimenting 85?d conjugate?conjugated 58?un structured?unstructured 17?re organization?reorganization 12?in organic?inorganic 7?non specifically?nonspecifically 6?dis satisfied?dissatisfied 5Table 4: The most common English suffixation andprefixation patternslanguages many of the most frequent rules (such as,e.g., ?s) are poly-functional, corresponding to anumber of different morphological relations withinand across categories.The results reported in these tables confirm thatthe algorithm is capturing common affixation pro-cesses, but they are based on patterns that are sofrequent that even a very naive procedure could un-cover them9More interesting observations emerge from fur-ther inspection of the ranked rule files.
For exam-ple, among the 70 most frequent German suffixationrules extracted by the procedure, we encounter thosein table 5.10The patterns in this table show that our algorithmis capturing the non-concatenative plural formation9For example, as shown by a reviewer, a procedure that pairswords that share the same first five letters, and extracts the di-verging substrings following the common prefix from each pair.10In order to find the set of rules presented in table 5 using thenaive algorithm described in the previous footnote, we wouldhave to consider the 2672 most frequent rules.
Most of these2672 rules, of course, do not correspond to true morphologicalpatterns ?
thus, the interesting rules would be buried in noise.rule example fqag?a?ge Anschlag?Anschla?ge 10ang?a?nge Ru?ckgang?Ru?ckga?nge 6all?a?lle ?Uberfall?
?Uberfa?lle 6ug?u?ge Tiefflug?Tiefflu?ge 5and?a?nde Vorstand?Vorsta?nde 5uch?u?che Einbruch?Einbru?che 3auf?a?ufe Verkauf?Verka?ufe 3ag?a?gen Vertrag?Vertra?gen 3Table 5: Some German rules involving stem vowelchanges found by the rule extractorprocess involving fronting of the stem vowel plusaddition of a suffix (-e/-en).
A smarter rule extractorshould be able to generalize from patterns like theseto a smaller number of more general rules capturingthe discontinuous change.
Other umlaut-based pat-terns that do not involve concomitant suffixation ?such as in Mutter/Mu?tter ?
were also found by ourcore algorithm, but they were wrongly parsed as in-volving prefixes (e.g., Mu?Mu?)
by the rule extrac-tor.Finally, it is very interesting to look at those pairsthat are morphologically related according to theXEROX analyzer, and that were discovered by ouralgorithm, but where the rule extractor could notposit a rule, since they do not share a substring ateither edge.
These are listed, for German, in table 6.Alter a?lteren fordern gefordertArzt ?Arzte forderten gefordertArztes ?Arzte fo?rdern gefo?rdertFesseln gefesselt genannt nannteFolter gefoltert genannten nanntePutsch geputscht geprallt prallteSpende gespendet gesetzt setzteSpenden gespendet gestu?rzt stu?rzteStreik gestreiktTable 6: Morphologically related German pairs thatdo not share an edge found by the basic algorithmWe notice in this table, besides three further in-stances of non-affixal morphology, a majority ofpairs involving circumfixation of one of the mem-bers.While a more in-depth qualitative analysis of ourresults should be conducted, the examples we dis-cussed here confirm that our algorithm is able to cap-ture a number of different morphological patterns,including some that do not fit into a strictly concate-native edge-bound stem+affix model.5 Conclusion and Future DirectionsWe presented an algorithm that, by taking a raw cor-pus as its input, produces a ranked list of morpho-logically related pairs at its output.
The algorithmfinds morphologically related pairs by looking at thedegree of orthographic similarity (measured by min-imum edit distance) and semantic similarity (mea-sured by mutual information) between words fromthe input corpus.Experiments with German and English inputsgave encouraging results, both in terms of precision,and in terms of the nature of the morphological pat-terns found within the output set.In work in progress, we are exploring various pos-sible improvements to our basic algorithm, includ-ing iterative re-estimation of edit costs, addition of acontext-similarity-based measure, and extension ofthe output set by morphological transitivity, i.e.
theidea that if word a is related to word b, and word bis related to word c, then word a and word c shouldalso form a morphological pair.Moreover, we plan to explore ways to relax the re-quirement that all pairs must have a certain degree ofsemantic similarity to be treated as morphologicallyrelated (there is evidence that humans treat certainkinds of semantically opaque forms as morpholog-ically complex ?
see Baroni (2000) and the refer-ences quoted there).
This will probably involve tak-ing distributional properties of word substrings intoaccount.From the point of view of the evaluationof the algorithm, we should design an as-sessment scheme that would make our exper-imental results more directly comparable tothose of Yarowsky and Wicentowski (2000),Schone and Jurafsky (2000) and others.
Moreover,a more in depth qualitative analysis of the resultsshould concentrate on identifying specific classes ofmorphological processes that our algorithm can orcannot identify correctly.We envisage a number of possible uses for theranked list that constitutes the output of our model.First, the model could provide the input for amore sophisticated rule extractor, along the lines ofthose proposed by Albright and Hayes (1999) andNeuvel (2002).
Such models extract morphologi-cal generalizations in terms of correspondence pat-terns between whole words, rather than in terms ofaffixation rules, and are thus well suited to iden-tify patterns involving non-concatenative morphol-ogy and/or morphophonological changes.
A listof related words constitutes a more suitable inputfor them than a list of words segmented into mor-phemes.Rules extracted in this way would have a numberof practical uses ?
for example, they could be usedto construct stemmers for information retrieval ap-plications, or they could be integrated into morpho-logical analyzers.Our procedure could also be used to re-place the first step of algorithms, such as thoseof Goldsmith (2001) and Snover and Brent (2001),where heuristic methods are employed to generatemorphological hypotheses, and then an information-theoretically/probabilistically motivated measure isused to evaluate or improve such hypotheses.
Morein general, our algorithm can help reduce the sizeof the search space that all morphological discoveryprocedures must explore.Last but not least, the ranked output of (an im-proved version of) our algorithm can be of use tothe linguist analyzing the morphology of a language,who can treat it as a way to pre-process her/hisdata, while still relying on her/his analytical skillsto extract the relevant morphological generalizationsfrom the ranked pairs.AcknowledgementsWe would like to thank Adam Albright, BruceHayes and the anonymous reviewers for helpfulcomments, and the Austria Presse Agentur forkindly making the APA corpus available to us.
Thiswork was supported by the European Union in theframework of the IST programme, project FASTY(IST-2000-25420).
Financial support for ?OFAI isprovided by the Austrian Federal Ministry of Edu-cation, Science and Culture.ReferencesA.
Albright and B. Hayes.
1999.
An automated learnerfor phonology and morphology.
UCLA manuscript.M.
Baroni.
2000.
Distributional cues in morphemediscovery: A computational model and empirical ev-idence.
Ph.D. dissertation, UCLA.M.
Baroni, J. Matiasek and H. Trost.
2002.
Wordform-and class-based prediction of the components of Ger-man nominal compounds in an AAC system.
To ap-pear in Proceedings of COLING 2002.P.
Brown, P. Della Pietra, P. DeSouza, J. Lai, and R. Mer-cer.
1990.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18:467-479.K.
Church and P. Hanks.
1989.
Word association norms,mutual information, and lexicography.
Proceedings ofACL 27, 76?83.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, 27:153-198.C.
Jacquemin.
1997.
Guessing morphology from termsand corpora.
Proceedings of SIGIR 97, 156?265.D.
Jurafsky and J. Martin.
2000.
Speech and LanguageProcessing.
Prentice-Hall, Upper Saddle River, NJ.L.
Karttunen, K. Gaa?l, and A. Kempe.
1997.
Xe-rox Finite-State Tool Xerox Research Centre Europe,Grenoble.H.
Kuc?era and N. Francis.
1967.
Computational analysisof present-day American English.
Brown UniversityPress, Providence, RI.C.
Manning and H. Schu?tze.
1999.
Foundations of sta-tistical natural language processing.
MIT Press, Cam-bridge, MASS.S.
Neuvel.
2002.
Whole word morphologizer.
Expand-ing the word-based lexicon: A non-stochastic compu-tational approach.
Brain and Language, in press.R.
Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
ComputerSpeech and Language, 10:187?228.P.
Schone and D. Jurafsky.
2000.
Knowldedge-free in-duction of morphology using latent semantic analysis.Proceedings of the Conference on Computational Nat-ural Language Learning.M.
Snover and M. Brent.
2001.
A Bayesian model formorpheme and paradigm identification.
Proceedingsof ACL 39, 482-490.D.
Yarowksy and R. Wicentowski.
2000.
Minimally su-pervised morphological analysis by multimodal align-ment.
Proceedings of ACL 38, 207?216.
