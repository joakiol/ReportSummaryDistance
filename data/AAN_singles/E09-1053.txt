Proceedings of the 12th Conference of the European Chapter of the ACL, pages 460?468,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsDependency trees and the strong generative capacity of CCGAlexander KollerSaarland UniversitySaarbr?cken, Germanykoller@mmci.uni-saarland.deMarco KuhlmannUppsala UniversityUppsala, Swedenmarco.kuhlmann@lingfil.uu.seAbstractWe propose a novel algorithm for extract-ing dependencies from the derivations ofa large fragment of CCG.
Unlike earlierproposals, our dependency structures arealways tree-shaped.
We then use these de-pendency trees to compare the strong gen-erative capacities of CCG and TAG andobtain surprising results: Both formalismsgenerate the same languages of derivationtrees ?
but the mechanisms they use tobring the words in these trees into a linearorder are incomparable.1 IntroductionCombinatory Categorial Grammar (CCG; Steed-man (2001)) is an increasingly popular grammarformalism.
Next to being theoretically well-mo-tivated due to its links to combinatory logic andcategorial grammar, it is distinguished by the avail-ability of efficient open-source parsers (Clark andCurran, 2007), annotated corpora (Hockenmaierand Steedman, 2007; Hockenmaier, 2006), andmechanisms for wide-coverage semantic construc-tion (Bos et al, 2004).However, there are limits to our understandingof the formal properties of CCG and its relationto other grammar formalisms.
In particular, whileit is well-known that CCG belongs to a family ofmildly context-sensitive formalisms that all gener-ate the same string languages (Vijay-Shanker andWeir, 1994), there are few results about the stronggenerative capacity of CCG.
This makes it difficultto gauge the similarities and differences betweenCCG and other formalisms in how they model lin-guistic phenomena such as scrambling and relat-ive clauses (Hockenmaier and Young, 2008), andhampers the transfer of algorithms from one form-alism to another.In this paper, we propose a new method for deriv-ing a dependency tree from a CCG derivation treefor PF-CCG, a large fragment of CCG.
We thenexplore the strong generative capacity of PF-CCGin terms of dependency trees.
In particular, we castnew light on the relationship between CCG andother mildly context-sensitive formalisms such asTree-Adjoining Grammar (TAG; Joshi and Schabes(1997)) and Linear Context-Free Rewrite Systems(LCFRS; Vijay-Shanker et al (1987)).
We showthat if we only look at valencies and ignore wordorder, then the dependency trees induced by a PF-CCG grammar form a regular tree language, justas for TAG and LCFRS.
To our knowledge, this isthe first time that the regularity of CCG?s deriva-tional structures has been exposed.
However, if wetake the word order into account, then the classesof PF-CCG-induced and TAG-induced dependencytrees are incomparable; in particular, CCG-induceddependency trees can be unboundedly non-project-ive in a way that TAG-induced dependency treescannot.The fact that all our dependency structures aretrees brings our approach in line with the emergingmainstream in dependency parsing (McDonald etal., 2005; Nivre et al, 2007) and TAG derivationtrees.
The price we pay for restricting ourselves totrees is that we derive fewer dependencies than themore powerful approach by Clark et al (2002).
In-deed, we do not claim that our dependencies are lin-guistically meaningful beyond recording the way inwhich syntactic valencies are filled.
However, weshow that our dependency trees are still informativeenough to reconstruct the semantic representations.The paper is structured as follows.
In Section 2,we introduce CCG and the fragment PF-CCG thatwe consider in this paper, and compare our contri-bution to earlier research.
In Section 3, we thenshow how to read off a dependency tree from aCCG derivation.
Finally, we explore the stronggenerative capacity of CCG in Section 4 and con-clude with ideas for future work.460mernp : we?Lem Hansnp : Hans?Les huusnp : house?Lh?lfed((s\np)\np)/vp : help?Laastrichevp\np : paint?L((s\np)\np)\np : ?x.
help?(paint?
(x))F(s\np)\np : help?
(paint?(house?
))Bs\np : help?
(paint?(house?))
Hans?Bs : help?
(paint?(house?))
Hans?
we?BFigure 1: A PF-CCG derivation2 Combinatory Categorial GrammarsWe start by introducing the Combinatory CategorialGrammar (CCG) formalism.
Then we introducethe fragment of CCG that we consider in this paper,and discuss some related work.2.1 CCGCombinatory Categorial Grammar (Steedman,2001) is a grammar formalism that assigns categor-ies to substrings of an input sentence.
There areatomic categories such as s and np; and if A and Bare categories, then A\B and A/B are functionalcategories representing a constituent that will havecategory A once it is combined with another con-stituent of type B to the left or right, respectively.Each word is assigned a category by the lexicon;adjacent substrings can then be combined by com-binatory rules.
As an example, Steedman and Bald-ridge?s (2009) analysis of Shieber?s (1985) SwissGerman subordinate clause (das) mer em Hans eshuus h?lfed aastriiche (?
(that) we help Hans paintthe house?)
is shown in Figure 1.Intuitively, the arguments of a functional cat-egory can be thought of as the syntactic valenciesof the lexicon entry, or as arguments of a func-tion that maps categories to categories.
The corecombinatory mechanism underlying CCG is thecomposition and application of these functions.
Intheir most general forms, the combinatory rules of(forward and backward) application and compos-ition can be written as in Figure 2.
The symbol |stands for an arbitrary (forward or backward) slash;it is understood that the slash before each Bi abovethe line is the same as below.
The rules derive state-ments about triples w ` A : f , expressing that thesubstring w can be assigned the category A and thesemantic representation f ; an entire string countsas grammatical if it can be assigned the start cat-egory s. In parallel to the combination of substringsby the combinatory rules, their semantic represent-ations are combined by functional composition.We have presented the composition rules of CCGin their most general form.
In the literature, thespecial cases for n = 0 are called forward andbackward application; the cases for n > 0 wherethe slash before Bn is the same as the slash be-fore B are called composition of degree n; andthe cases where n > 0 and the slashes have dif-ferent directions are called crossed composition ofdegree n. For instance, the F application that com-bines h?lfed and aastriche in Figure 1 is a forwardcrossed composition of degree 1.2.2 PF-CCGIn addition to the composition rules introducedabove, CCG also allows rules of substitution andtype-raising.
Substitution is used to handle syn-tactic phenomena such as parasitic gaps; type-rais-ing allows a constituent to serve syntactically as afunctor, while being used semantically as an argu-ment.
Furthermore, it is possible in CCG to restrictthe instances of the rule schemata in Figure 2?forinstance, to say that the application rule may onlybe used for the case A = s. We call a CCG gram-mar pure if it does not use substitution, type-raising,or restricted rule schemata.
Finally, the argumentcategories of a CCG category may themselves befunctional categories; for instance, the category ofa VP modifier like passionately is (s\np)\(s\np).We call a category that is either atomic or only hasatomic arguments a first-order category, and call aCCG grammar first-order if all categories that itslexicon assigns to words are first-order.In this paper, we only consider CCG grammarsthat are pure and first-order.
This fragment, whichwe call PF-CCG, is less expressive than full CCG,but it significantly simplifies the definitions in Sec-tion 3.
At the same time, many real-world CCGgrammars do not use the substitution rule, and type-raising can be compiled into the grammar in thesense that for any CCG grammar, there is an equi-valent CCG grammar that does not use type-raisingand assigns the same semantic representations to461(a,A, f) is a lexical entrya ` A : fLv ` A/B : ?x.
f(x) w ` B |Bn | .
.
.
|B1 : ?y1, .
.
.
, yn.
g(y1, .
.
.
, yn)vw ` A |Bn | .
.
.
|B1 : ?y1, .
.
.
, yn.
f(g(y1, .
.
.
, yn))Fv ` B |Bn | .
.
.
|B1 : ?y1, .
.
.
, yn.
g(y1, .
.
.
, yn) w ` A\B : ?x.
f(x)vw ` A |Bn | .
.
.
|B1 : ?y1, .
.
.
, yn.
f(g(y1, .
.
.
, yn))BFigure 2: The generalized combinatory rules of CCGeach string.
On the other hand, the restriction tofirst-order grammars is indeed a limitation in prac-tice.
We take the work reported here as a first steptowards a full dependency-tree analysis of CCG,and discuss ideas for generalization in the conclu-sion.2.3 Related workThe main objective of this paper is the definitionof a novel way in which dependency trees canbe extracted from CCG derivations.
This is sim-ilar to Clark et al (2002), who aim at capturing?deep?
dependencies, and encode these into annot-ated lexical categories.
For instance, they write(npi\npi)/(s\npi) for subject relative pronouns toexpress that the relative pronoun, the trace of therelative clause, and the modified noun phrase areall semantically the same.
This means that the rel-ative pronoun has multiple parents; in general, theirdependency structures are not necessarily trees.
Bycontrast, we aim to extract only dependency trees,and achieve this by recording only the fillers of syn-tactic valencies, rather than the semantic dependen-cies: the relative pronoun gets two dependents andone parent (the verb whose argument the modifiednp is), just as the category specifies.
So Clark etal.
?s and our dependency approach represent twoalternatives of dealing with the tradeoff betweensimple and expressive dependency structures.Our paper differs from the well-known resultsof Vijay-Shanker and Weir (1994) in that they es-tablish the weak equivalence of different grammarformalisms, while we focus on comparing the deriv-ational structures.
Hockenmaier and Young (2008)present linguistic motivations for comparing thestrong generative capacities of CCG and TAG, andthe beginnings of a formal comparison betweenCCG and spinal TAG in terms of Linear IndexedGrammars.3 Induction of dependency treesWe now explain how to extract a dependency treefrom a PF-CCG derivation.
The basic idea is toassociate, with every step of the derivation, a cor-responding operation on dependency trees, in muchthe same way as derivation steps can be associatedwith operations on semantic representations.3.1 Dependency treesWhen talking about a dependency tree, it is usuallyconvenient to specify its tree structure and the lin-ear order of its nodes separately.
The tree structureencodes the valency structure of the sentence (im-mediate dominance), whereas the linear precedenceof the words is captured by the linear order.For the purposes of this paper, we represent adependency tree as a pair d = (t, s), where t is aground term over some suitable alphabet, and s isa linearization of the nodes (term addresses) of t,where by a linearization of a set S we mean a list ofelements of S in which each element occurs exactlyonce (see also Kuhlmann and M?hl (2007)).
Asexamples, consider(f(a, b), [1, ?, 2]) and (f(g(a)), [1 ?
1, ?, 1]) .These expressions represent the dependency treesd1 =a f band d2 =a f g.Notice that it is because of the separate specifica-tion of the tree and the order that dependency treescan become non-projective; d2 is an example.A partial dependency tree is a pair (t, s) where tis a term that may contain variables, and s is alinearization of those nodes of t that are not labelledwith variables.
We restrict ourselves to terms inwhich each variable appears exactly once, and willalso prefix partial dependency trees with ?-bindersto order the variables.462e = (a,A |Am ?
?
?
|A1) is a lexical entrya ` A |Am ?
?
?
|A1 : ?x1, .
.
.
, xm.
(e(x1, .
.
.
, xm), [?
])Lv ` A |Am ?
?
?
|A1/B : ?x, x1, .
.
.
, xm.
d w ` B |Bn ?
?
?
|B1 : ?y1, .
.
.
, yn.
d?vw ` A |Am ?
?
?
|A1 |Bn ?
?
?
|B1 : ?y1, .
.
.
, yn, x1, .
.
.
, xm.
d[x := d?
]FFw ` B |Bn ?
?
?
|B1 : ?y1, .
.
.
, yn.
d?
v ` A |Am ?
?
?
|A1\B : ?x, x1, .
.
.
, xm.
dwv ` A |Am ?
?
?
|A1 |Bn ?
?
?
|B1 : ?y1, .
.
.
, yn, x1, .
.
.
, xm.
d[x := d?
]BBFigure 3: Computing dependency trees in CCG derivations3.2 Operations on dependency treesLet t be a term, and let x be a variable in t. Theresult of the substitution of the term t?
into t for xis denoted by t[x := t?
].
We extend this opera-tion to dependency trees as follows.
Given a listof addresses s, let xs be the list of addresses ob-tained from s by prefixing every address with theaddress of the (unique) node that is labelled with xin t. Then the operations of forward and backwardconcatenation are defined as(t, s)[x := (t?, s?)
]F = (t[x := t?
], s ?
xs?)
,(t, s)[x := (t?, s?)
]B = (t[x := t?
], xs?
?
s) .The concatenation operations combine two givendependency trees (t, s) and (t?, s?)
into a new treeby substituting t?
into t for some variable x of t,and adding the (appropriately prefixed) list s?
ofnodes of t?
either before or after the list s of nodesof t. Using these two operations, the dependencytrees d1 and d2 from above can be written as fol-lows.
Let da = (a, [?])
and db = (b, [?
]).d1 = (f(x, y), [?
])[x := da ]F [ y := db ]Fd2 = (f(x), [?
])[x := (g(y), [?])
]F [ y := da ]BHere is an alternative graphical notation for thecomposition of d2:f gy264 y :=a375B=a f gIn this notation, nodes that are not marked withvariables are positioned (indicated by the dottedprojection lines), while the (dashed) variable nodesdangle unpositioned.3.3 Dependency trees for CCG derivationsTo encode CCG derivations as dependency trees,we annotate each composition rule of PF-CCG withinstructions for combining the partial dependencytrees for the substrings into a partial dependencytree for the larger string.
Essentially, we now com-bine partial dependency trees using forward andbackward concatenation rather than combining se-mantic representations by functional compositionand application.
From now on, we assume that thenode labels in the dependency trees are CCG lex-icon entries, and represent these by just the wordin them.The modified rules are shown in Figure 3.
Theyderive statements about triples w ` A : p, where wis a substring, A is a category, and p is a lambdaexpression over a partial dependency tree.
Eachvariable of p corresponds to an argument categoryin A, and vice versa.
Rule L covers the base case:the dependency tree for a lexical entry e is a treewith one node for the item itself, labelled with e,and one node for each of its syntactic arguments,labelled with a variable.
Rule F captures forwardcomposition: given two dependency trees d and d?,the new dependency tree is obtained by forwardconcatenation, binding the outermost variable in d.Rule B is the rule for backward composition.
Theresult of translating a complete PF-CCG derivation?
in this way is always a dependency tree withoutvariables; we call it d(?
).As an example, Figure 4 shows the construc-tion for the derivation in Figure 1.
The induceddependency tree looks like this:mer em Hans es huus h?lfed aastricheFor instance, the partial dependency tree for thelexicon entry of aastriiche contains two nodes: theroot (with address ?)
is labelled with the lexiconentry, and its child (address 1) is labelled with the463mer(mer, [?
])Lem Hans(Hans, [?
])Les huus(huus, [?
])Lh?lfed?x, y, z.
(h?lfed(x, y, z), [?])Laastriiche?w.
(aastriiche(w), [?
])L?w, y, z.
(h?lfed(aastriiche(w), y, z), [?, 1])F?y, z.
(h?lfed(aastriiche(huus), y, z), [11, ?, 1])B?z.
(h?lfed(aastriiche(huus),Hans, z), [2, 11, ?, 1])B(h?lfed(aastriiche(huus),Hans,mer), [3, 2, 11, ?, 1])BFigure 4: Computing a dependency tree for the derivation in Figure 1variable x.
This tree is inserted into the tree fromh?lfed by forward concatenation.
The variable w ispassed on into the new dependency tree, and laterfilled by backward concatenation to huus.
Passingthe argument slot of aastriiche to h?lfed to be filledon its left creates a non-projectivity; it correspondsto a crossed composition in CCG terms.
Noticethat the categories derived in Figure 1 mirror thefunctional structure of the partial dependency treesat each step of the derivation.3.4 Semantic equivalenceThe mapping from derivations to dependency treesloses some information: different derivations mayinduce the same dependency tree.
This is illus-trated by Figure 5, which provides two possiblederivations for the phrase big white rabbit, bothof which induce the same dependency tree.
Espe-cially in light of the fact that our dependency treeswill typically contain fewer dependencies than theDAGs derived by Clark et al (2002), one could askwhether dependency trees are an appropriate wayof representing the structure of a CCG derivation.However, at the end of the day, the most import-ant information that can be extracted from a CCGderivation is the semantic representation it com-putes; and it is possible to reconstruct the semanticrepresentation of a derivation ?
from d(?)
alone.
Ifwe forget the word order information in the depend-ency trees, the rules F and B in Figure 3 are merely?-expanded versions of the semantic constructionrules in Figure 2.
This means that d(?)
recordseverything we need to know about constructing thesemantic representation: We can traverse it bottom-up and apply the lexical semantic representationof each node to those of its subterms.
So whilethe dependency trees obliterate some informationin the CCG derivations (particularly its associativestructure), they are indeed appropriate represent-ations because they record all syntactic valenciesand encode enough information to recompute thesemantics.4 Strong generative capacityNow that we know how to see PF-CCG derivationsas dependency trees, we can ask what sets of suchtrees can be generated by PF-CCG grammars.
Thisis the question about the strong generative capa-city of PF-CCG, measured in terms of dependencytrees (Miller, 2000).
In this section, we give apartial answer to this question: We show that thesets of PF-CCG-induced valency trees (dependencytrees without their linear order) form regular treelanguages, but that the sets of dependency treesthemselves are irregular.
This is in contrast to otherprominent mildly context-sensitive grammar form-alisms such as Tree Adjoining Grammar (TAG;Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker etal.
(1987)), in which both languages are regular.4.1 CCG term languagesFormally, we define the language of all dependencytrees generated by a PF-CCG grammar G as the setLD(G) = { d(?)
| ?
is a derivation of G } .Furthermore, we define the set of valency trees tobe the set of just the term parts of each d(?
):LV (G) = { t | (t, s) ?
LD(G) } .By our previous assumption, the node labels of avalency tree are CCG lexicon entries.We will now show that the valency tree lan-guages of PF-CCG grammars are regular tree lan-guages (G?cseg and Steinby, 1997).
Regular treelanguages are sets of trees that can be generatedby regular tree grammars.
Formally, a regular treegrammar (RTG) is a construct ?
= (N,?, S, P ),where N is an alphabet of non-terminal symbols,?
is an alphabet of ranked term constructors calledterminal symbols, S ?
N is a distinguished startsymbol, and P is a finite set of production rules ofthe form A ?
?, where A ?
N and ?
is a termover ?
and N , where the nonterminals can be used464bignp/npwhitenp/npnp/nprabbitnpnp big white rabbitbignp/npwhitenp/nprabbitnpnp/npnpFigure 5: Different derivations may induce the same dependency treeas constants.
The grammar ?
generates trees fromthe start symbol by successively expanding occur-rences of nonterminals using production rules.
Forinstance, the grammar that contains the productionsS ?
f(A,A), A ?
g(A), and A ?
a generatesthe tree language { f(gm(a), gn(a)) | m,n ?
0 }.We now construct an RTG ?
(G) that generatesthe set of valency trees of a PF-CCG G. For theterminal alphabet, we choose the lexicon entries:If e = (a,A | B1 .
.
.
| Bn, f) is a lexicon entry ofG, we take e as an n-ary term constructor.
We alsotake the atomic categories of G as our nonterminalsymbols; the start category s of G counts as thestart symbol.
Finally, we encode each lexicon entryas a production rule: The lexicon entry e aboveencodes to the rule A?
e(Bn, .
.
.
, B1).Let us look at our running example to see howthis works.
Representing the lexicon entries as justthe words for brevity, we can write the valency treecorresponding to the CCG derivation in Figure 4as t0 = h?lfed(aastriiche(huus),Hans,mer); hereh?lfed is a ternary constructor, aastriiche is unary,and all others are constants.
Taking the lexicalcategories into account, we obtain the RTG withs?
h?lfed(vp, np, np)vp?
aastriiche(np)np?
huus | Hans | merThis grammar indeed generates t0, and all othervalency trees induced by the sample grammar.More generally, LV (G) ?
L(?
(G)) becausethe construction rules in Figure 3 ensure that ifa node v becomes the i-th child of a node u inthe term, then the result category of v?s lexiconentry equals the i-th argument category of u?s lex-icon entry.
This guarantees that the i-th nonter-minal child introduced by the production for u canbe expanded by the production for v. The con-verse inclusion can be shown by reconstructing,for each valency tree t, a CCG derivation ?
thatinduces t. This construction can be done by ar-ranging the nodes in t into an order that allowsus to combine every parent in t with its childrenusing only forward and backward application.
TheCCG derivation we obtain for the example is shownin Figure 6; it is a derivation for the sentencedas mer em Hans h?lfed es huus aastriiche, usingthe same lexicon entries.
Together, this shows thatL(?
(G)) = LV (G).
Thus:Theorem 1 The sets of valency trees generated byPF-CCG are regular tree languages.
2By this result, CCG falls in line with context-freegrammars, TAG, and LCFRS, whose sets of deriva-tional structures are all regular (Vijay-Shanker etal., 1987).
To our knowledge, this is the first timethe regular structure of CCG derivations has beenexposed.
It is important to note that while CCGderivations themselves can be seen as trees as well,they do not always form regular tree languages(Vijay-Shanker et al, 1987).
Consider for instancethe CCG grammar from Vijay-Shanker and Weir?s(1994) Example 2.4, which generates the string lan-guage anbncndn; Figure 7 shows the derivation ofaabbccdd.
If we follow this derivation bottom-up,starting at the first c, the intermediate categoriescollect an increasingly long tail of\a arguments; forlonger words from the language, this tail becomesas long as the number of cs in the string.
The in-finite set of categories this produces translates intothe need for an infinite nonterminal alphabet in anRTG, which is of course not allowed.4.2 Comparison with TAGIf we now compare PF-CCG to its most promin-ent mildly context-sensitive cousin, TAG, the reg-ularity result above paints a suggestive picture: APF-CCG valency tree assigns a lexicon entry toeach word and says which other lexicon entry fillseach syntactic valency.
In this respect, it is theanalogue of a TAG derivation tree (in which thelexicon entries are elementary trees), and we justsaw that PF-CCG and TAG generate the same treelanguages.
On the other hand, CCG and TAG areweakly equivalent (Vijay-Shanker and Weir, 1994),i.e.
they generate the same linear word orders.
Soone could expect that CCG and TAG also inducethe same dependency trees.
Interestingly, this isnot the case.465mernpLem HansnpLh?lfeds\np\np/vpLes huusnpLaastriichevp\npLvpBs\np\npFs\npBsBFigure 6: CCG derivation reconstructed from the dependency tree from Figure 4 using only applicationsWe know from the literature that those depend-ency trees that can be constructed from TAG deriva-tion trees are exactly those that are well-nested andhave a block-degree of at most 2 (Kuhlmann andM?hl, 2007).
The block-degree of a node u in a de-pendency tree is the number of ?blocks?
into whichthe subtree below u is separated by interveningnodes that are not below u, and the block-degreeof a dependency tree is the maximum block-degreeof its nodes.
So for instance, the dependency treeon the right-hand side of Figure 8 has block-degreetwo.
It is also well-nested, and can therefore beinduced by TAG derivations.Things are different for the dependency trees thatcan be induced by PF-CCG.
Consider the left-handdependency tree in Figure 8, which is induced bya PF-CCG derivation built from words with thelexical categories a/a, b\a, b\b, and a. Whilethis dependency tree is well-nested, it has block-degree three: The subtree below the leftmost nodeconsists of three parts.
More generally, we can in-sert more words with the categories a/a and b\bin the middle of the sentence to obtain depend-ency trees with arbitrarily high block-degrees fromthis grammar.
This means that unlike for TAG-induced dependency trees, there is no upper boundon the block-degree of dependency trees inducedby PF-CCG?as a consequence, there are CCGdependency trees that cannot be induced by TAG.On the other hand, there are also dependencytrees that can be induced by TAG, but not by PF-CCG.
The tree on the right-hand side of Figure 8is an example.
We have already argued that thistree can be induced by a TAG.
However, it con-tains no two adjacent nodes that are connected bya/a b\a a/a b\b a b\b 1 2 3 4Figure 8: The divergence between CCG and TAGan edge; and every nontrivial PF-CCG derivationmust combine two adjacent words at least at onepoint during the derivation.
Therefore, the treecannot be induced by a PF-CCG grammar.
Further-more, it is known that all dependency languagesthat can be generated by TAG or even, more gener-ally, by LCRFS, are regular in the sense of Kuhl-mann and M?hl (2007).
One crucial property ofregular dependency languages is that they have abounded block-degree; but as we have seen, thereare PF-CCG dependency languages with unboun-ded block-degree.
Therefore there are PF-CCGdependency languages that are not regular.
Hence:Theorem 2 The sets of dependency trees gener-ated by PF-CCG and TAG are incomparable.
2We believe that these results will generalize tofull CCG.
While we have not yet worked out theinduction of dependency trees from full CCG, thebasic rule that CCG combines adjacent substringsshould still hold; therefore, every CCG-induceddependency tree will contain at least one edgebetween adjacent nodes.
We are thus left witha very surprising result: TAG and CCG both gener-ate the same string languages and the same sets ofvalency trees, but they use incomparable mechan-isms for linearizing valency trees into sentences.4.3 A note on weak generative capacityAs a final aside, we note that the construction forextracting purely applicative derivations from theterms described by the RTG has interesting con-sequences for the weak generative capacity of PF-CCG.
In particular, it has the corollary that for anyPF-CCG derivation ?
over a string w, there is a per-mutation of w that can be accepted by a PF-CCGderivation that uses only application?that is, everystring language L that can be generated by a PF-CCG grammar has a context-free sublanguage L?such that all words in L are permutations of wordsin L?.This means that many string languages that wecommonly associate with CCG cannot be generated466aa/dLaa/dLbbLbbLcs\a/t\bLs\a/tBct\a\bLs\a\a\bFs\a\aBs\a/dBddLs\aFs/dBddLsFFigure 7: The CCG derivation of aabbccdd using Example 2.4 in Vijay-Shanker and Weir (1994)by PF-CCG.
One such language is anbncndn.
Thislanguage is not itself context-free, and thereforeany PF-CCG grammar whose language contains italso contains permutations in which the order ofthe symbols is mixed up.
The culprit for this amongthe restrictions that distinguish PF-CCG from fullCCG seems to be that PF-CCG grammars mustallow all instances of the application rules.
Thiswould mean that the ability of CCG to generate non-context-free languages (also linguistically relevantones) hinges crucially on its ability to restrict theallowable instances of rule schemata, for instance,using slash types (Baldridge and Kruijff, 2003).5 ConclusionIn this paper, we have shown how to read deriva-tions of PF-CCG as dependency trees.
Unlike pre-vious proposals, our view on CCG dependenciesis in line with the mainstream dependency parsingliterature, which assumes tree-shaped dependencystructures; while our dependency trees are less in-formative than the CCG derivations themselves,they contain sufficient information to reconstructthe semantic representation.
We used our new de-pendency view to compare the strong generativecapacity of PF-CCG with other mildly context-sensitive grammar formalisms.
It turns out thatthe valency trees generated by a PF-CCG grammarform regular tree languages, as in TAG and LCFRS;however, unlike these formalisms, the sets of de-pendency trees including word order are not regular,and in particular can be more non-projective thanthe other formalisms permit.
Finally, we foundnew formal evidence for the importance of restrict-ing rule schemata for describing non-context-freelanguages in CCG.All these results were technically restricted tothe fragment of PF-CCG, and one focus of futurework will be to extend them to as large a fragmentof CCG as possible.
In particular, we plan to extendthe lambda notation used in Figure 3 to cover type-raising and higher-order categories.
We would thenbe set to compare the behavior of wide-coveragestatistical parsers for CCG with statistical depend-ency parsers.We anticipate that our results about the stronggenerative capacity of PF-CCG will be useful totransfer algorithms and linguistic insights betweenformalisms.
For instance, the CRISP generationalgorithm (Koller and Stone, 2007), while specifiedfor TAG, could be generalized to arbitrary gram-mar formalisms that use regular tree languages?given our results, to CCG in particular.
On theother hand, we find it striking that CCG and TAGgenerate the same string languages from the sametree languages by incomparable mechanisms forordering the words in the tree.
Indeed, the exactcharacterization of the class of CCG-inducable de-pendency languages is an open issue.
This alsohas consequences for parsing complexity: We canunderstand why TAG and LCFRS can be parsed inpolynomial time from the bounded block-degreeof their dependency trees (Kuhlmann and M?hl,2007), but CCG can be parsed in polynomial time(Vijay-Shanker and Weir, 1990) without being re-stricted in this way.
This constitutes a most inter-esting avenue of future research that is opened upby our results.Acknowledgments.
We thank Mark Steedman,Jason Baldridge, and Julia Hockenmaier for valu-able discussions about CCG, and the reviewers fortheir comments.
The work of Alexander Kollerwas funded by a DFG Research Fellowship and theCluster of Excellence ?Multimodal Computing andInteraction?.
The work of Marco Kuhlmann wasfunded by the Swedish Research Council.467ReferencesJason Baldridge and Geert-Jan M. Kruijff.
2003.Multi-modal Combinatory Categorial Grammar.
InProceedings of the Tenth EACL, Budapest, Hungary.Johan Bos, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-coverage semantic representations from a CCGparser.
In Proceedings of the 20th COLING, Geneva,Switzerland.Stephen Clark and James Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4).Stephen Clark, Julia Hockenmaier, and Mark Steed-man.
2002.
Building deep dependency structureswith a wide-coverage CCG parser.
In Proceedingsof the 40th ACL, Philadelphia, USA.Ferenc G?cseg and Magnus Steinby.
1997.
Tree lan-guages.
In Rozenberg and Salomaa (Rozenberg andSalomaa, 1997), pages 1?68.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Julia Hockenmaier and Peter Young.
2008.
Non-localscrambling: the equivalence of TAG and CCG re-visited.
In Proceedings of TAG+9, T?bingen, Ger-many.Julia Hockenmaier.
2006.
Creating a CCGbank anda wide-coverage CCG lexicon for German.
In Pro-ceedings of COLING/ACL, Sydney, Australia.Aravind K. Joshi and Yves Schabes.
1997.
Tree-Adjoining Grammars.
In Rozenberg and Salomaa(Rozenberg and Salomaa, 1997), pages 69?123.Alexander Koller and Matthew Stone.
2007.
Sentencegeneration as planning.
In Proceedings of the 45thACL, Prague, Czech Republic.Marco Kuhlmann and Mathias M?hl.
2007.
Mildlycontext-sensitive dependency languages.
In Pro-ceedings of the 45th ACL, Prague, Czech Republic.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT/EMNLP.Philip H. Miller.
2000.
Strong Generative Capacity:The Semantics of Linguistic Formalism.
Universityof Chicago Press.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?lsen Eryigit, Sandra K?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Grzegorz Rozenberg and Arto Salomaa, editors.
1997.Handbook of Formal Languages.
Springer.Stuart Shieber.
1985.
Evidence against the context-freeness of natural language.
Linguistics and Philo-sophy, 8:333?343.Mark Steedman and Jason Baldridge.
2009.
Combin-atory categorial grammar.
In R. Borsley and K. Bor-jars, editors, Non-Transformational Syntax.
Black-well.
To appear.Mark Steedman.
2001.
The Syntactic Process.
MITPress.K.
Vijay-Shanker and David Weir.
1990.
Polynomialtime parsing of combinatory categorial grammars.In Proceedings of the 28th ACL, Pittsburgh, USA.K.
Vijay-Shanker and David J. Weir.
1994.
The equi-valence of four extensions of context-free grammars.Mathematical Systems Theory, 27(6):511?546.K.
Vijay-Shanker, David J. Weir, and Aravind K. Joshi.1987.
Characterizing structural descriptions pro-duced by various grammatical formalisms.
In Pro-ceedings of the 25th ACL, Stanford, CA, USA.468
