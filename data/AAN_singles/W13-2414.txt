Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 94?99,Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational LinguisticsRecognition of Named Entities Boundaries in Polish TextsMicha?
Marcin?czuk and Jan Kocon?Institute of Informatics, Wroc?aw University of TechnologyWybrzez?e Wyspian?skiego 27Wroc?aw, Poland{michal.marcinczuk,jan.kocon}@pwr.wroc.plAbstractIn the paper we discuss the problem of lowrecall for the named entity (NE) recogni-tion task for Polish.
We discuss to whatextent the recall of NE recognition can beimproved by reducing the space of NE cat-egories.
We also present several exten-sions to the binary model which give animprovement of the recall.
The extensionsinclude: new features, application of ex-ternal knowledge and post-processing.
Forthe partial evaluation the final model ob-tained 90.02% recall with 91.30% preci-sion on the corpus of economic news.1 IntroductionNamed entity recognition (NER) aims at identi-fying text fragments which refer to some objectsand assigning a category of that object from a pre-defined set (for example: person, location, orga-nization, artifact, other).
According to the ACE(Automatic Content Extraction) English Annota-tion Guidelines for Entities (LDC, 2008) there areseveral types of named entities, including: propernames, definite descriptions and noun phrases.In this paper we focus on recognition of propernames (PNs) in Polish texts.For Polish there are only a few accessible mod-els for PN recognition.
Marcin?czuk and Jan-icki (2012) presented a hybrid model (a statisti-cal model combined with some heuristics) whichobtained 70.53% recall with 91.44% precision fora limited set of PN categories (first names, lastnames, names of countries, cities and roads) testedon the CEN corpus1 (Marcin?czuk et al 2013).A model for an extended set of PN categories(56 categories) presented by Marcin?czuk et al(2013) obtained much lower recall of 54% with93% precision tested on the same corpus.
Savary1Home page: http://nlp.pwr.wroc.pl/cen.and Waszczuk (2012) presented a statistical modelwhich obtained 76% recall with 83% precision fornames of people, places, organizations, time ex-pressions and name derivations tested on the Na-tional Corpus of Polish2 (Przepi?rkowski et al2012).There are also several other works on PN recog-nition for Polish where a rule-based approach wasused.
Piskorski et al(2004) constructed a set ofrules and tested them on 100 news from the Rzecz-pospolita newspaper.
The rules obtained 90.6%precision and 85.3% recall for person names and87.9% precision and 56.6% recall for companynames.
Urban?ska and Mykowiecka (2005) alsoconstructed a set of rules for recognition of personand organization names.
The rules were tested on100 short texts from the Internet.
The rules ob-tained 98% precision and 89% recall for personnames and 85% precision and 73% recall for orga-nization names.
Another rule-based approach foran extended set of proper names was presented byAbramowicz et al(2006).
The rules were testedon 156 news from the Rzeczpospolita newspaper,the Tygodnik Powszechny newspaper and the newsweb portals.
The rules obtained 91% precision and93% recall for country names, 55% precision and73% recall for city names, 87% precision and 70%recall for road names and 82% precision and 66%recall for person names.The accessible models for PN recognition forPolish obtain relatively good performance in termsof precision.
However, in some NLP tasks likerecognition of semantic relations between PNs(Marcin?czuk and Ptak, 2012), coreference reso-lution (Kopec?
and Ogrodniczuk, 2012; Broda etal., 2012a), machine translation (Gralin?ski et al2009a) or sensitive data anonymization (Gralin?skiet al 2009b) the recall is much more impor-tant than the fine-grained categorization of PNs.2Home page: http://nkjp.pl94Unfortunately, the only model recognising widerange of PN categories obtains only 54% recall.Therefore, our goal is to evaluate to what extentthe recall for this model can be improved.2 Evaluation methodologyIn the evaluation we used two corpora anno-tated with 56 categories of proper names: KPWr3(Broda et al 2012b) and CEN (already men-tioned in Section 1).
The KPWr corpus consists of747 documents containing near 200K tokens and16.5K NEs.
The CEN corpus consists of 797 doc-uments containing 148K tokens and 13.6K NEs.Both corpora were tagged using the morphologi-cal tagger WCRFT (Radziszewski, 2013).We used a 10-fold cross validation on the KPWrcorpus to select the optimal model.
The CEN cor-pus was used for a cross-corpus evaluation of theselected model.
In this case the model was trainedon the KPWr corpus and evaluated on the CENcorpus.
We presented results for strict and partialmatching evaluation (Chinchor, 1992).
The ex-periments were conducted using an open-sourceframework for named entity recognition calledLiner24 (Marcin?czuk et al 2013).3 Reduction of NE categoriesIn this section we investigate to what extent the re-call of NE recognition can be improved by reduc-ing the number of NE categories.
As a referencemodel we used the statistical model presented byMarcin?czuk and Janicki (2012).
The model usesthe Conditional Random Fields method and uti-lize four types of features, i.e.
orthographic (18features), morphological (6 features), wordnet (4features) and lexicon (10 features) ?
38 featuresin total.
The model uses only local features froma window of two preceding and two following to-kens.
The detailed description of the features ispresented in Marcin?czuk et al(2013).
We didnot used any post-processing methods describedby Marcin?czuk and Janicki (2012) (unambiguousgazetteer chunker, heuristic chunker) because theywere tuned for the specific set of NE categories.We have evaluated two schemas with a limitednumber of the NE categories.
In the first morecommon (Finkel et al 2005) schema, all PNsare divided into four MUC categories, i.e.
per-son, organization, location and other.
In the other3Home page: http://nlp.pwr.wroc.pl/kpwr.4http://nlp.pwr.wroc.pl/liner2schema, assuming a separate phases for PN recog-nition and classification (Al-Rfou?
and Skiena,2012), we mapped all the PN categories to a singlecategory, namely NAM.For the MUC schema we have tested two ap-proaches.
In the first approach we trained a sin-gle classifier for all the NE categories and in thesecond approach we trained four classifiers ?
onefor each category.
This way we have evaluatedthree models: Multi-MUC ?
a cascade of fourclassifiers, one classifier for every NE category;One-MUC ?
a single classifier for all MUC cat-egories; One-NAM ?
a single classifier for NAMcategory.Model P R FMulti-MUC 76.09% 57.41% 65.44%One-MUC 70.66% 65.39% 67.92%One-NAM 80.46% 78.59% 79.52%Table 1: Strict evaluation of the three NE modelsFor each model we performed the 10-fold cross-validation on the KPWr corpus and the results arepresented in Table 1.
As we expected the high-est performance was obtained for the One-NAMmodel where the problem of PN classification wasignored.
The model obtained recall of 78% with80% precision.
The results also show that the lo-cal features used in the model are insufficient topredict the PN category.4 Improving the binary modelIn this section we present and evaluate several ex-tensions which were introduced to the One-NAMmodel in order to increase its recall.
The exten-sions include: new features, application of exter-nal resources and post processing.4.1 Extensions4.1.1 Extended gazetteer featuresThe reference model (Marcin?czuk and Janicki,2012) uses only five gazetteers of PNs (first names,last names, names of countries, cities and roads).To include the other categories of PNs we used twoexisting resources: a gazetteer of proper namescalled NELexicon5 containing ca.
1.37 millionof forms and a gazetteer of PNs extracted fromthe National Corpus of Polish6 containing 153,4775http://nlp.pwr.wroc.pl/nelexicon.6http://clip.ipipan.waw.pl/Gazetteer95forms.
The categories of PNs were mapped intofour MUC categories: person, location, organi-zation and other.
The numbers of PNs for eachcategory are presented in Table 2.Category Symbol Form countperson per 455,376location loc 156,886organization org 832,339other oth 13,612TOTAL 1,441,634Table 2: The statistics of the gazetteers.We added four features, one for every category.The features were defined as following:gaz(n, c) =??????????
?B if n-th token starts a sequence of wordsfound in gazetteer cI if n-th token is part of a sequence ofwords found in gazetteer c excludingthe first token0 otherwisewhere c ?
{per, loc, org, oth} and n is the tokenindex in a sentence.
If two or more PNs from thesame gazetteer overlap, then the first and longestPN is taken into account.4.1.2 Trigger featuresA trigger is a word which can indicate presenceof a proper name.
Triggers can be divided intotwo groups: external (appear before or after PNs)and internal (are part of PNs).
We used a lexi-con of triggers called PNET (Polish Named En-tity Triggers)7.
The lexicon contains 28,000 in-flected forms divided into 8 semantic categories(bloc, country, district, geogName, orgName, per-sName, region and settlement) semi-automaticallyextracted from Polish Wikipedia8.
We divided thelexicon into 16 sets ?
two for every semantic cat-egory (with internal and external triggers).
We de-fined one feature for every lexicon what gives 16features in total.
The feature were defined as fol-lowing:trigger(n, s) =????
?1 if n-th token base is foundin set s0 otherwise7http://zil.ipipan.waw.pl/PNET.8http://pl.wikipedia.org4.1.3 Agreement featureAn agreement of the morphological attributes be-tween two consecutive words can be an indicatorof phrase continuity.
This observation was used byRadziszewski and Pawlaczek (2012) to recognizenoun phrases.
This information can be also help-ful in PN boundaries recognition.
The feature wasdefined as following:agr(n) =??????
?1 if number[n] = number[n?
1]and case[n] = case[n?
1]and gender[n] = gender[n?
1]0 otherwiseThe agr(n) feature for a token n has value 1when the n-th and n ?
1-th words have the samecase, gender and number.
In other cases the valueis 0.
If one of the attributes is not set, the value isalso 0.4.1.4 Unambiguous gazetteer look-upThere are many proper names which are wellknown and can be easily recognized usinggazetteers.
However, some of the proper namespresent in the gazetteers can be also commonwords.
In order to avoid this problem we used anunambiguous gazetteer look-up (Marcin?czuk andJanicki, 2012).
We created one gazetteer contain-ing all categories of PNs (see Section 4.1.1) anddiscarded all entries which were found in the SJPdictionary9 in a lower case form.4.1.5 HeuristicsWe created several simple rules to recognize PNson the basis of the orthographic features.
The fol-lowing phrases are recognized as proper names re-gardless the context:?
a camel case word ?
a single word contain-ing one or more internal upper case lettersand at least one lower case letter, for exam-ple RoboRally ?
a name of board game,?
a sequence of words in the quotationmarks ?
the first word must be capitalisedand shorter than 5 characters to avoid match-ing ironic or apologetic words and citations,?
a sequence of all-uppercase words ?
wediscard words which are roman numbers andignore all-uppercase sentences.9http://www.sjp.pl/slownik/ort.964.1.6 Names propagationThe reference model does not contain anydocument-based features.
This can be a prob-lem for documents where the proper names oc-cur several times but only a few of its occur-rences are recognised by the statistical model.
Theother may not be recognized because of the un-seen or unambiguous contexts.
In such cases theglobal information about the recognized occur-rences could be used to recognize the other unrec-ognized names.
However, a simple propagation ofall recognized names might cause loss in the preci-sion because of the common words which are alsoproper names.
To handle this problem we defineda set of patterns and propagate only those propernames which match one of the following pattern:(1) a sequence of two or more capitalised words;(2) all-uppercase word ended with a number; or(3) all-uppercase word ended with hyphen and in-flectional suffix.4.2 EvaluationTable 3 contains results of the 10-fold cross valida-tion on the KPWr corpus for the One-NAM model,One-NAM with every single extension and a com-plete model with all extensions.
The bold valuesindicate an improvement comparing to the baseOne-NAM model.
To check the statistical signif-icance of precision, recall and F-measure differ-ence we used Student?s t-test with a significancelevel ?
= 0.01 (Dietterich, 1998).
The asteriskindicates the statistically significant improvement.Model P R FOne-NAM 80.46% 78.59% 79.52%Gazetteers 80.60% 78.71% 79.64%Triggers 80.60% 78.58% 79.58%Agreement 80.73% 78.90% 79.80%Look-up 80.18% 79.56%* 79.87%Heuristics 79.98% 79.20%* 79.59%Propagate 80.46% 78.59% 79.52%Complete 80.33% 80.61%* 80.47%*Table 3: The 10-fold cross validation on the KPWrcorpus for One-NAM model with different exten-sions.Five out of six extensions improved the perfor-mance.
Only for the name propagation we didnot observe any improvement because the KPWrcorpus contains only short documents (up to 300words) and it is uncommon that a name will appearmore than one time in the same fragment.
How-ever, tests on random documents from the Internetshowed the usefulness of this extension.For the unambiguous gazetteer look-up and theheuristics we obtained a statistically significantimprovement of the recall.
In the final model weincluded all the presented extensions.
The finalmodel achieved a statistically significant improve-ment of the recall and the F-measure.To check the generality of the extensions, weperformed the cross-domain evaluation on theCEN corpus (see Section 2).
The results for the56nam, the One-NAM and the Improved One-NAM models are presented in Table 4.
For thestrict evaluation, the recall was improved by al-most 4 percentage points with a small precisionimprovement by almost 2 percentage points.Evaluation P R F56nam model (Marcin?czuk et al 2013)Strict 93% 54% 68%One-NAM modelStrict 85.98% 81.31% 83.58%Partial 91.12% 86.65% 88.83%Improved One-NAM modelStrict 86.61% 85.05% 85.82%Partial 91.30% 90.02% 90.65%Table 4: The cross-domain evaluation of the basicand improved One-NAM models on CEN.5 ConclusionsIn the paper we discussed the problem of low re-call of models for recognition of a wide range ofPNs for Polish.
We tested to what extent the reduc-tion of the PN categories can improve the recall.As we expected the model without PN classifica-tion obtained the best results in terms of precisionand recall.Then we presented a set of extensions to theOne-NAM model, including new features (mor-phological agreement, triggers, gazetteers), ap-plication of external knowledge (a set of heuris-tics and a gazetteer-based recogniser) and post-processing (proper names propagation).
The finalmodel obtained 90.02% recall with 91.30% preci-sion on the CEN corpus for the partial evaluationwhat is a good start of further NE categorizationphase.97AcknowledgmentsThis work was financed by Innovative EconomyProgramme project POIG.01.01.02-14-013/09.ReferencesWitold Abramowicz, Agata Filipowska, Jakub Pisko-rski, Krzysztof We?cel, and Karol Wieloch.
2006.Linguistic Suite for Polish Cadastral System.
InProceedings of the LREC?06, pages 53?58, Genoa,Italy.Rami Al-Rfou?
and Steven Skiena.
2012.
SpeedRead:A fast named entity recognition pipeline.
In Pro-ceedings of COLING 2012, pages 51?66, Mumbai,India, December.
The COLING 2012 OrganizingCommittee.Bartosz Broda, Lukasz Burdka, and Marek Maziarz.2012a.
Ikar: An improved kit for anaphora resolu-tion for polish.
In Martin Kay and Christian Boitet,editors, COLING (Demos), pages 25?32.
Indian In-stitute of Technology Bombay.Bartosz Broda, Micha?
Marcin?czuk, Marek Maziarz,Adam Radziszewski, and Adam Wardyn?ski.
2012b.KPWr: Towards a Free Corpus of Polish.
In Nico-letta Calzolari, Khalid Choukri, Thierry Declerck,Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of LREC?12.
ELRA.Nancy Chinchor.
1992.
MUC-4 Evaluation Metrics.In Proceedings of the Fourth Message Understand-ing Conference, pages 22?29.Thomas G. Dietterich.
1998.
Approximate statisticaltests for comparing supervised classification learn-ing algorithms.
Neural Computation, 10(7):1895?1924.Jenny Rose Finkel, Trond Grenager, and Christo-pher D. Manning.
2005.
Incorporating Non-localInformation into Information Extraction Systems byGibbs Sampling.
In The Association for Com-puter Linguistics, editor, Proceedings of the 43ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL 2005), pages 363?370.Filip Gralin?ski, Krzysztof Jassem, and Micha?
Mar-cin?czuk.
2009a.
An Environment for Named En-tity Recognition and Translation.
In L M?rquez andH Somers, editors, Proceedings of the 13th AnnualConference of the European Association for Ma-chine Translation, pages 88?95, Barcelona, Spain.Filip Gralin?ski, Krzysztof Jassem, Micha?
Marcin?czuk,and Pawe?
Wawrzyniak.
2009b.
Named EntityRecognition in Machine Anonymization.
In M AK?opotek, A Przepi?rkowski, A T Wierzchon?, andK Trojanowski, editors, Recent Advances in Intel-ligent Information Systems., pages 247?260.
Aca-demic Pub.
House Exit.Mateusz Kopec?
and Maciej Ogrodniczuk.
2012.Creating a coreference resolution system for pol-ish.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Thierry Declerck, Mehmet Ug?urDog?an, Bente Maegaard, Joseph Mariani, JanOdijk, and Stelios Piperidis, editors, Proceedingsof the Eight International Conference on LanguageResources and Evaluation (LREC?12), Istanbul,Turkey, may.
European Language Resources Asso-ciation (ELRA).LDC.
2008.
ACE (Automatic Content Extraction) En-glish Annotation Guidelines for Relations (Version6.2).Micha?
Marcin?czuk and Maciej Janicki.
2012.
Opti-mizing CRF-Based Model for Proper Name Recog-nition in Polish Texts.
In Alexander F. Gelbukh, ed-itor, CICLing (1), volume 7181 of Lecture Notes inComputer Science, pages 258?269.
Springer.Micha?
Marcin?czuk, Jan Kocon?, and Maciej Janicki.2013.
Liner2 - A Customizable Framework forProper Names Recognition for Polish.
In RobertBembenik, ?ukasz Skonieczny, Henryk Rybin?ski,Marzena Kryszkiewicz, and Marek Niezg?dka, ed-itors, Intelligent Tools for Building a Scientific In-formation Platform, volume 467 of Studies in Com-putational Intelligence, pages 231?253.
Springer.Micha?
Marcin?czuk and Marcin Ptak.
2012.
Prelimi-nary study on automatic induction of rules for recog-nition of semantic relations between proper namesin polish texts.
In Petr Sojka, Ales Hor?k, IvanKopecek, and Karel Pala, editors, Text, Speech andDialogue ?
15th International Conference, TSD2012, Brno, Czech Republic, September 3-7, 2012.Proceedings, volume 7499 of Lecture Notes in Arti-ficial Intelligence (LNAI).
Springer-Verlag, Septem-ber.Jakub Piskorski, Peter Homola, Ma?gorzata Marciniak,Agnieszka Mykowiecka, Adam Przepi?rkowski,and Marcin Wolin?ski.
2004.
Information Extrac-tion for Polish Using the SProUT Platform.
InMieczyslaw A. K?opotek, Slawomir T. Wierzchon?,and Krzysztof Trojanowski, editors, Intelligent In-formation Processing and Web Mining, Proceed-ings of the International IIS: IIPWM?04 Conference,Advances in Soft Computing, Zakopane.
Springer-Verlag.Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa?
L.G?rski, and Barbara Lewandowska-Tomaszczyk,editors.
2012.
Narodowy Korpus Je?zyka Polskiego[Eng.
: National Corpus of Polish].
WydawnictwoNaukowe PWN, Warsaw.Adam Radziszewski and Adam Pawlaczek.
2012.Large-Scale Experiments with NP Chunking of Pol-ish.
In Petr Sojka, Ale?
Hor?k, Ivan Kopec?ek,and Karel Pala, editors, TSD, volume 7499 of Lec-ture Notes in Computer Science, pages 143?149.Springer Berlin Heidelberg.98Adam Radziszewski.
2013.
A Tiered CRF Tagger forPolish.
In Robert Bembenik, ?ukasz Skonieczny,Henryk Rybin?ski, Marzena Kryszkiewicz, andMarek Niezg?dka, editors, Intelligent Tools forBuilding a Scientific Information Platform, volume467 of Studies in Computational Intelligence, pages215?230.
Springer Berlin Heidelberg.Agata Savary and Jakub Waszczuk.
2012.
Narze?dziado anotacji jednostek nazewniczych.
In AdamPrzepi?rkowski, Miros?aw Ban?ko, Rafa?
L. G?rski,and Barbara Lewandowska-Tomaszczyk, editors,Narodowy Korpus Je?zyka Polskiego.
WydawnictwoNaukowe PWN.
Creative Commons Uznanie Au-torstwa 3.0 Polska.Dominika Urban?ska and Agnieszka Mykowiecka.2005.
Multi-words Named Entity Recognition inPolish texts.
In Radovan Grab?k, editor, SLOVKO2005 ?
Third International Seminar ComputerTreatment of Slavic and East European Languages,Bratislava, Slovakia, pages 208?215.
VEDA.99
