Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 206?212,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsJoshua 5.0: Sparser, better, faster, serverMatt Post1 and Juri Ganitkevitch2 and Luke Orland1 and Jonathan Weese2 and Yuan Cao21Human Language Technology Center of Excellence2Center for Language and Speech ProcessingJohns Hopkins UniversityChris Callison-BurchComputer and Information Sciences DepartmentUniversity of PennsylvaniaAbstractWe describe improvements made over thepast year to Joshua, an open-source trans-lation system for parsing-based machinetranslation.
The main contributions thispast year are significant improvements inboth speed and usability of the grammarextraction and decoding steps.
We havealso rewritten the decoder to use a sparsefeature representation, enabling training oflarge numbers of features with discrimina-tive training methods.1 IntroductionJoshua is an open-source toolkit1 for hierarchicaland syntax-based statistical machine translationof human languages with synchronous context-free grammars (SCFGs).
The original version ofJoshua (Li et al 2009) was a port (from Python toJava) of the Hiero machine translation system in-troduced by Chiang (2007).
It was later extendedto support grammars with rich syntactic labels (Liet al 2010).
Subsequent efforts produced Thrax,the extensible Hadoop-based extraction tool forsynchronous context-free grammars (Weese et al2011), later extended to support pivoting-basedparaphrase extraction (Ganitkevitch et al 2012).Joshua 5.0 continues our yearly update cycle.The major components of Joshua 5.0 are:?3.1 Sparse features.
Joshua now supports aneasily-extensible sparse feature implementa-tion, along with tuning methods (PRO andkbMIRA) for efficiently setting the weightson large feature vectors.1joshua-decoder.org?3.2 Significant speed increases.
Joshua 5.0 is upto six times faster than Joshua 4.0, and alsodoes well against hierarchical Moses, whereend-to-end decoding (including model load-ing) of WMT test sets is as much as threetimes faster.
?3.3 Thrax 2.0.
Our reengineered Hadoop-basedgrammar extractor, Thrax, is up to 300%faster while using significantly less interme-diate disk space.
?3.4 Many other features.
Joshua now includes aserver mode with fair round-robin schedulingamong and within requests, a bundler for dis-tributing trained models, improvements to theJoshua pipeline (for managing end-to-end ex-periments), and better documentation.2 OverviewJoshua is an end-to-end statistical machine trans-lation toolkit.
In addition to the decoder com-ponent (which performs the actual translation), itincludes the infrastructure needed to prepare andalign training data, build translation and languagemodels, and tune and evaluate them.This section provides a brief overview of thecontents and abilities of this toolkit.
More infor-mation can be found in the online documentation(joshua-decoder.org/5.0/).2.1 The Pipeline: Gluing it all togetherThe Joshua pipeline ties together all the infrastruc-ture needed to train and evaluate machine transla-tion systems for research or industrial purposes.Once data has been segmented into parallel train-ing, development, and test sets, a single invocationof the pipeline script is enough to invoke this entireinfrastructure from beginning to end.
Each step is206broken down into smaller steps (e.g., tokenizing afile) whose dependencies are cached with SHA1sums.
This allows a reinvoked pipeline to reliablyskip earlier steps that do not need to be recom-puted, solving a common headache in the researchand development cycle.The Joshua pipeline is similar to other ?ex-periment management systems?
such as Moses?Experiment Management System (EMS), a muchmore general, highly-customizable tool that al-lows the specification and parallel execution ofsteps in arbitrary acyclic dependency graphs(much like the UNIX make tool, but written withmachine translation in mind).
Joshua?s pipelineis more limited in that the basic pipeline skeletonis hard-coded, but reduced versatility covers manystandard use cases and is arguably easier to use.The pipeline is parameterized in many ways,and all the options below are selectable withcommand-line switches.
Pipeline documentationis available online.2.2 Data preparation, alignment, and modelbuildingData preparation involves data normalization (e.g.,collapsing certain punctuation symbols) and tok-enization (with the Penn treebank or user-specifiedtokenizer).
Alignment with GIZA++ (Och andNey, 2000) and the Berkeley aligner (Liang et al2006b) are supported.Joshua?s builtin grammar extractor, Thrax, isa Hadoop-based extraction implementation thatscales easily to large datasets (Ganitkevitch et al2013).
It supports extraction of both Hiero (Chi-ang, 2005) and SAMT grammars (Zollmann andVenugopal, 2006) with extraction heuristics eas-ily specified via a flexible configuration file.
Thepipeline also supports GHKM grammar extraction(Galley et al 2006) using the extractors availablefrom Michel Galley2 or Moses.SAMT and GHKM grammar extraction requirea parse tree, which are produced using the Berke-ley parser (Petrov et al 2006), or can be done out-side the pipeline and supplied as an argument.2.3 DecodingThe Joshua decoder is an implementation of theCKY+ algorithm (Chappelier et al 1998), whichgeneralizes CKY by removing the requirement2nlp.stanford.edu/?mgalley/software/stanford-ghkm-latest.tar.gzthat the grammar first be converted to Chom-sky Normal Form, thereby avoiding the complex-ities of explicit binarization schemes (Zhang etal., 2006; DeNero et al 2009).
CKY+ main-tains cubic-time parsing complexity (in the sen-tence length) with Earley-style implicit binariza-tion of rules.
Joshua permits arbitrary SCFGs, im-posing no limitation on the rank or form of gram-mar rules.Parsing complexity is still exponential in thescope of the grammar,3 so grammar filtering re-mains important.
The default Thrax settings ex-tract only grammars with rank 2, and the pipelineimplements scope-3 filtering (Hopkins and Lang-mead, 2010) when filtering grammars to test sets(for GHKM).Joshua uses cube pruning (Chiang, 2007) witha default pop limit of 100 to efficiently explore thesearch space.
Other decoder options are too nu-merous to mention here, but are documented on-line.2.4 Tuning and testingThe pipeline allows the specification (and optionallinear interpolation) of an arbitrary number of lan-guage models.
In addition, it builds an interpo-lated Kneser-Ney language model on the targetside of the training data using KenLM (Heafield,2011; Heafield et al 2013), BerkeleyLM (Paulsand Klein, 2011) or SRILM (Stolcke, 2002).Joshua ships with MERT (Och, 2003) and PROimplementations.
Tuning with k-best batch MIRA(Cherry and Foster, 2012) is also supported viacallouts to Moses.3 What?s New in Joshua 5.03.1 Sparse featuresUntil a few years ago, machine translation systemswere for the most part limited in the number of fea-tures they could employ, since the line-based op-timization method, MERT (Och, 2003), was notable to efficiently search over more than tens offeature weights.
The introduction of discrimina-tive tuning methods for machine translation (Lianget al 2006a; Tillmann and Zhang, 2006; Chianget al 2008; Hopkins and May, 2011) has madeit possible to tune large numbers of features instatistical machine translation systems, and open-3Roughly, the number of consecutive nonterminals in arule (Hopkins and Langmead, 2010).207source implementations such as Cherry and Foster(2012) have made it easy.Joshua 5.0 has moved to a sparse feature rep-resentation internally.
First, to clarify terminol-ogy, a feature as implemented in the decoder isactually a template that can introduce any numberof actual features (in the standard machine learn-ing sense).
We will use the term feature functionfor these templates and feature for the individual,traditional features that are induced by these tem-plates.
For example, the (typically dense) featuresstored with the grammar on disk are each separatefeatures contributed by the PHRASEMODEL fea-ture function template.
The LANGUAGEMODELtemplate contributes a single feature value for eachlanguage model that was loaded.For efficiency, Joshua does not store the en-tire feature vector during decoding.
Instead, hy-pergraph nodes maintain only the best cumulativescore of each incoming hyperedge, and the edgesthemselves retain only the hyperedge delta (the in-ner product of the weight vector and features in-curred by that edge).
After decoding, the featurevector for each edge can be recomputed and ex-plicitly represented if that information is requiredby the decoder (for example, during tuning).This functionality is implemented via the fol-lowing feature function interface, presented herein simplified pseudocode:interface FeatureFunction:apply(context, accumulator)The context comprises fixed pieces of the inputsentence and hypergraph:?
the hypergraph edge (which represents theSCFG rule and sequence of tail nodes)?
the complete source sentence?
the input spanThe accumulator object?s job is to accumulatefeature (name,value) pairs fired by a feature func-tion during the application of a rule, via anotherinterface:interface Accumulator:add(feature_name, value)The accumulator generalization4 permits the useof a single feature-gathering function for two ac-cumulator objects: the first, used during decoding,maintains only a weighted sum, and the second,4Due to Kenneth Heafield.used (if needed) during k-best extraction, holdsonto the entire sparse feature vector.For tuning large sets of features, Joshua sup-ports both PRO (Hopkins and May, 2011), an in-house version introduced with Joshua 4.0, and k-best batch MIRA (Cherry and Foster, 2012), im-plemented via calls to code provided by Moses.3.2 Performance improvementsWe introduced many performance improvements,replacing code designed to get the job done underresearch timeline constraints with more efficientalternatives, including smarter handling of lockingamong threads, more efficient (non string-based)computation of dynamic programming state, andreplacement of fixed class-based array structureswith fixed-size literals.We used the following experimental setup tocompare Joshua 4.0 and 5.0: We extracted a largeGerman-English grammar from all sentences withno more than 50 words per side from Europarl v.7(Koehn, 2005), News Commentary, and the Com-mon Crawl corpora using Thrax default settings.After filtering against our test set (newstest2012),this grammar contained 70 million rules.
We thentrained three language models on (1) the targetside of our grammar training data, (2) EnglishGigaword, and (3) the monolingual English datareleased for WMT13.
We tuned a system usingkbMIRA and decoded using KenLM (Heafield,2011).
Decoding was performed on 64-core 2.1GHz AMD Opteron processors with 256 GB ofavailable memory.Figure 1 plots the end-to-end runtime5 as afunction of the number of threads.
Each point inthe graph is the minimum of at least fifteen runscomputed at different times over a period of a fewdays.
The main point of comparison, betweenJoshua 4.0 and 5.0, shows that the current versionis up to 500% faster than it was last year, espe-cially in multithreaded situations.For further comparison, we took these models,converted them to hierarchical Moses format, andthen decoded with the latest version.6 We com-piled Moses with the recommended optimizationsettings7 and used the in-memory (SCFG) gram-5i.e., including model loading time and grammar sorting6The latest version available on Github as of June 7, 20137With tcmalloc and the following compile flags:--max-factors=1 --kenlm-max-order=5debug-symbols=off208500 1000 2000 3000 40005000 100002 4  8  16  32  48decoding time (seconds) thread countJoshua 4.0 (in-memory)Moses (in-memory)Joshua 4.0 (packed)Joshua 5.0 (packed)Figure 1: End-to-end runtime as a function of thenumber of threads.
Each data point is the mini-mum of at least fifteen different runs.200 300 400 500 1000 20003000 4000 50002 4  8  16  32  48decoding time (seconds) thread countJoshua 5.0MosesFigure 2: Decoding time alone.mar format.
BLEU scores were similar.8 In thisend-to-end setting, Joshua is about 200% fasterthan Moses at high thread counts (Figure 1).Figure 2 furthers the Moses and Joshua com-parison by plotting only decoding time (subtract-ing out model loading and sorting times).
Moses?decoding speed is 2?3 times faster than Joshua?s,suggesting that the end-to-end gains in Figure 1are due to more efficient grammar loading.3.3 Thrax 2.0The Thrax module of our toolkit has undergonea similar overhaul.
The rule extraction code was822.88 (Moses), 22.99 (Joshua 4), and 23.23 (Joshua 5).long-term investment holding on todetamodtheJJ NN VBG IN TO DTNPPPVP?
?the long-term?=~sig?dep-det-R-investmentpos-L-TOpos-R-NNlex-R-investmentlex-L-todep-amod-R-investmentsyn-gov-NP syn-miss-L-NNlex-L-on-topos-L-IN-TOdep-det-R-NN dep-amod-R-NNFigure 3: Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links,and features reflecting the phrase?s CCG-style la-bel NP/NN are included in the context vector.rewritten to be easier to understand and extend, al-lowing, for instance, for easy inclusion of alterna-tive nonterminal labeling strategies.We optimized the data representation used forthe underlying map-reduce framework towardsgreater compactness and speed, resulting in a300% increase in extraction speed and an equiv-alent reduction in disk I/O (Table 1).
Thesegains enable us to extract a syntactically labeledGerman-English SAMT-style translation grammarfrom a bitext of over 4 million sentence pairs injust over three hours.
Furthermore, Thrax 2.0 iscapable of scaling to very large data sets, likethe composite bitext used in the extraction of theparaphrase collection PPDB (Ganitkevitch et al2013), which counted 100 million sentence pairsand over 2 billion words on the English side.Furthermore, Thrax 2.0 contains a module fo-cused on the extraction of compact distributionalsignatures over large datasets.
This distribu-tional mode collects contextual features for n-gram phrases, such as words occurring in a win-dow around the phrase, as well as dependency-based and syntactic features.
Figure 3 illustratesthe feature space.
We then compute a bit signaturefrom the resulting feature vector via a randomizedlocality-sensitive hashing projection.
This yields acompact representation of a phrase?s typical con-text.
To perform this projection Thrax relies onthe Jerboa toolkit (Van Durme, 2012).
As part ofthe PPDB effort, Thrax has been used to extractrich distributional signatures for 175 million 1-to-4-gram phrases from the Annotated Gigawordcorpus (Napoles et al 2012), a parsed and pro-209Cs-En Fr-En De-En Es-EnRules 112M 357M 202M 380MSpace Time Space Time Space Time Space TimeJoshua 4.0 120GB 112 min 364GB 369 min 211GB 203 min 413GB 397 minJoshua 5.0 31GB 25 min 101GB 81 min 56GB 44 min 108GB 84 minDifference -74.1% -77.7% -72.3% -78.0% -73.5% -78.3% -73.8% -78.8%Table 1: Comparing Hadoop?s intermediate disk space use and extraction time on a selection of Europarlv.7 Hiero grammar extractions.
Disk space was measured at its maximum, at the input of Thrax?s finalgrammar aggregation stage.
Runtime was measured on our Hadoop cluster with a capacity of 52 mappersand 26 reducers.
On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.cessed version of the English Gigaword (Graff etal., 2003).Thrax is distributed with Joshua and is alsoavailable as a separate download.93.4 Other featuresJoshua 5.0 also includes many features designedto increase its usability.
These include:?
A TCP/IP server architecture, designed tohandle multiple sets of translation requestswhile ensuring fairness in thread assignmentboth across and within these connections.?
Intelligent selection of translation and lan-guage model training data using cross-entropy difference to rank training candidates(Moore and Lewis, 2010; Axelrod et al2011) (described in detail in Orland (2013)).?
A bundler for easy packaging of trained mod-els with all of its dependencies.?
A year?s worth of improvements to theJoshua pipeline, including many new featuresand supported options, and increased robust-ness to error.?
Extended documentation.4 WMT SubmissionsWe submitted a constrained entry for all tracks ex-cept English-Czech (nine in total).
Our systemswere constructed in a straightforward fashion andwithout any language-specific adaptations usingthe Joshua pipeline.
For each language pair, wetrained a Hiero system on all sentences with nomore than fifty words per side in the Europarl,News Commentary, and Common Crawl corpora.9github.com/joshua-decoder/thraxWe built two interpolated Kneser-Ney languagemodels: one from the monolingual News Crawlcorpora (2007?2012), and another from the tar-get side of the training data.
For systems translat-ing into English, we added a third language modelbuilt on Gigaword.
Language models were com-bined linearly into a single language model usinginterpolation weights from the tuning data (new-stest2011).
We tuned our systems with kbMIRA.For truecasing, we used a monolingual translationsystem built on the training data, and finally deto-kenized with simple heuristics.5 SummaryThe 5.0 release of Joshua is the result of a signif-icant year-long research, engineering, and usabil-ity effort that we hope will be of service to theresearch community.
User-friendly packages ofJoshua are available from joshua-decoder.org, while developers are encouraged to partic-ipate via github.com/joshua-decoder/joshua.
Mailing lists, linked from the mainJoshua page, are available for both.Acknowledgments Joshua?s sparse feature rep-resentation owes much to discussions with ColinCherry, Barry Haddow, Chris Dyer, and KennethHeafield at MT Marathon 2012 in Edinburgh.This material is based on research sponsoredby the NSF under grant IIS-1249516 and DARPAunder agreement number FA8750-13-2-0017 (theDEFT program).
The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernmental purposes.
The views and conclusionscontained in this publication are those of the au-thors and should not be interpreted as representingofficial policies or endorsements of DARPA or theU.S.
Government.210ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of EMNLP, pages 355?362, Edinburgh, Scotland, UK., July.J.C.
Chappelier, M. Rajman, et al1998.
A generalizedCYK algorithm for parsing stochastic CFG.
In FirstWorkshop on Tabulation in Parsing and Deduction(TAPD98), pages 133?137.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of NAACL-HLT, pages 427?436, Montre?al,Canada, June.David Chiang, Yuval Marton, and Philip Resnik.2008.
Online large-margin training of syntactic andstructural translation features.
In Proceedings ofEMNLP, Waikiki, Hawaii, USA, October.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL, Ann Arbor, Michigan.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.John DeNero, Adam Pauls, and Dan Klein.
2009.Asynchronous binarization for synchronous gram-mars.
In Proceedings of ACL, Suntec, Singapore,August.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of ACL/COLING, Sydney, Australia, July.Juri Ganitkevitch, Yuan Cao, Jonathan Weese, MattPost, and Chris Callison-Burch.
2012.
Joshua 4.0:Packing, PRO, and paraphrases.
In Proceedings ofthe Workshop on Statistical Machine Translation.Juri Ganitkevitch, Chris Callison-Burch, and BenjaminVan Durme.
2013.
Ppdb: The paraphrase database.In Proceedings of HLT/NAACL.D.
Graff, J. Kong, K. Chen, and K. Maeda.
2003.English gigaword.
Linguistic Data Consortium,Philadelphia.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of ACL, Sofia, Bulgaria, August.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of theWorkshop on Statistical Machine Translation, pages187?197.
Association for Computational Linguis-tics.Mark Hopkins and Greg Langmead.
2010.
SCFGdecoding without binarization.
In Proceedings ofEMNLP, pages 646?655.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of EMNLP.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit, vol-ume 5.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar Zaidan.
2009.Joshua: An open source toolkit for parsing-basedmachine translation.
In Proceedings of the Work-shop on Statistical Machine Translation, Athens,Greece, March.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren N.G.
Thornton, Ziyuan Wang,Jonathan Weese, and Omar F. Zaidan.
2010.
Joshua2.0: a toolkit for parsing-based machine translationwith syntax, semirings, discriminative training andother goodies.
In Proceedings of the Workshop onStatistical Machine Translation.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,and Ben Taskar.
2006a.
An end-to-end discrimi-native approach to machine translation.
In Proceed-ings of ACL/COLING.Percy Liang, Ben Taskar, and Dan Klein.
2006b.Alignment by agreement.
In Proceedings ofNAACL, pages 104?111, New York City, USA, June.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of ACL (short papers), pages 220?224.Courtney Napoles, Matt Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In Proceedingsof AKBC-WEKEX 2012.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of ACL, pages 440?447, Hong Kong, China, October.Franz Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL,Sapporo, Japan.Luke Orland.
2013.
Intelligent selection of trans-lation model training data for machine translationwith TAUS domain data: A summary.
Master?s the-sis, Johns Hopkins University, Baltimore, Maryland,June.Adam Pauls and Dan Klein.
2011.
Faster and smallern-gram language models.
In Proceedings of ACL,pages 258?267, Portland, Oregon, USA, June.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of ACL,Sydney, Australia, July.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Seventh InternationalConference on Spoken Language Processing.211Christoph Tillmann and Tong Zhang.
2006.
A discrim-inative global training algorithm for statistical mt.
InProceedings of ACL/COLING, pages 721?728, Syd-ney, Australia, July.Benjamin Van Durme.
2012.
Jerboa: A toolkit for ran-domized and streaming algorithms.
Technical Re-port 7, Human Language Technology Center of Ex-cellence, Johns Hopkins University.Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez.
2011.
Joshua3.0: Syntax-based machine translation with theThrax grammar extractor.
In Proceedings of theWorkshop on Statistical Machine Translation.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proceedings of HLT/NAACL.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart pars-ing.
In Proceedings of the Workshop on StatisticalMachine Translation, New York, New York.212
