Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 363?373,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsMulti-Predicate Semantic Role LabelingHaitong Yang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing, 100190, China{htyang, cqzong}@nlpr.ia.ac.cnAbstractThe current approaches to Semantic RoleLabeling (SRL) usually perform role clas-sification for each predicate separately andthe interaction among individual predi-cate?s role labeling is ignored if there ismore than one predicate in a sentence.
Inthis paper, we prove that different predi-cates in a sentence could help each otherduring SRL.
In multi-predicate role label-ing, there are mainly two key points: argu-ment identification and role labeling of thearguments shared by multiple predicates.To address these issues, in the stage ofargument identification, we propose nov-el predicate-related features which help re-move many argument identification errors;in the stage of argument classification, weadopt a discriminative reranking approachto perform role classification of the sharedarguments, in which a large set of glob-al features are proposed.
We conductedexperiments on two standard benchmarks:Chinese PropBank and English PropBank.The experimental results show that ourapproach can significantly improve SRLperformance, especially in Chinese Prop-Bank.1 IntroductionSemantic Role Labeling (SRL) is a kind of shal-low semantic parsing task and its goal is to rec-ognize some related phrases and assign a jointstructure (WHO did WHAT to WHOM, WHEN,WHERE, WHY, HOW) to each predicate of a sen-tence (Gildea and Jurafsky, 2002).
Because ofthe ability of encoding semantic information, SR-L has been applied in many tasks of NLP, such asquestion and answering (Narayanan and Haraba-gir, 2004), information extraction (Surdeanu etThe justices will be forced to reconsider  the questions.
[      A1      ] [  Pred  ][      A0      ] [    Pred    ] [      A1      ]Figure 1: A sentence from English PropBank,with an argument shared by multiple predicatesal., 2003; Christensen et al., 2005), and machinetranslation (Wu and Fung, 2009; Liu and Gildea,2010; Xiong et al., 2012; Zhai et al., 2012).Currently, an SRL system works as follows:first identify argument candidates and then per-form classification for each argument candidate.However, this process only focuses on one inde-pendent predicate without considering the internalrelations of multiple predicates in a sentence.
Ac-cording to our statistics, more than 80% sentencesin Propbank carry multiple predicates.
One exam-ple is shown in Figure 1, in which there are twopredicates ?Force?
and ?Reconsider?.
Moreover,the constituent ?the justices?
is shared by the twopredicates and is labeled as A1 for ?Force?
but asA0 for ?Reconsider?.
We call this phenomenon ofthe shared arguments Role Transition .
Intuitive-ly, all predicates in a sentence are closely related toeach other and the internal relations between themwould be helpful for SRL.This paper has made deep investigation onmulti-predicate semantic role labeling.
We thinkthere are mainly two key points: argument identi-fication and role labeling of the arguments sharedby multiple predicates.
We adopt different strate-gies to address these two issues.During argument identification, there are a largenumber of identification errors caused by the poorperformance of auto syntax trees.
However, manyof these errors can be removed, if we take otherpredicates into consideration.
To achieve this pur-pose, we propose novel predicates-related featureswhich have been proved to be effective to recog-363nize many identification errors.
After these fea-tures added, the precision of argument identifica-tion improves significantly by 1.6 points and 0.9points in experiments on Chinese PropBank andEnglish PropBank respectively, with a slight lossin recall.Role labeling of the shared arguments is anoth-er key point.
The predicates and their shared argu-ment could be considered as a joint structure, withstrong dependencies between the shared argumen-t?s roles.
If we consider linguistic basis for jointmodeling of the shared argument?s roles, there areat least two types of information to be captured.The first type of information is the compatibilityof Role Transition among the shared argument?sroles.
A noun phrase may be labeled as A0 for apredicate and at the same time, it can be labeledas A1 for another predicate.
However, there arefew cases that a noun phrase is labeled as A0 for apredicate and as AM-ADV for another predicateat the same time.
Secondly, joint modeling theshared arguments could explore global informa-tion.
For example, in ?The columbia mall is ex-pected to open?, there are two predicates ?expect?and ?open?
and a shared argument ?the columbi-a mall?.
Because this shared argument is before?open?
and the predicate ?open?
is in active voice,a base classifier often incorrectly label this argu-ment A0 for ?open?.
But if we observe that the ar-gument is also an argument of ?expect?, it shouldbe labeled as A1 for ?expect?
and ?open?.Motivated by the above observations, we at-tempt to jointly model the shared arguments?
roles.Specifically, we utilize the discriminative rerank-ing approach that has been successfully employedin many NLP tasks.
Typically, this method firstcreates a list of n-best candidates from a base sys-tem, and then reranks them with arbitrary features(both local and global), which are either not com-putable or are computationally intractable withinthe base model.We conducted experiments on Chinese Prop-Bank and English PropBank.
Results show thatcompared with a state-of-the-art base model, theaccuracy of our joint model improves significant-ly by 2.4 points and 1.5 points on Chinese Prop-Bank and English PropBank respectively, whichsuggests that there are substantial gains to be madeby jointly modeling the shared arguments of mul-tiple predicates.Our contributions can be summarized as fol-lows:?
To the best of our knowledge, this is the firstwork to investigate the mutual effect of mul-tiple predicates?
semantic role labeling.?
We present a rich set of features for argumentidentification and shared arguments?
classifi-cation that yield promising performance.?
We evaluate our method on two standardbenchmarks: Chinese PropBank and EnglishPropBank.
Our approach performs well inboth, which suggests its good universality.The remainder of this paper is organized as fol-lows.
Section 2 gives an overview of our approach.We discuss the mutual effect of multi-predicate?argument identification and argument classifica-tion in Section 3 and Section 4 respectively.
Theexperiments and results are presented in Section5.
Some discussion and analysis can be found inSection 6.
Section 7 discusses the related work-s.
Finally, the conclusion and future work are inSection 8.2 Approach OverviewAs illustrated in Figure 2, our approach follows thestandard separation of the task of semantic role la-beling into two phases: Argument Identificationand Argument Classification .
We investigate theeffect of multiple predicates in Argument Identi-fication and Argument Classification respectively.Specifically, in the stage of Argument Identifica-tion, we introduce new features related to predi-cates which are effective to recognize many argu-ment identification errors.
In the stage of Argu-ment Classification, we concentrate on the classi-fication of the arguments shared by multiple pred-icates.
We first use a base model to generate n-best candidates for the shared arguments and thenconstruct a joint model to rerank the n-best list, inwhich a rich set of global features are proposed.3 Argument IdentificationIn this section, we investigate multi-predicate?
mu-tual effects in Argument Identification.
ArgumentIdentification is to recognize the arguments fromall candidates of each predicate.
Here, we usethe Maximum Entropy (ME) classifier to performbinary classification.
As a discriminative model,ME can easily incorporate arbitrary features and364C andidatesPhase 1:Argument Identification BaseFeaturesNewFeaturesCl assifierRefined ArgumentC andidatesPhase 2 :Argument ClassificationJoint ModelB ase ModelN - Best listFinal ResultsIs - SharedFigure 2: The overview of our approachachieve good performance.
The model is formu-lated as follows:p(y|x) =1Z(x)exp(?i?ifi(x, y)) (1)in which x is the input sample, y(0 or 1) is the out-put label, f(x, y) are feature functions and Z(x)is a normalization term as follows:Z(x) =?yexp(?i?ifi(x, y))3.1 Base FeaturesXue (2008) took a critical look at the features usedin SRL and achieved good performance.
So, weuse the same features in Xue (2008) as the basefeatures:?
Predicate lemma?
Path from node to predicate?
Head word?
Head word?s part-of-speech?
Verb class (Xue, 2008)?
Predicate and Head word combination?
Predicate and Phrase type combination?
Verb class and Head word combination?
Verb class and Phrase type combination3.2 Additional FeaturesIn the SRL community, it is widely recognizedthat the overall performance of a system is large-ly determined by the quality of syntactic parsers(Gildea and Palmer, 2002), which is particularlynotable in the identification stage.
Unfortunate-ly, the state-of-the-art auto parsers fall short of thedemands of applications.
Moreover, when thereare multiple predicates, or even multiple clausesin a sentence, the problem of syntactic ambiguityincreases drastically (Kim et al., 2000).
For ex-ample, in Figure 3, there is a sentence with twoconsecutive predicates ?/?
(is) and ? ??
(devel-op).
Compared with the gold tree, the auto tree isless preferable, which makes the classifier easilymistake ??Q?
(building) as an argument of ? ??
(develop) with base features.
But this identifi-cation error can be removed if we note that thereis another predicate ?/?
(is) before ? ??
(devel-IPNP VPV V NPDNP NPVC VPN N D EGIPNP VPVC NPCP NPIP DECV V NP?????
??????????????
?????
( a ) ( b)??
?
??
??
?
???
?Building is an economic activity of developing Pudong .Figure 3: An example from Chinese PropBank.Tree (a) is the gold syntax tree and (b) is parsed bya state of-the-art parser Berkeley parser.
On tree(b), ??Q?
(building) is mistaken as an argumentof ? ??
(develop) with base features.365op).
Similar examples with the pattern ?NP +/ +VV?
can be found in PropBank, in which the sub-ject NP of the sentence is usually not an argumentof the latter predicate.
Thus, ?/?
(is) is an effec-tive clue to detect this kind of identification error.It is challenging to obtain a fully correct syntaxtree for a complex sentence with multiple predi-cates.
Therefore, base features that heavily relyon syntax trees often fail in discriminating argu-ments from candidates as demonstrated in Figure3.
However, by considering the elements of neigh-boring predicates, we could capture useful clueslike in the above example and remove many iden-tification errors.
Below, we define novel predi-caterelated features to encode these ?clues?
to re-fine candidates.There are mainly five kinds of features as fol-lows.?
Is the given predicate the nearest one?This is a binary feature that indicates whetherthe predicate is the nearest one to the candi-date.?
Local adjunctThis is a binary feature designed for adjectiveand adverbial phrases.
Some adjunct phras-es, such as ???
(only), have a limited sphereof influence.
If the candidate is ?local?
butthe given predicate is not the nearest one, thecandidate is often not an argument for thegiven predicate.
To collect local adjuncts, wetraverse the whole training set to get the ini-tial lexicon of adjuncts and refine it manually.?
Cut-ClauseThis type of feature is a binary feature de-signed to distinguish identification errors ofnoun phrase candidates.
If a noun phrase can-didate is separated from the given predicateby a clause consisting of a NP and VP, thecandidate is usually not the argument of thegiven predicate.?
Different Relative Positions with Conjunc-tionsThis is a binary feature that describes whetherthe candidate and the predicate are located indifferent positions as separated by conjunc-tions such as ?F/?
(but).
Conjunctions areoften used to concatenate two clauses, but thefirst clause commonly describes one proposi-tion and the second clause describes anoth-er one.
Thus, if the candidate and the givenpredicate have different positions relative tothe conjunctions, the candidate is usually notthe argument of the given predicate.?
Consecutive Predicates SequenceWhen multiple predicates appear in a sen-tence consecutively, parse errors frequentlyoccurs due to the problems of syntactic am-biguity as demonstrated in Figure 2.
To in-dicate such errors, sequence features of thecandidates and consecutive predicates are de-fined specifically.
For instance, for the candi-date ??Q?
(building) of ? ??
(develop),the features are ?cand-/- ??
and ?cand-/-VV?, in which we use ?cand?
to representthe position of the candidate.4 Argument ClassificationIn this section, we investigate multi-predicate?
mu-tual effects in Argument Classification.
ArgumentClassification is to assign a label to each argumen-t candidate recognized by the phase of ArgumentIdentification.4.1 Base ModelA conventional method in Argument Classifica-tion is to assign a label to each argument candidateby a classifier independently.
We call this kind ofmethod Base Model.
In the base model, we stilladopt ME (1) as our classifier; all base features ofArgument Identification are contained (shown insubsection 3.1).
In addition, there are some otherfeatures:?
Position: the relative position of the candi-date argument compared to the predicate?
Subcat frame: the syntactic rule that expandsthe parent of the verb?
The first and the last word of the candidate?
Phrase type: the syntactic tag of the candidateargument?
Subcat frame+: the frame that consists of theNPs (Xue, 2008).3664.2 Joint ModelAs discussed briefly in Section 1, there are manydependencies between the shared arguments?
la-beling for different predicates, but the base modelcompletely ignores such useful information.
Toincorporate these dependencies, we employ thediscriminative reranking method.
Here, we firstestablish a unified framework for reranking.
Foran input x, the generic reranker selects the bestoutput y?among the set of candidates GEN(x)according to the scoring function:y?= argmaxy?GEN(x)score(y) (2)In our task, GEN(x) is a set of the n-best can-didates generated from the base model.
As usual,we calculate the score of a candidate by the dotproduct between a high dimensional feature and aweight W:score(y) = W ?
f(y) (3)We estimate the weight W using the aver-aged perceptron algorithm (Collins, 2002a) whichis well known for its fast speed and good per-formance in similar large-parameter NLP tasks(Huang, 2008).
The training algorithm of thegeneric averaged perceptron is shown in Table 1.In line 5, the algorithm updates W with the differ-ence (if any) between the feature representationsof the best scoring candidate and the gold candi-date.
We also use a refinement called ?averagedparameters?
that the final weight vector W is theaverage of weight vectors over T iterations and Nsamples.
This averaging effect has been shown toreduce overfitting and produces more stable results(Collins, 2002a).Pseudocode: Averaged Structured Perceptron1: Input: training data(xt, y?t) for t = 1, ..., T ;2: w?(0)?
0; v ?
0; i?
03: for n in 1,...,N do4: for t in 1, ..., T do5: w?(i+1)?
update w?
(i)according to (xt, y?t)6: v ?
v + w?i+17: i?
i+ 18: w?
?
v//(N ?
T )9: return w?Table 1: The perceptron training algorithm4.3 Features for Joint ModelHere, we introduce features used in the joint mod-el.
For clear illustration, we describe these fea-tures in the context of the example in Figure 1.Role Transition (RT): a binary feature to in-dicate whether the transitions among roles of thecandidate are reasonable.
Because all roles are as-signed to the same candidate, all role transitionsshould be compatible.
For instance, if an argu-ment is labeled as AM-TMP for one predicate, itcannot be labeled as AM-LOC for another pred-icate.
This feature is constructed by traversingthe training data to ascertain whether transitionsbetween all roles are reasonable.
In Table 2, welist some role transitions which are obtained fromthe training data of experiments on Chinese Prop-Bank.Roles and Predicates?
Sequence (RPS): ajoint feature template that concatenates roles andthe given predicates.
For the gold candidate?Arg1, Arg0?, the feature is ?Arg1-force, Arg0-reconsider?.Roles and Predicates?
Sequence with Rela-tive Orders (RPSWR): the template is similar tothe above one except that relative orders betweenroles and predicates are added.
If the shared argu-ment is before the given predicate, the feature isdescribed as ?Role-Predicate?
; otherwise, the fea-ture is ?Predicate-Role?.
And, if the predicate?svoice is passive, the order is reversed.
Thus, forthe gold candidate ?Arg1, Arg0?, this feature is?force-Arg1, Arg0-reconsider?.Roles and Phrase Type Sequence (RPTS)Roles and Head Word Sequence (RHWS)Roles and Head Word?s POS Sequence(RHWPS)These three features are utilized to explore theshared argument?s relations with roles.Time and Location Class (TLC): We findthere are much confusions between AM-TMP andAM-LOC in the base model.
To fix these errors,we add two features: Time and Location Class.For these features, we just collect phrases labeledas AM-TMP and AM-LOC from the training da-ta.
When the argument belongs to Time or Loca-tion Class, we add a sequence template consistingof ?Role-Time?
for Time Class or ?Role-Location?for Location Class.
For the gold candidate ?Arg1,Arg0?, the feature is ?Arg1-none, Arg0-none?
be-cause ?the justices?
belongs neither to Time Classnor to Location Class.367Role Arg0 Arg1 Arg2 AM-LOC AM-TMP AM-ADV AM-MNR AM-TPCArg0 + + + + + + + +Arg1 + + + + - + + +Arg2 + + + + - - - +AM-LOC + + + + - + - +AM-TMP + - - - + + - -AM-ADV + - + - + + - -AM-MNR + + - - - - + -AM-TPC + + + + + + - +Table 2: Some role transitons from Chinese PropBank.
?+?
means reasonable role transition and ?-?means illegal.5 Experiments5.1 Experimental SettingTo evaluate the performance of our approach, wehave conducted on two standard benchmarks: Chi-nese PropBank and English PropBank.
The exper-imental setting is as follows:Chinese:We use Chinese Proposition Bank 1.0.
All dataare divided into three parts.
648 files (from cht-b 081.fid to chtb 899.fid) are used as the trainingset.
40 files (from chtb 041.fid to chtb 080.fid)constitutes the development set.
The test set con-sists of 72 files (chtb 001.fid to chtb 040.fid andchtb 900.fid to chtb 931.fid).
This data setting isthe same as in (Xue, 2008; Sun et al., 2009).
Weadopt Berkeley Parser1to carry out auto parsingfor SRL and the parser is retrained on the trainingset.
We used n =10 joint assignments for trainingthe joint model and testing.English:We choose English Propbank as the evaluationcorpus.
According to the traditional partition, thetraining set consists of the annotations in Sections2 to 21, the development set is Section 24, andthe test set is Section 23.
This data setting is thesame as in (Xue and Palmer, 2004; Toutanova etal., 2005).
We adopt Charniak Parser2to carry outauto parsing for SRL and the parser is retrained onthe training set.
We used n =10 joint assignmentsfor training the joint model and testing.5.2 Experiment on Argument IdentificationWe first investigate the performance of our ap-proach in Argument Identification.For the task of Argument Identification (AI), we1http://code.google.com/p/berkeleyparser/2https://github.com/BLLIP/bllip-parseradopt auto parser to produce auto parsing trees forSRL.
The results are shown in Table 3.
We cansee that in the experiment of Chinese, the F1 scorereaches to 78.79% with base features.
While afteradditional predicates-related features are added,the precision has improved by 1.6 points with s-light loss in recall, which leads to the improve-ment of 0.6 points in F1.
The similar effect oc-curred in the experiment of English.
After addi-tional features added in the identification module,the precision is improved by about 0.9 points witha slight loss in recall, leading to an improvementof 0.3 points in F1.
However, the improvemen-t in English is slight smaller than in Chinese.
Wethink the main reason is that there are less parse er-rors in English than in Chinese.
All results demon-strate that the novel predicted-related features areeffective in recognizing many identification errorswhich are difficult to discriminate with base fea-tures.P(%) R(%) F1(%)ChBase 84.36 73.90 78.79+Additional 85.97 73.72 79.38*EnBase 82.86 76.83 79.73+Additional 83.75 76.69 80.06Table 3: Comparison with Base Features in Ar-gument Identification.
Scores marked by ?*?
aresignificantly better (p < 0.05) than base features.5.3 Experiment on Argument Classification5.3.1 ResultsErrors produced in AI will influenced the evalu-ation of Argument Classification (AC).
So, to e-valuate fairly we assume that the argument con-stituents of a predicate are already known, and the368Num Acc(%)ChShared 2060 91.36All 8462 92.77EnShared 2015 93.85All 14061 92.30Table 4: Performance of the Base Model in Argu-ment ClassificationMethods Acc(%)ChBase 91.36Joint 93.74*EnBase 93.85Joint 95.33*Table 5: Comparison with Base Model on sharedarguments.
Scores marked by ?*?
are significantlybetter (p < 0.05) than base model.task is only to assign the correct labels to the con-stituents.
The evaluation criterion is Accuracy.The results of the base model are shown in Ta-ble 4.
We first note that in testing set, there are alarge number of shared arguments, which weigh-s about one quarter of all arguments in Chineseand 14% in English.
Therefore, the fine process-ing of these arguments is essential for argumen-t classification.
However, the base model cannothandle these shared arguments so well in Chinesethat the accuracy of the shared arguments is lowerby about 1.4 points than the average value of allarguments.
Nevertheless, from Table 5 we can seethat our joint model?s accuracy on the shared argu-ments reaches 93.74%, 2.4 points higher than thebase model in Chinese.
Although the base mod-el obtain good performance on shared argumentsof English, our joint model?s performance reach-es 95.33%, 1.5 points higher than the base mod-el.
This indicates that even though the base modelis optimized to utilize a large set of features andachieves the state-of-the-art performance, it is stilladvantageous to model the joint information of theshared arguments.Another point to note is that our joint model inresolving English SRL task is not so good as inChinese SRL.
There are mainly two reasons.
Thefirst reason is that the shared arguments occur lessin English than in Chinese so that training sam-ples are insufficient for our discriminative model.The second reason is the annotation of some in-transitive verbs.
In English PropBank, there is aclass of intransitive verbs such as ?land?
(knownas verbs of variable behavior), for which the ar-gument can be tagged as either ARG0 or ARG1.Here, we take examples from the guideline3of En-glish PropBank to explain.
?A bullet (ARG1) landed at his feet?
?He (ARG0) landed?In the above examples, the two arguments andthe predicate ?land?
have the same relative orderand voice but the arguments have different label-s for their respective predicates.
In fact, accord-ing to the intention of the annotator, ARG0 andARG1 are both correct.
Unfortunately, in EnglishPropBank, there is only one gold label for each ar-gument, which leads to much noise for our jointmodel.
Moreover, such situations are not rare inthe corpus.5.3.2 Feature PerformanceWe investigate effects of the features of joint mod-el to the performance and results are shown in Ta-ble 6.
Each row shows the improvement over thebaseline when that feature is used in the joint mod-el.
We can see that features proposed are beneficialto the performance of the joint model.
But somefeatures like ?RPS?
and ?RPSRO?
play a more im-portant role.Features Chinese Englishbase 91.36 93.85RT 91.70 94.10RPS 92.30 94.70RPSRO 92.24 94.50RPTS 91.80 94.18RHWS 91.63 93.95RHWPS 91.43 94.23TCL 91.93 94.23All 93.74 95.33Table 6: Features performance in the Joint Model.We use first letter of words to represent features.5.4 SRL ResultsWe also conducted the complete experiment on theauto parse trees.
The results are shown in Table7.
In experiments on Chinese PropBank, we cansee that after novel predicate-related features areadded in the stage of Argument Identification, ourmodel outperforms the base model by 0.5 points3http://verbs.colorado.edu/propbank/EPB-AnnotationGuidelines.pdf369F1(%)ChineseBase 74.04Base + AI 74.50Base + AI + AC 75.31EnglishBase 76.44Base + AI 76.70Base + AI + AC 77.00Table 7: Results on auto parse trees.
Base mean-s the baseline system, +AI meaning predcates-related features added in AI, + AC meaning jointmodule added.Methods F1(%)ChineseXue(2008) 71.90Sun et al.
(2009) 74.12Ours 75.31EnglishSurdeanu and Turmo(2005) 76.46Ours 77.00Table 8: Comparison with Other Methodsin F1.
Furthermore, after incorporating the jointmodule, the performance goes up to 75.31%, 1.3points higher than the base model.
We obtain sim-ilar observations in experiments on English Prop-Bank, but due to reasons illustrated in Subsection5.3, the performance of our method is slight betterthan the base model.We compare our method with others and the re-sults are shown in Table 8.
In Chinese, Xue (2008)and Sun et al.
(2009) are pioneer works in ChineseSRL.
Our approach outperforms these approachesby about 3.4 and 1.9 F1 points respectively.
InEnglish SRL, we compare out method with Sur-deanu and Turmo (2005) which is best result ob-tained with single parse tree as the input in CON-LL 2005 SRL evaluation.
Our approach is betterthan their approach which ignores the relation ofmultiple predicates?
SRL.6 Discussion and AnalysisIn this section, we discuss some case studies thatillustrate the advantages of our model.
Some ex-amples from our experiments are shown in Table9.
In example (1), the argument is a preposition-al phrase ?
( n? ]
t I?
Y?
?
??
(at the same time of compulsory education) andshared by two predicates ??0?
(witness) and ?i'?
(expand).
In the corpus, a prepositional phraseis commonly labeled as ARGM-LOC and ARGM-TMP.
Thus, the base model labeled the argumentinto these classes but one as ARGM-LOC, anotheras ARGM-TMP.
Unfortunately, ARGM-LOC for??0?
(witness) is wrong while our joint modeloutputs both correct answers, which benefits fromthe role transition feature.
From Table 1, we cansee that the role transition between ARGM-TMPand ARGM-LOC is impossible, which lowers thescore of candidates containing both ARGM-LOCand ARGM-TMP in the joint model.
Thus, thejoint model is more likely to output the gold can-didate.In example (2), the argument is ?w? ?::?
(Hailar Airport) and shared by two predicates?i??
(expand) and ?:?
(become).
Because ofthe high similarity of the features in the base mod-el, the argument for both predicates is classifiedinto the same class ARG0, but the label for ?i??
(expand) is wrong.
Nevertheless, our joint mod-el obtains both correct labels, which benefits fromthe global features.
After searching the trainingdata, we find some similar examples to this one,such as ?0?
?L ?
?
i?
?
 l??
(The railway operation mileage is expanded to120 kilometers), in which ?0?
?L ?
?
(therailway operation mileage) is labeled as ARG1 for?i??
(expand) but ARG0 for ???
(to).
We thinkthese samples provide evidence for our joint mod-el while these information has not been capturedby the base model.In example (3), the argument is ??
? ???
?
? '
???
(a large group with highreputation) and shared by predicates ??U?
(de-velop) and ?:?
(become).
Different from theabove cases in which only one label is wrong inthe base model, both labels for ??U?
(develop)and ?:?
(become) are misclassified by the basemodel.
However, our method still gets correct an-swers for both predicates, which also benefits fromthe global features.7 Related workOur work is related to semantic role labelingand discriminative reranking.
In this section, webriefly review these two types of work.On Semantic Role LabelingGildea and Jurafsky (2002) first presented a sys-tem based on a statistical classifier which is trainedon a hand-annotated corpora FrameNet.
In theirpioneering work, they used a gold or autoparsedsyntax tree as the input and then extracted vari-ous lexical and syntactic features to identify the370Examples Base Ours1.
(n?]tI?Y??
?
-ILY?_?01?U?!
?i'2(At the same time of compulsory education,secondary vocational education have achieved1significant development and constant expanding2)ARGM-LOC |?0ARGM-TMP |i'ARGM-TMP |?0ARGM-TMP |i'2.
w??
::i?1:2 ?E*z/(Hailar Airport had been expanded1and became2an international airport)ARG0 |i?ARG0 |:ARG1 |i?ARG0 |:3.
w??? .
6e?
?4mA?C,?U1:2 ???????'??
(Haier Group?s sales revenue has exceeded sixbillion yuan and it has developed1to be2alarge group with high reputation)ARG1 |?UARG0 |:ARG3 |?UARG1 |:Table 9: Some examples in our experimentssemantic roles for a given predicate.
After Gildeaand Jurafsky (2002), there have been a large num-ber of works on automatic semantic role label-ing.
Based on a basic discriminative model, Pun-yakanok et al.
(2004) constructed an integer linearprogramming architecture, in which the dependen-cy relations among arguments are implied in theconstraint conditions.
Toutanova et al.
(2008) pro-posed a joint model to explore relations of all ar-guments of the same predicate.
Unlike them, thispaper focus on mining relations of different pred-icates?
semantic roles in one sentence.
And, therehave been many extensions in machine learningmodels (Moschitti et al., 2008), feature engineer-ing (Xue and Palmer, 2004), and inference pro-cedures (Toutanova et al., 2005; Punyakanok etal., 2008; Zhuang and Zong, 2010a; Zhuang andZong, 2010b).Sun and Jurafsky (2004) did the preliminarywork on Chinese SRL without employing anylarge semantically annotated corpus of Chinese.They just labeled the predicate-argument struc-tures of ten specified verbs to a small collectionof Chinese sentences, and utilized Support Vec-tor Machine to identify and classify the arguments.They made the first attempt on Chinese SRL andproduced promising results.
After the PropBank(Xue and Palmer, 2003) was built, Xue and Palmer(2004) and Xue (2008) took a critical look at fea-tures of argument detection and argument classi-fication.
Unlike others?
using syntax trees as theinput of SRL, Sun et al.
(2009) performed Chi-nese semantic role labeling with shallow parsing.Li et al.
(2010) explored joint syntactic and se-mantic parsing of Chinese to further improve theperformance of both syntactic parsing and SRL.However, to the best of our knowledge, inthe literatures, there is no work related to multi-predicate semantic role labeling.On Discriminative RerankingDiscriminative reranking is a common approachin the NLP community.
Its general procedure isthat a base system first generates n-best candidatesand with the help of global features, we obtainbetter performance through reranking the n-bestcandidates.
It has been shown to be effective forvarious natural language processing tasks,such assyntactic parsing (Collins, 2000; Collins, 2002b;Collins and Koo, 2005; Charniak and Johnson,2005; Huang, 2008), semantic parsing (Lu et al.,2008; Ge and Mooney, 2006), part-of-speech tag-ging (Collins, 2002a), named entity recognition(Collins, 2002c), machine translation (Shen et al.,2004) and surface realization in generation (Kon-stas and Lapata, 2012).8 Conclusion and Feature WorkThis paper investigates the interaction effect a-mong multi-predicate?s SRL.
Our investigationhas shown that there is much interaction effec-t of multi-predicate?s SRL both in Argument Iden-tification and in Argument Classification.
In thestage of argument identification, we proposed nov-el features related to predicates and successfullyremoved many argument identification errors.
Inthe stage of argument classification, we concen-trate on the classification of the arguments sharedby multiple predicates.
Experiments have shown371that the base model often fails in classifying theshared arguments.
To perform the classification ofthe shared arguments, we propose a joint modeland with the help of the global features, our join-t model yields better performance than the basemodel.
To the best of our knowledge, this is thefirst work of investigating the interaction effect ofmulti-predicate?s SRL.In the future, we will explore more effective fea-tures for multi-predicate?s identification and clas-sification.
Since we adopt reranking approach inthe shared arguments?
classification, the perfor-mance is limited by n-best list.
Also, we wouldlike to explore whether there is another method toresolve the problem.AcknowledgmentsWe thank the three anonymous reviewers for theirhelpful comments and suggestions.
We would al-so like to thank Nianwen Xue for help in base-line system, and Tao Zhuang, Feifei Zhai, Lu Xi-ang and Junjie Li for insightful discussions.
Theresearch work has been partially funded by theNatural Science Foundation of China under GrantNo.61333018 and the Hi-Tech Research and De-velopment Program (?863?
Program) of China un-der Grant No.2012AA011101, and also the HighNew Technology Research and Development Pro-gram of Xinjiang Uyghur Autonomous Region un-der Grant No.201312103 as well.ReferencesCollin F. Baker, Charles j. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of COLING-ACL 1998.Janara Christensen, Mausam, Stephen Soderland andOren Etzioni.
2010.
Semantic Role Labeling forOpen Information Extraction.
In Proceedings of A-CL 2010.Eugene Charniak and Mark Johnson.
2005.
Coarsetofine n-best parsing and maxent discriminative r-eranking.
In Proceedings of ACL 1998.Michael Collins and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
Computa-tional Linguistics, 31(1):2569.Michael Collins.
2000.
Discriminative reranking for-natural language parsing.
In Proceedings of ICML2000.Michael Collins.
2002a.
Discriminative training meth-ods for hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP 2002.Michael Collins.
2002b.
New ranking algorithms forparsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In Proceedings ofACL 2002.Michael Collins.
2002c.
Ranking algorithms fornamed-entity extraction: Boosting and the voted perceptron.In Proceedings of ACL 2002.Ruifang Ge and Raymond J. Mooney.
2006.
Discrimi-native reranking for semantic parsing.
In Proceed-ings of COLING/ACL 2002.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling for semantic roles.
In Computational Lin-guistics, 28(3): 245-288.Daniel Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
InACL 2002.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL 2008.Sung Dong Kim, Byoung-Tak Zhang and Yung TaekKim.
2000.
Reducing Parsing Complexity by In-traSentence Segmentation based on Maximum En-tropy Model.
In Proceedings of SIGDAT 2000.Paul Kingsbury and Martha Palmer.
2002.
From Tree-bank to PropBank.
In Proceedings of LREC 2002.Junhui Li, Guodong Zhou and Hwee Tou Ng.
2010.Joint Syntactic and Semantic Parsing of Chinese.
InProceedings of ACL 2010.Ding Liu and Daniel Gildea.
2010.
Semantic role fea-tures for machine translation.
In Proceedings ofCOLING 2010.Junhui Li, Guodong Zhou and Hwee Tou Ng.
2010.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and LukeS.
Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProceedings of EMNLP 2008.Alessandro Moschitti, Daniel Pighin and RobertoBasili.
2008.
Tree Kernels for Semantic Role Label-ing.
In Computational Linguistics, 34(2): 193-224.Srini Narayanan and Sanda Harabagiu.
2004.
QuestionAnswering based on Semantic Structures.
In Pro-ceedings of COLING 2004.Vasin Punyakanok, Dan Roth, Wen-tau Yih and DavZimak.
2004.
Semantic Role Labeling via IntegerLinear Programming Inference.
In Proceedings ofCOLING 2004.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.Discriminative reranking for machine translation.
InProceedings of HLT/NAACL 2004.372Mihai Surdeanu, Sanda Harabagiu, John Williamsand Paul Aarseth.
2003.
Using Predicate-ArgumentStructures for Information Extraction.
In Proceed-ings of ACL 2003.Mihai Surdeanu and Jordi Turmo.
2005.
Semantic RoleLabeling Using Complete Syntactic Analysis.
InProceedings of CONLL 2005.Weiwei Sun, Zhifang Sui, Meng Wang and Xin Wang.2009.
Chinese Semantic Role Labeling with ShallowParsing.
In Proceedings of ACL 2009.Ioannis Konstas and Mirella Lapata.
2012.
Concept-totext generation via discriminative reranking.
InProceedings of ACL 2012.Kristina Toutanova, Aria Haghighi and ChristopherD.Manning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of ACL 2005.Kristina Toutanova, Aria Haghighi and ChristopherD.Manning.
2008.
A Global Joint Model for Seman-tic Role Labeling.
In Computational Linguistics,34(2): 161-191.Dekai Wu and Pascale Fung.
2009.
Can semantic rolelabeling improve smt.
In Proceedings of EAMT2009.Deyi Xiong, Min Zhang and Haizhou Li.
2012.
Model-ing the Translation of Predicate-Argument Structurefor SMT.
In Proceedings of ACL 2012.Nianwen Xue and Martha Palmer.
2003.
Annotatingthe Propositions in the Penn Chinese Treebank.
InProceedings of the 2nd SIGHAN Workshop on Chi-nese Language Processing..Nianwen Xue and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In Proceed-ings of EMNLP 2004.Nianwen Xue.
2008.
Labeling Chinese Predicates withSemantic Roles.
In Computational Linguistics,34(2): 225-255.Feifei Zhai, Jiajun Zhang, Yu Zhou and ChengqingZong.
2012.
Machine Translation by ModelingPredicate-Argument Structure Transformation.
InProceedings of COLING 2012.Tao Zhuang and Chengqing Zong.
2010a.
A MinimumError Weighting Combination Strategy for ChineseSemantic Role Labeling.
In Proceedings of COL-ING 2010.Tao Zhuang and Chengqing Zong.
2010b.
Joint Infer-ence for Bilingual Semantic Role Labeling.
In Pro-ceedings of EMNLP 2010.373
