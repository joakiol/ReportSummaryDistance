A Proposal for Task-based Evaluation of Text Summarization SystemsTh~rtse Firmin HandDepartment of Defense9800 Savage RdFt Meade, MD 20755-6000, USAt f imlnOromulus,  ncsc ?
milAbstractEvaluauon is a key part of any researchand development effort, but the goals andfocus of evaluat:ons are often narrow mscope, addressing a specific algonthm ortechnique, or analyzing a single resultAll of the evaluation work clone to date ontext summarization systems has been bythe developers of mdlvldual systems, usu-ally to study and improve sentence selec-tion cntena Under TIPSTER III, DARPA~s sponsoring a task-based evaluauon ofmultiple text summarization systemsThis focus of this evaluation wall be onuser needs, and the feaslbdlty of applyingsummarization technology to a variety oftasks1 IntroductionThe explosion of on-lme textual matenal and theadvances m text.processing technology have pro-wded an important opportunity for broad apphca-tJon of text summanzauon systems Numeroustechmques for denwng summaries from full textdocuments have already been implemented, andthere are several commercial summarization prod-ucts available The summaries generated by thesesystems are potenually useful m a variety of set-tmgs In 1997, the US Government wall begin aDefense Advanced Research Projects Agency(DARPA)-sponsored program under the TIPSTERumbrella to evaluate full text summanzaruon sys--terns to prowde feedback to researchers and com-mercial msututtons on the utdtty of variousapproaches to spec~ffic summanzauon tasks TIP-STER, discussed m more detml ater, is a DARPAlmtlattve wxth participation from multiple USgovernment agencies.and research and commer-cial msmut~ons to pushthe stareof the art m textprocessing technologies2 Concepts of  Text SummarizationAutomatic summaries are usually descnbed mterms of certain key features which relate to theconcepts of intent, focus, and Coverage?
Intent describes the potential use of the sum-mary, either mdlcattve or reformative Indlca-ttve summaries, used m this context, providejust enough mformatlon to judge the relevancyof the full text Informattve or substantttve sum-manes erve as substztutes for the full docu-ments, retaining all tmportant detads?
Focus refers to the scope of the summary,either generic or user-directed A generic sum-mary Is based on the mmn concept(s) of a doc-ument, whereas a user- or goal-directedsummary Is based on the topic of interest indi-cated by the recipient of the summary?
Coverage tn&cates whether the summary isbased on a single document or multiple docu-" mentsMuch of the historical work m automatic textsummarization has been geared towards the cre-ation of indicative, generic summaries of singledocuments For example, the work of Luhn(1958), Edmundson (1969), Johnson et al(1993)and Brandow et al(1995) all generated this typeof summary, although their approaches haveincluded different combmauons of staustJcal andhngmsttc techmques Luhn (1958) considered fre-quency of word occurrence within a document31and the posmon of the word m a sentence,Edmundson (1969) looked at cue words, taOe andbeading words, and structural indicators,Johnsonet al(1993) used md~tbr  phrases, andBrandow et al(1995) apphed sentence welghungusing signature word selection Most of theseapproaches claim some degree of domain inde-pendence, however they have been tested only ona specific type of data, such as newspaper a ucles(Brandow et al1995) or techmcal hterature(Edmundson 1969)More recently, the scope of research hasexpanded to include reformative, user-directed,and multi-document summaries Reamer andHahn (1988), Maybury (1993), and McKeownand Radev (1995) used knowledge-basedapproaches to generate mformauve surmnanesthat can serve as subsututes for the original docu-mentThe expansion m focus to include user-dtrectedsummaries has been influenced by research mreformation retrieval cormnumty on passage-based retrieval, as m the work of KhanS et al(1996) Also, advances m stalasUcal learning algo-rithms, such as those maplemented by Kup~ee tal (1995) and Aone et al(1997) have combinedgeneric surnmartes and user-customtzatlon, allow-mg the userto affect he content of the summariesby mampulaung sentence extractaon featuresThe potenual for multt-document summariza-tion as proposed by the work of Strzalkowska(1996) and Mare and Bloedom (1997) is based mpart on advances m mformauon retrteval andmformauon extracuon performance3 Previous EvaluationsDuring the course of their development, most ofthe above systems were subject o some form ofevaluation Many of these evalualaons rehed onthe presence of a human-generated target abstract,or the notion of a single 'best' abstract, althoughthere is fairly uniform acceptance of the behefthat any number of acceptable abstracts couldeffectwely represent the content of a single docu-ment Human-generated abstracts attempt to cap-ture the central concept(s) of a document usingthe terminology of the document, along the linesof a generic summary The comparisons madebetween the human-generated versus machme-generated summaries were mtended pnmanly forthe developers' own benefit, and evaluate the tech-nology ~tself, rather than the utahty of the technol-ogy for a given task .Other evaluauons did focuson specific tasks and potenual uses of automatacsummaries, but only w~th respect to a single sys-?
tern and a hrmted ocument setMany dttferent techmques were attempted mthe area of mtrmslc or developer-oriented evalua-uons, wluch judge the quahty of summariesEdmundson (1969) compared sentence selectionm the automauc abstracts to the target abstracts,and also performed a subjective valuation of thecontent Johnson et al(1993) proposed matchinga template of manually generated key conceptswith the concepts included m the abstract, andperformed one sample abstract evaluation Pa~ceand Jones (1993) used a set of statlslacs to deter-anne ff the summary effeclavely captured the focalconcepts, the non-focal concepts, and conclu-sions Using a smctly statastlcal measure, Kuplecet al(i995) calculated the percentage of sentencematches and parual matches between thetr auto-matlc summary and a manually generatedabstract The mare problem with this type of eval-uation is ~ts rehance on the nouon of a single 'cor-rect' abstract Smce many differentrepresentations of a document can form an effec-ave summary, this is an inappropriate measureIn extrinsic or task-oriented evaluations, themformat~on retrieval notlon of relevancy of a doc-ument o a specific topic ~s the common measurefor summarization testing Make et al(1994)analyzed key sentence coverage and'also recordedt~rmng and precision/recall stattstles to make rele-vance decisions based on summaries for adomain-specific summarizer Brandow et al(1995) had news analysts compare the summariesgenerated using stat~stteal nd natural anguageprocessing (NLP) techniques to suramanes usingthe mmal sentences (called the "lead summaries")of the document Brandow et al(1995) discov-ered that m general, expenenced news analystsfelt that the lead summaries were more acceptablethan the summartes created using sophisticatedNLP teehmques Mare and Bloedom (1997) gen-erated smnlar precision/recall and tmung mea-sures for an mformalaon retrieval experimentusmg a graph search and matching techmque and32IIIIIIIIIIIIIIIIIIIEvaluation QuanfitabveDecision TaskCatcgonzaUonAdhocTABLE 1.
ProIntentInchcauveIndlcaUveFocusGenencUser-dn'ectedCoverageSmgledocumentSingledocumentappropriatecategoryrelevant totopicmeasuresUmeaccuracyumoaccuracy)osed Evaluationlearned that then" summaries were effecuveenough to support accurate retrieval4 Proposed EvaluationFull text summanzauon as a major task an TIP-STER Phase HI TIPSTER Phase I sponsoredresearch an reformation extracUon and reforma-tion retrieval, and supported the Message Under-standing Conferences (MUC).
and Text REtrieval.Conferences (TREC) for evaluating extractionand retrieval performance, respectively (Mer-chant, 1993) TIPSTER Phase II concentrated ondefining a common architecture to facdnate mte-gratxon of the two technologaes TIPSTER PhaseHI conUnues to advance research m extraction andretneval, and adds text summarization i  both theresearch and formal evaluation arenas (Merchant,1996) Thas propose d evaluation wdl be a formal,large scale, multiple task, multiple system evalua-tion independent from any single approach ormethodologyAs outlined m Table 1, the proposed evaluationfor text summarization will be task-based, judgingthe utdny of a summary to a particular task It wallbe an evaluation for users, determining fitness fora pamcular purpose, versus an evaluation stnctlyfor developers It is not intended to pick the bestsystems, but to understand some of the ~ssuesrevolved an budding summanzauon systems andevaluating them It wall provade an enwronmentwhereby systems wdl be judged independently ontheir apphcablhty o a given taskWe will began with at least wo tasks for the firstevaluatton, following the MUC and TREC exam-ples of testing along multiple &mensions Wehope thin will avoad any re&rection of researchefforts based on relative performance on anygaven taskAd&Uonal tasks will be added m subsequentyears to evaluate other aspects of text summariesThese tasks will also reflect continued maturationof the technology4.1 GoalsAutomauc text summarization systems lend them-selves to many tasks An mformaUve summarymay be used as the basis for execuUve decisionsAn mdlcauve summary may be used as an mmalln&cator of relevance prior to reviewing the full.
text of a document (and possibly ehmmatlng theneed to view that full text) Summaries (used mplace of full text documents) may also be used to~mprove precision m mformatton retrieval sys-tems, since users would be searching only thecontent-relevant words or phrases gathm a docu-ment (Brandow et al 1995) Forthis mlual evalu-ation, we wdl concentrate on tasks that appear tooffer the possablhty of near term payoff or usersWe attempted to devise tasks that model the realworld activmes of reformation analysts and con-sumers of large quantities of text These taskswere designed based on anterwews with userswho spend a majority of their workday searchingthrough volumes of on-hne text for reformationrelevant to thear area of interestWe will begm with tasks that address the focus(genenc or user-darected) of the summaries Thefirst task, categortzation, wdl evaluate genericsummaries, and the other, adhoc retrieval, wall33evaluate user-&rected summaries, as describedbelow4.1.1 Task 1 - CategorizationWhale mformauon routang systems are beeormngprevalent m many work enwronments, there Isstall a role m many such places for a centralrewew authority to scan and &stnbute all incom-ing documents based on their content, essentmllyperforrmng a manual routing task These rewew-' ers deal bothwith abroad topic base and with datafrom mulUple sources They must browse a docu-ment qmckly to determine the key concepts, andforward that document to the appropnate mdwzd-ualA related task revolves canmng a large set ofdocuments that has beenselected using anextremely broad m&cator or concept A user willbrowse through this data and categorize ~t accord-mg to various parameters For example; on theWorld-Wide-Web (WWW), mformataon seekersfrequently enter short, broad queries that return?
hundreds or even thousands of documents Theuser must determine which documents representthe greatest potentaal for prowdmg mformataon fmterestIntegrating text summanzataon into each of theabove scenarios, the user would be presented ageneric summary m heu of the full text, fromwhich he or she wall make a categonzataon deci-sionThe evaluataon task wall simulate the manualrouting scenario described above The goal wdl beto decide qmckly whether or not a document con-tmns mforrnataon about any of a hrmted numberof topic areas The document wall be hrmted to asingle topicSelectaons from the TREC test collectaons ofquery topics and documents wall be used as thedata for the evaluauon we roll select a mtmmumof five d~stlnct topics, approxlmately 200 docu-ments per topic At least wo of the topics wdl beentaty-based 0 e based on the MUC categories ofperson, Iocataon, and organ~zataon) The toplcswdl be related at a very broad level The docu-ment set prowded will be that returned as a resultof five simple queries to a commonly used mfor-mataon retrieval system, wluch should provade anadequate m~x of shorter and longer documentsThe resultmg documents wall be randomly mixedThe TREC test collectaons are described m detmlm Harman (1993)Only the documents will be provided to theevaluauon partac~pants Summanzauon systemsdeveloped by the parttclpants wall autornaucallygenerate a generic summary of each documentThere wall not be any constraints on the format ofthe summary All summaries ubmated by thepartactpants wdl be combined by the evaluataon.
organizers into a single group and randomlymtxedThe full text of the document and the lead sen-tences of the document (up t.o the specified cutofflength) wall be used as baselines The summariesprowded by the paraclpants, the basehne leadsummaries, and the full text documents wdl bem~xed together, esulting m N+2 versions of a sin-gle document, where N is the number of evalua-taon paruc~pants This document set .wdl berandomly &wded among the assessors Assessorsfor the evaluataon wdl be professional reformationanalysts Each assessor wall read a summary ordocument and categorize it into one of the fivetopic areas that were selected by the organmers, or'none of the above', which can be considered asixth category No assessor will read more thanone version (summary or full-text) of a single doc-ument The assessor's decision-making processwdl be tamed The assessor wdl then move on tothe next document of summaryIn addmon to the TREC relevance judgments, aminimum of two addmonal ssessors wall read allof the full text documents to estabhsh a groundtruth relevance decision for eachThe assessors wdl be tamed, and their categon-zataon decisions wall be compared to the groundtruth assessments This methodology wall assurethat the assessors' own categonzataon perfor-mance can be measured along with the perfor-~mance of the summanzataon systems4.1.2 Task 2 - Adhoc retrievalBoth the volume of data avadable on-line and theprevalence of mformataon retaaeval engines havecreated an ~mmechate apphcataon for Implement-hag a text summanzataon filter as a back end to anmformatton retrieval engine, whereby the usercould qmckly and accurately judge the relevancy34IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIof documents returned as a result of a query Theuser's query has threet bearing on the content ofthe documents returnedApplying text surnmanzatlon to the above sce-nario, the user would be presented a summarybased on  the query (a user-dlrected summary),instead of the full text, from wtuch he or she wdlmake a relevance assessmentThe second evaluatmn task wall simulate theadhoc retrieval scenario descnbed above Thegoal wdl be to decide the relevancy of a retrieveddocument by looking only at the user-directedsummary that has been generated by the systemunder evaluationThe TREe collection will also provide the com-mon test data used for fiats task m the same pro-.porUons as for the categorization tasks, five hand-selected topics and approxmaately 200 documentsfor each topic The document set prowded wdl bethat returned as a result- of five queries to a com-monly used mformauon retrieval system In tinscase, both the topics and documents wdl be pro-v~ded to the pamclpants Summanzanon systemsdeveloped by the parucipants wdl then autornati-tally generate a summary using the topic as themdlcatlon of user interest The full text and a key-word-m-context (KWIC) hst wall be used as base-linesAssessors wall work wRh one topic at a ttmeAll summaries received from the participants for agiven topic, along with the full text and the KWICsummaries will be combined rote a single grou p,randomly mtxed, and divided among the asses-sors Each assessor will review atoplc, then readeach summary or document and judge whether ornot it is relevant to the topic at hand The assessorwdl then move on to the next topic No assessorwall read more that one representaUon f a smgledocumentIn addmon to the TREC relevance judgments, aminimum of two addmonal assessors ,roll read allof the full text documents to estabhsh a groundtruth relevance deczsmn4.2 Evaluation CriteriaBoth evaluations lughhght he acceptabdzty of asummary for a gwen task, wRh the assumptionthat there ~s not a single 'correct' summary Themare purpose wdl be to deterrnme ff the evaluatorwould make the same decision ff gwen the fulltext, and how much longer it would take to makethat declsmn The ~deal outcome would be that thedeclsmn_could be made with the same accuracy mshorter urne, given the document summary Foreach task, we wdl record the time reqmred tomake each decision, and the actual decision Thedeclsmn for each evaluator wdl then be comparedto the relevance decision for the basehnes Analy-sis of the results wdl include consideration of theeffects of summary length on the Ume taken tomake the relevance declsmn as well as Its effectson decision accuracyQuanutative measures?
CategonzauonfRelevance DecisionsDetermining relevance to a given topic Is an inher-ently subjecttve actwlty We intend to waUgatethis by using a sound statis~cal model to deter-mine the appropriate number of summaries toevaluate, and by structuring the evaluation m sucha way as to avoid bias of any single assessor Aspreviously discussed, we wdl esmbhsh low-endand high-end basdmes and use multiple assessorsto create ground truth declsmns?
Ttme ReqmredThe time reqmred to make a relevance or categori-zation decision using a summary will be recordedand compared with the time reqmred to make thesame decision using the full text?
Summary LengthIn prevmus stu&es, 20-30% of full documentlength was often used as optnnal cutoff length forreformative summaries, wlth the supposmon thatm&cattvo summaries would reqmre far less refor-mation ((Brandow et al 1995) and (Kuplec et al1995)) For the tmual evaluatlon, whlch wdl usem&catwe summaries only, a document cutofflength wdl be estabhshed at 10% of the originaldocument length Any summary exceeding thatmargin wall be truncatedQuahtative measures?
User PreferenceEvaluators wdl be asked to indicate whether theyprefer the full text or the summary as a basis fordeclslon-malang In addmon to this quahtatwe35.
.
.
.
; i 'i Evaluation I QnantitatrveIntent Focus Coverage Goa l  I measuresIndlcaUve Genenc Smgle Improve IR Precmondocument.TaskIndexsummaries forI mformaUoni remeval ?SummanzeacrossdocumentsExe~tlUvedecisionmalongInchcatwe orInformauveGeneric orUser-directedMulupledocumentpreclslonReducemformauonprocessingloadRecallAccuracyInformattve G~nerlc orUser-&rectedSlrlgle ormultt-documentInclude allrelevantmformaUonKey Concept"matchmg?
FormattedquesUo, nsTABLE 2.
Future EvaluaUonsassessment, the evaluator will be encouraged toprovide feedback as to why the summary was orwas not acceptable for a given task This feedbackwill then he made avmlable for system developersIt could also provide a basis for subsequent evalu-attons5 Future Direction of  EvaluationThls mmal evaluauon will address only a hn~tednumber of issues mvolwng automaUc text sum-manzauon technology As we gmn more experi-ence working wlth these systems and integratingthem mto a user's work flow, the scope of theevaluauons wdl necessarily grow and changeSome addtuonal features and tasks to beaddressed petenually m future evaluations havealready been identified, including cohesiveness ofa summary, optunal ength of a summary, andmultl-document summaries Selected tasks areoutlined m Table 2 and described briefly below5.1 Tasks and MeasuresWe are addr'essmg two mformaUon retrieval typesof tasks dunng the first evaluatton, however,potenttal apphcatxons go beyond this hrmtedscope One of the frequently mentloned uses of atext summary is as a substttute for the documentdunng the indexing process of an mfonnmonretrieval system The nouon is that mdeyang basedon summaries would result m more results retriev-als because only the key concepts and content-beanng words would have been indexed Thisidea could be evaluated using standard precmmnand recall mforraauon retrieval measuresSummarizing across muluple documents manother extremely useful apphcatton Whde sin-gle document summaries are expected to prowdeimproved efficlency for the end-user, much of themformanon rewewed from one summary to thenext will be redundant Automatically generatedsummaries could result m even larger efficiencygains and productivity Lmprovements bydls~hngthe mformauon from muluple documents rote asingle summary An evaluauon of tius type ofsummary would be much more complex, posstblycomparing at a phrase-matching or key conceptlevel the combined factual mforma~on i cludedm a single summary with manually ldenufied keymformauon m mdwldual documents The evalua-Uon would verify that the relevant aspects of keyfacts across documents have been successfullyIdentified and combmed mthe resulting summary?
A thlrd apphcat~on could focus on a decmon-malong task based on an mformauve summaryAn evaluaUon of thls type of summary couldinclude filling out a template mdlcatmg key con-cepts m a document, slrmlar to the Pmce andJones (1993) and Johnson et al(1993) evalua-uons, possibly augmented by a quesuon/answermeasure based on the full text and the summary365.2 DataNewspaper arucles, such as those which wall beused for the first evaluation, represent only a smallpornon of the type of information avmlable on-line A useful, effective surnrnanzer should beable to accept ext m a variety of formats Waheach subsequent evaluataon, ew sources of datawill be added These new sources could be newsfeeds or web pages They will tend to be less for-matted, vary greatly m length, and cover multipletopics At some point, we hope to introduce docu-.ments m languages other than Enghsh for summa-nzatlon either into then" native language or intoEnghsh6 AcknowledgmentsThe author is grateful to Donna Harman and BethSundhelm for then" support and assmance mdesigning the evaluauonThe views expressed m this paper are those ofthe author and do not necessarily reflect he views?
of the Department ofDefense or any of its agen-ciesReferencesChmatsu Aone, Mary Ellen Okurowska, JamesGorhnsky, and Bjomar Larsen 1997 A Scal-able Summarization System Usmg Robust NLPIn Proceedings ofACL-97, Madrid, Spmn, JulyTo appear "Ronald Brandow, Karl Mltze, and Lisa F Rau1995 Automatic Condensation of ElectromcPubhcations by Sentence Selection InformationProcessmg and Management, 31 (5) 675-685Kenneth W Church and Lisa F Ran 1995 Com-mercial Apphcations of Natural Language Pro-cessmg Commumcattons f the ACM, 38(11)71-79H P Edmundson 1969 New Methods m Auto-mattc Abstracting Journal of the ACM, 16(2)264-285Bngette Endres-Nlggemeyer, Jerry Hobbs, andKaren Sparck Jones 1993 Summarizing Textfor Intelhgent Commumcation In DagstuhlSeminar Report, IBFI GmbH, Schloss Dagstuhl,Wadem, GermanyJ R GaUlers and Karen Sparck Jones.
i993Evaluating Natural Language Pr~essmg Sys-tems Umverstty ofCambridge Computer Labo-ratory TechmcalReport No 291, ComputerLaboratory, University of CambridgeDonna Harman 1993 Overview of the First TextREtrieval Conference CI"REC-1) In TREC-2Proceedings, Gmthersburg, MarylandDonna Harman 1996 Overview of the FourthText REtrieval Conference (TREC-4) In TheFourth Text REtrieval Conference (TREC.4),pages 1-24, Gaithersburg, Maryland, 1995F C Johnson, C D Patce, W J Black, and A PNeal 1993 The appllcaUon of hngtustac pro-cessmg to automatic abstract generaUon Jour-nal of Document and Text Management, 1(3)215-241 .Daniel Knaus, Elke Mtttendorf, Peter Schauble,and P~Lrmc Sheridan i996 Highlighting Rele-vant Passages for Users of the Interactave SPI-DER Retrieval System In The Fourth TextREtrieval Conference (TREC-4), pages 233-238, Gaxthersburg, Maryland, 1995Juhan Kuplec, Jan Pedersen, and Francme Chen1995 A Trmnable Document SummarizerSIGIR "95, pages 68-73, Seattle, Washington,1995H P Luhn 1958 The Automatic Creation of LR-erature Abstracts IBM Journal, pages 159-165Inderjeet Mare and Erie Bloedorn 1997 Multi-document Summarization byGraph Search andMatching In Proceedings of AAAI-97, Provi-dence Rhode Island, 1997 To appearMark T Maybury.
1993.
Automated Event Sum-manzation Techniques In Dagstuhl SeminarReport, pages 100-108, IBFI GmbH, SchlossDagstuhl, Wadem, GermanyKathleen McKeown and Dragormr R Radev1995 Generating Summaries of Multiple NewsArticles SIGIR "95. pages 74-82, Seattle, Wash-region37Roberta Merchant 1993 Tipster Program Over-view In Tzpster Text Program, pages 1-2, Fred-encksburg, Virginia ~Roberta Merchant 1996 TIPSTER Phase III InTIPSTER Text Phase II1 Ktckoff Workshop,Columbia, Maryland, OctoberAndrew H Morns, George M Kasper, and Den-ms A Adams 1992 The Effects and Lwmta-uons of Autoraated Text Condensing onReading Comprehension Performance Infor-mauon Systems Research 3 1, pages 17-35Selj1 Muke, Etsuo Itoh,.
Kenjl Ono, and KazuoSurmta 1994 A Full-Text Retrieval Systemwith a Dynarmc Abstract Generauon FuncUonSIGIR '94, pages 152-161, Seattle, WashingtonC D Palce 1990 Constracung .LiteratureAbstracts by Computer Techmques and Pros-pects Informatwn Processmg and Manage-ment, 26(1) 171-186Chris D Pmce and Paul A Jones 1993 TheIdenuficatlon of Important Concepts m I-hghlyStructured Teehmcal Papers SIGIR '93, pages69-77G J Rath, A Restock, and T R Savage 1961The FormaUon of Abstract by the Seleclaon ofSentences American Documentatwn, pages139-143U Relraer and U Hahn 1988 Text Condensationas a Knowledge Base Abstractton IEEE Con-ference on AI Apphcatwns, pages 338-344Tomek Strzalkowski 1996 Robust Natural Lan-guage Processmg and User-Graded ConceptDiscovery for Information Retrieval, Extraction,and Surnmanzatlon Tipster Phase III In T/P-STER Text Phase III Ktckoff Workshop, Colum-bin, Maryland, OctoberBeth Sundhesm 1995 Overview of Results of theMUC-6 Evaluauon In Sooth Message Under-standmg Conference (MUC-6), pages 13-31,Columbm, MarylandSarah Taylor 1996 TIPSTER Text ProgramOverwew In TIPSTER Text Phase 11, TysonsCorner, V~rgmm38IIIIIIIIIIIIIIIIIII
