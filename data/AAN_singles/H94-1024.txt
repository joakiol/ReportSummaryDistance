Evaluation in the ARPA Machine Translation Program:John S. White, Theresa A. O'ConnellPRC Inc.McLean, VA 221021993 MethodologyABSTRACTIn the second year of evaluations of the ARPA HLT MachineTranslation (MT) Initiative, methodologies developed andtested in 1992 were applied to the 1993 MT test runs.
Thecurrent methodology optimizes the inherently subjectivejudgments on translation accuracy and quality by channelingthe judgments of non-translators into many data pointswhich reflect both the comparison of the performance of theresearch MT systems with production MT systems andagainst the performance of novice translators.
This paperdiscusses the three evaluation methods used in the 1993evaluation, the results of the evaluations, and preliminarycharacterizations of the Winter 1994 evaluation, nowunderway.
The efforts under discussion focus on measuringthe.progress of core MT technology and increasing thesensitivity and portability of MT evaluation methodology.1.
INTRODUCTION.Evaluation of Machine Translation (MT) has proven tobe a particularly difficult challenge over the course of itshistory.
As has been noted elsewhere (White et al,1993), assessment of how well an expression in onelanguage isconveyed inanother is loaded with subjectivejudgments, even when the expressions are translated byprofessional translators.
Among these judgments are theextent to which the information was conveyed accurately,and the extent o which the information conveyed wasfluently expressed in the target language.
The inherentsubjectivity has been noted, and attempts have been madein MT evaluation to use such judgments to bestqualitative advantage ( .g., van Slype 1979).
The meansof capturing judgments into quantifiably usefulcomparisons among systems have led to legitimateconstraints on the range of the evaluation, such as to thescope of the intended end-use (Church and Hovy 1991), orto the effectiveness ofthe linguistic model (Jordan et al1992, Nomura 1992, Gamback et al 1991).The ARPA MT Initiative ncompasses radically differentapproaches, potential end-uses, and languages.Consequently, the evaluation methodologies developedfor it must capture quantifiable judgments fromsubjectivity, while being relatively unconstrainedotherwise.
This paper presents the 1993 methodologies,and the results of the 1993 MT evaluation.
We furtherdiscuss the preliminary status of an evaluation owunderway that greatly increases the participation of theentire MT community, while refining the sensitivity andportability of the evaluation techniques.2.
MT  EVALUATION IN THE ARPAMT IN IT IAT IVEThe mission of the ARPA MT initiative is "to makerevolutionary advances in machine translationtechnology" (Doddington, personal communication).
Thefocus of the investigation is the "core MT technology.
"This focus tends, ultimately, away from the tools of MTand toward the (fully automatic) central engines.
It iswell understood that practical MT will always use toolsby which humans interact with the algorithms in thetranslation process.
However, the ARPA aim is toconcentrate on fully-automatic (FA) output in order toassess the viability of radical new approaches.The May-August 1993 evaluation was the second in thecontinuing series, along with dry runs and pre-tests ofparticular evaluation methods.
In 1992, evaluationmethods were built on human testing models.
Onemethod employed the same criteria used in the U.S.government to determine the competence of humantranslators.
The other method was an "SAT"-typeevaluation for determining the comprehensibility ofEnglish texts translated manually into the test sourcelanguages and then back into English.
The methods havebeen replaced by methods which maintain familiarity interms of human testing, but which are both moresensitive and more portable to other settings and systems.The Fluency, Adequacy, and Comprehension evaluationsdeveloped for the 1993 evaluation are described below;system outputs from 1992 were subjected to 1993methods, which determined their enhanced sensitivity(White et al, op.
cit.
).The 1993 evaluation included output from the threeresearch systems, five production systems, andtranslations from novice translators.
Professionaltranslators produced reference translations, by whichoutputs were compared in tile Adequacy evaluation, andwhich were used as controls in the Comprehensionevaluation.The research systems were:?
CANDIDE (IBM Research: French - English(FE)),produced both FA and human-assisted (HA) outputs.135Candide uses a statistics-based, language modelingMT technique.?
PANGLOSS (Carnegie Mellon, New Mexico State,University of Southern California: Spanish - English(SE)), produced three output ypes: fully automaticpre-processing, interactive pre-processing, and post-edited (PE).
Both pre-processing operations aremapped into one version (XP) for evaluationpurposes, though the difference in performancebetween the operational types was measured.
ThePangloss system uses both knowledge-based andlinguistic techniques.?
LINGSTAT (Dragon Systems: Japanese - English(JE)), performed in human-assisted mode.
Lingstatis a hybrid MT system, combining statistical andlinguistic techniques.To provide comparison against state of the art FAMT,production systems ran in fully automatic mode.
Thesesystems are in current commercial use and developed overa wide range of subject areas.
SPANAM, from the PANAMERICAN HEALTH ORGANIZATION (PAHO)produced SE.
SYSTRAN, a commercial system,produced FE.
'naree unidentified systems based in Japaneach contributed JE.
Their outputs were made availableto the test and evaluation by Professor Makoto Nagao atKyoto University.Manual translations (MA) were provided by novice,usually student, translators at each of the research sites.These persons also developed the haman-assisted outputs,controlled for pre-/post-test bias.
Finally, expert manualtranslation of the same material into English wasperformed as a reference set as noted above.SYSTEM TESTSThe first phase of the ARPA MT Evaluation was tileSystem Test.
The research and production sites eachreceived a set of 22 French, Japanese or Spanish sourcetexts for translation into English.
Each set comprisedeight general news stories and 14 articles on financialmergers and acquisitions, retrieved from commercialdatabases.
The lexical domain was extended in 1993 toinclude general news texts to determine whether thetraining and development of the systems wasgeneralizable to other subject domains.
French andSpanish texts ranged between 300 and 500 words;Japanese articles between 600 and 1,000 characters.EVALUATION COMPONENTSThe evaluators were eleven highly verbal native speakersof American English.
Evaluation books were assembledaccording to a matrix based on a Latin square, designed toguarantee that each passage was evaluated once and thatno evaluator saw more than one translation version of apassage.
Because of technical problems, two of theKyoto system outputs were evaluated in a subsequentevaluation that reproduced as closely as possible theconstruct of the preceding evaluation.
The 1993 seriestested the systems with source-only text, measuring theresults with a suite of three different evaluations.All participants evaluated first for fluency, then adequacyand finally for comprehensibility.
Fluency and anadequacy components contained the same 22 texts.
Thecomprehension component included a subset of nine totwelve of these texts.
The Comprehension Evaluationwas presented to evaluators last, in order to avoid biasingthe performance of the fluency and adequacy over thepassages that appeared in the comprehension set.Fluency EvaluationThe Fluency Evaluation assessed intuitive native speakersenses about the well-formedness of the English outputon a sentence by sentence basis.
Evaluators assigned ascore from one to five with five denoting a perfectlyformed English sentence.Adequacy  Eva luat ionThe Adequacy Evaluation measured the extent o whichmeaning present in expert ranslations i present in theFAMT, HAMT, PE and MA versions.
In order to avoidbias toward any natural anguage processing approach,passages were broken down into linguistic componentscorresponding to grammatical units of varying depths,generally confined to clause level constituents between 5and 20 words in length.
Average word count within aunit was 11 for SE and FE, 12 for JE.
The averagenumber of fragments for a passage varied: 33 for FE, 41for JE, 31 for SE.
The evaluators viewed parallel texts,an expert ranslation broken into brackets on the left andthe version to be evaluated presented in paragraph formon the right.
They were instructed to ascertain themeaning present in each bracketed fragment and rate thedegree to which it was present in the right column on ascale of one to five.
IF tile meaning was absent oralmost incomprehensible, the score was one; if it wascompletely represented the score was five.Comprehension EvaluationThe Comprehension Evaluation measured the amount ofinformation that is correctly conveyed, i.e.
the degree towhich a reader can find integral information in thepassage version.
This evaluation was in the format of astandardized comprehension test.
Questions weredeveloped based on tile expert versions and then applied toall translation versions.
Evaluators were instructed tobase their answers only on information present in the136translation.
The Comprehension Evaluation is probablythe most portable valuation, as it is a common testformat for literate English speakers.RESULTS OF THE 1993 EVALUATIONThe evaluations resulted in a total of over 12,500decision points.
These are in turn represented on twoaxes: the time ratio (x-axis) and normalized quality (y-axis).
Both axes represent results as scores on a 0-1scale.
The time ratio is the ratio of the time taken toproduce a system translation compared to the time takenfor the novice MA translation.
Thus, the novice MAtranslations all appear at time value 1.
Since time takento translate isnot recorded for the FAMT systems, all ofthese are set at time 0.
The quality (that is, fluency,adequacy, or comprehension) axis is the raw score, dividedby the scoring scale (5 for fluency/adequacy, 6 forcomprehension), in turn divided by the number ofdecision points (sentences for fluency, fragments foradequacy, or questions for comprehension) i  the totalpassage set for that language pair.Common characteristics an be observed in all of theevaluation measurements taken in 1993.
First, it isevident that all of the HAMT systems performed better intime than the corresponding MA systems.
This is achange from 1992, where one system took more time tooperate than it took the same persons to translatemanually.
Each PE system also performed better inadequacy, and very slightly better in fluency, than theMA translations.
While a reasonable and desirable result,this outcome was not necessarily expected at a relativelyearly stage in the development of the research systems.Another general observation is that PE versions coredbetter in quality than non-post-edited (i.e., raw FAMT orinteractively pre-processed) versions.
This too is anexpected and desirable result.
The benchmark FAMT forFrench and Spanish (SPANAM and SYSTRAN,respectively) scored better in quality than the non-post-edited research systems, except in fluency, whereCANDIDE scored .040 higher than SYSTRAN's .540.It was expected that comprehension scores would risewith the amount of human intervention.
This provedtrue for FE.
At .896, CANDIDE HAMT scored highestfor FE comprehension; SYSTRAN (.813) scored aboveCANDIDE FAMT (.729).
PANGLOSS SE scores alsodemonstrated this trend: FA at .583, HA at .750 and PEat .833, however, the HA and PE are unexpectedly belowSPANAM (.854).
LINGSTAT HA .771 also scoredhigher than the JE FAMT: KYOTO A (.479) KYOTO B(.5625) and KYOTO C (.563).COMPARISON BETWEEN 1992 AND1993 SYSTEM PERFORMANCEFigures 1 and 2 show comparisons and trends between1992 and 1993 for the elements of data and evaluationthat are comparable.
These include the fluency andadequacy measures for all of the 1993 test output and thatportion of the 1992 data that was based on source-onlytext.
The Comprehension Evaluation was not compared,since the 1992 data involved back-translations, and thenumbers of questions per passage was different, thuscreating the potential for uncontrolled bias in thecomparison.In 1993 all systems improved both in time and in fluency/ adequacy over 1992.
The PANGLOSS system showsthe most apparent improvement in time, from 1.403 in1992 to.
691 in 1993.
LINGSTAT also shows aconsiderable improvement from .721 to .395.
AllARPA research systems howed improvement i  fluencyand adequacy over 1992 scores.
CANDIDE FAMT scoresincrease from .511 to .580 in fluency and .575 to .670 inadequacy.
PANGLOSS PE improved from .679 to .712for fluency and rose from .748 to .801 in adequacy.LINGSTAT improved from .790 in 1992 to .859 influency and went from .671 to .707 in adequacy.It should also be noted that the benchmark systems usedin both 1992 and 1993 (SYSTRAN French andSPANAM) showed improved fluency/adequacy s ores aswell.
For fluency, SYSTRAN improved from .466 to.540; for adequacy, SYSTRAN went from .686 to .743.SPANAM went from .557 to .634 for fluency and from.674 to .790 for adequacy.
It was verified that these arereflections of system improvements.1993 demonsUated a significant increase in sensitivity ofthe evaluation methodology.
Sensitivity is gauged bycomputing an F ratio, i.e., the correlation betweenindependent values.
A high F ratio indicates that therange of values is wide; the wider the range of values themore sensitive the method is.
For the FluencyEvaluation, the F ratio rose from 3.158 in 1992 to12.084 in 1993.
In the Adequacy Evaluation, the F ratiorose from 2.753 to 6.696.1994 EVALUATION IN PROGRESSThe 1994 Evaluation presently underway focuses on coreFAMT technology.
Its scope has been broadened toincrease sensitivity and portability.
In keeping with theARPA MT Initiative goal to foster development ofFAMT, input will move away from HAMT and include alarger proportion of FAMT.
To better measure theexpanded lexical capabilities of the systems underdevelopment, half of the test passages will be generalnews articles.
The Winter 1994 evaluation alone will137generate 25,000 data points to manage humansubjectivity.
This increase in data points has beenaccomplished by successfully porting the methodology toevaluation of 14 production systems in addition to thethree ARPA research systems.
To maximize therandomness of passage assignment in the evaluationmatrix, the Latin square has been replaced with a matrixordered by a random number generator.
The methodologyhas been simplified to optimize the elicitation ofintuitive judgments.
For example, the fluencycomponent which formerly measured only well-formedoess has been modified to recognize the influenceof contextual meaning.The broadened scope of the 1994 Evaluation offersbenefits, for the evaluation of the core technology for theprofoundly different systems of the ARPA MT Initiative.It also contributes to the advancement of the MTcommunity as a whole through providing a consistentportable suite of evaluation methodologies.REFERENCESI.2.3.4.5.6.Church, Kenneth, and Eduard Hovy.
1991.
"GoodApplications for Crummy Machine Translation."
InJeannette G. Neal and Sharon M. Walter (eds.
)Proceedings of the 1991 Natural LanguageProcessing Systems Evaluation Workshop.
RomeLaboratory Final Technical Report RL-TR-91-362.Gamblick, Bj~rn, Hiyan Alshawi, David Carter, andManny Rayner.
1991.
"MeasuringCompositionality in Transfer-Based MachineTranslation Systems."
in Neal and Walter (eds.
).Jordan, Pamela W., Bonnie J. Dorr, John W. BenoiL1993.
"A First-Pass Approach for EvaluatingMachine Translation Systems:" to appear in MachineTranslation.Nomura, Hirosato.
1992.
JEIDA Methodology andCriteria on Machine Translation Evaluation.
JapanElectronic Industry Development Association.van Slype, Georges.
1979.
"Critical Study ofMethods for Evaluating the Quality of MachineTranslation."
Final Report o the Commission of theEuropean Communities Directorate General Scientificand Technical Information and InformationManagement.White, J.S., T.A.
O'Connell, L. M. Carlson.
1993.
"Evaluation of Machine Translation".
{Proceedingsof the 1993 Human Language TechnologiesConference.
Morgan Kaufmann.138uJCL'50F#.
?3~ C3u~~o ~o~ o> >~ o C~o o?3 ?3o~ ,~ .
.
.
.
.
.
~ oo  ooo  ~o oo  ooo?
.
, .
:~  3~ , ~- ddo - c ;  o~ oo  o o~ ~o o '~ 0d~ ooo  |~ ~ ~ ~ ~o~,~ .
-~u~d dd  d " " u~o do  "d  "d d ~o "o ?oOc; "~ oo  ~oo~ o -u~c~ c~d " " "0 o~ ~ ~o~0t  o  i.~?rJ o o o  ~ ~1~ o o o<~ oo  ~oo ~ ~c~ " "~0~Z ~ to ~ ~Zp,.4>LMI--O.?i -a .I -0OF)P>Po>.ozUJ14,,|<"~, i  "1?i,.~ ~ o ~ ~ ~ c; " ",~,0N~!
"1:1139<DNS0 <o~(3 ?
><~ Q~ o 66  ooo~o 66 - -06-6  o oo6% =~ oo~ ~ ~ oo~o 66 006uJ?
.
.o6 66  " " "0~o o~ o o oo~Z>N~?
@g~0 O0 0 0 0&oo< - -66~ o?~ ~-o"o~ ~oO?o o  ~o.o~ ??-66?
6 J66~o 66 6 " "| oO?oO?~, .
:66~'~  0 " "0 a~ ooo6 ~6 "6oo o~ ~ eD ~ ~=~E ~~z>~ ~6W!-- =EQ.
.v-iI--Q.I--0(,1P(Wi>.0UJ aEUJ%i<x!o D~6e~~ ~ ~ oooo6 o 6 6 " 6 " " 6 6AOYnO3QV140J
