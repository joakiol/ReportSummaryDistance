A Stochast i c  Parser  Based  on a S t ructura l  Word  Pred ic t ion  Mode lShinsuke MORI, Masafumi NISHIMURA, Nobuyasu ITOH,Shiho OGINO, Hideo WATANABEIBM Research ,  Tokyo  Resem:ch Laboratory ,  IBM Japan ,  Ltd.1623-14 Sh imotsurumz~ Ym~atosh i ,  24:2-8502, Japan.mor i~t r l .
ibm.co .
jpAbstract\]in this paper, we present a stochastic languagemodel using dependency.
This model considers asentence as a word sequence and predicts each wordfrom left to right.
The history at each step of pre-diction is a sequence of partial parse krees coveringthe preceding words.
First ore: model predicts thepartial parse trees which have a dependency relationwith the next word among them and then predictsthe next word fi'om only the trees which have a de-pendency relation with the next word.
Our: model isa generative stochastic model, thus this can be usednot only as a parser but also as ~ language modelof a speech recognizer.
In our experiment, we pre-pared about 1,000 syntactically annotated Japanesesentences xtracted fl'om a financial newspaper andestimated the parameters of our model.
We built aparser based on our: model and tested it on approx-imately 10O sentences of the same newspaper.
Theaccuracy of the dependency relation was 89.9%, thehighest, accuracy level obtained by Japanese stochas-tic parsers.1 In t roduct ionThe stochastic language modeling, imported fl:omthe speech recognition area, is one of the snccessflflmethodologies of natural language processing.
Infact, all language models for speech recognition are,as far" a.s we know, based on an n-gram model andmost practical part-of-speech (POS) taggers are alsobased on a word or POS n-gram model or its exten-sion (Church, 1.988; Cutting et el., 1992; Merialdo,1994; l)ennatas and Kokkinakis, 1.995).
POS tag-ging is the first step of natural language process-ing, and stochastic taggers have solved this problemwith satisfying accuracy for many applications.
Thenext step is parsing, or that is to say discoveringthe structure of a given sentence.
Recently, manyparsers based on the stochastic approach ave beenproposed.
Although their reported accuracies arehigh, they are not accurate nough for many appli-cations at this stage, and more attempts have to bemade to improve them fm:ther.One of the major applications of a parser is toparse the spoken text recognized by a speech rec-ognizer.
This attempt is clearly aiming at spokenlanguage understanding.
If we consider how to con>bine a parser and a speech recognizer~ it is better ifthe parser is based on a generative stochastic model,as required for the language model of a speech rec-ognizer.
Here, "generative" means that the sum ofprobabilities over all possible sentences is equal toor less than 1.
If the language model is generative,it allows a seamless combination of the parser andthe speech recognizer.
This means that the speechrecognizer has the stochastic parser as its languagemodel and benefits richer information than a nor-mal n-gram model.
Even though such a Colnbiim-tion is not possible in practices , the recognizer out-puts N-best sentences with their probabilities, andthe parser, taking them as input, parses all of themand outputs the sentence with its parse tree thathas the highest probability of all possible combina-tions.
As a resnlt, a parser based on a generativestochastic language model may hell) a speech rec-ognizer to select the most syntactically reasonablesentence among candidates.
Therefore, it is betterif the language model of a parser is generative.In this paper, taking Japanese as the object lan-guage, we propose a generative stochastic languagemodel and a parser based on it.
This model treats asentence as a word sequence and predicts each wordfrom left to right.
The history at each step of predic-tion is a sequence of partial parse trees covering thepreceding words.
To predict a word, our model firstpredicts which of the partial parse trees at this stagehave dependency relation with the word, and thenpredicts the word fi'om the selected partial parsetrees.
In Japanese each word depends on a subse-quent word, that is to say, each dependency relationis left to right, it is not necessary to predict the di-rection of each dependency relation.
So in order toextend our model to other languages, the model mayhave to predict the direction of each dependency.
Webuilt a parser based on this model, whose parame-ters are estimated fl:om 1,072 sentences in a finan-cial newspaper, and tested it on 1.19 sentences fl:omthe same newspaper:.
The accuracy of the depen-558dency relation was 89.9%, the highest obt.ained byany aapa.nese stochastic parsers.2 Stochast i c  Language Mode l  basedon DependencyIn this section, we propose a stochastic /angua.gemodel based on dependency.
Unlike most stochas-tic language models %r a. parser, our model is the-oreticMly based on a hidden Markov model.
In ourmodel a. sentence is predicted word by word fi'om leftto right and the state at ea.ch step of prediction isbasieMly a. sequence of words whose modifiea.nd hasnot appeared yet.
According to a psyeholinguisticreport on la.nguage structure (Yngve, 1960), there isan upper limit on the number of the words whoseinodificaJ~ds ha.ve not appeared yet.
This limit is de-termined by tim mmfloer of slots in sl~ort-term em-ory, 7 :k 2 (Miller, 1956).
With this limitation, weCall design a pa.rser based on a linite state model.2.1 Sentence  Mode l'\]'he I)asic idett of our model is that each word wouldbe better predicted from the words that have a. de-pendency rela.tion with the.
word to be predictedthan from the preceding two words (l.ri-gram model).Let us consider the complete structur('~ of the sen-tence in /"igure I and a \]tyl)otheti(:al struetm:e afterthe 1)rediction of tile lifth word at the top of Fig-ure 2.
In this hypothetica.l st ; ructure ,  there are threetrees: one root-only tree (/q, eomposc'd of wa) a.ndtwo two-node trees (l. conta.ining 'wz and 'w2, and l(,containing w4 an(1 'w5).
If the last two trees (& andle) de4)end on the word we, this word may betterbe predicted from thes(~ two trees.
I"rom this I)ointof view, our model Ill-st: predicts the t rees  del)cndingon the next word and then l)redicts the next wordfrom thes(" trees.Now, let us make the tbllowing definitions in orderto explain our model formally.?
11~ ~- t tq lv2 .
.
. '
t t )~  : a, seq l l cnce  o f  words .
\ ] \ ]ere a.word is define(l as a, pair consisting of a string ofalplmbetic hara.cters and a, pa.rt of speech (e.g.the/DT).?
ti = l i l 2 " " lk ,  : a, sequence of parrtiM parsetrees covering the i-pretix words ('w~ w~.
.
.
wi).?
t + trod t~- : subsequences of ti ha.ving a.ndnot having a. dependeney relation with the nextword respectively.
In .h~p~mese, like many otherlangua.ges, no two dependency relations crosseach other; thus tl = t~ t +,?
(t w) : a tree with 'w as its root a.nd t as thesequence of all subtrees connected to the root.After wi+l has been predicted from the treesdepending on it (t+), there a.re trees renmin-i,,~ (iT) a.d a. ,,ewly prod.eed t,.,'~e ((t?w~+,));th,,s t~+, = t~ .
(~,+,,,,~+,).1'(t~)--.~5 ................................................................................ , (i ~ - -~ I i - -~1  - '~  "Wl W 2 W 3 W 4 W 5 W 6I }  ........................ +~hei  isubj\]il lending}!\[  : j ...................... ~la, ~  ~t,,el _ _ , _ _1 z ii - -3  w4 W5 i 1?6?;'(~,,01q)....
t.} .............................. <1 ~ w,~ > ....................................................w, w, }{ w:, w~ w.~ I% {.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
* ' .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2/)(t~;), where t ,  t~ 4-.
= = .
(tat~;)Figure 2: Word prediction from a par t ia l  parse?
Jhna:r : upper l imit on the munber number ofwords whose moditicands have not appearedyet.Under these definitions, our stochastic languagemodel is defined as follows:~'("") = H t'(,,,I,,,,, w~...,,,~_~)i=1~ H l '(w, l t L , ) I ' ( t L , I t ; -~)  (:l)t,,, ET',~ i=1where 7;, is all possible bhm.ry trees with n nodes.lie,','., the first fi~.ctor, (P(wi l t+ 1)), is ca.lled the wordprediction model and the second, (P (~'~1 } ti-1 ))' thestate prediction model.
Let us consider Figure 2aga.in.
At.
the top is the state just a.fter the predic-tion of the tilth word.
The state prediction modelthen predicts the pa.rtial purse trees depending onthe next word a.mong all partial parse trees, as shownin the second figure.
Finally, the word predictionmodel predicts the next word Dora the partial parsetrees depending on it.As described above, there may be an upper limiton the number of words whose modificands ha.ve notyet appeared.
To put it in a.nother way, the lengthof the sequence of l)artial parse trees (ti) is limited.559W I W 2 W 3 W 4 W 5 W 6 W 7 W 8 W 9 WloFigure 1: Dependency struetm:e of a sentence.There%re, if the depth of the partial parse tree is alsolimited, the number of possible states is limited.
Un-der this constraint, our model can be considered asa hidden Markov model.
In a hidden Marker model,the first factor is called the output probability andthe second, the transition probability.Since we assmne that no two dependency relationscross each other, the state prediction model onlyhas to predict the mmaber of the trees dependingon the ,text word.
Tln,  s S'(t+_,lt, ._,) = ~':'(ylt~_~)where y is the number of trees in the sequence t?_ 1,According to the above assumption, the last y par-tial parse trees depend on the i-th word.
Since thenmnber of possible parse trees for a word sequencegrows exponentially with the number of the words,the space of the sequence of partial parse trees ishuge even if the length of the sequence is limited.
'?his inevitably causes a data-sparseness problem.To avoid this problern, we limited the number oflevels of nodes used to distinguish trees.
In our ex-periment, only the root and its children are checkedto identify a partial parse tree.
Hereafter, we repre-sent \]JLL to denote this model, in which the lexiconof the first level and thai; of the second level are con-sidered.
Thus, in our experiment each word and thenumber of partial parse trees depending on it arepredicted by a sequence of partial parse trees thattake account of the nodes whose depth is two or less.It is worth noting that if the dependency structureof a sentence is linear - -  that is to say, if each worddepends on the next word, - -  then our model willbe equivalent to a word tri-gram model.We introduce an interpolation technique (Jelineket al, 1991) into our model ike those used in n-grammodels.
By loosening tree identification regulations,we obtain a more general model.
For example, ifwe check only the POS of the root and the I?OSof its children, we will obtain a model similar to aPOS tri-gram model (denoted PPs' hereafter).
Ifwe check the lexicon of the root, but not that of itschildren, the model will be like a word bi-gram model(denoted PNL hereafter).
As a smoothing method,we can interpolate the model PLL, similar to a wordtri-gram model, with a more general model, PPP orPNL.
In our experiment, as the following formulaindicates, we interpolated seven models of differentgeneralization levels:P(wiltt_l) ~--- ~6PLL(WiI',?_I) q- ,~5\]3pL(Wiit+i_l)q-,~4\]3pp (Wtit?_1) @ )t3PNL(W i\[tF_\] )+ s'N,, It +, ) + x, PNN I*?-,)+,X0 (2)where X in PYx is the check level of the first levelof the tree (N: none, P: POS, L: lexicon) and Y isthat of the second level, and lG,c-gr<~m is the uniformdistribution over the vocabulary W (-\])~U,O--gI'D,I~\](*/)) =l / IWl).The state predictio,, model also in-terpola.ted in the salne way.
in this case, the possi-ble events are y = 1 ,2 , .
.
.
,  Ym(~x, thus; /~a,0-gr<~m =l / y,,,ax .2.2 Parmneter  Es t imat ionSince our model is a hidden Markov model, the pa-rameters of a model can l)e estimated from at.
rowcorpus by EM algorithm (13amn, 1972).
With thisalgorithm, the probability of the row corpus is ex-pected to be maxinfized regardless of the structure ofea.ch sentence.
So the obtained model is not alwaysappropriate for a. parser.In order to develop a model appropriate for aparser, it is better that the parameters are estimatedfrom a syntactically annotated corlms by a maxi-mmn likelihood estimation (MI,E) (Meriaklo, 1994:)as follows:1,(wit+) MZ,,  j'(<t,+-- f(< t+ w,:>)P(t+lt) M&E f(t+,t)f(t)where f (x)  represents the frequency of an event x intile training corpus.The interpolation coeificients in the formula (2)are estimated by the deleted interpolation method(aelinek et al, 1991).2.3 Se lec t ing  Words  to  be Lex iea l i zedGenerally speaking, a word-based n-gram model isbetter than a l>OS-based 'n-gram model in terms of560predictive power; however lexica.lization of some in-frequent words may be ha.rmfu\] beta.use it may c;msea.
data-sparseness problem.
In a. practiea.1 tagger(I(upiec, \] 989), only the nlost, frequent \] 00 words a.relexicalized.
Also, in a, sta.te-ofthe-a.rt English pa.rser(Collins, 1997) only the words tha, t occur more tha,nd times in training data.
are lexicalized.For this reason, our pa.rser selectn the words to belexicalized at the time of lea.rning.
In the lexical-ized models described above (P/A;, I},L and f~VL),only the selected words a.re \]exica.lized.
The selec-tion criterion is parsing a.ccuracy (see section 4) ofa.
hekl-out corpus, a small part of the learning co lpus excluded from l)a, ramcter cstima.tion.
Thus onlythe words tliat a.re 1)redicte(1 to improve the parsinga.Ccllra.oy of  the test corpilS> or i l l lkl loWll illpll{,> i/3"elexicalized.
The algorithm is as follows (see l,'igurca):\].
In the initial sta.te a.ll words are in the class oftheir I)OS.2.
All words are sorted ill descending order of theirfrequency, a.nd the following 1)rocens is executedfor each word in this order:(a.)
The word is lexicalizcd provisionally andthe accura.cy el tile held-oul, corpus is (:;/l-cilia.ted.
(b) Ir a.n illiproven\]ont in observed, the word is10xica.lized definitively.Tile result of this \]exica.liza.tion algoril.lun is used toidentil~y a.
\])a.rtia.l l)arse tree.
That is to say, ()ill 3, Icx-iealized wordn are distinguished in lexicalized mod-els.
It" IlO wordn Were nelcctxxl I:o be lexica/ized, {;hell/ 'NL  = \])N1 ' a,ii(I \ ])LL - -  \[)t"L = 191'1'.
I t  is wort \ ] lnol ; ing that  i f  we t ry  to .ioi,, a word  w i th  al lo/,/ lerwet( l ,  then this  a,lgOlJithnl w i l l  be a, l lo r lna\ ]  top-downc, lustcring a.igorithnl.2.4 Unknown Word  Mode lTo enable olir stocllastic la.nguage lnodel to handleunknowil words> we added a.li ui/knowii word modelbased Oil a cha.ra,cter \])i-giPa,nl nio(M, lr the nextword is not in the vocabula.ry, the n\]o(lel predictsits POS a.nd the llllklloWll word model predicts thestring of the word as folkm, s:re.q- \]s'(,wlS'OS) = 1-\[ )i=1where 'w = x tx2 .
.
.xm,  xcl == aSm+l = \]~'\]'.1}'1", a special character corresponding to a wordl)oundary, in introduced so tha.t the ntlilt of the l)rob-ability over all sl, r i l lgs is eqlla,\] to 71.In the l)ara.lneter cstima.tion described a.1)ove, a.learning corpus is divided into k parts.
In our ex-i)erirnent, the vocabulary is composed of the wordn: - (~  ~ ,- '" l  .............................. -10& ....................................... 10S2 L_I @.
.
.................................. .
,,,,,.i* / /  .....................
I 'OS ~ ........... ~ ...............
I 'OS 2: - ~  ...............
I 'OS I ......................
I 'OS 2lexicalizeyesI10yesyesFigure 3: A conceptual figure of the le, xicaliza.tionalgorit.hm.occurring in more than one l)artia\] corlmn and theother words are used for 1)arameter entima, tion ofthe unknown word model.
The l)i-gram l)robal)ilityof the unknown word model of a. I)OS in estimated\['ronl the words among them and belonging to thePOS as follows:1 !l, o s (.~, i la: i - J ) M)~V .fP o s (*  i, * i -  ~ )fPos (,,;~-,)The character I)i%ram model is also interl)ola.tedWi/ , \ ] I  a l l i l i-~r~iill i l lode\]  and  a Zel'O-~l'aill l l lodc\] .The interl)olation coellicients are estinmi.d by thedeleted interpolation method (,lelinek el.
al., 1991).3 Syntact i c  Analysis(,el cJ dl.y, a. l)a.rscr may I)c considered an a modulethat recdvcs a. sequence of words annotated with a,I'()S and oul.putn its structm'e.
Our parner, whichincludes a stochastic mflmown word model, however,is a.I)le to a.cc.el)t a cha.ra.ctc'r sequence as an input andexecute segmenta.tion, POS tagging, and syntacticanalysis nimultaneously I .
In this section, wc exphfinour pa.rser, which is based on the language modaldescribed in the preceding section.3.1 S to ( 'has t ie  Syntac | , i c  Ana lyzerA syntactic analyzer, bancd on a. stochastic languagemodel, ca.lculatc's the pa.rse tree (see Figure 1) withthe highest probabil ity for a given scquencc of char-acters x according to the following tbrmula.
::/' = , , , 'g i i i , ixP(Tl .
,  0"H~ (fl') :=;/~1 There is no space \])etweell words ill ,Japo.llese561= argmax P(TIx)P(~ )w(:c)=m= argn,~,xP(mlT)I~(r) ('."
13ayes' forn~ula)w(T)=.
'e= argnmxP(T)  ('."
P(mlT)= 1),W(T)=mwhere w(T) represents the concatenation of theword string in the syntactic trek T. P(T) in the lastline is a stochastic language model, in our parser,it is the probability of a parse tree T defined by thestochastic dependency model including the unknownword model described in section 2.p(T) = I I  Its_,), (a)i=1where wlw2".
"wn = w(T).3.2 Solut ion Search Algor i thmAs shown in formula (3), our parser is based on a hid-den Markov model.
It follows that Viterbi algorithmis applicable to search the best solution.
Viterbi al-gorithm is capable of calculating the best solution inO(n) time, where n is the number of input charac-ters.The parser repeats a state tra.nsition, readingcharacters of the input sentence from left to right.
Inorder that the structure of the input sentence may bea tree, the number of trees of the final state tn mustbe 1 and no more.
Among the states that satisfy thiscondition, the parser selects the state with the high-est probability.
Since our language model uses onlythe root and its children of a partial parse tree to dis-tinguish states, the last state does not have enoughinformation to construct he parse tree.
The parsercan, however, calculate the parse tree fi'om the se-quence of states, or both the word sequence and thesequence of y, the number of trees that depend onthe next word.
Thus it memorizes these values ateach step of prediction.
After the most probablelast state has been selected, the parser constructsthe parse tree by reading these sequences fi:om topto bottom.4 Eva luat ionWe developed a POS-based model and its lexical-ized version explained in section 2 to evaluate theirpredictive power, and implemented parsers based onthem that calculate the most probable dependencytree fi'om a given character sequence, using the so-lution search algorithm explained in section 3 to ob-serve their accuracy.
In this section, we present anddiscuss the experimental results.4.1 Condit ions on the Exper imentsThe corpus used in our experiments consists of ar-ticles extracted from a financial newspaper (NihonTable 1: Corpus.learningtest#sentences ~words1,072 30,292119 3,268#chars46,2124,909Keizai ,%inbun).
Each sentence in tile articles issegmented into words and its dependency structureis annotated by linguists using an editor speciallydesigned for this task at our site.
The corpus wasdivided into ten parts; the parameters of the modelwere estimated fi:om nine of them and the modelwas tested on the rest (see Table 1).
A small partof each leaning corpus is withheld from parameterestimation and used to select the words to be lex-icalized.
After checking the learning corpus, themaximum number of partial parse trees is set to 10To evaluate the predictive power of our model, wecalculated their cross entropy on the test corpns.
Inthis process, the annotated tree in the test corpus isused as the structure of the sentences.
Therefore theprobability of each sentence in the test corpus is notthe summation over all its possible derivations.
Tocompare the POS-based model and the \]exicalizedmodel, we constructed these models using the samelearning corpus and calcnlated their cross entropyon the same test corpus.
The POS-based model andthe }exicalized model have the same mfl~nown wordmodel, thus its contribution to the cross entropy isconstant.We implemented a parser based on the depen-dency models.
Since our models, inchsding acharacter-l)ased unknown word model, can returnthe best parse tree with its probability for any in-put, we can build a parser that receives a charactersequence as input.
It is not easy to evaluate, how-ever, because errors may occur in segmentation ofthe sequence into words and in estimation of theirPOSs.
For this reason, in the tbllowing description,we assume a word sequence as the input.The criterion for a parser is the accuracy of its out-put dependency relations.
This criterion is widelyused to evahmte Japanese dependency parsers.
Theaccuracy is the ratio of the nnmber of the words a.n-notated with the same dependency to the numl)er ofthe words as in the corpus:accuracy=#=words ependiug on tilt correct word~wordsTile last word and the second-to-last word of" a sen-tence are excluded, because there is no ambiguity.The last word has no word to depend on and thesecond-todast word depends always on the last word.562Table 2: Czoss entorpy alld acellraey of each model.language model cross enl\[,lTopy a, CCUFaicyselectively lexicalized 6.927 - ~ -completely lexicalized 6.651 87.1%POS-based 7.000 87.5%linear structure* -- 78.7%* F, adl word del)ends on l;he next word.4.2 Ewduat ionTable 2 shows the cross entropy and parsing accu-racy Of the baseline, where all words depend on thenext word, the POS-based dependency model andtwo lexicalized dependency models.
In the selec-tively lexicalized model, words to be lexicalized areselected by the aJgo,:ithm described in section 2.
Inthe completely lexicalized model, all words arc lcxi-calized.
This result attests experimentally that thepa.rser based on the selectively lexicalized model isthe best parser.
As for predictive power, however,the completely lexica.lized model has the lowest crosse~/tropy.
Thus this model is estimated to be the bestlanguage model for speech recognition.
Althoughthere is no direct relation between cross entropy ofl;he language model and error ra.te of a speech rec-ognizer, if we consider a spoken la.nguage parser, itma.y be better to select the words to be lexicalizedusing other criterion.We calculated the cross entropy and the parsingaccuracy (if' the model whose parameters arc esti-mated fi'om ;I/d, 1/16, and 1/64 of the learningcorpus.
The relation between the learning corpussize and the cross entrol)y or the l)arsing a.ccm:acy isshown in Figure d. The cross ent ropy  has a strongertendency to decrease as the corpus size increases.As for accuracy, there is aJso a tendency for parsersto become more accurate as the size of the learningincreases.
The size of the cor/)us we h~we all thisstage is not at all large, ltowever, its accuracy is atthe top level of Japanese parsers, which nse 50,000-1.90,000 sentences.
Therefore, we conclude that ourapproach is quite promising.5 Re la ted  WorkslIistorica.lly, structures of natural languages havebeen described by a context-free grammar a.nd all\]-biguities have been resolved by parsers based on acontext-free grammar (Fujisaki et al, 1989).
In re-eenl, years, some attempts have been made in thearea of parsing by a tinite state model (Otlazer, 1999)etc.
Our parser is also based on a finite state model.Unlike these models, we focused on reports on alimit on language structure caused by the capacityour memory (Yngve, 1960) (Miller, 19561.
Thus our20- -16q120 ,-~ 8accuracy - -4  - _--484.0% 86.2% 88.1% 89.9%%t'ross Clltropy \+\  "\I00 %8O60 N'40200100 101 102 103 104 105 0#characters in learning corpusI,'igure d: Relation between cross entropy a.nd pars-i~lg accuracy .model is psycholinguistically more al)propriate.Recently, in the area of parsers based oll a. stochas-tic context-fi:ee grammar (SCFG), some researchershave pointed out the importance of t.he lexiconand proposed lexiealized models (Charniak, 1997;Collins, 1997).
111 these papers, they reported sig-nificant improvement of parsing accuracy.
Takingthese reports into account, we introduced a methodof pa.rlJal lexicalization and reported significant im--provement of parsing accuracy.
Our lexicalizationmethod is also a.pplicable to a. SCFG-based parserand improves its parsing accuracy.The model we present in this pal)er is a genera-tire stochastic language model.
Chelba and aelinek119981 presented a similar model.
In their model,each word is predicted t?om two right-most headwords regardless of dependency rela.tion betweenthese head words and the word.
Eisner (\[996) alsopresented a. st;ochastie structura.1 language model, inwhich ea.ch word is predicted t?om its head wordand the nearest one.
This model is very similar tothe parser presented by Collins 11.9961.
The great-est difference between our model and these modelsis in that our model predicts the next word from thehead words, or partial parse trees, depending on it.Clearly, it is not always two right-most head wordsthat have dependency relation with the next word.It.
follows that our model is linguistically more ap-propirate.There have been some attempts at stochasticJapal, ese parser (llaruno et al, 1998) (l"ujio andMatsmnoto, 19981 (Mori and Naga.o, 1.998).
TheseJapanese parsers are based on a unit called bunsetsu,a sequence of one or more content words followed byzero or more traction words.
The parsers take asequence of units and outputs dependency relationsbetween them.
Unlike these parsers, our model de-563scribes dependencies between words; thus our modelcan easily be extended to other languages.
As tbrthe accuracy, although a direct comparison is noteasy between our parser (89.9%; 1.,072 sentences)and these parsers (82% - 85%; 50,000 - 190,000 sen-tenees) because of the difference of the units andthe corpus, our parser is one of the state-of-the-artparsers \[br Japanese language.
It should be notedthat ore: model describes relations among three ormore units (case frame, consecutive dependency re-lations, etc.
); thus our model benefits a greater dealfrom increase ot.'
corpus size.6 Conc lus ionIn this paper we have presented a stochastic lan-guage model based on dependency structure.
Thismodel treats a sentence as a word sequence and pre-dicts each word from left to right.
"The history ateach step of prediction is a sequence of partial parsetrees covering the preceding words.
To predict aword, ore: model first selects the partial parse treesthat have a dependency relation with the word, andthen predicts the next word from the selected partialparse trees.
We also presented an algorithm %r lexi-calization.
We lmilt parsers based on the POS-basedmodel and its lexicalized version, whose parametersare estimated from 1,072 sentences of a financialnewspaper.
We tested the parsers on 119 sentencesDom the same newspaper, which we.re excluded fl:omthe learning.
The accuracy of the dependency rela-tion of the lexicalized parser was 89.9%, the highestobtained by any Japanese stochastic parser.ReferencesL.
E. Baum.
1.972.
An inequality and associatedmaximization technique in statistical estimationfor probabilistie functions of Ma.rkov process.
In-equalities, 3:1-8.Eugene Charniak.
1997.
Statistical parsing with acontext-fl:ee grammar and word statistics.
In Pro-ceedings of the l/ith National ConfeTvnce on Arti-ficial Intclligence, pages 598-603.Ciprian Chelba and Frederic .l elinek.
1998.
F, xploit-ing syntactic structure for language modeling.
InProceedings of the I Tth hdernational Conferenceon Computational Linguistics, pages 225-231.Kenneth Ward Church.
1988.
A stochastic pa.rtsprogram and noun phrase parser for unrestrictedtext.
In Proceedings of the 3eeond Conference onApplied Natural Language Processing, pages 136-143.Michael John Collins, 1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pages 184-191.Michael Collins.
1.997.
Three genera.tire, lexiea.lisedmodels for statistical parsing.
In Proceedings ofth(~ 35th Annual Meeting of the Association forComputational Linguistics, pages 16 -23.l)oug Cutting, Julian Kupiec, .Jan 1)edersen, andPenelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedings of the "lldrd Conference onApplied Natural Language Processing, pages 133l.d0.Evangelos Dermatas and George Kokkinakis.
1995.Automatic stochastic tagging of ha.tin:el languagetexts.
Computational Linguistics, 21 (2):137-103.Jason M. Eisner.
1.996.
Three new probabilisticmodels for dependency parsing: An exploration.In Proceedings of the 16th lnlernational Co~@r-ence on Computational Linguistics , pages 340345.Masakazu Fujio and Yuji Matsumoto.
1998.Japanese dependency structure analysis based onlexicalized statistics.
In Proceedings (of the ThirdConference on Empirical Methods in Natural Lan-guage Processing, pages 87-96.T.
Fujisaki, F. ,\]elinek, .1.
Cocke, E. Black, andT.
Nishino.
1.989.
A probabilistic parsing methodfor sentence disambiguation.
In Proceedings of theInternational .Parsing Workshop.Masahiko Itarmm, Satoshi Shirai, and YoshifnmiOoyama.
1998, Using decision trees to construct apractical parser.
In Proceedings of the ITlh Inter-national Confer(race on Compul, alior~al Linguis-tics~ pages 505-511.Fredelick .\]elinek, 11,obert L. Mercer, and Salinr\[{oukos.
1991.
Principles of lexica.1 languagemodeling for speech recognition.
In Advances in,5'peeeh ,5'ignal Processing, chapter 21, pages 651-699. l)ekker.Julian Knpic'c, 1989.
Augmenting a hidden Markovmodel 'or phrase-dependent word t.agging.
In .Pro-ceedings of the DAI{I)A ,5'peeeh and Natural Lan-guage Workshop, pages 92- 08.Bernard Merialdo.
1994.
'15~.gging English text witha probabilistie model.
Computational Linguislics,~0(~):155 -171.George A. Miller.
1956.
The magical number seven,plus or minus two: Some limits on our capacityfor processing information.
The Psychological l~e-view, 63:81-97.Shinsuke Mori and Makoto Nagao.
1998.
A stochas-tic language model using dependency and its im-provement by word clustering.
In Proceedings ofthe ITth International Co@'renee on Computa-tional Linguistics, pages 898-90~t.Kernel Ollazer.
1.999. l)ependency parsing with anextended finite state approach.
In Proceedings ofthe 37t, h Annual Mecti~,g of the Association forComputational Linguistics, pages 254-260.Victor H. Yngve.
11960.
A model and a hypothesisfor language structure.
The American Philosoph-ical Society, 104(5):444-466.564
