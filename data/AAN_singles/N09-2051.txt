Proceedings of NAACL HLT 2009: Short Papers, pages 201?204,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving Coreference Resolution by Using Conversational MetadataXiaoqiang Luo and Radu Florian and Todd WardIBM T.J. Watson Research CenterYorktown Heights, NY 10598{xiaoluo,raduf,toddward}@us.ibm.comAbstractIn this paper, we propose the use of metadatacontained in documents to improve corefer-ence resolution.
Specifically, we quantify theimpact of speaker and turn information on theperformance of our coreference system, andshow that the metadata can be effectively en-coded as features of a statistical resolution sys-tem, which leads to a statistically significantimprovement in performance.1 IntroductionCoreference resolution aims to find the set of lin-guistic expressions that refer to a common entity.
Itis a discourse-level task given that the ambiguity ofmany referential relationships among linguistic ex-pressions can only be correctly resolved by examin-ing information extracted from the entire document.In this paper, we focus on exploiting the struc-tural information (e.g., speaker and turn in conversa-tional documents) represented in the metadata of aninput document.
Such metadata often coincides withthe discourse structure, and is presumably useful tocoreference resolution.
The goal of this study is toquantify the effect metadata.
To this end, informa-tion contained in metadata is encoded as features inour coreference resolution system, and statisticallysignificant improvement is observed.The rest of the paper is organized as follows.In Section 2 we describe the data set on whichthis study is based.
In Section 3 we first showhow to incorporate information carried by metadatainto a statistical coreference resolution system.
Wealso quantify the impact of metadata when they aretreated as extraneous data.
Results and discussionsof the results are also presented in that section.2 Data SetThis study uses the 2007 ACE data.
In the ACEprogram, a mention is textual reference to anobject of interest while the set of mentions in adocument referring to the same object is calledentity.
Each mention is of one of 7 entitytypes: FAC(cility), GPE (Geo-Political Entity),LOC(ation), ORG(anization), PER(son), VEH(icle),and WEA(pon).
Every entity type has a prede-fined set of subtypes.
For example, ORG sub-types include commercial,governmental andeducational etc, which reflect different sub-groups of organizations.
Mentions referring to thesame entity share the same type and subtype.
Amention can also be assigned with one of 3 men-tion types: either NAM(e), NOM(inal), or PRO(noun).Accordingly, entities have ?levels:?
if an entity con-tains at least one NAM mention, its level is NAM; orif it does not contain any NAM mention, but containsat least one NOM mention, then the entity is of levelNOM; if an entity has only PRO mention(s), then itslevel is PRO.
More information about ACE entityannotation can be found in the official annotationguideline (Linguistic Data Consortium, 2008).The ACE 2007 documents come from a variety ofsources, namely newswire, broadcast conversation,broadcast news, Usenet, web log and telephone con-versation.
Some of them contain rich metadata, asillustrated in the following excerpt of one broadcastconversation document:<DOC><DOCID>CNN_CF_20030303.1900.00</DOCID><TEXT><TURN><SPEAKER> Begala </SPEAKER>Well, we?ll debate that later on in theshow.
We?ll have a couple of expertscome out, ...201</TURN><TURN><SPEAKER> Novak </SPEAKER>Paul, as I understand your definitionof a political -- of a professionalpolitician based on that is somebodywho is elected to public office.
...</TURN>...</TEXT></DOC>In this example, SPEAKER and TURN informa-tion are marked by their corresponding SGML tags.Such metadata provides structural information: forinstance, the metadata implies that Begala is thespeaker of the utterance ?Well, we?ll debate ..., ?and Novak the speaker of the utterance ?Paul, asI understand your definition ...?
Intuitively, knowingthe speakers of the previous and current turn wouldmake it a lot easier to find the right antecedent ofpronominal mentions I and your in the sentence:?Paul, as I understand your definition ...?Documents in non-conversational genres (e.g.newswire documents) also contain speaker and quo-tation, which resemble conversational utterance, butthey are not annotated.
For these documents, weuse heuristics (e.g., existence of double or singlequote, a short list of communication verb lemmassuch as ?say,?
?tell?
and ?speak?
etc) to determinethe speaker of a direct quotation if necessary.3 Impact of MetadataIn this section we describe how metadata is used toimprove our statistical coreference resolution sys-tem.3.1 Resolution SystemThe coreference system used in our study is a data-driven, machine-learning-based system.
Mentionsin a document are processed sequentially by men-tion type: NAM mentions are processed first, fol-lowed by NOM mentions and then PRO mentions.The first mention is used to create an initial entitywith a deterministic score 1.
The second mentioncan be either linked to the first entity, or used to cre-ate a new entity, and the two actions are assigned ascore computed from a log linear model.
This pro-cess is repeated until all mentions in a document areprocessed.
During training time, the process is ap-plied to the training data and training instances (bothpositive and negative) are generated.
At testing time,the same process is applied to an input documentand the hypothesis with the highest score is selectedas the final coreference result.
At the core of thecoreference system is a conditional log linear modelP (l|e,m) which measures how likely a mention mis or is not coreferential with an existing entity e.The modeling framework provides us with the flexi-bility to integrate metadata information by encodingit as features.The coreference resolution system employs a va-riety of lexical, semantic, distance and syntacticfeatures(Luo et al, 2004; Luo and Zitouni, 2005).The full-blown system achieves an 56.2% ACE-value score on the official 2007 ACE test data,which is about the same as the best-performing sys-tem in the Entity Detection and Recognition (EDR)task (NIST, 2007).
So we believe that the resolutionsystem is fairly solid.The aforementioned 56.2% score includes men-tion detection (i.e., finding mention boundaries andpredicting mention attributes) and coreference res-olution.
Since this study is about coreference res-olution only, the subsequent experiments, are thusperformed on gold-standard mentions.
We split theACE 2007 data into a training set consisting of 499documents, and a test set of 100 documents.
Thetraining and test split ratio is roughly the same acrossgenres.
The performance numbers reported in thesubsequent subsections are on the 100-document de-velopment test set.3.2 Metadata FeaturesFor conversational documents with speaker and turninformation, we compute a group of binary featuresfor a candidate referent r and the current mentionm.
Feature values are 1 if the conditions describedbelow hold:?
if r is a speaker, m is a pronominal mention andr utters the sentence containing m.?
if r is a speaker, m is pronoun and r utters thesentence one turn before the one containing m.?
if mention r and mention m are seen in thesame turn.?
if mention r and mention m are in two consec-utive turns.Note that the first feature is not subsumed by thethird one since a turn may contain multiple sen-tences.
For the same reason, the last feature does notsubsume the second one.
For the sample documentin Section 2, the first feature fires if r = Novak andm = I; the second features fires if r = Begala202and m = I; the third feature fires if r = Pauland m = I; and lastly, the fourth feature fires ifr = We and m = I.
For ACE documents thatdo not carry turn and speaker information such asnewswire, we use heuristic rules to empirically de-termine the speaker and the corresponding quota-tions before computing these features.To test the effect of the feature group, we trainedtwo models: a baseline system without speaker andturn features, and a contrast system by adding thespeaker and turn features to the baseline system.
Thecontrast results are tabulated in Table 1.
We observean overall 0.7 point ACE-value improvement.
Wealso compute the ACE-values at document level forthe two systems, and a paired Wilcoxon (Wilcoxon,1945) rank-sum test is conducted, which indicatesthat the difference between the two systems is statis-tically significant at level p ?
0.002.Note that the features often help link pronounswith their antecedents in conversational documents.But ACE-value is a weighted metric which heav-ily discounts pronominal mentions and entities.
Wesuspect that the effect of speaker and turn informa-tion could be larger if we weigh all mention typesequally.
This is confirmed when we looked at the un-weighted B3 (Bagga and Baldwin, 1998) numbersreported by the official ACE08 scorer (column B3in Table 1): the overall B3 score is improved from73.8% to 76.4% ?
a 2.6 point improvement, whichis almost 4 times as large as the ACE-value change.System ACE-Value B3baseline 78.7 73.8+ Spkr/Turn 79.4 76.4Table 1: Coreference performance: baseline vs. systemwith speaker and turn features.3.3 Metadata: To Use Or Not to Use?In the ACE evaluations prior to 2008, mentions in-side metadata (such as speaker and poster) are anno-tated and scored as normal mentions, although suchmetadata is not part of the actual content of a doc-ument.
An interesting question is: how large an ef-fect do mentions inside metadata have on the systemperformance?
If metadata are not annotated as men-tions, is it still useful to look into them?
To answerthis question, we remove speaker mentions in con-versational documents (i.e., broadcast conversationand telephone conversation) from both the trainingand test data.
Then we train two systems:?
System A: the system totally disregards meta-data.?
System B: the system first recovers speakermetadata using a very simple rule: all to-kens within the <SPEAKER> tags are treatedas one PER mention.
This rule recovers mostspeaker mentions, but it can occasionally re-sult in errors.
For instance, the speaker ?CNNcorrespondent John Smith?
includes affilia-tion and profession information and ought tobe tagged as three mentions: ?CNN?
as anORG(anization) mention, ?correspondent?
and?John Smith?
as two PER mentions.
With re-covered speaker mentions, we train a modeland resolve coreference as normal.After mentions in the test data are chained in Sys-tem B, speaker mentions are then removed from sys-tem output so that the coreference result is directlycomparable with that of System A.The ACE-value comparison between System Aand System B is shown in Table 2.
As can beseen, System B works much better than System A,which ignores SPEAKER tags.
For telephone con-versations (cts), ACE-value improves as much as 4.6points.
A paired Wilcoxon test on document-levelACE-values indicates that the difference is statisti-cally significant at p < 0.016.System bc ctsA 75.2 66.8B 76.6 71.4Abs.
Change 1.4 4.6Table 2: Metadata improves the ACE-value for broadcastconversation (bc) and telephone conversation (cts) docu-ments.The reason why metadata helps is that speakermention can be used to localize the coreference pro-cess and therefore improves the performance.
Forexample, in the sentences uttered by ?Novak?
(cf.the sample document in Section 2), it is intuitivelystraightforward to link mention I with Novak, andyour with Begala ?
when speaker mentions aremade present in the coreference system B.
On theother hand, in System A, ?I?
is likely to be linkedwith ?Paul?
because of its proximity of ?Paul?
in theabsence of speaker information.The result of this experiment suggests that, unsur-prisingly, speaker and turn metadata carry structural203information helpful for coreference resolution.
Evenif speaker mentions are not annotated (as in SystemA), it is still beneficial to make use of it, e.g., by firstidentifying them automatically as in System B.4 Related WorkThere is a large body of literature for coreferenceresolution based on machine learning (Kehler, 1997;Soon et al, 2001; Ng and Cardie, 2002; Yang et al,2008; Luo et al, 2004) approach.
Strube and Muller(2003) presented a machine-learning based pronounresolution system for spoken dialogue (Switchboardcorpus).
The document genre in their study is simi-lar to the ACE telephony conversation documents,and they did include some dialogue-specific fea-tures, such as an anaphora?s preference for S, VPor NP, in their system, but they did not use speakeror turn information.
Gupta et al (2007) presentsan algorithm disambiguating generic and referential?you.
?Cristea et al (1999) attempted to improve coref-erence resolution by first analyzing the discoursestructure of a document with rhetoric structure the-ory (RST) (Mann and Thompson, 1987) and thenusing the resulted discourse structure in coreferenceresolution.
Since obtaining reliably the discoursestructure itself is a challenge, they got mixed resultscompared with a linear structure baseline.Our work presented in this paper concentrates onthe structural information represented in metadata,such as turn or speaker information.
Such metadataprovides reliable discourse structure, especially forconversational documents, which is proven benefi-cial for enhancing the performance of our corefer-ence resolution system.AcknowledgmentsThis work is partially supported by DARPA GALEprogram under the contract number HR0011-06-02-0001.
We?d also like to thank 3 reviewers for theirhelpful comments.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at The First Interna-tional Conference on Language Resources and Eval-uation (LREC?98), pages 563?566.Dan Cristea, Nancy lde, Daniel Marcu, Valentin Tablan-livia Polanyi, and Martin van den Berg.
1999.
Dis-course structure and co-reference: An empirical study.In Proceedings of ACL Workshop ?The Relation ofDiscourse/Dialogue Structure and Reference?.
Asso-ciation for Computational Linguistics.Surabhi Gupta, Matthew Purver, and Dan Jurafsky.
2007.Disambiguating between generic and referential ?you?in dialog.
In Proceedings of the 45th ACL(the Demoand Poster Sessions), pages 105?108, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Andrew Kehler.
1997.
Probabilistic coreference in infor-mation extraction.
In Proc.
of EMNLP.Linguistic Data Consortium.
2008.
ACE (AutomaticContent Extraction) English annotation guidelinesfor entities.
http://projects.ldc.upenn.edu/ace/docs/English-Entities-Guidelines v6.5.pdf.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-lingual coreference resolution with syntactic fea-tures.
In Proc.
of Human Language Technology(HLT)/Empirical Methods in Natural Language Pro-cessing (EMNLP).Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the bell tree.
In Proc.
of ACL.William C. Mann and Sandra A. Thompson.
1987.Rhetorical structure theory: A theory of text organiza-tion.
Technical Report RS-87-190, USC/InformationSciences Institute.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proc.
of ACL, pages 104?111.NIST.
2007.
2007 automatic con-tent extraction evaluation official results.http://www.nist.gov/speech/tests/ace/2007/doc/ace07 eval official results 20070402.html.Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguis-tics, 27(4):521?544.Michael Strube and Christoph Muller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics.Frank Wilcoxon.
1945.
Individual comparisons by rank-ing methods.
Biometrics, I:80?83.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, TingLiu, and Sheng Li.
2008.
An entity-mention model forcoreference resolution with inductive logic program-ming.
In Proceedings of ACL-08: HLT, pages 843?851, Columbus, Ohio, June.
Association for Computa-tional Linguistics.204
