Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 98?106,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsTowards Automatic Scoring of a Test of Spoken Language withHeterogeneous Task TypesKlaus Zechner     and     Xiaoming XiEducational Testing ServiceRosedale Road, Princeton, NJ 08541, USA{kzechner,xxi}@ets.orgAbstractThis paper describes a system aimed at auto-matically scoring two task types of high andmedium-high linguistic entropy from a spokenEnglish test with a total of six widely differingtask types.We describe the speech recognizer used forthis system and its acoustic model and lan-guage model adaptation; the speech featurescomputed based on the recognition output;and finally the scoring models based on mul-tiple regression and classification trees.For both tasks, agreement measures betweenmachine and human scores (correlation,kappa) are close to or reach inter-humanagreements.1 IntroductionAs demand for spoken language testing and cost ofhuman scoring have increased in recent years,there is a growing interest in building both researchand industrial systems for automatically scoringnon-native speech (Bernstein, 1999, Zechner andBejar, 2006, Zechner et al 2007).However, past approaches have focused typi-cally only on one type of spoken language, or on arange of types similar in linguistic entropy.
En-tropy in this context can be seen as a measure forhow predictable the language in the expected spo-ken response is: Some tests, such as SET-10 (Bern-stein 1999), are focused mostly on the lowerentropy aspects of language, using tasks such as?reading?
or ?repetition?, where the expected se-quence of words is highly predictable.
Other as-sessments, such as the TOEFL?
Practice OnlineSpeaking test, on the other hand, focus on morespontaneous, high-entropy responses (Zechner etal., 2007).In this paper, we describe a spoken language testwith heterogeneous task types, ranging from readspeech to tasks that require candidates to give theiropinions on an issue, whose goal is to assess com-municative competence (Bachman, 1990; Bach-man & Palmer, 1996); we call this test THT (Testwith Heterogeneous Tasks).
Communicative com-petence, in this context, refers to a speaker's abilityto use the language for communicative purposes.The effectiveness of the communication typicallyconsists of a few aspects including comprehensibil-ity, accuracy, clarity, coherence and appropriate-ness, and is evident in a speaker's pronunciation,fluency, use of grammar and vocabulary, develop-ment  of ideas, and sensitivity to the context of thecommunication.This test has the advantage of being able to as-sess a wide range of non-native speakers?
profi-ciencies by using tasks of varying difficulty levelsto allow even low proficiency speakers some de-gree of success on easier task types.We select two tasks from this test, one of higherand one of medium to high entropy, and first adapta non-native English speech recognizer (trained onTOEFL?
Practice Online data) to transcribed THTtask responses, then compute a set of relevantspeech features based on the recognition output,and finally build a scoring model using a subset ofthese features to predict trained human rater scores.In this paper, we will demonstrate that the ma-chine-human score agreements on these two tasktypes come close to or even exceed the level ofinter-human agreement.This paper is organized as follows: Section 2discusses related work, Section 3 describes the testand the challenges for automatic scoring involved,Section 4 discusses the speech recognizer and theacoustic and language model adaptations per-98formed, and Section 5 describes the speech fea-tures selected for use in the scoring model.
In Sec-tion 6, we report the construction of the scoringmodel and its results, Section 7 contains a generaldiscussion and Section 8 concludes the paper witha brief discussion of future research.2 Related workThere has been previous work to automaticallycharacterize aspects of communicative competencesuch as fluency, pronunciation, and prosody.Franco et al (2000) present a system for automaticevaluation of the pronunciation quality of both na-tive and non-native speakers of English on a phonelevel and a sentence level (EduSpeak).
Candidatesread English texts and a forced alignment betweenthe speech signal and the ideal path through theHidden Markov Model (HMM) is computed.
Next,the log posterior probabilities for pronouncing acertain phone at a certain position in the signal arecomputed to achieve a local pronunciation score.These scores are then combined with other auto-matically derived measures such as the rate ofspeech (number of words per second) or the dura-tion of phonemes to yield global pronunciationscores.Cucchiarini et al (1997a, 1997b) describe a sys-tem for Dutch pronunciation scoring along similarlines.
Their feature set, however, is more extensiveand contains, in addition to log likelihood HiddenMarkov Model scores, various duration scores, andinformation on pauses, word stress, syllable struc-ture, and intonation.
In an evaluation, correlationsbetween four human scores and five machinescores range from 0.67 to 0.92.Bernstein (1999) presents a test for spoken Eng-lish (SET-10) that uses the following types of task-s: reading, sentence repetition, sentence building,opposites, short questions, and open-ended ques-tions.
All types except for the last are scored auto-matically and a score is reported that can beinterpreted as an indicator of how native-like aspeaker?s speech is.
In Bernstein et al (2000), anexperiment is performed to investigate the per-formance of the SET-10 test in predicting speak-ers?
oral proficiency.
It is shown that the SET-10test scores can predict different levels on the OralInteraction Scale of the Council of Europe?sFramework (North, 2000) for describing oral pro-ficiency of second/foreign language speakers withreasonable accuracy.
This paper further reports onstudies done to correlate the SET-10 automatedscores with the human scores from two other testsof oral English communication skills.
Correlationsare found to be between 0.73 and 0.88.Zechner and Bejar (2006) investigate the auto-mated scoring of unrestricted, spontaneous speechof non-native speakers.
They focus on exploring anumber of different fluency features for the auto-mated scoring of short (one minute) responses totest questions in a TOEFL-related program.
Theyexplore scoring models based on classification andregression trees (CART) as well as support vectormachines (SVM).
Their findings are that the SVMmodels are more useful for a quantitative analysis,whereas the CART models allow for a more trans-parent summary of the patterns underlying thedata.In this paper, we use CART to build the scoringmodel for one task type.
We also adopt multipleregression for another task type which has the ad-vantage of being more easily interpreted than, forexample, SVMs.
Another major difference be-tween previous work and the work reported in thispaper is that we use feature normalization andtransformation to obtain statistically more mean-ingful input variables for the scoring model.
In ad-dition, we do not use the whole set of features in anexploratory fashion.
Instead, we have carefullyselected a subset of features that are both good pre-dictors of human scores and maximize the repre-sentation of the concept of communicativecompetence.3 The THT test3.1 Task types and scoring rubrics of the THTSpeaking testThere are six task types in the THT Speaking test,ranging from reading-aloud tasks to tasks that re-quire short answers and tasks that require extendedresponses of one minute.
The rubrics differ in boththe dimensions of speaking skills measured and thepossible score points.
(Rubrics are characteriza-tions of candidates?
competence at given score lev-els and are used by human raters to determine theappropriate score for a response.)
Below is a briefdescription of the task types and the rubrics.99Task type 1: Reading-aloud (Planning time: 45seconds; Response time: 45 seconds; zero/very-low entropy)There are two read-aloud tasks.
Each task requiresthe test-taker to read a short paragraph of 40-60words aloud.
The reading materials include an-nouncements, advertisements, introductions, etc.These two tasks are rated analytically on pronun-ciation and intonation and stress on a 3-point scale.That is to say, two separate scores are given oneach task ?
one for pronunciation and one for into-nation and stress.Task type 2: Picture description (Planning time:30 seconds; Response time: 45 seconds; me-dium-high entropy)This task requires the test-taker to describe a pic-ture in as much detail as possible.This task is rated holistically on the combinedimpact of delivery (fluency, pronunciation etc.
),use of structures, vocabulary, content relevanceand fullness on a 3-point scale.Task type 3: Open-ended short-answer ques-tions (Planning time: none; Response time: 15-30 seconds; low/low-medium entropy)The test-taker responds, without preparation, tothree questions about familiar and accessible topicsthat draw on immediate personal experience.
Thefirst two questions each elicit a 15-second responsethat covers one or two pieces of information re-lated to the specified topic.
The third question re-quires a 30-second response that expresses anopinion or gives an explanation related to the topic.This task is rated holistically on the combined im-pact of delivery, use of structures, vocabulary, andtask appropriateness on a 3-point scale.Task type 4: Constrained short-answer ques-tions (Planning time: none; Response time: 15-30 seconds; low/low-medium entropy)The test-taker responds to three questions about aschedule/agenda that is provided in written form.All the information needed to answer the questionsshould be included on or easily inferred from theschedule.
The test-taker has 15 seconds to respondto each of the first two questions.
These questionsask for specific information on the schedule or eas-ily inferred information about the schedule.
Thetest-taker has 30 seconds to respond to the lastquestion which requires a summary of multipleevents or multiple pieces of information on theschedule.
This task is rated holistically on thecombined impact of delivery, use of structures,vocabulary, task appropriateness and content accu-racy on a 3-point scale.Task type 5: Respond to a voice mail (Planningtime: 30 seconds; Response time: 60 seconds;high entropy)In this task, the test-taker listens to a voicemail thatdescribes a problem, question or situation and thenassumes a particular role (bank teller, office assis-tant, etc.)
to respond with a proposed solution oranswer.
This task is rated holistically on the com-bined impact of fluency, pronunciation, intonationand stress, grammar, vocabulary, register, contentrelevance, and cohesion and idea progression on a5-point scale.Task type 6: Opinion task (Planning time: 15seconds; Response time: 60 seconds; high en-tropy)In this task, the test-taker is expected to state anopinion or position on an issue that is familiar andaccessible and to express support for the opinion orposition with reasons, examples, arguments, etc.This task is rated holistically on the combined im-pact of fluency, pronunciation, intonation andstress, grammar, vocabulary, content relevance,and cohesion and idea progression on a 5-pointscale.3.2 Challenges of the THT test design to auto-matic scoring1.
Some of the tasks require responses that are ex-pected to vary very little in vocabulary and contentacross examinees (e.g., Reading-aloud and Con-strained short-answer questions) whereas othersallow much more flexibility and variation in theuse of vocabulary and grammatical structure andtopical content (e.g.
Respond to a voicemail andOpinion task).
The predictability of the expectedresponse will dictate what type of language model-ing technique is preferable to optimize speech rec-ognition results.
Therefore, unlike in other systemsfocusing either on high or low entropy speech(e.g., Zechner and Bejar, 2006; Bernstein, 1999),in which a single speech recognizer is employed, itis anticipated that different types of speech recog-nizers are needed to suit different THT task types.This may increase both the amount of development100work and the complexity in integrating differenttypes of recognizers into the real-time automatedscoring system.2.
Furthermore, the scoring criteria of these sixdifferent task types are somewhat different.
Thissuggests that different scoring models may need tobe developed for different task types since therelevant speech features to be included in the scor-ing model for each task type may differ.3.
THT speaking tasks use two kinds of scorescales: 0-3 and 0-5.
Classification techniques, suchas classification trees or cumulative logit models(Agresti, 2002; Menard, 2001), may be more ap-propriate for task types that use a 3-point scale.Prediction techniques such as multiple regressionmay be better suited for task types that are on a 5-point scale.
Training different types of scoringmodels will certainly increase the complexity andthe amount of scoring model development andevaluation work.In summary, the complexity of the designof the THT Speaking test is expected to have a ma-jor impact on our efforts to develop an automatedscoring system.
Given these challenges and theresearch resources available, we decided on a strat-egy of starting with high entropy task types andproceeding to low entropy task types.
For this pa-per, we selected the high entropy Opinion task andthe medium-high entropy Picture tasks for systemdevelopment.4 Adaptation of the speech recognizerFor this work, we are using a state-of-the-art gen-der-independent Hidden Markov Model speechrecognizer whose acoustic model was trained onabout 30 hours of non-native speech and whoselanguage model was built on several hundred hoursof both native and non-native speech.
The non-native data came from the TOEFL?
PracticeOnline system, a web-based practice program forprospective takers of the Test Of English as a For-eign Language (TOEFL) (Zechner et al, 2007).This data is somewhat different from the THT, asthere are only high-entropy tasks in TOEFL Speak-ing and as the speakers are generally more profi-cient.
Due to this difference, the baseline wordaccuracy was fairly low (see Table 1).Therefore, as a first step, we needed to adapt theautomatic speech recognition engine to the THTspeech data.We had approximately 1,000 responses eachfrom the Picture and Opinion tasks transcribed.
Asmentioned above, while the Opinion task responsesare generally more spontaneous, the Picture taskrequires the candidate to accurately describe a pic-ture and thus restricts the possible answer spaceconsiderably.
Still, there is more room for individ-ual choice and variation in the vocabulary, gram-mar and content produced than there is in the morerestricted low-medium and low entropy task typesin the THT Speaking test.When using our baseline automatic speech rec-ognition (ASR)  engine without any adaptation tothe THT speech data, we only obtained word accu-racies between 25% and 33%, which was clearlyinadequate, and far below a word accuracy where,at least for some speakers, meaningful informationcan be drawn from the ASR hypothesis.Therefore, we undertook a series of adaptationand optimization steps with the goal of maximizingthe word accuracy on the two task types for theTHT Speaking test.
We first adapted the acousticmodel in batch mode with supervised maximum a-posteriori (MAP) adaptation using the combineddata from both tasks, then the language model, op-timized the filler cost parameter and finally con-ducted unsupervised maximum likelihood linearregression (MLLR) acoustic model adaptationbased on individual speakers.4.1 Acoustic model batch adaptationWe randomly selected about 90% of Picture andOpinion task response data for acoustic model(AM) adaptation, which contained 1,800 responsefiles (over 25 hours of speech, adult speakers withtypically low to intermediate English proficiency).Results are always reported on the held-out evalua-tion data containing 100 files for the Picture taskand 80 files for the Opinion task.We performed supervised maximum a posteriori(MAP) adaptation which is the method of choicefor larger amounts of data and is typically per-formed in batch mode (Tomokiyo and Waibel,2001; Wang et al, 2003).
After one cycle of adap-tation, word accuracy improved by about 8%, as isshown in Table 1.
We also performed unsupervisedmaximum likelihood linear regression (MLLR)adaptation, which is discussed in Section 4.4 be-low.101Picture task wordaccuracyOpinion task wordaccuracyMethod Absolute IncreasefrompreviousstepAbsolute IncreasefrompreviousstepBaselinerecognizer25.8% NA32.2%NAAM MAPadaptation33.6% 7.8%40.0%7.8%LM adap-tation50.4% 16.8%51.0%11.0%Filleroptimiza-tion57.0% 6.6%56.3%5.3%Ignoringfillers60.5% 3.5%59.2%2.9%MLLRSpeakeradaptation62.4% 1.9%61.2%2.0%Table 1.
Word accuracies after each incrementalstep of adaptation or optimization and performanceimprovement within each step for Picture and Opin-ion task types.4.2 Language model adaptationThe second step was language model (LM) adapta-tion.
The Picture and Opinion tasks were adaptedseparately using the same training sets as above.We built interpolated models between the task-specific LM and the baseline LM (from the origi-nal recognizer).We obtained the best results using only the task-specific LM trained on the THT data set (given inTable 1).
This indicates that the domain of each ofthe tasks is narrow enough that it can be suffi-ciently described with a set of about 900 tran-scribed examples each and it does not benefit froma larger LM such as our baseline LM.4.3 Filler cost optimization?Filler cost?
is a recognizer-internal parameter thatdetermines the likelihood of filler and noise wordsto be inserted into the hypothesis before or after?real?
words.
The higher the parameter?s value, theless likely fillers will be inserted.The experiments with the filler cost parametergrew out of an observation that the baseline recog-nizer has a tendency to hypothesize too manywords when faced with different kinds of ?uncer-tain?
audio, such as mumbled words, noises or fill-ers.
Therefore we conjectured that having therecognizer hypothesize more filler and noise wordsin these cases and be more restrictive with actualword hypotheses might increase the word accuracyoverall.We varied the filler cost parameter from its de-fault, 3, down to its lowest meaningful value, 0.Our experiments show that for fillercost=0, amaximum word accuracy was achieved (given inTable 1), albeit at the cost of more than doublingthe length of the recognizer?s hypothesis by intro-ducing a large amount of fillers (such as ?um?
or?uh?, noises, mumbles etc.).
We observe that usingsuch a low filler cost parameter setting can nega-tively affect some speech features which are can-didates for being used in a scoring model, such as?language model score?.
Therefore we have tocarefully assess whether achieving a higher wordaccuracy is more beneficial to the overall perform-ance of the feature set or whether it has too manynegative effects on some important speech fea-tures.
In future work we will attempt to tune therecognizer in such a way that it is not only opti-mized for a high word accuracy, but also for highaccuracy in filler (and noise) prediction.Word accuracy was computed with the fillersincluded or excluded.
Since fillers are not realwords, and in this round of scoring model devel-opment we did not use any features based on fill-ers, it was reasonable to compute the overall wordaccuracy with the fillers removed from the humanand recognizer transcriptions, resulting in a moder-ate performance gain (see Table 1).4.4 Unsupervised speaker adaptationWe used unsupervised maximum likelihood lin-ear regression (MLLR) AM adaptation on top ofthe previous adaptation and optimization steps (To-mokiyo and Waibel, 2001; Wang et al, 2003).
Inthis step, all words whose confidence score washigher than a pre-set threshold were collected andtheir acoustic information was used to adapt theacoustic model.
All adaptations were done basedon the utterances of a single speaker and pertainedto that speaker only, i.e., it was not incremental orcumulative.
Since a second decoding run is neededafter the actual MLLR adaptations, the recog-nizer?s response time more than doubles when thismethod is employed.
The unsupervised speakeradaptation led to an additional increase of102FeatureNumberFeatureNameFeatureClassDescription  Used in1 hmmscore Pronuncia-tionAcoustic Model score: sum of the log probabilities ofevery frame, normalized for lengthOpinion & Picture2 typesper-secondFluency &VocabularydiversityNumber of unique words in response (?types?)
di-vided by length of responseOpinion & Picture3silences-persecond Fluency  Number of silences per second Opinion & Picture4 repetitions Fluency  Number of repetitions divided by number of words Opinion5 relevance-cos5Vocabulary& ContentCosine word vector product between a response andall responses in the training set that have the highestscore (5 for the Opinion task)Opinion6 relevance-cos3Vocabulary& ContentCosine word vector product between a response andall responses in the training set that have the highestscore (3 for the Picture task)PictureTable 2.
Final features used for the scoring models for the Opinion and Picture tasksapproximately 2% for the Picture and Opiniontasks (see Table 1).
There were large differencesbetween different speakers in terms of the per-formance gain of MLLR adaptation on our dataset, however.
There was also a large variation ofword accuracies between speakers (13-100%).
Thevariation in accuracy across speakers can be due tomany different factors, including the degree of ac-cent, the grammaticality of the response, the voicequality and the recording quality.5 Speech featuresBased on the output of the ASR engine, a featurecomputation module computes a set of about 40features for each response, mostly in the fluencydomain (e.g.
?average silence duration?
), but alsosome features related to pronunciation, vocabularydiversity and content.Instead of using all of these features in a scoringmodel, we used a process of iterative refinementand selection to narrow down the feature set, basedon both the coverage of the concept of communica-tive competence and empirical performance (corre-lations with human scores) of the features.Following this process, five features were selectedto be included in developing the scoring models forthe Opinion task type and four for the Picture tasktype (see Table 2).When we look at the correlations of these fea-tures to the human scores, we find that hmmscore,after being transformed to improve normality, wasthe strongest predictor of human scores for boththe Opinion and Picture tasks with typespersecondas the second strongest (0.5 <= Pearson r <= 0.7).6 Scoring modelsAll the responses were double scored by a ran-domly selected pair of raters who were trained forscoring this test.
The agreements between the tworatings (both kappa and Pearson r correlation) werearound 0.50 for the Picture and 0.72 for the Opin-ion task.
(Note that the fewer points a scale has, thelower correlation we can expect due to less scorevariability, everything else being equal.
)While we use the same training sets for the scor-ing model experiments as for the above ASR ex-periments (sm-train), we add about 600 responseseach to the evaluation sets (these responses wereuntranscribed) to yield a scoring model evaluationset size of about 700 responses each (sm-eval).Scoring models were developed and evaluatedfor the Opinion and Picture task types separately.The Opinion tasks are on a 0-5 point scale whereasthe Picture tasks are on a 0-3 point scale.
Therewere only a handful of 0s on each task and theywere excluded in building the scoring models.For the Opinion tasks, multiple regression mod-els employing different weights for the featureswere developed, namely an Equal Weights model,an Expert Weights model and an Optimal Weightsmodel.
In the Equal Weights model, each featurewas assigned the same weight, indicating that allfeatures are equally important in the prediction.
Inthe Expert Weights model, different weights wereassigned to different features that reflected our un-derstanding of the different roles features play inindicating the overall speech quality.
In the Opti-mal Weights model, weights were determined by103the least squares optimization procedure using thesm-train data.
All features were normalized tohave a mean of 0 and a standard deviation of 1,such that their respective baseline influence on themodel is comparable across features.For the Picture task type, CART was used topredict the score class each response should beassigned to.
CART 5.0 (Steinberg & Colla, 1997)was used to build the classification trees.In addition, generic and task-specific modelswere developed for both task types.
The task-specific models made use of task-specific vocabu-lary features (Features 5 and 6 in Table 2) whichrequired using previous response data to each ofthe tasks within a particular task type.
(Both tasktypes had 4 different tasks each).
The genericmodels, in contrast, used features that were thesame across all tasks for a particular task type anddid not use any task-specific vocabulary features.As it would be much more time-consuming andcostly to build task-specific models, it is worth-while to investigate how much more predictivepower the task-specific vocabulary features couldadd over and beyond the features in the genericmodels.6.1    Opinion task typeFor the Opinion tasks, four features were used inbuilding the generic models and five in developingthe task-specific models.
The following featuresWere used: hmmscore, typespersecond, silences-persecond, repetitions and relevancecos5 (the latteronly in the task-specific model).Table 3 shows the results on the sm-eval set.The Expert Weights model and the OptimalWeights models yielded very similar results(weighted kappa and correlation = 0.61-0.63) if welook at predicted scores that were rounded to thenearest integer.
The agreements between regres-sion model predicted scores and scores of humanrater 1 were just a little below the agreements be-tween two human raters (weighted kappa and cor-relation = 0.72).
However, the results for the EqualWeights model were inferior.The results for the task-specific models showedno improvement over the generic models, suggest-ing that the task-specific vocabulary feature did notcontribute more predictive power beyond the fourfeatures already in the generic models.ModelMultipleRegres-sion(EqualWeights)MultipleRegression(ExpertWeights)MultipleRegres-sion (Op-timalWeights)Weighted ?
0.53 0.62 0.61Pearson rCorrelation(unrounded)0.62 0.68 0.69Pearson rCorrelation(rounded)0.56 0.63 0.63Table 3.
Performance of different weighting schemeson THT scoring model evaluation set for Opiniontasks (generic model)6.2    Picture task typeAs mentioned earlier, the Picture tasks are on a 0-3point scale and we removed a small number of 0-scores from the analyses, making it a 3-point scale.Given this particular score scale, multiple regres-sion may not be appropriate for this data as it re-quires a continuous or a quasi-continuousdependent variable (i.e.
a variable that has at least5 or more data points).
Some classification tech-niques such as CART (Brieman et al, 1984) orlogistic regression, which can take ordered scorecategories as the outcome variable, are bettersuited for this data.
In this study, we analyzed thedata with CART models.CART 5.0 (Steinberg and Colla, 1997) was usedto build the classification trees.
We built two setsof CART models, one set with the task-specificvocabulary feature (relevancecos3) and one setwithout it.
We explored different model configura-tions, i.e., different combinations of priors andsplitting rules.
For each combination, a 10-foldcross-validation was conducted.
Subsequently, theoptimal sub tree that was a relatively small treewith the highest or near-highest agreement with thehuman scores (weighted kappa) on the cross-validation sample was identified.
Then the cases inthe sm-eval data set were dropped down the opti-mal tree to obtain the evaluation results on theheld-out data.The results for the generic model vs. task-specific models are compared in Table 4.
For both104models, CART trees built using the Twoing1 split-ting rule combined with mixed priors (average ofequal priors for different score classes and sm-trainsample priors) yielded the best kappa values on thecross-validation data and were selected as the op-timal trees.
The agreements between the CARTmodel predicted scores and first rater scoresslightly exceeded that between two human raterson the sm-eval data set.
Another observation fromTable 4 was that for this task type, the task-specificCART model did not demonstrate an advantageover the generic model; actually, its performancewas slightly worse than that of the generic model, afinding in line with the Opinion task.Generic Task-specificInter-humanagreementWeighted ?
0.51 0.50 0.49Pearson rCorrelation  0.52 0.50 0.50Table 4.
Performance of CART models on THTscoring model evaluation set for Picture tasks (ge-neric model vs. task-specific model)7 DiscussionThis paper investigates the feasibility of develop-ing an automatic scoring system for the THTSpeaking test, focusing on the particular challengesposed by the design of the test.
The main challengeposed by the test design is the high variability intask types -- ranging from low-entropy Reading-aloud tasks to high-entropy Opinion tasks.
Whileprevious tests of spoken language have focusedmainly on either high or low entropy tasks (Bern-stein, 1999; Zechner and Bejar, 2006), we havemade an attempt at starting to address the wholescale of entropy within a single test.In this paper, we selected one high entropy task(Opinion) and one medium-high entropy task (Pic-ture) to start our explorations.
While we found thatwe could, for the most part, use a similar set offeatures for both tasks, we had to address the dif-ference in score scales between these two tasktypes.
While we could use multiple regression forscoring the 5-point-scale Opinion task, we had to1 The Twoing rule divides the cases into twogroups, gathers similar classes together, and at-tempts to separate the two groups in descendantnodes.employ CART trees for the 3-point-scale Picturetask, demonstrating that one can not necessarilyuse one type of scoring model for all tasks.When moving to low and low-medium entropytasks, we expect further adaptations, both in termsof the feature set (e.g., the higher importance ofpronunciation features in Reading-aloud tasks),and in speech recognition, where more restrictivelanguage models will be needed.We have reported findings associated with theperformance of the scoring models for the Opinionand Picture task types.
Overall, the preliminaryfindings are quite promising: with a few keyspeech features, we were able to achieve predictionaccuracies that could almost emulate or slightlyexceed the agreements between two human ratersat task level.
Once we have developed scoringmodels for all task types, it is conceivable to ag-gregate the task level scores to produce a totalsummary score at the test level and it is very likelywe would see a much stronger association betweenhuman scores and automated scores for the wholetest.The findings also suggest that task-specificmodeling efforts did not seem to be necessary forthe two task types investigated.
This does not pre-clude the possibility, though, that task-specificscoring models are superior for other task types inwhich the expected content is much more restricted(such as the Constrained short-answer questions).8 Conclusions and future workWe have demonstrated that by using a three-stagearchitecture of automatic speech recognition, fea-ture computation, and scoring models, we are ableto achieve some degree of success in generatingautomated scores for two task types of a spokenlanguage test with a wide variation in entropy in itstasks.
The agreement between machine scores andhuman scores comes close to or reaches the inter-human agreement levels for these two tasks.In future work, we will switch our focus to tasktypes that elicit more constrained speech (such asthe Reading-aloud tasks and Constrained short-answer questions).
In the meantime, we will con-tinue to refine and evaluate the preliminary scoringmodels developed in this paper.
In particular, wewill explore cumulative logit models for tasks thatare on a 0-3 point scale and compare the results tothose of CART models.105ReferencesAgresti, A.
(2002).
Categorial data analysis (2nded.).
New York: Wiley.Bachman, L.F. (1990).
Fundamental Considera-tions in Language Testing.
New York: OxfordUniversity Press.Bachman, L. F., and Palmer, A. S. (1996).
Lan-guage testing in practice.
Ox-ford:OxfordUniversity Press.Bernstein, J.
(1999).
PhonePass testing: Structureand construct.
Menlo Park, CA: Ordinate Corpo-ration.Bernstein, J., DeJong, J., Pisoni, D., and Town-shend, B.
(2000).
Two experiments in automaticscoring of spoken language proficiency.
In-STILL2000, Dundee, Scotland.Brieman, L., Jerome F., Olshen, R., and Stone, C.(1984).
Classification and Regression Trees.
Pa-cific Grove: Wadsworth.Cucchiarini, C., Strik, H., & Boves, L. (1997a).Using speech recognition technology to assessforeign speakers' pronunciation of Dutch.
Thirdinternational symposium on the acquisition ofsecond language speech: NEW SOUNDS 97,Klagenfurt, Austria.Cucchiarini, C., Strik, S., and Boves, L. (1997b).Automatic evaluation of Dutch pronunciation byusing speech recognition technology.
IEEEAutomatic Speech Recognition and Understand-ing Workshop, Santa Barbara, CA.Franco, H., Abrash, V., Precoda, K., Bratt, H.,Rao, R., and Butzberger, J.
(2000).
The SRIEduSpeak system: Recognition and pronuncia-tion scoring for language learning.
InSTiLL-2000 (Intelligent Speech Technology in Lan-guage Learning), Dundee, Scotland.Menard, S. (2001).
Applied logistic regressionanalysis.
Sage University Paper Series on Quan-titative Applications in the Social Sciences 07-106, Thousand Oaks, CA: Sage.North, B.
(2000).
The Development of a CommonFramework Scale of Language Proficiency.New York, NY: Peter Lang.Steinberg, D., and Colla, P. (1997).
CART -- Clas-sification and Regression Trees.
San Diego, CA:Salford Systems.Tomokiyo, L. M., and Waibel, A.
(2001).
Adapta-tion methods for non-native speech.
Multilin-guality in Spoken Language Processing,Aalborg.Wang, Z., Schultz, T., and Waibel, A.
(2003).Comparison of acoustic model adaptation tech-niques on non-native speech.
IEEE InternationalConference on Acoustics, Speech, and Signal Proc-essing (ICASSP-2003), Hong Kong, China.Zechner, K., and Bejar, I.
(2006).
Towards Auto-matic Scoring of Non-Native SpontaneousSpeech.
HLT-NAACL-06, New York, NY.Zechner, K., Higgins, D., and Xi, X.
(2007).SpeechRater?
: A Construct-Driven Approach toScore Spontaneous Non-Native Speech.
Pro-ceedings of the 2007 Workshop of the Interna-tional Speech Communication Association(ISCA) Special Interest Group on Speech andLanguage Technology in Education (SLaTE),Farmington, PA, October.106
