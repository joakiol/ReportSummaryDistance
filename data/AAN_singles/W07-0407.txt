Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49?56,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsDiscriminative word alignment by learning the alignment structureand syntactic divergence between a language pairSriram Venkatapathy1Language Technologies ResearchCentre, IIIT -HyderabadHyderabad - 500019, India.sriram@research.iiit.ac.inAravind K. JoshiDepartment of Computer andInformation Science and Institute forResearch in Cognitive Science,University of Pennsylvania, PA, USA.joshi@linc.cis.upenn.eduAbstractDiscriminative approaches for word align-ment have gained popularity in recentyears because of the flexibility that theyoffer for using a large variety of featuresand combining information from varioussources.
But, the models proposed in thepast have not been able to make much useof features that capture the likelihood of analignment structure (the set of alignmentlinks) and the syntactic divergence be-tween sentences in the parallel text.
This isprimarily because of the limitation of theirsearch techniques.
In this paper, we pro-pose a generic discriminative re-rankingapproach for word alignment which allowsus to make use of structural features effec-tively.
These features are particularly use-ful for language pairs with high structuraldivergence (like English-Hindi, English-Japanese).
We have shown that by us-ing the structural features, we have ob-tained a decrease of 2.3% in the absolutevalue of alignment error rate (AER).
Whenwe add the cooccurence probabilities ob-tained from IBM model-4 to our features,we achieved the best AER (50.50) for theEnglish-Hindi parallel corpus.1 IntroductionIn this paper, we propose a discriminative re-ranking approach for word alignment which al-lows us to make use of structural features effec-tively.
The alignment algorithm first generates11Part of the work was done at Institute for Researchin Cognitive Science (IRCS), University of Pennsylvania,Philadelphia, PA 19104, USA, when he was visiting IRCSas a Visiting Scholar, February to December, 2006.a list of k-best alignments using local features.Then it re-ranks this list of k-best alignments us-ing global features which consider the entire align-ment structure (set of alignment links) and the syn-tactic divergence that exists between the sentencepair.
Use of structural information associated withthe alignment can be particularly helpful for lan-guage pairs for which a large amount of unsuper-vised data is not available to measure accuratelythe word cooccurence values but which do have asmall set of supervised data to learn the structureand divergence across the language pair.
We havetested our model on the English-Hindi languagepair.
Here is an example of an alignment betweenEnglish-Hindi which shows the complexity of thealignment task for this language pair.Figure 1: An example of an alignment between anEnglish and a Hindi sentenceTo learn the weights associated with the param-eters used in our model, we have used a learningframework called MIRA (The Margin Infused Re-laxed Algorithm) (McDonald et al, 2005; Cram-mer and Singer, 2003).
This is an online learningalgorithm which looks at one sentence pair at atime and compares the k-best predictions of thealignment algorithm with the gold alignment toupdate the parameter weights appropriately.In the past, popular approaches for doing wordalignment have largely been generative (Och andNey, 2003; Vogel et al, 1996).
In the past cou-ple of years, the discriminative models for doingword alignment have gained popularity because of49the flexibility they offer in using a large variety offeatures and in combining information from vari-ous sources.
(Taskar et al, 2005) cast the problem of align-ment as a maximum weight bipartite matchingproblem, where nodes correspond to the wordsin the two sentences.
The link between a pairof words, (ep,hq) is associated with a score(score(ep,hq)) reflecting the desirability of the ex-istence of the link.
The matching problem issolved by formulating it as a linear programmingproblem.
The parameter estimation is done withinthe framework of large margin estimation by re-ducing the problem to a quadratic program (QP).The main limitation of this work is that the fea-tures considered are local to the alignment linksjoining pairs of words.
The score of an align-ment is the sum of scores of individual alignmentlinks measured independently i.e., it is assumedthat there is no dependence between the align-ment links.
(Lacoste-Julien et al, 2006) extendthe above approach to include features for fertil-ity and first-order correlation between alignmentlinks of consecutive words in the source sentence.They solve this by formulating the problem as aquadratic assignment problem (QAP).
But, eventhis algorithm cannot include more general fea-tures over the entire alignment.
In contrast to theabove two approaches, our approach does not im-pose any constraints on the feature space exceptfor fertility (?1) of words in the source language.In our approach, we model the one-to-one andmany-to-one links between the source sentenceand target sentence.
The many-to-many alignmentlinks are inferred in the post-processing stage us-ing simple generic rules.
Another positive aspectof our approach is the application of MIRA.
It, be-ing an online approach, converges fast and still re-tains the generalizing capability of the large mar-gin approach.
(Moore, 2005) has proposed an approach whichdoes not impose any restrictions on the form ofmodel features.
But, the search technique has cer-tain heuristic procedures dependent on the typesof features used.
For example, there is little vari-ation in the alignment search between the LLR(Log-likelihood ratio) based model and the CLP(Conditional-Link Probability) based model.
LLRand CLP are the word association statistics usedin Moore?s work (Moore, 2005).
In contrast tothe above approach, our search technique is moregeneral.
It achieves this by breaking the searchinto two steps, first by using local features to getthe k-best alignments and then by using struc-tural features to re-rank the list.
Also, by usingall the k-best alignments for updating the parame-ters through MIRA, it is possible to model the en-tire inference algorithm but in Moore?s work, onlythe best alignment is used to update the weightsof parameters.
(Fraser and Marcu, 2006) haveproposed an algorithm for doing word alignmentwhich applies a discriminative step at every iter-ation of the traditional Expectation-Maximizationalgorithm used in IBM models.
This model stillrelies on the generative story and achieves only alimited freedom in choosing the features.
(Blun-som and Cohn, 2006) do word alignment by com-bining features using conditional random fields.Even though their approach allows one to includeoverlapping features while training a discrimina-tive model, it still does not allow us to use fea-tures that capture information of the entire align-ment structure.In Section 2, we describe the alignment searchin detail.
Section 3 describes the features thatwe have considered in our paper.
Section 4 talksabout the Parameter optimization.
In Section 5,we present the results of our experiments.
Section6 contains the conclusion and our proposed futurework.2 Alignment SearchThe goal of the word alignment algorithm is to linkwords in the source language with words in the tar-get language to get the alignments structure.
Thebest alignment structure between a source sen-tence and a target sentence can be predicted byconsidering three kinds of information, (1) Prop-erties of alignment links taken independently, (2)Properties of the entire alignment structure takenas a unit, and (3) The syntactic divergence betweenthe source sentence and the target sentence, giventhe alignment structure.
Using the set of alignmentlinks, the syntactic structure of the source sentenceis first projected onto the target language to ob-serve the divergence.Let ep and hq denote the source and targetwords respectively.
Let n be the number of wordsin source sentence and m be the number of wordsin target sentence.
Let S be the source sentenceand T be the target sentence.502.1 Populate the BeamThe task in this step is to obtain the k-best candi-date alignment structures using the local features.The local features mainly contain the cooccurenceinformation between a source and a target wordand are independent of other alignment links inthe sentence pair.
Let the local feature vector bedenoted as fL(ep, hq).
The score of a particularalignment link is computed by taking a dot prod-uct of the weight vector W with the local featurevector of the alignment link.
More formally, thelocal score of an alignment link isscoreL(ep, hq) = W.fL(ep, hq)The total score of an alignment structure is com-puted by adding the scores of individual alignmentlinks present in the alignment.
Hence, the score ofan alignment structure a?
is,scoreLa(a?, S, T ) =?
(ep,hq)?a?scoreL(ep, hq)We have proposed a dynamic programming al-gorithm of worst case complexity O(nm2 + nk2)to compute the k-best alignments.
First, the localscore of each source word with every target wordis computed and stored in local beams associatedwith the source words.
The local beams corre-sponding to all the source words are sorted and thetop-k alignment links in each beam are retained.This operation has the worst-case complexity ofO(nm2).Now, the goal is to get the k-best alignments inthe global beam.
The global beam initially con-tains no alignments.
The k best alignment links ofthe first source word e0 are added to the globalbeam.
To add the alignment links of the nextsource word to the global beam, the k2 (if k < m)combinations of the alignments in the global beamand alignments links in the local beam are takenand the best k are retained in the global beam.If k > m, then the total combinations taken aremk.
This is repeated till the entries in all the lo-cal beams are considered, the overall worst casecomplexity being O(nk2) (or O(nmk) if k > m).2.2 Reorder the beamWe now have the k-best alignments using the localfeatures from the last step.
We then use global fea-tures to reorder the beam.
The global features lookat the properties of the entire alignment structureinstead of the alignment links locally.Let the global feature vector be represented asfG(a?).
The global score is defined as the dot prod-uct of the weight vector and the global feature vec-tor.scoreG(a?)
= W.fG(a?
)The overall score is calculated by adding the localscore and the global score.score(a?)
= scoreLa(a?)
+ scoreG(a?
)The beam is now sorted based on the overall scoresof each alignment.
The alignment at the top ofthe beam is the best possible alignment betweensource sentence and the target sentence.2.3 Post-processingThe previous two steps produce alignment struc-tures which contain one-to-one and many-to-onelinks.
In this step, the goal is to extend the bestalignment structure obtained in the previous stepto include the other alignments links of one-to-many and many-to-many types.The majority of the links between the sourcesentence and the target sentence are one-to-one.Some of the cases where this is not true are the in-stances of idioms, alignment of verb groups whereauxiliaries do not correspond to each other, thealignment of case-markers etc.
Except for thecases of idioms in target language, most of themany-to-many links between a source and targetsentences can be inferred from the instances ofone-to-one and many-to-one links using three lan-guage language specific rules (Hindi in our case)to handle the above cases.
Figure 1, Figure 2 andFigure 3 depict the three such cases where many-to-many alignments can be inferred.
The align-ments present at the left are those which can bepredicted by our alignment model.
The alignmentson the right side are those which can be inferred inthe post-processing stage......  are  playing ............. khel rahe hain.....  are  playing ............. khel rahe hain(play  cont  be)Figure 2: Inferring the many-to-many alignmentsof verb and auxiliariesAfter applying the language specific rules, thedependency structure of the source sentence is tra-versed to ensure the consistency of the alignment51John  ne  ....John ..........John  ne  ....John ..........Figure 3: Inferring the one-to-many alignment tocase-markers in Hindi... kicked the bucket..........  mara gaya... kicked the bucket..........  mara gaya(die   go?light verb)Figure 4: Inferring many-to-many alignment forsource idiomsstructure.
If there is a dependency link betweentwo source words eo and ep, where eo is the headand ep is the modifier and if eo and ep are linkedto one or more common target word(s), it is log-ical to imagine that the alignment should be ex-tended such that both eo and ep are linked to thesame set of target words.
For example, in Figure 4,new alignment link is first formed between ?kick?and ?gayA?
using the language specific rule, andas ?kick?
and ?bucket?
are both linked to ?mara?,?bucket?
is also now linked to ?gayA?.
Similarity,?the?
is linked to both ?mara?
and ?gayA?.
Hence,the rules are applied by traversing through the de-pendency tree associated with the source sentencewords in depth-first order.
The dependency parserused by us was developed by (Shen, 2006).
Thefollowing summarizes this step,?
Let w be the next word considered in the dependencytree, let pw be the parent of w.?
If w and pw are linked to one or more commonword(s) in target language, align w to all targetwords which are aligned to pw.?
Else, Use the target-specific rules (if they match)to extend the alignments of w.?
Recursively consider all the children of w3 ParametersAs the number of training examples is small, wechose to use features (both local and structural)which are generic.
Some of the features which weused in this experiment are as follows:3.1 Local features (FL)The local features which we consider are mainlyco-occurrence features.
These features estimatethe likelihood of a source word aligning to a tar-get word based on the co-occurrence informationobtained from a large sentence aligned corpora1.3.1.1 DiceWordsDice Coefficient of the source word and the tar-get word (Taskar et al, 2005).DCoeff(ep, hq) = 2 ?
Count(ep, hq)Count(ep) + Count(hq)where Count(ep, hq) is the number of times theword hq was present in the translation of sentencescontaining the word ep in the parallel corpus.3.1.2 DiceRootsDice Coefficient of the lemmatized forms of thesource and target words.
It is important to considerthis feature for language pairs which do not have alarge unsupervised sentence aligned corpora.
Co-occurrence information can be learnt better afterwe lemmatize the words.3.1.3 DictThis feature tests whether there exists a dictio-nary entry from the source word ep to the targetword hq.
For English-Hindi, we used a medium-coverage dictionary (25000 words) available fromIIIT - Hyderabad, India 2.3.1.4 Null POSThese parameters measures the likelihood of asource word with a particular part of speech tag3 tobe aligned to no word (Null) on the target languageside.
This feature was extremely useful becauseit models the cooccurence information of wordswith nulls which is not captured by the featuresDiceWords and DiceRoots.
Here are some of thefeatures of this type with extreme estimated pa-rameter weights.3.2 Lemmatized word pairsThe word pairs themselves are a good indicatorof whether an alignment link exists between theword pair or not.
Also, taking word-pairs as fea-ture helps in the alignment of some of the mostcommon words in both the languages.
A variationof this feature was used by (Moore, 2005) in hispaper.150K sentence pairs originally collected as part of TIDESMT project and later refined at IIIT-Hyderabad, India.2http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict Frame.html3We have limited the number of POS tags by consideringonly the first alphabets of Penn Tags as our POS tag cate-gories52Param.
weight Param.
weightNull ?
0.2737 null C -0.7030Null U 0.1969 null D -0.6914Null L 0.1814 null V -0.6360Null .
0.0383 null N -0.5600Null : 0.0055 null I -0.4839Table 1: Top Five Features each with Maximumand Minimum weightsOther parameters like the relative distance be-tween the source word ep and the target word hq,RelDist(ep, hq) = abs(j/|e| ?
k/|h|), which arementioned as important features in the previousliterature, did not perform well for the English-Hindi language pair.
This is because of the pre-dominant word-order variation between the sen-tences of English and Hindi (Refer Figure 1).3.3 Structural Features (FG)The global features are used to model the prop-erties of the entire alignment structure taken as aunit, between the source and the target sentence.In doing so, we have attempted to exploit the syn-tactic information available on both the source andthe target sides of the corpus.
The syntactic infor-mation on the target side is obtained by projectingthe syntactic information of the source using thealignment links.
Some of the features which wehave used in our work are in the following subsec-tion.3.3.1 OverlapThis feature considers the instances in a sen-tence pair where a source word links to a targetword which is a participant in more than one align-ment links (has a fertility greater than one).
Thisfeature is used to encourage the source words tobe linked to different words in the target language.For example, we would prefer the alignment inFigure 6 when compared to the alignment in Fig-ure 5 even before looking at the actual words.
Thisparameter captures such prior information aboutthe alignment structure.Figure 5: Alignment where many source words arelinked to one target wordFigure 6: Alignment where the source words arealigned to many different target wordsFormally, it is defined asOverlap(a?)
=?hq?T,Fert(hq)>1 Fert2(hq)?h?T Fert(h)where T is the Hindi sentence.
?
Fert2(hq) ismeasured in the numerator so that a more uniformdistribution of target word fertilities be favored incomparison to others.
The weight of overlap asestimated by our model is -6.1306 which indicatesthe alignments having a low overlap value are pre-ferred.3.3.2 NullPercentThis feature measures the percentage of wordsin target language sentence which are not alignedto any word in the source language sentence.
It isdefined asNullPercent =|hq|hq?T,Fertility(hq)==0|h|h?T3.3.3 Direction DepPairThe following feature attempts to capture thefirst order interdependence between the alignmentlinks of pairs of source sentence words which areconnected by dependency relations.
One way inwhich such an interdependence can be measuredis by noting the order of the target sentence wordslinked to the child and parent of a source sentencedependency relation.
Figures 7, 8 and 9 depictthe various possibilities.
The words in the sourcesentence are represented using their part-of-speechtags.
These part-of-speech tags are also projectedonto the target words.
In the figures p is the parentand c is the part-of-speech of the child.p cc pFigure 7: Target word linked to a child precedesthe target word linked to a parent53p cp cFigure 8: Target word linked to a parent precedesthe target word linked to a childp cp cFigure 9: Parent and the child are both linked tosame target wordThe situation in Figure 9 is an indicator that theparent and child dependency pair might be part orwhole of a multi-word expression on the sourceside.
This feature thus captures the divergence be-tween the source sentence dependency structureand the target language dependency structure (in-duced by taking the alignment as a constraint).Hence, in the test data, the alignments which donot express this divergence between the depen-dency trees are penalized.
For example, the align-ment in Figure 10 will be heavily penalized bythe model during re-ranking step primarily for tworeasons, 1) The word aligned to the preposition?of?
does not precede the word aligned to the noun?king?
and 2) The word aligned to the preposition?to?
does not succeed the word aligned to the noun?king?..........
to the king of Rajastan .............  Rajastan  ke   Raja  ko   ..........( Rajastan   of    King   to  )Figure 10: A simple example of an alignmentthat would be penalized by the feature Direc-tion DepPair3.3.4 Direction BigramThis feature is a variation of the previous fea-ture.
In the previous feature, the dependency pairon the source side was projected to the target sideto observe the divergence of the dependency pair.In this feature, we take a bigram instead of a de-pendency pair and observe its order in the targetside.
This feature is equivalent to the first-orderfeatures used in the related work.There are three possibilities here, (1) The wordsof the bigram maintain their order when projectedonto the target words, (2) The words of the bigramare reversed when projected, (3) Both the wordsare linked to the same word of the target sentence.4 Online large margin trainingFor parameter optimization, we have used an on-line large margin algorithm called MIRA (Mc-Donald et al, 2005) (Crammer and Singer, 2003).We will briefly describe the training algorithm thatwe have used.
Our training set is a set of English-Hindi word aligned parallel corpus.
Let the num-ber of sentence pairs in the training data be t. Wehave {Sr, Tr, a?r} for training where r ?
t is theindex number of the sentence pair {Sr, Tr} in thetraining set and a?r is the gold alignment for thepair {Sr, Tr}.
Let W be the weight vector whichhas to be learnt, Wi be the weight vector after theend of ith update.
To avoid over-fitting, W is ob-tained by averaging over all the weight vectors Wi.A generic large margin algorithm is definedfollows for the training instances {Sr, Tr, a?r},Initialize W0, W , ifor p = 1 to Iterations dofor r = 1 to t doGet K-Best predictions ?r = {a1, a2...ak}for the training example (Sr, Tr, a?r)using the current model W i and applyingstep 1 and 2 of section 4.
Compute W i+1by updating W i based on(Sr, Tr, a?r, ?r).i = i + 1W = W + W i+1W = WIterations?mend forend forThe goal of MIRA is to minimize the change inW i such that the score of the gold alignment a?
ex-ceeds the score of each of the predictions in ?
by amargin which is equal to the number of mistakes inthe predictions when compared to the gold align-ment.
One could choose a different loss functionwhich assigns greater penalty for certain kinds ofmistakes when compared to others.Step 4 (Get K-Best predictions) in the algo-54rithm mentioned above can be substituted by thefollowing optimization problem,minimize ?
(W i+1 ?
W i)?s.t.
?k, score(a?r, Sr, Tr)?
score(aq,k, Sr, Tr)>= Mistakes(ak, a?r, Sr, Tr)For optimization of the parameters, ideally, weneed to consider all the possible predictions andassign margin constraints based on every predic-tion.
But, here the number of such classes is ex-ponential and therefore we restrict ourselves to thek ?
best predictions.We estimate the parameters in two steps.
In thefirst step, we estimate only the weights of the lo-cal parameters.
After that, we keep the weightsof local parameters constant and then estimate theweights of global parameters.
It is important todecouple the parameter estimation to two steps.We also experimented estimating the parametersin one stage but as expected, it had an adverseimpact on the parameter weights of local featureswhich resulted in generation of poor k-best list af-ter the first step while testing.5 Experiments and Results5.1 DataWe have used English-Hindi unsupervised data of50000 sentence pairs4.
This data was used to ob-tain the cooccurence statistics such as DiceWordsand DiceRoots which we used in our model.
Thisdata was also used to obtain the predictions ofGIZA++ (Implements the IBM models and theHMM model).
We take the alignments of GIZA++as baseline and evaluate our model for the English-Hindi language pair.The supervised training data which is used toestimate the parameters consists of 4252 sentencepairs.
The development data consists of 100 sen-tence pairs and the test data consists of 100 sen-tence pairs.
This supervised data was obtainedfrom IRCS, University of Pennsylvania.
For train-ing our model, we need to convert the many-to-many alignments in the corpus to one-to-one ormay-to-one alignments.
This is done by applyinginverse operations of those performed during thepost-processing step (section 2.3).4Originally collected as part of TIDES MT project andlater refined at IIIT-Hyderabad, India.5.2 ExperimentsWe first obtain the predictions of GIZA++ to ob-tain the baseline accuracies.
GIZA++ was run infour different modes 1) English to Hindi, 2) Hindito English, 3) English to Hindi where the words inboth the languages are lemmatized and 4) Hindi toEnglish where the words are lemmatized.
We thentake the intersections of the predictions run fromboth the directions (English to Hindi and Hindi toEnglish).
Table 2 contains the results of experi-ments with GIZA++.
As the recall of the align-ment links of the intersection is very low for thisdataset, further refinements of the alignments assuggested by (Och and Ney, 2003) were not per-formed.Mode Prec.
Rec.
F-meas.
AERNormal: Eng-Hin 47.57 40.87 43.96 56.04Normal: Hin-Eng 47.97 38.50 42.72 57.28Normal: Inter.
88.71 27.52 42.01 57.99Lemma.
: Eng-Hin 53.60 44.58 48.67 51.33Lemma.
: Hin-Eng 53.83 42.68 47.61 52.39Lemma.
: Inter.
86.14 32.80 47.51 52.49Table 2: GIZA++ ResultsIn Table 3, we observe that the best result(51.33) is obtained when GIZA++ is run after lem-matizing the words on the both sides of the unsu-pervised corpus.
The best results obtained withoutlemmatizing is 56.04 when GIZA++ is run fromEnglish to Hindi.The table 4 summarizes the results when weused only the local features in our model.Features Prec.
Rec.
F-meas.
AERDiceRoots 41.49 38.71 40.05 59.95+ DiceWords+ Null POS 42.82 38.29 40.43 59.57+ Dict.
43.94 39.30 41.49 58.51+ Word pairs 46.27 41.07 43.52 56.48Table 3: Results using local featuresWe now add the global features.
While esti-mating the parameter weights associated with theglobal features, we keep the weights of local fea-tures constant.
We choose the appropriate beamsize as 50 after testing with several values on thedevelopment set.
We observed that the beam sizes(between 10 and 100) did not affect the alignmenterror rates very much.55Features Prec.
Rec.
F-meas.
AERLocal feats.
46.27 41.07 43.52 56.48Local feats.
48.17 42.76 45.30 54.70+ OverlapLocal feats.
47.93 42.55 45.08 54.92+ Direc.
DeppairLocal feats.
48.31 42.89 45.44 54.56+ Direc.
BigramLocal feats.
48.81 43.31 45.90 54.10+ All Global feats.Table 4: Results after adding global featuresWe see that by adding global features, we ob-tained an absolute increase of about 2.3 AER sug-gesting the usefulness of structural features whichwe considered.
Also, the new AER is much betterthan that obtained by GIZA++ run without lem-matizing the words.We now add the IBM Model-4 parameters (co-occurrence probabilities between source and tar-get words) obtained using GIZA++ and our fea-tures, and observe the results (Table 6).
We cansee that structural features resulted in a significantdecrease in AER.
Also, the AER that we obtainedis slightly better than the best AER obtained by theGIZA++ models.Features Prec.
Rec.
F-meas.
AERIBM Model-4 Pars.
48.85 43.98 46.29 52.71+ LocalFeatsIBM Model-4 Pars.
48.95 50.06 49.50 50.50+ All feats.Table 5: Results after combining IBM model-4 pa-rameters with our features6 Conclusion and Future WorkIn this paper, we have proposed a discriminativere-ranking approach for word alignment which al-lows us to make use of structural features effec-tively.
We have shown that by using the structuralfeatures, we have obtained a decrease of 2.3% inthe absolute value of alignment error rate (AER).When we combine the prediction of IBM model-4with our features, we have achieved an AER whichis slightly better than the best AER of GIZA++for the English-Hindi parallel corpus (a languagepair with significant structural divergences).
Weexpect to get large improvements when we addmore number of relevant local and structural fea-tures.
We also plan to design an appropriate de-pendency based decoder for machine translationto make good use of the parameters estimated byour model.ReferencesPhil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InProceedings of the 21st COLING and 44th AnnualMeeting of the ACL, Sydney, Australia, July.
ACL.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
InJournal of Machine Learning Research.Alexander Fraser and Daniel Marcu.
2006.
Semi-supervised training for statistical word alignment.
InProceedings of the 21st COLING and 44th AnnualMeeting of the ACL, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Simon Lacoste-Julien, Ben Taskar, Dan Klein, andMichael I. Jordan.
2006.
Word alignment viaquadratic assignment.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 112?119, New York City,USA, June.
Association for Computational Linguis-tics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-project dependency pars-ing using spanning tree algorithms.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 523?530, Vancouver,British Columbia, Canada, October.
Association ofComputational Linguistics.Robert C. Moore.
2005.
A discriminative frame-work for bilingual word alignment.
In Proceedingsof Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, pages 81?88, Vancouver, BritishColumbia, Canada, October.
Association of Compu-tational Linguistics.F.
Och and H. Ney.
2003.
A systematic comparisoinof various statistical alignment models.
In Compu-tational Linguistics.Libin Shen.
2006.
Statistical LTAG Parsing.
Ph.D.thesis.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A discriminative machine approach to wordalignment.
In Proceedings of HLT-EMNLP, pages73?80, Vancouver, British Columbia, Canada, Octo-ber.
Association of Computational Linguistics.Stefan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statisticaltranslation.
In Proceedings of the 16th InternationalConference on Computational Linguistics.56
