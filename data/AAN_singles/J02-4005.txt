c?
2002 Association for Computational LinguisticsGenerating Indicative-InformativeSummaries with SumUMHoracio Saggion?
Guy Lapalme?University of Sheffield Universite?
de Montre?alWe present and evaluate SumUM, a text summarization system that takes a raw technical textas input and produces an indicative informative summary.
The indicative part of the summaryidentifies the topics of the document, and the informative part elaborates on some of these topicsaccording to the reader?s interest.
SumUM motivates the topics, describes entities, and definesconcepts.
It is a first step for exploring the issue of dynamic summarization.
This is accomplishedthrough a process of shallow syntactic and semantic analysis, concept identification, and textregeneration.
Our method was developed through the study of a corpus of abstracts writtenby professional abstractors.
Relying on human judgment, we have evaluated indicativeness,informativeness, and text acceptability of the automatic summaries.
The results thus far indicategood performance when compared with other summarization technologies.1.
IntroductionA summary is a condensed version of a source document having a recognizable genreand a very specific purpose: to give the reader an exact and concise idea of the contentsof the source.
In most cases, summaries are written by humans, but nowadays, theoverwhelming quantity of information,1 and the need to access the essential contentof documents accurately in order to satisfy users?
demands calls for the developmentof computer programs able to produce text summaries.
The process of automaticallyproducing a summary from a source text consists of the following steps:1. interpreting the text2.
extracting the relevant information, which ideally includes the ?topics?of the source3.
condensing the extracted information and constructing a summaryrepresentation4.
presenting the summary representation to the reader in natural language.Even though some approaches to text summarization produce acceptable summariesfor specific tasks, it is generally agreed that the problem of coherent selection andexpression of information in text summarization is far from being resolved.
SparckJones and Endres-Niggemeyer (1995) stated the need for a research program in text?
Department of Computer Science, University of Sheffield, Sheffield, England, United Kingdom, S1 4DP.E-mail: saggion@dcs.shef.ac.uk?
De?partement d?Informatique et Recherche Ope?rationnelle, Universite?
de Montre?al, CP 6128, SuccCentre-Ville, Montre?al, Que?bec, Canada, H3C 3J7.
E-mail: lapalme@iro.umontreal.ca1 In 1998, the volume of this information was calculated at somewhere between 400 and 500 milliondocuments (Filman and Pant 1998).498Computational Linguistics Volume 28, Number 4summarization that would study the relation between source document and summary,the different types of summaries and their functions, the development of new methodsand/or combination of already existing techniques for text summarization, and thedevelopment of evaluation procedures for summaries and systems.
Rowley (1982)proposes the following typology of different types of document condensations:?
the extract, which is a set of passages selected from a source documentto represent the whole document?
the summary, which occurs at the end of the document and is arestatement of the salient findings of a work?
the abridgment, which is a reduction of the original document thatnecessarily omits secondary points?
the precis, which stands for the main points of an argument?
the digest, which is a condensation of a book or news article?
the highlight, which is a comment included in specific parts of adocument to alert a reader?
the synopsis, which in cinematography represents a script of a film.In our research, we are concerned only with summaries of technical articles, whichare called abstracts.
In this context, two main types of abstracts are considered (ANSI1979; ERIC 1980; Maizell, Smith, and Singer 1971): indicative abstracts, which point toinformation alerting the reader about the content of an article in a given domain (theseabstracts will contain sentences like ?The work of Consumer Advice Centres is exam-ined.?
), and informative abstracts, which provide as much quantitative or qualitativeinformation contained in the source document as possible (these abstracts will containsentences like ?Consumer Advice Centres have dealt with preshopping advice, edu-cation on consumers?
rights and complaints about goods and services, advising theclient and often obtaining expert assessments.?).
In the course of our research, we havestudied the relation between abstracts and source documents, and as a result, we havedeveloped SumUM (Summarization at Universite?
de Montre?al), a text summarizationsystem that produces an indicative-informative abstract for technical documents.
Theabstracts are produced in two steps: First, the reader is presented with an indicativeabstract that identifies the topics of the document (what the authors present, discuss,etc.).
Then, if the reader is interested in some of the topics, specific information aboutthem from the source document is presented in an informative abstract.Figure 1 shows an automatic abstract produced by our system.
The abstract wasproduced by a process of conceptual identification and text re-generation we call se-lective analysis.
The indicative abstract contains information about the topic of thedocument.
It describes the topics of sections and introduces relevant entities.
The iden-tified topics are terms either appearing in the indicative abstract or obtained from theterms and words of the indicative abstract through a process of term expansion.
Theone particular feature of these terms is that they can be used to obtain more conceptualinformation from the source document, such as definitions or statements of relevance,usefulness, and development, as can be seen in Figure 2.This article is organized as follows.
In the next section, we describe the analysisof a corpus of professional abstracts used to specify selective analysis; conceptual andlinguistic information for the task of summarization of technical texts deduced fromthis corpus is also presented.
An overview of selective analysis and the implementation499Saggion and Lapalme Generating Summaries with SumUMDesigning for human-robot symbiosisPresents the views on the development of intelligent interactive service robots.The authors have observed that a key research issue in service robotics is the inte-gration of humans into the system.
Discusses some of the technologies with par-ticular emphasis on human-robot interaction, and system integration; describeshuman direct local autonomy (HuDL) in greater detail; and also discusses systemintegration and intelligent machine architecture (IMA).
Gives an example imple-mentation; discusses some issues in software development; and also presents thesolution for integration, the IMA.
Shows the mobile robot.Identified Topics: HuDL - IMA - aid systems - architecture - holonic manu-facturing system - human - human-robot interaction - intelligent interactiveservice robots - intelligent machine architecture - intelligent machine software- interaction - key issue - widely used interaction - novel software architecture- overall interaction - robot - second issue - service - service robots - software- system - TechnologiesFigure 1Indicative abstract and identified topics for the text ?Designing for Human-Robot Symbiosis,?D.
M. Wilkes et al, Industrial Robot, 26(1), 1999, 49?58.Development of a service robot is an extremely challenging task.In the IRL, we are using HuDL to guide the development of a cooperative servicerobot team.IMA is a two-level software architecture for rapidly integrating these elements,for an intelligent machine such as a service robot.A holonic manufacturing system is a manufacturing system having autonomousbut cooperative elements called holons (Koestler, 1971).Communication between the robot and the human is a key concern for intelligentservice robotics.Figure 2Informative abstract elaborating some topics.of our experimental prototype, SumUM, is then presented in section 3.
In section 4, wediscuss the limitations of our approach; then, in section 5, we present an evaluation andcomparison of our method with state-of-the art summarization systems and humanabstracts.
Related work on text summarization is discussed in section 6.
Finally, insection 7, we draw our conclusions and discuss prospects for future research.2.
Observations from a CorpusWe have developed our method of text summarization by studying a corpus of profes-sional abstracts and source documents.
Our corpus contains 100 items, each composedof a professional abstract and its source document.
As sources for the abstracts we usedthe journals Library & Information Science Abstracts (LISA), Information Science Abstracts500Computational Linguistics Volume 28, Number 4(ISA), and Computer & Control Abstracts.
The source documents were found in journalsof computer science (CS) and information science (IS), such as AI Communications, AIMagazine, American Libraries, Annals of Library Science & Documentation, Artificial Intelli-gence, Computers in Libraries, and IEEE Expert, among others (a total of 44 publicationswere examined).
The professional abstracts contained three sentences on the average,with a maximum of seven and a minimum of one.
The source documents covered a va-riety of subjects from IS and CS.
We examined 62 documents in CS and 38 in IS, someof them containing author-provided abstracts.
Most of the documents are structured insections; but apart from conceptual sections such as ?Introduction?
and ?Conclusion,?they do not follow any particular style (articles from medicine, for example, usuallyhave a fixed structure like ?Introduction,?
?Method,?
?Statistical Analysis,?
?Result,??Discussion,?
?Previous Work,?
?Limitations,?
?Conclusion,?
but this was not the casein our corpus).
The documents were 7 pages on average, with a minimum of 2 anda maximum of 45.
Neither the abstracts nor the source documents were electronicallyavailable, so the information was collected through photocopies.
Thus we do not haveinformation regarding number of sentences and words in the source document.Our methodological approach consisted of the manual alignment of sentencesfrom the professional abstract with elements of the source document.
This was accom-plished by looking for a match between the information in the professional abstractand the information in the source document.
The structural parts of the source doc-ument we examined were the title of the source document, the author abstract, thefirst section, the last section, the section headings, and the captions of tables and fig-ures.
When the information was not found, we looked in other parts of the sourcedocument.
The information is not always found anywhere in the source document, inwhich case we acknowledge that fact.
This methodological process was established af-ter studying procedures for abstract writing (Cremmins 1982; Rowley 1982) and someinitial observations from our corpus.
One alignment is shown in Table 1.
All align-ments are available for research purposes at the SumUM Web page ?http://www-rali.iro.umontreal.ca/sumum.html?.In this example, the three sentences of the professional abstract were aligned withfour elements of the source document, two in the introduction and two in the author-provided abstract.
The information of the abstract was found ?literally?
in the sourcedocument.
The differences between the sentences of the professional abstract and thoseof the source document are the persons of the verbs (?Presents?
vs. ?We present?
inalignment (1)), the verbs (?were discovered?
vs. ?We found?
in alignment (3)), theimpersonal versus personal styles (?Uses?
vs. ?Our experiment used?
in alignment(2)), and the use of markers in the source document (?In this paper?
in alignment (1)).This example shows that the organization of the abstract does not always mirror theorganization of the source document.2.1 Distributional ResultsThe 309 sentences of the professional abstracts in our corpus were manually alignedwith 568 elements in the source documents.
(We were not able to align six sentencesof the professional abstracts.)
Other studies have already investigated the alignmentbetween sentences in the abstract and sentences in the source document.
Kupiec, Ped-ersen, and Chen (1995) report on the semiautomatic alignment of 79% of sentences ofprofessional abstracts in a corpus of 188 documents with professional abstracts.
Us-ing automatic means, it is difficult to deal with conceptual alignments that appearedin our corpus.
Teufel and Moens (1998) report on a similar work, but this time onthe alignment of sentences from author-provided abstracts.
They use a corpus of 201articles, obtaining only 31% of alignable sentences by automatic means.
No informa-501Saggion and Lapalme Generating Summaries with SumUMTable 1Item of corpus.
Professional abstract: Library & Information Science abstract 3024 and sourcedocument: ?Movement Characteristics Using a Mouse with Tactile and Force Feedback,?International Journal of Human-Computer Studies, 45(5), October 1996, pages 483?493.Ex.
Professional Abstract Source Document Position/Type(1) Presents the results of anempirical study that investigatesthe movement characteristics ofa multi-modal mouse?a mousethat includes tactile and relevancefeedback.In this paper, we present theresults of an empirical studythat investigates the movementcharacteristics of a multi-modalmouse?a mouse that includestactile and force feedback.1st/Intr.
(2) Uses a simple target selectiontask while varying the targetdistance, target size, and thesensory modality.Our experiment used a simpletarget selection task while varyingthe target distance, target size, andthe sensory modality.1st/Intr.
(3) Significant reduction in the overallmovement times and in the timetaken to stop the cursor after en-tering the target were discovered,indicating that modifying a mouseto include tactile feedback, andto a lesser extent, force feedback,offers performance advantages intarget selecting tasks.We found significant reductions inthe overall movement time and inthe time to stop the cursor afterentering the target.
?/Abs.The results indicate that modi-fying a mouse to include tac-tile feedback, and to a lesser ex-tent, force feedback, offers perfor-mance advantages in target selec-tion tasks.
?/Abs.Table 2Distribution of information.Documents With Author Abstract Without Author Abstract Average# % # % # % %Title 10 2 6 2 4 1 2Author abstract 83 15 83 34 20First section 195 34 61 26 134 42 40Last section 18 3 6 2 12 4 4Headlinesand captions 191 33 76 31 115 36 23Other sections 71 13 13 5 58 17 11Total 568 100 245 100 323 100 100tion is given about the distribution of the sentences in structural parts in the sourcedocument.In Table 2, we present the distribution of the sentences in the source documentsthat were aligned with the professional abstracts in our corpus.
We consider all thestructured documents of our corpus (97 documents).
The first three columns containthe information for all documents, for documents with author abstracts, and for docu-ments without author abstracts (the information is given in terms of total elements and502Computational Linguistics Volume 28, Number 4percentage of elements).
We also recorded how the types of information are distributedin the professional abstract.
For each abstract, we computed the ratio of the numberof elements of each type contributing to the abstract to the total number of elementsin the abstract (for example, the abstract in Table 1 contains 50% of first section and50% of author abstract).
The last column gives the average of the information over allabstracts.
In this corpus, we found that 72% of the information for the abstracts comesfrom the following structural parts of the source documents: the title of the document,the first section, the last section, and the section headers and captions of tables andfigures (sum of these entries on the first column of Table 2).
Sharp (1989) reports onexperiments carried out with abstractors in which it is shown that introductions andconclusions provide a basis for producing a coherent and informative abstract.
In factabstractors use a short cut strategy (looking at the introduction and conclusion) priorto looking at the whole paper.
But our results indicate that using just those parts is notenough to produce a good informative abstract.
Important information is also foundin sections other than the introduction and conclusion.
Abstractors not only select theinformation for the abstract because of its particular position in the source document,but they also look for specific types of information that happen to be lexically marked.In Table 1 the information reported is the topic of the document, the method, and theauthor?s discovery.
This information is lexically marked in the source document byexpressions such as we, paper, present, study, experiment, use, find, and indicate.
Basedon these observations we have defined a conceptual and linguistic model for the taskof text summarization of technical articles.2.2 Conceptual Information for Text SummarizationA scientific and technical article is the result of the complex process of scientific in-quiry, which starts with the identification of a problem and ends with its solution.
It isa complex linguistic record of knowledge referring to a variety of real and hypotheticalconcepts and relations.
Some of them are domain dependent (like diseases and treat-ments in medical science; atoms and fusion in physics; and algorithms and proofs incomputer science), whereas others are generic to the technical literature (authors, theresearch article, the problem, the solution, etc.).
We have identified 55 concepts and 39relations that are typical of a technical article and relevant for identifying types of in-formation for text summarization by collecting domain-independent lexical items andlinguistic constructions from the corpus and classifying them using thesauri (Vianna1980; Fellbaum 1998).
We expanded the initial set with other linguistic constructionsnot observed in the corpus.Concepts.
Concepts can be classified in categories referring to the authors (theauthors of the article, their affiliation, researchers, etc.
), the work of the authors (work,study, etc.
), the research activity (current situation, need for research, problem, solu-tion, method, etc.
), the research article (the paper, the paper components, etc.
), theobjectives (objective, focus, etc.
), and the cognitive activities (presentation, introduc-tion, argument, etc.).Relations.
Relations refer to general activities of the author during the researchand writing of the work: studying (investigate, study, etc.
), reporting the work (present,report, etc.
), motivating (objective, focus, etc.
), thinking (interest, opinion, etc.
), andidentifying (define, describe, etc.
).Types of Information.
We have identified 52 types of information for the processof automatic text summarization referring to the following aspects of the technical503Saggion and Lapalme Generating Summaries with SumUMTable 3Conceptual information for text summarization.Domain concepts author, institutions, affiliation, author related, research group, project, researchpaper, others?
paper, study, research, problem, solution, method, result, experiment,need, goal, focus, conclusion, recommendation, summary, researcher, work,hypothesis, research question, future plan, reference, acronym, expansion,structural, title, caption, quantity, mathematical, paper component, date,conceptual goal, conceptual focus, topic, introduction, overview, survey,development, analysis, comparison, discussion, presentation, definition,explanation, suggestion, discovery, situation, advantage, exampleDomain relations make known, show graphical material, study, investigate, summarize, situation,need, experiment, discover, infer, problem, solution, objective, focus, conclude,recommend, create, open, close, interest, explain, opinion, argue, comment, suggest,evidence, relevance, define, describe, elaborate, essential, advantage, use, identifyentity, exemplify, effective, positive, novel, practicalIndicative types topic of document, possible topic, topic of section, conceptual goal, conceptualfocus, author development, development, inference, author interest, interest,author study, study, opening, closing, problem, solution, topic, entity introduction,acronym identification, signaling structure, signaling concept, experiments,methodology, explaining, commenting, giving evidence, need for research,situation, opinion, discovery, demonstration, investigation, suggestion, conclusion,summarizationInformative types relevance, goal, focus, essential, positiveness, usefulness, effectiveness, description,definition, advantage, practicality, novelty, elaboration, exemplification,introduction, identification, developmentarticle: background information (situation, need, problem, etc.
), reporting of informa-tion (presenting entities, topic, subtopics, objectives, etc.
), referring to the work of theauthor (study, investigate, method, hypothesis, etc.
), cognitive activities (argue, infer,conclude, etc.
), and elaboration of the contents (definitions, advantages, etc.
).The complete list of concepts, relations, and types of information is provided inTable 3.
Concepts and relations are the basis for the classification of types of infor-mation referring to the essential contents of a technical abstract.
Nevertheless, thepresence of a single concept or relation in a sentence is not enough to understandthe type of information it conveys.
The co-occurrence of concepts and relations inappropriate linguistic-conceptual patterns is used in our case as the basis for the clas-sification of the sentences.
The types of information are classified as Indicative orInformative depending on the type of abstract to which they will contribute.
For ex-ample, Topic of Document and Topic of Section are indicative, whereas Goal ofEntity and Description of Entity are informative.
Note that we have identified only afew linguistic expressions used to express particular elements of the conceptual model,because we were mainly concerned with the development of a general method of textsummarization and because the task of constructing such linguistic resources is timeconsuming.2.3 From Source to AbstractAccording to Cremmins (1982), the last step in the human production of the sum-mary text is the ?extracting?
into ?abstracting?
step in which the extracted informa-tion will be mentally sorted into a preestablished format and will be ?edited?
usingcognitive techniques.
The editing of the raw material ranges from minor to majoroperations.
Cremmins gives little indication, however, about the process of editing.504Computational Linguistics Volume 28, Number 4Table 4Text editing in human abstracting.Professional Abstract Source DocumentMortality in rats and mice of both sexes wasdose related.There were significant positive associationsbetween the concentrations of the substanceadministered and mortality in rats and miceof both sexes.No treatment related tumors were found inany of the animals.There was no convincing evidence to indicatethat endrin ingestion induced any of thedifferent types of tumors which were foundin the treated animals.Major transformations are those of the complex process of language understandingand production, such as deduction, generalization, and paraphrase.
Some examplesof editing given by Cremmins are shown in Table 4.
In the first example, the con-cept mortality in rats and mice of both sexes is stated with the wording of the sourcedocument; however, the concept expressed by the concentrations of the substance admin-istered is stated with the expression dose.
In the second example, the relation betweenthe tumors and endrin ingestion is expressed through the complex nominal treatmentrelated tumors.In his rules for abstracting, Bernier (1985) states that redundancy, repetition, andcircumlocutions are to be avoided.
He gives a list of linguistic expressions that can besafely removed from extracted sentences or reexpressed in order to gain conciseness.These include expressions such as It was concluded that X, to be replaced by X, and Itappears that, to be replaced by Apparently.
Also, Mathis and Rush (1985) indicate thatsome transformations in the source material are allowed, such as concatenation, trun-cation, phrase deletion, voice transformation, paraphrase, division, and word deletion.Rowley (1982) mentions the inclusion of the lead or topical sentence and the use ofactive voice and advocates conciseness.
But in fact, the issue of editing in text summa-rization has usually been neglected, notable exceptions being the works by Jing andMcKeown (2000) and Mani, Gates, and Bloedorn (1999).
In our work, we partially ad-dress this issue by enumerating some transformations frequently found in our corpusthat are computationally implementable.
The transformations are always conceptualin nature and not textual (they do not operate on the string level), even if some ofthem seem to take the form of simple string deletion or substitution.
The rephras-ing transformations we have identified are outlined below.
We also include for eachtransformation the number and percentage of times the transformation was used toproduce a sentence of the professional abstract.
(Note that the percentages do not addup to 100, as sentences can be involved in more than one operation.
)Syntactic verb transformation: Some verbs from the source document arereexpressed in the abstract, usually in order to make the style impersonal.The person, tense, and voice of the original verb are changed.
Also, verbsthat are used to state the topic of the document are generally expressed inthe present tense (in active or passive voice).
The same applies to verbsintroducing the objective of the research paper or investigation (according toconvention, objectives are reported in the present tense and results in thepast tense).
This transformation was observed 48 times (15%).505Saggion and Lapalme Generating Summaries with SumUMLexical verb transformation: A verb used to introduce a topic is changed andrestated in the impersonal form.
This transformation was observed 13 times(4%).Verb selection: The topic or subtopic of the document is introduced by adomain verb, usually when information from titles is used to create asentence.
This transformation was observed 70 times (21%).Conceptual deletion: Domain concepts such as research paper and author areavoided in the abstract.
This transformation was observed 43 times (13%).Concept reexpression: Domain concepts such as author, research paper, andauthor-related entity are stated in the impersonal form.
This transformationwas observed 4 times (1%).Structural deletion: Discourse markers (contrast, structuring, logicalconsequence, adding, etc.)
such as first, next, finally, however, and although aredeleted.
This transformation was observed 7 times (2%).Clause deletion: One or more clauses (principal or complement) of thesentence are deleted.
This transformation was observed 47 times (14%).Parenthetical deletion: Some parenthetical expressions are eliminated.
Thistransformation was observed 10 times (3%).Acronym expansion: Acronyms introduced for the first time are presentedalong with their expansions, or only the expansion is presented.
Thistransformation was observed 7 times (2%).Abbreviation: A shorter expression (e.g., acronym or anaphoric expression) isused to refer to an entity.
This transformation was observed 3 times (1%).Merge: Information from several parts of the source document are mergedinto a single sentence.
This is the usual case when reporting entities stated intitles and captions.
This transformation was observed 124 times (38%).Split: Information from one sentence of the source document is presented inseparate sentences in the abstract.
This transformation was observed 3 times(1%).Complex reformulation: A complex reformulation takes place.
This couldinvolve several cognitive processes, such as generalization and paraphrase.This transformation was observed 75 times (23%).Noun transformations: Other transformations take place, such asnominalization, generalization, restatement of complex nominals, deletion ofcomplex nominals, expansion of complex nominals (different classes ofaggregation), and change of initial uppercase to lowercase (e.g., when wordsfrom titles or headlines, usually in upper initial, are used for the summary).This transformation was observed 70 times (21%).No transformation: The information is reported as in the source.
Thistransformation was observed 35 times (11%).We found that transformations involving domain verbs appeared in 40% of thesentences, noun editing occurred in 38% of the sentences, discourse level editing oc-curred in 19% of the sentences, merging and splitting of information occurred in 38%of the sentences, complex reformulation accounts for 23% of the sentences, and finally,506Computational Linguistics Volume 28, Number 4only 11% of the information from the source document is stated without transfor-mation.
Although most approaches to automatic text summarization present the ex-tracted information in both the order and the form of the original, this is not the casein human-produced abstracts.
Nevertheless, some transformations in the source doc-ument could be implemented by computers with state-of-the-art techniques in naturallanguage processing in order to improve the quality of the automatic summaries.In this section, we have studied relations between abstracts and their source doc-uments.
This study was motivated by the need to answer to the question of contentselection in text summarization (Sparck Jones 1993).
We have also addressed here an-other important research question: how the information is expressed in the summary.Our study was based on the manual construction of alignments between sentencesof professional abstracts and elements of source documents.
In order to obtain an ap-propriate coverage, abstracts from different secondary sources and source documentsfrom different journals were used.
We have shown that more than 70% of the informa-tion for abstracts comes from the introduction, conclusion, titles, and captioning of thesource document.
This is an empirical verification of what is generally acknowledgedin practical abstract writing in professional settings.
We have also identified 15 types oftransformation usually applied to the source document in order to produce a coherentpiece of text.
Of the sentences of our corpus, 89% have been edited.
In section 3.1, wedetail the specification of patterns of sentence and text production inspired from ourcorpus study that were implemented in our automatic system.Although the linguistic information for our model has been manually collected,Teufel (1998) has shown how this labor-intensive task can be accomplished in a semi-automatic fashion.
The analysis presented here and the idea of the alignments havebeen greatly influenced by the exploration of abstracting manuals (Cremmins 1982).Our conceptual model comes mainly from the empirical analysis of the corpus buthas also been influenced by work on discourse modeling (Liddy 1991) and in the phi-losophy of science (Bunge 1967).
It is interesting to note that our concerns regardingthe presentation and editing of the information for text summarization are now be-ing addressed by other researchers as well.
Jing and McKeown (2000) and Jing (2000)propose a cut-and-paste strategy as a computational process of automatic abstractingand a sentence reduction strategy to produce concise sentences.
They have identifiedsix ?editing?
operations in human abstracting that are a subset of the transformationfound in our study.
Jing and McKeown?s work on sentence reduction will be dis-cussed in section 6.
Knight and Marcu (2000) propose a noisy-channel model and adecision-based model for sentence reduction also aiming at conciseness.3.
Selective Analysis and Its ImplementationSelective analysis is a method for text summarization of technical articles whose designis based on the study of the corpus described in section 2.
The method emphasizesthe selection of particular types of information and its elaboration, exploring the is-sue of dynamic summarization.
It is independent of any particular implementation.Nevertheless, its design was motivated by actual needs for accessing the content oflong documents and the current limitations of natural language processing of domain-independent texts.
Selective analysis is composed of four main steps, which are brieflymotivated here and fully explained in the rest of the section.?
Indicative selection: The function of indicative selection is to identifypotential topics of the document and to instantiate a set of indicativetemplates.
These templates are instantiated with sentences matching507Saggion and Lapalme Generating Summaries with SumUMspecific patterns.
A subset of templates is retained based on a matchingprocess between terms from titles and terms from the indicativetemplates.
From the selected templates, terms are extracted for furtheranalysis (i.e., potential topics).?
Informative selection: The information selection process determines thesubset of topics computed by the indicative selection that can beinformatively expanded according to the interest of the reader.
Thisprocess considers sentences in which informative markers andinteresting topics co-occur and instantiates a set of informative templatesthat elaborate the topics.?
Indicative generation: In indicative generation, the set of templatesdetected by the indicative selection are first sorted using a preestablishedconceptual order.
Then, the templates are used to generate sentencesaccording to the style observed in the corpus of professional abstracts(i.e., verbs in the impersonal and reformulation of some domainconcepts).
When possible, information from different templates isintegrated in order to produce a single sentence.
A list of topics is alsopresented to the reader.?
Informative generation: In informative generation, the reader selects someof the topics presented as a result of indicative generation, therebyasking for more information about those topics.
Templates instantiatedby the informative selection associated with the selected topics are usedto present additional information to the reader.Whereas the indicative abstract depends on the structure, content, and to some ex-tent, on specific types of information generally reported in this kind of summary, theinformative abstract relies on the interests of the reader to determine the topics toexpand.3.1 Implementing SumUMThe architecture of SumUM is depicted in Figure 3.
Our approach to text summariza-tion is based on a superficial analysis of the source document to extract appropriatetypes of information and on the implementation of some text regeneration techniques.SumUM has been implemented in SICStus Prolog (release 3.7.1) (SICStus 1998) andPerl (Wall, Christiansen, and Schwartz 1996) running on Sun workstations (5.6) andLinux machines (RH 6.0).
For a complete description of the system and its implemen-tation, the reader is referred to Saggion (2000).The sources of information we use for implementing our system are a POS tag-ger (Foster 1991); linguistic and conceptual patterns specified by regular expressionscombining POS tags, our syntactic categories, domain concepts, and words; and aconceptual dictionary that implements our conceptual model (241 domain verbs, 163domain nouns, and 129 adjectives); see Table 5.3.1.1 Preprocessing and Interpretation.
The input article (plain ASCII text in Englishwithout markup) is segmented in main units (title, author information, main sectionsand references) using typographic information (i.e., nonblank lines ending with a char-acter different from punctuation surrounded by blank lines) and some keywords like?Introduction?
and ?References.?
Each unit is passed through the statistical tagger(based on bigrams).
A scanning process reads each element of the tagged files and508Computational Linguistics Volume 28, Number 4DATABASEINDICATIVEINFORMATIVE DATABASEINFORMATIVE ABSTRACTTOPICSINDICATIVEINDICATIVE ABSTRACTSELECTED TOPICSGENERATIONGENERATIONUSERINDICATIVESELECTIONINFORMATIVEINFORMATIVESELECTIONRAW TEXTPREPROCESSINGINTERPRETATIONPOTENTIAL TOPICS INDICATIVE CONTENTTEXT REPRESENTATIONCONCEPTUAL INDEX TOPICAL STRUCTURETERM TREEACRONYMINFORMATIONCONCEPTUAL DICTIONARYFigure 3SumUM architecture.transforms sequences of tagged words into lists of elements, each element being alist of attribute-value pairs.
For instance, the word systems, which is a common nounis represented with the following attributes (cat,?NomC?
), (Nbr,plur), (canon,system)in addition to the original word.
The frequency of each noun (proper or common) isalso computed.
SumUM gradually determines the paragraph structure of the docu-ment, relying on end of paragraph markers.
Sentences are interpreted using finite-statetransducers we developed (implementing 334 linguistic and domain-specific patterns)and the conceptual dictionary.
The interpretation process produces a partial represen-tation that consists of the sentence position (section and sentence numbers) and a listof syntactic constituents annotated with conceptual information.
As title and sectionheaders are recognized by position (i.e., sentence number 0 of the section), only noungroup identification is carried out in those components.
Each sentence constituent isrepresented by a list of attribute-value pairs.
The parse of each element is as follows:?
Noun group parsing.
We identify only nonrecursive, base noun groups.The parse of a noun group contains information about the originalstring, the canonical or citation form, syntactic features, the semantics509Saggion and Lapalme Generating Summaries with SumUMTable 5Overview of the conceptual dictionary.Concept/Relation Lexical Itemmake known cover, describe, examine, explore, present, report, overview, outline, .
.
.create create, construct, ideate, develop, design, implement, produce, project,.
.
.study investigate, compare, analyze, measure, study, estimate, contrast, .
.
.interest address, interest, concern, matter, worry, .
.
.infer demonstrate, infer, deduce, show, conclude, draw, indicate, .
.
.identify entity include, classify, call, contain, categorize, divide, .
.
.paper paper, article, report, .
.
.paper component section, subsection, appendix, .
.
.structural figure, table, picture, graphic, .
.
.problem complexity, intricacy, problem, difficulty, lack, .
.
.goal goal, objective, .
.
.result finding, result, .
.
.important important, relevant, outstanding, .
.
.necessary needed, necessary, indispensable, mandatory, vital, .
.
.novelty innovative, new, novel, original, .
.
.
(i.e., the head of the group in citation form), adjectives, and informationreferring to the conceptual model that is optional.?
Verb group parsing.
The parse of a verb group contains information aboutthe original string, the semantics (i.e., the head of the group in citationform), the syntactic features, information about adverbs, and theconceptual information that is optional.?
Adjectives and adverbials.
The parse of adjectival and adverbial groupscontains the original string, the citation form, and the optionalinformation from the conceptual model.?
Other.
The rest of the elements (i.e., conjunctions, prepositions, etc.)
areleft unanalyzed.In order to assess the accuracy of the parsing process, we manually extracted basenoun groups and base verb groups from a set of 42 abstracts found on the INSPEC(2000) service (about 5,000 words).
Then, we parsed the abstracts and automaticallyextracted noun groups and verb groups with our finite-state machinery and computedrecall and precision measures.
Recall measures the ratio of the number of correctsyntactic constructions identified by the algorithm to the number of correct syntacticconstructions.
Precision is the ratio of the number of correct syntactic constructionsidentified by the algorithm to the total number of constructions identified by thealgorithm.
We found the parser to perform at 86% recall and 86% precision for noungroups and 85% recall and 76% precision for verb groups.Term extraction.
Terms are constructed from the citation form of noun groups.They are extracted from sentences and stored along with their semantics and positionin the term tree, an AVL tree structure for efficient access from the SICStus Prologassociation lists package.
As each term is extracted from a sentence, its frequency isupdated.
We also build a conceptual index that specifies the types of information ofeach sentence using the concepts and relations identified before.
Finally, terms and510Computational Linguistics Volume 28, Number 4Table 6Specification of indicative templates for the topic of the document and the topic of a section.Type: topicId: integer identifierPredicate: instance of make knownWhere: instance of {research paper, study, work, research}Who: instance of {research paper, author, study, work, research}What: parsed sentence fragmentPosition: section and sentence idTopic candidates: list of terms from the What fillerWeight: numberType: sec descId: integer identifierPredicate: instance of make knownSection: instance of section(Id)Argument: parsed sentence fragmentPosition: section and sentence idTopic candidates: list of terms from the Argument fillerWeight: numberwords are extracted from titles (identified as those sentences with numeral 0 in therepresentation) and stored in a list, the topical structure, and acronyms and theirexpansions are identified and recorded.3.1.2 Indicative Selection.
Simple templates are used to represent the types of informa-tion.
We have implemented 21 indicative templates in this version of SumUM.
Table 6presents two of these indicative templates and their slots.
The slot Topic candidates isfilled with terms and acronym expansions.
Term relevance is the total frequency of allnominal components of the term divided by the total number of nominal components.It is computed using the following formula:relevance(Term) =?{N?Term?
noun(N)} noun frequency(N)|N : N ?
Term ?
noun(N)|where noun(N) is true if N is a noun, noun frequency(N) is a function computed duringpreprocessing and interpretation that gives the word count for noun N, and the nota-tion |S| stands for the number of elements in the set S. As complex terms have lowerdistribution than single terms, this formula gives us an estimate of the distribution ofthe term and its components in the document.
In doing so, a low-frequency term likerobot architecture is assigned a high degree of relevance because chances are that robotand architecture occur frequently on their own.
Other techniques exist for boosting thescore of longer phrases, such as adjusting the score of the phrase by a fixed factor thatdepends on the length of the phrase (Turney 1999).
The Weight slot is filled in withthe sum of the relevance of the terms on the Topic candidates slot.For determining the content of the indicative abstract, SumUM considers onlysentences that have been identified as carrying indicative information; excludes sen-tences containing problematic anaphoric references (?the first.
.
.
,?
?the previous.
.
.
,??that.
.
.
,?
quantifiers in sentence initial, etc.
), those that are not domain concepts (e.g.,?These results,?
?The first section,?
etc.
), and some connectives (?although,?
?how-ever,?
etc.
); and checks whether the sentence matches an indicative pattern.
Indicative511Saggion and Lapalme Generating Summaries with SumUMTable 7Indicative pattern specification and sentence fragments matching the patterns (in parentheses).Signaling structural SKIP1 + GN + Prep + GN + show graphical material + Prep + structural (In ourcase, the architecture of the self-tuner is shown in Figure 3 Auto-tuning.
.
.
)Topic SKIP1 + research paper + SKIP2 + author + make known + ARGUMENT (In thisarticle, we overview the main techniques used in order.
.
.
)Author?s Goal SKIP + conceptual goal + SKIP + define + GOAL (Our goals within the HMS projectare to develop a holonic architecture for.
.
.
)Signaling concept SKIP + development + Prep + GN (Implementation of industrial robots)Section Topic paper component + make known + ARGUMENT + ConC + paper component(Section 2 describes HuDL in greater detail and Section 3. .
.
)Problem/Solution SKIP + solution (dr) + problem (The proposed methodology overcomes the problemscaused by.
.
.
)Introduce Entity GN + define + SKIP (Rapid Prototyping (RP) is a technique.
.
.
)patterns contain variables, syntactic constructions, domain concepts, and relations.One hundred seventy-four indicative patterns have been implemented; some of themare shown in Table 7.For each matched pattern, SumUM verifies some restrictions, such as verb tensesand voice, extracts information from pattern variables, and instantiates a template ofthe appropriate type.
All the instantiated templates constitute the indicative database(IDB).
SumUM matches each element of the topical structure with the terms of theTopic candidate slots of templates in the IDB.
Two terms Term1 and Term2 match ifTerm1 is a substring of Term2 or if Term2 is a substring of Term1 (e.g., robotic fruit harvestermatches harvester).Then, SumUM selects the template with the greatest Weight.
In case of conflict,types are selected following the precedence given in Table 8.
This order gives prefer-ence to explicit topical information more usually found in indicative abstracts.
Wherethere is conflict, the Position and the Id slots are used to decide: If two topic tem-plates have the same Weight, the template with position closer to the beginning of thedocument is selected, and if they are still equal, the template with lower Id is used.SumUM prioritizes topical information by selecting the topical template with greatestweight.
The selected templates constitute the indicative content (IC), and the termsand words appearing in the Topic candidate slots and their expansions constitute thepotential topics (PTs) of the document.
Expansions are obtained by looking for termsin the term tree sharing the semantics of any term in the IC.3.1.3 Informative Selection.
For each potential topic PT and sentence in which itappears, SumUM checks whether the sentence contains an informative marker andmatches a dynamic informative pattern.
Dynamic patterns include a TOPIC slot in-stantiated with the PT before trying a match.
They also include concepts, relations,and linguistic information.
Eighty-seven informative patterns have been implemented,Table 8Precedence for content selection.Topic of Document > Topic of Section > Topic Description > Possible Topic > AuthorStudy > Author Development > Author Interest > Conceptual Goal, Research Goal> Conceptual Focus, Focus > Entity Introduction > Entity Identification > SignalingStructural, Signaling Concepts > Other Indicative Types512Computational Linguistics Volume 28, Number 4Table 9Informative pattern specification and sentence fragments matching the patterns (inparentheses).Definition SKIP + TOPIC + define + GN (The RIMHO walking robot is a prototype developedwith the aim of.
.
.
)Description SKIP + TOPIC + describe (The hardware of the MMI consists of a main pendant (MP),an operator pendant.
.
.
)Use SKIP + use + TOPIC (To realize the control using an industrial robot, such as.
.
.
)Advantage SKIP + advantage + Prep + TOPIC (The biggest advantage of SWERS is the easier andfaster.
.
.
)Effectiveness SKIP + TOPIC + define + effective (The system is effective in the task of.
.
.
)some of which are presented in Table 9.
If a sentence satisfies an informative pattern,the PT is considered a topic of the document, and an informative template is instan-tiated with the sentence.
The informative templates contain a Content slot to recordthe information from the sentence, a Topic slot to record the topic, and a Positionslot to record positional information.
Examples are presented in Tables 10 and 11.
Thetemplates obtained by this process constitute the Informative Data Base (InfoDB), andthe topics are the terms appearing in the slot Topic of the templates in the InfoDB.23.1.4 Generation.
The process of generation consists of the arrangement of the infor-mation in a preestablished conceptual order, the merging of some types of information,and the reformulation of the information in one text paragraph.
The IC is sorted us-ing positional information and the order presented in Table 12, which is typical oftechnical articles.SumUM merges groups of up to three templates of type Topic of Document toproduce more complex sentences (Merge transformation).
The same is done for tem-plates of type Topic of Section, Signaling Concept, and Signaling Structural.
Thetemplate Signaling Concept contains information about concepts found on sectionheadings; SumUM selects an appropriate verb to introduce that information in theabstract (Verb Selection).
In this way, for example, given the section heading ?Ex-perimental Results,?
SumUM is able to produce the sentence ?Presents experimentalresults.
?The sorted templates constitute the text plan.
Each element in the text plan is usedto produce a sentence the structure of which depends on the template.
The schema ofpresentation of a text plan composed of n(?
1) templates Tmpli is as follows:Text =n?i=1[Tmpli ?
?.?
].The notation A?
means the string produced by the generation of A, ?
denotes con-catenation, and?ni=1 Ai stands for the concatenation of all Ai.
We assume that allthe parameters necessary for the generation are available (i.e., voice, tense, number,position, etc.
).The schema of presentation of a template Tmpl of type Topic of the Document is:3Tmpl = Tmpl.Predicate ?
Tmpl.What2 TOPIC = {Term : ?Template ?
InfoDB ?
Template.Topic = Term}.3 The notation Tmpl.Slot denotes the content of slot Slot of template Tmpl.513Saggion and Lapalme Generating Summaries with SumUMTable 10Specification of the templates for the description and definition of a topic.Type: descriptionId: integer identifierTopic: termPredicate: instance of describe (i.e., X is composed of Y)Content: parsed sentence fragmentPosition: section and sentence idType: definitionId: integer identifierTopic: termPredicate: instance of define (i.e., X is a Y)Content: parsed sentence fragmentPosition: section and sentence idTable 11Definition template instantiated with sentence ?REVERSA is a dual viewpoint noncontactlaser scanner which comes complete with scanning software and data manipulation tools.
?Type: definitionId: 41Topic: REVERSAPredicate: be, .
.
.Content: REVERSA is a dual viewpoint noncontact laser scanner which.
.
.Position: Sentence 1 from Section 2The predicate is generated in the present tense of the third-person singular (SyntacticVerb Transformation).
So sentences like ?X will be presented?
or ?X have been pre-sented?
or ?We have presented here X,?
which are usually found in source documents,will be avoided because they are awkward in an abstract.
Arguments are generated bya procedure that expands/abbreviates acronyms (Acronym Expansion and Abbrevi-ation), presents author-related entities in the impersonal form (concept reexpression),uses fixed expressions in order to refer to the authors and the research paper, andproduces correct case and punctuation.
Examples of sentences generated by the sys-tem have been presented in Saggion and Lapalme (2000a).
In this way we implementsome of the transformations studied in section 2.3.
The schema of presentation of theTable 12Conceptual order for content expression.Problem Solution, Problem Identification, Need and Situation in positional orderTopic of Document sorted in descending order of WeightPossible Topic sorted in descending order of WeightTopic Description, Study, Interest, Development, Entity Introduction, Research Goal,Conceptual Goal, Conceptual Focus and Focus in positional orderMethod and Experiment in positional orderResults, Inference, Knowledge and Summarization in positional orderEntity Identification in positional orderTopic of Section in section orderSignaling Structural and Signaling Concepts in positional order514Computational Linguistics Volume 28, Number 4Topic of Section isTmpl = Tmpl.Predicate ?
Tmpl.Argument.The schema of generation of a merged template Tmpl isTmpl =(n?1?i=1[Tmpl.Templatesi ?
?;?])?
?and also?
?
Tmpl.Templatesn,where Tmpl.Templatesi is the ith template in the merge.
Note that if n adjacent templatesin the merge share the same predicate, then only one verb is generated, and thearguments are presented as a conjunction (i.e., ?Presents X and Y.?
instead of ?PresentsX and presents Y.?).
This is specified with the following schema:Tmpl = Predicate ?
Tmpl1.Arg ?
(n?1?i=2[Tmpli.Arg ?
?;?])?
?and?
?
Tmpln.Arg,where Predicate is the predicate common to the merged templates.The indicative abstract is presented along with the list of topics that are obtainedfrom the list Topics.
SumUM presents in alphabetical order the first superficial occur-rence of the term in the source document (this information is found in the term tree).For the informative abstract, the system retrieves from the InfoDB those templatesmatching the topics selected by the user (using the slot Topic for that purpose) andpresents the information on the Content slots in the order of the original text (usingthe Position for that purpose).4.
Limitations of the ApproachOur approach is based on the empirical examination of abstracts published by sec-ond services and on assumptions about technical text organization (Paice 1991; Bhatia1993; Jordan 1993, 1996).
In our first study, we examined 100 abstracts and sourcedocuments in order to deduce a conceptual and linguistic model for the task of sum-marization of technical articles.
Then we expanded the corpus with 100 more itemsin order to validate the model.
We believe that the concepts, relations, and types ofinformation identified account for interesting phenomena appearing in the corpus andconstitute a sound basis for text summarization.
The conceptual information has notbeen formalized in ontological form, opening an avenue for future developments.
Allthe knowledge of the system (syntactic and conceptual) was manually acquired dur-ing specification, implementation, and testing.
The coverage and completeness of themodel have not been assessed in this work and will be the subject of future studies.Nevertheless SumUM has been tested in different technical domains.The implementation of our method relies on noun and verb group identification,conceptual tagging, pattern matching, and template instantiation we have developedfor the purpose of this research.
The interpreter relies on the output produced by a shal-low text segmenter and on a statistical part-of-speech tagger.
Our prototype analyzessentences for the specific purpose of text summarization and implements some pat-terns of generation observed in the corpus, including the reformulation of verb groupsand noun groups, sentence combination or fusion, and conceptual deletion, amongothers.
We have not addressed here the question of text understanding: SumUM isable to produce text summaries, but it is not able to demonstrate intelligent behavior515Saggion and Lapalme Generating Summaries with SumUM(answering questions, paraphrasing, anaphora resolution, etc.).
Concerning the prob-lem of text coherence, we have not properly addressed the problem of identification ofanaphoric expressions in technical documents: SumUM excludes from the content ofthe indicative abstract sentences containing expressions considered problematic.
Theproblem of anaphoric expressions in technical articles has been extensively addressedin research work carried out under the British Library Automatic Abstracting Project(BLAB) (Johnson et al 1993; Paice et al 1994).
Although some of the exclusion rulesimplemented in the BLAB project are considered in SumUM (exclusion of sentenceswith quantifier subject, sentences with demonstratives, some initial connectives, andpronouns), our approach lacks coverage of some important cases dealt with in theBLAB rules, such as the inclusion of sentences because of dangling anaphora.This implementation of SumUM ignores some aspects of the structure of textlikelists and enumerations, and most of the process overlooks the information about para-graph structure.
Nevertheless, in future improvements of SumUM, these will be takeninto consideration to produce better results.5.
Evaluating the SummariesAbstracts are texts used in tasks such as assessing the content of a source documentand deciding if it is worth reading.
If text summarization systems are designed tofulfill the requirements of those tasks, the quality of the generated texts has to be eval-uated according to their intended function.
The quality of human-produced abstractshas been examined in the literature (Grant 1992; Kaplan et al 1994; Gibson 1993),using linguistic criteria such as cohesion and coherence, thematic structure, sentencestructure, and lexical density; in automatic text summarization, however, such detailedanalysis is only just emerging.
Content evaluation assesses whether an automatic sys-tem is able to identify the intended ?topics?
of the source document.
Text qualityevaluation assesses the readability, grammar, and coherence of a summary.
The eval-uations can be made in intrinsic or extrinsic fashions as defined by Sparck Jones andGalliers (1995).An intrinsic evaluation measures the quality of the summary itself by compar-ing the summary with the source document, by measuring how many ?main?
ideasof the source document are covered by the abstract, or by comparing the content ofthe automatic summary with an ideal abstract (gold standard) produced by a human(Mariani 1995).
An extrinsic evaluation measures how helpful a summary is in thecompletion of a given task.
For example, given a document that contains the answersto some predefined questions, readers are asked to answer those questions using thedocument?s abstract.
If the reader correctly answers the questions, the abstract is con-sidered of good quality for the given question-answering task.
Variables measured canbe the number of correct answers and the time to complete the task.
Recent experi-ments (Jing et al 1998) have shown how different parameters such as the length ofthe abstract can affect the outcome of the evaluation.5.1 Evaluation of Indicative Content and Text QualityOur objective in the evaluation of indicative content is to see whether the abstractsproduced by our method convey the essential content of the source documents inorder to help readers complete a categorization task.
In the evaluation of text quality,we want to determine whether the abstracts produced by our method are acceptableaccording to a number of acceptability criteria.516Computational Linguistics Volume 28, Number 45.1.1 Design.
In both evaluations we are interested in comparing our summaries withsummaries produced using other methodologies, including human-written ones.
In or-der to evaluate the content, we presented evaluators with abstracts and five descriptors(lists of keywords) for each abstract.
The evaluators had to find the correct descrip-tor for the abstract.
One of the descriptors was the correct descriptor of the abstractand the others were descriptors from the same domain, obtained from the journalsin which the source documents were published.
In order to evaluate text quality, weasked the evaluators to provide an acceptability score between 0?5 for the abstract(0 for unacceptable and 5 for acceptable) based on the following criteria taken fromRowley (1982): good spelling and grammar, clear indication of the topic of the sourcedocument, conciseness, readability and understandability, and whether acronyms arepresented along with their expansions.
We told the evaluators that we would considerthe abstracts with scores above 2.5 acceptable; with this information, they could usescores below or above that borderline to enforce acceptability.
The design of this ex-periment was validated by three IS specialists.
The experiment was run three timeswith different data each time and with a different set of summarizers (human or auto-matic).
When we first designed this experiment, only one text summarization systemwas available to us, so we performed the experiment comparing automatic abstractsproduced by two summarizers and abstracts published with the source documents.Later on, we found two other summarizers, and we decided to repeat the experimentonly considering three automatic systems.Our evaluation mirrors the TIPSTER SUMMAC categorization task (Firmin andChrzanowski 1999; Mani et al 1998) in which given a generic summary (or a fulldocument), the human participant chooses a single category (out of five categories)to which the document is relevant.
The evaluation seeks to determine whether thesummary is effective in capturing whatever information in the document is neededto correctly categorize the document.
In the TIPSTER SUMMAC evaluation, 10 TextRetrieval Conference (TREC) topics and 100 documents per topic were used, and16 systems participated.
The results for TREC indicate that there are no significantdifferences among the systems for the categorization task and that the performanceusing the full document is not much better.5.1.2 Subjects and Materials.
All our evaluators were IS students/staff from Uni-versite?
de Montre?al, McGill University, and John Abbott College.
They were chosenbecause they have knowledge about what constitutes a good indicative abstract.
Weused the Latin square experimental design, whereby forms included n abstracts fromn different documents, where n depends on the number of subjects (thus an evaluatornever compared different summaries of the same document).
Each abstract was printedon a different page including the five descriptors, a field to be completed with the qual-ity score associated with the abstract, and a field to be filled with comments about theabstract.
In order to produce the evaluation forms, we used source documents (all tech-nical articles) from the journal Industrial Robots, found in the Emerald Electronic Library?http://www.emerald-library.com?.
In addition to the abstracts published with sourcedocuments, we produced automatic abstracts using the following systems: SumUM,Microsoft?97 Autosummarize, Extractor, and n-STEIN.
Microsoft?97 Autosummarize isdistributed with Word?97.
Extractor (Turney 1999) is a system that takes a text fileas input (plain ASCII text, HTML, or e-mail) and generates a list of keywords andkeyphrases as output.
On average, it generates the number of phrases requested bythe user, but the actual number for any given document may be slightly below or abovethe requested number, depending mainly on the length of the input document.
Ex-tractor has 12 parameters relevant for keyphrase extraction that are tuned by a genetic517Saggion and Lapalme Generating Summaries with SumUMalgorithm to maximize performance on training data.
We used Extractor 5.1, whichis distributed for demonstration (downloaded from ?http://extractor.iit.nrc.ca/?).
n-STEIN is a commercial system that was available for demonstration purposes at thetime we were conducting our research (n-STEIN 2000) (January 2000).
The systemis based on a combination of statistical and linguistic processing.
Unfortunately notechnical details of the system are given.5.1.3 Procedure.
Each abstract was evaluated by three different evaluators, who werenot aware of the method used to produce the abstracts.
In order to measure the out-come of the categorization task, we considered the abstract to have helped in catego-rizing the source document if two or more evaluators were able to chose the correctdescriptor for the abstract.
In order to measure the quality of the abstract, we computedthe average quality using the scores given by the evaluators.5.1.4 Results and Discussion.
In Table 13 we present the average information forthree runs of this experiment.
?Success?
refers to the percentage of cases in whichsubjects identified the correct descriptor.
?Quality?
refers to subjects?
summary qualityscore.
Note that because of this particular design, we cannot compare numbers acrossexperiments, we can only discuss results for each experiment.Overall, for each experiment no significant differences were observed between thedifferent automatic systems in the categorization task.
All automatic methods per-formed similarly, though we believe that documents and descriptors of narrower do-mains are needed in order to correctly assess the effectiveness of each summarizationmethod.
Unfortunately, the construction of such resources goes beyond our presentresearch and will be addressed in future work.The figures for text acceptability indicate that abstracts produced by Autosum-marize are below the acceptability level of 2.5.
The abstracts produced by SumUM,Extractor, and n-STEIN are above the acceptability level of 2.5, and the human abstractsare highly acceptable.
In the first experiment, an analysis of the variance (ANOVA) fortext quality (Oakes 1998) showed differences among the three methods at p ?
0.005(observed F(2, 27) = 9.66).
Tukey?s multiple-comparison test (Byrkit 1987) shows statis-Table 13Results of human judgment in a categorization task and assessment about text quality.Experiment Summarization MethodsFirst Autosummarize SumUM Human15 evaluators Success Quality Success Quality Success Quality10 documents 80% 1.46 80% 3.23 100% 4.25Second Autosummarize SumUM Human18 evaluators Success Quality Success Quality Success Quality12 documents 70% 1.98 70% 3.15 80% 4.04Third n-STEIN SumUM Extractor20 evaluators Success Quality Success Quality Success Quality15 documents 67% 2.76 80% 3.13 73% 3.47518Computational Linguistics Volume 28, Number 4tical differences in text quality at p ?
0.01 for the two automatic systems (SumUM andAutosummarize), but no conclusion can be drawn about differences between the ab-stracts produced by those systems and the author abstract at levels 0.01 or 0.05.
Inthe second experiment, the ANOVA showed differences at p ?
0.01 between thethree methods (observed F(2, 33) = 10.35).
Tukey?s test shows statistical differencesat p ?
0.01 between the two automatic systems (SumUM and Autosummarize) anddifferences with the author abstract at 0.05.
In the third experiment, the ANOVA fortext quality did not allow us to draw any conclusions about differences in text quality(F(2, 42) = 0.83).5.2 Evaluation of Content in a Coselection ExperimentOur objective in the evaluation of content in a coselection experiment is to measurecoselection between sentences selected by our system and a set of ?correct?
extractedsentences.
This method of evaluation has already been used in other summarizationevaluations such as Edmundson (1969) and Marcu (1997).
The idea is that if we finda high degree of overlap between the sentences selected by an automatic methodand the sentences selected by a human, the method can be regarded as effective.Nevertheless, this method of evaluation has been criticized not only because of thelow rate of agreement between human subjects in this task (Jing et al 1998), but alsobecause there is no unique ideal or target abstract for a given document.
Instead,there is a set of main ideas that a good abstract should contain (Johnson 1995).
In ourcoselection experiment, we were also interested in comparing our system with othersummarization technologies.5.2.1 Materials.Data used.
We used 10 technical articles from two different sources: 5 from thejournal Rapid Prototyping and 5 from the journal Internet Research.
The documents weredownloaded from the Emerald Electronic Library.
The abstracts and lists of keywordsprovided with the documents were deleted before the documents were used in theevaluation.Reference extracts.
We used 30 automatic abstracts (three for each article) and nineassessors with a background in dealing with technical articles, on whom we relied toobtain an assessment of important sentences in the source documents.
Eight assessorsread two articles each, and one read four articles, because no other participants wereavailable when the experiment was conducted.
The assessor of each article chose anumber of important sentences from that article (up to a maximum of Ni, the numberof sentences chosen by the summarization methods).
Each article was read by twodifferent assessors; we thus had two sets of sentences for each article.
We call thesesets Si,j (i ?
[1 .
.
.
10]?
j ?
[1 .
.
.
2]).
Most of the assessors found the task quite complex.Agreement between human assessors was only 37%.Automatic extracts.
We considered three automatic systems in this evaluation:SumUM, Autosummarize, and Extractor.
We produced three abstracts for each doc-ument.
First we produced an abstract using SumUM.
We counted the number ofsentences selected by SumUM in order to produce the indicative-informative abstract(we verified that the number of sentences selected by the system represented between10% and 25% of source documents).
Then, we produced two other automatic abstracts,one using Autosummarize and another using Extractor.
We specified to each systemthat it should select the same number of sentences as SumUM selected.519Saggion and Lapalme Generating Summaries with SumUM5.2.2 Procedure.
We measure coselection between sentences produced by each methodand the sentences selected by the assessors, computing recall, precision, and F-scoreas in Firmin and Chrzanowski (1999).
In order to obtain a clear picture, we borrowedthe scoring methodology proposed by Salton et al (1997), additionally considering thefollowing situations:?
Union scenario: For each document we considered the union of thesentences selected by the two assessors (Si,1 ?
Si,2) and computed recall,precision, and F-score for each method.?
Intersection scenario: For each document we considered the intersection ofthe sentences selected by the two assessors (Si,1 ?
Si,2) and computedrecall, precision, and F-score for each method.?
Optimistic scenario: For each document and method we considered thecase in which the method performed the best (highest F-score) andcomputed recall, precision, and F-score.?
Pessimistic scenario: For each document and method we considered thecase in which the method performed the worst (lowest F-score) andcomputed recall, precision, and F-score.5.2.3 Results and Discussion.
For each scenario we present the average information inTable 14 (Saggion and Lapalme [2000c] presented detailed results of this experiment).For the scenario in which we consider the 20 human abstracts, SumUM obtained thebest F-score in 60% of the cases, Extractor in 25% of the cases, and Autosummarize in15% of the cases.
If we assume that the sentences selected by the human assessors rep-resent the most important or interesting information in the documents, then we canconclude that on average, SumUM performed better than the other two summarizationtechnologies, even if these results are not exceptional in individual cases.
An ANOVAshowed statistical differences in the F-score measure at p ?
0.01 between the differ-ent automatic abstracts (observed F(2, 57) = 5.28).
Tukey?s tests showed differencesbetween SumUM and the two other automatic methods at p ?
0.01.Here, we have compared three different methods of producing abstracts that aredomain independent.
Nevertheless, whereas Autosummarize and Extractor are trulytext independent, SumUM is genre dependent: It was designed for the technical arti-cle and takes advantage of this fact in order to produce abstracts.
We think that thisTable 14Coselection between sentences selected by human assessors and sentences selected by threeautomatic summarization methods in recall (R), precision (P) and F-score (F).SumUM Autosummarize ExtractorR P F R P F R P FAverage .23 .20 .21 .14 .11 .12 .12 .18 .14Union .21 .31 .25 .16 .19 .17 .11 .26 .15Intersection .28 .09 .14 .13 .04 .06 .08 .04 .06Optimistic .26 .23 .25 .16 .14 .15 .14 .25 .18Pessimistic .19 .17 .18 .11 .08 .09 .08 .11 .09520Computational Linguistics Volume 28, Number 4is the reason for the better performance of SumUM in this evaluation.
The resultsof this experiment are encouraging considering the limited capacities of the actualimplementation.
We expect to improve the results in future versions of SumUM.
Ad-ditional evaluations of SumUM using sentence acceptability criteria and content-basedmeasures of indicativeness have been presented in Saggion and Lapalme (2000b) andSaggion (2000).6.
Related Work on SummarizationAs a human activity, the production of summaries is directly associated with theprocesses of language understanding and production: A source text is read and un-derstood to recognize its content, which is then compiled in a concise text.
In order toexplain this process, several theories have been proposed and tested in text linguistics,cognitive science, and artificial intelligence, including macro structures (Kintsch andvan Dijk 1975; van Dijk 1977), history grammars (Rumelhart 1975), plot units (Lehn-ert 1981), and concept/coherence relations (Alterman and Bookman 1990).
Computershave been producing summaries since the original work of Luhn (1958).
Since thenseveral methods and theories have been applied, including the use of term frequency?
inverse document frequency (TF ?
IDF) measures, sentence position, and cue and ti-tle words (Luhn 1958; Edmundson 1969; Kupiec, Pedersen, and Chen 1995; Brandow,Mitze, and Rau 1995); partial understanding using conceptual structures (DeJong 1982;Tait 1982); bottom-up understanding, top-down parsing, and automatic linguistic ac-quisition (Rau, Jacobs, and Zernik 1989); recognition of thematic text structures (Hahn1990); cohesive properties of texts (Benbrahim and Ahmad 1995; Barzilay and Elhadad1997); and rhetorical structure theory (Ono, Sumita, and Miike 1994; Marcu 1997).In the context of the scientific article, Rino and Scott (1996) have addressed theproblem of coherent selection for text summarization, but they depend on the avail-ability of a complex meaning representation, which in practice is difficult to obtainfrom the raw text.
Instead, superficial analysis in scientific text summarization usinglexical information was applied by Lehmam (1997) for the French language.
Liddy(1991) produced one of the most complete descriptions of conceptual information forabstracts of empirical research.
In our work, we concentrated instead on conceptual in-formation that is common across domains.
Liddy?s model includes three typical levelsof information.
The most representative level, called the prototypical structure, in-cludes the information categories subjects, purpose, conclusions, methods, references,and hypotheses.
The other two levels are the typical structure and the elaboratedstructure, which include information less frequently found in abstracts of empiricalresearch.
To our knowledge Liddy?s model has never been implemented; nevertheless,it could be used as a starting point for improving our flat-domain model.
Relevantwork in rhetorical classification for scientific articles, which is the first step towardthe production of scientific abstracts, is due to Teufel and Moens (1998), who usedstatistical approaches borrowed from Kupiec, Pedersen, and Chen (1995).Our method is close to concept-based abstracting (CBA) (Jones and Paice 1992;Paice and Jones 1993) but differs from this approach in several aspects.
CBA is used toproduce abstracts of technical articles in specific domains, for example, in the domainof agriculture.
Semantic roles such as species, cultivar, high-level property, and low-level property are first identified by the manual analysis of a corpus, and then patternsare specified that account for stylistic regularities of expression of the semantic rolesin texts.
These patterns are used in an information extraction process that instantiatesthe semantic roles.
Selective analysis, although genre dependent, was developed asdomain independent and tested in different technical domains without the need to521Saggion and Lapalme Generating Summaries with SumUMadapt the conceptual model, the patterns, or the conceptual dictionary.
In order toadapt CBA to new domains, the semantic roles representing the ?key?
information inthe new domain need to be identified, and new templates and patterns need to be con-structed (Oakes and Paice 2001).
Although such adaptation is generally done manually,recent work has shown how to export CBA to new domains automatically (Oakes andPaice 1999).
CBA uses a fixed canned template for summary generation, whereas ourmethod allows greater stylistic variability because the main ?content?
of the summarygenerated is expressed in the words of the authors of the paper.
Selective analysisis used to produce indicative-informative abstracts, whereas CBA is mainly used toproduce indicative abstracts, though some informative content is included in the formof extracted sentences containing results and conclusions (Paice and Oakes 1999).
Ourmethod can be seen as an extension of CBA that allows for domain independence andinformativeness.
We believe that the indicative patterns we have designed are genredependent, whereas the informative patterns are general and can be used in any do-main.
Our implementation of patterns for information extraction is similar to Black?s(1990) implementation of Paice?s (1981) indicative phrases method, but whereas Blackscores sentences based on indicative phrases contained in the sentences, our methodscores the information from the sentences based on term distribution.Our work in sentence reformulation is different from cut-and-paste summariza-tion (Jing and McKeown 2000) in many ways.
Jing (2000) proposes a novel algorithmfor sentence reduction that takes into account different sources of information to de-cide whether or not to remove a particular component from a sentence to be includedin a summary.
The decision is made based on (1) the relation of the component toits context, (2) the probability of deleting such a component (estimated from a cor-pus of reduced sentences), and (3) linguistic knowledge about the essentiality of thecomponent in the syntactic structure.
Sentence reduction is concerned only with theremoval of sentence components, so it cannot explain transformations observed in ourcorpus and in summarization in general, such as the reexpression of domain conceptsand verbs.
We achieve sentence reduction through a process of information extractionthat extracts verbs and arguments, sometimes considering only sentence fragments(for example, initial prepositional phrases, parenthetical expressions, and some adver-bials are ignored for some templates).
The process removes domain concepts, avoidsunnecessary grammatical subjects, and generates coordinate structures, avoiding verbrepetition.
Whereas our algorithm is genre dependent, requiring only shallow parsing,Jing?s algorithm is genre and domain independent and requires full syntactic parsingand disambiguation and extensive linguistic resources.Regarding the fusion of information, we have concentrated only on the fusionof explicit topical information (document topic, section topic, and signaling struc-tural and conceptual elements).
Jing and McKeown (2000) have proposed a rule-basedalgorithm for sentence combination, but no results have been reported.
Radev andMcKeown (1998) have already addressed the issue of information fusion in the con-text of multidocument summarization in one specific domain (i.e., terrorism): Thefusion of information is achieved through the implementation of summary operatorsthat integrate the information of different templates from different documents refer-ring to the same event.
Although those operators are dependent on the specific taskof multidocument summarization, and to some extent on the particular domain theydeal with, it is interesting to observe that some of Radev and McKeown?s ideas couldbe applied in order to improve our texts.
For example, their ?refinement?
operatorcould be used to improve the descriptions of the entities of the indicative abstract.The entities from the indicative abstract could be refined with definitions or descrip-tions from the InfoDB in order to obtain a better and more compact text.
The idea of522Computational Linguistics Volume 28, Number 4elaborating topics has also been addressed by Mani, Gates, and Bloedorn (1999).
Theyhave proposed a number of rules for summary revision aiming at conciseness; theirelimination rule discards parenthetical and initial prepositional phrases, as does ourapproach.
Their aggregation operation combines two constituents on the basis of ref-erential identity and so is more general than our combination of topical information.Although their approach is domain independent, it requires full syntactic analysis andcoreference resolution.7.
ConclusionsSumUM has been fully implemented to take a raw text as input and produce a sum-mary.
This involves the following successive steps: text segmentation, part-of-speechtagging, partial syntactic and semantic analysis, sentence classification, template in-stantiation, content selection, text regeneration, and topic elaboration.
Our researchwas based on the intensive study of manual alignments between sentences of pro-fessional abstracts and elements of source documents and on the exploration of theessential differences between indicative and informative abstracts.Although our method was deeply influenced by the results of our corpus study,it nevertheless has many points in common with recent theoretical and program-matic directions in automatic text summarization.
For example, Sparck Jones (1997)argues in favor of a kind of ?indicative, skeletal summary?
and the need to explore dy-namic, context-sensitive summarization in interactive situations in which the summarychanges according to the user needs.
Hutchins (1995) advocates indicative summaries,produced from parts of a document in which the topics are likely to be stated.
Theseabstracts are well suited for situations in which the actual user is unknown (i.e., ageneral reader), since the abstract will provide the reader with good entry pointsfor retrieving more detailed information.
If the users are known, the abstract can betailored to their specific profiles; such profiles might specify the reader?s interest invarious types of information, such as conclusions, definitions, methods, or user needsexpressed in a ?query?
to an information retrieval system (Tombros, Sanderson, andGray 1998).
Our method, however, was designed without any particular reader inmind and with the assumption that a text does have a ?main?
topic.In this article, we have presented an evaluation of automatically generatedindicative-informative abstracts in terms of content and text quality.
In the evalua-tion of the indicative content in a categorization task, no differences were observedamong the different automatic systems.
The automatic abstracts generated by SumUMwere considered more acceptable than other systems?
abstracts.
In the evaluation ofthe informative content, SumUM selected sentences that were evaluated as more rele-vant by human assessors than sentences selected by other summarization technologies;statistical tests showed significant differences between the automatic methods in thatevaluation.
In the future, we plan to address several issues, including the study ofrobust automatic text classification techniques, anaphora resolution, and lexical cohe-sion for improving elaboration of topics as well as the incorporation of local discourseanalysis to improve the coherence of abstracts.AcknowledgmentsWe would like specially to thank EduardHovy for his valuable comments andsuggestions, which helped improve andclarify the present work.
We are indebted tothe three anonymous reviewers for theirextensive suggestions, which also helpedimprove this work.
We are grateful toProfessor Miche`le Hudon from Universite?de Montre?al for fruitful discussion and toProfessor John E. Leide from McGillUniversity, to Mme.
Gracia Pagola from523Saggion and Lapalme Generating Summaries with SumUMUniversite?
de Montre?al, and to ChristineJacobs from John Abbott College for theirhelp in recruiting assessors for theexperiments.
We thank also ElliottMacklovitch and Diana Maynard, whohelped us improve the quality of our article,and the members of the Laboratoire deRecherche Applique?e en LinguistiqueInformatique (RALI) for their participationin our experiments.
The first author wassupported by Agence Canadienne deDe?veloppement International (ACDI)during his Ph.D. research.
He also receivedsupport from Fundacio?n Antorchas(A-13671/1-47), Ministerio de Educacio?n dela Nacio?n de la Repu?blica Argentina(Resolucio?n 1041/96) and Departamento deComputacio?n, Facultad de Ciencias Exactasy Naturales, Universidad de Buenos Aires,Argentina.ReferencesAlterman, Richard andLawrence A. Bookman.
1990.
Somecomputational experiments insummarization.
Discourse Processes,13:143?174.American National Standards Institute.(ANSI).
1979.
Writing Abstracts.Barzilay, Regina and Michael Elhadad.
1997.Using lexical chains for textsummarization.
In Proceedings of theACL/EACL?97 Workshop on IntelligentScalable Text Summarization, pages 10?17,Madrid, July.Benbrahim, Mohamed and Kurshid Ahmad.1995.
Text summarisation: The role oflexical cohesion analysis.
New Review ofDocument & Text Management, 1:321?335.Bernier, Charles L. 1985.
Abstracts andabstracting.
In E. D. Dym, editor, Subjectand Information Analysis, volume 47 ofBooks in Library and Information Science.Marcel Dekker, Inc., pages 423?444.Bhatia, Vijay K. 1993.
Analysing Genre:Language Use in Professional Settings.Longman.Black, William J.
1990.
Knowledge basedabstracting.
Online Review, 14(5):327?340.Brandow, Ronald, K. Mitze, and Lisa F. Rau.1995.
Automatic condensation ofelectronic publications by sentenceselection.
Information Processing &Management, 31(5):675?685.Bunge, Mario.
1967.
Scientific Research I. TheSearch for System.
Springer-Verlag, NewYork.Byrkit, Donald R. 1987.
Statistics Today: AComprehensive Introduction.Benjamin/Cummings.Cremmins, Eduard T. 1982.
The Art ofAbstracting.
ISI Press.DeJong, Gerald.
1982.
An overview of theFRUMP system.
In W. G. Lehnert andM.
H. Ringle, editors, Strategies for NaturalLanguage Processing.
Lawrence Erlbaum,pages 149?176.Edmundson, H. P. 1969.
New methods inautomatic extracting.
Journal of theAssociation for Computing Machinery,16(2):264?285.Educational Resources Information Center(ERIC).
1980.
Processing Manual: Rules andGuidelines for the Acquisition, Selection, andTechnical Processing of Documents andJournal Articles by the Various Components ofthe ERIC Network.
ERIC.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press.Filman, Roger E. and Sangan Pant.
1998.Searching the Internet.
IEEE InternetComputing, 2(4):21?23.Firmin, The?re`se andMichael J. Chrzanowski.
1999.
Anevaluation of automatic textsummarization systems.
In I. Mani andM.
T. Maybury, editors, Advances inAutomatic Text Summarization.
MIT Press,pages 325?336.Foster, George.
1991.
Statistical lexicaldisambiguation.
Master?s thesis, School ofComputer Science, McGill University,Montre?al, Que?bec, Canada.Gibson, Timothy R. 1993.
Towards a DiscourseTheory of Abstracts and Abstracting.Department of English Studies,University of Nottingham.Grant, Pamela.
1992.
The Integration of Theoryand Practice in the Development ofSummary-Writing Strategies.
Ph.D. thesis,Faculte?
des E?tudes Supe?rieures,Universite?
de Montre?al.Hahn, Udo.
1990.
Topic parsing: Accountingfor text macro structures in full-textanalysis.
Information Processing &Management, 26(1):135?170.Hutchins, John.
1995.
Introduction to textsummarization workshop.
InB.
Engres-Niggemeyer, J. Hobbs, andK.
Sparck Jones, editors, Summarising Textfor Intelligent Communication, DagstuhlSeminar Report 79.
IBFI, SchlossDagstuhl, Wadern, Germany,INSPEC.
2000.
INSPEC database for physics,electronics and computing.http://www.iee.org.uk/publish/inspec/Jing, Hongyan.
2000.
Sentence reduction forautomatic text summarization.
InProceedings of the Sixth Applied NaturalLanguage Processing Conference, pages310?315, Seattle, April 29?May 4.524Computational Linguistics Volume 28, Number 4Jing, Hongyan and Kathleen McKeown.2000.
Cut and paste based textsummarization.
In Proceedings of the FirstMeeting of the North American Chapter of theAssociation for Computational Linguistics,pages 178?185, Seattle, April 29?May 4.Jing, Hongyan, Kathleen McKeown, ReginaBarzilay, and Michael Elhadad.
1998.Summarization evaluation methods:Experiments and analysis.
In IntelligentText Summarization: Papers from the 1998AAAI Spring Symposium.
Stanford, March23?25.
Technical Report SS-98-06, AAAIPress, pages 60?68.Johnson, Frances.
1995.
Automaticabstracting research.
Library Review,44(8):28?36.Johnson, Frances C., Chris D. Paice,William J.
Black, and A. P. Neal.
1993.
Theapplication of linguistic processing toautomatic abstract generation.
Journal ofDocument & Text Management, 1(3):215?241.Jones, Paul A. and Chris D. Paice.
1992.
A?select and generate?
approach toautomatic abstracting.
In A. M. McEnryand C. D. Paice, editors, Proceedings of the14th British Computer Society InformationRetrieval Colloquium.
Springer Verlag,pages 151?154.Jordan, Michael P. 1993.
Openings in veryformal technical texts.
Technostyle,11(1):1?26.Jordan, Michael P. 1996.
The Language ofTechnical Communication: A Practical Guidefor Engineers, Technologists and Technicians.Quarry.Kaplan, Robert B., Selena Cantor, CynthiaHagstrom, Lia D. Kamhi-Stein, YumikoShiotani, and Cheryl B. Zimmerman.
1994.On abstract writing.
Text, 14(3):401?426.Kintsch, Walter and Teun A. van Dijk.
1975.Comment on se rappelle et on re?sume deshistoires.
Langages, 40:98?116.Knight, Kevin and Daniel Marcu.
2000.Statistics-based summarization?Step one:Sentence compression.
In Proceedings of the17th National Conference of the AmericanAssociation for Artificial Intelligence (AAAI),July 30?August 3.Kupiec, Julian, Jan Pedersen, and FrancineChen.
1995.
A trainable documentsummarizer.
In Proceedings of the 18thACM-SIGIR Conference, pages 68?73.Lehmam, Abderrafih.
1997.
Unestructuration de texte conduisant a` laconstruction d?un syste`me de re?sume?automatique.
In Actes des journe?esscientifiques et techniques du re?seaufrancophone de l?inge?nierie de la langue del?AUPELF-UREF, pages 175?182, 15?16April.Lehnert, Wendy.
1981.
Plot units andnarrative summarization.
CognitiveScience, 5:293?331.Liddy, Elizabeth D. 1991.
Thediscourse-level structure of empiricalabstracts: An exploratory study.Information Processing & Management,27(1):55?81.Luhn, Hans P. 1958.
The automatic creationof literature abstracts.
IBM Journal ofResearch Development, 2(2):159?165.Maizell, Robert E., Julian F. Smith, andTibor E. R. Singer.
1971.
AbstractingScientific and Technical Literature.Wiley-Interscience.Mani, Inderjeet, Barbara Gates, and EricBloedorn.
1999.
Improving summaries byrevising them.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics, pages 558?565,College Park, MD, 20?26 June.Mani, Inderjeet, David House, Gary Klein,Lynette Hirshman, Leo Obrst, The?re`seFirmin, Michael Chrzanowski, and BethSundheim.
1998.
The TIPSTER SUMMACtext summarization evaluation.
TechnicalReport, Mitre Corporation.Marcu, Daniel.
1997.
From discoursestructures to text summaries.
InProceedings of the ACL?97/EACL?97 Workshopon Intelligent Scalable Text Summarization,pages 82?88, Madrid, July 11.Mariani, Joseph.
1995.
Evaluation.
InRonald E. Cole, editor, Survey of the State ofthe Art in Human Language Technology.Cambridge University Press, chapter 13,pages 475?518.Mathis, Betty A. and James E. Rush.
1985.Abstracting.
In E. D. Dym, editor, Subjectand Information Analysis, volume 47 ofBooks in Library and Information Science.Marcel Dekker, pages 445?484.n-STEIN.
2000. n-STEIN Web page.http://www.gespro.com.Oakes, Michael P. 1998.
Statistics for CorpusLinguistics.
Edinburgh University Press.Oakes, Michael P. and Chris D. Paice.
1999.The automatic generation of templates forautomatic abstracting.
In 21st BCS IRSGColloquium on IR, Glasgow.Oakes, Michael P. and Chris D. Paice.
2001.Term extraction for automatic abstracting.In D. Bourigault, C. Jacquemin, andM.-C. L?Homme, editors, Recent Advancesin Computational Terminology, volume 2 ofNatural Language Processing.
JohnBenjamins, chapter 17, pages 353?370.Ono, Kenji, Kazuo Sumita, and Seiji Miike.1994.
Abstract generation based onrhetorical structure extraction.
InProceedings of the International Conference on525Saggion and Lapalme Generating Summaries with SumUMComputational Linguistics, pages 344?348.Paice, Chris D. 1981.
The automaticgeneration of literary abtracts: Anapproach based on identification ofself-indicating phrases.
In O. R. Norman,S.
E. Robertson, C. J. van Rijsbergen, andP.
W. Williams, editors, InformationRetrieval Research.
London: Butterworth,pages 172?191.Paice, Chris D. 1991.
The rhetoricalstructure of expository text.
In K. P. Jones,editor, Informatics 11: The Structuring ofInformation, University of York, 20?22March.
Aslib.Paice, Chris D., William J.
Black, Frances C.Johnson, and A. P. Neal.
1994.
Automaticabstracting.
R&D Report 6166, BritishLibrary.Paice, Chris D. and Paul A. Jones.
1993.
Theidentification of important concepts inhighly structured technical papers.
InR.
Korfhage, E. Rasmussen, and P. Willett,editors, Proceedings of the 16th ACM-SIGIRConference, pages 69?78.Paice, Chris D. and Michael P. Oakes.
1999.A concept-based method for automaticabstracting.
Research Report 27, Libraryand Information Commission.Radev, Dragomir R. and Kathleen R.McKeown.
1998.
Generating naturallanguage summaries from multipleon-line sources.
Computational Linguistics,24(3):469?500.Rau, Lisa F., Paul S. Jacobs, and Uri Zernik.1989.
Information extraction and textsummarization using linguisticknowledge acquisition.
InformationProcessing & Management, 25(4):419?428.Rino, Lucia H. M. and Donia Scott.
1996.
Adiscourse model for gist preservation.
InD.
L. Borges and C. A.
A. Kaestner,editors, Proceedings of the 13th BrazilianSymposium on Artificial Intelligence(SBIA?96): Advances in Artificial Intelligence,October 23?25, Curitiba, Brazil.
Springer,pages 131?140.Rowley, Jennifer.
1982.
Abstracting andIndexing.
Clive Bingley, London.Rumelhart, David E. 1975.
Notes on aschema for stories.
In Language, Thought,and Culture: Advances in the Study ofCognition.
Academic Press.Saggion, Horacio.
2000.
Ge?ne?rationautomatique de re?sume?s par analyse se?lective.Ph.D.
thesis, De?partement d?informatiqueet de recherche ope?rationnelle, Faculte?des arts et des sciences, Universite?
deMontre?al, Montre?al.Saggion, Horacio and Guy Lapalme.
2000a.Concept identification and presentation inthe context of technical textsummarization.
In Proceedings of theWorkshop on Automatic Summarization(ANLP-NAACL2000), Seattle, 30 April.Association for ComputationalLinguistics.Saggion, Horacio and Guy Lapalme.
2000b.Selective analysis for automaticabstracting: Evaluating indicativeness andacceptability.
In Proceedings of theComputer-Assisted Information Searching onInternet Conference (RIAO?2000), Paris,12?14 April.Saggion, Horacio and Guy Lapalme.
2000c.Summary generation and evaluation inSumUM.
In Advances in ArtificialIntelligence.
International Joint Conference:Seventh Ibero-American Conference onArtificial Intelligence and 15th BrazilianSymposium on Artificial Intelligence(IBERAMIA-SBIA 2000), volume 1952 ofLecture Notes in Artificial Intelligence.Springer-Verlag, pages 329?338.Salton, Gerald, Amit Singhal, Mandar Mitra,and Chris Buckley.
1997.
Automatic textstructuring and summarization.Information Processing & Management,33(2):193?207.Sharp, Bernadette.
1998.
Elaboration andTesting of New Methodologies for AutomaticAbstracting.
Ph.D. thesis, University ofAston in Birmingham.SICStus.
1998.
SICStus Prolog User?s Manual.SICStus.Sparck Jones, Karen.
1993.
What might be ina summary?
In K. Knorz andC.
Womser-Hacker, editors, InformationRetrieval 93: Von der Modellierung zurAnwendung.Sparck Jones, Karen.
1997.
Documentprocessing: Summarization.
In R. Cole,editor, Survey of the State of the Art inHuman Language Technology.
CambridgeUniversity Press, chapter 7, pages266?269.Sparck Jones, Karen and BrigitteEndres-Niggemeyer.
1995.
Automaticsummarizing.
Information Processing &Management, 31(5):625?630.Sparck Jones, Karen and Julia R. Galliers.1995.
Evaluating Natural LanguageProcessing Systems: An Analysis and Review,number 1083 in Lecture Notes in ArtificialIntelligence.
Springer.Tait, John I.
1982.
Automatic Summarising ofEnglish Texts.
Ph.D. thesis, ComputerLaboratory, Cambridge University,Cambridge.Teufel, S. 1998.
Meta-discourse markers andproblem-structuring in scientific texts.
InM.
Stede, L. Wanner, and E. Hovy,editors, Proceedings of the Workshop on526Computational Linguistics Volume 28, Number 4Discourse Relations and Discourse Markers(COLING-ACL?98), pages 43?49, 15August.Teufel, S. and M. Moens.
1998.
Sentenceextraction and rhetorical classification forflexible abstracts.
In Intelligent TextSummarization: Papers from the 1998 AAAISpring Symposium, Stanford, March 23?25.Technical Report SS-98-06, AAAI Press,pages 16?25.Tombros, Anastasios, Mark Sanderson, andPhil Gray.
1998.
Advantages of querybiased summaries in informationretrieval.
In Intelligent Text Summarization:Papers from the 1998 AAAI SpringSymposium, Stanford, March 23?25.Technical Report SS-98-06, AAAI Press,pages 34?43.Turney, Peter D. 1999.
Learning to extractkeyphrases from text.
Technical ReportERB-1051, National Research Council ofCanada.van Dijk, Teun A.
1977.
Recalling andsummarizing complex discourse.
InW.
Burghardt and K. Holzer, editors, TextProcessing.
De Gruyter, New York andBerlin, pages 49?118.Vianna, Fernando de Melo, editor.
1980.Roget?s II: The New Thesaurus.
HoughtonMifflin, Boston.Wall, Larry, Tom Christiansen, andRandal L. Schwartz.
1996.
ProgrammingPerl.
O?Reilly & Associates, secondedition.
