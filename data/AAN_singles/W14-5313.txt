Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 110?119,Dublin, Ireland, August 23 2014.Improved Sentence-Level Arabic Dialect ClassificationChristoph Tillmann and Yaser Al-OnaizanIBM T.J. Watson Research CenterYorktown Heights, NY, USA{ctill,onaizan}@us.ibm.comSaab Mansour?Aachen UniversityAachen, Germanymansour@cs.rwth-aachen.deAbstractThe paper presents work on improved sentence-level dialect classification of Egyptian Arabic(ARZ) vs. Modern Standard Arabic (MSA).
Our approach is based on binary feature functionsthat can be implemented with a minimal amount of task-specific knowledge.
We train a feature-rich linear classifier based on a linear support-vector machine (linear SVM) approach.
Our bestsystem achieves an accuracy of 89.1 % on the Arabic Online Commentary (AOC) dataset (Zaidanand Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracyimprovement over the results published by (Zaidan and Callison-Burch, 2014).
We also evaluatethe classifier on dialect data from an additional data source.
Here, we find that features whichmeasure the informalness of a sentence actually decrease classification accuracy significantly.1 IntroductionThe standard form of written Arabic is Modern Standard Arabic (MSA) .
It differs significantly fromvarious spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014;Elfardy and Diab, 2013).
Even though these dialects do not originally exist in written form, they arepresent in social media texts.
Recently a dataset of dialectal Arabic has been made available in the formof the Arabic Online Commentary (AOC) set (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014).
The data consists of reader commentary from the online versions of Arabic newspapers,which have a high degree of dialect content.
Data for the following dialects has been collected: Levan-tine, Gulf, and Egyptian.
The data had been obtained by a crowd-sourcing effort.
In the current paper, wepresent results for a binary classification task only, where we predict the dialect of Egyptian Arabic ARZvs.
MSA sentences from the Al-Youm Al-Sabe?
newspaper online commentaries1.
Our ultimate goalis to use the dialect classifier for building a dialect-aware Arabic-English statistical machine translation(SMT) system.
Our Arabic-English training data contains a significant amount of Egyptian dialect dataonly, and we would like to adapt the components of our hierarchical phrase-based SMT system (Zhaoand Al-Onaizan, 2008) to that data.Similar to (Elfardy and Diab, 2013), we present a sentence-level classifier that is trained in a supervisedmanner.
Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizersor orthography normalizers.
In contrast to the language-model (LM) based classifier used by (Zaidan andCallison-Burch, 2014), we present a linear classifier approach that works best without the use of LM-based features.
Some improvements in terms of classification accuracy and 10-fold cross validation underthe same data conditions as (Zaidan and Callison-Burch, 2011; Elfardy and Diab, 2013) are presented.In general, we aim at a smaller amount of domain specific feature engineering than previous relatedapproaches.The paper is structured as follows.
In Section 2, we present related work on language and dialectidentification.
In Section 3, we discuss the linear classification model used in this paper.
In Section 4, weevaluate the classifier performance in terms of classification accuracy on two data sets and present some?Part of the work was done while the author was a student intern at the IBM T.J. Watson Research Center.1We use the ISO 639-3 code ARZ for denoting Egyptian Arabic.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/110error analysis.
Finally, in Section 5, we discuss future work on improved dialect-level classification andits application to system adaptation for machine translation.2 Related WorkFrom a computational perpective, we can view dialect identification as a more fine-grained form of lan-guage identification (ID).
Previous work on language ID examined the use of character histograms (Cav-nar and Trenkle, 1994; Dunning, 1994), and high accuracy prediction results have been reported evenfor languages with a common character set.
(Baldwin and Lui, 2010) present a range of document-levellanguage identification techniques on three different data sets.
They use n-gram counting techniques anddifferent tokenization schemes that are adopted to those data sets.
Their classification task deals withseveral languages, and it becomes more difficult as the number of languages increases.
They present anSVM-based multiclass classification approach similar to the one presented in this paper which performswell on one of their data sets.
(Trieschnigg et al., 2012) generates n-gram features based on character orword sequences to classify dialectal documents in a dutch-language fairy-tale collection.
Their baselinemodel uses N -gram based text classification techniques as popularised in the TextCat tool (Cavnar andTrenkle, 1994).
Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features withnearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics.
(Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Por-tuguese.
They use word and character-based language model classification techniques similar to (Zaidanand Callison-Burch, 2014).
(Huang and Lee, 2008) present simple bag-of-word techniques to classifyvarieties of Chinese from the Chinese Gigaword corpus.
(Kruengkrai et al., 2005) extend the use of n-gram features to using string kernels: they may take into account all possible sub-strings for comparisonpurposes.
The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle,1994).
(Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, andCanadian English.
They present results where they draw training and test data from different sources.The successful transfer of models from one text source to another is evidence that their classifier indeedcaptures dialectal rather than stylistic or formal differences.
Language identification of related languagesis also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshopat COLING 14 (Tan et al., 2014).While most of the above work focuses on document-level language classification, recent work onhandling Arabic dialect data addresses the problem of sentence-level classification (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Zaidan and Callison-Burch,2014).
The work is based on the data collection effort by (Zaidan and Callison-Burch, 2014) whichcrowdsources the annotation task to workers on Amazons Mechanical Turk.
The classification resultsby (Zaidan and Callison-Burch, 2014) are based on n-gram language-models, where the n-grams aredefined both on words and characters.
The authors find that unigram word-based models perform best.The word-based models are obtained after a minimal amount of preprocessing such as proper handlingof HTML entities and Arabic numbers.
Classification accuracy is significantly reduced for shorter sen-tences.
(Elfardy and Diab, 2013) presents classifcation result based on various tokinization and ortho-graphic normalization techniques as well as so-called meta features that estimate the informalness of thedata.
Like our work, the authors focus on a binary dialect classification based on the ARZ-MSA portionof the dataset in (Zaidan and Callison-Burch, 2011).3 Classification ModelWe use a linear model and compute a score s(tn1) for a tokenized input sentence consisting of n tokensti:s(tn1) =d?s=1ws?n?i=1?s(ci, ti) (1)where ?s(ci, ti) is a binary feature function which takes into account the context ciof token ti.
w ?
Rdis a high-dimensional weight vector obtained during training.
In our experiments, we classify a tokenized111Description MSA ARZ# sentences # words # sentences # wordsARZ-MSA portion of AOC 13, 512 334K 12, 527 327KDEV12 tune set 585 8.4K 634 9.3KTable 1: We used the following dialect data: 1) the ARZ-MSA portion of the AOC data from commen-taries of the Egyptian newspaper Al-Youm Al-Sabe?, and 2) the DEV12 tune set (1219 sentences) whichis the LDC2012E30 corpus BOLT Phase 1 dev-tune set.
The DEV12 tune set was annotated by a nativespeaker of Arabic.sentence as being Egyptian dialect (ARZ) if s(tn1) > 0.
To train the weights w in Eq.
1, we use a linearSVM approach (Hsieh et al., 2008; Fan et al., 2008).
The trainer can easily handle a huge number ofinstances and features.
The training data is given as instance-label pairs (xi, yi) where i ?
{1, ?
?
?
, l} andl is the number of training sentences.
The xiare d-dimensional vectors of integer-valued features thatcount how often a binary feature fired for a tokenized sentence tn1.
yi?
{+1,?1} are the class labelswhere a label of ?+1?
represents Egyptian dialect.
During training, we solve the following optimizationproblem:minw||w||1+ Cl?i=1max(0, 1?
yiwTxi) , (2)i.e.
we use L1 regularized L2-loss support vector classification.
We set the penalty term C = 0.5.
Forour experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011) which also has beenused in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014).
We focus on thebinary classification between MSA and ARZ.
Details on the data sources can be found in Table 1.
Wepresent accuracy results in terms of 10-fold stratified cross-validation which are comparable to previouslypublished work.3.1 Tokenization and DictionariesThe Arabic tokenizer used in the current paper is based on (Lee et al., 2003).
It is a general purposetokenizer which has been optimized towards improving machine translation quality of SMT systemsrather than dialect classification.
Together with the tokenized text, a maximum-entropy based taggerprovides the part-of-speech (PoS) tags for each token.
In addition, we have explored a range of featuresthat are based on the output of the AIDA software package (Elfardy and Diab, 2012; Mona Diab etal., 2009 2011).
The AIDA software has been made available to the participants of the DARPA-fundedBroad Operational Language Translation (BOLT) project.
AIDA is a system for dialect identification,classification and glossing on the token and sentence level for written Arabic.
AIDA aggregates severalcomponents including dictionaries and language models in order to perform named entity recognition,dialect identification classification, and MSA English linearized glossing of the input text.
We createda dictionary from AIDA resources that includes about 41 000 ARZ tokens.
In addition, we obtained asecond small dictionary of about 70 ARZ dialect tokens with the help of a native speaker of Arabic.
Thelist was created by training two IBM Model 1 lexicons, one on Egyptian Arabic data and another onMSA data.
We then inspected the ARZ lexicon entries with the highest cosine distance to their MSAcounterparts and kept the ones that are strong ARZ words.
The tokens in both dictionaries are not ARZexclusive, but could occur in MSA as well.3.2 Feature SetIn our work, we employ a simple set of binary feature functions based on the tokenized Arabic sentence.For example, we define a token bigram feature as follows:?Bi(tk, tk?1) ={1 tk= ?????
and tk?1= ??
?g?0 otherwise.
(3)112Token unigram and trigram features are defined accordingly.
We also define unigram, bigram, and tri-gram features based on PoS tags.
Currently, just PoS unigrams are used in the experiments.
We definedictionary-based features as follows:?Dictl(tk) ={1 tk= ?I???X?
and tk?
Dictl0 otherwise, (4)where we use the two dictionaries Dict1and Dict2as described in Section 3.1.
The dictionaries arehandled as token sets and we generate separate features for each of them.
We generate some featuresbased on the AIDA tool output.
AIDA provides a dialect label for each input token tkas well as a singledialect label at the sentence level.
A sentence-level binary feature based on the AIDA sentence levelclassification is defined as follows:?AIDA(tn1) ={1 AIDA(tn1) is ARZ0 otherwise(5)where AIDA(tn1) is the sentence-level classification of the AIDA tool.
A word-level feature ?AIDA(tk) isdefined accordingly.
These features improve the classification accuracy of our best system significantly.We have also experimented with some real-valued feature.
For example, we derived a feature fromdialect-specific language model probabilities:?LM(tn1) = 1/n ?
[ log(pMSA(tn1)) ?
log(pARZ(tn1))] ,where log(pARZ(tn1)) is the language-model log probability for the dialect class ARZ .
We used a trigramlanguage model.
pMSA(?)
is defined accordingly.
In addition, we have implemented a range of so-called?meta?
features similar to the ones defined in (Elfardy and Diab, 2013).
For example, we define a feature?Excl(tn1) which is equal to the length of the longest consecutive sequence of exclamation marks inthe tokenized sentence tn1.
Similarly, we define features that count the longest sequence of punctuationmarks, the number of tokens, the averaged character-length of a token in the sentence, and the percentageof words with word-lengthening effects.
These features do not directly model dialectalness of the databut rather try to capture the degree of in-formalness.
Contrary to (Elfardy and Diab, 2013) we find thatthose features do not improve accuracy of our best model in the cross-validation experiments.
On theDEV12 set, the use of the meta features results in a significant drop in accuracy.4 ExperimentsIn this section, we present experimental results.
Firstly, Section 4.1 demonstrates that our data is anno-tated consistently.
In Section 4.2, we present dialect prediction results in terms of accuracy and F-scoreon our two data sets.
In Section 4.3, we perform some qualitative error analysis for our classifier.
InSection 4.4, we present some preliminary effects on training a SMT system.4.1 Annotator AgreementTo confirm the consistent annotation of our data, we have measured some inter-annotator and intra-annotator agreement on it.
A native speaker of Arabic was asked to classify the ARZ-MSA portionof the dialect data using the following three labels: ARZ, MSA, Other.
We randomly sampled 250sentences from the ARZ-MSA portion of the Zaidan data maintaining the original dialect distribution.The confusion matrix is shown in Table 2.
It corresponds to a kappa value of 0.84 (using the definition of(Fleiss, 1971)), which indicates a very high agreement.
In addition, we did re-annotate a sub-set of 200sentences from the DEV12 set over a time period of three months using our own annotator.
The kappavalue of the corresponding confusion matrix is 0.93, indicating very high agreement as well.4.2 Classification ExperimentsFollowing previous work, we present dialect prediction results in terms of accuracy:ACC =# sent correctly tagged# sent, (6)113Predicted Class (IBM)ARZ MSA OtherActual ARZ 125 4 1Class MSA 14 105 1(AOC) Other 0 0 0Table 2: Inter annotator agreement on 250 randomly selected AOC sentences from the data in Table 1.An in-lab annotator?s dialect prediction is compared against the AOC data gold-standard dialect labels.where ?# sent?
is the number of sentences.
In addition, we present dialect prediction results in terms ofprecision, recall, and F-score.
They are defined as follows:Prec =# sent correctly tagged as ARZ# sent tagged as ARZ(7)Recall =# sent correctly tagged as ARZ# ref sent tagged as ARZF =2 ?
Prec ?Recall(Prec+Recall).MSA prediction F-score is defined analogously.
Experimental results are presented in Table 3, where wepresent results for different sets of feature types and the two test sets in Table 1.
In the top half of thetable, results are presented in terms of 10-fold cross validation on the ARZ-MSA portion of the AOCdata.
In the bottom half, we present results on DEV12 tune set, where we use the entire dialect data inTable 1 for training (about 26K sentences).As our baseline we have re-implemented the language-model-perplexity based approach reported in(Zaidan and Callison-Burch, 2011).
We train language models on the dialect-labeled commentary train-ing data for each of the dialect classes c ?
{MSA,ARZ}.
During testing, we compute the languagemodel probability of a sentence s for each of the classes c. We assign a sentence to the class c with thehighest probability (or the lowest perplexity) .
For the 10-fold cross validation experiments, 10 languagemodels are built and perplexities are computed on 10 different test sets.
The resulting (averaged) ac-curacy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set.
In comparison, (Elfardy andDiab, 2013) reports an accuracy of 80.4 % as perplexity-based baseline.
We have carried out additionalexperiments with a simple feature set that consists of only unigram token and bigram token features asdefined in Eq.
3.
Such a system performs surprisingly well under both testing conditions: we achieved anaccuracy of 87.7 % on the AOC data and an accuracy of 83.4 % on the DEV12 test set.
On the AOC setusing 10-fold cross validation, we achieve only a small improvement from using the dictionary featuresdefined in Eq.
4.
The accuracy is improved from 87.7 % to 88.0 %.
On the DEV12 set, we obtain amuch larger improvement from using these features.
Furthermore, we have investigated the usefulnessof the AIDA-based features.
The stand-alone sentence-level classification of the AIDA tool performsquite poorly.
On the DEV12 set, it achieves an accuracy of just 77.9 %.
But using the AIDA assignedsentence-level and token-level dialect labels based on the binary features defined in Eq.
5 improves ac-curacy significantly, e.g.
from 85.3 % to 87.8 % on the DEV12 set.
In the current experiments, theso-called meta features which are computed at the sentence level do not improve classification accuracy.The meta features are only useful in classifying dialect data based on the in-formalness of the data, i.e.the ARZ news commentaries tend to exhibit more in-formalness than the MSA commentaries.
Finally,the sentence-level perplexity feature defined in Eq.
6 did not improve accuracy as well (no results for thisfeature are presented in Table 3).4.3 Classifier AnalysisIn this section, we perform a simple error analysis of the classifier performance on some dialect data forwhich the degree of dialectalness is known.
The data comes from news sources that differ from the dataused to train the classifier.
The classifier is evaluated on data from the DARPA-funded BOLT project.114Feature Types MSA ARZACC [%] PREC REC F PREC REC F10-fold language-model 83.3 86.7 90.2 88.4 89.0 85.0 86.9AOC aida-sentence label 81.0 84.2 78.0 81.0 78.0 84.3 81.0uni,bi 87.7 86.6 90.2 88.4 89.0 85.0 86.9uni,bi,dict,pos 88.0 86.9 90.4 88.6 89.2 85.3 87.2uni,bi,dict,pos,aida 89.1 87.5 92.2 89.8 91.1 85.7 88.3uni,bi,dict,pos,aida,meta 88.8 87.4 91.7 89.5 90.6 85.7 88.1DEV12 language-model 82.2 85.1 76.2 80.4 80.0 87.7 83.7aida-sentence label 77.9 80.9 70.8 75.5 75.8 84.5 79.9uni,bi 83.4 81.1 85.1 83.1 85.6 81.7 83.6uni,bi,dict,pos 85.3 83.5 87.5 85.5 88.0 84.1 86.0uni,bi,dict,pos,aida 87.8 83.4 93.0 88.0 92.8 83.0 87.6uni,bi,dict,pos,aida,meta 68.3 61.8 90.8 73.5 85.0 48.3 61.6Table 3: Arabic Dialect Classification Results: predicting MSA vs. (ARZ) dialect in terms of 10-foldcross-validation on the AOC data and on the DEV12 set using all the AOC data for training.Corpus #Sent #Sent [ARZ] %[ARZ]ARZ web forum 299K 183K 61%Broadcast 169K 18K 11%Newswire 885K 29K 3%Table 4: Sub-corpora together with total number as well as percentage of sentences that are classified asARZ.The BOLT data consists of several corpora collected from various resources.
These resources includenewswire, web-logs, ARZ web forum data and others.
Classification statistics are presented in Table 4,where we report the number of sentences along with the percentage of those sentences classified as ARZ.The distribution of the dialect labels in the classifier output appears to correspond to the expected originof the data.
For example, the ARZ web forum data contains a majority of ARZ sentences, but quite afew sentences are MSA such as greetings and quotations from Islamic resources (Quran, Hadith ...).
Thebroadcast conversation data is mainly MSA, but sometimes the speaker switches to dialectal usage fora short phrase and then switches back to MSA.
Lastly, the newswire data has a vast majority of MSAsentences.
Examining a small portion of newswire sentences classified as ARZ, the sentences labeled asARZ are mostly classification errors.Example sentence classifications from the BOLT data are shown in Table 5.
The first two text frag-ments are taken from the Egyptian Arabic (ARZ) web forum data.
In the first document fragment, theuser starts with MSA sentences, then switches to Egyptian (ARZ) dialect marked by the ARZ indicator???
@ and using the prefix # H.before a verb which is not allowed in MSA.
The user then switches backto MSA.
The classifier is able to classify the Egyptian Arabic (ARZ) sentence correctly.
In the seconddocument fragment, the user uses several Egyptian Arabic (ARZ) words.
In the forth sentence no ARZwords exist, and the classifier correctly classifies the sentence as MSA.
The third text fragment shows115Predicted Arabic EnglishDialectMSA .
X?XQ?
@ ?
??????@H@Q?
AK @ i read the topic and the replies .MSA .???g?Q??
??????
@ the topic is great !ARZ ?
??K# H.???
@ pB@ ??
AK @ # ?
i agree with the brother who saidMSA?k.Ag ??
??
????KY?
@ Islam is significant in allARZ ZCJ.?
@???HQ.?
?X ?AJ?
@?A???
because they accept affliction with patienceARZ PA?JK @Q.?
@?X ?A?g?+I??????
@ ?
what Hamas did was a victoryARZ ?CJk@??
??
@????
?A?g?P who encountered the occupationMSA PA?k???
@?Q.?
?
and they were patient despite the siegeARZ ??+??A?
AK+ H.P ?Y??A???
that ?s why Allah rewarded themARZ* ??
?X ?GHXA?
Y?
# ?
tdk ...
ledARZ*???C?
@# H.??J?
@ Z @Q.g ?jJK# ?
transport experts blameARZ* .
?+ # ?
?+ ?A?
A?
Q?YK ?J??
@ B i cannot remember what he told meTable 5: Automatic classification examples for the dialect classes ARZ and MSA.
Arabic source andEnglish target sentences are given.
Dialectal words are in bold.
Incorrect predictions are marked by anasterisk (*).some sentences from the newswire corpus that are mis-classified.
The first sentence contains the word?X which corresponds to the letter ?d?
in the abbreviation ?tdk?.
The word is contained in one of our ARZdictionaries such that the binary AIDA-based feature in Eq.
5 fires and triggers a mis-classification.
Inthis context, the word is part of an abbreviation which is split in the Arabic text.
In the other examples,only a few of the binary features defined in Section 3.2 apply and features that correspond to Arabicprefixes tend to support a classification as ARZ dialect.4.4 Preliminary Application for SMTThe dialect classification of Arabic data for SMT can be used in various ways.
Examples include domain-specific tuning, mixture modeling, and the use of so-called provenance features (Chiang et al., 2011)among others .
As a motivation for the future use of the dialect classifier in SMT, we classify the BOLTbilingual training data into ARZ and MSA parts and examine the effect on the phrase table scores.
Phrasetranslation pairs demonstrating the use of the classified training data are shown in Table 6.
The ARZ webforum data is split into an ARZ part and an MSA part and two separate phrase probability tables aretrained on these two splits.
The ARZ web forum data is highly ambiguous with respect to dialect and itis difficult to obtain good dialect-dependent splits of the data.
In the first example in the table, the word?JK.Q??
@ could mean ?Arab?
in MSA, but in ARZ it could also mean ?car?.
The phrase table scores obtainedfrom the classifier-split training data correctly reflect this ambiguity.
The phrase pair with ?car?
has thelowest translation score for the BOLT.ARZ phrase table, while it has a higher cost in the BOLT.MSAphrase table.
In the full phrase table (BOLT), ?car?
is the fifth translation candidate with a score of 2.09.116BOLT.ARZ BOLT.MSAf e cost e cost?JK.Q??
@the car 1.20 arab 0.80arab 1.25 the arab 1.32the arab 1.70 Arabic 1.52??
?Q?merci 1.53 marsa 1.99marsa 1.63 thanks 2.01mursi 1.91 morcy 2.13Table 6: Phrase tables based on classified training data.
BOLT.ARZ is trained on the ARZ portion ofthe ARZ web forums data, while BOLT.MSA is trained on the MSA part.
The table includes Arabicwords and the top three phrase translation candidates, sorted (first is best) by the phrase model cost(cost= ?log(p(f |e)) ).In the second example, the word ??
?Q ?
could function as a proper noun with its English translation?mursi?
or ?marsa?, but only in ARZ it could also be translated as ?thanks?
(?merci?).
In this case, theclassifier is unable to distinguish between the ARZ dialect and the MSA usage.
We found out that theword token ?merci?
appears only 4 times in the training data, rendering its binary features unreliablereliable.
In general we note that the phrase tables build on the classified data become more domain-specific, and it is left to future work to check whether improvements could carry over to the translationquality.5 Discussion and Future WorkThe ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system.We split the training data at the sentence level using our classifier and train dialect-specific systemson each of these splits along with a general dialect-independent system.
We will be using techniquessimilar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adaptthe general SMT system to a target domain with a predominant dialect.
Or, we will be adopting anSMT system to a development or test set where we use the classifier to predict the dialect for eachsentence and use a dialect-specific SMT system on each of them individually.
Our approach of usingjust binary feature functions in connection with a sentence-level global linear model can be related towork on PoS-tagging (Collins, 2002).
(Collins, 2002) trains a linear model based on Viterbi decodingand the perceptron algorithm.
The gold-standard PoS tags are given at the word-level, but the traininguses a global representation at the sentence level.
Similarly, we use linear SVMs (Hsieh et al., 2008)to train a classification model at the sentence level without access to sentence length statistics, i.e.
ourbest performing classifier does not compute features like the percentage of punctuation, numbers, oraveraged word length as has been proposed previously (Elfardy and Diab, 2013).
All of our features areactually computed at the token level (with the exception of a single sentence-level AIDA-based feature).An interesting direction for future work could be to train the dialect classifier at the sentence level, butuse it to compute token-level predictions for a more fine-grained analysis.
Even though the token-levelprediction task corresponds to a word-level tag set of just size 2, Viterbi decoding techniques could beused to introduce novel context-dependent features, e.g.
dialect tag n-gram features.
Such a token-levelpredictions might be used for weighting each phrase pair in an SMT system using methods like theinstance-based adaptation approach in (Foster et al., 2010).AcknowledgementThe current work has been funded through the Broad Operational Language Translation (BOLT) programunder the project number DARPA HR0011-12-C-0015.117ReferencesTimothy Baldwin and Marco Lui.
2010.
Language Identification: The Long and the Short of the Matter.
In Proc.of HLT?10, pages 229?237, Los Angeles, California, June.William Cavnar and John M. Trenkle.
1994.
N-gram-based Text Categorization.
In In Proceedings of SDAIR-94,3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161?175.Boxing Chen, George Foster, and Roland Kuhn.
2013.
Adaptation of reordering models for statistical machinetranslation.
In Proc.
of HLT?13, pages 938?946, Atlanta, Georgia, June.David Chiang, Steve DeNeefe, and Michael Pust.
2011.
Two Easy Improvements to Lexical Weighting.
In Proc.of HLT?11, pages 455?460, Portland, Oregon, USA, June.Michael Collins.
2002.
Discriminative Training Methods for Hidden Markov Models: Theory and Experimentswith Perceptron Algorithms.
In Proceedings of EMNLP?02, pages 1?8, Philadelphia,PA, July.Ted Dunning.
1994.
Statistical Identification of Language.
technical report mccs 94-273.
Technical report, NewMexico State University.Heba Elfardy and Mona Diab.
2012.
Aida: Automatic Identification and Glossing of Dialectal Arabic.
In Pro-ceedings of the 16th EAMT Conference (Project Papers), pages 83?83, Trento, Italy, May.Heba Elfardy and Mona Diab.
2013.
Sentence level dialect Identification in arabic.
In Proc.
of the ACL 2013(Volume 2: Short Papers), pages 456?461, Sofia, Bulgaria, August.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR: a Libraryfor Large Linear Classification.
Machine Learning Journal, 9:1871?1874.Joseph L Fleiss.
1971.
Measuring nominal scale agreement among many raters.
Psychological Bulletin,76(5):378.George Foster, Cyril Goutte, and Roland Kuhn.
2010.
Discriminative Instance Weighting for Domain Adaptationin Statistical Machine Translation.
In Proc.
of EMNLP?10, pages 451?459.C.-J.
Hsieh, K.-W. Chang, C.-J.
Lin, S.S. Keerthi, and S.Sundararajan.
2008.
A Dual Coordinate Descent Methodfor Large-scale linear SVM.
In ICML, pages 919?926, Helsinki,Finland.Chu-Ren Huang and Lung-Hao Lee.
2008.
Contrastive Approach towards Text Source Classication based ontop-bag-of-word Similarity.
In PACLIC 2008, pages 404?410, Cebu City, Philippines.Philipp Koehn and Josh Schroeder.
2007.
Experiments in Domain Adaptation for Statistical Machine Translation.In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07), pages 224?227.Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara.
2005.
LanguageIdentification based on string kernels.
In In Proceedings of the 5th International Symposium on Communicationsand Information Technologies (ISCIT-2005, pages 896?899.Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, and Hany Hassan.
2003.
Language ModelBased Arabic Word Segmentation.
In Proc.
of the 41st Annual Conf.
of the Association for ComputationalLinguistics (ACL 03), pages 399?406, Sapporo, Japan, July.Marco Lui and Paul Cook.
2013.
Classifying English Documents by National Dialect.
In Proc.
AustralasianLanguage Technology Workshop, pages 5?15.Mona Mona Diab, Heba Elfardy, and Yassine Benajiba.
2009?2011.
AIDA Automatic Identification of Arabic Di-alectal Text.
a Tool for Dialect Identification & Classification, Named Entity Recognition, English and ModernStandard Arabic Glossing and Normalization.Rico Sennrich.
2012.
Perplexity minimization for translation model domain adaptation in statistical machinetranslation.
In Proc.
of EACL?12, pages 539?549.Liling Tan, Marcos Zampieri, Nicola Ljube?si?c, and J?org Tiedemann.
2014.
Merging Comparable Data Sourcesfor the Discrimination of Similar Languages: The DSL Corpus Collection.
In 7th Workshop on Building andUsing Comparable Corpora at LREC?14, Reykjavik, Iceland, September.D.
Trieschnigg, D. Hiemstra, M. Theune F. Jong, and T. Meder.
2012.
An Exploration of Language IdenticationTechniques for the Dutch Folktale Database.
In Adaptation of Language Resources and Tools for ProcessingCultural Heritage Workshop (LREC 2012), Istanbul, Turkey, May.118Omar F. Zaidan and Chris Callison-Burch.
2011.
Crowdsourcing translation: Professional quality from non-professionals.
In Proc.
of ACL / HLT 11, pages 1220?1229, Portland, Oregon, USA, June.Omar F. Zaidan and Chris Callison-Burch.
2014.
Arabic Dialect Classification.
CL, 40(1):171?202.Marcos Zampieri and Binyam Gebrekidan Gebre.
2012.
Automatic Identication of Language Varieties: The caseof Portuguese.
In Konvens 12, pages 233?237, Vienna, Austria.Bing Zhao and Yaser Al-Onaizan.
2008.
Generalizing Local and Non-Local word-reordering patterns for syntax-based machine translation.
In Proc.
of EMNLP?08, pages 572?581, Honolulu, Hawaii, October.119
