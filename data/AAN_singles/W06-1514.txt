Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 103?108,Sydney, July 2006. c?2006 Association for Computational LinguisticsGenerating XTAG Parsers from Algebraic Specifications?Carlos Go?mez-Rodr?
?guez and Miguel A. AlonsoDepartamento de Computacio?nUniversidade da Corun?aCampus de Elvin?a, s/n15071 A Corun?a, Spain{cgomezr, alonso}@udc.esManuel VilaresE.
S. de Ingenier?
?a Informa?ticaUniversidad de VigoCampus As Lagoas, s/n32004 Ourense, Spainvilares@uvigo.esAbstractIn this paper, a generic system that gener-ates parsers from parsing schemata is ap-plied to the particular case of the XTAGEnglish grammar.
In order to be able togenerate XTAG parsers, some transforma-tions are made to the grammar, and TAGparsing schemata are extended with fea-ture structure unification support and asimple tree filtering mechanism.
The gen-erated implementations allow us to studythe performance of different TAG parserswhen working with a large-scale, wide-coverage grammar.1 IntroductionSince Tree Adjoining Grammars (TAG) were in-troduced, several different parsing algorithms forthese grammars have been developed, each withits peculiar characteristics.
Identifying the advan-tages and disadvantages of each of them is nottrivial, and there are no comparative studies be-tween them in the literature that work with real-life, wide coverage grammars.
In this paper, weuse a generic tool based on parsing schemata togenerate implementations of several TAG parsersand compare them by parsing with the XTAG En-glish Grammar (XTAG, 2001).The parsing schemata formalism (Sikkel, 1997)is a framework that allows us to describe parsers ina simple and declarative way.
A parsing schema?
Partially supported by Ministerio de Educacio?n y Cien-cia and FEDER (TIN2004-07246-C03-01, TIN2004-07246-C03-02), Xunta de Galicia (PGIDIT05PXIC30501PN,PGIDIT05PXIC10501PN, PGIDIT05SIN044E andPGIDIT05SIN059E), and Programa de becas FPU (Mi-nisterio de Educacio?n y Ciencia).
We are grateful to EricVillemonte de la Clergerie and Franc?ois Barthelemy for theirhelp in converting the XTAG grammar to XML.is a representation of a parsing algorithm as aset of inference rules which are used to performdeductions on intermediate results called items.These items represent sets of incomplete parsetrees which the algorithm can generate.
An inputsentence to be analyzed produces an initial set ofitems.
Additionally, a parsing schema must de-fine a criterion to determine which items are final,i.e.
which items correspond to complete parses ofthe input sentence.
If it is possible to obtain a fi-nal item from the set of initial items by using theschema?s inference rules (called deductive steps),then the input sentence belongs to the language de-fined by the grammar.
The parse forest can then beretrieved from the intermediate items used to inferthe final items, as in (Billot and Lang, 1989).As an example, we introduce a CYK-basedalgorithm (Vijay-Shanker and Joshi, 1985) forTAG.
Given a tree adjoining grammar G =(VT , VN , S, I, A)1 and a sentence of length nwhich we denote by a1 a2 .
.
.
an2, we de-note by P (G) the set of productions {N?
?N?1 N?2 .
.
.
N?r } such that N?
is an inner node ofa tree ?
?
(I ?
A), and N?1 N?2 .
.
.
N?r is the or-dered sequence of direct children of N?
.The parsing schema for the TAG CYK-basedalgorithm (Alonso et al, 1999) is a function thatmaps such a grammar G to a deduction systemwhose domain is the set of items{[N?
, i, j, p, q, adj]}verifying that N?
is a tree node in an elementary1Where VT denotes the set of terminal symbols, VN theset of nonterminal symbols, S the axiom, I the set of initialtrees and A the set of auxiliary trees.2From now on, we will follow the usual conventions bywhich nonterminal symbols are represented by uppercase let-ters (A, B .
.
.
), and terminals by lowercase letters (a, b .
.
.
).Greek letters (?, ?...)
will be used to represent trees, N?
anode in the tree ?, and R?
the root node of the tree ?.103tree ?
?
(I ?
A), i and j (0 ?
i ?
j) are stringpositions, p and q may be undefined or instanti-ated to positions i ?
p ?
q ?
j (the latter onlywhen ?
?
A), and adj ?
{true, false} indi-cates whether an adjunction has been performedon node N?
.The positions i and j indicate that a substringai+1 .
.
.
aj of the string is being recognized, andpositions p and q denote the substring dominatedby ?
?s foot node.
The final item set would be{[R?, 0, n,?,?, adj] | ?
?
I}for the presence of such an item would indicatethat there exists a valid parse tree with yield a1 a2.
.
.
an and rooted at R?, the root of an initial tree;and therefore there exists a complete parse tree forthe sentence.A deductive step ?1...?m?
?
allows us to inferthe item specified by its consequent ?
from thosein its antecedents ?1 .
.
.
?m.
Side conditions (?
)specify the valid values for the variables appearingin the antecedents and consequent, and may referto grammar rules or specify other constraints thatmust be verified in order to infer the consequent.The deductive steps for our CYK-based parser areshown in figure 1.
The steps DScanCYK and D?CYK areused to start the bottom-up parsing process by rec-ognizing a terminal symbol for the input string, ornone if we are using a tree with an epsilon node.The DBinaryCYK step (where the operation p ?
p?
re-turns p if p is defined, and p?
otherwise) representsthe bottom-up parsing operation which joins twosubtrees into one, and is analogous to one of thedeductive steps of the CYK parser for CFG.
TheDUnaryCYK step is used to handle unary branching pro-ductions.
DFootCYK and DAdjCYK implement the adjunc-tion operation, where a tree ?
is adjoined into anode N?
; their side condition ?
?
adj(N?)
meansthat ?
must be adjoinable into the node N?
(whichinvolves checking that N?
is an adjunction node,comparing its label to R?
?s and verifying that noadjunction constraint disallows the operation).
Fi-nally, the DSubsCYK step implements the substitutionoperation in grammars supporting it.As can be seen from the example, parsingschemata are simple, high-level descriptions thatconvey the fundamental semantics of parsing algo-rithms while abstracting implementation details:they define a set of possible intermediate resultsand allowed operations on them, but they don?tspecify data structures for storing the results or anorder for the operations to be executed.
This highabstraction level makes schemata useful for defin-ing, comparing and analyzing parsers in pencil andpaper without worrying about implementation de-tails.
However, if we want to actually executethe parsers and analyze their results and perfor-mance in a computer, they must be implementedin a programming language, making it necessaryto lose the high level of abstraction in order to ob-tain functional and efficient implementations.In order to bridge this gap between theory andpractice, we have designed and implemented asystem able to automatically transform parsingschemata into efficient Java implementations oftheir corresponding algorithms.
The input to thissystem is a simple and declarative representationof a parsing schema, which is practically equal tothe formal notation that we used previously.
Forexample, this is the DBinaryCYK deductive step shownin figure 1 in a format readable by our compiler:@step CYKBinary[ Node1 , i , k , p , q , adj1 ][ Node2 , k , j , p?
, q?
, adj2 ]-------------------------------- Node3 -> Node1 Node2[ Node3 , i , j , Union(p;p?)
, Union(q;q?)
, false ]The parsing schemata compilation techniqueused by our system is based on the following fun-damental ideas (Go?mez-Rodr?
?guez et al, 2006a):?
Each deductive step is compiled to a Java classcontaining code to match and search for an-tecedent items and generate the correspondingconclusions from the consequent.?
The step classes are coordinated by a deduc-tive parsing engine, as the one described in(Shieber et al, 1995).
This algorithm ensuresa sound and complete deduction process, guar-anteeing that all items that can be generatedfrom the initial items will be obtained.?
To attain efficiency, an automatic analysis ofthe schema is performed in order to create in-dexes allowing fast access to items.
As eachdifferent parsing schema needs to perform dif-ferent searches for antecedent items, the indexstructures we generate are schema-specific.
Inthis way, we guarantee constant-time access toitems so that the computational complexity ofour generated implementations is never abovethe theoretical complexity of the parsers.?
Since parsing schemata have an open notation,for any mathematical object can potentiallyappear inside items, the system includes an ex-tensibility mechanism which can be used todefine new kinds of objects to use in schemata.104DScanCYK =[a, i, i + 1][N?
, i, i + 1 | ?,?
| false] a = label(N?)
D?CYK = [N?
, i, i | ?,?
| false] ?
= label(N?
)DUnaryCYK =[M?
, i, j | p, q | adj][N?
, i, j | p, q] | false] N?
?
M?
?
P(?)
DBinaryCYK =[M?
, i, k | p, q | adj1],[P ?
, k, j | p?, q?
| adj2][N?
, i, j | p ?
p?, q ?
q?
| false] N?
?
M?P ?
?
P(?
)DFootCYK =[N?
, i, j | p, q | false][F?
, i, j | i, j | false] ?
?
adj(N?)
DAdjCYK =[R?
, i?, j?
| i, j | adj],[N?
, i, j | p, q | false][N?
, i?, j?
| p, q | true] ?
?
adj(N?
)DSubsCYK =[R?, i, j | ?,?
| adj][N?
, i, j | ?,?
| false] ?
?
subs(N?
)Figure 1: A CYK-based parser for TAG.2 Generating parsers for the XTAGgrammarBy using parsing schemata as the ones in (Alonsoet al, 1999; Nederhof, 1999) as input to our sys-tem, we can easily obtain efficient implementa-tions of several TAG parsing algorithms.
In thissection, we describe how we have dealt with theparticular characteristics of the XTAG grammarin order to make it compatible with our genericcompilation technique; and we also provide em-pirical results which allow us to compare the per-formance of several different TAG parsing algo-rithms in the practical case of the XTAG gram-mar.
It shall be noted that similar comparisonshave been made with smaller grammars, such assimplified subsets of the XTAG grammar, but notwith the whole XTAG grammar with all its treesand feature structures.
Therefore, our compari-son provides valuable information about the be-havior of various parsers on a complete, large-scale natural language grammar.
This behavioris very different from the one that can be ob-served on small grammars, since grammar size be-comes a dominant factor in computational com-plexity when large grammars like the XTAG areused to parse relatively small natural language sen-tences (Go?mez-Rodr?
?guez et al, 2006b).2.1 Grammar conversionThe first step we undertook in order to generateparsers for the XTAG grammar was a full conver-sion of the grammar to an XML-based format, avariant of the TAG markup language (TAGML).In this way we had the grammar in a well-definedformat, easy to parse and modify.
During this con-version, the trees?
anchor nodes were duplicated inorder to make our generic TAG parsers allow ad-junctions on anchor nodes, which is allowed in theXTAG grammar.2.2 Feature structure unificationTwo strategies may be used in order to take uni-fication into account in parsing: feature structurescan be unified after parsing or during parsing.
Wehave compared the two approaches for the XTAGgrammar (see table 1), and the general conclusionis that unification during parsing performs betterfor most of the sentences, although its runtimeshave a larger variance and it performs much worsefor some particular cases.In order to implement unification during parsingin our parsing schemata based system, we must ex-tend our schemata in order to perform unification.This can be done in the following way:?
Items are extended so that they will hold a fea-ture structure in addition to the rest of the infor-mation they include.?
We need to define two operations on featurestructures: the unification operation and the?keep variables?
operation.
The ?keep vari-ables?
operation is a transformation on featurestructures that takes a feature structure as anargument, which may contain features, values,symbolic variables and associations betweenthem, and returns a feature structure contain-ing only the variable-value associations relatedto a given elementary tree, ignoring the vari-ables and values not associated through theserelations, and completely ignoring features.?
During the process of parsing, feature structuresthat refer to the same node, or to nodes that aretaking part in a substitution or adjunction and105Strategy Mean T. Mean 10% T. Mean 20% 1st Quart.
Median 3rd Quart.
Std.
Dev.
WilcoxonDuring 108,270 12,164 7,812 1,585 4,424 9,671 388,010 0.4545After 412,793 10,710 10,019 2,123 9,043 19,073 14,235Table 1: Runtimes in ms of an Earley-based parser using two different unification strategies: unificationduring and after parsing.
The following data are shown: mean, trimmed means (10 and 20%), quartiles,standard deviation, and p-value for the Wilcoxon paired signed rank test (the p-value of 0.4545 indicatesthat no statistically significant difference was found between the medians).are going to collapse to a single node in the finalparse tree, must be unified.
For this to be done,the test that these nodes must unify is added asa side condition to the steps that must handlethem, and the unification results are includedin the item generated by the consequent.
Ofcourse, considerations about the different roleof the top and bottom feature structures in ad-junction and substitution must be taken into ac-count when determining which feature struc-tures must be unified.?
Feature structures in items must only holdvariable-value associations for the symbolicvariables appearing in the tree to which thestructures refer, for these relationships hold theinformation that we need in order to propa-gate values according to the rules specified inthe unification equations.
Variable-value asso-ciations referring to different elementary treesare irrelevant when parsing a given tree, andfeature-value and feature-variable associationsare local to a node and can?t be extrapolated toother nodes, so we won?t propagate any of thisinformation in items.
However, it must be usedlocally for unification.
Therefore, steps performunification by using the information in their an-tecedent items and recovering complete featurestructures associated to nodes directly from thegrammar, and then use the ?keep-variables?
op-eration to remove the information that we don?tneed in the consequent item.?
In some algorithms, such as CYK, a single de-ductive step deals with several different elemen-tary tree nodes that don?t collapse into one in thefinal parse tree.
In this case, several ?keep vari-ables?
operations must be performed on eachstep execution, one for each of these nodes.
Ifwe just unified the information on all the nodesand called ?keep variables?
at the end, we couldpropagate information incorrectly.?
In Earley-type algorithms, we must take a de-cision about how predictor steps handle fea-ture structures.
Two options are possible: oneis propagating the feature structure in the an-tecedent item to the consequent, and the other isdiscarding the feature structure and generatinga consequent whose associated feature structureis empty.
The first option has the advantage thatviolations of unification constraints are detectedearlier, thus avoiding the generation of someitems.
However, in scenarios where a predic-tor is applied to several items differing only intheir associated feature structures, this approachgenerates several different items while the dis-carding approach collapses them into a singleconsequent item.
Moreover, the propagatingapproach favors the appearance of items withmore complex feature structures, thus makingunification operations slower.
In practice, forXTAG we have found that these drawbacks ofpropagating the structures overcome the advan-tages, especially in complex sentences, wherethe discarding approach performs much better.2.3 Tree filteringThe full XTAG English grammar contains thou-sands of elementary trees, so performance is notgood if we use the whole grammar to parse eachsentence.
Tree selection filters (Schabes and Joshi,1991) are used to select a subset of the grammar,discarding the trees which are known not to beuseful given the words in the input sentence.To emulate this functionality in our parsingschema-based system, we have used its exten-sibility mechanism to define a function Selects-tree(a,T) that returns true if the terminal symbol aselects the tree T. The implementation of this func-tion is a Java method that looks for this informa-tion in XTAG?s syntactic database.
Then the func-tion is inserted in a filtering step on our schemata:106[a, i, j][Selected, ?]
alpha ?
Trees/SELECTS-TREE(A;?
)The presence of an item of the form[Selected, ?]
indicates that the tree ?
hasbeen selected by the filter and can be used forparsing.
In order for the filter to take effect, weadd [Selected, ?]
as an antecedent to every stepin our schemata introducing a new tree ?
into theparse (such as initters, substitution and adjoiningsteps).
In this way we guarantee that no trees thatdon?t pass the filter will be used for parsing.3 Comparing several parsers for theXTAG grammarIn this section, we make a comparison of severaldifferent TAG parsing algorithms ?
the CYK-based algorithm described at (Vijay-Shankerand Joshi, 1985), Earley-based algorithms with(Alonso et al, 1999) and without (Schabes, 1994)the valid prefix property (VPP), and Nederhof?salgorithm (Nederhof, 1999) ?
on the XTAG En-glish grammar (release 2.24.2001), by using oursystem and the ideas we have explained.
Theschemata for these algorithms without unificationsupport can be found at (Alonso et al, 1999).These schemata were extended as described in theprevious sections, and used as input to our sys-tem which generated their corresponding parsers.These parsers were then run on the test sentencesshown in table 2, obtaining the performance mea-sures (in terms of runtime and amount of itemsgenerated) that can be seen in table 3.
Note thatthe sentences are ordered by minimal runtime.As we can see, the execution times are not asgood as the ones we would obtain if we usedSarkar?s XTAG distribution parser written in C(Sarkar, 2000).
This is not surprising, since ourparsers have been generated by a generic toolwithout knowledge of the grammar, while theXTAG parser has been designed specifically foroptimal performance in this grammar and uses ad-ditional information (such as tree usage frequencydata from several corpora, see (XTAG, 2001)).However, our comparison allows us to drawconclusions about which parsing algorithms arebetter suited for the XTAG grammar.
In termsof memory usage, CYK is the clear winner, sinceit clearly generates less items than the other al-gorithms, and a CYK item doesn?t take up morememory than an Earley item.On the other hand, if we compare executiontimes, there is not a single best algorithm, since theperformance results depend on the size and com-plexity of the sentences.
The Earley-based algo-rithm with the VPP is the fastest for the first, ?eas-ier?
sentences, but CYK gives the best results forthe more complex sentences.
In the middle of thetwo, there are some sentences where the best per-formance is achieved by the variant of Earley thatdoesn?t verify the valid prefix property.
Therefore,in practical cases, we should take into account themost likely kind of sentences that will be passedto the parser in order to select the best algorithm.Nederhof?s algorithm is always the one with theslowest execution time, in spite of being an im-provement of the VPP Earley parser that reducesworst-case time complexity.
This is probably be-cause, when extending the Nederhof schema inorder to support feature structure unification, weget a schema that needs more unification opera-tions than Earley?s and has to use items that storeseveral feature structures.
Nederhof?s algorithmwould probably perform better in relation to theothers if we had used the strategy of parsing with-out feature structures and then performing unifica-tion on the output parse forest.4 ConclusionsA generic system that generates parsers from al-gebraic specifications (parsing schemata) has beenapplied to the particular case of the XTAG gram-mar.
In order to be able to generate XTAG parsers,some transformations were made to the grammar,and TAG parsing schemata were extended withfeature structure unification support and a simpletree filtering mechanism.The generated implementations allow us tocompare the performance of different TAG parserswhen working with a large-scale grammar, theXTAG English grammar.
In this paper, we haveshown the results for four algorithms: a CYK-based algorithm, Earley-based algorithms withand without the VPP, and Nederhof?s algorithm.The result shows that the CYK-based parser is theleast memory-consuming algorithm.
By measur-ing execution time, we find that CYK is the fastestalgorithm for the most complex sentences, but theEarley-based algorithm with the VPP is the fastestfor simpler cases.
Therefore, when choosing aparser for a practical application, we should take1071.
He was a cow 9.
He wanted to go to the city2.
He loved himself 10.
That woman in the city contributed to this article3.
Go to your room 11.
That people are not really amateurs at intelectual duelling4.
He is a real man 12.
The index is intended to measure future economic performance5.
He was a real man 13.
They expect him to cut costs throughout the organization6.
Who was at the door 14.
He will continue to place a huge burden on the city workers7.
He loved all cows 15.
He could have been simply being a jerk8.
He called up her 16.
A few fast food outlets are giving it a tryTable 2: Test sentences.Sentence Runtimes in milliseconds Items generatedParser ParserCYK Ear.
no VPP Ear.
VPP Neder.
CYK Ear.
no VPP Ear.
VPP Neder.1 2985 750 750 2719 1341 1463 1162 12492 3109 1562 1219 6421 1834 2917 2183 21833 4078 1547 1406 6828 2149 2893 2298 23044 4266 1563 1407 4703 1864 1979 1534 20855 4234 1921 1421 4766 1855 1979 1534 20856 4485 1813 1562 7782 2581 3587 2734 27427 5469 2359 2344 11469 2658 3937 3311 34098 7828 4906 3563 15532 4128 8058 4711 47169 10047 4422 4016 18969 4931 6968 5259 527910 13641 6515 7172 31828 6087 8828 7734 834411 16500 7781 15235 56265 7246 12068 13221 1337612 16875 17109 9985 39132 7123 10428 9810 1001913 25859 12000 20828 63641 10408 12852 15417 1509414 54578 35829 57422 178875 20760 31278 40248 4757015 62157 113532 109062 133515 22115 37377 38824 5960316 269187 3122860 3315359 68778 152430 173128Table 3: Runtimes and amount of items generated by different XTAG parsers on several sentences.
Themachine used for all the tests was an Intel Pentium 4 / 3.40 GHz, with 1 GB RAM and Sun Java Hotspotvirtual machine (version 1.4.2 01-b06) running on Windows XP.
Best results for each sentence are shownin boldface.into account the kinds of sentences most likely tobe used as input in order to select the most suitablealgorithm.ReferencesM.
A. Alonso, D. Cabrero, E. de la Clergerie, and M.Vilares.
1999.
Tabular algorithms for TAG parsing.Proc.
of EACL?99, pp.
150?157, Bergen, Norway.S.
Billot and B. Lang.
1989.
The structure of sharedforest in ambiguous parsing.
Proc.
of ACL?89, pp.143?151, Vancouver, Canada.C.
Go?mez-Rodr?
?guez, J. Vilares and M. A.Alonso.
2006.
Automatic Generation ofNatural Language Parsers from DeclarativeSpecifications.
Proc.
of STAIRS 2006, Rivadel Garda, Italy.
Long version available athttp://www.grupocole.org/GomVilAlo2006a long.pdfC.
Go?mez-Rodr?
?guez, M. A. Alonso and M. Vilares.2006.
On Theoretical and Practical Complexity ofTAG Parsers.
Proc.
of Formal Grammars 2006,Malaga, Spain.M.-J.
Nederhof.
1999.
The computational complexityof the correct-prefix property for TAGs.
Computa-tional Linguistics, 25(3):345?360.A.
Sarkar.
2000.
Practical experiments in parsing us-ing tree adjoining grammars.
Proc.
of TAG+5, Paris.Y.
Schabes and A. K. Joshi.
1991.
Parsing with lexi-calized tree adjoining grammar.
In Masaru Tomita,editor, Current Issues in Parsing Technologies, pp.25?47.
Kluwer Academic Publishers, Norwell.Y.
Schabes.
1994.
Left to right parsing of lexical-ized tree-adjoining grammars.
Computational Intel-ligence, 10(4):506?515.S.
M. Shieber, Y. Schabes, and F. C. N. Pereira.
1995.Principles and implementation of deductive parsing.Journal of Logic Programming, 24(1?2):3?36.K.
Sikkel.
1997.
Parsing Schemata ?
A Frame-work for Specification and Analysis of Parsing Al-gorithms.
Springer-Verlag, Berlin.K.
Vijay-Shanker and A. K. Joshi.
1985.
Some com-putational properties of tree adjoining grammars.Proc.
of ACL?85, pp.
82?93, Chicago, USA.XTAG Research Group.
2001.
A lexicalized treeadjoining grammar for english.
Technical ReportIRCS-01-03, IRCS, University of Pennsylvania.108
