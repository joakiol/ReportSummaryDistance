A Multi-Perspective Evaluation of the NESPOLE!Speech-to-Speech Translation SystemAlon LavieCarnegie Mellon University, Pittsburgh, PA, USAalavie@cs.cmu.eduFlorian MetzeUniversity of Karlsruhe, GermanyRoldano CattoniITC-irst, Trento, ItalyErica CostantiniUniversity of Trieste, Trieste, ItalyAbstractPerformance and usability of real-world speech-to-speech translation sys-tems, like the one developed withinthe Nespole!
project, are aected byseveral aspects that go beyond thepure translation quality provided bythe underlying components of the sys-tem.
In this paper we describe theseaspects as perspectives along whichwe have evaluated the Nespole!
sys-tem.
Four main issues are investigated:(1) assessing system performance un-der various network trac conditions;(2) a study on the usage and utility ofmulti-modality in the context of multi-lingual communication; (3) a compar-ison of the features of the individualspeech recognition engines, and (4) anend-to-end evaluation of the system.1 IntroductionNespole!1 is a speech-to-speech machine tran-slation project designed to provide fully func-tional speech-to-speech capabilities within real-world settings of common users involved in e-commerce applications.
The project is a collab-oration between three European research groups1Nespole!
{ NEgotiation through SPOken Lan-guage in E-commerce.
See the project web-site athttp://nespole.itc.it for further details.
(IRST in Trento, Italy; ISL at Universita?t Karl-sruhe (TH); and CLIPS at Universite JosephFourier in Grenoble, France), one US researchgroup (ISL at Carnegie Mellon University inPittsburgh, PA) and two industrial partners(APT; Trento, Italy { the Trentino provincialtourism board, and AETHRA; Ancona, Italy {a tele-communications company).
The project isfunded jointly by the European Commission andthe US NSF.
Over the past two years, we havedeveloped a fully functional showcase of the Ne-spole!
system within the domain of travel andtourism, and have signicantly improved systemperformance and usability based on a series ofstudies and evaluations with real users.
Our ex-perience has shown that improving translationquality is only one of several important issuesthat must be addressed in achieving a practicalreal-world speech-to-speech translation system.This paper describes how we tackled these is-sues and evaluates their eect on system per-formance and usability.
We focus on four mainissues: (1) assessing system performance undervarious network trac conditions and architec-tural congurations; (2) a study on the usageand utility of multi-modality in the context ofmulti-lingual communication; (3) a comparisonof the features of the individual speech recogni-tion engines, and (4) an end-to-end evaluationof the demonstration system.Association for Computational Linguistics.Algorithms and Systems, Philadelphia, July 2002, pp.
121-128.Proceedings of the Workshop on Speech-to-Speech Translation:2 The NESPOLE!
SystemThe Nespole!
system (Lazzari, 2001) uses aclient-server architecture to allow a commonuser, who is initially browsing through the webpages of a service provider on the Internet, toconnect seamlessly to a human agent of the ser-vice provider who speaks another language, andprovides speech-to-speech translation service be-tween the two parties.
Standard commerciallyavailable PC video-conferencing technology suchas Microsoft?s NetMeeting r?
is used to connectbetween the two parties in real-time.In the rst showcase which we describe in thispaper, the scenario is the following: a clientis browsing through the web-pages of APT {the tourism bureau of the province of Trentinoin Italy { in search of tour-packages in theTrentino region.
If more detailed informationis desired, the client can click on a dedicated\button" within the web-page in order to estab-lish a video-conferencing connection to a humanagent located at APT.
The client is then pre-sented with an interface consisting primarily ofa standard video-conferencing application win-dow and a shared whiteboard application.
Us-ing this interface, the client can carry on a con-versation with the agent, where the Nespole!server provides two-way speech-to-speech trans-lation between the parties.
In the current setup,the agent speaks Italian, while the client canspeak English, French or German.2.1 System ArchitectureThe Nespole!
system architecture is shownin Figure 1.
A key component in the Ne-spole!
system is the \Mediator" module,which is responsible for mediating the commu-nication channel between the two parties aswell as interfacing with the appropriate HumanLanguage Technology (HLT) speech-translationservers.
The HLT servers provide the actualspeech recognition and translation capabilities.This system design allows for a very flexible anddistributed architecture: Mediators and HLT-servers can be run in various physical locations,so that the optimal conguration, given the lo-cations of the client and the agent and antic-Figure 1: The Nespole!
System Architectureipated network trac, can be taken into ac-count at any time.
A well-dened API allowsthe HLT servers to communicate with each otherand with the Mediator, while the HLT moduleswithin the servers for the dierent languages areimplemented using very dierent software pack-ages.
Further details of the design principles ofthe system are described in (Lavie et al, 2001).The computationally intensive part of speechrecognition and translation is done on dedicatedserver machines, whose nature and location isof no concern to the user.
A wide range ofclient-machines, even portable devices or pub-lic information kiosks, are therefore able to runthe client software, so that the service can bemade available nearly everywhere.The system architecture shown in Figure 1contains two dierent types of Internet connec-tions with dierent characteristics.
The connec-tion between Client/Agent PCs and the Media-tor is a standard video-conferencing connectionthat uses H323 and UDP protocols.
In casesof insucient network bandwidth, these proto-cols compromise performance by allowing de-layed or lost packets of data to be \dropped" onthe receiving side, in order to minimize delaysand ensure close to real-time performance.
Theconnection between the Mediator and the HLTservers uses TCP over IP in order to achieve loss-less communication between the Mediator andthe translation components.
For practical rea-sons, Mediator and HLT servers in our currentsystem usually run in separate and distant loca-tions, which can introduce some additional timedelay.
System response times in recent demon-strations have been about three times real-time.2.2 User interfaceThe user interface display is designed forWindows r?and consists of four windows: (1)a Microsoft r?Internet Explorer web browser;(2) a Microsoft r?Windows NetMeeting video-conferencing application; (3) the AeWhite-board; and (4) the Nespole Monitor.
UsingInternet Explorer, the client initiates the au-dio and video call with an agent of the serviceprovider, by a simple click of a button on thebrowser page.
Microsoft Windows Netmeeting isautomatically opened and the audio and videoconnection is established.
The two additionaldisplays { the AeWhiteboard and the NespoleMonitor are also launched at the same time.Client and agent can then proceed in carryingout a dialogue with the help of the speech trans-lation system.
For a screen snapshot of thesefour displays, see (Metze et al, 2002).We found it important to visually present as-pects of the speech-translation process to theend users.
This is accomplished via the Ne-spole Monitor display.
Three textual represen-tations are displayed in clearly identied elds:(1) a transcript of their spoken input (the outputfrom the speech recognizer); (2) a paraphrase oftheir input { the result of translating the recog-nized input back into their own language; and(3) the translated textual output of the utter-ance spoken by the other party.
These textualrepresentations provide the users with the capa-bility to identify mis-translations and indicateerrors to the other party.
A bad paraphrase isoften a good indicator of a signicant error inthe translation process.
When a mis-translationis detected, the user can press a dedicated but-ton that informs the other party to ignore thetranslation being displayed, by highlighting thetextual translation in red on the monitor displayof the other party.
The user can then repeat theturn.
The current system also allows the partic-ipants to correct speech recognition and transla-tion errors via keyboard input, a feature whichis very eective when bandwidth limitations de-grade the system performance.3 Multi-Perspective EvaluationsSeveral dierent evaluation experiments havebeen conducted, targeting dierent aspects ofour system: (1) the impact of network tracand the consequences of real packet-loss on sys-tem performance; (2) the impact and usability ofmulti-modality; (3) a comparison of the featuresof the various speech recognition engines, devel-oped independently for dierent languages withdierent techniques; and (4) end-to-end perfor-mance evaluations.
The data used in the evalu-ations is part of a database collected during theproject (Burger et al, 2001).3.1 Network Trac ImpactIn our various user studies and demonstrations,we have been forced to deal with the detrimentaleects of network congestion on the transmissionof Voice-over-IP in our system.
The critical net-work paths are the H323 connections betweenthe Mediator and the client and agent, whichrely on the UDP protocol in order to guaran-tee real-time, but potentially lossy, human-to-human communication.
This can potentially bevery detrimental to the performance of speechrecognizers (Metze et al, 2001).
The commu-nication between the Mediator and HLT serverscan, in principle, be within a local network, al-though we currently run the HLT servers at thesites of the developing partners.
This introducestime delays, but no packet loss, due to the useof TCP, rather than the UDP used for the H323connections.To quantify the influence of UDP packet-losson system performance, we ran a number of testswith German client installations in the USA(CMU at Pittsburgh) and Germany (UKA atKarlsruhe) calling a Mediator in Italy (IRST),which in turn contacted the German HLT serverlocated at UKA.
The tests were conducted byfeeding a high-quality recording of the German363738394041420 1 2 3 4 5 6WordError Rate(%)Packet Loss (%)ITA-USITA-GERFigure 2: Influence of packet loss on word accuracy ofthe German Nespole!
recognizerdevelopment test-set collected at the beginningof the project into a computer set-up for a video-conference, i.e.
we replaced the microphone bya DAT recorder (or a computer) playing a tape,while leaving everything else as it would be forsessions with real subjects.
In particular, seg-mentation was based on silence detection per-formed automatically by NetMeeting.
Each testconsisted of several dialogues, lasting about anhour.
These tests (a total of more than 16 hours)were conducted at dierent times of the day ondierent days of the week, in an attempt to in-vestigate a wide as possible variety of real-lifenetwork conditions.We were able to run 16 complete tests, re-sulting in an average word accuracy of 60.4%,2with single values in the 63% to 59% range forpacket-loss conditions between 0.1% and 5.2%.The results of these tests are presented in graph-ical from in Figure 2.
On a couple of occasionswe experienced abnormally bad network condi-tions for short periods of time.
These led to abreakdown of the Client-Mediator or Mediator-HLT server link due to time-out conditions beingreached, or the inability to establish a connec-tion at all.
We were able, however, to record onefull test with 21.0% packet loss, which resultedin a word accuracy of 50.3%.
These dialoguesare very dicult to understand even for humans.Our conclusion from the packet loss experi-2The word accuracy on the clean 16kHz recording is71.2%.ment is that our speech recognition engine isrelatively robust to packet loss rates of up to5%, since there is no clear degradation in theword accuracy of the recognizer as a functionof packet loss rate (in this range).
This is verygood news, since our experience indicates thatpacket loss rates of over 5% are quite rare un-der normal network trac conditions.
For 20%packet-loss, the increase in WER is signicant,but the degradation is less severe than that re-ported in (Milner and Semnani, 2000) on syn-thetic data.
We suspect that this is due to thenon-random distribution of lost packets.The tests described above were the rst phaseof our research on the impact of network tracon system performance.
We are currently in theprocess of conducting several further experimen-tal investigations concerning dierent conditionsin which the system may run:Transmission of video in addition to audiothrough the video-conferencing communi-cation channel: in this case we expect a sub-stantial increase in UDP packet-loss rates dueto audio and video competing for the networkbandwidth over the H323 connections.
It is notclear, however, how this competition takes placein practice and what are the resulting repercus-sions on the audio quality (and consequently onthe recognizers?
performance).The use of low-bandwidth network con-nections (such as standard 56Kbpsmodems): This is the most common networkscenario for real client users using a home in-stalled computer.
We are currently exploringhow the bandwidth limitations in this settingaect audio quality and system usability.
In lowbandwidth conditions, NetMeeting supports en-coding the speech with the G.723 codec, whichcan consume a much lower bandwidth (lessthen 6.4Kbps) compared to the G.711 codec(64Kbps), which we currently use in our system.We are in the process of testing the G.723 codecwithin our system.
Preliminary results indicatethat the recognizers used in the Nespole!
sys-tem are quite robust with respect to this newfront-end processing.3.2 Experiments on Multi-ModalityThe nature of the e-commerce scenario and ap-plication in which our system is situated re-quires that speech-translation be well-integratedwith additional modalities of communicationand information exchange between the agentand client.
Signicant eort has been devotedto this issue within the project.
The mainmulti-modal component in the current versionof our system is the AeWhiteboard { a specialwhiteboard, which allows users to share mapsand web-pages.
The functionalities providedby the AeWhiteboard include: image loading,free-hand drawing, area selecting, color choos-ing, scrolling the image loaded, zooming the im-age loaded, URL opening, and Nespole!
Monitoractivation.
The most important feature of thewhiteboard is that each gesture performed by auser is mirrored on the whiteboard of the otheruser.
Both users communicate while viewing thesame images and annotated whiteboards.Typically, the client asks for spatial informa-tion regarding locations, distances, and naviga-tion directions (e.g., how to get from a hotelto the ski slopes).
By using the whiteboard,the agent can indicate the locations and drawroutes on the map, point at areas, select items,draw connections between dierent locations us-ing a mouse or an optical pen, and accompanyhis/her gestures with verbal explanations.
Sup-porting such combined verbal and gesture in-teractions has required modications and exten-sions of both HLT modules and the IF.During July 2001, we conducted a detailedstudy to evaluate the eect of multi-modality onthe communication eectiveness and usability ofour system.
The goals of the experiment wereto test: (1) whether multi-modality increasesthe probability of successful interaction, espe-cially when spatial information is the focus ofthe communicative exchange; (2) whether multi-modality helps reduce mis-communications anddisfluencies; and (3) whether multi-modalitysupports a faster recovery from recognition andtranslation errors.
For these purposes, two ex-perimental conditions were devised: a speech-only condition (SO), involving multilingual com-munication and the sharing of images; and amulti-modal condition (MM), where users couldadditionally convey spatial information by pen-based gestures on shared maps.The setting for the experiment was the sce-nario described earlier, involving clients search-ing for winter tour-package information in theTrentino province.
The client?s task was to se-lect an appropriate resort location and hotelwithin the specied constraints concerning therelevant geographical area, the available bud-get, etc.
The agent?s task was to provide thenecessary information.
Novice subjects, previ-ously unfamiliar with the system and task wererecruited to play the role of the clients.
Subjectswore a head-mounted microphone, using it in apush-to-talk mode, and drew gestures on mapsby means of a table-pen device or a mouse.
Eachsubject could only hear the translated speech ofthe other party (original audio was disabled inthis experiment).
28 dialogues were collected,with 14 dialogues each for English and for Ger-man clients, and Italian agents in all cases.
Eachgroup contained 7 SO and 7 MM dialogues.
Thedialogue transcriptions include: orthographicaltranscription, annotations for spontaneous phe-nomena and disfluencies, turn information andannotations for gestures.
Translated turns wereclassied into successful, partially successful andunsuccessful by comparing the translated turnswith the responses they generated.
Repeatedturns were also counted.The average duration of dialogues was 35 min-utes (35.8 for SO and 35.5 for MM).
On aver-age, a dialogue contained 35 turns, 247 tokensand 97 token types per speaker.
Average val-ues and variance of all measures are very similarfor agents and clients and across conditions andLanguages.
ANOVA tests (p=0.05) on the num-ber of turns and the number of spontaneous phe-nomena and disfluencies, agents and customersseparately, did not produce any evidence thatmodality or language aected these variables.Hence the spoken input is homogeneous acrossgroups.
Details on the experimental databasecollected and the various statistical analyses per-formed appear in (Costantini et al, 2002).
Theanalysis of the results indicated that both theSO and MM versions of the system were eec-tive for goal completion: 86% of the users wereable to complete the task?s goal by choosing ahotel meeting the pre-specied budget and loca-tion constraints.In the MM dialogues, there were 7.6 gesturesper dialogue on average.
The agents performedalmost all gestures (98%), with a clear prefer-ence for area selections (61% of total gestures).Most gestures (79%) followed a dialogue con-tribution; none of the gestures were performedduring speech.
Overall, few or no deictics wereused.
We believe that these ndings are relatedto the push-to-talk procedure and to the timeneeded to transfer gestures across the network:agents often preceded gestures with appropriateverbal cues e.g., \I?ll show you the hotel on themap", in order to notify the other party of anupcoming gesture.
These verbal cues indicatethat gestures were well integrated in the com-munication.We found signicant dierences between theSO and MM dialogues in terms of unsuccessfuland repeated turns, particularly so in the spatialsegments of the dialogues.
In the English-Italiandialogues the MM dialogues contained 19% un-successful turns versus 30% for the SO dialogues.For German-Italian dialogues we found 18% inMM versus 31% in SO.
English-Italian MM dia-logues contained 11% repeated turns versus 17%for SO.
For German-Italian dialogues repeatedturns amounted to 18% for MM versus 23% forSO.
In addition we found smoother dialoguesunder MM condition, with fewer returns to al-ready discussed topics for MM (one return every19 turns in SO versus one return every 31 turnsin MM).
MM also exhibited a lower number ofdialogue segments containing identiable misun-derstandings between the two parties (one suchsegment in each of 3 of the MM dialogues, ver-sus a total of seven such segments in the SO dia-logues { one dialogue with 3 segments, one withtwo, and a third with a single segment of mis-communication).
Furthermore, the misunder-standings in MM conditions were often immedi-ately solved by resorting to MM resources, whilein case of SO ambiguous or mis-understood sub-dialogues often remained unresolved.
Finally,the experiment subjects, given the choice be-tween the MM and the SO system, expresseda clear preference for the former.
In summary,we found strong supporting evidence that mul-timodality has a positive eect on the qualityof interaction by reducing ambiguity, making iteasier to resolve ambiguous utterances and to re-cover from system errors, improving the flow ofthe dialogue, and enhancing the mutual compre-hension between the parties, in particular whenspatial information is involved.3.3 Features of Automatic SpeechRecognition EnginesThe Speech Recognition modules of the Nespo-le!
system were developed separately at the dif-ferent participating sites, using dierent toolk-its, but communicate with the Mediator usinga standardized interface.
The French and Ger-man ASR modules are described in more detailin (Vaufreydaz et al, 2001; Metze et al, 2001).The German engine was derived from the UKArecognizer developed for the German VerbmobilTask (Soltau et al, 2001).All systems were derived from existingLVCSR recognizers and adapted to the Nespo-le!
task using less than 2 hours of adaptationdata.
This data was collected during an initialuser-study, in which clients from all countriescommunicated with an APT agent fluent in theirmother tongue through the Nespole!
system,but without recognition and translation compo-nents in place.
Segmentation of input speech isdone based on automatic silence detection per-formed by NetMeeting at the site of the origi-nating audio.
The audio is encoded accordingto the G.711 standard at a sampling frequencyof 8kHz.
The characteristics of the dierent rec-ognizers are summarized in Table 1.
The wordaccuracy rates of the recognizers are presentedin Section 3.4.3.4 End-to-End System EvaluationIn December 2001, we conducted a large scalemulti-lingual end-to-end translation evaluationof the Nespole!
rst-showcase system.
Foreach of the three language pairs (English-Italian,German-Italian and French-Italian), four previ-English French German ItalianVocabulary size 8,000 20,000 12,000 4,000OOV rate 0.3% <1% 3.0%LM training Verbmobil (E), C-Star Internet Verbmobil (D) C-StarData 550k words 1,500M words 500k words 100k words+ adaptation Nespole none Nespole NespolePerplexity 33 98 150Microphone type head-set head-set table-top head-setSpeaking style spontaneous read spontaneous readAc.
training 16kHz G711 recoded 16kHz G711 recodedData 90h 12h 65h Verbmobil-II 11h C-Star+ adaptation Up-sampling of G711 MLLR 80min.
+ FSAReal-time factor 2.5, 1GHz P-III 1.1, 1GHz P-III 1.8, 650Mhz P-IIIMemory consumption 280Mb 200Mb 100Mb 100MbWER on clean data 19.9% 28% 29.8% 31.5%Table 1: Features of the Speech Recognition EnginesLanguage WARs SR Graded (% Acc)English 61.9% 66.0%German 63.5% 68.0%French 71.2% 65.0%Italian 76.5% 70.6%Table 2: Speech Recognition Word Accuracy Rates andResults of Human Grading (Percent Acceptable) of Recog-nition Output as a ParaphraseLanguage Transcribed Speech Rec.English-to-English 58% 45%German-to-German 46% 40%French-to-French 54% 41%Italian-to-Italian 61% 48%Table 3: Monolingual End-to-End Translation Results(Percent Acceptable) on Transcribed and Speech Recog-nized Inputously unseen test dialogues were used to evaluatethe performance of the translation system.
Thedialogues included two scenarios: one coveringwinter ski vacations, the other about summerresorts.
One or two of the dialogues for each lan-guage contained multi-modal expressions.
Thetest data included a mixture of dialogues thatwere collected mono-lingually prior to systemdevelopment (both client and agent spoke thesame language), and data collected bilingually(during the July 2001 MM experiment), usingthe actual translation system.
This mixture ofdata conditions was intended primarily for com-prehensiveness and not for comparison of the dif-ferent conditions.We performed an extensive suite of evalua-Language Transcribed Speech Rec.English-to-Italian 55% 43%German-to-Italian 32% 27%French-to-Italian 44% 34%Italian-to-English 47% 37%Italian-to-German 47% 31%Italian-to-French 40% 27%Table 4: Cross-lingual End-to-End Translation Results(Percent Acceptable) on Transcribed and Speech Recog-nized Inputtions on the above data.
The evaluations wereall end-to-end, from input to output, not as-sessing individual modules or components.
Weperformed both mono-lingual evaluation (wheregenerated output language was the same as theinput language), as well as cross-lingual evalu-ation.
For cross-lingual evaluations, translationfrom English German and French to Italian wasevaluated on client utterances, and translationfrom Italian to each of the three languages wasevaluated on agent utterances.
We evaluated onboth manually transcribed input as well as onactual speech-recognition of the original audio.We also graded the speech recognized output asa \paraphrase" of the transcriptions, to measurethe levels of semantic loss of information dueto recognition errors.
Speech recognition wordaccuracies and the results of speech graded asa paraphrase appear in Table 2.
Translationswere graded by multiple human graders at thelevel of Semantic Dialogue Units (SDUs).
Foreach data set, one grader rst manually seg-mented each utterance into SDUs.
All gradersthen used this segmentation in order to assignscores for each SDU present in the utterance.We followed the three-point grading scheme pre-viously developed for the C-STAR consortium,as described in (Levin et al, 2000).
Each SDU isgraded as either \Perfect" (meaning translatedcorrectly and output is fluent), \OK" (meaningis translated reasonably correct but output maybe disfluent), or \Bad" (meaning not properlytranslated).
We calculate the percent of SDUsthat are graded with each of the above cate-gories.
\Perfect" and \OK" percentages are alsosummed together into a category of \Accept-able" translations.
Average percentages are cal-culated for each dialogue, each grader, and sep-arately for client and agent utterances.
We thencalculated combined averages for all graders andfor all dialogues for each language pair.Table 3 shows the results of the monolingualend-to-end translation for the four languages,and Table 4 shows the results of the cross-lingual evaluations.
The results indicate accept-able translations in the range of 27{43% of SDUs(interlingua units) with speech recognized in-puts.
While this level of translation accuracycannot be considered impressive, our user stud-ies and system demonstrations indicate that it isalready sucient for achieving eective commu-nication with real users.
We expect performancelevels to reach a range of 60{70% within the nextyear of the project.AcknowledgementsAdditional Authors: S. Burger, D. Gates, C.Langley, K. Laskowski, L. Levin, K. Peterson, T.Schultz, A. Waibel, D. Wallace, Carnegie Mel-lon University; J. McDonough, H. Soltau, Uni-versity of Karlsruhe, Germany; G. Lazzari, N.Manna, F. Pianesi, E. Pianta, ITC-irst, Trento,Italy; L. Besacier, H. Blanchon, D. Vaufreydaz,Universite Joseph Fourier, Grenoble, France; L.Taddei, AETHRA, Ancona, Italy.This work was supported by NSF Grant9982227 and EU Grant IST 1999-11562 as partof the joint EU/NSF MLIAM research initiative.ReferencesSusanne Burger, Laurent Besacier, Paolo Coletti,Florian Metze, and Celine Morel.
2001.
The NE-SPOLE!
VoIP Dialogue Database.
In Proc.
Eu-roSpeech 2001, Aalborg, Denmark.
ISCA.Erica Costantini, Susanne Burger, and Fabio Pianesi.2002.
Nespole!
?s multilingual and multimodal cor-pus.
In Proceedings of the Third InternationalConference on Language Resources and Evalua-tion (LREC-2002), Grand Canary Island, Spain,June.
To appear.Alon Lavie, Fabio Pianesi, and al.
2001.
Architec-ture and Design Considerations in NESPOLE!
: aSpeech Translation System for E-Commerce Ap-plications.
In Proc.
of the HLT2001, San Diego,CA.
ACM.Gianni Lazzari.
2001.
Spoken translation: chal-lenges and opportunities.
In Proc.
ICSLP 2001,Beijing, China, 10.Lori Levin, Donna Gates, Fabio Pianesi, Donna Wal-lace, Takeshi Watanabe, and Monika Woszczyna.2000.
Evaluation of a Practical Interlingua forTask-Oriented Dialogues.
In Proceedings NAACL-2000 Workshop On Interlinguas and InterlingualApproaches, Seattle, WA.
AMTA.Florian Metze, John McDonough, and Hagen Soltau.2001.
Speech Recognition over NetMeeting Con-nections.
In Proc.
EuroSpeech 2001, Aalborg,Denmark.
ISCA.Florian Metze, John McDonough, Hagen Soltau,Alex Waibel, Alon Lavie, Susan Burger, ChadLangley, Kornel Laskowski, Lori Levin, TanjaSchultz, Fabio Pianesi, Roldano Cattoni, GianniLazzari, Nadia Mana, Emanuele Pianta, LaurentBesacier, Herve Blanchon, Dominique Vaufreydaz,and Loredana Taddei.
2002.
The NESPOLE!Speech-to-Speech Translation System.
In Proc.HLT 2002, San Diego, CA, 3.Ben Milner and Sharam Semnani.
2000.
RobustSpeech Recognition over IP Networks.
In Pro-ceedings of International Conference on AcousticsSpeech and Signal Processing (ICASSP-00), Istan-bul, Turkey, June.Hagen Soltau, Thomas Schaaf, Florian Metze, andAlex Waibel.
2001.
The ISL Evaluation Systemfor Verbmobil - II.
In Proc.
ICASSP 2001, SaltLake City, USA, 5.D.
Vaufreydaz, L. Besacier, C. Bergamini, andR.
Lamy.
2001. presented at ISCA ITRW Work-shop on Adaptation Methods for Speech Recogni-tion, August.
Sophia-Antipolis, France.
