IIIl//l/////Cross-Entropy and Linguistic Typology/ .Pat r i ck  Juo laDepar tment  of  Exper imenta l  PsychologyUnivers i ty  of OxfordOxford,  UK  OX1 3UDpat r i ck ,  j uo la@psy ,  ox.
ac .
ukAbst rac tThe idea of '~familial relationships" among lan-guages is well-established and accepted, al-though some controversies persist in a fewspecific instances.
By painstakingly record-ing and identifying regularities and similaritiesand comparing these to the historical record,linguists have been able to produce a general"family tree" incorporating most natural an-guages.We suggest here that much of these trees canbe automatically determined by a complemen-tary technique of distributional nalysis.
Re-cent work by (Farach et al, 1995) and (Juola,1997) suggests that Kullback-Leibler diver-gence (or cross-entropy) can be meaningfullymeasured from small samples, in some casesas small as only 20 or so words.
Using thesetechniques, we define and measure a distancefunction between translations ofa small corpus(c. 70 words/sample) covering much of the ac-cepted Indo-European family, and reconstructa relationship tree by hierarchical duster anal-ysis.
The resulting tree shows remarkable sim-ilarity to the accepted Indo-European family;this we read as evidence both for the immensepower of this measurement technique and forthe validity of this kind of mechanical similar-ity judgement in the identification of typologi-cal relationships.
Furthermore, this techniqueis in theory sensitive to different sorts of rela-tionships than more common word-list basedmethods and may help ilium;hate these from adifferent direction.1 I n t roduct ionOver the past century, a large amount of researcheffort has gone into the establishment of structuresdescribing the typological and taxonomic relation-ships among languages past and present; the well-known "Romance language" group, consisting of allthe languages in some sense "descended from" Latinis an example.
In addition to their inherent interest,the results of these studies can be of use in telling usabout the relationships, cultures, and environmentsof people and tribes long-distant from our presentworld.Although these techniques are powerful, they arelimited in their application in several ways.
Thetraditional focus on word lists as the primary toolfor language classification excludes yntax and mor-phology from consideration.
By constructing theseword lists out of only basic lexical items, the appli-cability is further limited.
Although in theory theseproblems could be avoided by simply constructingdifferent lists, there is still a problem with the vol-ume of data to be processed - -  if the comparisonsare performed at the level of "language," it is dif-ficult if not impossible to discuss questions uch aswhether "legal English" shows more French influencethan "standard English" or vice versa.
However,the answers (were they available) to questions likethis could be useful to, for example, socioliaguists inattempting to trace the relationships between andamong subgroups within a culture.The results presented in this paper suggest hatdistributional nalyses can provide much of the samesort of relationships, but by a different route andtherefore with different limitations and complemen-tary to more standard techniques.
This is develolSedfurther in a set of experiments which approximatelyreconstruct the accepted Indo-European family treebased on samples of running text of less than a pagein length (and, in fact, typically under 70 words).2 TaxonomyGiven the broad agreement found on the taxonomicrelationships among languages \[for example, see theintroductory textbooks by (Gleason, 1955; Crystal,1987; Finegan and Besnier, 1987), or the more au-thoritative (Bright, 1992; Asher and Simpson, 1994;Warnow, 1997)\] the classifications and relationshipsof figure 1 can be described as uncontroversial.
Forexample, the languages of Dutch and German areJuola 141 Cross-Entropy and Linguistic TypologyPatrick Juola (1998) Cross-Entropy and Linguistic Typology.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods inLanguage Processing and Computational Natural Language Learning, ACL, pp 141-149.Indo-Europeant Aus ooosiaUralicGermanic,SlavonicItalicW.
GermanicN.
GermanicDutchGermanEnglishDanishRussianFrenchMaoriFinnishFigure 1: Genetic taxonomy of various languagesrather self-evidently similar; they are also closelylinked in terms of history, culture, and linguistic bor-rowing; this similarity is one of the sources of evi-dence for such linkages.
Meanwhile, there's little orno evidence that the Germans and the Maori wereever in significant day-to-day contact, a judgementborne out by apparent dissimilarity.
The most con-troversial point of the diagram, as a matter of fact,may be its tree-like structure, as will be discussedlater.The usual method for generating such trees (orother representational structures) is to painstakinglycompare representative samples of language, usuallylists of lexical items, and identify similar or isomor-phic changes from among the lists (taking into ac-count historical and archeological evidence as appro-priate).
(Swadesh, 1955), for example, has identifieda hundred basic concepts that are, in theory, partof the basic vocabulary of a language and thus re-sistant to borrowing and replacement and subjectonly to the slow "evolutionary" pressures of linguis-tic change.
By comparing the presentation of theseconcepts as lexical items and measuring the degreeof change between two languages' presentations, onecan determine the amount by which two languageshave "drifted.
"In summary of the results of these and similarstudies, (Finegan and Besnier, 1987) identify no lessthan eleven subgroups within the Indo-Europeanfamily.
In addition to the weli-known groups likeGermanic, Italic, and "Slavonic" (described here),they list Albanian, Anatolian, Armenian, Baltic,Celtic, Greek, Indo-Iranian, and Tocharian.
(Crys-tal, 1987) groups Baltic and Slavic but otherwiseagrees with Finegan and Besnier, as does (Gleason,1955).
This shows both the power of this techniqueas well as the degree to which it requires ubjec-tive evaluation; the overall relationships are gener-ally agreed upon, but "the devil is in the details"and opinions about exactly which changes are simi-lar remain to a certain extent educated guesses.Other minor problems with this technique xist;for example, Swadesh's vocabulary list is completelyinsensitive to other aspects of language such as mor-phology, syntax, and so forth.
Because of its fo-cus on specific, basic words, it can be trapped (ortricked) by lexical drift (for example, "meat" is nolonger the English word for "any foodstuff") or lex-ical holes where a clear cognate is not necessarilythe most common or most frequent lexeme ((Forsteret al, in press) has found that some of his Alpinelanguages have no lexeme for "to sit," for example.
)Similar problems exist with regard to lexical bor-rowing; resistant o borrowing does not equate toproof against borrowing.
Finally, this focus on thesevery basic terms and the evaluation of language as awhole may, to a certain extent, preclude the analysisof the paths of borrowing and the degree to whichlinguistic change is confined to or driven by partic-ular fields, social strata, and so forth.
By confin-ing ourselves to pre-set lists of specific concepts, oneruns the risk of picking the wrong concepts, espe-cially for specific sub-field s (which can be as finelysubdivided as one likes; is this paper an example of"science," of "computer science," of "computationallinguistics," or of "information-theoretic approachesto corpus-based computational linguistics"?)
As asimple example, the phrase for "TCP/IP protocol"in most languages of the world is recognizably a bor-rowing from English, while much of the jargon in themartial arts community shows a strong Japanese in-fluence, even when the martial art itself derives fromother countries or cultures.This suggests that there is a place for other mea-sures, both of language-in-use and of smaller sam-ples, as a supplement to traditional typological andtaxonomic measures.
The claim made here is thatcross-entropy (or Kullback-Leibler divergence) canbe the basis for such a measurement.3 Ent ropy  Es t imat ion3.1 BackgroundEnglish, as is well-known, is very predictable.
Flu-ent English readers can confirm this for themselvesby guessing which letter comes next in a word be-~aming psyc-.
Experiments by (Shannon, 1951) in-dicate that most readers can guess more than halfof the letters in running text based on their expertknowledge of the lexicon, structure, and semanticsof English.This notion of predictability, as well as the asso-ciated concepts of complexity, compressiveness, andrandomness, can be mathematically modelled usinginformation entropy.
As developed by (Shannon,Juola 142 Cross-Entropy and Linguistic Typology!i|IIIIIIIIIIilIIIIIII!mIIIIIIIIIIIIII1948), the entropy of a (stationary, ergodic) messagesource is the amount of information, typically mea-sured in bits (yes/no questions), required to describethe successive messages emitted by that source to arecipient.
As the set of possible messages becomeslarger, or the distribution of messages becomes lesspredictable, the entropy of the source increases cor-respondingly, in accordance with Shannon's equa-tion:NH(P)  = - Z Pi" logz Pi (1)i= lwhere P is (the probability distribution of) asource capable of sending any of the messages1, 2, .
.
.
,  N, each with some probability Pi.
(For con-tinuous distributions, imply replace the summationwith the appropriate integral.
)An important aspect of this brief description hassignificant ypological and taxonomic implications.Against what is the predictability ofthe distributionmeasured?
The second term in the above equationis a measure of the efficiency of the representation fmessage i (obviously, more frequent messages shouldbe made shorter for maximal efficiency, an observa-tion often attributed to Zipf), based on our estimateof the frequency with which i is transmitted.
There-fore, we can generalize equation 1 toN/:/(P' Q) = - Z Pi" log2 qi (2)i----1where Q is a different distribution representingour best estimate of the true distribution P. Thisvalue (called the cross-entropy) achieves a minimumwhen P = Q, and H(P, P) = H(P) .
The differencebetween/: /and H , the so-called Kullback-Leiblerdivergence, can be taken as a measurement of thedegree of similarity between P and Q.1 For furtherelaboration on this point, the reader is referred tothe excellent treatment in (Bishop, 1995).This technique lends itself to a measurement ofsimilarity between two different sources, by estimat-ing the distributional parameters and calculatingtheir cross-entropy.3.2 MethodObviously, much research has been done in theproper development of distributional models of En-glish (or other languages) and in the efficient estima-tion of the probability distribution; (Brown et al,1N.b.
this is not a "distance metric" in the formalsense of the word (it's not symmetric, for one thing),but can be thought of as a distance for these purposes.1992) calculate the entropy of a statistical modelof English that was produced by training a com-puter on literally billions of observations comprisinga huge corpus of written English.
(Wyner, in press)has suggested that one can determine the entropyto nearly as good accuracy based on much smallersample sizes, but it remains an open research ques-tion how much text is actually needed.
At billionsof observations per test, it is obviously impracticalto determine document-level properties (such as, forinstance, authorship, register, difficulty of reading,or even the language in which a novel document iswritten), but if the tests can be made sufficientlysensitive to work with small texts, tests like this maybe practical.
(Farach et al, 1995; Wyner, in press) describea novel algorithm for entropy estimation for whichthey claim very fast convergence time; using no morethan about five pages of text, they can achieve nearlythe same accuracy as (Brown et al, 1992).
Theheart of this technique is a measurement of "matchlength within a database."
Wyner defines the matchlength Ln(x) of a sequence (xl, x2, .
.
.
,  xn, xn+x,...)as the length of the the longest prefix of the sequence(xn+x,...) that matches a contiguous ubstring of(z l , z2 , .
.
.
,xn), and proves that this converges inthe limit to the value ~ as n increases.A simple example should make this more clear :we consider for a moment he phraseHAMLET : TO BE OR NOT TO BE THAT ISTHE QUESTIONand fix n at 21.
Thus, the "database" is the char-acters "HAMLET : TO BE OR NOT" (length 21)and the string " TO BE THAT IS THE QUES-TION" is the remaining data; the prefix " TO BE"exactly matches the contiguous substring beginningof the eighth character and itself runs for seven char-acters, but the prefix " TO BE T" does not match acontinuous substring of the database, and hence thematch length L21 is seven.Using this technique, one can estimate the en-tropy of a sequence by sliding a block of n obser-vations along the sequence and calculating the themean match length L (averaged over each step) andthus the estimated entropy/:/.
So one calculates L21above, then calculates L21 for the string "AMLET: TO BE OR NOT TO BE THAT IS THE QUES-TION ", then for "MLET : TO BE OR NOT TOBE THAT IS THE QUESTION W',  and so on.The application of this to measurement of cross-entropy is relatively straightforward.
A "database"of n observations i  compiled for each language ofinterest and each successive symbol of the messagestream of interest is used as the starting point forJuola .
?
143 Cross-Entropy and Linguistic Typologythe maximal prefix to be found within the database.Although this loses some of the time-varying prop-erties of an entropy estimator (in particular, thedatabase is fixed and will not shift to capture long-term regularities in an input stream), this shouldpreserve the fundamental relationship that a closerfit (smaller cross-entropy) results in a longer meanmatch length.
This permits us to measure cross-entropy with approximately the same convergenceproperties as the entropy estimation itself.The primary claim made in this paper is that thesimilarity measured by cross-entropy will have someof the same properties for typological nd taxonomicresearch as those of more conventional word-lists,but that cross-entropy is complementary in severalways.
It is easier and more accurate to measurecross-entropy in this way, is sensitive to the sublan-guage of the samples used (and hence can be usedfor smaller-scale experiments), and is sensitive to as-pects of language, such as syntax, lexical choice, andstyle, that are not commonly found in word lists.
Forexample, languages with similar lexical items butdifferent structures (perhaps verb-medial instead ofverb-final) will find fewer multi-word matches be-tween the databases, and thus will produce agreatermeasured istance, indicative not of the lexical dis-tance but of the syntactic.3.3 CorporaSeveral experiments have been performed to testthis hypothesis.
The first, detailed in (Juola, 1997)simply approaches this as a language-identificationproblem.
Given a set of linguistic samples (in thiscase, Danish, Dutch, English, French, German, andSpanish, plus, as distractors, Finnish, Finni.qh, andMaori) in which of the sampled languages was anovel text written?
Using samples of 100, 250, and500 characters, 472 documents, ranging in size from<500 to several million characters.
The remarkableaccuracy possible, even with very small samples, isshown by the fact that, for instance, at the 250 char-acter level, only one document was miscategorized(German misclassified as Dutch), even when textsto be identified were from completely separate reg-isters.The second experiment involved the languages de-scribed in figure 1.
Samples of 1000 characters fromthe beginning of the book of Genesis were taken fromeach of the languages (the Russian sample beingautomatically transliterated into a Latin-character"equivalent") and cross-entropy between each pair(e.g.
how close German is to the Dutch database)was measured.
These pairs were averaged (n.b.
thecross-entropy between Dutch and German is not nec-Please read the following aloud:I hereby undertake not to remove from the Library,or to mark, deface, or injure in any way, any vol-ume, document, or other object belonging to it or inits custody; not to bring into the Library or kindletherein any fire or flame, and not to smoke in theLibrary; and I promise to obey all the rules of theLibrary.Figure 2: Bodleian declaration i Englishessarily the same as the cross-entropy between Ger-man and Dutch) to produce a symmetric "distance"matrix, and agglomerative cluster analysis was per-formed to produce set of binary "tree" relationships.This analysis consisted of simply taking all pairwisedistances, and making a "cluster" of the two clus-ters with the smallest minimum (mean, or maxi-mum) distance and continuing until the entire setwas combined into a single cluster.
(Obviously, thesemight produce three slightly different rees; resultsreported here are from the minimum tree through-out.
)The third experiment was similar to but broaderthan the second.
For the past several decades, aninformal project of the Bodleian Library, Oxford,has been the gathering of translations of the tradi-tional declaration to be taken by all new membersof the University (and others) before access can begranted to the books.
As a convenience to the inter-national community of scholars, the librarians haveattempted to gather translations of this declarationin as many languages as possible so that scholarscan be made aware of what they are promising; as agoal, they have set for themselves the task of acquir-ing the declaration both in every language spokenin Europe (including some nearly "dead" languagessuch as Cornish and Breton) as well as in at leastone official language for every country in the world(or at least every country represented at the UnitedNations).
The definitive version of the declaration isthe one in English, reproduced here as figure 2; alsoreproduced is the translation into Basque.From this collection were taken samples of fifty-three languages, mostly spoken in Europe or derivedfrom European languages (n.b.
not necessarily ofthe Indo-European family, e.g.
Basque and Maltese)and written primarily in the standard Latin script.These samples typically range between 300-400 char-acters each.
As before, cross-entropy measurementswere taken (and symmetrized) between every pairand used as the basis for an agglomerative clusteranalysis.We expect, of course, in the second and third ex-Juola , 144 Cross-Entropy and Linguistic TypologyIiIiIiI!IIIIIIIIIIIIIIIIIIIImIIIIIIIIIIIIIIIIAgintzerakoan, adierazpen hau irakur ezazumesedez, ahots gora~.Honen bidez Liburutegiari dagozkion liburuki, es-kribu, edo beste inolako gauzarik ez eraman, ezmarkatu, ez hondatu, edo beste edozein moduzkokalte ez dudanik egingo hitz ematen dut; Liburutegibarnean ez erre, ez piztu, ezta beste inolako suasartu, eta Liburutegiko araudi guziak obedituko di-tudala hitz ematen dut.Figure 3: Bodleian declaration i BasqueAfrikaans, Albanian, Basque, Breton, Catalan, Cor-nish, Croatian, Czech #1, Czech #2, Danish, Dutch,English (Middle), English (Modern), English (Old),Esperanto, Estonian, Faeroese, Finnish, French,Frisian, Galacian, German, Hungarian, Icelandic,Irish (Gaelic), Italian, Ladin (Dolomitic), Ladin(Friulan), Ladin (Romontsch), Lappish, Latvian,Lithuanian, Macedonian, Maltese, Manx, Norwe-gian, Occitan, Polish, Portuguese, Provenqal, Rou-manian, Scots English, Scottish (Gaelic), Serbo-Croat, Slovak, Slovenian, Sorbian, Spanish, UrbanSuebian, Swedish, WelshFigure 4: List of languages studiedperiments that known linguistic groupings (such asRomance, Germanic, Slavic, and so forth) would ap-pear as clusters within the final tree.4 Resu l t sAs alluded to earlier, the results from the first exper-iment indicate that as few as 100 characters can be ?sufficient to identify the language in which a docu-ment is written; (Juola, 1997) contains more details.Vqithin the limitations of binary branching im-posed by the cluster analysis algorithm, the fam-ily tree of figure 1 was reproduced perfectly in thesecond experiment; the circled nodes are, of course,ternary in this figure but binary in the recoveredtree.
The experimental results show that, insteadof ternary branching, Maori is considered to bemore distant from the Indo-European cluster thanis Finnish and that (transliterated) Russian is moredistinct from the Germanic luster than is French;these findings, although not necessarily convincingfrom the standpoint of statistical significance, arecertainly intuitively plausible given the geographiccloseness and ease of communication a d thereforelinguistic borrowing.
On the other hand, (Warnow,1997) claims a greater degree of similarity betweenSlavic and Germanic languages than between Slavicand Romance; this discrepancy may simply reflectthe accuracy limits of the corpus sizes used or maybe evidence of a greater degree of cultural influenceon Germany from the West than from the East whichis not reflected in the basic vocabulary.The results of the third experiment are less per-fect, but in many regards more interesting.
In gen-eral, the best results were obtained at what mightbe called "mid-level" regularities.
(For simplicity,we concentrate here on the results of the mini-real distance cluster analysis.)
For example, allthe languages of the Iberian peninsula (Galacian,Portuguese, Occitan, Catalan, and Spanish) weregrouped into one tree, which was attached to twoof the three Ladin samples (Friulan and Romontsch)but not to Dolomitic Ladin, a result compatible withthe findings of (Forster et al, in press) that the levelof linguistic diversity within the "Alpine Romance"languages i as great as the difference between, e.g.French and Italian.
This cluster itself can be ex-tended to incorporate all the Italic/Romance lan-guages except Latin itself; again, this is compatiblewith the findings of (Forster et al, in press), andplausible in itself if one assumes that it's more usefulfor a speaker of modern Ladin to be able to under-stand modern Italian than classical Latin.Similarly, (some of) the North Germanic lan-guages (Danish, Norwegian, and Swedish) wereclustered, as were the South Germanic languagesAfrikaans, Dutch, German, Luxemburgish, andFrisian - -  but these two groups were themselves sep-arated, with Danish et al being measured as be-ing closer to the Romance cluster than to the SouthGermanic.
Similarly, the different varieties of En-glish were widely separated, with Modern English,(Modern) Scots English, and Middle English beingan identifiable cluster, but with Old English beinggrouped with Icelandic and Faeroese in a cluster dis-taut from anything else.The complete tree which the computer generatedis attached on the following page.
Each leaf is la-beled with the appropriate language and with thesubfamily of Indo-European from which it derives.Non-Indo-European languages, uch as Basque orFinnish, are labelled with their families (in paren-theses).
All labels are to be regarded as largely con-sensual and representing common opinions, ratherthan as necessarily authoritative statements; in somecases, even the existence of languages (e.g.
Croat-inn vs. Serbo-Croatian) can be divisive, as much forpolitical and nationalistic as for scientific reasons.5 D iscuss ionThe results presented above, while preliminary (asa result of the small number of languages on theJuola 145 Cross-Entropy and Linguistic Typology+-  Basque  ( i so la te )?-?+-  Corn ish :  Ce l?
ic, - -++-  F-s~onia.n (F inno-Ugr ic )+.-e+-  Breton :  Ce le ice -++-  Czech~l:SlavlcI +-?I ~ S lovLk  : S lav ic+-+I +-  Sorb ia~:S lav lc+-++-  Afzi~Juuls:S.
Ge~an??
c~eo le+-+I I +-C ,  ezmm:S .
C,e:~a.u icI +-+I +-  Luxemburg ish :S .
Gez la~ni?+-+I +-Fr?s lau2:W.  Gez~u2 ic+~+- Alban ian :A lban ian, ' : -  .~  .
.
.
.
( s .~)~-+I +-  Ro~-~Lu: I~a l i c+ .
?+-  F~ench : l~a l i?+-  I '~m.l i l~ : I'clLl.ic~,.--?+-  Gal i c i~n:  I~a l i c+-T  +-  Por~uguese :Z~al i?I +-  Oc?
i l ;a .11 : l l ;a l i?+..+I ~ Ca~alan : I ca l i?+-  Span ish :  l~a l l c~ + -  Lad in  (Fr  iu~lan) : I~a l  ?cI * -  Lad in (Romon~sch)  : Iga l i c+-+"~-  P rovenca l : I I~a l i c+-++-  Lad in (Do lom?~ic )  : I~a l i c+/  +-  Esper~o: I~a l tc  a rc i f i c?a l+~ +-  L?thuant~:Ba l~ icI + -  C:roa~iau : S lav icI I ~-  Se=bo-C~o~:S lav lc+.++-  Macedon i~ : S lav l?I I ~-  No~vog?~n:N.  Gezmani?| +-+I +-  S;ed i~u:N .
C~=~uaic+-  S~ove~m:  $1av lc+-++-  La '~ in :  I t ;a l i c+ '~ Latv ian :  Ba l~ $c+-  Eng l?sh(Nodern l  :W. Germanic| +-+I I +-Sco~sEnE l i sh :W.  Gezmantc+-  Eug l i shCMidd le ) :W.  Germanic~+--  Po l i sh  : S lav~c| +-  l r?shGao l l c :Ce l~$c+-++-  S?o~ i shGle l i c  : Ce  l~ ic+'~+- Lopp?sh (F inno-UsT ic )T- i -  Urba=S.
.b i ,m:ue~mic  a ia~ec~I ?-  Wo lsh :Ce l~ i?+-~ +-  ~t~l i sh(O ld) :W.  Gez~a~i?+-  Fae~oose :N .
Gez~an ic+-++-  I ce l lh l~c :N .
Ge~tcJuola 146 Cross-Entropy and Linguistic TypologyIiIIIIIIIIIIIIIIi lIiII!1IIIIIIIIIII!IIIIilI!one hand, and the small samples on the other), arepromising; mid-range similarities, which might beindependently expected to be the most stable, are in-deed picked up with remarkable accuracy.
Very sub-tle and distant relations are more likely to be maskedby simple noise or random chance (cf.
(Ringe,1992)), while closely similar languages may be sosimilar that lexical choice and style, in some cases ofa single word (do I describe something as "big" or"large"?
), may be enough to alter the very closely-knit relationships.
(For example, the two Czechsamples are not sisters, but aunt/niece, as the Slo-vak sample intervenes - - however, the Czech/Slovaksamples themselves form a cluster.)
Both of theseeffects can be expected to be reduced as the sam-ple sizes increase; the primary finding that a fewhundred characters of language in use can discovermany of the relationships captured by more tradi-tional methods in a numerical and objective way,avoiding the difficulties of interpreting whether twodifferences are "similar.
"One major point of controversy will undoubtedlybe the use of a tree structure for describing these re-lationships.
There are, of course, two major modelsfor describing linguistic families, the "tree" modeland the "wave" model, and although (Warnow,1997) may claim that the tree model is universallyaccepted except in cases of extremely closely relatedlanguages, this statement seems more firm than ab-solutely justified.
However, the tree structure pre-sented here is more an artifact of the cluster analy-sis technique used (and certainly the forced binarybranching is artifactual) than a property of the en-tropy measurement technique.One significant problem which has not been ad-dressed entirely is the question of alphabet effects.First, the very idea of evaluating linguistic simi-larity by examination of letters, instead of sounds,will strike a traditional comparativist asalmost non-sensical.
Letter comparisons will only work to theextent hat correspondence in written form reflectsregularities in linguistic forms.
Fortunately, the let-ter/sound correspondence for most languages, andparticularly for most alphabetic languages, is sig-nificantly better than random, if not quite perfect.Comparisons between languages using different al-phabets (for example between (Cyrillic) Russian and(Latin) English) produce uniformly and unsurpris-ingly huge differences.The work presented here restricts itself almost en-tirely to languages written in the conventional Latinalphabet (with occasional diacritical mark or un-usual character such as the Icelandic eth).
However,even within this subset, focusing on written charac-ters, as opposed to sounds, can change the similaritymetrics.
In some cases, the letter/letter similaritycan actually be better than the sound/sound similar-ity, for example in cases where accents have driftedwhile the written form has been stabilized (e.g.
con-sider the English, American, and Australian pronun-ciations of the word "grass"), or in cases where par-ticular words have been borrowed but have had theirpronunciation regularized to a local standard.
Inother cases, however, the same sound may be rep-resented by different characters (the German 'W'vs the English 'V', or the Old English thorn, tran-scribed in modern English as the digraph 'th').
Aparticularly problematic area can be in the represen-tation of diacritical marks - intuitively, one wouldexpect hat the letters 5 and o would be somehowmore similar than the letters e and o (or than t ando), particularly when one is considering words thatmay have been explicitly borrowed and lost their di-acritics in the process).In either of these instances, the borrowing itselfcan be read as evidence of cultural contact, pos-sibly in connection with geographic proximity.
Inthis case, the difference in apparent similarity be-tween word-list methods (which presumably mea-sure more of the historical relationships of descentand derivation) and the proposed method (which in-corporates measurings of borrowing, and so forth)can be used as a complementary technique to mea-sure such things as the rate, source, and paths ofborrowing.
In particular, measuring letter/letter aswell as sound/sound ifferences might be a usefuladditional source of information for comparativists.The possibility of two letters (or sounds) being"more similar" should also not be discounted (as hasbeen done in this work).
It was suggested above that5 and o are a "similar" letter pair; one would alsoexpect hat, for ins tance , / f /and/v /are  "similar",especially in words borrowed into a language thatdoesn't have unvoiced consonants - while / f /  and/g/would be almost universally distinct.
By treat-ing individual words/sounds a distinct, orthogonal,and unanalyzed symbols, the current echnique maylose this sort of information i  its measurements.On the other hand, this sort of measurement ex-plicitly allows document and subject level distinc-tions to be observed and validated.
It is a com-monplace observation, for example, that there is agreater preponderance of Latin- and Greek- basedwords in (English) scientific discourse than in gen-eral conversation; this is not especially based on anyparticular difference in the choice of lexical items,but more generally on the subject of discourse andthe fact that the lexical items available for scien-Juola 147 Cross-Entropy and Linguistic Typologytific discussions tend to be Latinate as opposed toAnglo-Saxon.
(In other words, you can choose anyword you like from the standard list - all of which areLatin-derived.)
Thus, word-list based methods areunable to validate this distinction, and some othermethod such as comparative etymology might be re-quired.
Again, the proposed method can be usedto determine complementary information to thatgained via traditional techniques; the observation ofthe Latineque words in scientific, but not conversa-tional, English will quite reasonably support he in-ference that scientists (or the group that gave rise tomodem scientists) are more likely to have been ex-posed extensively to Latin than the general public,and thus that knowledge of Latin was characteristicof that particular segment of society.6 Future Work and ConclusionsOne obvious aspect of the Bodleian corpus is that,by construction, all items are translations of eachother (or more accurately of the English).
The ac-quisition of translated corpora in a sufficiently variedset of languages can be problematic; it would obvi-ously be useful to test to what extent cross-entropycan be used as a taxonomic relationship on relatedcorpora that are not necessarily translations of eachother.
Similarly, much further work is required todetermine the best method of analysis, whether bycluster analysis or other techniques, and what de-gree of accuracy can be expected with various corpussizes, registers, &c. On the other hand, if it's hardto acquire small translated corpora, it's even harderto acquire large ones, and the sensitivity of Wyner'sentropy estimation technique is an undoubted ad-vantage.Further research will also be required to deter-mine when to stop proclaiming relationships.
As hasbeen argued by (Ringe, 1992), the mere fact that twostructures are similar does not imply that they arerelated; similarity may arise through mere chance.Given a reasonable model of language, it should bepossible to determine what level of cross-entropychance should predict, and thus when to stop ag-glutinating languages into proto-World and beyond,or determining whether a particular piano sonatashould be classified as closer to Indo-European orSino-Tibetian.Going further afield, once the possibility of pro-ducing document, instead of language, taxonomiesis accepted, it is possible to discuss meaningfullyand to consider concepts uch as the rate of changeof a language (did English change more between1600-1650 than between 1900-1950?)
or the vary-ing degrees of taxonomic relationships between var-ious stylistic or subject classes.
More generally, thiscross-entropic method provides a way of combininginformation about relationships from a variety ofsources, including lexical availability, lexical choice,pronuncations, syntax, and so forth.Ultimately, cross-entropy will probably not re-place the word-list differentiation method of deter-mining historic and familial relationships betweenlanguages, but can provide a valuable supplementto more traditional methods, as well as being ableto address questions that are currently unanswerableby standard methods.
Cross-entropy appears to bea meaningful and easy to measure method of deter-mining "linguistic distance" that is more sensitiveto variances in lexical choice, word usage, style, andsyntax than conventional methods.
Furthermore,this allows scientists to study taxonomic relation-ships among much smaller samples of language thanwere previously possible and to provide some sort ofnumerical validation (to be confirmed or rejected).Although much further work is necessary to deter-mine the exact limitations of this sort of similaritymeasurements, preliminary results indicate that theaccepted taxonomy is nearly reconstructable fromremarkably little corpora, which shows at least inprinciple the power of this technique.7 AcknowledgementsThis work was funded primarily by ESRC grant 70.The author would also like to thank Dr. John G.Pusey, Admissions Officer at the Bodleian Library,.for making the Bodleian corpus available; shouldanyone wish to assist in this project (a wish withwhich the author heartily concurs), please contactDr.
Pusey at admissions@bodley.ox.ac.uk or at theBodleian Library, Broad Street, Oxford, UK.
Theauthor would also like to acknowledge the valuablecontributions of Jodi Affuso in transcribing the cor-pus onto disk, of Alex Popiel for his programmingexpertise, and of Todd Bailey and Anna Morpurgo-Davies for critical reading and discussion of themanuscript.ReferencesRonald Eaton Asher and J. M. Y. Simpson, editors.1994.
The Encyclopedia of Language and Linguis-tics.
Pergamon, Oxford.Christopher M. Bishop.
1995.
Neural Networks forPattern Recognition.
Clarendon Press, Oxford.William Bright, editor.
1992. International Ency-clopedia of Linguistics.
Oxford University Press,Oxford.Juola 148 Cross-Entropy and Linguistic TypologyIIIiIIIIIIIIII/IIIIIIIIIIIIIIIIIIIIIIIIIIIIPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, Jennifer C. Lai, and Robert L. Mer-cer.
1992.
An estimate of an upper bound forthe entropy of English.
Computational Linguis-tics, 18(1).David Crystal.
1987.
The Cambridge Encyclopediaof Language.
Cambridge University Press, Cam-bridge, UK.Martin Farach, Michiel Noordewier, Serap Savari,Lary Shepp, Abraham Wyner, and Jacob Ziv.1995.
On the entropy of DNA: Algorithms andmeasurements based on memory and rapid con-vergence.
In Proceedings of the 6th Annual Sym-posium on Discrete Algorithms (SODA95).
ACMPress.Edward Finegan and Niko Besnier.
1987.
Lan-guage, Its Structure and Use.
Harcourt Brace Jo-vanovich, San Diego.Peter Forster, Alfred Toth, and Hans-Juergen Ban-delt.
in press.
Phylogenetic network analysis ofword lists.
Journal of Quantitative Linguistics.H.
A. Gleason.
1955.
Introduction to DescriptiveLinguistics.
Holt, Rinehart and Winston, NewYork.Patrick Juola.
1997.
What can we do with small cor-pora?
Document categorization via cross-entropy.In Proceedings of an Interdisciplinary Workshopon Similarity and Categorization, Edinburgh, UK.Department ofArtificial Intelligence, University ofEdinburgh.Donald A. Ringe.
1992.
On calculating the factorof chance in language comparison, volume 82 ofTransactions of the American Philosophical Soci-ety.
American Philosophical Society.Claude Elmwood Shannon.
1948.
A mathematicaltheory of commtmication.
Bell System TechnicalJournal, 27:379-423.Claude Elmwood Shannon.
1951.
Prediction andentropy of printed English.
Bell System TechnicalJournal, 30:50-64.Morris Swadesh.
1955.
Towards greater accuracyin lexicostatic dating.
International Journal ofAmerican Linguistics, 21:121-37.Tandy Warnow.
1997.
Mathematical pproaches tocomparative linguistics.
Proceedings of the Na-tional Academy of Sciences of the USA, 94:6585-90.Abraham J. Wyner.
in press.
Entropy estimationand patterns.Juola 149 Cross-Entropy and Linguistic Typologymmmmmmmmmmmmmmmm
