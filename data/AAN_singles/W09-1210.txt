Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 67?72,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEfficient Parsing of Syntactic and Semantic Dependency StructuresBernd BohnetInternational Computer Science Institute1947 Center StreetBerkeley 94704, Californiabohnet@icsi.Berkeley.eduAbstractIn this paper, we describe our system for the2009 CoNLL shared task for joint parsing ofsyntactic and semantic dependency structuresof multiple languages.
Our system combinesand implements efficient parsing techniques toget a high accuracy as well as very good pars-ing and training time.
For the applicationsof syntactic and semantic parsing, the pars-ing time and memory footprint are very im-portant.
We think that also the development ofsystems can profit from this since one can per-form more experiments in the given time.
Forthe subtask of syntactic dependency parsing,we could reach the second place with an ac-curacy in average of 85.68 which is only 0.09points behind the first ranked system.
For thistask, our system has the highest accuracy forEnglish with 89.88, German with 87.48 andthe out-of-domain data in average with 78.79.The semantic role labeler works not as well asour parser and we reached therefore the fourthplace (ranked by the macro F1 score) in thejoint task for syntactic and semantic depen-dency parsing.1 IntroductionDepedendency parsing and semantic role labelingimproved in the last years significantly.
One of thereasons are CoNLL shared tasks for syntactic de-pendency parsing in the years 2006, 2007 (Buch-holz and Marsi, 2006; Nivre et al, 2007) and theCoNLL shared task for joint parsing of syntactic andsemantic dependencies in the year 2008 and of causethis shared task in 2009, cf.
(Surdeanu et al, 2008;Hajic?
et al, 2009).
The CoNLL Shared Task 2009is to parse syntactic and semantic dependencies ofseven languages.
Therefore, training and develop-ment data in form of annotated corpora for Cata-lan, Chinese, Czech, English, German, Japanese andSpanish is provided, cf.
(Taule?
et al, 2008; Palmerand Xue, 2009; Hajic?
et al, 2006; Surdeanu et al,2008; Burchardt et al, 2006; Kawahara et al, 2002).There are two main approaches to dependencyparsing: Maximum Spanning Tree (MST) based de-pendency parsing and Transition based dependencyparsing, cf.
(Eisner, 1996; Nivre et al, 2004; Mc-Donald and Pereira, 2006).
Our system uses the firstapproach since we saw better chance to improve theparsing speed and additionally, the MST had so farslightly better parsing results.
For the task of seman-tic role labeling, we adopted a pipeline architecturewhere we used for each step the same learning tech-nique (SVM) since we opted for the possibility tobuild a synchronous combined parser with one scorefunction.2 Parsing AlgorithmWe adopted the second order MST parsing algo-rithm as outlined by Eisner (1996).
This algorithmhas a higher accuracy compared to the first orderparsing algorithm since it considers also siblings andgrandchildren of a node.
Eisner?s second order ap-proach can compute a projective dependency treewithin cubic time (O(n3)).Both algorithms are bottom up parsing algorithmsbased on dynamic programming similar to the CKYchart parsing algorithm.
The score for a dependencytree is the score of all edge scores.
The following67equation describes this formally.score(S, t) =??
(i,j)?E score(i, j)The score of the sentence S and a tree t over Sis defined as the sum of all edge scores where thewords of S are w0...w1.
The tree consists of set ofnodes N and set of edges E = ?N ?N?.
The wordindices (0..n) are the elements of the node set N .The expression (i, j) ?
E denotes an edge which isgoing from the node i to the node j.The edge score (score(i, j)) is computed as thescalar product of a feature vector representation ofeach edge ?
?fS(i, j) with a weight vector ?
?w wherei, j are the indices of the words in a sentence.
Thefeature vector fS might take not only into accountthe words with indices i and j but also additionalvalues such as the words before and after the wordswi and wj .
The following equation shows the scorefunction.score(i, j) = ?
?fS(i, j) ?
?
?wMany systems encode the features as strings andmap the strings to a number.
The number becomesthe index of the feature in the feature vector andweight vector.
In order to compute the weight vec-tor, we reimplemented the support vector machineMIRA which implements online Margin Infused Re-laxed Algorithm, cf.
(Crammer et al, 2003).3 Labeled Dependency ParsingThe second order parsing algorithm builds an un-labeled dependency tree.
However, all dependencytree banks of the shared task provide trees with edgelabels.
The following two approaches are commonto solve this problem.
An additional algorithm la-bels the edges or the parsing algorithm itself is ex-tended and the labeling algorithm is integrated intothe parsing algorithm.
McDonald et al (2006) usean additional algorithm.
Their two stage model hasa good computational complexity since the label-ing algorithm contributes again only a cubic timecomplexity to the algorithm and keeps therefore thejoint algorithm still cubic.
The algorithm selectsthe highest scored label due to the score functionscore(wi, label) + score(wj , label) and inserts thehighest scored label into a matrix.
The scores arealso used in the parsing algorithms and added tothe edge scores which improves the overall pars-ing results as well.
In the first order parsing sce-nario, this procedure is sufficient since no combi-nation of edges are considered by the parsing algo-rithm.
However, in the second order parsing sce-nario where more than one edge are considered bythe parsing algorithm, combinations of two edgesmight be more accurate.Johansson and Nugues (2008) combines the edgelabeling with the second order parsing algorithm.This adds an additional loop over the edge labels.The complexity is therefore O(n4).
However, theycould show that a system can gain accuracy of about2-4% which is a lot.4 Non-Projective Dependency ParsingThe dependency parser developed in the last yearsuse two different techniques for non-projective de-pendency parsing.Nivre and Nilsson (2005) uses tree rewritingwhich is the most common technique.
With thistechnique, the training input to the parser is first pro-jectivized by applying a minimal number of liftingoperations to the non-projective edges and encodinginformation about these lifts in edge labels.
Afterthese operations, the trees are projective and there-fore a projective dependency parser can be applied.During the training, the parser learns also to builttrees with the lifted edges and so indirect to builtnon-projective dependency trees by applying the in-verse operations to the lifting on the projective tree.McDonald and Pereira (2006) developed a tech-nique to rearrange edges in the tree in a postpro-cessing step after the projective parsing has takenplace.
Their Approximate Dependency Parsing Al-gorithm searches first the highest scoring projectiveparse tree and then it rearranges edges in the treeuntil the rearrangements does not increase the scorefor the tree anymore.
This technique is computa-tionally expensive for trees with a large number ofnon-projective edges since it considers to re-attachall edges to any other node until no higher scoringtrees can be found.
Their argument for the algo-rithm is that most edges in a tree even in languagewith lot of non-projective sentences, the portion ofnon-projective edges are still small and therefore bystarting with the highest scoring projective tree, typ-68ically the highest scoring non-projective tree is onlya small number of transformations away.Our experiments showed that with the non-projective Approximate Dependency Parsing Algo-rithm and a threshold for the improvment of scorehigher than about 0.7, the parsing accuracy improveseven for English slightly.
With a threshold of 1.1, wegot the highest improvements.5 Learning FrameworkAs learning technique, we use Margin Infused Re-laxed Algorithm (MIRA) as developed by Crammeret al (2003) and applied to dependency parsing byMcDonald et al (2005).
The online Algorithm inFigure 1 processes one training instance on each it-eration, and updates the parameters accordingly.Algorithm 1: MIRA?
= {Sx, Tx}Xx=1 // The set of training data consists// of sentences and the corresponding dependency trees?
?w (0) = 0,?
?v = 0for n = 1 to Nfor x = 1 to Xwi+1 = update wi according to instance (Sx, Tx)v = v + wi+1i = i+ 1end forend forw = v/(N ?X)The inner loop iterates over all sentences x of thetraining set while the outer loop repeats the train itimes.
The algorithm returns an averaged weightvector and uses an auxiliary weight vector v that ac-cumulates the values of w after each iteration.
Atthe end, the algorithm computes the average of allweight vectors by dividing it by the number of train-ing iterations and sentences.
This helps to avoidoverfitting, cf.
(Collins, 2002).The update function computes the update to theweight vector wi during the training so that wrongclassified edges of the training instances are possiblycorrectly classified.
This is computed by increasingthe weight for the correct features and decreasing theweight for wrong features of the vectors for the treeof the training set ?
?fTx ?
wi and the vector for thepredicted dependency tree ?
?fT ?x ?
wi.The update function tries to keep the change tothe parameter vector wi as small as possible for cor-rectly classifying the current instance with a differ-ence at least as large as the loss of the incorrect clas-sifications.6 Selected Parsing FeaturesTable 1, 4 and 2 give an overview of the selectedfeatures for our system.
Similar to Johansson andNugues (2008), we add the edge labels to each fea-tures.
In the feature selection, we follow a bit moreMcDonald and Pereira (2006) since we have in addi-tion the lemmas, morphologic features and the dis-tance between the word forms.For the parsing and training speed, most impor-tant is a fast feature extraction beside of a fast pars-ing algorithm.Standard Featuresh-f/l h-f/l, d-posh-pos h-pos, d-f/ld-f/l h-f/l, d-f/ld-pos h-pos, d-posh-f/l,h-pos h-f/l, d-f/l, h-posd-f/l,d-pos h-f/l, d-f/l, d-posh-pos, d-pos, h-f/lh-pos, d-pos, d-f/lh-pos, d-pos, h-f/l, d-f/lTable 1: Selected standard parsing features.
h is the ab-brevation for head, d for dependent, g for grandchild, ands for sibling.
Each feature contains also the edge labelwhich is not listed in order to save some space.
Addi-tional features are build by adding the direction and thedistance plus the direction.
The direction is left if the de-pendent is left of the head otherwise right.
The distanceis the number of words between the head and the depen-dent, if ?5, 6 if >5 and 11 if >10.
?
means that anadditional feature is built with the previous part plus thefollowing part.
f/l represent features that are built oncewith the form and once with the lemma.Selected morphologic parsing features.?
h-morpheme ?
head-morphologic-feature-set do?
d-morpheme ?
dependent-morphologic-feature-set dobuild-feautre: h-pos, d-pos, h-morpheme, d-morpheme7 Implementation AspectsIn this section, we provide implementation detailsconsidering improvements of the parsing and train-ing time.
The training of our system (parser) has69Linear Features Grandchild Features Sibling Featuresh-pos, d-pos, h-pos + 1 h-pos, d-pos, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l ?
dir(d,s) ?dist(d,s)h-pos, d-pos, h-pos - 1 h-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ?
dir(d,s) ?dist(d,s)h-pos, d-pos, d-pos + 1 d-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ?
dir(d,s)+ ?
dist(d,s)h-pos, d-pos, d-pos - 1 h-pos, g-f/l, dir(h,d), dir(d,g) d-pos, s-pos ?dir(d,s)?dist(d,s)h-pos, d-pos, h-pos - 1, d-pos - 1 d-pos, g-f/l, dir(h,d), dir(d,g) h-pos, d-pos, s-pos, dir(h,d), dir(d,s) ?dist(h,s)h-f/l, g-pos, dir(h,d), dir(d,g) h-f/l, s-f/l, dir(h,d), dir(d,s)?dist(h,s) h-pos, s-f/l, dir(h,d), dir(d,s)?dist(h,s)d-f/l, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l, dir(h,d), dir(d,s) ?dist(h,s) d-pos, s-f/l, dir(h,d), dir(d,s)?dist(h,s)h-f/l, s-pos, dir(h,d), dir(d,s)?dist(h,s)d-f/l, s-pos, dir(h,d), dir(d,s)?dist(h,s)Table 2: Selected Features.three passes.
The goal of the first two passes is tocollect the set of possible features of the trainingset.
In order to determine the minimal descriptionlength, the feature extractor collects in the first passall attributes that the features can contain.
For eachattribute (labels, part-of-speech, etc.
), the extractorcomputes a mapping to a number which is continousfrom 1 to the count of elements without duplicates.We enumerate in the same way the feature pat-terns (e.g.
h-pos, d-pos) in order to distinguish thepatterns.
In the second pass, the extractor builds thefeatures for all training examples which occur in thetrain set.
This means for all edges in the trainingexamples.We create the features with a function that adds it-eratively the attributes of a feature to a number rep-resented with 64 bits and shifts it by the minimalnumber of bits to encode the attribute and then enu-merates and maps these numbers to 32 bit numbersto save even more memory.Beside this, the following list shows an overviewof the most important implementation details to im-prove the speed:1.
We use as feature vector a custom array imple-mentation with only a list of the features thatmeans without double floating point value.2.
We store the feature vectors forf(label, wi, wj), f(label, wi, wj , wg),f(label, wi, wj , ws) etc.
in a compressedfile since otherwise it becomes the bottleneck.3.
After the training, we store only the parametersof the support vector machine which are higherthan a threshold of 1?10?7Table 3 compares system regarding their perfor-mance and memory usage.
For the shared task, weSystem (1) (2) (3)Type 2nd order 2nd order 2nd orderLabeling separate separate integratedSystem baseline this thisTraining 22 hours 3 hours 15 hours7GB 1.5 GB 3 GBParsing 2000 ms 50 ms 610 ms700 MB 1 GBLAS 0.86 0.86 0.88Table 3: Performance Comparison.
For the baseline sys-tem (1), we used the system of McDonald and Pereira(2006) on a MacPro 2.8 Ghz as well for our implementa-tion (2).
For system (3), we use a computer with Intel i73.2 Ghz which is faster than the MacPro.
For all systems,we use 10 training iterations for the SVM Mira.use the system (3) with integrated labeling.8 Semantic Role LabelingThe semantic role labeler is implemented as apipeline architecture.
The components of thepipeline are predicate selection (PS), argument iden-tification (AI), argument classification (AC), andword sense disambiguation (WSD).In order to select the predicates, we look up thelemmas in the Prob Bank, Nom Bank, etc.
if avail-able, cf.
(Palmer et al, 2005; Meyers et al, 2004).For all other components, we use the support vec-tor machine MIRA to select and classify the seman-tic role labels as well as to disambiguate the wordsenese.The AI component identifies the arguments ofeach predicate.
It iterates over the predicates andover the words of a sentence.
In the case that thescore function is large or equal to zero the argumentis added to the set of arguments of the predicate inquestion.
Table 5 lists for the attribute identificationand semantic role labeling.The argument classification algorithm labels each70Language Catalan Chinese Czech English German Japanese Spanish Czech English GermanDevelopment SetLAS 86.69 76.77 80.75 87.97 86.46 92.37 86.53Semantic Unlabeled 93.92 85.09 94.05 91.06 91.61 93.90 93.87Semantic Labeled 74.98 75.94 78.07 78.79 72.66 72.86 73.01Macro (F1) 80.84 76.48 79.42 84.52 79.56 82.63 79.77Test Set Out-of-domain dataLAS 86.35 76.51 80.11 @89.88 @87.48 92.21 87.19 76.40 @82.64 @77.34Semantic Labeled 74.53 75.29 79.02 80.39 75.72 72.76 74.31 78.01 68.44 63.36Macro (F1) 80.44 75.91 79.57 85.14 81.60 82.51 80.75 77.20 75.55 70.35Table 4: Syntactic and Semantic Scores.
@ indicate values that are the highest scores of all systems.Features with part-of-speech tags Features with lemmas Features with relsarg, path-len arg, p-lemma ?
dir ?
path-len arg, a-rel ?
path-lenarg, p-pos arg, a-lemma, path, dir arg, a-pos, p-pos, p-rel ?
path-lenarg, sub-cat, a-pos, p-pos arg, p-lemma - 1, a-pos, path-len, dir arg, p-rel, a-pos, lms-lemmaarg, p-pos, a-pos, a-rmc-pos arg, p-lemma + 1, a-lemma, path, dir arg, a-pos, p-pos, a-relarg, p-pos, a-pos, a-lmc-pos arg, p-lemma - 1, a-lemma, path, dir arg, path-relarg, p-pos, a-pos, a-lemma-1 arg, p-lemma - 2, a-lemma, path, dir arg, p-lemma, a-pos, path-relarg, sub-cat, a-lemma, dir, path-len arg, p-lemma, a-lemma, pathPos, dirarg, a-pos, a-lemma + 1 arg, p-lemma, p-lemma + 1arg, a-pos, a-lemma + 2 arg, p-lemma, p-lemma - 1, a-pos ?
dir ?
path-lenarg, a-pos, a-lemma-lmc arg, p-lemma, a-lms-lemma, a-pos ?
dir ?
path-lenarg, p-sub-cat, p-pos ?
dir arg, p-lemma, path-len ?
dirarg, p-pos, path, p-parent-lemma arg, p-lemma, path-len ?
dirarg, p-pos, path, p-parent-pos ?
dir arg, a-pos, patharg, p-pos, a-pos, familyship(p,a) arg, p-pos, p-lemma, familyship(p,a)arg, path-pos arg, a-pos, p-lemma, familyship(p,a)arg, p-pos, a-lemma, familyship(p,a)Table 5: Argument identification and semantic role labeling Features.
p is the abbrivation for predicate and a forargument.
For the AI component, the attribute arg is either the value yes and no and for the SRL component, ars isthe role label.
path is the path in terms of up?s and down?s.
pathPos is a path plus the part-of-speech on the path.
diris left, if the argument is left of the predicate, equal if the predicate and argument are equal, otherwise right.
rmc isthe abbrivation for right most child, lmc for left most child, and lms left most sibling.
familiship(x,y) is a function thatcomputes the relation between two words: self, parent, child, ancestor, decendant and none.identified argument with a semantic role label.
Theargument classification algorithm selects with abeam search algorithm the combination of argu-ments with the highest score.The last component of our pipeline is the wordsense disambiguation.
We put this against the in-tuition at the end of our pipeline since experimentsshowed that other components could not profit fromdisambiguated word senses but on the other handthe word sense disambiguation could profit from theargument identification and argument classification.In order to disambiguate, we iterate over the wordsin the corpus that have more than one sense and takethe sense with the highest score.The average time to execute the SRL pipeline ona sentence is less than 0.15 seconds and the trainingtime for all languages less than 2 hours.9 ConclusionWe provided a fast implementation with good pars-ing time and memory footprint.
Even if we tradedoff a lot of the speed improvement by using amore expensive decoder and more attributes to geta higher accuracy.For some languages, features are not provided orthe parser does not profit from using these features.For instance, the English parser does not profit fromthe lemmas and the Chinese as well as the Japanese71corpus does not have lemmas different from theword forms, etc.
Therefore, a possible further ac-curacy and parsing speed improvement would be toselect different features sets for different languagesor to leave out some features.AcknowledgmentsThis work was supported by the German AcademicExchange Service (DAAD).
We gratefully acknowl-edge this support.ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.
InIn Proc.
of CoNLL, pages 149?164.Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In EMNLP.Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, andYoram Singer.
2003.
Online Passive-Aggressive Al-gorithms.
In Sixteenth Annual Conference on NeuralInformation Processing Systems (NIPS).Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of the 16th International Conference on Com-putational Linguistics (COLING-96), pages 340?345,Copenhaen.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan ?Ste?pa?nek, Jir???
Havelka, MarieMikulova?, and Zden?k ?Zabokrtsky?.
2006.
Prague De-pendency Treebank 2.0.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?isMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Miahi Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 Shared Task: Syntactic and Semantic Dependen-cies in Multiple Languages.
In Proceedings of the 13thCoNLL-2009, June 4-5, Boulder, Colorado, USA.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In Proceedings of theShared Task Session of CoNLL-2008, Manchester,UK.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.Ryan McDonald and Fernando Pereira.
2006.
OnlineLearning of Approximate Dependency Parsing Algo-rithms.
In In Proc.
of EACL, pages 81?88.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online Large-margin Training of DependencyParsers.
In Proc.
ACL, pages 91?98.Ryan McDonald, Kevin Lerman, Koby Crammer, andFernando Pereira.
2006.
Multilingual DependencyParsing with a Two-Stage Discriminative Parser.
InTenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 91?98.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
The nombank project: An interimreport.
In A. Meyers, editor, HLT-NAACL 2004 Work-shop: Frontiers in Corpus Annotation, pages 24?31,Boston, Massachusetts, USA, May 2 - May 7.
Associ-ation for Computational Linguistics.Joakim.
Nivre and Jens Nilsson.
2005.
Pseudo-Projective Dependency Parsing.
In In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, pages 99?106.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-Based Dependency Parsing.
In Proceed-ings of the 8th CoNLL, pages 49?56, Boston, Mas-sachusetts.Joakim Nivre, Johan Hall, Sandra Ku?bler, Rayn McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The conll 2007 shared task on dependency pars-ing.
In Proc.
of the CoNLL 2007 Shared Task.
JointConf.
on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), June.Martha Palmer and Nianwen Xue.
2009.
Adding Seman-tic Roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: An Annotated Corpusof Semantic Roles.
volume 31, pages 71?106.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic andsemantic dependencies.
In Proceedings of the 12thCoNLL-2008.Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the LREC-2008, Marrakesh, Morroco.72
