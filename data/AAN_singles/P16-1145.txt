Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1535?1545,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsWIKIREADING: A Novel Large-scale Language Understanding Taskover WikipediaDaniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin,Andrew Fandrianto, Jay Han, Matthew Kelcey and David BerthelotGoogle Research{dhewlett,allac,llion,ipolosukhin,fto,hanjay,matkelcey,dberth}@google.comAbstractWe present WIKIREADING, a large-scalenatural language understanding task andpublicly-available dataset with 18 millioninstances.
The task is to predict textualvalues from the structured knowledge baseWikidata by reading the text of the cor-responding Wikipedia articles.
The taskcontains a rich variety of challenging clas-sification and extraction sub-tasks, mak-ing it well-suited for end-to-end modelssuch as deep neural networks (DNNs).We compare various state-of-the-art DNN-based architectures for document classifi-cation, information extraction, and ques-tion answering.
We find that models sup-porting a rich answer space, such as wordor character sequences, perform best.
Ourbest-performing model, a word-level se-quence to sequence model with a mecha-nism to copy out-of-vocabulary words, ob-tains an accuracy of 71.8%.1 IntroductionA growing amount of research in natural languageunderstanding (NLU) explores end-to-end deepneural network (DNN) architectures for tasks suchas text classification (Zhang et al, 2015), rela-tion extraction (Nguyen and Grishman, 2015), andquestion answering (Weston et al, 2015).
Thesemodels offer the potential to remove the interme-diate steps traditionally involved in processing nat-ural language data by operating on increasinglyraw forms of text input, even unprocessed char-acter or byte sequences.
Furthermore, while thesetasks are often studied in isolation, DNNs have thepotential to combine multiple forms of reasoningwithin a single model.Supervised training of DNNs often requires alarge amount of high-quality training data.
To thisend, we introduce a novel prediction task and ac-companying large-scale dataset with a range ofsub-tasks combining text classification and infor-mation extraction.
The dataset is made publicly-available at http://goo.gl/wikireading.The task, which we call WIKIREADING, is to pre-dict textual values from the open knowledge baseWikidata (Vrande?ci?c and Kr?otzsch, 2014) giventext from the corresponding articles on Wikipedia(Ayers et al, 2008).
Example instances are shownin Table 1, illustrating the variety of subject mat-ter and sub-tasks.
The dataset contains 18.58M in-stances across 884 sub-tasks, split roughly evenlybetween classification and extraction (see Section2 for more details).In addition to its diversity, the WIKIREADINGdataset is also at least an order of magnitude largerthan related NLU datasets.
Many natural lan-guage datasets for question answering (QA), suchas WIKIQA (Yang et al, 2015), have only thou-sands of examples and are thus too small for train-ing end-to-end models.
Hermann et al (2015)proposed a task similar to QA, predicting entitiesin news summaries from the text of the originalnews articles, and generated a NEWS dataset with1M instances.
The bAbI dataset (Weston et al,2015) requires multiple forms of reasoning, but iscomposed of synthetically generated documents.WIKIQA and NEWS only involve pointing to lo-cations within the document, and text classifica-tion datasets often have small numbers of outputclasses.
In contrast, WIKIREADING has a rich out-put space of millions of answers, making it a chal-lenging benchmark for state-of-the-art DNN archi-tectures for QA or text classification.We implemented a large suite of recent models,and for the first time evaluate them on commongrounds, placing the complexity of the task in con-text and illustrating the tradeoffs inherent in each1535Categorization ExtractionDocument Folkart Towers aretwin skyscrapers in theBayrakli district of theTurkish city of Izmir.Reaching a structuralheight of 200 m (656 ft)above ground level, theyare the tallest .
.
.Angeles blancos is aMexican telenovela pro-duced by Carlos So-tomayor for Televisa in1990.
Jacqueline An-dere, Rogelio Guerraand Alfonso Iturraldestar as the main .
.
.Canada is a countryin the northern part ofNorth America.
Its tenprovinces and three ter-ritories extend from theAtlantic to the Pacificand northward into theArctic Ocean, .
.
.Breaking Bad is anAmerican crime dramatelevision series createdand produced by VinceGilligan.
The showoriginally aired on theAMC network for fiveseasons, from January20, 2008, to .
.
.Property country original language ofworklocated next to body ofwaterstart timeAnswer Turkey Spanish Atlantic Ocean, ArcticOcean, Pacific Ocean20 January 2008Table 1: Examples instances from WIKIREADING.
The task is to predict the answer given the document and property.
Answertokens that can be extracted are shown in bold, the remaining instances require classification or another form of inference.approach.
The highest score of 71.8% is achievedby a sequence to sequence model (Kalchbrennerand Blunsom, 2013; Cho et al, 2014) operating onword-level input and output sequences, with spe-cial handing for out-of-vocabulary words.2 WIKIREADINGWe now provide background information relatingto Wikidata, followed by a detailed description ofthe WIKIREADING prediction task and dataset.2.1 WikidataWikidata is a free collaborative knowledgebase containing information about approximately16M items (Vrande?ci?c and Kr?otzsch, 2014).Knowledge related to each item is expressedin a set of statements, each consisting of a(property, value) tuple.
For example,the item Paris might have associated state-ments asserting (instance of, city) or(country, France).
Wikidata contains over80M such statements across 884 properties.
Itemsmay be linked to articles on Wikipedia.2.2 DatasetWe constructed the WIKIREADING dataset fromWikidata and Wikipedia as follows: We consoli-dated all Wikidata statements with the same itemand property into a single (item, property,answer) triple, where answer is a set of val-ues.
Replacing each item with the text ofthe linked Wikipedia article (discarding unlinkeditems) yields a dataset of 18.58M (document,property, answer) instances.
Importantly,all elements in each instance are human-readablestrings, making the task entirely textual.
Theonly modification we made to these strings was toconvert timestamps into a human-readable format(e.g., ?4 July 1776?
).The WIKIREADING task, then, is to predict theanswer string for each tuple given the documentand property strings.
This setup can be seen assimilar to information extraction, or question an-swering where the property acts as a ?question?.We assigned all instances for each document ran-domly to either training (12.97M instances), val-idation (1.88M), and test (3.73M ) sets followinga 70/10/20 distribution.
This ensures that, duringvalidation and testing, all documents are unseen.2.3 DocumentsThe dataset contains 4.7M unique Wikipedia ar-ticles, meaning that roughly 80% of the English-language Wikipedia is represented.
Multiple in-stances can share the same document, with a meanof 5.31 instances per article (median: 4, max:879).
The most common categories of docu-ments are human, taxon, film, album, andhuman settlement, making up 48.8% of thedocuments and 9.1% of the instances.
The meanand median document lengths are 489.2 and 203words.2.4 PropertiesThe dataset contains 884 unique properties,though the distribution of properties across in-stances is highly skewed: The top 20 proper-ties cover 75% of the dataset, with 99% cov-erage achieved after 180 properties.
We dividethe properties broadly into two groups: Categor-ical properties, such as instance of, genderand country, require selecting between a rel-atively small number of possible answers, whilerelational properties, such as date of birth,1536Property Frequency Entropyinstance of 2,574,038 0.431sex or gender 941,200 0.189country 803,252 0.536date of birth 785,049 0.936given name 767,916 0.763occupation 716,176 0.589country of citizenship 674,560 0.501located in .
.
.
entity 478,372 0.802place of birth 384,951 0.800date of death 364,910 0.943Table 2: Training set frequency and scaled answer entropyfor the 10 most frequent properties.parent, and capital, typically require ex-tracting rare or totally unique answers from thedocument.To quantify this difference, we compute the en-tropy of the answer distribution A for each prop-erty p, scaled to the [0, 1] range by dividing by theentropy of a uniform distribution with the samenumber of values, i.e.,?H(p) = H(Ap)/ log |Ap|.Properties that represent essentially one-to-onemappings score near 1.0, while a property withjust a single answer would score 0.0.
Table 2 listsentropy values for a subset of properties, showingthat the dataset contains a spectrum of sub-tasks.We label properties with an entropy less than 0.7as categorical, and those with a higher entropy asrelational.
Categorical properties cover 56.7% ofthe instances in the dataset, with the remaining43.3% being relational.2.5 AnswersThe distribution of properties described above hasimplications for the answer distribution.
There area relatively small number of very high frequency?head?
answers, mostly for categorical properties,and a vast number of very low frequency ?tail?
an-swers, such as names and dates.
At the extremes,the most frequent answer human accounts for al-most 7% of the dataset, while 54.7% of the an-swers in the dataset are unique.
There are somespecial categories of answers which are systemati-cally related, in particular dates, which comprise8.9% of the dataset (with 7.2% being unique).This distribution means that methods focused oneither head or tail answers can each perform mod-erately well, but only a method that handles bothtypes of answers can achieve maximum perfor-mance.
Another consequence of the long tail ofanswers is that many (30.0%) of the answers in thetest set never appear in the training set, meaningthey must be read out of the document.
An answeris present verbatim in the document for 45.6% ofthe instances.3 MethodsRecently, neural network architectures for NLUhave been shown to meet or exceed the perfor-mance of traditional methods (Zhang et al, 2015;Dai and Le, 2015).
The move to deep neuralnetworks also allows for new ways of combin-ing the property and document, inspired by recentresearch in the field of question answering (withthe property serving as a question).
In sequen-tial models such as Recurrent Neural Networks(RNNs), the question could be prepended to thedocument, allowing the model to ?read?
the doc-ument differently for each question (Hermann etal., 2015).
Alternatively, the question could beused to compute a form of attention (Bahdanau etal., 2014) over the document, to effectively focusthe model on the most predictive words or phrases(Sukhbaatar et al, 2015; Hermann et al, 2015).As this is currently an ongoing field of research,we implemented a range of recent models and forthe first time compare them on common grounds.We now describe these methods, grouping theminto broad categories by general approach and not-ing necessary modifications.
Later, we introducesome novel variations of these models.3.1 Answer ClassificationPerhaps the most straightforward approach toWIKIREADING is to consider it as a special caseof document classification.
To fit WIKIREAD-ING into this framework, we consider each pos-sible answer as a class label, and incorporate fea-tures based on the property so that the model canmake different predictions for the same document.While the number of potential answers is too largeto be practical (and unbounded in principle), a sub-stantial portion of the dataset can be covered by amodel with a tractable number of answers.3.1.1 BaselineThe most common approach to document classi-fication is to fit a linear model (e.g., Logistic Re-gression) over bag of words (BoW) features.
Toserve as a baseline for our task, the linear modelneeds to make different predictions for the sameWikipedia article depending on the property.
Weenable this behavior by computing two Nwele-ment BoW vectors, one each for the document1537and property, and concatenating them into a sin-gle 2Nwfeature vector.3.1.2 Neural Network MethodsAll of the methods described in this section en-code the property and document into a joint rep-resentation y ?
Rdout, which serves as input fora final softmax layer computing a probability dis-tribution over the top Nansanswers.
Namely, foreach answer i ?
{1, .
.
.
, Nans}, we have:P (i|x) = ey>ai/?Nansj=1ey>aj, (1)where ai?
Rdoutcorresponds to a learned vec-tor associated with answer i.
Thus, these modelsdiffer primarily in how they combine the propertyand document to produce the joint representation.For existing models from the literature, we providea brief description and note any important differ-ences in our implementation, but refer the readerto the original papers for further details.Except for character-level models, documentsand properties are tokenized into words.
The Nwmost frequent words are mapped to a vector inRdinusing a learned embedding matrix1.
Otherwords are all mapped to a special out of vocabu-lary (OOV) token, which also has a learned em-bedding.
dinand doutare hyperparameters forthese models.Averaged Embeddings (BoW): This is the neu-ral network version of the baseline method de-scribed in Section 3.1.1.
Embeddings for wordsin the document and property are separately aver-aged.
The concatenation of the resulting vectorsforms the joint representation of size 2din.Paragraph Vector: We explore a variant of theprevious model where the document is encoded asa paragraph vector (Le and Mikolov, 2014).
Weapply the PV-DBOW variant that learns an embed-ding for a document by optimizing the predictionof its constituent words.
These unsupervised doc-ument embeddings are treated as a fixed input tothe supervised classifier, with no fine-tuning.LSTM Reader: This model is a simplified ver-sion of the Deep LSTM Reader proposed by Her-mann et al (2015).
In this model, an LSTM(Hochreiter and Schmidhuber, 1997) reads theproperty and document sequences word-by-word1Limited experimentation with initialization frompublicly-available word2vec embeddings (Mikolov et al,2013) yielded no improvement in performance.and the final state is used as the joint representa-tion.
This is the simplest model that respects theorder of the words in the document.
In our imple-mentation we use a single layer instead of two anda larger hidden size.
More details on the architec-ture can be found in Section 4.1 and in Table 4.Attentive Reader: This model, also presentedin Hermann et al (2015), uses an attention mech-anism to better focus on the relevant part of thedocument for a given property.
Specifically, At-tentive Reader first generates a representation u ofthe property using the final state of an LSTM whilea second LSTM is used to read the document andgenerate a representation ztfor each word.
Then,conditioned on the property encoding u, a normal-ized attention is computed over the document toproduce a weighted average of the word represen-tations zt, which is then used to generate the jointrepresentation y.
More precisely:mt= tanh(W1concat(zt,u))?t= exp (v?mt)r =?t?t???
?zty = tanh(W2concat(r,u)),where W1, W2, and v are learned parameters.Memory Network: Our implementation closelyfollows the End-to-End Memory Network pro-posed in Sukhbaatar et al (2015).
This modelmaps a property p and a list of sentencesx1, .
.
.
,xnto a joint representation y by attend-ing over sentences in the document as follows:The input encoder I converts a sequence of wordsxi= (xi1, .
.
.
, xiLi) into a vector using an embed-ding matrix (equation 2), where Liis the length ofsentence i.2The property is encoded with the em-bedding matrix U (eqn.
3).
Each sentence is en-coded into two vectors, a memory vector (eqn.
4)and an output vector (eqn.
5), with embedding ma-tricesM and C, respectively.
The property encod-ing is used to compute a normalized attention vec-tor over the memories (eqn.
6).3The joint repre-sentation is the sum of the output vectors weighted2Our final results use the position encoding method pro-posed by Sukhbaatar et al (2015), which incorporates posi-tional information in addition to word embeddings.3Instead of the linearization method of Sukhbaatar et al(2015), we applied an entropy regularizer for the softmax at-tention as described in Kurach et al (2015).1538Ada , daughter of Lord Byronparent <sep>0 0 0 0 1 100Lord Byron<go><end>ByronLord(a) RNN Labeler:(b) Basic seq2seq:(c) Seq2seq with Placeholders:Ada , daughter of LordByronparent <sep>Lord PH_7<go><end>PH_7LordPH_3 , daughter of LordPH_7parent <sep>Figure 1: Illustration of RNN models.
Blocks with samecolor share parameters.
Red words are out of vocabulary andall share a common embedding.by this attention (eqn.
7).I(xi,W ) =?jWxij(2)u = I(p, U) (3)mi= I(xi,M) (4)ci= I(xi, C) (5)pi= softmax(q?mi) (6)y = u +?ipici(7)3.2 Answer ExtractionRelational properties involve mappings betweenarbitrary entities (e.g., date of birth,mother, and author) and thus are lessamenable to document classification.
For these,approaches from information extraction (es-pecially relation extraction) are much moreappropriate.
In general, these methods seek toidentify a word or phrase in the text that standsin a particular relation to a (possibly implicit)subject.
Section 5 contains a discussion of priorwork applying NLP techniques involving entityrecognition and syntactic parsing to this problem.RNNs provide a natural fit for extraction, asthey can predict a value at every position in asequence, conditioned on the entire previous se-quence.
The most straightforward application toWIKIREADING is to predict the probability that aword at a given location is part of an answer.
Wetest this approach using an RNN that operates onthe sequence of words.
At each time step, we use asigmoid activation for estimating whether the cur-rent word is part of the answer or not.
We referto this model as the RNN Labeler and present itgraphically in Figure 1a.For training, we label all locations where anyanswer appears in the document with a 1, andother positions with a 0 (similar to distant super-vision (Mintz et al, 2009)).
For multi-word an-swers, the word sequences in the document andanswer must fully match4.
Instances where no an-swer appears in the document are discarded fortraining.
The cost function is the average cross-entropy for the outputs across the sequence.
Whenperforming inference on the test set, sequences ofconsecutive locations scoring above a thresholdare chunked together as a single answer, and thetop-scoring answer is recorded for submission.53.3 Sequence to SequenceRecently, sequence to sequence learning (orseq2seq) has shown promise for natural languagetasks, especially machine translation (Cho et al,2014).
These models combine two RNNs: an en-coder, which transforms the input sequence into avector representation, and a decoder, which con-verts the encoder vector into a sequence of outputtokens, one token at a time.
This makes them ca-pable, in principle, of approximating any functionmapping sequential inputs to sequential outputs.Importantly, they are the first model we considerthat can perform any combination of answer clas-sification and extraction.3.3.1 Basic seq2seqThis model resembles LSTM Reader augmentedwith a second RNN to decode the answer as a se-quence of words.
The embedding matrix is sharedacross the two RNNs but their state to state tran-sition matrices are different (Figure 1b).
Thismethod extends the set of possible answers to anysequence of words from the document vocabulary.3.3.2 Placeholder seq2seqWhile Basic seq2seq already expands the expres-siveness of LSTM Reader, it still has a limitedvocabulary and thus is unable to generate someanswers.
As mentioned in Section 3.2, RNN La-beler can extract any sequence of words present inthe document, even if some are OOV.
We extendthe basic seq2seq model to handle OOV words byadding placeholders to our vocabulary, increasingthe vocabulary size fromNwtoNw+Ndoc.
Then,when an OOV word occurs in the document, itis replaced at random (without replacement).
byone of these placeholders.
We also replace thecorresponding OOV words in the target output se-4Dates were matched semantically to increase recall.5We chose an arbitrary threshold of 0.5 for chunking.
Thescore of each chunk is obtained from the harmonic mean ofthe predicted probabilities of its elements.1539L o<go>rA d a , oLpa r?? d a uen tn<end>?Figure 2: Character seq2seq model.
Blocks with the samecolor share parameters.
The same example as in Figure 1 isfed character by character.quence by the same placeholder,6as shown in Fig-ure 1c.
Luong et al (2015) developed a similarprocedure for dealing with rare words in machinetranslation, copying their locations into the outputsequence for further processing.This makes the input and output sequences amixture of known words and placeholders, and al-lows the model to produce any answer the RNNLabeler can produce, in addition to the ones thatthe basic seq2seq model could already produce.This approach is comparable to entity anonymiza-tion used in Hermann et al (2015), which replacesnamed entities with random ids, but simpler be-cause we use word-level placeholders without en-tity recognition.3.3.3 Basic Character seq2seqAnother way of handling rare words is to processthe input and output text as sequences of charac-ters or bytes.
RNNs have shown some promiseworking with character-level input, includingstate-of-the-art performance on a Wikipedia textclassification benchmark (Dai and Le, 2015).
Amodel that outputs answers character by charactercan in principle generate any of the answers in thetest set, a major advantage for WIKIREADING.This model, shown in Figure 2, operates only onsequences of mixed-case characters.
The propertyencoder RNN transforms the property, as a charac-ter sequence, into a fixed-length vector.
This prop-erty encoding becomes the initial hidden state forthe second layer of a two-layer document encoderRNN, which reads the document, again, charac-ter by character.
Finally, the answer decoder RNNuses the final state of the previous RNN to decodethe character sequence for the answer.6The same OOV word may occur several times in thedocument.
Our simplified approach will attribute a differentplaceholder for each of these and will use the first occurrencefor the target answer.3.3.4 Character seq2seq with PretrainingUnfortunately, at the character level the lengthof all sequences (documents, properties, and an-swers) is greatly increased.
This adds more se-quential steps to the RNN, requiring gradients topropagate further, and increasing the chance of anerror during decoding.
To address this issue in aclassification context, Dai and Le (2015) showedthat initializing an LSTM classifier with weightsfrom a language model (LM) improved its accu-racy.
Inspired by this result, we apply this prin-ciple to the character seq2seq model with a two-phase training process: In the first phase, we traina character-level LM on the input character se-quences from the WIKIREADING training set (nonew data is introduced).
In the second phase, theweights from this LM are used to initialize thefirst layer of the encoder and the decoder (purpleand green blocks in Figure 2).
After initialization,training proceeds as in the basic character seq2seqmodel.4 ExperimentsWe evaluated all methods from Section 3 on thefull test set with a single scoring framework.
Ananswer is correct when there is an exact stringmatch between the predicted answer and the goldanswer.
However, as describe in Section 2.2, someanswers are composed from a set of values (e.g.third example in Table 1).
To handle this, we de-fine the Mean F1 score as follows: For each in-stance, we compute the F1-score (harmonic meanof precision and recall) as a measure of the degreeof overlap between the predicted answer set andthe gold set for a given instance.
The resulting per-instance F1 scores are then averaged to produce asingle dataset-level score.
This allows a methodto obtain partial credit for an instance when it an-swers with at least one value from the golden set.In this paper, we only consider methods for an-swering with a single value, and most answers inthe dataset are also composed of a single value, sothis Mean F1 metric is closely related to accuracy.More precisely, a method using a single value asanswer is bounded by a Mean F1 of 0.963.4.1 Training DetailsWe implemented all models in a single frame-work based on TensorFlow (Abadi et al, 2015)with shared pre-processing and comparable hyper-parameters whenever possible.
All documents are1540Method Mean F1 Bound Categorical Relational Date ParamsAnswer ClassifierSparse BoW Baseline 0.4380.8310.725 0.063 0.004 500.5MAveraged Embeddings 0.583 0.849 0.234 0.080 120MParagraph Vector 0.552 0.787 0.227 0.033 30MLSTM Reader 0.680 0.880 0.421 0.311 45MAttentive Reader 0.693 0.886 0.441 0.337 56MMemory Network 0.612 0.861 0.288 0.055 90.1MAnswer ExtractionRNN Labeler 0.357 0.471 0.240 0.536 0.626 41MSequence to SequenceBasic seq2seq 0.708 0.925 0.844 0.530 0.738 32MPlaceholder seq2seq 0.718 0.948 0.835 0.565 0.730 32MCharacter seq2seq 0.677 0.963 0.841 0.462 0.731 4.1MCharacter seq2seq (LM) 0.699 0.963 0.851 0.501 0.733 4.1MTable 3: Results for all methods described in Section 3 on the test set.
F1 is the Mean F1 score described in 4.
Bound is theupper bound on Mean F1 imposed by constraints in the method (see text for details).
The remaining columns provide scorebreakdowns by property type and the number of model parameters.truncated to the first 300 words except for Charac-ter seq2seq, which uses 400 characters.
The em-bedding matrix used to encode words in the doc-ument uses din= 300 dimensions for the Nw=100, 000 most frequent words.
Similarly, answerclassification over the Nans= 50, 000 most fre-quent answers is performed using an answer rep-resentation of size dout= 300.7The first 10words of the properties are embedded using thedocument embedding matrix.
Following Cho etal.
(2014), RNNs in seq2seq models use a GRUcell with a hidden state size of 1024.
More detailson parameters are reported in Table 4.MethodEmb.DimsDoc.LengthPropertyLengthDoc.Vocab.SizeSparseBoWBaselineN/A300words10 words50KwordsParagraphVectorN/A N/A 10 words N/ACharacterseq2seq30400chars20 chars76charsAll others 300300words10 words100KwordsTable 4: Structural model parameters.
Note that the Para-graph Vector method uses the output from a separate, unsu-pervised model as a document encoding, which is not countedin these parameters.Optimization was performed with the Adamstochastic optimizer8(Kingma and Adam, 2015)over mini-batches of 128 samples.
Gradient clip-ping9(Graves, 2013) is used to prevent instabilityin training RNNs.
We performed a search over7For models like Averaged Embedding and ParagraphVector, the concatenation imposes a greater dout.8Using ?1= 0.9, ?2= 0.999 and  = 10?8.9When the norm of gradient g exceeds a threshold C, it is50 randomly-sampled hyperparameter configura-tions for the learning rate and gradient clip thresh-old, selecting the one with the highest Mean F1on the validation set.
Learning rate and clippingthreshold are sampled uniformly, on a logarithmicscale, over the range [10?5, 10?2] and [10?3, 101]respectively.4.2 Results and DiscussionResults for all models on the held-out set of test in-stances are presented in Table 3.
In addition to theoverall Mean F1 scores, the model families differsignificantly in Mean F1 upper bound, and theirrelative performance on the relational and categor-ical properties defined in Section 2.4.
We also re-port scores for properties containing dates, a sub-set of relational properties, as a separate columnsince they have a distinct format and organization.For examples of model performance on individualproperties, see Table 5.As expected, all classifier models perform wellfor categorical properties, with more sophisticatedclassifiers generally outperforming simpler ones.The difference in precision reading ability be-tween models that use broad document statistics,like Averaged Embeddings and Paragraph Vectors,and the RNN-based classifiers is revealed in thescores for relational and especially date proper-ties.
As shown in Table 5, this difference is mag-nified in situations that are more difficult for aclassifier, such as relational properties or proper-ties with fewer training examples, where AttentiveReader outperforms Averaged Embeddings by awide margin.
This model family also has a highscaled down i.e.
g?
g ?min(1,C||g||).1541Mean F1PropertyTestIn-stancesAveragedEmbed-dingsAttentiveReaderMemoryNetworkBasicseq2seqPlaceholderseq2seqCharacterseq2seqCharacterseq2seq(LM)Categorical Propertiesinstance of 734187 0.8545 0.8978 0.8720 0.8877 0.8775 0.8548 0.8659sex or gen-der267896 0.9917 0.9966 0.9936 0.9968 0.9952 0.9943 0.9941genre 32531 0.5320 0.6225 0.5625 0.5511 0.5260 0.5096 0.5283instrument 3665 0.7621 0.8415 0.7886 0.8377 0.8172 0.7529 0.7832Relational Propertiesgivenname218625 0.4973 0.8486 0.7206 0.8669 0.8868 0.8606 0.8729located in 137253 0.4140 0.6195 0.4832 0.5484 0.6978 0.5496 0.6365parenttaxon62685 0.1990 0.3467 0.2077 0.2044 0.7997 0.4979 0.5748author 9517 0.0309 0.2088 0.1050 0.6094 0.6572 0.1403 0.3748Date Propertiesdate ofbirth223864 0.0626 0.3677 0.0016 0.8306 0.8259 0.8294 0.8303date ofdeath103507 0.0417 0.2949 0.0506 0.7974 0.7874 0.7897 0.7924publicationdate31253 0.3909 0.5549 0.4851 0.5988 0.5902 0.5903 0.5943date ofofficialopening1119 0.1510 0.3047 0.1725 0.3333 0.3012 0.1457 0.1635Table 5: Property-level Mean F1 scores on the test set for selected methods and properties.
For each property type, the twomost frequent properties are shown followed by two less frequent properties to illustrate long-tail behavior.Figure 3: Per-answer Mean F1 scores for Attentive Reader(moving average of 1000), illustrating the decline in predic-tion quality as the number of training examples per answerdecreases.upper bound, as perfect classification across the50, 000 most frequent answers would yield a MeanF1 of 0.831.
However, none of them approachesthis limit.
Part of the reason is that their accuracyfor a given answer decreases quickly as the fre-quency of the answer in the training set decreases,as illustrated in Figure 3.
As these models haveto learn a separate weight vector for each answeras part of the softmax layer (see Section 3.1), thismay suggest that they fail to generalize across an-swers effectively and thus require significant num-ber of training examples per answer.The only answer extraction model evaluated,RNN Labeler, shows a complementary set ofstrengths, performing better on relational proper-ties than categorical ones.
While the Mean F1 up-per bound for this model is just 0.434 because itcan only produce answers that are present verba-tim in the document text, it manages to achievemost of this potential.
The improvement on dateproperties over the classifier models demonstratesits ability to identify answers that are typicallypresent in the document.
We suspect that answerextraction may be simpler than answer classifica-tion because the model can learn robust patternsthat indicate a location without needing to learnabout each answer, as the classifier models must.The sequence to sequence models show agreater degree of balance between relational andcategorical properties, reaching performance con-sistent with classifiers on the categorical questionsand with RNN Labeler on relational questions.Placeholder seq2seq can in principle produce anyanswer that RNN Labeler can, and the perfor-mance on relational properties is indeed similar.As shown in Table 5, Placeholder seq2seq per-forms especially well for properties where the an-swer typically contains rare words such as thename of a place or person.
When the set ofpossible answer tokens is more constrained, such1542as in categorical or date properties, the Basicseq2seq often performs slightly better.
Characterseq2seq has the highest upper bound, limited to0.963 only because it cannot produce an answerset with multiple elements.
LM pretraining con-sistently improves the performance of the Charac-ter seq2seq model, especially for relational prop-erties as shown in Table 5.
The performance ofthe Character seq2seq, especially with LM pre-training, is a surprising result: It performs com-parably to the word-level seq2seq models eventhough it must copy long character strings whendoing extraction and has access to a smaller por-tion of the document.
We found the characterbased models to be particularly sensitive to hyper-parameters.
However, using a pretrained languagemodel reduced this issue and significantly accel-erated training while improving the final score.We believe that further research on pretraining forcharacter based models could improve this result.5 Related WorkThe goal of automatically extracting structured in-formation from unstructured Wikipedia text wasfirst advanced by Wu and Weld (2007).
As Wiki-data did not exist at that time, the authors re-lied on the structured infoboxes included in someWikipedia articles for a relational representationof Wikipedia content.
Wikidata is a cleaner datasource, as the infobox data contains many slightvariations in schema related to page formatting.Partially to get around this issue, the authors re-strict their prediction model Kylin to 4 specific in-fobox classes, and only common attributes withineach class.A substantial body of work in relation extrac-tion (RE) follows the distant supervision paradigm(Craven and Kumlien, 1999), where sentencescontaining both arguments of a knowledge base(KB) triple are assumed to express the triple?s re-lation.
Broadly, these models use these distant la-bels to identify syntactic features relating the sub-ject and object entities in text that are indicative ofthe relation.
Mintz et al (2009) apply distant su-pervision to extracting Freebase triples (Bollackeret al, 2008) from Wikipedia text, analogous tothe relational part of WIKIREADING.
Extensionsto distant supervision include explicitly modellingwhether the relation is actually expressed in thesentence (Riedel et al, 2010), and jointly reason-ing over larger sets of sentences and relations (Sur-deanu et al, 2012).
Recently, Rockt?aschel et al(2015) developed methods for reducing the num-ber of distant supervision examples required bysharing information between relations.6 ConclusionWe have demonstrated the complexity of theWIKIREADING task and its suitability as a bench-mark to guide future development of DNN modelsfor natural language understanding.
After compar-ing a diverse array of models spanning classifica-tion and extraction, we conclude that end-to-endsequence to sequence models are the most promis-ing.
These models simultaneously learned to clas-sify documents and copy arbitrary strings fromthem.
In light of this finding, we suggest somefocus areas for future research.Our character-level model improved substan-tially after language model pretraining, suggest-ing that further training optimizations may yieldcontinued gains.
Document length poses a prob-lem for RNN-based models, which might be ad-dressed with convolutional neural networks thatare easier to parallelize.
Finally, we note that thesemodels are not intrinsically limited to English, asthey rely on little or no pre-processing with tradi-tional NLP systems.
This means that they shouldgeneralize effectively to other languages, whichcould be demonstrated by a multilingual versionof WIKIREADING.AcknowledgmentsWe thank Jonathan Berant for many helpful com-ments on early drafts of the paper, and Cather-ine Finegan-Dollak for an early implementation ofRNN Labeler.ReferencesMart?n Abadi, Ashish Agarwal, Paul Barham, EugeneBrevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,Andy Davis, Jeffrey Dean, Matthieu Devin, et al2015.
Tensorflow: Large-scale machine learningon heterogeneous systems.
Software available fromtensorflow.
org.Phoebe Ayers, Charles Matthews, and Ben Yates.2008.
How Wikipedia works: And how you can be apart of it.
No Starch Press.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
International Con-ference on Learning Representations (ICLR).1543Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: Acollaboratively created graph database for structur-ing human knowledge.
In Proceedings of the 2008ACM SIGMOD International Conference on Man-agement of Data, SIGMOD ?08, pages 1247?1250,New York, NY, USA.
ACM.Kyunghyun Cho, Bart Van Merri?enboer, C?alarG?ulc?ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder?decoderfor statistical machine translation.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1724?1734, Doha, Qatar, October.
Association for Com-putational Linguistics.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In Proceedings of the Sev-enth International Conference on Intelligent Systemsfor Molecular Biology, pages 77?86.
AAAI Press.Andrew M Dai and Quoc V Le.
2015.
Semi-supervised sequence learning.
In Advances in Neu-ral Information Processing Systems, pages 3061?3069.Alex Graves.
2013.
Generating sequences with recur-rent neural networks.
CoRR, abs/1308.0850.Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems, pages 1684?1692.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentconvolutional neural networks for discourse compo-sitionality.
In Proceedings of the CVSC Workshop,Sofia, Bulgaria.
Association of Computational Lin-guistics.Diederik P Kingma and Jimmy Ba Adam.
2015.
Amethod for stochastic optimization.
In InternationalConference on Learning Representation.Karol Kurach, Marcin Andrychowicz, and IlyaSutskever.
2015.
Neural random-access machines.In International Conference on Learning Represen-tations (ICLR).Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In Pro-ceedings of The 31st International Conference onMachine Learning, pp.
, 2014, pages 1188?
?1196.Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,and Wojciech Zaremba.
2015.
Addressing the rareword problem in neural machine translation.
In Pro-ceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th In-ternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pages 11?19,Beijing, China, July.
Association for ComputationalLinguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems, pages 3111?3119.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2 - Volume 2, ACL ?09, pages 1003?1011,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Thien Huu Nguyen and Ralph Grishman.
2015.
Rela-tion extraction: Perspective from convolutional neu-ral networks.
In Proceedings of NAACL-HLT, pages39?48.Sebastian Riedel, Limin Yao, and Andrew McCal-lum.
2010.
Modeling relations and their men-tions without labeled text.
In Machine Learning andKnowledge Discovery in Databases, pages 148?163.Springer.Tim Rockt?aschel, Sameer Singh, and Sebastian Riedel.2015.
Injecting Logical Background Knowledgeinto Embeddings for Relation Extraction.
In An-nual Conference of the North American Chapterof the Association for Computational Linguistics(NAACL).Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advancesin Neural Information Processing Systems, pages2431?2439.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 455?465.
Association for Computational Linguistics.Denny Vrande?ci?c and Markus Kr?otzsch.
2014.
Wiki-data: A free collaborative knowledgebase.
Com-mun.
ACM, 57:78?85.Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2015.
Towards ai-complete ques-tion answering: A set of prerequisite toy tasks.
May.1544Fei Wu and Daniel S Weld.
2007.
Autonomously se-mantifying wikipedia.
In Proceedings of the six-teenth ACM conference on Conference on infor-mation and knowledge management, pages 41?50.ACM.Yi Yang, Wen-tau Yih, and Christopher Meek.
2015.Wikiqa: A challenge dataset for open-domain ques-tion answering.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 2013?2018.Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015.Character-level convolutional networks for text clas-sification.
In Advances in Neural Information Pro-cessing Systems, pages 649?657.1545
