Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 117?120,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsArabic Morphological Tagging, Diacritization, and LemmatizationUsing Lexeme Models and Feature RankingRyan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia RudinCenter for Computational Learning SystemsColumbia UniversityNew York, NY 10115 USA{ryanr,rambow,habash,mdiab,rudin}@ccls.columbia.edu,AbstractWe investigate the tasks of general morpho-logical tagging, diacritization, and lemmatiza-tion for Arabic.
We show that for all tasks weconsider, both modeling the lexeme explicitly,and retuning the weights of individual classi-fiers for the specific task, improve the perfor-mance.1 Previous WorkArabic is a morphologically rich language: in ourtraining corpus of about 288,000 words we find 3279distinct morphological tags, with up to 100,000 pos-sible tags.1 Because of the large number of tags, itis clear that morphological tagging cannot be con-strued as a simple classification task.
Hajic?
(2000)is the first to use a dictionary as a source of possiblemorphological analyses (and hence tags) for an in-flected word form.
He redefines the tagging task asa choice among the tags proposed by the dictionary,using a log-linear model trained on specific ambi-guity classes for individual morphological features.Hajic?
et al (2005) implement the approach of Hajic?
(2000) for Arabic.
In previous work, we follow thesame approach (Habash and Rambow, 2005), usingSVM-classifiers for individual morphological fea-tures and a simple combining scheme for choosingamong competing analyses proposed by the dictio-nary.
Since the dictionary we use, BAMA (Buck-walter, 2004), also includes diacritics (orthographic1This work was funded under the DARPA GALE, program,contract HR0011-06-C-0023.
We thank several anonymous re-viewers for helpful comments.
A longer version of this paper isavailable as a technical report.marks not usually written), we extend this approachto the diacritization task in (Habash and Rambow,2007).
The work presented in this paper differs fromthis previous work in that (a) we introduce a newtask for Arabic, namely lemmatization; (b) we usean explicit modeling of lexemes as a component inall tasks discussed in this paper (morphological tag-ging, diacritization, and lemmatization); and (c) wetune the weights of the feature classifiers on a tuningcorpus (different tuning for different tasks).2 Morphological Disambiguation Tasks forArabicWe define the task of morphological taggingas choosing an inflectional morphological tag (inthis paper, the term ?morphological tagging?
neverrefers to derivational morphology).
The morphol-ogy of an Arabic word can be described by the 14(nearly) orthogonal features shown in Figure 1.
Fordifferent tasks, different subsets may be useful: forexample, when translating into a language withoutcase, we may want to omit the case feature.
For theexperiments we discuss in this paper, we investigatethree variants of the morphological tagging tasks:MorphPOS (determining the feature POS, which isthe core part-of-speech ?
verb, noun, adjective, etc.
);MorphPart (determining the set of the first ten basicmorphological features listed in Figure 1); andMor-phAll (determining the full inflectional morpholog-ical tag, i.e., all 14 features).The task of diacritization involves adding diacrit-ics (short vowels, gemination marker shadda, andindefiniteness marker nunation) to the standard writ-ten form.
We have two variants of the diacritization117Feature name ExplanationPOS Simple part-of-speechCNJ Presence of a conjunction cliticPRT Presence of a particle cliticPRO Presence of a pronominal cliticDET Presence of the definite deter-minerGEN GenderNUM NumberPER PersonVOX VoiceASP AspectMOD MoodNUN Presence of nunation (indefinite-ness marker)CON Construct state (head of a geni-tive construction)CAS CaseFigure 1: List of (inflectional) morphological featuresused in our system; the first ten are features which(roughly) can be determined with higher accuracy sincethey rely less on syntactic context and more on visibleinflectional morphologytasks: DiacFull (predicting all diacritics of a givenword), which relates to lexeme choice and morphol-ogy tagging, and DiacPart (predicting all diacriticsof a given word except those associated with the fi-nal letter), which relates largely to lexeme choice.Lemmatization (LexChoice) for Arabic has notbeen discussed in the literature to our knowledge.
Alexeme is an abstraction over a set of inflected wordforms, and it is usually represented by its citationform, also called lemma.Finally, AllChoice is the combined task of choos-ing all inflectional and lexemic aspects of a word incontext.This gives us a total of seven tasks.
AllChoice isthe hardest of our tasks, since it subsumes all othertasks.
MorphAll is the hardest of the three mor-phological tagging tasks, subsuming MorphPartand MorphPOS, and DiacFull is the hardest lexicaltask, subsuming DiacPart, which in turn subsumesLexChoice.
However, MorphAll and DiacFull are(in general) orthogonal, since MorphAll has no lex-emic component, while DiacFull does.3 Our SystemOur system, MADA, makes use of 19 orthogonalfeatures to select, for each word, a proper anal-ysis from a list of potential analyses provided bythe BAMA dictionary.
The BAMA analysis whichmatches the most of the predicted features wins; theweighting of the features is one of the topics of thispaper.
These 19 features consist of the 14 morpho-logical features shown in Figure 1, which MADApredicts using 14 distinct Support Vector Machinestrained on ATB3-Train (as defined by Zitouni et al(2006)).
In addition, MADA uses five additionalfeatures.
Spellmatch determines whether the dia-critized form of the suggested analysis and the inputword match if both are stripped of all of their di-acritics.
This is useful because sometimes BAMAsuggests analyses which imply a different spellingof the undiacritized word, but these analyses are of-ten incorrect.
Isdefault identifies those analyses thatare the default output of BAMA (typically, theseare guesses that the word in question is a propernoun); these analyses are less likely to be correctthan others suggested by BAMA.
MADA can de-rive the values of Spellmatch and Isdefault by di-rect examination of the analysis in question, andno predictive model is needed.
The fourteen mor-phological features plus Spellmatch and Isdefaultform a feature collection that is entirely based onmorphological (rather than lexemic) features; we re-fer to this collection as BASE-16.
UnigramDiacand UnigramLex are unigram models of the sur-face diacritized form and the lexeme respectively,and contain lexical information.
We also build 4-gram lexeme models using an open-vocabulary lan-guage model with Kneser-Ney smoothing, by meansof the SRILM toolkit (Stolcke, 2002).
The model istrained on the same corpus used to train the otherclassifiers, ATB3-Train.
(We also tested other n-grammodels, and found that a 4-gram lexeme modeloutperforms the other orders with n ?
5, althoughthe improvement over the trigram and 5-gram mod-els was less than 0.01%.)
The 4-gram model, onits own, correctly selects the lexeme of words inATB3-DevTest 94.1% of the time.
The 4-gram lex-eme model was incorporated into our system as afull feature (NGRAM).
We refer to the feature setconsisting of BASE-16 plus the two unigram mod-118els and NGRAM as FULL-19.Optimizing the feature weights is a machinelearning task.
To provide learning data for this task,we take the ATB3-DevTest data set and divide it intotwo sections; the first half (?26K words) is usedfor tuning the weights and the second half (?25Kwords) for testing.
In a pre-processing step, eachanalysis in appended with a set of labels which in-dicate whether the analysis is correct according toseven different evaluation metrics.
These metricscorrespond in a one-to-one manner to the seven dif-ferent disambiguation tasks discussed in Section 2,and we use the task name for the evaluation la-bel.
Specifically, the MorphPOS label is positiveif the analysis has the same POS value as the cor-rect analysis in the gold standard; the LexChoicelabel provides the same information about the lex-eme choice.
The MorphPart label is positive if theanalysis agrees with the gold for each of the 10 ba-sic features used by Habash and Rambow (2005).A positive MorphAll label requires that the analy-sis match the gold in all morphological features, i.e.,in every feature except the lexeme choice and dia-critics.
The DiacFull label is only positive if thesurface diacritics of the analysis match the gold di-acritics exactly; DiacPart is less strict in that thetrailing sequence diacritic markers in each surfacediacritic are stripped before the analysis and the goldare compared.
Finally, AllChoice is only positive ifthe analysis was one chosen as correct in the gold;this is the strictest form of evaluation, and there canbe only one positive AllChoice label per word.In addition to labeling as described in the preced-ing paragraph, we run MADA on the tuning and testsets.
This gives us a set of model predictions for ev-ery feature of every word in the tuning and test sets.We use an implementation of a Downhill SimplexMethod in many dimensions based on the methoddeveloped by Nelder and Mead (1965) to tune theweights applied to each feature.
In a given itera-tion, the Simplex algorithm proposes a set of featureweights.
These weights are given to a weight eval-uation function; this function determines how effec-tive a particular set of weights is at a given disam-biguation task by calculating an overall score forthe weight set: the number of words in the tuningset that were correctly disambiguated.
In order tocompute this score, the weight evaluation functionexamines each proposed analysis for each word inthe tuning set.
If the analysis and the model predic-tion for a feature of a given word agree, the analysisscore for that analysis is incremented by the weightcorresponding to that feature.
The analysis with thehighest analysis score is selected as the proper anal-ysis for that word.
If the selected analysis has a pos-itive task label (i.e., it is a good answer for the dis-ambiguation task in question), the overall score forthe proposed weight set is incremented.
The Sim-plex algorithm seeks to maximize this overall score(and thus choose the weight set that performs bestfor a given task).Once the Simplex algorithm has converged, theoptimal feature weights for a given task are known.Our system makes use of these weights to select acorrect analysis in the test set.
Each analysis of eachword is given a score that is the sum of optimal fea-ture weights for features where the model predic-tion and the analysis agree.
The analysis with thehighest score is then chosen as the correct analysisfor that word.
The system can be evaluated simplyby comparing the chosen analysis to the gold stan-dard.
Since the Simplex weight evaluation functionand the system use identical means of scoring anal-yses, the Simplex algorithm has the potential to findvery optimized weights.4 ExperimentsWe have three main research hypotheses: (1) Usinglexemic features helps in all tasks, but especially inthe diacritization and lexeme choice tasks.
(2) Tun-ing the weights helps over using identical weights.
(3) Tuning to the task that is evaluated improves overtuning to other tasks.
For each of the two featuresets, BASE-16 and FULL-19, we tune the weightsusing seven tuning metrics, producing seven sets ofweights.
We then evaluate the seven automaticallyweighted systems using seven evaluation metrics.The tuning metrics are identical to the evaluationmetrics and they correspond to the seven tasks de-scribed in Section 2.
Instead of showing 98 results,we show in Figure 2 four results for each of theseven tasks: for both the BASE-16 and FULL-19feature sets, we give the untuned performance, andthen the best-performing tuned performance.
We in-dicate which tuning metric provided the best tun-119BASE-16 (Morph Feats Only) FULL-19 (All Feats)Task Baseline Not Tuned Tuned Tuning metric Not Tuned Tuned Tuning metricMorphPOS 95.5 95.6 96.0 MorphAll 96.0 96.4 MorphPOSMorphPart 93.8 94.1 94.8 AllChoice 94.7 95.1 DiacPartMorphAll 83.8 84.0 84.8 AllChoice 82.2 85.1 MorphAllLexChoice 85.5 86.6 87.5 MorphAll 95.4 96.3 LexChoiceDiacPart 85.1 86.4 87.3 AllChoice 94.8 95.4 DiacPartDiacFull 76.0 77.1 78.2 MorphAll 82.6 86.1 MorphAllAllChoice 73.3 74.5 75.6 AllChoice 80.3 83.8 MorphAllFigure 2: Results for morphological tagging tasks (percent correct); the baseline uses only 14 morphological featureswith identical weights; ?Tuning Metric?
refers to the tuning metric that produced the best tuned results, as shown inthe ?Tuned?
columning performance.
The Baseline indicated in Fig-ure 2 uses the 14 morphological features (listed inFigure 1) only, with no tuning (i.e., all 14 featureshave a weight of 1).
The untuned results were deter-mined by also setting almost all feature weights to 1;the only exception is the Isdefault feature, which isgiven a weight of -(8/14) when included in untunedsets.
Since this feature is meant to penalize analy-ses, its value must be negative; we use this particu-lar value so that our results can be readily comparedto previous work.
All results are the best publishedresults to date on these test sets; for a deeper discus-sion, see the longer version of this paper which isavailable as a technical report.We thus find our three hypotheses confirmed: (1)Using lexemic features reduces error for the mor-phological tagging tasks (measured on tuned data)by 3% to 11%, but by 36% to 71% for the diacriticand lexeme choice tasks.
The highest error reduc-tion is indeed for the lexical choice task.
(2) Tuningthe weights helps over using identical weights.
Withonly morphological features, we obtain an error re-duction of between 4% and 12%; with all features,the error reduction from tuning ranges between 8%and 20%.
(3) As for the correlation between tuningtask and evaluation task, it turned out that when weuse only morphological features, two tuning tasksworked best for all evaluation tasks, namely Mor-phAll and AllChoice, thus not confirming our hy-pothesis.
We speculate that in the absence of the lex-ical features, more features is better (these two tasksare the two hardest tasks for morphological featuresonly).
If we add the lexemic features, we do findour hypothesis confirmed, with almost all evaluationtasks performing best when the weights are tuned forthat task.
In the case of the three exceptions, the dif-ferences between the best performance and perfor-mance when tuned to the same task are very slight(< 0.06%).ReferencesTim Buckwalter.
2004.
Buckwalter Arabic morphologi-cal analyzer version 2.0.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In ACL?05, AnnArbor, MI, USA.Nizar Habash and Owen Rambow.
2007.
Arabic di-acritization through full morphological tagging.
InNAACL HLT 2007 Companion Volume, Short Papers,Rochester, NY, USA.Jan Hajic?, Otakar Smrz?, Tim Buckwalter, and HubertJin.
2005.
Feature-based tagger of approximationsof functional Arabic morphology.
In Proceedings ofthe Workshop on Treebanks and Linguistic Theories(TLT), Barcelona, Spain.Jan Hajic?.
2000.
Morphological tagging: Data vs. dic-tionaries.
In 1st Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL?00), Seattle, WA.J.A Nelder and R Mead.
1965.
A simplex method forfunction minimization.
In Computer Journal, pages303?333.Andreas Stolcke.
2002.
Srilm - an extensible languagetoolkit.
In Proceedings of the International Confer-ence on Spoken Language Processing (ICSLP).Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.2006.
Maximum entropy based restoration of arabicdiacritics.
In Coling-ACL?06, pages 577?584, Sydney,Australia.120
