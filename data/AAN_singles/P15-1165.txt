Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1713?1722,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsInverted indexing for cross-lingual NLPAnders S?gaard?
?Zeljko Agi?c?H?ector Mart?
?nez Alonso?Barbara Plank?Bernd Bohnet?
Anders Johannsen?
?Center for Language Technology, University of Copenhagen, Denmark?Google, London, United Kingdomsoegaard@hum.ku.dkAbstractWe present a novel, count-based approachto obtaining inter-lingual word represen-tations based on inverted indexing ofWikipedia.
We present experiments ap-plying these representations to 17 datasetsin document classification, POS tagging,dependency parsing, and word alignment.Our approach has the advantage that itis simple, computationally efficient andalmost parameter-free, and, more im-portantly, it enables multi-source cross-lingual learning.
In 14/17 cases, we im-prove over using state-of-the-art bilingualembeddings.1 IntroductionLinguistic resources are hard to come by and un-evenly distributed across the world?s languages.Consequently, transferring linguistic resources orknowledge from one language to another has beenidentified as an important research problem.
Mostwork on cross-lingual transfer has used Englishas the source language.
There are two reasonsfor this; namely, the availability of English re-sources and the availability of parallel data for(and translations between) English and most otherlanguages.In cross-lingual syntactic parsing, for exam-ple, two approaches to cross-lingual learninghave been explored, namely annotation projec-tion and delexicalized transfer.
Annotation pro-jection (Hwa et al, 2005) uses word-alignmentsin human translations to project predicted source-side analyses to the target language, producing anoisy syntactically annotated resource for the tar-get language.
On the other hand, delexicalizedtransfer (Zeman and Resnik, 2008; McDonald etal., 2011; S?gaard, 2011) simply removes lexi-cal features from mono-lingual parsing models,but assumes reliable POS tagging for the targetlanguage.
Delexicalized transfer works particu-larly well when resources from several source lan-guages are used for training; learning from mul-tiple other languages prevents over-fitting to thepeculiarities of the source language.
Some au-thors have also combined annotation projectionand delexicalized transfer, e.g., McDonald et al(2011).
Others have tried to augment delexical-ized transfer models with bilingual word repre-sentations (T?ackstr?om et al, 2013; Xiao and Guo,2014).In cross-lingual POS tagging, mostly annotationprojection has been explored (Fossum and Abney,2005; Das and Petrov, 2011), since all features inPOS tagging models are typically lexical.
How-ever, using bilingual word representations was re-cently explored as an alternative to projection-based approaches (Gouws and S?gaard, 2015).The major drawback of using bi-lexical repre-sentations is that it limits us to using a singlesource language.
T?ackstr?om et al (2013) ob-tained significant improvements using bilingualword clusters over a single source delexicalizedtransfer model, for example, but even better re-sults were obtained with delexicalized transfer inMcDonald et al (2011) by simply using severalsource languages.This paper introduces a simple method for ob-taining truly inter-lingual word representations inorder to train models with lexical features on sev-eral source languages at the same time.
Brieflyput, we represent words by their occurrence inclusters of Wikipedia articles linking to the sameconcept.
Our representations are competitive with1713state-of-the-art neural net word embeddings whenusing only a single source language, but also en-able us to exploit the availability of resources inmultiple languages.
This also makes it possible toexplore multi-source transfer for POS tagging.
Weevaluate the method across POS tagging and de-pendency parsing datasets in four languages in theGoogle Universal Treebanks v. 1.0 (see ?3.2.1),as well as two document classification datasetsand four word alignment problems using a hand-aligned text.
Finally, we also directly compare ourresults to Xiao and Guo (2014) on parsing data forfour languages from CoNLL 2006 and 2007.Contribution?
We present a novel approach to cross-lingualword representations with several advantagesover existing methods: (a) It does not requiretraining neural networks, (b) it does not relyon the availability of parallel data betweensource and target language, and (c) it enablesmulti-source transfer with lexical representa-tions.?
We present an evaluation of our inter-lingualword representations, based on inverted in-dexing, across four tasks: document classi-fication, POS tagging, dependency parsing,and word alignment, comparing our repre-sentations to two state-of-the-art neural netword embeddings.
For the 17 datasets, forwhich we can make this comparison, our sys-tem is better than these embedding modelson 14 datasets.
The word representationsare made publicly available at https://bitbucket.org/lowlands/2 Distributional word representationsMost NLP models rely on lexical features.
En-coding the presence of words leads to high-dimensional and sparse models.
Also, simple bag-of-words models fail to capture the relatedness ofwords.
In many tasks, synonymous words shouldbe treated alike, but their bag-of-words representa-tions are as different as those of dog and therefore.Distributional word representations are sup-posed to capture distributional similarities be-tween words.
Intuitively, we want similar words tohave similar representations.
Known approachesfocus on different kinds of similarity, some moresyntactic, some more semantic.
The representa-tions are typically either clusters of distribution-ally similar words, e.g., Brown et al (1992), orvector representations.
In this paper, we focuson vector representations.
In vector-based ap-proaches, similar representations are vectors closein some multi-dimensional space.2.1 Count-based and prediction-basedrepresentationsThere are, briefly put, two approaches to inducingvector-based distributional word representationsfrom large corpora: count-based and prediction-based approaches (Baroni et al, 2014).
Count-based approaches represent words by their co-occurrences.
Dimensionality reduction is typicallyperformed on a raw or weighted co-occurrencematrix using methods such as singular value de-composition (SVD), a method for maximizing thevariance in a dataset in few dimensions.
In ourinverted indexing, we use raw co-occurrence data.Prediction-based methods use discriminativelearning techniques to learn how to predict wordsfrom their context, or vice versa.
They rely ona neural network architecture, and once the net-work converges, they use word representationsfrom a middle layer as their distributional repre-sentations.
Since the network learns to predictcontexts from this representation, words occurringin the same contexts will get similar representa-tions.
In ?2.1.2, we briefly introduce the skip-gram and CBOW models (Mikolov et al, 2013;Collobert and Weston, 2008).Baroni et al (2014) argue in favor of prediction-based representations, but provide little explana-tion why prediction-based representations shouldbe better.
One key finding, however, is thatprediction-based methods tend to be more robustthan count-based methods, and one reason for thisseems to be better regularization.2.1.1 Monolingual representationsCount-based representations rely on co-occurrence information in the form of binarymatrices, raw counts, or point-wise mutual in-formation (PMI).
The PMI between two wordsisP (wi;wj) = logP (wi| wj)P (wi)and PMI representations associate a word wiwitha vector of its PMIs with all other words wj.
Di-mensionality reduction is typically performed us-ing SVD.
We will refer to two prediction-basedapproaches to learning word vectors, below: the1714KLEMENTIEV CHANDAR INVERTEDescoche (?car?, NOUN) approximately beyond upgrading car bicycle cars driving car carsexpressed (?expressed?, VERB) 1.61 55.8 month-to-month reiterates reiterating confirming exists defining exampletel?efono (?phone?, NOUN) alexandra davison creditor phone telephone e-mail phones phone telecommunication?arbol (?tree?, NOUN) tree market-oriented assassinate tree bread wooden tree trees growsescribi?o (?wrote?, VERB) wrote alleges testified wrote paul palace wrote inspired inspirationamarillo (?yellow?, ADJ) yellow louisiana 1911 crane grabs outfit colors yellow oohsdeauto (?car?, NOUN) car cars camaroausgedr?uckt (?expressed?, VERB) adjective decimal imperativefrvoiture (?car?, NOUN) mercedes-benz cars quickestexprim?e (?expressed?, VERB) simultaneously instead possiblet?el?ephone (?phone?, NOUN) phone create allowingarbre (?tree?, NOUN) tree trees grows?ecrit (?wrote?, VERB) published writers booksjaune (?yellow?, ADJ) classification yellow stagessvbil (?car?, NOUN) cars car automobilesuttryckte (?expressed?, VERB) rejected threatening unacceptabletelefon (?phone?, NOUN) telephones telephone sharetr?ad (?tree?, NOUN) trees tree trunksskrev (?wrote?, VERB) death wrote biographygul (?yellow?, ADJ) greenish bluish coloredTable 1: Three nearest neighbors in the English training data of six words occurring in the Spanish testdata, in the embeddings used in our experiments.
Only 2/6 words were in the German data.skip-gram model and CBOW.
The two modelsboth rely on three level architectures with input,output and a middle layer for intermediate tar-get word representations.
The major differenceis that skip-gram uses the target word as inputand the context as output, whereas the CBOWmodel does it the other way around.
Learning goesby back-propagation, and random target wordsare used as negative examples.
Levy and Gold-berg (2014) show that prediction-based represen-tations obtained with the skip-gram model can berelated to count-based ones obtained with PMI.They argue that which is best, varies across tasks.2.1.2 Bilingual representationsKlementiev et al (2012) learn distinct embeddingmodels for the source and target languages, butwhile learning to minimize the sum of the twomodels?
losses, they jointly learn a regularizing in-teraction matrix, enforcing word pairs aligned inparallel text to have similar representations.
Notethat Klementiev et al (2012) rely on word-alignedparallel text, and thereby on a large-coverage softmapping of source words to target words.
Otherapproaches rely on small coverage dictionarieswith hard 1:1 mappings between words.
Klemen-tiev et al (2012) do not use skip-gram or CBOW,but the language model presented in Bengio etal.
(2003).Chandar et al (2014) also rely on sentence-aligned parallel text, but do not make use of wordalignments.
They begin with bag-of-words repre-sentations of source and target sentences.
Theythen use an auto-encoder architecture.
Auto-encoders for document classification typically tryto reconstruct bag-of-words input vectors at theoutput layer, using back-propagation, passing therepresentation through a smaller middle layer.This layer then provides a dimensionality reduc-tion.
Chandar et al (2014) instead replace the out-put layer with the target language bag-of-wordsreconstruction.
In their final set-up, they simul-taneously minimize the loss of a source-source, atarget-target, a source-target, and a target-sourceauto-encoder, which corresponds to training a sin-gle auto-encoder with randomly chosen instancesfrom source-target pairs.
The bilingual word vec-tors can now be read off the auto-encoder?s middlelayer.Xiao and Guo (2014) use a CBOW model andrandom target words as negative examples.
Thetrick they introduce to learn bilingual embeddings,relies on a bilingual dictionary, in their case ob-tained from Wiktionary.
They only use the unam-biguous translation pairs for the source and targetlanguages in question and simply force translationequivalents to have the same representation.
Thiscorresponds to replacing words from unambigu-1715ous translation pairs with a unique dummy sym-bol.Gouws and S?gaard (2015) present a much sim-pler approach to learning prediction-based bilin-gual representations.
They assume a list of source-target pivot word pairs that should obtain simi-lar representations, i.e., translations or words withsimilar representations in some knowledge base.They then present a generative model for con-structing a mixed language corpus by randomlyselecting sentences from source and target cor-pora, and randomly replacing pivot words withtheir equivalent in the other language.
They showthat running the CBOW model on such a mixedcorpus suffices to learn competitive bilingual em-beddings.
Like Xiao and Guo (2014), Gouws andS?gaard (2015) only use unambiguous translationpairs.There has, to the best of our knowledge, been noprevious work on count-based approaches to bilin-gual representations.2.2 Inverted indexingIn this paper, we introduce a new count-basedapproach, INVERTED, to obtaining cross-lingualword representations using inverted indexing,comparing it with bilingual word representationslearned using discriminative techniques.
The mainadvantage of this approach, apart for its simplic-ity, is that it provides truly inter-lingual represen-tations.Our idea is simple.
Wikipedia is a cross-lingual,crowd-sourced encyclopedia with more than 35million articles written in different languages.
Atthe time of writing, Wikipedia contains more than10,000 articles in 129 languages.
52 languageshad more than 100,000 articles.
Several articlesare written on the same topic, but in different lan-guages, and these articles all link to the same nodein the Wikipedia ontology, the same Wikipediaconcept.
If for a set of languages, we identifythe common subset of Wikipedia concepts, we canthus describe each concept by the set of terms usedin the corresponding articles.
Each term set willinclude terms from each of the different languages.We can now present a word by the corre-sponding row in the inverted indexing of thisconcept-to-term set matrix.
Instead of repre-senting a Wikipedia concept by the terms usedacross languages to describe it, we describe aword by the Wikipedia concepts it is used to de-scribe.
Note that because of the cross-lingualconcepts, this vector representation is by defini-tion cross-lingual.
So, for example, if the wordglasses is used in the English Wikipedia article onHarry Potter, and the English Wikipedia article onGoogle, and the word Brille occurs in the corre-sponding German ones, the two words are likelyto get similar representations.In our experiments, we use the common sub-set of available German, English, French, Span-ish, and Swedish Wikipedia dumps.1We leave outwords occurring in more than 5000 documents andperform dimensionality reduction using stochas-tic, two-pass, rank-reduced SVD - specifically, thelatent semantic indexing implementation in Gen-sim using default parameters.22.3 Baseline embeddingsWe use the word embedding models of Klemen-tiev et al (2012)3(KLEMENTIEV), and Chandaret al (2014) (CHANDAR) as baselines in the ex-periments below.
We also ran some of our exper-iments with the embeddings provided by Gouwsand S?gaard (2015), but results were very similarto Chandar et al (2014).
We compare the near-est cross-language neighbors in the various rep-resentations in Table 1.
Specifically, we selectedfive words from the Spanish test data and searchedfor its three nearest neighbors in KLEMENTIEV,CHANDAR and INVERTED.
The nearest neighborsare presented left to right.
We note that CHANDARand INVERTED seem to contain less noise.
KLE-MENTIEV is the only model that relies on word-alignments.
Whether the noise originates fromalignments, or just model differences, is unclearto us.2.4 Parameters of the word representationmodelsFor KLEMENTIEV and CHANDAR, we rely on em-beddings provided by the authors.
The only pa-rameter in inverted indexing is the fixed dimen-sionality in SVD.
Our baseline models use 40 di-mensions.
In document classification, we alsouse 40 dimensions, but for POS tagging and de-pendency parsing, we tune the dimensionality pa-rameter ?
?
{40, 80, 160} on Spanish develop-ment data when possible.
For document clas-1https://sites.google.com/site/rmyeid/projects/polyglot2http://radimrehurek.com/gensim/3http://klementiev.org/data/distrib/1716TRAIN TEST TOKEN COVERAGElang data points tokens data points tokens KLEMENTIEV CHANDAR INVERTEDRCV ?
DOCUMENT CLASSIFICATIONen 10000 ?
?
?
0.314 0.314 0.779de ?
?
4998 ?
0.132 0.132 0.347AMAZON ?
DOCUMENT CLASSIFICATIONen 6000 ?
?
?
0.314 0.314 0.779de ?
?
6000 ?
0.132 0.132 0.347GOOGLE UNIVERSAL TREEBANKS ?
POS TAGGING & DEPENDENCY PARSINGen 39.8k 950k 2.4k 56.7k ?
?
?de 2.2k 30.4k 1.0k 16.3k 0.886 0.884 0.587es 3.3k 94k 0.3k 8.3k 0.916 0.916 0.528fr 3.3k 74.9k 0.3k 6.9k 0.888 0.888 0.540sv 4.4k 66.6k 1.2k 20.3k n/a n/a 0.679CONLL 07 ?
DEPENDENCY PARSINGen 18.6 447k ?
?
?
?
?es ?
?
206 5.7k 0.841 0.841 0.455de ?
?
357 5.7k 0.616 0.612 0.294sv ?
?
389 5.7k n/a n/a 0.561EUROPARL ?
WORD ALIGNMENTen ?
?
100 ?
0.370 0.370 0.370es ?
?
100 ?
0.533 0.533 0.533Table 2: Characteristics of the data sets.
Embeddings coverage (token-level) for KLEMENTIEV, CHAN-DAR and INVERTED on the test sets.
We use the common vocabulary on WORD ALIGNMENT.sification and word alignment, we fix the num-ber of dimensions to 40.
For both our base-lines and systems, we also tune a scaling fac-tor ?
?
{1.0, 0.1, 0.01, 0.001} for POS taggingand dependency parsing, using the scaling methodfrom Turian et al (2010), also used in Gouws andS?gaard (2015).
We do not scale our embeddingsfor document classification or word alignment.3 ExperimentsThe data set characteristics are found in Table 2.3.3.1 Document classificationData Our first document classification task is topicclassification on the cross-lingual multi-domainsentiment analysis dataset AMAZON in Pretten-hofer and Stein (2010).4We represent each docu-ment by the average of the representations of thosewords that we find both in the documents and inour embeddings.
Rather than classifying reviewsby sentiment, we classify by topic, trying to dis-criminate between book reviews, music reviewsand DVD reviews, as a three-way classificationproblem, training on English and testing on Ger-man.
Unlike in the other tasks below, we always4http://www.webis.de/research/corpora/use unscaled word representations, since these areour only features.
All word representations have40 dimensions.The other document classification task is a four-way classification problem distinguishing betweenfour topics in RCV corpus.5See Klementiev et al(2012) for details.
We use exactly the same set-upas for AMAZON.Baselines We use the default parameters of the im-plementation of logistic regression in Sklearn asour baseline.6The feature representation is the av-erage embedding of non-stopwords in KLEMEN-TIEV, resp., CHANDAR.
Out-of-vocabulary wordsdo not affect the feature representation of the doc-uments.System For our system, we replace the above neu-ral net word embeddings with INVERTED repre-sentations.
Again, out-of-vocabulary words do notaffect the feature representation of the documents.3.2 POS taggingData We use the coarse-grained part-of-speech an-notations in the Google Universal Treebanks v. 1.05http://www.ml4nlp.de/code-and-data6http://scikit-learn.org/stable/1717(McDonald et al, 2013).7Out of the languages inthis set of treebanks, we focus on five languages(de, en, es, fr, sv), with English only used as train-ing data.
Those are all treebanks of significantsize, but more importantly, we have baseline em-beddings for four of these languages, as well as tagdictionaries (Li et al, 2012) needed for the POStagging experiments.Baselines One baseline method is a type-constrained structured perceptron with only orto-graphic features, which are expected to transferacross languages.
The type constraints come fromWiktionary, a crowd-sourced tag dictionary.8Typeconstraints from Wiktionary were first used by Liet al (2012), but note that their set-up is unsu-pervised learning.
T?ackstr?om et al (2013) alsoused type constraints in a supervised set-up.
Ourlearning algorithm is the structured perceptron al-gorithm originally proposed by Collins (2002).
Inour POS tagging experiments, we always do 10passes over the data.
We also present two otherbaselines, where we augment the feature repre-sentation with different embeddings for the targetword, KLEMENTIEV and CHANDAR.
With all theembeddings in POS tagging, we assign a meanvector to out-of-vocabulary words.System For our system, we simply augment thedelexicalized POS tagger with the INVERTED dis-tributional representation of the current word.
Thebest parameter setting on Spanish developmentdata was ?
= 0.01, ?
= 160.3.3 Dependency parsingData We use the same treebanks from the GoogleUniversal Treebanks v. 1.0 as used in our POS tag-ging experiments.
We again use the Spanish de-velopment data for parameter tuning.
For compat-ibility with Xiao and Guo (2014), we also presentresults on CoNLL 2006 and 2007 treebanks forlanguages for which we had baseline and systemword representations (de, es, sv).
Our parametersettings for these experiments were the same asthose tuned on the Spanish development data fromthe Google Universal Treebanks v. 1.0.Baselines The most obvious baseline in our exper-iments is delexicalized transfer (DELEX) (McDon-ald et al, 2011; S?gaard, 2011).
This baseline sys-tem simply learns models without lexical features.We use a modified version of the first-order Mate7http://code.google.com/p/uni-dep-tb/8https://code.google.com/p/wikily-supervised-pos-tagger/parser (Bohnet, 2010) that also takes continuous-valued embeddings as input an disregards featuresthat include lexical items.For our embeddings baselines, we augment thefeature space by adding embedding vectors forhead h and dependent d. We experimented withdifferent versions of combining embedding vec-tors, from firing separate h and d per-dimensionfeatures (Bansal et al, 2014) to combining theirinformation.
We found that combining the em-beddings of h and d is effective and consistentlyuse the absolute difference between the embed-ding vectors, since that worked better than addi-tion and multiplication on development data.Delexicalized transfer (DELEX) uses three (3)iterations over the data in both the single-sourceand the multi-source set-up, a parameter set onthe Spanish development data.
The remaining pa-rameters were obtained by averaging over perfor-mance with different embeddings on the Spanishdevelopment data, obtaining: ?
= 0.005, ?
=20, i = 3, and absolute difference for vector com-bination.
With all the embeddings in dependencyparsing, we assign a POS-specific mean vector toout-of-vocabulary words, i.e., the mean of vectorsfor words with the input word?s POS.System We use the same parameters as those usedfor our baseline systems.
In the single-source set-up, we use absolute difference for combining vec-tors, while addition in the multi-source set-up.3.4 Word alignmentData We use the manually word-aligned English-Spanish Europarl data from Graca et al (2008).The dataset contains 100 sentences.
The annota-tors annotated whether word alignments were cer-tain or possible, and we present results with allword alignments and with only the certain ones.See Graca et al (2008) for details.Baselines For word alignment, we simply alignevery aligned word in the gold data, for which wehave a word embedding, to its (Euclidean) nearestneighbor in the target sentence.
We evaluate thisstrategy by its precision (P@1).System We compare INVERTED with KLEMEN-TIEV and CHANDAR.
To ensure a fair comparison,we use the subset of words covered by all threeembeddings.1718de es fr sv av-svEN?TARGETEMBEDSK12 80.20 73.16 47.69 - 67.02C14 74.85 83.03 48.24 - 68.71INVERTED SVD 81.18 82.12 49.68 78.72 70.99MULTI-SOURCE?TARGETINVERTED SVD 80.10 84.69 49.68 78.72 70.66Table 4: POS tagging (accuracies), K12: KLEMENTIEV, C14: CHANDAR.
Parameters tuned on devel-opment data: ?
= 0.01, ?
= 160.
Iterations not tuned (i = 10).
Averages do not include Swedish, forcomparability.Dataset KLEMENTIEV CHANDAR INVERTEDAMAZON 0.32 0.36 0.49RCV 0.75 0.90 0.55Table 3: Document classification results (F1-scores)UASde es svEN?TARGETDELEX - 44.78 47.07 56.75DELEX-XIAO - 46.24 52.05 57.79EMBEDSK12 44.77 47.31 -C14 44.32 47.56INVERTED - 45.01 47.45 56.15XIAO - 49.54 55.72 61.88Table 6: Dependency parsing for CoNLL2006/2007 datasets.
Parameters same as on theGoogle Universal Treebanks.4 Results4.1 Document classificationOur document classification results in Table 3 aremixed, but we note that both Klementiev et al(2012) and Chandar et al (2014) developed theirmethods using development data from the RCVcorpus.
It is therefore not surprising that theyobtain good results on this data.
On AMAZON,INVERTED is superior to both KLEMENTIEV andCHANDAR.4.2 POS taggingIn POS tagging, INVERTED leads to signifi-cant improvements over using KLEMENTIEV andCHANDAR.
See Table 4 for results.
Somewhatsurprisingly, we see no general gain from usingmultiple source languages.
This is very differentfrom what has been observed in dependency pars-ing (McDonald et al, 2011), but may be explainedby treebank sizes, language similarity, or the noiseintroduced by the word representations.4.3 Dependency parsingIn dependency parsing, distributional word rep-resentations do not lead to significant improve-ments, but while KLEMENTIEV and CHANDARhurt performance, the INVERTED representationslead to small improvements on some languages.The fact that improvements are primarily seen onSpanish suggest that our approach is parameter-sensitive.
This is in line with previous ob-servations that count-based methods are moreparameter-sensitive than prediction-based ones(Baroni et al, 2014).For comparability with Xiao and Guo (2014),we also did experiments with the CoNLL 2006and CoNLL 2007 datasets for which we hadembeddings (Table 6).
Again, we see little effectsfrom using the word representations, and we alsosee that our baseline model is weaker than the onein Xiao and Guo (2014) (DELEX-XIAO).
See ?5for further discussion.4.4 Word alignmentThe word alignment results are presented in Ta-ble 7.
On the certain alignments, we see an ac-curacy of more than 50% with INVERTED in onecase.
KLEMENTIEV and CHANDAR have the ad-vantage of having been trained on the English-Spanish Europarl data, but nevertheless we seeconsistent improvements with INVERTED overtheir off-the-shelf embeddings.1719UAS LASde es fr sv de es fr svEN?TARGETDELEX - 56.26 62.11 64.30 66.61 48.24 53.01 54.98 56.93EMBEDSK12 56.47 61.92 61.51 - 48.26 52.88 51.76 -C14 56.19 61.97 62.95 - 48.11 52.97 53.90 -INVERTED - 56.18 61.71 63.81 66.54 48.82 53.04 54.81 57.18MULTI-SOURCE?TARGETDELEX - 56.80 63.21 66.00 67.49 49.32 54.77 56.53 57.86INVERTED - 56.56 64.03 66.22 67.32 48.82 55.03 56.79 57.70Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachmentscores).
Parameters tuned on development data: ?
= 0.005, ?
= 20, i = 3.KLEMENTIEV CHANDAR INVERTEDEN-ES (S+P) 0.20 0.24 0.25ES-EN (S+P) 0.35 0.32 0.41EN-ES (S) 0.20 0.25 0.25ES-EN (S) 0.38 0.39 0.53Table 7: Word alignment results (P@1).
S=sure (certain) alignments.
P=possible alignments.5 Related WorkAs noted in ?1, there has been some work on learn-ing word representations for cross-lingual parsinglately.
T?ackstr?om et al (2013) presented a bilin-gual clustering algorithm and used the word clus-ters to augment a delexicalized transfer baseline.Bansal et al (2014), in the context of monolingualdependency parsing, investigate continuous wordrepresentation for dependency parsing in a mono-lingual cross-domain setup and compare them toword clusters.
However, to make the embeddingswork, they had to i) bucket real values and performhierarchical clustering on them, ending up withword clusters very similar to those of T?ackstr?omet al (2013); ii) use syntactic context to estimateembeddings.
In the cross-lingual setting, syntacticcontext is not available for the target language, butdoing clustering on top of inverted indexing is aninteresting option we did not explore in this paper.Xiao and Guo (2014) is, to the best of ourknowledge, the only parser using bilingual em-beddings for unsupervised cross-lingual parsing.They evaluate their models on CoNLL 2006 andCoNLL 2007, and we compare our results totheirs in ?4.
They obtain much better relativeimprovements on dependency parsing that we do- comparable to those we observe in documentclassification and POS tagging.
It is not clear tous what is the explanation for this improvement.The approach relies on a bilingual dictionaryas in Klementiev et al (2012) and Gouws andS?gaard (2015), but none of these embeddingsled to improvements.
Unfortunately, we did nothave the code or embeddings of Xiao and Guo(2014).
One possible explanation is that they usethe embeddings in a very different way in theparser.
They use the MSTParser.
Unfortunately,they do not say exactly how they combine theembeddings with their baseline feature model.The idea of using inverted indexing inWikipedia for modelling language is not entirelynew either.
In cross-lingual information retrieval,this technique, sometimes referred to as explicitsemantic analysis, has been used to measuresource and target language document relatedness(Potthast et al, 2008; Sorg and Cimiano, 2008).Gabrilovich and Markovitch (2009) also use thistechnique to model documents, and they evaluatetheir method on text categorization and on com-puting the degree of semantic relatedness betweentext fragments.
See also M?uller and Gurevych(2009) for an application of explicit semantic anal-ysis to modelling documents.
This line of workis very different from ours, and to the best ofour knowledge, we are the first to propose to useinverted indexing of Wikipedia for cross-lingualword representations.17206 ConclusionsWe presented a simple, scalable approach to ob-taining cross-lingual word representations that en-ables multi-source learning.
We compared theserepresentations to two state-of-the-art approachesto neural net word embeddings across four tasksand 17 datasets, obtaining better results than bothapproaches in 14/17 of these cases.ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In ACL.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In ACL.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In COLING.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Sarath Chandar, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas CRaykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In NIPS.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Exper-iments with Perceptron Algorithms.
In EMNLP.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In ICML.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In ACL.Victoria Fossum and Steven Abney.
2005.
Automati-cally inducing a part-of-speech tagger by projectingfrom multiple source languages across aligned cor-pora.
In IJCNLP.Evgeniy Gabrilovich and Shaul Markovitch.
2009.Wikipedia-based semantic interpretation for naturallanguage processing.
Journal of Artificial Intelli-gence Research, pages 443?498.Stephan Gouws and Anders S?gaard.
2015.
Sim-ple task-specific bilingual word embeddings.
InNAACL.Joao Graca, Joana Pardal, Lu?
?sa Coheur, and Dia-mantino Caseiro.
2008.
Building a golden collec-tion of parallel multi-language word alignments.
InLREC.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(3):311?325.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In COLING.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In ACL.Shen Li, Jo?ao Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In EMNLP.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In EMNLP.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuz-man Ganchev, Keith Hall, Slav Petrov, HaoZhang, Oscar T?ackstr?om, Claudia Bedini, N?uriaBertomeu Castell?o, and Jungmee Lee.
2013.
Uni-versal dependency annotation for multilingual pars-ing.
In ACL.Tomas Mikolov, Ilya Sutskever, Kai Chen, GregoryCorrado, and Jeffrey Dean.
2013.
Distributed rep-resentations of words and phrases and their compo-sitionality.
In NIPS.Christof M?uller and Iryna Gurevych.
2009.
A studyon the semantic relatedness of query and documentterms in information retrieval.
In EMNLP.Martin Potthast, Benno Stein, and Maik Anderka.2008.
A wikipedia-based multilingual retrievalmodel.
In Advances in Information Retrieval.Peter Prettenhofer and Benno Stein.
2010.
Cross-language text classification using structural corre-spondence learning.
In ACL.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In Pro-ceedings of ACL.Philipp Sorg and Philipp Cimiano.
2008.
Cross-lingual information retrieval with explicit seman-tic analysis.
In Working Notes for the CLEF 2008Workshop.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
TACL, 1:1?12.1721Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In ACL.Min Xiao and Yuhong Guo.
2014.
Distributed wordrepresentation learning for cross-lingual dependencyparsing.
In CoNLL.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In IJCNLP.1722
