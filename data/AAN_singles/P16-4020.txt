Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 115?120,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsPersonalized Exercises for Preposition LearningJohn Lee, Mengqi LuoThe Halliday Centre for Intelligent Applications of Language StudiesDepartment of Linguistics and TranslationCity University of Hong Kong{jsylee, mengqluo}@cityu.edu.hkAbstractWe present a computer-assisted languagelearning (CALL) system that generatesfill-in-the-blank items for preposition us-age.
The system takes a set of carrier sen-tences as input, chooses a preposition ineach sentence as the key, and then auto-matically generates distractors.
It person-alizes item selection for the user in twoways.
First, it logs items to which the userpreviously gave incorrect answers, and of-fers similar items in a future session as re-view.
Second, it progresses from easierto harder sentences, to minimize any hin-drance on preposition learning that mightbe posed by difficult vocabulary.1 IntroductionMany learners of English find it challenging tomaster the use of prepositions.
Preposition usageis a frequent error category in various learner cor-pora (Izumi et al, 2003; Dahlmeier et al, 2013;Lee et al, 2015); indeed, entire exercise bookshave been devoted to training learners on preposi-tion usage (Watcyn-Jones and Allsop, 2000; Yates,2010).
To address this area of difficulty, wepresent a system that automatically generates fill-in-the-blank (FIB) preposition items with multiplechoices.Also known as gap-fill or cloze items, FIBitems are a common form of exercise in computer-assisted language learning (CALL) applications.Table 1 shows an example item designed for teach-ing English preposition usage.
It contains a sen-tence, ?The objective is to kick the ball intothe opponent?s goal?, with the preposition ?into?blanked out; this sentence serves as the stem (orcarrier sentence).
It is followed by four choicesfor the blank, one of which is the key (i.e., thecorrect answer), and the other three are distrac-tors.
These choices enable the CALL applicationto provide immediate and objective feedback tothe learner.Traditional exercise books no longer meet althe needs of today?s learners.
The pedagogicalbenefits of using authentic textual material havebeen well documented (Larimer and Schleicher,1999; Erbaggio et al, 2012).
One recent approachturns text on web pages into slot-fill items (Meur-ers et al, 2010).
By offering the learner the free-dom to choose his or her own preferred text, thisapproach motivates the learner to complete the ex-ercises.Our system automatically constructs FIB prepo-sition items from sentences in Wikipedia, a cor-pus that contains authentic language.
As moreusers own mobile devices, mobile applications arenow among the most efficient ways to provideon-demand language learning services.
Althoughuser attention on mobile devices can be brief andsporadic, each FIB item can be completed withina short time, and therefore our system offers aneducational option for users to spend their idlemoments.
Focusing on prepositions, the systemgenerates distractors based on error statistics com-piled from learner corpora.
Further, it maintainsan estimate of the user?s vocabulary level, and tai-The objective is to kick the ball theopponent?s goal.
(A) in(B) into(C) to(D) withTable 1: An automatically generated fill-in-the-blank item, where ?into?
is the key, and the otherthree choices are distractors.115lors item selection to address his or her areas ofweakness.
To the best of our knowledge, this isthe first system that offers these personalizationfeatures for preposition items.The rest of the paper is organized as follows.Section 2 reviews previous work.
Section 3 out-lines the algorithms for generating the fill-in-the-blank items.
Section 4 gives details about the per-sonalization features in the item selection process.Section 5 reports implementation details and eval-uation results.2 Previous workThe Internet presents the language learner withan embarassment of riches.
A plethora of CALLwebsites?Duolingo, LearnEnglish Grammar bythe British Council, or Rosetta Stone, to namejust a few?provide a variety of speaking, listen-ing, translation, matching and multiple choice ex-ercises.
In these exercises, the carrier sentencesand other language materials are typically hand-crafted.
As a result, the number of items are lim-ited, the language use can sometimes lack authen-ticity, and the content may not match the users?individual interests.Promoting use of authentic material, the WERTisystem provides input enhancement to web pagesfor the purpose of language learning (Meurers etal., 2010).
It highlights grammatical constructionson which the user needs practice, and turns theminto slot-fill exercises.
It handles a wide rangeof constructions, including prepositions, determin-ers, gerunds, to-infinitives, wh-questions, tensesand phrasal verbs.
On the one hand, the systemoffers much flexibility since it is up to the user toselect the page.
On the other, the selected text doesnot necessarily suit the user in terms of its lan-guage quality, level of difficulty and the desiredgrammatical constructions.A number of other systems use text corpora tocreate grammar exercises.
The KillerFiller tool inthe VISL project, for example, generates slot-fillitems from texts drawn from corpora (Bick, 2005).Similar to the WERTi system, an item takes theoriginal word as its only key, and does not accountfor the possibility of multiple correct answers.Other systems attempt to generate distractorsfor the key.
Chen et al (2006) manually designedpatterns for this purpose.
Smith et al (2010)utilized a theusaurus, while Zesch and Mela-mud (2014) developed context-sensitive rules.The meeting on Monday went well ...NP head prep objprep pobj... kick the ball into the opponents?
goalVP head prep objprep pobjFigure 1: Parse tree for example carrier sentences.Distractors are generated on the basis of the prepo-sitional object (?obj?
), and the NP head or VPhead to which the prepositional phrase is attached(Section 3).
See Table 1 for the item producedfrom the bottom sentence.Unlike our approach, they did not adapt to thelearner?s behavior.
While some of these systemsserve to provide draft FIB items for teachers topost-edit (Skory and Eskenazi, 2010), most remainresearch prototypes.A closely related research topic for this paper isautomatic correction of grammatical errors (Ng etal., 2014).
While the goal of distractor generationis to identify words that yield incorrect sentences,it is not merely the inverse of the error correctiontask.
An important element of the distractor gener-ation task is to ensure that distractor appears plau-sible to the user.
In contrast to the considerable ef-fort in developing tools for detecting and correct-ing preposition errors (Tetreault and Chodorow,2008; Felice and Pulman, 2009), there is only oneprevious study on preposition distractor genera-tion (Lee and Seneff, 2007).
Our system builds onthis study by incorporating novel algorithms fordistractor generation and personalization features.3 Item creationThe system considers all English sentences in theWikicorpus (Reese et al, 2010) that have fewerthan 20 words as carrier sentence candidates.
Ineach candidate sentence, the system scans forprepositions, and extracts two features from thelinguistic context of each preposition:?
The prepositional object.
In Figure 1, forexample, the words ?Monday?
and ?goal?
arerespectively the prepositional objects of thekeys, ?on?
and ?into?.116Co-occurrence method... kicked the chair with ...... kicked the can with ...... with the goal of ...Learner Error method... kicked it <error>in</error> the goal.... kick the ball <error>in</error> theother team?s goal.Learner Revision method... kick the ball to his own goal.... kick the ball into the goal.... kick the ball to the goal.... kick it towards the goal.Table 2: The Co-occurrence method (Section 3.1)generates ?with?
as the distractor for the carriersentence in Figure 1; the Learner Error method(Section 3.2) generates ?in?
; the Learner Revisionmethod (Section 3.3) generates ?to?.?
The head of the noun phrase or verb phrase(NP/VP head) to which the prepositionalphrase (PP) is attached.
In Figure 1, the PP?into the opponents?
goal?
is attached to theVP head ?kick?
; the PP ?on Monday?
is at-tached to the NP head ?meeting?.In order to retrieve the preposition, the preposi-tional object, and the NP/VP head (cf.
Section 3),we parsed the Wikicorpus, as well as the corporamentioned below, with the Stanford parser (Man-ning et al, 2014).
The system passes the two fea-tures above to the following methods to attempt togenerate distractors.
If more than one key is possi-ble, it prefers the one for which all three methodscan generate a distractor.3.1 Co-occurrence methodThis method requires co-occurrence statistics froma large corpus of well-formed English sentences.It selects as distractor the preposition that co-occurs most frequently with either the preposi-tional object or the NP/VP head, but not both.
Asshown in Table 2, this method generates the dis-tractor ?with?
for the carrier sentence in Figure 1,since many instances of ?kick ... with?
and ?with... goal?
are attested.
The reader is referred to Leeand Seneff (2007) for details.Our system used the English portion of Wiki-corpus (Reese et al, 2010) to derive statistics forthis method.3.2 Learner error methodThis method requires examples of English sen-tences from an error-annotated learner corpus.The corpus must indicate the preposition errors,but does not need to provide corrections for theseerrors.
The method retrieves all sentences thathave a PP with the given prepositional object andattached to the given NP/VP head, and selectsthe preposition that is most frequently marked aswrong.To derive statistics for this method, our sys-tem used the NUS Corpus of Learner En-glish (Dahlmeier et al, 2013), the EF-CambridgeOpen Language Database (Geertzen et al, 2013)and a corpus of essay drafts written by Chineselearners of English (Lee et al, 2015).3.3 Learner revision methodFinally, our system exploits the revision behaviorof learners in their English writing.
This methodrequires draft versions of the same text written bya learner.
It retrieves all learner sentences in a draftthat contains a PP with the given prepositional ob-ject, and attached to the given NP/VP head.
It thenselects as distractor the preposition that is most of-ten edited in a later draft.
As shown in Table 2, thismethod generates the distractor ?to?
for the carriersentence in Figure 1, since it is most often editedin the given linguistic context.
The reader is re-ferred to Lee et al (2016) for details.To derive statistics for this method, our sys-tem also used the aforementioned corpus of essaydrafts.4 Item selectionLearners benefit most from items that are neithertoo easy nor too difficult.
Following principlesfrom adaptive testing (Bejar et al, 2003), the sys-tem tracks the user?s performance in order to selectthe most suitable items.
It does so by consideringthe vocabulary level of the carrier sentence (Sec-tion 4.1) and the user?s previous mistakes (Sec-tion 4.2).4.1 Sentence difficultyA potential pitfall with the use of authentic sen-tences, such as those from Wikipedia, is that dif-117ficult vocabulary can hinder the learning of prepo-sition usage.
To minimize this barrier, the systemstarts with simpler carrier sentences for each newuser, and then progresses to harder ones.For simplicity, we chose to estimate the diffi-culty of a sentence with respect to its vocabulary.1Specifically, we categorized each word into one often levels, using graded vocabulary lists compiledby the Hong Kong Education Bureau (2012) andthe Google Web Trillion Word Corpus.2The listsconsist of about 4,000 words categorized into foursets, namely, those suitable for students in juniorprimary school, senior primary, junior secondary,or senior secondary.
Levels 1 to 4 correspond tothese four sets.
If the word does not belong tothese sets, it is classified at a level between 5 and10, according to decreasing word frequency in theGoogle corpus.
The difficulty level of a sentence isthen defined as the level of its most difficult word.For each new user, the system starts with sen-tences at Level 4 or lower.
It keeps track of hisor her performance for the last ten items.
If theuser gave correct answers for more than 60% ofthe items from the current level, the system incre-ments the difficulty level by one.
Otherwise, it de-creases the difficulty level by one.4.2 Preposition difficultyIn Figure 2, the system presents an item to the user.If the user selects a distractor rather than the key,he or she is informed by a pop-up box (Figure 3),and may then make another attempt.
At this point,the user may also request to see a ?similar?
itemto reinforce the learning of the preposition usage(Figure 4).
Two items are defined as ?similar?if they have the same preposition as key, and thesame prepositional object and NP/VP head.The system records all items to which the usergave incorrect answers; we will refer to this setof items as the ?wrong list?.
When the user logsin next time, the system begins with a review ses-sion.
For each item in the ?wrong list?, it retrievesa ?similar?
item from the database (Figure 4), thusfacilitating the user in reviewing prepositional us-age with which he had difficulty in a previous ses-sion.
If the user now successfully chooses the key,1The difficulty level of a sentence depends also on syntac-tic and semantic features.
Most metrics for measuring read-ability, however, have focused on the document rather thanthe sentence level (Miltsakaki and Troutt, 2008; Pitler andNenkova, 2008).2http://norvig.com/ngrams/Figure 2: The system displays a carrier sentencewith the key ?in?
and the distractors ?on?
and ?of?.Figure 3: After the user selected the distractor?on?
for the item in Figure 2, a pop-up box alertsthe user.the item is taken off the ?wrong list?.
After thereview session, the system resumes random selec-tion of items within the estimated level of sentencedifficulty, as described in the last section.5 Implementation and evaluation5.1 ArchitectureWe used the MySQL database, and JSP for thewebsite backend.
There are three main tables.
TheQuestion table stores all carrier sentences selectedfrom the English portion of the Wikicorpus (Reeseet al, 2010).
To expedite item retrieval and iden-tification of ?similar?
items, the table stores thekey, prepositional object and NP/VP head of eachitem, as well as the difficulty level of the carriersentence.The Answer table stores the distractors for eachitem.
Currently, the distractors do not change ac-cording to user identity, but we anticipate a futureversion that personalizes the distractors with re-spect to the user?s mother tongue.The User table stores the user profile.
Informa-tion includes the user?s personal ?wrong list?, hisor her estimated vocabulary level, as well as logintime stamps.118Figure 4: As review for the user, the system offersan item that is similar to the one in Figure 2, whichalso has ?in?
as the key, ?eat?
as the VP head and?restaurant?
as the prepositional object.5.2 InterfaceFor a better user experience on mobile devices, weused JQuery Mobile for interface development.
Atthe start page, the user can register for a new ac-count, or log in with an existing user name andpassword.
Alternatively, the user can choose toaccess the system as a guest.
In this case, he orshe would be treated as a new user, but no userhistory would be recorded.The user can attempt an arbitrary number ofpreposition items before logging out.
Each itemis presented on its own page, with the distractorand key displayed in random order (Figure 2).
Theuser chooses the best preposition by tapping on itsbutton.
If the answer is correct, the system ad-vances to the next item; otherwise, it informs theuser via a pop-up box (Figure 3), and then flagsthe distractor in red.
The user may try again untilhe or she successfully chooses the key.5.3 EvaluationTo assess system quality, we asked two profes-sional English teachers to annotate a set of 400items, which included both automatically gener-ated and human-crafted items.
For each choice inan item, the teachers judged whether it is correct orincorrect.
They did not know whether each choicewas the key or a distractor.
They may judge one,multiple, or none of the choices as correct.A distractor is called ?reliable?
if it yieldsan incorrect sentence.
As reported in Lee etal.
(2016), the proportion of distractors judgedreliable reached 97.4% for the Learner Revisionmethod, 96.1% for the Co-occurrence method, and95.6% for the Learner Error method.For each incorrect choice, the two annotatorsfurther assessed its plausibility as a distractorfrom their experience in teaching English to na-tive speakers of Chinese.
They may label it as ei-ther ?obviously wrong?, ?somewhat plausible?, or?plausible?.
The Learner Error method producedthe best distractors, with 51.2% rated ?plausible?,followed by the Learner Revision method (45.4%)and the Co-occurrence method (34.6%).
The num-ber of plausible distractors per item among the au-tomatically generated items compares favourablyto the human-crafted ones (Lee et al, 2016).6 ConclusionWe have presented a CALL system that turns sen-tences from Wikipedia into fill-in-the-blank itemsfor preposition usage.
Using statistics from bothstandard and learner corpora, it generates plausi-ble distractors to provide multiple choices.The system tailors item selection for individuallearners in two ways.
First, it chooses carrier sen-tences that matches the learner?s estimated vocab-ulary level.
Second, to facilitate learning, it of-fers review sessions with items that are similar tothose with which the learner previously demon-strated difficulty.In future work, we plan to extend the systemcoverage beyond preposition to other commonlearner error types.AcknowledgmentsWe thank the reviewers for their very helpful com-ments.
This work was supported in part by an Ap-plied Research Grant (Project no.
9667115) fromCity University of Hong Kong.ReferencesIsaac I. Bejar, Ren?e R. Lawless, Mary E. Morley,Michael E. Wagner, Randy E. Bennett, and JavierRevuelta.
2003.
A Feasibility Study of On-the-FlyItem Generation in Adaptive Testing.
The Journalof Technology, Learning, and Assessment, 2(3).Eckhard Bick.
2005.
Grammar for Fun: IT-basedGrammar Learning with VISL.
In P. Juel, edi-tor, CALL for the Nordic Languages, pages 49?64.Copenhagen: Samfundslitteratur, Copenhagen Stud-ies in Language.Hong Kong Education Bureau.
2012.Enhancing English Vocabulary Learn-119ing and Teaching at Secondary Level.http://www.edb.gov.hk/vocab learning sec.Chia-Yin Chen, Hsien-Chin Liou, and Jason S. Chang.2006.
FAST: An Automatic Generation System forGrammar Tests.
In Proc.
COLING/ACL InteractivePresentation Sessions.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of Learner En-glish.
In Proc.
8th Workshop on Innovative Use ofNLP for Building Educational Applications.Pierluigi Erbaggio, Sangeetha Gopalakrishnan, SandraHobbs, and Haiyong Liu.
2012.
Enhancing Stu-dent Engagement through Online Authentic Mate-rials.
The International Association for LanguageLearning Technology Journal, 42(2):27?51.Rachele De Felice and Stephen Pulman.
2009.
Au-tomatic Detection of Preposition Errors in LearnerWriting.
CALICO Journal, 26(3):512?528.Jeroen Geertzen, Theodora Alexopoulou, and AnnaKorhonen.
2013.
Automatic Linguistic Annotationof Large Scale L2 Databases: The EF-CambridgeOpen Language Database (EFCAMDAT).
In Proc.31st Second Language Research Forum (SLRF).Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-chai Supnithi, and Hitoshi Isahara.
2003.
Auto-matic Error Detection in the Japanese Learners?
En-glish Spoken Data.
In Proc.
ACL.Ruth E. Larimer and Leigh Schleicher.
1999.
NewWays in Using Authentic Materials in the Class-room.
Teachers of English to Speakers of OtherLanguages, Inc., Alexandria, VA.John Lee and Stephanie Seneff.
2007.
Automatic Gen-eration of Cloze Items for Prepositions.
In Proc.
In-terspeech.John Lee, Chak Yan Yeung, Amir Zeldes, MarcReznicek, Anke L?udeling, and Jonathan Webster.2015.
CityU Corpus of Essay Drafts of EnglishLanguage Learners: a Corpus of Textual Revisionin Second Language Writing.
Language Resourcesand Evaluation, 49(3):659?683.John Lee, Donald Sturgeon, and Mengqi Luo.
2016.
ACALL System for Learning Preposition Usage.
InProc.
ACL.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP Natural Lan-guage Processing Toolkit.
In Proc.
ACL SystemDemonstrations, pages 55?60.Detmar Meurers, Ramon Ziai, Luiz Amaral, AdrianeBoyd, Aleksandar Dimitrov, Vanessa Metcalf, andNiels Ott.
2010.
Enhancing Authentic Web Pagesfor Language Learners.
In Proc.
Fifth Workshop onInnovative Use of Nlp for Building Educational Ap-plications.Eleni Miltsakaki and Audrey Troutt.
2008.
Real TimeWeb Text Classification and Analysis of ReadingDifficulty.
In Proc.
Third Workshop on InnovativeUse of NLP for Building Educational Applications.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 Shared Taskon Grammatical Error Correction.
In Proc.
8th Con-ference on Computational Natural Language Learn-ing: Shared Task, pages 1?14.Emily Pitler and Ani Nenkova.
2008.
Revisiting Read-ability: a Unified Framework for Predicting TextQuality.
In Proc.
EMNLP.Samuel Reese, Gemma Boleda, Montse Cuadros, Llu?
?sPadr?o, and German Rigau.
2010.
Wikicorpus: AWord-Sense Disambiguated Multilingual WikipediaCorpus.
In Proc.
LREC.Adam Skory and Maxine Eskenazi.
2010.
Predict-ing Cloze Task Quality for Vocabulary Training.
InProc.
NAACL HLT 2010 Fifth Workshop on Innova-tive Use of NLP for Building Educational Applica-tions.Simon Smith, P. V. S. Avinesh, and Adam Kilgar-riff.
2010.
Gap-fill Tests for Language Learners:Corpus-Driven Item Generation.
In Proc.
8th Inter-national Conference on Natural Language Process-ing (ICON).Joel Tetreault and Martin Chodorow.
2008.
The Upsand Downs of Preposition Error Detection in ESLWriting.
In Proc.
COLING.Peter Watcyn-Jones and Jake Allsop.
2000.
Test YourPrepositions.
Penguin Books Ltd.Jean Yates.
2010.
The Ins and Outs of Prepositions.Hauppauge, New York : Barron?s.Torsten Zesch and Oren Melamud.
2014.
Auto-matic Generation of Challenging Distractors UsingContext-Sensitive Inference Rules.
In Proc.
Work-shop on Innovative Use of NLP for Building Educa-tional Applications (BEA).120
