Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1334?1338,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsGPU-Friendly Local Regression for Voice ConversionTaylor Berg-Kirkpatrick Dan KleinComputer Science DivisionUniversity of California, Berkeley{tberg,klein}@cs.berkeley.eduAbstractVoice conversion is the task of transforminga source speaker?s voice so that it sounds likea target speaker?s voice.
We present a GPU-friendly local regression model for voice con-version that is capable of converting speechin real-time and achieves state-of-the-art ac-curacy on this task.
Our model uses a newapproximation for computing local regressioncoefficients that is explicitly designed to pre-serve memory locality.
As a result, our infer-ence procedure is amenable to efficient imple-mentation on the GPU.
Our approach is morethan 10X faster than a highly optimized CPU-based implementation, and is able to convertspeech 2.7X faster than real-time.1 IntroductionVoice conversion is the task of transforming an ut-terance from a source speaker?s voice into a targetspeaker?s voice.
The primary setup in recent workhas been to learn this transformation from a paral-lel corpus consisting of recordings of the same se-quence of sentences read by both source and tar-get speakers (Stylianou et al, 1998).
The convertedspeech is evaluated by how well its spectral proper-ties match those of the target voice.While various models have been proposed(Stylianou et al, 1998; Toda et al, 2007; Toda et al,2005), the most accurate ones are non-parametricbecause the mapping between two voices?
spectracan be highly non-linear (Helander et al, 2012; Popaet al, 2012).
Unfortunately, while non-parametricmethods are accurate, they are also slow ?
currentnon-parametric approaches to voice conversion aretoo compute-intensive for the real-time speed re-quired by many voice conversion applications.
Inthis paper, we begin with the state-of-the-art locallinear regression (LLR) model used by by Popa etal.
(2012) for voice conversion, and present a newGPU-based inference approach that greatly acceler-ates it, to much faster than real-time.LLR, in principle, requires each new model pre-diction to be a function of the entire set of trainingexamples.
In practice, LLR depends most stronglyon nearby points, so a standard CPU implementa-tion will skip distant points, with limited loss of ac-curacy.
A GPU cannot exploit sparsity in the sameway (scan and skip) without suffering from memorybottlenecks, but even a GPU will be relatively slow ifall training points are included in each computation.Our primary algorithmic change is to make use of anew sparsity structure that allows the GPU to skipmajor sections of the training data while still usingdense memory access patterns on the points it doesprocess.
In experiments, this inference technique ismore than 10X faster than a highly-optimized CPU-based implementation, operates almost three timesfaster than real-time, and is only slightly less accu-rate than the CPU-based method.2 Background and ModelMost representations of speech that are useful forspeech processing break the acoustic signal into sep-arate components that represent the sound source(the lungs and vocal folds) and sound filter (the vo-cal tract) portions of the vocal apparatus.
Work onvoice conversion is generally focused on transform-ing the representation of the vocal tract.
We followthis approach and learn a transformation of a mel-cepstral representation of the acoustic signal (Kawa-hara, 2006).We treat the task as a multiple regression prob-lem.
In order to produce the transformed signal, webreak the source signal into a sequence of frames,each of which is a 24-dimensional vector of mel-1334cepstral coefficients.
We denote a single frame ofinput mel-cepstral coefficients as x.
In order toproduce the transformed signal, we simply predictframe-by-frame.
Specifically, for a frame x of theinput we predict a transformed frame y?
as the modeof density p(y|x), which we estimate from trainingdata.
A naive approach would be to use a linearmodel:y = Ax+ Here,  is a Gaussian noise term, and the model isparameterized by the transformation matrix, A. Fornow, we assume training data are already frame-aligned (see Section 4).
Let yibe frame i of thetarget signal in the training data.
Similarly, let xibe the corresponding frame of the source signal inthe training data.
Thus, using this linear model, wewould estimate the transformation as:?A = argminA?i?yi?Axi?2This very simple approach works, to some extent,but, because it cannot capture important non-linearrelationships between x and y, it is far from state-of-the-art.
A more popular approach is to use a Gaus-sian mixture model (GMM) to jointly generate bothsource and target cepstral features (Stylianou et al,1998; Toda et al, 2007; Toda et al, 2005).
Thisapproach essentially learns different linear transfor-mations for different regions of the input space, cap-turing some non-linearity.
However, the GMM in-troduces a new problem: the posterior over the la-tent clusters learned by the GMM can be highlypeaked (Popa et al, 2012) and as a result distortion isintroduced by discontinuities at cluster boundaries.Thus, we adopt neither the simple linear approachnor the GMM.
We instead the state-of-the-art fullynon-parametric approach introduced by Popa et al(2012).
This method, described in the next section,learns transformations that capture non-linearity butvary smoothly as the input changes.2.1 Local RegressionLike Popa et al (2012), we use local linear regres-sion (LLR) (Cleveland, 1979) to estimate p(y|x).LLR is a non-parametric method that estimatesp(y|x) as a linear transformation that varies slowlywith the input x.
Specifically, p(y|x) is estimated asfollows:y = A(x) ?
x+ ?A(x) = argminA?i[w(x, xi) ?
?yi?Axi?2]The transformation?A is a function of x, and is com-puted by solving a weighted least squares problemthat depends on x. w is a kernel function that mea-sures similarity between the current input, x, andeach of the source training frames, xi.
We use aGaussian kernel:w(x, xi) = exp(??x?
xi?22?2)Intuitively, for each input frame x we solve a sepa-rate least squares regression where each training da-tum is weighted by its similarity to the input.
As theinput varies, so will the weighting, and thus so willthe linear transformation.3 InferenceExact inference using LLR is too computationallyexpensive for most applications since it means solv-ing a least squares problem over the entire trainingset for each input frame x.
A common approach isto define a neighborhood function that, for each in-put frame x, selects K training frames xithat aremost relevant to x.
Then,?A(x) is computed by onlysolving the least squares problem over this neigh-borhood.
This approach can work well in practicesince the support for each local least squares prob-lem is relatively sparse.
By choosing the right neigh-borhood function, work can be skipped without sub-stantially impacting learning.3.1 Inference on the CPUThe standard approach when using a CPU is to letthe neighborhood function pick out the indices of theK training frames xithat maximize w(x, xi).
Welet this particular neighborhood function be calledg(x), depicted in Figure 1.
Using this approach, forinput frame x,?A(x) has the following closed formexpression:?A(x) = H(x)>W (x)G(x)(G(x)>W (x)G(x))?11335G(x) is a matrix formed by appending the train-ing source vectors in the neighborhood of x. Morespecifically, G(x) is a matrix that has the vectors x>is.t.
i ?
g(x) as its rows.
Similarly, H(x) is a matrixthat has the vectors y>is.t.
i ?
g(x) as its rows (inthe corresponding order).
W (x) is a matrix that hasthe weights w(x, xi) s.t.
i ?
g(x) along its diagonal(also in the corresponding order).This approach is much faster than the exhaustivemethod, but at typical audio sampling frequencies,inferring the transformation of the signal for an en-tire sentence can take over 30 seconds on a mod-ern CPU, which is too slow for real-time conversion.In order to transform the signal for a new sentence,?A(x) must be computed for each frame.
This meansthat for each frame x, the distance to all trainingsource frames must be computed, neighborhood ma-trices G(x) and H(x) must be formed, followed byseveral matrix multiplies, an LU decomposition, anda triangular solve operation.3.2 Inference on the GPUThe computation of?A(x) for a block of multiple in-put frames can be done in parallel if a fixed amountof lag is tolerated in the conversion process.
Theparallel computation of large number of small densematrix operations (multiply, LU decomposition, tri-angular solve) is a perfect fit for implementationa GPU, which can achieve vastly more throughputthan modern CPUs can.
However, using the CPU?sneighborhood structure on the GPU has a cripplingbottleneck.
The extraction of the neighborhood ma-tricesG(x) andH(x) from the training data requiresa large number of memory accesses that are effec-tively random.
The indices of the closest K train-ing source vectors to an input x are generally non-contiguous.
As a result, the K vectors in each of theneighborhood matrices must be copied with sepa-rate memory accesses, and since random access timeon modern GPUs is very slow, extraction becomesa bottleneck.
In initial experiments, we found thatwhen this approach is implemented on a GPU it iseven slower than the CPU-based implementation.In contrast, memory bandwidth is extremely highon modern GPUs.
Thus, if it were possible to orderthe training vectors xiand yiin GPU memory suchthat neighborhood matrices G(x) and H(x) werecomposed of contiguous blocks of training vectors,x1x2surrogateneighborhoodg?
(x)g(x)neighborhoodinput framexfirst principaldirectionGPUCPUFigure 1: Depiction of standard neighborhood function g(x)used for local regression on the CPU and surrogate neighbor-hood function g?
(x) used for inference on the GPU, plotted fortwo-dimensional input data.extraction on the GPU could be made very effi-cient.
Unfortunately, with the current definition ofthe neighborhood function, g, such an ordering doesnot exist.
Therefore, we define a new GPU-friendlyneighborhood function, g?, for which an ordering thatpermits contiguous extraction does exist.Let u(x) be the projection of source vector x ontothe first principal component resulting from runningPCA on the source side of the training data.
Now,we define g?
as follows: g?
(x) is the set of K indicesfor which |u(x)?
u(xi)| is the smallest, or, in otherwords, the set of training indices with source projec-tions closest to the projection of the input frame.
Byordering the training data by their projection ontothe first principal component, we can ensure thatG(x) and H(x) are contiguous in memory.The hope is that this approach yields substantialspeedups on the GPU without negatively impactingthe learned transformation (see Section 4).
Figure1 depicts the difference between the CPU neighbor-hood function g and the GPU function g?.
Intuitively,when most of the variance in the training data occursalong the first principal direction, the CPU and GPUneighborhood functions may be similar since thedistance between projections is a good proxy for dis-tance in the original space.
The weighting function,w, is still computed in the original space, so distanttraining vectors that are inadvertently included in theneighborhood will be severely down-weighted.
Thepotential pitfall is that for a fixed neighborhood size,g?
may be less efficient at collecting training pointsthat are relevant to the input.1336SystemScottish Male to US Female US Female to Scottish MaleCD Time Frac.
RT CD Time Frac.
RTGMM 7.05 1.1 3.7X 7.01 0.7 3.9XCPU LLR 5.99 12.2 0.3X 6.51 8.75 0.3XGPU LLR 5.98 1.4 2.7X 6.55 0.9 2.9XTable 1: Voice conversion results for the GMM baseline system, CPU-based local linear regression baseline system, and the GPU-based local linear regression method.
The cepstral distortion (CD), average inference time per sentence in seconds (Time), andfraction of real-time (Frac.
RT) are shown.
Smaller cepstral distortion corresponds to more accurate transformations and fractionsof real-time that exceed one imply faster than real-time operation.4 ExperimentsWe run a series of experiments to determine whetherour GPU-based inference technique offers speeds-ups and at what cost to accuracy.Baselines We compare our LLR-based conversionsystem that performs inference on the GPU (usingthe GPU-friendly neighborhood function) with twodifferent baseline systems.
The first baseline sys-tem also uses LLR, but performs inference on theCPU using the standard neighborhood function.
Thesecond baseline is the GMM model of Toda et al(2007), which is known to be fast and is widely usedin practice.
The size, K, of both CPU and GPUneighborhoods was set on a development data to thesmallest value that did not show degraded perfor-mance compared to exact local regression.Implementation We implemented our GPU-based LLR technique using the CUDA API(Nickolls et al, 2008), and the CUBLAS APIwhich contains bindings for GPU BLAS routines.We ran the system using an NVIDIA Tesla K40cGPU.
We built a multi-threaded implementationof CPU-based inference for local regression usingcalls to CPU BLAS routines, and ran this system ona 4.4GHz 4-core Intel CPU.Data We train and test on a portion of the CMUArctic database.
The training data consists of 70sentences spoken by both a US female speaker anda Scottish male speaker.
The testing data consists of20 sentences spoken by the same two speakers.
Wegive results for converting in both directions, fromthe female voice to the male voice, and from themale voice to the female voice.Frame Alignment Since the source and targetspeakers speak at slightly different rates, our train-ing data consist of different numbers of frames foreach training sentence.
We use dynamic time warp-ing to induce the frame alignment.
Specifically, wefind the minimum cost monotonic alignment fromsource frames into target frames where the cost ofeach alignment edge is the L2 distance between thecorresponding vectors.
We use a distortion limit of2, and a linear distortion cost.Analysis and Synthesis We use the CMU imple-mentation of the STRAIGHT analysis and synthe-sis methods introduced by Kawahara (2006).
Thisis the same method used many state-of-the-art voiceconversion systems, included our GMM baseline ofToda et al (2007).
We transform the top 24 cep-stral coefficients using our system, but process thepower coefficient and fundamental frequency sep-arately, using simple transformations for the lattertwo components.Evaluation In order to evaluate the accuracy ofour model we measure the cepstral distortion be-tween the predicted ceptstral frames y?
and the actualcepstral frames for the target voice y.
The cepstraldistortion is calculated as follows:distortion(y?, y) ?
?y?
?
y?We using dynamic time warping to align the pre-dicted frame sequence to the target frame sequence.4.1 ResultsThe results of our experiments are displayed in Ta-ble 1.
For the Scottish male to US female and the USfemale to Scottish male transformations the systemsthat use local regression outperform the parametric1337GMM baseline in terms of average cepstral distor-tion.
The GMM baseline, is however, the fastestof the compared systems.
For both experiments,it is able to produce transformations substantiallyfaster than real-time.
The CPU-based local regres-sion baseline achieves the best overall cepstral dis-tortion, but is also the slowest method.
In both ex-periments, it operates at 0.3X real-time speed.
TheGPU-based local regression method performs onlyslightly worse overall than the exact method in termsof cepstral distortion, yet in both experiments it op-erates substantially faster than real-time, nearly asfast as the parametric baseline.5 ConclusionWe have demonstrated a method for substantiallyspeeding-up inference using a non-parametric es-timator for spectral voice conversion.
Relatedapproaches may prove useful for making non-parametric estimators more efficient in other areasof speech and language processing.AcknowledgementsThis work was partially supported by BBN un-der DARPA contract HR0011-12-C-0014 and by anNSF fellowship for the first author.
Thanks to theanonymous reviewers for their insightful comments.We further gratefully acknowledge a hardware do-nation by NVIDIA Corporation.ReferencesWilliam S Cleveland.
1979.
Robust locally weightedregression and smoothing scatterplots.
Journal of theAmerican statistical association, 74(368):829?836.E.
Helander, H. Silen, T. Virtanen, and M. Gabbouj.2012.
Voice conversion using dynamic kernel partialleast squares regression.
IEEE transactions on audio,speech, and language processing, 20(3):806?817.H.
Kawahara.
2006.
Straight, exploitation of the otheraspect of vocoder: Perceptually isomorphic decompo-sition of speech sounds.
Acoustical science and tech-nology, 27(6):349?353.John Nickolls, Ian Buck, Michael Garland, and KevinSkadron.
2008.
Scalable parallel programming withcuda.
Queue, 6(2):40?53.Victor Popa, Hanna Silen, Jani Nurminen, and MoncefGabbouj.
2012.
Local linear transformation for voiceconversion.
In Acoustics, Speech and Signal Process-ing (ICASSP), 2012 IEEE International Conferenceon, pages 4517?4520.
IEEE.Yannis Stylianou, Olivier Capp?, and Eric Moulines.1998.
Continuous probabilistic transform for voiceconversion.
Speech and Audio Processing, IEEETransactions on, 6(2):131?142.Tomoki Toda, Alan W Black, and Keiichi Tokuda.
2005.Spectral conversion based on maximum likelihood es-timation considering global variance of converted pa-rameter.
In ICASSP (1), pages 9?12.T.
Toda, A.W.
Black, and K. Tokuda.
2007.
Voice con-version based on maximum-likelihood estimation ofspectral parameter trajectory.
Audio, Speech, and Lan-guage Processing, IEEE Transactions on, 15(8):2222?2235.1338
