Use of Dependency Tree Structures for the MicrocontextExtractionMartin HOLUBDepartment of Software EngineeringMFF UK, Malostransk6 n~irn.
25CZ-118 00 Praha, Czech Republicholub @ ksi.ms.mff.cuni.czAlena BOI-IMOVAInstitute of Formal and Applied LinguisticsMFF UK, Malostransk6 nAm.
25CZ-118 00 Praha, Czech Republicbohmova @ufal.ms.mff.cuni.czAbstractIn several recent years, naturallanguage processing (NLP) has broughtsome very interesting and promisingoutcomes.
In the field of informationretrieval (IR), however, these significantadvances have not been applied in anoptimal way yet.Author argues that traditional IRmethods, i.e.
methods based on dealingwith individual terms without consideringtheir relations, can be overcome usingNLP procedures.
The reason for thisexpectation is the fact that NLP methodsare able to detect the relations amongterms in sentences and that theinformation obtained can be stored andused for searching.
Features of wordsenses and the significance of wordcontexts are analysed and possibility ofsearching based on word senses instead ofmere words is examined.The core part of the paper focuses onanalysing Czech sentences and extractingthe  context relations among words fromthem.
In order to make use oflemmatisation and morphological andsyntactic tagging of Czech texts, authorproposes a method for construction ofdependency word microcontexts fullyautomatically extracted from texts, andseveral ways how to exploit themicrocontexts for the sake of increasingretrieval performance.1 IntroductionEmpirical methods in natural languageprocessing (NLP) employ learning techniques toautomatically extract linguistic knowledge fromnat~al language corpora; for an overview of thisfield see (Bfill and Mooney 1997).
This paperwants to show their usefulness in the field ofinformation retrieval (IR).
As the effects and thecontribution of this discipline to IR has not beenwell examined and evaluated yet, various uses ofNLP techniques in IR are only marginallymentioned in well known monographs publishedin last ten years, e.g.
(Salton 1989), (Frakes andBaeza-Yates 1992), (Korfhage 1997).A textual IR system stores a collection ofdocuments and special data structures foreffective searching.
A textual document is asequence of terms.
When analysing the contentof a document, terms are the basic processedunits - -  usually they are words of naturallanguage.
When retrieving, the IR system returnsdocuments presumed to be of interest to the userin response to a query.
The user's query is aformal statement of user's information eed.
Thedocuments that are interesting for the user(relative to the put query) are relevant; the othersare non-relevant.
The effectiveness of IRsystems is usually measured in terms ofprecision, the percentage of retrieved ocumentsthat are relevant, and recall, the percentage ofrelevant documents hat are retrieved.The starting point of our consideration on IRwas a critique of word-based retrievaltechniques.
Traditional IR systems treat thequery as a pattern of words to be matched bydocuments.
Unfortunately, the effectiveness ofthese word-matching systems is mostly poor23because the system retrieves only the documentsthat contain words that occttr also in the query.However, in fact, the user &Des not look for thewords used in  the query.
The user desires thesense of the words and wants to retrieve thedocuments containing word,,; having the samesense.
In contrast to the word-based approach, asense-based IR system treats the query as apattern of the required sense.
In order to matchthis sense by the sense of words in documents,the senses of ambiguous words must bedetermined.
Therefore a good word sensedisambiguation is necessary ha a sense-based IRsystem.Ambiguity and synonymity of words is aproperty of natural language causing a veryserious problem in IR.
Both ambiguous wordsand synonyms depress the effectiveness of word-matching systems.
The direct effect of polysemyon word-matching systems is to decreaseprecision (e.g., queries about financial banksretrieve documents about rivers).
Synonymitydecreases recall.
If one sense is expressed bydifferent synonyms in different documents, theword-matching system will retrieve all thedocuments only if all the synonyms are given inthe query.
Unfortunately, polysemy has anothernegative effect: polysemy also prevents theeffective use of thesauri.
Consequently, thesauricannot be directly used to eliminate the problemwith synonyms.In our opinion, if a retrieval system is notable to identify homonyms and synonyms and todiscriminate their senses, ambiguity andsynonymity will remain one of the main factorscausing 1) low recall, 2) low precision, and 3)the known and inevitable fact that recall andprecision are inversely related.
There are someevidences that lexical context analysis could be agood way how to eliminate or at least decreasethese difficulties m see below.How to take the step from words towardssenses?
Since an application of word contexts isthe only possibility to estimate the sense ofwords, the way of dealing with word contexts isa central problem in sense-based retrieval.Knowing word contexts we can determine themeasure of collocating, i.e.
the extent o which apair of words collocates.
The knowledge ofcollocations can be used in IR for severalpurposes: making up contextual representationsof words, resolving word ambiguity, estimatingsemantic word similarity, tuning the user's queryin interaction with the user and quantifying thesignificance of words for retrieval according toentropy of their contexts.Section 2 expresses our motivation: theinvestigation of word contexts helps us todevelop an efficient IR system.
Next section isdevoted to analysing Czech texts and suggests aconstruction of dependency microcontextstructures making use of the tree structureautomatically created in the process of PragueDependency Treebank annotation.
Further partfocuses on applications of contextual knowledgein IR and refers to the project working on anexperimental IR textual database.
Finally wesummarise the results of this study.2 Significance of word contextsWord senses are not something iven a priori.Humans create word senses in the process ofthinking and using language.
Thinking formslanguage and language influences thinking.
It isimpossible to separate them.
Word senses areproducts of their interaction.
In our opinion, theeffort to represent word senses as fixed elementsin a textual information system is amethodological mistake.Many researchers consider the sense of aword as an average of its linguistic uses.
Then,the investigation of sense distinctions i based onthe knowledge of contexts in which a wordappears in a text corpus.
Sense representationsare computed as groups of similar contexts.
Forinstance, Schiitze (1998) creates sense clustersfrom a corpus rather than relying on a pre-established sense list.
He makes up the clustersas the sets of contextually similar occurrences ofan ambiguous word.
These clusters are theninterpreted as senses.According to how wide vicinity of the targetword we include into the context we can speakabout the local context and the topical context.The local or "micro"context is generallyconsidered to be some small window of wordssurrounding a word occurrence in a text, from afew words of context o the entire sentence inwhich the target word appears.
The topicalcontext includes ubstantive words that co-occur24with a given word, usually within a window ofseveral sentences.
In contrast with the topicalcontext, the microcontext may includeinformation on word order, distance,grammatical inflections and syntactic strncture.In one study, Miller and Charles (1991) foundevidence that human subjects determine thesemandc similarity of words from the similarityof the contexts they are used in.
Theysurnmarised this result in the so-called strongcontextual hypothesis:Two words are semantically similar to theextent hat their contextual representations aresimilar.The contextual representation of a word hasbeen defined as a characterisation of thelinguistic context in which a word appears.Leacock, Towell and Voorhees (1996)demonstrated that contextual representationsconsisting of both local and topical componentsare effective for resolving word senses and canbe automatically extracted from sample texts.
Nodoubt information from both microcontext andtopical context contributes to sense selection, butthe relative roles and importance of informationfrom different contexts, and their interrelations,are not well understood yet.Not only computers but even humans learn,realise, get to know and understand the meaningsof words from the contexts in which they meetthem.
The investigation of word contexts is themost important, essential, unique andindispensable means of understanding the senseof words and texts.3 Analysing Czech textsLinguistic analysis of an input Czech textconsists of a sequence of procedures depicted inFigure 1.
The input is a Czech sentence and theresults of the analysis are the two targetstructures: the dependency microcontextstructure (DMCS) which we use for themicrocontext extraction and thetectogrammatical tree structure (TGTS) whichrepresents he underlying syntactic structure ofthe sentence.
As the main intention of this paperis to describe the DMCS, building of the TGTSis distinguished by dashed line in Figure 1; wemention it here only for completeness and forcomparison with the DMCS.InputCzechsentence~keniza~n I~  / and lexical l ~ "o~aw.nt JATS ~g.
.
.
.
.
.
.
.
.
.
.
.
.
.
-.
':' /l .... t\[ TGTS attribute L Context\[ assignment transformationsn UTGTS DMCSFigure 1: The sequence of procedures in the analysis of a Czech sentence.25Key algorithms used in the process of theanalysis are based on empirical methods and onprevious statistical processing of training data,i.e.
natural " language corpora providingstatistically significant sample of correctdecisions.
Consequently, the ability of theseprocedures to provide a correct output has astochastic character.
These procedures weredeveloped during the past years in the process ofthe Czech National Corpus and PragueDependency Treebank creation.
For a detaileddescriptions see Haji6 (1998), Hladkfi (2000) andCollins, Haji6, Ram~haw, Tillmann (1999).As shown in Figure 1, the first procedure istokenizafion.
The output of tokenization is thetext divided into lexical atoms or tokens, i.e.words, numbers, punctuation marks and specialgraphical symbols.
At the same time theboundaries of sentences and paragraphs aredetermined.The following procedure, i.e.
morphologicaltagging and lexical disambiguation, works in twostages.
The first is the morphological nalysis,which assigns each word its lemma, i.e.
its basicword form, and its morphological tag.
Since weoften meet lexical ambiguity (i.e.
it is notpossible to determine the lemma and the taguniquely without the knowledge of the wordcontext), the morphological analyser oftenprovides several alternatives.
In the secondstage, the result of the analysis is further used asan input for the lexical disambiguation assigninga given word form its unique lemma andmorphological tag.The next procedures work with syntactic treestn~ctures.
This process is described in thefollowing subsection.3.1 Syntactical analysisThe first step of the syntactic tagging consistsin the building of the anatytic tree structure(ATS) representing the surface syntacticdependency relations in the sentence.
We use thestatistical Collins's parser to create the stnlctureof the tree and then a statistical procedure toassign words their syntactic functions.
Twoexamples of the ATS are given in figures2and 3.lami.~"Bylo zfejmE 3e stav se pHlig rychle nezm#nl."(Lit.
: It-was clear that the-state too fast will-not-change.
E: It was clear, that the state will notchange too fast.
)Figure 2: An example of an ATS.The automatically created ATS is a labelledoriented acyclic graph with a single root(dependency tree).
In the ATS every word formand punctuation mark is explicitly represented asa node of the tree.
Each node of the tree isannotated by a set of attribute-value pairs.
Oneof the attributes is the analytic function thatexpresses the syntactic function of the word.
Thenumber of nodes in the graph is equal to thenumber of word form tokens in the sentence plusthat of punctuation signs and a symbol for thesentence as such (the root of the tree).
The graphedges represent surface syntactic relations withinthe sentence as defined in B6movfi et al(1997).The created ATS is further transformed eitherto the TGTS or to the DMCS.
In the PragueDependency Treebank annotation, thetransduction of the ATS to the TGTS isperformed (see BShmovfi and I-Iaji~ovfi 1999).For the sake of the construction of word contexts,we use the lemmas of word forms, their part ofspeech, their analytic function and we adapted thealgorithms aiming towards the TGTS to build asimilar structure, DMCS.
Since, in comparisonwith the ATS, in both the TGTS and the DMCSonly autosemantic words have nodes of their own,the first stage of this transformation (i.e.
thepruning of the tree structure) is common.26KClO:  n  red ,e AuxK elekne "t i s | c  t automobilu zikonakorun nov~ho'Tfdo dye ~ovatdv~asfc /z run  do ovel azgomob//g ne/e~ne s /e b~nz/n by/zmfnouza~na D'ochuzaka~n"OiL: Who warss to-hvest two lamchedthousard crowns m new c~, l-c-docs-nct-get-ffigttemd that pearl was by-change el-law a-little n~de-rmte.expmsive.
E:'Ihcse who ve~toinvest twohunckedthousand crowns in anew car, donct getegl'mnedthatpeadwas made alittlemae exF~iveby ~change cflaw.
)Figure 3: An example of an ATS.3.2 F rom ATS towards  DMCSThe transduction of the ATS to the DMCSconsists of the four procedures:1.
Pruning of the tree structure, i.e.elimination of the auxiliary nodes andjoining the complex word forms into onenode.2.
Transformation of the structures ofcoordinations and appositions.3.
Transformation of the nominalpredicates.4.
Transformation f the complements.The first step of the transformation of theATS to the respective DMCS is deletion of theauxiliary nodes.
By the auxiliary nodes weunderstand nodes for prepositions, subordinateconjunctions, rhematizers (including negation)and punctuation.
In case the deleted node is not aleaf of the tree, we reorganise the tree.
For the IRpurposes the auxiliary verbs do not carry anysense, so the analytical verb forms are treated asone single node with the lernma of the mainverb.
The purpose of the next three procedures ito obtain the context relations among words fromthe sentence, so we call them contexttransformations.The constructions of coordination andapposition are represented by a special node(usually the node of the coordinating conjunctionor other expression) that is the governor of thecoordinated subtrees and their commoncomplementation in the ATS.
The heads of thecoordinated subtrees are marked by a specialfeature.
In case of coordinated attributes, thetransformation algorithm deletes the specialnode, which means that a separate microcontext(X, Atr, Y) is extracted for each member ofcoordination.
The same procedure is used foradverbials, objects and subjects.
If two clausesoccur coordinated, the special node remains inthe structure, as the clauses are handledseparately.27z~ejm~staySb, V)zmi~nitFigure 4: The DMCS of the sentence fromFigure 2.Probably the main difference from thesyntactic analysis is the way we are dealing withthe nominal predicate.
We consider the nominalpredicate to act as a normal predicate, though notexpressed by a verb.
This way of understandinga predicate is very close to predicate logic, wherethe sentence "The grass is green" is considered toexpress aformula such as "green(grass)".In the ATS the complement (wordsyntactically depending both on the verb and thenoun) is placed as a daughter node of the nounand marked by the analytical function of Atv.
Inthe DMCS this node is copied and its analyticalfunction is changed to Attr for the occurrence ofthe daughter of the noun and Adv for the newtoken of the daughter of the governing verb.As we cannot go into details here, weillustrate the DMCS by two examples given infigures 4 and 5.
The nodes of the trees representsemantically significant words.
The edges of thegraphs are labelled by so called dependencytypes (see below).3.3 Extraction of microcontexts from theDMCSThere are I0 parts of speech in Czech and 18types of analytic function in ATSs.
However, wewill consider only four parts of speech, namelynouns (N), adjectives (A), verbs (V) and adverbs(D), and four types of analytic function, namelysubject (Sb), object (Obj), adverbial (Adv) andattribute (Attr), because only these aresignificant for the purpose of retrieval.The construction of the dependencymicrocontext is based on the identification ofsignificant dependency relationships (SDRs) inthe sentence.
An SDR consists of two words anda dependency t pe.
An SDR is a triple \[wl, DT,w2\], where wl is a head word (lexical unit), DTis a dependency t pe and w2 is a depending word(lexical unit).
A dependency t pe is a triple (P1,AF, P2), where Pi is the part of speech of thehead word, AF is an analytic function and P2 isthe part of speech of the depending word.For example, (A, Adv, D) is a dependencytype expressing the relationship between wordsin expression "very large" where "very" is adepending adverb and "large" is a head adjective.\[large, (A, Adv, D), very\] is an example of anSDR.Considering 4 significant parts of speech and4 analytic functions, we have 64 (= 4x4x4)possible distinct dependency types.
In Czech,however, only 28 of them really occur.
Thus, wehave 28 distinct dependency types shown inTable 1.
Table 2 surnmarises the number ofdependency t pes for each part of speech.
Thedependency t pes marked by an asterisk are notthe usual syntactic relations in Czech, they wereadded on account of the transformation f thenominal predicate.The number of SDRs extracted from onesentence is always only a little smaller than thenumber of significant, autosemantic words in thesentence, because almost all these words aredepending on another word and make an SDRwith it.Now we define the dependency wordmicrocontext (DMC).
A DMC of a given word wis a list of its microcontext elements (MCEs).
AnMCE is a pair consisting of a word and adependency type.
If a word w occurs in asentence and forms an SDR with another wordwl, i.e.
if there is an SDR \[w, DT, wd or \[wl,DT', w\], then w~ and the dependency t pe DT orDT', respectively, constitute a mierocontextelement \[DT, wd or \[wl, DT'\], respectively, ofthe word w. The first case implies that w is ahead word in the SDR and in the second case theword w is a dependant.Thus, each SDR \[wl, DT, w2\] in a textproduces two MCEs: \[w~, DT\] is an dement ofthe context of Wz and \[DT, w2\] is an element ofthe context of w~.28In the following Table 3 we exemplify themicrocontexts extracted from the sentences usedin the examples above.Dependency types(N, Atr, N) (V, Sb, N) (V, Obj, N)(N, Atr, A) (V, Sb, V) (V, Obj, V)(N, Atr, V) (V, Sb, A) (A, Obj, A)(N, Adv, N)* (N, Sb, N)* (D, Obj, N)(N, Adv, V)" (N, Sb, A)* (A, Adv, A)(N, Adv, D)" (IN, Sb, V)* (A, Adv, D)(V, Adv, N) (A, Sb, N)* (A, Adv, N)*(V, Adv, V) (A, Sb, A)* (A, Adv, V)*(V, Adv, D) (A, Sb, V)* (D, Adv, D)(D, Adv, N)Table 1: Dependency t pes.Number of dependency t pesas governing as dependingN 9 10A 8 6V 8 8D 3 4Table 2: Number of dependency t pes for eachpart of speech.4 Applications4.1 Contextual  knowledge in IRAs we have already mentioned, theknowledge of word contexts can be used forresolving word ambiguity.
Word sensedisambiguation is a central problem in NLP.
Itstask is to assign sense labels to occurrences of anambiguous word.
Researchers dealing with WSDmethods often inspect also the way it affectsretrieval performance if used in a retrievalmodel.
Krovetz and Croft (1992) demonstratedthat WSD can improve text retrievalperformance.
Later, Schiitze and Pedersen(1995) found a noticeable improvement inprecision using sense-based retrieval and wordsense discrimination.
Towell and Voorhees(1998) showed that, given accurate WSD, thelexical relations encoded in lexicons such asWordNet can be exploited to improve theeffectiveness of IR systems.Schiitze (1998) introduced an interestingmethod: word sense discrimination.
Thistechnique is easier than full disambiguation sinceit only determines which occurrences of a givenword have the same meaning and not what the?
meaning actually is.
Moreover, while otherdisambiguation algorithms employ varioussources of information, this method ispenses ofan outside source of knowledge for definingsenses.
For many problems in informationaccess, it is sufficient to solve the discriminationproblem only.
Schiitze and Pedersen (1995)measured ocument-query similarity based onword senses rather on words and achieved aconsiderable improvement in ranking relevantdocuments.
No references to externally definedsenses are necessary for measurement ofsimilarity.4.2 Using microcontextsIn this subsection we give several ideas howto employ the microcontexts for improving theretrieval performance.
Their significance and theextent of their usefulness is to be verifiedexperimentally.
For more details refer to Holub(2000).In the literature, we can meet differentdefinitions of collocation (cf.
Ide and Vdronis,1998).
Following Yarowsky (1993), whoexplicitly addresses the use of collocations in theWSD work, we adopt his definition, adapted toour purpose: A collocation is a co-occurrence oftwo words in a defined relation.
Dependencymicrocontexts and collocations can be treated asmutually equivalent concepts in the sense thatcollocations can be derived from the knowledgeof microcontexts and vice versa.
In order toseparate significant collocations from word pairswhich occurred merely by a coincidence, we cancompute the measure of collocating of a wordand an MCE as the mutual information of theprobability of their occurrence.We also use the knowledge of collocationsfor computing the context similarity measure oftwo words.
Assuming the "strong contextualhypothesis", the context similarity of wordsimplies their semantic similarity, too.Consequently, we can estimate the semanticsimilarity of words.29Ileknout-seinvestovat oh j, v) zdra~itW, Obj, N) ~koruna~uto :mob i l~,atr, A)benzinW, ~, h3trochuFigure 5: The DMCS of the sentence from Figure 3.Word Extracted MCEs SDR used for derivation~ejm~, \[(A, Sb, V), zm~niq \[zfejm2~, (A Sb, V), zm6nit\]zm~nit \[(V, Sb, N), stay\] \[zm~nit, (V, Sb, N), stay\]\[(V, Adv, D), rychle\] \[zm~nit, (V, Adv, D), rychle\]\[~ejm~, (A, Sb, V)\] \[zi'ejm~, (A, Sb, V), zm6nit\]stay \[zm~nit, (V, Sb, N)\] \[zm~nit, (V, Sb, N), stav\]rychle \[zm~nit, (V, Adv, D)\] \[zm~nit, (V, Adv, D), rychle\]p~ili~ \[rychle, (D, Adv, D)\] \[rychle, (D, Adv, D), pfili~\]leknout-se \[(V, Sb, V), investovat\] \[leknout-se, (V Sb, V), investovat\]\[(V, Sb, V), zdra~it\] \[leknout-se, (V Sb, V), zdra~it\]investovat \[leknout-se, (V, Sb, V)\] \[leknout-se, (V Sb, V), investovat\]\[(V, Obj, N), koruna\] \[investovat, (V Obj, N), koruna\]\[(V, Adv, N), automobil\] \[investovat, (V Adv, N), automobil\]koruna \[investovat, (V Obj, 1?
)\] \[investovat, (V Obj, N),.koruna\]automobil \[investovat, (V, Adv, N)\] \[investovat, (V Adv, N), automobil\]\[(N, Atr, A), nov~\] \[automobil, (N, Atr, A), nov~\]nov~ \[automobil, (N, Atx, A)\] \[automobil, (N, Atr, A), nov~\]zdra~.it \[leknout-se, (V Sb, V)\] \[leknout-se, (V Sb, V), zdra~it\]\[(V, Sb, N), benzfn\] \[zdr'~.it, (V, Sb, iN), benzfm\]\[(V, Adv, N), zm~na\] \[zdra~it, (V, Adv, N), zm~na\]\[(V, Adv, D), trochu\] \[zdra~.it, (V, Adv, D), trochu\]benzin \[zdra~it, (V, Sb, N)\] \[zdra~.it, (V, Sb, N), benzin\]zm6na \[zdra~.it, (V, Adv, N)\] \[zdra~.it, (V, Adv, N), zm~na\]\[(N, Atr, N), z~kon\] \[zm~na, (N, Atr, N), z~kon\]zfd~:on \[zm6na, (hi, Air, N)\] \[zm~na, (N, Air, N), zfikon\]trochu \[zdra~.it, (V, Adv, D)\] \[zdra~..it, (V Adv, D), trochu\]Table 3: Dependency microcontexts extracted from the two example sentences (from figures 2 and 3).30Another application of microcontexts consists indetermining the context entropy of the words.Based on the context entropy we can distinguishvague and specific words and give them differentweights for retrieval.In order to improve retrieval performance by amodification of the query, two methods can beemployed.
The first is query expansion replacingwords in the query with a set of words of the samemeaning.
It should ensure a higher recall.
Thesecond is query refinement, i.e.
specifying thesenses of query terms more precisely to avoidambiguity of the query.Asking a query, the user can be offeredcollocations of the terms used in the query.
Thenthe user can decrease the vagueness of the(ambiguous) query terms by the choice ofcollocations that are characteristic for the senserequired.
It seems to be a good way of refining aquery.
The user can be also offered a list of wordsidentified by the system as similar to query terms.Then the user can modify the query or evencompose an aitemative xpression for the samequery sense.
This is a way to decrease or eliminatethe negative influence of synonyms in relevantdocuments.4.3 Experimental databasesIn order to test the methods mentioned abovewe are developing two experimental databases.The first is the database of dependencymicrocontexts extracted from a large text corpus.We should obtain a lot of useful statistical datafrom it.The second experimental database is a textualsystem MATES (MAster of TExt Sources).
Themain purpose of MATES is to serve as a textualdatabase for experiments with various informationretrieval methods.MATES is constructed universally, not only forcertain given retrieval algorithms, and it is adaptedfor the work with Czech language.
Using MATES,it is possible to store both the originals of the inputdocuments and their linguistically pre-processedversions.
MATES supports grouping ofdocuments into collections.
For each collection anindex is built and additional data structures arecreated that enable storing all the additionalinformation about each term, each document andabout their relations.
This additional data can beused by the retrieval module.In the near future, the MATES system shouldenable us to test the methods proposed here andevaluate their contribution to IR as well.5 ConclusionIn the presented study, it is pointed out thatambiguity of language as well as synonymy arethe serious obstacles preventing retrieval based onsense of the user's query.
We describe anapproach employing the lexical contexts toovercome or at least to reduce these difficulties.
Inorder to recapitulate he results of this study and tomake them more clear, we can sum up theessential and most important ideas into thefollowing principles:1.
As to retrieval performance, word-based IRsystems can be superseded by sense-basedones using effective techniques that are able toidentify and compare meanings or senses ofwords.
The structure of the IR system shouldcontain the word context information retrievedfrom texts.2.
The closest core of the word context cannot beextracted based on word order.
Thereforeknowledge of the syntactic relations, whichdoes carry this information, should be used.3.
The dependency tree containing all the surfacedependency relations (ATS) containsinformation not relevant for the contextsextraction (with respect to IR needs), thereforewe reduce this structure and we gather astructure containing only the semanticallysignificant words and 4 main types ofsyntactic dependencies.4.
We present an algorithm for construction ofthe DMCS meeting the previously mentionedrequirements, the DMCS allows for extractionof word microcontexts.
The accuracy of thisprocess depends on the quality of the usedsyntactic parser.5.
The statistical knowledge of lexical contextscan help especially to determine theimportance of lexical units for retrieval and totune the user's query in interaction with the31.user using the knowledge of collocations andword similaritty.
Thus, the database of theretrieved microcontexts can be used forimproving the performance of sense-based IRsystems.Uncertainty and vagueness in the text retrievalcannot be eliminated entirely since they arecaused primarily by the character of thehuman thinking necessarily determining alsothe character of natural language.Our long-term goal is to design an efficient IRsystem using the best methods of natural languageanalysis.
The presented analyses as well asbuilding the experimental textual databaseMATES are likely to be significant steps towardsthat goal.AcknowledgementsThis study has been supported by MSMT (theFRVS grant no 1909).ReferencesBdmovfi, A.; Burfifiovfi, E.; Haji~, J.; Kfirnik, J.;Pajas, P.; Panevovfi, J.; St6pfinek, J.; Ure~ovfi, Z.
(1997) Anotace na analytickd roving- p~iru~kapro anotdtory, Technical Report #4, LJD UFALMFF UK, Prague, Czech Republic.
(in Czech)Bnll, E.; Mooney, R. J.
1997.
An Overview ofEmpirical Natural Language Processing.
In: AIMagazine, Vol.
18, No.
4.B/Shmovfi, A.; Haji~ovfi, E. (1999) How much ofthe underlying syntactic structure can be taggedautomatically?
In: Proceedings of the ATALATreebanks Workshop, Paris.Collins, M.; Haji~, J.; Ramshaw, L.; Tillmann, Ch.
(1999) A Statistical Parser for Czech.
37thAnnual Meeting of the ACL, Proceedings of theconference, pp.
505-512.Frakes, W. B.; Baeza-Yates, R. 1992.
InformationRetrieval.
Data structures and Algorithms.
504pp.
Prentice Hall, Englewood Cliffs, NewJersey.Haji~, J.
(1998) Building a syntacticallyannotated corpus: The Prague DependencyTreebank.
In: Issues of Valency and Meaning.Studies in Honour of Jarmila Panevov~i (ed.
byE.
Haji~ovfi) (pp.
106-132).
Prague: Karolinum.I-Iladkfi, B.
(2000) Morphological Tagging ofCzech language.
HaD Thesis.
MFF UK Prague.Holub, M. (2000) Use of DependencyMicrocontexts in Information Retrieval.Accepted for publication at Sofsem 2000conference.Ide and Vdronis (1998)Korfhage, R. 1997.
Information Storage andRetrieval.
349 pp.
John Wiley & Sons.Krovetz, R.; Croft, W. B.
(1992) Lexicalambiguity and information retrieval.
In: ACMTransactions on Information Systems, 10(2),1992, pp.
115-141.Leacock, C.; Towell, G.; Voorhees, E. M. (1996)Toward building contextual representations ofword senses using statistical models.
In: B.Boguraev and J. Pustejovsky (editors), CorpusProcessing for Lexical Acquisitions.
pp.
97-113,MIT Press.Lin, D. (1998) Extracting Collocations from TextCorpora.
In: Computerm '98.
Proceedings ofthe First Workshop on ComputationalTerminology.
Montreal.Lyons, J.
(1977) Semantics.
CambridgeUniversity Press.Miller, (3.
A.; Charles, W. G. 1991.
Contextualcorrelates of semantic similarity.
In: Languageand cognitive processes, 6(1):Salton, G. 1989.
Automatic Text Processing.
TheTransformation, Analysis, and Retrieval oJInformation by Computer.
530 pp.
Addison-Wesley.Schiitze, H.; Pedersen, J. O.
(1995) InformationRetrieval Based on Word Senses.In: Proceedings of the Fourth AnnualSymposium on Document Analysis andInformation retrieval, pp.
161-175,/.,as Vegas,NV.Schiitze, H. (1998) Automatic Word SenseDiscrimination.
In: Computational Linguistics,March 1998, Vol.
24, Number 1, pp.
97-123.Towell G.; Voorhees, E. M. (1998)Disambiguating Highly Ambiguous Words.32In: Computational Linguistics, March 1998,Vol.
24, Number 1, pp.
125-145.Yarowsky, D. 1993.
One sense per  collocation.
In:Proceedings of ARPA Human LanguageTechnology Workshop, pp.
266-271, Princeton,NJ.33
