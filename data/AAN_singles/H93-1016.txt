An Overview of the SPHINX-II Speech Recognition SystemXuedong Huang, Fileno Alleva, Mei-Yuh Hwang, and Ronald RosenfeldSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTIn the past year at Carnegie Mellon steady progress has been madein the area of acoustic and language modeling.
The result has beena dramatic reduction in speech recognition errors in the SPHINX-IIsystem.
In this paper, we review SPHINX-I/and summarize our re-cent efforts on improved speech recognition.
Recently SPHINX-I/achieved the lowest error ate in the November 1992 DARPA eval-uations.
For 5000-word, speaker-independent, continuous, peechrecognition, the error ate was reduced to 5%.1.
~TRODUCTIONAt Carnegie Mellon, we have made significant progressin large-vocabulary speaker-independent continuous peechrecognition during the past years \[16, 15, 3, 18, 14\].
In com-parison with the SPHINX system \[23\], SPHINX-II offers notonly significantly fewer recognition errors but also the capa-bility to handle amuch larger vocabulary size.
For 5,000-wordspeaker-independent speech recognition, the recognition errorrate has been reduced to 5%.
This system achieved the lowesterror rate among all of the systems tested in the November1992 DARPA evaluations, where the testing set has 330 utter-ances collected from 8 new speakers.
Currently we are refin-ing and extending these and related technologies to developpractical unlimited-vocabulary dictation systems, and spokenlanguage systems for general application domains with largervocabularies and reduced linguistic onstraints.One of the most important contributions to our systems de-velopment has been the availability of large amounts of train-ing data.
In our current system, we used about 7200 utter-ances of read Wall Street Journal (WSJ) text, collected from84 speakers (half male and half female speakers) for acous-tic model training; and 45-million words of text publishedby the WSJ for language model training.
In general, moredata requires different models o that more detailed acoustic-phonetic phenomena can be well characterized.
Towardsthis end, our recent progress can be broadly classified intofeature xtraction, detailed representation through parametersharing, search, and language modeling.
Our specific contri-butions in SPHINX-II include normalized feature represen-tations, multiple-codebook semi-continuous hidden Markovmodels, between-word senones, multi-pass search algorithms,long-distance language models, and unified acoustic and lan-guage representations.
The SPHINX-II system block diagramis illustrated in Figure 1, where feature codebooks, dictionary,senones, and language models are iteratively reestimated withthe semi-continuous hidden Markov model (SCHMM), albeitnot all of them are jointly optimized for the WSJ task atpresent.
In this paper, we will characterize our contributions~::'~-~:'&: ::::" ...:.-.. :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: ~4t..~ ..".".':.:..
:~&...... .
.......................................... .
~ ....................: " ~ ~.~:~.
'::&:::: ::: !:::!
:::~ !
:i: :~'$ i:~&::':::: ?
~:::::: ::::::I Data  V~.
'I l..'.
'~| I~  i~.
: .~?.1 Data  li !
i ~ i t:--!~i.."tt.
";~ ~ r .
~ I~ ~ ~ .."..'%"..".':.."..".".:'.
":: t li~ ~ ~ .::::'::~tt::i.:..".
".-"::: ~ ~?.~ .
.
.
.
.
.
:.
:s:'s: ~N:?k <.<.....iiiiiNiiiiiiiI .
/ ~ i i l  MFCC NornlalIIz?d I I/" ~ ~  Features  and  . '
IrA   .ilg i ' '< '=?"
!Nii i i iNiNi~:::@i@iiiIFigure 1: Sphinx-II System Diagramby percent error rate reduction.
Most of these experimentswere performed on a development test set for the 5000-wordWSJ task.
This set consists of 410 utterances from 10 newspeakers.2.
FEATURE EXTRACTIONThe extraction of reliable features is one of the most impor-tant issues in speech recognition and as a result the trainingdata plays a key role in this research.
However the curse ofdimensionality reminds us that the amount of training datawill always be limited.
Therefore incorporation of additionalfeatures may not lead to any measurable error reduction.
Thisdoes not necessarily mean that the additional features arepoor ones, but rather that we may have insufficient data toreliably model those features.
Many systems that incorporate81environmentally-robust \[1\] and speaker-robust \[11\] modelsface similar constraints.2.1.
MFCC Dynamic  FeaturesTemporal changes in the spectra re believed to play an im-portant role in human perception.
One way to capture this in-formation is to use delta coefficients that measure the changein coefficients over time.
Temporal information is particu-larly suitable for HMMs, since HMMs assume ach frame isindependent of the past, and these dynamic features broadenthe scope of a frame.
In the past, the SPHINX system hasutilized three codebooks containing \[23\]: (1) 12 LPC cep-strum coefficients x~(k), 1 <= k <= 12; (2) 12 differencedLPC cepstrum coefficients (40 msec.
difference) Axt(k),1 <= k <= 12; (3) Power and differenced power (40 msec.
)zt(0) and Azt(0).
Since we are using a multiple-codebookhidden Markov model, it is easy to incorporate new featuresby using an additional codebook.
We experimented with anumber of new measures of spectral dynamics, including:(1) second order differential cepstrum and power (AAzt(k),1 <= k <= 12, and AAzt(0))  and third order differentialcepstrum and power.
The first set of coefficients is incor-porated into a new codebook, whose parameters are secondorder differences of the cepstrum.
The second order differ-ence for frame t, AAx~(k), where t is in units of 10ms,is the difference between t + 1 and t - 1 first order differ-ential coefficients, or AAz~(k) = AX~_l(k) - Ax~+l(k).Next, we incorporated both 40 msec.
and 80 msec.
dif-ferences, which represent short-term and long-term spectraldynamics, respectively.
The 80 msec.
differenced cepstrumAz't (k)  is computed as: Az'~(k) = z~_4(k) - xt+4(k).We believe that these two sources of information are morecomplementary than redundant.
We incorporated both Aztand Aztt into one codebook (combining the two into onefeature vector), weighted by their variances.
We attemptedto compute optimal inear combination of cepstral segment,where weights are computed from linear discriminants.
Butwe found that performance deteriorated slightly.
This maybe due to limited training data or there may be little informa-tion beyond second-order differences.
Finally, we comparedmel-frequency epstral coefficients (MFCC) with our bilineartransformed LPC cepstral coefficients.
Here we observed asignificant improvement for the SCHMM model, but noth-ing for the discrete model.
This supported our early findingsabout problems with modeling assumptions \[15\].
Thus, the fi-nal configuration i volves 51 features distributed among fourcodebooks, each with 256 entries.
The codebooks are: (1) 12mel-scale cepstrum coefficients; (2) 12 40-msec differencedMFCC and 12 80-msec differenced MFCC; (3) 12 second-order differenced MFCC; and (4) power, 40-msec differencedpower, second-order differenced power.
The new feature setreduced errors by more than 25% over the baseline SPHINXresults on the WSJ task.3.
DETAILED MODEL ING THROUGHPARAMETER SHARINGWe need to model a wide range of acoustic-phonetic phenom-ena, but this requires a large amount of training data.
Sincethe amount of available training data will always be finite oneof the central issues becomes that of how to achieve the mostdetailed modeling possible by means of parameter sharing.Our successful examples include SCHMMs and senones.3.1.
Semi-Continuous HMMsThe semi-continuous hidden Markov model (SCHMM) \[12\]has provided us with an an excellent tool for achieving detailedmodeling through parameter sharing.
Intuitively, from thecontinuous mixture HMM point of view, SCHMMs employ ashared mixture of continuous output probability densities foreach individual HMM.
Shared mixtures ubstantially reducethe number of free parameters and computational complex-ity in comparison with the continuous mixture HMM, whilemaintaining, reasonably, its modeling power.
From the dis-crete HMM point of view, SCHMMs integrate quantizationaccuracy into the HMM, and robustly estimate the discreteoutput probabilities by considering multiple codeword can-didates in the VQ procedure.
It mutually optimizes the VQcodebook and HMM parameters under a unified probabilisticframework \[13\], where each VQ codeword is regarded as acontinuous probability density function.For the SCHMM, an appropriate acoustic representation forthe diagonal Gaussian density function is crucial to the recog-nition accuracy \[13\].
We first performed exploratory semi-continuous experiments on our three-codebook system.
TheSCHMM was extended to accommodate a multiple featurefront-end \[13\].
All codebook means and covariance matriceswere reestimated together with the HMM parameters exceptthe power covariance matrices, which were fixed.
When threecodebooks were used, the diagonal SCHMM reduced the er-ror rate of the discrete HMM by 10-15% for the RM task \[16\].When we used our improved 4-codebook MFCC front-end,the error rate reduction is more than 20% over the discreteHMM.Another advantage ofusing the SCHMM is that it requires lesstraining data in comparison with the discrete HMM.
There-fore, given the current limitations on the size of the trainingdata set, more detailed models can be employed to improvethe recognition accuracy.
One way to increase the numberof parameters is to use speaker-clustered models.
Due to thesmoothing abilities of the SCHMM, we were able to trainmultiple sets of models for different speakers.
We investi-gated automatic speaker clustering as well as explicit male,female, and generic models.
By using sex dependent modelswith the SCHMM, the error rate is further educed by 10% onthe WSJ task.823.2.
SenonesTo share parameters among different word models, context-dependent subword models have been used successfully inmany state-of-the-art speech recognition systems \[26, 21, 17\].The principle of parameter sharing can also be extended tosubphonetic models \[19, 18\].
We treat the state in pho-netic hidden Markov models as the basic subphonetic unitsenone.
Senones are constructed by clustering the state-dependent output distributions across different phonetic mod-els.
The total number of senones can be determined by clus-tering all the triphone HMM states as the shared-distributionmodels \[18\].
States of different phonetic models may thusbe tied to the same senone if they are close according tothe distance measure.
Under the senonic modeling frame-work, we could also use a senonic decision tree to predict un-seen triphones.
This is particularly important for vocabulary-inc~pendence \[10\], as we need to find subword models whichare detailed, consistent, trainable and especially generalizable.Recently we have developed a new senonic decision-tree topredict he subword units not covered in the training set \[18\].The decision tree classifies enones by asking questions in ahierarchical manner \[7\].
These questions were first createdusing speech knowledge from human experts.
The tree wasautomatically constructed by searching for simple as well ascomposite questions.
Finally, the tree was pruned using crossvalidation.
When the algorithm terminated, the leaf nodesof the tree represented the senones to be used.
For the WSJtask, our overall senone models gave us 35% error reductionin comparison with the baseline SPHINX results.The advantages of senones include not only better param-eter sharing but also improved pronunciation optimization.Clustering at the granularity of the state rather than the entiremodel (like generalized triphones \[21\]) can keep the dissimi-lar states of two models apart while the other correspondingstates are merged, and thus lead to better parameter shar-ing.
In addition, senones give us the freedom to use a largernumber of states for each phonetic model to provide moredetailed modeling.
Although an increase in the number ofstates will increase the total number of free parameters, withsenone sharing redundant s ates can be clustered while othersare uniquely maintained.Pronunciation Optimization.
Here we use the forward-backward algorithm to iteratively optimize asenone sequenceappropriate for modeling multiple utterances of a word.
Toexplore the idea, given the multiple xamples, we train a wordHMM whose number of states is proportional to the averageduration.
When the Baum-Welch reestimation reaches itsoptimum, each estimated state is quantized with the senonecodebook.
The closest one is used to label the states of theword HMM.
This sequence of senones becomes the senonicbaseform of the word.
Here arbitrary sequences ofsenones areallowed to provide the flexibility for the automatically learnedpronunciation.
When the senone sequence of every word isdetermined, the parameters ( enones) may be re-trained.
Al-though each word model generally has more states than thetraditional phoneme-concatenated word model, the numberof parameters emains the same since the size of the senonecodebook isunchanged.
When senones were used for pronun-ciation optimization i a preliminary experiment, we achieved10-15% error reduction in a speaker-independent continuousspelling task \[ 19\].4.
MULTI -PASS SEARCHRecent work on search algorithms for continuous peechrecognition has focused on the problems related to large vo-cabularies, long distance language models and detailed acous-tic modeling.
A variety of approaches based on Viterbi beamsearch \[28, 24\] or stack decoding \[5\] form the basis for mostof this work.
In comparison with stack decoding, Viterbibeam search is more efficient but less optimal in the senseof MAR For stack decoding, a fast-match is necessary to re-duce a prohibitively arge search space.
A reliable fast-matchshould make full use of detailed acoustic and language mod-els to avoid the introduction of possibly unrecoverable errors.Recently, several systems have been proposed that use Viterbibeam search as a fast-match \[27, 29\], for stack decoding or theN-best paradigm \[25\].
In these systems, N-best hypothesesare produced with very simple acoustic and language models.A multi-pass rescoring is subsequently applied to these hy-potheses to produce the final recognition output.
One problemin this paradigm is that decisions made by the initial phaseare based on simplified models.
This results in errors thatthe N-best hypothesis list cannot recover.
Another problemis that the rescoring procedure could be very expensive perse as many hypotheses may have to be rescored.
The chal-lenge here is to design a search that makes the appropriatecompromises among memory bandwidth, memory size, andcomputational power \[3\].To meet this challenge we incrementally apply all availableacoustic and linguistic information in three search phases.Phase one is a left to right Viterbi Beam search which producesword end times and scores using right context between-wordmodels with a bigram language model.
Phase two, guidedby the results from phase one, is a right to left Viterbi Beamsearch which produces word beginning times and scores basedon left context between-word models.
Phase three is an A*search which combines the results of phases one and two witha long distance language model.4.1.
Modified A* Stack SearchEach theory, th, on the stack consists of five entries.
A partialtheory, th.pt, a one word extension th.w, a time th.t whichdenotes the boundary between th.pt and th.w, and two scoresth.g, which is the score for th.pt up to time th.t and th.h which83is the best score for the remaining portion of the input startingwith ti~.w at time th.t+l through to the end.
Unique theoriesare detlermined by th.pt and th.w.
The algorithm proceeds asfollows.l.
Add initial states to the stack.2.
According to the evaluation function th.g+ th.h, removethe best theory, th, from the stack.3.
Ifth accounts for the entire input then output he sentencecorresponding to th.
Halt if this is the Nth utteranceoutput.4.
For the word th.w consider all possible nd times, t asprovided by the left/right lattice.
(a) For all words, w, beginning at time t+ 1 as providedby the right/left latticei.
Extend theory th with w. Designate thistheory as th'.
Set th'.pt = th.pt + th.w,th'.w ::= w and th'.t = t.ii.
Compute scoresth'.g = th.g + w_score(w, th.t + 1,t), andth'.h.
See following for definition of w_scoreand thqh computation.iii.
If th' is already on the stack then choose thebest instance of th' otherwise push th' ontothe stack.5.
Goto step 2.4.2.
DiscussionWhen tit is extended we are considering all possible nd timest for th.w and all possible xtensions w. When extending thwith w to obtain th' we are only interested in the value forth'.t which gives the best value for th'.h + th'.g.
For any tand w, th'.h is easily determined via table lookup from theright/left lattice.
Furthermore the value of th'.g is given byth.g + w_score (w, th.t+l, t).
The function w_score(w,b,e)computes the score for the word w with begin time b and endtime e.Our objective is to maximize the recognition accuracy witha minimal increase in computational complexity.
Withour decomposed, incremental, semi-between-word-triphonessearch, we observed that early use of detailed acoustic mod-els can significantly reduce the recognition error rate witha negligible increase computational complexity as shown inFigure 2.By incrementally applying knowledge we have been able todecompose the search so that we can efficiently apply de-tailed acoustic or linguistic knowledge in each phase.
Further13.012.o~11.o~10.0 t9.08.07.06.05.04.03.o i2.0"1: ..................................................... i ............ i__~....?
_.~..__.
0 with-irt wordi........................ 2 .. ii ..!
I !4 8 16 32 64 128number of alternativesFigure 2: Comparison between early and late use of knowl-edge.more, each phase defers decisions that are better made by asubsequent phase that will apply the appropriate acoustic orlinguistic information.5.
UNIFIED STOCHASTIC ENGINEAcoustic and language models are usually constructed sepa-rately, where language models are derived from a large textcorpus without consideration for acoustic data, and acousticmodels are constructed from the acoustic data without ex-ploiting the existing text corpus used for language training.We recently have developed a unified stochastic engine (USE)that jointly optimizes both acoustic and language models.
Asthe true probability distribution of both the acoustic and lan-guage models can not be accurately estimated, they can not beconsidered as real probabilities but scores from two differentsources.
Since they are scores instead of probabilities, thestraightforward implementation of the Bayes equation willgenerally not lead to a satisfactory recognition performance.To integrate language and acoustic probabilities for decoding,we are forced to weight acoustic and language probabilitieswith a so called language weight \[6\].
The constant languageweight is usually tuned to balance the acoustic probabilitiesand the language probabilities such that the recognition errorrate can be minimized.
Most HMM-based speech recognitionsystems have one single constant language weight hat is in-dependent of any specific acoustic or language information,and that is determined using a hill-climbing procedure on de-velopment data.
It is often necessary to make many runs withdifferent language weights on the development data in orderto determine the best value.In the unified stochastic engine (USE), not only can we iter-atively adjust language probabilities to fit our given acous-tic representations but also acoustic models.
Our multi-pass84search algorithm generates N-best hypotheses which are usedto optimize language weights or implement many discrimina-tive training methods, where recognition errors can be usedas the objective function \[20, 25\].
With the progress of newdatabase construction such as DARPA's CSR Phase II, we be-lieve acoustically-driven language modeling will eventuallyprovide us with dramatic performance improvements.In the N-best hypothesis list, we can assume that the correcthypothesis is always in the list (we can insert the correctanswer if it is not there).
Let hypothesis be a sequence ofwords wl, w2, ...w~ with corresponding language and acousticprobabilities.
We denote the correct word sequence as 0, andall the incorrect sentence hypotheses as 0.
We can assign avariable weight o each of the n-gram probabilities such thatwe have a weighted language probability as:W(W) = r r , _ , _  ,_ _ ,~,(x,, , , , ,w,_,, .
.
.)
llr,tw~lw~-rw~-2...) (1)iwhere the weight c~ 0 is a function of acoustic data, Xi, forwi, and words wi, Wi-l, .... For a given sentence k, a verygeneral objective function can be defined asLk(A) = EPr (0 ) l - -~\ [ l ?gPr ( 'V ' lw i  ) +i~o+a( Xi, wi wi_ l...)logPr(wi Iwi_ l wi-2...)\] ++ ~\[ logPr( ,Vdw,)  ++a( .V i ,  w iw i_ l .
.
. )
l ogPr (w i lw i_ l .
.
. )
\ ]} .
(2)where A denotes acoustic and language model parameters awell as language weights, Pr(O) denotes the a priori proba-bility of the incorrect path 0, and Pr(Xi  \]wi) denotes acousticprobability generated by word model w~.
It is obvious thatwhen Lk (A) > 0 we have a sentence classification error.
Min-imization of Equation 2 will lead to minimization of sentencerecognition error rate.
To jointly optimize the whole train-ing set, we first define a nondecreasing, differentiable costfunction Ik (A) (we use the sigmoid function here) in the samemanner as the adaptive probabilistic decent method \[4, 20\].There exist many possible gradient decent procedures for theproposed problems.The term o~(,?i,wiwi_l...)logPr(wilwi_l...) could bemerged as one item in Equation 2.
Thus we can have lan-guage probabilities directly estimated from the acoustic train-ing data.
The proposed approach is fundamentally differentfrom traditional stochastic language modeling.
Firstly, con-ventional language modeling uses a text corpus only.
Anyacoustical confusable words will not be reflected in languageprobabilities.
Secondly, maximum likelihood estimation isusually used, which is only loosely related to minimum sen-tence error.
The reason for us to keep a 0 separate from thelanguage probability is that we may not have sufficient acous-tic data to estimate the language parameters atpresent.
Thus,we are forced to have a0  shared across different words so wemay have n-gram-dependent, word-dependent or even word-count-dependent language weights.
We can use the gradientdecent method to optimize all of the parameters in the USE.When we jointly optimize L(A), we not only obtain our uni-fied acoustic models but also the unified language models.
Apreliminary experiment reduced error rate by 5% on the WSJtask \[14\].
We will extend the USE paradigm for joint acousticand language model optimization.
We believe that t_he USEcan further educe the error rate with an increased amount oftraining data.6.
LANGUAGE MODELINGLanguage Modeling is used in Sphinx-II at two differentpoints.
First, it is used to guide the beam search.
For thatpurpose we used a conventional backoff bigram for that pur-pose.
Secondly, it is used to recalculate linguistic scores forthe top N hypotheses, as part of the N-best paradigm.
Weconcentrated most of our language modeling effort on thelatter.Several variants of the conventional backoff trigram languagemodel were applied at the reordering stage of the N-bestparadigm.
(Eventually we plan to incorporate this languagemodel into the A* phase of the multi-pass earch with theUSE).
The best result, a 22% word error rate reduction, wasachieved with the simple, non-interpolated "backward" tri-gram, with the conventional forward trigram finishing a closesecond.7.
SUMMARYOur contributions in SPHINX-II include improved featurerepresentations, multiple-codebook semi-continuous hiddenMarkov models, between-word senones, multi-pass earchalgorithms, and unified acoustic and language modeling.
Thekey to our success is our data-driven unified optimization ap-proach.
This paper characterized our contributionsby percenterror rate reduction on the 5000-word WSJ task, for which wereduced the word error rate from 20% to 5% in the past year\[2\].Although we have made dramatic progress there remains alarge gap between commercial applications and laboratorysystems.
One problem is the large number of out of vocabu-lary (OOV) words in real dictation applications.
Even for a20000-word ictation system, on average more than 25% ofthe utterances in a test set contain OOV words.
Even if weexclude those utterance containing OOV words, the error rateis still more than 9% for the 20000-word task due to the lim-itations of current echnology.
Other problems are illustratedby the November 1992 DARPA stress test evaluation, wheretesting data comprises both spontaneous speech with manyOOV words but also speech recorded using several differentmicrophones.
Even though we augmented our system with85more than 20,000 utterances in the training set and a noisenormalization component \[1\], our augmented system only re-duced the error rate of our 20000-word baseline result from12.8% to 12.4%, and the error rate for the stress test was evenworse 'when compared with the baseline (18.0% vs. 12.4%).To sunmaarize, our current word error rates under differenttesting conditions are listed in Table 1.
We can see from thisSystems Vocabulary Test Set Error RateBaseline 5000 330 utt.
5.3%Baseline 20000 333 utt.
12.4%Stress Test 20000 320 utt.
18.0%Table 1: Performance ofSPHINX-II in real applications.table that improved modeling technology is still needed tomake speech recognition a reality.8.
AcknowledgementsThis research was sponsored by the Defense Advanced Re-search Projects Agency and monitored by the Space and NavalWarfare Systems Command under Contract N00039-91-C-0158, ARPA Order No.
7239.The authors would like to express their gratitude to Raj Reddyand other members of CMU speech group for their help.Re ferences1.
Acero, A. Acoustical and Environmental Robustness inAuto-matic Speech Recognition.
Department of Electrical Engineer-ing, Carnegie-Mellon U iversity, September 1990.2.
Alleva, E, Hon, H., Huang, X., Hwang, M., Rosenfeld, R., andWeide, R. Applying SPHINX-H to the DARPA WaU Street Jour-nal CSR Task.
in: DARPASpeechandLanguageWorkshop.Morgan Kaufmann Publishers, San Mateo, CA, 1992.3.
AUeva, E, Huang, X., and Hwang, M. An Improved SearchAlgorithm for Continuous Speech Recognition.
in: IEEE In-ternational Conference on Acoustics, Speech, and SignalProcessing.
1993.4.
Amari, S. A Theory of Adaptive Pattern Classifiers.
IEEETrans.
Electron.
Comput., vol.
EC-16 (1967), pp.
299-307.5.
Bahl, L. R., Jelinek, E, and Mercer, R. A Maximum LikelihoodApproach to Continuous Speech Recognition.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence, vol.PAMI-5 (1983), pp.
179-190.6.
Bahl, L., Bakis, R., Jelinek, E, and Mercer, R. Language-Model~Acoustic Channel Balance Mechanism.
IBM TechnicalDisclosure Bulletin, vol.
23 (1980), pp.
3464-3465.7.
Breiman, L., Friedman, J., Olshen, R., and Stone, C. Clas-sification and Regression Trees.
Wadsworth, Inc., Belmont,CA., 1984.8.
Hon, H. and Lee, K. CMU Robust Vocabulary-lndependentSpeech Recognition System.
in: IEEE International Confer-ence on Acoustics, Speech, and Signal Processing.
Toronto,Ontario, CANADA, 1991, pp.
889-892.9.
Huang, X.
Minimizing Speaker Variations Effects for Speaker-Independent Speech Recognition.
in: DARPA Speech andLanguage Workshop.
Morgan Kaufmann Publishers, SanMateo, CA, 1992.10.
Huang, X. Phoneme Classification Using Semicontinuous Hid-den Markov Models.
IEEE Transactions on Signal Process-ing, vol.
40 (1992), pp.
1062-1067.11.
Huang, X., Ariki, Y., and Jack, M. Hidden Markov Modelsfor Speech Recognition.
Edinburgh University Press, Edin-burgh, U.K., 1990.12.
Huang, X., Belin, M., AUeva, E, and Hwang, M. UnifiedStochastic Engine (USE)for Speech Recognition.
in: IEEEInternational Conference on Acoustics, Speech, and SignalProcessing.
1993.13.
Huang, X., Hon, H., Hwang, M., and Lee, K. A ComparativeStudy of Discrete, Semicontinuous, and Continuous HiddenMarkov Models.
Computer Speech and Language, Inpress,1993.14.
Huang, X., Lee, K., Hon, H., and Hwang, M. ImprovedAcoustic Modelingforthe SPHINXSpeechRecognitionSystem.in: IEEE International Conference on Acoustics, Speech,and Signal Processing.
Toronto, Ontario, CANADA, 1991,pp.
345-348.15.
Hwang, M., Hon, H., and Lee, K. Modeling Between-WordCoarticulation in Continuous Speech Recognition.
in: Pro-ceedings of Eurospeech.
Paris, FRANCE, 1989, pp.
5-8.16.
Hwang, M. and Huang, X. Shared-Distribution HiddenMarkov Models for Speech Recognition.
IEEE Transactionson Speech and Audio Processing, vol.
1 (1993).17.
Hwang, M. and Huang, X. SubphoneticModeling with MarkovStates - -  Senone.
in: IEEE International Conference onAcoustics, Speech, and Signal Processing.
1992.18.
Juang, B.-H. and Katagiri, S. Discriminative LearningforMin-imum Error Classification.
IEEE Trans on Signal Processing,to appear, December 1992.19.
Lee, K. Context-Dependent Phonetic Hidden Markov Mod-els for Continuous Speech Recognition.
IEEE Transactionson Acoustics, Speech, and Signal Processing, April 1990,pp.
599-609.20.
Lee, K., Hon, H., and Reddy, R. An Overview of the SPHINXSpeech Recognition System.
IEEE Transactions on Acous-tics, Speech, and Signal Processing, January 1990, pp.
35-45.21.
Lowerre, B. and Reddy, D. The Harpy Speech UnderstandingSystem.
in: The Harpy Speech Understanding System, byB.
Lowerre and D. Reddy, edited by W. Lee.
Prentice-Hall,Englewood Cliffs, N J, 1980.22.
Schwartz, R., Austin, S., Kubala, F., and Makhoul, J.
New Usesfor the N-Best Sentence Hypotheses Within the Byblos SpeechRecognition System.
in: IEEE International Conference onAcoustics, Speech, and Signal Processing.
1992, pp.
1-4.23.
Schwartz, R., Chow, Y., Kimball, O., Roucos, S., Krasner, M.,and Makhoul, J. Context-Dependent Modeling for Acoustic-Phonetic Recognition of Continuous Speech.
in: IEEE In-ternational Conference on Acoustics, Speech, and SignalProcessing.
1985, pp.
1205-1208.24.
Soong, E and Huang, E. A Tree-Trellis Based Fast Search forFinding the N-Best Sentence Hypothesis.
in: DARPA Speechand Language Workshop.
1990.25.
Viterbi, A. J.
Error Bounds for Convolutional Codes and anAsymptotically Optimum Decoding Algorithm.
IEEE Trans-actions on Information Theory, vol.
IT-13 (1967), pp.
260-269.86
