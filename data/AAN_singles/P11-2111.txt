Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 631?635,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsChinese sentence segmentation as comma classificationNianwen Xue and Yaqin YangBrandeis University, Computer Science DepartmentWaltham, MA, 02453{xuen,yaqin}@brandeis.eduAbstractWe describe a method for disambiguating Chi-nese commas that is central to Chinese sen-tence segmentation.
Chinese sentence seg-mentation is viewed as the detection of looselycoordinated clauses separated by commas.Trained and tested on data derived from theChinese Treebank, our model achieves a clas-sification accuracy of close to 90% overall,which translates to an F1 score of 70% fordetecting commas that signal sentence bound-aries.1 IntroductionSentence segmentation, or the detection of sentenceboundaries, is very much a solved problem for En-glish.
Sentence boundaries can be determined bylooking for periods, exclamation marks and ques-tion marks.
Although the symbol (dot) that is used torepresent period is ambiguous because it is also usedas the decimal point or in abbreviations, its resolu-tion only requires local context.
It can be resolvedfairly easily with rules in the form of regular expres-sions or in a machine-learning framework (Reynarand Ratnaparkhi, 1997).Chinese also uses periods (albeit with a differentsymbol), question marks, and exclamation marks toindicate sentence boundaries.
Where these punctua-tion marks exist, sentence boundaries can be unam-biguously detected.
The difference is that the Chi-nese comma also functions similarly as the Englishperiod in some context and signals the boundary of asentence.
As a result, if the commas are not disam-biguated, Chinese would have these ?run-on?
sen-tences that can only be plausibly translated into mul-tiple English sentences.
An example is given in (1),where one Chinese sentence is plausibly translatedinto three English sentences.
(1) ?this?period??time??AS?AS?
?pay attention to?this?CLnanoNano33?,[1]?even?
?in person?visit?AS?a few?AS??computer??market,,[2]???comparatively??speaking,,[3]??Zhuoyue??s??price?relatively?low?DE,,[4]??and?can??guarantee?be??genuine?[5],???therefore?place?[AS]?order?.
?I have been paying attention to this Nano 3 re-cently, [1] and I even visited a few computerstores in person.
[2] Comparatively speaking,[3] Zhuoyue?s prices are relatively low, [4]and they can also guarantee that their productsare genuine.
[5] Therefore I placed the order.
?In this paper, we formulate Chinese sentence seg-mentation as a comma disambiguation problem.
Theproblem is basically one of separating commas thatmark sentence boundaries (such as [2] and [5] in (1))from those that do not (such as [1], [3] and [4]).Sentences that can be split on commas are gener-ally loosely coordinated structures that are syntacti-cally and semantically complete on their own, andthey do not have a close syntactic relation with oneanother.
We believe that a sentence boundary detec-tion task that disambiguates commas, if successfully631solved, simplifies downstream tasks such as parsingand Machine Translation.The rest of the paper is organized as follows.
InSection 2, we describe our procedure for derivingtraining and test data from the Chinese Treebank(Xue et al, 2005).
In Section 3, we present ourlearning procedure.
In Section 4 we report our re-sults.
Section 5 discusses related work.
Section 6concludes our paper.2 Obtaining dataTo our knowledge, there is no data in the publicdomain with commas explicitly annotated based onwhether they mark sentence boundaries.
One couldimagine using parallel data where a Chinese sen-tence is word-aligned with multiple English sen-tences, but such data is generally noisy and com-mas are not disambiguated based on a uniform stan-dard.
We instead pursued a different path and de-rived our training and test data from the ChineseTreebank (CTB).
The CTB does not disambiguatecommas explicitly, and just like the Penn EnglishTreebank (Marcus et al, 1993), the sentence bound-aries in the CTB are identified by periods, exclama-tion and question marks.
However, there are clearsyntactic patterns that can be used to disambiguatethe two types of commas.
Commas that mark sen-tence boundaries delimit loosely coordinated top-level IPs, as illustrated in Figure 1, and commas thatdon?t cover all other cases.
One such example isFigure 2, where a PP is separated from the rest ofthe sentence with a comma.
We devised a heuristicalgorithm to detect loosely coordinated structures inthe Chinese Treebank, and labeled each comma witheither EOS (end of a sentence) or Non-EOS (not theend of a sentence).3 LearningAfter the commas are labeled, we have basicallyturned comma disambiguation into a binary classi-fication problem.
The syntactic structures are anobvious source of information for this classificationtask, so we parsed the entire CTB 6.0 in a round-robin fashion.
We divided CTB 6.0 into 10 portions,and parsed each portion with a model trained onother portions, using the Berkeley parser (Petrov andKlein, 2007).
The labels for the commas are derived???????
???????????????
????
?IP PU IP PU IP PUIPNP VP?
?NPVPVVNPNPVPVVIPNP VPVV NP*pro*ADVPVP?????
?
?ADVPVPVVFigure 1: Sentence-boundary denoting commaIPPP PU NP NP VP PU?P NP DNP NPNPDEGVV????
??
?
??
???
??
?
???
??
??????
?Figure 2: Non-sentence boundary denoting commafrom the gold-standard parses using the heuristicsdescribed in Section 2, as they obviously should be.We first established a baseline by applying the sameheuristic algorithm to the automatic parses.
This willgive us a sense of how accurately commas can bedisambiguated given imperfect parses.
The researchquestion we?re trying to address here basically is:can we improve on the baseline accuracy with a ma-chine learning model?We conducted our experiments with a MaximumEntropy classifier trained with the Mallet package(McCallum, 2002).
The following are the featureswe used to train our classifier.
All features are de-scribed relative to the comma being classified andthe context is the sentence that the comma is in.
Theactual feature values for the first comma in Figure 1are given as examples:1.
Part-of-speech tag of the previous word, andthe string representation of the previous wordif it has a frequency of greater than 20 in thetraining corpus, e.g., f1=VV, f2=??.2.
Part-of-speech of the following word and the632string representation of the following word if ithas a frequency of greater than 20 in the train-ing corpus, e.g., f3=JJ, f4=??3.
The string representation of the following wordif it occurs more than 12,000 times in sentence-initial positions in a large corpus external to ourtraining and test data.14.
The phrase label of the left sibling and thephrase label of their right sibling in the syntac-tic parse tree, as well as their conjunction, e.g,f6=IP, f7=IP, f8=IP+IP5.
The conjunction of the ancestors, the phrase la-bel of the left sibling, and the phrase label ofthe right sibling.
The ancestor is defined as thepath from the parent of the comma to the rootnode of the parse tree, e.g., f9=IP+IP+IP.6.
Whether there is a subordinating conjunction(e.g., ?if?, ?because?)
to the left of the comma.The search starts at the comma and stops at theprevious punctuation mark or the beginning ofthe sentence, e.g., f10=noCS.7.
Whether the parent of the comma is a coordi-nating IP construction.
A coordinating IP con-struction is an IP that dominates a list of coor-dinated IPs, e.g., f11=CoordIP.8.
Whether the comma is a top-level child, definedas the child of the root node of the syntactictree, e.g., f12=top.9.
Whether the parent of the comma is atop-level coordinating IP construction, e.g.,f13=top+coordIP.10.
The punctuation mark template for this sen-tence, e.g., f14=,+,+?11.
whether the length difference between the leftand right segments of the comma is smallerthan 7.
The left (right) segment spans from theprevious (next) punctuation mark or the begin-ning (end) of the sentence to the comma, e.g.,f15=>74 Results and discussionOur comma disambiguation models are trained andevaluated on a subset of the Chinese TreeBank(CTB) 6.0, released by the LDC.
The unused por-tion of CTB 6.0 consists of broadcast news data that1This feature is not instantiated here because the followingword in this example does not occur with sufficient accuracy.contains disfluencies, different from the rest of theCTB 6.0.
We used the training/test data split rec-ommended in the Chinese Treebank documentation.The CTB file IDs used in our experiments are listedin Table 1.
The automatic parses in each test setare produced by retraining the Berkeley parser onits corresponding training set, plus the unused por-tion of the CTB 6.0.
Measured by the ParsEval met-ric (Black et al, 1991), the parsing accuracy on theCTB test set stands at 83.63% (F-score), with a pre-cision of 85.66% and a recall of 81.69%.Data Train TestCTB41-325, 400-454, 500-554 1-40590-596, 600-885, 900 901-9311001-1078, 1100-1151Table 1: Data set division.There are 1,510 commas in the test set, and ourheuristic baseline algorithm is able to correctly label1,321 or 87.5% of the commas.
Among these, 250or 16.6% of them are EOS commas that mark sen-tence boundaries and 1,260 of them are Non-EOScommas.
The results of our experiments are pre-sented in Table 2.
The baseline precision and recallfor the EOS commas are 59.1% and 79.6% respec-tively with an F1 score of 67.8% .
For Non-EOScommas, the baseline precision and recall are 95.7%and 89.0% respectively, amounting to an F1 score of70.1%.
The learned maximum classifier achieved amodest improvement over the baseline.
The over-all accuracy of the learned model is 89.2%, just shyof 90%.
The precision and recall for EOS commasare 64.7% and 76.4% respectively and the combinedF1 score is 70.1%.
For Non-EOS commas, the pre-cision and recall are 95.1% and 91.7% respectively,with the F1 score being 93.4%.
Other than a listof most frequent words that start a sentence, all thefeatures are extracted from the sentence the commaoccurs in.
Given that the heuristic algorithm and thelearned model use essentially the same source of in-formation, we attribute the improvement to the useof lexical features that the heuristic algorithm cannoteasily take advantage of.Table 3 shows the contribution of individual fea-ture groups.
The numbers reflect the accuracy wheneach feature group is taken out of the model.
Whileall the features have made a contribution to the over-633Baseline Learning(%) p r f1 p r f1Overall 87.5 89.2EOS 59.1 79.6 67.8 64.7 76.4 70.1Non-EOS95.7 89.0 92.2 95.1 91.7 93.4Table 2: Accuracy for the baseline heuristic algorithmand the learned modelall accuracy on the development set, some of thefeatures (3 and 8) actually hurt the overall perfor-mance slightly on the test set.
What?s interesting iswhile the heuristic algorithm that is based entirelyon syntactic structure produced a strong baseline,when formulated as features they are not at all effec-tive.
In particular, feature groups 7, 8, 9 are explicitreformulations of the heuristic algorithm, but theyall contributed very little to or even slightly hurt theoverall performance.
The more effective features arethe lexical features (1, 2, 10, 11) probably becausethey are more robust.
What this suggests is that wecan get reasonable sentence segmentation accuracywithout having to parse the sentence (or rather, themulti-sentence group) first.
The sentence segmenta-tion can thus come before parsing in the processingpipeline even in a language like Chinese where sen-tences are not unambiguously marked.overall f1 (EOS) f1 (non-EOS)all 89.2 70.1 93.4- (1,2) 87.5 67.7 92.3-10 87.8 67.5 92.5-11 88.6 68.6 93.1-4 89.0 69.6 93.3-5 89.1 69.5 93.3-6 89.1 69.9 93.4-7 89.1 70.1 93.4-9 89.1 69.7 93.3-8 89.2 70.5 93.4- 3 89.4 70.5 93.5Table 3: Feature effectiveness5 Related workThere has been a fair amount of research on punctua-tion prediction or generation in the context of spokenlanguage processing (Lu and Ng, 2010; Guo et al,2010).
The task presented here is different in that thepunctuation marks are already present in the text andwe are only concerned with punctuation marks thatare semantically ambiguous.
Our specific focus ison the Chinese comma, which sometimes signals asentence boundary and sometimes doesn?t.
The Chi-nese comma has also been studied in the context ofsyntactic parsing for long sentences (Jin et al, 2004;Li et al, 2005), where the study of comma is seen aspart of a ?divide-and-conquer?
strategy to syntacticparsing.
Long sentences are split into shorter sen-tence segments on commas before they are parsed,and the syntactic parses for the shorter sentence seg-ments are then assembled into the syntactic parse forthe original sentence.
We study comma disambigua-tion in its own right aimed at helping a wide range ofNLP applications that include parsing and MachineTranslation.6 ConclusionThe main goal of this short paper is to bring tothe attention of the field a problem that has largelybeen taken for granted.
We show that while sen-tence boundary detection in Chinese is a relativelyeasy task if formulated based on purely orthographicgrounds, the problem becomes much more challeng-ing if we delve deeper and consider the semantic andpossibly the discourse basis on which sentences aresegmented.
Seen in this light, the central problemto Chinese sentence segmentation is comma disam-biguation.
We trained a statistical model using dataderived from the Chinese Treebank and reportedpromising preliminary results.
Much remains to bedone regarding how sentences in Chinese should besegmented and how this problem should be modeledin a statistical learning framework.AcknowledgmentsThis work is supported by the National ScienceFoundation via Grant No.
0910532 entitled ?RicherRepresentations for Machine Translation?.
Allviews expressed in this paper are those of the au-thors and do not necessarily represent the view ofthe National Science Foundation.634ReferencesE.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitively comparing the syntactic coverageof English grammars.
In Proceedings of the DARPASpeech and Natural Language Workshop, pages 306?311.Yuqing Guo, Haifeng Wang, and Josef Van Genabith.2010.
A Linguistically Inspired Statistical Model forChinese Punctuation Generation.
ACM Transactionson Asian Language Processing, 9(2).Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-Hyeok Lee.
2004.
Segmentation of Chinese LongSentences Using Commas.
In Proceedings of theSIGHANN Workshop on Chinese Language Process-ing.Xing Li, Chengqing Zong, and Rile Hu.
2005.
A Hier-archical Parsing Approach with Punctuation Process-ing for Long Sentence Sentences.
In Proceedings ofthe Second International Joint Conference on NaturalLanguage Processing: Companion Volume includingPosters/Demos and Tutorial Abstracts.We Lu and Hwee Tou Ng.
2010.
Better PunctuationPrediction with Dynamic Conditional Random Fields.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, MIT, Mas-sachusetts.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a Large Annotated Corpus of English:the Penn Treebank.
Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Slav Petrov and Dan Klein.
2007.
Improved Inferencingfor Unlexicalized Parsing.
In Proc of HLT-NAACL.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
AMaximum Entropy Approach to Identifying SentenceBoundaries.
In Proceedings of the Fifth Conference onApplied Natural Language Processing (ANLP), Wash-ington, D.C.Nianwen Xue, Fei Xia, Fu dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: PhraseStructure Annotation of a Large Corpus.
Natural Lan-guage Engineering, 11(2):207?238.635
