Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1625?1630,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDeriving adjectival scales from continuous space word representationsJoo-Kyung KimDepartment of Computer Science and EngineeringThe Ohio State UniversityColumbus, OH 43210, USAkimjook@cse.ohio-state.eduMarie-Catherine de MarneffeDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210, USAmcdm@ling.ohio-state.eduAbstractContinuous space word representations ex-tracted from neural network language mod-els have been used effectively for natural lan-guage processing, but until recently it was notclear whether the spatial relationships of suchrepresentations were interpretable.
Mikolovet al(2013) show that these representationsdo capture syntactic and semantic regularities.Here, we push the interpretation of continuousspace word representations further by demon-strating that vector offsets can be used to de-rive adjectival scales (e.g., okay < good < ex-cellent).
We evaluate the scales on the indirectanswers to yes/no questions corpus (de Marn-effe et al 2010).
We obtain 72.8% accuracy,which outperforms previous results (?60%)on this corpus and highlights the quality of thescales extracted, providing further support thatthe continuous space word representations aremeaningful.1 IntroductionThere has recently been a surge of interest for deeplearning in natural language processing.
In particu-lar, neural network language models (NNLMs) havebeen used to learn distributional word vectors (Ben-gio et al 2003; Schwenk, 2007; Mikolov et al2010): the models jointly learn an embedding ofwords into an n-dimensional feature space.
One ofthe advantages put forth for such distributed rep-resentations compared to traditional n-gram mod-els is that similar words are likely to have similarvector representations in a continuous space model,whereas the discrete units of an n-gram model donot exhibit any inherent relation with one another.It has been shown that the continuous space repre-sentations improve performance in a variety of NLPtasks, such as POS tagging, semantic role labeling,named entity resolution, parsing (Collobert and We-ston, 2008; Turian et al 2010; Huang et al 2012).Mikolov et al(2013) show that there are somesyntactic and semantic regularities in the word rep-resentations learned, such as the singular/plural rela-tion (the difference of singular and plural word vec-tors are equivalent: apple ?
apples ?
car ?
cars ?family ?
families) or the gender relation (a mascu-line noun can be transformed into the feminine form:king ?
man + woman ?
queen).We extend Mikolov et al(2013)?s approach andexplore further the interpretation of the vector space.We show that the word vectors learned by NNLMsare meaningful: we can extract scalar relationshipsbetween adjectives (e.g., bad < okay < good < ex-cellent), which can not only serve to build a senti-ment lexicon but also be used for inference.
To eval-uate the quality of the scalar relationships learnedby NNLMs, we use the indirect yes/no question an-swer pairs (IQAP) from (de Marneffe et al 2010),where scales between adjectives are needed to infera yes/no answer from a reply without explicit yes orno such as Was the movie good?
It was excellent.Our method reaches 72.8% accuracy, which is thebest result reported so far when scales are used.2 Previous workWe use the continuous word representations from(Mikolov et al 2011), extracted from a recurrentneural network language model (RNNLM), whose1625three-layer architecture is represented in Figure 1.sigmoidsoftmaxUWw(t)s(t-1)s(t) y(t)VFigure 1: The architecture of the RNNLM.In the input layer, w(t) is the input word repre-sented by 1-of-N coding at time t when the vocabu-lary size is N .
When there are M nodes in the hid-den layer, the number of connections between theinput layer and the hidden layer is NM and the con-nections can be represented by a matrix U .The hidden layer is also connected recurrently tothe context s(t ?
1) at time t ?
1 (s(0) is initial-ized with small values like 0.1).
The connectionsbetween the previous context and the hidden layerare represented by a matrix W .
The dimension-ality of the word representations is controlled bythe size of W .
The output of the hidden layer iss(t) = f(Uw(t) + Ws(t ?
1)), where f is a sig-moid function.Because the inputs of the hidden layer consist ofthe word w(t) and the previous hidden layer outputs(t ?
1), the current context of the RNN is influ-enced by the current word and the previous context.Therefore, we can regard that the continuous repre-sentations from the RNNLM exploit the context im-plicitly considering the word sequence information(Mikolov et al 2010).V is a N by M matrix representing the connec-tions between the hidden layer and the output layer.The final output is y(t) = g(V s(t)), where g is asoftmax function to represent the probability distri-bution over all the words in the vocabulary.When the RNN is trained by the back propagationalgorithm, we can regard the ith column vector of Uas the continuous representation of the ith word inthe vocabulary since the column was adjusted corre-spondingly to the ith element of w(t).
Because thes(t) outputs of two input words will be similar whenthey have similar s(t?
1) values, the correspondingcolumn vectors of the words will also be similar.Mikolov et al(2013) showed that constant vectoroffsets of word pairs can represent linguistic regu-larities.
Let wa and wb denote the vectors for thewords a and b, respectively.
Then the vector offsetof the word pair is wa ?
wb.
If a and b are syn-tactically or semantically related, the vector offsetcan be interpreted as a transformation of the syn-tactic form or the meaning.
The offset can also beadded to another word vector c. The word vectornearest to wa ?
wb + wc would be related to word cwith the syntactic or semantic difference as the dif-ference between a and b, as it is the case for theking, man, and woman example, where king ?
man+ woman would approximately represent king withfeminine gender (i.e., queen).
They also tried to usethe continuous representations generated by LatentSemantic Analysis (LSA) (Landauer et al 1998).However, the results using LSA were worse becauseLSA is a bag-of-words model, in which it is difficultto exploit word sequence information as the context.For all the experiments in this paper, we use theprecomputed word representations generated by theRNNLM from (Mikolov et al 2013).
Their RNN istrained with 320M words from the Broadcast Newsdata (the vocabulary size is 82,390 words), and weused word vectors with a dimensionality of 1,600(the highest dimensionality provided).1 We stan-dardized the dataset so that the mean and the vari-ance of the representations are 0 and 1, respec-tively.23 Deriving adjectival scalesHere we explore further the interpretation of wordvectors.
Assuming that the transformation of formor meaning represented by the vector offset is lin-ear, an intermediate vector between two word vec-tors would represent some ?middle?
form or mean-ing.
For example, given the positive and superlativeforms of an adjective (e.g., good and best), we ex-pect that the word representation in the middle of1We also experimented with smaller dimensions, but con-sistent with the analyses in (Mikolov et al 2013), the highestdimensionality gave better results.2http://www.fit.vutbr.cz/?imikolov/rnnlm/word_projections-1600.txt.gz1626Input words Words with highest cosine similarities to the mean vectorgood:best better: 0.738 strong: 0.644 normal: 0.619 less: 0.609bad:worst terrible: 0.726 great: 0.678 horrible: 0.674 worse: 0.665slow:slowest slower: 0.637 sluggish: 0.614 steady: 0.558 brisk: 0.543fast:fastest faster: 0.645 slower: 0.602 quicker: 0.542 harder: 0.518Table 1: Words with corresponding vectors closest to the mean of positive:superlative word vectors.First word (-) 1st quarter Half 3rd quarter Second word (+)furious 1 angry 0.632 unhappy 0.640 pleased 0.516 happy 1furious 1 angry 0.615 tense 0.465 quiet 0.560 calm 1terrible 1 horrible 0.783 incredible 0.714 wonderful 0.772 terrific 1cold 1 mild 0.348 warm 0.517 sticky 0.424 hot 1ugly 1 nasty 0.672 wacky 0.645 lovely 0.715 gorgeous 1Table 2: Adjectival scales extracted from the RNN: each row represent a scale, and for each intermediate point theclosest word in term of cosine similarity is given.them will correspond to the comparative form (i.e.,better).
To extract the ?middle?
word between twoword vectors wa and wb, we take the vector offsetwa?wb divided by 2, and addwb: wb+(wa?wb)/2.The result corresponds to the midpoint between thetwo words.
Then, we find the word whose cosinesimilarity to the midpoint is the highest.Table 1 gives some positive:superlative pairs andthe top four closest words to the mean vectors, wherethe distance metric is the cosine similarity.
Thecorrect comparative forms (in bold) are quite closeto the mean vector of the positive and superlativeform vectors, highlighting the fact that there is somemeaningful interpretation of the vector space: theword vectors are constituting a scale.We can extend this idea of extracting an or-dering between two words.
For any two seman-tically related adjectives, intermediate vectors ex-tracted along the line connecting the first and sec-ond word vectors should exhibit scalar properties, asseen above for the positive-comparative-superlativetriplets.
If we take two antonyms (furious andhappy), words extracted at the intermediate pointsx1, x2 and x3 should correspond to words lying ona scale of happiness (from ?less furious?
to ?morehappy?
), as illustrated in Figure 2.
Table 2 givessome adjectival scales that we extracted from thecontinuous word space, using antonym pairs.
Wepicked three points with equal intervals on the linefrom the first to the second word (1st quarter, halfand 3rd quarter).
The extracted scales look quitereasonable: the words form a continuum from morenegative to more positive meanings.x2a=furiousb=happyx1x3angryunhappypleasedFigure 2: An example of vectors with the highest cosinesimilarity to intermediate points on the line between furi-ous and happy.Tables 1 and 2 demonstrate that the word vectorspace is interpretable: intermediate vectors betweentwo word vectors represent a semantic continuum.4 Evaluation: Indirect answers to yes/noquestionsTo evaluate the quality of the adjective scales learnedby the neural network approach, we use the cor-pus of indirect answers to yes/no questions createdby (de Marneffe et al 2010), which consists ofquestion-answer pairs involving gradable modifiersto test scalar implicatures.
We focus on the 125 pairsin the corpus where both the question and answercontain an adjective: e.g., Is Obama qualified?
Ithink he?s young.3 Each question-answer pair has3These 125 pairs correspond to the ?Other adjective?
cate-gory in (de Marneffe et al 2010).1627been annotated via Mechanical Turk for whether theanswer conveys yes, no or uncertain.4.1 MethodThe previous section showed that we can draw a linepassing through an adjective and its antonym andthat the words extracted along the line are roughlysemantically ordered.
To infer a yes or no answer inthe case of the IQAP corpus, we use the followingapproach illustrated with the Obama example above(Figure 3).
Using WordNet 3.1 (Fellbaum, 1998),we look for an antonym of the adjective in the ques-tion qualified: unqualified is retrieved.
Since thescales extracted are only roughly ordered, to inferyes when the question and answer words are veryclose, we set the decision boundary perpendicularto the line connecting the two words and passingthrough the midpoint of the line.Since the answer word is young, we checkwhether young is in the area including qualified orin the other area.
We infer a yes answer in the for-mer case, and a no answer in the latter case.
If youngis on the boundary, we infer uncertain.
If a sentencecontains a negation (e.g., Are you stressed?
I amnot peaceful.
), we compute the scale for stressed-peaceful and then reverse the answer obtained, sim-ilarly to what is done in (de Marneffe et al 2010).qualifiedunqualifiedyoungFigure 3: An example of the decision boundary givenqualified as the question and young as the answer.Since a word can have multiple senses and differ-ent antonyms for the senses, it is important to selectthe most appropriate antonym to build a more accu-rate decision boundary.
We consider all antonymsacross senses4 and select the antonym that is mostcollinear with the question and the answer.
For theword vectors of the question wq, the ith antonymwanti , and the answer wa, we select anti whereargmaxanti |cos(wq ?
wa, wq ?
wanti)|.
Figure 4schematically shows antonym selection when the4Antonyms in WordNet can be directly opposed to a givenword or indirectly opposed via other words.
When there aredirect antonyms for the question word, we only consider those.MacroAcc P R F1de Marneffe (2010) 60.00 59.72 59.40 59.56Mohtarami (2011) ?
62.23 60.88 61.55RNN model 72.80 69.78 71.39 70.58Table 3: Score (%) comparison on the 125 scalar adjec-tive pairs in the IQAP corpus.question is good and the answer is excellent: badand evil are the antonym candidates of good.Because the absolute cosine similarity of good-excellent to good-bad is higher than to good-evil, wechoose bad as the antonym in this case.badexcellentgoodevilFigure 4: An example of antonym selection.4.2 Results and discussionTable 3 compares our results with previous oneswhere adjectival scales are considered: de Marn-effe et al(2010) propose an unsupervised approachwhere scales are learned from distributional infor-mation in a Web corpus; Mohtarami et al(2011)?smodel is similar to ours but uses word represen-tations obtained by LSA and a word sense disam-biguation system (Zhong and Ng, 2010) to chooseantonyms.
To compare with Mohtarami et al(2011), we use macro-averaged precision and recallfor yes and no.
For the given metrics, our model sig-nificantly outperforms the previous ones (p < 0.05,McNemar?s test).Mohtarami et al(2011) present higher numbersobtained by replacing the answer words with theirsynonyms in WordNet.
However, that approach failsto capture orderings.
Two words of different degreeare often regarded as synonyms: even though furi-ous means extremely angry, furious and angry aresynonyms in WordNet.
Therefore using synonyms,the system will output the same answer irrespectiveof the order in the pair.
Mohtarami et al(2012)also presented results on the interpretation of indi-rect questions on the IQAP corpus, but their method1628-20 -15 -10 -5 0 5 10 15 20-25-20-15-10-505101520goodbadterrible confidentdiffidentsurehappyunhappydelightedqualifiedunqualifiedyoungdim 1dim2Figure 5: Question words (bold), their antonyms (italic), and answer words (normal) of four pairs from the IQAPdataset.
The words are visualized by MDS.did not involve learning or using scalar implicatures.Figure 5 gives a qualitative picture: the questionwords, antonyms and answer words for four of theIQAP pairs are visualized in 2D space by multi-dimensional scaling (MDS).
Note that MDS intro-duces some distortion in the lower dimensions.
Bul-let markers correspond to words in the same pair.Question words, antonyms, and answer words aredisplayed by bold, italic, and normal fonts, respec-tively.
In the Obama example previously mentioned(Is Obama qualified?
I think he?s young.
), the ques-tion word is qualified and the answer word is young.In Figure 5, qualified is around (2,-20) while itsantonym unqualified is around (-6,-24).
Since youngis around (-7,-8), we infer that young is semanti-cally closer to unqualified which corroborates withthe Turkers?
intuitions in this case.
(1), (2) and (3)give the other examples displayed in Figure 5.
(1) A: Do you think she?d be happy with thisbook?B: I think she?d be delighted by it.
(2) A: Do you think that?s a good idea?B: It?s a terrible idea.
(3) A: The president is promising support forAmericans who have suffered from thishurricane.
Are you confident you aregoing to be getting that?B: I?m not so sure about my insurancecompany.In (1), delighted is stronger than happy, leading toa yes answer, whereas in (2), terrible is weaker thangood leading to a no answer.
In (3), the presence ofa negation will reverse the answer inferred, leadingto no.5 ConclusionIn this paper we give further evidence that the rela-tionships in the continuous vector space learned byrecurrent neural network models are interpretable.We show that using vector offsets, we can success-fully learn adjectival scales, which are useful forscalar implicatures, as demonstrated by the high re-sults we obtain on the IQAP corpus.AcknowledgementsWe thank Eric Fosler-Lussier and the anonymous re-viewers for their helpful comments on previous ver-sions of this paper.1629ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference on Machine learn-ing, pages 160?167.Marie-Catherine de Marneffe, Christopher D. Manning,and Christopher Potts.
2010.
Was it good?
It wasprovocative.
Learning the meaning of scalar adjec-tives.
In Proceedings of the 48th Meeting of the Asso-ciation for Computational Linguistics, pages 167?176.Christiane Fellbaum.
1998.
WordNet: An electronic lex-ical database.
MIT Press.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 873?882.Association for Computational Linguistics.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse Processes, 25:259?284.Tomas Mikolov, Martin Karafia?t, Luka?s?
Burget, Jan Cer-nocky, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proceedings ofInterspeech, pages 1045?1048.Tomas Mikolov, Daniel Povey, Luka?s?
Burget, and JanCernocky.
2011.
Strategies for training large scaleneural network language models.
In Proceedings ofASRU, pages 196?201.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL-HLT,pages 746?751.Mitra Mohtarami, Hadi Amiri, Man Lan, and Chew LimTan.
2011.
Predicting the uncertainty of sentimentadjectives in indirect answers.
In Proceedings of the20th ACM international conference on Informationand knowledge management, pages 2485?2488.Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh PhuTran, and Chew Lim Tan.
2012.
Sense sentiment sim-ilarity: an analysis.
In Proceedings of the 26th AAAIConference on Artificial Intelligence, pages 1706?1712.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech & Language, 21(3):492?518.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 384?394.Zhi Zhong and Hwee Tou Ng.
2010.
It makes sense: awide-coverage word sense disambiguation system forfree text.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 78?83.1630
