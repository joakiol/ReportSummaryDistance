Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 786?794,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsChinese Comma Disambiguation for Discourse AnalysisYaqin YangBrandeis University415 South StreetWaltham, MA 02453, USAyaqin@brandeis.eduNianwen XueBrandeis University415 South StreetWaltham, MA 02453, USAxuen@brandeis.eduAbstractThe Chinese comma signals the boundary ofdiscourse units and also anchors discourserelations between adjacent text spans.
Inthis work, we propose a discourse structure-oriented classification of the comma that canbe automatically extracted from the ChineseTreebank based on syntactic patterns.
Wethen experimented with two supervised learn-ing methods that automatically disambiguatethe Chinese comma based on this classifica-tion.
The first method integrates comma clas-sification into parsing, and the second methodadopts a ?post-processing?
approach that ex-tracts features from automatic parses to traina classifier.
The experimental results showthat the second approach compares favorablyagainst the first approach.1 IntroductionThe Chinese comma, which looks graphically verysimilar to its English counterpart, is functionallyquite different.
It has attracted a significant amountof research that studied the problem from the view-point of natural language processing.
For exam-ple, Jin et al( 2004) and Li et al( 2005) viewthe disambiguation of the Chinese comma as a wayof breaking up long Chinese sentences into shorterones to facilitate parsing.
The idea is to split along sentence into multiple comma-separated seg-ments, parse them individually, and reconstruct thesyntactic parse for the original sentence.
Althoughboth studies show a positive impact of this approach,comma disambiguation is viewed merely as a con-venient tool to help achieve a more important goal.Xue and Yang ( 2011) point out that the very rea-son for the existence of these long Chinese sentencesis because the Chinese comma is ambiguous and insome context, it identifies the boundary of a sentencejust as a period, a question mark, or an exclamationmark does.
The disambiguation of comma is viewedas a necessary step to detect sentence boundaries inChinese and it can benefit a whole range of down-stream NLP applications such as syntactic parsingand Machine Translation.
In Machine Translation,for example, it is very typical for ?one?
Chinesesentence to be translated into multiple English sen-tences, with each comma-separated segment corre-sponding to one English sentence.
In the presentwork, we expand this view and propose to look atthe Chinese comma in the context of discourse anal-ysis.
The Chinese comma is viewed as a delimiterof elementary discourse units (EDUs), in the senseof the Rhetorical Structure Theory (Carlson et al,2002; Mann et al, 1988).
It is also considered tobe the anchor of discourse relations, in the sense ofthe Penn Discourse Treebank (PDT) (Prasad et al,2008).
Disambiguating the comma is thus necessaryfor the purpose of discourse segmentation, the iden-tification of EDUs, a first step in building up the dis-course structure of a Chinese text.Developing a supervised or semi-supervisedmodel of discourse segmentation would requireground truth annotated based on a well-establishedrepresentation scheme, but as of right now no suchannotation exists for Chinese to the best of ourknowledge.
However, syntactically annotated tree-banks often contain important clues that can be usedto infer discourse-level information.
We present786a method of automatically deriving a preliminaryform of discourse structure anchored by the Chinesecomma from the Penn Chinese Treebank (CTB)(Xue et al, 2005), and using this information totrain and test supervised models.
This discourseinformation is formalized as a classification of theChinese comma, with each class representing theboundary of an elementary discourse unit as wellas the anchor of a coarse-grained discourse rela-tion between the two discourse units that it delimits.We then develop two comma classification methods.In the first method, we replace the part-of-speech(POS) tag of each comma in the CTB with a de-rived discourse category and retrain a state-of-the-art Chinese parser on the relabeled data.
We thenevaluate how accurately the commas are classifiedin the parsing process.
In the second method, weparse these sentences and extract lexical and syn-tactic information as features to predict these newdiscourse categories.
The second approach gives usmore control over what features to extract and ourresults show that it compares favorably against thefirst approach.The rest of the paper is organized as follows.
InSection 2, we present our approach to automati-cally extract discourse information from a syntac-tically annotated treebank and present our classifi-cation scheme.
In Section 3, we describe our su-pervised learning methods and the features we ex-tracted.
Section 4 presents our experiment setup andexperimental results.
Related work is reviewed inSection 5.
We conclude in Section 6.2 Chinese comma classificationThere are many ways to conceptualize the discoursestructure of a text (Mann et al, 1988; Prasad etal., 2008), but there is more of a consensus amongresearchers about the fundamental building blocksof the discourse structure.
For the Rhetorical Dis-course Theory, the building blocks are ElementaryDiscourse Units (EDUs).
For the PDT, the build-ing blocks are abstract objects such as propositions,facts.
Although they are phrased in different ways,syntactically these discourse units are generally re-alized as clauses or built on top of clauses.
So thefirst step in building the discourse structure of a textis to identify these discourse units.In Chinese, these elementary discourse units aregenerally delimited by the comma, but not all com-mas mark the boundaries of a discourse unit.
In (1),for example, Comma [1] marks the boundary of adiscourse unit while Comma [2] does not.
This isreflected in its English translation: while the firstcomma corresponds to an English comma, the sec-ond comma is not translated at all, as it marks theboundary between a subject and its predicate, whereno comma is needed in English.
Disambiguatingthese two types of commas is thus an important firststep in identifying elementary discourse units andbuilding up the discourse structure of a text.
(1) ?
?Wang Xiang?although?age?over??50?[1],?but?his??abundant?DE??energy?and??quick?DE??thinking?[2],?give?people?one?CL???challenger?DE??impression?.
?Although Wang Xiang is over 50 years old, hisabundant energy and quick thinking leave peo-ple the impression of a challenger.
?Although to the best of our knowledge, no suchdiscourse segmented data for Chinese exists in thepublic domain, this information can be extractedfrom the syntactic annotation of the CTB.
In thesyntactic annotation of the sentence, illustrated in(a), it is clear that while the first comma in the sen-tence marks the boundary of a clause, the secondone marks the demarcation between the subject NPand the predicate VP and thus is not an indicator ofa discourse boundary.
(a)IPIP-CND, 1ADVP NP , 2 VPIn addition to a binary distinction of whether acomma marks the boundary of a discourse unit,the CTB annotation also allows the extraction of amore elaborate classification of commas based oncoordination and subordination relations of comma-separated clauses.
This classification of the Chinese787comma can be viewed as a first approximation of thediscourse relations anchored by the comma that canbe refined later via a manual annotation process.Based on the syntactic annotation in the CTB, weclassify the Chinese comma into seven hierarchi-cally organized categories, as illustrated in Figure1.
The first distinction is made between commasthat indicate a discourse boundary (RELATION)and those that do not (OTHER).
Commas that in-dicate discourse boundaries are further divided intocommas that separate coordinated discourse units(COORD) vs commas that separate discourse unitsin a subordination relation (SUBORD).
Based onthe levels of embedding and the syntactic categoryof the coordinated structures, we define three dif-ferent types of coordination (SB, IP COORD andVP COORD).
We also define three types of subordi-nation relations (ADJ, COMP, Sent SBJ), based onthe syntactic structure.
As we will show below, eachof the six relations has a clear syntactic pattern thatcan be exploited for their automatic detection.ALLOTHERRELATIONSB COORD_IP COORD_VP ADJ COMP Sent_SBJCOORD SUBORDFigure 1: Comma classificationSentence Boundary (SB): Following (Xue andYang, 2011), we consider the loosely coordinatedIPs that are the immediate children of the root IP tobe independent sentences, and the commas separat-ing them to be delimiters of sentence boundary.
Thisis illustrated in (2), where a Chinese sentence can besplit into two independent shorter sentences at thecomma.
We view this comma to be a marker of thesentence boundary and it serves the same function asthe unambiguous sentence boundary delimitors (pe-riods, question marks, exclamation marks) in Chi-nese.
The syntactic pattern that is used to infer thisrelation is illustrated in (b).
(2) ??
?Guangdong province??establish?ASP??natural??science??foundation?[3],?
?every year??investment?at?
?one hundred millioin?yuan??above?.
?Natural Science Foundation is established inGuangdong Province.
More than one hundredmillion yuan is invested every year.?
(b) IP-RootIPClause, IPClauseIP Coordination (IP COORD): Coordinated IPsthat are not the immediate children of the root IP arealso considered to be discourse units and the com-mas linking them are labeled IP COORD.
Differentfrom the sentence boundary cases, these coordinatedIPs are often embedded in a larger structure.
An ex-ample is given in (3) and its typical syntactic patternis illustrated in (c).
(3) ?According to??
?Lu Renfa??presentation?[4],?
?the whole country??revenue??goal?already?
?exceeding quota??complete?[5],??overall??situation??fairly?
?good .
?According to Lu Renfa, the national revenuegoal is met and exceeded, and the overall situa-tion is fairly good.?
(c) IPPPModifier, IPIPConjunct, IPConjunctVP Coordination (VP COORD): CoordinatedVPs, when separated by the comma, are not seman-tically different from coordinated IPs.
The only dif-ference is that in the latter case, the coordinated VPs788share a subject, while coordinated IPs tend to havedifferent subjects.
Maintaining this distinction allowus to model subject (dis)continuity, which helps re-cover a subject when it is dropped, a prevalent phe-nomenon in Chinese.
As shown in (4), the VPs in thetext spans separated by Comma [6] have the samesubject, thus the subject in the second VP is dropped.The syntactic pattern that allows us to extract thisstructure is given in (d).
(4) ??China??Bank?is?
?four major??state-owned??commercial??bank?
?one of these?[6],?also?is??China?DE??major?
?foreign exchange??bank?.
?Bank of China is one of the four major state-owned commercial banks, and it is also China?smajor foreign exchange bank.?
(d) IPNPSubjectVPVPConjunct, VPConjunctAdjunction (ADJ): Adjunction is one of threetypes of subordination relations we define.
It holdsbetween a subordinate clause and its main clause.The subordinate clause is normally introduced by asubordinating conjunction and it typically providesthe cause, purpose, manner, or condition for themain clause.
In the PDT terms, these subordinateconjunctions are discourse connectives that anchora discourse relation between the subordinate clauseand the main clause.
In Chinese, with few excep-tions, the subordinate clause comes before the mainclause.
(5) is an example of this relation.
(5) ?if??project??happen??insurance??liability??scope?inside?DE??natural??disaster?[7],?
?China Insurance??property??insurance?
?company?will?according to??provision??excecute??compensation?.
?If natural disasters within the scope of the in-surance liability happen in the project, PICCProperty Insurance Company will providecompensations according to the provisions.?
(e) IPCP/IP-CNDSubordinate Clause,Main Clause(e) shows how (5) is represented in the syntac-tic structure in the CTB.
Extracting this relation re-quires more than just the syntactic configuration be-tween these two clauses.
We also take advantageof the functional (dash) tags provided in the tree-bank.
The functional tags are attached to the sub-ordinate clause and they include CND (conditional),PRP (purpose or reason), MNR (manner), or ADV(other types of subordinate clauses that are adjunctsto the main clause).Complementation (COMP): When a commaseparates a verb governor and its complementclause, this verb and its subject generally describethe attribution of the complement clause.
Attribu-tion is an important notion in discourse analysis inboth the RST framework and in the PDT.
An exam-ple of this is given in (6), and the syntactic patternused to extract this relation is illustrated in (f).
(6) ?The??company??present?[8],?at??future?DE?
?five year?within??they?will??additionally??invest??
?ninety million??U.S.
dollars?[9],??estimate??
?annual output?will?reach?
?three hundred million??U.S.
dollars?.
?According to the the company?s presentation,they will invest an additional ninety million789U.S.
dollars in the next five years, and the esti-mated annual output will reach $ 300 million.?
(f) IP....VPVV , IP......Sentential Subject (SBJ): This category is forcommas that separate a sentential subject from itspredicate VP.
An example is given in (7) and thesyntactic pattern used to extract this relation is il-lustrated in (g).
(7) ??export??rapid??grow?[10],??become??promote??economy??growth?DE??important??force?.
?The rapid growth of export becomes an impor-tant force in promoting economic growth.?
(g) IPIP-SBJSentential Subject, VP......Others (OTHER): The remaining cases ofcomma receive the OTHER label, indicating they donot mark the boundary of a discourse segment.Our proposed comma classification schemeserves the dual purpose of identifying elementarydiscourse units and at the same time detectingcoarse-grained discourse relations anchored by thecomma.
The discourse relations identified in thismanner by no means constitute the full discourseanalysis of a text, they are, however, a good firstapproximation.
The advantage of our approach isthat we do not require manual discourse annotations,and all the information we need is automatically ex-tracted from the syntactic annotation of the CTBand attached to instances of the comma in the cor-pus.
This makes it possible for us to train supervisedmodels to automatically classify the commas in anyChinese text.3 Two comma classification methodsGiven the gold standard parses, based on the syntac-tic patterns described in Section 2, we can map thePOS tag of each comma instance in the CTB to oneof the seven classes described in Section 2.
Usingthis relabeled data as training data, we experimentedwith two automatic comma disambiguation meth-ods.
In the first method, we simply retrained theBerkeley parser (Petrov and Klein, 2007) on the re-labeled data and computed how accurately the com-mas are labeled in a held-out test set.
In the secondmethod, we trained a Maximum Entropy classifierwith the Mallet (McCallum et al, 2002) machinelearning package to classify the commas.
The fea-tures are extracted from the CTB data automaticallyparsed with the Berkeley parser.
We implementedfeatures described in (Xue and Yang, 2011), andalso experimented with a set of new features as fol-lows.
In general, these new features are extractedfrom the two text spans surrounding the comma.Given a comma, we define the preceding text span asi span and the following text span as j span.
We alsocollected a number of subject-predicate pairs from alarge corpus that doesn?t overlap with the CTB.
Werefer to this corpus as the auxiliary corpus.Subject and Predicate features: We exploredvarious combinations of the subject (sbj), predicate(pred) and object (obj) of the two spans.
The sub-ject of i span is represented as sbji, etc.1.
The existence of sbji, sbjj , both, or neither.2.
The lemma of predi, the lemma of predj , theconjunction of sbji and predj , the conjunctionof predi and sbjj3.
whether the conjunction of sbji and predj oc-curs more than 2 times in the auxiliary corpuswhen j does not have a subject.4.
whether the conjunction of obji and predj oc-curs more than 2 times in the auxiliary corpuswhen j does not have a subject5.
Whether the conjunction of predi and sbjj oc-curs more than 2 times in the auxiliary corpuswhen i does not have a subject.Mutual Information features: Mutual informa-tion is intended to capture the association strengthbetween the subject of a previous span and the predi-cate of the current span.
We use Mutual Information790(Church and Hanks, 1989) as shown in Equation(1) and the frequency count computed based on theauxiliary corpus to measure such constraints.MI = log2# co-occur of S and P * corpus size# S occur * # P occur(1)1.
The conjunction of sbji and predj when j doesnot have a subject if their MIvalue is greaterthan -8.0, an empirically established threshold.2.
Whether obji and predj has an MI valuegreater than 5.0 if j does not have a subject.3.
Whether the MI value of sbji and predj isgreater than 0.0, and they occur 2 times in theauxiliary corpus when j doesn?t have a subject.4.
Whether the MI value of obji and predj isgreater than 0.0 and they occur 2 times in theauxiliary corpus when j doesn?t have a subject.5.
Whether the MI value of predi and sbjj isgreater than 0.0 and they occur more than 2times in the auxiliary corpus when i does nothave a subject.Span features: We used span features to cap-ture syntactic information, e.g.
the comma separatedspans are constituents in Tree (b) but not in Tree (d).1.
Whether i forms a single constituent, whetherj forms a single constituent.2.
The conjunction and hierarchical relation of allconstituent labels in i/j, if i/j does not forma single constituent.
The conjunction of allconstituent labels in both spans, if neither spanform a single constituent.Lexical features:1.
The first word in i if it is an adverb, the firstword in j if it is an adverb.2.
The first word in i span if it is a coordinatingconjunction, the first word in j if it is a coordi-nating conjunction.4 Experiments4.1 DatasetsWe use the CTB 6.0 in our experiments and divideit into training, development and test sets using thedata split recommended in the CTB 6.0 documenta-tion, as shown in Table 1.
There are 5436 commasin the test set, including 1327 commas that are sen-tence boundaries (SB), 539 commas that connect co-ordinated IPs (IP COORD), 1173 commas that joincoordinated VPs (VP COORD), 379 commas thatdelimits a subordinate clause and its main clause(ADJ), 314 commas that anchor complementationrelations (COMP), and 1625 commas that belong tothe OTHER category.4.2 ResultsAs mentioned in Section 3, we experimented withtwo comma classification methods.
In the firstmethod, we replace the part-of-speech (POS) tags ofthe commas with the seven classes defined in Sec-tion 2.
We then retrain the Berkeley parser (Petrovand Klein, 2007) using the training set as presentedin Table 1, parse the test set, and evaluate the commaclassification accuracy.In the second method, we use the relabeled com-mas as the gold-standard data to train a supervisedclassifier to automatically classify the commas.
Asshown in the previous section, syntactic structuresare an important source of information for our clas-sifier.
For feature extraction purposes, the entireCTB6.0 is automatically parsed in a round-robinfashion.
We divided CTB 6.0 into 10 portions,and parsed each portion with a model trained onother portions, using the Berkeley parser (Petrov andKlein, 2007).
Measured by the ParsEval metric(Black et al, 1991), the parsing accuracy on theCTB test set stands at 83.29% (F-score), with a pre-cision of 85.18% and a recall of 81.49%.The results are presented in Table 2, which showsthe overall accuracy of the two methods as well asthe results for each individual category.
As shouldbe clear from Table 2, the results for the two meth-ods are very comparable, with the second methodperforming modestly better than the first method.4.2.1 Subject continuityOne of the goals for this classification scheme isto model subject continuity, which answers the ques-tion of how accurately we can predict whether twocomma-separated text spans have the same subjector different subjects.
When the two spans sharethe same subject, the comma belongs to the cate-gory VP COORD.
When they have different sub-jects, they belong to the categories IP COORD or791Data Train Dev TestCTB-6.081-325, 400-454, 500-554 41-80 (1-40,901-931 newswire)590-596, 600-885, 900 1120-1129 (1018, 1020, 1036, 10441001-1017, 1019, 1021-1035 2140-2159 1060-1061,1037-1043, 1045-1059,1062-1071 2280-2294 1072, 1118-1119, 11321073-1078, 1100-1117, 1130-1131 2550-2569 1141-1142, 1148 magazine)1133-1140, 1143-1147, 1149-1151 2775-2799 (2165-2180, 2295-23102000-2139, 2160-2164, 2181-2279 3080-3109 2570-2602, 2800-28192311-2549, 2603-2774, 2820-3079 3110-3145 broadcast news)Table 1: CTB 6.0 data set division.SB.
When this question is meaningless, e.g., whenone of the span does not even have a subject, thecomma belongs to other categories.
To evaluate theperformance of our model on this problem, we re-computed the results by putting IP COORD and SBin one category, putting VP COORD in another cat-egory and the rest of the labels in a third category.The results are presented in Table 3.4.2.2 The effect of genreCTB 6.0 consists of data from three different gen-res, including newswire, magazine and broadcastnews.
Data genres may have very different char-acteristics.
To evaluate how our model works ondifferent genres, we train a model using trainingand development sets, and test the model on differ-ent genres as described in Table 1.
The results onthese three genres are presented in Table 4, and theyshows a significant fluctuation across genres.
Ourmodel works the best on newswire, but not as goodon broadcast news and magazine articles.4.2.3 Comparison with prior work(Xue and Yang, 2011) presented results on abinary classification of whether or not a commamarks a sentence boundary, while the present workaddresses a multi-category classification problemaimed at identifying discourse segments and prelim-inary discourse relations anchored by the comma.However, since we also have a SB category, com-parison is possible.
For comparison purposes, weretrained our model on their data sets, and computedthe results of SB vs other categories.
The results areshown in Table 5.
Our results are very comparablewith (Xue and Yang, 2011) despite that we are per-forming a multicategory classification.4.3 Error analysisEven though our feature-based approach can the-oretically ?correct?
parsing errors, meaning that acomma can in theory be classified correctly even if asentence is incorrectly parsed, when examining thesystem output, errors in automatic parses often leadto errors in comma classification.
A common pars-ing error is the confusion between Structures (h) and(i).
If the subject of the text span after a comma isdropped as shown in (h), the parser often producesa VP coordination structure as shown in (i) and viceversa.
This kind of parsing errors would lead to er-rors in our syntactic features and thus directly affectthe accuracy of our model.
(h) IPIPNP VP, IPVP(i) IPNP VPVP , VP5 Related WorkThere is a large body of work on discourse analysisin the field of Natural Language Processing.
Most ofthe work, however, are on English.
An unsupervisedapproach was proposed to recognize discourse rela-tions in (Marcu and Echihabi, 2002), which extractsdiscourse relations that hold between arbitrary spansof text making use of cue phrases.
Like the presentwork, a lot of research on discourse analysis is car-ried out at the sentence level.
(Soricut and Marcu,2003; Sporleder and Lapata, 2005; Polanyi et al,2004).
(Soricut and Marcu, 2003) and (Polanyi etal., 2004) implement models to perform discourseparsing, while (Sporleder and Lapata, 2005) intro-duces discourse chunking as an alternative to full-792Class Metric Method 1 Method 2all acc.
(%) 71.5 72.9SBPrec.
(%) 65.6 66.2Rec.
(%) 71.7 73.1F.
(%) 68.5 69.5IP COORDPrec.
(%) 53.3 56.0Rec.
(%) 50.5 48.6F.
(%) 52.0 52.0VP CoordPrec.
(%) 65.6 68.3Rec.
(%) 76.3 78.2F.
(%) 70.5 72.9ADJPrec.
(%) 66.9 66.8Rec.
(%) 29.3 37.7F.
(%) 40.8 48.2CompPrec.
(%) 88.3 91.2Rec.
(%) 93.9 92.4F.
(%) 91.0 91.8SentSBJPrec.
(%) 25.0 31.8Rec.
(%) 6 10F.
(%) 9.7 15.6OtherPrec.
(%) 86.9 85.6Rec.
(%) 83.4 84.1F.
(%) 85.1 84.8Table 2: Overall accuracy of the two methods as well asthe results for each individual category.scale discourse parsing.The emergence of linguistic corpora annotatedwith discourse structure such as the RST DiscourseTreebank (Carlson et al, 2002) and PDT (Miltsakakiet al, 2004; Prasad et al, 2008) have changed thelandscape of discourse analysis.
More robust, data-driven models are starting to emerge.Compared with English, much less work hasbeen done in Chinese discourse analysis, presum-ably due to the lack of discourse resources in Chi-nese.
(Huang and Chen, 2011) constructs a smallcorpus following the PDT annotation scheme andPrec.
(%) Rec.
(%) F. (%)VP COORD 68.3 78.2 72.9IP COORD+SB 76.0 78.7 77.3Other 89.0 80.2 84.4Table 3: Subject continuity results based on MaximumEntropy modelGenre NW BN MZAccuracy.
(%) 79.1 73.6 67.7Table 4: Results on different genres based on MaximumEntropy modelXue and Yang our model(%) p r f1 p r f1Overall 89.2 88.7EOS 64.7 76.4 70.1 63.0 77.9 69.7NEOS 95.1 91.7 93.4 95.3 90.8 93.0Table 5: Comparison of (Xue and Yang, 2011) and thepresent work based on Maximum Entropy modeltrains a statistical classifier to recognize discourserelations.
Their work, however, is only concernedwith discourse relations between adjacent sentences,thus side-stepping the hard problem of disambiguat-ing the Chinese comma and analyzing intra-sentencediscourse relations.
To the best of our knowledge,our work is the first in attempting to disambiguatingthe Chinese comma as the first step in performingChinese discourse analysis.6 Conclusions and future workWe proposed a approach to disambiguate the Chi-nese comma as a first step toward discourse analy-sis.
Training and testing data are automatically de-rived from a syntactically annotated corpus.
We pre-sented two automatic comma disambiguation meth-ods that perform comparably.
In the first method,comma disambiguation is integrated into the parsingprocess while in the second method we train a super-vised classifier to classify the Chinese comma, us-ing features extracted from automatic parses.
Muchneeds to be done in the area, but we believe our workprovides insight into the intricacy and complexity ofdiscourse analysis in Chinese.AcknowledgmentThis work is supported by the IIS Division of Na-tional Science Foundation via Grant No.
0910532entitled ?Richer Representations for MachineTranslation?.
All views expressed in this paper arethose of the authors and do not necessarily representthe view of the National Science Foundation.793ReferencesL Carlson, D Marcu, M E Okurowski.
2002.
RST Dis-course Treebank.
Linguistic Data Consortium 2002.Caroline Sporleder, Mirella Lapata.
2005.
Discoursechunking and its application to sentence compression.In Proceedings of HLT/EMNLP 2005.Livia Polanyi, Chris Culy, Martin Van Den Berg, GianLorenzo Thione and David Ahn.
2004.
Sententialstructure and discourse parsing.
In Proceeedings ofthe ACL 2004 Workshop on Discourse Annotation2004.Hen-Hsen Huang and Hsin-Hsi Chen.
2011.
ChineseDiscourse Relation Recognition.
In Proceedings ofthe 5th International Joint Conference on Natural Lan-guage Processing 2011,pages 1442-1446.Daniel Marcu and Abdessamad Echihabi.
2002.
An Un-supervised Approach to Recognizing Discourse Rela-tions.
In Proceedings of the ACL, July 6-12, 2002,Philadelphia, PA, USA.Radu Soricut and Daniel Marcu.
2003.
Sentence LevelDiscourse Parsing using Syntactic and Lexical Infor-mation.
In Proceedings of the ACL 2003.Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi andBon-nie Webber.
2004.
The Penn Discourse Treebank.
InProceedings of LREC 2004.Nianwen Xue and Yaqin Yang.
2011.
Chinese sentencesegmentation as comma classification.
In Proceedingsof ACL 2011.Nianwen Xue, Fei Xia, Fu-Dong Chiou and MarthaPalmer.
2005.
The Penn Chinese Treebank: PhraseStructure Annotation of a Large Corpus.
Natural Lan-guage Engineering, 11(2):207-238.Slav Petrov and Dan Klein.
2007.
Improved Inferenc-ing for Unlexicalized Parsing.
In Proceedings of HLT-NAACL 2007.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos, B.Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitively comparing the syntactic coverage ofEnglish grammars.
In Proceedings of the DARPASpeech and Natural Language Workshop, pages 306-311.Mann, William C. and Sandra A. Thompson.
1988.Rhetorical Structure Theory: Toward a functional the-ory of text organization.
Text 8 (3): 243-281.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0..In Proceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC 2008).Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-Hyeok Lee.
2004.
Segmentation of Chinese LongSentences Using Commas.
In Proceedings of theSIGHANN Workshop on Chinese Language Process-ing.Xing Li, Chengqing Zong, and Rile Hu.
2005.
A Hier-archical Parsing Approach with Punctuation Process-ing for Long Sentence Sentences.
In Proceedings ofthe Second International Joint Conference on NaturalLanguage Processing: Companion Volume includingPosters/Demos and Tutorial Abstracts.Andrew Kachites McCallum.
2002.
MALLET:A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu.Church, K., and Hanks, P. 1989.
Word AssociationNorms, Mutual Information and Lexicography.
As-sociation for Computational Linguistics, Vancouver ,Canada794
