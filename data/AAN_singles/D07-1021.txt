Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
199?207, Prague, June 2007. c?2007 Association for Computational LinguisticsCompressing Trigram Language Models With Golomb CodingKen ChurchMicrosoftOne Microsoft WayRedmond, WA, USATed HartMicrosoftOne Microsoft WayRedmond, WA, USAJianfeng GaoMicrosoftOne Microsoft WayRedmond, WA, USA{church,tedhar,jfgao}@microsoft.comAbstractTrigram language models are compressedusing a Golomb coding method inspired bythe original Unix spell program.Compression methods trade off space, timeand accuracy (loss).
The proposedHashTBO method optimizes space at theexpense of time and accuracy.
Trigramlanguage models are normally consideredmemory hogs, but with HashTBO, it ispossible to squeeze a trigram languagemodel into a few megabytes or less.HashTBO made it possible to ship atrigram contextual speller in MicrosoftOffice 2007.1 IntroductionThis paper will describe two methods of com-pressing trigram language models: HashTBO andZipTBO.
ZipTBO is a baseline compression me-thod that is commonly used in many applicationssuch as the Microsoft IME (Input Method Editor)systems that convert Pinyin to Chinese and Kana toJapanese.Trigram language models have been so success-ful that they are beginning to be rolled out to appli-cations with millions and millions of users: speechrecognition, handwriting recognition, spelling cor-rection, IME, machine translation and more.
TheEMNLP community should be excited to see theirtechnology having so much influence and visibilitywith so many people.
Walter Mossberg of theWall Street Journal called out the contextual spel-ler (the blue squiggles) as one of the most notablefeatures in Office 2007:There are other nice additions.
In Word, Out-look and PowerPoint, there is now contextual spellchecking, which points to a wrong word, even ifthe spelling is in the dictionary.
For example, ifyou type ?their?
instead of ?they're,?
Officecatches the mistake.
It really works.
1The use of contextual language models in spel-ling correction has been discussed elsewhere:(Church and Gale, 1991), (Mays et al 1991), (Ku-kich, 1992) and (Golding and Schabes, 1996).This paper will focus on how to deploy such me-thods to millions and millions of users.
Dependingon the particular application and requirements, weneed to make different tradeoffs among:1.
Space (for compressed language model),2.
Runtime (for n-gram lookup), and3.
Accuracy (losses for n-gram estimates).HashTBO optimizes space at the expense of theother two.
We recommend HashTBO when spaceconcerns dominate the other concerns; otherwise,use ZipTBO.There are many applications where space is ex-tremely tight, especially on cell phones.
HashTBOwas developed for contextual spelling in MicrosoftOffice 2007, where space was the key challenge.The contextual speller probably would not haveshipped without HashTBO compression.We normally think of trigram language modelsas memory hogs, but with HashTBO, a few mega-bytes are more than enough to do interesting thingswith trigrams.
Of course, more memory is alwaysbetter, but it is surprising how much can be donewith so little.For English, the Office contextual speller startedwith a predefined vocabulary of 311k word typesand a corpus of 6 billion word tokens.
(About a1http://online.wsj.com/public/article/SB116786111022966326-T8UUTIl2b10DaW11usf4NasZTYI_20080103.html?mod=tff_main_tff_top199third of the words in the vocabulary do not appearin the corpus.)
The vocabularies for other lan-guages tend to be larger, and the corpora tend to besmaller.
Initially, the trigram language model isvery large.
We prune out small counts (8 or less)to produce a starting point of 51 million trigrams,14 million bigrams and 311k unigrams (for Eng-lish).
With extreme Stolcke, we cut the 51+14+0.3million n-grams down to a couple million.
Using aGolomb code, each n-gram consumes about 3bytes on average.With so much Stolcke pruning and lossy com-pression, there will be losses in precision and re-call.
Our evaluation finds, not surprisingly, thatcompression matters most when space is tight.Although HashTBO outperforms ZipTBO on thespelling task over a wide range of memory sizes,the difference in recall (at 80% precision) is mostnoticeable at the low end (under 10MBs), and leastnoticeable at the high end (over 100 MBs).
Whenthere is plenty of memory (100+ MBs), the differ-ence vanishes, as both methods asymptote to theupper bound (the performance of an uncompressedtrigram language model with unlimited memory).2 PreliminariesBoth methods start with a TBO (trigrams withbackoff) LM (language model) in the standardARPA format.
The ARPA format is used by manytoolkits such as the CMU-Cambridge StatisticalLanguage Modeling Toolkit.22.1 Katz BackoffNo matter how much data we have, we neverhave enough.
Nothing has zero probability.
Wewill see n-grams in the test set that did not appearin the training set.
To deal with this reality, Katz(1987) proposed backing off from trigrams to bi-grams (and from bigrams to unigrams) when wedon?t have enough training data.Backoff doesn?t have to do much for trigramsthat were observed during training.
In that case,the backoff estimate of  ?(??
|???2??
?1)  is simplya discounted probability ??(??|???2??
?1).The discounted probabilities steal from the richand give to the poor.
They take some probabilitymass from the rich n-grams that have been seen intraining and give it to poor unseen n-grams that2 http://www.speech.cs.cmu.edu/SLMmight appear in test.
There are many ways to dis-count probabilities.
Katz used Good-Turingsmoothing, but other smoothing methods such asKneser-Ney are more popular today.Backoff is more interesting for unseen trigrams.In that case, the backoff estimate is:?
???2??
?1 ??(??|??
?1)The backoff alphas (?)
are a normalization fac-tor that accounts for the discounted mass.
That is,?
???2???1=1?
?(??|???2???1)?
?
:?(?
??2?
??1?
?)1?
?(??|???1)?
?
:?(?
??2?
??1?
?
)where ?
???2???1??
> 0  simply says that thetrigram was seen in training data.3 Stolcke PruningBoth ZipTBO and HashTBO start with Stolckepruning (1998).3   We will refer to the trigram lan-guage model after backoff and pruning as a prunedTBO LM.Stolcke pruning looks for n-grams that wouldreceive nearly the same estimates via Katz backoffif they were removed.
In a practical system, therewill never be enough memory to explicitly mate-rialize all n-grams that we encounter during train-ing.
In this work, we need to compress a large setof n-grams (that appear in a large corpus of 6 bil-lion words) down to a relatively small languagemodel of just a couple of megabytes.
We prune asmuch as necessary to make the model fit into thememory allocation (after subsequent Hash-TBO/ZipTBO compression).Pruning saves space by removing n-grams sub-ject to a loss consideration:1.
Select a threshold ?.2.
Compute the performance loss due to prun-ing each trigram and bigram individually us-ing the pruning criterion.3.
Remove all trigrams with performance lossless than ?4.
Remove all bigrams with no child nodes (tri-gram nodes) and with performance loss lessthan ?5.
Re-compute backoff weights.3http://www.nist.gov/speech/publications/darpa98/html/lm20/lm20.htm200Stolcke pruning uses a loss function based onrelative entropy.
Formally, let P denote the tri-gram probabilities assigned by the original un-pruned model, and let P?
denote the probabilities inthe pruned model.
Then the relative entropyD(P||P?)
between the two models is?
?
?,?
[log??
?
?
?
log?(?,?)]?
,?where h is the history.
For trigrams, the history isthe previous two words.
Stolcke showed that thisreduces to??
?
{?(?|?)[log?
?
??
+ log??
?
?
log?(?|?)]+[log??(?)
?
log?(?)]
?
?
??
:?
?
,?
>0}where ??(?)
is the revised backoff weight afterpruning and h?
is the revised history after droppingthe first word.
The summation is over all the tri-grams that were seen in training: ?
?,?
> 0.Stolcke pruning will remove n-grams as neces-sary, minimizing this loss.3.1 Compression on Top of PruningAfter Stolcke pruning, we apply additional com-pression (either ZipTBO or HashTBO).
ZipTBOuses a fairly straightforward data structure, whichintroduces relatively few additional losses on topof the pruned TBO model.
A few small losses areintroduced by quantizing the log likelihoods andthe backoff alphas, but those losses probably don?tmatter much.
More serious losses are introducedby restricting the vocabulary size, V, to the 64kmost-frequent words.
It is convenient to use bytealigned pointers.
The actual vocabulary of morethan 300,000 words for English (and more for oth-er languages) would require 19-bit pointers (ormore) without pruning.
Byte operations are fasterthan bit operations.
There are other implementa-tions of ZipTBO that make different tradeoffs, andallow for larger V without pruning losses.HashTBO is more heroic.
It uses a method in-spired by McIlroy (1982) in the original Unix SpellProgram, which squeezed a word list of N=32,000words into a PDP-11 address space (64k bytes).That was just 2 bytes per word!HashTBO uses similar methods to compress acouple million n-grams into half a dozen mega-bytes, or about 3 bytes per n-gram on average (in-cluding log likelihoods and alphas for backing off).ZipTBO is faster, but takes more space (about 4bytes per n-gram on average, as opposed to 3 bytesper n-gram).
Given a fixed memory budget,ZipTBO has to make up the difference with moreaggressive Stolcke pruning.
More pruning leads tolarger losses, as we will see, for the spelling appli-cation.Losses will be reported in terms of performanceon the spelling task.
It would be nice if lossescould be reported in terms of cross entropy, but thevalues output by the compressed language modelscannot be interpreted as probabilities due to quan-tization losses and other compression losses.4 McIlroy?s Spell ProgramMcIlroy?s spell program started with a hash ta-ble.
Normally, we store the clear text in the hashtable, but he didn?t have space for that, so hedidn?t.
Hash collisions introduce losses.McIlroy then sorted the hash codes and storedjust the interarrivals of the hash codes instead ofthe hash codes themselves.
If the hash codes, h,are distributed by a Poisson process, then the inte-rarrivals, t, are exponentially distributed:Pr ?
= ?????
,where ?
=??.
Recall that the dictionary containsN=32,000 words.
P is the one free parameter, therange of the hash function.
McIlroy hashed wordsinto a large integer mod P, where P is a largeprime that trades off space and accuracy.
Increas-ing P consumes more space, but also reduceslosses (hash collisions).McIlroy used a Golomb (1966) code to store theinterarrivals.
A Golomb code is an optimal Huff-man code for an infinite alphabet of symbols withexponential probabilities.The space requirement (in bits per lexical entry)is close to the entropy of the exponential.?
= ?
Pr ?
log2 Pr ?
????=0?
=1log?
2+  log21?201The ceiling operator ?
?
is introduced becauseHuffman codes use an integer number of bits toencode each symbol.We could get rid of the ceiling operation if wereplaced the Huffman code with an Arithmeticcode, but it is probably not worth the effort.Lookup time is relatively slow.
Technically,lookup time is O(N), because one has to start at thebeginning and add up the interarrivals to recon-struct the hash codes.
McIlroy actually introduceda small table on the side with hash codes and off-sets so one could seek to these offsets and avoidstarting at the beginning every time.
Even so, ourexperiments will show that HashTBO is an orderof magnitude slower than ZipTBO.Accuracy is also an issue.
Fortunately, we don?thave a problem with dropouts.
If a word is in thedictionary, we aren?t going to misplace it.
But twowords in the dictionary could hash to the same val-ue.
In addition, a word that is not in the dictionarycould hash to the same value as a word that is inthe dictionary.
For McIlroy?s application (detect-ing spelling errors), the only concern is the lastpossibility.
McIlroy did what he could do to miti-gate false positive errors by increasing P as muchas he could, subject to the memory constraint (thePDP-11 address space of 64k bytes).We recommend these heroics when space domi-nates other concerns (time and accuracy).5 Golomb CodingGolomb coding takes advantage of the sparsenessin the interarrivals between hash codes.
Let?s startwith a simple recipe.
Let t be an interarrival.
Wewill decompose t into a pair of a quotient (tq) and aremainder (tr).
That is, let ?
= ???
+ ??
where??
= ?
?/ ??
and ??
= ?
mod ?.
We choose m tobe a power of two near ?
??
?2=?2?, whereE[t] is the expected value of the interarrivals, de-fined below.
Store tq in unary and tr in binary.Binary codes are standard, but unary is not.
Toencode a number z in unary, simply write out asequence of z-1 zeros followed by a 1.
Thus, ittakes z bits to encode the number z in unary, asopposed to  log2 ?
bits in binary.This recipe consumes ??
+ log2?
bits.
Thefirst term is for the unary piece and the secondterm is for the binary piece.Why does this recipe make sense?
As men-tioned above, a Golomb code is a Huffman codefor an infinite alphabet with exponential probabili-ties.
We illustrate Huffman codes for infinite al-phabets by starting with a simple example of asmall (very finite) alphabet with just three sym-bols: {a, b, c}.
Assume that half of the time, wesee a, and the rest of the time we see b or c, withequal probabilities:Symbol Code Length PrA 0 1 50%B 10 2 25%C 11 2 25%The Huffman code in the table above can be readoff the binary tree below.
We write out a 0 when-ever we take a left branch and a 1 whenever wetake a right branch.
The Huffman tree is con-structed so that the two branches are equally likely(or at least as close as possible to equally likely).Now, let?s consider an infinite alphabet wherePr ?
=12, Pr ?
=14and the probability of thet+1st symbol is Pr ?
= (1?
?)??
where ?
=12.In this case, we have the following code, which issimply t in unary.
That is, we write out 1?t  zerosfollowed by a 1.Symbol Code Length PrA 1 1 2?1B 01 2 2?2C 001 3 2?3202The Huffman code reduces to unary when theHuffman tree is left branching:In general, ?
need not be ?.
Without loss of ge-nerality, assume Pr ?
=  1 ?
?
??
where12?
?
< 1 and ?
?
0.  ?
depends on E[t], the ex-pected value of the interarrivals:?
?
=??=?1?
??
?
=?
?1 + ?
?Recall that the recipe above calls for expressingt as ?
?
??
+ ??
where ??
= ????
and ??
=?
mod ?.
We encode tq in unary and trin binary.
(The binary piece consumes log2?
bits, since trranges from 0 to m.)How do we pick m?
For convenience, let m bea power of 2.
The unary encoding makes sense asa Huffman code if ??
?12.Thus, a reasonable choice 4  is ?
??
?2.
If?
=?
?1+?
?, then ??
=?
?
?1+?
?
??
1???
?.
Set-ting ??
?12, means ?
??
?2.4 This discussion follows slide 29 ofhttp://www.stanford.edu/class/ee398a/handouts/lectures/01-EntropyLosslessCoding.pdf.
See (Witten et al6  HashTBO FormatThe HashTBO format is basically the same as McI-lroy?s format, except that McIlroy was storingwords and we are storing n-grams.
One couldstore all of the n-grams in a single table, though weactually store unigrams in a separate table.
An n-gram is represented as a key of n integers (offsetsinto the vocabulary) and two values, a log likelih-ood and, if appropriate, an alpha for backing off.We?ll address the keys first.6.1 HashTBO KeysTrigrams consist of three integers (offsets intothe Vocabulary): ?1?2?3.
These three integers aremapped into a single hash between 0 and ?
?
1 inthe obvious way:????
=  ?3?0 +?2?1 +?1?2  mod ?where V is vocabulary size.
Bigrams are hashedthe same way, except that the vocabulary is paddedwith an extra symbol for NA (not applicable).
Inthe bigram case, ?3 is NA.We then follow a simple recipe for bigrams andtrigrams:1.
Stolcke prune appropriately2.
Let N be the number of n-grams3.
Choose an appropriate P (hash range)4.
Hash the N n-grams5.
Sort the hash codes6.
Take the first differences (which are mod-eled as interarrivals of a Poisson process)7.
Golomb code the first differencesWe did not use this method for unigrams, sincewe assumed (perhaps incorrectly) that we will haveexplicit likelihoods for most of them and thereforethere is little opportunity to take advantage ofsparseness.Most of the recipe can be fully automated with aturnkey process, but two steps require appropriatehand intervention to meet the memory allocationfor a particular application:1.
Stolcke prune appropriately, and2.
Choose an appropriate P1999) and http://en.wikipedia.org/wiki/Golomb_coding,for similar discussion, though with slightly differentnotation.
The primary reference is (Golomb, 1966).203Ideally, we?d like to do as little pruning as poss-ible and we?d like to use as large a P as possible,subject to the memory allocation.
We don?t have aprincipled argument for how to balance Stolckepruning losses with hashing losses; this can be ar-rived at empirically on an application-specific ba-sis.
For example, to fix the storage per n-gram ataround 13 bits:13 =1log?
2+ log21?If we solve for ?, we obtain 0000,20/1??
.
Inother words, set P to a prime near N000,20 andthen do as much Stolcke pruning as necessary tomeet the memory constraint.
Then measure yourapplication?s accuracy, and adjust accordingly.6.2 HashTBO Values and AlphasThere are N log likelihood values, one for eachkey.
These N values are quantized into a smallnumber of distinct bins.
They are written out as asequence of N Huffman codes.
If there are Katzbackoff alphas, then they are also written out as asequence of N Huffman codes.
(Unigrams andbigrams have alphas, but trigrams don?t.
)6.3 HashTBO LookupThe lookup process is given an n-gram,???2???1??
, and is asked to estimate a log likelih-ood, log Pr ??
???2??
?1) .
Using the standardbackoff model, this depends on the likelihoods forthe unigrams, bigrams and trigrams, as well as thealphas.The lookup routine not only determines if the n-gram is in the table, but also determines the offsetwithin that table.
Using that offset, we can find theappropriate log likelihood and alpha.
Side tablesare maintained to speed up random access.7 ZipTBO FormatZipTBO is a well-established representation oftrigrams.
Detailed descriptions can be found in(Clarkson and Rosenfeld 1997; Whittaker and Raj2001).ZipTBO consumes 8 bytes per unigram, 5 bytesper bigram and 2.5 bytes per trigram.
In practice,this comes to about 4 bytes per n-gram on average.Note that there are some important interactionsbetween ZipTBO and Stolcke pruning.
ZipTBO isrelatively efficient for trigrams, compared to bi-grams.
Unfortunately, aggressive Stolcke pruninggenerates bigram-heavy models, which don?t com-press well with ZipTBO.probs&weightsboundsBIGRAMidsprobs&weightsW[i-2]w[i-1]W[i-2]w[i-1]w[i]ids probsbounds2 1/2TRIGRAMUNIGRAMids2 1 22 2 4Figure 1.
Tree structure of n-grams in ZipTBOformat, following Whittaker and Ray (2001)7.1 ZipTBO KeysThe tree structure of the trigram model is im-plemented using three arrays.
As shown in Figure1, from left to right, the first array (called unigramarray) stores unigram nodes, each of whichbranches out into bigram nodes in the second array(bigram array).
Each bigram node then branchesout into trigram nodes in the third array (trigramarray).The length of the unigram array is determinedby the vocabulary size (V).
The lengths of the oth-er two arrays depend on the number of bigramsand the number of trigrams, which depends on howaggressively they were pruned.
(We do not pruneunigrams.
)We store a 2-byte word id for each unigram, bi-gram and trigram.The unigram nodes point to blocks of bigramnodes, and the bigram nodes point to blocks of tri-gram nodes.
There are boundary symbols betweenblocks (denoted by the pointers in Figure 1).
Theboundary symbols consume 4 bytes for each uni-gram and 2 bytes for each bigram.In each block, nodes are sorted by their wordids.
Blocks are consecutive, so the boundary value204of an n?1-gram node together with the boundaryvalue of its previous n?1-gram node specifies, inthe n-gram array, the location of the block contain-ing all its child nodes.
To locate a particular childnode, a binary search of word ids is performedwithin the block.Figure 3.
The differences between the methods inFigure 2 vanish if we adjust for prune size.7.2 ZipTBO Values and AlphasLike HashTBO, the log likelihood values andbackoff alphas are quantized to a small number ofquantization levels (256 levels for unigrams and 16levels for bigrams and trigrams).
Unigrams use afull byte for the log likelihoods, plus another fullbyte for the alphas.
Bigrams use a half byte for thelog likelihood, plus another half byte for the al-phas.
Trigrams use a half byte for the log likelih-ood.
(There are no alphas for trigrams.
)7.3 ZipTBO Bottom Line1.
8 bytes for each unigram:a.
2 bytes for a word id +b.
4 bytes for two boundary symbols +c.
1 byte for a log likelihood +d.
1 byte for an alpha2.
5 bytes for each bigram:a.
2 bytes for a word id +b.
2 bytes for a boundary symbol +c.
?
bytes for a log likelihood +d.
?
bytes for an alpha3.
2.5 bytes for each trigram:a.
2 bytes for a word id +b.
?
bytes for a log likelihood8 EvaluationWe normally think of trigram language modelsas memory hogs, but Figure 2 shows that trigramscan be squeezed down to a megabyte in a pinch.Of course, more memory is always better, but it issurprising how much can be done (27% recall at80% precision) with so little memory.Given a fixed memory budget, HashTBO out-performs ZipTBO which outperforms StdTBO, abaseline system with no compression.
Compres-sion matters more when memory is tight.
The gapbetween methods is more noticeable at the low end(under 10 megabytes) and less noticeable at thehigh end (over 100 megabytes), where both me-thods asymptote to the performance of the StdTBObaseline.All methods start with Stolcke pruning.
Figure3 shows that the losses are largely due to pruning.0.250.350.450.551 10 100 1000Recallat80%PrecisionPrune Size (MBs)HashTBO ZipTBO StdTBOFigure 2.
When there is plenty of memory, per-formance (recall @ 80% precision) asymptotes tothe performance of baseline system with no com-pression (StdTBO).
When memory is tight,HashTBO >> ZipTBO >> StdTBO.Figure 4.
On average, HashTBO consumes about3 bytes per n-gram, whereas ZipTBO consumes 4.0.250.350.450.551 10 100Recallat80%PrecisionMemory (MBs)HashTBO ZipTBO StdTBOy = 3E-06x - 0.0519y = 4E-06x + 1.5112024681012140 500,0001,000,0001,500,0002,000,0002,500,0003,000,0003,500,0004,000,0004,500,000NgramsMegabytesHashTBO ZipTBO205All three methods perform about equally well, as-suming the same amount of pruning.The difference is that HashTBO can store moren-grams in the same memory and therefore itdoesn?t have to do as much pruning.
Figure 4shows that HashTBO consumes 3 bytes per n-gramwhereas ZipTBO consumes 4.Figure 4 combines unigrams, bigrams and tri-grams into a single n-gram variable.
Figure 5 drillsdown into this variable, distinguishing bigramsfrom trigrams.
The axes here have been reversedso we can see that HashTBO can store more ofboth kinds in less space.
Note that both HashTBOlines are above both ZipTBO lines.Figure 5.
HashTBO stores more bigrams and tri-grams than ZipTBO in less space.In addition, note that both bigram lines areabove both trigram lines (triangles).
Aggressivelypruned models have more bigrams than trigrams!Linear regression on this data shows that Hash-TBO is no better than ZipTBO on trigrams (withthe particular settings that we used), but there is abig difference on bigrams.
The regressions belowmodel M (memory in bytes) as a function of bi andtri, the number of bigrams and trigrams, respec-tively.
(Unigrams are modeled as part of the inter-cept since all models have the same number of un-igrams.)????????
= 0.8 + 3.4??
+ 2.6??????????
= 2.6 + 4.9??
+ 2.6??
?As a sanity check, it is reassuring that ZipTBO?scoefficients of 4.9 and 2.6 are close to the true val-ues of 5 bytes per bigram and 2.5 bytes per tri-gram, as reported in Section 7.3.According to the regression, HashTBO is nobetter than ZipTBO for trigrams.
Both models useroughly 2.6 bytes per trigram.
When trigram mod-els have relatively few trigrams, the other coeffi-cients matter.
HashTBO uses less space for bi-grams (3.4 bytes/bigram << 4.9 bytes/bigram) andit has a better intercept (0.8 << 2.6).We recommend HashTBO if space is so tightthat it dominates other concerns.
However, if thereis plenty of space, or time is an issue, then the tra-deoffs work out differently.
Figure 6 shows thatZipTBO is an order of magnitude faster thanHashTBO.
The times are reported in microsecondsper n-gram lookup on a dual Xeon PC with a 3.6ghz clock and plenty of RAM (4GB).
These timeswere averaged over a test set of 4 million lookups.The test process uses a cache.
Turning off thecache increases the difference in lookup times.Figure 6.
HashTBO is slower than ZipTBO.9 ConclusionTrigram language models were compressedusing HashTBO, a Golomb coding methodinspired by McIlroy?s original spell program forUnix.
McIlroy used the method to compress adictionary of 32,000 words into a PDP-11 addressspace of 64k bytes.
That is just 2 bytes per word!We started with a large corpus of 6 billion wordsof English.
With HashTBO, we could compressthe trigram language model into just a couple ofmegabytes using about 3 bytes per n-gram(compared to 4 bytes per n-gram for the ZipTBObaseline).
The proposed HashTBO method is notfast, and it is not accurate (not lossless), but it ishard to beat if space is tight, which was the casefor the contextual speller in Microsoft Office 2007.0.00.20.40.60.81.01.21.41.61.82.00 5 10 15Memory (MB)Ngrams(Millions)HashTBO Bigrams HashTBO TrigramsZipTBO Bigrams ZipTBO Trigrams012345670 5 10 15Memory (MB)TimeHashTBO ZipTBO206AcknowledgmentsWe would like to thank Dong-Hui Zhang for hiscontributions to ZipTBO.ReferencesAshok K. Chandra, Dexter C. Kozen, and LarryJ.Stockmeyer.
1981 Alternation.
Journal of the Asso-ciation for Computing Machinery, 28(1):114-133.Church, K., and Gale, W. 1991 Probability Scoring forSpelling Correction, Statistics and Computing.Clarkson, P. and Robinson, T. 2001 Improved languagemodeling through better language model evaluationmeasures, Computer Speech and  Language, 15:39-53, 2001.Dan Gusfield.
1997 Algorithms on Strings, Trees andSequences.
Cambridge University Press, Cambridge,UKGao, J. and Zhang, M., 2002 Improving language modelsize reduction using better pruning criteria.
ACL2002: 176-182.Gao, J., Goodman, J., and Miao, J.
2001 The use ofclustering techniques for language modeling ?
appli-cation to Asian languages.
Computational Linguis-tics and Chinese Language Processing, 6:1, pp 27-60.Golding, A. R. and Schabes, Y.
1996 Combining Tri-gram-based and feature-based methods for context-sensitive spelling correction,  ACL, pp.
71-78.Golomb, S.W.
1966 Run-length encodings IEEE Trans-actions on Information Theory, 12:3, pp.
399-40.Goodman, J. and Gao, J.
2000 Language model sizereduction by pruning and clustering, ICSLP-2000,International Conference on Spoken LanguageProcessing, Beijing, October 16-20, 2000.Mays, E., Damerau, F. J., and Mercer, R. L. 1991 Con-text based spelling correction.
Inf.
Process.
Manage.27, 5 (Sep. 1991), pp.
517-522.Katz, Slava, 1987 Estimation of probabilities fromsparse data for other language component of aspeech recognizer.
IEEE transactions on Acoustics,Speech and Signal Processing,  35:3, pp.
400-401.Kukich, Karen, 1992 Techniques for automatically cor-recting words in text, Computing Surveys, 24:4, pp.377-439.M.
D. McIlroy, 1982 Development of a spelling list,IEEE Trans.
on Communications 30 pp.
91-99.Seymore, K., and Rosenfeld, R. 1996 Scalable backofflanguage models.
Proc.
ICSLP, Vol.
1, pp.232-235.Stolcke, A.
1998 Entropy-based Pruning of Backoff Lan-guage Models.
Proc.
DARPA News Transcription andUnderstanding Workshop, 1998, pp.
270--274, Lans-downe, VA.Whittaker, E. and Ray, B.
2001 Quantization-based lan-guage model compression.
Proc.
Eurospeech, pp.33-36.Witten, I. H., Moffat, A., and Bell, T. C. 1999 Manag-ing Gigabytes (2nd Ed.
): Compressing and IndexingDocuments and Images.
Morgan Kaufmann Publish-ers Inc.207
