Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 77?82,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsChunking Clinical Text Containing Non-Canonical LanguageAleksandar SavkovDepartment of InformaticsUniversity of SussexBrighton, UKa.savkov@sussex.ac.ukJohn CarrollDepartment of InformaticsUniversity of SussexBrighton, UKj.a.carroll@sussex.ac.ukJackie CassellPrimary Care and Public HealthBrighton and Sussex Medical SchoolBrighton, UKj.cassell@bsms.ac.ukAbstractFree text notes typed by primary carephysicians during patient consultationstypically contain highly non-canonicallanguage.
Shallow syntactic analysis offree text notes can help to reveal valu-able information for the study of diseaseand treatment.
We present an exploratorystudy into chunking such text using off-the-shelf language processing tools andpre-trained statistical models.
We evalu-ate chunking accuracy with respect to part-of-speech tagging quality, choice of chunkrepresentation, and breadth of context fea-tures.
Our results indicate that narrow con-text feature windows give the best results,but that chunk representation and minordifferences in tagging quality do not havea significant impact on chunking accuracy.1 IntroductionClinical text contains rich, detailed information ofgreat potential use to scientists and health serviceresearchers.
However, peculiarities of languageuse make the text difficult to process, and the pres-ence of sensitive information makes it hard to ob-tain adequate quantities for developing processingsystems.
The short term goal of most researchin the area is to achieve a reliable language pro-cessing foundation that can support more complextasks such as named entity recognition (NER) to asufficiently reliable level.Chunking is the task of identifying non-recursive phrases in text (Abney, 1991).
It is atype of shallow parsing that is a less challeng-ing task than dependency or constituency parsing.This makes it likely to give more reliable results onclinical text, since there is a very limited amount ofannotated (or even raw) text of this kind availablefor system development.
Even though chunkingdoes not provide as much syntactic information asfull parsing, it is an excellent method for identify-ing base noun phrases (NP), which is a key issuein symptom and disease identification.
Identify-ing symptoms and diseases is at the heart of har-nessing the potential of clinical data for medicalresearch purposes.There are few resources that enable researchersto adapt general domain techniques to clinical text.Using the Harvey Corpus1?
a chunk annotatedclinical text language resource ?
we present an ex-ploratory study into adapting general domain toolsand models to apply to free text notes typed by UKprimary care physicians.2 Related WorkThe Mayo Clinic Corpus (Pakhomov et al., 2004)is a key resource that has been widely used asa gold standard in part-of-speech (POS) taggingof clinical text.
Based on that corpus and thePenn TreeBank (Marcus et al., 1993), Coden et al.
(2005) present an analysis of the effects of domaindata on the performance of POS tagging mod-els, demonstrating significant improvements withmodels trained entirely on domain data.
Savovaet al.
(2010) use this corpus for the developmentof cTAKES, Mayo Clinic?s processing pipeline forclinical text.Fan et al.
(2011) show that using more diverseclinical data can lead to more accurate POS tag-ging.
They report that models trained on clinicaltext datasets from two different institutions per-form on each of the datasets better than both mod-els trained only on the same or the other dataset.Fan et al.
(2013) present guidelines for syntac-tic parsing of clinical text and a clinical Treebankannotated according to them.
The guidelines aredesigned to help the annotators handle the non-canonical language that is typical of clinical text.1An article describing the corpus is currently under re-view.773 DataThe Harvey Corpus is a chunk-annotated corpusconsisting of pairs of manually anonymised UKprimary care physician (General Practitioner, orGP) notes and associated Read codes (Bentley etal., 1996).
Each Read code has a short textualgloss.
The purpose of the codes is to make it easyto extract structured data from clinical records.The reason we include the codes in the corpus isthat GPs often use their glosses as the beginning oftheir note.
Two typical examples (without chunkannotation for clarity) are shown below.Birth details | | Normal deliviery GirlWeight - 3.
960kg Apgar score @ 1min- 9 Apgar score @ 5min - 9 Vit K givenPaed check NAD HC - 34.
9cm Hip testperformed(1)Chest pain | | musculoskel pain last w/e,nil to find, ecg by paramedic no change,reassured, rev sos(2)The corpus comprises 890 pairs of Read codesand notes, each annotated by medical experts us-ing a chunk annotation scheme that includes non-recursive noun phrases (NPs), main verb groups(MVs), and a common annotation for adjectivaland adverbial phrases (APs).
Example (3) be-low illustrates the annotation.
The majority ofthe records (750) were double blind annotated bymedical experts, after which the resulting annota-tion was adjudicated by a third medical expert an-notator.
[Chest pain]NP| | [musculoskel pain]NP[last w/e]NP, [nil]APto [find]MV, [ecg]NPby [paramedic]NP[no change]NP,[reassured]MV, [rev]MV[sos]AP(3)Inter-annotator agreement was 0.86 f-score, tak-ing one annotator to be the gold standard and theother the candidate.
We calculate the f-score ac-cording to the MUC-7 (Chinchor, 1998) specifica-tion, with the standard f-score formula.
The calcu-lation is kept symmetric with regard to the choiceof gold standard annotator by limiting the countingof incorrect categories to one per tag, and equat-ing the missing and spurious categories.
For ex-ample, three words annotated as one three-tokenchunk by annotator A and three one-token chunksby annotator B will have one incorrect and twomissing/spurious elements.The rest of the records are a by-product of thetraining process.
Ninety records were triple anno-tated by three different medical experts with thehelp of a computational linguist, and fifty recordswere double annotated by a medical expert ?
aloneand together with a computational linguist.It is important to note that the text in the corpusis not representative of all types of GP notes.
It isfocused on text that represents the dominant partof day-to-day notes, rather than standard editedtext such as copies of letters to specialists andother medical practitioners.Even though the corpus data is very rich in in-formation, its non-canonical language means thatit is very different from other clinical corporasuch as the Mayo Clinic Corpus (Pakhomov et al.,2004) and poses different challenges for process-ing.
The GP notes in the Harvey Corpus can beregarded as groups of medical ?tweets?
meant tobe used mainly by the author.
Sentence segmenta-tion in the classical sense of the term is often im-possible, because there are no sentences.
Insteadthere are short bursts of phrases concatenated to-gether often without any indication of their bound-aries.
The average length of a note is roughly 30tokens including the Read code.
This is in con-trast to notes in other clinical text datasets, whichrange from 100 to 400 tokens on average (Fan etal., 2011; Pakhomov et al., 2004).
As well as typ-ical clinical text characteristics such as domain-specific acronyms, slang, and abbreviations, punc-tuation and casing are often misleading (if presentat all), and some common classes of words (e.g.auxiliary verbs) are almost completely absent.4 ChunkingState-of-the-art text chunking accuracy reaches anf-score of 95% (Sun et al., 2008).
However, thisis for standard, edited text, and relies on accuratePOS tagging in a pre-processing step.
However,the characteristics of GP-written free text make ac-curate part of speech (POS) tagging and chunkingdifficult.
Major problems are caused by unknowntokens and ambiguities due to omitted words orphrases.We evaluate two standard chunking tools, Yam-Cha (Kudo and Matsumoto, 2003) and CRF++2,selected based on their support for trainable con-text features.
The tools were applied to the Har-2http://crfpp.googlecode.com/svn/trunk/doc/index.html78POS YamCha IOB YamCha BEISO CRF++ IOB CRF++ BEISOARKIRC75.35 76.63 ?1.04 76.87 ?2.91 75.87 ?1.64 76.23 ?1.99ARKTwitter?
76.72 ?2.11 77.53 ?1.65 76.63 ?2.36 77.23 ?1.06ARKRitter75.70 76.59 ?2.01 76.72 ?2.11 76.63 ?1.05 77.17 ?1.77cTAKES 82.42 75.32 ?2.52 75.85 ?2.02 75.43 ?1.79 75.53 ?1.90GENIA 80.63 71.70 ?2.27* 74.86 ?1.41 74.16 ?2.03* 74.19 ?1.72RASP ?
74.24 ?1.84 75.10 ?1.31 75.63 ?2.33 75.76 ?2.18Stanford 80.68 76.40 ?1.69 76.36 ?2.92 75.95 ?1.25 75.94 ?1.91SVMTool 76.40 74.32 ?2.57 74.30 ?2.71 74.66 ?1.77 74.68 ?2.28Wapiti 73.39 74.74 ?2.29 74.78 ?1.33 73.59 ?2.62 73.83 ?2.31baseline ?
69.66 ?1.89* 69.76 ?1.24 67.05 ?1.15* 68.65 ?1.41Table 1: Chunking results using YamCha and CRF++ on data automatically POS tagged using ninedifferent models; the baseline is with no tagging.
The IOB and BEISO columns compare the impactof two chunk representation strategies.
The POS column indicates the part-of-speech tagging accuracyfor a subset of the corpus.
Asterisks indicate pairs of significantly different YamCha and CRF++ results(t-test with 0.05 p-value).vey Corpus with automatically generated POS an-notation.
Given the small amount of data andthe challenges presented above, we expected thatour results would be lower than those reported bySavova et al.
(2010).
The aim of these experi-ments is to find the best performance obtainablewith standard chunking tools, which we will buildon in further stages of our research.We conducted pairs of experiments, one witheach chunking tool, divided into three groups: thefirst investigates the effects of choice of POS tag-ger for training data annotation (Section 4.1); thesecond compares two chunk representations (Sec-tion 4.2); and the third searches for the optimalcontext features (Section 4.3).
All feature tuningexperiments were conducted on a development setand tested using 10-fold cross-validation on therest of the data.
We used 10% of the whole datafor the development set and 90% of the remain-ing data for a training sample during development.This guarantees the development model is trainedon the same amount of data as the testing model.4.1 Part-of-Speech TaggingWe evaluated and compared the results yieldedby the two chunkers, having applied each ofseven off-the-shelf POS taggers.
Of these tag-gers, cTAKES (Savova et al., 2010) and GENIA(Tsuruoka et al., 2005) are the only ones trainedon data that resembles ours, which suggests thatthey should have the best chance of performingwell.
We also selected a number of other taggerswhile trying to diversify their algorithms and train-ing data as much as possible: the POS tagger partof the Stanford NLP package (Toutanova et al.,2003) because it is one of the most successfullyapplied in the field; the RASP tagger (Briscoeet al., 2006) because of its British National Cor-pus (Clear, 1993) training data; the ARK tagger(Owoputi et al., 2013) because of the terseness ofthe tweet language; and the SVMTool (Gim?enezand M`arquez, 2004) and Wapiti (Lavergne et al.,2010) because they use SVM and CRF algorithms.Our baseline model uses no part of speech infor-mation.Using the Penn TreeBank tagset (Marcus et al.,1993), we manually annotated a subset of the cor-pus of comparable size to the development set.
Us-ing this dataset we estimated the tagging accuracyfor all models that support that tagset (omittingRASP and ARK Twitter since they use differenttagsets).
In this evaluation, cTAKES is the bestperforming model, followed closely by the Stan-ford POS tagger and GENIA.The results in Table 1 show that the differ-ences between chunking models trained on differ-ent POS annotations are small and mostly not sta-tistically significant from each other.
However, allthe results are significantly better than the base-line, apart from those based on the GENIA taggeroutput.4.2 Chunk RepresentationThe dominant chunk representation standard in-side, outside, begin (IOB) introduced by Ramshawand Marcus (1995) and established with the79CoNLL-2000 shared task (Sang and Buchholz,2000) takes a minimalistic approach to the rep-resentation problem in order to keep the numberof labels low.
Note that for chunking representa-tions the total number of labels is the product ofthe chunk types and the set of representation typesplus the outside tag, meaning that for IOB withour set of three chunk types (NP, MV, AP) thereare seven labels.Alternative chunk representations, such as be-gin, end, inside, single, outside (BEISO)3as usedby Kudo and Matsumoto (2001), offer more fine-grained tagsets, presumably at a performance cost.That cost is unnecessary unless there is somethingto be gained from a more fine-grained tagset at de-coding time, because the two representations aredeterministically inter-convertible.
For instance,an end tag could be useful for better recognisingboundaries between chunks of the same type.
TheBEISO tagset model looks for the boundary be-fore and after crossing it, while an IOB modelonly looks after.
This should give only a smallgain with standard edited text because the chunktype distribution is fairly well balanced and punc-tuation divides ambiguous cases such as lists ofcompound nouns.
However, the Harvey Corpusis NP-heavy and contains many sequences of NPchunks that do not have any punctuation to marktheir boundaries.We evaluated the two chunk representations incombination with each POS tagger.
Table 1 showsthat the differences between the results for thetwo representations are small and never statisti-cally significant.
We also evaluated the two chunkrepresentations with different amounts of trainingdata.
The resulting learning curves (Figure 1) arealmost identical.4.3 Context FeaturesWe approached the feature tuning task by first ex-ploring the smaller feature space of YamCha andthen using the trends there to constrain the fea-tures of CRF++.
YamCha has three groups of fea-tures responsible for tokens, POS tags and dynam-ically generated (i.e.
preceding) chunk tags.
Forall experiments we determined the best feature setby exhaustively testing all context feature combi-nations within a predefined range.
We used thesame context window for the token and tag fea-tures in order to reduce the search space.
Given3Also sometimes abbreviated IOBSEFeature Set CV DevW-1-W1, T-1-T1, C-177.28 ?1.9 75.28W-1-W1, T-1-T1, C-2-C-177.27 ?2.6 74.70W-1-W2, T-1-T2, C-176.86 ?1.5 74.08W-2-W1, T-2-T1, C-276.46 ?1.3 74.00W-1-W1, T-1-T1, C-276.89 ?2.1 73.92W-2-W1, T-2-T1, C-3-C-176.52 ?0.9 73.91W-1-W1, T-1-T1, C-3-C-177.02 ?2.0 73.90W-2-W2, T-2-T2, C-177.03 ?1.9 73.86W-1-W1, T-1-T1, C-377.15 ?1.5 73.63W-3-W1, T-3-T1, C-2-C-175.71 ?1.9 73.63Table 2: Development set and 10-fold cross-validation results for the top ten feature sets ofYamCha models trained on ARKTwitterPOS an-notation.
Token features are represented withW, POS features with T, and dynamically gener-ated chunk features with C. None of the cross-validation results are significantly different fromeach other (t-test with 0.05 p-value).the terseness of the text we expected that widercontext windows of more than three tokens wouldnot be beneficial to the model, and therefore didnot consider them.
Our experiments using Yam-Cha confirmed this hypothesis and showed a con-sistent trend among all experiments in favouring awindow of -1 to +1 for tokens and slightly widerfor chunk tags (see Table 2).CRF++ provides a more powerful feature con-figuration allowing for unary and pairwise4fea-tures of output tags.
The unary features allow theconstruction of token or POS tag bigrams and tri-grams in addition to the standard context windows.The feature tuning search space with so many pa-rameters is enormous, which required us to use ourfindings from the YamCha experiments to trim itdown and make it computationally feasible.
First,we decreased the search window of all features byone in each direction from -3:3 to -2:2.
Second, weused the top scoring POS model from the first ex-perimental runs to constrain the features even fur-ther by selecting only the top one hundred for therest of the models.We could not identify the same uniform trend inthe top feature sets as we could with YamCha.
Ourresults ranged from very small context windowsto the maximum size of our search space.
How-4The unary and pairwise features of output tags are re-ferred to as unigram and bigram features of output tags onthe CRF++ web page.
Although this is correct, it can alsobe confused with unigrams and bigrams of tokens, which areexpressed as unary (unigram) output tag features.8055606570758020 80 140 200 260 320 380 440 500 560 620 680 740 800IOB BEISOFigure 1: Chunking results for YamCha IOB andBEISO models with increasing amounts of train-ing data.ever, we noticed that BEISO feature sets tend tobe smaller than the IOB ones.
We also found thatthe pairwise features normally improve the results.5 Discussion and Future WorkWe were surprised that the experiments did notshow a clear correlation between POS tagging ac-curacy and chunking accuracy.
On the other hand,the chunking results using POS tagged data aresignificantly better than the baseline, except whenusing the GENIA tagger output.
The small dif-ferences between training sets of similar POS ac-curacy could be explained due to the non-uniformimpact of the wrong POS tag on the chunking pro-cess.
Some mistakes such as labelling a noun asa verb in the middle of a NP chunk are almostsure to propagate and cause further chunking er-rors, whereas others may have minimal or no ef-fect, for example labelling a singular noun as aproper noun.
An error analysis of verb tags andnoun tags (Table 3) shows that the ARK modelstend to make more mistakes that keep the anno-tation within the same tag group compared to theGENIA model (see column pairs 1 and 3, and 2and 4).
This is a possible explanation for the loweraccuracy of the chunking model trained on datatagged by GENIA.Our experiments showed that the models usingthe two chunk representations did not perform sig-nificantly differently from each other.
We alsoshowed that this conclusion is likely to hold ifModel NgroupVgroupNouns VerbsARKIRC67.17 78.26 88.26 85.99ARKTwitter- - 86.97 88.71ARKRitter68.57 77.29 90.64 85.02cTAKES 83.93 62.80 93.85 69.08GENIA 81.56 61.83 92.03 71.01RASP - - 84.59 83.58Stanford 80.30 73.42 91.89 83.09SVMTool 69.97 70.04 90.08 80.19Wapiti 65.64 66.66 87.84 74.87Table 3: Detailed view of the POS model re-sults focusing on the noun and verb tag groups.The leftmost two columns of figures show accura-cies over tags in the respective groups; the right-most two columns show the accuracies of the samegroups if all tags in a group are replaced with agroup tag, e.g.
V for verbs5.more training data were available.There are a number of ways we could improvechunking accuracy besides increasing the amountof training data.
Although our results do not showa clear trend, Fan et al.
(2011) demonstrate that thedomain of part-of-speech training data has a sig-nificant impact on tagging accuracy, which couldpotentially impact chunking results if it decreasesthe number of errors that propagate during chunk-ing.
An important problem in that area is dealingwith present and past participles, which are almostsure to cause error propagation if mislabelled (asnouns or adjectives, respectively).
Participles aremore ambiguous in terse contexts lacking auxil-iary verbs, which are natural disambiguation indi-cators.
Another direction in processing that couldcontribute to better chunking is better token andsentence segmentation.
Finally, unknown words,which may potentially have the largest impact onchunking accuracy, could be dealt with using ageneric solution such as feature expansion basedon distributional similarity.ReferencesS.
Abney.
1991.
Parsing by chunks.
In Robert C.Berwick, Steven P. Abney, and Carol Tenny, editors,Principle-Based Parsing: Computation and Psy-cholinguistics, pages 257?278.
Kluwer, Dordrecht.T.
Bentley, C. Price, and P. Brown.
1996.
Structuraland lexical features of successive versions of the5Note that these results are different from what would beyielded by a classifier trained on data subjected to the sametag substitution.81read codes.
In Proceedings of the Annual Confer-ence of The Primary Health Care Specialist Groupof the British Computer Society, pages 91?103.T.
Briscoe, J. Carroll, and R. Watson.
2006.
The sec-ond release of the RASP system.
In Proceedings ofthe COLING/ACL on Interactive Presentation Ses-sions, COLING-ACL?06, pages 77?80, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.N.
Chinchor.
1998.
Appendix B: Test scores.
InProceedings of the Seventh Message UnderstandingConference (MUC-7), Fairfax, VA, April.J.
Clear.
1993.
The British National Corpus.
InGeorge P. Landow and Paul Delany, editors, TheDigital Word, pages 163?187.
MIT Press, Cam-bridge, MA, USA.A.
Coden, S. Pakhomov, R. Ando, P. Duffy, andC.
Chute.
2005.
Domain-specific language mod-els and lexicons for tagging.
Journal of BiomedicalInformatics, 38:422?430.J.-W.
Fan, R. Prasad, R.M.
Yabut, R.M.
Loomis, D.S.Zisook, J.E.
Mattison, and Y. Huang.
2011.
Part-of-speech tagging for clinical text: Wall or bridge be-tween institutions?
In American Medical Informat-ics Association Annual Symposium, 1, pages 382?391.
American Medical Informatics Association.J.-W.
Fan, E. Yang, M. Jiang, R. Prasad, R. Loomis,D.
Zisook, J. Denny, H. Xu, and Y. Huang.
2013.Research and applications: Syntactic parsing of clin-ical text: guideline and corpus development withhandling ill-formed sentences.
JAMIA, 20(6):1168?1177.J.
Gim?enez and L. M`arquez.
2004.
SVMTool: A gen-eral POS tagger generator based on support vectormachines.
In Proceedings of the 4th LREC, Lisbon,Portugal.T.
Kudo and Y. Matsumoto.
2001.
Chunking with sup-port vector machines.
In Proceedings of the SecondMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics on LanguageTechnologies, NAACL?01, pages 1?8, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.T.
Kudo and Y. Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 24?31, Morristown, NJ,USA.
Association for Computational Linguistics.T.
Lavergne, O. Capp?e, and F. Yvon.
2010.
Practi-cal very large scale CRFs.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 504?513.
Association forComputational Linguistics, July.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.O.
Owoputi, B. O?Connor, C. Dyer, K. Gimpel,N.
Schneider, and N. Smith.
2013.
Improvedpart-of-speech tagging for online conversational textwith word clusters.
In Proceedings of NAACL-HLT,pages 380?390.S.
Pakhomov, A. Coden, and C. Chute.
2004.
Creat-ing a test corpus of clinical notes manually taggedfor part-of-speech information.
In Proceedings ofthe International Joint Workshop on Natural Lan-guage Processing in Biomedicine and Its Applica-tions, JNLPBA?04, pages 62?65, Stroudsburg, PA,USA.
Association for Computational Linguistics.L.
Ramshaw and M. Marcus.
1995.
Text chunking us-ing transformation-based learning.
In Proceedingsof the Third Workshop on Very Large Corpora, pages82?94.E.
Sang and S. Buchholz.
2000.
Introduction to theCoNLL-2000 shared task: Chunking.
In Proceed-ings of the 2nd Workshop on Learning Languagein Logic and the 4th Conference on ComputationalNatural Language Learning, pages 13?14.G.
Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,K.
Kipper-Schuler, and C. Chute.
2010.
Mayo clin-ical text analysis and knowledge extraction system(cTAKES): architecture, component evaluation andapplications.
Journal of the American Medical In-formatics Association, 17(5):507?513.X.
Sun, L.-P. Morency, D. Okanoharay, and J. Tsujii.2008.
Modeling latent-dynamic in shallow parsing:A latent conditional model with improved inference.In Proceedings of the 22nd International Confer-ence on Computational Linguistics (COLING?08),Manchester, UK, August.K.
Toutanova, D. Klein, C. D. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proceedings ofthe 2003 Conference of the North American Chap-ter of the Association for Computational Linguis-tics on Human Language Technology - Volume 1,NAACL?03, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Y.
Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-Naught, S. Ananiadou, and J. Tsujii.
2005.
Devel-oping a robust part-of-speech tagger for biomedicaltext.
In Proceedings of the 10th Panhellenic Con-ference on Advances in Informatics, PCI?05, pages382?392, Berlin, Heidelberg.
Springer-Verlag.82
